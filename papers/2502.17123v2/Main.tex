\documentclass[preprint,12pt]{elsarticle}
%\documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols

\usepackage{amssymb,amsthm,mathtools,mathabx,mathrsfs} % AMS packages
\usepackage[cal=boondoxo]{mathalfa} % for special \mathcal{r}
\usepackage{hyperref,url} % for hyperlink

\usepackage{slashed}
\usepackage{stmaryrd}
\usepackage{comment}
%\usepackage{bm,bbm}
%\usepackage{bbold}
%\usepackage[cal=boondoxo]{mathalfa}%per mathcal minuscolo
\usepackage{fancyvrb}
\biboptions{numbers}
\usepackage{setspace}
\usepackage{xcolor,color} %per usare i colori
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage[normalem]{ulem}




%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Journal of Computational and Applied Mathematics}

% *** Flavia's comment
\newcommand{\fec}[1]{{\color{blue} (\textbf{FE:} #1)}}
\newcommand{\fei}[1]{{{\color{blue} #1}}}

\newcommand{\lac}[1]{{\color{red} (\textbf{LA:} #1)}}
\newcommand{\lai}[1]{{{\color{red} #1}}}



\input{MathSymbols}

\newtheorem{remark}{Remark}

\DeclareMathOperator*{\AlgoName}{\texttt{SHINBO}}  %%%%<------------!!!!!!!!!!!!

\begin{document}
\begin{frontmatter}
\title{Sparse Hyperparametric Itakura-Saito NMF 
\\ via Bi-Level Optimization}

\author[inst1,inst2]{Laura Selicato}
\author[inst2]{Flavia Esposito\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{flavia.esposito@uniba.it}
\author[inst3]{Andersen Ang}
\author[inst2]{Nicoletta Del Buono}
\author[inst4]{Rafa\l $ $ Zdunek}

\affiliation[inst1]{organization={Water Research Institute – National Research Council (IRSA-CNR)},
            %addressline={Viale Francesco de Blasio, 5},
            %city={Bari},
            %postcode={70132}, 
            country={Italy}}

\affiliation[inst2]{organization={Department of Mathematics, University of Bari Aldo Moro},
            %addressline={Via Orabona 4},
            %city={Bari},
            %postcode={70125}, 
            country={Italy}}

\affiliation[inst3]{organization={School of Electronics and Computer Science, University of Southampton},
            %city={Southampton},
            %postcode={70132}, 
            country={UK}}

\affiliation[inst4]{organization={Faculty of Electronics, Photonics, and Microsystems, Wroc\l aw University of Science and Technology}, 
            %addressline={27 Wybrzeze Wyspianskiego st.},
            %city={Wroclaw},
            %postcode={50370}, 
            country={Poland}
            }



\begin{abstract}
The selection of penalty hyperparameters is a critical aspect in Nonnegative Matrix Factorization (NMF), since these values control the trade-off between the reconstruction accuracy and the adherence to desired constraints. 
In this work, we focus on an NMF problem involving the Itakura-Saito (IS) divergence, effective for extracting low spectral density components from spectrograms of mixed signals, enhanced with sparsity constraints. 
We propose a new algorithm called SHINBO, which introduces a bi-level optimization framework to automatically and adaptively tune the row-dependent penalty hyperparameters, enhancing the ability of IS-NMF to isolate sparse, periodic signals against noise.
Experimental results showed SHINBO ensures precise spectral decomposition and demonstrates superior performance in both synthetic and real-world applications.
For the latter, SHINBO is particularly useful, as noninvasive vibration-based fault detection in rolling bearings, where the desired signal components often reside in high-frequency subbands but are obscured by stronger, spectrally broader noise.
By addressing the critical issue of hyperparameter selection, SHINBO advances the state-of-the-art in signal recovery for complex, noise-dominated environments. 
\end{abstract}

\begin{keyword}
NMF,
Itakura-Saito divergence,
Hyperamater Optimization,
Sparsity,
Bi-level Optimization,
Dynamical System
\end{keyword}


\end{frontmatter}
\section{Introduction}
Nonnegative Matrix Factorization (NMF) is a dimensionality reduction technique that approximates a nonnegative data matrix as the product of two (lower-dimensional) nonnegative matrices. 
A key challenge in NMF is setting penalty coefficients when additional constraints, such as sparsity or smoothness, are imposed \cite{gillis2020nonnegative}. 
These coefficients control the trade-off between reconstruction accuracy and constraint adherence, but their optimal values are highly dataset- and application-dependent, making the selection process non-trivial. For example, \cite{corsetti2020nonnegative} proposes a variant of NMF that incorporates data-dependent penalties and introduces auxiliary constraints to enhance performance in tasks such as face recognition. Additionally, \cite{lanteri2011nonnegative} presents multiplicative algorithms for NMF that enforce non-negativity and flow preservation constraints while introducing regularizations to ensure smoothness or sparsity. Finally, \cite{fabregat2019solving} adapts a minimization scheme for functions with non-differentiable constraints, known as PALM, to solve NMF problems, yielding solutions that can be both smooth and sparse—two highly desirable properties. In this work, we rely on prior studies \cite{del2023bi,del2025penalty} in which the penalized problem is reformulated in a general form, and a strategy is proposed to tune the penalty coefficient automatically.

Formally, let $\X \in \IRmn_+$ with $m,n \in \IN$ be a data matrix, NMF aims to approximate it as the product of $\W\in\IRmr_+$ and $\H\in\IRrn_+$ with $r \leq \min\{m,n\}$, so that $\X\approx \W\H$. 
In our problem, we want to solve
\begin{equation}\label{min_prob}
(\W^*,\H^*)
~\in
    \displaystyle\argmin_{\substack{\W \geq \zeros, \H \geq \zeros  }} 
    D_{\beta}(\X, \W\H) 
    + \mathcal{P}\big(
    \Diag(\blambda)\H
    \big) 
\end{equation}
with the objective function $D_\beta(\cdot,\cdot)$ being the $\beta$-divergence \cite{fevotte2011algorithms,kompass2007generalized}, assessing  the quality of the reconstruction $\W\H$ in fitting $\X$. 
We remind that the $\beta$-divergence for matrices is defined as $D_\beta(\A,\B)=\sum_i{\sum_j{d_\beta(a_{ij},b_{ij})}}$, where the function $d_\beta$ for each $x,y \in \IR$ is defined as
\[
d_{\beta}(x,y)= \left \{
\begin{array}{lc}
\frac{1}{\beta(\beta-1)}(x^{\beta}+(\beta-1) y^{\beta}-\beta xy^{\beta-1}) &\beta\in \IR \setminus \{0,1\};
\vspace{2mm}
\\
x\log(\frac{x}{y})-x+y &\beta=1;
\vspace{2mm}
\\
\frac{x}{y}-\log(\frac{x}{y})-1 &\beta=0.
\end{array}
\right.
\]
The function $\cP : \IRrn \to \IR$ is a penalty term on $\H$ that enforces a particular constraint.
The vector $\blambda \in \IRr_+$ in \eqref{min_prob} contains a nonnegative penalty hyperparameters $[\lambda_1, \dots, \lambda_r]$ associated to each row of $\H$. 

In this study, we are interested in a special case of the $\beta$-divergence. 
When $\beta = 0$, the $\beta$-divergence boils down to the Itakura-Saito (IS) divergence which is a scale-invariant measure of dissimilarity, useful for extracting low spectral density components from spectrograms of mixed signals. 
In many practical applications, the desired components are located in a high frequency subband and are often masked by much stronger and spectrally wider noisy disturbances. 
This is the case of a noninvasive vibration-based fault detection in rolling bearings \cite{wodecki2019novel,wodecki2019impulsive,wodecki2020separation}. 
The vibrational diagnostic signals measured from faulty bearings on the laboratory test rig \cite{GABOR2023110430} are used in our study to validate the proposed algorithm. 

The signal of interest (SOI) in this application is represented by a periodic and impulsive signal. 
The observed signal should contain the SOI and other perturbations, usually expressed by a strong independent and identically distributed (i.i.d.) Gaussian noise. 
Applying NMF to the spectrogram of the measured signal, we expect that the representative of the SOI will be expressed by one of the temporal components of NMF, i.e., one row of matrix $\H$. 
This component, which presents a periodic spiky signal, is obviously very sparse. 
The other components, which represent the noisy perturbations, should not be sparse. 
To enforce NMF to search for the desired component, we introduced the penalty for the rows of $\H$ using the term $\cP(\Diag(\blambda)\H)$ in (\ref{min_prob}).
The proposed bi-level approach should perform data-driven adaptation of the hyperparameters (vector $\blambda$) to the desired nature of the estimated components. 

%{\color{blue} The hyperparameter that corresponds to the SOI should be the largest, and the others should be relatively low to penalize the sparseness in the noisy components.  } 
%\lac{Prof. Zdunek, in the experiment, the algorithm penalizes noise more than signal. Can this be considered correct based on what is written here?}




\paragraph{\textbf{Contribution}}
The contribution of this work is two folded.
\begin{enumerate}
\item \textbf{New model}.
In this work, we present a new model for minimizing the Itakura-Saito divergence (Problem \eqref{min_prob}  with $\beta=0$) while penalizing rows of $\H$.
In particular
\begin{itemize}
    \item The penalty hyperparameter is not known in advance.
    This parameter is formulated as an optimization variable (in form of bi-level optimization model) and is solved by a bi-level optimization method.

    \item The penalty hyperparameter is row-dependent.
    Note that Problem~\eqref{min_prob} is not the standard penalized NMF \cite{gillis2020nonnegative}, which applies the same penalty coefficient to all rows in the matrix $\H$.
    In Problem~\eqref{min_prob} each row of $\H$ is penalized by its own penalty parameter.
\end{itemize}

\item \textbf{New algorithm}.
For Problem~\eqref{min_prob}, we present a new multiplicative update (see Equation~\eqref{H_up_fro_J_row}), and a way to automatically tune the penalty hyperparameter based on bi-level strategy (see details in Section~\ref{sec:bilevel_general}). 
\end{enumerate}




\paragraph{\textbf{Paper Organization}}
We introduce the problem and overall algorithm framework in Section~\ref{sec:overall}.
In Section~\ref{sec:bilevel_general} we first review the bi-level optimization, then we discuss the details of the bi-level approach proposed in this work for solving the Problem~\ref{min_prob} in Section \ref{sec:algo}.
Experimental results on synthetic and real datasets are presented in Section~\ref{sec:exp}.
We conclude the paper in Section~\ref{sec:conl}, giving an outline of possible future directions.



\paragraph{\textbf{Notation}}
The symbol $\zeros$ denote the zero matrix, the symbol $\E_{a\times b}$ is all-one matrix sizing $a\times b$ with $a,b\in\IN$.
The notation $\v$ denotes a column vector and the notation $\underline{\v}$ means $\v$ is a row vector.

On matrix, $\A^\top$ is the transpose of $\A$, and $\A^2 = \A\A$.
The symbol $\A \odot \B$ refers to the Hadamard (element-wise) product between $\A$ and $\B$ of conformal dimensions, and the symbol $\A \oslash \B$ with $\B \neq \zeros$ refers to the Hadamard division, and
we denote $\A^{\odot k}$ as the Hadamard power-$k$ of $\A$.

Given $n \in \IN$, we denote $[n] \coloneqq \{1,2,\ldots,n\}$.
The symbols $k,t \in \IN$ indicate iteration counters.
The symbol $\A^{k}$ refers to the variable $\A$ at the iteration $k$, $A_{ij}$ or $a_{ij}$ is the $(i,j)$-th element of $\A$.
Lastly $\A_{i:}$ and $\A_{:j}$ are the $i$-th and $j$-th row and column of $\A$, respectively.
 

\section{The overall optimization framework of $\AlgoName$}\label{sec:overall}
In this section, we discuss the main focus of the paper, Problem~\eqref{min_prob} and the overall framework of the proposed algorithm.

\paragraph{\textbf{The optimization problem}}
We focus on Problem~\eqref{min_prob} with $\beta = 0$. As $\cP$ we chose a particular penalty function, effective in increasing sparsity, that is the diversity measure $J$ \cite{cotter2005sparse,del2025penalty}. % $J$ defined as
In a particular case, if the matrix is nonnegative, the diversity measure can be written in terms of the trace operator as

\begin{equation}\label{J_row}
    J(\A) = \sum_{i=1}^n \|\A_{i:}\|_1^2 = Tr(\A \E \A^\top),
\end{equation}
where $\mathbf{E}$ is the squared matrix of ones. % {\color{blue} Andersen: you mean ``matrix of ones''?}
Thus, by properties of trace, Problem~\eqref{min_prob} becomes
\begin{equation}\label{probl}
(\W^*,\H^*,\blambda^*)
~\in
\argmin_{\substack{\W \geq \zeros, \H \geq \zeros \\ \blambda \geq \zeros}} 
D_{0}(\X, \W\H) 
+ \tr\big(\Diag(\blambda)^2 \H\E \H^\top\big)
.
\end{equation}

\begin{comment}
    We focus on Problem~\eqref{min_prob} with $\beta = 0$ and $\cP(\H)$ defined as 
    $
\cP(\H)
\coloneqq
\tr{\big(
\Diag(\blambda) \H\E\big(\Diag(\blambda)\H\big)^\top
\big)
}
$ in \cite{cotter2005sparse} \lai{aggiungere definizione di J}.
\end{comment}

We remark that \eqref{probl} is a nonconvex minimization problem, in which finding global minima is NP-hard, therefore we are interested in finding local minima for the triple $(\W^*,\H^*,\blambda^*)$.


\paragraph{\textbf{The proposed optimization algorithm $\AlgoName$}}
We propose an algorithm, called $\AlgoName$, to find a local minima for Problem~\eqref{probl} as follows.

\begin{algorithm}[ht]
\SetAlgoLined
\textbf{Input:}{ $\X \in \IRmn_+$ and factorization rank $r$.}\\
Initialize $\W^{0} \in \IRmr_+$, $\H^{0} \in \IRrn_+$ and $\blambda^{0} \in \IRr_+$\\
\For{$k = 1,2,...$ }{
$\W^k = \text{update}(\X,\W^{k-1},\H^{k-1})$ 
\hfill \% classic MU-update
\\
\For{$l \in [r]$}{
$\underline{\h}_l^{k-1,0} = \underline{\h}_l^{k-1}, \lambda^{k-1,0}_l = \lambda^{k-1}_l$
\hfill \% initialization
\\
\For{$t \in [T]$}{
$\underline{\h}_l^{k,t} = \text{update} (\underline{\h}_l^{k,t-1},\W^k,\X,\lambda_l^{k})$ as in $\text{\eqref{H_up_fro_J_row}}$\\
$\dfrac{\partial \cR}{\partial \lambda_l}$ as in \eqref{hyppp}
\hfill \% hypergradient
}
$\blambda^{k,T} = \text{update}(\blambda^{k,T},\nabla_{\blambda}\cR(\blambda))$
\hfill \% projected gradient update
}
}
\textbf{Return }{ $\W,\H,\blambda$ at the last iteration}.
\caption{$\AlgoName$}
\label{alg1}
\end{algorithm}

\begin{comment} 
Get 
$\underline{\h}_l^{k,t}$ by $\text{\eqref{H_up_fro_J_row}}$ 
using $\underline{\h}_l^{k,t-1},\W^k,\X,\lambda_l^{k}$
\\
Get $\dfrac{\partial \cR}{\partial \lambda_l}$ following \eqref{hyppp}
\hfill \% hypergradient
\end{comment}
Note that $\AlgoName$ is composed by two main parts: one devoted to the update of $\W$ (reviewed in the following) and the other one based on bi-level strategy to optimize simultaneously on $\H$ and $\blambda$.
%This second part is the main contribution of this work and it is detailed in the rest of the paper.


\paragraph{\textbf{Update on W}}
The update of $\W$ can be done simply by the following Multiplicative Update \cite{fevotte2009nonnegative} as $\W = \W \odot  (\X\H^\top) \oslash  (\W\H\H^\top)$.

The next section introduces the update on $\H$ and $\blambda$ by a bi-level method, which is the main contribution of the paper.


%
% Sec : H and L and bi
%
\section{Bi-level Optimization for the subproblem}\label{sec:bilevel_general}
In this section, we discuss the steps for updating $\H$ and $\blambda$ in Algorithm~\ref{alg1}.
Given a fixed $\W^k$, we have the following optimization subproblem
\begin{equation}\label{prob_HL}
(\H^*,\blambda^*)
~\in
    \argmin_{\substack{\H \geq \zeros, \blambda \geq \zeros}} 
    D_{0}(\X, \W\H) 
    + \tr\big(\Diag(\blambda)^2 \H\E \H^\top\big)
    .
\end{equation}
The core idea of this paper is to solve Problem~\eqref{prob_HL} as a bi-level problem, in which we incorporate the problem of tuning hyperparameter $\blambda$ simultaneously into the update of $\H$. %by introducing a bi-level strategy. 
To do so, first we review the general theory of bi-level optimization
and its application to Problem \eqref{prob_HL}. %, \lai{see Section~\ref{sec:algo}.}


\textbf{\textit{Section organization and general overview of the approach.}}
Under a fixed and given $\W^k$, the goal is to obtain an updated version of $\H$ and $\blambda$ that approximately solves Problem~\eqref{prob_HL}.
The bi-level approach has the following steps:
\begin{enumerate}
    \item First we replace the constrained optimization problem \eqref{prob_HL} by a bi-level optimization problem for $(\underline{\h}_l,\lambda_l)$, with $l \in [r]$, see \eqref{subproblem:bilevel}.
    \item %\lai{\sout{Problem~\eqref{subproblem:bilevel} has two variable $\H, \blambda$.}}
    The inner problem (IP) in Problem~\eqref{subproblem:bilevel}  %\lai{\sout{which is a problem only depending on (each row of) $\H$, }}
    is then approximated by the solution of a dynamical system.
    See Problem~\eqref{IVP}.
    \item We then solve Problem~\eqref{IVP} to obtain a solution for $\underline{\h}_l$, and also the hypergradient at the last time point $T$.
    See \eqref{hypergradient}.
    \item Lastly we use the hypergradient to obtain the solution $\blambda$ by a gradient descent approach.
    See \eqref{min2}.
\end{enumerate}
We now proceed to discuss each of the steps below.


\paragraph{\textbf{1. Bi-level formulation}}
In Algorithm~\ref{alg1}, the update of $\H$ and $\blambda$ is performed in $r$ steps, where each step is aimed to update the $l$-th component $(\underline{\h}_l, \lambda_l)$.
This is achieved by solving the following bi-level problem %\lai{\sout{for $(\underline{\h}_l^*,\lambda_l^*)$}}
\begin{equation}\label{subproblem:bilevel}
\begin{array}{rcl}
\displaystyle
\min_{\lambda_l \geq 0} 
\left\{
    \begin{array}{rl}
    \displaystyle
    \mathcal{r}(\lambda_l) 
    =&
    \displaystyle \inf_{\underline{\h}_l(\lambda_l) } 
                  D_2\big(\X,\R+\w_l\underline{\h}_l(\lambda_l)\big) \qquad \text{(OP)}
                  \vspace{2mm}
                  \\
    & \st \,\, \displaystyle \underline{\h}_l(\lambda_l) \in \argmin_{\u \in \IRn_+} D_0(\X,\R+\w_l\underline{\u})+\lambda_l^2\|\underline{\u}\|_1^2 \quad \text{(IP)}
    \end{array}
\right\},
\end{array}
\end{equation}
where the matrix $\R$ is the residual obtained isolating in $\W\H$ the $l$-th component of $\w_l\in\IRm$ (column of $\W$) and $\underline{\h}_l \in \IRn$ (row of $\H$), which is
\[
\R = \X - \sum_{j \neq \ell} \w_j \underline{\h}_j.
\]
The function $\mathcal{r}: \IR \to \IR$ is called Response function of the outer problem related to $\underline{\h}_l$. 
In the outer problem, the objective $D_2$ is the $\beta$-divergence with $\beta = 2$, which is the Frobenius norm.
%In the inner problem, the inner objective function is the $\beta$-divergence with $\beta = 0$, which is the Itakura-Saito divergence $D_0$, regularized by the squared $\ell_1$-norm.
The inner problem is represented by the $\beta$-divergence with $\beta = 0$, which is the Itakura-Saito divergence $D_0$, regularized by the row-wise diversity measure defined in \eqref{J_row}. % the squared $\ell_1$-norm. The use of the squared $\ell_1$ norm is motivated by \cite{cotter2005sparse} for enforcing sparsity constraint. \lai{aggiustare in base a quanto scritto sopra}

\begin{remark}
Note that the squared $\ell_1$-norm in the inner problem on $\u$ can be seen as a non-smooth regularization term and therefore proximal gradient descent can be applied on $\u$ (see \cite{evgeniou2015regularized} and \cite[Lemma 6.70]{beck2017first}), however, their approach is applied to convex objective function, where here in \eqref{subproblem:bilevel} the objective function (the IS-divergence) is possibly nonconvex \cite{fevotte2009nonnegative} thus proximal gradient descent do not have convergence guarantee.    
\end{remark}








\paragraph{\textbf{2. Dynamical system approach on H}}
An approach to solve the bi-level Problem \eqref{subproblem:bilevel} over $\H$ is to replace the inner problem with a dynamical system~\cite{franceschi2021unified,franceschi2017forward,Mc} and compute an approximation solution.\\
%\lai{\sout{\textbf{Notation simplification}}}.
We now omit the $k$ index in $\underline{\h}_l^{k,t}$, due to the fact that the update focuses on the iteration over $t$ under a constant $k$.\\
Given $\underline{\h}_l^{0}$ (which depends implicitly on $\lambda_l$), we build a dynamical system (IVP-$\Phi$) in the form of a discrete initial value problem as
\begin{equation}\label{IVP}
\begin{cases}
\underline{\h}_l^t= \Phi_t(\underline{\h}_l^{t-1}, \lambda_l), \quad t \in [T] 
\\
\underline{\h}_l^{0} = \Phi_0(\lambda_l)
\end{cases}
\tag{IVP-$\Phi$}
\end{equation}
where $\Phi_t : \IRn \times \IR \rightarrow \IRr$ is a smooth map for $ t \in [T]$.

The idea of the bi-level optimization strategy is to use the IVP-$\Phi$ to approximate the solution of the Problem \eqref{subproblem:bilevel}.
We do so by solving the following minimization 
\begin{equation}\label{min2}
\begin{array}{rl}
\displaystyle 
\lambda_l^* 
=
\argmin_{\lambda_l} & \mathcal{r}(\lambda_l)
\\
\st ~~ &
\underline{\h}_l^{t} = \Phi_t(\underline{\h}_l^{t-1}, \lambda_l) \quad \text{for } t \in [T]
\end{array}
\end{equation}
in which we approximate the solution of the inner problem with the solution of the dynamical system. 
This is possible because problem~\eqref{min2} satisfies the existence and convergence theorems as proved in \cite{del2023bi}. %This can be done thanks to the fact that problem~\eqref{min2} satisfies existence and convergence theorems in \cite{del2023bi}.

As a preview, we will derive $\Phi_t(\underline{\h}_l^{t-1}, \lambda_l)$ in Section~\ref{sec:algo}.


\paragraph{\textbf{3. Hypergradient}}
To find the solution $\blambda^*$ for Problem~\eqref{prob_HL}, we solve the problem formed by joining all the Response functions $\mathcal{r}(\lambda_l)$ in \eqref{subproblem:bilevel} as
\begin{equation}\label{probl_big_R}
    \argmin_{\blambda \in \IRr_+} 
    \Bigg\{ \cR(\blambda) \coloneqq  \sum_j \mathcal{r} (\lambda_l) \Bigg\}.
\end{equation}
We would like to compute the hypergradient, i.e. the gradient of $\cR$ with respect to $\blambda$, in order to use a gradient descent approach on $\blambda$.
%To solve it we compute the  approximation of t
The hypergradient $\nabla_{\blambda}\cR$, by using chain rule, is
\begin{equation}\label{hypergradient}
\frac{\partial \cR}{\partial \lambda_l} 
= 
\dfrac{\partial \mathcal{r}}{\partial \lambda_l} +
\frac{\partial \mathcal{r}}{\partial \underline{\h}_l^{T}} \cdot \frac{d \underline{\h}_l^{T}}{d \lambda_l}, 
\quad l \in [r].
\tag{hypergrad}
\end{equation}
%It is important to note that $\underline{\h}_l^{T}$ is not denoting the transpose of $\underline{\h}_l$, but it denotes the row-vector $\underline{\h}_l$ at the time $T$.
Note that $\underline{\h}_l^{T}$ denotes the row-vector $\underline{\h}_l$ at the time $T$.

It is well known that the computation of the hypergradient can be done using Reverse-Mode Differentiation (RMD) or Forward-Mode Differentiation (FMD).
Since RMD requires storing specific variables across all iterations and indices in memory, in this work we use FMD, making it more suitable for scenarios where the total quantity of interest is small. For details we refer the reader to \cite{del2023bi,franceschi2017forward}.


\textbf{Forward-Mode}.
FMD computes the differentiation in \eqref{hypergradient} using the chain rule.
Function $\Phi_t$ for $t \in [T]$ depends on $\lambda_l$ explicit and on $\underline{\h}_l^{t-1}$ implicitly, then we have the derivative
\[
\frac{d \underline{\h}_l^{t}}{d \lambda_l}
= 
\frac{\partial \Phi_t(\underline{\h}_l^{t-1},\lambda_l)}{\partial \underline{\h}_l^{t-1}}
\cdot
\frac{d \underline{\h}_l^{t-1}}{d \lambda_l}+\frac{\partial \Phi_t(\underline{\h}_l^{t-1}, \lambda_l)}{\partial \lambda_l}.
\]
Let $\s^t = \dfrac{d \underline{\h}_l^{t}}{d \lambda_l}$, then each FMD iterate behaves as
\begin{equation}\label{eqdiff_As}
\begin{cases}
    \s^t = \A_t \s^{t-1}+ \b_t, \quad t \in [T]
    \\
    \s^0 = \b_0
\end{cases}
\end{equation}
where $\A_t  = \dfrac{\partial{\Phi_t(\underline{\h}_l^{t-1}, \lambda_l)}}{\partial{\underline{\h}_l^{t-1}}} \in \IRnn $ and 
$
\b_t =  \dfrac{\partial{\Phi_t(\underline{\h}_l^{t-1}, \lambda_l)}}{\partial{\lambda_l}}
\in \IRn$.
Now \eqref{hypergradient} can be expressed as
\begin{equation}\label{Hypergradiente_R}
\dfrac{\partial \cR}{\partial \lambda_l} 
= \langle \underline{\g}^T, \s^T \rangle \in \IR
\quad
\text{where} 
\quad
\underline{\g}^T
= \dfrac{\partial \mathcal{r}}{\partial \underline{\h}_l } \in \IRn.
\end{equation}
We note that $\underline{\g}^T$ is %not denoting transpose of $\underline{\g}$, but  the row-vector $\underline{\g}$ at the time $T$.
denoting the row-vector $\underline{\g}$ at the time $T$.
Lastly, the solution of Problem \eqref{eqdiff_As} solves the following equation
\begin{equation}\label{hyppp}
\dfrac{\partial \cR}{\partial \lambda_l} 
=
\dfrac{\partial \mathcal{r}}{\partial \underline{\h}_l^{T}} \left(
\b_T + \sum_{t=0}^{T-1}\bigg(\prod_{s=t+1}^T \A_s \bigg) \b_t
\right).
\end{equation}



\paragraph{\textbf{4. Update of $\blambda$}}
Given all the components $\underline{\h}_1^T,\underline{\h}_2^T,\ldots,\underline{\h}_r^T$ at the last time point of the dynamic system and the hypergradient (discussed previously), we update $\blambda$ with a projected gradient descent approach %on problem \eqref{probl_big_R} 
as
$
\blambda = \big[\blambda - \alpha \nabla_{\blambda}\cR(\blambda)\big]_+,
$
for a pre-defined stepsize $\alpha > 0$.

%We now are ready to introduce the overall bi-level optimization approach to Problem~\eqref{probl}.

%
%
%
\section{Derivation of the algorithm SHINBO}\label{sec:algo}
We now introduce the overall bi-level optimization approach, discussed in the previous section, to the Subproblem~\eqref{prob_HL}.
We use the method of partial Lagrangian multiplier, which is applied only on $\H$.  %but not on $\blambda$, with a reason to be explained in Remark~\ref{remark:why_partial_Lag}.
First, let $\bPsi \in \IRrn_+$ be the matrix of Lagrangian multipliers associated to the nonnegative constraints of $\H$, then the expression of the (partial) Lagrangian of Subproblem~\eqref{prob_HL} is
\[
\cL(\H)
= 
D_0(\X,\W\H)
+ \tr\big(\Diag(\blambda)^2\H\E\H^\top\big)+\tr\big(\bPsi\H^\top\big).
\]
Recall that $\M^{\odot2}$ denotes the Hadarmard power of $\M$, then the partial derivative of $\cL$ with respect to $\H$ is
\[
\dfrac{\partial{\mathcal{L}}}{\partial \H} 
= 
\W^\top\Big(
(\W\H)^{\odot -2}(\W\H-\X)
\Big)
+
2\Diag(\blambda)^2\H \E+\bPsi.
\]
Now denote $H_{ij}$ the $(i,j)$-th element of the matrix $\H$, and recall that $\odot$ is the Hadarmard product, then by the complementary slackness $\Psi_{ij} H_{ij} = 0$ in the KKT conditions, we get 
\[
    \Big[
    \W^\top\Big(
    (\W\H)^{\odot -2} \odot (\W\H-\X)
    \Big) 
    \Big]_{ij}
    H_{ij}
+
    2
    \Big[
    \Diag(\blambda)^2\H\E
    \Big]_{ij} H_{ij} 
+ 
\Psi_{ij} H_{ij}
= 
0.
\]
These equations lead to the multiplicative update
\begin{equation}\label{H_up_fro_J}
H_{ij} 
= 
H_{ij}
\Big[
\W^\top(\W\H)^{\odot -2}\X
\Big]_{ij}
\bigg\slash
\Big[
\W^\top(\W\H)^{\odot -1}+2\Diag(\blambda)^2\H\E
\Big]_{ij}
,
\end{equation}
where the division is performed element-wise as the Hadamard division $\oslash$.
By fixing $l \in \{1, \dots, r\}$, the update in row-wise format for the $l$-th row of $\H$ can be rewritten equivalently as
\begin{equation}\label{H_up_fro_J_row}
\underline{\h}^t_l
= 
\underline{\h}^{t-1}_l
\odot 
\dfrac{
    \Big(
    \W^\top(\W\H)^{\odot -2}\X
    \Big)_{l:}^{t-1}
}
{
    \Big(
    \W^\top \odot (\W\H)^{\odot -1} 
    \Big)_{l:}
    +
    2(\lambda_l^{t-1})^2\|\underline{\h}^{t-1}_l\|_1 \E_{l:}
}.
\end{equation}
We remark that Equation~\eqref{H_up_fro_J_row} is one of the main contribution of this paper.

\begin{remark}\label{remark:why_partial_Lag}
%We now explain the reason why we use partial Lagrangian $\cL$. The key is that we need the row-wise update in the form of expression ~\eqref{H_up_fro_J_row} so as to implement the bi-level approach that we have discussed in Section~\ref{sec:bilevel_general}.
The key idea in this methodology is that $\Phi_t(\underline{\h}_l^{t-1}, \lambda_l) $ in the dynamical system \eqref{IVP} is the right hand side of the update~\eqref{H_up_fro_J_row}.
\end{remark}





\subsection{The implementation of the bi-level approach in SHINBO}
%We now discuss the bi-level approach introduced in Section~\ref{sec:bilevel_general} on solving the subproblem~\eqref{prob_HL}.

%Consider the $\Phi_t(\underline{\h}_l^{t-1}, \lambda_l) $ in the dynamical system \eqref{IVP} approximated by the multiplicative update~\eqref{H_up_fro_J_row}, we compute $\A^t= \dfrac{\partial{\Phi_t}}{\partial{\underline{\h}^{t-1}_l}} $ and $\b^t = \dfrac{\partial{\Phi_t}}{\partial{\lambda_l}}$ for $t \in [T]$. Then following the procedure and the discussion in Section~\ref{sec:bilevel_general}, we have that the $\underline{\g}^T$ in \eqref{Hypergradiente_R} for computing \eqref{hypergradient} is $\underline{\g}^T = -2\w_l^\top(\X-\R-\w_l\h_l)$.

Following the procedure and the discussion in Section~\ref{sec:bilevel_general}, we consider $\Phi_t(\underline{\h}_l^{t-1}, \lambda_l) $ in the dynamical system \eqref{IVP} (represented by the multiplicative update~\eqref{H_up_fro_J_row}) and we compute $\A^t= \dfrac{\partial{\Phi_t}}{\partial{\underline{\h}^{t-1}_l}} $ and $\b^t = \dfrac{\partial{\Phi_t}}{\partial{\lambda_l}}$ for $t \in [T]$ required for the FMD.

%Recall that computing FMD requires the terms $\A_t$ and $\b_t$, where $\A_t$ and $\b_t$ can be computed as follows.
%Based on the structure of the problem (see Update~\eqref{H_up_fro_J} and Remark~\ref{remark:why_partial_Lag}):
\begin{itemize}
    \item The computation of $\A^t= \dfrac{\partial{\Phi_t}}{\partial{\underline{\h}^{t-1}_l}}$ gives a diagonal matrix
    \[
    A_{jj}^t = 
    \dfrac{N_{lj}}{D_{lj}}-h_{lj}
    \left(
    \frac{\displaystyle2\sum_i^n{w^2_{il} \dfrac{x_{ij}}{(\W\H)_{ij}^3} D_{lj}-N_{lj}\sum_{i}^n}{
    \dfrac{w^2_{il}}{(\W\H)^2_{lj}}
    }+2\lambda_l^2N_{lj}}{D_{lj}^2}
    \right),
    \]
    %\fei{Laura please check here if $l=j$}
    where the derivative is computed with respect to the $(l,j)$-th element of $\H$, and 
    \[
    N_{lj} = \big(\W^\top(\W\H)^{\odot -2}\X\big)_{lj},
    ~~
    D_{lj} = \big(\W^\top(\W\H)^{\odot -1}+2\Diag(\blambda)^2\H\E\big)_{lj}.
    \]

    \item The vector $\b_t$ is given by $\dfrac{\partial{\Phi_t}}{\partial{\lambda_l}} = - 4\lambda_l h^2_{lj} \dfrac{N_{lj}}{D_{lj}^2}
    $.
\end{itemize}

Finally, having chosen as an outer problem the Frobenius norm, $\underline{\g}^T$ in \eqref{Hypergradiente_R} for computing \eqref{hypergradient} is $\underline{\g}^T = -2\w_l^\top(\X-\R-\w_l\h_l)$.




















%
% experiment
%
\section{Experimental Results}\label{sec:exp}
In this section, we present the numerical results of comparing the proposed algorithm SHINBO with the multiplicative update (MU) algorithm \cite{fevotte2009nonnegative}.
%We also compare SHINBO with the penalized MU, which is MU update replaced by \eqref{H_up_fro_J_row} under a fixed penalty parameter $\blambda$ with $\lambda_1=\lambda_2=\ldots=\lambda_r$ for every row of $\H$.
We also compare SHINBO with the penalized MU, which is the update \eqref{H_up_fro_J_row} under a fixed penalty hyperparameter $\blambda$ with $\lambda_1=\lambda_2=\ldots=\lambda_r$ for every row of $\H$.
We summarize their difference in the table below.

\begin{table}[h!]
\centering
\caption{The method compared in the experiment.}
\begin{tabular}{lll}
Algorithm & $\lambda$  &  $\lambda$ setting \\ \hline
MU \cite{fevotte2009nonnegative}      & 0   &constant
\\
Penalized MU  & 0.1  &constant for each column
\\
Penalized MU  & 0.5  &constant for each column
\\
SHINBO  & not required  & automatically optimized,\\
  & & different penalization for each column
\end{tabular}
\end{table}

\paragraph{\textbf{Datasets}}
We evaluate the algorithms on two datasets: one generated synthetically, and the other one obtained by processing real vibrational signals measured in a laboratory condition from damage rolling bearing elements.
\begin{itemize}
\item The synthetic dataset is generated starting from a full-rank decomposition $\X\approx\W\H$, where the factor matrix $\W$ contains 10\% nonzeros in the $mr$ entries and the factor matrix $\H$ contains 70\% nonzeros in the $rn$ entries.
In this dataset, we have $(m,n,r) = (100,7,3)$.
\item The vibrational signals were measured on the test rig presented in Fig.\,\ref{fig:rig}.
The platform is equipped with an electric motor, gearbox, couplings, and two bearings.
One of the latter was deliberately damaged.
Diagnostic signals were measured with the accelerometer (KISTLER Model 8702B500), stacked horizontally to the shaft bearing.
The 40-second-long vibration signal was recorded with the sampling rate of 50 kHz. 
For easier visualization, one second excerpt was selected, and then transformed to a spectrogram using the 128-window length, 100 sample overlapping, and 512 frequency bins.
The raw observed signal and its spectrogram are shown in Fig.\,\ref{fig:real_signals}. 
Assuming $r = 4$ (four main components of the mixed signal), we have $(m,n,r) = (257, 1782, 4)$.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.78\textwidth]{figures/rig_compressed.png}
    \caption{The test rig used in the experiment \cite{GABOR2023110430}.}
    \label{fig:rig}
\end{figure}

\begin{figure}[h!]
     \centering
        \includegraphics[width=0.88\textwidth]{figures/vibration_signal_sub.png}
     \caption{Recorded vibration signal and its spectrogram.}
     \label{fig:real_signals}
\end{figure}

\subsection{Experimental setup}
\paragraph{\textbf{Initialization}}
To ensure a fair comparison, all algorithms were initialized using the same initial factor matrices, which were generated from an unpenalized MU-based NMF warm start, that is initialized via nonnegative double singular value decomposition (NNDSVD) of the matrix $\X$ \cite{boutsidis2008svd}.

\paragraph{\textbf{Simulation}}
We perform 100 Monte Carlo simulations, where each run uses a different random data matrix. %initialization for the matrices $\W$ and $\H$. 
At the start of each run, the initial value of $\blambda^0$ was selected randomly following a uniform distribution $\cU[0,1]$.


\paragraph{\textbf{Normalization}}
We perform a normalization step on each column of $\W$, $\w_k$ for all $k=1, \dots, r$:
%\begin{equation}
%    \begin{array}{ccc}
%    \w_k &=& \w_k/\max(\w_k); \nonumber\\
%    \underline{\h}_k &=& \underline{\h}_k*\max(\w_k).
%    \end{array}
%\end{equation}
\[
\w_k = \w_k/\max(\w_k); 
\qquad    \underline{\h}_k = \underline{\h}_k*\max(\w_k).
\]

\paragraph{\textbf{Termination}}
All the algorithms were allowed to run a maximum number of iterations of $500$ for the synthetic dataset and $100$ for the real one, with an early termination tolerance of $10^{-6}$ on the relative fitting error using the IS-divergence $D_0$, defined as 
\[
\big|
D_0(\X, \W^{k+1}\H^{k+1})- D_0(\X, \W^k\H^k)
\big|
\,\big\slash\,
\big|D_0(\X, \W^k\H^k)\big|
\,\leq\,
10^{-6},
\]
where $k$ indicates the iteration of the outer loop of the algorithm. 
For the inner loop of the bi-level approach on iteration counter $t$, we stop at the maximum number of (inner) $T = 4$ iterations.



\paragraph{\textbf{Evaluation}}
We evaluate the algorithms according to different criteria on synthetic and real-world datasets. For both datasets, we plot the convergence of the algorithms by looking at the behavior of the Response function.

For the synthetic dataset, we evaluate the quality of the factorization with respect to the identification problem for the synthetic dataset using the Signal-to-Interference
Ratio (SIR) measure \cite{cichocki2009amari} between the estimated signals and the true signals.
This is a log-scale similarity measure (expressed in decibels), often used in signal processing applications, and its higher value indicates a higher similarity level.
To highlight the effectiveness of the proposed method, we conduct a statistical comparison of SIR values across algorithms using the Kruskal-Wallis test, followed by a post-hoc multiple comparison based on the Mann-Whitney test \cite{lehmann1986testing} with Benjamini-Hochberg (BH) correction \cite{benjamini1995controlling}, maintaining a significance level of $\alpha = 0.05$. 
We also investigate the sparsity of the results obtained for the synthetic dataset using the following measure of sparsity for a generic matrix $\A\in\IRmn$:
\[
Sp(\A)\coloneqq \big(1-\|\A-10^{-6}\E\|_0 /mn\big) 100\%,
\]
where $\|\cdot\|_0$ denotes the number of non-zeros elements. 
To demonstrate the advantages of the proposed method, we analyze sparsity values across algorithms using the same statistical test as for the evaluation of the SIR (Kruskal-Wallis test, post-hoc Mann-Whitney test with BH and $\alpha = 0.05$).

On real-world dataset we check the goodness of the factorization by quantifying the impulsive and cyclic behavior of the signal under analysis, using a modified envelope spectrum based indicator (ENVSI) \cite{gabor2023bearing, berrouche2024local} on time profiles. 
ENVSI can be expressed as spectrum based indicator (SBI), defined as:
\[
SBI \coloneqq \sum_{i=1}^{M_1}{\text{AIS}_i^2} \Bigg/ \sum_{k=1}^{M_2}{\text{S}_k^2},   
\]
where
%\begin{itemize}
%    \item 
    $\text{AIS}_i$ is the magnitude of the $i$-th harmonic of the estimated signal in the frequency domain,
%    \item 
    $\text{S}_k$ is the magnitude in the $k$-th frequency bin in the spectrum of the time profile,
%    \item 
    $M_1$ is the number of harmonics to be analyzed (assuming a periodic signal), and
%   \item 
    $M_2$ is the number of frequency bins to calculate the total energy.
%\end{itemize}

SBI is zero if there are no impulsive components in the time profile, whereas a larger SBI occurs when the impulses in the time profile are stronger (which corresponds to the amplitudes in the spectrum) and the noise is weaker.
In the experiments, the number of harmonics $M_1$ is set to 6, and it was experimentally found to be sufficient for demonstrating the impulsive and period behavior of the SOI representing time profile in the analyzed application. 
To demonstrate the superiority of the proposed method, we statistically compare ENVSI values across algorithms using a Kruskall-Wallis test and a post-hoc multiple comparison based on Mann-Whitney test with a BH correction, with a statistical significance level set at $\alpha= 0.05$.

\paragraph{\textbf{Computer}}
All the experiments were conducted in MATLAB 2021a and executed on a machine with an i7 octa-core processor and 16GB of RAM.





\subsection{Results on synthetic dataset}
Refers to Figure \ref{fig:figures_response_A}, the proposed algorithm demonstrates superior performance in terms of convergence rate of Response function compared to the other methods.
We remark that, despite SHINBO exhibited similar and rapid convergence as unpenalized MU for the first 450 iterations (on average), the obtained decompositions have a different SIR value. See the discussion below.


\begin{figure}[ht!]
\centering
\includegraphics[width=0.65\textwidth]{figures/plot_100MC_median.pdf}
\caption{Behavior of the Response function (outer problem) for the synthetic dataset.}
\label{fig:figures_response_A}
\end{figure}
Refers to Figure \ref{fig:SIR_SP}, we can see that SHINBO obtain the best SIR values on both matrices and the higher sparsity on $\H$ compared to the other methods.
On average, SHINBO achieved 10\% better SIR and 5\% better sparsity on $\H$ than other algorithms.



\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{figures/SIR_SP.pdf}
\caption{Results of sparsity and SIR on the factor matrices on the synthetic dataset.
Left two columns are the sparsity of $\W$ (left) and $\H$ (right) of the four algorithms: from the left to right are MU with $\lambda=0$, MU with $\lambda=0.1$, MU with $\lambda=0.5$ and SHINBO.
The right two columns are the SIR of $\W$ (left) and $\H$ (right) of the four algorithms: from the left to right are MU with $\lambda=0$, MU with $\lambda=0.1$, MU with $\lambda=0.5$ and SHINBO.
}
\label{fig:SIR_SP}
\end{figure}

These results are also confirmed by the statistical comparisons. 
The Kruskall-Wallis tests present $p$-values lesser that $10^{-14}$ and the details of the pair-wise comparison with a BH comparison are presented in the tables \ref{tab:sir_A} and \ref{tab:sparsity_A}.


\begin{table*}[h!]\caption{$p$-values results of pairwise comparisons for SIR coefficients on ($\W, \,\H$). In the table $\epsilon = 10^{-16}$.}
\label{tab:sir_A}
\centering
\begin{tabular}{c| c c c }
& MU & MU $\lambda=0.1$ & MU $\lambda=0.5$\\ \hline
MU $\lambda=0.1$ & $8.1 \cdot 10^{-5}$, $6.3 \cdot 10^{-10}$ & -  & - \\
MU $\lambda=0.5$ & $<2\epsilon$,$<2\epsilon$ & $<2\epsilon$,$<2\epsilon$ & - \\
SHINBO & 0.14,0.0042 & $5.4 \cdot 10^{-5},1.2  \cdot 10^{-9}$ & $<2\epsilon$, $<2\epsilon$ 
\end{tabular}
\end{table*}
 
\begin{table*}[h!]\caption{$p$-values results of pairwise comparisons for sparsity coefficients on $(\W,\H)$.}
        \label{tab:sparsity_A}
            \centering
            \begin{tabular}{c| c c c }
                 & MU & MU $\lambda=0.1$ & MU $\lambda=0.5$\\
                 \hline
            MU $\lambda=0.1$ & 0.00036, 0.0075 & -  & - \\
            MU $\lambda=0.5$ & $<2\epsilon$, $<2\epsilon$ & $<2\epsilon$,$<2\epsilon$ & - \\
            SHINBO  & 0.023, $<2\epsilon$  & $3.12 \cdot 10^{-6}$, $<2\epsilon$  & $<2\epsilon, <2\epsilon$
            \end{tabular}    
	\end{table*}


\subsection{Results on real-world dataset}

As shown in Figure \ref{fig:envsi1}, the proposed algorithm outperforms other methods in terms of the convergence of the Response function.
From Figure \ref{fig:envsi2}, we observe that SHINBO achieves the highest ENVSI on $\H$. A higher value of the ENVSI score (above 0.77) and a lower number of outliers confirm that the proposed algorithm finds the SOI in the observed diagnostic signal, which is consistent with our prior knowledge about the physical state of the tested rolling bearing. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\textwidth]{figures/real_res.pdf}
\caption{Behavior of the Response function (outer problem) for the real dataset.}
\label{fig:envsi1}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.23\textwidth]{figures/envsi.pdf}
\caption{The ENVSI score of $\H$ of the four algorithms: from the left to right are MU with $\lambda=0$, MU with $\lambda=0.1$, MU with $\lambda=0.5$ and SHINBO.
}
\label{fig:envsi2}
\end{figure}

\begin{comment}
\begin{figure}[ht!]
\centering
\includegraphics[width=0.71\textwidth]{figures/real_res.png}
\includegraphics[width=0.28\textwidth]{figures/envsi.pdf}
\caption{
Left: Behavior of the Response function (outer problem) for the real dataset.
Right: The ENVSI score of $\H$ of the four algorithms: from the left to right are MU with $\lambda=0$, MU with $\lambda=0.1$, MU with $\lambda=0.5$ and SHINBO.
}
\label{fig:envsi}
\end{figure}
\end{comment}

These results are also confirmed by the statistical comparisons.
The Kruskall-Wallis tests present $p$-values lesser than the fixed significant level (0.04) and the details of the pair-wise comparison with a BH comparison are presented in Table \ref{tab:envsi}.


\begin{table*}[h!]\caption{$p$-values results of pairwise comparisons for ENVSI coefficients.}
        \label{tab:envsi}
        \centering
            \begin{tabular}{c| c c c }
                 & MU & MU $\lambda=0.1$ & MU $\lambda=0.5$\\
                 \hline
            MU $\lambda=0.1$ & 0.98 & -  & - \\
            MU $\lambda=0.5$ & 0.98 & 0.98  & - \\
            SHINBO  & 0.04 & 0.04 & 0.04 
            \end{tabular}
	\end{table*}


The concept is that SHINBO is designed to effectively identify which components of $\blambda$ are associated with noise and which are linked to the true sparse signal.
The goal is to apply greater penalization to certain components, thereby filtering out the noise while preserving the component representing the SOI. The results indicate that SHINBO performs very well in this context, as it successfully isolates and preserves the meaningful signal while suppressing irrelevant noise. %\fei{Vedere qui la question delle componenti}
\begin{comment}
In the experiment conducted, we obtained $\blambda = (0.106, 0.777, 0.727, 0.910)$, each component is associated with the recontacted row of $\H$ in Figure \ref{fig:rowsH}. We can observe that the first component is associated with the SOI and is penalized with a lower value than the three other rows that represent the noise, which are penalized more by SHINBO.
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/rowsH.pdf}
    \caption{Reconstructed rows of $\H$ with SHINBO algorithm.}
    \label{fig:rowsH}
\end{figure}
%\fei{As shown in Fig. \ref{fig:envsi}, a higher value of the ENVSI score (above 0.77) and a lower number of outliers confirm that the proposed algorithm finds the SOI in the observed diagnostic signal, which is consistent with our prior knowledge about the physical state of the tested rolling bearing. }
\end{comment}


%
% conclusion
%
\section{Conclusion}\label{sec:conl}
In this work, we addressed the critical challenge of selecting penalty hyperparameters in NMF by introducing SHINBO, a novel algorithm that employs a bi-level optimization framework to adaptively tune row-dependent penalties. 
By focusing on the IS divergence, SHINBO proves highly effective for extracting low spectral density components in spectrograms, particularly in the presence of noise. 
The ability of the algorithm to enforce sparsity constraints and dynamically adjust penalties ensures a more precise separation of meaningful signals from noisy disturbances.  

Through experiments on both synthetic and real-world datasets, SHINBO demonstrated its superior performance compared to traditional NMF methods.
For real-world applications, such as noninvasive vibration-based fault detection in rolling bearings, SHINBO excelled at isolating sparse, periodic signal components in high-frequency subbands, even when heavily masked by broader noise.  

The results highlight SHINBO’s potential to significantly enhance signal recovery in complex, noise-dominated environments.
By tackling the hyperparameter selection problem with an adaptive, data-driven approach, SHINBO not only advances the field of NMF but also provides a robust tool for applications requiring precise spectral decomposition and noise suppression.
Future work will explore the scalability of SHINBO for larger datasets and its adaptability to other domains with similar challenges.

\section*{Acknowledgment}
The authors would like to express their sincere appreciation to Professor Radoslaw Zimroz from Faculty of Geoengineering, Mining and Geology at Wroclaw University of Science and Technology, for providing the real data collected in his laboratory. 

N.D.B., F.E., L.S. are members of the Gruppo Nazionale Calcolo Scientifico - Istituto Nazionale di Alta Matematica (GNCS-INdAM). 

\section*{Funding}
N.D.B., F.E., and L.S. are partially supported by "INdAM - GNCS Project", CUP: E53C24001950001. 
\\
N.D.B. and F.E. are supported by Piano Nazionale di Ripresa e Resilienza (PNRR), Missione 4 ``Istruzione e Ricerca''-Componente C2 Investimento 1.1, ``Fondo per il Programma Nazionale di Ricerca e Progetti di Rilevante Interesse Nazionale'', Progetto PRIN-2022 PNRR, P2022BLN38, Computational approaches for the integration of multi-omics data. CUP: H53D23008870001.


\section*{Authors contribution}
All authors contributed equally to this work.

\section*{Conflict of interest}
The authors have no relevant financial interest to disclose.

\bibliographystyle{abbrv}
\bibliography{reference}
\end{document}