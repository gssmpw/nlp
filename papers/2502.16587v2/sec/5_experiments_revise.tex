\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setups}
\noindent\textbf{Task Definition.}
As visualized in Figure~\ref{fig:task_demo}, we conduct tests across the following four tasks:
\begin{itemize}
    \item \textbf{Push the box. }Push the box from left to right.
    \item \textbf{Pick the cube to the designated plate. }Pick up the cube and place it on the bottom or top plate depending on the human demonstration.
    \item \textbf{Pick up and hold the brush upright. }Pick up the brush, hold it upright, and get ready to write.
    \item \textbf{Handwriting. }Write the corresponding character depending on the human demonstration.
\end{itemize}
The four carefully selected tasks above are designed to test the capabilities in multiple aspects: (1) The push box task is to test the accuracy in predicting height, position, and trajectory. (2) The pick cube task requires the model to understand the task objective in the video. (3) The upright task is a challenging fully 6-DOF task. (4) The handwriting task is to test the generalization ability. 

Notably, in the handwriting task, the models are evaluated on unseen characters. Since there are countless characters in the world, it is impossible to demonstrate all of them. Therefore, having such a generalization ability is crucial. Thus, this task demands exceptional generalization capabilities, which we have not observed in other models. 

\noindent\textbf{Baselines.} We compare with the following baselines:
\begin{itemize}
    \item \textbf{RVCD Policy}: We design a robot-video-conditioned policy $\pi(a_t|s_t,v)$, where $v$ refers to the robot video that indicates the task. Specifically, we modify ACT~\cite{zhao2023learning} to a video conditioned policy. Details can be found in Supplementary materials~\ref{supp:baselines}. 
    \item \textbf{RVCD Policy w. Generated video}: The framework and weights of this model is the same as RVCD Policy, but the robot video, generated by \system using human demonstration, is provided during inference.
    \item \textbf{XSkill~\cite{xu2023xskill}:} XSkill is a human-video-conditioned policy tries to bridge the embodiment gap between human and robot. Details can be found in Supplementary materials~\ref{supp:baselines}.
    \item \textbf{\system w. KNN} \system with our KNN method proposed in Section~\ref{sec:inference}, which enables to perform tasks without explicit demonstrations.
\end{itemize}
The baselines and \system utilize the same H\&R dataset for training across all tasks except handwriting. For the handwriting task, the models are trained on handwriting play data~\cite{wang2023mimicplay} (randomly moving data).
\input{sec/t_generalization}

\subsection{Main results}
\input{sec/t_task_demo}

\noindent\textbf{Effectiveness.} As shown in Table~\ref{tab:comp_baselines}, \system achieves a high success rate across all tasks. In the pick cube to the designated plate task, half of the test cases require placing the cube on the bottom plate, while the other half require placing it on the top plate. Therefore, a success rate above 50\% indicates that the model can correctly follow human demonstrations to perform distinct tasks. However, all baselines perform poorly on this task, consistently placing the cube on the bottom plate regardless of the given demonstration. This proves that only \system can truly understand human demonstration and execute the correct tasks. In addition, all baselines can successfully complete the handwriting task that requires strong generalization exhibitting they have limited generalization capabilities. These results highlight the satisfactory performance and strong generalization capabilities of \system. 

We also test our KNN method proposed in Section~\ref{sec:inference}, in a setting where no demonstrations are given. The performance of \system w. KNN is shown in Table~\ref{tab:comp_baselines}. \system with KNN outperforms all other baselines across all tasks, demonstrating that even without direct demonstrations, \system can still achieve strong performance. However, the KNN method experiences a 10-20\% drop in success rate on the push box and upright tasks, which is within an acceptable range. Meanwhile, a significant performance drop is observed in the pick cube to the designated plate task. This is expected, as the KNN method struggles to select the desired demonstration video when faced with the same initial scene. Finally, the KNN method cannot select demonstrations that were not present during training, as these demonstrations are only used during inference. As a result, it fails to accomplish the handwriting task. As a whole, such competitive results demonstrate the high efficiency and accuracy of \system in action prediction with episodes retrieved by KNN.

\noindent\textbf{Quality of Generated Videos.} Besides predicting actions, \system can visually simulate future dynamics of robot movements, requiring precise predictions of the motions of the robot arm and its interactions with objects. The visual quality of the generated video of \system is shown in Figure~\ref{fig:task_demo}.

However, to evaluate the quality of generated videos, the traditional methods of using video distribution scores presents the following challenges: (1) Since the test set is small, which is common in the robotics domain, typically containing only 5 to 20 test samples, the scores may be unreliable. (2) No existing generation methods are directly applicable to our task.

To this end, we introduce a new visual quality evaluation pipeline: training a RVCD policy as an upper bound and replacing robot-conditioned videos with generated videos during inference. The performance drop serves as a proxy for evaluating visual quality. As shown in Table~\ref{tab:comp_baselines}, the performance of RVCD w. Gen achieve a similar score, demonstrating the realism of our video prediction.

\subsection{Generalization}

\noindent\textbf{Position generalization.} \system uses its generative capability to enable the policy to understand human intentions and perform corresponding tasks at different positions. As shown in Table~\ref{tab:generalization}, \system achieves better results in different testing positions.

\noindent\textbf{Appearance generalization.} Appearance generalization is crucial for the model's robustness. Since \system is based on Stable Diffusion~\cite{rombach2022high}, which contains a vast amount of prior knowledge, it naturally inherits strong generalization capabilities in terms of appearance without any data augmentation. As shown in the Table~\ref{tab:generalization}, \system demonstrates excellent generalization across  different colors, which are unseen during training.

\noindent\textbf{Instance generalization.} Instance generalization requires the ability to generalize across objects of different sizes and shapes, making it even more critical than appearance generalization. As illustrated in the Table~\ref{tab:generalization}, thanks to the generative capability of diffusion models, \system achieves strong performance across various objects. \system successfully completes tasks on unseen instances, demonstrating its powerful generalization ability.

\noindent\textbf{Background generalization.} Although the training data primarily consists of white backgrounds, as shown in the Figure~\ref{fig:task_demo}, we find that \system maintains stable performance even when the background color changes or the scene becomes cluttered. As presented in the Table~\ref{tab:generalization}, \system successfully completes tasks across different background, demonstrating its strong capability.

\noindent\textbf{Task generalization.} We train the model on play data, which involves moving randomly on the table rather than writing any specific character, and then evaluate its one-shot task execution ability given human handwriting prompts of never-before-seen character. To keep the scene clean during data collection, no actual handwriting was performed, resulting in blank backgrounds in the training data, which differ from those in the evaluation phase. Consequently, the handwriting task simultaneously challenges the generalization ability of models in both scene adaptation and task execution. The satisfying results shown in the Table~\ref{tab:generalization} demonstrate the effectiveness and generalization capability of \system in task transfer.


\subsection{Discussion}
\label{sec:ablation}

\noindent\textbf{The Choice of Upsampling Layers for Feature Extraction.}
As shown in Table~\ref{tab:upsample}, we examine the impact of extracting features from different UNet upsample layers for action prediction. Using the 2nd up-sampling block (Up Index 2) achieves a 90\% success rate on seen tasks, while the 1st up-sampling block (Up Index 1) reaches a 95\% success rate on unseen tasks. The results indicate that the choice of upsampling layer has minimal effect on outcomes.

\begin{table}[h]
 \small
  \centering
  \begin{tabular}{ccc}
    \toprule
    {\it Up Index}~&Seen Task  &Unseen Task\\
    \midrule
    1 &85 &{\bf 95} \\
    2 &{\bf90} &80\\
    3 &80 &70\\
    \bottomrule
  \end{tabular}
  \vspace{-0.4em}
  \caption{Results of different up-sampling blocks.}
  \label{tab:upsample}
  \vspace{-1.2em}
\end{table}



\noindent\textbf{Studying the Choice of Denoising Step.}
As shown in Tab~\ref{tab:denosing}, During inference, we fix the upsampling layer to extract features from different denoising steps. We find that, due to the MLP being trained with random noise, features from just one denoising step already allow the model to perform well, achieving 80\% accuracy on seen tasks and 90\% on unseen tasks, with an inference time of 0.6 seconds for 30 step trajectory prediction. Features after 7 denoising steps (1/4 of the total steps) yield the highest success rate. However, the results are worse at 15 (1/2 total steps) and 22 (3/4 total steps), possibly due to the following reason: the features in the early denoising stages, such as the 1st and 7th denoising stage shown in Figure~\ref{fig:step1}, already capture sufficient spatial information to complete the tasks. However, in later denoising stages, the model focuses more on reconstructing detailed visual information, which increases action variance, making the movements jerky of the robot and ultimately reducing the success rate.

\begin{table}[ht]
 \small
  \centering
  \begin{tabular}{cccc}
    \toprule
    {\it Denoising Step}~&Seen Task &Unseen Task & Times(s) \\
    \midrule
    1 &80 &90 & {\bf0.6}\\
    7 &{\bf 85} &{\bf95} & 3.2\\
    15 &80 &80 & 6.5\\
    22 & 70 &80  & 9.3\\
    \bottomrule
  \end{tabular}
  \vspace{-0.4em}
  \caption{Results of different denoising steps.}
  \label{tab:denosing}
  \vspace{-1.6em}
\end{table}

\begin{figure}[ht]
\centering
% \vspace{-1em}
\includegraphics[width=\linewidth]{assets/step1+7.pdf}
\vspace{-2em}
\caption{Visual result of 1 and 7 step DDIM.}
\vspace{-1em}
\label{fig:step1}
\end{figure}

\noindent\textbf{The Effect of Data Scale.}
After achieving strong image generation capabilities after the first training stage using the whole dataset, we train the model using different data scales in the second stage to assess its performance across varying amounts of data.
As shown in Table~\ref{tab:data_scale}, we conduct tests across various data scales and discover that even a small amount of data can lead to promising results on previously encountered tasks. For instance, using just 5\% of the data(116 episodes) is sufficient to achieve a 70\% success rate on seen tasks. The success rate on unseen tasks increases significantly as the amount of data increases.

% \newpage 
\begin{table}[h]
 \small
  \centering
  \begin{tabular}{ccc}
    \toprule
    {\it Data Scale}~&Seen Task &Unseen Task\\
    \midrule
    0.05 &70 &30\\
    0.1 &70 &40\\
    0.2 &80 &60\\
    1 & \bf{85} &\bf{80}\\
    \bottomrule
  \end{tabular}
  \vspace{-0.4em}
  \caption{Results of different data scales.}
  \label{tab:data_scale}
  \vspace{-5mm}
\end{table}

