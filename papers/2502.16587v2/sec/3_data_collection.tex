\section{H\&R Dataset}
\label{sec:pipeline}
\subsection{Data Collection Pipeline}
% Data capture
Our system setup is straightforward, consisting of a Meta Quest 3 VR headset, a 6-DoF xArm robotic arm with a xArm gripper, and two Intel Realsense D435 cameras capturing images of the human hand and robotic arm from the same viewpoint.

The dual-camera setup ensures capturing the synchronized movements of both the human and robotic arms. With the integration of the hardware components, our system aims to achieve a critical objective:
\textit{Producing a paired dataset where each data point reflects a seamless synchronization of human hand action and robotic arm response, captured synchronously from identical viewpoints.}


Towards this goal, we build our H\&R system focusing on developing software and establishing data collection protocols. We build the control system with OpenTeach~\cite{iyer2024open}. When the user puts on the headset and performs several hand operations, the VR headset sends the coordinates of the hand movements of the user to the robotic arm and achieves synchronized actions. 

Achieving this functionality requires addressing issues such as coordinate alignment, precision, and jitter. The details of the implementation are provided in the Supplementary materials~\ref{supp:sys_sta}.

\subsection{Statistics of H\&R Dataset}
With our designed system and data collection pipeline, we propose our H\&R dataset, the first dataset featuring paired human and robotic videos. 
Human events vary from 8 fundamental tasks and 6 long-horizon tasks. The whole dataset includes 2,600 episodes, and each episode contains frames ranging from 300 to 600. Details can be found in Supplementary materials~\ref{supp:dataset}.

To the best of our knowledge, the H\&R dataset is the first video dataset that ensures perfectly aligned video between human and robot.

\begin{figure}[t]
\centering
\vspace{-0.5em}
\includegraphics[width=\linewidth]{assets/data_demo.pdf}
\caption{\textbf{Data demonstration.} The image above shows a sample from our dataset, recorded simultaneously by two cameras—one for the human and one for the robot—while the user wears a VR headset to perform the operation using our pipeline.}
\vspace{-1.5em}
\label{fig:data_demo}
\end{figure}

