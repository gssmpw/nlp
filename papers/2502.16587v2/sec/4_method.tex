\section{\system}
\label{sec:method}
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{assets/framework-revise}
\vspace{-2.0em}
\caption{\textbf{Architecture overview of \system.} Our approach consists of two training stages. In the first stage, only Behavior Extractors and Spatial Layer in both Spatial UNet and Spatial-Temporal UNet are trainable. In the second stage, we only train Temporal Layer and Policy Head. The Pretrained Encoder and Decoder are frozen all the time. In inference, we have two different modes: the KNN-Based mode can complete seen tasks without hand video, while the Manual Guided mode can complete both seen and unseen tasks. Proprio refers to the proprioception of the robot.}
\label{fig:framework}
\vspace{-1.2em}
\end{figure*}
Our goal is to develop a model capable of learning from human demonstrations to predict future robot observations and corresponding actions. Formally, we aim to establish a universal policy $\pi$ that predicts future robot observations $o^r_{t:t+n}$ and action trajectories $a_{t:t+n}$, leveraging inputs from a sequence of hand demonstration videos $o^h_{t:t+n}$, environmental observations $o^r_{t}$, and the current proprioception  $s_t$:
\begin{equation}
a_{t:t+n},\ o^r_{t:t+n} = \pi(o^h_{t:t+n}, o^r_{t}, s_t),
\end{equation} where $n$ represents the length of the action trajectory we aim to predict. 
This setting requires the model to not only predict the executed actions but also visually simulate future interactions between the robot arm and objects.

\subsection{Architecture}
Inspired by the recent success of video diffusion models that are able to predict high-fidelity future frames~\cite{guo2023animatediff}, \system formulates the behavior learning process from human demonstrations as a generative task---generating robot videos and their corresponding actions simultaneously, conditioned on human videos and historical context. Inspired by ~\cite{hu2024animate}, \system explores a Spatial UNet and a Spatial-Temporal UNet, working collaboratively to learn from humans. In particular, the Spatial UNet extracts features from the robot arm, which is further fed into the Spatial-Temporal UNet for temporal modeling. \system also contains two behavior extractors that estimate position and motion clues using human videos. To achieve action prediction, we use the intermediate features from the Spatial-Temporal UNet to predict the action trajectory via a Policy Head. Figure~\ref{fig:framework} gives an overview of our pipeline. Below, we introduce the architecture in detail.

\noindent\textbf{Spatial UNet.} 
The Spatial UNet, consisting of 4 upsampling blocks and 4 downsampling blocks, extracts useful clues from the robot arm. In particular, each block, known as the Spatial Layer, leverages self-attention to learn features tailored for robot arms, as well as cross-attention with pre-extracted CLIP embedding to incorporate semantic clues. The weights of the Spatial-UNet are initialized from those of the Stable Diffusion~\cite{rombach2022high} to ease the learning process. The features derived by Spatial-UNet provide a condition for predicting future frames as well as actions.

While Spatial UNet introduces a similar number of parameters as the denoising UNet, it only extracts features once during the entire process, unlike diffusion-based video generation where each frame undergoes denoising multiple times. Therefore, it does not significantly increase computational overhead during inference.

\noindent\textbf{Behavior Extractor.}
Recall that we aim to learn from human demonstrations, and thus it is important to extract motion and position information from human videos. This is achieved by Behavior Extractor, which contains four convolution layers (4$\times$ kernels, with a stride of 2). To focus more on the motion trajectories, we employ two independent Behavior Extractors. Behavior Extractor 1 takes the image of the hand corresponding to the robot at the first time step, and Behavior Extractor 2 takes the future image or the entire video of the hand as inputs depending on the training stage, as will be introduced in Section~\ref{sec:training}.



\noindent\textbf{Spatial-Temporal UNet.} 
The Spatial-Temporal UNet aims to predict future frames as well as actions to take simultaneously, requiring the model to learn temporal dynamics. Thus, each block of the Spatial-Temporal UNet (ST-UNet in short) has a Spatial Layer which is the same as the one in Spatial UNet (S-UNet in short), followed by an additional Temporal Layer. More specifically, the ST-UNet takes the features from the behavior extractors and noise latent as inputs. Each Spatial Layer in ST-UNet also concatenates the features of the corresponding layer of the S-UNet, exploring the robot arm as references for prediction. The Temporal Layer focuses on temporal modeling so as to produce high-fidelity future frames as well as accurate action trajectories.

\noindent\textbf{Policy Head.} 
We aim to generate future frames as well as actions to take within a single unified framework. Drawing from the insights of~\cite{weng2024genrec}, we propose to divide the UNet mapping function into two distinct components: \(F = F_{tail} \cdot F_{head}\). We treat \(F_{head}\) as a feature extractor for motion information. This is achieved by passing the intermediate features from UNet and proprioception data $s_t$ to a light-weighted Multilayer Perceptron to predict the actions of the robotic arm.

\noindent\textbf{Pretrained Encoder and Decoder.} 
\system builds upon Stable Diffusion~\cite{rombach2022high}, which consists of an encoder and a decoder. The encoder turns images into latent embeddings for fast denoising, and the decoder maps the latents back to images. During training, both the encoder and decoder are kept frozen.



\subsection{Training}
\label{sec:training}
Our approach consists of two training stages. In the first stage, we focus on image generation by taking the first frame of $o^r_t$, the first frame of human $o^h_t$ and future frame of human $o^h_{t+i}$ as inputs to predict the future frame of robot $o^r_{t+i}$. In this stage, we train the S-UNet, the Behavior Extractors, and the ST-UNet without the temporal layers. The spatial layers in both the ST-UNet and S-UNet are instantiated using the powerful open-source Stable Diffusion model (SD) ~\cite{rombach2022high}, while the Behavior Extractors are initialized with Gaussian weights, except for the final projection layer, which utilizes zero initialized convolution.

In the second stage, we concentrate the training effort on the temporal layer. In this stage, we take a 30-frame segment of human video $o^h_{t:t+n}$ and the first frame $o^r_t$ of the robot video $o^r_{t:t+n}$ as inputs to predict the future robot observation $o^r_{t:t+n}$ and action $a_{t:t+n}$. The first frame of the robot video is fed into Behavior Extractor 1, while the entire video segment is passed into Behavior Extractor 2, yielding features $x_1^t$ and $x_2^{t:t+n}$, respectively. Then, we replicate $x_1^t$ by n times and add it to $x_2^{t:t+n}$, which is subsequently added to noise latent and fed to our ST-UNet.
The weights of the VAE Encoder and Decoder, as well as the CLIP image encoder are frozen all the time. During our training process, we find that dense sampling (30fps) results in jittering in generated videos. Therefore, we reduce the frame rate to one-fourth of the original, namely cutting the number of frames by three-quarters.

\noindent\textbf{Optimization Objectives.}
We train \system with both generation and action prediction objectives, aiming for the model to achieve high-quality video generation and accurate action prediction simultaneously. The goal of generation for the model is to predict the applied noise, so the objective of the optimization process is:
\begin{equation}
    L_G = \mathop{\mathbb{E}_{z_{ts},c,\epsilon,ts}}(||\epsilon - \epsilon_\theta(z_{ts},c,ts)||_2^2),
\end{equation}
the parameters of the formula represent the latent state $z_{ts}$ at time step $ts$, which is obtained from Spatial-Temporal UNet. The conditioning variable $c$ includes the first frame $o^r_t$ of the robot and a human video $o^h_{t:t+n}$. $\epsilon$ denotes the added noise, and $\epsilon_\theta(z_{ts},c,ts)$ represents the model's predicted noise.

Our action is divided into three components: $a_t^{pos} \in \mathbb{R}^3$ for 3D position, $a_t^{rot} \in \mathbb{R}^6$ for rotation and $a_t^{grip} \in \{0,1\}$ for gripper state. We use 6D rotation representation proposed in \cite{zhou2019continuity,ke20243d} to guarantee continue representation. We use MSE loss $L_p$ and $L_r$ for position and rotation respectively, and use BCE loss $L_g$ for gripper state.

To balance the generation and action prediction losses, we used a set of balancing weights to control the contribution of each loss to the overall objective.
\begin{equation}
    L = L_G + \lambda_p L_p + \lambda_r L_r + \lambda_g L_g,
\end{equation}
where, $\lambda_p$, $\lambda_r$ and $\lambda_g$ are the weights of $L_p$, $L_r$ and $L_g$ respectively.

\subsection{Inference}
\label{sec:inference}
With the aforementioned designs, \system is able to generate high-quality robot videos and perform actions, using observation captured by the camera of the robot and the provided hand video. We further introduce a set of strategies that  
facilitate more effective task execution by the robot arm, as will be introduced below.

\noindent\textbf{KNN + \system.}
To avoid the need to explicitly provide human demonstration videos for every prediction, we use a k-nearest neighbors (KNN) approach to identify the most probable task for the current scene and retrieve the human demonstration video corresponding to the closest matching features as the conditioning input to guide the task execution. Specifically, we use DINOv2~\cite{oquab2023dinov2} and CLIP~\cite{radford2021learning} as feature extractors to capture features from the first frame of each robotic arm video in the training set. During prediction, we select the $k$ closest features based on the current environment, and the episode with the most frequent and closest match is chosen as the conditioning input, which is depicted in the KNN-Based Inference section of Figure~\ref{fig:framework}.

Using KNN significantly simplifies the process for known tasks. However, for tasks involving different objectives within the same setting---such as pick cube task in Figure~\ref{fig:task_demo} and Table~\ref{tab:comp_baselines}, as well as for unseen tasks such as handwriting, explicit human video guidance is required to accomplish these tasks effectively.

\vspace{0.1cm}
\noindent\textbf{Temporal Addition of Actions.}
To mitigate trajectory jittering during sequential prediction, inspired by the approach used in ACT~\cite{zhao2023learning}, we derive the action not from a single prediction but from a weighted average of multiple prediction steps. In particular, we predict a series of actions $a_{t:t+n}$ at time t, which are then combined with the previously predicted actions to compute the aggregated actions $A_{t:t+n}^{t}$. This represents the set of actions predicted at time $t$, to be executed from time $t$ to $t+n$, and can be computed as follows:
\begin{equation}
A_{t:t-m+n}^{t} = (1-\gamma) A_{t:t-m+n}^{t-m} + \gamma a_{t:t-m+n},
\end{equation}
\begin{equation}    
A_{t-m+n:t+n}^{t}=a_{t-m+n:t+n},
\end{equation}
where $\gamma$ is a hyperparameter, with a larger $\gamma$ indicating a stronger emphasis on the current prediction, and $m$ ($m\leq n$) refers to the length of the action executed after each prediction. Specifically, when $m=n$, the inference degenerates into sequential prediction.
\vspace{0.1cm}

\noindent\textbf{Inference Acceleration.}
During inference, the diffusion model typically goes through $T$ denoising steps to generate the output. We explore the effectiveness of using features from different denoising steps in Section~\ref{sec:ablation}, and find that features obtained from the first half of the denoising stages lead to better results. Therefore, to balance speed and quality, we halt the denoising process after obtaining features from the $\frac{T}{4}$ step, directly predict and execute the action, without waiting for the entire denoising process to complete. It is worth noting that if we aim to achieve the fastest possible speed, we can directly use the one-step denoising features for prediction, which allows us to predict the next 30 steps of actions within 0.6 seconds.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{assets/task_demo.pdf}
\vspace{-1.6em}
\caption{\textbf{Qualitative results.} Visualization of generated result and robot action result.}
\vspace{-1em}
\label{fig:task_demo}
\end{figure*}
