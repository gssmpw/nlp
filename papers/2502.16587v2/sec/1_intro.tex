\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{assets/overview.pdf}
\caption{\system: An end-to-end generative framework that simultaneously predicts robotic videos and actions.} 
\vspace{-1.2em}
\label{fig:overview}
\vspace{-0.4em}
\end{figure}
The ability to observe others, whether people or animals, and learn skills to solve new tasks is a key reason humans can handle a wide range of complex challenges. To develop robots capable of assisting with various everyday problems, they must also acquire this skill, particularly the ability to learn directly from humans. This motivates a plethora of work studying how to effectively learn from human demonstrations~\cite{bahl2023affordances,srirama2024hrp,nair2022rm,wang2023mimicplay,bahl2022human,smith2019avid}. 
However, due to the discrepancies between humans and robots, existing work~\cite{wang2023mimicplay,nair2022rm,zeng2024learning} struggles to transfer learned representations from humans to robots, or simply enforce consistent outputs between humans and robots~\cite{bahl2022human} without considering motion trajectories.

In this paper, we advocate that fine-grained alignment between humans and robots is of great significance. Coarse-grained data provides limited clues for key points and cannot capture detailed information, such as different movement trajectories or grasping positions. While annotating detailed correspondences between humans and robots is feasible~\cite{jain2024vid2robot,liu2018imitation}, it is time-consuming and labor-intensive. Fortunately, virtual reality(VR)-based teleoperation~\cite{iyer2024open,ding2024bunny,qin2023anyteleop,cheng2024open} offers a promising way to collect synchronized actions of humans and robots, making it efficient and affordable. This motivates us to build upon VR-based teleoperation for studying paired data that can improve robot learning. In particular, we focus on obtaining perfectly aligned data between humans and robots as a testbed for learning from human demonstrations.

To this end, we introduce a novel data collection pipeline for gathering paired videos of human hands and robotic arms, aligned frame by frame. Unlike previous work that solely collects demonstration data, our method requires minimal additional setup, equipment, or time, making it exceptionally easy to implement and facilitating efficient and consistent data collection. We alse introduce the H\&R dataset, a third-person dataset featuring perfectly aligned videos of human hands and robotic arms, which includes 8 fundamental tasks and 6 long-horizon tasks, totalling 2,600 episodes. 
Our goal is to train a model that mimics human actions and performs corresponding tasks based on human demonstrations. This requires the robot to understand how its end effector is aligned with human hands, which is readily available in our dataset, but more importantly, has the ability to act correspondingly. Therefore, the robot needs to predict what actions to take in the future conditioned on human motions as well as historical clues. This is similar in spirit to video generation tasks, that predict future frames (actions) based on various cues~\cite{dhariwal2021diffusion,hertz2022prompt,ho2022classifier,ho2022cascaded,meng2021sdedit,nichol2021improved}. As a result, we build upon video generation models to mimic human actions.


In light of this, we introduce \system , an end-to-end generative framework that predicts robotic videos and actions to take simultaneously. 
In particular, \system is built upon a pretrained Stable Diffusion model, including a spatial UNet for feature extraction, two behavior extractors for motion and position encoding, a spatial-temporal UNet that leverages temporal clues for frame and action prediction. As a result, \system is not only able to generate high-quality robotic videos, 
which facilitating the expansion of robot-conditioned policies to human-conditioned ones, but can also predict corresponding action trajectory required to perform the task.
With carefully designed training strategies, \system not only excels on seen tasks but also generalizes to different positions, unseen appearances, instances and even new backgrounds and task types without additional training. Furthermore, we introduce a KNN-based approach, enabling \system to perform seen tasks even without human demonstrations, providing a flexible and scalable solution to robotic learning from demonstrations.

In summary, our main contributions include:
\begin{itemize}
    \item We introduce a novel low-cost, low-requirement, high-efficiency, and highly scalable pipeline for collecting perfectly aligned data of humans and robots at the frame level. Additionally, we present H\&R, the first dataset featuring perfectly aligned videos of human hands and robotic arms across a variety of tasks, enabling high-fidelity learning from human demonstrations.
    \item We introduce \system, an end-to-end generative framework built upon a diffusion-based model, capable of generating robotic videos from human videos while simultaneously predicting the corresponding robotic action trajectories. This approach combines state-of-the-art video generation techniques with multi-task action prediction, facilitating accurate task execution. 
    \item We propose a KNN+\system method, which integrates KNN for task prediction, enabling to perform tasks even without human hand videos as input. This further enhances the scalability and flexibility of the system. 
    \item \system can generate high-quality robotic arm videos, which facilitates the expansion of robot-conditioned policy to human-conditioned policy, while also excelling in four carefully selected tasks, even with variations in positions, appearances, instances, backgrounds and different task types.
\end{itemize}