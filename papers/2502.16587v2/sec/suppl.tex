% \clearpage
% \setcounter{page}{1}
% \maketitlesupplementary
\appendix

\section{System Setup Details}
\label{sec:detail_system_setup}
The H\&R system is equipped with state-of-the-art technology specifically chosen to facilitate seamless and precise interactions between human operators and robotic mechanisms, including a Meta Quest 3 VR headset, a 7-DoF xArm robotic arm, and two Intel Realsense D435 cameras as shown in Figure~\ref{fig:human_operating_space}~\ref{fig:robot_operating_space}.

The Meta Quest 3 VR headset is selected for its advanced gesture recognition capability and affordable cost. This gesture recognition capability is integral to our data collection pipeline, providing an efficient and user-friendly way to enhance the natural interaction between human operators and the robotic system.
The Xarm robotic arm, which features seven degrees of freedom, provides the flexibility and precision required for complex and varied tasks. This arm is integral to replicating human hand movements accurately, ensuring that the physical actions translated through the VR system are executed precisely by the robotic counterpart.
% Data capture
Visual data collection is handled by two Intel Realsense D435 cameras strategically positioned to monitor both human and robot actions. These cameras operate at a resolution of $240\times424$ pixels, capturing images at 30Hz. The dual-camera setup captures the synchronized movements of both the human and robotic arms. With the integration of the hardware components, our system aims to achieve a critical objective: 

\begin{abox}
Producing a paired dataset where each data point reflects a seamless synchronization of human hand action and robotic arm response, captured synchronously from identical viewpoints.
\end{abox}

Toward this goal, we build our H\&R system to develop software and establish data collection protocols. We build the control system with OpenTeach~\cite{iyer2024open}, connecting different hardware components to enable the operator to perform manual tasks and simultaneously monitor the robotic arm’s actions through the VR headset display. Specifically, we set up two similar environments for human operation and robotic arm operation respectively, as shown in Figure~\ref{fig:operating_space}, and one camera is placed in each operating space. 
When the user wears the VR headset and performs hand movements, the headset transmits the coordinates of these movements to the backend, where they are retargeted to control the robotic arm, enabling synchronized actions.
Section~\ref{sec:detail_data_collection} illustrates more details about the retargeting process. Here, one of the biggest challenges lies in the inconsistency of the scale and the orientation between the two coordinate systems. In the following, we mainly illustrate our implementation for achieving the coordinate alignment.

% section~\ref{para:position}. During the data collection pipeline. 
% At the same time, when the user puts on the headset, it creates a coordinate system $C_h$ , as illustrated in Figure~\ref{fig:pipeline:alignment}. 

% During subsequent operations, the VR headset sends the coordinates of the user’s hand relative to $C_h$ to our program. Similarly, the robotic arm has its own coordinate system $C_r$, which is very similar to the one shown in Figure~\ref{fig:pipeline:alignment}. However, the scale and orientation of these two coordinate systems are not aligned, which is the key issue we need to address to achieve precise control.

\section{Data Collection Details}
\label{sec:detail_data_collection}
% 前期准备：记录机器臂的anchor，设置初始位置。
% 收集数据：带上头显，把手放到三个anchor上，程序记录位置。左手放置两边物体，用手对其位置。开始操作，左手按下记录键，右手开始收集，按下停止键，停止收集。
% 在前期准备阶段，我们会记录四个机器臂位姿的信息，包括三个机器臂的锚点，还有一个机器臂的初始位置，它们会在下面的部分[cite]详细介绍。
% 每次开始收集数据时，机器臂会回到初始位置，人带上头显开始操作。此时，机器臂不会跟随手移动，直到人通过停留的方法设置好人的操作空间中的三个锚点为止。我们的程序会使用这六个锚点来对其两边的坐标系，这会在下面的部分[cite]详细介绍。由于头显每次启动坐标系并不完全相同，所以在正式开始收集之前，还需要根据实际情况对环境中的物体的位置进行微调。
% 在我们收集数据的过程中，我们会用右手进行操作，而用左手按键盘来控制开始记录和结束记录。

While collecting data, the operating spaces are shown in Figure~\ref{fig:operating_space}. 
During the preparation phase, we will record the positional information of four points within the robot's operating space: three anchor points and the initial position of the robotic arm. The starting position of the robotic arm is recorded and detailed anchor configurations are provided in Section~\ref{sec:alignmet}. Operators are required to wear headsets to perform operations throughout the data collection process. The robotic arm will not follow the hand's movements until the operator sets three anchor points in their workspace by putting hand on each anchor for a brief period. Our program utilizes six anchor points—three for the human and three for the robot—to align the coordinate systems of both sides, as detailed in Section~\ref{sec:alignmet}. Since the coordinate system initiated by the headset may vary slightly each time, it is necessary to make a few adjustments to the positions of objects in the environment before starting the actual data collection. 
During the data collection process, we will use the right hand for demonstration, while the left hand will control the start and end of recording by pressing the keyboard.
\subsection{Coordinate System Alignment}
\label{sec:alignmet}
% 这里我们详细讲一下我们的anchor-based method是如何对齐头显的坐标系和机器臂的坐标系的。
In this section, we provide a detailed explanation of how we align the position and rotation of the coordinate systems for the VR headset and the robotic arm.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{assets/hand_coordinate.pdf}
\vspace{-0.5em}
\caption{VR headset Coordinate System.}
\label{fig:pipeline:alignment}
\vspace{-0.5em}
\end{figure}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/human_space.pdf}
        \caption{Human Operating Space.}
        \label{fig:human_operating_space}
    \end{subfigure}
    \hspace{0.1cm}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/robot_space.pdf}
        \caption{Robot Operating Space.}
        \label{fig:robot_operating_space}
    \end{subfigure}
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/data_detailed_demo.pdf}
        \caption{Data demonstration.}
        \label{fig:data_detailed_demo}
    \end{subfigure}
\caption{The operating spaces of human and robot}
\label{fig:operating_space}
\end{figure*}

\vspace{-0.3cm}
\noindent\textbf{Position alignment.}
\label{para:position}
We utilized the introduced anchor points to align the coordinate systems. As illustrated in Figure~\ref{fig:pipeline:alignment}, we select three anchors within the operating space and derive two mutually perpendicular vectors, $e_h^x$ and $e_h^y$ for the human side, and $e_r^x$ and $e_r^y$ for the robot side. These are accompanied by the coordinates of the bottom-right anchor point, $o_h$ and $o_r$, and are subject to the following constraints in the real world, $|e_h^x|_w=|e_r^x|_w$, $|e_h^y|_w=|e_r^y|_w$ and $ e_h^x \cdot e_h^y = e_r^x \cdot e_r^y = 0$, where $|\cdot|_w$ indicates the real-world distance. From these vectors, we calculate the unit vectors $e_h^z$ and $e_r^z$, which are perpendicular to $e_h^x$ and $e_h^y$ respectively.

Then, given any hand coordinate $p_h$, the corresponding robotic arm coordinate $p_r$ can be computed as:
\begin{equation}
p_r = o_r + \mu_x e_r^x + \mu_y e_r^y + \eta \mu_z e_r^z,
\end{equation}
where $\eta$ is a hyperparameter that represents the scaling factor of motion amplitude along the z-axis. The value of $\mu$ can be computed as follows:
\begin{equation}
\mu_i = \frac{e_h^i\cdot (p_h - o_h)}{|e_h^i|^2}, i \in \{ x, y, z \}.
\end{equation}
\noindent\textbf{Rotation alignment.}
After aligning the positions, additional alignment is necessary for the orientations. Our goal is to ensure that the relative rotations of the human hand and the robotic arm, relative to their initial states, remain consistent. 

To synchronize the rotation of the robotic arm with the human hand, we establish a 3D coordinate system similar to the approach taken by OpenTeach\cite{iyer2024open}. This coordinate system enables us to compute the rotation matrix $M_h^t$ for the human hand at time $t$. Additionally, we derive the rotation matrix $M_r^t$ for the robotic arm at the same time $t$, using the robot's state parameters such as pitch, yaw, and roll. The rotation matrix $M_h^t$ and $M_r^t$ at the time $t$ can be expressed in terms of the rotation matrices $M_h^0$ and $M_r^0$ at time $t = 0$ as:
\begin{equation}
    M_h^t = M_h^0 R_h^t,
    \label{equ:rotation_h}
\end{equation}
\begin{equation}
    M_r^t = M_r^0 R_r^t,
    \label{equ:rotation_r}
\end{equation}
where $R_h^t$ and $R_h^t$ represent the rotation relative to the coordinate system $M_h^0$ and $M_r^0$ respectively. Recall that our goal is to ensure the relative rotations are the same, which allows us to derive the following equation:
\begin{equation}
    R_r^t = P^{-1}R_h^tP,
    \label{equ:rotation_relative}
\end{equation}
where $P$ is the transformation matrix defined from the basis of the coordinate system $M_r^0$ to that of $M_h^0$.

Organize Equation~\ref{equ:rotation_h}, ~\ref{equ:rotation_r}, and~\ref{equ:rotation_relative}, and the target rotation matrix of the robotic arm can be calculated as follows:
\begin{equation}
M_r^{i} = M_r^0 P^{-1} (M_h^{0})^{-1} M_h^{i} P,
\end{equation}

% \section{Supplementary Experiment Results}
\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{assets/dataset_overview.pdf}
\vspace{-1.6em}
\caption{\textbf{Overview of the dataset for base tasks.} }
\vspace{-1em}
\label{fig:dataset_overview}
\end{figure*}

\subsection{System stability}
\label{supp:sys_sta}
\noindent\textbf{Accuracy.} 
There are two main factors affecting control accuracy: one is the precision of the data source, i.e., the accuracy of hand gesture recognition, and the other is the accuracy loss caused by mapping hand movements to the robotic arm. Our gesture recognition data comes from the Meta Quest 3, and we found that under sufficient lighting conditions, it provides highly accurate hand tracking.

A traditional mapping approach is to map the wrist position to the  end-effector position of robot. However, this creates a topological inconsistency, leading to accuracy errors. A more topologically consistent approach is to map the midpoint between the thumb and index fingertip. While this method preserves structural consistency, it introduces jitter due to the differing speeds of the thumb and index finger when grasping objects. Thus, we track the Metacarpophalangeal Joint (MCP) of the index finger, ensuring both the prevention of jitter and the maintenance of accuracy.

\noindent\textbf{Jitter Handling.}
We set the control mode to serial, making the system’s sensitivity depend on execution speed. By allowing 100–300ms latency, we trade speed for stability, reducing jitter and ensuring smoother motion.


\input{sec/f_generalization_demo}


\section{H\&R Details}
\label{supp:dataset}
\subsection{Task Details}
Our H\&R dataset contains 8 base tasks and 6 long-horizon tasks. The definition of our tasks in our dataset is listed below.

\noindent\textbf{Base tasks.}
The visual representations of the tasks are shown in Figure~\ref{fig:dataset_overview}.
\begin{itemize}
    \item Pick and place the cup: The cup can initially be placed anywhere on the table. The goal of this task is to pick up the cup and place it in another location.
    \item Pick and place the cube: The cube comes in two colors (red and green), and it can initially be placed within a designated area. The goal of this task is to pick up the cube and place it on a plate.
    \item Pick and place pencil: The pencil comes in two colors (red and black), the goal is to pick up the pencil and place it on a plate.
    \item Push the box: The box comes in two colors (red and green), and its initial position of it is random. The goal is to push the box from left to right
    \item Pull the plate: The goal is to pull the plate from bottom to top.
    \item Push the plate: The goal is to push the plate from top to bottom.
    \item Pick up and hold the brush upright: The goal is to pick up the brush and upright it.
    \item Handwriting: Play data, write aimlessly on a blank desk.
\end{itemize}

\noindent\textbf{Long-Horizon tasks.}
These tasks are a combination of the base tasks.
\begin{itemize}
    \item Pick both cubes: There are two red cubes on the table, and the goal is to pick up each cube individually and place them onto the plate.
    \item Pick the designated cube: There is a red cube and a green cube on the table, and the goal is to pick up one of the cubes onto the plate.
    \item Pick to the designated plate: There is a red cube and two plates on the table, and the goal is to pick up the cubes onto one of the plates.
    \item Pick the cube and pull the plate: The combination of pinch a cube and pull the plate. The goal is to first pick the cube onto the plate and pull the plate.
    \item Pull the plate and pick the cube: The combination of pull the plate and pinch a cube. After pulling the plate, the cube should be placed in the plate's new position.
    \item Return to the initial position: Complete the base tasks and return to the initial position.
\end{itemize}

\subsection{Episode Details}
H\&R consists of 2,600 episodes, each containing the following information, adhering to the RT-X standards:
\begin{itemize}
    \item a paired human hand and robotic arm video containing frames between 200 and 600, with variations depending on the task.
    \item state of the robotic arm at each timestamp, including position and rotation.
    \item joint of the robotic arm velocity at each timestamp.
    \item transform matrix of the human hand, including position and rotation.
    \item position of each key point of the human hand.
    \item action of the robotic arm retargeted based on human hand data.
    \item timestamp.
\end{itemize}


\section{Experimental Details}
\subsection{Experimental Details}
\noindent\textbf{Implementation Details.} 
To demonstrate the effectiveness of our approach, we train our model on H\&R. We used 4 NVIDIA A100 GPUs for training in the first stage and 8 4090 GPUs for the second stage. In the first training stage, we use the native image size of 424×240 and conduct training for 15,000 steps and set the total batch size as 128. In the second training stage, the temporal layer and policy header are trained for 50,000 steps using 30-frame video sequences and the total batch size is 8. The learning rates of the two stages are set to 1e-5, and the weights of each objective are set to $\lambda_p = \lambda_r = \lambda_g = 1$. During inference, we utilize a DDIM sampler for 30 denoising steps and extract features from the first upsample layer and the 7th denoising step for action prediction. The smoothing parameter $\gamma$ is set to 0.5.

\subsection{Baseline Details}
\label{supp:baselines}
\noindent\textbf{RVCD Policy.} Here we talk about how we add conditions to ACT~\cite{zhao2023learning} in detail. We input the observation information of the robotic arm into the encoder. Then, we add the features of the next $n$ frames of the human hand as a condition to the position embedding and input them into the decoder. Specifically, similar to the method used for processing the original robot images, we input human images into ResNet18~\cite{he2016deep} and obtain a feature map of size 300x512. We then pass it through a shared MLP to obtain a feature of size 512, which is added to the position embedding and fed into the Transformer decoder. Both the ResNet18~\cite{he2016deep} and MLP are trainable.

\noindent\textbf{XSkill.}
We replicated xSkill in a real-world setting, where the authors did not fully open-source the code. As a result, we completed the remaining code based on the code from the simulation environment. During the skill discovery and labeled dataset phases, we used the default settings, except for adjusting the num frames parameter of the frame sampler from 100 to 50 to reduce GPU memory consumption. We train this stage for 300 epochs.

In the skill transfer composing phase, we found that performance of XSkill was quite poor. Although the behavior cloning loss decreased significantly, the diffusion policy barely demonstrated any task execution ability after denoising. Since the XSkill~\cite{xu2023xskill} paper mentions that any imitation learning policy can be used with the XSkill framework, we replaced the diffusion policy with an action head the same as \system, a simple MLP, for training. During training, we adjusted the prediction horizon to 32, resized the pretraining to 160x120 to maintain consistency with the previous training process, and set the vision feature dimension to 64, in line with the simulation environment. We trained this stage for a total of 2000 epochs until the loss showed no significant changes.