\section{Related Work}
\label{sec:realted_work}

% \paragraph{Teleoperation.}
\noindent\textbf{Teleoperation.} 
Researchers have developed various forms of teleoperation approaches for data collection~\cite{liu2023libero, DexPilot,sivakumar2022robotic,qin2023anyteleop,song2020grasping, arunachalam2022dexterous, arunachalam2023holo, MVP, george2023openvr, zhao2023learning, fu2024mobile, aloha2team2024aloha2enhancedlowcost}.
 However, these approaches may entail specific device requirements. Recently, VR-based methodologies~\cite{iyer2024open, ding2024bunny} have attracted considerable attention due to their cost-effectiveness, efficiency and versatility.
 However, these methods often overlook the importance of the paired visual information. We leverage this perception to capture perfectly aligned videos of humans and robots, which are crucial for robotic imitation.
 
\vspace{0.1cm}
\noindent\textbf{Learning from Human Videos.}
Researchers have recently been attempting to leverage existing human-centric video datasets to enhance robot policy learning~\cite{liu2018imitation, smith2020avid, chen2021learning, videodex, zeng2024learning}. Researchers propose to learn representations from human videos to assist in task execution~\cite{xiao2022masked, wang2023mimicplay, majumdar2024searchartificialvisualcortex}. However, these approaches need substantial prior knowledge and struggle to transfer to robots~\cite{nair2022rm, bahl2023affordances, bahl2022human}. Meanwhile, some human-video-conditioned methods ~\cite{gen2act, jain2024vid2robot, xu2023xskill} have focused on aligning representations across human and robot videos, they are neither efficient nor capable of generalization. In contrast, our approach utilizes paired data and diffusion models to achieve strong generalization capabilities. 

\vspace{0.1cm}
\noindent\textbf{Diffusion Models for Video Generation.} 
Due to the remarkable performance of diffusion models in image generation~\cite{dalle2,ldm,glide,imagen,ediffi}, many approaches have explored their potential for video generation~\cite{vdm,gen1,align,tuneavideo,text2videozero,simda,tu2024motioneditor,tu2024motionfollower,tu2024stableanimator}. Moreover, conditionally controlled video generation~\cite{LFDM,dreampose,disco,hu2024animate, vidiff} has broader and more practical application scenarios.
In recent work, several approaches~\cite{cheang2024gr,wu2023unleashing,gen2act} have utilized generative diffusion models to aid policy learning for robots. However, these methods primarily generate videos from a conventional human perspective, often overlooking the distributional differences between human and robotic actions.
In contrast, we introduce a novel approach by applying video diffusion models to robotic scenarios, utilizing a generative model conditioned on human behaviors to guide robotic action outputs.

\vspace{0.1cm}
\noindent\textbf{Diffusion Models for Visual Understanding.}
% \paragraph{Diffusion model for visual understanding.} 
The diffusion model not only performs impressively in generative domains but also shows promising applications in visual understanding tasks~\cite{yu2024representation, senane2024self, luo2024learning, instructdiffusion}. Some notable approaches involve using diffusion models for object detection~\cite{diffusiondet}, image segmentation~\cite{xu2023open}, and visual representation learning~\cite{yu2024representation, chen2024deconstruct}.  The approach most similar to ours, GenRec~\cite{weng2024genrec} proposes a unified video diffusion model that jointly optimizes video generation and video classification, exhibiting the effectiveness of the diffusion model in video understanding. Our approach is designed for robotic tasks, constructing a human-to-robot video generative model that can simultaneously predict actions for manipulation tasks.

