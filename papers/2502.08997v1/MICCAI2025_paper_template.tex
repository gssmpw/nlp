% This is a modified version of Springer's LNCS template suitable for anonymized MICCAI 2025 main conference submissions. 
% Original file: samplepaper.tex, a sample chapter demonstrating the LLNCS macro package for Springer Computer Science proceedings; Version 2.21 of 2022/01/12

\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encodings may result in incorrect characters.
%
\usepackage{graphicx,verbatim}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor}
\definecolor{mydarkblue}{RGB}{38,84,124}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{hyperref}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm}
%
\begin{document}
%
\title{Hierarchical Vision Transformer with Prototypes for Interpretable Medical Image Classification}
\titlerunning{Hierarchical Vision Transformer with Prototypes}
%
\begin{comment}  %% Removed for anonymized MICCAI 2025 submission
\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Princeton University, Princeton NJ 08544, USA \and
Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}

\end{comment}

% \author{Anonymized Authors}  %% Added for anonymized MICCAI 2025 submission
% \authorrunning{Anonymized Author et al.}
% \institute{Anonymized Affiliations \\
%     \email{email@anonymized.com}}
    
\author{Luisa Gallée\inst{1,4}\orcidID{0000-0001-5556-7395} \and  
Catharina Silvia Lisson\inst{2} \and
Meinrad Beer\inst{2,3,4,5}\orcidID{0000-0001-7523-1979} \and
Michael Götz\inst{1,2,3,4,5}\orcidID{0000-0003-0984-224X}}
%
\authorrunning{L. Gallée et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%

\institute{Experimental Radiology, Ulm University Medical Center, Germany \email{luisa.gallee@uni-ulm.de} \and
Department of Diagnostic and Interventional Radiology, Ulm University Medical Center, Germany \and
i2SouI - Innovative Imaging in Surgical Oncology Ulm, Ulm University Medical Center, Germany \and
XAIRAD - Cooperation for Artificial Intelligence in Experimental Radiology, Germany \and
BGZ - Bildgebungszentrum, Ulm University Medical Center, Germany
}
    
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% The abstract should briefly summarize the contents of the paper in 150--250 words.  If you are to include a link to your Repository, please make sure it is anonymized for the double-blind review phase.
Explainability is a highly demanded requirement for applications in high-risk areas such as medicine. Vision Transformers have mainly been limited to attention extraction to provide insight into the model's reasoning. Our approach combines the high performance of Vision Transformers with the introduction of new explainability capabilities.
We present HierViT, a Vision Transformer that is inherently interpretable and adapts its reasoning to that of humans. A hierarchical structure is used to process domain-specific features for prediction. 
It is interpretable by design, as it derives the target output with human-defined features that are visualized by exemplary images (prototypes). By incorporating domain knowledge about these decisive features, the reasoning is semantically similar to human reasoning and therefore intuitive.
Moreover, attention heatmaps visualize the crucial regions for identifying each feature, thereby providing HierViT with a versatile tool for validating predictions.
Evaluated on two medical benchmark datasets, LIDC-IDRI for lung nodule assessment and derm7pt for skin lesion classification, HierViT achieves superior and comparable prediction accuracy, respectively, while offering explanations that align with human reasoning.

\keywords{Explainable AI  \and Hierarchical Prediction \and Prototype Learning \and Vision Transformer.}
% Authors must provide keywords and are not allowed to remove this Keyword section.

\end{abstract}
%
%
%

\section{Introduction}
\label{sec:introduction}


Since their adaptation from NLP to computer vision in 2021, Vision Transformers (ViTs) have revolutionized image processing, excelling in areas like image segmentation and self-supervised learning \cite{dosovitskiy2021vit,Kirillov_2023_ICCV,NEURIPS2021_64f1f27b,Caron_2021_ICCV,He_2022_CVPR}. 
In the field of explainable AI, ViTs inherently provide an initial insight into the model's logic through attention extraction \cite{dosovitskiy2021vit}. However, in high-risk domains like medicine, interpretability must extend beyond visual attention alone \cite{rudin2019stop}. Many existing explainable AI approaches are designed for Convolutional Neural Networks (CNNs) and do not directly apply to ViTs, requiring adaptations for this new architecture.

A promising line of research involves \textbf{hierarchical models}, which closely align with human decision-making by structuring predictions through step-by-step reasoning. Also, \textbf{prototype-based models} have made a significant impact in explainable AI by providing tangible, case-based examples that help ground a model’s logic \cite{NEURIPS2019_adf7ee2d,gallee2023interpretable}. While prototype learning has been applied to ViT-based backbones \cite{xue2922ProtoPFormer,xu2024Interpretable,demir2024explainable}, existing approaches primarily focus on highlighting key areas of attention. However, for complex medical applications, further explanation is required to justify why specific regions are relevant for a given prediction \cite{rudin2019stop,reyes2020interpretability}. Our proposed method addresses this limitation by integrating domain knowledge to generate prototypes that represent predefined, clinically meaningful features. The \textbf{integration of domain knowledge} about discriminative features has largely been absent in Vision Transformer architectures. While Rigotti \textit{et al.} \cite{rigotti2022attentionbased} introduce attention mechanisms for user-defined concepts in ViTs, their approach is limited to binary attributes and has only been evaluated on general-domain data.
With  each of these approaches offering individual advantages, a recent trend is to combine those approaches to benefit from the complementary aspects of each interpretable tool  \cite{gallee2023interpretable,reyes2020interpretability}. 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{miccai25_fig1.png}
%     \caption{\textbf{HierViT} A hierarchical decision process first extracts high-level features (attributes) and the region of interest. Target prediction is based on the extracted attribute information. The human-defined attributes are interpretable, explained by a rating, an attention heatmap, and a prototype image illustrating a similar sample.}
%     \label{fig:hvitga}
% \end{figure}

With our research, we aim to address the need for more explainable approaches for high-performing Vision Transformers by incorporating recent trends from explainable CNNs. Our proposed model, HierViT, transforms the Vision Transformer into an interpretable tool by adapting established strategies and integrating  prototype learning, domain knowledge, and a hierarchical, feature-focused prediction strategy.
% (see Fig. \ref{fig:hvitga}). 
Just as radiologists rely on a structured approach to evaluate features before reaching a conclusion, HierViT mirrors this by identifying essential criteria prior to final output. This method fosters AI outputs that can be evaluated with statements like: "The AI recognized the pathological structure accurately," or "It missed essential features, indicating an unreliable prediction." By aligning model reasoning with human-defined criteria, HierViT enhances user trust, as supported by empirical studies \cite{gallee2024evaluating}, while also outperforming previous CNN-based approaches.

Our work leverages the unique potential of Vision Transformers for developing inherently interpretable models. The novelties presented in this work are summarized as follows:
\begin{itemize}
    \item \textbf{Hierarchical ViT}: We present, to the best of our knowledge, the first ViT-based model to integrate hierarchical prediction with feature-specific prototype learning for image classification.
    \item \textbf{Multimodal interpretability}: HierViT combines predefined feature reasoning through feature scores, case-based prototypes, and attention visualizations, enabling prediction validation.
    \item \textbf{State-of-the-art (SOTA) performance}: HierViT achieves superior prediction performance on the medical benchmark dataset LIDC-IDRI, and comparable performance on the derm7pt dataset.
\end{itemize}

The code is publicly available at \url{https://github.com/XXX}.

\section{Method}

The proposed model uses a ViT encoder with twelve layers as described by Dosovitskiy \textit{et al.} \cite{dosovitskiy2021vit} as backbone  and weights pre-trained on ImageNet-1K. Two branches derive from the extracted features (see Fig. \ref{fig:modelarchitecture}). 

% todo bei backbon die 2 token ein mu nach unten
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{miccai25_fig2.png}
    \caption{\textbf{Proposed model} The patchified image is linearly projected and processed by a transformer encoder, producing a token vector that serves as the input for both a hierarchical classifier and a decoder. The hierarchical classifier processes the token vector through multiple transformer layers, one for each attribute, with individual heads providing attribute ratings. For target prediction, the token vectors from the attribute layers are stacked and further processed by the target branch. The optional decoder segments a region of interest mask.}
    \label{fig:modelarchitecture}
\end{figure*}

The first branch functions as a hierarchical classifier, mapping extracted features to predefined attributes for target classification.
Each attribute score is calculated using an individual transformer encoder and a linear layer.
The loss function term for attribute learning $\mathcal{L}_{attr}$ minimizes the mean value of the classification error $\mathcal{L}_{class}$ over all attributes. In the following $y_{attr_a}$ is the ground truth label of attribute $a=1...A$, and $\hat{y}_{attr_a}$ is the respective prediction:
\begin{equation}
\mathcal{L}_{attr} = \frac{1}{A} \sum^A_a \mathcal{L}_{class}(y_{attr_a},\hat{y}_{attr_a}).
\end{equation}

Prototypes serve as visual examples of the extracted attributes and are derived from the attribute features. Each prototype layer consists of a set of learnable vectors representing different attribute values, thereby enabling the mapping of diverse characteristics within the same attribute value. 
The loss function $\mathcal{L}_{proto}$ encourages training samples to be similar to the prototypes of the correct attribute class. It is implemented by the Euclidean distance between the sample's attribute vector $\vec{c}^{\,a}$ and the prototypes $\vec{p}^{\,a,p}$, where $a$ denotes the respective attribute, and $p$ the index of the prototype vector of the correct class $P_{a}$, where $p=1...16$: 
\begin{equation}
    \mathcal{L}_{proto} = \frac{1}{A} \frac{1}{P} \sum^A_a \sum^{P_a}_p \left\Vert \vec{c}^{\,a}-\vec{p}^{\,a,p}\right\Vert_2 .
\end{equation}
A push operation saves for each prototype vector a sample from the training dataset whose attribute vector is closest. This step allows the prototypes to be visualized with real images, as shown in section \ref{subsec:explainability}. 
The repetition rate of the push operation is determined by the hyperparameter \textit{push step=2}.

Following the attribute extraction, the target is predicted based on the encoded attribute information. All tokenized attribute vectors are stacked for processing. A linear layer combines these features into a single token vector, which is then processed by a target transformer encoder. Finally, classification is performed by a linear layer on the classification token.
The target loss function $\mathcal{L}_{tar}$ represents the classification error between the ground truth $y_{tar}$ and the predicted value $\hat{y}_{tar}$:
\begin{equation}
\mathcal{L}_{tar} = \mathcal{L}_{class}(y_{tar},\hat{y}_{tar}).
\end{equation}
Depending on the scale of the data, either the mean square error (MSE) or the cross entropy loss (CSE) was chosen for the classification loss function $\mathcal{L}_{class}$:
\begin{equation}
\mathcal{L}_{class} = 
\begin{cases} 
\text{MSE}(y, \hat{y}) & \text{for ordinal data (LIDC-IDRI),} \\ 
\text{CSE}(y, \hat{y}) & \text{for nominal data (derm7pt).}
\end{cases}
\end{equation}

The second branch is an optional ViT-based decoder for creating a segmentation mask if labels are available. Symmetrically to the encoder, twelve transformer layers process the image tokens of the ViT backbone. 
% To restore the image size from the tokenised vector, it is first reshaped from the flattened dimension to two-dimension. A transposed convolution then scales the patches to pixel size, followed by batch normalization, sigmoid activation, and averaging along the hidden dimension to return the original image size.
The segmentation loss term $\mathcal{L}_{seg}$ calculates the mean square error between the segmentation mask label $y_{mask}$ and the decoder prediction $\hat{y}_{mask}$:
\begin{equation}
    \mathcal{L}_{seg} =  \text{MSE}(y_{mask}, \hat{y}_{mask})
\end{equation}

\paragraph{Training Algorithm}
The model can simultaneously address several semantically related tasks, including object region segmentation, extraction of specific high-level visual features, target prediction based on these features, and generation of attribute-specific prototypes. Previous work \cite{gallee2023interpretable} shows that prototype learning should begin only after a warm-up phase, during which the model weights are adjusted to the core task. The loss function is therefore composed as follows for the warm-up phase: 
\begin{equation}
    \mathcal{L}_{warm-up} = \mathcal{L}_{tar}+\mathcal{L}_{attr}+\mathcal{L}_{seg}, 
\end{equation}
and for the final phase:
\begin{equation}
    \mathcal{L}_{final} = \mathcal{L}_{tar}+\mathcal{L}_{attr}+\mathcal{L}_{seg}+\lambda_{proto}\cdot\mathcal{L}_{proto}.
\end{equation}
The hyperparameter $\lambda_{proto}$ was set to $0.01$ in order to maintain a focus on the primary task.




\section{Experiments and Results}

\subsection{Datasets}

\paragraph{LIDC-IDRI}
% The Lung Image Database Consortium and Image Database Resource Initiative (CC BY 3.0 licensed) \cite{armato_iii_lung_data_2015} is a large, extensively annotated medical dataset of chest CT scans from patients with non-small cell lung cancer. Up to four radiologists segmented lung nodules and labeled their appearance and malignancy \cite{armato_iii_lung_2011}. In our experiments, we used lung nodule cropout images as input, segmentation masks as decoder targets, malignancy ratings as prediction targets, and appearance ratings (subtlety, internal structure, calcification, sphericity, margin, lobulation, spiculation, texture) as attributes.
% , see Table \ref{tab:lidcidridataset}.
The Lung Image Database Consortium and Image Database Resource Initiative (CC BY 3.0) \cite{armato_iii_lung_data_2015} is an extensively annotated CT dataset of non-small cell lung cancer patients. Up to four radiologists segmented nodules and labeled their appearance and malignancy \cite{armato_iii_lung_2011}. Our experiments use lung nodule cropouts as input, segmentation masks as decoder targets, malignancy ratings as prediction targets, and appearance ratings (subtlety, internal structure, calcification, sphericity, margin, lobulation, spiculation, texture) as attributes.

% \begin{table}[ht]
%     \centering
%     \fontsize{8}{9}\selectfont
%     \setlength{\tabcolsep}{4pt}
%     \caption{Annotations of the LIDC-IDRI dataset used in experiments.}
%     \begin{tabular}{llll}
%          \toprule
%          \multicolumn{1}{c}{\textbf{image}} & \multicolumn{1}{c}{\textbf{mask}} & \multicolumn{1}{c}{\textbf{target: malignancy}} & \multicolumn{1}{c}{\textbf{attributes}} \\
%          \multicolumn{1}{c}{
%          \begin{minipage}{.1\linewidth}
%             \includegraphics[width=\linewidth,]{miccai25_fig3.png}
%          \end{minipage}}
%          &\multicolumn{1}{c}{
%          \begin{minipage}{.1\linewidth}
%             \includegraphics[width=\linewidth]{miccai25_fig4.png}
%          \end{minipage}}
%          &\multicolumn{1}{l}{
%          \makecell{1-\textit{highly unlikely},\\5-\textit{highly suspicious}}}
%          &
%          \makecell[l]{
%          $\bullet$ subtlety 1-\textit{extremely sub}, 5-\textit{obvious}\\
%          $\bullet$ internal structure 1-\textit{soft tissue}, 4-\textit{air}\\
%          $\bullet$ calcification 1-\textit{popcorn}, 6-\textit{absent}\\
%          $\bullet$ sphericity 1-\textit{linear}, 5-\textit{round}\\
%          $\bullet$ margin 1-\textit{poorly def}, 5-\textit{sharp}\\
%          $\bullet$ lobulation 1-\textit{none}, 5-\textit{marked}\\
%          $\bullet$ spiculation 1-\textit{none}, 5-\textit{marked}\\
%          $\bullet$ texture 1-\textit{non-solid}, 5-\textit{solid}
%          }
%          \\       
%          \bottomrule
%     \end{tabular}
%     \label{tab:lidcidridataset}
% \end{table}

% Data preprocessing excluded nodules identified by fewer than three radiologists or smaller than 3\,mm. Cropouts were created using the smallest square bounding box for the nodules, and each slice was resized to 224 × 224 pixels, implemented with the \texttt{pylidc} framework \cite{LIDC_hancock}. The final dataset includes 27,379 samples and was evaluated using 5-fold stratified cross-validation by patient, with $10\,\%$ of the training data used for validation.

Preprocessing excludes nodules detected by fewer than three radiologists or smaller than 3\,mm. Cropouts are generated using the smallest square bounding box, and resized to 224 × 224 pixels with \texttt{pylidc} \cite{LIDC_hancock}. The final dataset (27,379 samples) is evaluated with 5-fold stratified cross-validation by patient, reserving 10\% of training data for validation.

% For the experiments the model layers were optimised using an Adam optimizer with a learning rate of $0.001$. 
% A warm-up phase of two epochs was chosen for prototype learning once the validation accuracy was sufficient. 
% The LIDC IDRI experiments on a GeForce RTX 3090 graphics card, conducted over 30 epochs, yielded an average runtime of 18 hours.
Model layers are optimized using Adam (learning rate, lr = 0.001), with a two-epoch warm-up for prototype learning after achieving sufficient validation accuracy. LIDC-IDRI experiments run for 30 epochs on a GeForce RTX 3090, averaging 18 hours.

\paragraph{derm7pt}
% The derm7pt dataset is a publicly available medical benchmark for dermatology, containing comprehensive annotated images of skin lesions \cite{kawahara2019seven}. It includes 1,011 samples, each with a clinical and a dermoscopic image, alongside patient metadata. In addition to lesion classification, labels encompass seven visual features commonly used by dermatologists \cite{argenziano1998epiluminescence}.
% In this study, we use dermoscopic images as input data, as they provide a standardized view with fewer artifacts compared to clinical images. Lesion classification targets include nevus, seborrheic keratosis, miscellaneous, basal cell carcinoma, and melanoma. Visual attributes include as pigment network, blue whitish veil, vascular structures, pigmentation, streaks, dots and globules, and regression structures.
% % , are detailed in Table \ref{tab:derm7ptdataset} based on the work of Kawahara \textit{et al.} \cite{kawahara2019seven}. 
% Unlike the LIDC-IDRI dataset, derm7pt does not include segmentation masks for regions of interest.
The derm7pt dataset is a publicly available dermatology benchmark with 1,011 annotated skin lesion images, each paired with clinical and dermoscopic views, and patient metadata \cite{kawahara2019seven}. Labels include lesion classification and seven visual features used by dermatologists \cite{argenziano1998epiluminescence}.
We use dermoscopic images as input due to their standardized view and fewer artifacts. Classification targets include nevus, seborrheic keratosis, miscellaneous, basal cell carcinoma, and melanoma. Visual attributes encompass pigment network, blue-whitish veil, vascular structures, pigmentation, streaks, dots and globules, and regression structures. Unlike LIDC-IDRI, derm7pt does not include segmentation masks.


% \begin{table}[ht]
%     \centering
%     \fontsize{8}{9}\selectfont
%     \caption{Annotations of the derm7pt dataset used in experiments.}
%     \begin{tabular}{ccc}
%          \toprule
%          \textbf{image}&\textbf{target: diagnosis}&\textbf{attributes}\\
%          \begin{minipage}{.1\linewidth}
%             \includegraphics[width=\linewidth,]{miccai25_fig5.jpg}
%          \end{minipage}
%          &
%          \makecell{\textit{nevus (nev)}\\ \textit{seborrheic keratosis (sk)}\\\textit{miscellaneous (misc)}\\ \textit{basal cell carcinoma (bcc)}\\ \textit{melanoma (mel)}}
%          &
%          \makecell[l]{
%          $\bullet$ pigment network (pn): \textit{absent, typical, atypical}\\
%          $\bullet$ blue whitish veil (bmw): \textit{absent, present}\\
%          $\bullet$ vascular structures (vs): \textit{absent, regular, irregular}\\
%          $\bullet$ pigmentation (pig): \textit{absent, regular, irregular}\\
%          $\bullet$ streaks (str): \textit{absent, regular, irregular}\\
%          $\bullet$ dots and globules (dag): \textit{absent, regular, irregular}\\
%          $\bullet$ regression structures (rs): \textit{absent, present}
%          }
%          \\    
%          \bottomrule
%     \end{tabular}
%     \label{tab:derm7ptdataset}
% \end{table}

% Data preprocessing included centered cropping to 450 × 450 pixels and resizing to 224 × 224 pixels. Training samples were randomly rotated horizontally and vertically. For comparability with related work, we used the test split from Kawahara \textit{et al.} \cite{kawahara2019seven}.

% For the experiments the model layers were optimized using an Adam optimizer. Preliminary tests were used to determine the optimal learning rate of $0.00001$ for all layers except the prototype vectors, which were optimized with a learning rate of $0.01$. A warm-up phase of twenty epochs was chosen. 
% As this dataset does not provide segmentation masks, the decoder branch was deactivated by setting the segmentation loss function $L_{seg}=0$. To compensate for the imbalance in the data set, appropriate weighting of the target and attribute classes was implemented in the cross entropy loss terms.
% The derm7pt experiments on a GeForce RTX 3090 graphics card, conducted over 400 epochs, yielded an average runtime of four hours.
Data pre-processing includes center cropping to 450 × 450 pixels and resizing to 224 × 224 pixels. Training samples are randomly rotated. For comparability, we use the test split from Kawahara \textit{et al.} \cite{kawahara2019seven}.
Model layers are optimized using Adam, with a learning rate of $lr=0.00001$ for all layers except prototype vectors ($lr=0.01$). A 20-epoch warm-up phase is used. As segmentation masks are unavailable, the decoder branch is disabled ($L_{seg} = 0$). Class imbalances are addressed by weighting target and attribute classes in the cross-entropy loss.
derm7pt experiments run for 400 epochs on a GeForce RTX 3090, averaging four hours.



% Since the proposed method aims to be an interpretable medical assistance tool, the evaluation experiments are comprehensive, covering multiple aspects. Alongside a quantitative performance comparison with current SOTA methods, the explainability aspect is analyzed qualitatively and through a detailed analysis of the prototypes, which contributes to the method's explainability. Additionally, various ablation studies on the model and annotation availability assess functionality.


\subsection{Qualitative Evaluation}
\label{subsec:explainability}
% The hierarchical architecture of the model enables the generation of prediction results that build upon one another. High-level visual attributes are first learned from the latent vector of the ViT encoder, and target prediction follows based on this information. This step-by-step flow mimics human reasoning to facilitate a comparable thought process and discussion of the results.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{miccai25_fig6.png}
    \caption{\textbf{Reasoning process} Three sample cases are illustrated, (a) correctly predicted, (b) and (c) incorrectly predicted. For three of the eight attributes (spiculation, sphericity, lobulation), the score, attention heatmap, and prototype image of the respective attribute are displayed.
    }
    \label{fig:hierarchicalreasoning}
\end{figure*}

The model output includes three predictions of the detected attributes that justify the target prediction: associated scores, attention heatmaps, and the closest prototypical samples. Fig. \ref{fig:hierarchicalreasoning} illustrates the model output, showcasing three of the eight attributes.
In case (a), the model correctly predicted the target. The attribute ratings reflect the visual characteristics of the sample nodule, and the closest attribute prototypes exhibit similar traits. The attention heatmaps further support the model's prediction by highlighting the attribute-specific region of interest. Consultation with a pulmonary nodules expert confirmed the significance of the attention areas, with the model focusing on the nodule's edges for spiculation assessment and its interior for evaluating sphericity and lobulation.

In cases (b) and (c), the attention heatmaps and prototypes indicate a misclassification, showing a discrepancy between the prototypes' characteristics and their ratings compared to the inference image for certain attributes. Additionally, the attention is not focused on the nodule. These signals should raise user doubts about the model's results, helping prevent incorrect conclusions in the diagnosis.

% Compared to 3D-CNN+MTL \cite{hussein_risk_2017} and TumorNet \cite{hussein_tumornet_2017}, HierViT provides interpretability by predicting attributes as domain knowledge. While X-Caps \cite{lalonde_encoding_2020} predicts attribute values and Proto-Caps \cite{gallee2023interpretable} shows prototypical images of attribute characteristics, our method differs in its underlying network. Instead of using an iterative attention mechanism, we employ ViT layers, allowing for the extraction and visualization of attention regions for interpretation, as shown in Fig. \ref{fig:hierarchicalreasoning}.

\subsection{Quantitative Evaluation}
\paragraph{LIDC-IDRI}
% In accordance with previous studies on the LIDC-IDRI dataset \cite{gallee2023interpretable,lalonde_encoding_2020}, the proposed model was evaluated using the Within-1-Accuracy metric. This metric for ordinal labels defines a prediction with a tolerance of one point value to the ground truth label as correct.
Following previous studies on LIDC-IDRI \cite{gallee2023interpretable,lalonde_encoding_2020}, the proposed model was evaluated using the Within-1-Accuracy metric, which considers predictions within one point of the ground truth as correct.
% As demonstrated in Table \ref{lidcPerformance}, the proposed HierViT model outperforms the SOTA methods in target and attribute prediction. Related research using Vision Transformers \cite{wang2022accurate,liu2022res,wang2022transpnd} focuses on binary classification of lung nodules, summarizing malignancy annotations into benign and malignant while disregarding nodules of intermediate suspicion. 
As shown in Table \ref{lidcPerformance}, the proposed HierViT model outperforms SOTA methods in target and attribute prediction. In contrast, related Vision Transformer research \cite{wang2022accurate,liu2022res,wang2022transpnd} focuses on binary lung nodule classification, merging malignancy annotations into benign and malignant while ignoring intermediate cases.
% These works do not prioritize model interpretability, nor do they include domain knowledge of visual appearance as attribute labels in training or predictions.
% \paragraph{Using prototypes during inference}
% This experiment follows the same training procedure as the proposed method. However, during inference, the token vectors of the attributes are replaced by the closest prototype vectors for target prediction, similar to Proto-Caps \cite{gallee2023interpretable}. The exemplar sample linked to the prototype vector is used for predicting attribute scores based on its ground truth label, while the attribute heads are ignored. Although this method slightly reduces prediction performance, it makes the prototypes causal for target prediction, enhancing the credibility of prototype explainability.
The \textit{w. proto. inference} variant extends the method by replacing attribute token vectors with the closest prototype vectors during inference for target prediction, similar to Proto-Caps \cite{gallee2023interpretable}. The prototype’s ground truth attribute value is used for prediction, ignoring attribute heads.
While this slightly reduces prediction performance, it makes prototypes directly causal for target prediction, enhancing explainability credibility.



\begin{table*}[ht]
  \caption{\textbf{Performance LIDC-IDRI} 
  % The performance is reported in the Within-1-Accuracy metric. Results marked with an asterisk* represent accuracy (ACC) of the binary classification between benign and malignant.  
  % Mean (black) and standard deviation (gray) are 
  % % calculated from 5-fold experiments and 
  % reported if available in literature. Methods denoted with "P" provide prototype-reasoning. 
  % The $95\%$ binomial confidence interval is reported for the target if the test dataset is detailed in literature. 
  % Best result is in bold.
  Performance is reported in the Within-1-Accuracy metric (\%). Asterisk (*) indicates binary classification accuracy (ACC). Mean (black) and standard deviation (gray) are shown if available. Methods with "P" offer prototype reasoning. A 95\% binomial confidence interval is provided for the target if the test dataset is specified. Best result is in bold.
  }
  \label{lidcPerformance}
  \centering
  \fontsize{8}{9}\selectfont
  \begin{tabular}{lcccccccccl}
    \toprule
    &&\multicolumn{8}{c}{attributes} & \multicolumn{1}{c}{target}\\%&\multirow{3}{*}{\makecell{malig-\\nancy}}\\
    \cmidrule(lr{0.5em}){3-10}
    \cmidrule(lr{0.5em}){11-11}
    && sub & is & cal & sph & mar & lob & spic & tex & \multicolumn{1}{c}{malignancy}\\
    \midrule
    \textbf{CNN-based}&&&&&&&&&&\\
    3D-CNN+MTL \cite{hussein_risk_2017}&&-&-&-&-&-&-&-&-&91.3
    [89.7,92.9]
    \\
    TumorNet \cite{hussein_tumornet_2017}&&-&-&-&-&-&-&-&-&92.3
    [90.8,93.8]
    \\
    X-Caps \cite{lalonde_encoding_2020}&&90.4&-&-&85.4&84.1&70.7&75.2&93.1&86.4\\
    Proto-Caps \cite{gallee2023interpretable}&P&89.1&\textbf{99.8}&95.4&96.0&88.3&87.9&89.1&93.3&93.0
    [92.7,93.3]
    \\
    &&\textcolor{gray}{5.2}&\textcolor{gray}{0.2}&\textcolor{gray}{1.3}&\textcolor{gray}{2.2}&\textcolor{gray}{3.1}&\textcolor{gray}{0.8}&\textcolor{gray}{1.3}&\textcolor{gray}{1.0}&\textcolor{gray}{1.5}\\
    \cmidrule{1-1}
    \textbf{ViT-based}&&&&&&&&&&\\
    TransUnet \cite{wang2022accurate}&&-&-&-&-&-&-&-&-&84.62*\\
    Res-trans \cite{liu2022res}&&-&-&-&-&-&-&-&-&92.92*\\
    TransPND \cite{wang2022transpnd}&&-&-&-&-&-&-&-&-&93.33*\\
    \textcolor{mydarkblue}{\multirow{2}{*}{\makecell[l]{HierViT\\ \quad \textit{proposed}}}}&P&\textbf{96.3}&\textbf{99.8}&\textbf{95.5}&\textbf{97.4}&\textbf{92.7}&\textbf{94.3}&\textbf{90.8}&\textbf{93.3}&\textbf{94.8}
    [94.5,95.1]
    \\
    &&\textcolor{gray}{0.9}&\textcolor{gray}{0.3}&\textcolor{gray}{2.1}&\textcolor{gray}{0.8}&\textcolor{gray}{1.7}&\textcolor{gray}{2.9}&\textcolor{gray}{1.4}&\textcolor{gray}{1.4}&\textcolor{gray}{1.4}\\
    % \midrule
    \multirow{2}{*}{\makecell[l]{HierViT \\\quad \textit{w proto. inference}}}
    &P&93.7&99.8&95.1&92.2&87.9&88.7&86.0&93.0&94.4
    [94.1,94.7]
    \\
    &&\textcolor{gray}{2.0}&\textcolor{gray}{0.3}&\textcolor{gray}{1.8}&\textcolor{gray}{7.5}&\textcolor{gray}{3.4}&\textcolor{gray}{3.7}&\textcolor{gray}{1.9}&\textcolor{gray}{3.2}&\textcolor{gray}{1.9}\\
    \bottomrule
  \end{tabular}
\end{table*}

% The proposed method also outperforms other approaches focusing on binary classification of lung nodules using Vision Transformers \cite{wang2022accurate, liu2022res, wang2022transpnd} ...

The proposed model HierViT achieved a Dice score of $68.2\,\%$, with a standard deviation of $2.5\,\%$ in the reconstruction of the segmentation mask.

\paragraph{derm7pt}
% For a fair comparison with existing approaches, we list methods that use only dermoscopic images.
The HierViT method achieves comparable accuracy, attaining the best accuracy in target prediction and similarly high accuracy in attribute prediction, matching SOTA methods. Given the limited test data from derm7pt, the statistical analysis using the $95\%$ binomial confidence interval shows a wide and overlapping range of true classification accuracies, demonstrating that HierViT and FusionM4Net perform similarly well on average.

All comparing methods provide some interpretability by predicting the seven lesion features. The Inception model \cite{kawahara2019seven} and the AMFAM model \cite{wang2022adversarial} further enhance interpretability through visualization of prediction importance heatmaps. HierViT is the first method to capture the hierarchical relationship between lesion features and classification in the derm7pt dataset, treating it as more than a multi-label task. In addition to attention heatmaps, HierViT offers validation through prototype images, aiding in differentiating recognized attributes.
% We compare our proposed method with approaches using different data modalities from the derm7pt dataset, which includes clinical and dermoscopic images along with metadata. To ensure a fair comparison, we evaluate our method against models that operate solely on dermoscopic images (upper half of Tables \ref{dermacc}). Additionally, we report the best performance of comparative methods that use more than just dermoscopic images (bottom half of Tables \ref{dermacc}).

\begin{table*}[ht]
  \caption{\textbf{Performance derm7pt} 
  % The performance is reported in accuracy percentage.
  % % , differentiated by data modality. 
  % The average \text{\O} is the mean over the attributes and target. 
  % The $95\%$ binomial confidence interval is specified for 
  % % the target and 
  % the average.
  % % In the HierViT's category using dermoscopic images only
  % Best results are in bold, second-best in underlined.
  Performance is reported as accuracy (\%). The average \text{\O} represents the mean over attributes and target, with a 95\% binomial confidence interval. Best results are bold; second-best are underlined.}
  \label{dermacc}
  \centering
  \fontsize{8}{9}\selectfont
  \begin{tabular}{lccccccccl}
    \toprule
    &\multicolumn{7}{c}{attributes}&target&\multicolumn{1}{c}{\multirow{3}{*}{\text{\O}}}\\%&\multirow{3}{*}{diag}&\multirow{3}{*}{$\O$}\\
    \cmidrule(lr{0.5em}){2-8}
    \cmidrule(lr{0.5em}){9-9}
    &pn&bmv&vs&pig&str&dag&rs&diag&\\
    % \cmidrule{1-1}
    \midrule
    % \textbf{using dermoscopic images only}&&&&&&&&&\\
    Inception-$x_d$ \cite{kawahara2019seven}&\textbf{69.4}&85.8&80.3&62.8&71.4&\textbf{60.8}&77.5&71.9
    % [67.5,76.3]
    &72.5
     [68.1,76.9]
    \\
    AMFAM derm. only \cite{wang2022adversarial}&66.1&87.1&\underline{80.5}&66.6&71.1&\underline{60.0}&78.5&69.4
    % [64.9,73.9]
    &72.4
     [68.0,76.8]
    \\
    FusionM4Net derm. only \cite{tang2022Fusion}&\underline{69.0}&\underline{87.2}&\textbf{81.4}&\textbf{68.3}&\underline{73.7}&\underline{60.0}&\textbf{80.1}&\underline{74.7}
    % [70.4,79.0]
    &\textbf{74.3}
     [70.0,78.6]
    \\
    MTL-standard \cite{coppola2020interpreting}&55.4&85.1&63.5&62.5&49.1&48.6&65.1&45.8
    % [40.9,50.7]
    &59.4
     [54.6,64.2]
    \\
    \textcolor{mydarkblue}{HierViT \textit{proposed}}&65.3&\textbf{87.6}&80.3&\underline{67.3}&\textbf{74.4}&59.2&\underline{79.8}&\textbf{76.5}
    % [72.3,80.7]
    &\underline{73.8}
     [69.5,78.1]
    \\
    % \cmidrule{1-1}
    % \textcolor{gray}{\makecell[l]{\textbf{using dermoscopic,} \\ \textbf{ and clinical images,} \\\textbf{  and meta data*}}}&&&&&&&&&\\
    % \textcolor{gray}{AMFAM \cite{wang2022adversarial}}&\textcolor{gray}{70.6}&\textcolor{gray}{88.1}&\textcolor{gray}{83.3}&\textcolor{gray}{70.9}&\textcolor{gray}{74.7}&\textcolor{gray}{63.8}&\textcolor{gray}{80.8}&\textcolor{gray}{75.4}
    % % [71.2,79.6]}
    % &\textcolor{gray}{76.0}
    % % [71.8,80.2]}
    % \\
    % \textcolor{gray}{HcCCN \cite{bi2020multi}}&\textcolor{gray}{70.6}&\textcolor{gray}{87.1}&\textcolor{gray}{84.8}&\textcolor{gray}{68.6}&\textcolor{gray}{71.6}&\textcolor{gray}{65.6}&\textcolor{gray}{80.8}&\textcolor{gray}{69.9}
    % % [65.4,74.4]}
    % &\textcolor{gray}{74.9}
    % % [70.6,79.2]}
    % \\
    % \textcolor{gray}{Inception-combine* \cite{kawahara2019seven}}&\textcolor{gray}{70.9}&\textcolor{gray}{87.1}&\textcolor{gray}{79.7}&\textcolor{gray}{66.1}&\textcolor{gray}{74.2}&\textcolor{gray}{60.0}&\textcolor{gray}{77.2}&\textcolor{gray}{74.2}
    % % [69.9,78.5]}
    % &\textcolor{gray}{73.7}
    % % [69.4,78.0]}
    % \\
    % \textcolor{gray}{FusionM4Net* \cite{tang2022Fusion}}&\textcolor{gray}{71.1}&\textcolor{gray}{88.1}&\textcolor{gray}{81.8}&\textcolor{gray}{70.1}&\textcolor{gray}{78.0}&\textcolor{gray}{66.1}&\textcolor{gray}{81.5}&\textcolor{gray}{78.5}
    % % [74.4,82.6]}
    % &\textcolor{gray}{77.0}
    % % [72.8,81.2]}
    % \\
    \bottomrule
  \end{tabular}
\end{table*}



% Detailed performance results, including sensitivity, specificity, precision, AUC, and ROC curves, are available in the supplemental materials.

% Given the limited test data from derm7pt, the statistical analysis using the $95\%$ binomial confidence interval shows a wide and overlapping range of true classification accuracies for the best models, namely HierViT and FusionM4Net.% derm. only. 



% \subsection{Ablation study}
% The ablation studies on the LIDC-IDRI dataset were conducted with the objective of evaluating the efficacy of the individual components of the model architecture. Four reduced variants were subjected to comparative analysis with the proposed method, see Table \ref{lidcablation}.
% \begin{table*}[ht]
%   \caption{\textbf{Ablation study} Performance of the proposed method when model components were abstracted. The experiments were conducted with the LIDC-IDRI dataset and the Within-1-Accuracy was measured. Mean (in black) and standard deviation (in grey) are calculated from 5-fold experiments. 
%   % The $95\%$ binomial confidence interval is reported for the target.
%   }
%   \label{lidcablation}
%   \centering
%   \begin{tabular}{lccccccccc}
%     \toprule
%     &\multicolumn{8}{c}{attributes} &\multicolumn{1}{c}{target}\\%&\multirow{3}{*}{\makecell{malig-\\nancy}}\\
%     \cmidrule(lr{0.5em}){2-9}
%     \cmidrule(lr{0.5em}){10-10}
%     & sub & is & cal & sph & mar & lob & spic & tex &\multicolumn{1}{c}{malignancy}\\
%     \midrule
%     HierViT (proposed)&96.3&99.8&95.5&97.4&92.7&94.3&90.8&93.3&94.8
%     % [94.5,95.1]
%     \\
%     &\textcolor{gray}{0.9}&\textcolor{gray}{0.3}&\textcolor{gray}{2.1}&\textcolor{gray}{0.8}&\textcolor{gray}{1.7}&\textcolor{gray}{2.9}&\textcolor{gray}{1.4}&\textcolor{gray}{1.4}&\textcolor{gray}{1.4}\\
%     w proto. inference&93.7&99.8&95.1&92.2&87.9&88.7&86.0&93.0&94.4
%     % [94.1,94.7]
%     \\
%     &\textcolor{gray}{2.0}&\textcolor{gray}{0.3}&\textcolor{gray}{1.8}&\textcolor{gray}{7.5}&\textcolor{gray}{3.4}&\textcolor{gray}{3.7}&\textcolor{gray}{1.9}&\textcolor{gray}{3.2}&\textcolor{gray}{1.9}\\
%     w/o proto. learning&95.6&99.8&95.9&97.3&91.5&94.3&91.3&93.9&95.0
%     % [94.7,95.3]
%     \\
%     &\textcolor{gray}{1.7}&\textcolor{gray}{0.3}&\textcolor{gray}{1.7}&\textcolor{gray}{0.7}&\textcolor{gray}{2.8}&\textcolor{gray}{2.1}&\textcolor{gray}{1.9}&\textcolor{gray}{1.8}&\textcolor{gray}{2.0}\\
%     w/o attribute learning&-&-&-&-&-&-&-&-&93.7 
%     % [93.4,94.0]
%     \\
%     &-&-&-&-&-&-&-&-&\textcolor{gray}{2.5}\\
%     w/o ROI segmentation&96.5&99.8&96.0&97.6&92.5&94.5&91.3&93.5&95.6
%     % [95.4,95.8]
%     \\
%     &\textcolor{gray}{1.2}&\textcolor{gray}{0.3}&\textcolor{gray}{1.9}&\textcolor{gray}{0.7}&\textcolor{gray}{1.6}&\textcolor{gray}{1.9}&\textcolor{gray}{1.2}&\textcolor{gray}{1.8}&\textcolor{gray}{1.2}\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% \paragraph{Using prototypes during inference}
% This experiment follows the same training procedure as the proposed method. However, during inference, the token vectors of the attributes are replaced by the closest prototype vectors for target prediction, similar to Proto-Caps \cite{gallee2023interpretable}. The exemplar sample linked to the prototype vector is used for predicting attribute scores based on its ground truth label, while the attribute heads are ignored. Although this method slightly reduces prediction performance, it makes the prototypes causal for target prediction, enhancing the credibility of prototype explainability.

% \paragraph{Prototype learning}
% In this experiment, the prototype layer was deactivated, reducing the explainability of target prediction to attribute scores and attention outputs. The results show comparable performance to the proposed method, indicating that the additional task of prototype learning enhances interpretability without negatively impacting performance.

% \paragraph{Attribute learning}
% This variant aims to assess the impact of incorporating domain knowledge through attribute labels on target performance. The model was optimized only for the target label and segmentation mask, not for attributes. The observed decline in target prediction performance suggests that including feature information improves classification. Additionally, predicting these attributes enhances interpretability.

% \paragraph{Segmentation learning}
% This experiment simulates the lack of segmentation mask annotations. Deactivating the decoder branch yields comparable performance, suggesting that learning segmentation masks neither hinders nor enhances target and attribute learning. Thus, this method is applicable for classification on datasets without ground truth segmentation masks. However, including a segmentation branch can boost user confidence by showing the model's ability to recognize the object being classified.


% \subsection{Prototype analysis}
% The goal of explainable AI methods is to achieve high prediction accuracy while providing understandable explanations. The accuracy of target and attribute predictions has been evaluated in previous experiments. This section focuses on analyzing the quality of HierViT's prototypical explanations. Inspired by Pathak \textit{et al.} \cite{pathak2024Prototype}, we assess the prototypes generated by HierViT in terms of truthfulness, diversity, and counterfactuality, as shown in Fig. \ref{fig:protoanalysis}.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}{0.23\textwidth}
%         \includegraphics[width=\textwidth]{miccai25_fig7.png}
%         \caption{}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.18\textwidth}
%         \includegraphics[width=\textwidth]{miccai25_fig8.png}
%         \caption{}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.36\textwidth}
%         \includegraphics[width=\textwidth]{miccai25_fig9.png}
%         \caption{}
%     \end{subfigure}
%     % \caption{\textbf{Prototype analysis} We evaluate the learned attribute prototypes in regards of the (a) agreement with attribute branch predictions, (b) diversity, and (c) counterfactual prototypes.}
%     \caption{\textbf{Analysis of learned prototypes} (a) agreement between prototypes ground truth and made predictions, (b) diversity of used prototypes, and (c) counterfactual prototypes.}
%     \label{fig:protoanalysis}
% \end{figure}

% \paragraph{Prototype truthfulness}
% The model aims to generate visual prototypes that represent the recognized features. In HierViT's architecture, this means the prototypes reflect the attribute classification predictions. Specifically, the attributes of the selected prototype sample for each latent vector should align with the predicted attributes from these vectors. 

% In the case of the LIDC-IDRI dataset, the following levels of agreement were determined for the attributes: $99.0\%$ (sub), $100.0\%$ (is), $99.2\%$ (cal), $97.0\%$ (sph), $97.7\%$ (mar), $99.9\%$ (lob), $95.5\%$ (spic), $96.1\%$ (tex).

% For the derm7pt dataset, the following levels of agreement were determined for the attributes: $96.0\%$ (pn), $99.2\%$ (bmv), $98.5\%$ (vs), $96.2\%$ (pig), $98.5\%$ (str), $96.2\%$ (dag), $69.4\%$ (rs).

% The agreement between the prototype values and the attribute predictions is predominantly high. The results of these experiments demonstrate that the prototypes effectively represent the extracted attribute features.

% \paragraph{Prototype diversity}
% The diversity of generated prototypes indicates whether they provide local or global explanations. Local explainability highlights sample-specific features, allowing different prototypes for inference samples of the same class. In contrast, global explainability offers class-specific insights by generating cluster prototypes that effectively differentiate between classes.
% In the following experiment, we counted the number of prototypes that were actually used during the inference phase for the test data splits, i.e. that were most similar to the inference samples. In the experiments, a set of sixteen prototypes per attribute class was provided for both experiments on the LIDC-IDRI and on the derm7pt dataset. The average number of prototypes per attribute class used was 3.8 in the LIDC-IDRI experiment and 7.6 in the derm7pt dataset. 

% The latter result indicates a high degree of diversity and differentiation among the features of the same attribute class. The model's explanation of the different attribute predictions varies depending on the inference sample, making it sample-specific.

% \paragraph{Counterfactual Prototypes}
% Counterfactual prototypes are those furthest from the inference sample in the model's latent space. For instance, in the LIDC-IDRI dataset, prototypes representing 'linear' and 'round' should be distant for the attribute \textit{sphericity}. Consequently, employing counterfactual prototypes should result in changed predictions. This effect is measured using the inverse accuracy (1-ACC) for attribute classification with counterfactual prototypes.

% For the LIDC-IDRI dataset the following inverse accuracies were measured on average over the 5-fold experiments: $97.0\%$ (sub), $0.2\%$ (is), $99.7\%$ (cal), $97.2\%$ (sph), $90.9\%$ (mar), $34.5\%$ (lob), $56.7\%$ (spic), $77.0\%$ (tex). The \textit{internal structure} (is) attribute exhibits a low level of inverse accuracy, which can be attributed to a significant degree of data imbalance with regard to one class (with $98.9\%$ of data points occurring for class 1).

% For the derm7pt dataset, the following inverse accuracies were measured: $91.4\%$ (pn), $86.8\%$ (bmv), $93.2\%$ (vs), $92.4\%$ (pig), $91.9\%$ (str), $86.3\%$ (dag), $36.0\%$ (rs).

% Both experiments showed high inverse accuracy for most attributes. This suggests that the model effectively differentiates prototypes belonging to different attribute classes in the latent space, demonstrating its ability to reason with meaningful prototypes and achieve semantically distinct learning.

% \subsection{Sparse data study}
% The HierViT learning algorithm uses human-defined attributes as domain knowledge for reasoning. We initially tested it on densely annotated datasets with both target and attribute ground truth labels. However, generating such labels in the medical field is time-consuming and costly due to the need for expert knowledge.
% To evaluate HierViT in scenarios with fewer annotations, we conducted experiments using the LIDC-IDRI dataset with sparse attribute labels. We tested HierViT with and without prototype inference, selecting random subsets of the data: $1\%$, $5\%$, $10\%$, and $50\%$ of samples with full attribute annotations, while the remaining samples lacked attribute labels.
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.75\linewidth]{miccai25_fig10.png}
%     \caption{\textbf{Sparse data study} Performance results with reduced availability of attribute annotations using the LIDC-IDRI dataset. Error bars show standard deviation of 5-fold experiments.
%     }
%     \label{fig:sparsedatastudy}
% \end{figure}

% As demonstrated in Fig. \ref{fig:sparsedatastudy}, our results indicate that comparable performance can be achieved with reduced annotations. For example, using only $10\%$ of the attribute labels (1,907 training samples), we obtained malignancy scores of 92.9/2.8, compared to 94.8/1.4 with $100\%$ of the labels, and attribute means of 92.7/4.7 versus 95.0/1.4 in the proposed method.

% These findings suggest that while HierViT benefits from domain knowledge, it does not depend on densely annotated datasets, enhancing its practicality and applicability to other datasets with limited annotations.

\section{Discussion and Conclusion}
HierViT advances Vision Transformers in explainable AI by leveraging domain knowledge and a hierarchical architecture for human-like reasoning. Experiments on the LIDC-IDRI and derm7pt benchmark datasets demonstrate high performance alongside enhanced explainability. The model infers target predictions from recognized visual features using ratings (e.g., “The shape is round and the texture is solid”), prototypical sample images (e.g., “The sphericity is similar to this sample”), and attention heatmaps (e.g., “This area is crucial for recognizing sphericity”). These human-defined attributes are understandable and learned in a supervised manner, providing reliable evidence for target outputs as they feed into the target prediction branch.
% The proposed model architecture allows for substantial domain knowledge integration. However, ablation studies show that HierViT maintains comparable performance even when certain components and annotations are omitted. For instance, it performs robustly with a $90\%$ reduction in attribute annotations, indicating its applicability to other datasets with fewer annotations.
Intuitive reasoning enhances model confidence by using predefined attributes, aligning with human language, which can affect radiologists' diagnostic accuracy positively or negatively. A user-centered study on reasoning by attribute prototypes found that these explanations boost radiologists' confidence in diagnoses \cite{gallee2024evaluating}. The model's explanations were persuasive, leading radiologists to favor the model's predictions, even when they were incorrect. Thus, while explanations can improve confidence, they may also reduce human performance if the model's predictions are wrong \cite{koehler1991explanation}.

% The prototype analysis demonstrates that the predicted prototype samples accurately represent the identified attributes and effectively distinguish from counterfactuals. Additionally, using attribute prototypes for target prediction inference yields high accuracy, indicating an effective prototype learning strategy. 

\textit{Limitations}
While specific predefined attributes are crucial for explainable models in medical applications, there's a scarcity of medical datasets with such discrete annotations, highlighting the need for further research. There is potential to transfer attribute knowledge to radiological diagnosis tasks with similar visual criteria. Additionally, we could enhance the transformer architecture by integrating a text processing branch, allowing the fusion of information from radiology reports with image data to incorporate more domain knowledge.
Another promising research direction is data synthesis to expand small-scale datasets. It would be interesting to explore whether generative AI models can be conditioned on complex combinations of attributes.

\textit{Conclusion}
This work presents an image classifier that incorporates multiple interpretable modalities for intrinsic explainability. A Vision Transformer serves as a high-performing backbone, while a hierarchical structure captures semantic relationships between radiologist-defined high-level features (attributes) for target classification.
% classifier. In the initial step, high-level features specified by radiologists are learned. Based on these recognized attributes, the target classification is made. 
% This step-by-step structure enables the establishment of a semantic relationship between the attributes and the target result.
The explanation of the model is a detailed description of the recognized attributes, including ratings, visual prototypes and attention heatmaps. The model generates an exemplary image that represents a specific attribute and highlights the corresponding focus area. This approach captures the complexity and detail of medical image diagnosis, mirroring the reasoning process of human experts and offering intuitive and trustworthy interpretation.

\begin{thebibliography}{00}

\bibitem{dosovitskiy2021vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T. \textit{et al.}
: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In {\it Proc. ICLR} (2021)

\bibitem{Kirillov_2023_ICCV}
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L. \textit{et al.}: Segment Anything. In {\it Proc. ICCV,} 
% Paris, France,
pp. 4015--4026 (2023)

\bibitem{NEURIPS2021_64f1f27b}
Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. In {\it Proc. NeurIPS,} vol. 34, pp. 12077--12090 (2021)

\bibitem{Caron_2021_ICCV}
Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P. \textit{et al.}: Emerging Properties in Self-Supervised Vision Transformers. In {\it Proc. ICCV,} pp. 9650--9660 (2021)

\bibitem{He_2022_CVPR}
He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked Autoencoders Are Scalable Vision Learners. In {\it Proc. CVPR,} 
% New Orleans, LA, USA,
pp. 16000--16009 (2022)

\bibitem{rudin2019stop} 
Rudin, C.: Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. In {\it Nat Mach Intell}, vol. 1, pp. 206--215 (2019) \doi{10.1038/s42256-019-0048-x}

\bibitem{NEURIPS2019_adf7ee2d}
Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.: This Looks Like That: Deep Learning for Interpretable Image Recognition. In {\it Proc. NeurIPS,} 
% Vancouver, BC, Canada,
vol. 32 (2019)

\bibitem{gallee2023interpretable} 
Gallée, L., Beer, M., Götz, M.: Interpretable Medical Image Classification Using Prototype Learning and Privileged Information. In {\it Proc. MICCAI,} 
% Vancouver, BC, Canada, 
pp. 435--445 (2023) \doi{10.1007/978-3-031-43895-0\_41}

\bibitem{reyes2020interpretability} 
Reyes, M., Meier, R., Pereira, S., Silva, C.A., Dahlweid, F., von Tengg-Kobligk, H. \textit{et al.}: On the interpretability of artificial intelligence in radiology: challenges and opportunities. In {\it Radiology: artificial intelligence}, vol. 2, no. 3 (2020) \doi{10.1148/ryai.2020190043}

\bibitem{gallee2024evaluating} 
Gallée, L., Lisson, C.S., Lisson, C.G., Drees, D., Weig, F., Vogele, D. \textit{et al.}: Evaluating the Explainability of Attributes and Prototypes for a Medical Classification Model. In {\it Proc. xAI, }
% Valetta, Malta 
(2024) \doi{10.1007/978-3-031-63787-2\_3}

\bibitem{xue2922ProtoPFormer}
Xue, M., Huang, Q., Zhang, H., Cheng, L., Song, J., Wu, M. \textit{et al.}: ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition. (2022) \doi{10.48550/arXiv.2208.10431}

\bibitem{xu2024Interpretable}
Xu, Y., Meng, Z.: Interpretable vision transformer based on prototype parts for COVID-19 detection. In {\it IET Image Processing} (2024) \doi{10.1049/ipr2.13074}

\bibitem{demir2024explainable}
Demir, U., Jha, D., Zhang, Z., Keles, E., Allen, B., Katsaggelos, A.K. \textit{et al.}: Explainable Transformer Prototypes for Medical Diagnoses. (2024) \doi{10.48550/arXiv.2403.06961}

\bibitem{rigotti2022attentionbased}
Rigotti, M., Miksovic, C., Giurgiu, I., Gschwind, T., Scotton, P.: Attention-based Interpretability with Concept Transformers. In {\it Proc. ICLR} (2022)

\bibitem{armato_iii_lung_data_2015}
Armato III, S.G., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer, C.R., Reeves, A.P. \textit{et al.}.: Data From LIDC-IDRI. In {\it TCIA} (2015) \doi{10.7937/K9/TCIA.2015.LO9QL9SX}

\bibitem{armato_iii_lung_2011}
Armato III, S.G., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer, C.R., Reeves, A.P. \textit{et al.}: The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans. In {\it Med. Phys.}, vol. 38, no. 2, pp. 915--931 (2011) \doi{10.1118/1.3528204}

\bibitem{LIDC_hancock}
Hancock, M.C., Magnan, J.F.: Lung nodule malignancy classification using only radiologist-quantified image features as inputs to statistical learning algorithms: probing the Lung Image Database Consortium dataset with two statistical learning methods. In {\it J. Med. Imaging,} vol. 3, no. 4, pp. 044504--044504 (2016) \doi{10.1117/1.JMI.3.4.044504}

\bibitem{kawahara2019seven}
Kawahara, J., Daneshvar, S., Argenziano, G., Hamarneh, G.: Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets. In {\it JBHI,} vol. 23,  pp. 538--546 (2019) \doi{10.1109/JBHI.2018.2824327}

\bibitem{argenziano1998epiluminescence}
Argenziano, G., Fabbrocini, G., Carli, P., De Giorgi, V., Sammarco, E., Delfino, M.: Epiluminescence microscopy for the diagnosis of doubtful melanocytic skin lesions: comparison of the ABCD rule of dermatoscopy and a new 7-point checklist based on pattern analysis. In {\it Arch Dermatol.}, vol. 134, pp. 1563--1570 (1998) \doi{10.1001/archderm.134.12.1563}

\bibitem{hussein_risk_2017}
Hussein, S., Cao, K., Song, Q., Bagci, U.: Risk stratification of lung nodules using 3D CNN-based multi-task learning. In {\it Proc. IPMI,} 
% Boone, NC, USA, 
pp. 249--260 (2017) \doi{10.1007/978-3-319-59050-9\_20}

\bibitem{hussein_tumornet_2017}
Hussein, S., Gillies, R., Cao, K., Song, Q., Bagci, U.: Tumornet: Lung nodule characterization using multi-view convolutional neural network with gaussian process. In {\it Proc. ISBI,} 
% Melbourne, VIC, Australia, 
pp. 1007--1010 (2017) \doi{10.1109/ISBI.2017.7950686}

\bibitem{lalonde_encoding_2020}
LaLonde, R., Torigian, D., Bagci, U.: Encoding visual attributes in capsules for explainable medical diagnoses. In {\it Proc. MICCAI,} 
% Lima, Peru, 
pp. 294--304 (2020) \doi{10.1007/978-3-030-59710-8\_29}

\bibitem{wang2022accurate}
Wang, H., Zhu, H., Ding, L.: Accurate classification of lung nodules on CT images using the TransUnet. In {\it Front. Public Health,} vol. 10 (2022) \doi{10.3389/fpubh.2022.1060798}

\bibitem{liu2022res}
Liu, D., Liu, F., Tie, Y., Qi, L., Wang, F.: Res-trans networks for lung nodule classification. In {\it Int J CARS,} vol. 17, no. 6, pp. 1059--1068 (2022) \doi{10.1007/s11548-022-02576-5}

\bibitem{wang2022transpnd}
Wang, R., Zhang, Y., Yang, J.: TransPND: A Transformer Based Pulmonary Nodule Diagnosis Method on CT Image. In {\it Proc. PRCV,} 
% Shenzhen, China, 
pp. 348--360 (2022) \doi{10.1007/978-3-031-18910-4\_29}

\bibitem{wang2022adversarial}
Wang, Y., Feng, Y., Zhang, L., Zhou, J.T., Liu, Y., Goh, R.S.M. \textit{et al.}: Adversarial multimodal fusion with attention mechanism for skin lesion classification using clinical and dermoscopic images. In {\it MIA,} vol. 81, pp. 102535 (2022) \doi{10.1016/j.media.2022.102535}

\bibitem{pathak2024Prototype}
Pathak, S., Schlötterer, J., Veltman, J., Geerdink, J., van Keulen, M., Seifert, C.: Prototype-Based Interpretable Breast Cancer Prediction Models: Analysis and Challenges. In {\it Proc. xAI, }
% Valetta, Malta 
(2024) \doi{10.1007/978-3-031-63787-2\_2}

\bibitem{koehler1991explanation}
Koehler, D.J.: Explanation, imagination, and confidence in judgment. In {\it Psychological bulletin,} vol. 110, nr. 3, pp. 499-–519, (1991) \doi{10.1037/0033-2909.110.3.499}

\bibitem{bi2020multi}
Bi, L., Feng, D.D., Fulham, M., Kim, J.: Multi-Label classification of multi-modality skin lesion via hyper-connected convolutional neural network. In {\it Pattern Recognition,} vol. 107, pp. 107502 (2020) \doi{10.1016/j.patcog.2020.107502}

\bibitem{tang2022Fusion}
Tang, P., Yan, X., Nan, Y., Xiang, S., Krammer, S., Lasser, T.: FusionM4Net: A multi-stage multi-modal learning algorithm for multi-label skin lesion classification. In {\it MIA,} vol. 76, pp. 102307 (2022) \doi{10.1016/j.media.2021.102307}

\bibitem{coppola2020interpreting}
Coppola, D., Lee, H.K., Guan, C.: Interpreting mechanisms of prediction for skin cancer diagnosis using multi-task learning. In {\it Proc. CVPR Workshops}, pp. 734--735 (2020)



\end{thebibliography}

% \section{Important Messages to MICCAI Authors}

% This is a modified version of Springer's LNCS template suitable for anonymized MICCAI 2025 main conference submissions. The author section has already been anonymized for you.  You cannot remove the author section to gain extra writing space.  You must not remove the abstract and the keyword sections in your submission.

% To avoid an accidental anonymity breach, you are not required to include the acknowledgments and the Disclosure of Interest sections for the initial submission phase.  If your paper is accepted, you must reinsert these sections to your final paper. 

% To avoid desk rejection of your paper, you are encouraged to read "Avoiding Desk Rejection" by the MICCAI submission platform manager.  A few important rules are summarized below for your reference:  
% \begin{enumerate}
%     \item Do not modify or remove the provided anonymized author section.
%     \item Do not remove the abstract and keyword sections to gain extra writing space. Authors must provide at least one keyword.
%     \item Inline figures (wrapping text around a figure) are not allowed.
%     \item Authors are not allowed to change the default margins, font size, font type, and document style.  If you are using any additional packages, it is the author's responsibility that any additional packages do not inherently change the document's layout.
%     \item It is prohibited to use commands such as \verb|\vspace| and \verb|\hspace| to reduce the pre-set white space in the document.
%     \item Please make sure your figures and tables do not span into the margins.
%     \item If you have tables, the smallest font size allowed is 8pt.
%     \item Please ensure your paper is properly anonymized.  Refer to your previously published paper in the third person.  Data collection location and private dataset should be masked.
%     \item Your paper must not exceed the page limit.  You can have 8 pages of the main content (text (including paper title, author section, abstract, keyword section), figures and tables, and conclusion) plus up to 2 pages of references.  The reference section does not need to start on a new page.  If you include the acknowledgments and the Disclosure of Interest sections, these two sections are considered part of the main content.
%     \item If you submit supplementary material, please note that reviewers are encouraged but not required to read it.  The main paper should be self-contained.  *NEW* Supplementary materials are limited to multimedia content (e.g., videos) as warranted by the technical application (e.g. robotics, surgery, ….). These files should not display any proofs, analysis, or additional results, and should not show any identification markers either. Violation of this guideline will lead to desk rejection. PDF files may NOT be submitted as supplementary materials in 2025 unless authors are citing a paper that has not yet been published.  In such a case, authors are required to submit an anonymized version of the cited paper.
% \end{enumerate}


% \section{Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

% \begin{comment}  %% removed for anonymized MICCAI 2025 submission.
    
%     % The following acknowledgement and disclaimer sections should be removed for the double-blind review process.  
%     % If and when your paper is accepted, reinsert the acknowledgement and the disclaimer clause in your final camera-ready version.

% \begin{credits}
% \subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
% used for general acknowledgments, for example: This study was funded
% by X (grant number Y).

% \subsubsection{\discintname}
% It is now necessary to declare any competing interests or to specifically
% state that the authors have no competing interests. Please place the
% statement with a bold run-in heading in small font size beneath the
% (optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
% system, is used, then the disclaimer can be provided directly in the system.},
% for example: The authors have no competing interests to declare that are
% relevant to the content of this article. Or: Author A has received research
% grants from Company W. Author B has received a speaker honorarium from
% Company X and owns stock in Company Y. Author C is a member of committee Z.
% \end{credits}

% \end{comment}
% %
% % ---- Bibliography ----
% %
% % BibTeX users should specify bibliography style 'splncs04'.
% % References will then be sorted and formatted in the correct style.
% %
% % \bibliographystyle{splncs04}
% % \bibliography{mybibliography}
% %
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
% \end{thebibliography}
\end{document}
