\section{Extending to Non-Markov Policy}\label{sec:non-markov}

The results in the previous section hold for Markov linear policies, where the control 
$u_t$ depends only on the most recent observation $y_t$. However, in partially observable control problems, it is well-known that Markov policies may not be expressive enough to capture the optimal policy, even in the i.i.d. noise setting \citep{bacsar2008h}. Thus, it is crucial to study whether optimal regret bounds can be achieved for a broader class of non-Markov policies.

\subsection{Motivation: The Need for a Richer Policy Class}
Prior work by \cite{simchowitz2020improper} studied control with partial observations under $\ell_2$ constraints. They noted that a richer policy class is required to capture the $H_{\infty}$ control policy in the i.i.d. noise setting. To address this, they introduced the class of stabilizing linear dynamic controller (LDC):
\begin{align*}
s_{t+1}=A_{\pi}s_t+B_{\pi}y_t, \quad u_t=C_{\pi}s_t+D_{\pi}y_t. 
\end{align*}

Here, the control is computed as a linear combination of the current observation $y_t$ and a hidden state $s_t$, which evolves according to a linear system driven by past observations. This structure enables the policy to incorporate the full history of observations rather than relying solely on 
$y_t$.

The LDC structure originates from the optimal solution for Linear Quadratic Gaussian (LQG) control with partial observations \citep{bacsar2008h}. In LQG, the key idea is to estimate an approximate state $\tilde{x}_t$ using a Kalman filter, then compute the optimal control $u_t$, as if $\tilde{x}_t$ were the true state $x_t$. The estimation evolves as 
$$\tilde{x}_{t+1}=A \tilde{x}_t+Bu_t+D(y_{t+1}-C(A \tilde{x}_t+Bu_t)),$$
where $D(y_{t+1}-C(A \tilde{x}_t+Bu_t))$ is the correction term to the original system based on the current observation $y_{t+1}$. Since $u_t$ is linear in $\tilde{x}_t$, $\tilde{x}_{t+1}$ is simply linear in $\tilde{x}_t$ and $y_{t+1}$, LDCs are expressive enough to capture classical LQG control with i.i.d. Gaussian noise.

\subsection{Extending to Population Dynamics}
For population dynamics, the optimal linear control policy under full observability was derived by \cite{golowich2024online}. To extend this to partial observability, we need an analogous comparator class to LDCs. Similarly we need to simulate $x_t$ by $s_t$ in the stochastic setting, the main difference lies in the original evolution of $x_t$:
$$
x_{t+1}=(1-\gamma_t)(Ax_t+Bu_t)+\gamma_t w_t.
$$
In contrast to classical control, where noise is Gaussian with zero mean, population dynamics involve disturbances $w_t$ over the simplex $\Delta^{d_x}$ with a nonzero mean $v$. A natural choice is to set
$$
s_{t+1}=(1-\gamma_t)(As_t+Bu_t)+\gamma_t v.
$$
Since $\gamma_t$ and $w_t$ are independent, $E[x_t]=E[s_t]$ holds by the linearity of expectation when $s_1=x_1$. Thus, this system is expressive enough to capture optimal population control with stochastic noise. For simplicity, we assume $v=\frac{1}{d} \mathbf{1}_d$ which is the most natural prior, and our analysis can be generalized to for any $v\in \Delta_d$.

\begin{definition}[simplex-LDC]
\label{def:po-comparator-ldc}
Given $d_x, d_y\in\mathbb{N}$, a simplex linear dynamical controller (simplex-LDC) is a LDC $\pi=(A_{\pi}, B_{\pi}, C_{\pi}, D_{\pi})$ of appropriate dimensions with the additional assumptions that
\begin{enumerate}
\item $s_1=\frac{1}{d_x}\mathbf{1}_{d_x}\in\Delta^{d_x}$.
\item $\forall s\in\Delta^{d_x}, y\in\Delta^{d_y}$, there holds that the next inner state and control evolved according to the simplex LDC are in the simplex, i.e.
\begin{align*}
s_{t+1}=(1-\gamma_t)((1-\alpha)A_{\pi}s_t+\alpha B_{\pi}y_t)+\gamma_t\cdot\frac{1}{d_x}\mathbf{1}_{d_x}\in\Delta^{d_x}, \quad u_t=C_{\pi}s_t+D_{\pi}y_t\in\Delta^{d_x}. 
\end{align*} 
\end{enumerate}
\end{definition}

Unlike standard control settings where states decay to zero without noise, population dynamics maintain a unit $\ell_1$-norm due to the simplex constraint. Therefore, we replace strong stabilizability with a mixing time assumption.

\begin{definition}[$\tau$-mixing simplex-LDC]
\label{def:mixing-simplex-ldc}
Given $\tau>0$ and a partially observable simplex LDS 
%$$\mathcal{L}=(A, B, C, x_1, (\gamma_t)_{t\in\mathbb{N}}, (w_t)_{t\in\BN},(c_t)_{t\in\BN}, \alpha)$$ 
with state and observation dimensions $d_x, d_y\in\mathbb{N}$, we denote as $\Pi_{\tau}(\ML)$ the class of $\tau$-mixing simplex-LDCs as any simplex-LDC $\pi=(A_{\pi}\in\mathbb{R}^{d_x\times d_x}, B_{\pi}\in\mathbb{R}^{d_x\times d_y}, C_{\pi}\in\mathbb{R}^{d_x\times d_x}, D_{\pi}\in\mathbb{R}^{d_x\times d_y}, s_1\in\mathbb{R}^{d_x})$ satisfying that for
\begin{align}
\label{eq:ldc-transition-matrix}
A_{\pi,\mathrm{cl}}=\begin{bmatrix}
(1-\alpha)A+\alpha BD_{\pi}C & \alpha BC_{\pi} \\
 B_{\pi}C & A_{\pi},
\end{bmatrix}
\end{align}
where $A_{\pi,\mathrm{cl}}:\Delta^{d_x}\times\Delta^{d_x}\rightarrow \Delta^{d_x}\times\Delta^{d_x}$, there exists unique $x\in\Delta^{d_x}$, $s\in\Delta^{d_x}$ such that 
\begin{align*}
t^{\mathrm{mix}}(A_{\pi,\mathrm{cl}})=\min_{t\in\mathbb{N}}\left\{t: \quad \forall t'\ge t, \sup_{p\in\Delta^{d_x},q\in\Delta^{d_x}}\|A_{\pi,\mathrm{cl}}^t(p,q)-(x,s)\|_1\le \frac{1}{4}\right\}\le \tau,
\end{align*}
where $(p,q), (x,s)\in\mathbb{R}^{d_x+d_x}$ denote the concatenated vectors of $p, q$ and $x,s$, respectively.
\end{definition}

In order to approximate the set of $\tau$-mixing simplex-LDC as in Definition~\ref{def:mixing-simplex-ldc}, we consider a slightly modified convex learning class than the one in Definition~\ref{def:learning-class} used in the Markov setting. 

\begin{definition} [Learning policy class for $\tau$-mixing simplex-LDC]
\label{def:learning-class-ldc}
Let $d_x,d_y,H \in \BN$ and let $\BS_{0}^{d_x,d_y}$ be set of matrices $K\in[-1,1]^{d_x\times d_y}$ such that $\sum_{i=1}^{d_x} K_{ij} = 0$ for all $j \in [d_y]$. Then let $\MM_{d_x,d_y,H}^{+}$ be the set of tuples ${M^{[0:H]}} = (M^{[0]},\dots,M^{[H]})$ defined as follows:
\begin{align}
\label{eq:M-class-nonmarkov}
\MM_{d_x,d_y,H}^{+} := \{M^{[0:H]}\mid M^{[0]}\in\BS^{d_x,d_y}\otimes \BS^{d_x,d_x} \text{ and } M^{[i]}\in\BS_{0}^{d_x,d_y}\otimes \BS_{0}^{d_x,d_x} \text{ for all }  i \in [H]\}.
\end{align}
\end{definition}
First, we observe that $\MM_{d_x,d_y,H}^{+}$ defined in Definition~\ref{def:learning-class-ldc} is convex. We will show that $\MM_{d_x,d_y,H}^{+}$ approximates the class of $\tau$--mixing simplex-LDC well. Therefore, a regret bound with respect to $\MM_{d_x,d_y,H}^{+}$ translates to the regret bound with respect to the class of $\tau$--mixing simplex-LDC. 

\subsection{Our Algorithm and Result}

With the learning policy class defined, our new controller algorithm (deferred to appendix) is a modified version of Algorithm~\ref{alg:gpc-po} to accommodate the new class of learning policies. Essentially, we raise the dimension of the signals to incorporate the uniform prior.  

For any $\pi \in \Pi_{\tau}(\ML)$ and partially observable simplex LDS $\ML$, we let $(y_t(\pi))_t$ and $(u_t(\pi))_t$ denote the counterfactual sequences of observations and controls that would have been obtained by playing the policy $\pi$ from the beginning of the time. We can now state our main result for controlling partially observable simplex LDS, with respect to the class of $\tau$-mixing simplex-LDC.

\begin{theorem} [PO regret ($\tau$-mixing simplex-LDC)]
\label{thm:po-main-ldc-simplex} 
Let $d_x,d_y, T\in\BN$ with $d_y\leq d_x$. %Let $\ML=(A,B, C, x_1,(\gamma_t)_{t\in\BN},(w_t)_{t\in\BN},(c_t)_{t\in\BN}, \alpha)$ be a partially observable simplex LDS with cost functions $(c_t)_t$ satisfying \cref{asm:po-convex-loss} for some $L > 0$. 
Set $H=\lceil 2\tau\lceil \log\frac{4096\tau d_x^{7/2} L T^2}{\epsilon}\rceil \rceil$, $\eta=1/(LH^{2}d_x^{6}\sqrt{T})$. Then for any $\tau > 0$, there exists an algorithm with the following policy regret guarantee with respect to $\Pi_{\tau}(\ML)$: \begin{align*}
\sum_{t=1}^T c_t(y_t,u_t)-\min_{\pi\in\Pi_{\tau}(\ML)}\sum_{t=1}^T c_t(y_t(\pi),u_t(\pi))\le \tilde{O}\left(L\tau^{4}d_x^{8}\sqrt{T}\right),
\end{align*}
where $\tilde{O}(\cdot)$ hides all universal constants and poly-logarithmic dependence in $T$. 
\end{theorem}

\subsection{New Technical Ingredients} Similarly to the previous setting, Theorem~\ref{thm:po-main-ldc-simplex} depends on several building blocks. 
First, we will show that the convex learning policy class (Definition~\ref{def:learning-class-ldc}) used in our algorithm approximates $\Pi_{\tau}(\ML)$ well. To incorporate the structure of population dynamics, we design a new LDC analogue
\begin{align*}
\begin{bmatrix}
x_{t+1}(\pi) \\
s_{t+1}(\pi)
\end{bmatrix}
&=(1-\gamma_t)A_{\pi,\mathrm{cl}}\begin{bmatrix}
x_t(\pi)\\
s_t(\pi)
\end{bmatrix}+\gamma_t \begin{bmatrix}
w_t\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}, \\
\begin{bmatrix}
y_{t}(\pi) \\
u_{t}(\pi)
\end{bmatrix}
&=\begin{bmatrix}
C & 0\\
D_{\pi}C & C_{\pi}
\end{bmatrix}\begin{bmatrix}
x_t(\pi)\\
s_t(\pi)
\end{bmatrix},
\end{align*}
coupled with a change in the control $\tilde{u}_t$ by appending the prior below $o_t$. The additional $\frac{1}{d_x}\mathbf{1}_{d_x}$ term is crucial to proving the desired equivalence, such that after careful decomposition and rewriting of the control, we can use the important matrix identity $X^n=Y^n+\sum_{i=1}^n X^{i-1}(X-Y)Y^{n-i}$ as a bridge.

Next, we will show that the memory mismatch error is small. In this step, because our mixing assumption differs from that in the previous section, a fine-grained analysis is conducted on lower bounding $\alpha$ when the mixing time of $A$ is large. A key step is to bound the mixing time of $A$ by the mixing time of $A_{\pi,\mathrm{cl}}=\begin{bmatrix}
(1-\alpha)A+\alpha BD_{\pi}C & \alpha BC_{\pi} \\
 B_{\pi}C & A_{\pi}
\end{bmatrix}.$ Because they have different dimensions, we need an auxiliary matrix $A_{compare}=\begin{bmatrix}
A & 0 \\
 B_{\pi}C & A_{\pi}
\end{bmatrix}$ as a bridge. This requires a new analysis along with a slightly worse dependence on $d_x$, which comes from the $\ell_1$ norm estimation of a key term, which is no longer in $\Delta^{d_x}$ but lives in $\Delta^{d_x}-\Delta^{d_x}$.