\tableofcontents

\newpage

\section{Omitted Proofs from Section \ref{sec:min}}

\subsection{Proof of Lemma~\ref{lem:extension-property}}
\begin{proof} The first two properties follow immediately from construction. 
    We only need to prove the third property, and we do this by showing that the directional derivative satisfies $\partial_x e_f(x)\ge 0, \forall x\notin \mathcal{K}$.
    
    For any two points $x_1,x_2$, denote $y_1=\frac{x_1}{\pi_{\mathcal{K}}(x_1)}, y_2=\frac{x_2}{\pi_{\mathcal{K}}(x_2)}$, we need to prove
    \begin{align}
    \label{eq:minkowski-convex-inequality}
    \pi_{\mathcal{K}}(x_1)f(y_1)+\pi_{\mathcal{K}}(x_2)f(y_2)\ge 2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right)f\left(\frac{x_1+x_2}{2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right)}\right).
    \end{align}
    We notice that
    $$
    \frac{x_1+x_2}{2\pi_{\mathcal{K}}(\frac{x_1+x_2}{2})}=ay_1+by_2,
    $$
    where 
    \begin{align*}
        a=\frac{\pi_{\mathcal{K}}(x_1)}{2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right)}, \quad b=\frac{\pi_{\mathcal{K}}(x_2)}{2\pi_{\mathcal{K}}(\frac{x_1+x_2}{2})}.
    \end{align*}
    By the convexity of $f$, the right hand side of \cref{eq:minkowski-convex-inequality} can be bounded by
    \begin{align*}
    2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right)f(ay_1+by_2)&\le 2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right)\frac{af(y_1)+bf(y_2)}{a+b}\\
    &=\frac{4\pi_{\mathcal{K}}^2\left(\frac{x_1+x_2}{2}\right)}{\pi_{\mathcal{K}}(x_1)+\pi_{\mathcal{K}}(x_2)}[af(y_1)+bf(y_2)],
    \end{align*}
    while the left hand side of \cref{eq:minkowski-convex-inequality} is just
    $$
    2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right)[af(y_1)+bf(y_2)].
    $$
    It suffice to prove 
    $$
    \frac{4\pi_{\mathcal{K}}^2\left(\frac{x_1+x_2}{2}\right)}{\pi_{\mathcal{K}}(x_1)+\pi_{\mathcal{K}}(x_2)}\le 2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right),
    $$
    which is equivalent to 
    $$
    \pi_{\mathcal{K}}(x_1)+\pi_{\mathcal{K}}(x_2)\ge 2\pi_{\mathcal{K}}\left(\frac{x_1+x_2}{2}\right),
    $$
    by the convexity of $\pi_{\mathcal{K}}$. Because both $\pi_{\mathcal{K}}, f$ are continuous and $f$ is non-negative, mid-point convexity implies convexity.
\end{proof}




\subsection{Proof of Lemma~\ref{lem:extension-lip}}

\begin{proof}
To analyze the curvature of Minkowski projections and extensions, we introduce the concept of non-expanding functions.

\begin{definition}[Non-expanding functions]
Given $d_1,d_2\in\mathbb{N}$. Let $\K\subseteq \mathbb{R}^{d_1}$.
Consider a function $f:\K\rightarrow\mathbb{R}^{d_2}$. Let $\|\cdot\|_a, \|\cdot\|_b$ be any two well-defined norms on $\mathbb{R}^{d_1}$, $\mathbb{R}^{d_2}$, respectively. $\forall L>0$, we say that $f$ is $L$-non-expanding w.r.t. the norm tuple $(\|\cdot\|_a, \|\cdot\|_b)$ if $\|f(x_1)-f(x_2)\|_b\le L\|x_1-x_2\|_a$, $\forall x_1,x_2\in\K$. 
\end{definition}

Lemma~\ref{thm:proj-lip} bounds the Lipschitzness of Minkowski projection onto an aspherical convex set. 

\begin{lemma} [Lipschitzness of Minkowski projection]
\label{thm:proj-lip}
Consider a convex set $\mathcal{K}\subset\mathbb{R}^d$ that satisfies $\kappa$-aspherity ($\kappa\ge 1$ by Definition~\ref{def:aspherity}) and its associated Minkowski functional $\pi_{\mathcal{K}}$.  We have that the Minkowski projection function $x\mapsto \frac{x}{\pi(x)}$ is $4\kappa^3$-non-expanding w.r.t. $(\|\cdot\|_2,\|\cdot\|_2)$ on $\mathbb{R}^d$, i.e. $\forall x_1,x_2\in\mathbb{R}^d$,
\begin{align*}
\left\|\frac{x_1}{\pi_{\mathcal{K}}(x_1)}-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\right\|_2\le 4\kappa^3\cdot\|x_1-x_2\|_2. 
\end{align*}
\end{lemma}

Using this, we obtain a Lipschitz bound for the Minkowski extension. Fix $u,v\in D\mathbb{B}^d$. By Lipschitz condition on $f$ and Lemma~\ref{thm:proj-lip}, we have that
\begin{align*}
\left|f\left(\frac{u}{\pi_{\mathcal{K}}(u)}\right)-f\left(\frac{v}{\pi_{\mathcal{K}}(v)}\right)\right|\le 4L_f\kappa^3\|u-v\|_2. 
\end{align*}
In addition, by \cite{lu2023projection}, we have that $\pi_{\mathcal{K}}$ is $\frac{1}{r}$-Lipschitz.
As a result,
\begin{align*}
|e(u)-e(v)|&=\left|\pi_{\mathcal{K}}(u)f\left(\frac{u}{\pi_{\mathcal{K}}(u)}\right)-\pi_{\mathcal{K}}(v)f\left(\frac{v}{\pi_{\mathcal{K}}(v)}\right)\right|\\
&\le \left|\pi_{\mathcal{K}}(u)f\left(\frac{u}{\pi_{\mathcal{K}}(u)}\right)-\pi_{\mathcal{K}}(u)f\left(\frac{v}{\pi_{\mathcal{K}}(v)}\right)\right|+\left|\pi_{\mathcal{K}}(u)f\left(\frac{v}{\pi_{\mathcal{K}}(v)}\right)-\pi_{\mathcal{K}}(v)f\left(\frac{v}{\pi_{\mathcal{K}}(v)}\right)\right|\\
&\le 4|\pi_{\mathcal{K}}(u)|\cdot L_f\kappa^3\|u-v\|_2+\left|f\left(\frac{v}{\pi_{\mathcal{K}}(v)}\right)\right|\cdot \frac{1}{r}\|u-v\|_2\\
&\le 4 DL_f\kappa^4\|u-v\|_2+D_{\K}L_f\cdot \frac{1}{r}\|u-v\|_2\\
&\le (4DL_f\kappa^4+\frac{D_{\K}L_f}{r})\|u-v\|_2, 
\end{align*}
where the second inequality uses the Lipschitz condition of $f$ and $\pi_{\mathcal{K}}$; the third inequality uses the fact that $\frac{u}{D\kappa}\in \K$. 
\end{proof}

\subsection{Proof of Lemma~\ref{thm:proj-lip}}

\begin{proof} Let $\kappa=\frac{R}{r}$ such that $r\mathbb{B}^d\subset\K\subset R\mathbb{B}^{d}$. We discuss three cases of $x_1,x_2$.

\paragraph{Case 1, $x_1,x_2\in \mathcal{K}$:} in this case $\pi_{\mathcal{K}}(x_1)=\pi_{\mathcal{K}}(x_2)$, and clearly $\|\frac{x_1}{\pi_{\mathcal{K}}(x_1)}-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2=\|x_1-x_2\|_2$.


\paragraph{Case 2, $x_1,x_2\notin \mathcal{K}$:} first we prove the following useful lemma.

\begin{lemma}
\label{lem lip}
    A function $f$ is $L$-non-expanding, if and only if there exists some constant $C>0$, for any $x_1,x_2$ with $\|x_1-x_2\|_2\le C$, we have that
    $$
    \|f(x_1)-f(x_2)\|_2\le L\|x_1-x_2\|_2.
    $$
\end{lemma} 
\begin{proof}
    The only if direction is straightforward. For the if direction, for any $x_1,x_2$ with distance $\|x_1-x_2\|_2=N>0$, denote $n=\lfloor \frac{N}{C}\rfloor+1$, and denote $y_i=\frac{i}{n}x_1+\frac{n-i}{n}x_2$, for $i\in [n-1]$. Then $\|x_2-y_1\|_2, \|x_1-y_{n-1}\|_2, \|y_j-y_{j+1}\|_2$ are all bounded by $C$ for any $j\in [n-2]$. As a result, $\|f(x_2)-f(y_1)\|_2, \|f(x_1)-f(y_{n-1})\|_2, \|f(y_j)-f(y_{j+1})\|_2$ are all bounded by $\frac{NL}{n}$ for any $j\in [n-2]$. Therefore,
    \begin{align*}
         \|f(x_1)-f(x_2)\|_2&\le \|f(x_2)-f(y_1)\|_2+ \|f(x_1)-f(y_{n-1})\|_2+\sum_{j=1}^{n-2} \|f(y_j)-f(y_{j+1})\|_2 \\
         &\le NL =L\|x_1-x_2\|_2.
    \end{align*}
\end{proof}

Intuitively, Lemma~\ref{lem lip} tells us that we only need to consider $x_1,x_2$ with a small distance for proving the theorem. In particular, since $r\mathbb{B}^d\subset \mathcal{K}$, which means $\|x_1\|_2,\|x_2\|_2\ge r$, we know that the angle $\theta\in [0,\pi]$ between $x_1,x_2$ should also be small. Then by the convexity of $\mathcal{K}$, we can show that a small angle implies a small distance after projection, via a geometric argument.

Denote $a=\|\frac{x_1}{\pi_{\mathcal{K}}(x_1)}\|_2, b= \|\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2$, where $a\ge b$. Without loss of generality, by a coordinate change, we may assume for some $\theta\in[0,\pi]$, 
$$
\frac{x_1}{\pi_{\mathcal{K}}(x_1)}=(a,0,\cdots, 0), \quad \frac{x_2}{\pi_{\mathcal{K}}(x_2)}=(b\cos\theta,b\sin \theta, 0, \cdots, 0).
$$
By the law of cosines, we have that
$$
\left\|\frac{x_1}{\pi_{\mathcal{K}}(x_1)}-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\right\|_2=\sqrt{a^2+b^2-2ab\cos \theta}.
$$
The above expression is convex in $a$ for a fixed $b$ and vice versa. Therefore, the derivative is monotone. Thus, it's upper bounded by 
\begin{align*}
\max\{\sqrt{b^2+b^2-2b^2\cos \theta},\sqrt{R^2+b^2-2Rb\cos \theta}\}
\end{align*}
for any fixed $b$. Similarly, it's bounded by 
\begin{align*}
\max\{\sqrt{r^2+a^2-2ra\cos \theta},\sqrt{a^2+a^2-2a^2\cos \theta}\}
\end{align*}
for any fixed $a$. We discuss two cases of $\theta$.

\textbf{Case A, $\cos \theta\le \frac{r}{a}$:}
in this case, we have that
$$
x_2^{\top}(\frac{x_1}{\pi_{\mathcal{K}}(x_1)}-\frac{rx_2}{\|x_2\|_2})\le 0,
$$
which is equivalent to $\cos \theta(a-r\cos \theta)-r\sin^2 \theta\le 0$. As a result, for a fixed $a$, the maximum of distance is taken at $b=a$, therefore
$$
\sqrt{r^2+a^2-2ra\cos \theta}\le \sqrt{a^2+a^2-2a^2\cos \theta}\le R\theta,
$$
since $1-\cos \theta=2\sin^2 \frac{\theta}{2}$, and $\sin \theta\le \theta$ for $\theta\ge 0$.

\textbf{Case B, $\cos \theta> \frac{r}{a}$:} 
in this case, the estimate $\sqrt{r^2+a^2-2ra\cos \theta}$ is too rough, and we will make use of the assumption to obtain a lower bound on $b$.
Because $\mathcal{K}$ is convex, the triangle formed by 
$$(a,0,\cdots, 0),(\frac{r^2}{a},\frac{r\sqrt{a^2-r^2}}{a},\cdots, 0),(0,0,\cdots, 0)$$
is a subset of $\mathcal{K}$. Slightly abusing the notations, denote $x,y$ as the first two coordinates for now. We denote $P$ as the intersection point between the two lines
$$
y=\tan \theta \times x, y=-\frac{r}{\sqrt{a^2-r^2}}(x-a).
$$
Then we have that
$$
P=(\frac{ra}{\sqrt{a^2-r^2}\tan \theta+r},\frac{\tan \theta \times ra}{\sqrt{a^2-r^2}\tan \theta+r}),
$$
therefore $b\ge \|P\|_2=\frac{ra}{\sqrt{a^2-r^2}\sin \theta +r\cos \theta}$. As a result, inserting $b$ into the law of cosines, the distance is at most
$$
\max\{\frac{a^2\sin \theta}{\sqrt{a^2-r^2}\sin \theta +r\cos \theta}, \sqrt{2a^2(1-\cos \theta)}\}\le \frac{R^3 \theta}{r^2}.
$$

Now we only need to establish an upper bound on $\theta$. We set $C=\frac{r}{100}$ in Lemma \ref{lem lip}. Since $\|x_1-x_2\|_2\le C$, we have that $\sin \frac{\theta}{2}\le \frac{\|x_1-x_2\|_2}{2r}\le \frac{1}{200}$, and in this regime, 
$$
\theta \le 4 \sin \frac{\theta}{2} \le \frac{2}{r} \|x_1-x_2\|_2.
$$
Combining the two bounds, we have that
$$
\|\frac{x_1}{\pi_{\mathcal{K}}(x_1)}-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2\le \frac{2}{r}\max\{R, \frac{R^3}{r^2}\} \|x_1-x_2\|_2\le 2\kappa^3 \|x_1-x_2\|_2.
$$

\paragraph{Case 3, $x_1\in \mathcal{K}$, $x_2\notin \mathcal{K}$:} we denote $a=\|x_1\|_2\le R$, $b=\|x_2\|_2\ge r$ and $c=\|\frac{x_2}{\pi(x_2)}\|_2\ge r$. We make an argument based on the angle $\theta\in [0,\pi]$ between $x_1,x_2$. Without loss of generality, by a coordinate change, we may assume 
$$
x_1=\frac{x_1}{\pi_{\mathcal{K}}(x_1)}=(a\cos \theta,a\sin \theta,\cdots, 0),\frac{x_2}{\pi_{\mathcal{K}}(x_2)}=(c, 0, \cdots, 0).
$$
We have that
$$
\|x_1-x_2\|_2^2=a^2+b^2-2ab\cos\theta, \|x_1-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2^2=a^2+c^2-2ac\cos\theta.
$$
Clearly, when $a\cos\theta\le r$, $x_2^{\top}(x_1-\frac{x_2}{\pi_{\mathcal{K}}(x_2)})\le 0$, and $\|\frac{x_1}{\pi_{\mathcal{K}}(x_1)}-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2\le \|x_1-x_2\|_2$. Therefore we only need to consider the case when $\cos\theta> \frac{r}{a}$ (and $a\ge r$). In this case, $\|x_1-x_2\|_2\ge \sqrt{a^2\sin^2 \theta}=a\sin \theta$. If $a\sin \theta\ge \frac{r}{2}$, then because $\|x_1-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2\le 2R$, we have that
$$
\|x_1-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2\le \frac{4R}{r}\|x_1-x_2\|_2.
$$
If $a\sin \theta< \frac{r}{2}$, because $r\mathbb{B}^d\subset \mathcal{K}$, similar to case 2B, we have that 
$$
c\ge \frac{ar(r\cos\theta+\sqrt{a^2-r^2}\sin\theta)}{r^2-a^2\sin^2\theta},
$$
and
$$
\|x_1-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2^2\le a^2\sin^2 \theta+a^2\sin^2 \theta[\frac{a^2\sin\theta \cos \theta+r\sqrt{a^2-r^2}}{r^2-a^2\sin^2\theta}]^2.
$$
We notice that
$$
a^2\sin\theta \cos \theta+r\sqrt{a^2-r^2}\le \frac{3R^2}{2}= 2\kappa^2 \frac{3r^2}{4}\le 2\kappa^2(r^2-a^2\sin^2\theta).
$$
As a result, 
$$
\|x_1-\frac{x_2}{\pi_{\mathcal{K}}(x_2)}\|_2\le a\sin \theta (1+2\kappa^2)\le (1+2\kappa^2)\|x_1-x_2\|_2.
$$
Notice that both $4\kappa, 1+2\kappa^2$ are upper bounded by $4\kappa^2$, we conclude our proof.
\end{proof}

\subsection{Proof of Lemma~\ref{cor:lip-control}}

\begin{proof}
We will show in the following lemma that there exists a linear mapping between $\mathbb{R}^{d_x}$ and $\mathcal{K}$ that is bijective when restricted to $\Delta^{d_x}$, and $\K$ satisfies the aspherity condition. 


\begin{lemma} [Transformation]
\label{lem:transformation}
The sets $\Delta^{d_x}$, $\mathcal{K}$ satisfy the following properties:
\begin{enumerate}
\item There exists a linear map $\phi:\mathbb{R}^{d_x}\rightarrow\mathcal{K}$ that is bijective when restricted to $\Delta^{d_x}$. 
\item $\phi$ is $1$-non-expanding w.r.t. both $(\|\cdot\|_1,\|\cdot\|_1)$ and $(\|\cdot\|_2, \|\cdot\|_2)$. 
Let $\phi^{-1}:\K\rightarrow\Delta^{d_x}$ be the inverse of $\phi$ restricted to $\Delta^{d_x}$. $\phi^{-1}$ is $2$-non-expanding w.r.t. $(\|\cdot\|_1, \|\cdot\|_1)$ and $d_x$-non-expanding w.r.t. $(\|\cdot\|_2, \|\cdot\|_2)$. 
\item $\K$ is $\kappa$-aspherical with $\kappa=R/r=2d_x$, where $R=1$, and $r=1/(2d_x)$.
\end{enumerate}
\end{lemma}

As a result, (1) follows from that $\pi$ is $(1/r)$-Lipschitz (\cite{lu2023projection}) and $r=2d_x$ (Lemma~\ref{lem:transformation}). (2) follows from Lemma~\ref{lem:extension-lip} with $\|\cdot\|=\|\cdot\|_1$ and $\kappa=2d_x$ in Lemma~\ref{lem:transformation}.
\end{proof}


\subsection{Proof of Lemma~\ref{lem:transformation}}

\begin{proof}
The linear map $\phi$ is simply given by $\forall u=(u_1,\dots,u_{d_x})\in\mathbb{R}^{d_x}$,
\begin{align}
\label{eq:transformation-map}
(u_1,\dots,u_{d_x})\mapsto (u_1,\dots,u_{d_x-1})-\frac{\mathbf{1}_{d_x-1}}{2(d_x-1)}.
\end{align}
It is easy to see that $\phi$ is bijective when restricted to $\Delta^{d_x}$, and $\phi$ is $1$-non-expanding; $\phi^{-1}$ is $2$-non-expanding w.r.t. $(\|\cdot\|_1, \|\cdot\|_1)$ and $d_x$-non-expanding w.r.t. $(\|\cdot\|_2, \|\cdot\|_2)$.

We move on to show the aspherity of $\K$. Fix $v\in\mathbb{R}^{d_y-1}$ such that $\|v\|_2\le r$. By choice of $r$, we have
\begin{align*}
&\min_{1\le i\le d_x-1}e_i^{\top}\left(\frac{\mathbf{1}_{d_x-1}}{2(d_x-1)}+v\right)\ge -r+\frac{1}{2(d_x-1)}\ge  0,\\
&\left\|v+\frac{\mathbf{1}_{d_x-1}}{2(d_x-1)}\right\|_1=\|v\|_1+\frac{1}{2}\le \sqrt{d_x-1}\|v\|_2+\frac{1}{2}\le 1.
\end{align*}
To see the upper bound, we have that $\forall u\in\K$, $u=v+\frac{\mathbf{1}_{d_x-1}}{2(d_x-1)}$, $v\ge 0, \|v\|_1\le 1$ and thus satisfies
\begin{align*}
\|u\|_2=\sqrt{\sum_{i=1}^{d_x-1}\left(v_i-\frac{1}{2(d_x-1)}\right)^2}\le 1-\frac{1}{2(d_x-1)}\le R. 
\end{align*}
\end{proof}

\newpage

\section{Proof of Theorem~\ref{thm:po-main}}

\subsection{Proof of Observation~\ref{obs:ext}}
\begin{proof}
$e_t$ is convex since $c_t^y$ is convex, $c_t^u\circ\phi^{-1}$ is a convex function (composition of convex and linear functions) over $\mathcal{K}$, and thus the Minkowski extension of $c_t^u\circ\phi^{-1}$ on $\K$ is convex (Lemma~\ref{lem:extension-property}). The inequality follows from the third property in Lemma~\ref{lem:extension-property}.
\begin{align*}
e_t(M)&\ge c_t^y(y_t(M; K_0))+c_t^u(\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))}))\\
&=c_t^y(y_t(M; K_0))+c_t^u(u_t(M; K_0))\\
&=c_t(y_t(M;K_0),u_t(M;K_0)).
\end{align*}
\end{proof}

\subsection{Signal Realizability}
Unlike $\GPCS$ (and $\GPC$), the signals that $\GPCPOS$ combines to construct the control at each step are not the disturbances $w_{t-1:t-H}$, which are no longer identifiable due to the observation mechanism. Instead, like the partially observable variant of $\GPC$ \cite[Chapter 9]{hazan2022introduction}, the signals are the counterfactual observations that would have been observed had a ``default'' linear policy $y \mapsto K_0 y$ been followed for all time. Indeed, the following lemma shows that the signal $o_t$ computed in Line~\ref{line:compute-signal} of \cref{alg:gpc-po} is exactly $y_t(K_0)$ where $K_0$ is chosen (arbitrarily) in Line~\ref{line:choose-k0}.

\begin{lemma} [Signal realizability]
\label{lemma:signal-realizability}
Let $d_x,d_y,T \in \BN$ and let $\ML = (A,B,C,x_1,(\gamma_t)_t,(w_t)_t,(c_t)_t,\alpha)$ be a partially observable simplex LDS. For any $u_1,\dots,u_T \in \Delta^{d_x}$, let $y_t(u_{1:t-1})$ denote the observation at time $t$ in $\ML$ if controls $u_{1:t-1}$ were played at times $1,\dots,t-1$. Then for each $t$ it holds that
\begin{equation} y_t(u_{1:t-1}) = \sum_{i=1}^{t-1} \bar\lambda_{t,i}C((1-\alpha)A)^{i-1}\alpha Bu_{t-i} + \sum_{i=1}^t \lambda_{t,i}C((1-\alpha)A)^{i-1} w_{t-i},\label{eq:yt-unfold-po}\end{equation}
where $w_0:=x_1$.
Thus, for any $K_0 \in \BS^{d_y,d_x}$ and $t \in [T]$, it holds that
\begin{align}
  \label{eq:yt-unfold-po-2}y_t(u_{1:t-1}) - \sum_{i=1}^{t-1} \bar\lambda_{t,i}C((1-\alpha)A)^{i-1}\alpha B(u_{t-i} - K_0 y_{t-i}(K_0)) = y_t(K_0).\end{align}
\end{lemma}

\begin{proof}
It suffices to show that $\forall t$,
\begin{equation} x_t(u_{1:t-1}) = \sum_{i=1}^{t-1} \bar\lambda_{t,i}((1-\alpha)A)^{i-1}\alpha Bu_{t-i} + \sum_{i=1}^t \lambda_{t,i}((1-\alpha)A)^{i-1} w_{t-i}.\label{eq:xt-unfold-po}\end{equation}
We prove \cref{eq:xt-unfold-po} via induction on $t\in\mathbb{N}$. When $t=1$, $x_1=\lambda_{1,1}w_0$ by definition. Suppose for $t\in\mathbb{N}$, \cref{eq:xt-unfold-po} holds. Then,
\begin{align*}
x_{t+1}(u_{1:t})&=(1-\gamma_t)(1-\alpha)Ax_{t}(u_{1:t-1})+(1-\gamma_ t)\alpha Bu_t+\gamma_tw_t\\
&=(1-\gamma_t)\sum_{i=2}^{t}\bar{\lambda}_{t,i-1}((1-\alpha)A)^{i-1}\alpha Bu_{t+1-i}+(1-\gamma_t)\alpha Bu_t\\
& \quad +(1-\gamma_t)\sum_{i=2}^{t+1}\lambda_{t,i-1}((1-\alpha)A)^{i-1}w_{t+1-i}+\gamma_tw_t\\
&=\sum_{i=1}^{t}\bar{\lambda}_{t+1,i}((1-\alpha)A)^{i-1}\alpha Bu_{t+1-i}+\sum_{i=1}^{t+1}\lambda_{t+1,i}((1-\alpha)A)^{i-1}w_{t+1-i},
\end{align*}
where the last equality uses that $(1-\gamma_t)\bar{\lambda}_{t,i-1}=\bar{\lambda}_{t+1,i}$, $(1-\gamma_t)=\bar{\lambda}_{t+1,1}$, $(1-\gamma_t)\lambda_{t,i-1}=\lambda_{t+1,i}$, and $\gamma_t=\lambda_{t+1,1}$. 
\end{proof}

\subsection{Approximation of Linear Markov Policies}
\label{sec:approx-po}
We start by proving that every policy in the comparator class can be approximated by $\pi^M$ for some $M \in \MM_{d_x,d_y,H}$.  
We will also use the following equality:

%Similar to the fully observable setting, we need to show that the learning class $\mathcal{M}$ defined in \cref{def:learning-class} approximates the comparator class defined in \cref{def:po-comparator} well. 

\begin{observation}
\label{fact:linear-unfold}
For any $K \in \BS^{d_x,d_y}$ and $t \in [T]$, the observation in $\ML$ at time $t$ under policy $y \mapsto Ky$ can be written as
\begin{align}
\label{eq:K0-signal-unfold}
y_t(K)&=\sum_{i=1}^{t}\lambda_{t,i}C(\mathbb{A}_{K}^{\alpha})^{i-1}w_{t-i}.
\end{align}
Moreover, we have $u_t(K) = K \cdot y_t(K)$. 
\end{observation}

\begin{lemma} [Approximation]
\label{lem:po-approx}
Suppose that the cost functions $c_1,\dots,c_T$ of $\ML$ satisfy \cref{asm:po-convex-loss} with Lipschitz parameter $L>0$. Fix $\tau,\eps>0$ and suppose $H$ satisfies $H\ge 2\tau\lceil \log(512d_x^{7/2}LT^2/\eps)\rceil$. For any $K^{\star}\in\Ksim_\tau$ and $K_0\in\BS^{d_x,d_y}$, there is some $M\in\MM_{d_x,d_y,H}$ such that
\begin{align*}
\left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^Tc_t(y_t(K^{\star}),u_t(K^{\star}))\right|\le \eps.
\end{align*}
\end{lemma}

\begin{proof}
We break down the proof into two parts. We will construct $M\in\MM_{d_x,d_y,H}$ and show separately that the following two inequalities hold.
\begin{align}
\left|\sum_{t=1}^T c_t(y_t(M;K_0),u_t(M;K_0))-\sum_{t=1}^T c_t(y_t(K^{\star}), u_t(K^{\star}))\right|\le \frac{\eps}{2} \label{eq:cost-approx},\\
\left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^T c_t(y_t(M;K_0),u_t(M;K_0))\right|\le \frac{\eps}{2} \label{eq:ext-approx}.
\end{align}
\paragraph{Proof of \cref{eq:cost-approx}.} We start with \cref{eq:cost-approx}. For each $M$, recall that $\tilde{u}_t(M;K_0)$ is the control parameterized by $M$ using signals generated by the linear policy $K_0$ before the Minkowski projection. Formally, this is defined as
\begin{align*}
\tilde{u}_t(M;K_0) := M^{[0]}y_t(K_0)+\alpha\sum_{i=1}^H \bar{\lambda}_{t,i}M^{[i]}y_{t-i}(K_0),
\end{align*}
so that $u_t(M;K_0)$ is obtained by the Minkowski projection of $(y_t(M;K_0),\tilde{u}_t(M;K_0))$. First, we show that there exists $M$ such that the sequence $(\tilde{u}_t(M;K_0))_t$ approximates $(u_t(K^\st))_t$ well. Second, we show that the error induced by the projection operation is small. Third, we use Lipschitzness of the cost functions to conclude. 

Concretely, define $M = M^{[0:H]}$ by $M^{[0]} := K^\st$ and $M^{[i]} := K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{i-1}B(K^{\star}-K_0)$ for each $i \in [H]$. Note that since $K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{i-1}BK^{\star}\in\mathbb{S}^{d_x, d_y}$ and $ K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{i-1}BK_0 \in \BS^{d_x,d_y}$, we have that $M^{[i]}\in\mathbb{S}_{0}^{d_x,d_y}$ for each $i \in [H]$, and hence $M\in \MM_{d_x,d_y,H}$. We will show that the following bound hold:
\begin{align}
\|\tilde{u}_t(M;K_0)-u_t(K^{\star})\|_1&\le \frac{\eps}{512d_x^{7/2}LT^2}, \label{eq:utilde-uk}
\end{align}
and use \cref{eq:utilde-uk} to show that
\begin{align}
\|u_t(M;K_0)-u_t(K^{\star})\|_1&\le \frac{\eps}{8LT^2}\label{eq:u-uk}.
\end{align}
We start with \cref{eq:utilde-uk}. For notational convenience, define $M^{[i]} := K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{i-1}B(K^{\star}-K_0)$ for each $H<i\le t-1$, and define
\begin{align*}
\hat{u}_{t}(M;K_0):=M^{[0]}y_t(K_0)+\alpha\sum_{i=1}^{t-1}\bar{\lambda}_{t,i}M^{[i]}y_{t-i}(K_0).
\end{align*}
Then by applying Observation~\ref{fact:linear-unfold} to $K_0$, we have
\begin{align*}
& \quad \hat{u}_{t}(M;K_0) \\
&=M^{[0]}y_t(K_0)+\alpha\sum_{i=1}^{t-1}\sum_{j=i+1}^{t}\bar{\lambda}_{t,i}\lambda_{t-i,j-i}M^{[i]}C(\mathbb{A}_{K_{0}}^{\alpha})^{j-i-1}w_{t-j}\\
&=M^{[0]}y_t(K_0)+\alpha\sum_{j=1}^{t}\left(\sum_{i=1}^{j-1}\bar{\lambda}_{t,i}\lambda_{t-i,j-i}M^{[i]}C(\mathbb{A}_{K_{0}}^{\alpha})^{j-i-1}\right)w_{t-j}\\
&\overset{(a)}{=} M^{[0]}y_t(K_0)+\sum_{j=1}^{t}\lambda_{t,j}K^{\star}C\left(\sum_{i=1}^{j-1}(\mathbb{A}_{K^\st}^{\alpha})^{i-1}\alpha B(K^{\star}-K_0)C(\mathbb{A}_{K_{0}}^{\alpha})^{j-i-1}\right)w_{t-j}\\
&=M^{[0]}y_t(K_0)+\sum_{j=1}^{t}\lambda_{t,j}K^{\star}C\left(\sum_{i=1}^{j-1}(\mathbb{A}_{K_{0}}^{\alpha}+\alpha B(K^{\star}-K_0)C)^{i-1}\alpha B(K^{\star}-K_0)C(\mathbb{A}_{K_{0}}^{\alpha})^{j-i-1}\right)w_{t-j}\\
%& \ \ \ \ \ + \sum_{j=1}^{t-1}\bar{\lambda}_{t,j}K^{\star}C\left[\sum_{i=1}^{j-1}(\tilde{A}+B(K^{\star}-K_0)C)^{i-1}B(K^{\star}-K_0)C\tilde{A}^{j-i-1}\right]BK_0e_{t-j}\\
%& \ \ \ \ \ + \sum_{i=1}^{t-1}\bar{\lambda}_{t,i}K^{\star}C(A+BK^{\star}C)^{i-1}B(K^{\star}-K_0)e_{t-i}\\
&\overset{(b)}{=} M^{[0]}y_t(K_0)+\sum_{j=1}^{t}\lambda_{t,j}K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{j-1}w_{t-j}-\sum_{j=1}^{t-1}\lambda_{t,j}K^{\star}C(\mathbb{A}_{K_{0}}^{\alpha})^{j-1}w_{t-j}\\
%& \ \ \ \ \ + \sum_{j=1}^{t-1}\bar{\lambda}_{t,j}K^{\star}C(A+BK^{\star}C)^{j-1}BK^{\star}e_{t-j}-\sum_{j=1}^{t-1}\bar{\lambda}_{t,j}K^{\star}C(A+BK_0C)^{j-1}BK_0e_{t-j}\\
&\overset{(c)}{=} \sum_{j=1}^{t}\lambda_{t,j}K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{j-1}w_{t-j}\\%+\sum_{j=1}^{t-1}\bar{\lambda}_{t,j}K^{\star}C(A+BK^{\star}C)^{j-1}BK^{\star}e_{t-j}+K^{\star}e_t\\
&\overset{(d)}{=}u_t(K^{\star}),
\end{align*}
where equality $(a)$ uses the definition of $M^{[i]}$ together with the fact that
\begin{align}
\label{eq:lambda-property}
\bar{\lambda}_{t,i}\lambda_{t-i, j-i}&=\left(\prod_{k=1}^i (1-\gamma_{t-k})\right)\gamma_{t-j}\cdot \prod_{k=1}^{j-i-1}(1-\gamma_{t-i-k}) \nonumber\\
&=\gamma_{t-j}\left(\prod_{k=1}^i (1-\gamma_{t-k})\right)\left(\prod_{k=i+1}^{j-1} (1-\gamma_{t-k})\right) \nonumber\\
&=\gamma_{t-j}\prod_{k=1}^{j-1}(1-\gamma_{t-k}) \nonumber\\
&=\lambda_{t,j}
\end{align}
by \cref{eq:define-lambdas}; equality $(b)$ follows from the matrix equality
\begin{align*}
X^n=Y^n+\sum_{i=1}^n X^{i-1}(X-Y)Y^{n-i},
\end{align*}
with 
\[X=(\mathbb{A}_{K_{0}}^{\alpha}+\alpha B(K^{\star}-K_0)C)=\mathbb{A}_{K^{\star}}^{\alpha}, \ \ \ Y=\mathbb{A}_{K_{0}}^{\alpha},\]
equality $(c)$ uses Observation~\ref{fact:linear-unfold} with $K_0$ together with the choice of $M\^0 = K^\star$; and equality $(d)$ uses Observation~\ref{fact:linear-unfold} with $K^\st$.

Let $p^{\star}\in\Delta^{d_x}$ be the unique stationary distribution of $\mathbb{A}_{K^{\star}}^{\alpha}$. Since $u_t(K^{\star})=\hat{u}_{t}(M;K_0)$, we have
\begin{align*}
\|u_{t}(K^{\star})-\tilde{u}_{t}(M;K_0)\|_1&\leq\left\|\sum_{i=H+1}^{t-1}\bar{\lambda}_{t,i}M^{[i]}y_{t-i}(K_0)\right\|_1\\
&=\left\|\sum_{i=H+1}^{t-1}\bar{\lambda}_{t,i}K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{i-1}B(K^{\star}-K_0)y_{t-i}(K_0)\right\|_1\\
&\le \left\|\sum_{i=H+1}^{t-1}\left(\bar{\lambda}_{t,i}K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{i-1}\left(BK^{\star}y_{t-i}(K_0)\right)-\bar{\lambda}_{t,i}K^{\star}Cp^{\star}\right)\right\|_1\\
&\qquad+ \left\|\sum_{i=H+1}^{t-1}\left(\bar{\lambda}_{t,i}K^{\star}C(\mathbb{A}_{K^{\star}}^{\alpha})^{i-1}\left(BK_0y_{t-i}(K_0)\right)-\bar{\lambda}_{t,i}K^{\star}Cp^{\star}\right)\right\|_1\\
&\leq \sum_{i=H+1}^{t-1} \bar\lambda_{t,i} \norm{K^\st C (\mathbb{A}_{K^\st}^\alpha)^{i-1}(BK^\st y_{t-i}(K_0) - p^\st)}_1 \\ 
&\qquad+ \sum_{i=H+1}^{t-1} \bar\lambda_{t,i} \norm{K^\st C (\mathbb{A}_{K^\st}^\alpha)^{i-1}(BK_0 y_{t-i}(K_0) - p^\st)}_1 \\
&\leq \sum_{i=H+1}^{t-1} \bar\lambda_{t,i} \norm{(\mathbb{A}_{K^\st}^\alpha)^{i-1}(BK^\st y_{t-i}(K_0) - p^\st)}_1 \\ 
&\qquad+ \sum_{i=H+1}^{t-1} \bar\lambda_{t,i} \norm{ (\mathbb{A}_{K^\st}^\alpha)^{i-1}(BK_0 y_{t-i}(K_0) - p^\st)}_1 \\
&\le 2\sum_{i=H+1}^{t-1} 2^{-H/\tau} \\
&\leq \frac{\eps}{512d_x^{7/2}LT^2}.
\end{align*}
where the penultimate inequality is by Lemma 18 in \citep{golowich2024online} and the final inequality is by choice of $H$. This proves \cref{eq:utilde-uk}. 

Recall that $u_t(M;K_0)$ is obtained via the operation
\begin{align*}
u_t(M;K_0)=\phi^{-1}(\frac{\phi( \tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi( \tilde{u}_t(M;K_0))}).
\end{align*}
We have that
\begin{align*}
\|u_t(M;K_0)-u_t(K^{\star})\|_1&=\left\|\phi^{-1}(\frac{\phi( \tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi( \tilde{u}_t(M;K_0))})-\phi^{-1}(\frac{\phi( u_t(K^{\star}))}{\pi_{\mathcal{K}}(\phi( u_t(K^{\star}))})\right\|_1\\
&\le 2\cdot \left\|\frac{\phi( \tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi( \tilde{u}_t(M;K_0))}-\frac{\phi( u_t(K^{\star}))}{\pi_{\mathcal{K}}(\phi( u_t(K^{\star}))}\right\|_1\\
&\le 64 d_x^{7/2}\cdot \|\tilde{u}_t(M;K_0))-u_t(K^{\star})\|_1\\
&\le \frac{\eps }{8LT^2},
\end{align*}
where the first inequality follows from that $\phi^{-1}$ is $2$-non-expanding with respect to $(\|\cdot\|_1, \|\cdot\|_1)$ (Lemma~\ref{lem:transformation}); the second inequality follows from Lemma~\ref{thm:proj-lip} by taking $\kappa=2d_x$; and that $\phi$ is $1$-non-expanding with respect to $(\|\cdot\|_1, \|\cdot\|_1)$ (Lemma~\ref{lem:transformation}); the third inequality follows from \cref{eq:utilde-uk}.

Thus, we proved \cref{eq:u-uk}, which
holds for all $t$. We next bound $\|y_t(K^{\star})-y_t(M;K_0)\|_1$.
From \cref{eq:yt-unfold-po-2} and \cref{eq:u-uk}, we have
\begin{align*}
\|y_t(K^{\star})-y_t(M;K_0)\|_1
&\le \sum_{i=1}^{t-1}\bar{\lambda}_{t,i}\left\|C[(1-\alpha)A]^{i-1}B\left[u_{t-i}(K^{\star})-u_{t-i}(M;K_0)\right]\right\|_1 \\ 
&\leq \frac{\vep}{8 LT^2} \sum_{i=1}^{t-1}\bar\lambda_{t,i} \leq \frac{\vep}{8LT}.
\end{align*}
Combining the bounds on $\|u_{t}(K^{\star})-u_t(M;K_0)\|_1$ and $\|y_t(K^{\star})-y_t(M;K_0)\|_1$ and using the fact that $c_t$ satisfies \cref{asm:po-convex-loss} for all $t$, we have that the approximation error is bounded by
\begin{align}
\label{eq:c-approx}
\left|\sum_{t=1}^T c_t(y_t(M;K_0),u_t(M;K_0))-\sum_{t=1}^Tc_t(y_t(K^{\star}),u_t(K^{\star}))\right|\le \frac{\eps}{4}.
\end{align}
\paragraph{Proof of \cref{eq:ext-approx}.} We move on to bound the difference between the pseudo loss and the cost function in \cref{eq:ext-approx}. 
\begin{align*}
& \quad \left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^T c_t(y_t(M;K_0),u_t(M;K_0))\right|\\
&\le \underbrace{\left|\sum_{t=1}^T (\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))-1) \cdot c_t^u(\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}))\right|}_{(1)} \\
& \quad +\underbrace{\left|\sum_{t=1}^T c_t^u(\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}))- \sum_{t=1}^T c_t^u(u_t(M;K_0))\right|}_{(2)}.
\end{align*}
By Lipschitz property of $\pi_{\mathcal{K}}$ (Lemma~\ref{cor:lip-control}) and non-expanding condition of $\phi$ (Lemma~\ref{lem:transformation}) and that $\phi(u_t(K^{\star}))\in\K$, we have that
\begin{align*}
|\pi_{\mathcal{K}}(\phi( \tilde{u}_t(M;K_0)))-1|&=
|\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))-\pi_{\mathcal{K}}(\phi( u_t(K^{\star})))|\\
&\le 2d_x\cdot \|\tilde{u}_t(M;K_0)-u_t(K^{\star})\|_1\\
&\le \frac{\eps }{256d_x^{5/2}LT^2},
\end{align*}
where the last inequality follows from \cref{eq:utilde-uk}. 
By the Lipschitz condition of $c_t^u$ and that $c_t^u$ attains $0$ on $\mathcal{V}$ (\cref{asm:po-convex-loss}), we have
\begin{align*}
(1)\le L \sum_{t=1}^T (\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))-1) \le \frac{\eps}{256d_x^{5/2}T}.
\end{align*}
We move on to bound $(2)$:
\begin{align*}
(2)&\le L\sum_{t=1}^T \left\|\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))})-u_t(M;K_0)\right\|_1\\
&\le 2L\sum_{t=1}^T \left\|\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}-\phi(u_t(M;K_0))\right\|_1\\
&\le 2L\sum_{t=1}^{T}  \frac{\|\phi(\tilde{u}_t(M;K_0))-\phi(u_t(M;K_0))\|_1}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))} \\
&\quad + 2L\sum_{t=1}^{T}  \left(1-\frac{1}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}\right)\cdot \|\phi(u_t(M;K_0))\|_1\\
&\le 2L\sum_{t=1}^T \|\tilde{u}_t(M;K_0)-u_t(M;K_0)\|_1+\frac{\eps}{64d_x^{5/2}T}\\
&\le \frac{\eps}{2T}+\frac{\eps}{64d_x^{5/2}T},
\end{align*}
where the first inequality follows from the Lipschitz assumption on $c_t^u$; the second inequality follows from that $\phi^{-1}$ is $2$-non-expanding w.r.t. ($\|\cdot\|_1$, $\|\cdot\|_1$) on $\K$; the second to last inequality follows from $\pi_{\mathcal{K}}\ge 1$, $\phi$ is $1$-non-expanding w.r.t. ($\|\cdot\|_1,\|\cdot\|_1$), and that $\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))\le 2$, $\|\phi(u_t(M;K_0))\|_1=1$; the last inequality follows from \cref{eq:utilde-uk} and \cref{eq:u-uk}.

Combining, we have that
\begin{align}
\label{eq:e-approx}
\left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^T c_t(y_t(M;K_0),u_t(M;K_0))\right|\le \frac{3\eps}{4}.
\end{align} 
Combining \cref{eq:c-approx} and \cref{eq:e-approx} gives the desired bound.
\end{proof}

\subsection{Bounding the Memory Mismatch Error}
\label{sec:mem-mismatch-po}
In this section we prove the analogue of Lemma 21 in \citep{golowich2024online} for the partially observable setting. %Analogous to \cref{sec:memory-mismatch}, in this section, we prove \cref{lem:po-mem-mismatch}, which allows us to show that an algorithm with bounded aggregate loss with respect to the loss functions $\ell_t$ defined on \cref{line:po-lt} of \cref{alg:gpc-po} in fact has bounded aggregate cost with respect to the cost functions $c_t$ chosen by the adversary. 
\begin{lemma} [Memory mismatch error]
\label{lem:po-mem-mismatch}
Suppose that $(c_t)_t$ satisfy \cref{asm:po-convex-loss} with Lipschitz parameter $L$. Let $\tau,\beta > 0$, and suppose that $\Ksim_\tau(\ML)$ is nonempty. Consider the execution of $\GPCPOS$ (\cref{alg:gpc-po}) on $\ML$. If the iterates $(M_t\^{0:H})_{t \in [T]}$ satisfy
 \begin{align}
   % \gpcnorm{(p_t, M_t\^{1:H}) - (p_{t+1}, M_{t+1}\^{1:H})} \leq \beta
   \max_{1 \leq t \leq T-1} \max_{i \in [H]} \oneonenorm{M_t\^i - M_{t+1}\^i} \leq \beta,
   \label{eq:ptmt-change-po}
 \end{align}
then for each $t \in [T]$, the loss function $\ell_t$ computed at time step $t$ satisfies
    \begin{align}
| c_t(y_t(M_t;K_0),u_t(M_t;K_0)) - c_t(y_t, u_t)| &\leq O\left(L d_x^{7/2} \beta\log^2(1/\beta)H\tau^2\right)\nonumber.
    \end{align}
\iffalse
Assume that \cref{alg:gpc-po} satisfies
\begin{align*}
\max_{1\le t\le T-1}\max_{0\le i\le H} \|M_{t}^{[i]}-M_{t+1}^{[i]}\|_{1\rightarrow1}\le \beta,
\end{align*}
then we have
\begin{align*}
\left|\sum_{t=1}^T \ell_t(M_t)-\sum_{t=1}^T c_t(y_t,u_t)\right|\le O(L\tau^2\beta\log^2(1/\beta)HT).
\end{align*}
\fi
\end{lemma}

\begin{proof} 
Fix $t \in [T]$. First, by \cref{eq:yt-unfold-po}, we have that for all $M\in\MM_{d_x,d_y,H}$, %\noah{instances of $y_t(M)$ should be $y_t(M; K_0)$ (and same for $u_t(M)$)?}
\begin{align}
\label{eq:yt-diff}
y_t-y_t(M;K_0) &= \sum_{i=1}^{t-1}\bar{\lambda}_{t,i}C ((1-\alpha)A)^{i-1}\alpha B(u_{t-i}-u_{t-i}(M;K_0)). 
\end{align}
We consider two cases of the mixing time $\tau_A$ of $A$. Let $\tau_A=t^{\mathrm{mix}}(A)$, and $t_0=\lceil \tau_A\log(2/\beta)\rceil$. 

\paragraph{Case 1: $\tau_A\le 4\tau$.} Let $p_{A}^{\star}$ satisfy $p_{A}^{\star}=Ap_{A}^{\star}$ be a stationary distribution of $A$, then by adding and subtracting $p_{A}^{\star}$, we have $\forall M\in\MM_{d_x,d_y,H}$,
\begin{align*}
&\left\|\sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}[(1-\alpha)A]^{i-1}\alpha B(u_{t-i}-u_{t-i}(M;K_0))\right\|_1\\
&\le \left\|\alpha \sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}[(1-\alpha)A]^{i-1}(Bu_{t-i}-p_{A}^{\star})\right\|_1+ \left\|\alpha\sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}[(1-\alpha)A]^{i-1}(Bu_{t-i}(M;K_0)-p_{A}^{\star})\right\|_1\\
&\le \alpha \sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}(1-\alpha)^{i-1}\|A^{i-1}(Bu_{t-i}-p_{A}^{\star})\|_1+ \alpha\sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}(1-\alpha)^{i-1}\|A^{i-1}(Bu_{t-i}(M;K_0)-p_{A}^{\star})\|_1\\
&\le 2 \sum_{i=t_0+1}^{t-1} \frac{1}{2^{\lfloor (i-1)/\tau_A\rfloor}}\le 2\tau_A\beta\sum_{i=0}^{\infty}\frac{1}{2^i}\le C\tau\beta\log(1/\beta).
\end{align*}
Thus, it suffices to show a bound for the first $t_0$ terms. We have that
\begin{align*}
\|\tilde{u}_{t-i}-\tilde{u}_{t-i}(M_t;K_0)\|_1&\le \|(M_{t-i}^{[0]}-M_{t}^{[0]})y_{t-i}(K_0)\|_1+\sum_{j=1}^{H}\bar{\lambda}_{t-i,j}\|(M_{t-i}^{[j]}-M_{t}^{[j]})y_{t-i-j}(K_0)\|_1\\
&\le (H+1)i\beta\le 2Hi\beta,
\end{align*}
since $y_t\in\Delta^{d_y}$, $\forall t$, and $\max_{0\le j\le H}\|M_{t-i}^{[j]}-M_{t}^{[j]}\|_{1\rightarrow 1}\le i\beta$ by the assumption. To bound the $\|u_{t-i}-u_{t-i}(M_t;K_0)\|_1$, we note that by the non-expanding condition established in Lemma~\ref{thm:proj-lip}, we have that
\begin{align*}
\|u_{t-i}-u_{t-i}(M_t;K_0)\|_1&\le 2\sqrt{d_x}\left\|u_{t-i}-u_{t-i}(M_t;K_0)\right\|_2\\
&\le C d_x^{7/2}\|\tilde{u}_{t-i}-\tilde{u}_{t-i}(M_t;K_0)\|_1\\
&\le Cd_x^{7/2}Hi\beta.
\end{align*}
Thus, we can bound $\|y_t-y_t(M_t;K_0)\|_1$ as the following: 
\begin{align*}
\|y_t-y_t(M_t;K_0)\|_1&\le \left\|\sum_{i=1}^{t_0}\bar{\lambda}_{t,i}C[(1-\alpha)A]^{i-1}\alpha B(u_{t-i}-u_{t-i}(M))\right\|_1+C\tau\beta\log(1/\beta)\\
&\le Cd_x^{7/2}\beta H t_0^2 +C\tau\beta\log(1/\beta)\\
&\le Cd_x^{7/2}\beta \log^2(1/\beta) H\tau^2
\end{align*}
for some constant $C$.


\paragraph{Case 2: $\tau_A>4\tau$.} In this case, we will give a lower bound on $\alpha$. Recall that $\tau=t^{\mathrm{mix}}(\mathbb{A}_{K^{\star}}^{\alpha})$, where  $\mathbb{A}_{K^{\star}}^{\alpha}=(1-\alpha)A+\alpha BK^{\star}C$. Suppose (for the purpose of contradiction) that $\alpha\le 1/(128\tau)$. Note that $\|A-\mathbb{A}_{K^{\star}}^{\alpha}\|_{1\rightarrow 1}=\alpha\| A -  BK^{\star}C\|_{1\rightarrow 1}\le \alpha(\|A\|_{1\rightarrow 1}+\|BK^{\star}C\|_{1\rightarrow 1})=2\alpha$. Here, we use Lemma 18 and Lemma 19 from \citep{golowich2024online}, which state the following facts: for $X\in\mathbb{S}^d$ with a unique stationary distribution $\pi$, then (1) $\forall c, t\in\mathbb{N}$, $D_X(t)\le \bar{D}_X(t)\le 2D_X(t)$ and $\bar{D}_X(ct)\le \bar{D}_X(t)^c$, and (2) if $Y\in\mathbb{S}^d$ satisfies $\|X-Y\|_{1\rightarrow 1}\le \delta$, then $\forall t\in\mathbb{N}$, there holds $D_Y(t)\le 2t\delta+2D_X(t)$, where $D_X(t)=\sup_{p\in\Delta^d}\|X^t p-\pi\|_1$ and $\bar{D}_X(t)=\sup_{p,q\in\Delta^d}\|X^t(p-q)\|_1$. These facts implies that $\bar{D}_{\mathbb{A}_{K^{\star}}^{\alpha}}(\tau)\le 2D_{\mathbb{A}_{K^{\star}}^{\alpha}}(\tau)\le 1/2$, and thus $D_{\mathbb{A}_{K^{\star}}^{\alpha}}(4\tau)\le \bar{D}_{\mathbb{A}_{K^{\star}}^{\alpha}}(4\tau)\le (\bar{D}_{\mathbb{A}_{K^{\star}}^{\alpha}}(\tau))^4\le 1/16$. Since $\|A-\mathbb{A}_{K^{\star}}^{\alpha}\|_{1\rightarrow 1}\le 2\alpha$, $D_A(4\tau)\le 2\cdot 4\tau\cdot 2\alpha+2D_{\mathbb{A}_{K^{\star}}^{\alpha}}(4\tau)\le 16\tau\alpha+1/8\le 1/4$, which means that $t^{\mathrm{mix}}(A)\le 4\tau$. Thus, this means that $\alpha > 1/(128\tau)$.

Thus, we have
\begin{align*}
\|y_t-y_t(M_t;K_0)\|_1&\le \sum_{i=1}^{t-1}(1-\alpha)^{i-1}\|u_{t-i}-u_{t-i}(M_t;K_0)\|_1\\
&\le Cd_x^{7/2}H\beta\sum_{i=1}^{t-1}(1-1/(128\tau))^{i-1}\cdot i\\
&\le Cd_x^{7/2}H\beta \tau^2.\\
\end{align*}
%\dhruv{(a minor error here, need to be a bit less lossy and not throw away the $\beta$ factor above)} 
By Lipschitz condition on $c_t$, we have
\begin{align*}
|c_t(y_t(M_t;K_0),u_t(M_t;K_0))-c_t(y_t,u_t)|&\le L(\|y_t-y_t(M_t;K_0)\|_1)\le C L d_x^{7/2} \beta\log^2(1/\beta)H\tau^2.
\end{align*}
\end{proof}

\subsection{Wrapping up the proof of Theorem~\ref{thm:po-main}}\label{sec:po-proof}

Before proving Theorem~\ref{thm:po-main}, we establish that the loss functions $\ell_t$ used in $\texttt{GPC-PO-Simplex}$ are Lipschitz with respect to $\|\cdot\|_{\Sigma}$ in Lemma~\ref{lem:po-lt-lip}, where  $\|\cdot\|_{\Sigma}$ measures the $\ell_1$-norm of $M=M^{[0:H]}$ as a flattened vector in $\mathbb{R}^{d_xd_y(H+1)}$, formally given by
\begin{align*}
\|M_t\|_{\Sigma}:=\sum_{i=0}^{H}\sum_{j\in[d_x],k\in[d_y]}|(M_t^{[i]})_{jk}|.
\end{align*}

\begin{lemma} [Lipschitzness of $e_t$]
\label{lem:po-lt-lip}
Suppose that a partially observable simplex LDS $\ML$ and $H\ge \tau>0$ are given, and $\Ksim_{\tau}(\ML)$ is nonempty. Fix $K_0\in\mathbb{S}^{d_x,d_y}$. For each $t\in[T]$, the pseudo loss function $e_t$ (as defined on Line~\ref{line:et-loss} of \cref{alg:gpc-po} is $O(Ld_x^{9/2}\tau)$-Lipschitz with respect to the norm $\|\cdot\|_{\Sigma}$ in $\MM_{d_x,d_y,H}$.  %\noah{did we say somewhere we're dropping the subscripts on $\mathcal{M}$?}
\end{lemma}

\begin{proof}
Since $e_t$ (Line~\ref{line:et-loss} of \cref{alg:gpc-po}) is a composite of many functions, we analyze the Lipschitz condition of each of its components. First, we show that the $M\mapsto y_t(M;K_0)$ and $M\mapsto \tilde{u}_t(M;K_0)$ are Lipschitz in $M$ on $\MM_{d_x,d_y,H}$.
In particular, fix $M_1,M_2\in\MM_{d_x,d_y,H}$, we will show that 
\begin{align*}
\|y_t(M_1;K_0)-y_t(M_2;K_0)\|_1&\le O(d_x^{7/2}\tau) \|M_1-M_2\|_{\Sigma}, \\ 
\|\tilde{u}_t(M_1;K_0)-\tilde{u}_t(M_2;K_0)\|_1&\le 2\sqrt{d_x} \|M_1-M_2\|_{\Sigma}.
\end{align*}
Expanding $\tilde{u_t}$, we have
\begin{align*}
\|\tilde{u}_t(M_1;K_0)-\tilde{u}_t(M_2;K_0)\|_1&\le \sqrt{d_x}\|\tilde{u}_t(M_1;K_0)-\tilde{u}_t(M_2;K_0)\|_2\\
&\le \sqrt{d_x}\left\|(M_1^{[0]}-M_2^{[0]})y_t(K_0)+\sum_{i=1}^H\bar{\lambda}_{t,i}(M_1^{[i]}-M_2^{[i]})y_{t-i}(K_0)\right\|_1\\
&\le 2\sqrt{d_x} \max_{0\le i\le H} \|M_1^{[i]}-M_2^{[i]}\|_{1\rightarrow 1} \\
&\le 2\sqrt{d_x}\|M_1-M_2\|_{\Sigma},
\end{align*}
where the second inequality follows from the fact that $\ell_2$ norm is bounded by $\ell_1$ norm. By Lemma~\ref{cor:lip-control}, we have
\begin{align*}
\|u_t(M_1;K_0)-u_t(M_2;K_0)\|_1\le O(d_x^{7/2})\|M_1-M_2\|_{\Sigma}.
\end{align*}
Expanding $y_t$, we have that
\begin{align*}
\|y_t(M_1;K_0)-y_t(M_2;K_0)\|_1&=\left\|\sum_{i=1}^{t-1}\bar{\lambda}_{t,i}C[(1-\alpha)A]^{i-1}\alpha B(u_t(M_1)-u_t(M_2))\right\|_1.
\end{align*}
We consider two cases, depending on the mixing time $\tau_A:=\tmix(A)$ of $A$. 

\paragraph{Case 1:} $\tau_A\le 4\tau$. Let the stationary distribution of $A$ be denoted $p^{\star}\in\Delta^{d_x}$. Then by Lemma 22 in \cite{golowich2024online}, $\forall i\in\mathbb{N}$ and $v\in\mathbb{R}^d$ with $\langle \mathbbm{1}, v\rangle=0$, $\|A^{i}v\|_1\le 2^{-\lfloor i/\tau_A\rfloor}\|v\|_1\le 2^{-\lfloor i/4\tau\rfloor}\|v\|_1$. Then,
\begin{align*}
\|y_t(M_1)-y_t(M_2)\|_1&\le \sum_{i=1}^{t-1} \|CA^{i-1}B(u_t(M_1)-u_t(M_2))\|_1\\
&\le \sum_{i=1}^{t-1} 2^{-\lfloor (i-1)/4\tau\rfloor}\|u_t(M_1)-u_t(M_2)\|_1\\
&\le Cd_x^{7/2}\|M_1-M_2\|_{\Sigma}\sum_{i=1}^{t-1} 2^{1-\lfloor (i-1)/4\tau\rfloor}\\
&\le Cd_x^{7/2}\tau \|M_1-M_2\|_{\Sigma}. 
\end{align*}

\paragraph{Case 2:} $\tau_A>4\tau$. As shown in the proof of Lemma~\ref{lem:po-mem-mismatch}, in this case we have $\alpha > 1/(128\tau)$, and therefore
\begin{align*}
\|y_t(M_1)-y_t(M_2)\|_1&\le\sum_{i=1}^{t-1}(1-\alpha)^{i-1}\|(u_t(M_1)-u_t(M_2))\|_1\\
&\le Cd_x^{7/2}\|M_1-M_2\|_{\Sigma}\sum_{i=1}^{t-1}(1-1/(128\tau))^{i-1}\\
&\le C d_x^{7/2}\tau\|M_1-M_2\|_{\Sigma}.
\end{align*}
Here we proved that $M\mapsto y_t(M;K_0)$ is $O(d_x^{7/2}\tau)$-Lipschitz, and $M\mapsto \tilde{u}_t(M;K_0)$ is $2\sqrt{d_x}$-Lipschitz. Immediately, $c_t^y(y_t(M;K_0))$ is $O(Ld_x^{7/2}\tau)$-Lipschitz on $\MM_{d_x,d_y,H}$. It suffices to show that
\begin{align*}
\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))) \cdot c_t^u(\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))}))
\end{align*}
is Lipschitz on $\MM_{d_x,d_y,H}$. Since $\phi$ is $1$-Lipschitz over $\mathbb{R}^{d_y+d_x}$, we have that $M\mapsto \phi(\tilde{u}_t(M;K_0))$ is $2\sqrt{d_x}$-Lipschitz. By assumption $c_t^{u}$ is $L$-Lipschitz on $\Delta^{d_x}$, and $\phi^{-1}$ is $2$-Lipschitz over $\mathcal{K}$, we have $c_t^{u}\circ\phi^{-1}$ is $O(L)$-Lipschitz on $\mathcal{K}$. 

By Lemma~\ref{cor:lip-control}, 
\begin{align*}
u\mapsto \pi_{\mathcal{K}}(u)\cdot \left(c_t^u\circ\phi^{-1} \left(\frac{u}{\pi_{\mathcal{K}}(u)}\right)\right)
\end{align*}
is $O(Ld_x^{4})$-Lipschitz over $\mathcal{K}$, making $c_t^u(u_t(M;K_0))$ $O(Ld_x^{9/2})$-Lipschitz on $\MM_{d_x,d_y,H}$.  . Combining, we have that $e_t$ is $O(Ld_x^{9/2}\tau)$-Lipschitz on $\MM_{d_x,d_y,H}$.  
\end{proof}

\begin{proof} [Proof of \cref{thm:po-main}]
We are ready to prove \cref{thm:po-main}. It is crucial to check the condition in Lemma~\ref{lem:po-mem-mismatch}. Define
\begin{align*}
\|M_t\|_{\star}:=\max_{0\le i\le H}\|M_{t}^{[i]}\|_{1\rightarrow 1}.
\end{align*}
It is easy to check that $\|M\|_{\Sigma}\ge \|M\|_{\star}$. With slight abuse of notation, let $\|M\|_2$ denote the $\ell_2$-norm of the flattened vector in $\mathbb{R}^{d_xd_y(H+1)}$ for $M\in\MM_{d_x,d_y,H}$. Let $\|\cdot\|_{\Sigma}^*$ denote the dual norm of $\|\cdot\|_{\Sigma}$. Lemma~\ref{lem:po-lt-lip} implies that
\cref{alg:gpc-po} guarantees
\begin{align*}
& \quad \|M_{t}-M_{t+1}\|_{\star}\le\|M_{t}-M_{t+1}\|_{\Sigma}\le_{(1)} \sqrt{d_xd_yH}\|M_{t}-M_{t+1}\|_2\le_{(2)} \eta \sqrt{d_xd_yH} \|\partial e_t(M_t)\|_{2}\\
&\le_{(3)}\eta \sqrt{d_xd_yH}\|\partial e_t(M_t)\|_{\Sigma}\le_{(4)} \eta (d_xd_yH)^{3/2} \|\partial e_t(M_t)\|_{\Sigma}^{*}\le_{(5)} \eta(d_xd_yH)^{3/2}Ld_x^{9/2}\tau\\
&\le \eta H^{5/2}Ld_x^{15/2}, 
\end{align*}
for some constant $C$, where $(1)$ and $(3)$ follow from $\ell_1$-norm-$\ell_2$-norm inequality that $\forall v\in\mathbb{R}^n$ $\|v\|_2\le \|v\|_1\le\sqrt{n}\|v\|_2$; $(2)$ follows from the update in Line~\ref{line:po-update} of \cref{alg:gpc-po};
$(4)$ follows from that $\|M\|_{\Sigma}\le d_xd_yH\|M\|_{\Sigma}^*$, and $(5)$ follows from Lemma~\ref{lem:po-lt-lip}, the assumption that $d_x\ge d_y$, and that $H\ge \tau$. 

Moreover, by the standard regret bound of OGD (Theorem 3.1 in \cite{hazan2016introduction}), we have
\begin{align*}
\sum_{t=1}^T e_t(M_t)-\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)\le O\left(\frac{D^2}{\eta}+(Ld_x^{9/2}\tau)^2\eta T\right), 
\end{align*}
where $D$ denotes the diameter of $\MM_{d_x,d_y,H}$ with respect to $\|\cdot\|_{\Sigma}$, which is bounded in this case by 
\begin{align*}
D=\sup_{M\in\mathcal{M}}\left\{ \sum_{i=0}^H\sum_{j\in[d_x],k\in[d_y]}|M^{[i]}_{jk}|\right\}\le 3d_yH.
\end{align*}
Take $\eta=1/(LH^{2}d_x^{4.5}\sqrt{T})$ and recall that $d_y\le d_x$, then we have
\begin{align*}
& \quad \sum_{t=1}^T c_t(y_t,u_t)-\min_{K^{\star}\in \Ksim_\tau(\ML)}\sum_{t=1}^T c_t(y_t(K^{\star}),u_t(K^{\star}))\\
&\le \sum_{t=1}^T c_t(y_t,u_t)-\sum_{t=1}^T c_t(y_t(M_t;K_0),u_t(M_t;K_0))+\sum_{t=1}^T c_t(y_t(M_t;K_0),u_t(M_t;K_0))- \sum_{t=1}^Te_t(M_t)\\
& \quad + \sum_{t=1}^Te_t(M_t)-\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)+1\\
&\le \sum_{t=1}^T e_t(M_t)-\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)+\tilde{O}\left(\eta H^{11/2}L^2d_x^{11} T\right)\\
&\le \tilde{O}\left(\frac{d_x^2H^2}{\eta}+(Ld_x^{9/2}\tau)^2\eta T+\eta H^{11/2}L^2d_x^{11} T\right)\\
&\le \tilde{O}\left(L\tau^{4}d_x^{6.5}\sqrt{T}\right),
\end{align*}
where the first inequality follows from Lemma~\ref{lem:po-approx} by taking $\eps=1$; the second inequality follows from Lemma~\ref{lem:po-mem-mismatch} by taking $\beta=\eta H^{5/2}Ld_x^{15/2}$ and since $c_t(y_t(M_t;K_0),u_t(M_t;K_0))\le e_t(M_t)$ from Observation~\ref{obs:ext}, and the third inequality follows from OGD regret guarantee. 
\end{proof}


