
\section{Introduction}

Online non-stochastic control \citep{agarwal2019online,hazan2020nonstochastic,hazan2022introduction} is an emerging framework at the intersection of online learning and control theory. By leveraging techniques from \emph{Online Convex Optimization} (OCO), it enables the design of \emph{no-regret} controllers that go beyond traditional assumptions, such as i.i.d. disturbances and time-invariant cost functions, in optimal control. In this work, we extend this framework to the control of \emph{population dynamics}, a class of \emph{Linear Dynamical Systems} (LDS) evolving on the probability simplex. These systems, which arise in epidemic modeling and evolutionary game theory, are not \emph{strongly stabilizable}—that is, there exists no state-feedback linear controller that can drive the system asymptotically to zero in the absence of disturbances. This violates a key assumption underpinning much of non-stochastic control theory. 

\cite{golowich2024online} recently studied this problem in the \emph{fully observable} setting, where the controller has access to the exact high-dimensional state (i.e., the full population distribution). However, in many practical applications, full observability is unrealistic due to measurement constraints. A more practical setting assumes that only a low-dimensional representation of the state is available—its dimensionality reflecting the number of effective measurements that can be made, as exemplified by the celebrated Kalman filter \citep{kalman1960new} and the rich literature on inverse problems. From a technical perspective, this \emph{partially observable} setting presents a fundamental challenge: the standard approach in non-stochastic control—designing controls as linear combinations of past disturbances \citep{hazan2022introduction}—becomes infeasible since past disturbances can no longer be directly computed.

This paper focuses on controlling partially observable population dynamics and introduces new techniques to handle the intricate interplay between partial observability and the simplex constraint. Our contributions include (i) constructing tailored signals as alternatives to incomputable disturbances, (ii) designing a novel convex controller parameterization that approximates control policies linear in non-oblivious observations, and (iii) introducing a convex extension surrogate loss to ensure that projections are applied only during control execution, thereby preserving the convexity in parameter updates.

\paragraph{Problem setting.} 
Let $\Delta^{d}:=\left\{p\in\BR^d: p\geq 0,\norm{p}_1=1\right\}$ be the probability simplex, and $\BS^{d_1,d_2}:=\left\{M\in\BR^{d_1 \times d_2}:Mx \in\Delta^{d_1} \textrm{ for all } x \in \Delta^{d_2}\right\}$ be the set of matrices mapping $\Delta^{d_2}$ to $\Delta^{d_1}$. Given the state-control dimension $d_x$ and measurement dimension $d_y$, we define a \emph{partially observable simplex LDS} as follows:

\begin{definition} [Partially observable simplex LDS]
\label{def:po-simplex-lds}
Let $A, B\in\mathbb{S}^{d_x}$ and $C\in\mathbb{S}^{d_y,d_x}$ be the system matrices, $x_1\in\Delta^{d_x}$ be some initial state, and $\alpha\in [0,1]$ be the fixed control power. A \emph{partially observable simplex LDS} is defined as a dynamical system such that at each time step $t\geq 1$, given the control input $u_t\in\Delta^{d_x}$, the disturbance strength $\gamma_t\in[0,1]$ and the disturbance $w_t\in\Delta^{d_x}$, its state $x_t\in\Delta^{d_x}$ and output $y_t\in\Delta^{d_y}$ evolve according to
\begin{align*}
x_{t+1}&=(1-\gamma_t)[(1-\alpha)Ax_t+\alpha Bu_t]+\gamma_tw_t, \\
y_{t+1} &= Cx_{t+1}.
\end{align*}
\end{definition}


The goal is to design controllers that, given knowledge of $A, B, C, \alpha$, and $\gamma_t$, can operate without direct access to the internal state. The input-output protocol is the following.
\begin{enumerate}
\item At the beginning of the $t$-th step, Nature picks $\gamma_t\in[0,1]$ and $w_t\in\Delta^{d_x}$ obliviously. 
\item The controller observes the measurement $y_t\in\Delta^{d_y}$ and the disturbance strength $\gamma_t\in[0,1]$,\footnote{See \citep[Appendix~B]{golowich2024online} for a justification of observing $\gamma_t$. Roughly speaking, it is often feasible to observe the total size of the population in each step, from which $\gamma_t$ can be computed.} and then picks a control $u_t\in\Delta^{d_x}$. 
\item Nature reveals a convex cost function $c_t:\Delta^{d_y}\times \Delta^{d_x}\rightarrow\BR_+$ to the controller, and the controller incurs the cost $c_t(y_t,u_t)$.
\end{enumerate}


For a given sequence of nature’s choices, let $y^\pi_t$ and $u^\pi_t$ denote the observations and controls under an arbitrary controller $\pi$. Our objective is to design a controller $\MA$ such that regardless of nature's decisions, the \emph{regret} of $\MA$ with respect to a suitable comparator class $\Pi$ satisfies 
\begin{equation*}
\breg_\MA(\Pi,T):=\sum_{t=1}^Tc_t\left(y_t^{\MA},u_t^{\MA}\right)-\min_{\pi^*\in \Pi}\sum_{t=1}^T c_t\left(y_t^{\pi^*},u_t^{\pi^*}\right)=o(T).
\end{equation*}


\paragraph{Technical challenges.} This problem presents unique challenges due to the interplay between population dynamics and partial observability:
\begin{itemize}
\item \textbf{Memory effects in non-stochastic control:} Unlike OCO, the cost function $c_t(y_t, u_t)$ depends not only on $u_t$ but on the entire history $(u_1, \dots, u_{t-1})$ through $x_t$. Many works assume strong stabilizability\footnote{There exists a
“default” controller such that $x_t$ decays to zero exponentially fast without disturbances.} to mitigate this issue, reducing the problem to an OCO-like setting with logarithmic memory. However, population dynamics are only \emph{marginally stable}: states neither explode nor decay to zero.
\item \textbf{Partial observability and convexity:} Classical non-stochastic control strategies parameterize $u_t$ as a linear combination of past disturbances, ensuring convexity. This relies on computing $w_{t-1}$ from state observations, which is impossible under partial observability.
\end{itemize}

While \cite{golowich2024online} and \cite{simchowitz2020improper} addressed these issues separately, their combination introduces new difficulties. \cite{simchowitz2020improper} proposed using counterfactual observations under zero control (termed \emph{nature’s $y$}) instead of disturbances, but in population dynamics, zero control is infeasible. This raises two key questions: (i) how to construct suitable signals under the simplex constraint and (ii) how to ensure control feasibility without violating convexity. In particular, if we directly apply the approach of \cite{simchowitz2020improper} to population dynamics, the resulting $u_t$ could easily violate the simplex constraint, and a naive projection would introduce nonconvexity to the subsequent optimization problem. 

\paragraph{Results and techniques.} 
We propose two controllers achieving the optimal $\tilde{O}(\sqrt{T})$ regret against different comparator classes. A simpler version competes with linear \emph{Markov} controllers of the form $u_t = K y_t$, while a more advanced version handles \emph{Linear Dynamical Controllers} (LDCs), a richer class with internal state tracking. Our main innovations include:
\begin{enumerate}
    \item \textbf{Convex extension surrogate loss:} We make a novel use of a convex extension surrogate loss, coupled with the \emph{Minkowski projection} \citep{mhammedi2022efficient}, to bypass the simplex constraint on the output of the controller. While similar ideas exist in OCO \citep{cutkosky2018black}, our specific use case in partially observable non-stochastic control is more challenging.
    \item \textbf{New signal construction:} Instead of counterfactual observations under zero control, we construct signals based on a constant control policy $u_t = K_0 y_t$, where $K_0$ is a stochastic matrix. This enables a new convex controller parameterization tailored to the simplex constraint, which requires new techniques beyond the existing unconstrained analysis \citep{simchowitz2020improper}.
\end{enumerate}

\paragraph{Related works.} There has been a surge of interest in online non-stochastic control since the seminal work of \citet{agarwal2019online}; see \citep{hazan2022introduction} for a recent survey. Beyond the full-information, fully observed setting originally studied by \cite{agarwal2019online}, a series of works extended this framework to settings with limited feedback, such as bandit feedback \citep{cassel2020bandit,sun2024optimal,suggala2024second,sun2024tight} and partial observations \citep{simchowitz2020improper,yan2023online}. A natural requirement of the partially observed setting is the comparator class being LDCs, as such controllers encompass the optimal $H_{\infty}$ control law which is the hallmark of classical optimal control theory \citep{bacsar2008h}. However, due to the difficulty of analyzing LDCs, technical refinements since \citep{simchowitz2020improper} have been scarce. 

Going beyond strong stabilizability, \citet{golowich2024online} recently studied the non-stochastic control of population dynamics in the fully observed setting. Readers are referred to their Section~1 and 5 for the background of such system models, as well as application-oriented case studies. 


\paragraph{Organization.} 
\cref{sec:min} introduces Minkowski projection and its applications. \cref{sec:po-prelim-and-approx} presents a controller for Markov comparators, while \cref{sec:non-markov} generalizes to LDCs. Proofs are deferred to the appendix due to space limit. 
