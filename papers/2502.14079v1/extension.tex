\section{Algorithm for non-Markov policies}
\begin{algorithm}[H]
\caption{$\NMGPCPOS$}
\label{alg:gpc-po-nonmarkov}
\begin{algorithmic}[1]
\REQUIRE Transition matrices $A,B \in \BS^{d_x}$, observation matrix $C \in \BS^{d_y,d_x}$, horizon parameter $H \in \BN$, control parameter $\alpha\in[0,1]$, step size $\eta>0$.  Let $\K$ be defined as in \cref{eq:pair-constrained-set}, and $\pi$ be the Minkowski functional associated with it. Let the transformation map $\phi$ be defined as in \cref{eq:transformation-map}. Denote as $\phi^{-1}$ the inverse of $\phi$ when restricted to $\Delta^{d_x}$.

\STATE Initialize $M_1^{[0:H]}\in \MM := \MM_{d_x,d_y,H}^{+}$ (Definition~\ref{def:learning-class-ldc}). Choose any $K_0\in\mathbb{S}^{d_x,d_y}$. \label{line:nonmarkovchoose-k0}
\STATE Observe initial observation $y_1$.
\FOR{$t=1,\cdots,T$}
    \STATE Compute $o_t := y_t-\sum_{i=1}^{t-1}\bar{\lambda}_{t,i}C[(1-\alpha)A]^{i-1}\alpha B(u_{t-i}-K_0 o_{t-i})$. \label{line:nonmarkovcompute-signal}
    \STATE Let $\tilde{u}_t=M_t(o_{t:t-H})=M_{t}^{[0]} \begin{bmatrix}
o_t\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}+\sum_{i=1}^H \bar{\lambda}_{t,i}M_{t}^{[i]} \begin{bmatrix}
o_{t-i}\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}$.
    \STATE Choose control $u_t=\phi^{-1}(\phi(\tilde{u}_t)/\pi(\phi(\tilde{u}_t))$.\label{line:nonmarkovcontrol-po}
    \STATE Receive cost $c_t(y_t,u_t)=c_t^y(y_t)+c_t^u(u_t)$, and observe $y_{t+1}$ and $\gamma_t$.
    \STATE Define pseudo loss function 
\begin{align*}
e_t(M) := c_t^y(y_t(M; K_0))+ \pi(\phi(\tilde{u}_t(M;K_0))) \cdot c_t^u(\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi(\phi(\tilde{u}_t(M;K_0))})).
\end{align*} \label{line:nonmarkovet-loss}
    \STATE Set $M_{t+1} := \Pi_{\mathcal{M}}^{\|\cdot\|_2}\left[M_t-\eta \cdot \partial e_t(M_t)\right]$, where $\partial e_t(M_t)$ is the subgradient of $e_t$ evaluated at $M_t$. \label{line:nonmarkovpo-update}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\newpage


\section{Proof of Theorem~\ref{thm:po-main-ldc-simplex}}

%iffalse begins here
\iffalse

The result established in the previous sections holds for the comparator class of linear policies in the most recent observation, i.e. the comparator we considered, $\mathcal{K}_{\tau}^{\Delta}$ given by Definition~\ref{def:po-comparator}, consists of policies parameterized by $K\in\mathbb{S}^{d_x,d_y}$ with certain mixing time assumptions that plays control $u_t=Ky_t$ at time $t$, where $y_t$ denotes the observation at time $t$. 
However, in partially observable control problems, these Markov policies may not be expressive enough to capture the $H_{\infty}$ policy in the i.i.d. noise setting. Therefore, in this section, we study the extension of our main algorithm and theoretical guarantee to the non-Markov comparator policy class, captured by the class of linear dynamical controllers.

\subsection{Simplex Linear Dynamical Controllers (simplex-LDC)}
\label{sec:simplex-ldc}

In fact, we can consider a more general comparator class.  Previously, \cite{simchowitz2020improper} studied control with partial observations with $\ell_2$ constraints. There, they noted that for the partial observation setting, a richer policy class is needed to capture the $H_{\infty}$ policy in the i.i.d. noise setting. The comparator policy class proposed in \citep{simchowitz2020improper} is the class of stabilizing linear dynamic controller (LDC). The general idea is that the control is given by a linear combination of the current observation $y_t$ and a hidden state $s_t$. The hidden state evolves according to a dynamical system with the observations $y_t$'s as inputs. In this way, LDC allows the control to incorporate all observations up to time $t$ rather than the single observation at time $t$.

\begin{definition}[LDC, \citep{simchowitz2020improper}]
\label{def:ldc-simchowitz}
Given $d_x, d_y, d_u\in\mathbb{N}$, a linear dynamical controller (LDC) is a policy parameterized by a tuple $\pi=(A_{\pi}, B_{\pi}, C_{\pi}, D_{\pi}, s_1)$, where $A_{\pi}\in\mathbb{R}^{d_x\times d_x}$, $B_{\pi}\in\mathbb{R}^{d_x\times d_{y}}$,
$C_{\pi}\in\mathbb{R}^{d_{u}\times d_x}$,
$D_{\pi}\in\mathbb{R}^{d_{u}\times d_{y}}$, $s_1\in\mathbb{R}^{d_x}$ such that given an observation sequence $\{y_t\}_{t\in\mathbb{N}}$ with $y_t\in\mathbb{R}^{d_y}$, the inner state $s_t\in\mathbb{R}^{d_x}$ and the control $u_t\in\mathbb{R}^{d_u}$ chosen by $\pi$ evolve according to the following linear dynamical system:
\begin{align*}
s_{t+1}=A_{\pi}s_t+B_{\pi}y_t, \quad u_t=C_{\pi}s_t+D_{\pi}y_t. 
\end{align*}
\end{definition}
Such construction roots in the optimal solution for LQG with partial observations \citep{bacsar2008h}. The main idea is to estimate an approximate $\tilde{x}_t$ of the inherent state $x_t$ with observations $y_1,\cdots,y_t$ via Kalman filter, then separately use the the estimated state $\tilde{x}_t$ to compute the optimal control $u_t$, as if $\tilde{x}_t$ were the true state $x_t$. The estimation evolves as 
$$\tilde{x}_{t+1}=A \tilde{x}_t+Bu_t+D(y_{t+1}-C(A \tilde{x}_t+Bu_t)),$$
where $D(y_{t+1}-C(A \tilde{x}_t+Bu_t))$ is the correction term to the original system based on the current observation $y_{t+1}$. Since $u_t$ is linear in $\tilde{x}_t$, $\tilde{x}_{t+1}$ is simply linear in $\tilde{x}_t$ and $y_{t+1}$, therefore the system described in Definition~\ref{def:ldc-simchowitz} considered by \cite{simchowitz2020improper} is expressive enough to cover the classical LQG control with i.i.d. Gaussian noise.

In the problem of controlling population dynamics, the optimal linear control policy with full observation was established by \cite{golowich2024online}. To extend to the partial observation setting, we need an analogous comparator class to LDC. To simulate $x_t$ by $s_t$ in the stochastic setting, the main difference lies in the original evolution of $x_t$:
$$
x_{t+1}=(1-\gamma_t)(Ax_t+Bu_t)+\gamma_t w_t.
$$
Here the noise $w_t$ is no longer Gaussian and zero-mean, instead it's over $\Delta_d$ with some non-zero mean $v$. A natural choice is
$$
s_{t+1}=(1-\gamma_t)(As_t+Bu_t)+\gamma_t v.
$$
Since $\gamma_t$ and $w_t$ are assumed to be independent, $E[x_t]=E[s_t]$ holds by the linearity of expectation when $s_1=x_1$, therefore such system is also expressive enough to cover optimal population control with stochastic noise. We assume $v=\frac{1}{d} \mathbf{1}_d$ which is the most natural prior, and our analysis can be generalized to for any $v\in \Delta_d$. This leads to the following definition of simplex linear dynamical controllers, which we will for simplicity refer to as simplex-LDC. 

\begin{definition}[simplex-LDC]
\label{def:po-comparator-ldc}
Given $d_x, d_y\in\mathbb{N}$, a simplex linear dynamical controller (simplex-LDC) is a LDC $\pi=(A_{\pi}, B_{\pi}, C_{\pi}, D_{\pi})$ of appropriate dimensions with the additional assumptions that
\begin{enumerate}
\item $s_1=\frac{1}{d_x}\mathbf{1}_{d_x}\in\Delta^{d_x}$.
\item $\forall s\in\Delta^{d_x}, y\in\Delta^{d_y}$, there holds that the next inner state and control evolved according to the simplex LDC are in the simplex, i.e.
\begin{align*}
s_{t+1}=(1-\gamma_t)((1-\alpha)A_{\pi}s_t+\alpha B_{\pi}y_t)+\gamma_t\cdot\frac{1}{d_x}\mathbf{1}_{d_x}\in\Delta^{d_x}, \quad u_t=C_{\pi}s_t+D_{\pi}y_t\in\Delta^{d_x}. 
\end{align*} 
\end{enumerate}
\end{definition}
Naturally, some kind of stability assumptions need to be imposed to the set of controllers. In \citep{simchowitz2020improper}, for a partially observable linear dynamical system $$\ML=(A,B,C,x_1,\{w_t\}_{t\in\mathbb{N}}, \{e_t\}_{t\in\mathbb{N}}, \{c_t\}_{t\in\mathbb{N}}),$$ 
they considered as the comparator class LDCs with $\pi$ satisfying that the aggregated matrix
\begin{align*}
A_{\pi,\mathrm{cl}}=\begin{bmatrix}
A+BD_{\pi}C & BC_{\pi}\\
B_{\pi}C & A_{\pi}
\end{bmatrix}
\end{align*}
is stable, i.e. $\rho(A_{\pi,\mathrm{cl}})<1$. When dealing with simplex, we have the states always have $\ell_1$ norm equal to $1$ and do not decay to $0$ even in the absence of noise. Therefore, we need a relaxed assumption instead on the mixing time of the aggregated transition matrix generated by the simplex linear dynamical controllers.

\begin{definition}[$\tau$-mixing simplex-LDC]
\label{def:mixing-simplex-ldc}
Given $\tau>0$ and a partially observable simplex LDS 
$$\mathcal{L}=(A, B, C, x_1, (\gamma_t)_{t\in\mathbb{N}}, (w_t)_{t\in\BN},(c_t)_{t\in\BN}, \alpha)$$ with state and observation dimensions $d_x, d_y\in\mathbb{N}$, we denote as $\Pi_{\tau}(\ML)$ the class of $\tau$-mixing simplex-LDCs as any simplex-LDC $\pi=(A_{\pi}\in\mathbb{R}^{d_x\times d_x}, B_{\pi}\in\mathbb{R}^{d_x\times d_y}, C_{\pi}\in\mathbb{R}^{d_x\times d_x}, D_{\pi}\in\mathbb{R}^{d_x\times d_y}, s_1\in\mathbb{R}^{d_x})$ satisfying that for
\begin{align}
\label{eq:ldc-transition-matrix}
A_{\pi,\mathrm{cl}}=\begin{bmatrix}
(1-\alpha)A+\alpha BD_{\pi}C & \alpha BC_{\pi} \\
 B_{\pi}C & A_{\pi},
\end{bmatrix}
\end{align}
where $A_{\pi,\mathrm{cl}}:\Delta^{d_x}\times\Delta^{d_x}\rightarrow \Delta^{d_x}\times\Delta^{d_x}$, there exists unique $x\in\Delta^{d_x}$, $s\in\Delta^{d_x}$ such that 
\begin{align*}
t^{\mathrm{mix}}(A_{\pi,\mathrm{cl}})=\min_{t\in\mathbb{N}}\left\{t: \quad \forall t'\ge t, \sup_{p\in\Delta^{d_x},q\in\Delta^{d_x}}\|A_{\pi,\mathrm{cl}}^t(p,q)-(x,s)\|_1\le \frac{1}{4}\right\}\le \tau,
\end{align*}
where $(p,q), (x,s)\in\mathbb{R}^{d_x+d_x}$ denote the concatenated vectors of $p, q$ and $x,s$, respectively.
\end{definition}


\subsection{Learning class, algorithm, and guarantees}

In order to approximate the set of $\tau$-mixing simplex-LDC as in Definition~\ref{def:mixing-simplex-ldc}, we consider a slightly modified convex learning class than the one in Definition~\ref{def:learning-class} used in the Markov setting. We begin with a formal definition of the learning class.

\begin{definition} [Learning policy class for $\tau$-mixing simplex-LDC]
\label{def:learning-class-ldc}
Let $d_x,d_y,H \in \BN$ and let $\BS_{0}^{d_x,d_y}$ be set of matrices $K\in[-1,1]^{d_x\times d_y}$ such that $\sum_{i=1}^{d_x} K_{ij} = 0$ for all $j \in [d_y]$. Then let $\MM_{d_x,d_y,H}^{+}$ be the set of tuples ${M^{[0:H]}} = (M^{[0]},\dots,M^{[H]})$ defined as follows:
\begin{align}
\label{eq:M-class-nonmarkov}
\MM_{d_x,d_y,H}^{+} := \{M^{[0:H]}\mid M^{[0]}\in\BS^{d_x,d_y}\otimes \BS^{d_x,d_x} \text{ and } M^{[i]}\in\BS_{0}^{d_x,d_y}\otimes \BS_{0}^{d_x,d_x} \text{ for all }  i \in [H]\}.
\end{align}
\end{definition}
First, we observe that $\MM_{d_x,d_y,H}^{+}$ defined in Definition~\ref{def:learning-class-ldc} is convex. We will show in Section~\ref{sec:approx-simplex-ldc} that $\MM_{d_x,d_y,H}^{+}$ approximates the class of $\tau$--mixing simplex-LDC well. Therefore, a regret bound with respect to $\MM_{d_x,d_y,H}^{+}$ translates to the regret bound with respect to the class of $\tau$--mixing simplex-LDC. 

With the learning policy class defined, we are ready to specify the controller algorithm. Algorithm~\ref{alg:gpc-po-nonmarkov} is a modified version of Algorithm~\ref{alg:gpc-po} to accommodate the new class of learning policies. Essentially, we raise the dimension of the signals to incorporate the uniform prior as described in Section~\ref{sec:simplex-ldc}. 

We can now state our main result for controlling partially observable simplex LDS, with respect to the class of $\tau$-mixing simplex-LDC.

\begin{theorem} [PO regret ($\tau$-mixing simplex-LDC)]
%\label{thm:po-main-ldc-simplex} 
Let $d_x,d_y, T\in\BN$ with $d_y\leq d_x$. Let $\ML=(A,B, C, x_1,(\gamma_t)_{t\in\BN},(w_t)_{t\in\BN},(c_t)_{t\in\BN}, \alpha)$ be a partially observable simplex LDS with cost functions $(c_t)_t$ satisfying \cref{asm:po-convex-loss} for some $L > 0$. Set $H=\lceil 2\tau\lceil \log\frac{4096\tau d_x^{7/2} L T^2}{\epsilon}\rceil \rceil$, $\eta=1/(LH^{2}d_x^{6}\sqrt{T})$. Then for any $\tau > 0$, the iterates  $(y_t, u_t)$ of $\GPCPOS$ (\cref{alg:gpc-po}) with input $(A, B, C, \tau, H,T)$ have the following policy regret guarantee with respect to $\Pi_{\tau}(\ML)$: \begin{align*}
\sum_{t=1}^T c_t(y_t,u_t)-\min_{\pi\in\Pi_{\tau}(\ML)}\sum_{t=1}^T c_t(y_t(\pi),u_t(\pi))\le \tilde{O}\left(L\tau^{4}d_x^{8}\sqrt{T}\right),
\end{align*}
where $\tilde{O}(\cdot)$ hides all universal constants and poly-logarithmic dependence in $T$. 
\end{theorem}

For any $\pi \in \Pi_{\tau}(\ML)$ and partially observable simplex LDS $\ML$, we let $(y_t(\pi))_t$ and $(u_t(\pi))_t$ denote the counterfactual sequences of observations and controls that would have been obtained by playing the policy $\pi$ from the beginning of the time. In this section, we omit the dependence on the system $\ML$, since it is always clear from context (in particular, it is the system that $\GPCPOS$ controls). 

\paragraph{Roadmap.} Similarly to the previous setting, Theorem~\ref{thm:po-main-ldc-simplex} depends on several building blocks. First, we will show that the convex learning policy class (Definition~\ref{def:learning-class-ldc}) used in Algorithm~\ref{alg:gpc-po-nonmarkov} approximates $\Pi_{\tau}(\ML)$ well. This effectively means that a regret bound with respect to $\MM_{d_x,d_y,H}^{+}$ implies a regret with respect to $\Pi_{\tau}(\ML)$. Next, we will show that the memory mismatch error is small. Finally we prove regularity properties and wrap up the proof.

% fi ends here
\fi

\subsection{Approximation of simplex-LDC}
\label{sec:approx-simplex-ldc}
In this section, we prove an approximation theorem of simplex-LDC that is analogous to Lemma~\ref{lem:po-approx}. In particular, we show that there exists a convex parametrization that approximates the class of $\tau$-mixing simplex-LDC well. We begin with Observation~\ref{obs:ldc-simplex-evolution}, which describes the system evolution following any simplex-LDC $\pi$.

\begin{observation}[System evolution using LDC-simplex control]
\label{obs:ldc-simplex-evolution}
Given $d_y,d_x,d_x\in\mathbb{N}$, $\tau>0$, a partially observable simplex LDS $\mathcal{L}=(A, B, C, x_1, (\gamma_t)_{t\in\mathbb{N}}, (w_t)_{t\in\BN},(c_t)_{t\in\BN}, \alpha)$, and a LDC-simplex $\pi=(A_{\pi}, B_{\pi}, C_{\pi}, D_{\pi})\in\Pi_{\tau, d_x}(\ML)$, the evolution of the states and observations when exerting controls according to $\pi$ can be described as the follows: 
\begin{align*}
\begin{bmatrix}
x_{t+1}(\pi) \\
s_{t+1}(\pi)
\end{bmatrix}
&=(1-\gamma_t)A_{\pi,\mathrm{cl}}\begin{bmatrix}
x_t(\pi)\\
s_t(\pi)
\end{bmatrix}+\gamma_t \begin{bmatrix}
w_t\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}, \\
\begin{bmatrix}
y_{t}(\pi) \\
u_{t}(\pi)
\end{bmatrix}
&=\begin{bmatrix}
C & 0\\
D_{\pi}C & C_{\pi}
\end{bmatrix}\begin{bmatrix}
x_t(\pi)\\
s_t(\pi)
\end{bmatrix},
\end{align*}
where $A_{\pi,\mathrm{cl}}$ is given by \cref{eq:ldc-transition-matrix}.
\end{observation} 

Based on Observation~\ref{obs:ldc-simplex-evolution}, we can unfold the controls and observations by a simplex-LDC $\pi$.

\begin{lemma}[System unfolding using simplex-LDC control]
\label{lem:ldc-simplex-unfolding} 
Consider a partially observable simplex LDS 
$$\ML=(A, B, C, x_1, (\gamma_t)_{t\in\mathbb{N}}, (w_t)_{t\in\BN},(c_t)_{t\in\BN}, \alpha)$$
with state and control dimension $d_x$ and observation dimension $d_y$. For a LDC-simplex $\pi\in\Pi_{\tau,d_x}(\ML)$ of appropriate dimension, the controls and observations $u_t(\pi)$, $y_t(\pi)$ according to $\pi$ unfold as the following expressions:
\begin{align}
\label{eq:control-unfolding}
u_t(\pi)&=\sum_{i=1}^t \lambda_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}\begin{bmatrix}
w_{t-i}\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix},
\end{align}
where we write $w_0=x_1$, $\xi_0=s_1$ for convention and $\Delta_{t,i}, C_{\pi,\mathrm{cl},u}$ are given by
\begin{align*}
C_{\pi,\mathrm{cl},u} =[D_{\pi}C \quad  C_{\pi}]\in\mathbb{R}^{d_x\times(d_x+d_x)},
\end{align*}
and $\lambda_{t,i}$ is defined as in \cref{eq:define-lambdas}.
\end{lemma}
\begin{proof}
By the first equality in Observation~\ref{obs:ldc-simplex-evolution}, we can unroll $(x_t,s_t)$ as the following:
\begin{align*}
\begin{bmatrix}
x_{t}(\pi) \\
s_{t}(\pi)
\end{bmatrix}
&=(1-\gamma_{t-1})A_{\pi,\mathrm{cl}}\begin{bmatrix}
x_{t-1}(\pi)\\
s_{t-1}(\pi)
\end{bmatrix}+\gamma_{t-1} \begin{bmatrix}
w_{t-1}\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=(1-\gamma_{t-1})(1-\gamma_{t-2})A_{\pi,\mathrm{cl}}^2\begin{bmatrix}
x_{t-2}(\pi)\\
s_{t-2}(\pi)
\end{bmatrix}+(1-\gamma_{t-1})\gamma_{t-2} A_{\pi, \mathrm{cl}}\begin{bmatrix}
w_{t-2}\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}+\gamma_{t-1} \begin{bmatrix}
w_{t-1}\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{i=1}^{t} \lambda_{t,i}A_{\pi,\mathrm{cl}}^{i-1}\begin{bmatrix}
w_{t-i}\\
\frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix},
\end{align*}
where $\lambda_{t,i}$ is defined as in \cref{eq:define-lambdas}, $w_0=x_1$. The last step follows by iteratively applying unfolding of $(x_t(\pi), s_t(\pi))$. Then by the second equality in Observation~\ref{obs:ldc-simplex-evolution}, we prove the inequality in Lemma~\ref{lem:ldc-simplex-unfolding}. 
\end{proof}

We consider the same learning policy class and policy as defined in Definition~\ref{def:learning-class} and Definition~\ref{def:learning-policy} and prove an analogous lemma to Lemma~\ref{lem:po-approx}, which states that the learning policy class in Definition~\ref{def:learning-policy} well approximates the class of LDC-simplex with finite mixing time $\Pi_{\tau, d_x}(\ML)$ in Definition~\ref{def:po-comparator-ldc}.

\begin{lemma} [Approximation of LDC-simplex, analogous to Lemma~\ref{lem:po-approx}]
\label{lem:po-approx-ldc}
Consider a partially observable LDS $\ML$ with state and control dimension $d_x$, observation dimension $d_y$. Suppose that the cost functions $c_1,\dots,c_T$ of $\ML$ satisfy \cref{asm:po-convex-loss} with Lipschitz parameter $L>0$. Fix $\tau, \eps>0$ and $d_x\in\BN$. Suppose $H$ satisfies $H\ge 2\tau\lceil \log\frac{4096\tau d_x^{7/2} L T^2}{\epsilon}\rceil$. Then, for any $\pi=(A_{\pi},B_{\pi},C_{\pi},D_{\pi})\in\Pi_{\tau, d_x}(\ML)$, there is some $M\in\MM_{d_x,d_y,d_x,H}$ such that 
\begin{align*}
\left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^T c_t(y_t(\pi), u_t(\pi))\right|\le \eps.  
\end{align*}
\end{lemma}
\begin{proof}
Fix a policy $\pi=(A_{\pi}, B_{\pi}, C_{\pi}, D_{\pi})\in\Pi_{\tau,d_x}(\ML)$. Note that for any fixed $K_0\in\mathcal{K}_{\tau}^{\Delta}(\ML)$, the LDC-simplex policy parameterized by $\pi_0=((1-\alpha)I, \alpha I, 0, K_0)$ satisfies $\pi_0\in \Pi_{\tau, d_x}(\ML)$. 
We show the following two inequalities separately.
\begin{align}
\left|\sum_{t=1}^T c_t(y_t(M;\pi_0),u_t(M;\pi_0))-\sum_{t=1}^T c_t(y_t(\pi), u_t(\pi))\right|\le \frac{\eps}{4} \label{eq:cost-approx-ldc},\\
\left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^T c_t(y_t(M;\pi_0),u_t(M;\pi_0))\right|\le \frac{3\eps}{4} \label{eq:ext-approx-ldc}.
\end{align}
\paragraph{Proof of \cref{eq:cost-approx-ldc}.} 
We start with \cref{eq:cost-approx-ldc}. For each $M$, recall that $\tilde{u}_t(M;\pi_0)$ is the (un-projected but truncated) control parameterized by $M$ using signals generated by the LDC-simplex policy $\pi_0$ before the Minkowski projection. 

Formally, we choose the following parameterization
\begin{align*}
\hat{u}_t(M;\pi_0):=M^{[0]}\begin{bmatrix}
    y_t(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} +\sum_{i=1}^t\bar{\lambda}_{t,i}M^{[i]}\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} ,
\end{align*}
and
\begin{align}
\label{eq:truncated-unprojected-control}
\tilde{u}_t(M;\pi_0):=M^{[0]}\begin{bmatrix}
    y_t(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} +\sum_{i=1}^H\bar{\lambda}_{t,i}M^{[i]}\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} ,
\end{align}
and $u_t(M;\pi_0)$ is obtained by the Minkowski projection of the tuple $(y_t(M;\pi_0), \tilde{u}_t(M;\pi_0))$. We further divide the proof of \cref{eq:cost-approx-ldc} into three steps: (1) we show the un-projected un-truncated control $\hat{u}_t(M;\pi_0)$ is equal to the comparator control $u_t(\pi)$, (2) we show the un-projected un-truncated control $\hat{u}_t(M;\pi_0)$ is close to the un-projected truncated control $\tilde{u}_t(M;\pi_0)$, (3) we show that since $u_t(\pi)$ always satisfies $u_t(\pi)\in\Delta^{d_x}$ and $\tilde{u}_t(M;\pi_0)$ is sufficiently close to $u_t(\pi)$, the projection operator does not change $\tilde{u}_t(M;\pi_0)$ much and therefore preserves the proximity of the projected $u_t(M;\pi_0)$ to $u_t(\pi)$. Then, \cref{eq:cost-approx-ldc} just follows by the regularity assumptions on the system and the Lipschitz assumption on the cost functions. 

\paragraph{Step 1: expressive power of the parameterization.} Our first step is to carefully choose $M$ and $\pi_0$, such that 
$$\exists \pi_0, \forall \pi, \exists M, \forall t, \hat{u}_t(M;\pi_0)=u_t(\pi).$$
In particular, we choose $\pi_0$ to be the linear policy $\begin{bmatrix}
\left(\mathbb{A}_{K_0}^{\alpha}\right)^{j-1} & 0_{d_x\times d_x}\\
0_{d_x\times d_x} & 0_{d_x\times d_x}
\end{bmatrix}$ where $\mathbb{A}_{K_0}^{\alpha}=(1-\alpha)A+\alpha B K_0 C$, and $M$ to be
$$
M^{[0]}=\begin{bmatrix}
D_{\pi} & C_{\pi}
\end{bmatrix}, \quad M^{[i]}=C_{\pi,cl,u} A_{\pi,cl}^{i-1} P_{\pi,\pi_0}, \quad \forall 1\le i \le H,
$$
where $P_{\pi,\pi_0} =\begin{bmatrix}
\alpha B(D_{\pi}-K_0) & \alpha B C_{\pi}\\
B_{\pi} & A_{\pi}-I
\end{bmatrix}$. Recall that from Lemma~\ref{lem:ldc-simplex-unfolding}, the target control $u_t(\pi)$ can be written as the following:
\begin{align*}
u_t(\pi)=\sum_{i=1}^t \lambda_{t,i} C_{\pi,cl,u} (A_{\pi,cl})^{i-1} \begin{bmatrix}
    w_{t-i} \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix},
\end{align*}
and we would like to show $\hat{u}_t(M;\pi_0)$ is exactly equal to it.

To this end, we analyze the two terms of $\hat{u}_t(M;\pi_0)$ separately. First, since $\pi_0$ is effectively a Markov policy, we can unfold the signals $y_t(\pi_0)$ generated by $\pi_0$ similarly as in the Markov comparator class setting, i.e.
\begin{align*}
y_t(\pi_0)=\sum_{i=1}^t\lambda_{t,i}C(\mathbb{A}_{K_0}^{\alpha})^{i-1}w_{t-i}, \quad \mathbb{A}_{K_0}^{\alpha}=(1-\alpha)A+\alpha BK_0C.
\end{align*}
Using the unfolding of the signals, we can expand the two terms of $\hat{u}_t(M;\pi_0)$ as the following:
\begin{align*}
    M^{[0]}\begin{bmatrix}
    y_t(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}&=\begin{bmatrix}
    D_{\pi} & C_{\pi}
\end{bmatrix} \begin{bmatrix}
    \sum_{i=1}^t \lambda_{t,i}C (\mathbb{A}_{K_0}^{\alpha})^{i-1} w_{t-i} \\ \sum_{i=1}^t \lambda_{t,i} \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{i=1}^t \lambda_{t,i} \begin{bmatrix}
    D_{\pi} & C_{\pi}
\end{bmatrix} \begin{bmatrix}
    C & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix} \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{i-1} \begin{bmatrix}
    w_{t-i} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{i=1}^t \lambda_{t,i} \begin{bmatrix}
    D_{\pi} C & C_{\pi}
\end{bmatrix} \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{i-1} \begin{bmatrix}
    w_{t-i} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{i=1}^t \lambda_{t,i} C_{\pi,cl,u} \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{i-1} \begin{bmatrix}
    w_{t-i} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix},
\end{align*}
where in the first equality we used the identity that $\forall t\ge 1$, $\sum_{i=1}^{t}\lambda_{t,i}=1$.
For the second part, we have that
\begin{align*}    
& \quad \sum_{i=1}^t\bar{\lambda}_{t,i}M^{[i]}\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&= \sum_{i=1}^t \bar{\lambda}_{t,i} C_{\pi,cl,u} A_{\pi,cl}^{i-1} P_{\pi,\pi_0} \begin{bmatrix}
    \sum_{j=1}^{t-i} \lambda_{t-i,j}C (\mathbb{A}_{K_0}^{\alpha})^{j-1} w_{t-i-j} \\ \sum_{j=1}^{t-i} \lambda_{t-i,j} \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{i=1}^t \sum_{j=i+1}^t \bar{\lambda}_{t,i} \lambda_{t-i,j-i} C_{\pi,cl,u} A_{\pi,cl}^{i-1} P_{\pi,\pi_0} \begin{bmatrix}
    C & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix} \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{j-i-1} \begin{bmatrix}
    w_{t-j} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&= \sum_{i=1}^t \sum_{j=i+1}^t \lambda_{t,j} C_{\pi,cl,u} A_{\pi,cl}^{i-1} P_{\pi,\pi_0} \begin{bmatrix}
    C & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix} \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{j-i-1} \begin{bmatrix}
    w_{t-j} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{j=2}^t \sum_{i=1}^{j-1} \lambda_{t,j} C_{\pi,cl,u} A_{\pi,cl}^{i-1} P_{\pi,\pi_0} \begin{bmatrix}
    C & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix} \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{j-i-1} \begin{bmatrix}
    w_{t-j} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix},
\end{align*}
where the second to the last equality follows from that $\bar{\lambda}_{t,i}\lambda_{t-i, j-i}=\lambda_{t,j}$ as shown in \cref{eq:lambda-property}. Here we recall that
$$
A_{\pi,cl}=\begin{bmatrix}
    (1-\alpha)A+\alpha BD_{\pi}C & \alpha B C_{\pi} \\ B_{\pi} C & A_{\pi}
\end{bmatrix}, \quad P_{\pi,\pi_0} \begin{bmatrix}
    C & 0 \\ 0 & I
\end{bmatrix}= \begin{bmatrix}
    \alpha B(D_{\pi} K_0)C & \alpha B C_{\pi} \\ B_{\pi} C & A_{\pi} - I_{d_x \times d_x}
\end{bmatrix},
$$
which yields the nice property
$$
A_{\pi,cl}-\begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}=P_{\pi,\pi_0} \begin{bmatrix}
    C & 0 \\ 0 & I_{d_x \times d_x}
\end{bmatrix}.
$$
Now, we can continue rewriting the previous equation
\begin{align*}
    & \quad \sum_{i=1}^t\bar{\lambda}_{t,i}M^{[i]}\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&= \sum_{j=2}^t \sum_{i=1}^{j-1} \lambda_{t,j} C_{\pi,cl,u} A_{\pi,cl}^{i-1} P_{\pi,\pi_0} \begin{bmatrix}
    C & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix} \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{j-i-1} \begin{bmatrix}
    w_{t-j} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{j=2}^t \lambda_{t,j} C_{\pi,cl,u} \left( \sum_{i=1}^{j-1} A_{\pi,cl}^{i-1} \left(A_{\pi,cl}-\begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}\right) \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{j-i-1} \right)\begin{bmatrix}
    w_{t-j} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&= \sum_{j=2}^t \lambda_{t,j} C_{\pi,cl,u} \left( A_{\pi,cl}^{j-1} - \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{j-1} \right)\begin{bmatrix}
    w_{t-j} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\\
&=\sum_{j=1}^t \lambda_{t,j} C_{\pi,cl,u} \left( A_{\pi,cl}^{j-1} - \begin{bmatrix}
    \mathbb{A}_{K_0}^{\alpha} & 0 \\ 0 & I_{d_x\times d_x}
\end{bmatrix}^{j-1} \right)\begin{bmatrix}
    w_{t-j} \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}.
\end{align*}
Combining the two parts, we have the desired equation
$$
\hat{u}_t(M;\pi_0):=M^{[0]}\begin{bmatrix}
    y_t(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} +\sum_{i=1}^t\bar{\lambda}_{t,i}M^{[i]}\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} = \sum_{i=1}^t \lambda_{t,i} C_{\pi,cl,u} (A_{\pi,cl})^{i-1} \begin{bmatrix}
    w_{t-i} \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} =u_t(\pi).
$$

\paragraph{Step 2: low truncation error.}
The next step is to show that with the mixing time assumption on $A_{\pi,\mathrm{cl}}$ in Definition~\ref{def:mixing-simplex-ldc}, the truncation loss $\|\hat{u}_t(M;\pi_0)-\tilde{u}_t(M;\pi_0)\|_2$ is small, where
\begin{align*}
\tilde{u}_t(M;\pi_0):=M^{[0]}\begin{bmatrix}
    y_t(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} +\sum_{i=1}^H\bar{\lambda}_{t,i}M^{[i]}\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} 
\end{align*}
is the truncated but unprojected control parameterized by $M$ (\cref{eq:truncated-unprojected-control}) operating on the Markov signals $y_t(\pi_0)$.

By definition, we unfold $\hat{u}_t(M;\pi_0)$ and $\tilde{u}_t(M;\pi_0)$ and get
\begin{align*}
\|\hat{u}_t(M;\pi_0)-\tilde{u}_t(M;\pi_0)\|&=\left\|\sum_{i=H+1}^t\bar{\lambda}_{t,i}M^{[i]}\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix} \right\|\\
&=\left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}\begin{bmatrix}
 \alpha B (D_{\pi}-K_0) & \alpha B C_{\pi} \\
    B_{\pi} & A_{\pi}-I
\end{bmatrix}
\begin{bmatrix}
    y_{t-i}(\pi_0) \\
    \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\right\|.
\end{align*}
We have the following equivalent form for any $v_y\in \Delta^{d_y}, v_x\in \Delta^{d_x}$:
\begin{align*}
\begin{bmatrix}
 \alpha B (D_{\pi}-K_0) & \alpha B C_{\pi} \\
    B_{\pi} & A_{\pi}-I
\end{bmatrix}
\begin{bmatrix}
    v_y \\
    v_x
\end{bmatrix}&=\left(\begin{bmatrix}
    B(D_{\pi}v_y+C_{\pi}v_x) \\ B_{\pi}v_y+A_{\pi}v_x
\end{bmatrix}-\begin{bmatrix}
    K_0 v_y \\ v_x
\end{bmatrix}\right) \\
& \quad + (1-\alpha)\left(\begin{bmatrix}
    B(D_{\pi}v_y+C_{\pi}v_x) \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}-\begin{bmatrix}
    K_0 v_y \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}\right).
\end{align*}
Rewrite as following for simplicity of notation:
\begin{align*}
v_1=\begin{bmatrix}
    B(D_{\pi}v_y+C_{\pi}v_x) \\ B_{\pi}v_y+A_{\pi}v_x
\end{bmatrix}, \quad v_2=\begin{bmatrix}
    K_0 v_y \\ v_x
\end{bmatrix}\\
v_3=\begin{bmatrix}
    B(D_{\pi}v_y+C_{\pi}v_x) \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}, \quad v_4 = \begin{bmatrix}
    K_0 v_y \\ \frac{1}{d_x}\mathbf{1}_{d_x}
\end{bmatrix}.
\end{align*}
Note that $v_1, v_2, v_3, v_4\in\Delta^{d_x}\times \Delta^{d_x}$. Recall the mixing-time assumption on $A_{\pi,\mathrm{cl}}$ as in Definition~\ref{def:mixing-simplex-ldc}, denote $x^{\star}, s^{\star}\in\Delta^{d_x}$ as the unique pair of vectors such that 
\begin{align*}
t^{\mathrm{mix}}(A_{\pi,\mathrm{cl}})=\min_{t\in\mathbb{N}}\left\{t: \quad \forall t'\ge t, \sup_{p\in\Delta^{d_x},q\in\Delta^{d_x}}\|A_{\pi,\mathrm{cl}}^t(p,q)-(x^{\star},s^{\star})\|_1\le \frac{1}{4}\right\}\le \tau.
\end{align*}
For simplicity, write $p^{\star}=(x^{\star}, s^{\star})\in\mathbb{R}^{2d_x}$ to be the concatenated vector.
Then, we can further bound $\|\hat{u}_t(M;\pi_0)-\tilde{u}_t(M;\pi_0)\|$ by
\begin{align*}
&\quad \|\hat{u}_t(M;\pi_0)-\tilde{u}_t(M;\pi_0)\|\\
&=\left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}[(v_1-v_2)+(1-\alpha)(v_3-v_4)]\right\|\\
&\le \left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}(v_1-v_2)\right\| + (1-\alpha)\left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}(v_3-v_4)\right\|\\
&\le \left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}(v_1-p^{\star})\right\| + \left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}(v_2-p^{\star})\right\|\\
&\quad + (1-\alpha)\left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}(v_3-p^{\star})\right\|+ (1-\alpha)\left\|\sum_{i=H+1}^t \bar{\lambda}_{t,i}C_{\pi,\mathrm{cl},u}A_{\pi,\mathrm{cl}}^{i-1}(v_4-p^{\star})\right\|.
\end{align*}
By the mixing-time assumption, choice of $H\ge 2\tau\lceil \log\frac{4096\tau d_x^{7/2} L T^2}{\epsilon}\rceil$ and Lemma 18 in \citep{golowich2024online}, we can further bound 
\begin{align*}
\|\hat{u}_t(M;\pi_0)-\tilde{u}_t(M;\pi_0)\|&\le 4 \sum_{i=H}^{\infty}2^{-i/\tau}\le \frac{\eps}{512d_x^{7/2}LT^2}
\end{align*}
Since in the previous step we have shown that $\hat{u}_t(M;\pi_0)=u_t(\pi)$, the above bound directly translates to
\begin{align*}
\|\tilde{u}_t(M;\pi_0)-u_t(\pi)\|&\le 4 \sum_{i=H}^{\infty}2^{-i/\tau}\le \frac{\eps}{512d_x^{7/2}LT^2}
\end{align*}

\paragraph{Step 3: low projection error.} By the previous step and the Lipschitz assumptions of the operators $\phi$ and $\pi$, we have that since $u_t(\pi)\in\Delta^{d_x}$,
\begin{align*}
\|u_t(M;\pi_0)-u_t(\pi)\|_1&=\left\|\phi^{-1}(\frac{\phi( \tilde{u}_t(M;\pi_0))}{\pi(\phi( \tilde{u}_t(M;\pi_0))})-\phi^{-1}(\frac{\phi( u_t(\pi))}{\pi(\phi( u_t(\pi))})\right\|_1\\
&\le 2\cdot \left\|\frac{\phi( \tilde{u}_t(M;\pi_0))}{\pi(\phi( \tilde{u}_t(M;\pi_0))}-\frac{\phi( u_t(\pi))}{\pi(\phi( u_t(\pi))}\right\|_1\\
&\le 64 d_x^{7/2}\cdot \|\tilde{u}_t(M;\pi_0)-u_t(\pi)\|_1\\
&\le 64 d_x^{7/2}\cdot \frac{\eps}{512d_x^{7/2}LT^2}\\
&= \frac{\eps}{8LT^2}.
\end{align*}
Note that the observation $y_t(M;\pi_0), y_t(\pi)$ can unfold as follows as in \cref{eq:yt-unfold-po} and thus
\begin{align*}
\|y_t(M;\pi_0)-y_t(\pi)\|_1&=\left\|\sum_{i=1}^{t-1}\bar{\lambda}_{t,i}C((1-\alpha)A)^{i-1}\alpha B(u_{t-i}(M;\pi_0)-u_{t-i}(\pi))\right\|_1\\
&\le \frac{\eps}{8LT^2} \sum_{i=1}^{t-1}\bar{\lambda}_{t,i} \le  \frac{\eps}{8LT^2} \cdot T. 
\end{align*}
By the Lipschitz assumption on the cost function $c_t$ in Assumption~\ref{asm:po-convex-loss}, we have that
\begin{align*}
&\quad \left|\sum_{t=1}^T c_t(y_t(M;\pi_0),u_t(M;\pi_0))-\sum_{t=1}^T c_t(y_t(\pi), u_t(\pi))\right|\\
&\le \sum_{t=1}^T |c_t^y(y_t(M;\pi_0))-c_t^y(y_t(\pi))|+\sum_{t=1}^T |c_t^u(u_t(M;\pi_0))-c_t^u(u_t(\pi))|\\
&\le L\cdot \left(\sum_{t=1}^T \|y_t(M;\pi_0)-y_t(\pi)\|_1+\sum_{t=1}^T \|u_t(M;\pi_0)-u_t(\pi)\|_1\right)\\
&\le 2LT \cdot \frac{\eps}{8LT}\\
&= \frac{\eps}{4}.
\end{align*}
Thus, we have proved \cref{eq:cost-approx-ldc}. We move on to prove \cref{eq:ext-approx-ldc}. 

\paragraph{Proof of \cref{eq:ext-approx-ldc}.} The proof follows similarly to the proof of \cref{eq:ext-approx}. By the definition of $e_t(M)$ in \cref{eq:et-def}, we have
\begin{align*}
&\quad \left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^T c_t(y_t(M;\pi_0),u_t(M;\pi_0))\right|\\
&=\left|\sum_{t=1}^T \pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))\cdot c_t^u\left(\phi^{-1}\left(\frac{\phi(\tilde{u}_t(M;\pi_0)}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}\right)\right)-c_t^u(u_t(M;\pi_0))\right|\\
&\le \left|\sum_{t=1}^T (\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))-1)\cdot c_t^u\left(\phi^{-1}\left(\frac{\phi(\tilde{u}_t(M;\pi_0)}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}\right)\right)\right|\\
&\quad +\left|\sum_{t=1}^T c_t^u\left(\phi^{-1}\left(\frac{\phi(\tilde{u}_t(M;\pi_0)}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}\right)\right)-c_t^u(u_t(M;\pi_0))\right|.
\end{align*}
To bound the first term, note that
\begin{align*}
|\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))-1|&=|\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))-\pi_{\mathcal{K}}(\phi(u_t(\pi)))|\\
&\le 2d_x\cdot \|\tilde{u}_t(M;\pi_0)-u_t(\pi)\|_1\\
&\le \frac{\eps}{256d_x^{5/2}LT^2},
\end{align*}
and therefore by Lipschitz assumption of $c_t$,
\begin{align*}
\left|\sum_{t=1}^T (\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))-1)\cdot c_t^u\left(\phi^{-1}\left(\frac{\phi(\tilde{u}_t(M;\pi_0)}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}\right)\right)\right|&\le \frac{\eps}{256d_x^{5/2}T}.
\end{align*}
To bound the second term, we have
\begin{align*}
& \quad \left|\sum_{t=1}^T c_t^u\left(\phi^{-1}\left(\frac{\phi(\tilde{u}_t(M;\pi_0)}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}\right)\right)-c_t^u(u_t(M;\pi_0))\right|\\
&\le L \sum_{t=1}^T \left\|\phi^{-1}\left(\frac{\phi(\tilde{u}_t(M;\pi_0)}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}\right)-u_t(M;\pi_0)\right\|_1\\
&\le 2L\sum_{t=1}^T \left\|\frac{\phi(\tilde{u}_t(M;\pi_0)}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0)))}-\phi(u_t(M;\pi_0))\right\|_1\\
&\le 2L\sum_{t=1}^{T}  \frac{\|\phi(\tilde{u}_t(M;\pi_0))-\phi(u_t(M;\pi_0))\|_1}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))} \\
&\quad + 2L\sum_{t=1}^{T}  \left(1-\frac{1}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))}\right)\cdot \|\phi(u_t(M;\pi_0))\|_1\\
&\le 2L\sum_{t=1}^T\|\tilde{u}_t(M;\pi_0)-u_t(M;\pi_0)\|_1+2L\sum_{t=1}^T (\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;\pi_0)))-1)\\
&\le 2LT\left(\frac{\eps}{8LT^2}+\frac{\eps}{512d_x^{7/2}LT^2}+\frac{\eps}{256d_x^{5/2}LT^2}\right)\le \frac{\eps}{2T}.
\end{align*}
Combining, we have that for all $T\ge 1$,
\begin{align*}
\left|\sum_{t=1}^T e_t(M)-\sum_{t=1}^T c_t(y_t(M;\pi_0),u_t(M;\pi_0))\right|\le \frac{3\eps}{4},
\end{align*}
and conclude the proof. 
\end{proof}

\subsection{Bounding the memory mismatch error}
\label{sec:mem-mismatch-po-nonmarkov}
\begin{lemma} [Memory mismatch error, non-Markov Policy]
\label{lem:nonmarkovpo-mem-mismatch}
Suppose that $(c_t)_t$ satisfy \cref{asm:po-convex-loss} with Lipschitz parameter $L$. Let $\tau,\beta > 0$, and suppose that $\Pi_{\tau}(\ML)$ is nonempty. Consider the execution of $\NMGPCPOS$ (\cref{alg:gpc-po-nonmarkov}) on $\ML$. If the iterates $(M_t\^{0:H})_{t \in [T]}$ satisfy
 \begin{align}
   % \gpcnorm{(p_t, M_t\^{1:H}) - (p_{t+1}, M_{t+1}\^{1:H})} \leq \beta
   \max_{1 \leq t \leq T-1} \max_{i \in [H]} \oneonenorm{M_t\^i - M_{t+1}\^i} \leq \beta,
   \label{eq:ptmt-change-po-nonmarkov}
 \end{align}
then for each $t \in [T]$, the loss function $\ell_t$ computed at time step $t$ satisfies
    \begin{align}
| c_t(y_t(M_t;K_0),u_t(M_t;K_0)) - c_t(y_t, u_t)| &\leq O\left(L d_x^{11/2} \beta\log^2(1/\beta)H\tau^2\right)\nonumber.
    \end{align}
\iffalse
Assume that \cref{alg:gpc-po} satisfies
\begin{align*}
\max_{1\le t\le T-1}\max_{0\le i\le H} \|M_{t}^{[i]}-M_{t+1}^{[i]}\|_{1\rightarrow1}\le \beta,
\end{align*}
then we have
\begin{align*}
\left|\sum_{t=1}^T \ell_t(M_t)-\sum_{t=1}^T c_t(y_t,u_t)\right|\le O(L\tau^2\beta\log^2(1/\beta)HT).
\end{align*}
\fi
\end{lemma}

\begin{proof} 
Fix $t \in [T]$. First, by \cref{eq:yt-unfold-po}, we have that for all $M\in\MM_{d_x,d_y,H}^{+}$, %\noah{instances of $y_t(M)$ should be $y_t(M; K_0)$ (and same for $u_t(M)$)?}
\begin{align}
\label{eq:yt-diff-nonmarkov}
y_t-y_t(M;K_0) &= \sum_{i=1}^{t-1}\bar{\lambda}_{t,i}C ((1-\alpha)A)^{i-1}\alpha B(u_{t-i}-u_{t-i}(M;K_0)). 
\end{align}
We consider two cases of the mixing time $\tau_A$ of $A$. Let $\tau_A=t^{\mathrm{mix}}(A)$, and $t_0=\lceil \tau_A\log(2/\beta)\rceil$. 

\paragraph{Case 1: $\tau_A\le 4\tau$.} Let $p_{A}^{\star}$ satisfy $p_{A}^{\star}=Ap_{A}^{\star}$ be a stationary distribution of $A$, then by adding and subtracting $p_{A}^{\star}$, we have $\forall M\in\MM_{d_x,d_y,H}^{+}$,
\begin{align*}
&\left\|\sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}[(1-\alpha)A]^{i-1}\alpha B(u_{t-i}-u_{t-i}(M;K_0))\right\|_1\\
&\le \left\|\alpha \sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}[(1-\alpha)A]^{i-1}(Bu_{t-i}-p_{A}^{\star})\right\|_1+ \left\|\alpha\sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}[(1-\alpha)A]^{i-1}(Bu_{t-i}(M;K_0)-p_{A}^{\star})\right\|_1\\
&\le \alpha \sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}(1-\alpha)^{i-1}\|A^{i-1}(Bu_{t-i}-p_{A}^{\star})\|_1+ \alpha\sum_{i=t_0+1}^{t-1}\bar{\lambda}_{t,i}(1-\alpha)^{i-1}\|A^{i-1}(Bu_{t-i}(M;K_0)-p_{A}^{\star})\|_1\\
&\le 2 \sum_{i=t_0+1}^{t-1} \frac{1}{2^{\lfloor (i-1)/\tau_A\rfloor}}\le 2\tau_A\beta\sum_{i=0}^{\infty}\frac{1}{2^i}\le C\tau\beta\log(1/\beta).
\end{align*}
Thus, it suffices to show a bound for the first $t_0$ terms. We have that
\begin{align*}
\|\tilde{u}_{t-i}-\tilde{u}_{t-i}(M_t;K_0)\|_1&\le \|(M_{t-i}^{[0]}-M_{t}^{[0]})y_{t-i}(K_0)\|_1+\sum_{j=1}^{H}\bar{\lambda}_{t-i,j}\|(M_{t-i}^{[j]}-M_{t}^{[j]})y_{t-i-j}(K_0)\|_1\\
&\le (H+1)i\beta\le 2Hi\beta,
\end{align*}
since $y_t\in\Delta^{d_y}$, $\forall t$, and $\max_{0\le j\le H}\|M_{t-i}^{[j]}-M_{t}^{[j]}\|_{1\rightarrow 1}\le i\beta$ by the assumption. To bound the $\|u_{t-i}-u_{t-i}(M_t;K_0)\|_1$, we note that by the non-expanding condition established in Lemma~\ref{thm:proj-lip}, we have that
\begin{align*}
\|u_{t-i}-u_{t-i}(M_t;K_0)\|_1&\le 2\sqrt{d_x}\left\|u_{t-i}-u_{t-i}(M_t;K_0)\right\|_2\\
&\le C d_x^{7/2}\|\tilde{u}_{t-i}-\tilde{u}_{t-i}(M_t;K_0)\|_1\\
&\le Cd_x^{7/2}Hi\beta.
\end{align*}
Thus, we can bound $\|y_t-y_t(M_t;K_0)\|_1$ as the following: 
\begin{align*}
\|y_t-y_t(M_t;K_0)\|_1&\le \left\|\sum_{i=1}^{t_0}\bar{\lambda}_{t,i}C[(1-\alpha)A]^{i-1}\alpha B(u_{t-i}-u_{t-i}(M))\right\|_1+C\tau\beta\log(1/\beta)\\
&\le Cd_x^{7/2}\beta H t_0^2 +C\tau\beta\log(1/\beta)\\
&\le Cd_x^{7/2}\beta \log^2(1/\beta) H\tau^2
\end{align*}
for some constant $C$.


\paragraph{Case 2: $\tau_A>4\tau$.} 

Similarly, we will give a lower bound on $\alpha$. Recall that 
$$\tau=t^{\mathrm{mix}}(A_{\pi,\mathrm{cl}})=\min_{t\in\mathbb{N}}\left\{t: \quad \forall t'\ge t, \sup_{p\in\Delta^{d_x},q\in\Delta^{d_x}}\|A_{\pi,\mathrm{cl}}^t(p,q)-(x,s)\|_1\le \frac{1}{4}\right\},$$
where $A_{\pi,\mathrm{cl}}=\begin{bmatrix}
(1-\alpha)A+\alpha BD_{\pi}C & \alpha BC_{\pi} \\
 B_{\pi}C & A_{\pi}
\end{bmatrix}$. Suppose (for the purpose of contradiction) that $\alpha\le 1/(256d_x\tau)$. Denote 
$A_{compare}=\begin{bmatrix}
A & 0 \\
 B_{\pi}C & A_{\pi}
\end{bmatrix}$. We have that
$$
A_{compare}-A_{\pi,\mathrm{cl}}=\begin{bmatrix}
-\alpha(BD_{\pi}C -A) & \alpha B C_{\pi} \\
 0 & 0
\end{bmatrix}.
$$
For any $v_1,v_2\in \Delta^{d_x}$, we have that
\begin{align*}
 A_{compare}^t \begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}-   A_{\pi,\mathrm{cl}}^t \begin{bmatrix}
v_1 \\
v_2
\end{bmatrix} &=\sum_{i=1}^t A_{\pi,\mathrm{cl}}^{t-i}(A_{compare}-A_{\pi,\mathrm{cl}}) A_{compare}^{i-1} \begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}\\
&=\alpha\sum_{i=1}^t A_{\pi,\mathrm{cl}}^{t-i} \begin{bmatrix}
A-BD_{\pi}C & B C_{\pi} \\
 0 & 0
\end{bmatrix}A_{compare}^{i-1} \begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}.
\end{align*}
Notice that $A_{compare}^{i-1}$ is always of form $\begin{bmatrix}
A & 0 \\
 B^* & C^*
\end{bmatrix}$ for $i>1$, where $B^*+C^*\in \BS^{d_x,d_x}$ and both $B^*, C^*$ are non-negative, meaning that $A_{compare}^{i-1}$ always maps $\Delta^{d_x} \times \Delta^{d_x}$ to itself. Therefore $A_{compare}^{i-1} \begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}\in \Delta^{d_x} \times \Delta^{d_x}$ for any $i\ge 1$.

The matrix $\begin{bmatrix}
A-BD_{\pi}C & B C_{\pi} \\
 0 & 0
\end{bmatrix}$ always maps $\Delta^{d_x} \times \Delta^{d_x}$ to $\Delta^{d_x} \times \Delta^{d_x}-\Delta^{d_x} \times \Delta^{d_x}$. Because $A_{\pi,\mathrm{cl}}$ also maps $\Delta^{d_x} \times \Delta^{d_x}$ to itself, we have that $A_{\pi,\mathrm{cl}}^{t-i}$ maps $\Delta^{d_x} \times \Delta^{d_x}-\Delta^{d_x} \times \Delta^{d_x}$ to itself. Combining these three steps, we have that
$$
\| A_{compare}^t \begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}-   A_{\pi,\mathrm{cl}}^t \begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}\|_1\le 2\alpha d_x t.
$$
Because of our mixing time assumption on $A_{\pi,\mathrm{cl}}$, we have that for the same $x,s$,
$$
\sup_{p\in\Delta^{d_x},q\in\Delta^{d_x}}\|A_{compare}^{4\tau}(p,q)-(x,s)\|_1\le \frac{1}{16}+\frac{1}{16}\le \frac{1}{4}.
$$
As a result, $t^{\text{mix}}(A_{compare})\le 4\tau$. However, $\tau_A\le t^{\text{mix}}(A_{compare})$ by definition since the first half of $A_{compare}^{t}(p,q)$ is exactly $A^t p$ and therefore $\|A^{4\tau}p-x\|_1\le 1/4$, which leads to a contradiction.



Thus, we have
\begin{align*}
\|y_t-y_t(M_t;K_0)\|_1&\le \sum_{i=1}^{t-1}(1-\alpha)^{i-1}\|u_{t-i}-u_{t-i}(M_t;K_0)\|_1\\
&\le Cd_x^{7/2}H\beta\sum_{i=1}^{t-1}(1-\alpha)^{i-1}\cdot i\\
&\le Cd_x^{11/2}H\beta \tau^2.\\
\end{align*}
%\dhruv{(a minor error here, need to be a bit less lossy and not throw away the $\beta$ factor above)} 
By Lipschitz condition on $c_t$, we have
\begin{align*}
|c_t(y_t(M_t;K_0),u_t(M_t;K_0))-c_t(y_t,u_t)|&\le L(\|y_t-y_t(M_t;K_0)\|_1)\le C L d_x^{11/2} \beta\log^2(1/\beta)H\tau^2.
\end{align*}
\end{proof}

\subsection{Wrapping up the proof of Theorem~\ref{thm:po-main-ldc-simplex}}\label{sec:po-proof-nonmarkov}

Before proving Theorem~\ref{thm:po-main-ldc-simplex}, we establish that the loss functions $\ell_t$ used in $\NMGPCPOS$ are Lipschitz with respect to $\|\cdot\|_{\Sigma}$ in Lemma~\ref{lem:po-lt-lip-nonmarkov}, where $\|\cdot\|_{\Sigma}$ measures the $\ell_1$-norm of $M=M^{[0:H]}$ as a flattened vector in $\mathbb{R}^{d_x(d_y+d_x)(H+1)}$, formally given by
\begin{align*}
\|M_t\|_{\Sigma}:=\sum_{i=0}^{H}\sum_{j\in[d_x],k\in[d_y+d_x]}|(M_t^{[i]})_{jk}|.
\end{align*}

\begin{lemma} [Lipschitzness of $e_t$]
\label{lem:po-lt-lip-nonmarkov}
Suppose that a partially observable simplex LDS $\ML$ and $H\ge \tau>0$ are given, and $\Pi_{\tau}(\ML)$ is nonempty. Fix $K_0\in\mathbb{S}^{d_x,d_y}$. For each $t\in[T]$, the pseudo loss function $e_t$ (as defined on Line~\ref{line:nonmarkovet-loss} of \cref{alg:gpc-po-nonmarkov} is $O(Ld_x^{11/2}\tau)$-Lipschitz with respect to the norm $\|\cdot\|_{\Sigma}$ in $\MM_{d_x,d_y,H}^{+}$.  %\noah{did we say somewhere we're dropping the subscripts on $\mathcal{M}$?}
\end{lemma}

\begin{proof}
Since $e_t$ (Line~\ref{line:nonmarkovet-loss} of \cref{alg:gpc-po-nonmarkov}) is a composite of many functions, we analyze the Lipschitz condition of each of its components. First, we show that the $M\mapsto y_t(M;K_0)$ and $M\mapsto \tilde{u}_t(M;K_0)$ are Lipschitz in $M$ on $\MM_{d_x,d_y,H}^{+}$.
In particular, fix $M_1,M_2\in\MM_{d_x,d_y,H}^{+}$, we will show that 
\begin{align*}
\|y_t(M_1;K_0)-y_t(M_2;K_0)\|_1&\le O(d_x^{7/2}\tau) \|M_1-M_2\|_{\Sigma}, \\ 
\|\tilde{u}_t(M_1;K_0)-\tilde{u}_t(M_2;K_0)\|_1&\le 2\sqrt{d_x} \|M_1-M_2\|_{\Sigma}.
\end{align*}
Expanding $\tilde{u_t}$, we have
\begin{align*}
\|\tilde{u}_t(M_1;K_0)-\tilde{u}_t(M_2;K_0)\|_1&\le \sqrt{d_x}\|\tilde{u}_t(M_1;K_0)-\tilde{u}_t(M_2;K_0)\|_2\\
&\le \sqrt{d_x}\left\|(M_1^{[0]}-M_2^{[0]})y_t(K_0)+\sum_{i=1}^H\bar{\lambda}_{t,i}(M_1^{[i]}-M_2^{[i]})y_{t-i}(K_0)\right\|_1\\
&\le 2\sqrt{d_x} \max_{0\le i\le H} \|M_1^{[i]}-M_2^{[i]}\|_{1\rightarrow 1} \\
&\le 2\sqrt{d_x}\|M_1-M_2\|_{\Sigma},
\end{align*}
where the second inequality follows from the fact that $\ell_2$ norm is bounded by $\ell_1$ norm. By Lemma~\ref{cor:lip-control}, we have
\begin{align*}
\|u_t(M_1;K_0)-u_t(M_2;K_0)\|_1\le O(d_x^{7/2})\|M_1-M_2\|_{\Sigma}.
\end{align*}
Expanding $y_t$, we have that
\begin{align*}
\|y_t(M_1;K_0)-y_t(M_2;K_0)\|_1&=\left\|\sum_{i=1}^{t-1}\bar{\lambda}_{t,i}C[(1-\alpha)A]^{i-1}\alpha B(u_t(M_1)-u_t(M_2))\right\|_1.
\end{align*}
We consider two cases, depending on the mixing time $\tau_A:=\tmix(A)$ of $A$. 

\paragraph{Case 1:} $\tau_A\le 4\tau$. Let the stationary distribution of $A$ be denoted $p^{\star}\in\Delta^{d_x}$. Then by Lemma 22 in \citep{golowich2024online}, $\forall i\in\mathbb{N}$ and $v\in\mathbb{R}^d$ with $\langle \mathbbm{1}, v\rangle=0$, $\|A^{i}v\|_1\le 2^{-\lfloor i/\tau_A\rfloor}\|v\|_1\le 2^{-\lfloor i/4\tau\rfloor}\|v\|_1$. Then,
\begin{align*}
\|y_t(M_1)-y_t(M_2)\|_1&\le \sum_{i=1}^{t-1} \|CA^{i-1}B(u_t(M_1)-u_t(M_2))\|_1\\
&\le \sum_{i=1}^{t-1} 2^{-\lfloor (i-1)/4\tau\rfloor}\|u_t(M_1)-u_t(M_2)\|_1\\
&\le Cd_x^{7/2}\|M_1-M_2\|_{\Sigma}\sum_{i=1}^{t-1} 2^{1-\lfloor (i-1)/4\tau\rfloor}\\
&\le Cd_x^{7/2}\tau \|M_1-M_2\|_{\Sigma}. 
\end{align*}

\paragraph{Case 2:} $\tau_A>4\tau$. 
We already know from the previous discussion that $\alpha\ge 1/(256d_x \tau)$


We have
\begin{align*}
\|y_t(M_1)-y_t(M_2)\|_1&\le\sum_{i=1}^{t-1}(1-\alpha)^{i-1}\|(u_t(M_1)-u_t(M_2))\|_1\\
&\le Cd_x^{7/2}\|M_1-M_2\|_{\Sigma}\sum_{i=1}^{t-1}(1-1/(256d_x\tau))^{i-1}\\
&\le C d_x^{11/2}\tau\|M_1-M_2\|_{\Sigma}.
\end{align*}
Here we proved that $M\mapsto y_t(M;K_0)$ is $O(d_x^{11/2}\tau)$-Lipschitz, and $M\mapsto \tilde{u}_t(M;K_0)$ is $2\sqrt{d_x}$-Lipschitz. Immediately, $c_t^y(y_t(M;K_0))$ is $O(Ld_x^{11/2}\tau)$-Lipschitz on $\MM_{d_x,d_y,H}^{+}$. It suffices to show that
\begin{align*}
\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))) \cdot c_t^u(\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))}))
\end{align*}
is Lipschitz on $\MM_{d_x,d_y,H}^{+}$. Since $\phi$ is $1$-Lipschitz over $\mathbb{R}^{d_y+d_x}$, we have that $M\mapsto \phi(\tilde{u}_t(M;K_0))$ is $2\sqrt{d_x}$-Lipschitz. By assumption $c_t^{u}$ is $L$-Lipschitz on $\Delta^{d_x}$, and $\phi^{-1}$ is $2$-Lipschitz over $\mathcal{K}$, we have $c_t^{u}\circ\phi^{-1}$ is $O(L)$-Lipschitz on $\mathcal{K}$. 

By Lemma~\ref{cor:lip-control}, 
\begin{align*}
u\mapsto \pi_{\mathcal{K}}(u)\cdot \left(c_t^u\circ\phi^{-1} \left(\frac{u}{\pi_{\mathcal{K}}(u)}\right)\right)
\end{align*}
is $O(Ld_x^{4})$-Lipschitz over $\mathcal{K}$, making $c_t^u(u_t(M;K_0))$ $O(Ld_x^{9/2})$-Lipschitz on $\MM_{d_x,d_y,H}^{+}$.  . Combining, we have that $e_t$ is $O(Ld_x^{11/2}\tau)$-Lipschitz on $\MM_{d_x,d_y,H}^{+}$.  
\end{proof}

\begin{proof} [Proof of \cref{thm:po-main-ldc-simplex}]
We are ready to prove \cref{thm:po-main-ldc-simplex}. It is crucial to check the condition in Lemma~\ref{lem:nonmarkovpo-mem-mismatch}. Define
\begin{align*}
\|M_t\|_{\star}:=\max_{0\le i\le H}\|M_{t}^{[i]}\|_{1\rightarrow 1}.
\end{align*}
It is easy to check that $\|M\|_{\Sigma}\ge \|M\|_{\star}$. With slight abuse of notation, let $\|M\|_2$ denote the $\ell_2$-norm of the flattened vector in $\mathbb{R}^{d_x(d_y+d_x)(H+1)}$ for $M\in\MM_{d_x,d_y,H}^{+}$. Let $\|\cdot\|_{\Sigma}^*$ denote the dual norm of $\|\cdot\|_{\Sigma}$. Lemma~\ref{lem:po-lt-lip-nonmarkov} implies that
\cref{alg:gpc-po-nonmarkov} guarantees
\begin{align*}
& \quad \|M_{t}-M_{t+1}\|_{\star}\le\|M_{t}-M_{t+1}\|_{\Sigma}\le_{(1)} \sqrt{d_x(d_y+d_x)H}\|M_{t}-M_{t+1}\|_2\\
&\le_{(2)} \eta \sqrt{d_x(d_y+d_x)H} \|\partial e_t(M_t)\|_{2}\le_{(3)}\eta \sqrt{d_x(d_y+d_x)H}\|\partial e_t(M_t)\|_{\Sigma}\\
&\le_{(4)} \eta (d_x(d_y+d_x)H)^{3/2} \|\partial e_t(M_t)\|_{\Sigma}^{*}\le_{(5)} \eta(d_x(d_y+d_x)H)^{3/2}Ld_x^{11/2}\tau\\
&\le 3\eta H^{5/2}Ld_x^{17/2}, 
\end{align*}
for some constant $C$, where $(1)$ and $(3)$ follow from $\ell_1$-norm-$\ell_2$-norm inequality that $\forall v\in\mathbb{R}^n$ $\|v\|_2\le \|v\|_1\le\sqrt{n}\|v\|_2$; $(2)$ follows from the update in Line~\ref{line:nonmarkovpo-update} of \cref{alg:gpc-po-nonmarkov};
$(4)$ follows from that $\|M\|_{\Sigma}\le d_xd_yH\|M\|_{\Sigma}^*$, and $(5)$ follows from Lemma~\ref{lem:po-lt-lip}, the assumption that $d_x\ge d_y$, and that $H\ge \tau$. 

Moreover, by the standard regret bound of OGD (Theorem 3.1 in \cite{hazan2016introduction}), we have
\begin{align*}
\sum_{t=1}^T e_t(M_t)-\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)\le O\left(\frac{D^2}{\eta}+(Ld_x^{11/2}\tau)^2\eta T\right), 
\end{align*}
where $D$ denotes the diameter of $\MM_{d_x,d_y,H}$ with respect to $\|\cdot\|_{\Sigma}$, which is bounded in this case by 
\begin{align*}
D=\sup_{M\in\mathcal{M}}\left\{ \sum_{i=0}^H\sum_{j\in[d_x],k\in[d_y+d_x]}|M^{[i]}_{jk}|\right\}\le 3(d_y+d_x)H.
\end{align*}
Take $\eta=1/(LH^{2}d_x^{6}\sqrt{T})$ and recall that $d_y\le d_x$, then we have
\begin{align*}
& \quad \sum_{t=1}^T c_t(y_t,u_t)-\min_{K^{\star}\in \Ksim_\tau(\ML)}\sum_{t=1}^T c_t(y_t(K^{\star}),u_t(K^{\star}))\\
&\le \sum_{t=1}^T c_t(y_t,u_t)-\sum_{t=1}^T c_t(y_t(M_t;K_0),u_t(M_t;K_0))+\sum_{t=1}^T c_t(y_t(M_t;K_0),u_t(M_t;K_0))- \sum_{t=1}^Te_t(M_t)\\
& \quad + \sum_{t=1}^Te_t(M_t)-\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)+1\\
&\le \sum_{t=1}^T e_t(M_t)-\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)+\tilde{O}\left(\eta H^{11/2}L^2d_x^{14} T\right)\\
&\le \tilde{O}\left(\frac{d_x^2H^2}{\eta}+(Ld_x^{11/2}\tau)^2\eta T+\eta H^{11/2}L^2d_x^{14} T\right)\\
&\le \tilde{O}\left(L\tau^{4}d_x^{8}\sqrt{T}\right),
\end{align*}
where the first inequality follows from Lemma~\ref{lem:po-approx-ldc} by taking $\eps=1$; the second inequality follows from Lemma~\ref{lem:nonmarkovpo-mem-mismatch} by taking $\beta=3\eta H^{5/2}Ld_x^{17/2}$ and since $c_t(y_t(M_t;K_0),u_t(M_t;K_0))\le e_t(M_t)$ from Observation~\ref{obs:ext}, and the third inequality follows from OGD regret guarantee. 
\end{proof}

