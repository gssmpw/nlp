\section{Minkowski Extension and Projection}\label{sec:min}

To address constraint issues in population dynamics, we introduce a convex extension technique. A similar approach has been explored in OCO (Online Convex Optimization), as presented in \citep[Section~4]{cutkosky2018black}. However, we extend this technique to the more complex setting of non-stochastic control.

\paragraph{Motivation.} Our control learning class outputs a ``raw control'' $\tilde{u}_t(M) \in \mathbb{R}^{d_x}$, which is linear in the learning parameters $M$. In population dynamics, this raw control may not necessarily satisfy the constraint $\Delta^{d_x}$. A naive approach would be to project $\tilde{u}_t$ onto $\Delta^{d_x}$, but this breaks the convexity: while $c_t(y_t, u_t)$ is convex in $u_t$, it becomes non-convex in $M$ after projection.

To resolve this, we separate control \emph{updating} from \emph{execution}. Specifically, we use a convex \emph{surrogate loss} $e_t$ defined on $\mathbb{R}^{d_x}$ rather than $c_t$, allowing learning updates to occur in an unconstrained space. Projection is applied only at execution time, preserving convexity in parameter updates. This mirrors the philosophy of mirror descent, where updates occur in a dual space before mapping back to the primal space.

Our approach leverages a convex extension technique based on the \emph{Minkowski functional} (also known as the \emph{gauge function}). While the Minkowski functional is a classical concept in convex analysis \citep{rockafellar1970convex}, it has recently been used in projection-free online learning \citep{mhammedi2022efficient} and bandit convex optimization \citep{fokkema2024online}. Inspired by \cite{lattimore2024bandit}, we construct a convex extension function using this tool.

\paragraph{Definitions.} Throughout the following definitions, we let $\K\subset\mathbb{R}^d$ be a convex set satisfying $0\in\K$. We define the Minkowski functional as follows:

\begin{definition} [Minkowski functional]
The Minkowski functional $\pi_\mathcal{K}:\mathbb{R}^d\rightarrow\mathbb{R}$ associated with the set $\MK$ is defined as $\pi_{\K}(x)=\inf\{t\ge 1: x\in t\mathcal{K}\}$.
\end{definition}

Intuitively, $\pi_{\mathcal{K}}(x)$ is the smallest positive scaling of $\mathcal{K}$ required to enclose $x$. Since we are interested in cases where $x \notin \mathcal{K}$, we enforce $\pi_{\mathcal{K}}(x) \geq 1$. By construction, $\pi_{\mathcal{K}}$ is convex, and $\pi_{\mathcal{K}}(x) = 1$ if and only if $x \in \mathcal{K}$.
We now define two key constructs: the \emph{Minkowski projection} and the \emph{Minkowski extension}\footnote{We note that the definition of the Minkowski function is not unique, as shown in \citep[Section~3.7]{lattimore2024bandit}. Here we use a simple form not considered there, but sufficient for our purpose.}.

\begin{definition} [Minkowski projection]
The Minkowski projection to the set $\MK$ is a function mapping $\mathbb{R}^d\rightarrow\K$, given by $\Proj_{\Delta^{d_x}}^{MP}(x)= \frac{x}{\pi_{\K}(x)}$. 
\end{definition}

\begin{definition}[Minkowski extension]\label{def:minkowski-extension}
For any convex function $f:\K\rightarrow\mathbb{R}_+$, its Minkowski extension $e_f:\mathbb{R}^d\rightarrow\mathbb{R}_+$ with respect to the set $\MK$ is defined as
\begin{align}
\label{eq:extension}
e_f(x)=\pi_{\mathcal{K}}(x)\cdot f\left(\frac{x}{\pi_{\mathcal{K}}(x)}\right).
\end{align}
Here the notation $\MK$ is omitted on the LHS for conciseness.
\end{definition}


\subsection{Key Properties}\label{sec:property-minkowski}

The Minkowski extension satisfies several useful properties, making it well-suited as a surrogate loss function.

\begin{lemma} [Validity as surrogate loss]
\label{lem:extension-property}
Let $\K\subset\mathbb{R}^d$ be a convex set satisfying $0\in\K$, and $f:\K\rightarrow\mathbb{R}_+$ be a convex function. 
The Minkowski extension $e_f:\mathbb{R}^d\rightarrow\mathbb{R}_+$ from Definition~\ref{def:minkowski-extension} satisfies the following properties:
\begin{enumerate}
    \item $e_f(x)=f(x)$ for all $x\in \mathcal{K}$.
    \item $e_f(x)\ge f(\frac{x}{\pi_{\mathcal{K}}(x)})$ for all $x\in\R^d$.
    \item $e_f$ is convex on $\mathbb{R}^d$.
\end{enumerate}
\end{lemma}

Another important aspect is the Lipschitzness of the Minkowski extension, such that using it as a surrogate loss is tractable from the perspective of optimization. We will first require a curvature property on $\mathcal{K}$.

\begin{definition} [$\kappa$-aspherity]
\label{def:aspherity}
Given $d\in\mathbb{N}$, a convex set $\K\subset\mathbb{R}^d$ is said to have $\kappa$-aspherity for some $\kappa\ge 1$ if $\exists r, R>0$ (without loss of generality, assume that $r\le 1\le R$) such that $\kappa=\frac{R}{r}$ and
\begin{align*}
r\mathbb{B}^d\subset\K\subset R\mathbb{B}^d, 
\end{align*}
where $\mathbb{B}^d=\{x\in\mathbb{R}^d\mid \|x\|_2\le 1\}$ denotes the unit ball in $\mathbb{R}^d$.  
\end{definition}

\begin{lemma} [Lipschitzness of extension function]
\label{lem:extension-lip}
Let $\K\subset\mathbb{R}^d$ be a convex set with $\kappa$-aspherity, and $f:\K\rightarrow\mathbb{R}_+$ be a convex, $L_f$-Lipschitz function (w.r.t. $\|\cdot\|$ satisfying $\|\cdot\|\ge \|\cdot\|_2$) that attains $0$ on $\K$. Let $D_{\K}$ be the diameter of $\K$ in $\|\cdot\|$.  Then the extension fucntion $e_f$ is $(4DL_f\kappa^4+\frac{D_{\K}L_f}{r})$-Lipschitz on $D\mathbb{B}^d$, $\forall D>0$. When $R\ge 1$, $e_f$ is $(4DL_f\kappa^4+D_{\K}L_f\kappa)$-Lipschitz w.r.t. $\|\cdot\|_2$.
\end{lemma}

First, $e_f$ is only Lipschitz around the set $\MK$, which is fine since the underlying unconstrained algorithm stays around $\MK$. Second, the Lipschitz constant increases with the aspherity of $\MK$. This motivates our reparameterization on the control set described next.

\subsection{Application to Control}\label{sec:control-using-minkowski}
To apply Minkowski extensions in non-stochastic control, we must adapt the simplex constraint because $\Delta^{d_x}$ doesn't contain 0 and has $\kappa=0$ in $\mathbb{R}^{d_x}$. The following


 linear bijective map $\phi$ from $\Delta^{d_x}$ to $\mathbb{R}^{d_x-1}$ re-centers the simplex in a low-dimensional space.
\begin{align}
\label{eq:phi-func}
    \forall u\in\Delta^{d_x}, \phi(u)= (u_1,\dots,u_{d_x-1})-\frac{\mathbf{1}_{d_x-1}}{2(d_x-1)}.
\end{align}

\begin{definition} [Control set]
\label{def:observation-control-set}
Given $d_x\in\mathbb{N}$, $d_x\ge 2$, the feasible control class is given by  $\Delta^{d_x}$.
Additionally, define a lower-dimensional class $\K\subset\mathbb{R}^{d_x-1}$ as the following:
\begin{align}
\label{eq:pair-constrained-set}
    \K=\left\{v-\frac{\mathbf{1}_{d_x-1}}{2(d_x-1)}:v\in\mathbb{R}^{d_x-1},v\ge 0, \|v\|_1\le 1\right\}.
\end{align}
\end{definition}

\begin{lemma}[Properties of the control set]
\label{cor:lip-control}
Let $\K$ be defined in Definition~\ref{def:observation-control-set}, and let $\pi_{\mathcal{K}}$ be the associated Minkowski functional. Then $\pi_{\mathcal{K}}$ satisfies the following properties:
\begin{enumerate}
\item $\pi_{\mathcal{K}}$ is $2d_x$-Lipschitz on $\mathbb{R}^{d_x-1}$. 
\item The extension $e_f$ of a convex $L_f$-Lipschitz (w.r.t. $\|\cdot\|_1$) function $f:\K\rightarrow\mathbb{R}_+$ is $O(DL_fd_x^4)$-Lipschitz (w.r.t. $\|\cdot\|_2$) over $D\mathbb{B}^{d_x-1}$ if $f$ is bounded by $B$ on $D\mathbb{B}^{d_x-1}$ for $D\ge 1$.
\end{enumerate}
\end{lemma}


