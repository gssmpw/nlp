\section{Controlling under Partial Observability}
\label{sec:po-prelim-and-approx}

In this section, we give an efficient online controller $\GPCPOS$ for partially observable simplex LDS, which has provable regret guarantees against a particular class of Markov linear policies. 
We begin with a standard assumption on the cost functions analogous to Assumption 1 in \citep{golowich2024online}:

\begin{assumption}[Lipschitz, decomposable convex cost]
\label{asm:po-convex-loss}
The cost functions $c_t:\Delta^{d_y}\times \Delta^{d_x} \to \BR_+$ take the form \footnote{$c_t$ is assumed to be separated for variables $y,u$, which is standard (e.g. in LQR, \citep{simchowitz2020naive}).} 
\begin{align*}
c_t(y,u)=c_t^y(y)+c_t^u(u),
\end{align*}
where $c_t^y:\Delta^{d_y}\rightarrow\mathbb{R}_+, c_t^u:\Delta^{d_x}\rightarrow\mathbb{R}_+$ are convex and $L$-Lipschitz in the following sense: for all $y,y'\in\Delta^{d_y}$ and $u,u'\in\Delta^{d_x}$, we have
\begin{align*}
|c_t^y(y)-c_t^y(y)|\le L\cdot \|y-y'\|_1, \ \ \ |c_t^u(u)-c_t^u(u')|\le L\cdot \|u-u'\|_1.
\end{align*}
Without loss of generality, assume that $L\ge 1$ and $\exists (y,u)\in\Delta^{d_y}\times\Delta^{d_u}$ such that $c_t(y,u)=0$. 
\end{assumption}

We define the class of policies against which we prove regret bounds. These are valid linear policies that “mix” the state of the system, akin to the fully observable setting.

\begin{definition} [Comparator policy class]
\label{def:po-comparator}
Let $\ML$ be a partially observable simplex LDS with transition matrices $A,B \in \mathbb{S}^{d_x}$, observation matrix $C \in \mathbb{S}^{d_y, d_x}$, and control power $\alpha \in [0,1]$. Given a matrix $K\in\mathbb{S}^{d_x, d_y}$, define \[\mathbb{A}_K^{\alpha}:=(1-\alpha)A+\alpha BKC\in\mathbb{S}^{d_x}.\]
We define the comparator policy class $\Ksim_\tau = \Ksim_\tau(\ML)$ as the set of linear, time-invariant policies $u_t=K y_t$ such that $K\in\mathbb{S}^{d_x,d_y}$ satisfies $\tmix(\mathbb{A}_{K}^{\alpha})\le \tau$. Here
\begin{align*}
t^{\mathrm{mix}}(\mathbb{A}_{K}^{\alpha})=\min_{t\in\mathbb{N}}\left\{t: \quad \forall t'\ge t, \sup_{p\in\Delta^{d_x}}\|(\mathbb{A}_{K}^{\alpha})^t(p)-\pi(\mathbb{A}_{K}^{\alpha})\|_1\le \frac{1}{4}\right\}\le \tau,
\end{align*}
where $\pi(\mathbb{A}_{K}^{\alpha})$ denotes the stationary of $\mathbb{A}_{K}^{\alpha}$.
\end{definition}

For any $K \in \BS^{d_x, d_y}$ and partially observable simplex LDS $\ML$, we let $(y_t(K))_t$ and $(u_t(K))_t$ denote the counterfactual sequences of observations and controls that would have been obtained by following the time-invariant, linear policy $u_t=K y_t$ in $\ML$.
Our result for controlling partially observable simplex with Markov linear comparator policy class is stated below
(we omit the dependence on the system $\ML$, since it is always clear from context).

\begin{theorem} [PO regret: Markov comparator policy class]
\label{thm:po-main} 
Let $d_x,d_y, T\in\BN$ with $d_y\leq d_x$. %Let $\ML=(A,B, C, x_1,(\gamma_t)_{t\in\BN},(w_t)_{t\in\BN},(c_t)_{t\in\BN}, \alpha)$ be a partially observable simplex LDS with cost functions $(c_t)_t$ satisfying \cref{asm:po-convex-loss} for some $L > 0$. 
Set $H := 2\tau \lceil \log(512d_x^{7/2}LT^2) \rceil$, $\eta=1/(Ld_x^{4.5}H^{2}\sqrt{T})$. Then for any $\tau > 0$, the iterates  $(y_t, u_t)$ of $\GPCPOS$ (\cref{alg:gpc-po}) with input $(A, B, C, \tau, H,T)$ have the following policy regret guarantee with respect to $\Ksim_\tau(\ML)$: \begin{align*}
\sum_{t=1}^T c_t(y_t,u_t)-\min_{K^{\star}\in\Ksim_\tau(\ML)}\sum_{t=1}^T c_t(y_t(K^{\star}),u_t(K^{\star}))\le \tilde{O}\left(L\tau^{4}d_x^{6.5}\sqrt{T}\right),
\end{align*}
where $\tilde{O}(\cdot)$ hides all universal constants and poly-logarithmic dependence in $T$. 
\end{theorem}

\subsection{Our Algorithm}
\label{sec:po-alg}

The algorithm $\GPCPOS$ (\cref{alg:gpc-po}) is in broad strokes similar to $\GPCS$ \citep{golowich2024online}; it uses online convex optimization over a class of policies that is expressive enough to approximate any policy in the comparator class.

The general idea is the following: although the comparator class consists of only linear Markov policies $u_t=K y_t$, we can't directly learn this $K$ by OCO because $y_t$ is non-oblivious. Instead, the non-stochastic control approach reparameterizes the control $u_t$ as a linear combinations of computable oblivious signals $o_t$ (Line~\ref{line:control-po-utot} in Algorithm~\ref{alg:gpc-po}) with small memory $H$. Then because $c_t$ is convex in $u_t$, it's also convex in the control parameters $M$, reducing the original control problem to a tractable OCO problem over $M$. As long as the comparator class is a subset of the learning class, any regret bound of this OCO problem directly transfers to the control comparator class of interest.

To state the algorithm precisely, we need to define the optimization domain $\MM$ (analogous to $\Xgpc[d,H,a_0,\alphaub]$ in $\GPCS$) and the correspondence between points in the domain and policies. First, the optimization domain $\MM_{d_x,d_y,H}$ is defined below:


\begin{algorithm}[t]
\caption{$\GPCPOS$}
\label{alg:gpc-po}
\begin{algorithmic}[1]
\REQUIRE Transition matrices $A,B \in \BS^{d_x}$, observation matrix $C \in \BS^{d_y,d_x}$, horizon parameter $H \in \BN$, control parameter $\alpha\in[0,1]$, step size $\eta>0$.  Let $\K$ be defined as in \cref{eq:pair-constrained-set}, and $\pi_{\mathcal{K}}$ be the Minkowski functional associated with it.  
Denote as $\phi^{-1}$ the inverse of $\phi$ when restricted to $\Delta^{d_x}$.

\STATE Initialize $M_1^{[0:H]}\in \MM := \MM_{d_x,d_y,H}$ (Definition~\ref{def:learning-class}). Choose any $K_0\in\mathbb{S}^{d_x,d_y}$. \label{line:choose-k0}
\STATE Observe the initial observation $y_1$.
\FOR{$t=1,\cdots,T$}
    \STATE Compute $o_t := y_t-\sum_{i=1}^{t-1}\bar{\lambda}_{t,i}C[(1-\alpha)A]^{i-1}\alpha B(u_{t-i}-K_0 o_{t-i})$. \label{line:compute-signal}
    \STATE Let $\tilde{u}_t=M_t(o_{t:t-H})=M_{t}^{[0]}o_t+\sum_{i=1}^H \bar{\lambda}_{t,i}M_{t}^{[i]}o_{t-i}$.\label{line:control-po-utot}
    \STATE Choose control $u_t=\phi^{-1}(\phi(\tilde{u}_t)/\pi_{\mathcal{K}}(\phi(\tilde{u}_t))$.\label{line:control-po}
    \STATE Receive cost $c_t(y_t,u_t)=c_t^y(y_t)+c_t^u(u_t)$, and observe $y_{t+1}$ and $\gamma_t$.
    \STATE Define surrogate loss function 
\begin{align*}
e_t(M) := c_t^y(y_t(M; K_0))+ \pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))) \cdot c_t^u\left(\phi^{-1}\left(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))}\right)\right).
\end{align*} \label{line:et-loss}
    \STATE Set $M_{t+1} := \Pi_{\mathcal{M}}^{\|\cdot\|_2}\left[M_t-\eta \cdot \partial e_t(M_t)\right]$, where $\Pi_{\mathcal{M}}^{\|\cdot\|_2}$ denotes the $\ell_2$ projection and $\partial e_t(M_t)$ is the subgradient of $e_t$ evaluated at $M_t$. \label{line:po-update}
\ENDFOR{}
\end{algorithmic}
\end{algorithm}


\begin{definition} [Learning policy class] 
\label{def:learning-class} 
Let $d_x,d_y,H \in \BN$ and let $\BS_{0}^{d_x,d_y}$ be the set of matrices $K\in[-1,1]^{d_x\times d_y}$ such that $\sum_{i=1}^{d_x} K_{ij} = 0$ for all $j \in [d_y]$. Then let $\MM_{d_x,d_y,H}$ be the set of tuples $M^{[0:H]} = (M^{[0]},\dots,M^{[H]})$ defined as follows:
\begin{align}
\label{eq:M-class}
\MM_{d_x,d_y,H} := \{M^{[0:H]}\mid M^{[0]}\in\BS^{d_x,d_y} \text{ and } M^{[i]}\in\BS_{0}^{d_x,d_y} \text{ for all }  i \in [H]\}.
\end{align}
\end{definition}

For dimension parameters $d_x,d_y \in \BN$ and history length $H \in \BN$, the domain $\MM_{d_x,d_y,H}$ parametrizes a class of policies $\{\pi^M: M \in \MM_{d_x,d_y,H}\}$ where $\pi^M$ is defined as follows:

\begin{definition} [Learning policy]
\label{def:learning-policy}
Fix $d_x,d_y \in \BN$ and $M = M^{[0:H]} \in \MM_{d_x,d_y,H}$. We define a policy $\pi^M$ that for any time $t$, given $\gamma_{t-1},\dots,\gamma_{t-H}$ and \emph{signals} $o_t,\dots,o_{t-H} \in \Delta^{d_y}$, produces the control
\begin{align*}
\pi^M_t(o_{t:t-H}) := \phi^{-1}\left(\Proj_{\Delta^{d_x}}^{MP}\left[\phi\left(M^{[0]}o_t+\alpha\sum_{i=1}^{H}\bar{\lambda}_{t,i}M^{[i]}o_{t-i}\right)\right]\right)
\end{align*}
where $\bar\lambda_{t,i}$ is defined for $t \in [T]$ and $i \in \BN^+$, 
\begin{align}
\lambda_{t,i} := \gamma_{t-i} \cdot \prod_{j=1}^{i-1} (1-\gamma_{t-j}), \qquad \bar \lambda_{t,i} := \prod_{j=1}^{i} (1-\gamma_{t-j})\label{eq:define-lambdas}, \qquad \lambda_{t,0} := 1 - \sum_{i=1}^H \lambda_{t,i}.
\end{align}
%and $\Proj_{\Delta^{d_x}}^{MP}: \BR^{d_x} \to \Delta^{d_x}$ denotes the Minkowski projection operator onto $\Delta^{d_x}$,\zz{let's define this in Sec 2?} and $\phi$ denotes the linear transformation \cref{eq:phi-func}. 
\end{definition}

\iffalse
where $u_t(M; K_0)$ is the control at time $t$ according to $M$ with signals $y_{t-H:t}(K_0)$, given by
\begin{align*}
u_t(M;K_0)=u_t(M^{[0:H]};K_0)=\Pi_{\Delta^{d_x}}^{\|\cdot\|_2}\left[M^{[0]}y_t(K_0)+\sum_{i=1}^{H}\bar{\lambda}_{t,i}M^{[i]}y_{t-i}(K_0)\right],
\end{align*}

\fi

In the above definitions, note that (1) $\mathcal{M}_{d_x,d_y,H}$ is convex, and (2) the projection step in the definition of $\pi^M$ is necessary (in order for $\pi^M(o_{t-1:t-H})$ to be a valid control, i.e. in $\Delta^{d_x}$) since the vector before projection has indices summing to $1$ but is not necessarily entry-wise non-negative.

\paragraph{Loss Function.}
For notational convenience, for any $M \in \MM_{d_x,d_y,H}$, $K_0 \in \BS^{d_x,d_y}$ and $H \in \BN$, we write
\begin{align*}
\tilde{u}_t(M;K_0) &:= M^{[0]}y_t(K_0) + \alpha \sum_{i=1}^H \bar\lambda_{t,i} M^{[i]} y_{t-i}(K_0), \\ 
y_t(M;K_0) &:= \sum_{i=1}^{t-1}\bar\lambda_{t,i} C((1-\alpha)A)^{i-1}\alpha B u_{t-i}(M;K_0) + \sum_{i=1}^t \lambda_{t,i} ((1-\alpha)A)^{i-1} w_{t-i}.
\end{align*}

By unfolding, $y_t(M;K_0)$ is exactly the counterfactual observation at time $t$ if control $u_s(M;K_0) = \pi^M(y_{s:s-H}(K_0))$ had been chosen at time $s$, for each $1 \leq s \leq t$. We then define the surrogate loss function at time $t$ to be
\begin{align}
\label{eq:et-def}
e_t(M) := c_t^y(y_t(M; K_0))+ \pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))) \cdot c_t^u(\phi^{-1}(\frac{\phi(\tilde{u}_t(M;K_0))}{\pi_{\mathcal{K}}(\phi(\tilde{u}_t(M;K_0))})).
\end{align}


\begin{observation}[Extension properties]
\label{obs:ext}
$e_t$ defined in \cref{eq:et-def} is convex and satisfies $e_t(M)\ge c_t(y_t(M;K_0),u_t(M;K_0))$, $\forall M\in\MM_{d_x,d_y,H}$.
\end{observation}


%In \cref{sec:approx-po}, we show that every policy in the comparator class (Definition~\ref{def:po-comparator}) can be approximated by $\pi^M$ for some $M \in \MM_{d_x,d_y,H}$. We bound the memory mismatch error in \cref{sec:mem-mismatch-po}, and conclude the proof in \cref{sec:po-proof}.% and revisit the online gradient descent regret guarantee in \cref{sec:gd-prelim}.


\subsection{Proof Overview}
Similar to $\GPCS$, $\GPCPOS$ runs gradient steps on the counterfactual loss, and thus the regret decomposition follows similarly to that of $\GPCS$ (Algorithm 1 in \cite{golowich2024online}). The analysis breaks down into three main ingredients: 
\begin{enumerate}
    \item We show that the counterfactual observations $y_t(K_0)$ under some ``default'' linear policy $K_0$, can be exactly computed to be the signals $o_t$ in Line~\ref{line:compute-signal} of \cref{alg:gpc-po}. In particular, through an unrolling technique, we will show that $y_t(K_0)$ can be constructed as adding a calibration term based on the difference between controls $u_{t-i} - K_0 y_{t-i}(K_0)$, to the raw observation $y_t$:
    \begin{align*} y_t(u_{1:t-1}) - \sum_{i=1}^{t-1} \bar\lambda_{t,i}C((1-\alpha)A)^{i-1}\alpha B(u_{t-i} - K_0 y_{t-i}(K_0)) = y_t(K_0).\end{align*}
   
    \item Next, we show the expressivity of the learning policy class: every policy $K^*$ in the comparator class (Definition~\ref{def:po-comparator}) can be approximated by $\pi^M$ for some $M \in \MM_{d_x,d_y,H}$. This is done by constructing $M$ such that $u_t(M;K_0)$ and $u_t(K^*)$ are very close (implying $y_t(K^*)$ also close to $y_t(M;K_0)$, then costs are close by Lipschitzness), which breaks into a few steps. First, we explicitly construct $M$ such that $u_t(K^*)$ is exactly equal to the control with full memory $\hat{u}_t(M;K_0)$, defined as below 
\begin{align*}
\hat{u}_{t}(M;K_0):=M^{[0]}y_t(K_0)+\alpha\sum_{i=1}^{t-1}\bar{\lambda}_{t,i}M^{[i]}y_{t-i}(K_0).
\end{align*}
    Then we show a low truncation error between $\hat{u}_t(M;K_0)$ and $\tilde{u}_t(M;K_0)$ (Line~\ref{line:control-po} in Algorithm~\ref{alg:gpc-po}) which has only $H$ history dependence: roughly speaking, we show the difference term $\left\|\sum_{i=H+1}^{t-1}\bar{\lambda}_{t,i}M^{[i]}y_{t-i}(K_0)\right\|_1$ can be written as $\sum_{i=H+1}^{t-1} O\left(\left\|A_{\text{mix}}^i(v_1-v_2)\right\|_1\right)$ for some mixing matrix $A_{\text{mix}}$ and probability vectors $v_1,v_2$, here $A_{\text{mix}}^iv_1, A_{\text{mix}}^iv_2$ are very close due to the mixing property. Finally we show $\tilde{u}_t(M;K_0)$ is close to its projection. 

    
    \item Then, we bound the memory mismatch error: the difference between the true loss and the would-be loss when all controls in history are replaced by $M_t$. Here a new treatment on the discussion about mixing time is required, due to the new parameterization.
\end{enumerate}
Finally, after proving regularity properties of $e_t$ required by the previous steps, we can put everything together and conclude the proof. Below is the regret decomposition, where the extension mismatch error is non-positive by definition, and the comparator mismatch error can be easily bounded by constant.
\begin{align*}
&\sum_{t=1}^T c_t(y_t,u_t)-\min_{K^{\star}\in \Ksim_\tau(\ML)}\sum_{t=1}^T c_t(y_t(K^{\star}),u_t(K^{\star}))\\
&\le \underbrace{\sum_{t=1}^T c_t(y_t,u_t)-\sum_{t=1}^T c_t(y_t(M_t;K_0),u_t(M_t;K_0))}_{\text{(memory mismatch error)}} + \underbrace{\sum_{t=1}^T c_t(y_t(M_t;K_0),u_t(M_t;K_0))- \sum_{t=1}^Te_t(M_t)}_{\text{(extension mismatch error})} \\
&\quad + \underbrace{\sum_{t=1}^Te_t(M_t)-\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)}_{\text{(regret on extension)}} + \underbrace{\min_{M\in\mathcal{M}}\sum_{t=1}^T e_t(M)- \min_{K^{\star}\in \Ksim_\tau(\ML)}\sum_{t=1}^T c_t(y_t(K^{\star}),u_t(K^{\star}))}_{\text{(comparator mismatch error)}}
\end{align*}

