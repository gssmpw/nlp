%\documentclass{article}
% \documentclass[acmlarge,review]{acmart} 
\documentclass[acmlarge]{acmart}
\usepackage{graphicx} % Required for inserting images
\usepackage{soul}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption} 
\usepackage{caption}

\usepackage{subcaption}


\title{\textbf{Investigating the Generalizability of ECG Noise Detection Across Diverse Data Sources and Noise Types} }

\author{Sharmad Kalpande}
\email{kalpande22@iiserb.ac.in}
\author{Nilesh Kumar Sahu}
\email{nilesh21@iiserb.ac.in}
\orcid{0000-0003-1675-7270}
\author{Haroon R Lone}
\orcid{0000-0002-1245-2974}
\email{haroon@iiserb.ac.in}
\affiliation{%
  \institution{Indian Institute of Science Education and Research Bhopal (IISERB)}
  \streetaddress{Bhauri}
  \city{Bhopal}
  \state{Madhya Pradesh}
  \country{India}
  \postcode{462066}
}
 
\renewcommand{\shortauthors}{Kalpande et al.}
\renewcommand{\shorttitle}{Investigating the Generalizability of ECG Noise}

\begin{document}

\begin{abstract}
Electrocardiograms (ECGs) are essential for monitoring cardiac health, allowing clinicians to analyze heart rate variability (HRV), detect abnormal rhythms, and diagnose cardiovascular diseases. However, ECG signals, especially those from wearable devices, are often affected by noise artifacts caused by motion, muscle activity, or device-related interference. These artifacts distort R-peaks and the characteristic QRS complex, making HRV analysis unreliable and increasing the risk of misdiagnosis.

Despite this, the few existing studies on ECG noise detection have primarily focused on a single dataset, limiting the understanding of how well noise detection models generalize across different datasets. In this paper, we investigate the generalizability of noise detection in ECG using a novel HRV-based approach through cross-dataset experiments on four datasets. Our results show that machine learning achieves an average accuracy of over 90\% and an AUPRC of more than 0.9. These findings suggest that regardless of the ECG data source or the type of noise, the proposed method maintains high accuracy even on unseen datasets, demonstrating the feasibility of generalizability.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010405.10010444.10010449</concept_id>
       <concept_desc>Applied computing~Health informatics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120</concept_id>
       <concept_desc>Human-centered computing</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Health informatics}
\ccsdesc[300]{Human-centered computing}
\ccsdesc[300]{Computing methodologies~Machine learning}

\keywords{Electrocardiogram, ECG, Noise, HRV, Machine learning, Generalizability}
\maketitle


\section{Introduction}
The electrocardiogram (ECG) records the heart's electrical activity and is vital for monitoring and evaluating cardiac health. It enables clinicians to thoroughly analyze heart rate variability (HRV), accurately detect abnormal rhythms, and diagnose various cardiovascular conditions. By offering critical insights into cardiac function, ECGs support the early detection of abnormalities, timely interventions, and the assessment of conduction delays, myocardial ischemia, electrolyte imbalances, and structural heart disorders\cite{de1998prognostic}. Thus, ECG functionalities are crucial in enhancing patient outcomes and preventing life-threatening complications.

Deaths from Cardiovascular Disease (CVD) increased worldwide from 12.1 million in 1990 to 20.5 million in 2021, according to a report by the World Heart Federation\cite{whf_report1}. Many of these deaths are caused by heart attacks (myocardial infarctions) and strokes, which are among the most common fatal outcomes of CVD. Other contributing conditions include heart failure, hypertensive heart disease, arrhythmias, and cardiomyopathies. ECG  signals help detect these cardiovascular conditions, including heart blockages, atrial fibrillation, and ventricular hypertrophy. They enable clinicians to identify early risks, implement targeted interventions, and prevent life-threatening complications.

A standard ECG recording in clinical settings is taken using 12-lead electrodes, providing a comprehensive assessment of heart activity\cite{corrado2010recommendations}. However, these 12-lead ECG systems are bulky, so recordings are typically done in a controlled environment, i.e., the patient is at rest (such as lying on a bed). This makes them impractical for wild environment uses. Recent advancements in wearable sensing technology have focused on developing wearable ECG sensors with three\cite{kristensen2016use}, two\cite{moody1984bih}, or even a single electrode\cite{nemcova2020brno} for real-time ECG recording and monitoring in wild environments. These wearable sensors enable continuous remote heart monitoring, particularly for individuals with CVD and other conditions. However, ECG signals recorded using wearable sensors are often affected by noise artifacts. These artifacts may arise from body motion, device-related interference, or muscle movement \cite{chatterjee2020review}, making it difficult to perform morphological and RR interval analysis. As a result of these artifacts, R-peaks become distorted, and the expected signatures of the QRS complex may not be clearly visible, thus misguiding the inference through these ECG signals. 

Recent machine learning (ML) and deep learning (DL) research has explored ECG signals collected in both controlled and real-world environments for applications such as cardiovascular disease (CVD) prediction and mental disorder detection \cite{abubaker2022detection,sahu2024wearable}. However, ML and DL pipelines often rely on manual intervention to remove noisy segments, limiting automation 
\cite{sahu2024wearable}. This challenge has led to efforts to automate noise detection, but existing methods lack generalizability. Noise artifacts can result from various factors, including limb and hand movements, walking, and running, each introducing distinct signal characteristics. Additionally, ECG data is collected using different sensors operating under diverse conditions, leading to variations in sensor performance, calibration, and external disturbances. Differences in skin-electrode contact, sampling rates, and environmental factors further contribute to inconsistencies in ECG recordings. These variations make it difficult to develop a noise detection approach that remains effective across different settings. Therefore, this work studies ML-based noise detection to see the generalization across multiple ECG sources, sensor types, and noise conditions, ensuring robustness in practical applications. 

In this paper, we present a novel HRV-based ECG noise detection approach using machine learning. The ECG signals were empirically divided into 20-second overlapping segments with a 50\% overlap. A segment was labeled as noisy if at least 50\% of its data points were noisy. HRV was computed for each segment, and the corresponding label was assigned. We experimented with various machine learning classification models and found that Random Forest (RF) performed best for within-dataset noise detection. Therefore, we used RF for cross-dataset and cross-combined dataset noise detection. Our results show that generalizability is achievable in ECG noise detection, even though the datasets originated from different sources and contained various types of noise. We achieved an average accuracy of 91.1\% in cross-dataset evaluation and 93.1\% in cross-combined dataset evaluation. These results suggest that generalizability is possible in noise detection in ECG, and combining data from different sources helps the model learn noisy and clean ECG patterns more effectively. Following are our major contributions are:

% \hl{Add more in contribution like dataset and all}
\begin{itemize}
    \item[$\bullet$] We propose a novel method for detecting noisy ECG segments, which involves data segmentation, filtration, R-peak detection, HRV-based feature extraction, and classification using machine learning algorithms
    \item[$\bullet$] We created two datasets in controlled and semi-controlled environments—one for training and the other for validation. The proposed method was evaluated by training and testing on two existing datasets and the dataset we created. Finally, we validated the model on a specifically curated testing dataset to demonstrate its robustness and reliability.
    \item[$\bullet$] We evaluated generalizability using cross-dataset training and testing, as well as cross-combined training and testing.
\end{itemize}

\section{Related Work}
ECG has a wide range of applications in the healthcare field. It plays a crucial role in monitoring heart activity and diagnosing various medical conditions. One of its most important and widely used applications is the detection of CVDs using machine learning algorithms \cite{7164783} and deep learning algorithms \cite{smigiel2021ecg,9991600}. ECG is used to check for different kinds of arrhythmia. These can be detected when we have a clean ECG signal, but signals are affected by various reasons, causing the signals to become distorted and noisy. Noisy signal hampers the ML and DL performance. There are various methods by which the noisy segments are removed; statistical methods like EMD and EEMD are used for denoising the ECG signal \cite{kabir2012denoising} and CEEMD to detect the noisy ECG signals using statistical features \cite{satija2017automated}. However, automation of noisy signal detection is necessary to automate the process of discarding the ECg noisy segment. 

% \subsection{ECG usage and importance}
% ECG has a wide range of applications in the healthcare field. It plays a crucial role in monitoring heart activity and diagnosing various medical conditions. One of its most important and widely used applications is the detection of cardiovascular diseases (CVDs) using various machine learning algorithms \cite{7164783} and various deep learning algorithms \cite{smigiel2021ecg,9991600}. ECG is used to check for different kinds of arrhythmia 
% \subsection{Effect of noise in ECG}
% These can be detected when we have a clean ECG signal, but signals are affected by various reasons, causing the singles to become distorted and noisy. There are various method by which the noisy are removed, statistical methods like EMD and EEMD used for denoising the ECG signal \cite{kabir2012denoising} and CEEMD to detect the noisy ECG signals using statistical features \cite{satija2017automated}.

% There methods use machine learning techniques to detect the noisy ECG segment \cite{aminifar2024recognoise} and some to detect the quality of the ECG signal \cite{article}. There are many Deep learning method used to detect the noise as they take the whole signal as input and classify the signal \cite{dua2022automatic}.

\section{Methodology}

\subsection{Noisy ECG Signal} 
Recorded ECG signals can contain various types of noise artifacts caused by factors such as body motion and device-related interference. These noise artifacts are generally classified into four categories: baseline wander, powerline interference, muscle artifacts, and motion artifacts. The ECG signatures of these artifacts are shown in Figure \ref{fig:ECG_artifacts}. Below, we discuss each of these artifacts in detail.

\begin{figure}[]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Baseline_wander.png}
        \caption{Baseline Wander}
        \label{fig:Baseline Wander}
        \Description{A plot showing baseline wander noise in an ECG signal.}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Powerline_interference.png}
        \caption{Powerline Interference}
        \label{fig:Powerline Interference}
        \Description{A plot depicting powerline interference in an ECG signal.}
    \end{subfigure}
    \vspace{1em}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Muscel_artifact.png}
        \caption{Muscle Artifact}
        \label{fig:muscle-artifact}
        \Description{An ECG signal with muscle artifact noise.}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Motion_artifact.png}
        \caption{Motion Artifact}
        \label{fig:Motion Artifact}
        \Description{An ECG signal showing motion artifact distortion.}
    \end{subfigure}
    
    \caption{Different kinds of noisy segments in an ECG signal.}
    \label{fig:ECG_artifacts}
\end{figure}

% \begin{figure}[]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Baseline wander.png}
%         \caption{Baseline Wander}
%         \label{fig:Baseline Wander}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Powerline_interference.png}
%         \caption{Powerline Interference}
%         \label{fig:Powerline Interference}
%     \end{subfigure}
%     \vspace{1em}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Muscel_artifact.png}
%         \caption{Muscle Artifact}
%         \label{fig:muscle-artifact}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Motion_artifact.png}
%         \caption{Motion Artifact}
%         \label{fig:Motion Artifact}
%     \end{subfigure}
    
%     % Add your collective explanation text here
%     \caption{Different kinds of noisy segments in an ECG signals.}
%     \label{fig:ECG_artifacts}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \begin{minipage}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Baseline wander.png}
%         \caption{Baseline Wander}
%         %\label{fig:Baseline Wander}
%     \end{minipage}
%     \begin{minipage}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Powerline_interference.png}
%         \caption{Powerline Interference}
%         \label{fig:Powerline Interference}
%     \end{minipage}
%     \vspace{1em}
%     \begin{minipage}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Muscel_artifact.png}
%         \caption{Muscle Artifact}
%         \label{fig:muscle-artifact}
%     \end{minipage}
%     \begin{minipage}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Images/Motion_artifact.png}
%         \caption{Motion Artifact}
%         \label{fig:Motion Artifact}
%     \end{minipage}
    
%     % Add your collective explanation text here
%     \par
%     \vspace{-0.5em} % Adds some space between figure and text
%     \noindent
%     \caption{The above figures are plotted from Private dataset, each plot shows a segment of five seconds with sampling rate of almost 1000.}
% \end{figure}

     \begin{itemize}
        \item \textbf{Baseline Wander:} Baseline wander is a low-frequency artifact in ECG signals that causes drift from the baseline shown in Figure \ref{fig:Baseline Wander}. It is caused by the inhalation and exhalation activity of the lungs, and the interconnection between the electrodes and the skin.
        \item \textbf{Power Line interference:} Power line interface appears as regular, high-amplitude oscillations at 50 Hz or 60 Hz, often manifesting as sharp, periodic spikes in the ECG signal, as shown in Figure \ref{fig:Powerline Interference}. This is caused by loose electrode contact with the patient and dirty electrodes. 
        \item \textbf{Muscle Artifacts:} Muscle artifacts are high-frequency noises with rapid, irregular spikes, as shown in Figure \ref{fig:muscle-artifact}. It is caused by tremors, shivering, and hiccups.
        \item \textbf{Motion Artifact:} Motion artifacts appears as sudden, large deviations or irregular shifts in the ECG signal, as shown in Fig \ref{fig:Motion Artifact}. It is caused by shaking with rhythmic movement and vibrations of the person.
    \end{itemize}

Noises such as baseline wander, powerline interference, and small muscle artifacts can be removed using standard filtering techniques. However, noises like motion artifacts and large muscle artifacts cannot be eliminated through these methods. Therefore, in this study, we define a portion of a signal (i.e., a segment) as noisy if it contains large muscle or motion artifacts, as the absence of clear R-peaks in these conditions can lead to incorrect ECG analysis. The absence of distinct R-peaks in such conditions can lead to incorrect interpretations, making it crucial to identify and exclude these noisy segments.



\subsection{Datasets} \label{section: dataset}
Our analysis uses ECG datasets from four different studies, each collected for a specific problem statement. We used data from two of our studies - Speech performance dataset (SD) and Activity dataset (AD) - as well as two open-source datasets, MIT-BIH-NST and BUTQDB. Now, we will describe these datasets in detail.

% and the ground truth used in our study.

\subsubsection{SD}

In this study ECG data was collected during speech activity. It is a 3-lead ECG dataset recorded using Shimmer sensors in a controlled lab setting. Participants were seated on a chair and given a topic to deliver a two-minute speech. They were allowed to do hand movements while seated. At a time, three participants were present in the room with a moderator, taking turns to give their speeches. A total of 101 participants took part in the study, consisting of x males and y females, within the age range of x-y years.

\subsubsection{MIT-BIH-NST}

This database is derived from the MIT-BHI Arrhythmia Database. It comprises 12 half-hour, 2-lead ECG recordings and 3 half-hour noise recordings that represent common artifacts in ambulatory ECG signals. The noise recordings include baseline wander (BW), muscle (EMG) artifacts (MA), and electrode motion artifacts (EM). The ECG recordings were generated by introducing controlled noise into clean signals from the MIT-BIH Arrhythmia Database using the nstdbgen script. Noise was added in alternating two-minute segments after the first five minutes of each recording, with predefined signal-to-noise ratios (SNRs). While the original dataset applied noise to only two recordings (118 and 119), this study extends the approach to the entire MIT-BIH Arrhythmia Database. The WFDB package was used for noise addition, and for this study, only muscle artifacts (MA) and electrode motion artifacts (EM) were considered, with SNR values of 24 dB, 0 dB, and -6 dB. These specific SNR values were chosen because, at these levels, the R-peaks are no longer visible.

\subsubsection{BUTQDB}

This large, publicly available dataset consists of 18 long-term single-lead ECG recordings collected from 15 subjects (9 females, 6 males) aged between 21 and 83 years. The recordings were obtained while the subjects were engaged in ordinary everyday activities under 'free-living conditions.' Data was collected at a sampling frequency of 1,000 Hz, with a minimum recording length of 24 hours. This dataset includes three classes.
 \begin{itemize}
        \item \textbf{Class 1}: Clean ECG signals.
        \item \textbf{Class 2}: Slightly noisy signals, but the \textbf{R-peak} are still visible.
        \item \textbf{Class 3}: Completely noisy signals and no \textbf{R-peaks} are  visible.
    \end{itemize}
    
\subsubsection{AD}

This dataset is specifically collected to test the generalizability of the noise detection model when new data arrives. For this, we did a small study where ECG data were collected in a semi-controlled lab setting. The participants did activities with muscle activity, which introduced noise into the ECG recordings, thus signifying the real-world settings. Participants performed a 10-minute activity consisting of 2 minutes of sitting, 2 minutes of standing with hand movements,  2 minutes of walking with extensive muscle movement, and a 1-minute baseline period following each activity. Data was collected from six participants at a sampling rate of 1024 Hz using Shimmer sensors, resulting in approximately 60–70 minutes of total data.


% \begin{itemize}
%     \item \textbf{MIT-BIH-NST}: This database is created from MIT-BIH Arrhythmia Database that has noise introduced in MIT-BIH Arrhythmia database in a way that after first 5 minutes of each record alternating two-minute of noise is introduced with alternating two-minute clean signal in this dataset noise is introduce in only two recordings (118 and 119), but we use entire MIT-BHI Arrhythmia Dataset for doing the same. WFDB package was used to introduce noise in dataset and for this paper only muscle artifact (in record 'ma'), and electrode motion artifact (in record 'em') were used as noise, with SNR (dB) of 24, 0 and -6. 
%     \item \textbf{BUTQDB}: A large publicly available dataset which has 18 signals with minimum length of recording is 24 hour. In this dataset there are three classes: 
%     \begin{itemize}
%         \item \textbf{Class 1}: Clean ECG signals.
%         \item \textbf{Class 2}: Slightly noisy signals, but the \textbf{R-peak} are still visible.
%         \item \textbf{Class 3}: Completely noisy signals and no \textbf{R-peaks} are  visible.
%     \end{itemize}
    
%     \item \textbf{Private Data}: We have our dataset which was collected during a speech activity, each recording with duration of approximately 2-minutes, taken from almost 101 participants. In this activity the participant was asked to give speech, and allowed hand movements which introduced noise in it.
%     \item \textbf{10 min activity dataset}: This dataset is specifically designed for testing the model. It was collected in a controlled lab setting, where participants performed a 10-minute activity consisting of 2 minutes of sitting, 2 minutes of standing with hand movements, and 2 minutes of walking, with a 1-minute baseline period following each activity. Data is collected from a total of 6 participants, resulting in approximately 60-70 minutes of data overall.
    
% \end{itemize}

\subsection{Data annotation}
Two human annotators with experience in ECG signal analysis annotated the SD and AD ECG datasets. We used an open-source labeling software, Trainset, where the annotators selected the start and end indices of noisy segments (i.e., distorted QRS complexes) in each ECG signal. In many cases, multiple noisy segments were present within a single signal. The Trainset software assigns a ``noisy'' label to the selected indices, covering the segment from the start index to the end index. The annotators followed a single rule: if R-peaks were not clearly visible, the segment from the last identifiable R-peak to the next clearly visible R-peak was marked as noisy; otherwise, it was labeled as clean. Figure \ref{fig:D1_clean_noisy} illustrates the noisy and clean segments identified by the annotators in one participant's data.

\begin{figure}[]
    \centering
    \includegraphics[width=0.25\linewidth]{Images/ECG_ideal_signature.pdf}
    \caption{Ideal ECG signature}
    \label{fig:ecg_signature}
    \Description{A plot illustrating the ideal ECG signature, showing typical waveforms such as P, QRS complex, and T wave.}
\end{figure}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.25\linewidth]{Images/ECG_ideal_signature.pdf}
%     \caption{Ideal ECG signature}
%     \label{fig:ecg_signature}
% \end{figure}

\begin{figure*}[]
    \centering
    % Subfigure (a)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D1_CLean.png}
        \caption{Clean ECG signal}
        \label{fig:fig2a}
        \Description{}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D1_Noisy.png}
        \caption{Noisy ECG signal}
        \label{fig:fig2b}
        \Description{}
    \end{subfigure}
    \hfill
    % Subfigure (c)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D1_noisy_2.png}
        \caption{Another noisy ECG signal}
        \label{fig:fig2c}
        \Description{}
    \end{subfigure}

    \caption{Comparison of ECG signals from D1: (a) Clean signal, (b) Noisy signal, (c) Another noisy signal.}
    \label{fig:D1_clean_noisy}
\end{figure*}
The publicly available datasets used in this study,i.e., MIT-BIH NST and BUTQDB, already contained annotated ECG segments labeled as noisy and clean, requiring no further annotations. Figures \ref{fig:D2_clean_noisy} and \ref{fig:D3_clean_noisy} show the noisy and clean segments in participant's ECGs in MIT-BIH NST and BUTQDB, respectively. For the rest of the paper, we refer to SD, BUTQDB, MIT-BIH-NST, and AD as D1, D2, D3, and D4, respectively. 

\begin{figure*}[]
    \centering
    % Subfigure (a)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D2_clean.png}
        \caption{Clean ECG signal}
        \label{fig:fig3a}
        \Description{}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D2_noisy_1.png}
        \caption{Noisy ECG signal}
        \label{fig:fig3b}
        \Description{}
    \end{subfigure}
    \hfill
    % Subfigure (c)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D2_noisy_2.png}
        \caption{Another noisy ECG signal}
        \label{fig:fig3c}
        \Description{}
    \end{subfigure}

    \caption{Comparison of ECG signals form D2 : (a) Clean signal, (b) Noisy signal, (c) Another noisy signal.}
    \label{fig:D2_clean_noisy}
\end{figure*}

\begin{figure*}[]
    \centering
    % Subfigure (a)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D3_clean.png}
        \caption{Clean ECG Segment D3}
        \label{fig:fig4a}
        \Description{}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D3_noisy_1.png}
        \caption{Noisy ECG Signal}
        \label{fig:fig4b}
        \Description{}
    \end{subfigure}
    \hfill
    % Subfigure (c)
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/D3_noisy_2.png}
        \caption{Another Noisy Signal}
        \label{fig:fig4c}
        \Description{}
    \end{subfigure}

    \caption{Comparison of ECG signals form D3: (a) Clean signal, (b) Noisy signal, (c) Another noisy signal.}
    \label{fig:D3_clean_noisy}
\end{figure*}

\subsection{ECG segmentation and Filtering}
In this paper, we propose a novel method for noise detection using R-peak-based HRV calculation. To achieve this, we divide the ECG signals into 50\% overlapping 20-second windows, i.e., segments. This window size was chosen empirically, as smaller windows were found to be insufficient for accurate HRV calculations. Using the annotated data, we determined that if at least 50\% of the indices within a window were labeled as noisy, the entire 20-second window was labeled as noisy; otherwise, it was labeled as clean. This 50\% threshold was also selected empirically after testing different percentages of noisy indices. Further, the HRV computation, i.e., feature extraction, was performed on the segment level. The number of clean and noisy segment samples in D1, D2, D3, and D4 is shown in table \ref{tab:label_counts}.

\begin{table}[]
    \centering
    \caption{Number of noisy and clean samples in each dataset used in this study.}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Dataset} & \textbf{\# Clean} & \textbf{\# Noisy} & \textbf{Total} \\         \midrule
        D1& 973 & 163 & 1136 \\
        D2& 77395 & 2644 & 80039 \\
        D3& 19440 & 12780 & 32220 \\
        D4& 128 & 83 & 211 \\
        \bottomrule
    \end{tabular}
    \label{tab:label_counts}
\end{table}

Before feature extraction, we applied standard filtering techniques as a preprocessing step. These techniques primarily address baseline wander, powerline interference, and minor muscle artifacts. However, it does not address motion artifacts and large muscle artifacts. They are ineffective for these noisy artifacts. So, to clean the ECG signals used in this study, we explored popular Python packages for physiological data processing, including HeartPy, BioSPPy, and NeuroKit2. We found that the filtering parameters in the BioSPPy package performed better than the others in cleaning raw ECG signals.
% Before feature extraction, we applied standard filtering techniques as a preprocessing step. These techniques primarily address baseline wander, powerline interference \hl{it does not address this it mainly focus on Muscle artifacts and motion artifactes}, and minor muscle artifacts. However, they are ineffective for highly noisy signals, as shown in Figure X. To clean the ECG signals used in this study, we explored popular Python packages for physiological data processing, including HeartPy, BioSPPy, and NeuroKit2. We found that the filtering parameters in the BioSPPy package performed better than the others in cleaning raw ECG signals.

\subsection{HRV computation}
We performed R-peak detection after obtaining a clean ECG segment through standard filtering. We explored existing literature on methods such as the Hamilton Segmenter and Pan-Tompkins algorithms to identify the most suitable R-peak detection algorithm. After going through the Rpeaks plot, we found that the Hamilton Segmenter algorithm performed best across all the datasets used in this study. Hamilton Segmenter algorithm in BioSPPy also includes an additional step after peak detection, which corrects the detected R-peaks. This ensures that each R-peak is correctly positioned at the actual maximum point within a small window around the initially detected peak. Then, we computed the RR interval (i.e., the time difference between consecutive R-peaks) using these detected R-peaks. Furthermore, with the help of the NeuroKit package, we extracted time-domain HRV features. We focused only on time-domain features because frequency-domain and non-linear HRV indices require larger ECG segments, which would undermine the purpose of noise detection, as noise artifacts are typically short in duration. The computed time domain HRV is shown in Table \ref{tab: computed_features}.

 \begin{table}[h!]
    \centering
    \caption{Computed Time-domain HRV features}
    % \renewcommand{\arraystretch}{1.3} % Adjust row height for better readability
    \begin{tabular}{c}
    \toprule
      \textbf{Features} \\ \midrule
      MeanNN, SDNN, RMSSD, SDSD, CVNN, CVSD, MedianNN, MadNN, \\
       MCVNN, IQRNN, SDRMSSD, Prc20NN, Prc80NN, pNN50, pNN20, MinNN, \\
       MaxNN, HTI, TINN \\ \bottomrule
    \end{tabular}
    \label{tab: computed_features}
  \end{table}

\subsection{Machine Learning}
Following our objective of automating noise and clean ECG segment detection, we implemented classical machine learning models. Specifically, we used Logistic Regression (LR), Random Forest (RF), Support Vector Classifier (SVM), Decision Tree (DT), and Gradient Boosting (GB). We chose Logistic Regression for its simplicity and effectiveness in handling linearly separable data. We included Random Forest and Gradient Boosting for their ensemble learning capabilities, which enhance accuracy and robustness. We selected SVM for its ability to model complex decision boundaries, and we used Decision Tree for its interpretability and computational efficiency.

\subsubsection{Evaluation}
To evaluate our trained classification model, we used 5-fold cross-validation. In this method, the dataset is divided into five equal folds. Four folds are used for training, while the remaining fold is used for testing. This process is repeated for all combinations of folds to ensure a comprehensive evaluation. Next, we assessed the model’s performance using Accuracy, Precision, Recall, F1-score, and Area Under the Precision-Recall Curve (AUPRC). Accuracy measures the percentage of correctly classified samples as noisy or clean. Precision indicates how often the model is correct when predicting a sample as noisy (or clean). Recall measures the percentage of actual noisy (or clean) samples that are correctly classified. The F1 score provides a balance between Precision and Recall. AUPRC evaluates the model’s ability to distinguish between noisy and clean samples by summarizing Precision-Recall performance across different thresholds.

Due to the imbalanced nature of the dataset (see Table \ref{tab:label_counts}), we specifically used the weighted average method (see formulas below) for accuracy, precision, recall, and F1-score to ensure a fair evaluation and prevent the majority class from dominating the results. Moreover, AUPRC is useful for understanding the classification performance in imbalanced datasets. 

\[
\text{Weighted Precision} = \frac{\sum_{i=1}^{n} w_i \cdot \text{Precision}_i}{\sum_{i=1}^{n} w_i}, \quad
\text{Weighted Recall} = \frac{\sum_{i=1}^{n} w_i \cdot \text{Recall}_i}{\sum_{i=1}^{n} w_i}
\]

\[
\text{Weighted F1-Score} = \frac{\sum_{i=1}^{n} w_i \cdot \text{F1-Score}_i}{\sum_{i=1}^{n} w_i}, \quad
\text{Weighted Accuracy} = \frac{\sum_{i=1}^{n} w_i \cdot \text{Accuracy}_i}{\sum_{i=1}^{n} w_i}
\]
 Where $w_i$ is the weight of class $i$ the number of instances in class $i$ (Noisy,i.e., 1 or Clean,i.e., 0).

\section{Results}
We present the classification results for noisy and clean data in two different settings: (i) within-dataset classification and (ii) cross-dataset classification. We now discuss each in detail.

\subsection{Within dataset evaluation}
Table \ref{tab:within_dataset_model_metrics} presents the results of 5-fold cross-validation for within-data classification. We achieved a high accuracy of 96.4\% and an AUPRC of 0.92 on D1 (i.e., SD) using LR. Similar accuracy was observed with RF, SVM, and GB; however, their AUPRC values were lower than that of LR. For D2 (i.e., BUTQDB), the highest accuracy of 99.7\% and an AUPRC of 0.98 were achieved using RF. Similarly, for D3 (i.e., MIT-BIH-NST), RF achieved the highest accuracy of 93.6\% and an AUPRC of 0.97. LR might have performed well on D1 due to its simplicity and effectiveness, particularly with smaller datasets. The dataset D1 contains 1,136 data points with a class ratio of approximately 6:1, making it enough for LR to model effectively. However, Logistic Regression struggled with larger and more complex datasets such as D2 and D3, which contain over 80,000 and 32,000 data points, respectively. Moreover, D2 has a class ratio of nearly 29:1, making it more challenging for LR to perform well. On closer inspection of results in D1, we found that RF achieved only 0.03\% lower accuracy and 0.02 lower AUPRC than LR. Given the strong overall performance of the RF Classifier within the dataset, we selected it for generalizability testing on the unseen dataset (i.e., cross-generalizability).

\begin{table}[ht]
\centering
\caption{Within dataset classification results}
\begin{tabular}{l l c c c c c}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\ 
\midrule
 & \textbf{D1}  & \textbf{0.964} & \textbf{0.920} & \textbf{0.822} & \textbf{0.867} & \textbf{0.918} \\
Logistic Regression (LR) & D2  & 0.995 & 0.917 & 0.947 & 0.931 & 0.968 \\
 & D3  & 0.912 & 0.876 & 0.906 & 0.891 & 0.944 \\
\midrule
 & D1  & 0.944 & 0.800 & 0.809 & 0.804 & 0.675 \\
Decision Tree (DT) & D2  & 0.995 & 0.914 & 0.929 & 0.921 & 0.851 \\
 & D3  & 0.917 & 0.906 & 0.882 & 0.894 & 0.846 \\
\midrule
 & D1  & 0.961 & 0.906 & 0.816 & 0.858 & 0.916 \\
Random Forest (RF) & \textbf{D2}  & \textbf{0.997} & \textbf{0.943} & \textbf{0.955} & \textbf{0.949} & \textbf{0.984} \\
 & \textbf{D3}  & \textbf{0.936} & \textbf{0.911} & \textbf{0.929} & \textbf{0.920} & \textbf{0.976} \\
\midrule
 & D1  & 0.960 & 0.922 & 0.791 & 0.851 & 0.892 \\
Support Vector Machine (SVM) & D2  & 0.996 & 0.941 & 0.936 & 0.938 & 0.972 \\
 & D3  & 0.924 & 0.894 & 0.918 & 0.906 & 0.959 \\
\midrule
 & D1  & 0.960 & 0.894 & 0.822 & 0.856 & 0.907 \\
Gradient Boosting (GB) & D2  & 0.996 & 0.940 & 0.950 & 0.945 & 0.971 \\
 & D3  & 0.927 & 0.902 & 0.916 & 0.909 & 0.968 \\
\bottomrule
\end{tabular}
\label{tab:within_dataset_model_metrics}
\end{table}

\subsection{Cross dataset evaluation}
For cross-dataset generalizability, we trained and tested models on individual datasets from {D1, D2, D3}.  Specifically, we trained on D1 and tested on D2 and D3, trained on D2 and tested on D1 and D3, and trained on D3 and tested on D1 and D2. Additionally, we explored the effect of combining two datasets (cross-combined) for training and testing on the remaining dataset, such as training on D1+D2 and testing on D3. Similarly, we evaluated the models on other combinations. 

Table \ref{tab:cross-dataset-performance} presents the results for training one dataset and testing on another dataset. We found that the model trained on D1 performed better when tested on D2, achieving an accuracy of 99\% and an AUPRC of 0.945. However, when tested on D3, the accuracy dropped to 80.8\%. When trained on D2, the highest accuracy was 91.3\% on D1, but only 82.9\% on D3. In contrast, training on D3 resulted in the highest accuracy of 93.9\% and 98.9\% when tested on D1 and D2, respectively. From Table \ref{tab:cross-dataset-performance}, we observed an interesting pattern: models trained and tested on real noisy datasets (D1 and D2) generalized well to each other but did not perform well on D3. On the other hand, training on D3 led to the best performance on both D1 and D2. This may be because D3 contains a larger number of noisy samples, allowing the model to learn noisy patterns more effectively.

\begin{table}[h!]
\centering
\caption{Cross dataset classification results.}
\begin{tabular}{l l c c c c c} 
\toprule
\textbf{Train} & \textbf{Test} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\ \midrule
\textbf{D1} & \textbf{D2} & \textbf{0.990} & \textbf{0.992} & \textbf{0.990} & \textbf{0.991} & \textbf{0.945} \\
D1   & D3   & 0.808 & 0.853 & 0.808 & 0.809 & 0.896 \\ \midrule
D2 & D1   & 0.913 & 0.919 & 0.913 & 0.896 & 0.824 \\
\textbf{D2} & \textbf{D3} & \textbf{0.829} & \textbf{0.843} & \textbf{0.829} & \textbf{0.821} & \textbf{0.893} \\ \midrule
D3   & D1   & 0.939 & 0.939 & 0.939 & 0.939 & 0.873 \\
\textbf{D3} & \textbf{D2} & \textbf{0.989} & \textbf{0.991} & \textbf{0.989} & \textbf{0.990} & \textbf{0.942} \\ \midrule
\multicolumn{2}{c}{\textbf{\textit{Average}}}  & \textbf{0.911} & \textbf{0.923} & \textbf{0.911} & \textbf{0.908} & \textbf{0.895} \\
\bottomrule
\end{tabular}
\label{tab:cross-dataset-performance}
\end{table}


% \begin{table}[h!]
% \centering
% \caption{Cross dataset classification results.}
% \begin{tabular}{l l c c c c c} 
% \toprule
% \textbf{Train} & \textbf{Test} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\ \midrule
% \textbf{D1} & \textbf{D2} & \textbf{0.990} & \textbf{0.992} & \textbf{0.990} & \textbf{0.991} & \textbf{0.945} \\
% D1   & D3   & 0.808 & 0.853 & 0.808 & 0.809 & 0.896 \\ \midrule
% D2 & D1   & 0.913 & 0.919 & 0.913 & 0.896 & 0.824 \\
% \textbf{D2} & \textbf{D3  } & \textbf{0.829} & \textbf{0.843} & \textbf{0.829} & \textbf{0.821} & \textbf{0.893} \\ \midrule
% D3   & D1   & 0.939 & 0.939 & 0.939 & 0.939 & 0.873 \\
% \textbf{D3} & \textbf{D2} & \textbf{0.989} & \textbf{0.991} & \textbf{0.989} & \textbf{0.990} & \textbf{0.942} \\
% \bottomrule
% \end{tabular}
% \label{tab:cross-dataset-performance}
% \end{table}

Table \ref{tab:cross_combined} presents the results for training on combined datasets and testing on the remaining dataset. We found that training on D1 and D2 together and testing on D3 achieved an accuracy of 86.5\%, which was higher than training individually on D1 or D2 and testing on D3 (see Table \ref{tab:within_dataset_model_metrics}). This suggests that combining D1 and D2 helped the model learn the patterns of noisy and clean data more effectively. A similar trend was observed when training on combined D2 and D3 and testing on D1, where the accuracy improved compared to individual training and testing on D1. However, when training on combined D1 and D3 and testing on D2, the results were similar to individual training and testing on D2, achieving an accuracy of 99\%.

\begin{table}[ht]
\centering
\caption{Classification results on cross combined datasets}
\begin{tabular}{l l c c c c c}
\toprule
\textbf{Train} & \textbf{Test} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\
\midrule
D2+D3 & D1 & 0.952 & 0.951 & 0.952 & 0.949 & 0.899 \\
D1+D3 & D2 & 0.992 & 0.993 & 0.992 & 0.992 & 0.943 \\
D1+D2 & D3 & 0.865 & 0.866 & 0.865 & 0.863 & 0.904 \\ \midrule
\multicolumn{2}{c}{\textbf{\textit{Average}}} & \textbf{0.936} & \textbf{0.937} & \textbf{0.936} & \textbf{0.935} & \textbf{0.915} \\
\bottomrule
\end{tabular}
\label{tab:cross_combined}
\end{table}


% \begin{table}[ht]
% \centering
% \caption{Classfication results on cross combined datasets}
% \begin{tabular}{l l c c c c c}
% \toprule
% \textbf{Train} & \textbf{Test} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\
% \midrule
% %D1 & D2+D3 & 0.937 & 0.953 & 0.937 & 0.941 & 0.901\\
% D2+D3 & D1& 0.952 & 0.951 & 0.952 & 0.949 & 0.899 \\
% %D2 & D1+D3 & 0.815 & 0.832 & 0.815 & 0.805 & 0.884\\
% D1+D3 & D2& 0.992 & 0.993 & 0.992 & 0.992 & 0.943 \\
% %D3 & D1+D2 & 0.990 & 0.991 & 0.990 & 0.990 & 0.936 \\
% D1+D2 & D3& 0.865 & 0.866 & 0.865 & 0.863 & 0.904 \\
% \bottomrule
% \end{tabular}
% \label{tab:cross_combined}
% \end{table}

\subsection{Ablation study}
Apart from within-dataset and cross-dataset generalizability testing, we also conducted an ablation study using a separate dataset containing muscle activity (see Section \ref{section: dataset} for details). This ablation study aimed to assess the robustness of our method. In this approach, we trained the model on individual datasets D1, D2, and D3 and tested it on the new D4, i.e., AD dataset. This allowed us to evaluate the model's performance on unseen data, providing insight into the generalizability of the noise detection model. Table \ref{tab:D4_testing} presents the results for training on individual and combined datasets from {D1, D2, D3} and testing on the D4. We found that the models trained on each dataset generalized well to AD, achieving the highest accuracy of 88.2\%.

\begin{table}[ht]
\centering
\caption{Classification results on training on individual and combined datasets from {D1, D2, D3} and testing on D4}
\begin{tabular}{l l c c c c c}
\toprule
\textbf{Train}  & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\
\midrule
D1         & 0.877 & 0.887 & 0.877 & 0.873 & 0.896 \\
D2       & 0.758 & 0.808 & 0.758 & 0.729 & 0.876 \\
D3         & 0.867 & 0.867 & 0.867 & 0.866 & 0.919 \\
D2+D3  & 0.863 & 0.883 & 0.863 & 0.856 & 0.940 \\
D1+D2  & 0.829 & 0.861 & 0.829 & 0.817 & 0.915 \\
D1+D3    & 0.882 & 0.888 & 0.882 & 0.878 & 0.933 \\
D1+D2+D3         & 0.863 & 0.883 & 0.863 & 0.856 & 0.940 \\
\bottomrule
\end{tabular}
\label{tab:D4_testing}
\end{table}

\subsection{Implication}
Our study is the first to assess the generalizability of an automated noise detection model. Our findings highlight that, regardless of the ECG collection device or types of noise, an ML-based noise detection model can accurately identify noisy segments. This work has practical applications in real-time ECG monitoring, where noisy segments can be automatically discarded. Additionally, it can benefit AI/ML-based CVD or mental disorder prediction by automatically dropping noisy data, thereby improving the accuracy of predictions and classifications.

\section{Conclusion}
In this work, we proposed a novel method for noise detection in ECG signals using HRV features extracted from the ECG signal. To evaluate the generalizability of our approach, we conducted experiments on four different datasets, each containing varying sources of ECG data and different types of noise. We trained machine learning models to distinguish between noisy and clean ECG segments based on their HRV patterns. Our results demonstrated that, regardless of the ECG data source or the nature of the noise, the models were highly generalizable, achieving an accuracy of over 90\% in classifying noisy and clean segments. These findings suggest that our method can effectively identify and filter out noisy ECG segments, making it suitable for real-world applications such as real-time ECG monitoring and AI/ML-based cardiovascular and mental disorder prediction.

%\end{comment}
\bibliographystyle{acm}
\bibliography{citations}

\end{document}

\section{Methodology}
\subsection{Data Labeling}\label{Data_labeling}\hl{}
 
    An ideal ECG signature has P wave, PR interval , QRS complex ST segment and T wave. For labeling the data we specifically focus on QRS complex and R-peaks. A data is labeled is noisy if the R-peaks are not clearly visible for noisy we use label \textbf{1} and for clean or not noisy \textbf{0}. Figure \ref{fig:ecg_signature} shows the signature of an ideal ECG signal. \hl{add the figure with proper annotations}.

  
    The Private dataset is labeled manually using two annotators which labeled the whole data point to point. For annotation have used a open source labeling Software \textbf{'Trainset'}, we marked  the data segment as \textbf{'1'} (noisy) if there are no clear R-peaks then the data from where the previous visible R-peak end till the next clear R-peak is found and \textbf{'0'} (not noisy or clean) otherwise.
    The above cases apply when point-to-point labeling is done.  For 10 min activity dataset, we followed the similar approach. 
    
    For BUTQDB database we have three classes , class 1 is clean signal , class 2 have has some noise but R-peaks are still visible  so these two classes are marked as not noisy (0) where as class 3 has no clear R-peaks it is marked as noisy (1). In MIT BHI NST database the first 5 minutes are clean so labeled as 0 , then for alternating 2 minutes segment is labeled as 1 (noisy) and the other 2 minute as 0 not noisy.

    For rest of the paper, we refer Private data, BUTQDB, and MIT-BIH-NST, and 10 min activity dataset as D1, D2, D3 and D4 respectively.
   
 \subsection{Filtering and R-peaks Detection} 
    We explored various open source python packages for analysis of ECG signal like HeartPy, Biosppy and Neurokit2 we used Biosppy and Neurokit2 in this work. Each has liblary has its own advantage in some way, NeuroKit2 applies a bandpass Butterworth filter (0.5 Hz to 50 Hz), along with outlier removal and for comprehensive cleaning of the ECG signal. Similarly, Biosppy also uses a bandpass Butterworth filter (3 Hz to 45 Hz), but includes an additional notch filter at 50 Hz to remove power line interference. We specifically use Biosppy as it focuses more on signal segmentation and feature extraction, especially for tasks like R-peak detection.
 
    \begin{figure}[htbp]
        \centering
        \begin{minipage}{0.45\linewidth}
            \centering
            \includegraphics[width=\linewidth]{Sharmad/Filter_comparison_1_final.jpg}
            \caption{In neurokit2 for red box the T-wave is enlarge and can be misclassified as R-peak}
            \label{fig:filter-comparison}
        \end{minipage}%
        \hspace{0.05\linewidth}  % Adjust the space between the images
        \begin{minipage}{0.45\linewidth}
            \centering
            \includegraphics[width=\linewidth]{Sharmad/R-peaks_comparison1_final.jpg}
            \caption{In neurokit2 the is no R-peak in the section but it marks the point as R-peak}
            \label{fig:rpeaks-comparison}
        \end{minipage}
    \end{figure}
    
    For R-peaks detection Neurokit2 have many algorithms like neurokit(its own) hamilton2002, pantompkins1985 and etc. where as Biosppy which use hamilton\_segmentor, but it has additional fuction correct\_rpeaks the function corrects the positions as ,instead of just trusting the initial detected R-peak positions by hamilton\_segmentor, the function ensures that the R-peaks are located at the actual maximum point within a small window around each initial R-peak.
 
    \subsection{Feature extraction}  
     We get the filtered ECG signal and the R-peaks using biopsy. R-peaks are used for Heart Rate variability (HRV)  based feature extraction. Both Biosppy and Neurokit2 has feature extraction function in it but neurokit gives more features and supports HRV feature across various domains as compared to Biosppy. Thus we use Neurokit2 for HRV feature extarction from 20 seconds segment.
     
     In Neurokit2 we can extract HRV features from three domain namely time domain , frequency domain and nonlinear domain. Frequency and nonlinear need a long duration of ECG data to extract features effective and even many of features can not be extracted for 20 seconds segment.
     \begin{table}[h!]
        \centering
        \renewcommand{\arraystretch}{1.3} % Adjust row height for better readability
        \begin{tabular}{ll}
        \hline
        \textbf{Time Domain Feature} &  \textbf{Features} \\
        \hline
        \textbf{Accepted} &  MeanNN, SDNN, RMSSD, SDSD, CVNN, CVSD, MedianNN, MadNN, \\
         &  MCVNN, IQRNN, SDRMSSD, Prc20NN, Prc80NN, pNN50, pNN20, MinNN, \\
         &  MaxNN, HTI, TINN \\
        \hline
        \end{tabular}
        \caption{Accepted and Dropped Time Domain Features}
      \end{table}

     
     The time domain features are used, it has twenty-five features in total out of which we use 19 features. Drop the reaming 6 features, as these features need at least one minute of ECG data.\\
 
    \subsection{Segmentation}
    ECG signal is taken as input, then it is divided into 20 seconds overlapping segments. We tried to segment with different windows like 5 seconds and 10 seconds, the minimum is the window the better is accuracy to detect and localize the noise. The main problem faced with 5 seconds and 10 seconds is the number of R-peaks in the segment, they were not sufficient to get the accurate HRV features which lead to wrong classification of the segment. While the 20 seconds segment had enough number of R-peaks to get HRV feature. After segmenting data we apply condition to mark whole as segment as noisy or not noisy. We use threshold of 50\% i.e. if more that 50\%data in segment is labeled as noisy then the segemnt is marked as noisy (1) otherwise not-noisy (0). Reason to choose this threshold is, if it is less than 50\% 
    it still has 60\% of data which is not-noisy but still labeled as noisy which leads to miss-classification. 
        \begin{figure}[h!]
        \centering
        \includegraphics[width=0.75\linewidth]{Sharmad/ECG_overlap_reason.pdf}
        \caption{The red box shows the noisy ECG Signal which is lost due to not overlapping. }
        \label{fig:overlap_reason}
    \end{figure}

    But know if there is a case that if one segment has less some noise but it is less then 50\% and the next frame has some noise which is also less than 50\% as in Figure \ref{fig:overlap_reason} those noisy ECG signal get classified as not noisy. For this we use an overlapping window, having overlap of 50\%, it helps in prevention of loss of ECG signal and it also solve the problem of missing R-peaks at boundaries of segment. 
    %We tried with overlapping window greater than 50\%    However, after segmenting data into 20-second segment, a single data point with a feature is labeled as 1 (noisy) if more than 50\% of the data points in the segment are labeled as 1 (noisy); otherwise, it is labeled as 0 (not-noisy). \hl{recheck it}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{Sharmad/wrong_R-peak_detection.jpg}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\subsection{Machine Learning}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{Sharmad/Schematic_of methodology.pdf}
    \caption{Schematic Representation of the whole model.}
    \label{fig:Schematic_of methodology}
\end{figure}
Figure \ref{fig:Schematic_of methodology} shows the architecture of our model. In this ECG data are passed as whole signal, it is segmented into 20 seconds, 50\% overlapping window.
Then this signal is filtered and R-peaks(indices) are extracted using Biosppy. These R-peaks are used for HRV based feature extraction Using Neurokit2 to get 25 time domain features out of which 6 are dropped and we use remaining 19 features for training the machine learning model and classifying noisy and not-noisy ECG segment.

For our binary classification problem, we employed Logistic Regression, Random Forest, Support Vector Classifier (SVM), Decision Tree, and Gradient Boosting. Logistic Regression was chosen for its simplicity and effectiveness in linear separability. Random Forest and Gradient Boosting were included for their ensemble learning capabilities, improving accuracy and robustness. SVM was selected for its ability to handle complex decision boundaries. Decision Tree was used for its interpretability and efficiency. 

\hl{improve it}
%To train model we have three datasets, we extract features using 20 seconds overlapping window. The window should be minimum as it will decrease chances of wrongly detecting labels as noisy, various window sizes were tried like 5 and 10 seconds but found that for short duration many features cannot be extracted as there are no sufficient R-peaks.A 10-second overlapping window is used to address the issue of missing R-peaks at the endpoints as Biosppy during detection.This window size also ensures that noisy labels are preserved to a certain extent, preventing their complete loss.

%For this window Biosppy 'ecg' function is used for filtering and obtaining R-peak indices. Using NeuroKit2, a total of 19 HRV-based time domain features are extracted from the R-peaks and sampling rate, namely HRV\_MeanNN, HRV\_SDNN, HRV\_RMSSD, HRV\_SDSD, HRV\_CVNN, HRV\_CVSD, HRV\_MedianNN, HRV\_MadNN, HRV\_MCVNN, HRV\_IQRNN, HRV\_SDRMSSD, HRV\_Prc20NN, HRV\_Prc80NN, HRV\_pNN50, HRV\_pNN20, HRV\_MinNN, HRV\_MaxNN, HRV\_HTI, and HRV\_TINN. The segment is labeled as noisy (1) if 50\% or more of the data in the segment is noisy.



%We use segment of 20 seconds, as it is optimal for feature extraction if we take shorter time segment then the features are not accurate and if we take longer it will result in the loss of noisy labels. 50\% overlapping segments are taken so that the R-peaks at the endpoints are not missed and the noisy labels are not lost. \\
%Segmented data is filtered and R-peaks are extracted, these R-peaks are used to get HRV based features and the segment is labeled as noisy is more than 50\% of data is noisy (1) otherwise not noisy (0). \\
\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.3} % Adjust row height
    \setlength{\tabcolsep}{12pt} % Adjust column width
    \begin{tabular}{lccc}
        \toprule
        \textbf{Dataset} & \textbf{0's} & \textbf{1's} & \textbf{Total} \\
        \midrule
        D1& 973 & 163 & 1136 \\
        D2& 77395 & 2644 & 80039 \\
        D3& 19440 & 12780 & 32220 \\
        D4& 128 & 83 & 211 \\
        \bottomrule
    \end{tabular}
    \caption{Label counts for each dataset}
    \label{tab:label_counts}
\end{table}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Sharmad/Dataset_comparison_with_labels .png}
    \caption{Count of lables in dataset}
    \label{fig:dataset_label_count}
\end{figure}


\subsection{Evaluation}
We focus on predicating the noisy segment (1) but from ~\ref{fig:dataset_label_count} it is clearly visible that there is data imbalance, noisy labels are much less than those not noisy in the case of BUTQDB and private dataset. To ensure a fair evaluation and prevent the majority class from dominating the results, we use the weighted average method, which adjusts for class distribution.
%Our goal is on prediction of noisy label we would use micro-average method keeping our positive labels as 1, so we have TP (true positive) as number of data points which are actually noisy and predicted correctly as noisy, TN (true negative) as number of data points which are actually not-noisy and predicted correctly as not-noisy, FP (false positive) number of data points which are actually not-noisy but predicted as noisy and FN(false negative) number of data points which are actually noisy but predicted as not-noisy.We have Precision, Recall, F1-Score and Accuracy.
\[
\text{Weighted Precision} = \frac{\sum_{i=1}^{n} w_i \cdot \text{Precision}_i}{\sum_{i=1}^{n} w_i}, \quad
\text{Weighted Recall} = \frac{\sum_{i=1}^{n} w_i \cdot \text{Recall}_i}{\sum_{i=1}^{n} w_i}
\]

\[
\text{Weighted F1-Score} = \frac{\sum_{i=1}^{n} w_i \cdot \text{F1-Score}_i}{\sum_{i=1}^{n} w_i}, \quad
\text{Weighted Accuracy} = \frac{\sum_{i=1}^{n} w_i \cdot \text{Accuracy}_i}{\sum_{i=1}^{n} w_i}
\]
 Where $w_i$ is the weight of class $i$ the number of instances in class $i$ (0 or 1).

\textbf{\Large AUPRC} (Area Under the Precision-Recall Curve) is a key scoring standard used in this evaluation. 
It measures the trade-off between precision and recall across various threshold levels. A higher AUPRC value indicates 
better performance, as it reflects both the model's ability to retrieve relevant instances (precision) and its sensitivity 
to identifying all positive instances (recall).

From Table~\ref{tab:label_counts}, we observe that the data is highly imbalanced, with noisy labels being very sparse in 
the D1 and D2. Therefore, using AUPRC as a scoring metric help evaluate the models effectively under these conditions, ensuring the models prioritize both recall and precision to handle the imbalance.


\section{Results}
\subsection{Within dataset evaluation}\label{Within dataset evaluation}
We used various machine learning algorithms such as Logistic Regression, Random Forest Classifier, SVM, Decision Tree, and Gradient Boosting.Table \ref{tab:model_metrics} shows results of 5 fold Cross Validation, it is evident that \textbf{Logistic Regression} has AUPRC of \textbf{0.918} performing better for D1 dataset whereas \textbf{Random Forest Classifier} performed the best for D2 and D3 with AUPRC of \textbf{0.984} \& \textbf{0.944} respectively.

\begin{comment}
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{Sharmad/Classifier_comparison_RF_LR.pdf}
    \caption{Comparison of Random Forest and Logistic Regression across three dataset \hl{show results of all algos in this bar plot. Then the table will go away.}}
    \label{fig:RF_LR_comparison}
\end{figure}
\end{comment}

\noindent Logistic Regression performs well on the Private dataset due to its simplicity and effectiveness in scenarios where the dataset is smaller as in case of Private dataset, with 1136 data points. And a class ratio of approximately 6:1, is small enough for Logistic Regression to model effectively without overfitting. However, Logistic Regression struggles with the larger and more complex dataset like D2 and D3 datasets, which contain over 80,000 and 32,000 data points, respectively, and BUTQDB has class ratio of almost 29:1 which is highly imbalance. These datasets likely involve non-linear relationships due to its complex nature and intricate feature interactions that Logistic Regression cannot model, leading to less performance.

Random Forest, on the other hand, outperforms Logistic Regression across all datasets, excluding D1, due to its ability to handle complex, non-linear relationships and large feature spaces effectively. It is robust to noise, outliers, and class imbalance, due to its ensemble nature. It even has good performance (not better than Logistic Regression) on the D1 dataset as it is flexible on smaller datasets without overfitting, making it model for this model. Its consistently high AUPRC across all the dataset.

Thus we use \textbf{Random Forest Classifier} as classification algorithm as it ensures that it generalizes well even on larger datasets like D2 and D3, also on small datasets like D1.

\begin{table}[ht]
\centering
\begin{tabular}{l l c c c c c}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\ 
\midrule
 & \textbf{D1}  & \textbf{0.964} & \textbf{0.920} & \textbf{0.822} & \textbf{0.867} & \textbf{0.918} \\
Logistic Regression & D2  & 0.995 & 0.917 & 0.947 & 0.931 & 0.968 \\
 & D3  & 0.912 & 0.876 & 0.906 & 0.891 & 0.944 \\
\midrule
 & D1  & 0.944 & 0.800 & 0.809 & 0.804 & 0.675 \\
Decision Tree & D2  & 0.995 & 0.914 & 0.929 & 0.921 & 0.851 \\
 & D3  & 0.917 & 0.906 & 0.882 & 0.894 & 0.846 \\
\midrule
 & D1  & 0.961 & 0.906 & 0.816 & 0.858 & 0.916 \\
Random Forest & \textbf{D2}  & \textbf{0.997} & \textbf{0.943} & \textbf{0.955} & \textbf{0.949} & \textbf{0.984} \\
 & \textbf{D3}  & \textbf{0.936} & \textbf{0.911} & \textbf{0.929} & \textbf{0.920} & \textbf{0.976} \\
\midrule
 & D1  & 0.960 & 0.922 & 0.791 & 0.851 & 0.892 \\
SVM & D2  & 0.996 & 0.941 & 0.936 & 0.938 & 0.972 \\
 & D3  & 0.924 & 0.894 & 0.918 & 0.906 & 0.959 \\
\midrule
 & D1  & 0.960 & 0.894 & 0.822 & 0.856 & 0.907 \\
Gradient Boosting & D2  & 0.996 & 0.940 & 0.950 & 0.945 & 0.971 \\
 & D3  & 0.927 & 0.902 & 0.916 & 0.909 & 0.968 \\
\bottomrule
\end{tabular}
\caption{Performance Metrics of Different Algorithms on Various Datasets}
\label{tab:model_metrics}
\end{table}
\subsection{Cross dataset evaluation}
\hl{take a pass of it.}
We did within dataset evaluation in which Random Forest Classifier gave the optimal results considering all the dataset, but to show genralizibility we need to show that this methods works even on cross data evaluation. In this we trained model on one data and test on other two. 

Table \ref{tab:model_metrics} show that when model is trained on D1 it works better for D2 than D3 with AUPRC score of \textbf{0.945}, when trained on D2 it works better for D3 than D2 with AUPRC score of \textbf{0.893} and for D3 it works better for D2 than D1 with AUPRC score of \textbf{0.942}

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength{\tabcolsep}{12pt} % Adjust column width
\begin{tabular}{l l c c c c c} 
\toprule
\textbf{Train Dataset} & \textbf{Test Dataset} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\ 
\midrule
\textbf{D1} & \textbf{D2} & \textbf{0.990} & \textbf{0.992} & \textbf{0.990} & \textbf{0.991} & \textbf{0.945} \\
D1   & D3   & 0.808 & 0.853 & 0.808 & 0.809 & 0.896 \\
D2 & D1   & 0.913 & 0.919 & 0.913 & 0.896 & 0.824 \\
\textbf{D2} & \textbf{D3  } & \textbf{0.829} & \textbf{0.843} & \textbf{0.829} & \textbf{0.821} & \textbf{0.893} \\
D3   & D1   & 0.939 & 0.939 & 0.939 & 0.939 & 0.873 \\
\textbf{D3} & \textbf{D2} & \textbf{0.989} & \textbf{0.991} & \textbf{0.989} & \textbf{0.990} & \textbf{0.942} \\
\bottomrule
\end{tabular}
\caption{Performance metrics of models across train and test datasets, including AUPRC.}
\label{tab:cross-dataset-performance}
\end{table}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Sharmad/cross_validate_all_inone (1).pdf}
    \caption{Comparison of matrix score on different training and testing data.}
    \label{fig:cross_data_comaparison}
\end{figure}

To assess the robustness of our method across diverse datasets, we adopted a combined dataset approach. This approach involves training the model on two datasets and testing it on a third, enabling us to evaluate its performance on a heterogeneous data pool. By doing so, we can effectively measure the model's generalizability and performance across varied data characteristics. As shown in Table \ref{tab:Train and tested of combined cross datasets} we get best AUPRC score of \textbf{0.943} when model is trained on D1+D3 and tested on D2.
\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength{\tabcolsep}{12pt} % Adjust column spacing
\begin{tabular}{l l c c c c c}
\toprule
\textbf{Train Dataset} & \textbf{Test Dataset} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\
\midrule
%D1 & D2+D3 & 0.937 & 0.953 & 0.937 & 0.941 & 0.901\\
D2+D3 & D1& 0.952 & 0.951 & 0.952 & 0.949 & 0.899 \\
%D2 & D1+D3 & 0.815 & 0.832 & 0.815 & 0.805 & 0.884\\
D1+D3 & D2& 0.992 & 0.993 & 0.992 & 0.992 & 0.943 \\
%D3 & D1+D2 & 0.990 & 0.991 & 0.990 & 0.990 & 0.936 \\
D1+D2 & D3& 0.865 & 0.866 & 0.865 & 0.863 & 0.904 \\

\bottomrule
\end{tabular}
\caption{Train and tested on cross combined datasets}
\label{tab:Train and tested of combined cross datasets}
\end{table}

We had created one more private dataset D4 which is specifically made for validation of the model, to show the generalizability of our method we train the model on all possible combination of existing dataset and test on D4. Table \ref{tab:Tested of 10_min_Activity dataset by traing on all dataset.} shows that D2+D3 and D1+D2+D3+D4 has highest AUPRC score of \texttbf{0.940}.

\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.5} % Adjust row height
\setlength{\tabcolsep}{10pt} % Adjust column spacing
\begin{tabular}{l l c c c c c}
\toprule
\textbf{Train Dataset}  & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUPRC} \\
\midrule
D1         & 0.877 & 0.887 & 0.877 & 0.873 & 0.896 \\
D2       & 0.758 & 0.808 & 0.758 & 0.729 & 0.876 \\
D3         & 0.867 & 0.867 & 0.867 & 0.866 & 0.919 \\
D2+D3  & 0.863 & 0.883 & 0.863 & 0.856 & 0.940 \\
D1+D2  & 0.829 & 0.861 & 0.829 & 0.817 & 0.915 \\
D1+D3    & 0.882 & 0.888 & 0.882 & 0.878 & 0.933 \\
D1+D2+D3         & 0.863 & 0.883 & 0.863 & 0.856 & 0.940 \\
\bottomrule
\end{tabular}
\caption{Tested of D4 dataset by training on all dataset.}
\label{tab:Tested of D4 dataset by traing on all dataset.}
\end{table}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Sharmad/t-sne_all_dataset_1.pdf}
    \caption{t-sne plot of all three dataset to prove genralizibility}
    \label{fig:t-sne_plot_all_dataset}
\end{figure}

\section{Limitations and Future Work}
\hl{fill it}
In this work we made method to built a model to detect noisy segment which is of 20 seconds due to which a lot of clean ECG is classified into noisy data thus there is loss of clean ECG signal. For large dataset like D2 and D3 to extract HRV features is is computational expensive and takes a lot of time. Further we can built a model which can detect a smaller segment of signal to avoid loss of signal.

\section{Conclusion}
Moreover, the proposed method outperforms conventional approaches by demonstrating strong generalization across datasets. It consistently achieves high accuracy, precision, recall, and AUPRC across different training and testing scenarios. This suggests that it can adapt to new datasets without requiring dataset-specific modifications or re-training.

\section{Discussion}
\subsection{Within data analysis} 
We can observe from Table \ref{tab:model_metrics} that logistic regression worked best for D1 whereas for D2 and D3, Random Forest Classifier worked best, and we used Random Forest Classifier as explained in Section \ref{Within dataset evaluation}. All results are averaged over a 5-fold cross-validation, ensuring robustness and minimizing variability in performance evaluation. In previous model like RecogNoise\cite{aminifar2024recognoise} using machine learning to detect noisy segments in dataset D3 with an F1-score of 86.9\% and an accuracy of 88.3\%(taking into consideration that they used all SNR levels of noise for detection and considered Baseline wander). At the same time, our proposed method has an accuracy of 93.6\% and F1-score of 92.0\%. For D2 the previous deep learning algorithm in \cite{dua2022automatic} has AUPRC of $94.25 \pm 1.13$\% on Level 3 of noise that we have considered as noise in our case as explained in Section \ref{Data_labeling} where we have achieved AUPRC of 98.4\% on the Random Forest classifier which has less computation cost compared to the deep learning algorithm.

The above results also show that this method improved results across D2 and D3 compared to previous works. The consistent improvements observed across different datasets highlight the approach's robustness and reliability in handling diverse noise conditions. The D1 dataset is small and highly class imbalanced but still gives high AUPRC score of 91.6\% using Random forest which shows that [it is effective feature selection which are able to classify the segments]. 

\subsection{Cross-Dataset analysis}\label{Cross-Dataset analysis}
There is no prior work that has employed cross-dataset evaluation for this task. Existing studies primarily focus on within-dataset performance, limiting their ability to assess generalizability across different data distributions. In contrast, our approach leverages cross-dataset evaluation, demonstrating its ability to maintain high performance across datasets with varying noise signal characteristics. Our study is distinct in that it utilizes datasets containing both naturally occurring noise D1 and D2 and artificially induced noise D3, providing a more rigorous assessment of model robustness. Evaluating performance across these diverse noise conditions ensures that the proposed method is not biased towards a specific type of noise but can generalize effectively to real-world ECG signals, which are subject to a wide range of artifacts. The combination of cross-dataset validation and mixed-noise evaluation establishes the reliability. In Table\ref{tab:cross-dataset-performance} we get the lowest AUPRC score 82.4\% when we train on D2 and test on D1, this is because D1 is very small dataset there are very less minority class (noisy,1), AUPRC is sensitive to Recall even small misclassifications can significantly drop AUPRC, similarly when train on D3 tested on D1 AUPRC of 87.3\% it is better than above as D3 has good ratio of class specifically more minority classes (noisy,1). The highest AUPRC value of 94.5\% when training on D1 and testing on D2.

We plotted the t-SNE plot for all three datasets as shown in Figure \ref{fig:t-sne_plot_all_dataset}. T-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique that helps visualize high-dimensional data in a lower-dimensional space (typically 2D) while preserving the local structure of the data. From Figure \ref{fig:t-sne_plot_all_dataset} we can see that D3 is spread across the whole plot, D2 is densely located at the center of the plot and D1 almost overlaps on D2 it means it has same feature dependence. These information also proves that like D1 is subset of D2 thus we get good AUPRC when we train on D1 and test on D2.

To show that this method works robustly, we added two datasets trained and tested on the third dataset as shown in Table \ref{tab:Train and tested of combined cross datasets} 

\subsection{Validating on D4}
We had specifically created dataset to validate our model on, we trained our model on every possible combination possible using D1, D2 and D3 and tested on D4. As we can see in Table \ref{tab:Tested of 10_min_Activity dataset by traing on all dataset.} we get highest AUPRC of 94.0\% in two cases train on D2+D3 and train on D1+D2+D3, this is because D1 is very small and does not affect the overall training distribution significantly when combined with the larger datasets (D2 and D3). D2 and D3 contain a more diverse set of features and noise variations, which helps the model generalize better to unseen data in D4. It gives lowest on D2 as same reason mentioned in Section \ref{Cross-Dataset analysis} for testing on D1. 

From Table\ref{tab:Tested of D4 dataset by traing on all dataset.} our method maintains high performance across different training conditions, effectively learning from diverse datasets while preserving classification accuracy and robustness. This approach achieves competitive results, with AUPRC consistently remaining above 87.6\% in all cases, indicating that the model can extract meaningful features from various data distributions and still perform reliably on unseen test data. 

\clearpage
%\begin{comment}
\section{Literature}
\begin{enumerate}

\item \href{https://link.springer.com/chapter/10.1007/978-3-031-37742-6_49}{Automatic Detection of Noisy Electrocardiogram Signals Without Explicit Noise Labels}

\item \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884975}{Automated ECG Noise Detection and Classification System for Unsupervised
Healthcare Monitoring}:[2018] This paper proposes a framework for automatic
detection, localization and classification of single and combined
ECG noises.  \hl{Ad: check it}

\item \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752361}{Noise Detection in Electrocardiogram Signals
for Intensive Care Unit Patients}:[2019] They developed an algorithm to automatically detect noises
in the long-term ICU ECG recordings from the MIMIC III
database. \hl{Ad: check it}

\item \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6689506/}{Deep Learning-Based Electrocardiogram Signal Noise Detection and Screening Model}:[2019] They used 1D-CNN to screen unacceptable electrocardiograms that include noise.

\item \href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0279305}{Cloud-based healthcare framework for realtime anomaly detection and classification of
1-D ECG signals}: Authors used \textbf{LSTM based auto-encoder} for anomaly detection in ECG signals. 

\item \href{https://dl.acm.org/doi/pdf/10.1145/3517247}{BayesBeat: Reliable Atrial Fibrillation Detection from Noisy Photoplethysmography Data}:[\textbf{2022}] They have done atrial fibrillation detection on PPG and we are targeting ECG. PPG is a  proxy for ECG. Ideally, it should be easy to fix ECG compared to PPG. \textbf{bit relevant}

\item \href{https://dl.acm.org/doi/abs/10.1145/3341105.3373945}{Robust ECG R-peak detection using LSTM} - This paper uses LSTM to detect R peaks in a signal. Furthermore they create noisy ECG data for some other purpose.

\item \href{https://dl.acm.org/doi/full/10.1145/3616019}{A Deep Learning–based PPG Quality Assessment Approach for Heart Rate and Heart Rate Variability}: Authors proposed DL-based PPG quality assessment methods using CNNs for (1) HR, (2) AVNN, (3) RMSSD, (4) SDNN, and (5) LF/HF ratio. They  utilized customized 1D CNN and three 2D deep neural networks architectures. Furthermore, they also presented an automatic annotation method for each HR-HRV parameter, where the PPG quality was labeled automatically as “reliable” or “unreliable” by comparing against an ECG signal, as the baseline method for HR and HRV measurements.

\item \href{https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2022.0012}{Robustness of electrocardiogram signal
quality indices}: It discusses several metrics to measures the robustness of an ECG signal. It is a \textbf{basic paper} to understand the dynamics of an ECG signal. Furthermore, they have mentioned several \textbf{publicly available datasets}. 
 
\item \href{https://www.nature.com/articles/s41598-022-21776-2}{A two-step pre-processing tool to remove Gaussian and ectopic noise for heart rate variability analysis}:Authors developed a  novel preprocessing two-step method to eliminate both technical and ectopic noise types from an ECG signal.

\item \href{https://dl.acm.org/doi/pdf/10.1145/3464423}{Deep Learning for Medical Anomaly Detection – A Survey}[\textbf{2021}] - A survey paper discussing recent approaches for identifying anomalous signatures in different signals. \hl{Good for knowing the methods}

\item \href{https://arxiv.org/abs/2310.18961}{AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection} - Paper using zero-shot learning. \textbf{Not relevant}

\item \href{https://dl.acm.org/doi/pdf/10.1145/3536428}{On Equivalence of Anomaly Detection Algorithms} - Paper shows that it is not a good idea to compare different algorithms for different anomalies. But, \textbf{it provides a basic intuition that we should be able to provide a coherent classification of our detected anomalous ECG signatures. }

\item \href{https://arxiv.org/pdf/2304.03243.pdf}{SYNTHETIC DATA IN HEALTHCARE} - This paper provides a framework of generating synethic data. We can use it to \hl{repair} the anomalous portion of ECG signals

\item \href{https://www.sciencedirect.com/science/article/pii/S0208521617302553}{An automated ECG signal quality assessment method for unsupervised diagnostic systems}: This paper detects and localizes noisy portions in an ECG signal.

\item \href{https://www.sciencedirect.com/science/article/pii/S2212017312004227?ref=cra_js_challenge&fr=RR-1}{R-Peak Detection Algorithm for ECG using Double Difference And RR Interval Processing }{- We can use for detecting noise part of the signal using the method used for finding QRS region.}

\item \href{https://www.sciencedirect.com/science/article/pii/S2590188520300123}{A review on deep learning methods for ECG arrhythmia classification}[\textbf{2020}]: A survey paper discussing several works in the area of arrhythmia detection/classification.


    
\end{enumerate}

% \endinput
%%
%% End of file `sample-authordraft.tex'.
