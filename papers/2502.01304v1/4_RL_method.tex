\section{Wood-Log Grasping Method}
\label{sec: method}
Since a varied-diameter wood log is considered for the grasping task, the latent Markov decision process (L-MDP) \cite{chen2021understanding,vuong2019pick} is utilized in this work. The reward function and the policy gradient method are briefly discussed in this section. 


\subsection{Latent-MDP for forestry crane with varied-diameter wood logs}
The control problem for the grasping task can be modeled as a Markov decision process (MDP), which emulates the interactive learning between the agent (the grasping controller) and the simulated environment. An MDP consists of the $4$-element tuple $(\mathcal{S},\mathcal{A},\mathbf{P},R)$ referring to the state space $\mathcal{S}$, the action space $\mathcal{A}$, the transition probability density function $\mathbf{P}$, and the reward function $R$, respectively. 
%At a given discrete time step $t$, the forestry crane system is in state $\mathbf{s}_t \in \mathcal{S}$ and the agent perceives observations  $\mathbf{o}_t \in \mathcal{O}$ 
At a given time step $t$, the agent, in the state $\mathbf{s}_t \in \mathcal{S}$, selects an action $\mathbf{a}_t \in \mathcal{A}$ to transition to the next state $\mathbf{s}_{t+1}$ %\marc{according to the distribution (instead of "with the function")} 
with the transition probability $\mathbf{P}(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$. 
This results in the immediate reward $R_t$. Note that $\mathbf{P}(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$ is obtained from the simulator introduced in Section \ref{sec: b simulator}. It is worth noting that the transition function $\mathbf{P}$ is uncertain since the contact dynamics are not the same for different wood log dimensions.  
%\marc{(The transition PDF in the RL setting always describes an uncertain process. Do you mean that it additionally depends on the log dimensions?)}. 
Details on the state $\mathbf{s}_t$, the action $\mathbf{a}_t$, and the reward function $R_t$ for the grasping task with the forestry crane are introduced in the following subsection. 

Since the diameter $d$ of a wood log and its mass vary during the training process, the latent MDP (L-MDP) is utilized, where the log's size can be considered a latent variable. Additionally, individual training episodes are considered as single MDPs with finite length $H$. We denote the L-MDP as $\{\mathcal{L}, p(d)\}$, where $\mathcal{L}$ is the set of single MDPs with different diameters $d$, and $p(d)$ is the \corr{uniform} distribution of the diameter $d$ over $\mathcal{L}$. To this end, the objective of the grasping task is as follows
\begin{equation}
    J(\bm{\pi_\theta}) = \mathrm{E}_{d\sim p(d)}\bigg[\mathrm{E}_{\mathbf{a}_t\sim \bm{\pi_\theta}(.|\mathbf{s}_t)} \Bigg[\sum_{t=0}^{H}\gamma R_t\Bigg] \bigg]\:,
    \label{eq: RL objective}
\end{equation}
\corr{where $\mathrm{E}$ is the expectation function, the symbol ``$\sim$'' denotes the sampling process from the corresponding distribution $\bm{\pi_\theta}$ over the action space $\mathcal{A}$, and $0<\gamma<1$ is the discount factor. The normal distribution is typically used to model $\bm{\pi_\theta}$.} 
Additionally, $\bm{\theta}$ combines the policy parameters that can be weights and biases of a neural network. 
An RL method is utilized to find the optimal policy $\bm{\pi}^*$ that maximizes (\ref{eq: RL objective}) 
\begin{equation}
    \bm{\pi_\theta}^* = \argmax_{\bm{\pi_\theta}} J(\bm{\pi_\theta}) \:.
    \label{eq: RL policy}
\end{equation}
To find the optimal policy (\ref{eq: RL policy}), we utilize the modified version of Proximal Policy Optimization (PPO) as discussed in Subsection \ref{sec: RPPO}. The following subsection presents the details of the observation space, action space, and reward function. 
%Acting $a_t \in \mathcal{A}$ according to the policy distribution $\pi(a|s)$, the agent receives an immediate scalar reward $r_t(s_t, a_t)$ according to the specified reward function $R(s, a)$. 
%The goal of RL algorithms is to find the optimal policy $\pi(a|s)^*$ such that the agent takes the optimal action at any given state to maximize the expected return. 
%Here, the deep RL approach involves parameterizing the policy $\pi$ as a neural network $\pi_\theta$ with parameters $\theta$. The resulting policy approximator outputs 
\subsection{Learning environment for the forestry crane}
\subsubsection{Observations and actions} 
\label{sec: observation}
The observations consist of joint angles of the forestry crane and the actuated joint velocities, i.e., $\mathcal{O} = \{\mathbf{q},\dot{\mathbf{q}}_{A}\}$.  
From the 6 DoF poses of the log's pose w.r.t the crane's base obtained by other algorithms \cite{wen2023bundlesdf,vuong2023grasp}, we consider the reduced poses of 4 DoF $\mathbf{q}_l = [x_{l},y_{l}, z_{l}, \psi_l]^\mathrm{T}$, consisting the 3D Cartesian position of the log's center point $\mathbf{p}_l = [x_{l},y_{l}, z_{l}]^\mathrm{T}$ and the yaw angle $\psi_l$, see Figure \ref{fig: rl explained}. The augmented relative Cartesian distance is computed as
\begin{equation}
    \bm{\Delta}_p =  [x_{l},y_{l}, z_{l} - (d_{max}-z_l)/2]^\mathrm{T} - \mathbf{p}_\mathrm{C}(\mathbf{q}) \:,
    \label{eq: relative distance}
\end{equation}
where $\mathbf{p}_\mathrm{C}(\mathbf{q}) = [p_{\mathrm{C},x},p_{\mathrm{C},y},p_{\mathrm{C},z}]^\mathrm{T}$ results from the forward kinematics
\begin{equation}
    \mathbf{H}_\mathrm{C}(\mathbf{q}) = 
    \begin{bmatrix}
        \mathbf{e}_{\mathrm{C},x} & \mathbf{e}_{\mathrm{C},y} & \mathbf{e}_{\mathrm{C},z} & \mathbf{p}_\mathrm{C} \\
        0 & 0 & 0 & 1
    \end{bmatrix}
\end{equation}
and is located at the center of the grapple, see Figure \ref{fig: rl explained}. Note that $\mathbf{e}_{\mathrm{C},x}$, $\mathbf{e}_{\mathrm{C},y}$, and $\mathbf{e}_{\mathrm{C},z}$ are column vectors of the orientation of the grapple's center. The term $d_{off} = (d_{max}-z_l)/2$ represents an offset in $z$-direction for different log sizes where $d_{max} = 0.8$ is the maximum diameter of the wood log. 
Since $\psi_l$ is the yaw rotation around the $z$-axis of the crane base, illustrated in Fig. \ref{fig: rl explained}, the unit vector $\mathbf{e}_{l,y}$ along the length of the log is computed as
\begin{equation}
    \mathbf{e}_{l,y} = [-\sin(\psi_l), \cos(\psi_l), 0]^\mathrm{T}
\end{equation}
In order to successfully grasp the wood log, the orientation of the grapple must be well-aligned with the wood log, as defined in the following condition
\begin{equation}
     \mathrm{mod}\bigg[\widehat{(\mathbf{e}_{l,y},\mathbf{e}_{\mathrm{C},x})},\pi \bigg] \approx 0 \:, 
     \label{eq: orientation condition}
\end{equation}
where the $\widehat{(\mathbf{e}_{l,y},\mathbf{e}_{\mathrm{C},x})}$ presents the angle between the two vectors $\mathbf{e}_{l,y}$ and $\mathbf{e}_{\mathrm{C},x}$. 
The condition (\ref{eq: orientation condition}) can be normalized as the angle distance function in the form
\begin{equation}
    %\Delta_{\psi} = \dfrac{\mathbf{e}_{\mathrm{C},x}\cdot \mathbf{e}_{l,y}}{\norm{\mathbf{e}_{\mathrm{C},x}} \norm{\mathbf{e}_{\mathrm{l},y}}} \:\:,
    \Delta_{\psi} = 1- |\mathbf{e}_{\mathrm{C},x}\cdot \mathbf{e}_{l,y} |\:\:,
    \label{eq: angle distance}
\end{equation}
where the symbol ``$\cdot$'' denotes the dot product between two vectors. 
To this end, the observation space also includes the relative distance (\ref{eq: relative distance}) and the angle distance (\ref{eq: angle distance}), i.e., $\mathcal{O} = \{\mathbf{q},\dot{\mathbf{q}}_{A},\bm{\Delta}_p,\Delta_\psi\}$. As ideal underlying velocity controllers are assumed, the action space consists of desired actuated joint velocities $\mathcal{A} = \{\dot{\mathbf{q}}_{A,d}\}$. 
    %The observation and action spaces are listed in Table \ref{tab:crane_obs_act}. 

\begin{figure}[t]
\centering
\scalebox{0.6}{
\def\svgwidth{1\columnwidth}
\input{figures/explain_rl_env_2.pdf_tex}
}
\vspace{0.1cm}
\caption{Details of variables used for constructing the observations and reward function.}
\label{fig: rl explained}
\vspace{-0.2ex}
\end{figure}
    
\iffalse
    \begin{table}
        
        \caption[abc]{Summary of the observations and actions.}
        \begin{center} 
        \begin{tabular}{c | c c}
        \hline
        & Observations & Actions\\
        \hline
        \multirow{8}{*}{Joint angles and angle rates} & $q_1,\:  \dot{q}_{1}$ & $\dot{q}_{1,d}$\\
         & $q_2 \:  \dot{q}_{2}$ & $\dot{q}_{2,d}$\\
         & $q_3 \:  \dot{q}_{3}$ & $\dot{q}_{3,d}$\\
         & $q_4\:  \dot{q}_{4}$ & $\dot{q}_{4,d}$\\
         & $q_5$ \\
         & $q_6$ \\
         & $q_7\:  \dot{q}_{7}$ & $\dot{q}_{7,d}$\\
         & $q_8\:  \dot{q}_{8}$ & $\dot{q}_{8,d}$\\
         \hline
        \multirow{3}{*}{Relative distance $\bm{\Delta}_p$} & $x_l - p_{\mathrm{C},x}$ \\
         & $y_l - p_{\mathrm{C},y}$\\ 
         & $z_l - d_{off} - p_{\mathrm{C},z}$ \\
              \hline
        \multirow{1}{*}{Angle distance} & $\Delta_\psi$ \\
         
         \hline 
        \end{tabular}
        \end{center}
        \label{tab:crane_obs_act}
    \end{table}
\fi
\subsubsection{Reward function}    
The reward function $R$ is designed to gradually guide the grapple along the actions, e.g., approaching, grasping, lifting, and balancing, to achieve the final goal. 
First, the forestry crane can grasp the wood log when the combined weighted distance 
\begin{equation}
    d_\mathrm{combine} = \norm{\bm{\Delta}_p} + \omega_1 \Delta_\psi
    \label{eq: d combine}
\end{equation}
is small enough. Consequently, the associated reward function term reads as
%on this factor is expressed as
\begin{equation}
    r_{\mathrm{distance}}  = \mathrm{exp}(-\omega_2 d_\mathrm{combine}) \:,
    \label{eq: r_distance}
\end{equation}
where $\omega_1 > 0 $ and $\omega_2 > 0$ are user-defined parameters. 
When the crane approaches the target, the RL agent is encouraged to close the grapple to hold the wood log. The reward function term for this behavior is
\begin{equation}
    r_{\mathrm{grapple}}  = r_{\mathrm{distance}}({q_{8}}/{\overline{q_8}}) + (1-{q_{8}}/{\overline{q_8}})(1-r_{\mathrm{distance}})\:,
\end{equation}
with $\overline{q_8} = \SI{3}{\radian}$ as the limit of the joint angle $q_8$. 
After holding the wood log inside the grapple, the forestry crane proceeds with the lifting action, represented by the reward function term%by giving the following reward 
\begin{equation}
    r_{\mathrm{lift}} = (1- \mathrm{tanh}(\omega_3|z_l-z_{l,d}|))(1-r_\mathrm{grapple})\:\:,
\end{equation}
where $z_{l,d}$ is the desired height of the log and $\omega_3 >0$ is a user-defined parameter. Finally, we encourage the forestry crane to stabilize after grasping the log by using
\begin{equation}
    r_{\mathrm{balance}} = (1 - \mathrm{tanh}(\norm{\dot{\mathbf{q}}_{A,d}}))(1-r_\mathrm{lift}) \:\:.
\end{equation}
Combining all parts, the overall reward function takes the form
\begin{equation}
    R = r_{\mathrm{distance}} + r_{\mathrm{grapple}} + r_{\mathrm{lift}} + r_{\mathrm{balance}} \:\:.
\end{equation}
%r_lift = 1 - np.tanh(z_log_desire_distance*4)
%distance_combine = cartersian_distance + w_angle_distance*angle_diff_distance
%r_distance_combine = np.exp(-distance_combine * w_distance)
%r_jaw_opening = (-jaw_angle / jaw_angle_max + 1) * (1 - r_distance_combine)*0.5
%r_jaw_closing = (jaw_angle / jaw_angle_max)

\subsubsection{Episode termination}
\label{sec: early termination}
Each training episode has a time limitation of $t_{max} = \SI{9}{\second}$. 
Additionally, other termination criteria are listed in the following: 
\begin{itemize}
    \item If the grapple point $\mathbf{p}_C$ is not close to the log's center point $\mathbf{p}_l$,  i.e., $d_\mathrm{combine} < \epsilon$, within $t_{limit} = \SI{6}{\second}$, the episode is early terminated. 
    \item One of the joint limits is violated. 
    \item The log is located more than \SI{8}{\meter} away from the grapple. 
    \item The velocity of the actuated joints exceeds the physical limits, i.e., $|\dot{\mathbf{q}}_A| > \dot{\mathbf{q}}_{A,max}$. 
    %This can happen when the forestry crane tries to push away the log. 
\end{itemize}
\subsection{Modified proximal policy optimization (mPPO) utilizing Beta distribution}
\label{sec: RPPO}

\begin{figure}[t]
\centering
\scalebox{0.8}{

\def\svgwidth{1\columnwidth}
\input{figures/learning_architecture-new.pdf_tex}
}
\vspace{1ex}
\caption{Overview of the learning process. $m$ randomized environments with different wood log sizes and poses are generated by our crane simulator, presented in Subsection \ref{sec: b simulator}. }
\label{fig: overview learning}
\end{figure}
The overview of the learning process is illustrated in Figure \ref{fig: overview learning}. Using the crane simulator in Mujoco, $m$ parallel environments are sampled to generate rollouts (trajectories) for training the agent. A standard architecture of an actor-critic network used in the PPO algorithm is illustrated on the right-hand side of Figure \ref{fig: overview learning}. Since the details on PPO are omitted, readers are referred to \cite{schulman2017proximal}. Only modifications of the PPO, named mPPO, are presented below. 
    
In deep RL, the policy $\bm{\pi}$ is a neural network with the parameter vector $\bm{\theta}$ that takes the state $\mathbf{s}_t$ as input and outputs the distribution of actions $\bm{\pi_{\theta}}$ modeled as Gaussian distribution in the form
\begin{equation}
    \bm{\pi_{\theta}}(\mathbf{a}_t|\mathbf{s}_t) = \dfrac{1}{\sqrt{2\pi}\bm{\sigma_\theta}}\mathrm{exp}
    \Bigg(
    \dfrac{-(\mathbf{a}_t-\bm{\mu_\theta})^2}{2\bm{\sigma_\theta}}
    \Bigg)\:.
\end{equation}
The control actions $\mathbf{a}_t$ can be sampled in the backpropagation process or the inference process as follows
\begin{equation}
    \mathbf{a}_t = \bm{\mu_\theta}(\mathbf{s}_t) + \bm{\sigma_\theta}(\mathbf{s}_t)\mathcal{N}(0,1)   \:,
\end{equation}
\corr{
where $\bm{\mu_\theta}$ and $\bm{\sigma_\theta}$ are the mean and standard deviation of the Gaussian distribution. Since the control input in $\mathbf{a}_t$ is modeled as a separate distribution, an element-wise product is used for all equations in this subsection.}

However, the control actions of the considered forestry crane are constrained in an admissible range $\underline{\mathbf{a}_t} \leq \mathbf{a}_t \leq \overline{\mathbf{a}_t}$ for safety reasons. Thus, in the mPPO algorithm, the Beta distribution is employed with the probability density function (PDF) \cite{chou2017improving}
\begin{equation}
    \bm{\pi_\theta}(\mathbf{a}_t|\mathbf{s}_t) = \bm{\mathrm{Beta}}(\mathbf{a}_{t,n};\bm{\alpha_\theta},\bm{\beta_\theta})\:\:, \bm{\alpha_\theta}>1, \:\bm{\beta_\theta} >1 \:\:,
\end{equation}
where $\mathbf{a}_{t,n} = \dfrac{\mathbf{a}_t- \overline{\mathbf{a}_t}}{\overline{\mathbf{a}_t} - \underline{\mathbf{a}_t}}$ is the normalized action and 
\begin{equation}
    \bm{\mathrm{Beta}}(\mathbf{a}_{t,n};\bm{\alpha_\theta},\bm{\beta_\theta}) = \dfrac{\bm{\Gamma}(\bm{\alpha_\theta}+\bm{\beta_\theta})}{\bm{\Gamma}(\bm{\alpha_\theta})\bm{\Gamma}(\bm{\beta_\theta})}\mathbf{a}_{t,n}^{\bm{\alpha_\theta}-1}(1-\mathbf{a}_{t,n})^{\bm{\beta_\theta}-1}\:\:. 
\end{equation}
Note that ${\Gamma}(i) = \int_0^\infty j^{i-1}\mathrm{exp}(-j)\mathrm{d}j$ is the Gamma function \cite{davis1959leonhard}. In this way, the action $\mathbf{a}_t$ is always sampled in the admissible range by using the mean of the Beta distribution 
\begin{equation}
    \bm{\mu}_{t,n} = \dfrac{\bm{\alpha_\theta}(\mathbf{s}_t)}{\bm{\alpha_\theta}(\mathbf{s}_t) + \bm{\beta_\theta}(\mathbf{s}_t)} \:\:\:. 
    \label{eq: sampling}
\end{equation}

In the PPO algorithm, the loss function consists of three parts, i.e., the surrogate loss to constrain the policy update, the error term of the value function, and the entropy term to encourage exploration, see \cite{weng2018policy}. 
In addition, in a conventional PPO algorithm, the agent shows more randomness in its actions, but the surrogate loss can constrain the updating policy during the training process. 
In a complex environment with large search areas, exploration is important for an agent like this forestry crane to complete the task. 
Inspired by Robust Policy Optimization (RPO) \cite{huang2022cleanrl}, at each step of the training process, we perturb the sampling action (\ref{eq: sampling}) by random values in the uniform distribution $\mathbf{g} \sim \mathcal{U(-\epsilon,\epsilon)}$ in the form
\begin{equation}
    \mathbf{a}_{t,n} \leftarrow \mathrm{clip}(\mathbf{a}_{t,n} + \mathbf{g},0,1) \:.
    \label{eq: robust ppo}
\end{equation}
The function $\mathrm{clip}$ limits the value of $\mathbf{a}_{t,n}$ in the range of $[0,1]$. In this work, $\epsilon$ is set to $0.1$. 

%\lipsum[1]
%\subsection{Part}
 