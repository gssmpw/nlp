\section{Related Work}
\label{sec:related_work}
We highlight related works not previously discussed above.

\newparagraph{Minimizing Mini Batch Preparation Overhead}
To reduce the overhead of mini batch preparation, many prior works seek to offload some of this work to the GPU, either by employing GPU-oriented communication, feature caching, or by directly sampling neighbors on the GPU when possible~\cite{largegcn, min2021pytorchdirect, salient, pagraph, dong2021global, nextdoor}. Other works seek to reduce the overhead of multi-hop neighborhood sampling itself~\cite{graphsage, chen2018fastgcn, zou2019layer, lazygcn, zeng2020graphsaint, clustergcn}. These improvements are orthogonal to our disaggregated architecture and could be incorporated in \systemname. 

\newparagraph{Systems for Large-Scale ML over Graphs}
Many systems have been introduced for training machine learning models over large-scale graphs~\cite{p3gnn, MLSYS2020_ROC, pyg, aligraph, pagraph, dong2021global, gnnsurvey, zheng2020dglke}. Some works focus on scaling training using disk-based storage~\cite{mohoney2021marius, partitioning, pytorchbiggraph}. The most closely related works to \systemname, however, are those that focus on scaling GNN training using distributed multi-GPU or multi-machine settings~\cite{p3gnn, MLSYS2020_ROC, aligraph, wang2021flexgraph}. Like Armada, several of these works aim to reduce cross-machine communication during multi-hop sampling, either by using min-edge-cut partitioning~\cite{distdglv2} or by employing feature replication on each machine~\cite{liu2023bgl, salient++}. To address this challenge, \systemname also supports replicating features and introduces \partitioning to scale min-edge-cut partitioning to large graphs. Among these systems, however, Armada is unique in its use of disaggregation to scale training.