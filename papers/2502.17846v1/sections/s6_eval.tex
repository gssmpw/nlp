\section{Evaluation}
\label{sec:eval}
We evaluate \partitioning and \systemname's disaggregated architecture on common large-scale graphs and compare against METIS~\cite{karypis1997metis} and the popular SoTA GNN systems DGL (version 2.4)~\cite{dgl, distDGL}, Salient++~\cite{salient++}, and MariusGNN~\cite{mariusgnn}. Our experiments show that:

\begin{enumerate}[topsep=0pt, left=0pt, noitemsep]
    \item \partitioning can efficiently scale min-edge-cut partitioning to large graphs, leading to up to 45$\times$ and 68$\times$ reduction in runtime and memory overheads compared to METIS.
    \item Disaggregation allows \systemname to achieve scalable, cost-effective GNN training---\systemname achieves a 7.5$\times$ speedup when using eight instead of one GPU when existing SoTA systems yield 4.3$\times$ speedup at best.
\end{enumerate}



\subsection{Experimental Setup}
We start by discussing the setup used in our experiments.

\newparagraph{\systemname and Baseline Details}
MariusGNN supports only single-GPU training, thus we modify it to support multi-GPU training using a standard distributed data parallel architecture. We report results for two versions of \systemname: 1) \systemname and 2) Armada - Aggregated. The former uses the disaggregated architecture described throughout the paper, while the latter uses the CPUs on the GPU machine(s) used for training to prepare batches. The two versions allow us to directly evaluate the benefit of disaggregation. For partitioning, we use \partitioning with Armada and place one partition on each batch construction worker; for baselines, we use their default partitioning plus feature replication and caching (which are METIS-based).

\newparagraph{Hardware Setup}
We partition and train all systems using AWS machines. 
To measure scalability, we use p3.16xlarge instances with eight NVIDIA V100 GPUs and vary how many GPUs are available to each system. These machines contain 64 vCPUs, 488 GiB of CPU memory, and 128 GiB of aggregate GPU memory. For \systemname, we use additional m6a.16xlarge machines for mini batch preparation. These machines have 64 vCPUs and 256 GiB of CPU memory. 

\newparagraph{Datasets, Models, and Metrics}
We report results using Open Graph Benchmark (OGB) datasets~\cite{hu2020ogb, hu2021ogblsc}; we use OGBN-Papers100M (111M nodes, 1.6B edges) and OGB-WikiKG90Mv2 (91M nodes, 601M edges) for large-scale studies, and OGBN-Products (2.5M nodes, 62M edges) plus FB15K-237~\cite{fb15k237} (14.5K nodes, 272K edges) for microbenchmarks. We train a three-layer GraphSage GNN on these datasets with two different hidden sizes: 256 (\textit{GraphSage-Small}) and 1024 (\textit{GraphSage-Large}). The former allows us to run experiments using a data-bound model while the latter aims to represent a compute-bound model. In both cases, we use 30, 20, and 10 neighbors per layer sampled from both incoming and outgoing edges, as done in~\cite{mariusgnn}. For partitioning experiments, we measure the resulting number of edge cuts, runtime, and peak memory usage. For GNN training, we run for 10 epochs and measure runtime and monetary cost. We do not include the time to partition when reporting GNN training times (we report partitioning time independently). We average experiments over three runs.

\newparagraph{Hyperparameters}
We use the same hyperparameters for GNN model architecture and training across systems (e.g., model hidden dimension, number of neighbors, batch size, etc.). These hyperparameters are chosen based on values from prior works~\cite{hu2020ogb, mariusgnn}. For hyperparameters specific to the throughput of each system (e.g., the number neighborhood sampling workers), we manually tune them and select the best configuration.



\subsection{Evaluating \partitioning Partitioning}
\label{subsec:eval_partitioning}
We now evaluate \partitioning and compare to METIS, the SoTA min-edge-cut algorithm used by existing GNN systems.

\begin{figure}[t]
  \centering
  \includegraphics[width=.4\textwidth]{figures/acc.pdf}
  \vspace{-0.15in}
  \caption{Percentage of edges cut when using \partitioning versus METIS on three common graphs. \partitioning  achieves comparable edge cuts to METIS, even with a chunk size of just 10\% or 1\%.}
  \label{fig:grem_acc}
  \vspace{-0.1in}
\end{figure}

\newparagraph{Partitioning Quality: Number of Edge Cuts}
In Figure~\ref{fig:grem_acc}, we show the number of edge cuts that result from running \partitioning and METIS on three common graphs. With a chunk size of 10\%, \partitioning partitions the graph with similar quality to METIS. For example, in the most challenging case ($p=128$ partitions), \partitioning cuts just 0.5\% and 1\% more of the graph than METIS on OGBN-Products and OGB-WikiKG90Mv2 respectfully. \partitioning even achieves comparable results with a chunk size of 1\%. Overall, Figure~\ref{fig:grem_acc} shows that with small chunk sizes (e.g., $\le 10$\%), \partitioning can partition graphs with comparable edge cuts to METIS.



\begin{figure}[t]
  \centering
  \includegraphics[width=.48\textwidth]{figures/memory_runtime.pdf}
  \vspace{-0.3in}
  \caption{Memory usage and runtime of \partitioning and METIS when partitioning subgraphs of OGBN-Papers100M of various size. \partitioning reduces the computational requirements of partitioning.}
  \label{fig:grem_memory_runtime}
  \vspace{-0.15in}
\end{figure}

\newparagraph{Partitioning Overhead: Runtime and Memory}
Next, we evaluate the peak memory usage and runtime of \partitioning versus METIS. To do so, we use both algorithms to partition subgraphs of varying size (number of edges), taken from OGBN-Papers100M (1.6B edges total), into two parts ($p=2$). Results are shown in Figure~\ref{fig:grem_memory_runtime}. We plot only $p=2$ for simplicity; as $p$ increases, peak memory remains constant and the runtime of each algorithm increases by the same factor (both \partitioning and METIS partition recursively for $p>2$). Figure~\ref{fig:grem_memory_runtime} (left) shows that METIS is able to partition 600M edges on the machine used for these experiments (250GB of memory). Based on the scaling of memory and runtime, we estimate that METIS needs 8000s and requires a machine with 630GB of memory to partition the entire OGBN-Papers100M graph; we confirmed this estimate on a special machine with 750GB. \partitioning, however, can partition the entire graph in just 976s with 73GB (8.2 and 8.3$\times$ reduction) or 175s with 9.3GB (46 and 65$\times$ reduction) when using a chunk size of 10\% or 1\% respectively.



\begin{figure}[t]
  \centering
  \includegraphics[width=.4\textwidth]{figures/analysis_plus_ablation.pdf}
  \vspace{-0.15in}
  \caption{Percentage of edges cut versus chunk size when using \partitioning on FB15K-237 (the hardest graph to partition in Figure~\ref{fig:grem_acc}) compared to standard streaming greedy approaches. We focus on chunk sizes $\le 30\%$ where the computational benefit of these methods compared to METIS (which is shown for reference, but partitions using the full graph, rather than in chunks) is maximal.}
  \label{fig:grem_ablation}
  \vspace{-0.15in}
\end{figure}

\newparagraph{The Benefit of Refinement}
Finally, we study the benefit of the refined greedy assignments used by \partitioning compared to the fixed greedy assignments of conventional streaming algorithms. For both approaches, we show in Figure~\ref{fig:grem_ablation} the number of edge cuts versus chunk size when partitioning FB15K-237 (the hardest graph to partition in Figure~\ref{fig:grem_acc}) into $p=2$ partitions (given the recursive nature of GREM, similar results hold for $p>2$). We include both the expected number of edge cuts from the theoretical analysis in Section~\ref{subsec:partitioning_analysis}, and the number of edge cuts measured in practice.

Figure~\ref{fig:grem_ablation} shows that for small chunk sizes (e.g., $\le$10\%), refinement is critical to minimizing edge cuts; we observe a reduction of up to 25\% of the graph (at a chunk size of 1\%). These improvements allow GREM to use smaller chunk sizes (e.g., 1-10\%) without suffering a significant increase in edge cuts compared to METIS. For example, with a chunk size of 5\%, \partitioning and METIS differ in edge cuts by $<$1\% of the graph; this difference would be 13\% with fixed greedy assignments. The consequence of these additional edge cuts is slower and more expensive GNN training---We observe that training in Armada is up to 2.4$\times$ slower when using the streaming greedy algorithm rather than \partitioning (for a chunk size of 1\%). This confirms recent results which highlight that high quality partitioning algorithms (e.g., METIS), can lead to faster GNN training compared to streaming greedy approaches (e.g., LDG)~\cite{merkel2023experimental}.

\newparagraph{Summary}
\partitioning can partition large-scale graphs with comparable quality to METIS but with orders of magnitude less computational resources, helping to address the bottleneck of min-edge-cut partitioning for distributed GNN training.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{figures/gs_scaling.pdf}
  \vspace{-0.15in}
  \caption{Epoch runtime versus number of GPUs for DGL, MariusGNN, Salient++, and \systemname using two different GraphSage GNNs on the OGBN-Papers100M dataset. Disaggregation allows \systemname to scale linearly with respect to the number of GPUs.}
  \vspace{-0.1in}
  \label{fig:scaling}
\end{figure}


\subsection{GNN Training: System Comparisons}
Given a partitioned graph, we now evaluate Armada's disaggregated architecture for GNN training. Runtime and cost per epoch for two models on OGBN-Papers100M with \systemname and existing systems is shown in Table~\ref{tab:runtime_nc}. We plot the runtime versus the number of GPUs in Figure~\ref{fig:scaling} to show the scaling of each system. 
For these experiments, all systems sample neighbors across the whole graph, and thus reach similar accuracy (e.g., see Table~\ref{tab:runtime_nc} right).


\newparagraph{Key Takeaways}
Across experiments and GPU counts, \systemname is the fastest and cheapest option; runtime and cost reductions are up to 4.5$\times$ and 3.1$\times$ versus existing systems. 



\begin{table*}[t]\tiny
    \centering
    \caption{Runtime and cost of DGL, MariusGNN, Salient++, and \systemname on OGBN-Papers100M using a GraphSage-Small (left) and -Large (right) GNN. With disaggregated mini batch preparation, \systemname can scale training from one to eight GPUs while existing systems cannot. Moreover, the extra disaggregated machines are cheap compared to the GPU machines used for model computation and do not prevent reductions in total training cost. \systemname uses 0, 1, 2, and 4 disaggregated batch construction workers for 1-, 2-, 4-, and 8-GPU training respectively. Relative improvement compared to single-GPU training for each system is shown in parentheses.}
    \vspace{0.05in}
    \label{tab:runtime_nc}
    {
        \setlength{\tabcolsep}{5pt}
        \begin{tabular}{l c c c c c c c c c c c c c c c c c}
            \toprule
            & \multicolumn{8}{c}{GraphSage-Small} & \multicolumn{9}{c}{GraphSage-Large} \\
            \cmidrule(lr){2-9}
            \cmidrule(lr){10-18}
            & \multicolumn{4}{c}{Epoch Runtime (s)} & \multicolumn{4}{c}{Epoch Cost (\$)} & \multicolumn{4}{c}{Epoch Runtime (s)} & \multicolumn{4}{c}{Epoch Cost (\$)} & Avg. Acc. \\
            \cmidrule(lr){2-5}
            \cmidrule(lr){6-9}
            \cmidrule(lr){10-13}
            \cmidrule(lr){14-17}
            \cmidrule(lr){18-18}
            \# GPUs & 1 & 2 & 4 & 8 & 1 & 2 & 4 & 8 & 1 & 2 & 4 & 8 & 1 & 2 & 4 & 8 & -\\
            \cmidrule(lr){1-18}
            % DGL & 186 & 177 & 161 & 79.4 (2.3$\times$) & 1.26 & 1.20 & 1.09 & 0.54 (2.3$\times$) & 235 & 202 & 170 & 86.5 (2.7$\times$) & 1.60 & 1.37 & 1.16 & 0.59 (2.7$\times$) & 67.35\\
            DGL v2.4 & 143 & 142 & 93.4 & 33.0 (4.3$\times$) & 0.97 & 0.97 & 0.64 & 0.22 (4.4$\times$) & 151 & 146 & 101 & 41.0 (3.7$\times$) & 1.03 & 0.99 & 0.69 & 0.28 (3.6$\times$) & 67.35\\
            MariusGNN & 84.0 & 77.1 & 62.1 & 61.5(1.4$\times$) & 0.57 & 0.52 & 0.42 & 0.42 (1.4$\times$) & 124 & 92.3 & 83.9 & 88.3 (1.4$\times$) & 0.84 & 0.63 & 0.57 & 0.60 (1.4$\times$) & 67.14\\
            Salient++ & 61.5 & 54.5 & 38.1 & 35.6 (1.7$\times$) & 0.42 & 0.37 & 0.26 & 0.24 (1.7$\times$) & 114 & 79.8 & 39.1 & 36.7 (3.1$\times$) & 0.78 & 0.54 & 0.27 & 0.25 (3.1$\times$) & 68.20\\
            % \cmidrule(lr){2-18}
            \systemname & \textbf{54.9} & \textbf{33.3} & \textbf{14.2} & \textbf{7.35 (7.5$\times$)} & \textbf{0.37} & \textbf{0.25} & \textbf{0.12} & \textbf{0.07 (5.3$\times$)} & \textbf{98.7} & \textbf{50.2} & \textbf{26.1} & \textbf{12.0 (8.2$\times$)} & \textbf{0.67} & \textbf{0.38} & \textbf{0.22} & \textbf{0.12 (5.6$\times$)} & 67.16\\
            \bottomrule
        \end{tabular}
    }
    \vspace{-0.1in}
\end{table*}



\newparagraph{Existing Systems}
We find that existing systems are unable to effectively scale GNN training across multiple GPUs. For GraphSage-Small, the most scalable system (DGL) achieves only a 4.3$\times$ speedup when moving from one to eight GPUs. With a more compute-intensive model (GraphSage-Large), baseline systems are able to scale better---e.g., Salient++ achieves a 3.1$\times$ speedup (rather than 1.7$\times$)---but they still suffer from sublinear speedups as a result of CPU-based mini batch preparation bottlenecks (Section~\ref{sec:prelim}).


\newparagraph{Armada: The Benefit of Disaggregation}
\systemname, however, achieves near-perfect scalability. For GraphSage-Small and -Large respectively, \systemname achieves a 7.5$\times$ and 8$\times$ speedup when moving from one to eight GPUs. The key reason Armada can scale linearly is because of its disaggregated architecture. The effect of disaggregation is evident by comparing \systemname to Armada - Aggregated in Figure~\ref{fig:scaling}. We also show the benefit of disaggregation in Figure~\ref{fig:armada_bw_scaling}; we report the epoch runtime in \systemname when training GraphSage-Large on OGBN-Papers100M with eight GPUs and a varying number of disaggregated batch construction workers. Figure~\ref{fig:armada_bw_scaling} shows that as the number of CPU resources used for mini batch preparation increases, the runtime decreases until the accelerators are fully saturated and the epoch runtime plateaus.

Although the additional machines needed for batch preparation incur additional cost, these machines are cheaper than the GPU machines used for computation. Thus, \systemname is still able to achieve total training cost reductions; we achieve a 5.3$\times$ and 5.6$\times$ reduction in cost when using eight instead of one GPU for GraphSage-Small and -Large respectively. 

\begin{figure}[t]
  \centering
  \includegraphics[width=.45\textwidth]{figures/bw_scaling.pdf}
  \vspace{-0.05in}
  \caption{Epoch runtime in \systemname when training GraphSage-Large on OGBN-Papers100M with eight GPUs and a varying number of disaggregated batch preparation workers; independently scaling these workers allows \systemname to minimize runtime.}
  \label{fig:armada_bw_scaling}
  \vspace{-0.2in}
\end{figure}



\newparagraph{Summary}
Armada's disaggregated architecture allows resource utilization to be optimized in the presence of GNN workload imbalance, leading to linear scaling and cost-effective distributed GNN training over large-scale graphs.