\vspace{0.05in}
\section{Conclusion}
\label{sec:conclusion}
This paper introduced \systemname, a new system for scalable, cost-effective, distributed GNN training. Armada's key contribution is \partitioning, a novel min-edge-cut partitioning algorithm that can efficiently scale to large graphs yet still achieve partition quality comparable to METIS. Armada also introduces a new architecture for GNN training that disaggregates the CPU resources used for GNN neighborhood sampling and feature loading from the GPU resources use for model computation, ensuring that the former can be scaled independently in order to saturate the latter. Overall, our results highlight the promise of new algorithms and systems to democratize large-scale GNN training.