\section{Disaggregated Implementation}
\label{sec:implementation}
Given a min-edge-cut partitioned graph, \systemname employs a disaggregated architecture to enable cost-effective, distributed GNN training (Section~\ref{sec:overview}). We now describe important design details that allow \systemname to independently scale each layer and minimize communication in this architecture. 



\newparagraph{Partition Assignment to Batch Construction Workers}
We first discuss the randomized algorithm used by Armada's designated worker, called the \textit{coordinator}, to assign partitions in the storage layer to machines in the mini batch preparation layer in order to complete one round of training. To support scaling each of these two layers independently, we require that the algorithm has the following guarantee: all partitions (and thus graph nodes used for training) must appear in memory at least once per epoch, regardless of whether the full graph (all partitions) fits in the aggregate CPU memory of the batch construction workers or not\footnote{If the full graph does not fit in the aggregate CPU memory of the batch construction workers, then neighborhood sampling cannot be done over the full graph, but can instead be done over the entire subgraph in the aggregate CPU memory of the layer.}.

The coordinator assigns partitions to workers as follows: First, the partitions are randomly split into disjoint subsets, one for each worker. This split occurs without data movement (only a mapping is maintained on the coordinator). Given an assigned set of partitions, each worker can then begin training by loading as many random partitions from its subset into memory as possible. After processing these partitions, any remaining partitions assigned to the worker are swapped in for training one by one in a random order. 

Randomized assignment, while simple, satisfies two desired properties: 1) opportunities for parallelism are maximized, as each batch worker operates on a disjoint set of partitions, and 2) each partition is read from the storage layer exactly once, ensuring all nodes appear in memory with minimal I/O between the two layers. \systemname, however, can easily support other partition assignment policies. In particular, to further minimize communication between workers due to cross-machine neighborhood sampling (in addition to min-edge-cut partitioning), \systemname supports partial or even entire (memory permitting) feature replication across workers, as done in prior work~\cite{cao2023communication, salient++}. In this case, the nodes to be replicated are placed in a special partition that is assigned to, and kept in memory, on \textit{all} workers. Min-edge-cut partitioning and randomized partition assignment are then used on the remaining nodes. 



\newparagraph{Mini Batch Grouping}
We next discuss mini batch grouping, the first of two techniques used by \systemname to minimize data transfer between batch preparation and compute workers.

Mini batch grouping applies when a batch construction worker is responsible for sending data to a compute worker that contains multiple GPUs. In this case, the batch construction worker must prepare and transfer one mini batch per GPU for each training iteration (such that each GPU can process a batch in parallel). \systemname groups these mini batches into a global batch, as mini batches contained in a global batch may require the same nodes. This allows \systemname to optimize feature loading and transfer: \systemname loads and transfers the feature vectors for the unique nodes in a global batch only once and copies them between GPUs as needed.
For compute workers with 8 GPUs, we find mini batch grouping reduces batch preparation time by 1.13$\times$ and transfer time by 1.72$\times$, increasing overall throughput by 1.15$\times$ on the common OGBN-Papers100M graph used in the experiments (Section~\ref{sec:eval}).



\newparagraph{Compute Worker Feature Caching}
Finally, \systemname can further minimize communication between the batch preparation and compute layers by caching feature vectors for frequently accessed nodes locally on compute workers (in CPU memory). In this case, \systemname needs to send only the non-cached features for each (global) batch between layers. Mini batches are then augmented as needed with the additional feature vectors once they are received in CPU memory by compute workers, before being transferred to the GPU(s) for training. We keep batch construction workers informed of the cache contents by listening to and acknowledging messages from the compute workers that describe planned updates and we use a simple LRU caching policy.