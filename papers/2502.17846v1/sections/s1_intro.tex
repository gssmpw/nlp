\section{Introduction}
Graph Neural Networks (GNNs) have emerged as the defacto approach for machine learning over graph-structured inputs~\cite{chami2021machine}; GNN-based models are currently used in navigation apps~\cite{derrow2021eta}, to predict protein structures~\cite{jumper2021highly}, and to create weather forecasts (GraphCast~\cite{lam2022graphcast}). These impressive results, however, require training GNNs over massive amounts of graph data. For example, GraphCast was trained on 53TB over four weeks using 32 Cloud TPU v4 nodes (10/2024 est. cost: \$70K), limiting the development of such a model to those with sufficient resources. 

Motivated by the above, this work focuses on scalable, cost-effective, distributed GNN training over large graphs using common cloud offerings. While recent works~\cite{salient++, distDGL, distdglv2} have sought to address this need, we find that existing pipelines face scalability challenges when graphs have billions of nodes or edges and when training with multiple GPUs. These challenges arise from the unique properties of the GNN workload itself.

In particular, distributed GNN training necessitates that the graph is partitioned across machines; yet, the partitioning has a direct impact on the subsequent training efficiency, as GNN systems must communicate across machines to sample the neighborhood of graph nodes~\cite{shao2024distributed}. This communication can be reduced using \textit{min-edge-cut partitioning} algorithms that minimize the number of edges with endpoints in different partitions (machines) (called \textit{cut edges}). Thus, min-edge-cut partitioning is widely used in GNN systems, and has been shown to lead to an order of magnitude faster training compared to random partitioning~\cite{merkel2023experimental, distdglv2}. 

Min-edge-cut partitioning, however, becomes increasingly expensive with graph size. For instance, many systems utilize the offline algorithm METIS~\cite{karypis1997metis} due to its ability to effectively minimize edge cuts by iteratively refining partitions across the whole graph and its comparatively efficient implementation~\cite{merkel2023experimental, shao2024distributed, lin2023comprehensive}; yet, METIS takes 8000s and requires a special machine with 630GB of memory to partition a common benchmark graph (the 1.6B edge OGBN-Papers100M), whereas GNN training takes only 549s (10 epochs, one GPU) and can run on cloud machines with 244GB of memory~\cite{mariusgnn} (details in Section~\ref{sec:eval}). Although the partitioning overhead can be amortized across models, it still presents a bottleneck to GNN training. To address this issue, streaming algorithms iterate over the graph and assign vertices to partitions greedily~\cite{abbas2018streaming}. While these algorithms offer improved scalability, they tend to result in more edge cuts than offline methods~\cite{zhang2018akin}; e.g., we find a streaming greedy approach cuts up to 4$\times$ more edges than METIS.

In this work, we introduce Armada, a new end-to-end system for large-scale distributed GNN training that aims to address the bottleneck of partitioning in existing GNN pipelines. To overcome this challenge, Armada's key contribution is a novel memory-efficient min-edge-cut partitioning algorithm called \partitioning (Greedy plus Refinement for Edge-cut Minimization). \partitioning can efficiently scale to massive graphs on common hardware by processing streaming chunks of graph edges, yet it still returns partitions with edge cuts comparable to METIS. For example, in the same setting in which METIS requires 8000s and 630GB, \partitioning can partition the graph with similar edge cuts in 175s using 9.3GB.

\partitioning's partitioning algorithm builds on existing streaming greedy approaches. 
Specifically, \partitioning iterates over the graph edges in chunks and greedily assigns the vertices in each chunk to partitions. The key idea behind \partitioning, however, is that it allows prior vertex assignments to be modified throughout the process, rather than freezing them after an initial greedy selection (as in existing algorithms~\cite{abbas2018streaming}). This approach, inspired by offline algorithms, refines the partitioning by leveraging lightweight statistics accumulated during streaming (these statistics provide estimates of the number of neighbors per node in each partition).

We analyze theoretically \partitioning's expected number of edge cuts versus chunk size, providing insight into its expected behavior. This analysis, confirmed by experiments, shows that refinement is critical for minimizing edge cuts when using small chunk sizes (e.g., $\le$10\% of the edges) and thus for minimizing \partitioning's computational requirements (which are proportional to chunk size): We show that \partitioning with a chunk size of 10\% and METIS cut a similar number of edges, but \partitioning does so with 8$\times$ less memory and runtime (see Section~\ref{sec:eval}). \partitioning even achieves comparable results with a chunk size of 1\%, leading to further reductions and enabling \partitioning to partition the largest public graphs (e.g., Hyperlink-2012~\cite{hyperlink}; 3.5B nodes, 128B edges) with only 500GB of memory.

Given a partitioned graph, Armada's second main contribution is the introduction of a new distributed architecture, that disaggregates the CPU resources used for neighborhood sampling from the GPU resources used for model computation, in order to achieve scalable, memory-efficient, and cost-effective GNN training on common hardware. Concretely, Armada consists of: 1) A partitioning layer that implements \partitioning. 2) A storage layer to store the partitioned graph, implemented over cheap disk-based storage. 3) A distributed mini batch preparation layer consisting of a set of workers running on cheap CPU-only machines; workers read graph partitions from storage and prepare batches (i.e., perform neighborhood sampling) for training. 4) A distributed model computation layer that utilizes a set of GPU machines to perform training over the prepared batches.

We chose a disaggregated architecture to optimize resource utilization during training. On common cloud machines, we find that even with zero communication, mini batch preparation can be up to an order of magnitude slower than mini batch computation (Figure~\ref{fig:armada_breakdown}). Disaggregation allows Armada to overcome this imbalance. By independently scaling the batch preparation layer, we can ensure that GPUs in the computation layer remain saturated during training. In contrast, existing systems, which rely only on the fixed set of CPU resources attached to the GPU machines used for training to prepare batches, are unable to parallelize mini batch preparation and suffer from sublinear speedups as compute resources are scaled. For example, on a cloud GPU machine, we find that two SoTA systems~\cite{salient++, distDGL} yield only 4.3$\times$ and 1.7$\times$ speedup when using eight instead of one GPU (Table~\ref{tab:runtime_nc} left). Sublinear speedups lead to higher than necessary total training cost and runtime over massive graphs, as expensive compute resources sit idle. Yet in the same setting, \systemname achieves a 7.5$\times$ speedup with eight instead of one GPU.

Despite the flexibility of disaggregation, challenges arise due to the communication overhead between various components. Thus, we carefully design Armada with a focus on minimizing communication between and within layers. In particular, Armada includes two optimizations to reduce the data sent between batch preparation and compute workers: 1) batch workers group mini batches destined for different GPUs on the same compute worker and transfer them together, rather than independently, in order to enable greater compression (mini batch grouping), and 2) compute workers in Armada maintain a cache of frequently accessed data in their local CPU memory (feature caching). Together, these optimizations enable Armada to scale each layer in the architecture independently without communication bottlenecks.

We evaluate Armada's disaggregated architecture for GNN training and compare against existing SoTA systems. Using popular GNN architectures, we show that while existing systems scale sublinearly, Armada does not, leading to runtime improvements up to 4.5$\times$ and monetary cost reductions up to 3.1$\times$ compared to existing systems.
