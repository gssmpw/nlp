@article{Bespalov2024TowardsBA,
  title={Towards building a robust toxicity predictor},
  author={Bespalov, Dmitriy and Bhabesh, Sourav and Xiang, Yi and Zhou, Liutong and Qi, Yanjun},
  journal={arXiv preprint arXiv:2404.08690},
  year={2024}
}

@article{Du2023ImprovingFA,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.14325},
  url={https://api.semanticscholar.org/CorpusID:258841118}
}

@article{Glukhov2023LLMCA,
  title={Llm censorship: A machine learning challenge or a computer security problem?},
  author={Glukhov, David and Shumailov, Ilia and Gal, Yarin and Papernot, Nicolas and Papyan, Vardan},
  journal={arXiv preprint arXiv:2307.10719},
  year={2023}
}

@inproceedings{Greshake2023NotWY,
  title={Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  booktitle={Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
  pages={79--90},
  year={2023}
}

@article{Helbling2023LLMSD,
  title={Llm self defense: By self examination, llms know they are being tricked},
  author={Phute, Mansi and Helbling, Alec and Hull, Matthew and Peng, ShengYun and Szyller, Sebastian and Cornelius, Cory and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2308.07308},
  year={2023}
}

@article{Hines2024DefendingAI,
  title={Defending Against Indirect Prompt Injection Attacks With Spotlighting},
  author={Hines, Keegan and Lopez, Gary and Hall, Matthew and Zarfati, Federico and Zunger, Yonatan and Kiciman, Emre},
  journal={arXiv preprint arXiv:2403.14720},
  year={2024}
}

@article{Hong2023MetaGPTMP,
  title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},
  author={Sirui Hong and Xiawu Zheng and Jonathan P. Chen and Yuheng Cheng and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zi Hen Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.00352},
  url={https://api.semanticscholar.org/CorpusID:260351380}
}

@article{Huang2023ASO,
  title={A survey of safety and trustworthiness of large language models through the lens of verification and validation},
  author={Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and others},
  journal={Artificial Intelligence Review},
  volume={57},
  number={7},
  pages={175},
  year={2024},
  publisher={Springer}
}

@article{Inan2023LlamaGL,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{Jain2023BaselineDF,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{Kumar2023CertifyingLS,
  title={Certifying llm safety against adversarial prompting},
  author={Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2309.02705},
  year={2023}
}

@article{Liu2023PromptIA,
  title={Prompt Injection attack against LLM-integrated Applications},
  author={Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and others},
  journal={arXiv preprint arXiv:2306.05499},
  year={2023}
}

@article{Maus2023AdversarialPF,
  title={Adversarial prompting for black box foundation models},
  author={Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob},
  journal={arXiv preprint arXiv:2302.04237},
  volume={1},
  number={2},
  year={2023}
}

@article{Perez2022IgnorePP,
  title={Ignore previous prompt: Attack techniques for language models},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  journal={arXiv preprint arXiv:2211.09527},
  year={2022}
}

@article{Perez2022RedTL,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{Schulhoff2024ThePR,
  title={The Prompt Report: A Systematic Survey of Prompting Techniques},
  author={Sander Schulhoff and Michael Ilie and Nishant Balepur and Konstantine Kahadze and Amanda Liu and Chenglei Si and Yinheng Li and Aayush Gupta and HyoJung Han and Sevien Schulhoff and Pranav Sandeep Dulepet and Saurav Vidyadhara and Dayeon Ki and Sweta Agrawal and Chau Minh Pham and Gerson C. Kroiz and Feileen Li and Hudson Tao and Ashay Srivastava and Hevander Da Costa and Saloni Gupta and Megan L. Rogers and Inna Goncearenco and Giuseppe Sarli and Igor Galynker and Denis Peskoff and Marine Carpuat and Jules White and Shyamal Anadkat and Alexander Miserlis Hoyle and Philip Resnik},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.06608},
  url={https://api.semanticscholar.org/CorpusID:270380093}
}

@article{Shaham2015UnderstandingAT,
  title={Understanding adversarial training: Increasing local stability of supervised models through robust optimization},
  author={Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
  journal={Neurocomputing},
  volume={307},
  pages={195--204},
  year={2018},
  publisher={Elsevier}
}

@article{Talebirad2023MultiAgentCH,
  title={Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents},
  author={Yashar Talebirad and Amirhossein Nadiri},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.03314},
  url={https://api.semanticscholar.org/CorpusID:259088724}
}

@inproceedings{Wang2023UnleashingTE,
  title={Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration},
  author={Zhenhailong Wang and Shaoguang Mao and Wenshan Wu and Tao Ge and Furu Wei and Heng Ji},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259765919}
}

@inproceedings{Wu2023AutoGenEN,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W. White and Doug Burger and Chi Wang},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263611068},
  booktitle={ArXiv preprint arXiv:2308.08155}
}

@article{Xu2022ExploringTU,
  title={Exploring the universal vulnerability of prompt-based learning paradigm},
  author={Xu, Lei and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Liu, Zhiyuan},
  journal={arXiv preprint arXiv:2204.05239},
  year={2022}
}

@article{Yao_2024,
  title={A survey on large language model (llm) security and privacy: The good, the bad, and the ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024},
  publisher={Elsevier}
}

@article{Yu2023GPTFUZZERRT,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}

@inproceedings{Zhang2023EffectivePE,
  title={Effective prompt extraction from language models},
  author={Zhang, Yiming and Carlini, Nicholas and Ippolito, Daphne},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{anonymous2024diverse,
  title={Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning},
  author={Beutel, Alex and Xiao, Kai and Heidecke, Johannes and Weng, Lilian},
  journal={arXiv preprint arXiv:2412.18693},
  year={2024}
}

@article{chao2024jailbreakingblackboxlarge,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{crothers2023machinegeneratedtextcomprehensive,
  title={Machine-generated text: A comprehensive survey of threat models and detection methods},
  author={Crothers, Evan N and Japkowicz, Nathalie and Viktor, Herna L},
  journal={IEEE Access},
  volume={11},
  pages={70977--71002},
  year={2023},
  publisher={IEEE}
}

@article{ganguli2022redteaminglanguagemodels,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@online{rebuff,
  author = {protectai},
  title = {rebuff},
  year = 2024,
  url = {https://github.com/protectai/rebuff},
  urldate = {2024-01-24}
}

@inproceedings{schulhoff-etal-2023-ignore,
    title = "Ignore This Title and {H}ack{AP}rompt: Exposing Systemic Vulnerabilities of {LLM}s Through a Global Prompt Hacking Competition",
    author = "Schulhoff, Sander  and
      Pinto, Jeremy  and
      Khan, Anaum  and
      Bouchard, Louis-Fran{\c{c}}ois  and
      Si, Chenglei  and
      Anati, Svetlina  and
      Tagliabue, Valen  and
      Kost, Anson  and
      Carnahan, Christopher  and
      Boyd-Graber, Jordan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.302/",
    doi = "10.18653/v1/2023.emnlp-main.302",
    pages = "4945--4977",
    abstract = "Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts."
}

@misc{shayegani2023surveyvulnerabilitieslargelanguage,
      title={Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks}, 
      author={Erfan Shayegani and Md Abdullah Al Mamun and Yu Fu and Pedram Zaree and Yue Dong and Nael Abu-Ghazaleh},
      year={2023},
      eprint={2310.10844},
      url={https://arxiv.org/abs/2310.10844}, 
}

@online{vigil-llm,
  author = {deadbits},
  title = {vigil-llm},
  year = 2024,
  url = {https://github.com/deadbits/vigil-llm},
  urldate = {2025-24-01}
}

@article{zhao2024weaktostrongjailbreakinglargelanguage,
  title={Weak-to-strong jailbreaking on large language models},
  author={Zhao, Xuandong and Yang, Xianjun and Pang, Tianyu and Du, Chao and Li, Lei and Wang, Yu-Xiang and Wang, William Yang},
  journal={arXiv preprint arXiv:2401.17256},
  year={2024}
}

