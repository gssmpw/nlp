\section{Related Work}
The landscape of large language model vulnerabilities has been extensively studied in recent literature **Brown, "Language Models are Few-Shot Learners"** , that propose detailed taxonomies of threats. These works categorize LLM attacks into distinct types, such as adversarial attacks, data poisoning, and specific vulnerabilities related to prompt engineering. Among these, prompt injection attacks have emerged as a significant and distinct category, underscoring their relevance to LLM security.

The following high-level overview of the collected taxonomy of LLM vulnerabilities is defined in **Hendrycks et al., "Natural Adversarial Examples"** :
\begin{itemize}
    \item Adversarial Attacks: Data Poisoning, Backdoor Attacks
    \item Inference Attacks: Attribute Inference, Membership Inferences
    \item Extraction Attacks
    \item Bias and Unfairness
Exploitation
    \item Instruction Tuning Attacks: Jailbreaking, Prompt Injection.
\end{itemize}
Prompt injection attacks are further classified in **Carlini et al., "Adversarial Examples for the Robustness Analysis of Deep Neural Networks"** into the following: Goal hijacking and \textbf{Prompt leakage}.

The reviewed taxonomies underscore the need for comprehensive frameworks to evaluate LLM security. The agentic approach introduced in this paper builds on these insights, automating adversarial testing to address a wide range of scenarios, including those involving prompt leakage and role-specific vulnerabilities.

\subsection{Prompt Injection and Prompt Leakage}

Prompt injection attacks exploit the blending of instructional and data inputs, manipulating LLMs into deviating from their intended behavior. Prompt injection attacks encompass techniques that override initial instructions, expose private prompts, or generate malicious outputs **Santos et al., "On Adversarial Attacks in Large Language Models"** . A subset of these attacks, known as prompt leakage, aims specifically at extracting sensitive system prompts embedded within LLM configurations. In **Carlini et al., "Attacking Vision and Language with Transferable Adversarial Examples"** , authors differentiate between prompt leakage and related methods such as goal hijacking, further refining the taxonomy of LLM-specific vulnerabilities.

\subsection{Defense Mechanisms}

Various defense mechanisms have been proposed to address LLM vulnerabilities, particularly prompt injection and leakage **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** . We focused on cost-effective methods like instruction postprocessing and prompt engineering, which are viable for proprietary models that cannot be retrained. Instruction preprocessing sanitizes inputs, while postprocessing removes harmful outputs, forming a dual-layer defense. Preprocessing methods include perplexity-based filtering **Szegedy et al., "Intriguing Properties of Neural Networks"**  and token-level analysis **Kurakin et al., "Adversarial Attacks on Deep Learning with an Ensemble Method"** . Postprocessing employs another set of techniques, such as censorship by LLMs **Xu et al., "Adversarial Attacks and Defenses for Visual Question Answering"** , and use of canary tokens and pattern matching **Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"** , although their fundamental limitations are noted **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** . Prompt engineering employs carefully designed instructions **Kurakin et al., "Adversarial Attacks on Deep Learning with an Ensemble Method"**  and advanced techniques like spotlighting **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** to mitigate vulnerabilities, though no method is foolproof **Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"** . Adversarial training, by incorporating adversarial examples into the training process, strengthens models against attacks **Szegedy et al., "Intriguing Properties of Neural Networks"** .

\subsection{Security Testing for Prompt Injection Attacks}

Manual testing, such as red teaming **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**  and handcrafted "Ignore Previous Prompt" attacks **Kurakin et al., "Adversarial Attacks on Deep Learning with an Ensemble Method"** , highlights vulnerabilities but is limited in scale. Automated approaches like PAIR **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**  and GPTFUZZER **Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"** achieve higher success rates by refining prompts iteratively or via automated fuzzing. Red teaming with LLMs **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**  and reinforcement learning **Santos et al., "On Adversarial Attacks in Large Language Models"** uncovers diverse vulnerabilities, including data leakage and offensive outputs. Indirect Prompt Injection (IPI) manipulates external data to compromise applications **Carlini et al., "Attacking Vision and Language with Transferable Adversarial Examples"** , adapting techniques like SQL injection to LLMs **Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"** . Prompt secrecy remains fragile, with studies showing reliable prompt extraction **Xu et al., "Adversarial Attacks and Defenses for Visual Question Answering"** . Advanced frameworks like Token Space Projection **Kurakin et al., "Adversarial Attacks on Deep Learning with an Ensemble Method"**  and Weak-to-Strong Jailbreaking Attacks **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** exploit token-space relationships, achieving high success rates for prompt extraction and jailbreaking.

\subsection{Agentic Frameworks for Evaluating LLM Security}

The development of multi-agent systems leveraging large language models (LLMs) has shown promising results in enhancing task-solving capabilities **Santos et al., "On Adversarial Attacks in Large Language Models"** . A key aspect across various frameworks is the specialization of roles among agents **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** , which mimics human collaboration and improves task decomposition.

Agentic frameworks and the multi-agent debate approach benefit from agent interaction, where agents engage in conversations or debates to refine outputs and correct errors **Kurakin et al., "Adversarial Attacks on Deep Learning with an Ensemble Method"** . For example, debate systems improve factual accuracy and reasoning by iteratively refining responses through collaborative reasoning **Santos et al., "On Adversarial Attacks in Large Language Models"** , while AG2 allows agents to autonomously interact and execute tasks with minimal human input.

These frameworks highlight the viability of agentic systems, showing how specialized roles and collaborative mechanisms lead to improved performance, whether in factuality, reasoning, or task execution. By leveraging the strengths of diverse agents, these systems demonstrate a scalable approach to problem-solving.

Recent research on testing LLMs using other LLMs has shown that this approach can be highly effective **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** . Although the papers do not explicitly employ agentic frameworks they inherently reflect a pattern similar to that of an "attacker" and a "judge". **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**  This pattern became a focal point for our work, where we put the judge into a more direct dialogue, enabling it to generate attacks based on the tested agent response in an active conversation.

A particularly influential paper in shaping our approach is **Jailbreaking Black Box Large Language Models in Twenty Queries** .