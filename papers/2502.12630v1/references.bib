@InProceedings{rogaway2005def,
author={Rogaway, Phillip},
editor={Maher, Michael J.},
title={On the Role Definitions in and Beyond Cryptography},
booktitle={Advances in Computer Science - ASIAN 2004. Higher-Level Decision Making},
year={2005},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={13--32},
abstract={More than new algorithms, proofs, or technologies, it is the emergence of definitions that has changed the landscape of cryptography. We describe how definitions work in modern cryptography, giving a number of examples, and we provide observations, opinions, and suggestions about the art and science of crafting them.},
isbn={978-3-540-30502-6}
}

@article {rogaway2012def,
author = {Rogaway, Phillip},
title = {Constructing cryptographic definitions},
journal = {The ISC International Journal of Information Security},
volume = {3},
number = {2},
pages = {69--76},
year  = {2012},
publisher = {Iranian Society of Cryptology},
issn = {2008-2045}, 
eissn = {2008-3076}, 
doi = {10.22042/isecure.2015.3.2.2},
abstract = {This paper mirrors an invited talk to ISCISC 2011. It is not a conventional paper so much as an essay summarizing thoughts on a little-talked-about subject. My goal is to intermix some introspection about definitions with examples of them, these examples drawn mostly from cryptography. Underpinning our discussion are two themes. The first is that definitions are constructed. They are invented by man, not unearthed from the maws of scientific reality. The second theme is that definitions matter. They have been instrumental in changing the character of modern cryptography, and, I suspect, have the potential to change the character of other fields as well.},
url = {https://www.isecure-journal.com/article_39188.html},
eprint = {https://www.isecure-journal.com/article_39188_e0940a4c698e61601af3a094963afe69.pdf}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@article{chao2024jailbreakingblackboxlarge,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{ganguli2022redteaminglanguagemodels,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{Perez2022RedTL,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{crothers2023machinegeneratedtextcomprehensive,
  title={Machine-generated text: A comprehensive survey of threat models and detection methods},
  author={Crothers, Evan N and Japkowicz, Nathalie and Viktor, Herna L},
  journal={IEEE Access},
  volume={11},
  pages={70977--71002},
  year={2023},
  publisher={IEEE}
}

@inproceedings{Greshake2023NotWY,
  title={Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  booktitle={Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
  pages={79--90},
  year={2023}
}

@article{Wei2023JailbrokenHD,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{Perez2022IgnorePP,
  title={Ignore previous prompt: Attack techniques for language models},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  journal={arXiv preprint arXiv:2211.09527},
  year={2022}
}

@article{Zou2023UniversalAT,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@inproceedings{Zhang2023EffectivePE,
  title={Effective prompt extraction from language models},
  author={Zhang, Yiming and Carlini, Nicholas and Ippolito, Daphne},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{Maus2023AdversarialPF,
  title={Adversarial prompting for black box foundation models},
  author={Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob},
  journal={arXiv preprint arXiv:2302.04237},
  volume={1},
  number={2},
  year={2023}
}

@article{Liu2023PromptIA,
  title={Prompt Injection attack against LLM-integrated Applications},
  author={Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and others},
  journal={arXiv preprint arXiv:2306.05499},
  year={2023}
}

@article{Yu2023GPTFUZZERRT,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}

@article{Mehrotra2023TreeOA,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@article{Huang2023ASO,
  title={A survey of safety and trustworthiness of large language models through the lens of verification and validation},
  author={Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and others},
  journal={Artificial Intelligence Review},
  volume={57},
  number={7},
  pages={175},
  year={2024},
  publisher={Springer}
}

@article{Li2023MultistepJP,
  title={Multi-step jailbreaking privacy attacks on chatgpt},
  author={Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Huang, Jie and Meng, Fanpu and Song, Yangqiu},
  journal={arXiv preprint arXiv:2304.05197},
  year={2023}
}

@article{zhao2024weaktostrongjailbreakinglargelanguage,
  title={Weak-to-strong jailbreaking on large language models},
  author={Zhao, Xuandong and Yang, Xianjun and Pang, Tianyu and Du, Chao and Li, Lei and Wang, Yu-Xiang and Wang, William Yang},
  journal={arXiv preprint arXiv:2401.17256},
  year={2024}
}

@article{yuan2024gpt4smartsafestealthy,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and He, Pinjia and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}

@misc{shayegani2023surveyvulnerabilitieslargelanguage,
      title={Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks}, 
      author={Erfan Shayegani and Md Abdullah Al Mamun and Yu Fu and Pedram Zaree and Yue Dong and Nael Abu-Ghazaleh},
      year={2023},
      eprint={2310.10844},
      url={https://arxiv.org/abs/2310.10844}, 
}
@article{anonymous2024diverse,
  title={Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning},
  author={Beutel, Alex and Xiao, Kai and Heidecke, Johannes and Weng, Lilian},
  journal={arXiv preprint arXiv:2412.18693},
  year={2024}
}

@article{Yao_2024,
  title={A survey on large language model (llm) security and privacy: The good, the bad, and the ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{brown2020languagemodelsfewshotlearners,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@misc{nori2023capabilitiesgpt4medicalchallenge,
      title={Capabilities of {GPT}-4 on Medical Challenge Problems}, 
      author={Harsha Nori and Nicholas King and Scott Mayer McKinney and Dean Carignan and Eric Horvitz},
      year={2023},
      eprint={2303.13375},
      url={https://arxiv.org/abs/2303.13375}, 
}
 

@article{Qian2023ChatDevCA,
  title={{ChatDev}: Communicative agents for software development},
  author={Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and others},
  journal={\url{https://arxiv.org/abs/2307}},
  volume={7924},
  year={2024}
}

@inproceedings{qian-etal-2024-chatdev,
    title = "{C}hat{D}ev: Communicative Agents for Software Development",
    author = "Qian, Chen  and
      Liu, Wei  and
      Liu, Hongzhang  and
      Chen, Nuo  and
      Dang, Yufan  and
      Li, Jiahao  and
      Yang, Cheng  and
      Chen, Weize  and
      Su, Yusheng  and
      Cong, Xin  and
      Xu, Juyuan  and
      Li, Dahai  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    url = "https://aclanthology.org/2024.acl-long.810/",
    abstract = "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev."
}

@article{lai2023largelanguagemodelslaw,
  title={Large language models in law: A survey},
  author={Lai, Jinqi and Gan, Wensheng and Wu, Jiayang and Qi, Zhenlian and Philip, S Yu},
  journal={AI Open},
  year={2024},
  publisher={Elsevier}
}

@article{Kumar2023CertifyingLS,
  title={Certifying llm safety against adversarial prompting},
  author={Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2309.02705},
  year={2023}
}

@article{Jain2023BaselineDF,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}


@article{Xu2022ExploringTU,
  title={Exploring the universal vulnerability of prompt-based learning paradigm},
  author={Xu, Lei and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Liu, Zhiyuan},
  journal={arXiv preprint arXiv:2204.05239},
  year={2022}
}


@article{Helbling2023LLMSD,
  title={Llm self defense: By self examination, llms know they are being tricked},
  author={Phute, Mansi and Helbling, Alec and Hull, Matthew and Peng, ShengYun and Szyller, Sebastian and Cornelius, Cory and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2308.07308},
  year={2023}
}


@article{Inan2023LlamaGL,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}


@article{Glukhov2023LLMCA,
  title={Llm censorship: A machine learning challenge or a computer security problem?},
  author={Glukhov, David and Shumailov, Ilia and Gal, Yarin and Papernot, Nicolas and Papyan, Vardan},
  journal={arXiv preprint arXiv:2307.10719},
  year={2023}
}


@article{Bespalov2024TowardsBA,
  title={Towards building a robust toxicity predictor},
  author={Bespalov, Dmitriy and Bhabesh, Sourav and Xiang, Yi and Zhou, Liutong and Qi, Yanjun},
  journal={arXiv preprint arXiv:2404.08690},
  year={2024}
}


@article{Morris2020TextAttackAF,
  title={Textattack: A framework for adversarial attacks in natural language processing},
  author={Morris, John X and Lifland, Eli and Yoo, Jin Yong and Qi, Yanjun},
  journal={Proceedings of the 2020 EMNLP, Arvix},
  year={2020}
}


@article{Shaham2015UnderstandingAT,
  title={Understanding adversarial training: Increasing local stability of supervised models through robust optimization},
  author={Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
  journal={Neurocomputing},
  volume={307},
  pages={195--204},
  year={2018},
  publisher={Elsevier}
}


@article{Hines2024DefendingAI,
  title={Defending Against Indirect Prompt Injection Attacks With Spotlighting},
  author={Hines, Keegan and Lopez, Gary and Hall, Matthew and Zarfati, Federico and Zunger, Yonatan and Kiciman, Emre},
  journal={arXiv preprint arXiv:2403.14720},
  year={2024}
}

@article{Schulhoff2024ThePR,
  title={The Prompt Report: A Systematic Survey of Prompting Techniques},
  author={Sander Schulhoff and Michael Ilie and Nishant Balepur and Konstantine Kahadze and Amanda Liu and Chenglei Si and Yinheng Li and Aayush Gupta and HyoJung Han and Sevien Schulhoff and Pranav Sandeep Dulepet and Saurav Vidyadhara and Dayeon Ki and Sweta Agrawal and Chau Minh Pham and Gerson C. Kroiz and Feileen Li and Hudson Tao and Ashay Srivastava and Hevander Da Costa and Saloni Gupta and Megan L. Rogers and Inna Goncearenco and Giuseppe Sarli and Igor Galynker and Denis Peskoff and Marine Carpuat and Jules White and Shyamal Anadkat and Alexander Miserlis Hoyle and Philip Resnik},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.06608},
  url={https://api.semanticscholar.org/CorpusID:270380093}
}

@inproceedings{schulhoff-etal-2023-ignore,
    title = "Ignore This Title and {H}ack{AP}rompt: Exposing Systemic Vulnerabilities of {LLM}s Through a Global Prompt Hacking Competition",
    author = "Schulhoff, Sander  and
      Pinto, Jeremy  and
      Khan, Anaum  and
      Bouchard, Louis-Fran{\c{c}}ois  and
      Si, Chenglei  and
      Anati, Svetlina  and
      Tagliabue, Valen  and
      Kost, Anson  and
      Carnahan, Christopher  and
      Boyd-Graber, Jordan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.302/",
    doi = "10.18653/v1/2023.emnlp-main.302",
    pages = "4945--4977",
    abstract = "Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts."
}

@online{vigil-llm,
  author = {deadbits},
  title = {vigil-llm},
  year = 2024,
  url = {https://github.com/deadbits/vigil-llm},
  urldate = {2025-24-01}
}

@online{rebuff,
  author = {protectai},
  title = {rebuff},
  year = 2024,
  url = {https://github.com/protectai/rebuff},
  urldate = {2024-01-24}
}

@article{Hong2023MetaGPTMP,
  title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},
  author={Sirui Hong and Xiawu Zheng and Jonathan P. Chen and Yuheng Cheng and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zi Hen Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.00352},
  url={https://api.semanticscholar.org/CorpusID:260351380}
}

@inproceedings{Wang2023UnleashingTE,
  title={Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration},
  author={Zhenhailong Wang and Shaoguang Mao and Wenshan Wu and Tao Ge and Furu Wei and Heng Ji},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259765919}
}

@article{Talebirad2023MultiAgentCH,
  title={Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents},
  author={Yashar Talebirad and Amirhossein Nadiri},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.03314},
  url={https://api.semanticscholar.org/CorpusID:259088724}
}

@inproceedings{Wu2023AutoGenEN,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W. White and Doug Burger and Chi Wang},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263611068},
  booktitle={ArXiv preprint arXiv:2308.08155}
}

@article{Du2023ImprovingFA,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.14325},
  url={https://api.semanticscholar.org/CorpusID:258841118}
}