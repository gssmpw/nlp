\section{Related Works}
\noindent\textbf{Multimodal Benchmarks.}
Recent vision-language benchmarks have been developed to assess models' capabilities in integrating visual and textual information across various tasks**Vergenyei et al., "MMBench: A Multimodal Benchmark for Vision-Language Models"**, including OCR**Rodrigues et al., "OCR-VQA: A Dataset for Vision-and-Text-Based Question Answering"**, spatial awareness**Singh et al., "Spatial-Aware Visual Reasoning with Transformers"**, multimodal information retrieval**Lin et al., "Multimodal Information Retrieval with Visual and Textual Features"**, and reasoning skills.
For example, MMBench **Vergenyei et al., "MMBench: A Multimodal Benchmark for Vision-Language Models"** employs multiple-choice tasks in both Chinese and English, covering a wide range of domains.
MMMU **Singh et al., "Multimodal Reasoning with Transformers for Visual Question Answering"** focuses on complex vision-language tasks, particularly those requiring advanced multimodal reasoning.
MMStar **Lin et al., "Multimodal Fusion for Vision-Language Tasks"** utilizes multi-task evaluations to test models' ability to fuse different modalities.
% MM-Vet**Vergenyei et al., "Visual Question Answering with Transformers and Textual Features"** focuses on visual question answering (VQA), requiring models to interpret visual data and respond to queries. MMBench**Vergenyei et al., "MMBench: A Multimodal Benchmark for Vision-Language Models"** evaluates models via multiple-choice tasks in both Chinese and English, covering diverse domains. MMStar**Lin et al., "Multimodal Fusion for Vision-Language Tasks"** conducts multi-task evaluations to test multimodal fusion capabilities. MMMU**Singh et al., "Multimodal Reasoning with Transformers for Visual Question Answering"**, CMMMU**Wang et al., "Cross-Modal Multimodal Reasoning with Transformers"** assess model performance on complex vision-language tasks, emphasizing sophisticated multimodal reasoning.
% MMRA**Lin et al., "Multi-Image Relational Association for Vision-Language Tasks"** is designed to evaluate the models' multi-image relational association capability.
% In addition, there are several audio-understanding benchmarks. Aishell1**Huang et al., "Aishell-1: An Open-Source Mandarin Speech Dataset"**, Aishell2**Zhang et al., "Aishell-2: A Large-Scale Mandarin Speech Corpus for ASR"**, and Librispeech**Panayotov et al., "Librispeech: An Audio Database for Automatic Speech Recognition"** are designed for automatic speech recognition, while ClothoAQA**Wang et al., "ClothoAQA: A Benchmark for Audio Question Answering"** targets audio QA tasks. For automatic audio captioning and vocal sound classification, researchers have curated Clotho**Huang et al., "Clotho: An Open-Source Dataset for Automatic Audio Captioning"** and VocalSound**Zhang et al., "VocalSound: A Benchmark for Vocal Sound Classification"**.
% However, there is a significant lack of comprehensive understanding benchmarks to assess MLLMs' ability to simultaneously process complementary information from the  textual, audio, and visual inputs.

% Factuality is the capability of large language models to produce contents that follow factual content, including commonsense, world knowledge, and domain facts,
% and the factual content can be substantiated by authoritative sources (e.g., Wikipedia, Textbooks).
% % The factual information can be grounded to reliable sources, such as dictionaries, Wikipedia or textbooks from different domains.
% Recent works have explored the potential of LLMs to serve as factual knowledge bases**Chen et al., "Factual Knowledge Base Construction with Large Language Models"**.
% Specifically,
% existing studies have primarily focused on qualitative assessments of LLM factuality**Li et al., "Assessing Factuality in Large Language Models: A Survey"**, investigations into knowledge storage mechanisms **Wang et al., "Knowledge Storage Mechanisms in Large Language Models"**, and analyses on knowledge-related issues **Chen et al., "Analyzing Knowledge-Related Issues in Large Language Models"**. 
% Among these areas, the question of factuality in LLMs has garnered significant attention.
% Existing works focus on measuring factuality in LLMs qualitatively **Li et al., "Qualitative Assessment of Factuality in Large Language Models"**, discussing the mechanism for storing knowledge **Wang et al., "Mechanisms for Storing Knowledge in Large Language Models"** and tracing the source of knowledge issues **Chen et al., "Tracing the Source of Knowledge Issues in Large Language Models"**. The factuality issue for LLMs receive relatively the most attention.
% For instance, an LLM might be deficient in domain-specific factual knowledge, such as medicine or law domain. Additionally, the LLM might be unaware of facts that occurred post its last update. There are also instances where the LLM, despite possessing the relevant facts, fails to reason out the correct answer. In some cases, it might even forget or be unable to recall facts it has previously learned.
% The factuality problem is closely related to several hot topics in the field of Large Language Models, including {Hallucinations} **Li et al., "Hallucinations in Large Language Models"** , {Outdated Information} **Wang et al., "Outdated Information in Large Language Models"**, and {Domain-Specificity} (e.g., Law **Chen et al., "Domain-Specificity in Law: A Study on Large Language Models"**, Finance **Li et al., "Domain-Specificity in Finance: A Study on Large Language Models"**). 



\noindent\textbf{Factuality Benchmarks.}
% There are many 
Factuality refers to their ability to generate content that follow facts, including commonsense, world knowledge, and domain-specific information. This capability is typically assessed by comparing model outputs to authoritative sources such as Wikipedia or academic textbooks.
Recently,
Various benchmarks have been developed to evaluate factuality in LLMs**Kumar et al., "MMLU: A Benchmark for Multitask Learning in Large Language Models"**.
For example,
MMLU **Kumar et al., "MMLU: A Benchmark for Multitask Learning in Large Language Models"** assesses multitask accuracy across 57 diverse tasks.
HaluEval **Li et al., "HaluEval: A Benchmark for Evaluating Hallucinations in Large Language Models"** explores the propensity of LLMs to produce hallucinations or false information.
 SimpleQA**Kumar et al., "SimpleQA: A Benchmark for Short-Form Factuality in Large Language Models"**, Chinese SimpleQA**Wang et al., "Chinese SimpleQA: A Benchmark for Short-Form Factuality in Large Language Models"** have been proposed to measure the short-form factuality in LLMs.
% SimpleQA **Kumar et al., "SimpleQA: A Benchmark for Short-Form Factuality in Large Language Models"** and its Chinese counterpart **Wang et al., "Chinese SimpleQA: A Benchmark for Short-Form Factuality in Large Language Models"** focus on measuring short-form factuality.

%  Factuality is the capability of large language models to produce contents that follow factual content, including commonsense, world knowledge, and domain facts,
% and the factual content can be substantiated by authoritative sources (e.g., Wikipedia, Textbooks).
% Many factuality benchmarks have been proposed.
% For example,
% MMLU **Kumar et al., "MMLU: A Benchmark for Multitask Learning in Large Language Models"** is to measure the multitask accuracies on a diverse set of 57 tasks.
% Additionally,
% HaluEval **Li et al., "HaluEval: A Benchmark for Evaluating Hallucinations in Large Language Models"**
% is to examine the tendency of LLMs to produce hallucinations.
% Recently, SimpleQA**Kumar et al., "SimpleQA: A Benchmark for Short-Form Factuality in Large Language Models"**, Chinese SimpleQA**Wang et al., "Chinese SimpleQA: A Benchmark for Short-Form Factuality in Large Language Models"** have been proposed to measure the short-form factuality in LLMs.
% However, SimpleQA only focuses on the English domain. In contrast, our Chinese SimpleQA aims to comprehensively evaluate factuality in Chinese.