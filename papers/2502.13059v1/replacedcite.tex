\section{Related Works}
\noindent\textbf{Multimodal Benchmarks.}
Recent vision-language benchmarks have been developed to assess models' capabilities in integrating visual and textual information across various tasks____, including OCR____, spatial awareness____, multimodal information retrieval____, and reasoning skills.
For example, MMBench ____ employs multiple-choice tasks in both Chinese and English, covering a wide range of domains.
MMMU ____ focuses on complex vision-language tasks, particularly those requiring advanced multimodal reasoning.
MMStar ____ utilizes multi-task evaluations to test models' ability to fuse different modalities.
% MM-Vet____ focuses on visual question answering (VQA), requiring models to interpret visual data and respond to queries. MMBench____ evaluates models via multiple-choice tasks in both Chinese and English, covering diverse domains. MMStar____ conducts multi-task evaluations to test multimodal fusion capabilities. MMMU____ and CMMMU____ assess model performance on complex vision-language tasks, emphasizing sophisticated multimodal reasoning.
% MMRA____ is designed to evaluate the models' multi-image relational association capability.
% In addition, there are several audio-understanding benchmarks. Aishell1____, Aishell2____, and Librispeech____ are designed for automatic speech recognition, while ClothoAQA targets audio QA tasks. For automatic audio captioning and vocal sound classification, researchers have curated Clotho____ and VocalSound____.
% However, there is a significant lack of comprehensive understanding benchmarks to assess MLLMs' ability to simultaneously process complementary information from the  textual, audio, and visual inputs.

%  Factuality is the capability of large language models to produce contents that follow factual content, including commonsense, world knowledge, and domain facts,
% and the factual content can be substantiated by authoritative sources (e.g., Wikipedia, Textbooks).
% % The factual information can be grounded to reliable sources, such as dictionaries, Wikipedia or textbooks from different domains.
% Recent works have explored the potential of LLMs to serve as factual knowledge bases____.
% Specifically,
% existing studies have primarily focused on qualitative assessments of LLM factuality____, investigations into knowledge storage mechanisms ____, and analyses on knowledge-related issues____. 
% Among these areas, the question of factuality in LLMs has garnered significant attention.
% Existing works focus on measuring factuality in LLMs qualitatively ____, discussing the mechanism for storing knowledge ____ and tracing the source of knowledge issues ____. The factuality issue for LLMs receive relatively the most attention.
% For instance, an LLM might be deficient in domain-specific factual knowledge, such as medicine or law domain. Additionally, the LLM might be unaware of facts that occurred post its last update. There are also instances where the LLM, despite possessing the relevant facts, fails to reason out the correct answer. In some cases, it might even forget or be unable to recall facts it has previously learned.
% The factuality problem is closely related to several hot topics in the field of Large Language Models, including {Hallucinations} ____, {Outdated Information} ____, and {Domain-Specificity} (e.g., Law ____, Finance ____). 



\noindent\textbf{Factuality Benchmarks.}
% There are many 
Factuality refers to their ability to generate content that follow facts, including commonsense, world knowledge, and domain-specific information. This capability is typically assessed by comparing model outputs to authoritative sources such as Wikipedia or academic textbooks.
Recently,
Various benchmarks have been developed to evaluate factuality in LLMs____.
For example,
MMLU ____ assesses multitask accuracy across 57 diverse tasks.
HaluEval ____ explores the propensity of LLMs to produce hallucinations or false information.
 SimpleQA____ and Chinese SimpleQA____ have been proposed to measure the short-form factuality in LLMs.
% SimpleQA ____ and its Chinese counterpart ____ focus on measuring short-form factuality.

%  Factuality is the capability of large language models to produce contents that follow factual content, including commonsense, world knowledge, and domain facts,
% and the factual content can be substantiated by authoritative sources (e.g., Wikipedia, Textbooks).
% Many factuality benchmarks have been proposed.
% For example,
% MMLU ____ is to measure the multitask accuracies on a diverse set of 57 tasks.
% Additionally,
% HaluEval ____
% is to examine the tendency of LLMs to produce hallucinations.
% Recently, SimpleQA____ and Chinese SimpleQA____ have been proposed to measure the short-form factuality in LLMs.
% However, SimpleQA only focuses on the English domain. In contrast, our Chinese SimpleQA aims to comprehensively evaluate factuality in Chinese.