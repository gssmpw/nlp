% @article{yao2023react,
   title={ReAct: synergizing reasoning and acting in language models (2022)},
   author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
   journal={arXiv preprint arXiv:2210.03629},
   year={2023}
 }

@misc{tan2024chinesesafetyqasafetyshortform,
      title={Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models}, 
      author={Yingshui Tan and Boren Zheng and Baihui Zheng and Kerui Cao and Huiyun Jing and Jincheng Wei and Jiaheng Liu and Yancheng He and Wenbo Su and Xiangyong Zhu and Bo Zheng},
      year={2024},
      eprint={2412.15265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15265}, 
}

@software{Liu_LlamaIndex_2022,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}

@inproceedings{lu2023instag,
  title={\# instag: Instruction tagging for analyzing supervised fine-tuning of large language models},
  author={Lu, Keming and Yuan, Hongyi and Yuan, Zheng and Lin, Runji and Lin, Junyang and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@inproceedings{Hemphill1990TheAS,
  title={The ATIS Spoken Language Systems Pilot Corpus},
  author={C. T. Hemphill and J. J. Godfrey and G. Doddington},
  booktitle={HLT},
  year={1990}
}

@article{wang2021evolving,
  title={Evolving Decomposed Plasticity Rules for Information-Bottlenecked Meta-Learning},
  author={Wang, Fan and Tian, Hao and Xiong, Haoyi and Wu, Hua and Fu, Jie and Cao, Yang and Kang, Yu and Wang, Haifeng},
  journal={arXiv preprint arXiv:2109.03554},
  year={2021}
}

@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}
@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{zador2023catalyzing,
  title={Catalyzing next-generation Artificial Intelligence through NeuroAI},
  author={Zador, Anthony and Escola, Sean and Richards, Blake and {\"O}lveczky, Bence and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and others},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={1597},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@book{hebb2005organization,
  title={The organization of behavior: A neuropsychological theory},
  author={Hebb, Donald Olding},
  year={2005},
  publisher={Psychology press}
}

@article{tay2022unifying,
  title={Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}

@inproceedings{GALACTICA,
    title={GALACTICA: A Large Language Model for Science},
    author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
    year={2022}
}

@article{vinyals2017starcraft,
  title={Starcraft ii: A new challenge for reinforcement learning},
  author={Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:1708.04782},
  year={2017}
}

@inproceedings{branavan2009reinforcement,
  title={Reinforcement learning for mapping instructions to actions},
  author={Branavan, Satchuthananthavale RK and Chen, Harr and Zettlemoyer, Luke and Barzilay, Regina},
  booktitle={Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
  pages={82--90},
  year={2009}
}

@article{yao2021reading,
  title={Reading and acting while blindfolded: The need for semantics in text game agents},
  author={Yao, Shunyu and Narasimhan, Karthik and Hausknecht, Matthew},
  journal={arXiv preprint arXiv:2103.13552},
  year={2021}
}

@article{li2022immersive,
  title={Immersive Text Game and Personality Classification},
  author={Li, Wanshui and Bai, Yifan and Lu, Jiaxuan and Yi, Kexin},
  journal={arXiv preprint arXiv:2203.10621},
  year={2022}
}

@article{dambekodi2020playing,
  title={Playing text-based games with common sense},
  author={Dambekodi, Sahith and Frazier, Spencer and Ammanabrolu, Prithviraj and Riedl, Mark O},
  journal={arXiv preprint arXiv:2012.02757},
  year={2020}
}

@article{susnjak2022chatgpt,
  title={ChatGPT: The End of Online Exam Integrity?},
  author={Susnjak, Teo},
  journal={arXiv preprint arXiv:2212.09292},
  year={2022}
}

@article{krugel2023moral,
  title={The moral authority of ChatGPT},
  author={Kr{\"u}gel, Sebastian and Ostermaier, Andreas and Uhl, Matthias},
  journal={arXiv preprint arXiv:2301.07098},
  year={2023}
}

@article{deng2023recent,
  title={Recent advances towards safe, responsible, and moral dialogue systems: A survey},
  author={Deng, Jiawen and Sun, Hao and Zhang, Zhexin and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2302.09270},
  year={2023}
}

@article{rahimi2023chatgpt,
  title={ChatGPT and publication ethics},
  author={Rahimi, Farid and Abadi, Amin Talebi Bezmin},
  journal={Archives of Medical Research},
  volume={54},
  number={3},
  pages={272--274},
  year={2023},
  publisher={Elsevier}
}

@article{zhuo2023exploring,
  title={Exploring ai ethics of chatgpt: A diagnostic analysis},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv preprint arXiv:2301.12867},
  year={2023}
}

@article{havrylov2017emergence,
  title={Emergence of language with multi-agent games: Learning to communicate with sequences of symbols},
  author={Havrylov, Serhii and Titov, Ivan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{wong2022deep,
  title={Deep multiagent reinforcement learning: Challenges and directions},
  author={Wong, Annie and B{\"a}ck, Thomas and Kononova, Anna V and Plaat, Aske},
  journal={Artificial Intelligence Review},
  pages={1--34},
  year={2022},
  publisher={Springer}
}

@article{xu2020deepRL,
  title={Deep reinforcement learning with stacked hierarchical attention for text-based games},
  author={Xu, Yunqiu and Fang, Meng and Chen, Ling and Du, Yali and Zhou, Joey Tianyi and Zhang, Chengqi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16495--16507},
  year={2020}
}

@article{chen2020ask,
  title={Ask your humans: Using human instructions to improve generalization in reinforcement learning},
  author={Chen, Valerie and Gupta, Abhinav and Marino, Kenneth},
  journal={arXiv preprint arXiv:2011.00517},
  year={2020}
}

@InProceedings{pmlr-v139-hanjie21a,
  title = 	 {Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning},
  author =       {Hanjie, Austin W. and Zhong, Victor Y and Narasimhan, Karthik},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4051--4062},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hanjie21a/hanjie21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hanjie21a.html},
  abstract = 	 {We investigate the use of natural language to drive the generalization of control policies and introduce the new multi-task environment Messenger with free-form text manuals describing the environment dynamics. Unlike previous work, Messenger does not assume prior knowledge connecting text and state observations {—} the control policy must simultaneously ground the game manual to entity symbols and dynamics in the environment. We develop a new model, EMMA (Entity Mapper with Multi-modal Attention) which uses an entity-conditioned attention module that allows for selective focus over relevant descriptions in the manual for each entity in the environment. EMMA is end-to-end differentiable and learns a latent grounding of entities and dynamics from text to observations using only environment rewards. EMMA achieves successful zero-shot generalization to unseen games with new dynamics, obtaining a 40\% higher win rate compared to multiple baselines. However, win rate on the hardest stage of Messenger remains low (10\%), demonstrating the need for additional work in this direction.}
}


@inproceedings{tennenholtz2019natural,
  title={The natural language of actions},
  author={Tennenholtz, Guy and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={6196--6205},
  year={2019},
  organization={PMLR}
}

@inproceedings{sironi2021adaptive,
  title={Adaptive General Search Framework for Games and Beyond},
  author={Sironi, Chiara F and Winands, Mark HM},
  booktitle={2021 IEEE Conference on Games (CoG)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

@article{zhang2022danli,
  title={DANLI: Deliberative Agent for Following Natural Language Instructions},
  author={Zhang, Yichi and Yang, Jianing and Pan, Jiayi and Storks, Shane and Devraj, Nikhil and Ma, Ziqiao and Yu, Keunwoo Peter and Bao, Yuwei and Chai, Joyce},
  journal={arXiv preprint arXiv:2210.12485},
  year={2022}
}

@article{zhang2023coig,
  title={Chinese Open Instruction Generalist: A Preliminary Release},
  author={Zhang, Ge and Shi, Yemin and Liu, Ruibo and Yuan, Ruibin and Li, Yizhi and Dong, Siwei and Shu, Yu and Li, Zhaoqun and Wang, Zekun and Lin, Chenghua and others},
  journal={arXiv preprint arXiv:2304.07987},
  year={2023}
}

@article{belle2023exploring,
  title={Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},
  author={Yunjie, Ji and Yong, Deng and Yan, Gong and Yiping, Peng and Qiang, Niu and Lei, Zhang and Baochang, Ma and Xiangang Li},
  journal={arXiv preprint arXiv:2303.14742},
  year={2023}
}

@article{yang2023large,
  title={Large language models can rate news outlet credibility},
  author={Yang, Kai-Cheng and Menczer, Filippo},
  journal={arXiv preprint arXiv:2304.00228},
  year={2023}
}

@article{liu2023gpteval,
  title={GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment},
  author={Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
  journal={arXiv preprint arXiv:2303.16634},
  year={2023}
}

@article{he2023annollm,
  title={AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators},
  author={He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2303.16854},
  year={2023}
}

@article{lu2023error,
  title={Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT},
  author={Lu, Qingyu and Qiu, Baopu and Ding, Liang and Xie, Liping and Tao, Dacheng},
  journal={arXiv preprint arXiv:2303.13809},
  year={2023}
}

@article{chen2023exploring,
  title={Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study},
  author={Chen, Yi and Wang, Rui and Jiang, Haiyun and Shi, Shuming and Xu, Ruifeng},
  journal={arXiv preprint arXiv:2304.00723},
  year={2023}
}

@article{luo2023chatgpt,
  title={ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization},
  author={Luo, Zheheng and Xie, Qianqian and Ananiadou, Sophia},
  journal={arXiv preprint arXiv:2303.15621},
  year={2023}
}

@article{wang2023chatgpt,
  title={Is chatgpt a good nlg evaluator? a preliminary study},
  author={Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and Qu, Jianfeng and Zhou, Jie},
  journal={arXiv preprint arXiv:2303.04048},
  year={2023}
}

@article{gao2023human,
  title={Human-like Summarization Evaluation with ChatGPT},
  author={Gao, Mingqi and Ruan, Jie and Sun, Renliang and Yin, Xunjian and Yang, Shiping and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2304.02554},
  year={2023}
}

@article{gilardi2023chatgpt,
  title={Chatgpt outperforms crowd-workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={arXiv preprint arXiv:2303.15056},
  year={2023}
}

@article{van2021human,
  title={Human evaluation of automatically generated text: Current trends and best practice guidelines},
  author={van der Lee, Chris and Gatt, Albert and van Miltenburg, Emiel and Krahmer, Emiel},
  journal={Computer Speech \& Language},
  volume={67},
  pages={101151},
  year={2021},
  publisher={Elsevier}
}

@article{celikyilmaz2020evaluation,
  title={Evaluation of text generation: A survey},
  author={Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2006.14799},
  year={2020}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

% xsum
@article{Narayan2018DontGM,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.08745}
}

% writing prompts
@inproceedings{fan2018hierarchical,
  title = {Hierarchical Neural Story Generation},
  author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  booktitle = {Conference of the Association for Computational Linguistics (ACL)},
  year = 2018,
}

% roc stories
@article{mostafazadeh2016corpus,
  title={A corpus and evaluation framework for deeper understanding of commonsense stories},
  author={Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
  journal={arXiv preprint arXiv:1604.01696},
  year={2016}
}

% summ metrics 1
@inproceedings{chen2020corpus,
  title={A Corpus of Very Short Scientific Summaries},
  author={Chen, Yifan and Polajnar, Tamara and Batchelor, Colin and Teufel, Simone},
  booktitle={Proceedings of the 24th Conference on Computational Natural Language Learning},
  pages={153--164},
  year={2020}
}

% summ metrics 2
@article{khashabi2021genie,
  title={GENIE: Toward Reproducible and Standardized Human Evaluation for Text Generation},
  author={Khashabi, Daniel and Stanovsky, Gabriel and Bragg, Jonathan and Lourie, Nicholas and Kasai, Jungo and Choi, Yejin and Smith, Noah A and Weld, Daniel S},
  journal={arXiv preprint arXiv:2101.06561},
  year={2021}
}

% summ metrics 3
@article{fabbri2021summeval,
  title={Summeval: Re-evaluating summarization evaluation},
  author={Fabbri, Alexander R and Kry{\'s}ci{\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={391--409},
  year={2021},
  publisher={MIT Press}
}

% story metrics 1 & hint
@article{guan2021long,
  title={Long text generation by modeling sentence-level and discourse-level coherence},
  author={Guan, Jian and Mao, Xiaoxi and Fan, Changjie and Liu, Zitao and Ding, Wenbiao and Huang, Minlie},
  journal={arXiv preprint arXiv:2105.08963},
  year={2021}
}

% story metrics 2
@article{wang2022open,
  title={Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey},
  author={Wang, Yuxin and Lin, Jieru and Yu, Zhiwei and Hu, Wei and Karlsson, B{\"o}rje F},
  journal={arXiv preprint arXiv:2212.04634},
  year={2022}
}

% story metrics 3
@article{yang2022re3,
  title={Re3: Generating longer stories with recursive reprompting and revision},
  author={Yang, Kevin and Peng, Nanyun and Tian, Yuandong and Klein, Dan},
  journal={arXiv preprint arXiv:2210.06774},
  year={2022}
}

% potato
@inproceedings{pei2022potato,
  title={POTATO: The Portable Text Annotation Tool},
  author={Pei, Jiaxin and Ananthasubramaniam, Aparna and Wang, Xingyao and Zhou, Naitian and Dedeloudis, Apostolos and Sargent, Jackson and Jurgens, David},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  year={2022}
}


% Brio
@article{liu2022brio,
  title={BRIO: Bringing order to abstractive summarization},
  author={Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham},
  journal={arXiv preprint arXiv:2203.16804},
  year={2022}
}

% gpt 2
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

% SimCTG
@article{su2022contrastive,
  title={A Contrastive Framework for Neural Text Generation},
  author={Su, Yixuan and Lan, Tian and Wang, Yan and Yogatama, Dani and Kong, Lingpeng and Collier, Nigel},
  journal={arXiv preprint arXiv:2202.06417},
  year={2022}
}

@article{zhu2023fireball,
  title={FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information},
  author={Zhu, Andrew and Aggarwal, Karmanya and Feng, Alexander and Martin, Lara J and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:2305.01528},
  year={2023}
}

@article{kramar2022negotiation,
  title={Negotiation and honesty in artificial intelligence methods for the board game of Diplomacy},
  author={Kram{\'a}r, J{\'a}nos and Eccles, Tom and Gemp, Ian and Tacchetti, Andrea and McKee, Kevin R and Malinowski, Mateusz and Graepel, Thore and Bachrach, Yoram},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={7214},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{jakesch2023co,
  title={Co-Writing with Opinionated Language Models Affects Users’ Views},
  author={Jakesch, Maurice and Bhat, Advait and Buschek, Daniel and Zalmanson, Lior and Naaman, Mor},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2023}
}

@inproceedings{dang2023choice,
  title={Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting},
  author={Dang, Hai and Goller, Sven and Lehmann, Florian and Buschek, Daniel},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2023}
}

@article{singh2022hide,
  title={Where to hide a stolen elephant: Leaps in creative writing with multimodal machine intelligence},
  author={Singh, Nikhil and Bernal, Guillermo and Savchenko, Daria and Glassman, Elena L},
  journal={ACM Transactions on Computer-Human Interaction},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{dang2022beyond,
  title={Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries},
  author={Dang, Hai and Benharrak, Karim and Lehmann, Florian and Buschek, Daniel},
  booktitle={Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--13},
  year={2022}
}

@inproceedings{arnold2021generative,
  title={Generative Models can Help Writers without Writing for Them.},
  author={Arnold, Kenneth C and Volzer, April M and Madrid, Noah G},
  booktitle={IUI Workshops},
  year={2021}
}

@article{zhang2022active,
  title={Active example selection for in-context learning},
  author={Zhang, Yiming and Feng, Shi and Tan, Chenhao},
  journal={arXiv preprint arXiv:2211.04486},
  year={2022}
}

@inproceedings{NEURIPS2019_0af78794,
 author = {Jiang, YiDing and Gu, Shixiang (Shane) and Murphy, Kevin P and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Language as an Abstraction for Hierarchical Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0af787945872196b42c9f73ead2565c8-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{ein-dor-etal-2020-active,
    title = "{A}ctive {L}earning for {BERT}: {A}n {E}mpirical {S}tudy",
    author = "Ein-Dor, Liat  and
      Halfon, Alon  and
      Gera, Ariel  and
      Shnarch, Eyal  and
      Dankin, Lena  and
      Choshen, Leshem  and
      Danilevsky, Marina  and
      Aharonov, Ranit  and
      Katz, Yoav  and
      Slonim, Noam",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.638",
    doi = "10.18653/v1/2020.emnlp-main.638",
    pages = "7949--7962",
    abstract = "Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.",
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{margatina-etal-2022-importance,
    title = "On the Importance of Effectively Adapting Pretrained Language Models for Active Learning",
    author = "Margatina, Katerina  and
      Barrault, Loic  and
      Aletras, Nikolaos",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.93",
    doi = "10.18653/v1/2022.acl-short.93",
    pages = "825--836",
    abstract = "Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.",
}


@article{zhang2022allsh,
  title={Allsh: Active learning guided by local sensitivity and hardness},
  author={Zhang, Shujian and Gong, Chengyue and Liu, Xingchao and He, Pengcheng and Chen, Weizhu and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:2205.04980},
  year={2022}
}

@inproceedings{seo2022active,
  title={Active Learning on Pre-trained Language Model with Task-Independent Triplet Loss},
  author={Seo, Seungmin and Kim, Donghyun and Ahn, Youbin and Lee, Kyong-Ho},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11276--11284},
  year={2022}
}

@inproceedings{settles-craven-2008-analysis,
    title = "An Analysis of Active Learning Strategies for Sequence Labeling Tasks",
    author = "Settles, Burr  and
      Craven, Mark",
    booktitle = "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2008",
    address = "Honolulu, Hawaii",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D08-1112",
    pages = "1070--1079",
}


@inproceedings{tong2000support,
  title={Support vector machine active learning with applications to text classification},
  author={Tong, Simon},
  booktitle={Proc. Seventeenth International Conference on Machine Learning, 2000},
  year={2000}
}

@inproceedings{yu-etal-2022-actune,
    title = "{A}c{T}une: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
    author = "Yu, Yue  and
      Kong, Lingkai  and
      Zhang, Jieyu  and
      Zhang, Rongzhi  and
      Zhang, Chao",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.102",
    doi = "10.18653/v1/2022.naacl-main.102",
    pages = "1422--1436",
    abstract = "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model{'}s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2{\%} on average. Our implementation is available at \url{https://github.com/yueyu1030/actune}.",
}


@inproceedings{zhao-etal-2020-active,
    title = "Active Learning Approaches to Enhancing Neural Machine Translation",
    author = "Zhao, Yuekai  and
      Zhang, Haoran  and
      Zhou, Shuchang  and
      Zhang, Zhihua",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.162",
    doi = "10.18653/v1/2020.findings-emnlp.162",
    pages = "1796--1806",
    abstract = "Active learning is an efficient approach for mitigating data dependency when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating active learning into various techniques such as transfer learning and iterative back-translation (IBT) under a limited human translation budget. We design a word frequency based acquisition function and combine it with a strong uncertainty based method. The combined method steadily outperforms all other acquisition functions in various scenarios. As far as we know, we are the first to do a large-scale study on actively training Transformer for NMT. Specifically, with a human translation budget of only 20{\%} of the original parallel corpus, we manage to surpass Transformer trained on the entire parallel corpus in three language pairs.",
}


@article{shen2021active,
  title={Active Learning for Event Extraction with Memory-based Loss Prediction Model},
  author={Shen, Shirong and Li, Zhen and Qi, Guilin},
  journal={arXiv preprint arXiv:2112.03073},
  year={2021}
}

@inproceedings{zhang-plank-2021-cartography-active,
    title = "Cartography Active Learning",
    author = "Zhang, Mike  and
      Plank, Barbara",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.36",
    doi = "10.18653/v1/2021.findings-emnlp.36",
    pages = "395--406",
    abstract = "We propose Cartography Active Learning (CAL), a novel Active Learning (AL) algorithm that exploits the behavior of the model on individual instances during training as a proxy to find the most informative instances for labeling. CAL is inspired by data maps, which were recently proposed to derive insights into dataset quality (Swayamdipta et al., 2020). We compare our method on popular text classification tasks to commonly used AL strategies, which instead rely on post-training behavior. We demonstrate that CAL is competitive to other common AL methods, showing that training dynamics derived from small seed data can be successfully used for AL. We provide insights into our new AL method by analyzing batch-level statistics utilizing the data maps. Our results further show that CAL results in a more data-efficient learning strategy, achieving comparable or better results with considerably less training data.",
}


@inproceedings{shelmanov-etal-2021-active,
    title = "Active Learning for Sequence Tagging with Deep Pre-trained Models and {B}ayesian Uncertainty Estimates",
    author = "Shelmanov, Artem  and
      Puzyrev, Dmitri  and
      Kupriyanova, Lyubov  and
      Belyakov, Denis  and
      Larionov, Daniil  and
      Khromov, Nikita  and
      Kozlova, Olga  and
      Artemova, Ekaterina  and
      Dylov, Dmitry V.  and
      Panchenko, Alexander",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.145",
    doi = "10.18653/v1/2021.eacl-main.145",
    pages = "1698--1712",
    abstract = "Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in transfer learning for natural language processing in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pre-trained models in the active learning framework and find the best combinations for different types of models. Besides, we also demonstrate that to acquire instances during active learning, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice.",
}


@inproceedings{schroder-etal-2022-revisiting,
    title = "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers",
    author = {Schr{\"o}der, Christopher  and
      Niekler, Andreas  and
      Potthast, Martin},
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.172",
    doi = "10.18653/v1/2022.findings-acl.172",
    pages = "2194--2203",
    abstract = "Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models ({``}transformers{''}) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
}


@inproceedings{zhang-etal-2022-survey,
    title = "A Survey of Active Learning for Natural Language Processing",
    author = "Zhang, Zhisong  and
      Strubell, Emma  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.414",
    pages = "6166--6190",
    abstract = "In this work, we provide a literature review of active learning (AL) for its applications in natural language processing (NLP). In addition to a fine-grained categorization of query strategies, we also investigate several other important aspects of applying AL to NLP problems. These include AL for structured prediction tasks, annotation cost, model learning (especially with deep neural models), and starting and stopping AL. Finally, we conclude with a discussion of related topics and future directions.",
}


@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{kiseleva2022interactive,
  title={Interactive grounded language understanding in a collaborative environment: Iglu 2021},
  author={Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and others},
  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},
  pages={146--161},
  year={2022},
  organization={PMLR}
}

@article{mehta2023improving,
  title={Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback},
  author={Mehta, Nikhil and Teruel, Milagro and Sanz, Patricio Figueroa and Deng, Xin and Awadallah, Ahmed Hassan and Kiseleva, Julia},
  journal={arXiv preprint arXiv:2304.10750},
  year={2023}
}

@article{meta2022human,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Meta, Fundamental AI Research Diplomacy Team (FAIR)† and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1067--1074},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{ammanabrolu2019toward,
  title={Toward automated quest generation in text-adventure games},
  author={Ammanabrolu, Prithviraj and Broniec, William and Mueller, Alex and Paul, Jeremy and Riedl, Mark O},
  journal={arXiv preprint arXiv:1909.06283},
  year={2019}
}

@inproceedings{zhang2020pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}

@article{karimi2021compacter,
  title={Compacter: Efficient low-rank hypercomplex adapter layers},
  author={Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1022--1035},
  year={2021}
}

@article{zhang2021beyond,
  title={Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with $1/n $ parameters},
  author={Zhang, Aston and Tay, Yi and Zhang, Shuai and Chan, Alvin and Luu, Anh Tuan and Hui, Siu Cheung and Fu, Jie},
  journal={arXiv preprint arXiv:2102.08597},
  year={2021}
}

@inproceedings{liu-etal-2022-aligning,
    title = "Aligning Generative Language Models with Human Values",
    author = "Liu, Ruibo  and
      Zhang, Ge  and
      Feng, Xinyu  and
      Vosoughi, Soroush",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.18",
    doi = "10.18653/v1/2022.findings-naacl.18",
    pages = "241--252",
    abstract = "Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral). Existing methods learn human values either by directly mimicking the behavior of human data, or rigidly constraining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values.This paper proposes SENSEI, a new reinforcement learning based method that can embed human values judgements into each step of language generation. SENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, SENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.",
}

@article{wu2022personalized,
  title   = {Personalized Prompt for Sequential Recommendation},
  author  = {Yiqing Wu and Ruobing Xie and Yongchun Zhu and Fuzhen Zhuang and Xu Zhang and Leyu Lin and Qing He},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2205.09666}
}

@article{wei2023multi,
  title={Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models},
  author={Wei, Jimmy and Shuster, Kurt and Szlam, Arthur and Weston, Jason and Urbanek, Jack and Komeili, Mojtaba},
  journal={arXiv preprint arXiv:2304.13835},
  year={2023}
}

@article{bao2022vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32897--32912},
  year={2022}
}




@inproceedings{lu2021neurologic,
  title={NeuroLogic Decoding:(Un) supervised Neural Text Generation with Predicate Logic Constraints},
  author={Lu, Ximing and West, Peter and Zellers, Rowan and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4288--4299},
  year={2021}
}



@misc{zhou2023controlled,
      title={Controlled Text Generation with Natural Language Instructions}, 
      author={Wangchunshu Zhou and Yuchen Eleanor Jiang and Ethan Wilcox and Ryan Cotterell and Mrinmaya Sachan},
      year={2023},
      eprint={2304.14293},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chada2023momo,
  title={MoMo: A shared encoder Model for text, image and multi-Modal representations},
  author={Chada, Rakesh and Zheng, Zhaoheng and Natarajan, Pradeep},
  journal={arXiv preprint arXiv:2304.05523},
  year={2023}
}

@inproceedings{singh2022flava,
  title={Flava: A foundational language and vision alignment model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15638--15650},
  year={2022}
}

@inproceedings{kim2021vilt,
  title={Vilt: Vision-and-language transformer without convolution or region supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={International Conference on Machine Learning},
  pages={5583--5594},
  year={2021},
  organization={PMLR}
}

@inproceedings{baevski2022data2vec,
  title={Data2vec: A general framework for self-supervised learning in speech, vision and language},
  author={Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  booktitle={International Conference on Machine Learning},
  pages={1298--1312},
  year={2022},
  organization={PMLR}
}

@article{xu2020layoutlmv2,
  title={Layoutlmv2: Multi-modal pre-training for visually-rich document understanding},
  author={Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and others},
  journal={arXiv preprint arXiv:2012.14740},
  year={2020}
}

@article{fu2023gptscore,
  title   = {GPTScore: Evaluate as You Desire},
  author  = {Jinlan Fu and See-Kiong Ng and Zhengbao Jiang and Pengfei Liu},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.04166}
}




@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{zeng2021pangu,
  title={PanGu-$\alpha$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation},
  author={Zeng, Wei and Ren, Xiaozhe and Su, Teng and Wang, Hui and Liao, Yi and Wang, Zhiwei and Jiang, Xin and Yang, ZhenZhang and Wang, Kaisheng and Zhang, Xiaoda and others},
  journal={arXiv preprint arXiv:2104.12369},
  year={2021}
}

@article{wang2021ernie,
  title={Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation},
  author={Wang, Shuohuan and Sun, Yu and Xiang, Yang and Wu, Zhihua and Ding, Siyu and Gong, Weibao and Feng, Shikun and Shang, Junyuan and Zhao, Yanbin and Pang, Chao and others},
  journal={arXiv preprint arXiv:2112.12731},
  year={2021}
}

@article{xue2020mt5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{iandola2020squeezebert,
  title={SqueezeBERT: What can computer vision teach NLP about efficient neural networks?},
  author={Iandola, Forrest N and Shaw, Albert E and Krishna, Ravi and Keutzer, Kurt W},
  journal={arXiv preprint arXiv:2006.11316},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{sun2020ernie,
  title={Ernie 2.0: A continual pre-training framework for language understanding},
  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8968--8975},
  year={2020}
}

@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press}
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}


@article{koksal2022meal,
  title={MEAL: Stable and Active Learning for Few-Shot Prompting},
  author={K{\"o}ksal, Abdullatif and Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2211.08358},
  year={2022}
}

@article{elnaggar2021prottrans,
  title={Prottrans: Toward understanding the language of life through self-supervised learning},
  author={Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={10},
  pages={7112--7127},
  year={2021},
  publisher={IEEE}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={arXiv preprint arXiv:2206.14858},
  year={2022}
}


@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}


@article{shen2023parachute,
  title={Parachute: Evaluating Interactive Human-LM Co-writing Systems},
  author={Shen, Hua and Wu, Tongshuang},
  journal={arXiv preprint arXiv:2303.06333},
  year={2023}
}

@inproceedings{lee2022coauthor,
  title={Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities},
  author={Lee, Mina and Liang, Percy and Yang, Qian},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@article{ippolito2022creative,
  title={Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers},
  author={Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Burnam, Sehmon},
  journal={arXiv preprint arXiv:2211.05030},
  year={2022}
}

@inproceedings{tang-etal-2022-ngep,
    title = "{NGEP}: A Graph-based Event Planning Framework for Story Generation",
    author = "Tang, Chen  and
      Zhang, Zhihao  and
      Loakman, Tyler  and
      Lin, Chenghua  and
      Guerin, Frank",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-short.24",
    pages = "186--193"
}

@inproceedings{li-etal-2023-framebert,
    title = "{F}rame{BERT}: Conceptual Metaphor Detection with Frame Embedding Learning",
    author = "Li, Yucheng  and
      Wang, Shun  and
      Lin, Chenghua  and
      Guerin, Frank  and
      Barrault, Loic",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.114",
    pages = "1558--1563"
}

@inproceedings{li-etal-2022-cm,
    title = "{CM}-Gen: A Neural Framework for {C}hinese Metaphor Generation with Explicit Context Modelling",
    author = "Li, Yucheng  and
      Lin, Chenghua  and
      Guerin, Frank",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.563",
    pages = "6468--6479",
    abstract = "Nominal metaphors are frequently used in human language and have been shown to be effective in persuading, expressing emotion, and stimulating interest. This paper tackles the problem of Chinese Nominal Metaphor (NM) generation. We introduce a novel multitask framework, which jointly optimizes three tasks: NM identification, NM component identification, and NM generation. The metaphor identification module is able to perform a self-training procedure, which discovers novel metaphors from a large-scale unlabeled corpus for NM generation. The NM component identification module emphasizes components during training and conditions the generation on these NM components for more coherent results. To train the NM identification and component identification modules, we construct an annotated corpus consisting of 6.3k sentences that contain diverse metaphorical patterns. Automatic metrics show that our method can produce diverse metaphors with good readability, where 92{\%} of them are novel metaphorical comparisons. Human evaluation shows our model significantly outperforms baselines on consistency and creativity.",
}


@inproceedings{lederer-2016-finding,
    title = "Finding metaphorical triggers through source (not target) domain lexicalization patterns",
    author = "Lederer, Jenny",
    booktitle = "Proceedings of the Fourth Workshop on Metaphor in {NLP}",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-1101",
    doi = "10.18653/v1/W16-1101",
    pages = "1--9",
}


@article{jiang2019accelerating,
  title={Accelerating deep learning by focusing on the biggest losers},
  author={Jiang, Angela H and Wong, Daniel L-K and Zhou, Giulio and Andersen, David G and Dean, Jeffrey and Ganger, Gregory R and Joshi, Gauri and Kaminksy, Michael and Kozuch, Michael and Lipton, Zachary C and others},
  journal={arXiv preprint arXiv:1910.00762},
  year={2019}
}


@inproceedings{yu2019avoid,
  title={How to avoid sentences spelling boring? towards a neural approach to unsupervised metaphor generation},
  author={Yu, Zhiwei and Wan, Xiaojun},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={861--871},
  year={2019}
}

@article{fokcan,
  title={What Can’t Large Language Models Do? The Future of AI-Assisted Academic Writing},
  author={Fok, Raymond and Weld, Daniel S}
}

@inproceedings{cheng-etal-2021-guiding,
    title = "Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting",
    author = "Cheng, Yi  and
      Li, Siyao  and
      Liu, Bang  and
      Zhao, Ruihui  and
      Li, Sujian  and
      Lin, Chenghua  and
      Zheng, Yefeng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.465",
    doi = "10.18653/v1/2021.acl-long.465",
    pages = "5968--5978"
}

@article{yang2022doc,
  title={DOC: Improving Long Story Coherence With Detailed Outline Control},
  author={Yang, Kevin and Klein, Dan and Peng, Nanyun and Tian, Yuandong},
  journal={arXiv preprint arXiv:2212.10077},
  year={2022}
}

@article{mirowski2022co,
  title={Co-writing screenplays and theatre scripts with language models: An evaluation by industry professionals},
  author={Mirowski, Piotr and Mathewson, Kory W and Pittman, Jaylen and Evans, Richard},
  journal={arXiv preprint arXiv:2209.14958},
  year={2022}
}

@Article{tool-embodiment,
author={Weser, Veronica
and Proffitt, Dennis R.},
title={Tool Embodiment: The Tool's Output Must Match the User's Input},
journal={Frontiers in Human Neuroscience},
year={2019},
volume={12},
abstract={The embodiment of tools and rubber hands is believed to involve the modification of two separate body representations: the body schema and the body image, respectively. It is thought that tools extend the capabilities of the body's action schema, whereas prosthetics like rubber hands are incorporated into the body image itself. Contrary to this dichotomy, recent research demonstrated that chopsticks can be embodied perceptually during a modified version of the rubber hand illusion (RHI) in which tools are held by the rubber hand and by the participant. In the present research, two experiments examined tool morpho-functional (tool output affordance, e.g., precision grasping) and sensorimotor (tool input, e.g., precision grip) match as a mechanism for this tool-use dependent change to the body image. Proprioceptive drift in the RHI occurred when the tool's output and the user's input matched, but not when this match was absent. This suggests that this factor may be necessary for tools to interact with the body image in the RHI.},
note={Original Research},
issn={1662-5161},
url={https://www.frontiersin.org/articles/10.3389/fnhum.2018.00537}
}



@inproceedings{hamalainen-2018-poem,
    title = "Poem Machine - a Co-creative {NLG} Web Application for Poem Writing",
    author = {H{\"a}m{\"a}l{\"a}inen, Mika},
    booktitle = "Proceedings of the 11th International Conference on Natural Language Generation",
    month = nov,
    year = "2018",
    address = "Tilburg University, The Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6525",
    doi = "10.18653/v1/W18-6525",
    pages = "195--196",
    abstract = "We present Poem Machine, an interactive online tool for co-authoring Finnish poetry with a computationally creative agent. Poem Machine can produce poetry of its own and assist the user in authoring poems. The main target group for the system is primary school children, and its use as a part of teaching is currently under study.",
}

@article{borji2023categorical,
  title     = {A Categorical Archive of ChatGPT Failures},
  author    = {A. Borji},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2302.03494},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cd0988714ea326642d2b1bb18753e187fec71e42}
}

@article{communicative-learning,
title = {Communicative Learning: A Unified Learning Formalism},
journal = {Engineering},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S2095809923001339},
author = {Luyao Yuan and Song-Chun Zhu},
keywords = {Artificial intelligencehine, Cooperative communication, Machine learning, Pedagogy, Theory of mind},
abstract = {In this article, we propose a communicative learning (CL) formalism that unifies existing machine learning paradigms, such as passive learning, active learning, algorithmic teaching, and so forth, and facilitates the development of new learning methods. Arising from human cooperative communication, this formalism poses learning as a communicative process and combines pedagogy with the burgeoning field of machine learning. The pedagogical insight facilitates the adoption of alternative information sources in machine learning besides randomly sampled data, such as intentional messages given by a helpful teacher. More specifically, in CL, a teacher and a student exchange information with each other collaboratively to transmit and acquire certain knowledge. Each agent has a mind, which includes the agent’s knowledge, utility, and mental dynamics. To establish effective communication, each agent also needs an estimation of its partner’s mind. We define expressive mental representations and learning formulation sufficient for such recursive modeling, which endows CL with human-comparable learning efficiency. We demonstrate the application of CL to several prototypical collaboration tasks and illustrate that this formalism allows learning protocols to go beyond Shannon’s communication limit. Finally, we present our contribution to the foundations of learning by putting forth hierarchies in learning and defining the halting problem of learning.}
}

@article{qin2023tool,
  title     = {Tool Learning with Foundation Models},
  author    = {Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Y. Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shi Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bo Li and Ziwei Tang and Jing Yi and Yu Zhu and Zhenning Dai and Lan Yan and Xin Cong and Ya-Ting Lu and Weilin Zhao and Yuxiang Huang and Jun-Han Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2304.08354},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/01f9b773408115a16fe872147348db175789e82f}
}


@inproceedings{oliveira2017co,
  title={Co-poetryme: a co-creative interface for the composition of poetry},
  author={Oliveira, Hugo Gon{\c{c}}alo and Mendes, Tiago and Boavida, Ana},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={70--71},
  year={2017}
}

@article{chakrabarty2022help,
  title={Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing},
  author={Chakrabarty, Tuhin and Padmakumar, Vishakh and He, He},
  journal={arXiv preprint arXiv:2210.13669},
  year={2022}
}

@inproceedings{astigarraga-etal-2017-poets,
    title = "Poet{'}s Little Helper: A methodology for computer-based poetry generation. A case study for the {B}asque language",
    author = "Astigarraga, Aitzol  and
      Mar{\'\i}a Mart{\'\i}nez-Otzeta, Jos{\'e}  and
      Rodriguez, Igor  and
      Sierra, Basilio  and
      Lazkano, Elena",
    booktitle = "Proceedings of the Workshop on Computational Creativity in Natural Language Generation ({CC}-{NLG} 2017)",
    month = sep,
    year = "2017",
    address = "Santiago de Compostela, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3901",
    doi = "10.18653/v1/W17-3901",
    pages = "2--10",
}


@inproceedings{tapscott-etal-2018-generating,
    title = "Generating Stories Using Role-playing Games and Simulated Human-like Conversations",
    author = "Tapscott, Alan  and
      Le{\'o}n, Carlos  and
      Gerv{\'a}s, Pablo",
    booktitle = "Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation ({CC}-{NLG} 2018)",
    month = nov,
    year = "2018",
    address = "Tilburg, the Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6606",
    doi = "10.18653/v1/W18-6606",
    pages = "34--42",
}


@inproceedings{manjavacas-etal-2017-synthetic,
    title = "Synthetic Literature: Writing Science Fiction in a Co-Creative Process",
    author = "Manjavacas, Enrique  and
      Karsdorp, Folgert  and
      Burtenshaw, Ben  and
      Kestemont, Mike",
    booktitle = "Proceedings of the Workshop on Computational Creativity in Natural Language Generation ({CC}-{NLG} 2017)",
    month = sep,
    year = "2017",
    address = "Santiago de Compostela, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3904",
    doi = "10.18653/v1/W17-3904",
    pages = "29--37",
}


@inproceedings{lane2020interactive,
  title={Interactive word completion for morphologically complex languages},
  author={Lane, William and Bird, Steven},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={4600--4611},
  year={2020}
}

@article{xie2023data,
  title={Data Selection for Language Models via Importance Resampling},
  author={Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2302.03169},
  year={2023}
}

@article{zhou2022ai,
  title     = {An AI Dungeon Master's Guide: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons},
  author    = {Pei Zhou and Andrew Zhu and Jennifer Hu and J. Pujara and Xiang Ren and Chris Callison-Burch and Yejin Choi and Prithviraj Ammanabrolu},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2212.10060},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/9b93ebd7ff4d995ad92902096f6c55d9451c2239}
}

@inproceedings{carlsson-etal-2022-fine,
    title = "Fine-Grained Controllable Text Generation Using Non-Residual Prompting",
    author = {Carlsson, Fredrik  and
      {\"O}hman, Joey  and
      Liu, Fangyu  and
      Verlinden, Severine  and
      Nivre, Joakim  and
      Sahlgren, Magnus},
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.471",
    doi = "10.18653/v1/2022.acl-long.471",
    pages = "6837--6857",
    abstract = "The introduction of immensely large Causal Language Models (CLMs) has rejuvenated the interest in open-ended text generation. However, controlling the generative process for these Transformer-based models is at large an unsolved problem. Earlier work has explored either plug-and-play decoding strategies, or more powerful but blunt approaches such as prompting. There hence currently exists a trade-off between fine-grained control, and the capability for more expressive high-level instructions. To alleviate this trade-off, we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps. We propose a resource-efficient method for converting a pre-trained CLM into this architecture, and demonstrate its potential on various experiments, including the novel task of contextualized word inclusion. Our method provides strong results on multiple experimental settings, proving itself to be both expressive and versatile.",
}

@inproceedings{contrastive-prefixes,
    title = "Controllable Natural Language Generation with Contrastive Prefixes",
    author = "Qian, Jing  and
      Dong, Li  and
      Shen, Yelong  and
      Wei, Furu  and
      Chen, Weizhu",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.229",
    doi = "10.18653/v1/2022.findings-acl.229",
    pages = "2912--2924",
    abstract = "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
}

@inproceedings{zheng-etal-2022-knowledge,
    title = "Knowledge Stimulated Contrastive Prompting for Low-Resource Stance Detection",
    author = "Zheng, Kai  and
      Sun, Qingfeng  and
      Yang, Yaming  and
      Xu, Fei",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.83",
    pages = "1168--1178",
    abstract = "Stance Detection Task (SDT) aims at identifying the stance of the sentence towards a specific target and is usually modeled as a classification problem. Backgound knowledge is often necessary for stance detection with respect to a specific target, especially when there is no target explicitly mentioned in text. This paper focuses on the knowledge stimulation for low-resource stance detection tasks. We firstly explore to formalize stance detection as a prompt based contrastive learning task. At the same time, to make prompt learning suit to stance detection, we design a template mechanism to incorporate corresponding target into instance representation. Furthermore, we propose a masked language prompt joint contrastive learning approach to stimulate the knowledge inherit from the pre-trained model. The experimental results on three benchmarks show that knowledge stimulation is effective in stance detection accompanied with our proposed mechanism.",
}


@inproceedings{sha-2020-gradient,
    title = "Gradient-guided Unsupervised Lexically Constrained Text Generation",
    author = "Sha, Lei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.701",
    doi = "10.18653/v1/2020.emnlp-main.701",
    pages = "8692--8703",
    abstract = "Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation. However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution. At the same time, stochastic search methods always cost too many steps to find the correct optimization direction. In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem. We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word). The word updating process of the inserted/replaced word also benefits from the guidance of gradient. Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model. We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation. The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.",
}


@article{nie2022lexical,
  title={Lexical Complexity Controlled Sentence Generation},
  author={Nie, Jinran and Yang, Liner and Chen, Yun and Kong, Cunliang and Zhu, Junhui and Yang, Erhong},
  journal={arXiv preprint arXiv:2211.14540},
  year={2022}
}

@inproceedings{miao2019cgmh,
  title={Cgmh: Constrained sentence generation by metropolis-hastings sampling},
  author={Miao, Ning and Zhou, Hao and Mou, Lili and Yan, Rui and Li, Lei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6834--6842},
  year={2019}
}

@article{settles2009active,
  title={Active learning literature survey},
  author={Settles, Burr},
  year={2009},
  publisher={University of Wisconsin-Madison Department of Computer Sciences}
}


@inproceedings{casacuberta-etal-2022-findings,
    title = "Findings of the Word-Level {A}uto{C}ompletion Shared Task in {WMT} 2022",
    author = "Casacuberta, Francisco  and
      Foster, George  and
      Huang, Guoping  and
      Koehn, Philipp  and
      Kovacs, Geza  and
      Liu, Lemao  and
      Shi, Shuming  and
      Watanabe, Taro  and
      Zong, Chengqing",
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.75",
    pages = "812--820",
    abstract = "Recent years have witnessed rapid advancements in machine translation, but the state-of-the-art machine translation system still can not satisfy the high requirements in some rigorous translation scenarios. Computer-aided translation (CAT) provides a promising solution to yield a high-quality translation with a guarantee. Unfortunately, due to the lack of popular benchmarks, the research on CAT is not well developed compared with machine translation. In this year, we hold a new shared task called Word-level AutoCompletion (WLAC) for CAT in WMT. Specifically, we introduce some resources to train a WLAC model, and particularly we collect data from CAT systems as a part of test data for this shared task. In addition, we employ both automatic and human evaluations to measure the performance of the submitted systems, and our final evaluation results reveal some findings for the WLAC task.",
}


@inproceedings{li-etal-2021-gwlan,
    title = "{GWLAN}: General Word-Level {A}utocompletio{N} for Computer-Aided Translation",
    author = "Li, Huayang  and
      Liu, Lemao  and
      Huang, Guoping  and
      Shi, Shuming",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.370",
    doi = "10.18653/v1/2021.acl-long.370",
    pages = "4792--4802",
    abstract = "Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.",
}


@inproceedings{sun-etal-2021-iga,
    title = "{IGA}: An Intent-Guided Authoring Assistant",
    author = "Sun, Simeng  and
      Zhao, Wenlong  and
      Manjunatha, Varun  and
      Jain, Rajiv  and
      Morariu, Vlad  and
      Dernoncourt, Franck  and
      Srinivasan, Balaji Vasan  and
      Iyyer, Mohit",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.483",
    doi = "10.18653/v1/2021.emnlp-main.483",
    pages = "5972--5985",
    abstract = "While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We fine-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to fill in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations confirm the quality of IGA{'}s generated outputs, while a small-scale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into AI-assisted writing.",
}


@inproceedings{van-etal-2020-automets,
    title = "{A}uto{M}e{TS}: The Autocomplete for Medical Text Simplification",
    author = "Van, Hoang  and
      Kauchak, David  and
      Leroy, Gondy",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.122",
    doi = "10.18653/v1/2020.coling-main.122",
    pages = "1424--1434",
    abstract = "The goal of text simplification (TS) is to transform difficult text into a version that is easier to understand and more broadly accessible to a wide variety of readers. In some domains, such as healthcare, fully automated approaches cannot be used since information must be accurately preserved. Instead, semi-automated approaches can be used that assist a human writer in simplifying text faster and at a higher quality. In this paper, we examine the application of autocomplete to text simplification in the medical domain. We introduce a new parallel medical data set consisting of aligned English Wikipedia with Simple English Wikipedia sentences and examine the application of pretrained neural language models (PNLMs) on this dataset. We compare four PNLMs (BERT, RoBERTa, XLNet, and GPT-2), and show how the additional context of the sentence to be simplified can be incorporated to achieve better results (6.17{\%} absolute improvement over the best individual model). We also introduce an ensemble model that combines the four PNLMs and outperforms the best individual model by 2.1{\%}, resulting in an overall word prediction accuracy of 64.52{\%}.",
}


@inproceedings{du-etal-2022-understanding-iterative,
    title = "Understanding Iterative Revision from Human-Written Text",
    author = "Du, Wanyu  and
      Raheja, Vipul  and
      Kumar, Dhruv  and
      Kim, Zae Myung  and
      Lopez, Melissa  and
      Kang, Dongyeop",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.250",
    doi = "10.18653/v1/2022.acl-long.250",
    pages = "3573--3590",
    abstract = "Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human{'}s revision cycles. This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text. In particular, IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities. When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations. Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions.",
}


@inproceedings{du-etal-2022-read,
    title = "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
    author = "Du, Wanyu  and
      Kim, Zae Myung  and
      Raheja, Vipul  and
      Kumar, Dhruv  and
      Kang, Dongyeop",
    booktitle = "Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.in2writing-1.14",
    doi = "10.18653/v1/2022.in2writing-1.14",
    pages = "96--108",
    abstract = "Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, iterative in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In R3, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. The collected human-model interaction dataset and system code are available at \url{https://github.com/vipulraheja/IteraTeR}. Our system demonstration is available at \url{https://youtu.be/lK08tIpEoaE}.",
}



@article{shi2022effidit,
  title={Effidit: Your AI Writing Assistant},
  author={Shi, Shuming and Zhao, Enbo and Tang, Duyu and Wang, Yan and Li, Piji and Bi, Wei and Jiang, Haiyun and Huang, Guoping and Cui, Leyang and Huang, Xinting and others},
  journal={arXiv preprint arXiv:2208.01815},
  year={2022}
}

% !duplicate of inter-decision-making
@article{li2022pre,
  title={Pre-trained language models for interactive decision-making},
  author={Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Aky{\"u}rek, Ekin and Anandkumar, Anima and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31199--31212},
  year={2022}
}



@article{yao2023tree,
  title   = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author  = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.10601}
}

@inproceedings{pfeiffer2020AdapterHub,
    title={AdapterHub: A Framework for Adapting Transformers},
    author={Jonas Pfeiffer and
            Andreas R\"uckl\'{e} and
            Clifton Poth and
            Aishwarya Kamath and
            Ivan Vuli\'{c} and
            Sebastian Ruder and
            Kyunghyun Cho and
            Iryna Gurevych},
    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020): Systems Demonstrations},
    year={2020},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.7",
    pages = "46--54",
}

@article{nucleus-sampling,
  title     = {The Curious Case of Neural Text Degeneration},
  author    = {Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
  journal   = {International Conference On Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1}
}


@article{reid2022can,
  title={Can wikipedia help offline reinforcement learning?},
  author={Reid, Machel and Yamada, Yutaro and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2201.12122},
  year={2022}
}

@article{yuan2023plan4mc,
  title={Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks},
  author={Yuan, Haoqi and Zhang, Chi and Wang, Hongcheng and Xie, Feiyang and Cai, Penglin and Dong, Hao and Lu, Zongqing},
  journal={arXiv preprint arXiv:2303.16563},
  year={2023}
}

@article{wang2023describe,
  title={Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents},
  author={Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  journal={arXiv preprint arXiv:2302.01560},
  year={2023}
}

@article{karamcheti2023language,
  title={Language-Driven Representation Learning for Robotics},
  author={Karamcheti, Siddharth and Nair, Suraj and Chen, Annie S and Kollar, Thomas and Finn, Chelsea and Sadigh, Dorsa and Liang, Percy},
  journal={arXiv preprint arXiv:2302.12766},
  year={2023}
}

@article{tuli2022learning,
  title={Learning to follow instructions in text-based games},
  author={Tuli, Mathieu and Li, Andrew and Vaezipoor, Pashootan and Klassen, Toryn and Sanner, Scott and McIlraith, Sheila},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19441--19455},
  year={2022}
}

@article{goyal2019using,
  title={Using natural language for reward shaping in reinforcement learning},
  author={Goyal, Prasoon and Niekum, Scott and Mooney, Raymond J},
  journal={arXiv preprint arXiv:1903.02020},
  year={2019}
}

@inproceedings{xu2020deep,
  title={Deep reinforcement learning with transformers for text adventure games},
  author={Xu, Yunqiu and Chen, Ling and Fang, Meng and Wang, Yang and Zhang, Chengqi},
  booktitle={2020 IEEE Conference on Games (CoG)},
  pages={65--72},
  year={2020},
  organization={IEEE}
}

@article{yin2020zero,
  title={Zero-shot learning of text adventure games with sentence-level semantics},
  author={Yin, Xusen and May, Jonathan},
  journal={arXiv preprint arXiv:2004.02986},
  year={2020}
}

@article{generative-agents,
  title={Generative Agents: Interactive Simulacra of Human Behavior},
  author={Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  journal={arXiv preprint arXiv:2304.03442},
  year={2023}
}

@inproceedings{chen-etal-2022-summscreen,
    title = "{S}umm{S}creen: A Dataset for Abstractive Screenplay Summarization",
    author = "Chen, Mingda  and
      Chu, Zewei  and
      Wiseman, Sam  and
      Gimpel, Kevin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.589",
    pages = "8602--8615",
    abstract = "We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to TV series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions.",
}

@inproceedings{toddtowards,
  title={Towards Knowledge-Graph Constrained Generation for Text Adventure Games},
  author={Todd, Graham and Cheng, Zegang and Liu, Yifan and Togelius, Julian},
  booktitle={The Third Wordplay: When Language Meets Games Workshop},
  year={2022}
}

@article{ammanabrolu2020graph,
  title={Graph constrained reinforcement learning for natural language action spaces},
  author={Ammanabrolu, Prithviraj and Hausknecht, Matthew},
  journal={arXiv preprint arXiv:2001.08837},
  year={2020}
}

@article{yuan2018counting,
  title={Counting to explore and generalize in text-based games},
  author={Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Sordoni, Alessandro and Laroche, Romain and Combes, Remi Tachet des and Hausknecht, Matthew and Trischler, Adam},
  journal={arXiv preprint arXiv:1806.11525},
  year={2018}
}

@article{chen2022would,
  title={What would Harry say? Building Dialogue Agents for Characters in a Story},
  author={Chen, Nuo and Wang, Yan and Jiang, Haiyun and Cai, Deng and Chen, Ziyang and Li, Jia},
  journal={arXiv preprint arXiv:2211.06869},
  year={2022}
}

@article{peiris2022synthesis,
  title={Synthesis and Evaluation of a Domain-specific Large Data Set for Dungeons \& Dragons},
  author={Peiris, Akila and de Silva, Nisansa},
  journal={arXiv preprint arXiv:2212.09080},
  year={2022}
}

@inproceedings{rameshkumar2020storytelling,
  title={Storytelling with dialogue: A critical role dungeons and dragons dataset},
  author={Rameshkumar, Revanth and Bailey, Peter},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5121--5134},
  year={2020}
}

@article{callison2022dungeons,
  title={Dungeons and dragons as a dialog challenge for artificial intelligence},
  author={Callison-Burch, Chris and Tomar, Gaurav Singh and Martin, Lara J and Ippolito, Daphne and Bailis, Suma and Reitter, David},
  journal={arXiv preprint arXiv:2210.07109},
  year={2022}
}

@inproceedings{tuin2021automatically,
  title={Automatically detecting player roles in Among Us},
  author={Tuin, Harro and Rooijackers, Martin},
  booktitle={2021 IEEE Conference on Games (CoG)},
  pages={1--5},
  year={2021},
  organization={IEEE}
}

@inproceedings{lin2020automatic,
  title={Automatic annotation of werewolf game corpus with players revealing oneselves as seer/medium and divination/medium results},
  author={Lin, Youchao and Kasamatsu, Miho and Chen, Tengyang and Fujita, Takuya and Deng, Huanjin and Utsuro, Takehito},
  booktitle={Workshop on Games and Natural Language Processing},
  pages={85--93},
  year={2020}
}

@article{lai2022werewolf,
  title={Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games},
  author={Lai, Bolin and Zhang, Hongxin and Liu, Miao and Pariani, Aryan and Ryan, Fiona and Jia, Wenqi and Hayati, Shirley Anugrah and Rehg, James M and Yang, Diyi},
  journal={arXiv preprint arXiv:2212.08279},
  year={2022}
}

@inproceedings{hausknecht2020interactive,
  title={Interactive fiction games: A colossal adventure},
  author={Hausknecht, Matthew and Ammanabrolu, Prithviraj and C{\^o}t{\'e}, Marc-Alexandre and Yuan, Xingdi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7903--7910},
  year={2020}
}

@inproceedings{cote2019textworld,
  title={Textworld: A learning environment for text-based games},
  author={C{\^o}t{\'e}, Marc-Alexandre and K{\'a}d{\'a}r, Akos and Yuan, Xingdi and Kybartas, Ben and Barnes, Tavian and Fine, Emery and Moore, James and Hausknecht, Matthew and El Asri, Layla and Adada, Mahmoud and others},
  booktitle={Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7},
  pages={41--75},
  year={2019},
  organization={Springer}
}


@article{salemi2023lamp,
  title={LaMP: When Large Language Models Meet Personalization},
  author={Salemi, Alireza and Mysore, Sheshera and Bendersky, Michael and Zamani, Hamed},
  journal={arXiv preprint arXiv:2304.11406},
  year={2023}
}

@article{osborne-etal-2022-survey,
    title = "A Survey of Text Games for Reinforcement Learning Informed by Natural Language",
    author = "Osborne, Philip  and
      N{\~o}mm, Heido  and
      Freitas, Andr{\'e}",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.51",
    doi = "10.1162/tacl_a_00495",
    pages = "873--887",
    abstract = "Reinforcement Learning has shown success in a number of complex virtual environments. However, many challenges still exist towards solving problems with natural language as a core component. Interactive Fiction Games (or Text Games) are one such problem type that offer a set of safe, partially observable environments where natural language is required as part of the Reinforcement Learning solution. Therefore, this survey{'}s aim is to assist in the development of new Text Game problem settings and solutions for Reinforcement Learning informed by natural language. Specifically, this survey: 1) introduces the challenges in Text Game Reinforcement Learning problems, 2) outlines the generation tools for rendering Text Games and the subsequent environments generated, and 3) compares the agent architectures currently applied to provide a systematic review of benchmark methodologies and opportunities for future researchers.",
}


@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{ren2021survey,
  title={A survey of deep active learning},
  author={Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B and Chen, Xiaojiang and Wang, Xin},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={9},
  pages={1--40},
  year={2021},
  publisher={ACM New York, NY}
}

@article{kasai2019low,
  title={Low-resource deep entity resolution with transfer and active learning},
  author={Kasai, Jungo and Qian, Kun and Gurajada, Sairam and Li, Yunyao and Popa, Lucian},
  journal={arXiv preprint arXiv:1906.08042},
  year={2019}
}

@misc{deshpande2023toxicity,
      title={Toxicity in ChatGPT: Analyzing Persona-assigned Language Models}, 
      author={Ameet Deshpande and Vishvak Murahari and Tanmay Rajpurohit and Ashwin Kalyan and Karthik Narasimhan},
      year={2023},
      eprint={2304.05335},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wang2022language,
  title={What Language Model Architecture and Pretraining Objective Works Best for Zero-Shot Generalization?},
  author={Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={22964--22984},
  year={2022},
  organization={PMLR}
}



@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}


@book{schelling2011philosophische,
  title={Philosophische Untersuchungen {\"u}ber das Wesen der menschlichen Freiheit und die damit zusammenh{\"a}ngenden Gegenst{\"a}nde},
  author={Schelling, Friedrich Wilhelm Joseph},
  volume={503},
  year={2011},
  publisher={Felix Meiner Verlag}
}

@inproceedings{
diao2023write,
title={Write and Paint: Generative Vision-Language Models are Unified Modal Learners},
author={Shizhe Diao and Wangchunshu Zhou and Xinsong Zhang and Jiawei Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=HgQR0mXQ1_a}
}

@inproceedings{
zhou2021pretraining,
title={Pre-training Text-to-Text Transformers for Concept-centric Common Sense},
author={Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Xiang Ren},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=3k20LAiHYL2}
}

@article{wu2023bloomberggpt,
  title={BloombergGPT: A Large Language Model for Finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

@inproceedings{zhou-etal-2020-improving-grammatical,
    title = "Improving Grammatical Error Correction with Machine Translation Pairs",
    author = "Zhou, Wangchunshu  and
      Ge, Tao  and
      Mu, Chang  and
      Xu, Ke  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.30",
    doi = "10.18653/v1/2020.findings-emnlp.30",
    pages = "318--328",
}

@inproceedings{zhou-etal-2021-improving-sequence,
    title = "Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting",
    author = "Zhou, Wangchunshu  and
      Ge, Tao  and
      Xu, Canwen  and
      Xu, Ke  and
      Wei, Furu",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.45",
    doi = "10.18653/v1/2021.emnlp-main.45",
    pages = "571--582",
    }

@misc{madaan2023selfrefine,
    title={Self-Refine: Iterative Refinement with Self-Feedback}, 
    author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Sean Welleck and Bodhisattwa Prasad Majumder and Shashank Gupta and Amir Yazdanbakhsh and Peter Clark},
    year={2023},
    eprint={2303.17651},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{sinha2019variational,
  title={Variational adversarial active learning},
  author={Sinha, Samarth and Ebrahimi, Sayna and Darrell, Trevor},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5972--5981},
  year={2019},
}


@misc{xie2023large,
      title={Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT}, 
      author={Tong Xie and Yuwei Wan and Wei Huang and Yufei Zhou and Yixuan Liu and Qingyuan Linghu and Shaozhou Wang and Chunyu Kit and Clara Grazian and Bram Hoex},
      year={2023},
      eprint={2304.02213},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhan2022comparative,
  title={A comparative survey of deep active learning},
  author={Zhan, Xueying and Wang, Qingzhong and Huang, Kuan-hao and Xiong, Haoyi and Dou, Dejing and Chan, Antoni B},
  journal={arXiv preprint arXiv:2203.13450},
  year={2022}
}

@article{kaushik2019learning,
  title={Learning the difference that makes a difference with counterfactually-augmented data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
  journal={arXiv preprint arXiv:1909.12434},
  year={2019}
}

@article{aljanabi2023chatgpt,
  title={ChatGPT: Future directions and open possibilities},
  author={Aljanabi, Mohammad and others},
  journal={Mesopotamian Journal of Cybersecurity},
  volume={2023},
  pages={16--17},
  year={2023}
}

@article{mijwil2023towards,
  title={Towards Artificial Intelligence-Based Cybersecurity: The Practices and ChatGPT Generated Ways to Combat Cybercrime},
  author={Mijwil, Maad and Aljanabi, Mohammad and others},
  journal={Iraqi Journal For Computer Science and Mathematics},
  volume={4},
  number={1},
  pages={65--70},
  year={2023}
}

@article{rao2023can,
  title={Can ChatGPT Assess Human Personalities? A General Evaluation Framework},
  author={Rao, Haocong and Leung, Cyril and Miao, Chunyan},
  journal={arXiv preprint arXiv:2303.01248},
  year={2023}
}

@article{kreutzer2018can,
  title={Can neural machine translation be improved with user feedback?},
  author={Kreutzer, Julia and Khadivi, Shahram and Matusov, Evgeny and Riezler, Stefan},
  journal={arXiv preprint arXiv:1804.05958},
  year={2018}
}

@article{liu2018dialogue,
  title={Dialogue learning with human teaching and feedback in end-to-end trainable task-oriented dialogue systems},
  author={Liu, Bing and Tur, Gokhan and Hakkani-Tur, Dilek and Shah, Pararth and Heck, Larry},
  journal={arXiv preprint arXiv:1804.06512},
  year={2018}
}



@article{wang2023chatcad,
  title={Chatcad: Interactive computer-aided diagnosis on medical image using large language models},
  author={Wang, Sheng and Zhao, Zihao and Ouyang, Xi and Wang, Qian and Shen, Dinggang},
  journal={arXiv preprint arXiv:2302.07257},
  year={2023}
}

@article{hancock2019learning,
  title={Learning from dialogue after deployment: Feed yourself, chatbot!},
  author={Hancock, Braden and Bordes, Antoine and Mazare, Pierre-Emmanuel and Weston, Jason},
  journal={arXiv preprint arXiv:1901.05415},
  year={2019}
}

@article{wang2021putting,
  title={Putting humans in the natural language processing loop: A survey},
  author={Wang, Zijie J and Choi, Dongjin and Xu, Shenyu and Yang, Diyi},
  journal={arXiv preprint arXiv:2103.04044},
  year={2021}
}

@article{wang2023open,
  title={Open world long-tailed data classification through active distribution optimization},
  author={Wang, Min and Zhou, Lei and Li, Qian and Zhang, An-an},
  journal={Expert Systems with Applications},
  volume={213},
  pages={119054},
  year={2023},
  publisher={Elsevier}
}

@article{li2023internet,
  title={Internet Explorer: Targeted Representation Learning on the Open Web},
  author={Li, Alexander C and Brown, Ellis and Efros, Alexei A and Pathak, Deepak},
  journal={arXiv preprint arXiv:2302.14051},
  year={2023}
}

@inproceedings{ostapuk2019activelink,
  title={Activelink: deep active learning for link prediction in knowledge graphs},
  author={Ostapuk, Natalia and Yang, Jie and Cudr{\'e}-Mauroux, Philippe},
  booktitle={The World Wide Web Conference},
  pages={1398--1408},
  year={2019}
}

@article{sun2021ernie,
  title={Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation},
  author={Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and others},
  journal={arXiv preprint arXiv:2107.02137},
  year={2021}
}

@inproceedings{bikaun2022quickgraph,
  title={QuickGraph: A Rapid Annotation Tool for Knowledge Graph Extraction from Technical Text},
  author={Bikaun, Tyler and Stewart, Michael and Liu, Wei},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={270--278},
  year={2022}
}

@inproceedings{agarwal2022active,
  title={Active Learning for Node Classification using a Convex Optimization approach},
  author={Agarwal, Deepesh and Natarajan, Balasubramaniam},
  booktitle={2022 IEEE Eighth International Conference on Big Data Computing Service and Applications (BigDataService)},
  pages={96--102},
  year={2022},
  organization={IEEE}
}

@article{seo2021active,
  title={Active Learning for Knowledge Graph Schema Expansion},
  author={Seo, Seungmin and Oh, Byungkook and Jo, Eunju and Lee, Sanghak and Lee, Dongho and Lee, Kyong-Ho and Shin, Donghoon and Lee, Yeonsoo},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={34},
  number={12},
  pages={5610--5620},
  year={2021},
  publisher={IEEE}
}

@article{liu2021activeea,
  title={ActiveEA: Active learning for neural entity alignment},
  author={Liu, Bing and Scells, Harrisen and Zuccon, Guido and Hua, Wen and Zhao, Genghong},
  journal={arXiv preprint arXiv:2110.06474},
  year={2021}
}


@article{diao2023active,
  title={Active Prompting with Chain-of-Thought for Large Language Models},
  author={Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang, Tong},
  journal={arXiv preprint arXiv:2302.12246},
  year={2023}
}

@inproceedings{nangia-etal-2020-crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}

@inproceedings{dossou-etal-2022-afrolm,
    title = "{A}fro{LM}: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 {A}frican Languages",
    author = "Dossou, Bonaventure F. P.  and
      Tonja, Atnafu Lambebo  and
      Yousuf, Oreen  and
      Osei, Salomey  and
      Oppong, Abigail  and
      Shode, Iyanuoluwa  and
      Awoyomi, Oluwabusayo Olufunke  and
      Emezue, Chris",
    booktitle = "Proceedings of The Third Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sustainlp-1.11",
    pages = "52--64",
    abstract = "In recent years, multilingual pre-trained language models have gained prominence due to their remarkable performance on numerous downstream Natural Language Processing tasks (NLP). However, pre-training these large multilingual language models requires a lot of training data, which is not available for African Languages. Active learning is a semi-supervised learning algorithm, in which a model consistently and dynamically learns to identify the most beneficial samples to train itself on, in order to achieve better optimization and performance on downstream tasks. Furthermore, active learning effectively and practically addresses real-world data scarcity. Despite all its benefits, active learning, in the context of NLP and especially multilingual language models pretraining, has received little consideration. In this paper, we present \textbf{AfroLM}, a multilingual language model pretrained from scratch on 23 African languages (the largest effort to date) using our novel self-active learning framework. Pretrained on a dataset significantly (14x) smaller than existing baselines, \textbf{AfroLM} outperforms many multilingual pretrained language models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text classification, and sentiment analysis). Additional out-of-domain sentiment analysis experiments show that \textbf{AfroLM} is able to generalize well across various domains. We release the code source, and our datasets used in our framework at https://github.com/bonaventuredossou/MLM{\_}AL.",
}

@misc{li2023apibank,
      title={API-Bank: A Benchmark for Tool-Augmented LLMs}, 
      author={Minghao Li and Feifan Song and Bowen Yu and Haiyang Yu and Zhoujun Li and Fei Huang and Yongbin Li},
      year={2023},
      eprint={2304.08244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{xu2023small,
  title   = {Small Models are Valuable Plug-ins for Large Language Models},
  author  = {Canwen Xu and Yichong Xu and Shuohang Wang and Yang Liu and Chenguang Zhu and Julian McAuley},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2305.08848}
}

@misc{deepseekv2,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{qwen1.5,
    title = {Introducing Qwen1.5},
    url = {https://qwenlm.github.io/blog/qwen1.5/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}

@inproceedings{yu2022actune,
  title={AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models},
  author={Yu, Yue and Kong, Lingkai and Zhang, Jieyu and Zhang, Rongzhi and Zhang, Chao},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1422--1436},
  year={2022}
}

@article{bengio2017consciousness,
  title     = {The Consciousness Prior},
  author    = {Yoshua Bengio},
  journal   = {ARXIV.ORG},
  year      = {2017},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/0fff5c49c05c27c22ac7685130197146491f0b36}
}

@article{liang2023unleashing,
  title   = {Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System},
  author  = {Xinnian Liang and Bing Wang and Hui Huang and Shuangzhi Wu and Peihao Wu and Lu Lu and Zejun Ma and Zhoujun Li},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2304.13343}
}

@article{hoeve2021towards,
  title   = {Towards Interactive Language Modeling},
  author  = {Maartje ter Hoeve and Evgeny Kharitonov and Dieuwke Hupkes and Emmanuel Dupoux},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2112.11911}
}

@misc{bills2023language,
     title={Language models can explain neurons in language models},
     author={
        Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William
     },
     year={2023},
     howpublished = {\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
  }

@article{sun2023assbert,
  title={ASSBert: Active and semi-supervised bert for smart contract vulnerability detection},
  author={Sun, Xiaobing and Tu, Liangqiong and Zhang, Jiale and Cai, Jie and Li, Bin and Wang, Yu},
  journal={Journal of Information Security and Applications},
  volume={73},
  pages={103423},
  year={2023},
  publisher={Elsevier}
}


@article{niu2020realtime,
  title   = {Real-Time Execution of Large-scale Language Models on Mobile},
  author  = {Wei Niu and Zhenglun Kong and Geng Yuan and Weiwen Jiang and Jiexiong Guan and Caiwen Ding and Pu Zhao and Sijia Liu and Bin Ren and Yanzhi Wang},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2009.06823}
}

@article{yang2018hotpotqa,
  title   = {HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author  = {Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
  year    = {2018},
  journal = {arXiv preprint arXiv: Arxiv-1809.09600}
}

@article{wang2016cost,
  title={Cost-effective active learning for deep image classification},
  author={Wang, Keze and Zhang, Dongyu and Li, Ya and Zhang, Ruimao and Lin, Liang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={27},
  number={12},
  pages={2591--2600},
  year={2016},
  publisher={IEEE}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{wang2022self,
  title   = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author  = {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2203.11171}
}

@inproceedings{maekawa2022low,
  title={Low-resource Interactive Active Labeling for Fine-tuning Language Models},
  author={Maekawa, Seiji and Zhang, Dan and Kim, Hannah and Rahman, Sajjadur and Hruschka, Estevam},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={3230--3242},
  year={2022}
}

@inproceedings{griesshaber-etal-2020-fine,
    title = "Fine-tuning {BERT} for Low-Resource Natural Language Understanding via Active Learning",
    author = "Grie{\ss}haber, Daniel  and
      Maucher, Johannes  and
      Vu, Ngoc Thang",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.100",
    doi = "10.18653/v1/2020.coling-main.100",
    pages = "1158--1171",
    abstract = "Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT - a pre-trained Transformer based language model - by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings.",
}


@article{peshterliev2018active,
  title={Active learning for new domains in natural language understanding},
  author={Peshterliev, Stanislav and Kearney, John and Jagannatha, Abhyuday and Kiss, Imre and Matsoukas, Spyros},
  journal={arXiv preprint arXiv:1810.03450},
  year={2018}
}

@inproceedings{quteineh-etal-2020-textual,
    title = "Textual Data Augmentation for Efficient Active Learning on Tiny Datasets",
    author = "Quteineh, Husam  and
      Samothrakis, Spyridon  and
      Sutcliffe, Richard",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.600",
    doi = "10.18653/v1/2020.emnlp-main.600",
    pages = "7400--7410",
    abstract = "In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria. We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function. Starting with a small set of data, our results show an increased performance with MCTS of 26{\%} on the TREC-6 Questions dataset, and 10{\%} on the Stanford Sentiment Treebank SST-2 dataset. Compared with NGDG, we are able to achieve increases of 3{\%} and 5{\%} on TREC-6 and SST-2.",
}


@inproceedings{birke-sarkar-2007-active,
    title = "Active Learning for the Identification of Nonliteral Language",
    author = "Birke, Julia  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the Workshop on Computational Approaches to Figurative Language",
    month = apr,
    year = "2007",
    address = "Rochester, New York",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W07-0104",
    pages = "21--28",
}


@inproceedings{zhu-hovy-2007-active,
    title = "Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem",
    author = "Zhu, Jingbo  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1082",
    pages = "783--790",
}


@article{li2020active,
  title={Active learning for coreference resolution using discrete annotation},
  author={Li, Belinda Z and Stanovsky, Gabriel and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2004.13671},
  year={2020}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@inproceedings{kim-etal-2022-improving,
    title = "Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks",
    author = "Kim, Zae Myung  and
      Du, Wanyu  and
      Raheja, Vipul  and
      Kumar, Dhruv  and
      Kang, Dongyeop",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.678",
    pages = "9986--9999",
    abstract = "Iterative text revision improves text quality by fixing grammatical errors, rephrasing for better readability or contextual appropriateness, or reorganizing sentence structures throughout a document.Most recent research has focused on understanding and classifying different types of edits in the iterative revision process from human-written text instead of building accurate and robust systems for iterative text revision.In this work, we aim to build an end-to-end text revision system that can iteratively generate helpful edits by explicitly detecting editable spans (where-to-edit) with their corresponding edit intents and then instructing a revision model to revise the detected edit spans.Leveraging datasets from other related text editing NLP tasks, combined with the specification of editable spans, leads our system to more accurately model the process of iterative text refinement, as evidenced by empirical results and human evaluations.Our system significantly outperforms previous baselines on our text revision tasks and other standard text revision tasks, including grammatical error correction, text simplification, sentence fusion, and style transfer.Through extensive qualitative and quantitative analysis, we make vital connections between edit intentions and writing quality, and better computational modeling of iterative text revisions.",
}


@article{zhang2023corgi,
  title={CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation},
  author={Zhang, Ge and Li, Yizhi and Wu, Yaoyao and Zhang, Linyuan and Lin, Chenghua and Geng, Jiayi and Wang, Shi and Fu, Jie},
  journal={arXiv preprint arXiv:2301.00395},
  year={2023}
}

@article{liu2022second,
  title={Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits},
  author={Liu, Ruibo and Jia, Chenyan and Zhang, Ge and Zhuang, Ziyu and Liu, Tony and Vosoughi, Soroush},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={181--196},
  year={2022}
}

@article{guu-etal-2018-generating,
    title = "Generating Sentences by Editing Prototypes",
    author = "Guu, Kelvin  and
      Hashimoto, Tatsunori B.  and
      Oren, Yonatan  and
      Liang, Percy",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1031",
    doi = "10.1162/tacl_a_00030",
    pages = "437--450",
    abstract = "We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.",
}

@inproceedings{awasthi-etal-2019-parallel,
    title = "Parallel Iterative Edit Models for Local Sequence Transduction",
    author = "Awasthi, Abhijeet  and
      Sarawagi, Sunita  and
      Goyal, Rasna  and
      Ghosh, Sabyasachi  and
      Piratla, Vihari",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1435",
    doi = "10.18653/v1/D19-1435",
    pages = "4260--4270",
    abstract = "We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modeling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1. predicting edits instead of tokens, 2. labeling sequences instead of generating sequences, 3. iteratively refining predictions to capture dependencies, and 4. factorizing logits over edits and their token argument to harness pre-trained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction.",
}


@inproceedings{mallinson-etal-2020-felix,
    title = "{FELIX}: Flexible Text Editing Through Tagging and Insertion",
    author = "Mallinson, Jonathan  and
      Severyn, Aliaksei  and
      Malmi, Eric  and
      Garrido, Guillermo",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.111",
    doi = "10.18653/v1/2020.findings-emnlp.111",
    pages = "1244--1255",
    abstract = "We present FELIX {--} a flexible text-editing approach for generation, designed to derive maximum benefit from the ideas of decoding with bi-directional contexts and self-supervised pretraining. In contrast to conventional sequenceto-sequence (seq2seq) models, FELIX is efficient in low-resource settings and fast at inference time, while being capable of modeling flexible input-output transformations. We achieve this by decomposing the text-editing task into two sub-tasks: tagging to decide on the subset of input tokens and their order in the output text and insertion to in-fill the missing tokens in the output not present in the input. The tagging model employs a novel Pointer mechanism, while the insertion model is based on a Masked Language Model (MLM). Both of these models are chosen to be non-autoregressive to guarantee faster inference. FELIX performs favourably when compared to recent text-editing methods and strong seq2seq baselines when evaluated on four NLG tasks: Sentence Fusion, Machine Translation Automatic Post-Editing, Summarization, and Text Simplification",
}

@misc{li2023chatharuhi,
      title={ChatHaruhi: Reviving Anime Character in Reality via Large Language Model}, 
      author={Cheng Li and Ziang Leng and Chenxi Yan and Junyi Shen and Hao Wang and Weishi MI and Yaying Fei and Xiaoyang Feng and Song Yan and HaoSheng Wang and Linkang Zhan and Yaokai Jia and Pingyu Wu and Haozhen Sun},
      year={2023},
      eprint={2308.09597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{shanahan2023roleplay,
  title   = {Role-Play with Large Language Models},
  author  = {Murray Shanahan and Kyle McDonell and Laria Reynolds},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.16367}
}

@inproceedings{metatool,
  author    = {Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong and Lichao Sun},
  title     = {MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher = {OpenReview.net},
  year      = {2024},
  url       = {https://openreview.net/forum?id=R0c2qtalgG},
  timestamp = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HuangSLFWZ000G024.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/2310.03128.pdf}
}



@article{palm-clinical,
  author  = {Karan Singhal na and Shekoofeh Azizi and Tao Tu na and S. Sara Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and Ajay Tanwani and Heather Cole-Lewis and Stephen Pfohl and Perry Payne and Martin Seneviratne and Paul Gamble and Chris Kelly and Abubakr Babiker and Nathanael Schärli and Aakanksha Chowdhery and Philip Mansfield and Dina Demner-Fushman and Blaise Agüera y Arcas and Dale Webster and Greg S. Corrado and Yossi Matias and Katherine Chou and Juraj Gottweis and Nenad Tomasev and Yun Liu and Alvin Rajkomar and Joelle Barral and Christopher Semturs and Alan Karthikesalingam na and Vivek Natarajan na},
  title   = {Large language models encode clinical knowledge},
  journal = {Nature},
  year    = {2023},
  doi     = {10.1038/s41586-023-06291-2},
  url     = {https://doi.org/10.1038/s41586-023-06291-2}
}

@article{huang2023ceval,
title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
journal={arXiv preprint arXiv:2305.08322},
year={2023}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}


@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}


@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}


@inproceedings{malmi-etal-2022-text,
    title = "Text Generation with Text-Editing Models",
    author = "Malmi, Eric  and
      Dong, Yue  and
      Mallinson, Jonathan  and
      Chuklin, Aleksandr  and
      Adamek, Jakub  and
      Mirylenka, Daniil  and
      Stahlberg, Felix  and
      Krause, Sebastian  and
      Kumar, Shankar  and
      Severyn, Aliaksei",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-tutorials.1",
    doi = "10.18653/v1/2022.naacl-tutorials.1",
    pages = "1--7",
    abstract = "Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, text simplification, and style transfer. These tasks share a common trait {--} they exhibit a large amount of textual overlap between the source and target texts. Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence. In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time. Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and interpretability of the outputs. This tutorial provides a comprehensive overview of the text-edit based models and current state-of-the-art approaches analyzing their pros and cons. We discuss challenges related to deployment and how these models help to mitigate hallucination and bias, both pressing challenges in the field of text generation.",
}


@inproceedings{kazemnejad-etal-2020-paraphrase,
    title = "Paraphrase Generation by Learning How to Edit from Samples",
    author = "Kazemnejad, Amirhossein  and
      Salehi, Mohammadreza  and
      Soleymani Baghshah, Mahdieh",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.535",
    doi = "10.18653/v1/2020.acl-main.535",
    pages = "6010--6021",
    abstract = "Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems, we propose a novel retrieval-based method for paraphrase generation. Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences. In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors. It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture. Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.",
}


@article{corso2022diffdock,
  title={Diffdock: Diffusion steps, twists, and turns for molecular docking},
  author={Corso, Gabriele and St{\"a}rk, Hannes and Jing, Bowen and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2210.01776},
  year={2022}
}

@article{ravi2023preditor,
  title={PRedItOR: Text Guided Image Editing with Diffusion Prior},
  author={Ravi, Hareesh and Kelkar, Sachin and Harikumar, Midhun and Kale, Ajinkya},
  journal={arXiv preprint arXiv:2302.07979},
  year={2023}
}

@inproceedings{zhang2022coditt5,
  title={CoditT5: Pretraining for Source Code and Natural Language Editing},
  author={Zhang, Jiyang and Panthaplackel, Sheena and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos},
  booktitle={37th IEEE/ACM International Conference on Automated Software Engineering},
  pages={1--12},
  year={2022}
}

@incollection{gollins2016framework,
  title={A framework for a cognitive theory of writing},
  author={Gollins, Allan and Gentner, Dedre},
  booktitle={Cognitive processes in writing},
  pages={51--72},
  year={2016},
  publisher={Routledge}
}

@article{vardi2012impact,
  title={The impact of iterative writing and feedback on the characteristics of tertiary students' written texts},
  author={Vardi, Iris},
  journal={Teaching in higher education},
  volume={17},
  number={2},
  pages={167--179},
  year={2012},
  publisher={Taylor \& Francis}
}

@inproceedings{lacruz2014cognitive,
  title={Cognitive demand and cognitive effort in post-editing},
  author={Lacruz, Isabel and Denkowski, Michael and Lavie, Alon},
  booktitle={Proceedings of the 11th Conference of the Association for Machine Translation in the Americas},
  pages={73--84},
  year={2014}
}

@article{wake2023chatgpt,
  title   = {ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application},
  author  = {Naoki Wake and Atsushi Kanehira and Kazuhiro Sasabuchi and Jun Takamatsu and Katsushi Ikeuchi},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2304.03893}
}

@article{delta-tuning, 
    author={Ning Ding and Yujia Qin and Guang Yang and Fuchao Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi{-}Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Hai{-}Tao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juanzi Li and Maosong Sun},
    title={Delta Tuning: {A} Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
    journal={CoRR},
    volume={abs/2203.06904},
    year={2022},
    url={https://doi.org/10.48550/arXiv.2203.06904},
    doi={10.48550/arXiv.2203.06904},
    eprinttype={arXiv},
    eprint={2203.06904},
    timestamp={Tue, 24 Jan 2023 15:06:31 +0100},
    biburl={https://dblp.org/rec/journals/corr/abs-2203-06904.bib},
    bibsource={dblp computer science bibliography, https://dblp.org},
}

@inproceedings{head-pruning,
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author    = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = {jul},
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P19-1580},
  doi       = {10.18653/v1/P19-1580},
  pages     = {5797-5808},
  abstract  = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.}
}

@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}



@InProceedings{pmlr-v97-houlsby19a,
  title = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
}

@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}


@article{bai2021syntaxbert,
  title   = {Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees},
  author  = {Jiangang Bai and Yujing Wang and Yiren Chen and Yaming Yang and Jing Bai and Jing Yu and Yunhai Tong},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2103.04350}
}

@article{sachan2020syntax,
  title     = {Do Syntax Trees Help Pre-trained Transformers Extract Information?},
  author    = {Devendra Singh Sachan and Yuhao Zhang and Peng Qi and William Hamilton},
  journal   = {Conference Of The European Chapter Of The Association For Computational Linguistics},
  year      = {2020},
  doi       = {10.18653/v1/2021.eacl-main.228},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/5bbf9555e9ec9e29ae4cd3c1253b1b74e8ddec20}
}

@misc{wang2023huatuo,
      title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge}, 
      author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},
      year={2023},
      eprint={2304.06975},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lawgpt,
  title     = {A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3},
  author    = {Nguyen Ha Thanh},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2302.05729},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/41b360c5b4ae9c5bfcf3891c45319a9e0b3e6d81}
}

@article{locascio2016neural,
  title     = {Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge},
  author    = {N. Locascio and Karthik Narasimhan and E. DeLeon and Nate Kushman and R. Barzilay},
  journal   = {Conference On Empirical Methods In Natural Language Processing},
  year      = {2016},
  doi       = {10.18653/v1/D16-1197},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/74157ae408173bf713f1e94f15aca1475c43bd74}
}

@article{han2022folio,
  title   = {FOLIO: Natural Language Reasoning with First-Order Logic},
  author  = {Simeng Han and Hailey Schoelkopf and Yilun Zhao and Zhenting Qi and Martin Riddell and Luke Benson and Lucy Sun and Ekaterina Zubova and Yujie Qiao and Matthew Burtell and David Peng and Jonathan Fan and Yixin Liu and Brian Wong and Malcolm Sailor and Ansong Ni and Linyong Nan and Jungo Kasai and Tao Yu and Rui Zhang and Shafiq Joty and Alexander R. Fabbri and Wojciech Kryscinski and Xi Victoria Lin and Caiming Xiong and Dragomir Radev},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2209.00840}
}

@article{cheng2022binding,
  title   = {Binding Language Models in Symbolic Languages},
  author  = {Zhoujun Cheng and Tianbao Xie and Peng Shi and Chengzu Li and Rahul Nadkarni and Yushi Hu and Caiming Xiong and Dragomir Radev and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.02875}
}

@inproceedings{NIPS2017_e7b24b11,
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning multiple visual domains with residual adapters},
 url = {https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}

@article{rafailov2023dpo,
  title   = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author  = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.18290},
  url     = {https://arxiv.org/abs/2305.18290v1},
  pdf     = {https://arxiv.org/pdf/2305.18290.pdf}
}

@article{lewis2020retrievalaugmented,
  title   = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author  = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2005.11401}
}

@article{instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{parmar2022boxbart,
  title={In-boxbart: Get instructions into biomedical multi-task learning},
  author={Parmar, Mihir and Mishra, Swaroop and Purohit, Mirali and Luo, Man and Murad, M Hassan and Baral, Chitta},
  journal={arXiv preprint arXiv:2204.07600},
  year={2022}
}



@inproceedings{lee2022plug,
  title={Plug-and-Play Adaptation for Continuously-updated QA},
  author={Lee, Kyungjae and Han, Wookje and Hwang, Seung-won and Lee, Hwaran and Park, Joonsuk and Lee, Sang-Woo},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={438--447},
  year={2022}
}

@inproceedings{li2022lpc,
  title={LPC: A Logits and Parameter Calibration Framework for Continual Learning},
  author={Li, Xiaodi and Wang, Zhuoyi and Li, Dingcheng and Khan, Latifur and Thuraisingham, Bhavani},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={7142--7155},
  year={2022}
}


@inproceedings{biesialska-etal-2020-continual,
  title     = { Continual Lifelong Learning in Natural Language Processing: A Survey},
  author    = {Biesialska, Magdalena and Biesialska, Katarzyna and Costa-juss{\`a}, Marta R.},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  month     = {dec},
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2020.coling-main.574},
  doi       = {10.18653/v1/2020.coling-main.574},
  pages     = {6523-6541},
  abstract  = {Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{weller2022use,
  title={When to use multi-task learning vs intermediate fine-tuning for pre-trained encoder transfer learning},
  author={Weller, Orion and Seppi, Kevin and Gardner, Matt},
  journal={arXiv preprint arXiv:2205.08124},
  year={2022}
}
@article{li2022consisttl,
  title={ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation},
  author={Li, Zhaocong and Liu, Xuebo and Wong, Derek F and Chao, Lidia S and Zhang, Min},
  journal={arXiv preprint arXiv:2212.04262},
  year={2022}
}

@inproceedings{
he2022towards,
title={Towards a Unified View of Parameter-Efficient Transfer Learning},
author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0RDcd5Axok}
}

@article{yaofu-notion-blog,
  title   = "How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources",
  author  = "Fu, Yao and Peng, Hao and Khot, Tushar",
  journal = "Yao Fu’s Notion",
  year    = "2022",
  month   = "Dec",
  url     = "https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-\\Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1"
}

@inproceedings{one-for-all,
  author    = {Peng Wang and An Yang and Rui Men and Junyang Lin and Shuai Bai and Zhikang Li and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},
  editor    = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesv{\'{a}}ri and Gang Niu and Sivan Sabato},
  title     = {{OFA:} Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {23318-23340},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/wang22al.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/WangYMLBLMZZY22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xie2022unifiedskg,
  title={Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models},
  author={Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I and others},
  journal={arXiv preprint arXiv:2201.05966},
  year={2022}
}

@article{ye2023context,
  title={In-Context Instruction Learning},
  author={Ye, Seonghyeon and Hwang, Hyeonbin and Yang, Sohee and Yun, Hyeongu and Kim, Yireun and Seo, Minjoon},
  journal={arXiv preprint arXiv:2302.14691},
  year={2023}
}

@inproceedings{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}



@article{ye2022guess,
  title={Guess the Instruction! Making Language Models Stronger Zero-Shot Learners},
  author={Ye, Seonghyeon and Kim, Doyoung and Jang, Joel and Shin, Joongbo and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.02969},
  year={2022}
}

@article{wu2022promptchainer,
  title   = {PromptChainer: Chaining Large Language Model Prompts through Visual Programming},
  author  = {Tongshuang Wu and Ellen Jiang and Aaron Donsbach and Jeff Gray and Alejandra Molina and Michael Terry and Carrie J Cai},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2203.06566}
}


@article{dblp:journals/corr/abs-1911-03090, 
    author={Jaejun Lee and Raphael Tang and Jimmy Lin},
    title={What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning},
    journal={CoRR},
    volume={abs/1911.03090},
    year={2019},
    url={http://arxiv.org/abs/1911.03090},
    eprinttype={arXiv},
    eprint={1911.03090},
    timestamp={Mon, 11 Nov 2019 18:38:09 +0100},
    biburl={https://dblp.org/rec/journals/corr/abs-1911-03090.bib},
    bibsource={dblp computer science bibliography, https://dblp.org},
}

@inproceedings{dblp:conf/acl/zakengr22, 
    author={Elad Ben Zaken and Yoav Goldberg and Shauli Ravfogel},
    editor={Smaranda Muresan and Preslav Nakov and Aline Villavicencio},
    title={BitFit: {Simple} Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
    booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022},
    pages={1--9},
    publisher={Association for Computational Linguistics},
    year={2022},
    url={https://doi.org/10.18653/v1/2022.acl-short.1},
    doi={10.18653/v1/2022.acl-short.1},
    timestamp={Mon, 01 Aug 2022 16:27:50 +0200},
    biburl={https://dblp.org/rec/conf/acl/ZakenGR22.bib},
    bibsource={dblp computer science bibliography, https://dblp.org},
}

@inproceedings{dblp:conf/acl/guork20, 
    author={Demi Guo and Alexander M. Rush and Yoon Kim},
    editor={Chengqing Zong and Fei Xia and Wenjie Li and Roberto Navigli},
    title={Parameter-Efficient Transfer Learning with Diff Pruning},
    booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021},
    pages={4884--4896},
    publisher={Association for Computational Linguistics},
    year={2021},
    url={https://doi.org/10.18653/v1/2021.acl-long.378},
    doi={10.18653/v1/2021.acl-long.378},
    timestamp={Sat, 09 Apr 2022 12:33:45 +0200},
    biburl={https://dblp.org/rec/conf/acl/GuoRK20.bib},
    bibsource={dblp computer science bibliography, https://dblp.org},
}


@inproceedings{dblp:conf/emnlp/zhaolmjs20, 
    author={Mengjie Zhao and Tao Lin and Fei Mi and Martin Jaggi and Hinrich Sch{\"{u}}tze},
    editor={Bonnie Webber and Trevor Cohn and Yulan He and Yang Liu},
    title={Masking as an Efficient Alternative to Finetuning for Pretrained Language Models},
    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
    pages={2226--2241},
    publisher={Association for Computational Linguistics},
    year={2020},
    url={https://doi.org/10.18653/v1/2020.emnlp-main.174},
    doi={10.18653/v1/2020.emnlp-main.174},
    timestamp={Wed, 23 Mar 2022 10:11:55 +0100},
    biburl={https://dblp.org/rec/conf/emnlp/ZhaoLMJS20.bib},
    bibsource={dblp computer science bibliography, https://dblp.org},
}


@article{model_written_evaluations,
  author    = {Ethan Perez and
               Sam Ringer and
               Kamile Lukosiute and
               Karina Nguyen and
               Edwin Chen and
               Scott Heiner and
               Craig Pettit and
               Catherine Olsson and
               Sandipan Kundu and
               Saurav Kadavath and
               Andy Jones and
               Anna Chen and
               Ben Mann and
               Brian Israel and
               Bryan Seethor and
               Cameron McKinnon and
               Christopher Olah and
               Da Yan and
               Daniela Amodei and
               Dario Amodei and
               Dawn Drain and
               Dustin Li and
               Eli Tran{-}Johnson and
               Guro Khundadze and
               Jackson Kernion and
               James Landis and
               Jamie Kerr and
               Jared Mueller and
               Jeeyoon Hyun and
               Joshua Landau and
               Kamal Ndousse and
               Landon Goldberg and
               Liane Lovitt and
               Martin Lucas and
               Michael Sellitto and
               Miranda Zhang and
               Neerav Kingsland and
               Nelson Elhage and
               Nicholas Joseph and
               Noem{\'{\i}} Mercado and
               Nova DasSarma and
               Oliver Rausch and
               Robin Larson and
               Sam McCandlish and
               Scott Johnston and
               Shauna Kravec and
               Sheer El Showk and
               Tamera Lanham and
               Timothy Telleen{-}Lawton and
               Tom Brown and
               Tom Henighan and
               Tristan Hume and
               Yuntao Bai and
               Zac Hatfield{-}Dodds and
               Jack Clark and
               Samuel R. Bowman and
               Amanda Askell and
               Roger Grosse and
               Danny Hernandez and
               Deep Ganguli and
               Evan Hubinger and
               Nicholas Schiefer and
               Jared Kaplan},
  title     = {Discovering Language Model Behaviors with Model-Written Evaluations},
  journal   = {CoRR},
  volume    = {abs/2212.09251},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.09251},
  doi       = {10.48550/arXiv.2212.09251},
  eprinttype = {arXiv},
  eprint    = {2212.09251},
  timestamp = {Mon, 02 Jan 2023 15:09:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-09251.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wu2018response,
  title     = {Response Generation by Context-aware Prototype Editing},
  author    = {Yu Wu and Furu Wei and Shaohan Huang and Zhoujun Li and Ming Zhou},
  journal   = {Aaai Conference On Artificial Intelligence},
  year      = {2018},
  doi       = {10.1609/aaai.v33i01.33017281},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/1a5cc4e66d50a21289799373876334385456b9fa}
}

@inproceedings{cai-etal-2019-skeleton,
  title     = {Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory},
  author    = {Cai, Deng and Wang, Yan and Bi, Wei and Tu, Zhaopeng and Liu, Xiaojiang and Lam, Wai and Shi, Shuming},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {jun},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1124},
  doi       = {10.18653/v1/N19-1124},
  pages     = {1219-1228},
  abstract  = {Traditional generative dialogue models generate responses solely from input queries. Such information is insufficient for generating a specific response since a certain query could be answered in multiple ways. Recently, researchers have attempted to fill the information gap by exploiting information retrieval techniques. For a given query, similar dialogues are retrieved from the entire training data and considered as an additional knowledge source. While the use of retrieval may harvest extensive information, the generative models could be overwhelmed, leading to unsatisfactory performance. In this paper, we propose a new framework which exploits retrieval results via a skeleton-to-response paradigm. At first, a skeleton is extracted from the retrieved dialogues. Then, both the generated skeleton and the original query are used for response generation via a novel response generator. Experimental results show that our approach significantly improves the informativeness of the generated responses}
}




@inproceedings{shin-etal-2021-constrained,
  title     = {Constrained Language Models Yield Few-Shot Semantic Parsers},
  author    = {Shin, Richard and Lin, Christopher and Thomson, Sam and Chen, Charles and Roy, Subhro and Platanios, Emmanouil Antonios and Pauls, Adam and Klein, Dan and Eisner, Jason and Van Durme, Benjamin},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = {nov},
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.608},
  doi       = {10.18653/v1/2021.emnlp-main.608},
  pages     = {7699-7715},
  abstract  = {We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.}
}

@article{hu2021knowledgeable,
  title     = {Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification},
  author    = {Shengding Hu and Ning Ding and Huadong Wang and Zhiyuan Liu and Juan-Zi Li and Maosong Sun},
  journal   = {Annual Meeting Of The Association For Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2022.acl-long.158},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/6f0aba8102d63938ce0b48ec23ff5ddd8110f2e8}
}

@article{human_machine_collaboration_dialogue_dataset,
  author    = {Helena Bonaldi and
               Sara Dellantonio and
               Serra Sinem Tekiroglu and
               Marco Guerini},
  title     = {Human-Machine Collaboration Approaches to Build a Dialogue Dataset
               for Hate Speech Countering},
  journal   = {CoRR},
  volume    = {abs/2211.03433},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.03433},
  doi       = {10.48550/arXiv.2211.03433},
  eprinttype = {arXiv},
  eprint    = {2211.03433},
  timestamp = {Wed, 09 Nov 2022 17:33:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-03433.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Dynabench,
  author    = {Christopher Potts and
               Zhengxuan Wu and
               Atticus Geiger and
               Douwe Kiela},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {DynaSent: {A} Dynamic Benchmark for Sentiment Analysis},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on Natural
               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
               Event, August 1-6, 2021},
  pages     = {2388--2404},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.acl-long.186},
  doi       = {10.18653/v1/2021.acl-long.186},
  timestamp = {Mon, 09 Aug 2021 16:25:37 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/PottsWGK20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ALFRED,
  author    = {Mohit Shridhar and
               Jesse Thomason and
               Daniel Gordon and
               Yonatan Bisk and
               Winson Han and
               Roozbeh Mottaghi and
               Luke Zettlemoyer and
               Dieter Fox},
  title     = {{ALFRED:} {A} Benchmark for Interpreting Grounded Instructions for
               Everyday Tasks},
  booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
  pages     = {10737--10746},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Shridhar\_ALFRED\_A\_Benchmark\_for\_Interpreting\_Grounded\_Instructions\_for\_Everyday\_Tasks\_CVPR\_2020\_paper.html},
  doi       = {10.1109/CVPR42600.2020.01075},
  timestamp = {Tue, 31 Aug 2021 14:00:04 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/ShridharTGBHMZF20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inproceedings{ALFREDL,
 author = {Arjun R. Akula and Spandana Gella and Aishwarya Padmakumar and Mahdi Namazifar and MOHIT BANSAL and Jesse Thomason and Dilek Hakkani-Tür},
 title = {ALFRED-L: Investigating the role of language for action learning in interactive visual environments},
 year = {2022},
 url = {https://www.amazon.science/publications/alfred-l-investigating-the-role-of-language-for-action-learning-in-interactive-visual-environments},
 booktitle = {EMNLP 2022},
}

@article{NND,
  author    = {Philippe Laban and
               Chien{-}Sheng Wu and
               Wenhao Liu and
               Caiming Xiong},
  title     = {Near-Negative Distinction: Giving a Second Life to Human Evaluation
               Datasets},
  journal   = {CoRR},
  volume    = {abs/2205.06871},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.06871},
  doi       = {10.48550/arXiv.2205.06871},
  eprinttype = {arXiv},
  eprint    = {2205.06871},
  timestamp = {Tue, 17 May 2022 17:31:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-06871.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{open_domain_dialog,
  author    = {Asma Ghandeharioun and
               Judy Hanwen Shen and
               Natasha Jaques and
               Craig Ferguson and
               Noah Jones and
               {\`{A}}gata Lapedriza and
               Rosalind W. Picard},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Approximating Interactive Human Evaluation with Self-Play for Open-Domain
               Dialog Systems},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {13658--13669},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/fc9812127bf09c7bd29ad6723c683fb5-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/GhandehariounSJ19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Evaluating_Human_Language_Model_Interaction,
  author    = {Mina Lee and
               Megha Srivastava and
               Amelia Hardy and
               John Thickstun and
               Esin Durmus and
               Ashwin Paranjape and
               Ines Gerard{-}Ursin and
               Xiang Lisa Li and
               Faisal Ladhak and
               Frieda Rong and
               Rose E. Wang and
               Minae Kwon and
               Joon Sung Park and
               Hancheng Cao and
               Tony Lee and
               Rishi Bommasani and
               Michael S. Bernstein and
               Percy Liang},
  title     = {Evaluating Human-Language Model Interaction},
  journal   = {CoRR},
  volume    = {abs/2212.09746},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.09746},
  doi       = {10.48550/arXiv.2212.09746},
  eprinttype = {arXiv},
  eprint    = {2212.09746},
  timestamp = {Tue, 03 Jan 2023 15:59:43 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-09746.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@Inproceedings{CompetencyNMT,
  author    = {Pei Zhang and
               Baosong Yang and
               Haoran Wei and
               Dayiheng Liu and
               Kai Fan and
               Luo Si and
               Jun Xie},
  title     = {Competency-Aware Neural Machine Translation: Can Machine Translation
               Know its Own Translation Quality?},
 year = {2022},
 url = {https://doi.org/10.48550/arXiv.2211.13865},
 booktitle = {EMNLP 2022},
}

@inproceedings{dialogue_coherence,
  author    = {Zheng Ye and
               Liucun Lu and
               Lishan Huang and
               Liang Lin and
               Xiaodan Liang},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {Towards Quantifiable Dialogue Coherence Evaluation},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on Natural
               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
               Event, August 1-6, 2021},
  pages     = {2718--2729},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.acl-long.211},
  doi       = {10.18653/v1/2021.acl-long.211},
  timestamp = {Mon, 09 Aug 2021 16:25:37 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/YeLHLL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hallucination-survey,
  author    = {Ziwei Ji and
               Nayeon Lee and
               Rita Frieske and
               Tiezheng Yu and
               Dan Su and
               Yan Xu and
               Etsuko Ishii and
               Yejin Bang and
               Andrea Madotto and
               Pascale Fung},
  title     = {Survey of Hallucination in Natural Language Generation},
  journal   = {CoRR},
  volume    = {abs/2202.03629},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.03629},
  eprinttype = {arXiv},
  eprint    = {2202.03629},
  timestamp = {Thu, 10 Feb 2022 09:09:21 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2202-03629.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{knowledge_f1,
  author    = {Kurt Shuster and
               Spencer Poff and
               Moya Chen and
               Douwe Kiela and
               Jason Weston},
  editor    = {Marie{-}Francine Moens and
               Xuanjing Huang and
               Lucia Specia and
               Scott Wen{-}tau Yih},
  title     = {Retrieval Augmentation Reduces Hallucination in Conversation},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
               2021},
  pages     = {3784--3803},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.findings-emnlp.320},
  doi       = {10.18653/v1/2021.findings-emnlp.320},
  timestamp = {Thu, 20 Jan 2022 10:02:30 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/0001PCKW21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ctg_survey,
  author    = {Hanqing Zhang and
               Haolin Song and
               Shaoyu Li and
               Ming Zhou and
               Dawei Song},
  title     = {A Survey of Controllable Text Generation using Transformer-based Pre-trained
               Language Models},
  journal   = {CoRR},
  volume    = {abs/2201.05337},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.05337},
  eprinttype = {arXiv},
  eprint    = {2201.05337},
  timestamp = {Mon, 30 Jan 2023 17:37:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-05337.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{DBLP:journals/fgcs/WuXSZM022,
  author    = {Xingjiao Wu and
               Luwei Xiao and
               Yixuan Sun and
               Junhang Zhang and
               Tianlong Ma and
               Liang He},
  title     = {A survey of human-in-the-loop for machine learning},
  journal   = {Future Gener. Comput. Syst.},
  volume    = {135},
  pages     = {364--381},
  year      = {2022},
  url       = {https://doi.org/10.1016/j.future.2022.05.014},
  doi       = {10.1016/j.future.2022.05.014},
  timestamp = {Mon, 25 Jul 2022 08:41:02 +0200},
  biburl    = {https://dblp.org/rec/journals/fgcs/WuXSZM022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{qa_human_in_the_loop_1,
    author = {Wallace, Eric and Rodriguez, Pedro and Feng, Shi and Yamada, Ikuya and Boyd-Graber, Jordan},
    title = "{Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {7},
    pages = {387-401},
    year = {2019},
    month = {07},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00279},
    url = {https://doi.org/10.1162/tacl\_a\_00279},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00279/1923125/tacl\_a\_00279.pdf},
}

@inproceedings{dialogue_human_in_the_loop_1,
  author    = {Jiwei Li and
               Alexander H. Miller and
               Sumit Chopra and
               Marc'Aurelio Ranzato and
               Jason Weston},
  title     = {Dialogue Learning With Human-in-the-Loop},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HJgXCV9xx},
  timestamp = {Thu, 25 Jul 2019 14:26:03 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LiMCRW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dialogue_human_in_the_loop_2,
  author    = {Bing Liu and
               G{\"{o}}khan T{\"{u}}r and
               Dilek Hakkani{-}T{\"{u}}r and
               Pararth Shah and
               Larry P. Heck},
  editor    = {Marilyn A. Walker and
               Heng Ji and
               Amanda Stent},
  title     = {Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable
               Task-Oriented Dialogue Systems},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume
               1 (Long Papers)},
  pages     = {2060--2069},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/n18-1187},
  doi       = {10.18653/v1/n18-1187},
  timestamp = {Fri, 06 Aug 2021 00:41:28 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/LiuTHSH18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dialogue_human_in_the_loop_3,
  author    = {Yichao Lu and
               Manisha Srivastava and
               Jared Kramer and
               Heba Elfardy and
               Andrea Kahn and
               Song Wang and
               Vikas Bhardwaj},
  editor    = {Anastassia Loukina and
               Michelle Morales and
               Rohit Kumar},
  title     = {Goal-Oriented End-to-End Conversational Models with Profile Features
               in a Real-World Setting},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 2 (Industry
               Papers)},
  pages     = {48--55},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-2007},
  doi       = {10.18653/v1/n19-2007},
  timestamp = {Fri, 06 Aug 2021 00:41:28 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/LuSKEKWB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dialfact,
    title = "{D}ial{F}act: A Benchmark for Fact-Checking in Dialogue",
    author = "Gupta, Prakhar  and
      Wu, Chien-Sheng  and
      Liu, Wenhao  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.263",
    doi = "10.18653/v1/2022.acl-long.263",
    pages = "3785--3801",
    abstract = "Fact-checking is an essential tool to mitigate the spread of misinformation and disinformation. We introduce the task of fact-checking in dialogue, which is a relatively unexplored area. We construct DialFact, a testing benchmark dataset of 22,245 annotated conversational claims, paired with pieces of evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable claim detection task distinguishes whether a response carries verifiable factual information; 2) Evidence retrieval task retrieves the most relevant Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue response to be supported, refuted, or not enough information. We found that existing fact-checking models trained on non-dialogue data like FEVER fail to perform well on our task, and thus, we propose a simple yet data-efficient solution to effectively improve fact-checking performance in dialogue. We point out unique challenges in DialFact such as handling the colloquialisms, coreferences, and retrieval ambiguities in the error analysis to shed light on future research in this direction.",
}

@article{izacard2022atlas,
  title   = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
  author  = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2208.03299}
}

@article{retro,
  title     = {Improving language models by retrieving from trillions of tokens},
  author    = {Sebastian Borgeaud and A. Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and J. Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and T. Hennigan and Saffron Huang and Lorenzo Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and K. Simonyan and Jack W. Rae and Erich Elsen and L. Sifre},
  journal   = {International Conference On Machine Learning},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641}
}

@article{qian2022limitations,
  title   = {Limitations of Language Models in Arithmetic and Symbolic Induction},
  author  = {Jing Qian and Hong Wang and Zekun Li and Shiyang Li and Xifeng Yan},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2208.05051}
}

@article{thoppilan2022lamda,
  title   = {LaMDA: Language Models for Dialog Applications},
  author  = {Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2201.08239}
}

@article{
pnas-simulation,
author = {Kyle Cranmer  and Johann Brehmer  and Gilles Louppe },
title = {The frontier of simulation-based inference},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {48},
pages = {30055-30062},
year = {2020},
doi = {10.1073/pnas.1912789117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1912789117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1912789117},
abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.}}

@article{parisi2022talm,
  title   = {TALM: Tool Augmented Language Models},
  author  = {Aaron Parisi and Yao Zhao and Noah Fiedel},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2205.12255}
}

@article{BEGIN,
  author    = {Nouha Dziri and
               Hannah Rashkin and
               Tal Linzen and
               David Reitter},
  title     = {Evaluating Attribution in Dialogue Systems: The {BEGIN} Benchmark},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {10},
  pages     = {1066--1083},
  year      = {2022},
  url       = {https://transacl.org/ojs/index.php/tacl/article/view/3977},
  timestamp = {Wed, 26 Oct 2022 16:52:10 +0200},
  biburl    = {https://dblp.org/rec/journals/tacl/DziriRLR22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{elkins2020can,
  title={Can GPT-3 pass a Writer’s turing test?},
  author={Elkins, Katherine and Chun, Jon},
  journal={Journal of Cultural Analytics},
  volume={5},
  number={2},
  year={2020}
}

@inproceedings{dialogues_1,
  author    = {Pei{-}Yu Chen},
  editor    = {Vincent Conitzer and
               John Tasioulas and
               Matthias Scheutz and
               Ryan Calo and
               Martina Mara and
               Annette Zimmermann},
  title     = {{AI} Alignment Dialogues: An Interactive Approach to {AI} Alignment
               in Support Agents},
  booktitle = {{AIES} '22: {AAAI/ACM} Conference on AI, Ethics, and Society, Oxford,
               United Kingdom, May 19 - 21, 2021},
  pages     = {894},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3514094.3539531},
  doi       = {10.1145/3514094.3539531},
  timestamp = {Fri, 29 Jul 2022 11:46:24 +0200},
  biburl    = {https://dblp.org/rec/conf/aies/Chen22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dialogues_2,
  author    = {Svetlana Stoyanchev and
               Suraj Pandey and
               Simon Keizer and
               Norbert Braunschweiler and
               Rama Sanand Doddipatla},
  editor    = {Oliver Lemon and
               Dilek Hakkani{-}T{\"{u}}r and
               Junyi Jessy Li and
               Arash Ashrafzadeh and
               Daniel Hern{\'{a}}ndez Garc{\'{\i}}a and
               Malihe Alikhani and
               David Vandyke and
               Ondrej Dusek},
  title     = {Combining Structured and Unstructured Knowledge in an Interactive
               Search Dialogue System},
  booktitle = {Proceedings of the 23rd Annual Meeting of the Special Interest Group
               on Discourse and Dialogue, {SIGDIAL} 2022, Edinburgh, UK, 07-09 September
               2022},
  pages     = {531--540},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.sigdial-1.50},
  timestamp = {Sat, 22 Oct 2022 00:37:58 +0200},
  biburl    = {https://dblp.org/rec/conf/sigdial/StoyanchevPKBD22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dialogues_3,
  author    = {Junfan Chen and
               Richong Zhang and
               Yongyi Mao and
               Jie Xu},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {Parallel Interactive Networks for Multi-Domain Dialogue State Generation},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {1921--1931},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.151},
  doi       = {10.18653/v1/2020.emnlp-main.151},
  timestamp = {Sat, 09 Apr 2022 12:34:21 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/ChenZMX20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{qa_1,
  author    = {Daniel Gordon and
               Aniruddha Kembhavi and
               Mohammad Rastegari and
               Joseph Redmon and
               Dieter Fox and
               Ali Farhadi},
  title     = {{IQA:} Visual Question Answering in Interactive Environments},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {4089--4098},
  publisher = {Computer Vision Foundation / {IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Gordon\_IQA\_Visual\_Question\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00430},
  timestamp = {Tue, 31 Aug 2021 14:00:32 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/GordonKRRFF18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xu2022zeroprompt,
  title={ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization},
  author={Xu, Hanwei and Chen, Yujun and Du, Yulun and Shao, Nan and Wang, Yanggang and Li, Haiyu and Yang, Zhilin},
  journal={arXiv preprint arXiv:2201.06910},
  year={2022}
}

@inproceedings{kumar-etal-2022-gradient,
    title = "Gradient-based Constrained Sampling from Language Models",
    author = "Kumar, Sachin  and
      Paria, Biswajit  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.144",
    pages = "2251--2277",
    abstract = "Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model{'}s performance in a downstream task. We propose MuCoLa{---}a sampling procedure that combines the log-likelihood of the language model with arbitrary (differentiable) constraints in a single energy function, and then generates samples in a non-autoregressive manner. Specifically, it initializes the entire output sequence with noise and follows a Markov chain defined by Langevin Dynamics using the gradients of this energy. We evaluate MuCoLa on text generation with soft and hard constraints as well as their combinations, obtaining significant improvements over competitive baselines for toxicity avoidance, sentiment control, and keyword-guided generation.",
}

@inproceedings{h-kumar-etal-2022-cuebot,
  title     = {{C}ue{B}ot: Cue-Controlled Response Generation for Assistive Interaction Usages},
  author    = {H. Kumar, Shachi and Su, Hsuan and Manuvinakurike, Ramesh and Pinaroc, Max and Prasad, Sai and Sahay, Saurav and Nachman, Lama},
  booktitle = {Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022)},
  month     = {may},
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.slpat-1.9},
  doi       = {10.18653/v1/2022.slpat-1.9},
  pages     = {66-79},
  abstract  = {Conversational assistants are ubiquitous among the general population, however, these systems have not had an impact on people with disabilities, or speech and language disorders, for whom basic day-to-day communication and social interaction is a huge struggle. Language model technology can play a huge role in empowering these users and help them interact with others with less effort via interaction support. To enable this population, we build a system that can represent them in a social conversation and generate responses that can be controlled by the users using cues/keywords. We build models that can speed up this communication by suggesting relevant cues in the dialog response context. We also introduce a keyword-loss to lexically constrain the model response output. We present automatic and human evaluation of our cue/keyword predictor and the controllable dialog system to show that our models perform significantly better than models without control. Our evaluation and user study shows that keyword-control on end-to-end response generation models is powerful and can enable and empower users with degenerative disorders to carry out their day-to-day communication.}
}

@article{Wei2022ChainOT,
  author    = {Jason Wei and
               Xuezhi Wang and
               Dale Schuurmans and
               Maarten Bosma and
               Ed H. Chi and
               Quoc Le and
               Denny Zhou},
  title     = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal   = {CoRR},
  volume    = {abs/2201.11903},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.11903},
  eprinttype = {arXiv},
  eprint    = {2201.11903},
  timestamp = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen-etal-2022-balanced,
  title     = {Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in {NLP} Models},
  author    = {Chen, Hannah and Ji, Yangfeng and Evans, David},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = {dec},
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.40},
  pages     = {632-647},
  abstract  = {Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input{'}s true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves the classifier{'}s prediction but changes the true label of an input.Adversarial training and certified robust training have shown some effectiveness in improving the robustness of machine learnt models to fickle adversarial examples. We show that standard adversarial training methods focused on reducing vulnerability to fickle adversarial examples may make a model more vulnerable to obstinate adversarial examples, with experiments for both natural language inference and paraphrase identification tasks. To counter this phenomenon, we introduce Balanced Adversarial Training, which incorporates contrastive learning to increase robustness against both fickle and obstinate adversarial examples.}
}

@article{kcn,
  journal = {IEEE Access},
  volume  = {7},
  pages   = {80542-80551},
  doi     = {10.1109/ACCESS.2019.2923057},
  title   = {User-Oriented Paraphrase Generation With Keywords Controlled Network},
  year    = {2019},
  author  = {Daojian Zeng and Haoran Zhang and Lingyun Xiang and Jin Wang and Guoliang Ji}
}

@inproceedings{wordcraft,
author = {Yuan, Ann and Coenen, Andy and Reif, Emily and Ippolito, Daphne},
title = {Wordcraft: Story Writing With Large Language Models},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511105},
doi = {10.1145/3490099.3511105},
abstract = {The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers’ custom requests expressed in natural language (such as ”rewrite this text to be more Dickensian”), and generate suggestions that serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {841–852},
numpages = {12},
keywords = {NLP},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{sorensen-etal-2022-information,
    title = "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels",
    author = "Sorensen, Taylor  and
      Robinson, Joshua  and
      Rytting, Christopher  and
      Shaw, Alexander  and
      Rogers, Kyle  and
      Delorey, Alexia  and
      Khalil, Mahmoud  and
      Fulda, Nancy  and
      Wingate, David",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.60",
    doi = "10.18653/v1/2022.acl-long.60",
    pages = "819--862",
    abstract = "Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \textit{without labeled examples} and \textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90{\%} of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.",
}

@inproceedings{qa_2,
    title = "Interactive Language Learning by Question Answering",
    author = "Yuan, Xingdi  and
      C{\^o}t{\'e}, Marc-Alexandre  and
      Fu, Jie  and
      Lin, Zhouhan  and
      Pal, Chris  and
      Bengio, Yoshua  and
      Trischler, Adam",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1280",
    doi = "10.18653/v1/D19-1280",
    pages = "2796--2813",
    abstract = "Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task: Question Answering with Interactive Text (QAit). In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The data is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of baseline models for the QAit task that includes deep reinforcement learning agents. Experiments show that the task presents a major challenge for machine reading systems, while humans solve it with relative ease.",
}

@article{sun2023principledriven,
  title     = {Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision},
  author    = {Zhiqing Sun and Yikang Shen and Qinhong Zhou and Hongxin Zhang and Zhenfang Chen and David D. Cox and Yiming Yang and Chuang Gan},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2305.03047},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/e01515c6138bc525f7aec30fc85f2adf028d4156}
}

@article{zhou2023lima,
  title   = {LIMA: Less Is More for Alignment},
  author  = {Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.11206}
}

@article{wei2023multiparty,
  title   = {Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models},
  author  = {Jimmy Wei and Kurt Shuster and Arthur Szlam and Jason Weston and Jack Urbanek and Mojtaba Komeili},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2304.13835}
}

@article{cabello2023pokemonchat,
  title     = {PokemonChat: Auditing ChatGPT for Pokémon Universe Knowledge},
  author    = {Laura Cabello and Jiaang Li and Ilias Chalkidis},
  journal   = {Social Science Research Network},
  year      = {2023},
  doi       = {10.2139/ssrn.4396798},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/253d67977dce90f8e74033589fadb6d4da6ad66c}
}

@article{rlhf-origin,
  title   = {Deep reinforcement learning from human preferences},
  author  = {Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
  year    = {2017},
  journal = {arXiv preprint arXiv: Arxiv-1706.03741}
}

@inproceedings{oscar,
  author    = {Xiujun Li and Xi Yin and Chunyuan Li and Pengchuan Zhang and Xiaowei Hu and Lei Zhang and Lijuan Wang and Houdong Hu and Li Dong and Furu Wei and Yejin Choi and Jianfeng Gao},
  editor    = {Andrea Vedaldi and Horst Bischof and Thomas Brox and Jan{-}Michael Frahm},
  title     = {Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part {XXX}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12375},
  pages     = {121-137},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58577-8\_8},
  doi       = {10.1007/978-3-030-58577-8\_8},
  timestamp = {Tue, 15 Feb 2022 08:52:28 +0100},
  biburl    = {https://dblp.org/rec/conf/eccv/Li0LZHZWH0WCG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bleu,
  author    = {Kishore Papineni and
               Salim Roukos and
               Todd Ward and
               Wei{-}Jing Zhu},
  title     = {Bleu: a Method for Automatic Evaluation of Machine Translation},
  booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational
               Linguistics, July 6-12, 2002, Philadelphia, PA, {USA}},
  pages     = {311--318},
  publisher = {{ACL}},
  year      = {2002},
  url       = {https://aclanthology.org/P02-1040/},
  doi       = {10.3115/1073083.1073135},
  timestamp = {Fri, 06 Aug 2021 00:40:58 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/PapineniRWZ02.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{icl_survey,
  author    = {Qingxiu Dong and
               Lei Li and
               Damai Dai and
               Ce Zheng and
               Zhiyong Wu and
               Baobao Chang and
               Xu Sun and
               Jingjing Xu and
               Lei Li and
               Zhifang Sui},
  title     = {A Survey for In-context Learning},
  journal   = {CoRR},
  volume    = {abs/2301.00234},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2301.00234},
  doi       = {10.48550/arXiv.2301.00234},
  eprinttype = {arXiv},
  eprint    = {2301.00234},
  timestamp = {Tue, 10 Jan 2023 15:10:12 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2301-00234.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  url= {https://aclanthology.org/W04-1013.pdf},
  pages={74--81},
  year={2004}
}

@article{honnibal2020spacy,
  added-at = {2023-05-22T04:49:27.000+0200},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  biburl = {https://www.bibsonomy.org/bibtex/2616669ca18ac051794c0459373696942/rerry},
  doi = {10.5281/zenodo.1212303},
  interhash = {2d1b3a0bb97e51df1b88d8852cd5ac01},
  intrahash = {616669ca18ac051794c0459373696942},
  keywords = {nlp},
  timestamp = {2023-05-22T04:49:27.000+0200},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020
}


@article{xiong2023uncertainty,
  title   = {Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs},
  author  = {Miao Xiong and Zhiyuan Hu and Xinyang Lu and Yifei Li and Jie Fu and Junxian He and Bryan Hooi},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2306.13063},
  url     = {https://arxiv.org/abs/2306.13063v1},
  pdf     = {https://arxiv.org/pdf/2306.13063.pdf}
}

@article{art-tool,
  title   = {ART: Automatic multi-step reasoning and tool-use for large language models},
  author  = {Bhargavi Paranjape and Scott Lundberg and Sameer Singh and Hannaneh Hajishirzi and Luke Zettlemoyer and Marco Tulio Ribeiro},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.09014}
}

@article{tang2023toolalpaca,
  title={ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases},
  author={Tang, Qiaoyu and Deng, Ziliang and Lin, Hongyu and Han, Xianpei and Liang, Qiao and Sun, Le},
  journal={arXiv preprint arXiv:2306.05301},
  year={2023}
}

@inproceedings{rastogi2020sgd,
  title={Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset},
  author={Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8689--8696},
  year={2020}
}
@article{byrne2019taskmaster1,
  title   = {Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset},
  author  = {Bill Byrne and Karthik Krishnamoorthi and Chinnadhurai Sankar and Arvind Neelakantan and Daniel Duckworth and Semih Yavuz and Ben Goodrich and Amit Dubey and Andy Cedilnik and Kyu-Young Kim},
  year    = {2019},
  journal = {arXiv preprint arXiv: 1909.05358}
}
@article{CRF-seq-label,
  title={Named entity recognition with bidirectional LSTM-CNNs},
  author={Chiu, Jason PC and Nichols, Eric},
  journal={Transactions of the association for computational linguistics},
  volume={4},
  pages={357--370},
  year={2016},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{simdial,
  title={Zero-Shot Dialog Generation with Cross-Domain Latent Actions},
  author={Zhao, Tiancheng and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:1805.04803},
  year={2018}
}

@inproceedings{shalyminov2020fast,
  title={Fast domain adaptation for goal-oriented dialogue using a hybrid generative-retrieval transformer},
  author={Shalyminov, Igor and Sordoni, Alessandro and Atkinson, Adam and Schulz, Hannes},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8039--8043},
  year={2020},
  organization={IEEE}
}

@inproceedings{rastogi2020towards,
  title={Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset},
  author={Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8689--8696},
  year={2020}
}

@article{snips,
  title     = {Unsupervised Transfer Learning for Spoken Language Understanding in Intelligent Agents},
  author    = {Aditya Siddhant and Anuj Goyal and A. Metallinou},
  journal   = {AAAI Conference on Artificial Intelligence},
  year      = {2018},
  doi       = {10.1609/AAAI.V33I01.33014959},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/9c24259972fdb12b62596f07de604b5ad9778bbe},
  url       = {https://arxiv.org/abs/1811.05370v1},
  pdf       = {https://arxiv.org/pdf/1811.05370.pdf}
}
@article{dubey2024llama3,
  title   = {The Llama 3 Herd of Models},
  author  = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzmán and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Govind Thattai and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Maria Tsimpoukelli and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2407.21783}
}

@inproceedings{lee2022sgd-x,
  title={SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems},
  author={Lee, Harrison and Gupta, Raghav and Rastogi, Abhinav and Cao, Yuan and Zhang, Bin and Wu, Yonghui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={10938--10946},
  year={2022}
}


@article{budzianowski2018multiwoz,
  title   = {MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling},
  author  = {Paweł Budzianowski and Tsung-Hsien Wen and Bo-Hsiang Tseng and Iñigo Casanueva and Stefan Ultes and Osman Ramadan and Milica Gašić},
  year    = {2018},
  journal = {arXiv preprint arXiv: 1810.00278}
}

@article{patil2023gorilla-apibench,
  title   = {Gorilla: Large Language Model Connected with Massive APIs},
  author  = {Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.15334}
}

@article{xu2023toolbench,
  title   = {On the Tool Manipulation Capability of Open-source Large Language Models},
  author  = {Qiantong Xu and Fenglu Hong and Bo Li and Changran Hu and Zhengyu Chen and Jian Zhang},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.16504}
}

@article{zhuang2023toolqa,
  title={ToolQA: A Dataset for LLM Question Answering with External Tools},
  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  journal={arXiv preprint arXiv:2306.13304},
  year={2023}
}

@inproceedings{meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}
@inproceedings{cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  url={https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf},
  year={2015}
}

@article{valm,
  title={Visually-augmented language modeling},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Song, Haoyu and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  journal={arXiv preprint arXiv:2205.10178},
  year={2022}
}



@inproceedings{realtoxicityprompts,
  author    = {Samuel Gehman and
               Suchin Gururangan and
               Maarten Sap and
               Yejin Choi and
               Noah A. Smith},
  editor    = {Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language
               Models},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2020, Online Event, 16-20 November 2020},
  series    = {Findings of {ACL}},
  volume    = {{EMNLP} 2020},
  pages     = {3356--3369},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.findings-emnlp.301},
  doi       = {10.18653/v1/2020.findings-emnlp.301},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/GehmanGSCS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ais,
  author    = {Hannah Rashkin and
               Vitaly Nikolaev and
               Matthew Lamm and
               Michael Collins and
               Dipanjan Das and
               Slav Petrov and
               Gaurav Singh Tomar and
               Iulia Turc and
               David Reitter},
  title     = {Measuring Attribution in Natural Language Generation Models},
  journal   = {CoRR},
  volume    = {abs/2112.12870},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.12870},
  eprinttype = {arXiv},
  eprint    = {2112.12870},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-12870.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tang-etal-2022-etrica,
    title = "{E}tri{CA}: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention",
    author = "Tang, Chen  and
      Lin, Chenghua  and
      Huang, Henglin  and
      Guerin, Frank  and
      Zhang, Zhihao",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.403",
    pages = "5504--5518",
}

@inproceedings{khapra-sai-2021-tutorial,
    title = "A Tutorial on Evaluation Metrics used in Natural Language Generation",
    author = "Khapra, Mitesh M.  and
      Sai, Ananya B.",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-tutorials.4",
    doi = "10.18653/v1/2021.naacl-tutorials.4",
    pages = "15--19",
    abstract = "The advent of Deep Learning and the availability of large scale datasets has accelerated research on Natural Language Generation with a focus on newer tasks and better models. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new metrics. This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions: (i) What makes NLG evaluation challenging? (ii) Why do we need automatic evaluation metrics? (iii) What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy? (iv) What are the criticisms and shortcomings of existing metrics? (v) What are the possible future directions of research?",
}

@article{meng2022memit,
  title={Mass Editing Memory in a Transformer},
  author={Kevin Meng and Sen Sharma, Arnab and Alex Andonian and Yonatan Belinkov and David Bau},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022}
}

@misc{wikipedia,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}

@article{wikidata,
author = {Vrandečić, Denny and Krötzsch, Markus},
year = {2014},
month = {09},
pages = {78-85},
title = {Wikidata: A Free Collaborative Knowledgebase},
volume = {57},
journal = {Communications of the ACM},
doi = {10.1145/2629489}
}

@inproceedings{freebase,
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376746},
doi = {10.1145/1376616.1376746},
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1250},
numpages = {4},
keywords = {collaborative systems, semantic network, tuple store},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@article{A2C,
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  author    = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and A. Graves and T. Lillicrap and Tim Harley and David Silver and K. Kavukcuoglu},
  journal   = {International Conference On Machine Learning},
  year      = {2016},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd}
}


@inproceedings{knowbert,
  title     = {Knowledge Enhanced Contextual Word Representations},
  author    = {Peters, Matthew E. and Neumann, Mark and Logan, Robert and Schwartz, Roy and Joshi, Vidur and Singh, Sameer and Smith, Noah A.},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = {nov},
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1005},
  doi       = {10.18653/v1/D19-1005},
  pages     = {43-54},
  abstract  = {Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert{'}s runtime is comparable to BERT{'}s and it scales to large KBs.}
}

@article{wang2019kepler,
  title     = {KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation},
  author    = {Xiaozhi Wang and Tianyu Gao and Zhaocheng Zhu and Zhiyuan Liu and Juan-Zi Li and Jian Tang},
  journal   = {Transactions Of The Association For Computational Linguistics},
  year      = {2019},
  doi       = {10.1162/tacl_a_00360},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb}
}

@article{self-ask,
  title   = {Measuring and Narrowing the Compositionality Gap in Language Models},
  author  = {Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and Mike Lewis},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.03350}
}

@article{zhang2023repocoder,
  title   = {RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation},
  author  = {Fengji Zhang and Bei Chen and Yue Zhang and Jin Liu and Daoguang Zan and Yi Mao and Jian-Guang Lou and Weizhu Chen},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.12570}
}

@article{wortsman2020supermasks,
  title     = {Supermasks in Superposition},
  author    = {Mitchell Wortsman and Vivek Ramanujan and Rosanne Liu and Aniruddha Kembhavi and Mohammad Rastegari and J. Yosinski and Ali Farhadi},
  journal   = {Neural Information Processing Systems},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/1a38bc074dffc4beb84a623c0ef904e47ed8f3a4}
}

@article{DBLP:journals/csur/SaiMK23,
  author    = {Ananya B. Sai and
               Akash Kumar Mohankumar and
               Mitesh M. Khapra},
  title     = {A Survey of Evaluation Metrics Used for {NLG} Systems},
  journal   = {{ACM} Comput. Surv.},
  volume    = {55},
  number    = {2},
  pages     = {26:1--26:39},
  year      = {2023},
  url       = {https://doi.org/10.1145/3485766},
  doi       = {10.1145/3485766},
  timestamp = {Wed, 18 May 2022 10:20:16 +0200},
  biburl    = {https://dblp.org/rec/journals/csur/SaiMK23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{
diplomacy,
author = {Meta Fundamental AI Research Diplomacy Team (FAIR)† and Anton Bakhtin  and Noam Brown  and Emily Dinan  and Gabriele Farina  and Colin Flaherty  and Daniel Fried  and Andrew Goff  and Jonathan Gray  and Hengyuan Hu  and Athul Paul Jacob  and Mojtaba Komeili  and Karthik Konath  and Minae Kwon  and Adam Lerer  and Mike Lewis  and Alexander H. Miller  and Sasha Mitts  and Adithya Renduchintala  and Stephen Roller  and Dirk Rowe  and Weiyan Shi  and Joe Spisak  and Alexander Wei  and David Wu  and Hugh Zhang  and Markus Zijlstra },
title = {Human-level play in the game of <i>Diplomacy</i> by combining language models with strategic reasoning},
journal = {Science},
volume = {378},
number = {6624},
pages = {1067-1074},
year = {2022},
doi = {10.1126/science.ade9097},
URL = {https://www.science.org/doi/abs/10.1126/science.ade9097},
eprint = {https://www.science.org/doi/pdf/10.1126/science.ade9097},
abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game. The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players’ motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. —YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.}}

@article{zhang2023instruction-survey,
  title   = {Instruction Tuning for Large Language Models: A Survey},
  author  = {Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2308.10792}
}

@online{halder2023harnessing,
    author = {Nilimesh Halder},
    title = {Harnessing the Power of Role-Playing in Advanced AI Language Models: A Comprehensive Guide to ChatGPT’s Potential},
    year = {2023},
    publisher = {Medium},
    howpublished = {https://medium.com/@HalderNilimesh/harnessing-the-power-of-role-playing-in-advanced-ai-language-models-a-comprehensive-guide-to-d50c818ddf59}
}


@inproceedings{10.1145/3544548.3581441,
author = {Ashby, Trevor and Webb, Braden K and Knapp, Gregory and Searle, Jackson and Fulda, Nancy},
title = {Personalized Quest and Dialogue Generation in Role-Playing Games: A Knowledge Graph- and Language Model-Based Approach},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581441},
doi = {10.1145/3544548.3581441},
abstract = {Procedural content generation (PCG) in video games offers unprecedented opportunities for customization and user engagement. Working within the specialized context of role-playing games (RPGs), we introduce a novel framework for quest and dialogue generation that places the player at the core of the generative process. Drawing on a hand-crafted knowledge base, our method grounds generated content with in-game context while simultaneously employing a large-scale language model to create fluent, unique, accompanying dialogue. Through human evaluation, we confirm that quests generated using this method can approach the performance of hand-crafted quests in terms of fluency, coherence, novelty, and creativity; demonstrate the enhancement to the player experience provided by greater dynamism; and provide a novel, automated metric for the relevance between quest and dialogue. We view our contribution as a critical step toward dynamic, co-creative narrative frameworks in which humans and AI systems jointly collaborate to create unique and user-specific playable experiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {290},
numpages = {20},
keywords = {human-AI co-creativity, text generation, natural language processing, language model, RPG, English, GPT-2, transformers, procedural content generation, large-scale language models, human-computer interaction, quest, World of Warcraft, knowledge graph, MMORPG, video games, dynamic quest generation, narrative, quests, computational creativity, knowledge-grounded text generation, NPC dialogue},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{qian2023communicative,
  title   = {Communicative Agents for Software Development},
  author  = {Chen Qian and Xin Cong and Cheng Yang and Weize Chen and Yusheng Su and Juyuan Xu and Zhiyuan Liu and Maosong Sun},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.07924}
}

@article{chen2023autoagent,
  title={AutoAgents: The Automatic Agents Generation Framework},
  author={Chen, Guangyao and Dong, Siwei and Shu, Yu and Zhang, Ge and Jaward, Sesay and Börje, Karlsson and Fu, Jie and Shi, Yemin},
  journal={arXiv preprint},
  year={2023}
}

 @article{fable2023showrunner,
          author    = {Maas and Carey and Wheeler and Saatchi and Billington and Shamash},
          title     = {To Infinity and Beyond: SHOW-1 and Showrunner Agents in Multi-Agent Simulations},
          journal   = {arXiv preprint},
          year      = {2023}
        }

@inproceedings{knowprompt,
  author    = {Xiang Chen and Ningyu Zhang and Xin Xie and Shumin Deng and Yunzhi Yao and Chuanqi Tan and Fei Huang and Luo Si and Huajun Chen},
  editor    = {Fr{\'{e}}d{\'{e}}rique Laforest and Rapha{\"{e}}l Troncy and Elena Simperl and Deepak Agarwal and Aristides Gionis and Ivan Herman and Lionel M{\'{e}}dini},
  title     = {KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction},
  booktitle = {{WWW} '22: The {ACM} Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022},
  pages     = {2778-2788},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3485447.3511998},
  doi       = {10.1145/3485447.3511998},
  timestamp = {Tue, 21 Mar 2023 20:56:40 +0100},
  biburl    = {https://dblp.org/rec/conf/www/ChenZXDYTHSC22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gehrmann-etal-2021-gem,
    title = "The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics",
    author = "Gehrmann, Sebastian  and
      Adewumi, Tosin  and
      Aggarwal, Karmanya  and
      Ammanamanchi, Pawan Sasanka  and
      Aremu, Anuoluwapo  and
      Bosselut, Antoine  and
      Chandu, Khyathi Raghavi  and
      Clinciu, Miruna-Adriana  and
      Das, Dipanjan  and
      Dhole, Kaustubh  and
      Du, Wanyu  and
      Durmus, Esin  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Emezue, Chris Chinenye  and
      Gangal, Varun  and
      Garbacea, Cristina  and
      Hashimoto, Tatsunori  and
      Hou, Yufang  and
      Jernite, Yacine  and
      Jhamtani, Harsh  and
      Ji, Yangfeng  and
      Jolly, Shailza  and
      Kale, Mihir  and
      Kumar, Dhruv  and
      Ladhak, Faisal  and
      Madaan, Aman  and
      Maddela, Mounica  and
      Mahajan, Khyati  and
      Mahamood, Saad  and
      Majumder, Bodhisattwa Prasad  and
      Martins, Pedro Henrique  and
      McMillan-Major, Angelina  and
      Mille, Simon  and
      van Miltenburg, Emiel  and
      Nadeem, Moin  and
      Narayan, Shashi  and
      Nikolaev, Vitaly  and
      Niyongabo Rubungo, Andre  and
      Osei, Salomey  and
      Parikh, Ankur  and
      Perez-Beltrachini, Laura  and
      Rao, Niranjan Ramesh  and
      Raunak, Vikas  and
      Rodriguez, Juan Diego  and
      Santhanam, Sashank  and
      Sedoc, Jo{\~a}o  and
      Sellam, Thibault  and
      Shaikh, Samira  and
      Shimorina, Anastasia  and
      Sobrevilla Cabezudo, Marco Antonio  and
      Strobelt, Hendrik  and
      Subramani, Nishant  and
      Xu, Wei  and
      Yang, Diyi  and
      Yerukola, Akhila  and
      Zhou, Jiawei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.10",
    doi = "10.18653/v1/2021.gem-1.10",
    pages = "96--120",
    abstract = "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
}

@article{DBLP:journals/corr/abs-2210-02406,
  author    = {Tushar Khot and
               Harsh Trivedi and
               Matthew Finlayson and
               Yao Fu and
               Kyle Richardson and
               Peter Clark and
               Ashish Sabharwal},
  title     = {Decomposed Prompting: {A} Modular Approach for Solving Complex Tasks},
  journal   = {CoRR},
  volume    = {abs/2210.02406},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.02406},
  doi       = {10.48550/arXiv.2210.02406},
  eprinttype = {arXiv},
  eprint    = {2210.02406},
  timestamp = {Fri, 07 Oct 2022 15:24:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2210-02406.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2022foundation,
  title   = {Foundation Transformers},
  author  = {Hongyu Wang and Shuming Ma and Shaohan Huang and Li Dong and Wenhui Wang and Zhiliang Peng and Yu Wu and Payal Bajaj and Saksham Singhal and Alon Benhaim and Barun Patra and Zhun Liu and Vishrav Chaudhary and Xia Song and Furu Wei},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.06423}
}

@article{yu2022coca,
  title   = {CoCa: Contrastive Captioners are Image-Text Foundation Models},
  author  = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2205.01917}
}

@article{li2019visualbert,
  title   = {VisualBERT: A Simple and Performant Baseline for Vision and Language},
  author  = {Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1908.03557}
}

@article{su2019vlbert,
  title   = {VL-BERT: Pre-training of Generic Visual-Linguistic Representations},
  author  = {Weijie Su and Xizhou Zhu and Yue Cao and Bin Li and Lewei Lu and Furu Wei and Jifeng Dai},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1908.08530}
}

@article{chatgpt-blip-2,
  title   = {ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions},
  author  = {Deyao Zhu and Jun Chen and Kilichbek Haydarov and Xiaoqian Shen and Wenxuan Zhang and Mohamed Elhoseiny},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.06594}
}

@article{xu2022bridge,
  title={BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning},
  author={Xu, Xiao and Wu, Chenfei and Rosenman, Shachar and Lal, Vasudev and Che, Wanxiang and Duan, Nan},
  journal={arXiv preprint arXiv:2206.08657},
  year={2022}
}

@article{mllm,
  title   = {Language Is Not All You Need: Aligning Perception with Language Models},
  author  = {Shaohan Huang and Li Dong and Wenhui Wang and Yaru Hao and Saksham Singhal and Shuming Ma and Tengchao Lv and Lei Cui and Owais Khan Mohammed and Barun Patra and Qiang Liu and Kriti Aggarwal and Zewen Chi and Johan Bjorck and Vishrav Chaudhary and Subhojit Som and Xia Song and Furu Wei},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.14045}
}

@book{using-language, place={Cambridge}, series={'Using' Linguistic Books}, title={Using Language}, DOI={10.1017/CBO9780511620539}, publisher={Cambridge University Press}, author={Clark, Herbert H.}, year={1996}, collection={'Using' Linguistic Books}}

@article{shridhar2022distilling,
  title   = {Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
  author  = {Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.00193}
}

@article{machine-language,
  title   = {Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks},
  author  = {Yuqi Wang and Xu-Yao Zhang and Cheng-Lin Liu and Zhaoxiang Zhang},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2201.05489}
}

@article{tan2019lxmert,
  title   = {LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author  = {Hao Tan and Mohit Bansal},
  year    = {2019},
  journal = {IJCNLP}
}

@article{beit3,
  title   = {Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks},
  author  = {Wenhui Wang and Hangbo Bao and Li Dong and Johan Bjorck and Zhiliang Peng and Qiang Liu and Kriti Aggarwal and Owais Khan Mohammed and Saksham Singhal and Subhojit Som and Furu Wei},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2208.10442}
}

@article{chen2022pali,
  title   = {PaLI: A Jointly-Scaled Multilingual Language-Image Model},
  author  = {Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme and Andreas Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2209.06794}
}

@article{zeng2022x2vlm,
  title   = {X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks},
  author  = {Yan Zeng and Xinsong Zhang and Hang Li and Jiawei Wang and Jipeng Zhang and Wangchunshu Zhou},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2211.12402}
}

@article{cappellazzo2022exploring,
  title     = {Exploring the Joint Use of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding},
  author    = {Umberto Cappellazzo and Daniele Falavigna and A. Brutti},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2211.08161},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/3d1f0ceea2a3d0bac9f608af85a7e284be332ea8}
}

@inproceedings{vilbert,
  author    = {Jiasen Lu and Dhruv Batra and Devi Parikh and Stefan Lee},
  editor    = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
  title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  pages     = {13-23},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/LuBPL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rt1,
    title={RT-1: Robotics Transformer for Real-World Control at Scale},
    author={Anthony	Brohan and  Noah Brown and  Justice Carbajal and  Yevgen Chebotar and  Joseph Dabis and  Chelsea Finn and  Keerthana Gopalakrishnan and  Karol Hausman and  Alex Herzog and  Jasmine Hsu and  Julian Ibarz and  Brian Ichter and  Alex Irpan and  Tomas Jackson and  Sally Jesmonth and  Nikhil Joshi and  Ryan Julian and  Dmitry Kalashnikov and  Yuheng Kuang and  Isabel Leal and  Kuang-Huei Lee and  Sergey Levine and  Yao Lu and  Utsav Malla and  Deeksha Manjunath and  Igor Mordatch and  Ofir Nachum and  Carolina Parada and  Jodilyn Peralta and  Emily Perez and  Karl Pertsch and  Jornell Quiambao and  Kanishka Rao and  Michael Ryoo and  Grecia Salazar and  Pannag Sanketi and  Kevin Sayed and  Jaspiar Singh and  Sumedh Sontakke and  Austin Stone and  Clayton Tan and  Huong Tran and  Vincent Vanhoucke and Steve Vega and  Quan Vuong and  Fei Xia and  Ted Xiao and  Peng Xu and  Sichun Xu and  Tianhe Yu and  Brianna Zitkovich},
    booktitle={arXiv preprint arXiv:2212.06817},
    year={2022}
}

@article{fromage,
  title   = {Grounding Language Models to Images for Multimodal Generation},
  author  = {Jing Yu Koh and Ruslan Salakhutdinov and Daniel Fried},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2301.13823}
}

@article{
gato,
title={A Generalist Agent},
author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio G{\'o}mez Colmenarejo and Alexander Novikov and Gabriel Barth-maron and Mai Gim{\'e}nez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=1ikK0kHjvj},
note={Featured Certification}
}

@inproceedings{uniter,
  author    = {Yen{-}Chun Chen and Linjie Li and Licheng Yu and Ahmed El Kholy and Faisal Ahmed and Zhe Gan and Yu Cheng and Jingjing Liu},
  editor    = {Andrea Vedaldi and Horst Bischof and Thomas Brox and Jan{-}Michael Frahm},
  title     = {{UNITER:} UNiversal Image-TExt Representation Learning},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part {XXX}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12375},
  pages     = {104-120},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58577-8\_7},
  doi       = {10.1007/978-3-030-58577-8\_7},
  timestamp = {Sun, 02 Oct 2022 15:59:30 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/ChenLYK0G0020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shah2022lmnav,
  title   = {LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action},
  author  = {Dhruv Shah and Blazej Osinski and Brian Ichter and Sergey Levine},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2207.04429}
}

@article{liu2023computational,
  title   = {Computational Language Acquisition with Theory of Mind},
  author  = {Andy Liu and Hao Zhu and Emmy Liu and Yonatan Bisk and Graham Neubig},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.01502}
}

@article{chen2022openvocabulary,
  title   = {Open-vocabulary Queryable Scene Representations for Real World Planning},
  author  = {Boyuan Chen and Fei Xia and Brian Ichter and Kanishka Rao and Keerthana Gopalakrishnan and Michael S. Ryoo and Austin Stone and Daniel Kappler},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2209.09874}
}

@article{kosinski2023theory,
  title   = {Theory of Mind May Have Spontaneously Emerged in Large Language Models},
  author  = {Michal Kosinski},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.02083}
}

@article{huang2022language,
  title   = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author  = {Wenlong Huang and Pieter Abbeel and Deepak Pathak and Igor Mordatch},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2201.07207}
}

@article{liu2022coordinating,
  title   = {Coordinating Policies Among Multiple Agents via an Intelligent Communication Channel},
  author  = {Dianbo Liu and Vedant Shah and Oussama Boussif and Cristian Meo and Anirudh Goyal and Tianmin Shu and Michael Mozer and Nicolas Heess and Yoshua Bengio},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2205.10607}
}

@inproceedings{workspace,
  author    = {Anirudh Goyal and Aniket Rajiv Didolkar and Alex Lamb and Kartikeya Badola and Nan Rosemary Ke and Nasim Rahaman and Jonathan Binas and Charles Blundell and Michael Curtis Mozer and Yoshua Bengio},
  title     = {Coordination Among Neural Modules Through a Shared Global Workspace},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=XzTtHjgPDsT},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/GoyalDLBKRBBMB22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2022stateful,
  title   = {Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning},
  author  = {Dianbo Liu and Vedant Shah and Oussama Boussif and Cristian Meo and Anirudh Goyal and Tianmin Shu and Michael Mozer and Nicolas Heess and Yoshua Bengio},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.03022}
}

@inproceedings{lu-etal-2021-neurologic,
    title = "{N}euro{L}ogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints",
    author = "Lu, Ximing  and
      West, Peter  and
      Zellers, Rowan  and
      Le Bras, Ronan  and
      Bhagavatula, Chandra  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.339",
    doi = "10.18653/v1/2021.naacl-main.339",
    pages = "4288--4299",
    abstract = "Conditional text generation often requires lexical constraints, i.e., which words should or shouldn{'}t be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models {--} supervised or not {--} to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.",
}

@article{mbart,
  title   = {Multilingual Denoising Pre-training for Neural Machine Translation},
  author  = {Yinhan Liu and Jiatao Gu and Naman Goyal and Xian Li and Sergey Edunov and Marjan Ghazvininejad and Mike Lewis and Luke Zettlemoyer},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2001.08210}
}


@article{bert-summary,
  title   = {Fine-tune BERT for Extractive Summarization},
  author  = {Yang Liu},
  year    = {2019},
  journal = {ARXIV}
}

@article{plm-summary-1,
  title     = {Text Summarization with Pretrained Encoders},
  author    = {Yang Liu and Mirella Lapata},
  journal   = {Conference On Empirical Methods In Natural Language Processing},
  year      = {2019},
  doi       = {10.18653/v1/D19-1387},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/63748e59f4e106cbda6b65939b77589f40e48fcb}
}

@inproceedings{lu-etal-2022-neurologic,
    title = "{N}euro{L}ogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics",
    author = "Lu, Ximing  and
      Welleck, Sean  and
      West, Peter  and
      Jiang, Liwei  and
      Kasai, Jungo  and
      Khashabi, Daniel  and
      Le Bras, Ronan  and
      Qin, Lianhui  and
      Yu, Youngjae  and
      Zellers, Rowan  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.57",
    doi = "10.18653/v1/2022.naacl-main.57",
    pages = "780--799",
    abstract = "The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the $A^*$ search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-$k$ sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.",
}

@article{social-neuro-ai,
  author  = {Samuele Bolotta and Guillaume Dumas},
  title   = {Social Neuro AI: Social Interaction as the “Dark Matter” of AI},
  journal = {Frontiers in Computer Science},
  volume  = {4},
  year    = {2022},
  url     = {https://www.frontiersin.org/articles/10.3389/fcomp.2022.846440},
  doi     = {10.3389/fcomp.2022.846440},
  issn    = {2624-9898}
}


@article{min2019multihop,
  title     = {Multi-hop Reading Comprehension through Question Decomposition and Rescoring},
  author    = {Sewon Min and Victor Zhong and Luke Zettlemoyer and Hannaneh Hajishirzi},
  journal   = {Annual Meeting Of The Association For Computational Linguistics},
  year      = {2019},
  doi       = {10.18653/v1/P19-1613},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b9372e972997c5056bb79c70526230baed2e372b}
}

@inproceedings{DBLP:conf/naacl/TalmorB18,
  author    = {Alon Talmor and Jonathan Berant},
  editor    = {Marilyn A. Walker and Heng Ji and Amanda Stent},
  title     = {The Web as a Knowledge-Base for Answering Complex Questions},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)},
  pages     = {641-651},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/n18-1059},
  doi       = {10.18653/v1/n18-1059},
  timestamp = {Fri, 06 Aug 2021 00:41:28 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/TalmorB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{perez-etal-2020-unsupervised,
  title     = {Unsupervised Question Decomposition for Question Answering},
  author    = {Perez, Ethan and Lewis, Patrick and Yih, Wen-tau and Cho, Kyunghyun and Kiela, Douwe},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = {nov},
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.713},
  doi       = {10.18653/v1/2020.emnlp-main.713},
  pages     = {8864-8880},
  abstract  = {We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.}
}

@article{zhou2022large,
  title   = {Large Language Models Are Human-Level Prompt Engineers},
  author  = {Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2211.01910}
}

@article{agrawal2022incontext,
  title   = {In-context Examples Selection for Machine Translation},
  author  = {Sweta Agrawal and Chunting Zhou and Mike Lewis and Luke Zettlemoyer and Marjan Ghazvininejad},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.02437}
}

@article{li2023finding,
  title   = {Finding Supporting Examples for In-Context Learning},
  author  = {Xiaonan Li and Xipeng Qiu},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.13539}
}

@article{li2021prefixtuning,
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author    = {Xiang Lisa Li and Percy Liang},
  journal   = {Annual Meeting Of The Association For Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2021.acl-long.353},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/53d8b356551a2361020a948f64454a6d599af69f}
}

@inproceedings{
ramamurthy2023is,
title={Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization},
author={Rajkumar Ramamurthy and Prithviraj Ammanabrolu and Kiant{\'e} Brantley and Jack Hessel and Rafet Sifa and Christian Bauckhage and Hannaneh Hajishirzi and Yejin Choi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8aHzds2uUyB}
}

@article{prompt-survey,
  title   = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author  = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2107.13586}
}

@article{DBLP:journals/corr/abs-2302-04166,
  author    = {Jinlan Fu and
               See{-}Kiong Ng and
               Zhengbao Jiang and
               Pengfei Liu},
  title     = {GPTScore: Evaluate as You Desire},
  journal   = {CoRR},
  volume    = {abs/2302.04166},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2302.04166},
  doi       = {10.48550/arXiv.2302.04166},
  eprinttype = {arXiv},
  eprint    = {2302.04166},
  timestamp = {Fri, 10 Feb 2023 12:26:39 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2302-04166.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{yang2019xlnet,
  title   = {Xlnet: Generalized autoregressive pretraining for language understanding},
  author  = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}


@article{gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}



@article{DBLP:journals/corr/abs-2302-04023,
  author    = {Yejin Bang and
               Samuel Cahyawijaya and
               Nayeon Lee and
               Wenliang Dai and
               Dan Su and
               Bryan Wilie and
               Holy Lovenia and
               Ziwei Ji and
               Tiezheng Yu and
               Willy Chung and
               Quyet V. Do and
               Yan Xu and
               Pascale Fung},
  title     = {A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning,
               Hallucination, and Interactivity},
  journal   = {CoRR},
  volume    = {abs/2302.04023},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2302.04023},
  doi       = {10.48550/arXiv.2302.04023},
  eprinttype = {arXiv},
  eprint    = {2302.04023},
  timestamp = {Fri, 10 Feb 2023 12:26:38 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2302-04023.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2023mmreact,
  author      = {Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and Ehsan Azarnasab and Faisal Ahmed and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang},
  title       = {MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action},
  publisher   = {arXiv},
  year        = {2023},
}

@article{schick2020exploiting,
  title={Exploiting cloze questions for few shot text classification and natural language inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2001.07676},
  year={2020}
}

@book{theoryaffordance,
  title={The ecological approach to visual perception: classic edition},
  author={Gibson, James J},
  year={2014},
  publisher={Psychology press}
}

@article{khetarpal2020theory,
  title   = {What can I do here? A Theory of Affordances in Reinforcement Learning},
  author  = {Khimya Khetarpal and Zafarali Ahmed and Gheorghe Comanici and David Abel and Doina Precup},
  year    = {2020},
  journal = {ICML}
}

@article{abramson2022improving,
  title   = {Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback},
  author  = {Josh Abramson and Arun Ahuja and Federico Carnevale and Petko Georgiev and Alex Goldin and Alden Hung and Jessica Landon and Jirka Lhotka and Timothy Lillicrap and Alistair Muldal and George Powell and Adam Santoro and Guy Scully and Sanjana Srivastava and Tamara von Glehn and Greg Wayne and Nathaniel Wong and Chen Yan and Rui Zhu},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2211.11602}
}

@article{liu2022mind,
  title={Mind's Eye: Grounded Language Model Reasoning through Simulation},
  author={Liu, Ruibo and Wei, Jason and Gu, Shixiang Shane and Wu, Te-Yen and Vosoughi, Soroush and Cui, Claire and Zhou, Denny and Dai, Andrew M},
  journal={arXiv preprint arXiv:2210.05359},
  year={2022}
}

@article{toolformer,
  author    = {Timo Schick and
               Jane Dwivedi{-}Yu and
               Roberto Dess{\`{\i}} and
               Roberta Raileanu and
               Maria Lomeli and
               Luke Zettlemoyer and
               Nicola Cancedda and
               Thomas Scialom},
  title     = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  journal   = {CoRR},
  volume    = {abs/2302.04761},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2302.04761},
  doi       = {10.48550/arXiv.2302.04761},
  eprinttype = {arXiv},
  eprint    = {2302.04761},
  timestamp = {Mon, 13 Feb 2023 14:23:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2302-04761.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/SellamDP20,
  author    = {Thibault Sellam and
               Dipanjan Das and
               Ankur P. Parikh},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {{BLEURT:} Learning Robust Metrics for Text Generation},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {7881--7892},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.acl-main.704},
  doi       = {10.18653/v1/2020.acl-main.704},
  timestamp = {Fri, 06 Aug 2021 00:40:55 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/SellamDP20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/ZhangKWWA20,
  author    = {Tianyi Zhang and
               Varsha Kishore and
               Felix Wu and
               Kilian Q. Weinberger and
               Yoav Artzi},
  title     = {BERTScore: Evaluating Text Generation with {BERT}},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SkeHuCVFDr},
  timestamp = {Wed, 03 Jun 2020 10:08:32 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/ReiSFL20,
  author    = {Ricardo Rei and
               Craig Stewart and
               Ana C. Farinha and
               Alon Lavie},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {{COMET:} {A} Neural Framework for {MT} Evaluation},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {2685--2702},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.213},
  doi       = {10.18653/v1/2020.emnlp-main.213},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/ReiSFL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2204-13346,
  author    = {Yu Wan and
               Dayiheng Liu and
               Baosong Yang and
               Haibo Zhang and
               Boxing Chen and
               Derek F. Wong and
               Lidia S. Chao},
  title     = {UniTE: Unified Translation Evaluation},
  journal   = {CoRR},
  volume    = {abs/2204.13346},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2204.13346},
  doi       = {10.48550/arXiv.2204.13346},
  eprinttype = {arXiv},
  eprint    = {2204.13346},
  timestamp = {Mon, 02 May 2022 17:29:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-13346.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{DBLP:journals/corr/abs-2111-05193,
  author       = {Jingjing Xu and
                  Wangchunshu Zhou and
                  Zhiyi Fu and
                  Hao Zhou and
                  Lei Li},
  title        = {A Survey on Green Deep Learning},
  journal      = {CoRR},
  volume       = {abs/2111.05193},
  year         = {2021}
}

@article{DBLP:journals/cacm/SchwartzDSE20,
  author       = {Roy Schwartz and
                  Jesse Dodge and
                  Noah A. Smith and
                  Oren Etzioni},
  title        = {Green {AI}},
  journal      = {Commun. {ACM}},
  volume       = {63},
  number       = {12},
  pages        = {54--63},
  year         = {2020}
}

@article{DBLP:journals/corr/abs-2302-14520,
  author    = {Tom Kocmi and
               Christian Federmann},
  title     = {Large Language Models Are State-of-the-Art Evaluators of Translation
               Quality},
  journal   = {CoRR},
  volume    = {abs/2302.14520},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2302.14520},
  doi       = {10.48550/arXiv.2302.14520},
  eprinttype = {arXiv},
  eprint    = {2302.14520},
  timestamp = {Thu, 02 Mar 2023 10:23:33 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2302-14520.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{driess2023palme,
    title={PaLM-E: An Embodied Multimodal Language Model},
    author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
    booktitle={arXiv preprint arXiv:2303.03378},
    year={2023}
}

@article{gao2023alexa,
  title={Alexa Arena: A User-Centric Interactive Platform for Embodied AI},
  author={Gao, Qiaozi and Thattai, Govind and Gao, Xiaofeng and Shakiah, Suhaila and Pansare, Shreyas and Sharma, Vasu and Sukhatme, Gaurav and Shi, Hangjie and Yang, Bofei and Zheng, Desheng and others},
  journal={arXiv preprint arXiv:2303.01586},
  year={2023}
}

@article{si2022reliable,
  title   = {Prompting GPT-3 To Be Reliable},
  author  = {Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Boyd-Graber and Lijuan Wang},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.09150}
}

@article{huang2023grounded,
  title={Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control},
  author={Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and others},
  journal={arXiv preprint arXiv:2303.00855},
  year={2023}
}

@inproceedings{ma-etal-2022-dorothie,
    title = "{DOROTHIE}: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents",
    author = "Ma, Ziqiao  and
      VanDerPloeg, Benjamin  and
      Bara, Cristian-Paul  and
      Huang, Yidong  and
      Kim, Eui-In  and
      Gervits, Felix  and
      Marge, Matthew  and
      Chai, Joyce",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.354",
    pages = "4800--4822",
    abstract = "In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often only human operators. Empowering autonomous driving agents with the ability to navigate in a continuous and dynamic environment and to communicate with humans through sensorimotor-grounded dialogue becomes critical. To this end, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a novel interactive simulation platform that enables the creation of unexpected situations on the fly to support empirical studies on situated communication with autonomous driving agents. Based on this platform, we created the Situated Dialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of 8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed audio. SDN is developed to evaluate the agent{'}s ability to predict dialogue moves from humans as well as generate its own dialogue moves and physical navigation actions. We further developed a transformer-based baseline model for these SDN tasks. Our empirical results indicate that language guided-navigation in a highly dynamic environment is an extremely difficult task for end-to-end models. These results will provide insight towards future work on robust autonomous driving agents",
}


@article{wang2022selfinstruct,
  title   = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author  = {Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.10560}
}

@article{yue2023mammoth,
  title   = {MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning},
  author  = {Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2309.05653},
  url     = {https://arxiv.org/abs/2309.05653v1},
  pdf     = {https://arxiv.org/pdf/2309.05653.pdf}
}


@article{knowledge-aware-fine-tuning,
  title   = {Large Language Models with Controllable Working Memory},
  author  = {Daliang Li and Ankit Singh Rawat and Manzil Zaheer and Xin Wang and Michal Lukasik and Andreas Veit and Felix Yu and Sanjiv Kumar},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2211.05110}
}

@inproceedings{DBLP:conf/cvpr/XiangQMXZLLJYWY20,
  author    = {Fanbo Xiang and
               Yuzhe Qin and
               Kaichun Mo and
               Yikuan Xia and
               Hao Zhu and
               Fangchen Liu and
               Minghua Liu and
               Hanxiao Jiang and
               Yifu Yuan and
               He Wang and
               Li Yi and
               Angel X. Chang and
               Leonidas J. Guibas and
               Hao Su},
  title     = {{SAPIEN:} {A} SimulAted Part-Based Interactive ENvironment},
  booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
  pages     = {11094--11104},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Xiang\_SAPIEN\_A\_SimulAted\_Part-Based\_Interactive\_ENvironment\_CVPR\_2020\_paper.html},
  doi       = {10.1109/CVPR42600.2020.01111},
  timestamp = {Thu, 20 Oct 2022 10:45:07 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/XiangQMXZLLJYWY20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1902-03570,
  author    = {Deshraj Yadav and
               Rishabh Jain and
               Harsh Agrawal and
               Prithvijit Chattopadhyay and
               Taranjeet Singh and
               Akash Jain and
               Shivkaran Singh and
               Stefan Lee and
               Dhruv Batra},
  title     = {EvalAI: Towards Better Evaluation Systems for {AI} Agents},
  journal   = {CoRR},
  volume    = {abs/1902.03570},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.03570},
  eprinttype = {arXiv},
  eprint    = {1902.03570},
  timestamp = {Sat, 23 Jan 2021 01:11:39 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-03570.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{emboded-bert,
  title   = {Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion},
  author  = {Alessandro Suglia and Qiaozi Gao and Jesse Thomason and Govind Thattai and Gaurav Sukhatme},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2108.04927}
}

@inproceedings{DBLP:conf/cvpr/DasDGLPB18,
  author    = {Abhishek Das and
               Samyak Datta and
               Georgia Gkioxari and
               Stefan Lee and
               Devi Parikh and
               Dhruv Batra},
  title     = {Embodied Question Answering},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {1--10},
  publisher = {Computer Vision Foundation / {IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Das\_Embodied\_Question\_Answering\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00008},
  timestamp = {Tue, 31 Aug 2021 14:00:32 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/DasDGLPB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Google Scholar
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

% Google Scholar
@article{yang2023foundation,
  title={Foundation Models for Decision Making: Problems, Methods, and Opportunities},
  author={Yang, Sherry and Nachum, Ofir and Du, Yilun and Wei, Jason and Abbeel, Pieter and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2303.04129},
  year={2023}
}

% Google Scholar
@article{saycan,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}

% Google Scholar
@article{ait2020kbot,
  title={KBot: A Knowledge graph based chatBot for natural language understanding over linked data},
  author={Ait-Mlouk, Addi and Jiang, Lili},
  journal={IEEE Access},
  volume={8},
  pages={149220--149230},
  year={2020},
  publisher={IEEE}
}

% Google Scholar
@article{huang2022inner,
  title={Inner monologue: Embodied reasoning through planning with language models},
  author={Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
  journal={arXiv preprint arXiv:2207.05608},
  year={2022}
}

% Google Scholar
@inproceedings{jiang2021talk,
  title={Talk-to-edit: Fine-grained facial editing via dialog},
  author={Jiang, Yuming and Huang, Ziqi and Pan, Xingang and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13799--13808},
  year={2021}
}


% Google Scholar
@article{jiang2022text2human,
  title={Text2human: Text-driven controllable human image generation},
  author={Jiang, Yuming and Yang, Shuai and Qiu, Haonan and Wu, Wayne and Loy, Chen Change and Liu, Ziwei},
  journal={ACM Transactions on Graphics (TOG)},
  volume={41},
  number={4},
  pages={1--11},
  year={2022},
  publisher={ACM New York, NY, USA}
}

% Google Scholar
@article{carta2023grounding,
  title={Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning},
  author={Carta, Thomas and Romac, Cl{\'e}ment and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:2302.02662},
  year={2023}
}

@article{ryoo2022token,
  title   = {Token Turing Machines},
  author  = {Michael S. Ryoo and Keerthana Gopalakrishnan and Kumara Kahatapitiya and Ted Xiao and Kanishka Rao and Austin Stone and Yao Lu and Julian Ibarz and Anurag Arnab},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2211.09119}
}

@article{camel,
  title={CAMEL: Communicative Agents for" Mind" Exploration of Large Scale Language Model Society},
  author={Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2303.17760},
  year={2023}
}

@article{madaan2022memory,
title={Memory-assisted prompt editing to improve GPT-3 after deployment},
author={Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
journal={arXiv preprint arXiv:2201.06009},
year={2022}
}

@article{dalvi2022towards,
  title     = {Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement},
  author    = {Bhavana Dalvi and Oyvind Tafjord and Peter Clark},
  journal   = {Conference On Empirical Methods In Natural Language Processing},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/e7d75b80e0fa3ae190ff91676dbf18a006d3a311}
}

@inproceedings{the-dynamics-1998,
author = {Claus, Caroline and Boutilier, Craig},
title = {The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems},
year = {1998},
isbn = {0262510987},
publisher = {American Association for Artificial Intelligence},
address = {USA},
abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multi agent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-leaming in cooperative multi agent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.},
booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
pages = {746–752},
numpages = {7},
location = {Madison, Wisconsin, USA},
series = {AAAI '98/IAAI '98}
}

@inproceedings{DBLP:conf/iclr/LazaridouPB17,
  author    = {Angeliki Lazaridou and Alexander Peysakhovich and Marco Baroni},
  title     = {Multi-Agent Cooperation and the Emergence of (Natural) Language},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Hk8N3Sclg},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LazaridouPB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{rabinowitz2018machine,
  title     = {Machine Theory of Mind},
  author    = {Neil C. Rabinowitz and Frank Perbet and H. F. Song and Chiyuan Zhang and S. Eslami and M. Botvinick},
  journal   = {International Conference On Machine Learning},
  year      = {2018},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/856d5dcba4772328b8fb784494e3d41d39669b0d}
}

@article{zhu2021fewshot,
  title   = {Few-shot Language Coordination by Modeling Theory of Mind},
  author  = {Hao Zhu and Graham Neubig and Yonatan Bisk},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2107.05697}
}

@inproceedings{verma-etal-2022-chai,
    title = "{CHAI}: A {CH}atbot {AI} for Task-Oriented Dialogue with Offline Reinforcement Learning",
    author = "Verma, Siddharth  and
      Fu, Justin  and
      Yang, Sherry  and
      Levine, Sergey",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.332",
    doi = "10.18653/v1/2022.naacl-main.332",
    pages = "4471--4491",
    abstract = "Conventionally, generation of natural language for dialogue agents may be viewed as a statistical learning problem: determine the patterns in human-provided data and generate appropriate responses with similar statistical properties. However, dialogue can also be regarded as a goal directed process, where speakers attempt to accomplish a specific task. Reinforcement learning (RL) algorithms are designed specifically for solving such goal-directed problems, but the most direct way to apply RL, through trial-and-error learning in human conversations, is costly. In this paper, we study how offline reinforcement learning can instead be used to train dialogue agents entirely using static datasets collected from human speakers. Our experiments show that recently developed offline RL methods can be combined with language models to yield realistic dialogue agents that better accomplish task goals.",
}

@inproceedings{elgohary-etal-2020-speak,
    title = "Speak to your Parser: Interactive Text-to-{SQL} with Natural Language Feedback",
    author = "Elgohary, Ahmed  and
      Hosseini, Saghar  and
      Hassan Awadallah, Ahmed",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.187",
    doi = "10.18653/v1/2020.acl-main.187",
    pages = "2065--2077",
    abstract = "We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5{\%}, our best model achieves only 25.1{\%}, which leaves a large gap for improvement in future research. SPLASH is publicly available at https://aka.ms/Splash{\_}dataset.",
}

@inproceedings{elgohary-etal-2021-nl,
    title = "{NL}-{EDIT}: Correcting Semantic Parse Errors through Natural Language Interaction",
    author = "Elgohary, Ahmed  and
      Meek, Christopher  and
      Richardson, Matthew  and
      Fourney, Adam  and
      Ramos, Gonzalo  and
      Awadallah, Ahmed Hassan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.444",
    doi = "10.18653/v1/2021.naacl-main.444",
    pages = "5599--5610",
    abstract = "We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20{\%} with only one turn of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit.",
}

@article{code-as-policies,
  title   = {Code as Policies: Language Model Programs for Embodied Control},
  author  = {Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2209.07753}
}

@article{generate-rather-than-retrieve,
  title   = {Generate rather than Retrieve: Large Language Models are Strong Context Generators},
  author  = {Wenhao Yu and Dan Iter and Shuohang Wang and Yichong Xu and Mingxuan Ju and Soumya Sanyal and Chenguang Zhu and Michael Zeng and Meng Jiang},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2209.10063}
}

@inproceedings{
dettmers2022gptint,
title={{GPT}3.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
author={Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=dXiGWqBoxaD}
}

@inproceedings{DBLP:conf/nips/ZhouXGM0W20,
  author       = {Wangchunshu Zhou and
                  Canwen Xu and
                  Tao Ge and
                  Julian J. McAuley and
                  Ke Xu and
                  Furu Wei},
  title        = {{BERT} Loses Patience: Fast and Robust Inference with Early Exit},
  booktitle    = {NeurIPS},
  year         = {2020}
}

@inproceedings{li-etal-2021-cascadebert-accelerating,
    title = "{C}ascade{BERT}: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade",
    author = "Li, Lei  and
      Lin, Yankai  and
      Chen, Deli  and
      Ren, Shuhuai  and
      Li, Peng  and
      Zhou, Jie  and
      Sun, Xu",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.43",
    doi = "10.18653/v1/2021.findings-emnlp.43",
    pages = "475--486",
}

@inproceedings{xu-etal-2021-beyond,
    title = "Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of {BERT} Compression",
    author = "Xu, Canwen  and
      Zhou, Wangchunshu  and
      Ge, Tao  and
      Xu, Ke  and
      McAuley, Julian  and
      Wei, Furu",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.832",
    doi = "10.18653/v1/2021.emnlp-main.832",
    pages = "10653--10659",
}



@inproceedings{varshney-baral-2022-model,
    title = "Model Cascading: Towards Jointly Improving Efficiency and Accuracy of {NLP} Systems",
    author = "Varshney, Neeraj  and
      Baral, Chitta",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.756",
    pages = "11007--11021",
}

@inproceedings{gordon-etal-2020-compressing,
    title = "Compressing {BERT}: Studying the Effects of Weight Pruning on Transfer Learning",
    author = "Gordon, Mitchell  and
      Duh, Kevin  and
      Andrews, Nicholas",
    booktitle = "Proceedings of the 5th Workshop on Representation Learning for NLP",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.repl4nlp-1.18",
    doi = "10.18653/v1/2020.repl4nlp-1.18",
    pages = "143--155",
    abstract = "Pre-trained universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40{\%}) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.",
}

@inproceedings{zhou-etal-2022-bert,
    title = "{BERT} Learns to Teach: Knowledge Distillation with Meta Learning",
    author = "Zhou, Wangchunshu  and
      Xu, Canwen  and
      McAuley, Julian",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.485",
    doi = "10.18653/v1/2022.acl-long.485",
    pages = "7037--7049",
}

@inproceedings{DBLP:conf/aaai/ShenDYMYGMK20,
  author       = {Sheng Shen and
                  Zhen Dong and
                  Jiayu Ye and
                  Linjian Ma and
                  Zhewei Yao and
                  Amir Gholami and
                  Michael W. Mahoney and
                  Kurt Keutzer},
  title        = {{Q-BERT:} Hessian Based Ultra Low Precision Quantization of {BERT}},
  booktitle    = {{AAAI}},
  pages        = {8815--8821},
  publisher    = {{AAAI} Press},
  year         = {2020}
}

@inproceedings{schwartz-etal-2020-right,
    title = "The Right Tool for the Job: Matching Model and Instance Complexities",
    author = "Schwartz, Roy  and
      Stanovsky, Gabriel  and
      Swayamdipta, Swabha  and
      Dodge, Jesse  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.593",
    doi = "10.18653/v1/2020.acl-main.593",
    pages = "6640--6651",
}

@misc{graves2017adaptive,
      title={Adaptive Computation Time for Recurrent Neural Networks}, 
      author={Alex Graves},
      year={2017},
      eprint={1603.08983},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@inproceedings{xu-etal-2020-bert,
    title = "{BERT}-of-Theseus: Compressing {BERT} by Progressive Module Replacing",
    author = "Xu, Canwen  and
      Zhou, Wangchunshu  and
      Ge, Tao  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.633",
    doi = "10.18653/v1/2020.emnlp-main.633",
    pages = "7859--7869",
}


@inproceedings{NEURIPS2019_2c601ad9,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}

% Google Scholar
@article{sharma2022correcting,
  title={Correcting robot plans with natural language feedback},
  author={Sharma, Pratyusha and Sundaralingam, Balakumar and Blukis, Valts and Paxton, Chris and Hermans, Tucker and Torralba, Antonio and Andreas, Jacob and Fox, Dieter},
  journal={arXiv preprint arXiv:2204.05186},
  year={2022}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}



@article{honovich2022unnatural,
  title   = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
  author  = {Or Honovich and Thomas Scialom and Omer Levy and Timo Schick},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.09689}
}

% Google Scholar
@inproceedings{patel2021interpretation,
  title={Interpretation of emergent communication in heterogeneous collaborative embodied agents},
  author={Patel, Shivansh and Wani, Saim and Jain, Unnat and Schwing, Alexander G and Lazebnik, Svetlana and Savva, Manolis and Chang, Angel X},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15953--15963},
  year={2021}
}

% Google Scholar
@article{puig2020watch,
  title={Watch-and-help: A challenge for social perception and human-ai collaboration},
  author={Puig, Xavier and Shu, Tianmin and Li, Shuang and Wang, Zilin and Liao, Yuan-Hong and Tenenbaum, Joshua B and Fidler, Sanja and Torralba, Antonio},
  journal={arXiv preprint arXiv:2010.09890},
  year={2020}
}

% Google Scholar
@article{o2019google,
  title={GOOGLE'S Duplex: Pretending to be human},
  author={O'Leary, Daniel E},
  journal={Intelligent Systems in Accounting, Finance and Management},
  volume={26},
  number={1},
  pages={46--53},
  year={2019},
  publisher={Wiley Online Library}
}

% Google Scholar
@inproceedings{suh2021development,
  title={Development of Speech Dialogue Systems for Social AI in Cooperative Game Environments},
  author={Suh, Jaeyoung and Bennett, Casey C and Weiss, Benjamin and Yoon, Eunseo and Jeong, Jihong and Chae, Yejin},
  booktitle={2021 IEEE Region 10 Symposium (TENSYMP)},
  pages={1--4},
  year={2021},
  organization={IEEE}
}

% Google Scholar
@article{huang2023audio,
  title={Audio Visual Language Maps for Robot Navigation},
  author={Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  journal={arXiv preprint arXiv:2303.07522},
  year={2023}
}

% Google Scholar
@article{gao2022dialfred,
  title={Dialfred: Dialogue-enabled agents for embodied instruction following},
  author={Gao, Xiaofeng and Gao, Qiaozi and Gong, Ran and Lin, Kaixiang and Thattai, Govind and Sukhatme, Gaurav S},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={4},
  pages={10049--10056},
  year={2022},
  publisher={IEEE}
}

@inproceedings{krishnaswamy-alalyani-2021-embodied,
    title = "Embodied Multimodal Agents to Bridge the Understanding Gap",
    author = "Krishnaswamy, Nikhil  and
      Alalyani, Nada",
    booktitle = "Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.hcinlp-1.7",
    pages = "41--46",
    abstract = "In this paper we argue that embodied multimodal agents, i.e., avatars, can play an important role in moving natural language processing toward {``}deep understanding.{''} Fully-featured interactive agents, model encounters between two {``}people,{''} but a language-only agent has little environmental and situational awareness. Multimodal agents bring new opportunities for interpreting visuals, locational information, gestures, etc., which are more axes along which to communicate. We propose that multimodal agents, by facilitating an embodied form of human-computer interaction, provide additional structure that can be used to train models that move NLP systems closer to genuine {``}understanding{''} of grounded language, and we discuss ongoing studies using existing systems.",
}

% Google Scholar
@article{zhao2023chat,
  title={Chat with the Environment: Interactive Multimodal Perception using Large Language Models},
  author={Zhao, Xufeng and Li, Mengdi and Weber, Cornelius and Hafez, Muhammad Burhan and Wermter, Stefan},
  journal={arXiv preprint arXiv:2303.08268},
  year={2023}
}


% Google Scholar
@article{korbak2023pretraining,
  title={Pretraining Language Models with Human Preferences},
  author={Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika and Buckley, Christopher L and Phang, Jason and Bowman, Samuel R and Perez, Ethan},
  journal={arXiv preprint arXiv:2302.08582},
  year={2023}
}

% Google Scholar
@article{yao2022webshop,
  title={Webshop: Towards scalable real-world web interaction with grounded language agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2207.01206},
  year={2022}
}

% Google Scholar
@article{sparrow-deepmind,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@article{albef,
  title     = {Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
  author    = {Junnan Li and Ramprasaath R. Selvaraju and Akhilesh Deepak Gotmare and Shafiq R. Joty and Caiming Xiong and S. Hoi},
  journal   = {Neural Information Processing Systems},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1}
}



% Google Scholar
@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@book{cognitive-gadgets,
url = {https://doi.org/10.4159/9780674985155},
title = {Cognitive Gadgets},
title = {The Cultural Evolution of Thinking},
author = {Cecilia Heyes},
publisher = {Harvard University Press},
address = {Cambridge, MA and London, England},
doi = {doi:10.4159/9780674985155},
isbn = {9780674985155},
year = {2018},
lastchecked = {2023-03-24}
}


% Google Scholar
@article{fan2022minedojo,
  title={Minedojo: Building open-ended embodied agents with internet-scale knowledge},
  author={Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2206.08853},
  year={2022}
}

@article{premack_woodruff_1978, title={Does the chimpanzee have a theory of mind?}, volume={1}, DOI={10.1017/S0140525X00076512}, number={4}, journal={Behavioral and Brain Sciences}, publisher={Cambridge University Press}, author={Premack, David and Woodruff, Guy}, year={1978}, pages={515–526}}

@inproceedings{
preference-transformer,
title={Preference Transformer: Modeling Human Preferences using Transformers for {RL}},
author={Changyeon Kim and Jongjin Park and Jinwoo Shin and Honglak Lee and Pieter Abbeel and Kimin Lee},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Peot1SFDX0}
}

@article{ptr-prompt,
  author    = {Xu Han and Weilin Zhao and Ning Ding and Zhiyuan Liu and Maosong Sun},
  title     = {{PTR:} Prompt Tuning with Rules for Text Classification},
  journal   = {{AI} Open},
  volume    = {3},
  pages     = {182-192},
  year      = {2022},
  url       = {https://doi.org/10.1016/j.aiopen.2022.11.003},
  doi       = {10.1016/j.aiopen.2022.11.003},
  timestamp = {Fri, 10 Feb 2023 23:34:52 +0100},
  biburl    = {https://dblp.org/rec/journals/aiopen/HanZDLS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rulebert,
  author    = {Mohammed Saeed and Naser Ahmadi and Preslav Nakov and Paolo Papotti},
  editor    = {Marie{-}Francine Moens and Xuanjing Huang and Lucia Specia and Scott Wen{-}tau Yih},
  title     = {RuleBERT: Teaching Soft Rules to Pre-Trained Language Models},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021},
  pages     = {1460-1476},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.emnlp-main.110},
  doi       = {10.18653/v1/2021.emnlp-main.110},
  timestamp = {Thu, 20 Jan 2022 10:02:14 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/0002ANP21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{learning-to-retrieve-prompts,
  author    = {Ohad Rubin and Jonathan Herzig and Jonathan Berant},
  editor    = {Marine Carpuat and Marie{-}Catherine de Marneffe and Iv{\'{a}}n Vladimir Meza Ru{\'{\i}}z},
  title     = {Learning To Retrieve Prompts for In-Context Learning},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022},
  pages     = {2655-2671},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.naacl-main.191},
  doi       = {10.18653/v1/2022.naacl-main.191},
  timestamp = {Mon, 01 Aug 2022 16:27:57 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/RubinHB22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{cheng2023uprise,
  title   = {UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation},
  author  = {Daixuan Cheng and Shaohan Huang and Junyu Bi and Yuefeng Zhan and Jianfeng Liu and Yujing Wang and Hao Sun and Furu Wei and Denvy Deng and Qi Zhang},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.08518}
}

@article{srivastava2022beyond,
  title   = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author  = {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramón Risco Delgado and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Timothy Telleen-Lawton and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2206.04615}
}

@inproceedings{korbak-etal-2022-rl,
    title = "{RL} with {KL} penalties is better viewed as {B}ayesian inference",
    author = "Korbak, Tomasz  and
      Perez, Ethan  and
      Buckley, Christopher",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.77",
    pages = "1083--1091",
    abstract = "Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.",
}


@article{alayrac2022flamingo,
  title   = {Flamingo: a Visual Language Model for Few-Shot Learning},
  author  = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  year    = {2022},
  journal = {DEEPMIND}
}

@article{sun2022recitationaugmented,
  title   = {Recitation-Augmented Language Models},
  author  = {Zhiqing Sun and Xuezhi Wang and Yi Tay and Yiming Yang and Denny Zhou},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.01296}
}

@inproceedings{jacob-lm-agent,
  author    = {Jacob Andreas},
  editor    = {Yoav Goldberg and Zornitsa Kozareva and Yue Zhang},
  title     = {Language Models as Agent Models},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},
  pages     = {5769-5779},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.findings-emnlp.423},
  timestamp = {Tue, 07 Feb 2023 17:10:52 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/Andreas22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{li2023llm,
  title     = {Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs},
  author    = {Jinyang Li and Binyuan Hui and Ge Qu and Binhua Li and Jiaxin Yang and Bowen Li and Bailin Wang and Bowen Qin and Rongyu Cao and Ruiying Geng and Nan Huo and Chenhao Ma and K. Chang and Fei Huang and Reynold Cheng and Yongbin Li},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2305.03111},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/521fba43e1078841876ecbfbc17ad6c56cd0f5b5}
}

@article{ultrachat,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@inproceedings{lmkb,
  author    = {Fabio Petroni and Tim Rockt{\"{a}}schel and Sebastian Riedel and Patrick S. H. Lewis and Anton Bakhtin and Yuxiang Wu and Alexander H. Miller},
  editor    = {Kentaro Inui and Jing Jiang and Vincent Ng and Xiaojun Wan},
  title     = {Language Models as Knowledge Bases?},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7, 2019},
  pages     = {2463-2473},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/D19-1250},
  doi       = {10.18653/v1/D19-1250},
  timestamp = {Thu, 07 Apr 2022 09:14:07 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/PetroniRRLBWM19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{sutskever2014sequence,
title = {Sequence to Sequence Learning with Neural Networks},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3104--3112},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}
@inproceedings{cai-etal-2021-neural,
    title = "Neural Machine Translation with Monolingual Translation Memory",
    author = "Cai, Deng  and
      Wang, Yan  and
      Li, Huayang  and
      Lam, Wai  and
      Liu, Lemao",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.567",
    doi = "10.18653/v1/2021.acl-long.567",
    pages = "7307--7318",
    abstract = "Prior work has proved that Translation Memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.",
}

@inproceedings{dua-etal-2019-drop,
    title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
    author = "Dua, Dheeru  and
      Wang, Yizhong  and
      Dasigi, Pradeep  and
      Stanovsky, Gabriel  and
      Singh, Sameer  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1246",
    doi = "10.18653/v1/N19-1246",
    pages = "2368--2378",
    abstract = "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4{\%} F1 on our generalized accuracy metric, while expert human performance is 96{\%}. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51{\%} F1.",
}

@article{easy-distracted,
  title     = {Large Language Models Can Be Easily Distracted by Irrelevant Context},
  author    = {Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and E. Chi and Nathanael Scharli and Denny Zhou},
  journal   = {International Conference on Machine Learning},
  year      = {2023},
  doi       = {10.48550/arXiv.2302.00093},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e}
}

@article{wei2022emergent,
  title   = {Emergent Abilities of Large Language Models},
  author  = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2206.07682}
}

@article{math-verifier,
  title   = {Training Verifiers to Solve Math Word Problems},
  author  = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2110.14168}
}

@article{llmteacher,
  title   = {Large Language Models Are Reasoning Teachers},
  author  = {Namgyu Ho and Laura Schmid and Se-Young Yun},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.10071}
}



@article{tay2022scalingvsarch,
  title   = {Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  author  = {Yi Tay and Mostafa Dehghani and Samira Abnar and Hyung Won Chung and William Fedus and Jinfeng Rao and Sharan Narang and Vinh Q. Tran and Dani Yogatama and Donald Metzler},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2207.10551}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{gpt-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{iyer2022opt,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, D{\'a}niel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}

@article{liu2023perspectives,
  title={Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback},
  author={Liu, Gabrielle Kaili-May},
  journal={arXiv preprint arXiv:2303.02891},
  year={2023}
}



@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}

@article{pilault2023interactivechainprompting,
  title   = {Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction},
  author  = {Jonathan Pilault and Xavier Garcia and Arthur Bražinskas and Orhan Firat},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2301.10309}
}

@article{guu2020realm,
  title   = {REALM: Retrieval-Augmented Language Model Pre-Training},
  author  = {Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2002.08909}
}

@article{see-think-confirm,
  title   = {See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning},
  author  = {Zhenfang Chen and Qinhong Zhou and Yikang Shen and Yining Hong and Hao Zhang and Chuang Gan},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2301.05226}
}

@inproceedings{see-etal-2017-get,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}

@inproceedings{gu-etal-2016-incorporating,
    title = "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
    author = "Gu, Jiatao  and
      Lu, Zhengdong  and
      Li, Hang  and
      Li, Victor O.K.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1154",
    doi = "10.18653/v1/P16-1154",
    pages = "1631--1640",
}

@article{kawaguchi2017generalization,
  title   = {Generalization in Deep Learning},
  author  = {Kenji Kawaguchi and Leslie Pack Kaelbling and Yoshua Bengio},
  year    = {2017},
  journal = {arXiv preprint arXiv: Arxiv-1710.05468}
}

@misc{träuble2023discrete,
      title={Discrete Key-Value Bottleneck}, 
      author={Frederik Träuble and Anirudh Goyal and Nasim Rahaman and Michael Mozer and Kenji Kawaguchi and Yoshua Bengio and Bernhard Schölkopf},
      year={2023},
      eprint={2207.11240},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{vlptm-survey,
  title   = {A Survey of Vision-Language Pre-Trained Models},
  author  = {Yifan Du and Zikang Liu and Junyi Li and Wayne Xin Zhao},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2202.10936}
}

@article{sanh2021multitask,
  title     = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author    = {Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal V. Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Févry and Jason Alan Fries and Ryan Teehan and Stella Rose Biderman and Leo Gao and T. Bers and Thomas Wolf and Alexander M. Rush},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/17dd3555fd1ccf1141cf984347fa1b3fd6b009ca}
}

@inproceedings{naturalinstructions,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle={ACL},
  year={2022}
}
@inproceedings{supernaturalinstructions,
  title={Super-NaturalInstructions:Generalization via Declarative Instructions on 1600+ Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  booktitle={EMNLP},
  year={2022}
}

@article{clip,
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  author    = {Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  journal   = {International Conference On Machine Learning},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4}
}

@article{zhang2023multimodal,
  title   = {Multimodal Chain-of-Thought Reasoning in Language Models},
  author  = {Zhuosheng Zhang and Aston Zhang and Mu Li and Hai Zhao and George Karypis and Alex Smola},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.00923}
}

@inproceedings{beinborn-etal-2018-multimodal,
    title = "Multimodal Grounding for Language Processing",
    author = "Beinborn, Lisa  and
      Botschen, Teresa  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1197",
    pages = "2325--2339",
    abstract = "This survey discusses how recent developments in multimodal processing facilitate conceptual grounding of language. We categorize the information flow in multimodal processing with respect to cognitive models of human information processing and analyze different methods for combining multimodal representations. Based on this methodological inventory, we discuss the benefit of multimodal grounding for a variety of language processing tasks and the challenges that arise. We particularly focus on multimodal grounding of verbs which play a crucial role for the compositional power of language.",
}

@article{liu2021discretevalued,
  title   = {Discrete-Valued Neural Communication},
  author  = {Dianbo Liu and Alex Lamb and Kenji Kawaguchi and Anirudh Goyal and Chen Sun and Michael Curtis Mozer and Yoshua Bengio},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2107.02367}
}

@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}

@inproceedings{zhao-etal-2019-improving,
    title = "Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data",
    author = "Zhao, Wei  and
      Wang, Liang  and
      Shen, Kewei  and
      Jia, Ruoyu  and
      Liu, Jingming",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1014",
    doi = "10.18653/v1/N19-1014",
    pages = "156--165",
    abstract = "Neural machine translation systems have become state-of-the-art approaches for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence. Since the GEC suffers from not having enough labeled training data to achieve high accuracy. We pre-train the copy-augmented architecture with a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons between the fully pre-trained model and a partially pre-trained model. It is the first time copying words from the source context and fully pre-training a sequence to sequence model are experimented on the GEC task. Moreover, We add token-level and sentence-level multi-task learning for the GEC task. The evaluation results on the CoNLL-2014 test set show that our approach outperforms all recently published state-of-the-art results by a large margin.",
}

@article{10.1162/tacl_a_00324,
    author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
    title = "{How Can We Know What Language Models Know?}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {423-438},
    year = {2020},
    month = {07},
    abstract = "{Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a \_\_ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a \_\_ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00324},
    url = {https://doi.org/10.1162/tacl\_a\_00324},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00324/1923867/tacl\_a\_00324.pdf},
}

@inproceedings{wallace-etal-2019-universal,
    title = "Universal Adversarial Triggers for Attacking and Analyzing {NLP}",
    author = "Wallace, Eric  and
      Feng, Shi  and
      Kandpal, Nikhil  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1221",
    doi = "10.18653/v1/D19-1221",
    pages = "2153--2162",
    abstract = "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94{\%} to 0.55{\%}, 72{\%} of {``}why{''} questions in SQuAD to be answered {``}to kill american people{''}, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",
}

@article{self-verification,
  title   = {Large Language Models are reasoners with Self-Verification},
  author  = {Yixuan Weng and Minjun Zhu and Shizhu He and Kang Liu and Jun Zhao},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.09561}
}

@article{ptuning,
  title   = {GPT Understands, Too},
  author  = {Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2103.10385}
}

@inproceedings{liu-etal-2022-generated,
    title = "Generated Knowledge Prompting for Commonsense Reasoning",
    author = "Liu, Jiacheng  and
      Liu, Alisa  and
      Lu, Ximing  and
      Welleck, Sean  and
      West, Peter  and
      Le Bras, Ronan  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.225",
    doi = "10.18653/v1/2022.acl-long.225",
    pages = "3154--3169",
    abstract = "It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning.Our code is available at \url{github.com/liujch1998/GKP}",
}

@inproceedings{autoprompt,
  author    = {Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh},
  editor    = {Bonnie Webber and Trevor Cohn and Yulan He and Yang Liu},
  title     = {AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {4222-4235},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.346},
  doi       = {10.18653/v1/2020.emnlp-main.346},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/ShinRLWS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Panthaplackel_Allamanis_Brockschmidt_2021
    , title={Copy That! Editing Sequences by Copying Spans}
    , volume={35}
    , url={https://ojs.aaai.org/index.php/AAAI/article/view/17606}
    , number={15}
    , journal={Proceedings of the AAAI Conference on Artificial Intelligence}
    , author={Panthaplackel, Sheena and Allamanis, Miltiadis and Brockschmidt, Marc}
    , year={2021}
    , month={May}
    , pages={13622-13630}
    }

@inproceedings{li-etal-2021-dynamic,
    title = "Dynamic Knowledge Distillation for Pre-trained Language Models",
    author = "Li, Lei  and
      Lin, Yankai  and
      Ren, Shuhuai  and
      Li, Peng  and
      Zhou, Jie  and
      Sun, Xu",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.31",
    doi = "10.18653/v1/2021.emnlp-main.31",
    pages = "379--389",
    abstract = "Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10{\%} informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.",
}

@inproceedings{zhang2019ernie,
    title = "{ERNIE}: Enhanced Language Representation with Informative Entities",
    author = "Zhang, Zhengyan  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Jiang, Xin  and
      Sun, Maosong  and
      Liu, Qun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1139",
    doi = "10.18653/v1/P19-1139",
    pages = "1441--1451",
    abstract = "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
}

@inproceedings{compute-compete,
 author = {Srivastava, Rupesh K and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Compete to Compute},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},
 volume = {26},
 year = {2013}
}


@inproceedings{berard-etal-2017-lig,
    title = "{LIG}-{CRIS}t{AL} Submission for the {WMT} 2017 Automatic Post-Editing Task",
    author = "B{\'e}rard, Alexandre  and
      Besacier, Laurent  and
      Pietquin, Olivier",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4772",
    doi = "10.18653/v1/W17-4772",
    pages = "623--629",
}

@article{relational-memory,
  title   = {Relational Memory Augmented Language Models},
  author  = {Qi Liu and Dani Yogatama and Phil Blunsom},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2201.09680}
}

@article{drqa,
  title   = {Reading Wikipedia to Answer Open-Domain Questions},
  author  = {Danqi Chen and Adam Fisch and Jason Weston and Antoine Bordes},
  year    = {2017},
  journal = {arXiv preprint arXiv: Arxiv-1704.00051}
}

@article{bm25,
author = {Robertson, Stephen and Zaragoza, Hugo},
title = {The Probabilistic Relevance Framework: BM25 and Beyond},
year = {2009},
issue_date = {April 2009},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {3},
number = {4},
issn = {1554-0669},
url = {https://doi.org/10.1561/1500000019},
doi = {10.1561/1500000019},
abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
journal = {Found. Trends Inf. Retr.},
month = {apr},
pages = {333–389},
numpages = {57}
}

@article{weir2022oneshot,
  title   = {One-Shot Learning from a Demonstration with Hierarchical Latent Language},
  author  = {Nathaniel Weir and Xingdi Yuan and Marc-Alexandre Côté and Matthew Hausknecht and Romain Laroche and Ida Momennejad and Harm Van Seijen and Benjamin Van Durme},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2203.04806}
}

@inproceedings{xu-etal-2022-bilingual,
    title = "Bilingual Synchronization: Restoring Translational Relationships with Editing Operations",
    author = "Xu, Jitao  and
      Crego, Josep  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.548",
    pages = "8016--8030",
    abstract = "Machine Translation (MT) is usually viewed as a one-shot process that generates the target language equivalent of some source text from scratch. We consider here a more general setting which assumes an initial target sequence, that must be transformed into a valid translation of the source, thereby restoring parallelism between source and target. For this bilingual synchronization task, we consider several architectures (both autoregressive and non-autoregressive) and training regimes, and experiment with multiple practical settings such as simulated interactive MT, translating with Translation Memory (TM) and TM cleaning. Our results suggest that one single generic edit-based system, once fine-tuned, can compare with, or even outperform, dedicated systems specifically trained for these tasks.",
}

@inproceedings{kasner-dusek-2020-data,
    title = "Data-to-Text Generation with Iterative Text Editing",
    author = "Kasner, Zden{\v{e}}k  and
      Du{\v{s}}ek, Ond{\v{r}}ej",
    booktitle = "Proceedings of the 13th International Conference on Natural Language Generation",
    month = dec,
    year = "2020",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.inlg-1.9",
    pages = "60--67",
    abstract = "We present a novel approach to data-to-text generation based on iterative text editing. Our approach maximizes the completeness and semantic accuracy of the output text while leveraging the abilities of recent pre-trained models for text editing (LaserTagger) and language modeling (GPT-2) to improve the text fluency. To this end, we first transform data items to text using trivial templates, and then we iteratively improve the resulting text by a neural model trained for the sentence fusion task. The output of the model is filtered by a simple heuristic and reranked with an off-the-shelf pre-trained language model. We evaluate our approach on two major data-to-text datasets (WebNLG, Cleaned E2E) and analyze its caveats and benefits. Furthermore, we show that our formulation of data-to-text generation opens up the possibility for zero-shot domain adaptation using a general-domain dataset for sentence fusion.",
}

@article{nlp-from-scratch,
  title   = {NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework},
  author  = {Xingcheng Yao and Yanan Zheng and Xiaocong Yang and Zhilin Yang},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2111.04130}
}

@inproceedings{omelianchuk-etal-2020-gector,
    title = "{GECT}o{R} {--} Grammatical Error Correction: Tag, Not Rewrite",
    author = "Omelianchuk, Kostiantyn  and
      Atrasevych, Vitaliy  and
      Chernodub, Artem  and
      Skurzhanskyi, Oleksandr",
    booktitle = "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications",
    month = jul,
    year = "2020",
    address = "Seattle, WA, USA → Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.bea-1.16",
    doi = "10.18653/v1/2020.bea-1.16",
    pages = "163--170",
    abstract = "In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an F{\_}0.5 of 65.3/66.5 on CONLL-2014 (test) and F{\_}0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.",
}



@inproceedings{hokamp-liu-2017-lexically,
    title = "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search",
    author = "Hokamp, Chris  and
      Liu, Qun",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1141",
    doi = "10.18653/v1/P17-1141",
    pages = "1535--1546",
    abstract = "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model{'}s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.",
}

@inproceedings{anderson-etal-2017-guided,
    title = "Guided Open Vocabulary Image Captioning with Constrained Beam Search",
    author = "Anderson, Peter  and
      Fernando, Basura  and
      Johnson, Mark  and
      Gould, Stephen",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1098",
    doi = "10.18653/v1/D17-1098",
    pages = "936--945",
    abstract = "Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.",
}

@inproceedings{post-vilar-2018-fast,
    title = "Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation",
    author = "Post, Matt  and
      Vilar, David",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1119",
    doi = "10.18653/v1/N18-1119",
    pages = "1314--1324",
    abstract = "The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm{'}s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.",
}

@article{wu2024mmra,
  title={MMRA: A Benchmark for Multi-granularity Multi-image Relational Association},
  author={Wu, Siwei and Zhu, Kang and Bai, Yu and Liang, Yiming and Li, Yizhi and Wu, Haoning and Liu, Jiaheng and Liu, Ruibo and Qu, Xingwei and Cheng, Xuxin and others},
  journal={arXiv preprint arXiv:2407.17379},
  year={2024}
}

@article{Zhang2024CMMMUAC,
  title={CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark},
  author={Ge Zhang and Xinrun Du and Bei Chen and Yiming Liang and Tongxu Luo and Tianyu Zheng and Kang Zhu and Yuyang Cheng and Chunpu Xu and Shuyue Guo and Haoran Zhang and Xingwei Qu and Junjie Wang and Ruibin Yuan and Yizhi Li and Zekun Wang and Yudong Liu and Yu-Hsuan Tsai and Fengji Zhang and Chenghua Lin and Wenhao Huang and Wenhu Chen and Jie Fu},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.11944},
  url={https://api.semanticscholar.org/CorpusID:267068665}
}

@inproceedings{hinson-etal-2020-heterogeneous,
    title = "Heterogeneous Recycle Generation for {C}hinese Grammatical Error Correction",
    author = "Hinson, Charles  and
      Huang, Hen-Hsen  and
      Chen, Hsin-Hsi",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.199",
    doi = "10.18653/v1/2020.coling-main.199",
    pages = "2191--2201"
}

@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}

@article{chen2024we,
    title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
    author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
    journal={arXiv preprint arXiv:2403.20330},
    year={2024}
}

@article{liu2023mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}
@article{wu2024scimmir,
  title={SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval},
  author={Wu, Siwei and Li, Yizhi and Zhu, Kang and Zhang, Ge and Liang, Yiming and Ma, Kaijing and Xiao, Chenghao and Zhang, Haoran and Yang, Bohao and Chen, Wenhu and others},
  journal={arXiv preprint arXiv:2401.13478},
  year={2024}
}


@misc{he2024chinesesimpleqachinesefactuality,
      title={Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models}, 
      author={Yancheng He and Shilong Li and Jiaheng Liu and Yingshui Tan and Weixun Wang and Hui Huang and Xingyuan Bu and Hangyu Guo and Chengwei Hu and Boren Zheng and Zhuoran Lin and Xuepeng Liu and Dekai Sun and Shirong Lin and Zhicheng Zheng and Xiaoyong Zhu and Wenbo Su and Bo Zheng},
      year={2024},
      eprint={2411.07140},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.07140}, 
}

@inproceedings{che2016punctuation,
  title={Punctuation prediction for unsegmented transcript based on word vector},
  author={Che, Xiaoyin and Wang, Cheng and Yang, Haojin and Meinel, Christoph},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={654--658},
  year={2016}
}

@inproceedings{kim2019deep,
  title={Deep Recurrent Neural Networks with Layer-wise Multi-head Attentions for Punctuation Restoration},
  author={Kim, Seokhwan},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7280--7284},
  year={2019},
  organization={IEEE}
}

@inproceedings{alam-etal-2020-punctuation,
    title = "Punctuation Restoration using Transformer Models for High-and Low-Resource Languages",
    author = "Alam, Tanvirul  and
      Khan, Akib  and
      Alam, Firoj",
    booktitle = "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wnut-1.18",
    doi = "10.18653/v1/2020.wnut-1.18",
    pages = "132--142",
    abstract = "Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this problem using different deep learning models. Recently, transformer models have proven their success in downstream NLP tasks, and these models have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For English, we obtain comparable state-of-the-art results, while for Bangla, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.",
}

@inproceedings{shi21_interspeech,
  author={Ning Shi and Wei Wang and Boxin Wang and Jinfeng Li and Xiangyu Liu and Zhouhan Lin},
  title={{Incorporating External POS Tagger for Punctuation Restoration}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={1987--1991},
  doi={10.21437/Interspeech.2021-1708}
}

@inproceedings{dong-etal-2019-editnts,
    title = "{E}dit{NTS}: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing",
    author = "Dong, Yue  and
      Li, Zichao  and
      Rezagholizadeh, Mehdi  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "3393--3402",
}

@inproceedings{agrawal-etal-2021-non,
    title = "A Non-Autoregressive Edit-Based Approach to Controllable Text Simplification",
    author = "Agrawal, Sweta  and
      Xu, Weijia  and
      Carpuat, Marine",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.330",
    doi = "10.18653/v1/2021.findings-acl.330",
    pages = "3757--3769",
}

@article{shi2023replug,
  title={REPLUG: Retrieval-Augmented Black-Box Language Models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@inproceedings{reid-zhong-2021-lewis,
    title = "{LEWIS}: {L}evenshtein Editing for Unsupervised Text Style Transfer",
    author = "Reid, Machel  and
      Zhong, Victor",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.344",
    doi = "10.18653/v1/2021.findings-acl.344",
    pages = "3932--3944",
}

@inproceedings{malmi-etal-2019-encode,
    title = "Encode, Tag, Realize: High-Precision Text Editing",
    author = "Malmi, Eric  and
      Krause, Sebastian  and
      Rothe, Sascha  and
      Mirylenka, Daniil  and
      Severyn, Aliaksei",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    pages = "5054--5065",
}

@article{creswell2022selectioninference,
  title   = {Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  author  = {Antonia Creswell and Murray Shanahan and Irina Higgins},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2205.09712}
}

@article{he2022rethinking,
  title   = {Rethinking with Retrieval: Faithful Large Language Model Inference},
  author  = {Hangfeng He and Hongming Zhang and Dan Roth},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2301.00303}
}

@inproceedings{stahlberg-kumar-2020-seq2edits,
    title = "{S}eq2{E}dits: Sequence Transduction Using Span-level Edit Operations",
    author = "Stahlberg, Felix  and
      Kumar, Shankar",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.418",
    doi = "10.18653/v1/2020.emnlp-main.418",
    pages = "5147--5159",
    abstract = "We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting {\&} rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.",
}



@inproceedings{ite-prompting,
  author    = {Boshi Wang and Xiang Deng and Huan Sun},
  editor    = {Yoav Goldberg and Zornitsa Kozareva and Yue Zhang},
  title     = {Iteratively Prompt Pre-trained Language Models for Chain of Thought},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},
  pages     = {2714-2730},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.174},
  timestamp = {Tue, 07 Feb 2023 17:10:51 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/Wang0S22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dasgupta2023collaborating,
  title   = {Collaborating with language models for embodied reasoning},
  author  = {Ishita Dasgupta and Christine Kaeser-Chen and Kenneth Marino and Arun Ahuja and Sheila Babayan and Felix Hill and Rob Fergus},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.00763}
}

@book{social-learning-theory,
	title = {Social learning theory},
	url = {https://books.google.co.jp/books?id=mjpbjgEACAAJ},
	publisher = {Prentice-Hall},
	author = {Bandura, A.},
	date = {1977},
	keywords = {},
}

@article{ndousse2020emergent,
  title   = {Emergent Social Learning via Multi-agent Reinforcement Learning},
  author  = {Kamal Ndousse and Douglas Eck and Sergey Levine and Natasha Jaques},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2010.00581}
}

@article{li2022explanations,
  title   = {Explanations from Large Language Models Make Small Reasoners Better},
  author  = {Shiyang Li and Jianshu Chen and Yelong Shen and Zhiyu Chen and Xinlu Zhang and Zekun Li and Hong Wang and Jing Qian and Baolin Peng and Yi Mao and Wenhu Chen and Xifeng Yan},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.06726}
}

@article{wikitext,
  title     = {Pointer Sentinel Mixture Models},
  author    = {Stephen Merity and Caiming Xiong and James Bradbury and R. Socher},
  journal   = {International Conference On Learning Representations},
  year      = {2016},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd}
}

@inproceedings{ge-etal-2018-fluency,
    title = "Fluency Boost Learning and Inference for Neural Grammatical Error Correction",
    author = "Ge, Tao  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1097",
    doi = "10.18653/v1/P18-1097",
    pages = "1055--1065",
    abstract = "Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence{'}s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence{'}s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.",
}

@incollection{levenshtein-transformer, 
    title = {Levenshtein Transformer}, 
    author = {Gu, Jiatao and Wang, Changhan and Zhao, Junbo}, 
    booktitle = {Advances in Neural Information Processing Systems 32}, 
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}, 
    pages = {11181--11191}, 
    year = {2019}, 
    publisher = {Curran Associates, Inc.},
}

@InProceedings{pmlr-v97-stern19a,
  title = 	 {Insertion Transformer: Flexible Sequence Generation via Insertion Operations},
  author =       {Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5976--5985},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/stern19a/stern19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/stern19a.html},
  abstract = 	 {We present the Insertion Transformer, an iterative, partially autoregressive model for sequence generation based on insertion operations. Unlike typical autoregressive models which rely on a fixed, often left-to-right ordering of the output, our approach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This flexibility confers a number of advantages: for instance, not only can our model be trained to follow specific orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entropy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and partially autoregressive generation (simultaneous insertions at multiple locations). We validate our approach by analyzing its performance on the WMT 2014 English-German machine translation task under various settings for training and decoding. We find that the Insertion Transformer outperforms many prior non-autoregressive approaches to translation at comparable or better levels of parallelism, and successfully recovers the performance of the original Transformer while requiring only logarithmically many iterations during decoding.}
}

@inproceedings{kumar-etal-2020-iterative,
    title = "Iterative Edit-Based Unsupervised Sentence Simplification",
    author = "Kumar, Dhruv  and
      Mou, Lili  and
      Golab, Lukasz  and
      Vechtomova, Olga",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.707",
    doi = "10.18653/v1/2020.acl-main.707",
    pages = "7918--7928",
    abstract = "We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.",
}

@misc{lin2022fewshot,
      title={Few-shot Learning with Multilingual Language Models}, 
      author={Xi Victoria Lin and Todor Mihaylov and Mikel Artetxe and Tianlu Wang and Shuohui Chen and Daniel Simig and Myle Ott and Naman Goyal and Shruti Bhosale and Jingfei Du and Ramakanth Pasunuru and Sam Shleifer and Punit Singh Koura and Vishrav Chaudhary and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Zornitsa Kozareva and Mona Diab and Veselin Stoyanov and Xian Li},
      year={2022},
      eprint={2112.10668},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{carlini2023extracting,
  title={Extracting training data from diffusion models},
  author={Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramer, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
  journal={arXiv preprint arXiv:2301.13188},
  year={2023}
}

@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models.},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security Symposium},
  volume={6},
  year={2021}
}

@article{time-aware,
    author = {Dhingra, Bhuwan and Cole, Jeremy R. and Eisenschlos, Julian Martin and Gillick, Daniel and Eisenstein, Jacob and Cohen, William W.},
    title = "{Time-Aware Language Models as Temporal Knowledge Bases}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {257-273},
    year = {2022},
    month = {03},
    abstract = "{Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum—those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently “refreshed” as new data arrives, without the need for retraining from scratch.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00459},
    url = {https://doi.org/10.1162/tacl\_a\_00459},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00459/2004543/tacl\_a\_00459.pdf},
}


@article{li2023privacy,
  title={Privacy-Preserving Prompt Tuning for Large Language Model Services},
  author={Li, Yansong and Tan, Zhixing and Liu, Yang},
  journal={arXiv preprint arXiv:2305.06212},
  year={2023}
}

@article{shi2022just,
  title={Just fine-tune twice: Selective differential privacy for large language models},
  author={Shi, Weiyan and Chen, Si and Zhang, Chiyuan and Jia, Ruoxi and Yu, Zhou},
  journal={arXiv preprint arXiv:2204.07667},
  year={2022}
}


@misc{maynez2020faithfulness,
      title={On Faithfulness and Factuality in Abstractive Summarization}, 
      author={Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan McDonald},
      year={2020},
      eprint={2005.00661},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{patel-etal-2021-nlp,
    title = "Are {NLP} Models really able to Solve Simple Math Word Problems?",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Goyal, Navin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.168",
    doi = "10.18653/v1/2021.naacl-main.168",
    pages = "2080--2094",
    abstract = "The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered {``}solved{''} with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.",
}

@article{resnet,
  title     = {Deep Residual Learning for Image Recognition},
  author    = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal   = {Computer Vision And Pattern Recognition},
  year      = {2015},
  doi       = {10.1109/cvpr.2016.90},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d}
}

@article{jiang2022vima,
  title   = {VIMA: General Robot Manipulation with Multimodal Prompts},
  author  = {Yunfan Jiang and Agrim Gupta and Zichen Zhang and Guanzhi Wang and Yongqiang Dou and Yanjun Chen and Li Fei-Fei and Anima Anandkumar and Yuke Zhu and Linxi Fan},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.03094}
}

@article{transformer,
  title     = {Attention Is All You Need},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal   = {NIPS},
  year      = {2017},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776}
}

@inproceedings{komeili-etal-2022-internet,
    title = "{I}nternet-Augmented Dialogue Generation",
    author = "Komeili, Mojtaba  and
      Shuster, Kurt  and
      Weston, Jason",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.579",
    doi = "10.18653/v1/2022.acl-long.579",
    pages = "8460--8478",
    abstract = "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
}

@article{pfeiffer2023modular,
  title   = {Modular Deep Learning},
  author  = {Jonas Pfeiffer and Sebastian Ruder and Ivan Vulić and Edoardo Maria Ponti},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.11529}
}

@article{shen2023hugginggpt,
  title   = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},
  author  = {Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.17580}
}

@misc{hallucination,
      title={Neural Text Generation with Unlikelihood Training}, 
      author={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
      year={2019},
      eprint={1908.04319},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{shi-etal-2020-recurrent,
    title = "Recurrent Inference in Text Editing",
    author = "Shi, Ning  and
      Zeng, Ziheng  and
      Zhang, Haotian  and
      Gong, Yichen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.159",
    doi = "10.18653/v1/2020.findings-emnlp.159",
    pages = "1758--1769",
    abstract = "In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation Simplification (AES), Arithmetic Equation Correction (AEC). Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.",
}


@article{schick2022peer,
  title   = {PEER: A Collaborative Language Model},
  author  = {Timo Schick and Jane Dwivedi-Yu and Zhengbao Jiang and Fabio Petroni and Patrick Lewis and Gautier Izacard and Qingfei You and Christoforos Nalmpantis and Edouard Grave and Sebastian Riedel},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2208.11663}
}

@inproceedings{shi-etal-2022-text,
    title = "Text Editing as Imitation Game",
    author = "Shi, Ning  and
      Tang, Bin  and
      Yuan, Bo  and
      Huang, Longtao  and
      Pu, Yewen  and
      Fu, Jie  and
      Lin, Zhouhan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.114",
    pages = "1583--1594",
    abstract = "Text editing, such as grammatical error correction, arises naturally from imperfect textual data. Recent works frame text editing as a multi-round sequence tagging task, where operations {--} such as insertion and substitution {--} are represented as a sequence of tags. While achieving good results, this encoding is limited in flexibility as all actions are bound to token-level tags. In this work, we reformulate text editing as an imitation game using behavioral cloning. Specifically, we convert conventional sequence-to-sequence data into state-to-action demonstrations, where the action space can be as flexible as needed. Instead of generating the actions one at a time, we introduce a dual decoders structure to parallel the decoding while retaining the dependencies between action tokens, coupled with trajectory augmentation to alleviate the distribution shift that imitation learning often suffers. In experiments on a suite of Arithmetic Equation benchmarks, our model consistently outperforms the autoregressive baselines in terms of performance, efficiency, and robustness. We hope our findings will shed light on future studies in reinforcement learning applying sequence-level action generation to natural language processing.",
}

@article{cheng2023batch,
  title   = {Batch Prompting: Efficient Inference with Large Language Model APIs},
  author  = {Zhoujun Cheng and Jungo Kasai and Tao Yu},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2301.08721}
}

@article{ram2023incontext,
  title   = {In-Context Retrieval-Augmented Language Models},
  author  = {Ori Ram and Yoav Levine and Itay Dalmedigos and Dor Muhlgay and Amnon Shashua and Kevin Leyton-Brown and Yoav Shoham},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.00083}
}


% arxiv
@article{faltings2023interactive,
  title={Interactive Text Generation},
  author={Faltings, Felix and Galley, Michel and Peng, Baolin and Brantley, Kiant{\'e} and Cai, Weixin and Zhang, Yizhe and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:2303.00908},
  year={2023}
}

@inproceedings{narayan-gardent-2014-hybrid,
    title = "Hybrid Simplification using Deep Semantics and Machine Translation",
    author = "Narayan, Shashi  and
      Gardent, Claire",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1041",
    doi = "10.3115/v1/P14-1041",
    pages = "435--445",
}

@inproceedings{malmi-etal-2020-unsupervised,
    title = "Unsupervised Text Style Transfer with Padded Masked Language Models",
    author = "Malmi, Eric  and
      Severyn, Aliaksei  and
      Rothe, Sascha",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.699",
    doi = "10.18653/v1/2020.emnlp-main.699",
    pages = "8671--8680",
    abstract = "We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source{--}target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that Masker performs competitively in a fully unsupervised setting. Moreover, in low-resource settings, it improves supervised methods{'} accuracy by over 10 percentage points when pre-training them on silver training data generated by Masker.",
}

@inproceedings{mallinson-etal-2022-edit5,
    title = "{E}di{T}5: Semi-Autoregressive Text Editing with T5 Warm-Start",
    author = "Mallinson, Jonathan  and
      Adamek, Jakub  and
      Malmi, Eric  and
      Severyn, Aliaksei",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.156",
    pages = "2126--2138",
    abstract = "We present EdiT5 - a novel semi-autoregressive text-editing approach designed to combine the strengths of non-autoregressive text-editing and autoregressive decoding. EdiT5 is faster at inference times than conventional sequence-to-sequence (seq2seq) models, while being capable of modeling flexible input-output transformations.This is achieved by decomposing the generation process into three sub-tasks: (1) tagging to decide on the subset of input tokens to be preserved in the output, (2) re-ordering to define their order in the output text, and (3) insertion to infill the missing tokens that are not present in the input. The tagging and re-ordering steps, which are responsible for generating the largest portion of the output, are non-autoregressive, while the insertion uses an autoregressive decoder.Depending on the task, EdiT5 requires significantly fewer autoregressive steps demonstrating speedups of up to 25x when compared to classic seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5 checkpoint yielding comparable performance to T5 in high-resource settings and clearly outperforms it on low-resource settings when evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction, and Decontextualization.",
}

@article{vq-vae,
  title     = {Neural Discrete Representation Learning},
  author    = {Aäron van den Oord and Oriol Vinyals and K. Kavukcuoglu},
  journal   = {NIPS},
  year      = {2017},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f466157848d1a7772fb6d02cdac9a7a5e7ef982e}
}

@article{vit-vqgan,
  title     = {Vector-quantized Image Modeling with Improved VQGAN},
  author    = {Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/9c7a2cd13b783bb73ad2d1ec2880bdd9b995cbdc}
}

@article{vqgan,
  title     = {Taming Transformers for High-Resolution Image Synthesis},
  author    = {Patrick Esser and Robin Rombach and B. Ommer},
  journal   = {Computer Vision And Pattern Recognition},
  year      = {2020},
  doi       = {10.1109/CVPR46437.2021.01268},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/47f7ec3d0a5e6e83b6768ece35206a94dc81919c}
}

@article{beit2,
  title   = {BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers},
  author  = {Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2208.06366}
}

@inproceedings{CodeReviewer,
  author    = {Zhiyu Li and Shuai Lu and Daya Guo and Nan Duan and Shailesh Jannu and Grant Jenks and Deep Majumder and Jared Green and Alexey Svyatkovskiy and Shengyu Fu and Neel Sundaresan},
  editor    = {Abhik Roychoudhury and Cristian Cadar and Miryung Kim},
  title     = {Automating code review activities by large-scale pre-training},
  booktitle = {Proceedings of the 30th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, {ESEC/FSE} 2022, Singapore, Singapore, November 14-18, 2022},
  pages     = {1035-1047},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3540250.3549081},
  doi       = {10.1145/3540250.3549081},
  timestamp = {Thu, 10 Nov 2022 12:19:51 +0100},
  biburl    = {https://dblp.org/rec/conf/sigsoft/LiLGDJJMGSFS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% google
@article{pomerleau1988alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@article{lahiri2022interactive,
  title   = {Interactive Code Generation via Test-Driven User-Intent Formalization},
  author  = {Shuvendu K. Lahiri and Aaditya Naik and Georgios Sakkas and Piali Choudhury and Curtis von Veh and Madanlal Musuvathi and Jeevana Priya Inala and Chenglong Wang and Jianfeng Gao},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2208.05950}
}

@article{codex,
  title   = {Evaluating Large Language Models Trained on Code},
  author  = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2107.03374}
}

@article{wu2022autoformalization,
  title   = {Autoformalization with Large Language Models},
  author  = {Yuhuai Wu and Albert Q. Jiang and Wenda Li and Markus N. Rabe and Charles Staats and Mateja Jamnik and Christian Szegedy},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2205.12615}
}

@article{lu2021intergps,
  title     = {Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning},
  author    = {Pan Lu and R. Gong and Shibiao Jiang and Liang Qiu and Siyuan Huang and Xiaodan Liang and Song-Chun Zhu},
  journal   = {Annual Meeting Of The Association For Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2021.acl-long.528},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/fb1c90806fc5ec72987f58110aa255edbce6620d}
}

@article{draft-sketch-prove,
  title   = {Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author  = {Albert Q. Jiang and Sean Welleck and Jin Peng Zhou and Wenda Li and Jiacheng Liu and Mateja Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.12283}
}

@article{Contriever,
  title   = {Unsupervised Dense Information Retrieval with Contrastive Learning},
  author  = {Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2112.09118}
}

@article{10.1162/neco.1991.3.1.88,
    author = {Pomerleau, Dean A.},
    title = "{Efficient Training of Artificial Neural Networks for Autonomous Navigation}",
    journal = {Neural Computation},
    volume = {3},
    number = {1},
    pages = {88-97},
    year = {1991},
    month = {03},
    abstract = "{The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN is a backpropagation network designed to drive the CMU Navlab, a modified Chevy van. This paper describes the training techniques that allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching the reactions of a human driver. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, and multilane lined and unlined roads, at speeds of up to 20 miles per hour.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1991.3.1.88},
    url = {https://doi.org/10.1162/neco.1991.3.1.88},
    eprint = {https://direct.mit.edu/neco/article-pdf/3/1/88/812106/neco.1991.3.1.88.pdf},
}

% google
@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{flan-collection,
  title     = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author    = {S. Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2301.13688},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f2b0017ddd77fa38760a18145e63553105a1a236}
}

@article{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  journal={arXiv preprint arXiv:2304.01373},
  year={2023}
}

@inproceedings{du2022glm,
  title={GLM: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@article{bulatov2022recurrent,
  title={Recurrent memory transformer},
  author={Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={11079--11091},
  year={2022}
}

@article{bulatov2023scaling,
  title={Scaling Transformer to 1M tokens and beyond with RMT},
  author={Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S},
  journal={arXiv preprint arXiv:2304.11062},
  year={2023}
}

@inproceedings{wumemorizing,
  title={Memorizing Transformers},
  author={Wu, Yuhuai and Rabe, Markus Norman and Hutchins, DeLesley and Szegedy, Christian},
  booktitle={International Conference on Learning Representations}
}

@article{flan,
  title     = {Finetuned Language Models Are Zero-Shot Learners},
  author    = {Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and A. Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd}
}



@article{ni2023lever,
  title   = {LEVER: Learning to Verify Language-to-Code Generation with Execution},
  author  = {Ansong Ni and Srini Iyer and Dragomir Radev and Ves Stoyanov and Wen-tau Yih and Sida I. Wang and Xi Victoria Lin},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.08468}
}

% arxiv
@article{bansal2018chauffeurnet,
  title={Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst},
  author={Bansal, Mayank and Krizhevsky, Alex and Ogale, Abhijit},
  journal={arXiv preprint arXiv:1812.03079},
  year={2018}
}

@inproceedings{10.1109/ICRA.2018.8461249,
author = {Zhang, Tianhao and McCarthy, Zoe and Jowl, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
title = {Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICRA.2018.8461249},
doi = {10.1109/ICRA.2018.8461249},
abstract = {Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.},
booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
pages = {1–8},
numpages = {8},
location = {Brisbane, Australia}
}

@techreport{chatgpt4robot,
author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
title = {ChatGPT for Robotics: Design Principles and Model Abilities},
institution = {Microsoft},
year = {2023},
month = {February},
url = {https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/},
number = {MSR-TR-2023-8},
}

@inproceedings{
	RoboImitationPeng20,
	author = {Peng, Xue Bin and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang-Wei Edward and Tan, Jie and Levine, Sergey},
	booktitle={Robotics: Science and Systems},
	year = {2020},
	month = {07},
	title = {Learning Agile Robotic Locomotion Skills by Imitating Animals},
	doi = {10.15607/RSS.2020.XVI.064}
}

@INPROCEEDINGS{Tanwani_M2V_ICRA_20, author={Ajay Kumar Tanwani and Pierre Sermanet and Andy Yan and Raghav Anand and Mariano Phielipp and Ken Goldberg}, booktitle={Proc. of {IEEE} Intl Conf. on Robotics and Automation ({ICRA})}, title={Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos}, year={2020}, pages={1--8} }

@inproceedings{
baker2022video,
title={Video PreTraining ({VPT}): Learning to Act by Watching Unlabeled Online Videos},
author={Bowen Baker and Ilge Akkaya and Peter Zhokov and Joost Huizinga and Jie Tang and Adrien Ecoffet and Brandon Houghton and Raul Sampedro and Jeff Clune},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=AXDNM76T1nc}
}

@InProceedings{pmlr-v164-jang22a,
  title = 	 {BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning},
  author =       {Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea},
  booktitle = 	 {Proceedings of the 5th Conference on Robot Learning},
  pages = 	 {991--1002},
  year = 	 {2022},
  editor = 	 {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  volume = 	 {164},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08--11 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v164/jang22a/jang22a.pdf},
  url = 	 {https://proceedings.mlr.press/v164/jang22a.html},
  abstract = 	 {In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. To that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pre-trained embeddings of natural language or videos of humans performing the task. When scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44\%, without any robot demonstrations for those tasks.}
}

@InProceedings{pmlr-v162-humphreys22a,
  title = 	 {A data-driven approach for learning to control computers},
  author =       {Humphreys, Peter C and Raposo, David and Pohlen, Tobias and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Santoro, Adam and Lillicrap, Timothy},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9466--9482},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/humphreys22a/humphreys22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/humphreys22a.html},
  abstract = 	 {It would be useful for machines to use computers as humans do so that they can aid us in everyday tasks. This is a setting in which there is also the potential to leverage large-scale expert demonstrations and human judgements of interactive behaviour, which are two ingredients that have driven much recent success in AI. Here we investigate the setting of computer control using keyboard and mouse, with goals specified via natural language. Instead of focusing on hand-designed curricula and specialized action spaces, we focus on developing a scalable method centered on reinforcement learning combined with behavioural priors informed by actual human-computer interactions. We achieve state-of-the-art and human-level mean performance across all tasks within the MiniWob++ benchmark, a challenging suite of computer control problems, and find strong evidence of cross-task transfer. These results demonstrate the usefulness of a unified human-agent interface when training machines to use computers. Altogether our results suggest a formula for achieving competency beyond MiniWob++ and towards controlling computers, in general, as a human would.}
}

@article{towards-reasoning,
  title     = {Towards Reasoning in Large Language Models: A Survey},
  author    = {Jie Huang and K. Chang},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2212.10403},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/3ee9c65366efbb17adf370c39f20dbef60d53670}
}

@inproceedings{du-ji-2019-empirical,
    title = "An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation",
    author = "Du, Wanyu  and
      Ji, Yangfeng",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1619",
    doi = "10.18653/v1/D19-1619",
    pages = "6012--6018",
    abstract = "Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a decoder, supervised learning which maximizes the likelihood of tokens always suffers from the exposure bias. Although both reinforcement learning (RL) and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how RL and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model. Experiments on the benchmark datasets show that (1) imitation learning is constantly better than reinforcement learning; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.",
}

@article{gu2022robustness,
  title   = {Robustness of Learning from Task Instructions},
  author  = {Jiasheng Gu and Hanzi Xu and Liangyu Nie and Wenpeng Yin},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.03813}
}

@article{chen2022relation,
  title   = {On the Relation between Sensitivity and Accuracy in In-context Learning},
  author  = {Yanda Chen and Chen Zhao and Zhou Yu and Kathleen McKeown and He He},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2209.07661}
}

@article{drissi2018hierarchical,
  title   = {Hierarchical Text Generation using an Outline},
  author  = {Mehdi Drissi and Olivia Watkins and Jugal Kalita},
  year    = {2018},
  journal = {arXiv preprint arXiv: Arxiv-1810.08802}
}

@article{sun2020summarize,
  title     = {Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries},
  author    = {Xiaofei Sun and Chun Fan and Zijun Sun and Yuxian Meng and Fei Wu and Jiwei Li},
  journal   = {International Conference On Computational Linguistics},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/94846f838853539e2ddf9440c09342cdb5b7a2c9}
}

@inproceedings{zhong-etal-2022-proqa,
    title = "{P}ro{QA}: Structural Prompt-based Pre-training for Unified Question Answering",
    author = "Zhong, Wanjun  and
      Gao, Yifan  and
      Ding, Ning  and
      Qin, Yujia  and
      Liu, Zhiyuan  and
      Zhou, Ming  and
      Wang, Jiahai  and
      Yin, Jian  and
      Duan, Nan",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.313",
    doi = "10.18653/v1/2022.naacl-main.313",
    pages = "4230--4243",
    abstract = "Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the QA-centric ability by structural prompt-based pre-training. Through a structurally designed prompt-based input schema, ProQA concurrently models the knowledge generalization for all QA tasks while keeping the knowledge customization for every specific QA task. Furthermore, ProQA is pre-trained with structural prompt-formatted large-scale synthesized corpus, which empowers the model with the commonly-required QA ability. Experimental results on 11 QA benchmarks demonstrate that ProQA consistently boosts performance on both full data fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore, ProQA exhibits strong ability in both continual learning and transfer learning by taking the advantages of the structural prompt.",
}

@article{jang2021towards,
  title     = {Towards Continual Knowledge Learning of Language Models},
  author    = {Joel Jang and Seonghyeon Ye and Sohee Yang and Joongbo Shin and Janghoon Han and Gyeonghun Kim and Stanley Jungkyu Choi and Minjoon Seo},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ce828f9986b196308a3e40b1de58af1e8e68d728}
}

@article{lu2021kelm,
  title   = {KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs},
  author  = {Yinquan Lu and Haonan Lu and Guirong Fu and Qun Liu},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2109.04223}
}

@inproceedings{liu-etal-2022-makes,
    title = "What Makes Good In-Context Examples for {GPT}-3?",
    author = "Liu, Jiachang  and
      Shen, Dinghan  and
      Zhang, Yizhe  and
      Dolan, Bill  and
      Carin, Lawrence  and
      Chen, Weizhu",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deelio-1.10",
    doi = "10.18653/v1/2022.deelio-1.10",
    pages = "100--114",
    abstract = "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3{'}s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3{'}s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3{\%} on the ToTTo dataset) and open-domain question answering (45.5{\%} on the NQ dataset).",
}

@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
    abstract = "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {``}fantastic{''} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks.",
}

@InProceedings{calibrate-before-use,
  title = 	 {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  author =       {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12697--12706},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhao21c.html},
  abstract = 	 {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model’s bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2’s accuracy (up to 30.0\% absolute) across different choices of the prompt, while also making learning considerably more stable.}
}


@inproceedings{chen-etal-2021-multi,
    title = "Multi-granularity Textual Adversarial Attack with Behavior Cloning",
    author = "Chen, Yangyi  and
      Su, Jin  and
      Wei, Wei",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.371",
    doi = "10.18653/v1/2021.emnlp-main.371",
    pages = "4511--4526",
    abstract = "Recently, the textual adversarial attack models become increasingly popular due to their successful in estimating the robustness of NLP models. However, existing works have obvious deficiencies. (1)They usually consider only a single granularity of modification strategies (e.g. word-level or sentence-level), which is insufficient to explore the holistic textual space for generation; (2) They need to query victim models hundreds of times to make a successful attack, which is highly inefficient in practice. To address such problems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to effectively generate high-quality adversarial samples with fewer queries to victim models. Furthermore, we propose a reinforcement-learning based method to train a multi-granularity attack agent through behavior cloning with the expert knowledge from our MAYA algorithm to further reduce the query times. Additionally, we also adapt the agent to attack black-box models that only output labels without confidence scores. We conduct comprehensive experiments to evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two different black-box attack settings and three benchmark datasets. Experimental results show that our models achieve overall better attacking performance and produce more fluent and grammatical adversarial samples compared to baseline models. Besides, our adversarial attack agent significantly reduces the query times in both attack settings. Our codes are released at https://github.com/Yangyi-Chen/MAYA.",
}

@article{kbert,
  title     = {K-BERT: Enabling Language Representation with Knowledge Graph},
  author    = {Weijie Liu and Peng Zhou and Zhe Zhao and Zhiruo Wang and Qi Ju and Haotang Deng and Ping Wang},
  journal   = {Aaai Conference On Artificial Intelligence},
  year      = {2019},
  doi       = {10.1609/AAAI.V34I03.5681},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/06a73ad09664435f8b3cd90293f4e05a047cf375}
}

@article{ye2022ontologyenhanced,
  title   = {Ontology-enhanced Prompt-tuning for Few-shot Learning},
  author  = {Hongbin Ye and Ningyu Zhang and Shumin Deng and Xiang Chen and Hui Chen and Feiyu Xiong and Xi Chen and Huajun Chen},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2201.11332}
}

% openreview
@inproceedings{
pang2021text,
title={Text Generation by Learning from Demonstrations},
author={Richard Yuanzhe Pang and He He},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=RovX-uQ1Hua}
}

@inproceedings{agrawal-carpuat-2022-imitation,
    title = "An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models",
    author = "Agrawal, Sweta  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.520",
    doi = "10.18653/v1/2022.acl-long.520",
    pages = "7550--7563",
    abstract = "We propose a framework for training non-autoregressive sequence-to-sequence models for editing tasks, where the original input sequence is iteratively edited to produce the output. We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining and poor generalization in editing scenarios. We address this issue with two complementary strategies: 1) a roll-in policy that exposes the model to intermediate training sequences that it is more likely to encounter during inference, 2) a curriculum that presents easy-to-learn edit operations first, gradually increasing the difficulty of training samples as the model becomes competent. We show the efficacy of these strategies on two challenging English editing tasks: controllable text simplification and abstractive summarization. Our approach significantly improves output quality on both tasks and controls output complexity better on the simplification task.",
}

@inproceedings{arora-etal-2022-exposure,
    title = "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation",
    author = "Arora, Kushal  and
      El Asri, Layla  and
      Bahuleyan, Hareesh  and
      Cheung, Jackie",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.58",
    doi = "10.18653/v1/2022.findings-acl.58",
    pages = "700--710",
    abstract = "Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis for this brittleness of generation models is that it is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality.",
}

% openreview
@inproceedings{
hao2022teacher,
title={Teacher Forcing Recovers Reward Functions for Text Generation},
author={Yongchang Hao and Yuxin Liu and Lili Mou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=1_gypPuWUC3}
}


@InProceedings{pmlr-v162-zhou22n,
  title = 	 {{VLUE}: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training},
  author =       {Zhou, Wangchunshu and Zeng, Yan and Diao, Shizhe and Zhang, Xinsong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27395--27411},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhou22n/zhou22n.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhou22n.html},
}

@inproceedings{li-zhou-2020-connecting,
    title = "Connecting the Dots Between Fact Verification and Fake News Detection",
    author = "Li, Qifei  and
      Zhou, Wangchunshu",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.165",
    doi = "10.18653/v1/2020.coling-main.165",
    pages = "1820--1825",
}

@article{hao2022structured,
  title   = {Structured Prompting: Scaling In-Context Learning to 1,000 Examples},
  author  = {Yaru Hao and Yutao Sun and Li Dong and Zhixiong Han and Yuxian Gu and Furu Wei},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.06713}
}

@inproceedings{zhou-etal-2022-efficiently,
    title = "Efficiently Tuned Parameters Are Task Embeddings",
    author = "Zhou, Wangchunshu  and
      Xu, Canwen  and
      McAuley, Julian",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.334",
    pages = "5007--5014",
}

@inproceedings{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}

@inproceedings{vu-etal-2022-spot,
    title = "{SP}o{T}: Better Frozen Model Adaptation through Soft Prompt Transfer",
    author = "Vu, Tu  and
      Lester, Brian  and
      Constant, Noah  and
      Al-Rfou{'}, Rami  and
      Cer, Daniel",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.346",
    doi = "10.18653/v1/2022.acl-long.346",
    pages = "5039--5059",
}

@inproceedings{DBLP:conf/iclr/ZhouGXW020,
  author    = {Wangchunshu Zhou and
               Tao Ge and
               Ke Xu and
               Furu Wei and
               Ming Zhou},
  title     = {Self-Adversarial Learning with Comparative Discrimination for Text
               Generation},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2020}
}


@article{DBLP:journals/corr/abs-2301-09008,
  author    = {Vil{\'{e}}m Zouhar and
               Shehzaad Dhuliawala and
               Wangchunshu Zhou and
               Nico Daheim and
               Tom Kocmi and
               Yuchen Eleanor Jiang and
               Mrinmaya Sachan},
  title     = {Poor Man's Quality Estimation: Predicting Reference-Based {MT} Metrics
               Without the Reference},
  journal   = {CoRR},
  volume    = {abs/2301.09008},
  year      = {2023}
}

@inproceedings{DBLP:conf/aaai/Zhou020,
  author    = {Wangchunshu Zhou and
               Ke Xu},
  title     = {Learning to Compare for Better Training and Evaluation of Open Domain
               Natural Language Generation Models},
  booktitle = {{AAAI}},
  pages     = {9717--9724},
  publisher = {{AAAI} Press},
  year      = {2020}
}


@inproceedings{bara-etal-2021-mindcraft,
    title = "{M}ind{C}raft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks",
    author = "Bara, Cristian-Paul  and
      CH-Wang, Sky  and
      Chai, Joyce",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.85",
    doi = "10.18653/v1/2021.emnlp-main.85",
    pages = "1112--1125",
    abstract = "An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners{'} beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.",
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@article{ai-chains,
  title     = {AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts},
  author    = {Tongshuang Sherry Wu and Michael Terry and Carrie J. Cai},
  journal   = {International Conference On Human Factors In Computing Systems},
  year      = {2021},
  doi       = {10.1145/3491102.3517582},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/d3640eb3b542eaf36fee2261f037a6bf0d8eac9c}
}


@article{sun2019ernie,
  title={Ernie: Enhanced representation through knowledge integration},
  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
  journal={arXiv preprint arXiv:1904.09223},
  year={2019}
}

@article{liu2019roberta,
  title     = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author    = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal   = {ARXIV.ORG},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de}
}


@article{xu2023baize,
  title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}


@article{augmented-lm,
  title   = {Augmented Language Models: a Survey},
  author  = {Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2302.07842}
}

@article{alignment-deepmind,
  title   = {Alignment of Language Agents},
  author  = {Zachary Kenton and Tom Everitt and Laura Weidinger and Iason Gabriel and Vladimir Mikulik and Geoffrey Irving},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2103.14659}
}

@article{li2016dataset,
  title={Dataset and neural recurrent sequence labeling model for open-domain factoid question answering},
  author={Li, Peng and Li, Wei and He, Zhengyan and Wang, Xuguang and Cao, Ying and Zhou, Jie and Xu, Wei},
  journal={arXiv preprint arXiv:1607.06275},
  year={2016}
}

@article{song2023reward,
  title={Reward collapse in aligning large language models},
  author={Song, Ziang and Cai, Tianle and Lee, Jason D and Su, Weijie J},
  journal={arXiv preprint arXiv:2305.17608},
  year={2023}
}


@article{gpt4,
  title   = {GPT-4 Technical Report},
  author  = {OpenAI},
  year    = {2023},
  journal = {PREPRINT}
}
@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{experience-ground,
  title   = {Experience Grounds Language},
  author  = {Yonatan Bisk and Ari Holtzman and Jesse Thomason and Jacob Andreas and Yoshua Bengio and Joyce Chai and Mirella Lapata and Angeliki Lazaridou and Jonathan May and Aleksandr Nisnevich and Nicolas Pinto and Joseph Turian},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2004.10151}
}

@article{iML,
  title   = {Interactive Machine Learning: A State of the Art Review},
  author  = {Natnael A. Wondimu and Cédric Buche and Ubbo Visser},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2207.06196}
}

@inproceedings{language-remodeling,
  title     = {{L}anguage (Re)modelling: {T}owards Embodied Language Understanding},
  author    = {Tamari, Ronen and Shani, Chen and Hope, Tom and Petruck, Miriam R L and Abend, Omri and Shahaf, Dafna},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = {jul},
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.559},
  doi       = {10.18653/v1/2020.acl-main.559},
  pages     = {6268-6281},
  abstract  = {While natural language understanding (NLU) is advancing rapidly, today{'}s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction. This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.}
}

@article{lake2016building,
  title     = {Building Machines That Learn and Think Like People},
  author    = {B. Lake and T. Ullman and J. Tenenbaum and S. Gershman},
  journal   = {Behavioral And Brain Sciences},
  year      = {2016},
  doi       = {10.1017/S0140525X16001837},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/7260c0692f8d265e11c4e9c4c8ef4c185bd587ad}
}

@article{knowledge-survey,
  title   = {A Survey of Knowledge-Enhanced Pre-trained Language Models},
  author  = {Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2211.05994}
}

@article{retrieval-survey,
  title   = {A Survey on Retrieval-Augmented Text Generation},
  author  = {Huayang Li and Yixuan Su and Deng Cai and Yan Wang and Lemao Liu},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2202.01110}
}


@article{reasoning-survey,
  title   = {Reasoning with Language Model Prompting: A Survey},
  author  = {Shuofei Qiao and Yixin Ou and Ningyu Zhang and Xiang Chen and Yunzhi Yao and Shumin Deng and Chuanqi Tan and Fei Huang and Huajun Chen},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.09597}
}

@article{lm-cascades,
  title   = {Language Model Cascades},
  author  = {David Dohan and Winnie Xu and Aitor Lewkowycz and Jacob Austin and David Bieber and Raphael Gontijo Lopes and Yuhuai Wu and Henryk Michalewski and Rif A. Saurous and Jascha Sohl-dickstein and Kevin Murphy and Charles Sutton},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2207.10342}
}

@article{help-harm,
  title   = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author  = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2204.05862}
}

@article{freedman2019responsive,
  title   = {Responsive Planning and Recognition for Closed-Loop Interaction},
  author  = {Richard G. Freedman and Yi Ren Fung and Roman Ganchin and Shlomo Zilberstein},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1909.06427}
}

@article{zeng2022socratic,
  title   = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  author  = {Andy Zeng and Maria Attarian and Brian Ichter and Krzysztof Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2204.00598}
}

@article{dsi,
  title   = {Transformer Memory as a Differentiable Search Index},
  author  = {Yi Tay and Vinh Q. Tran and Mostafa Dehghani and Jianmo Ni and Dara Bahri and Harsh Mehta and Zhen Qin and Kai Hui and Zhe Zhao and Jai Gupta and Tal Schuster and William W. Cohen and Donald Metzler},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2202.06991}
}

@article{jaccord,
 ISSN = {0028646X, 14698137},
 URL = {http://www.jstor.org/stable/2427226},
 author = {Paul Jaccard},
 journal = {The New Phytologist},
 number = {2},
 pages = {37--50},
 publisher = {[Wiley, New Phytologist Trust]},
 title = {The Distribution of the Flora in the Alpine Zone},
 urldate = {2023-03-22},
 volume = {11},
 year = {1912}
}



@book{fast-slow,
  abstract = {In this work the author, a recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology that challenged the rational model of judgment and decision making, has brought together his many years of research and thinking in one book. He explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. He exposes the extraordinary capabilities, and also the faults and biases, of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. He reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives, and how we can use different techniques to guard against the mental glitches that often get us into trouble. This author's work has transformed cognitive psychology and launched the new fields of behavioral economics and happiness studies. In this book, he takes us on a tour of the mind and explains the two systems that drive the way we think and the way we make choices.},
  added-at = {2013-01-10T15:41:11.000+0100},
  address = {New York},
  author = {Kahneman, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2f322864169411fd5914f3fa5488e288c/schmidt2},
  description = {Thinking, Fast and Slow: Amazon.de: Daniel Kahneman: Englische Bücher},
  interhash = {a1400a299a00de009ec8eda73e6289af},
  intrahash = {f322864169411fd5914f3fa5488e288c},
  isbn = {9780374275631 0374275637},
  keywords = {bib books psychology thinking toread},
  publisher = {Farrar, Straus and Giroux},
  refid = {706020998},
  timestamp = {2013-01-10T15:41:11.000+0100},
  title = {Thinking, fast and slow},
  url = {https://www.amazon.de/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdT1_nS_nC?ie=UTF8&colid=151193SNGKJT9&coliid=I3OCESLZCVDFL7},
  year = 2011
}


@article{scalable-align,
  title   = {Scalable agent alignment via reward modeling: a research direction},
  author  = {Jan Leike and David Krueger and Tom Everitt and Miljan Martic and Vishal Maini and Shane Legg},
  year    = {2018},
  journal = {arXiv preprint arXiv: Arxiv-1811.07871}
}

@inproceedings{closing-the-loop,
    title = "Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances",
    author = "Settles, Burr",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1136",
    pages = "1467--1478",
}

@article{zhou2022docprompting,
  title   = {DocPrompting: Generating Code by Retrieving the Docs},
  author  = {Shuyan Zhou and Uri Alon and Frank F. Xu and Zhiruo Wang and Zhengbao Jiang and Graham Neubig},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2207.05987}
}

@inproceedings{Godbole2004interactive,
author = {Godbole, Shantanu and Harpale, Abhay and Sarawagi, Sunita and Chakrabarti, Soumen},
title = {Document Classification through Interactive Supervision of Document and Term Labels},
year = {2004},
isbn = {3540231080},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Effective incorporation of human expertise, while exerting a low cognitive load, is a critical aspect of real-life text classification applications that is not adequately addressed by batch-supervised high-accuracy learners. Standard text classifiers are supervised in only one way: assigning labels to whole documents. They are thus deprived of the enormous wisdom that humans carry about the significance of words and phrases in context. We present HIClass, an interactive and exploratory labeling package that actively collects user opinion on feature representations and choices, as well as whole-document labels, while minimizing redundancy in the input sought. Preliminary experience suggests that, starting with essentially an unlabeled corpus, very little cognitive labor suffices to set up a labeled collection on which standard classifiers perform well.},
booktitle = {Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases},
pages = {185–196},
numpages = {12},
location = {Pisa, Italy},
series = {PKDD '04}
}

@article{fu2023specializing,
  title   = {Specializing Smaller Language Models towards Multi-Step Reasoning},
  author  = {Yao Fu and Hao Peng and Litu Ou and Ashish Sabharwal and Tushar Khot},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2301.12726}
}

@article{learning-to-summ,
  title   = {Learning to summarize from human feedback},
  author  = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2009.01325}
}

@article{blenderbot3,
  title   = {BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage},
  author  = {Kurt Shuster and Jing Xu and Mojtaba Komeili and Da Ju and Eric Michael Smith and Stephen Roller and Megan Ung and Moya Chen and Kushal Arora and Joshua Lane and Morteza Behrooz and William Ngan and Spencer Poff and Naman Goyal and Arthur Szlam and Y-Lan Boureau and Melanie Kambadur and Jason Weston},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2208.03188}
}


@article{yao2022react,
  title   = {ReAct: Synergizing Reasoning and Acting in Language Models},
  author  = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.03629}
}

@article{trivedi2022interleavingretrieval,
  title   = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
  author  = {Harsh Trivedi and Niranjan Balasubramanian and Tushar Khot and Ashish Sabharwal},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2212.10509}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{wang2023shall,
  title={Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study},
  author={Wang, Boxin and Ping, Wei and Xu, Peng and McAfee, Lawrence and Liu, Zihan and Shoeybi, Mohammad and Dong, Yi and Kuchaiev, Oleksii and Li, Bo and Xiao, Chaowei and others},
  journal={arXiv preprint arXiv:2304.06762},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{suris2023vipergpt,
  title   = {ViperGPT: Visual Inference via Python Execution for Reasoning},
  author  = {Didac Suris and Sachit Menon and Carl Vondrick},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.08128}
}



@article{fu2022complexity,
  title={Complexity-based prompting for multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  journal={arXiv preprint arXiv:2210.00720},
  year={2022}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@article{arora2022ask,
  title   = {Ask Me Anything: A simple strategy for prompting language models},
  author  = {Simran Arora and Avanika Narayan and Mayee F. Chen and Laurel Orr and Neel Guha and Kush Bhatia and Ines Chami and Frederic Sala and Christopher Ré},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.02441}
}

@article{honovich2022instruction,
  title   = {Instruction Induction: From Few Examples to Natural Language Task Descriptions},
  author  = {Or Honovich and Uri Shaham and Samuel R. Bowman and Omer Levy},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2205.10782}
}

@article{wu2022efficient,
  title={An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks},
  author={Wu, Yuxiang and Zhao, Yu and Hu, Baotian and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
  journal={EMNLP},
  year={2022}
}




@article{marasovic2021few,
 author = {Marasovi{\'c}, Ana and Beltagy, Iz and Downey, Doug and Peters, Matthew E},
 journal = {ArXiv preprint},
 title = {Few-Shot Self-Rationalization with Natural Language Prompts},
 url = {https://arxiv.org/abs/2111.08284},
 volume = {abs/2111.08284},
 year = {2021}
}

@inproceedings{
knn-lm,
title={Generalization through Memorization: Nearest Neighbor Language Models},
author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HklBjCEKvH}
}



@article{visual-chatgpt,
  title   = {Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models},
  author  = {Chenfei Wu and Shengming Yin and Weizhen Qi and Xiaodong Wang and Zecheng Tang and Nan Duan},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.04671}
}

@article{zelikman2022star,
  title={Star: Self-taught reasoner bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Mu, Jesse and Goodman, Noah D and Wu, Yuhuai Tony},
  year={2022}
}

@article{ge2023openagi,
  title={OpenAGI: When LLM Meets Domain Experts},
  author={Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2304.04370},
  year={2023}
}

@article{li2023blip2,
  title   = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author  = {Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2301.12597}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{chen2022large,
  title={Large language models are few (1)-shot table reasoners},
  author={Chen, Wenhu},
  journal={The 17th Conference of the European Chapter of the Association for Computational Linguistics},
  year={2022}
}

@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2211.10435},
  year={2022}
}

@inproceedings{
liang2022transformer,
title={Transformer Adapters for Robot Learning},
author={Anthony Liang and Ishika Singh and Karl Pertsch and Jesse Thomason},
booktitle={CoRL 2022 Workshop on Pre-training Robot Learning},
year={2022},
url={https://openreview.net/forum?id=H--wvRYBmF}
}

@inproceedings{he-etal-2021-effectiveness,
    title = "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation",
    author = "He, Ruidan  and
      Liu, Linlin  and
      Ye, Hai  and
      Tan, Qingyu  and
      Ding, Bosheng  and
      Cheng, Liying  and
      Low, Jiawei  and
      Bing, Lidong  and
      Si, Luo",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.172",
    doi = "10.18653/v1/2021.acl-long.172",
    pages = "2208--2222",
    abstract = "Adapter-based tuning has recently arisen as an alternative to fine-tuning. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning. However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness. In this paper, we study the latter. We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM. We then empirically compare the two tuning methods on several downstream NLP tasks and settings. We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates.",
}

@inproceedings{gururangan-etal-2020-dont,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}

@inproceedings{kadapter,
  author    = {Ruize Wang and Duyu Tang and Nan Duan and Zhongyu Wei and Xuanjing Huang and Jianshu Ji and Guihong Cao and Daxin Jiang and Ming Zhou},
  editor    = {Chengqing Zong and Fei Xia and Wenjie Li and Roberto Navigli},
  title     = {K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters},
  booktitle = {Findings of the Association for Computational Linguistics: {ACL/IJCNLP} 2021, Online Event, August 1-6, 2021},
  series    = {Findings of {ACL}},
  volume    = {{ACL/IJCNLP} 2021},
  pages     = {1405-1418},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.findings-acl.121},
  doi       = {10.18653/v1/2021.findings-acl.121},
  timestamp = {Thu, 23 Mar 2023 16:22:16 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/WangTDWHJCJZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liang2023taskmatrixai,
  title   = {TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs},
  author  = {Yaobo Liang and Chenfei Wu and Ting Song and Wenshan Wu and Yan Xia and Yu Liu and Yang Ou and Shuai Lu and Lei Ji and Shaoguang Mao and Yun Wang and Linjun Shou and Ming Gong and Nan Duan},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.16434}
}

@article{jin2021lifelong,
  title     = {Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora},
  author    = {Xisen Jin and Dejiao Zhang and Henghui Zhu and Wei Xiao and Shang-Wen Li and Xiaokai Wei and Andrew O. Arnold and Xiang Ren},
  journal   = {BIGSCIENCE},
  year      = {2021},
  doi       = {10.18653/v1/2022.bigscience-1.1},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ed8931af08ce757a92a01ed43a0619522e10e8ff}
}

@inproceedings{qin-etal-2022-elle,
    title = "{ELLE}: Efficient Lifelong Pre-training for Emerging Data",
    author = "Qin, Yujia  and
      Zhang, Jiajie  and
      Lin, Yankai  and
      Liu, Zhiyuan  and
      Li, Peng  and
      Sun, Maosong  and
      Zhou, Jie",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.220",
    doi = "10.18653/v1/2022.findings-acl.220",
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}


@inproceedings{gu-etal-2022-ppt,
    title = "{PPT}: Pre-trained Prompt Tuning for Few-shot Learning",
    author = "Gu, Yuxian  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Huang, Minlie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.576",
    doi = "10.18653/v1/2022.acl-long.576",
    pages = "8410--8423",
}

@misc{peng2023semiparametric,
      title={Semiparametric Language Models Are Scalable Continual Learners}, 
      author={Guangyue Peng and Tao Ge and Si-Qing Chen and Furu Wei and Houfeng Wang},
      year={2023},
      eprint={2303.01421},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
continual-comparative,
title={Pretrained Language Model in Continual Learning: A Comparative Study},
author={Tongtong Wu and Massimo Caccia and Zhuang Li and Yuan-Fang Li and Guilin Qi and Gholamreza Haffari},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=figzpGMrdD}
}

@article{continual-survey,
title = "A continual learning survey: Defying forgetting in classification tasks",
author = "Matthias Delange and Rahaf Aljundi and Marc Masana and Sarah Parisot and Xu Jia and Ales Leonardis and Greg Slabaugh and Tinne Tuytelaars",
year = "2021",
month = feb,
day = "5",
doi = "10.1109/TPAMI.2021.3057446",
language = "English",
volume = "PP",
journal = "IEEE Transactions on Software Engineering",
issn = "0098-5589",
publisher = "Institute of Electrical and Electronics Engineers",
}

% Google Scholar
@inproceedings{gan2020look,
  title={Look, listen, and act: Towards audio-visual embodied navigation},
  author={Gan, Chuang and Zhang, Yiwei and Wu, Jiajun and Gong, Boqing and Tenenbaum, Joshua B},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9701--9707},
  year={2020},
  organization={IEEE}
}

@article{sharma2021skill,
  title     = {Skill Induction and Planning with Latent Language},
  author    = {Pratyusha Sharma and A. Torralba and Jacob Andreas},
  journal   = {Annual Meeting Of The Association For Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2022.acl-long.120},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/2d4ca959cb3d544473cb661cefe76daabebcdff3}
}

@article{menick2022teaching,
  title={Teaching language models to support answers with verified quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others},
  journal={arXiv preprint arXiv:2203.11147},
  year={2022}
}

@inproceedings{fevry2020entities,
  title={Entities as Experts: Sparse Memory Access with Entity Supervision},
  author={F{\'e}vry, Thibault and Soares, Livio Baldini and Fitzgerald, Nicholas and Choi, Eunsol and Kwiatkowski, Tom},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4937--4951},
  year={2020}
}

@inproceedings{episodic-transformer,
  author    = {Alexander Pashevich and Cordelia Schmid and Chen Sun},
  title     = {Episodic Transformer for Vision-and-Language Navigation},
  booktitle = {2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2021, Montreal, QC, Canada, October 10-17, 2021},
  pages     = {15922-15932},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICCV48922.2021.01564},
  doi       = {10.1109/ICCV48922.2021.01564},
  timestamp = {Fri, 11 Mar 2022 10:01:59 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/PashevichS021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li2022overcoming,
  title={Overcoming catastrophic forgetting during domain adaptation of seq2seq language generation},
  author={Li, Dingcheng and Chen, Zheng and Cho, Eunah and Hao, Jie and Liu, Xiaohu and Xing, Fan and Guo, Chenlei and Liu, Yang},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5441--5454},
  year={2022}
}

@inproceedings{he2021analyzing,
  title={Analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response models},
  author={He, Tianxing and Liu, Jun and Cho, Kyunghyun and Ott, Myle and Liu, Bing and Glass, James and Peng, Fuchun},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={1121--1133},
  year={2021}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@inproceedings{chen2020recall,
  title={Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting},
  author={Chen, Sanyuan and Hou, Yutai and Cui, Yiming and Che, Wanxiang and Liu, Ting and Yu, Xiangzhan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7870--7881},
  year={2020}
}

@article{gao2022attributed,
  title={Attributed text generation via post-hoc research and revision},
  author={Gao, Luyu and Dai, Zhuyun and Pasupat, Panupong and Chen, Anthony and Chaganty, Arun Tejasvi and Fan, Yicheng and Zhao, Vincent Y and Lao, Ni and Lee, Hongrae and Juan, Da-Cheng and others},
  journal={arXiv preprint arXiv:2210.08726},
  year={2022}
}

@article{lazaridou2022internet,
  title={Internet-augmented language models through few-shot prompting for open-domain question answering},
  author={Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
  journal={arXiv preprint arXiv:2203.05115},
  year={2022}
}

@article{kasai2022realtime,
  title={RealTime QA: What's the Answer Right Now?},
  author={Kasai, Jungo and Sakaguchi, Keisuke and Takahashi, Yoichi and Bras, Ronan Le and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2207.13332},
  year={2022}
}

@article{tiny-episode-memories,
  author    = {Arslan Chaudhry and
               Marcus Rohrbach and
               Mohamed Elhoseiny and
               Thalaiyasingam Ajanthan and
               Puneet Kumar Dokania and
               Philip H. S. Torr and
               Marc'Aurelio Ranzato},
  title     = {Continual Learning with Tiny Episodic Memories},
  journal   = {CoRR},
  volume    = {abs/1902.10486},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10486},
  eprinttype = {arXiv},
  eprint    = {1902.10486},
  timestamp = {Tue, 21 May 2019 18:03:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-10486.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{izacarddistilling,
  title={Distilling Knowledge from Reader to Retriever for Question Answering},
  author={Izacard, Gautier and Grave, Edouard},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@INPROCEEDINGS{icarl,
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={iCaRL: Incremental Classifier and Representation Learning}, 
  year={2017},
  volume={},
  number={},
  pages={5533-5542},
  doi={10.1109/CVPR.2017.587}}

@article{aribandi2021ext5,
  title     = {ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
  author    = {V. Aribandi and Yi Tay and Tal Schuster and J. Rao and Huaixiu Zheng and Sanket Vaibhav Mehta and Honglei Zhuang and V. Tran and Dara Bahri and Jianmo Ni and Jai Gupta and Kai Hui and Sebastian Ruder and Donald Metzler},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cbf98ebe967e0f3f3236e7932f37013b98244e94}
}

@inproceedings{experience-replay,
 author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Experience Replay for Continual Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{selective-experience-replay,
author = {Isele, David and Cosgun, Akansel},
title = {Selective Experience Replay for Lifelong Learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {404},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{GAN,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{lin2021truthfulqa,
  title     = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author    = {Stephanie C. Lin and Jacob Hilton and Owain Evans},
  journal   = {Annual Meeting Of The Association For Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2022.acl-long.229},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749}
}

@inproceedings{GrIPS,
  author    = {Archiki Prasad and Peter Hase and Xiang Zhou and Mohit Bansal},
  editor    = {Andreas Vlachos and Isabelle Augenstein},
  title     = {GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models},
  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, {EACL} 2023, Dubrovnik, Croatia, May 2-6, 2023},
  pages     = {3827-3846},
  publisher = {Association for Computational Linguistics},
  year      = {2023},
  url       = {https://aclanthology.org/2023.eacl-main.277},
  timestamp = {Thu, 11 May 2023 17:08:21 +0200},
  biburl    = {https://dblp.org/rec/conf/eacl/PrasadHZB23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{catastrophic-forgetting,
author = { Anthony Robins },
title = {Catastrophic Forgetting, Rehearsal and Pseudorehearsal},
journal = {Connection Science},
volume = {7},
number = {2},
pages = {123-146},
year  = {1995},
publisher = {Taylor & Francis},
doi = {10.1080/09540099550039318},
URL = {https://doi.org/10.1080/09540099550039318},
eprint = {https://doi.org/10.1080/09540099550039318
}
}


@article{pseudo-recursal,
  author    = {Craig Atkinson and
               Brendan McCane and
               Lech Szymanski and
               Anthony V. Robins},
  title     = {Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep
               Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1802.03875},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.03875},
  eprinttype = {arXiv},
  eprint    = {1802.03875},
  timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-03875.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{deep-generative-replay,
  author    = {Hanul Shin and
               Jung Kwon Lee and
               Jaehong Kim and
               Jiwon Kim},
  title     = {Continual Learning with Deep Generative Replay},
  journal   = {CoRR},
  volume    = {abs/1705.08690},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.08690},
  eprinttype = {arXiv},
  eprint    = {1705.08690},
  timestamp = {Fri, 16 Dec 2022 08:18:50 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/ShinLKK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

# data focused 

@InProceedings{impoverished-data,
author="Silver, Daniel L.
and Mercer, Robert E.",
editor="Cohen, Robin
and Spencer, Bruce",
title="The Task Rehearsal Method of Life-Long Learning: Overcoming Impoverished Data",
booktitle="Advances in Artificial Intelligence",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="90--101",
isbn="978-3-540-47922-2"
}


@InProceedings {encorder-lifelong-learning,
author = {A. Rannen and R. Aljundi and M. B. Blaschko and T. Tuytelaars},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
title = {Encoder Based Lifelong Learning},
year = {2017},
volume = {},
issn = {2380-7504},
pages = {1329-1337},
doi = {10.1109/ICCV.2017.148},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.148},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@INPROCEEDINGS {class-incremental-learning,
author = {J. Zhang and J. Zhang and S. Ghosh and D. Li and S. Tasci and L. Heck and H. Zhang and C. Jay Kuo},
booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
title = {Class-incremental Learning via Deep Model Consolidation},
year = {2020},
volume = {},
issn = {},
pages = {1120-1129},
doi = {10.1109/WACV45572.2020.9093365},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV45572.2020.9093365},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@INPROCEEDINGS {expert-gate,
author = {R. Aljundi and P. Chakravarty and T. Tuytelaars},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Expert Gate: Lifelong Learning with a Network of Experts},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {7120-7129},
keywords = {training;data models;logic gates;training data;load modeling;neural networks},
doi = {10.1109/CVPR.2017.753},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.753},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@article{Jung_Ju_Jung_Kim_2018, 
title={Less-Forgetful Learning for Domain Expansion in Deep Neural Networks}, 
volume={32}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/11769}, 
DOI={10.1609/aaai.v32i1.11769}, 
number={1}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Jung, Heechul and Ju, Jeongwoo and Jung, Minju and Kim, Junmo}, 
year={2018}, 
month={Apr.} }

@article{learning-without-forgetting,
  author    = {Zhizhong Li and
               Derek Hoiem},
  title     = {Learning without Forgetting},
  journal   = {CoRR},
  volume    = {abs/1606.09282},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.09282},
  eprinttype = {arXiv},
  eprint    = {1606.09282},
  timestamp = {Thu, 31 Dec 2020 11:34:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/LiH16e.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

# prior focused method

@InProceedings{memory-aware-synapses-rahaf,
author="Aljundi, Rahaf
and Babiloni, Francesca
and Elhoseiny, Mohamed
and Rohrbach, Marcus
and Tuytelaars, Tinne",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="Memory Aware Synapses: Learning What (not) to Forget",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="144--161",

isbn="978-3-030-01219-9"
}

@inproceedings{continual-synaptic,
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
title = {Continual Learning through Synaptic Intelligence},
year = {2017},
publisher = {JMLR.org},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3987–3995},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{task-agnostic-continual,
    author = {Zeno, Chen and Golan, Itay and Hoffer, Elad and Soudry, Daniel},
    title = "{Task-Agnostic Continual Learning Using Online Variational Bayes With Fixed-Point Updates}",
    journal = {Neural Computation},
    volume = {33},
    number = {11},
    pages = {3139-3177},
    year = {2021},
    month = {10},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01430},
    url = {https://doi.org/10.1162/neco\_a\_01430},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/11/3139/1966626/neco\_a\_01430.pdf},
}



@article{task-free-continual,
  author    = {Rahaf Aljundi and
               Klaas Kelchtermans and
               Tinne Tuytelaars},
  title     = {Task-Free Continual Learning},
  journal   = {CoRR},
  volume    = {abs/1812.03596},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.03596},
  eprinttype = {arXiv},
  eprint    = {1812.03596},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-03596.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{
wang2022uncertaintybased,
title={Uncertainty-Based Active Learning for Reading Comprehension},
author={Jing Wang and Jie Shen and Xiaofei Ma and Andrew Arnold},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=QaDevCcmcg},
note={}
}

@inproceedings{
v.2018variational,
title={Variational Continual Learning},
author={Cuong V. Nguyen and Yingzhen Li and Thang D. Bui and Richard E. Turner},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkQqq0gRb},
}

@inproceedings{LeCun2022path,
  title  = {A Path Towards Autonomous Machine Intelligence},
  author = {Yann LeCun},
  year   = {2022},
  url    = {https://openreview.net/pdf?id=BZ5a1r-kVsf}
}



@article{papernot2016semisupervised,
  title   = {Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data},
  author  = {Nicolas Papernot and Martín Abadi and Úlfar Erlingsson and Ian Goodfellow and Kunal Talwar},
  year    = {2016},
  journal = {arXiv preprint arXiv: Arxiv-1610.05755}
}


@article{personalization-privacy,
  author    = {Matthias De Lange and
               Xu Jia and
               Sarah Parisot and
               Ales Leonardis and
               Gregory G. Slabaugh and
               Tinne Tuytelaars},
  title     = {Unsupervised Model Personalization while Preserving Privacy and Scalability:
               An Open Problem},
  journal   = {CoRR},
  volume    = {abs/2003.13296},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.13296},
  eprinttype = {arXiv},
  eprint    = {2003.13296},
  timestamp = {Wed, 01 Apr 2020 17:39:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-13296.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Mallya2018PiggybackAA,
  title={Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights},
  author={Arun Mallya and Dillon Davis and Svetlana Lazebnik},
  booktitle={European Conference on Computer Vision},
  year={2018}
}

@article{Mallya2017PackNetAM,
  title={PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning},
  author={Arun Mallya and Svetlana Lazebnik},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={7765-7773}
}


@article{
diao2023blackbox,
title={Black-Box Prompt Learning for Pre-trained Language Models},
author={Shizhe Diao and Zhichao Huang and Ruijia Xu and Xuechun Li and LIN Yong and Xiao Zhou and Tong Zhang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=IvsGP7xRvm},
}

@article{sun2022blackbox,
  author    = {Tianxiang Sun and
               Yunfan Shao and
               Hong Qian and
               Xuanjing Huang and
               Xipeng Qiu},
  title     = {Black-Box Tuning for Language-Model-as-a-Service},
  journal   = {CoRR},
  volume    = {abs/2201.03514},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.03514},
  eprinttype = {arXiv},
}

@inproceedings{DBLP:conf/emnlp/DuaG0G22,
  author    = {Dheeru Dua and
               Shivanshu Gupta and
               Sameer Singh and
               Matt Gardner},
  editor    = {Yoav Goldberg and
               Zornitsa Kozareva and
               Yue Zhang},
  title     = {Successive Prompting for Decomposing Complex Questions},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
               December 7-11, 2022},
  pages     = {1251--1265},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.81},
  timestamp = {Tue, 07 Feb 2023 17:10:51 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/DuaG0G22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{keskar2019ctrl,
  title={CTRL: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}


@article{clive2021control,
  title={Control prefixes for text generation},
  author={Clive, Jordan and Cao, Kris and Rei, Marek},
  journal={arXiv preprint arXiv:2110.08329},
  year={2021}
}

@article{lewis2021paq,
  title={Paq: 65 million probably-asked questions and what you can do with them},
  author={Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini, Pasquale and K{\"u}ttler, Heinrich and Piktus, Aleksandra and Stenetorp, Pontus and Riedel, Sebastian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1098--1115},
  year={2021},
  publisher={MIT Press}
}

@article{chen2022augmenting,
  title={Augmenting pre-trained language models with qa-memory for open-domain question answering},
  author={Chen, Wenhu and Verga, Pat and de Jong, Michiel and Wieting, John and Cohen, William},
  journal={EACL},
  year={2023}
}

@inproceedings{xiongpretrained,
  title={Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model},
  author={Xiong, Wenhan and Du, Jingfei and Wang, William Yang and Stoyanov, Veselin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{zou2021controllable,
  title={Controllable generation from pre-trained language models via inverse prompting},
  author={Zou, Xu and Yin, Da and Zhong, Qingyang and Yang, Hongxia and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={2450--2460},
  year={2021}
}

@inproceedings{hu2017toward,
  title={Toward controlled generation of text},
  author={Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2017},
  organization={PMLR}
}

@article{weng2021conditional,
  title   = "Controllable Neural Text Generation.",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2021",
  month   = "Jan",
  url     = "https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/"
}


@article{zhang2022survey,
  title   = {A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models},
  author  = {Hanqing Zhang and Haolin Song and Shaoyu Li and Ming Zhou and Dawei Song},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2201.05337}
}

@article{dathathri2019plug,
  title={Plug and play language models: A simple approach to controlled text generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  journal={arXiv preprint arXiv:1912.02164},
  year={2019}
}
@article{li2022diffusion,
  title={Diffusion-lm improves controllable text generation},
  author={Li, Xiang and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4328--4343},
  year={2022}
}

@article{russo2020control,
  title={Control, generate, augment: A scalable framework for multi-attribute text generation},
  author={Russo, Giuseppe and Hollenstein, Nora and Musat, Claudiu and Zhang, Ce},
  journal={arXiv preprint arXiv:2004.14983},
  year={2020}
}
@article{krause2020gedi,
  title={Gedi: Generative discriminator guided sequence generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  journal={arXiv preprint arXiv:2009.06367},
  year={2020}
}
@article{yu2021attribute,
  title={Attribute alignment: Controlling text generation from pre-trained language models},
  author={Yu, Dian and Yu, Zhou and Sagae, Kenji},
  journal={arXiv preprint arXiv:2103.11070},
  year={2021}
}

@article{shen2017style,
  title={Style transfer from non-parallel text by cross-alignment},
  author={Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{yuanzhe2022reward,
  title={Reward Gaming in Conditional Text Generation},
  author={Yuanzhe Pang, Richard and Padmakumar, Vishakh and Sellam, Thibault and Parikh, Ankur P and He, He},
  journal={arXiv e-prints},
  pages={arXiv--2211},
  year={2022}
}
@article{fan2022nano,
  title={Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control},
  author={Fan, Xiang and Lyu, Yiwei and Liang, Paul Pu and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2211.05750},
  year={2022}
}
@article{lu2022quark,
  title={Quark: Controllable text generation with reinforced unlearning},
  author={Lu, Ximing and Welleck, Sean and Hessel, Jack and Jiang, Liwei and Qin, Lianhui and West, Peter and Ammanabrolu, Prithviraj and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27591--27609},
  year={2022}
}

@inproceedings{alfworld,
  author    = {Mohit Shridhar and
               Xingdi Yuan and
               Marc{-}Alexandre C{\^{o}}t{\'{e}} and
               Yonatan Bisk and
               Adam Trischler and
               Matthew J. Hausknecht},
  title     = {ALFWorld: Aligning Text and Embodied Environments for Interactive
               Learning},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=0IOX0YcCdTn},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ShridharYCBTH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{yin-etal-2022-contintin,
    title = "{C}on{T}in{T}in: Continual Learning from Task Instructions",
    author = "Yin, Wenpeng  and
      Li, Jia  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.218",
    doi = "10.18653/v1/2022.acl-long.218",
    pages = "3062--3072",
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

% arxiv
@article{team2021creating,
  title={Creating multimodal interactive agents with imitation and self-supervised learning},
  author={Team, DeepMind Interactive Agents and Abramson, Josh and Ahuja, Arun and Brussee, Arthur and Carnevale, Federico and Cassin, Mary and Fischer, Felix and Georgiev, Petko and Goldin, Alex and Gupta, Mansi and others},
  journal={arXiv preprint arXiv:2112.03763},
  year={2021}
}

@ARTICLE{6795228,
  author={Williams, Ronald J. and Zipser, David},
  journal={Neural Computation}, 
  title={A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}, 
  year={1989},
  volume={1},
  number={2},
  pages={270-280},
  doi={10.1162/neco.1989.1.2.270}}


@article{erdem2022neural,
  title={Neural natural language generation: A survey on multilinguality, multimodality, controllability and learning},
  author={Erdem, Erkut and Kuyu, Menekse and Yagcioglu, Semih and Frank, Anette and Parcalabescu, Letitia and Plank, Barbara and Babii, Andrii and Turuta, Oleksii and Erdem, Aykut and Calixto, Iacer and others},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={1131--1207},
  year={2022}
}



@article{logeswaran2018content,
  title={Content preserving text generation with attribute controls},
  author={Logeswaran, Lajanugen and Lee, Honglak and Bengio, Samy},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@inproceedings{scienceqa,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={The 36th Conference on Neural Information Processing Systems (NeurIPS)},
    year={2022}
}

@article{ficler2017controlling,
  title={Controlling linguistic style aspects in neural language generation},
  author={Ficler, Jessica and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1707.02633},
  year={2017}
}

@article{li2016persona,
  title={A persona-based neural conversation model},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Spithourakis, Georgios P and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1603.06155},
  year={2016}
}

@inproceedings{Liang2005SemiSupervisedLF,
  title={Semi-Supervised Learning for Natural Language},
  author={Percy Liang},
  year={2005}
}

@book{semi-supervised-book-zien,
  added-at = {2019-07-22T00:00:00.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/265ac136f8b3a44d77fcf9ec42829296a/dblp},
  editor = {Chapelle, Olivier and Schölkopf, Bernhard and Zien, Alexander},
  ee = {https://doi.org/10.7551/mitpress/9780262033589.001.0001},
  interhash = {90eecf83da2790cac977f375160081fe},
  intrahash = {65ac136f8b3a44d77fcf9ec42829296a},
  isbn = {9780262033589},
  keywords = {dblp},
  publisher = {The MIT Press},
  timestamp = {2019-09-17T12:36:24.000+0200},
  title = {Semi-Supervised Learning},
  url = {http://dblp.uni-trier.de/db/books/collections/CSZ2006.html},
  year = 2006
}

@inproceedings{
qin2022cold,
title={{COLD} Decoding: Energy-based Constrained Text Generation with Langevin Dynamics},
author={Lianhui Qin and Sean Welleck and Daniel Khashabi and Yejin Choi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=TiZYrQ-mPup}
}

@article{brief-super-weak,
    author = {Zhou, Zhi-Hua},
    title = "{A brief introduction to weakly supervised learning}",
    journal = {National Science Review},
    volume = {5},
    number = {1},
    pages = {44-53},
    year = {2017},
    month = {08},
    abstract = "{Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.}",
    issn = {2095-5138},
    doi = {10.1093/nsr/nwx106},
    url = {https://doi.org/10.1093/nsr/nwx106},
    eprint = {https://academic.oup.com/nsr/article-pdf/5/1/44/31567770/nwx106.pdf},
}



@inproceedings{sheng-etal-2020-towards,
    title = "Towards {C}ontrollable {B}iases in {L}anguage {G}eneration",
    author = "Sheng, Emily  and
      Chang, Kai-Wei  and
      Natarajan, Prem  and
      Peng, Nanyun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.291",
    doi = "10.18653/v1/2020.findings-emnlp.291",
    pages = "3239--3254",
}

@article{liu2021dexperts,
  title={DExperts: Decoding-time controlled text generation with experts and anti-experts},
  author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi, Yejin},
  journal={arXiv preprint arXiv:2105.03023},
  year={2021}
}

@article{he2019negative,
  title={Negative training for neural dialogue response generation},
  author={He, Tianxing and Glass, James},
  journal={arXiv preprint arXiv:1903.02134},
  year={2019}
}

@article{Wu_Galley_2021_Grounded-Respsonse, title={A Controllable Model of Grounded Response Generation}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17658}, DOI={10.1609/aaai.v35i16.17658}, number={16}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wu, Zeqiu and Galley, Michel and Brockett, Chris and Zhang, Yizhe and Gao, Xiang and Quirk, Chris and Koncel-Kedziorski, Rik and Gao, Jianfeng and Hajishirzi, Hannaneh and Ostendorf, Mari and Dolan, Bill}, year={2021}, month={May}, pages={14085-14093} }

@inproceedings{jin-etal-2020-hooks,
    title = "Hooks in the Headline: Learning to Generate Headlines with Controlled Styles",
    author = "Jin, Di  and
      Jin, Zhijing  and
      Zhou, Joey Tianyi  and
      Orii, Lisa  and
      Szolovits, Peter",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.456",
    doi = "10.18653/v1/2020.acl-main.456",
    pages = "5082--5093",
}

@inproceedings{bao-etal-2019-generating,
    title = "Generating Sentences from Disentangled Syntactic and Semantic Spaces",
    author = "Bao, Yu  and
      Zhou, Hao  and
      Huang, Shujian  and
      Li, Lei  and
      Mou, Lili  and
      Vechtomova, Olga  and
      Dai, Xin-yu  and
      Chen, Jiajun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1602",
    doi = "10.18653/v1/P19-1602",
    pages = "6008--6019",
}
@inproceedings{dai-etal-2019-style,
    title = "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation",
    author = "Dai, Ning  and
      Liang, Jianze  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1601",
    doi = "10.18653/v1/P19-1601",
    pages = "5997--6007",
}

@inproceedings{fu2018style,
  title={Style transfer in text: Exploration and evaluation},
  author={Fu, Zhenxin and Tan, Xiaoye and Peng, Nanyun and Zhao, Dongyan and Yan, Rui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

% Google Scholar
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{gpt-2-story-gen,
    author = {Guan, Jian and Huang, Fei and Zhao, Zhihao and Zhu, Xiaoyan and Huang, Minlie},
    title = "{A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {93-108},
    year = {2020},
    month = {01},
    abstract = "{Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00302},
    url = {https://doi.org/10.1162/tacl\_a\_00302},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00302/1923261/tacl\_a\_00302.pdf},
}





@article{adhikari2019docbert,
  title   = {DocBERT: BERT for Document Classification},
  author  = {Ashutosh Adhikari and Achyudh Ram and Raphael Tang and Jimmy Lin},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1904.08398}
}

@article{dong2023raft,
  title     = {RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment},
  author    = {Hanze Dong and Wei Xiong and Deepanshu Goyal and Rui Pan and Shizhe Diao and Jipeng Zhang and Kashun Shum and T. Zhang},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2304.06767},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/c66050ae29b8b55fac48dfc9b931abf5c53f0c8d}
}

@article{fernandes2023bridging,
  title     = {Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation},
  author    = {Patrick Fernandes and Aman Madaan and Emmy Liu and António Farinhas and Pedro Henrique Martins and Amanda Bertsch and José G. C. de Souza and Shuyan Zhou and Tongshuang Sherry Wu and Graham Neubig and André F. T. Martins},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2305.00955},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/24519ceacbac17005c31920468e4871e5148a887}
}

@article{yuan2023rrhf,
  title     = {RRHF: Rank Responses to Align Language Models with Human Feedback without tears},
  author    = {Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Feiran Huang},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2304.05302},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/35e2fffe480e72e5362870398ab329f79d06b0da}
}

@article{alignment-tax,
  title     = {A General Language Assistant as a Laboratory for Alignment},
  author    = {Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and T. Henighan and Andy Jones and Nicholas Joseph and Benjamin Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and John Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and C. Olah and Jared Kaplan},
  journal   = {ARXIV.ORG},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e}
}

% Google Scholar
@article{snell2022offline,
  title={Offline rl for natural language generation with implicit language q learning},
  author={Snell, Charlie and Kostrikov, Ilya and Su, Yi and Yang, Mengjiao and Levine, Sergey},
  journal={arXiv preprint arXiv:2206.11871},
  year={2022}
}

@inproceedings{xu-etal-2022-perceiving,
    title = "Perceiving the World: Question-guided Reinforcement Learning for Text-based Games",
    author = "Xu, Yunqiu  and
      Fang, Meng  and
      Chen, Ling  and
      Du, Yali  and
      Zhou, Joey  and
      Zhang, Chengqi",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.41",
    doi = "10.18653/v1/2022.acl-long.41",
    pages = "538--560",
    abstract = "Text-based games provide an interactive way to study natural language processing. While deep reinforcement learning has shown effectiveness in developing the game playing agent, the low sample efficiency and the large action space remain to be the two major challenges that hinder the DRL from being applied in the real world. In this paper, we address the challenges by introducing world-perceiving modules, which automatically decompose tasks and prune actions by answering questions about the environment. We then propose a two-phase training framework to decouple language learning from reinforcement learning, which further improves the sample efficiency. The experimental results show that the proposed method significantly improves the performance and sample efficiency. Besides, it shows robustness against compound error and limited pre-training data.",
}

@article{llm-survey,
  title     = {A Survey of Large Language Models},
  author    = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Z. Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and J. Nie and Ji-rong Wen},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2303.18223},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cbdc75e9c80870164662bb0359e23fd4736c5f0e}
}

@article{from-bert-to-chatgpt,
  title     = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
  author    = {Ce Zhou and Qian Li and Chen Li and Jun Yu and Yixin Liu and Guan Wang and Kaichao Zhang and Cheng Ji and Qi Yan and Lifang He and Hao Peng and Jianxin Li and Jia Wu and Ziwei Liu and P. Xie and Caiming Xiong and Jian Pei and Philip S. Yu and Lichao Sun Michigan State University and B. University and Lehigh University and M. University and Nanyang Technological University and University of California at San Diego and D. University and U. Chicago and Salesforce AI Research},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2302.09419},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/4bf0ad8903a45469235355a5514da2596cb2a018}
}

% Google Scholar
@incollection{gupta2023artificial,
  title={Artificial Intelligence for Recruitment and Selection},
  author={Gupta, Aashima and Mishra, Mridula},
  booktitle={The Adoption and Effect of Artificial Intelligence on Human Resources Management, Part B},
  pages={1--11},
  year={2023},
  publisher={Emerald Publishing Limited}
}

% Google Scholar
@article{xiao2020tell,
  title={Tell me about yourself: Using an AI-powered chatbot to conduct conversational surveys with open-ended questions},
  author={Xiao, Ziang and Zhou, Michelle X and Liao, Q Vera and Mark, Gloria and Chi, Changyan and Chen, Wenxi and Yang, Huahai},
  journal={ACM Transactions on Computer-Human Interaction (TOCHI)},
  volume={27},
  number={3},
  pages={1--37},
  year={2020},
  publisher={ACM New York, NY, USA}
}

% Google Scholar
@inproceedings{bittner2019bot,
  title={Where is the bot in our team? Toward a taxonomy of design option combinations for conversational agents in collaborative work},
  author={Bittner, Eva AC and Oeste-Rei{\ss}, Sarah and Leimeister, Jan Marco},
  booktitle={Hawaii International Conference on System Sciences (HICSS)},
  year={2019}
}

% Google Scholar
@article{nawaz2019artificial,
  title={Artificial intelligence chatbots are new recruiters},
  author={Nawaz, Nishad and Gomes, Anjali Mary},
  journal={IJACSA) International Journal of Advanced Computer Science and Applications},
  volume={10},
  number={9},
  year={2019}
}

% arXiv
@article{kiseleva2022iglu,
      title={IGLU 2022: Interactive Grounded Language Understanding in a Collaborative Environment at NeurIPS 2022}, 
      author={Julia Kiseleva and Alexey Skrynnik and Artem Zholus and Shrestha Mohanty and Negar Arabzadeh and Marc-Alexandre Côté and Mohammad Aliannejadi and Milagro Teruel and Ziming Li and Mikhail Burtsev and Maartje ter Hoeve and Zoya Volovikova and Aleksandr Panov and Yuxuan Sun and Kavya Srinet and Arthur Szlam and Ahmed Awadallah},
      journal={arXiv preprint arXiv:2205.13771},
      year={2022}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{mitchell2021fast,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}

@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}

@article{mitchell2022enhancing,
  title={Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference},
  author={Mitchell, Eric and Noh, Joseph J and Li, Siyan and Armstrong, William S and Agarwal, Ananth and Liu, Patrick and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2211.11875},
  year={2022}
}

@unpublished{bubeck2023sparks-AGI,
author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
year = {2023},
month = {March},
url = {https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-/with-gpt-4/},
}


@article{wolf2023fundamental,
  title={Fundamental Limitations of Alignment in Large Language Models},
  author={Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

@article{ibarz2018reward,
  title={Reward learning from human preferences and demonstrations in atari},
  author={Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{hadfield2017inverse,
  title={Inverse reward design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lostmiddle,
  title   = {Lost in the Middle: How Language Models Use Long Contexts},
  author  = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.03172}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{confidence-score-gpt,
  title={Teaching models to express their uncertainty in words},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2205.14334},
  year={2022}
}

@article{de2021editing,
  title={Editing factual knowledge in language models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}

@misc{lynch2022interactive,
      title={Interactive Language: Talking to Robots in Real Time}, 
      author={Corey Lynch and Ayzaan Wahid and Jonathan Tompson and Tianli Ding and James Betker and Robert Baruch and Travis Armstrong and Pete Florence},
      year={2022},
      eprint={2210.06407},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{
google2023palm2,
title={PaLM 2},
author={Google},
year={2023},
url={https://ai.google/discover/palm2}
}

@inproceedings{henderson-etal-2014-second,
    title = "The Second Dialog State Tracking Challenge",
    author = "Henderson, Matthew  and
      Thomson, Blaise  and
      Williams, Jason D.",
    booktitle = "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL})",
    month = jun,
    year = "2014",
    address = "Philadelphia, PA, U.S.A.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4337",
    doi = "10.3115/v1/W14-4337",
    pages = "263--272",
}

@Inproceedings{Lu2020,
 author = {Weiyi Lu and Yi Xu and Erran Li},
 title = {Efficient evaluation of task oriented dialogue systems},
 year = {2020},
 url = {https://www.amazon.science/publications/efficient-evaluation-of-task-oriented-dialogue-systems},
 booktitle = {NeurIPS 2020 Workshop on Human in the Loop Dialogue Systems},
}

@inproceedings{testoni-bernardi-2021-interplay,
    title = "The Interplay of Task Success and Dialogue Quality: An in-depth Evaluation in Task-Oriented Visual Dialogues",
    author = "Testoni, Alberto  and
      Bernardi, Raffaella",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.178",
    doi = "10.18653/v1/2021.eacl-main.178",
    pages = "2071--2082",
    abstract = "When training a model on referential dialogue guessing games, the best model is usually chosen based on its task success. We show that in the popular end-to-end approach, this choice prevents the model from learning to generate linguistically richer dialogues, since the acquisition of language proficiency takes longer than learning the guessing task. By comparing models playing different games (GuessWhat, GuessWhich, and Mutual Friends), we show that this discrepancy is model- and task-agnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set.",
}

% Google Scholar
@article{yu2022multimodal,
  title={Multimodal knowledge alignment with reinforcement learning},
  author={Yu, Youngjae and Chung, Jiwan and Yun, Heeseung and Hessel, Jack and Park, JaeSung and Lu, Ximing and Ammanabrolu, Prithviraj and Zellers, Rowan and Bras, Ronan Le and Kim, Gunhee and others},
  journal={arXiv preprint arXiv:2205.12630},
  year={2022}
}

% Google Scholar
@article{prudencio2023survey,
  title={A survey on offline reinforcement learning: Taxonomy, review, and open problems},
  author={Prudencio, Rafael Figueiredo and Maximo, Marcos ROA and Colombini, Esther Luna},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}

% Google Scholar
@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@misc{harvard-whiten-resume,
title={Minorities Who 'Whiten' Job Resumes Get More Interviews},
author={Dina Gerdeman},
year={2017},
url={https://hbswk.hbs.edu/item/minorities-who-whiten-job-resumes-get-more-interviews}
}

@misc{amazon-resume,
title={Amazon scraps secret AI recruiting tool that showed bias against women},
author={Jeffrey Dastin},
year={2018},
url={https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}
}


@article{Leino-feature-bias-2018,
  author       = {Klas Leino and
                  Matt Fredrikson and
                  Emily Black and
                  Shayak Sen and
                  Anupam Datta},
  title        = {Feature-Wise Bias Amplification},
  journal      = {CoRR},
  volume       = {abs/1812.08999},
  year         = {2018},
  url          = {http://arxiv.org/abs/1812.08999},
  eprinttype    = {arXiv},
  eprint       = {1812.08999},
  timestamp    = {Wed, 02 Jan 2019 14:40:18 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1812-08999.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hutchinson-etal-2020-social,
    title = "Social Biases in {NLP} Models as Barriers for Persons with Disabilities",
    author = "Hutchinson, Ben  and
      Prabhakaran, Vinodkumar  and
      Denton, Emily  and
      Webster, Kellie  and
      Zhong, Yu  and
      Denuyl, Stephen",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.487",
    doi = "10.18653/v1/2020.acl-main.487",
    pages = "5491--5501",
}

@inproceedings{li-etal-2022-herb,
    title = "{HERB}: Measuring Hierarchical Regional Bias in Pre-trained Language Models",
    author = "Li, Yizhi  and
      Zhang, Ge  and
      Yang, Bohao  and
      Lin, Chenghua  and
      Ragni, Anton  and
      Wang, Shi  and
      Fu, Jie",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-aacl.32",
    pages = "334--346",
}

% Google Scholar
@inproceedings{goyal2021pixl2r,
  title={Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards},
  author={Goyal, Prasoon and Niekum, Scott and Mooney, Raymond},
  booktitle={Conference on Robot Learning},
  pages={485--497},
  year={2021},
  organization={PMLR}
}

% arXiv
@misc{scheurer2023training,
      title={Training Language Models with Language Feedback at Scale}, 
      author={Jérémy Scheurer and Jon Ander Campos and Tomasz Korbak and Jun Shern Chan and Angelica Chen and Kyunghyun Cho and Ethan Perez},
      year={2023},
      eprint={2303.16755},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% arXiv
@misc{chen2023improving,
      title={Improving Code Generation by Training with Natural Language Feedback}, 
      author={Angelica Chen and Jérémy Scheurer and Tomasz Korbak and Jon Ander Campos and Jun Shern Chan and Samuel R. Bowman and Kyunghyun Cho and Ethan Perez},
      year={2023},
      eprint={2303.16749},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{chatgpt-edu-2023,
    title={ChatGPT: Bullshit spewer or the end of traditional assessments in higher education}, 
    author={Jürgen Rudolph and Samson Tan and Shannon Tan},
    volume={6}, 
    number={1}, 
    year={2023}, 
    month={Jan} 
}

@inproceedings{10.5555/3157382.3157584,
author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4356–4364},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}


%%%%%  repeated references %%%%%

% inproceedings{post2018fast,
%   title={Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation},
%   author={Post, Matt and Vilar, David},
%   booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
%   pages={1314--1324},
%   year={2018}
% }



% article{andersonguided,
%   title={Guided Open Vocabulary Image Captioning with Constrained Beam Search},
%   author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
%   year={2017},
%   journal={EMNLP}
% }


% article{kim2022improving,
%   title={Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks},
%   author={Kim, Zae Myung and Du, Wanyu and Raheja, Vipul and Kumar, Dhruv and Kang, Dongyeop},
%   journal={arXiv preprint arXiv:2212.01350},
%   year={2022}
% }



% inproceedings{NEURIPS2020_ad1f8bb9,
%  author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
%  booktitle = {Advances in Neural Information Processing Systems},
%  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
%  pages = {15173--15184},
%  publisher = {Curran Associates, Inc.},
%  title = {Supermasks in Superposition},
%  url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ad1f8bb9b51f023cdc80cf94bb615aa9-Paper.pdf},
%  volume = {33},
%  year = {2020}
% }



% inproceedings{hamalainen2018poem,
%   title={Poem machine-a co-creative nlg web application for poem writing},
%   author={Hamaainen, Mika},
%   booktitle={Proceedings of the 11th International Conference on Natural Language Generation},
%   pages={195--196},
%   year={2018}
% }


% inproceedings{Ashish2017attention,
% title = {Attention is All You Need},
% author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
% year  = {2017},
% URL   = {https://arxiv.org/pdf/1706.03762.pdf}
% }

% article{Devlin2019BERTPO,
%   title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
%   author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
%   journal={ArXiv},
%   year={2019},
%   volume={abs/1810.04805}
% }




% article{li2016dialogue,
%   title={Dialogue learning with human-in-the-loop},
%   author={Li, Jiwei and Miller, Alexander H and Chopra, Sumit and Ranzato, Marc'Aurelio and Weston, Jason},
%   journal={arXiv preprint arXiv:1611.09823},
%   year={2016}
% }

% article{schroder2021revisiting,
%   title={Revisiting uncertainty-based query strategies for active learning with transformers},
%   author={Schr{\"o}der, Christopher and Niekler, Andreas and Potthast, Martin},
%   journal={arXiv preprint arXiv:2107.05687},
%   year={2021}
% }

% article{raffel2020exploring,
%   title={Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer},
%   author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and Malkan, Karishma and others},
%   journal={Journal of Machine Learning Research},
%   volume={21},
%   pages={1--67},
%   year={2020}
% }



% inproceedings{li-liang-2021-prefix,
%     title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
%     author = "Li, Xiang Lisa  and
%       Liang, Percy",
%     booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
%     month = aug,
%     year = "2021",
%     address = "Online",
%     publisher = "Association for Computational Linguistics",
%     url = "https://aclanthology.org/2021.acl-long.353",
%     doi = "10.18653/v1/2021.acl-long.353",
%     pages = "4582--4597",
%     abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
% }



% article{generative-agents,
%   title   = {Generative Agents: Interactive Simulacra of Human Behavior},
%   author  = {Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
%   year    = {2023},
%   journal = {arXiv preprint arXiv: Arxiv-2304.03442}
% }



% article{ye2023context,
%   title={In-Context Instruction Learning},
%   author={Ye, Seonghyeon and Hwang, Hyeonbin and Yang, Sohee and Yun, Hyeongu and Kim, Yireun and Seo, Minjoon},
%   journal={arXiv preprint arXiv:2302.14691},
%   year={2023}
% }




% article{instructions_eval,
%   author    = {Long Ouyang and
%                Jeff Wu and
%                Xu Jiang and
%                Diogo Almeida and
%                Carroll L. Wainwright and
%                Pamela Mishkin and
%                Chong Zhang and
%                Sandhini Agarwal and
%                Katarina Slama and
%                Alex Ray and
%                John Schulman and
%                Jacob Hilton and
%                Fraser Kelton and
%                Luke Miller and
%                Maddie Simens and
%                Amanda Askell and
%                Peter Welinder and
%                Paul F. Christiano and
%                Jan Leike and
%                Ryan Lowe},
%   title     = {Training language models to follow instructions with human feedback},
%   journal   = {CoRR},
%   volume    = {abs/2203.02155},
%   year      = {2022},
%   url       = {https://doi.org/10.48550/arXiv.2203.02155},
%   doi       = {10.48550/arXiv.2203.02155},
%   eprinttype = {arXiv},
%   eprint    = {2203.02155},
%   timestamp = {Wed, 16 Mar 2022 16:39:52 +0100},
%   biburl    = {https://dblp.org/rec/journals/corr/abs-2203-02155.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }

% article{human_in_the_loop_survey,
%   author    = {Zijie J. Wang and
%                Dongjin Choi and
%                Shenyu Xu and
%                Diyi Yang},
%   title     = {Putting Humans in the Natural Language Processing Loop: {A} Survey},
%   journal   = {CoRR},
%   volume    = {abs/2103.04044},
%   year      = {2021},
%   url       = {https://arxiv.org/abs/2103.04044},
%   eprinttype = {arXiv},
%   eprint    = {2103.04044},
%   timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
%   biburl    = {https://dblp.org/rec/journals/corr/abs-2103-04044.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }



% inproceedings{bleurt,
%   author    = {Thibault Sellam and
%                Dipanjan Das and
%                Ankur P. Parikh},
%   editor    = {Dan Jurafsky and
%                Joyce Chai and
%                Natalie Schluter and
%                Joel R. Tetreault},
%   title     = {{BLEURT:} Learning Robust Metrics for Text Generation},
%   booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
%                Linguistics, {ACL} 2020, Online, July 5-10, 2020},
%   pages     = {7881--7892},
%   publisher = {Association for Computational Linguistics},
%   year      = {2020},
%   url       = {https://doi.org/10.18653/v1/2020.acl-main.704},
%   doi       = {10.18653/v1/2020.acl-main.704},
%   timestamp = {Fri, 06 Aug 2021 00:40:55 +0200},
%   biburl    = {https://dblp.org/rec/conf/acl/SellamDP20.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }


% article{dua2022successive,
%   title     = {Successive Prompting for Decomposing Complex Questions},
%   author    = {Dheeru Dua and Shivanshu Gupta and Sameer Singh and Matt Gardner},
%   journal   = {Conference On Empirical Methods In Natural Language Processing},
%   year      = {2022},
%   doi       = {10.48550/arXiv.2212.04092},
%   bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/c90151f00b1ac4abf1cc353849b453aa21cc2df3}
% }


% article{fu2022does,
%   title={How does gpt obtain its ability? tracing emergent abilities of language models to their sources},
%   author={Fu, Yao and Peng, Hao and Khot, Tushar},
%   journal={Yao Fu’s Notion},
%   year={2022}
% }


% inproceedings{NEURIPS2020_1457c0d6,
% author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
%  booktitle = {Advances in Neural Information Processing Systems},
%  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
%  pages = {1877--1901},
%  publisher = {Curran Associates, Inc.},
%  title = {Language Models are Few-Shot Learners},
%  url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
%  volume = {33},
%  year = {2020}
% }


% article{gpt-2,
%   title={Language Models are Unsupervised Multitask Learners},
%   author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
%   year={2019}
% }

% misc{
% huang2023large,
% title={Large Language Models Can Self-improve},
% author={Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
% year={2023},
% url={https://openreview.net/forum?id=NiEtU7blzN}
% }



% Google Scholar
% inproceedings{humphreys2022data,
%   title={A data-driven approach for learning to control computers},
%   author={Humphreys, Peter C and Raposo, David and Pohlen, Tobias and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Santoro, Adam and Lillicrap, Timothy},
%   booktitle={International Conference on Machine Learning},
%   pages={9466--9482},
%   year={2022},
%   organization={PMLR}
% }

% misc{lazaridou2023internetaugmented,
%   title  = {Internet-augmented language models through few-shot prompting for open-domain question answering},
%   author = {Angeliki Lazaridou and Elena Gribovskaya and Wojciech Jan Stokowiec and Nikolai Grigorev},
%   year   = {2023},
%   url    = {https://openreview.net/forum?id=hFCUPkSSRE}
% }


% misc{scaling-law,
%       title={Scaling Laws for Neural Language Models}, 
%       author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
%       year={2020},
%       eprint={2001.08361},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }

% article{decomposed-prompting,
%   title   = {Decomposed Prompting: A Modular Approach for Solving Complex Tasks},
%   author  = {Tushar Khot and Harsh Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
%   year    = {2022},
%   journal = {arXiv preprint arXiv: Arxiv-2210.02406}
% }

% article{icil,
%   title   = {In-Context Instruction Learning},
%   author  = {Seonghyeon Ye and Hyeonbin Hwang and Sohee Yang and Hyeongu Yun and Yireun Kim and Minjoon Seo},
%   year    = {2023},
%   journal = {arXiv preprint arXiv: Arxiv-2302.14691}
% }



% InProceedings{pmlr-v15-ross11a,
%   title =      {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
%   author =     {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
%   booktitle =      {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
%   pages =      {627--635},
%   year =   {2011},
%   editor =     {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
%   volume =     {15},
%   series =     {Proceedings of Machine Learning Research},
%   address =    {Fort Lauderdale, FL, USA},
%   month =      {11--13 Apr},
%   publisher =    {PMLR},
%   pdf =    {http://proceedings.mlr.press/v15/ross11a/ross11a.pdf},
%   url =    {https://proceedings.mlr.press/v15/ross11a.html},
%   abstract =   {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.}
% }


% article{t5,
%   author    = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
%   title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
%   journal   = {J. Mach. Learn. Res.},
%   volume    = {21},
%   pages     = {140:1-140:67},
%   year      = {2020},
%   url       = {http://jmlr.org/papers/v21/20-074.html},
%   timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
%   biburl    = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }



% article{minds-eye,
%   title   = {Mind's Eye: Grounded Language Model Reasoning through Simulation},
%   author  = {Ruibo Liu and Jason Wei and Shixiang Shane Gu and Te-Yen Wu and Soroush Vosoughi and Claire Cui and Denny Zhou and Andrew M. Dai},
%   year    = {2022},
%   journal = {arXiv preprint arXiv: Arxiv-2210.05359}
% }


% duplicate of li2022pre
% article{inter-decision-making,
%   title   = {Pre-Trained Language Models for Interactive Decision-Making},
%   author  = {Shuang Li and Xavier Puig and Chris Paxton and Yilun Du and Clinton Wang and Linxi Fan and Tao Chen and De-An Huang and Ekin Akyürek and Anima Anandkumar and Jacob Andreas and Igor Mordatch and Antonio Torralba and Yuke Zhu},
%   year    = {2022},
%   journal = {arXiv preprint arXiv: Arxiv-2202.01771}
% }


% article{read-revise-repeat,
%   title   = {Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision},
%   author  = {Wanyu Du and Zae Myung Kim and Vipul Raheja and Dhruv Kumar and Dongyeop Kang},
%   year    = {acl},
%   journal = {IN2WRITING}
% }



% article{raffel2019exploring,
%  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
%  journal = {ArXiv preprint},
%  title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
%  url = {https://arxiv.org/abs/1910.10683},
%  volume = {abs/1910.10683},
%  year = {2019}
% }

% article{mishra2021cross,
%  author = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
%  journal = {ArXiv preprint},
%  title = {Cross-task generalization via natural language crowdsourcing instructions},
%  url = {https://arxiv.org/abs/2104.08773},
%  volume = {abs/2104.08773},
%  year = {2021}
% }

% article{ewc,
% author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz  and Joel Veness  and Guillaume Desjardins  and Andrei A. Rusu  and Kieran Milan  and John Quan  and Tiago Ramalho  and Agnieszka Grabska-Barwinska  and Demis Hassabis  and Claudia Clopath  and Dharshan Kumaran  and Raia Hadsell },
% title = {Overcoming catastrophic forgetting in neural networks},
% journal = {Proceedings of the National Academy of Sciences},
% volume = {114},
% number = {13},
% pages = {3521-3526},
% year = {2017},
% doi = {10.1073/pnas.1611835114},
% URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1611835114},
% eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114},
% }

% article{
% overcoming-catastrophic-attention,
% author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz  and Joel Veness  and Guillaume Desjardins  and Andrei A. Rusu  and Kieran Milan  and John Quan  and Tiago Ramalho  and Agnieszka Grabska-Barwinska  and Demis Hassabis  and Claudia Clopath  and Dharshan Kumaran  and Raia Hadsell },
% title = {Overcoming catastrophic forgetting in neural networks},
% journal = {Proceedings of the National Academy of Sciences},
% volume = {114},
% number = {13},
% pages = {3521-3526},
% year = {2017},
% doi = {10.1073/pnas.1611835114},
% URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1611835114},
% eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114},
% }


% article{DBLP:journals/corr/abs-2203-11147,
%   author    = {Jacob Menick and
%                Maja Trebacz and
%                Vladimir Mikulik and
%                John Aslanides and
%                H. Francis Song and
%                Martin Chadwick and
%                Mia Glaese and
%                Susannah Young and
%                Lucy Campbell{-}Gillingham and
%                Geoffrey Irving and
%                Nat McAleese},
%   title     = {Teaching language models to support answers with verified quotes},
%   journal   = {CoRR},
%   volume    = {abs/2203.11147},
%   year      = {2022},
%   url       = {https://doi.org/10.48550/arXiv.2203.11147},
%   doi       = {10.48550/arXiv.2203.11147},
%   eprinttype = {arXiv},
%   eprint    = {2203.11147},
%   timestamp = {Tue, 29 Mar 2022 18:07:24 +0200},
%   biburl    = {https://dblp.org/rec/journals/corr/abs-2203-11147.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }



% inproceedings{textworld,
%   author    = {Marc{-}Alexandre C{\^{o}}t{\'{e}} and
%                {\'{A}}kos K{\'{a}}d{\'{a}}r and
%                Xingdi Yuan and
%                Ben Kybartas and
%                Tavian Barnes and
%                Emery Fine and
%                James Moore and
%                Matthew J. Hausknecht and
%                Layla El Asri and
%                Mahmoud Adada and
%                Wendy Tay and
%                Adam Trischler},
%   editor    = {Tristan Cazenave and
%                Abdallah Saffidine and
%                Nathan R. Sturtevant},
%   title     = {TextWorld: {A} Learning Environment for Text-Based Games},
%   booktitle = {Computer Games - 7th Workshop, {CGW} 2018, Held in Conjunction with
%                the 27th International Conference on Artificial Intelligence, {IJCAI}
%                2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers},
%   series    = {Communications in Computer and Information Science},
%   volume    = {1017},
%   pages     = {41--75},
%   publisher = {Springer},
%   year      = {2018},
%   url       = {https://doi.org/10.1007/978-3-030-24337-1\_3},
%   doi       = {10.1007/978-3-030-24337-1\_3},
%   timestamp = {Wed, 09 Feb 2022 09:38:53 +0100},
%   biburl    = {https://dblp.org/rec/conf/ijcai/CoteKYKBFMHAATT18.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }



% article{unilm,
%   title={Unified language model pre-training for natural language understanding and generation},
%   author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
%   journal={Advances in neural information processing systems},
%   volume={32},
%   year={2019}
% }

@article{wei2023jailbroken,
  title   = {Jailbroken: How Does LLM Safety Training Fail?},
  author  = {Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.02483},
  url     = {https://arxiv.org/abs/2307.02483v1},
  pdf     = {https://arxiv.org/pdf/2307.02483.pdf}
}

@article{liu2023jailbreaking,
  title   = {Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},
  author  = {Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.13860}
}

@article{mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@misc{zhong2023agieval,
      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
      year={2023},
      eprint={2304.06364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@article{llm-as-a-judge,
  title   = {Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author  = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2306.05685}
}

@article{chatgpt-better-annotators,
  title   = {ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks},
  author  = {Fabrizio Gilardi and Meysam Alizadeh and Maël Kubli},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.15056}
}

@inproceedings{dai2020deepct,
  title={Context-aware term weighting for first stage passage retrieval},
  author={Dai, Zhuyun and Callan, Jamie},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={1533--1536},
  year={2020}
}

@article{zhao2020sparta,
  title={SPARTA: Efficient open-domain question answering via sparse transformer matching retrieval},
  author={Zhao, Tiancheng and Lu, Xiaopeng and Lee, Kyusong},
  journal={arXiv preprint arXiv:2009.13013},
  year={2020}
}

@inbook{formal2021splade,
author = {Formal, Thibault and Piwowarski, Benjamin and Clinchant, St\'{e}phane},
title = {SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463098},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2288–2292},
numpages = {5}
}

@misc{chen2023symbolic,
      title={Symbolic Discovery of Optimization Algorithms}, 
      author={Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
      year={2023},
      eprint={2302.06675},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%Google Scholar
@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{tian2023chatplug,
  title   = {ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human},
  author  = {Junfeng Tian and Hehong Chen and Guohai Xu and Ming Yan and Xing Gao and Jianhai Zhang and Chenliang Li and Jiayi Liu and Wenshen Xu and Haiyang Xu and Qi Qian and Wei Wang and Qinghao Ye and Jiejing Zhang and Ji Zhang and Fei Huang and Jingren Zhou},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2304.07849}
}
@misc{wolf2020huggingfaces,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang2023interactive,
  title   = {Interactive Natural Language Processing},
  author  = {Zekun Wang and Ge Zhang and Kexin Yang and Ning Shi and Wangchunshu Zhou and Shaochun Hao and Guangzheng Xiong and Yizhi Li and Mong Yuan Sim and Xiuying Chen and Qingqing Zhu and Zhenzhu Yang and Adam Nik and Qi Liu and Chenghua Lin and Shi Wang and Ruibo Liu and Wenhu Chen and Ke Xu and Dayiheng Liu and Yike Guo and Jie Fu},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.13246},
  url     = {https://arxiv.org/abs/2305.13246v1},
  pdf     = {https://arxiv.org/pdf/2305.13246.pdf}
}

@inproceedings{Chai2024mceval,
  title  = {MCEVAL: Massively Multilingual Code Evaluation},
  author = {Linzheng Chai and Shukai Liu and Jian Yang and Yuwei Yin and JinKe and Jiaheng Liu and Xianjie Wu and Tao Sun and Changyu Ren and Noah Wang and Bing Wang and Hongcheng Guo and Boyang Wang and Tongliang Li and Liqun Yang and Ge Zhang and Zhoujun Li and Sufeng Duan},
  year   = {2024},
  url    = {https://openreview.net/forum?id=um9CyJx3lS},
  pdf    = {https://openreview.net/pdf?id=um9CyJx3lS}
}

@article{execrepobench,
  title={Execrepobench: Multi-level executable code completion evaluation},
  author={Yang, Jian and Zhang, Jiajun and Yang, Jiaxi and Jin, Ke and Zhang, Lei and Peng, Qiyao and Deng, Ken and Miao, Yibo and Liu, Tianyu and Cui, Zeyu and others},
  journal={arXiv preprint arXiv:2412.11990},
  year={2024}
}

@article{codearena,
  title={Evaluating and aligning codellms on human preference},
  author={Yang, Jian and Yang, Jiaxi and Jin, Ke and Miao, Yibo and Zhang, Lei and Yang, Liqun and Cui, Zeyu and Zhang, Yichang and Hui, Binyuan and Lin, Junyang},
  journal={arXiv preprint arXiv:2412.05210},
  year={2024}
}

@article{wang2023rolellm,
  title   = {RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models},
  author  = {Zekun Moore Wang and Zhongyuan Peng and Haoran Que and Jiaheng Liu and Wangchunshu Zhou and Yuhan Wu and Hongcheng Guo and Ruitong Gan and Zehao Ni and Man Zhang and Zhaoxiang Zhang and Wanli Ouyang and Ke Xu and Wenhu Chen and Jie Fu and Junran Peng},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2310.00746},
  url     = {https://arxiv.org/abs/2310.00746v1},
  pdf     = {https://arxiv.org/pdf/2310.00746.pdf}
}

@article{rozière2023codellama,
  title   = {Code Llama: Open Foundation Models for Code},
  author  = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2308.12950}
}

@article{shao2024deepseekmath,
  title   = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models},
  author  = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2402.03300},
  url     = {https://arxiv.org/abs/2402.03300v3},
  pdf     = {https://arxiv.org/pdf/2402.03300.pdf}
}
@article{farn2023tooltalk,
  title={ToolTalk: Evaluating Tool-Usage in a Conversation Setting},
  author={Nicholas Farn and Richard Shin},
  year={2023},
  journal={arXiv preprint arXiv:2311.10775},
}
@misc{qin2023toolllm,
      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2307.16789},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{shinn2023reflexionlanguageagentsverbal,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2303.11366}, 
}


@misc{li2023cmmlu,
      title={CMMLU: Measuring massive multitask language understanding in Chinese}, 
      author={Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao and Yeyun Gong and Nan Duan and Timothy Baldwin},
      year={2023},
      eprint={2306.09212},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{hotpotqa,
      title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}, 
      author={Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
      year={2018},
      eprint={1809.09600},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{BigBench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}

@misc{alkhamissi2022review,
      title={A Review on Language Models as Knowledge Bases}, 
      author={Badr AlKhamissi and Millicent Li and Asli Celikyilmaz and Mona Diab and Marjan Ghazvininejad},
      year={2022},
      eprint={2204.06031},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{li2023halueval,
      title={HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}, 
      author={Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2305.11747},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{chern2023factool,
      title={FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios}, 
      author={I-Chun Chern and Steffi Chern and Shiqi Chen and Weizhe Yuan and Kehua Feng and Chunting Zhou and Junxian He and Graham Neubig and Pengfei Liu},
      year={2023},
      eprint={2307.13528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{TruthfulQA,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
}

@misc{BloombergGPT,
      title={BloombergGPT: A Large Language Model for Finance}, 
      author={Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and Prabhanjan Kambadur and David Rosenberg and Gideon Mann},
      year={2023},
      eprint={2303.17564},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ChatLaw,
      title={ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases}, 
      author={Jiaxi Cui and Zongjian Li and Yang Yan and Bohua Chen and Li Yuan},
      year={2023},
      eprint={2306.16092},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{WebGPT,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Hallucination_Survey,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
}


@inproceedings{kandpal2023large,
  title={Large language models struggle to learn long-tail knowledge},
  author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={15696--15707},
  year={2023},
  organization={PMLR}
}

@misc{gou2023critic,
      title={CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing}, 
      author={Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Nan Duan and Weizhu Chen},
      year={2023},
      eprint={2305.11738},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023journey,
      title={Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons}, 
      author={Yuheng Chen and Pengfei Cao and Yubo Chen and Kang Liu and Jun Zhao},
      year={2023},
      eprint={2308.13198},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pan2023unifying,
      title={Unifying Large Language Models and Knowledge Graphs: A Roadmap}, 
      author={Shirui Pan and Linhao Luo and Yufei Wang and Chen Chen and Jiapu Wang and Xindong Wu},
      year={2023},
      eprint={2306.08302},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yu2023generate,
  title={Generate rather than retrieve: Large language models are strong context generators},
  author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
  booktitle={International Conference for Learning Representation (ICLR)},
  year={2023}
}

@article{qu2024tool,
  title={Tool Learning with Large Language Models: A Survey},
  author={Qu, Changle and Dai, Sunhao and Wei, Xiaochi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Xu, Jun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2405.17935},
  year={2024}
}

@article{kong2023tptu,
  title={Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems},
  author={Kong, Yilun and Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Bao, Tianpeng and Shi, Shiwei and Du, Guoqing and Hu, Xiaoru and Mao, Hangyu and Li, Ziyue and others},
  journal={arXiv preprint arXiv:2311.11315},
  year={2023}
}
@article{Li2023CMMLUMM,
  title={CMMLU: Measuring massive multitask language understanding in Chinese},
  author={Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao and Yeyun Gong and Nan Duan and Tim Baldwin},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.09212},
  url={https://api.semanticscholar.org/CorpusID:259164635}
}

@misc{cai2024internlm2,
      title={InternLM2 Technical Report},
      author={InternLM2 Team},
      year={2024},
      eprint={2403.17297},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{baichuan2023baichuan2,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Baichuan},
  journal={arXiv preprint arXiv:2309.10305},
  url={https://arxiv.org/abs/2309.10305},
  year={2023}
}

@article{deepseek-llm,
  author = {DeepSeek-AI},
  title = {DeepSeek LLM: Scaling Open-Source Language Models with Longtermism},
  journal = {arXiv preprint arXiv:2401.02954},
  year = {2024},
  url = {https://github.com/deepseek-ai/DeepSeek-LLM}
}

@misc{ai2024yiopenfoundationmodels,
      title={Yi: Open Foundation Models by 01.AI}, 
      author={01. AI and : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Tao Yu and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.04652}, 
}

@misc{geminiteam2024gemini15unlockingmultimodal,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini Team},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05530}, 
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@inproceedings{Wei2024MeasuringSF,
  title={Measuring short-form factuality in large language models},
  author={Jason Wei and Nguyen Karina and Hyung Won Chung and Yunxin Joy Jiao and Spencer Papay and Amelia Glaese and John Schulman and William Fedus},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273877483}
}
@article{Li2024FromCD,
  title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline},
  author={Tianle Li and Wei-Lin Chiang and Evan Frick and Lisa Dunlap and Tianhao Wu and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.11939},
  url={https://api.semanticscholar.org/CorpusID:270562889}
}

@article{Zhao-2023-arxiv-survey,
  author       = {Wayne Xin Zhao and
                  Kun Zhou and
                  Junyi Li and
                  Tianyi Tang and
                  Xiaolei Wang and
                  Yupeng Hou and
                  Yingqian Min and
                  Beichen Zhang and
                  Junjie Zhang and
                  Zican Dong and
                  Yifan Du and
                  Chen Yang and
                  Yushuo Chen and
                  Zhipeng Chen and
                  Jinhao Jiang and
                  Ruiyang Ren and
                  Yifan Li and
                  Xinyu Tang and
                  Zikang Liu and
                  Peiyu Liu and
                  Jian{-}Yun Nie and
                  Ji{-}Rong Wen},
  title        = {A Survey of Large Language Models},
  volume       = {abs/2303.18223},
  year         = {2023},
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{Zhao-2023-arxiv-survey,
  author       = {Wayne Xin Zhao and
                  Kun Zhou and
                  Junyi Li and
                  Tianyi Tang and
                  Xiaolei Wang and
                  Yupeng Hou and
                  Yingqian Min and
                  Beichen Zhang and
                  Junjie Zhang and
                  Zican Dong and
                  Yifan Du and
                  Chen Yang and
                  Yushuo Chen and
                  Zhipeng Chen and
                  Jinhao Jiang and
                  Ruiyang Ren and
                  Yifan Li and
                  Xinyu Tang and
                  Zikang Liu and
                  Peiyu Liu and
                  Jian{-}Yun Nie and
                  Ji{-}Rong Wen},
  title        = {A Survey of Large Language Models},
  volume       = {abs/2303.18223},
  year         = {2023},
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{fu2024data,
  title={Data Engineering for Scaling Language Models to 128K Context},
  author={Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi, Hannaneh and Kim, Yoon and Peng, Hao},
  journal={arXiv preprint arXiv:2402.10171},
  year={2024}
}

@article{bai2024longalign,
  title={Longalign: A recipe for long context alignment of large language models},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi},
  journal={arXiv preprint arXiv:2401.18058},
  year={2024}
}

@article{mohtashami2023landmark,
  title={Landmark attention: Random-access infinite context length for transformers},
  author={Mohtashami, Amirkeivan and Jaggi, Martin},
  journal={arXiv preprint arXiv:2305.16300},
  year={2023}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{ratner2022parallel,
  title={Parallel context windows for large language models},
  author={Ratner, Nir and Levine, Yoav and Belinkov, Yonatan and Ram, Ori and Magar, Inbal and Abend, Omri and Karpas, Ehud and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal={arXiv preprint arXiv:2212.10947},
  year={2022}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{peng2023yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}
@article{wu2024long,
  title={Long Context Alignment with Short Instructions and Synthesized Positions},
  author={Wu, Wenhao and Wang, Yizhong and Fu, Yao and Yue, Xiang and Zhu, Dawei and Li, Sujian},
  journal={arXiv preprint arXiv:2405.03939},
  year={2024}
}

@article{ding2024longrope,
  title={LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@article{chen2023longlora,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}

@article{zhu2023pose,
  title={Pose: Efficient context window extension of llms via positional skip-wise training},
  author={Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},
  journal={arXiv preprint arXiv:2309.10400},
  year={2023}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{wu2020memformer,
  title={Memformer: A memory-augmented transformer for sequence modeling},
  author={Wu, Qingyang and Lan, Zhenzhong and Qian, Kun and Gu, Jing and Geramifard, Alborz and Yu, Zhou},
  journal={arXiv preprint arXiv:2010.06891},
  year={2020}
}

@article{munkhdalai2024leave,
  title={Leave no context behind: Efficient infinite context transformers with infini-attention},
  author={Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal={arXiv preprint arXiv:2404.07143},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{chen2017reading,
  title={Reading wikipedia to answer open-domain questions},
  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  journal={arXiv preprint arXiv:1704.00051},
  year={2017}
}

@article{izacard2020leveraging,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2020}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{qin2023webcpm,
  title={Webcpm: Interactive web search for chinese long-form question answering},
  author={Qin, Yujia and Cai, Zihan and Jin, Dian and Yan, Lan and Liang, Shihao and Zhu, Kunlun and Lin, Yankai and Han, Xu and Ding, Ning and Wang, Huadong and others},
  journal={arXiv preprint arXiv:2305.06849},
  year={2023}
}

@article{khandelwal2019generalization,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:1911.00172},
  year={2019}
}

@article{fevry2020entities,
  title={Entities as experts: Sparse memory access with entity supervision},
  author={F{\'e}vry, Thibault and Soares, Livio Baldini and FitzGerald, Nicholas and Choi, Eunsol and Kwiatkowski, Tom},
  journal={arXiv preprint arXiv:2004.07202},
  year={2020}
}

@article{de2021mention,
  title={Mention memory: incorporating textual knowledge into transformers through entity mention attention},
  author={De Jong, Michiel and Zemlyanskiy, Yury and FitzGerald, Nicholas and Sha, Fei and Cohen, William},
  journal={arXiv preprint arXiv:2110.06176},
  year={2021}
}

@article{lee2024human,
  title={A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts},
  author={Lee, Kuang-Huei and Chen, Xinyun and Furuta, Hiroki and Canny, John and Fischer, Ian},
  journal={arXiv preprint arXiv:2402.09727},
  year={2024}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{khattab2020colbert,
  title={Colbert: Efficient and effective passage search via contextualized late interaction over bert},
  author={Khattab, Omar and Zaharia, Matei},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={39--48},
  year={2020}
}

@article{sachan2023questions,
  title={Questions are all you need to train a dense passage retriever},
  author={Sachan, Devendra Singh and Lewis, Mike and Yogatama, Dani and Zettlemoyer, Luke and Pineau, Joelle and Zaheer, Manzil},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={600--616},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{sun2021long,
  title={Do long-range language models actually use long-range context?},
  author={Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2109.09115},
  year={2021}
}

@article{yu2022generate,
  title={Generate rather than retrieve: Large language models are strong context generators},
  author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
  journal={arXiv preprint arXiv:2209.10063},
  year={2022}
}

@software{LlamaIndex,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
url = {https://github.com/jerryjliu/llama_index},
year = {2024}
}

@software{LangChain,
author = {LangChain-team},
title = {{LangChain}},
url = {https://github.com/langchain-ai/langchain},
year = {2024}
}

@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--22},
  year={2023}
}
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}


@article{sarthi2024raptor,
  title={RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval},
  author={Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D},
  journal={arXiv preprint arXiv:2401.18059},
  year={2024}
}

@article{chen2023walking,
  title={Walking down the memory maze: Beyond context limit through interactive reading},
  author={Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli},
  journal={arXiv preprint arXiv:2310.05029},
  year={2023}
}

@inproceedings{wang2024knowledge,
  title={Knowledge graph prompting for multi-document question answering},
  author={Wang, Yu and Lipka, Nedim and Rossi, Ryan A and Siu, Alexa and Zhang, Ruiyi and Derr, Tyler},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={19206--19214},
  year={2024}
}

@article{Bai2023LongBenchAB,
  title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
  author={Yushi Bai and Xin Lv and Jiajie Zhang and Hong Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.14508},
  url={https://api.semanticscholar.org/CorpusID:261245264}
}

@article{Yuan2024LVEvalAB,
  title={LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K},
  author={Tao Yuan and Xuefei Ning and Dong Zhou and Zhijie Yang and Shiyao Li and Minghui Zhuang and Zheyue Tan and Zhuyu Yao and Dahua Lin and Boxun Li and Guohao Dai and Shengen Yan and Yu Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.05136},
  url={https://api.semanticscholar.org/CorpusID:267547607}
}

@article{Robertson2009ThePR,
  title={The Probabilistic Relevance Framework: BM25 and Beyond},
  author={Stephen E. Robertson and Hugo Zaragoza},
  journal={Found. Trends Inf. Retr.},
  year={2009},
  volume={3},
  pages={333-389},
  url={https://api.semanticscholar.org/CorpusID:207178704}
}

@article{yang2022re3,
  title={Re3: Generating longer stories with recursive reprompting and revision},
  author={Yang, Kevin and Tian, Yuandong and Peng, Nanyun and Klein, Dan},
  journal={arXiv preprint arXiv:2210.06774},
  year={2022}
}

@inproceedings{lu2023instag,
  title={\# InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models},
  author={Lu, Keming and Yuan, Hongyi and Yuan, Zheng and Lin, Runji and Lin, Junyang and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{sun2023think,
  title={Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph},
  author={Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Shum, Heung-Yeung and Guo, Jian},
  journal={arXiv preprint arXiv:2307.07697},
  year={2023}
}

@article{mavromatis2024gnn,
  title={GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning},
  author={Mavromatis, Costas and Karypis, George},
  journal={arXiv preprint arXiv:2405.20139},
  year={2024}
}

@article{edge2024local,
  title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
  author={Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  journal={arXiv preprint arXiv:2404.16130},
  year={2024}
}

@article{luo2023reasoning,
  title={Reasoning on graphs: Faithful and interpretable large language model reasoning},
  author={Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
  journal={arXiv preprint arXiv:2310.01061},
  year={2023}
}

@inproceedings{li2023chain,
  title={Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources},
  author={Li, Xingxuan and Zhao, Ruochen and Chia, Yew Ken and Ding, Bosheng and Joty, Shafiq and Poria, Soujanya and Bing, Lidong},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{yang-2018-emnlp-hotpotqa,
  author       = {Zhilin Yang and
                  Peng Qi and
                  Saizheng Zhang and
                  Yoshua Bengio and
                  William W. Cohen and
                  Ruslan Salakhutdinov and
                  Christopher D. Manning},
  title        = {HotpotQA: {A} Dataset for Diverse, Explainable Multi-hop Question
                  Answering},
  booktitle    = {{EMNLP}},
  pages        = {2369--2380},
  publisher    = {Association for Computational Linguistics},
  year         = {2018}
}

@inproceedings{ho-2020-coling-2wikimultihopQA,
  author       = {Xanh Ho and
                  Anh{-}Khoa Duong Nguyen and
                  Saku Sugawara and
                  Akiko Aizawa},
  title        = {Constructing {A} Multi-hop {QA} Dataset for Comprehensive Evaluation
                  of Reasoning Steps},
  booktitle    = {{COLING}},
  pages        = {6609--6625},
  publisher    = {International Committee on Computational Linguistics},
  year         = {2020}
}

@article{Trivedi-2022-acltrans-musique,
  author       = {Harsh Trivedi and
                  Niranjan Balasubramanian and
                  Tushar Khot and
                  Ashish Sabharwal},
  title        = {MuSiQue: Multihop Questions via Single-hop Question
                  Composition},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {10},
  pages        = {539--554},
  year         = {2022}
}

@article{Kocisky-2018-acltrans-narrativeqa,
  author       = {Tom{\'{a}}s Kocisk{\'{y}} and
                  Jonathan Schwarz and
                  Phil Blunsom and
                  Chris Dyer and
                  Karl Moritz Hermann and
                  G{\'{a}}bor Melis and
                  Edward Grefenstette},
  title        = {The NarrativeQA Reading Comprehension Challenge},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {6},
  pages        = {317--328},
  year         = {2018}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{bai2024mt,
  title={MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues},
  author={Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others},
  journal={arXiv preprint arXiv:2402.14762},
  year={2024}
}

@article{cheng2024xformparser,
  title={XFormParser: A Simple and Effective Multimodal Multilingual Semi-structured Form Parser},
  author={Cheng, Xianfu and Zhang, Hang and Yang, Jian and Li, Xiang and Zhou, Weixiao and Wu, Kui and Liu, Fei and Zhang, Wei and Sun, Tao and Li, Tongliang and others},
  journal={arXiv preprint arXiv:2405.17336},
  year={2024}
}

@article{wu2024conceptmath,
  title={ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models},
  author={Wu, Yanan and Liu, Jie and Bu, Xingyuan and Liu, Jiaheng and Zhou, Zhanhui and Zhang, Yuanxing and Zhang, Chenchen and Bai, Zhiqi and Chen, Haibin and Ge, Tiezheng and others},
  journal={arXiv preprint arXiv:2402.14660},
  year={2024}
}
@article{liu2024iterative,
    title = {Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level},
    author = {Liu, Jie and Zhou, Zhanhui and Liu, Jiaheng and Bu, Xingyuan and Yang, Chao and Zhong Han-Sen and Ouyang, Wanli},
    journal={arXiv preprint arXiv:2406.11817},
    year={2024}
}
@article{feng2022beyond,
  title={Beyond bounding box: Multimodal knowledge learning for object detection},
  author={Feng, Weixin and Bu, Xingyuan and Zhang, Chenchen and Li, Xubin},
  journal={arXiv preprint arXiv:2205.04072},
  year={2022}
}
@inproceedings{peng2020large,
  title={Large-scale object detection in the wild from imbalanced multi-labels},
  author={Peng, Junran and Bu, Xingyuan and Sun, Ming and Zhang, Zhaoxiang and Tan, Tieniu and Yan, Junjie},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9709--9718},
  year={2020}
}
@inproceedings{xv2022visual,
  title={Visual Encoding and Debiasing for CTR Prediction},
  author={Xv, Guipeng and Chen, Si and Lin, Chen and Guan, Wanxian and Bu, Xingyuan and Li, Xubin and Deng, Hongbo and Xu, Jian and Zheng, Bo},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={4615--4619},
  year={2022}
}
@article{peng2023gaia,
  title={GAIA-Universe: Everything is Super-Netify},
  author={Peng, Junran and Chang, Qing and Yin, Haoran and Bu, Xingyuan and Sun, Jiajun and Xie, Lingxi and Zhang, Xiaopeng and Tian, Qi and Zhang, Zhaoxiang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={10},
  pages={11856--11868},
  year={2023},
  publisher={IEEE}
}
@inproceedings{bu2021gaia,
  title={Gaia: A transfer learning system of object detection that fits your needs},
  author={Bu, Xingyuan and Peng, Junran and Yan, Junjie and Tan, Tieniu and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={274--283},
  year={2021}
}

@inproceedings{pang-etal-2022-quality,
    title = "{Q}u{ALITY}: Question Answering with Long Input Texts, Yes!",
    author = "Pang, Richard Yuanzhe  and
      Parrish, Alicia  and
      Joshi, Nitish  and
      Nangia, Nikita  and
      Phang, Jason  and
      Chen, Angelica  and
      Padmakumar, Vishakh  and
      Ma, Johnny  and
      Thompson, Jana  and
      He, He  and
      Bowman, Samuel",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.391",
    doi = "10.18653/v1/2022.naacl-main.391",
    pages = "5336--5358",
    abstract = "To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Our baseline models perform poorly on this task (55.4{\%}) and significantly lag behind human performance (93.5{\%}).",
}

@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@inproceedings{sun-etal-2024-pearl,
    title = "{PEARL}: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
    author = "Sun, Simeng  and
      Liu, Yang  and
      Wang, Shuohang  and
      Iter, Dan  and
      Zhu, Chenguang  and
      Iyyer, Mohit",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.29",
    pages = "469--486",
    abstract = "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND{\_}EVENT, FIND{\_}RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.",
}
@article{li2024graphreader,
  title={GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models},
  author={Li, Shilong and He, Yancheng and Guo, Hangyu and Bu, Xingyuan and Bai, Ge and Liu, Jie and Liu, Jiaheng and Qu, Xingwei and Li, Yangguang and Ouyang, Wanli and others},
  journal={arXiv preprint arXiv:2406.14550},
  year={2024}
}

@inproceedings{zhong-etal-2024-agieval,
    title = "{AGIE}val: A Human-Centric Benchmark for Evaluating Foundation Models",
    author = "Zhong, Wanjun  and
      Cui, Ruixiang  and
      Guo, Yiduo  and
      Liang, Yaobo  and
      Lu, Shuai  and
      Wang, Yanlin  and
      Saied, Amin  and
      Chen, Weizhu  and
      Duan, Nan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.149",
    doi = "10.18653/v1/2024.findings-naacl.149",
    pages = "2299--2314",
    abstract = "Assessing foundation models{'} abilities for human-level tasks is crucial for Artificial General Intelligence (AGI) development.Traditional benchmarks, which rely on artificial datasets, may not accurately represent these capabilities. In this paper, we introduce AGIEval, a novel bilingual benchmark designed to assess foundation models in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models on our benchmark. Impressively, we show that GPT-4 exceeds the average human performance in SAT, LSAT, and math contests, with 95{\%} accuracy on SAT Math and 92.5{\%} on the Chinese college entrance English exam. This demonstrates the exceptional performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks requiring complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal their strengths and limitations, providing valuable insights into future directions for enhancing general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a meaningful and robust evaluation of foundation models{'} performance in real-world scenarios.",
}

@article{li20242d,
  title={2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision},
  author={Li, Shilong and He, Yancheng and Huang, Hui and Bu, Xingyuan and Liu, Jiaheng and Guo, Hangyu and Wang, Weixun and Gu, Jihao and Su, Wenbo and Zheng, Bo},
  journal={arXiv preprint arXiv:2410.19720},
  year={2024}
}

@article{bai2024mt,
  title={Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues},
  author={Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others},
  journal={arXiv preprint arXiv:2402.14762},
  year={2024}
}



@article{graphrag,
  title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
  author={Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.16130},
  url={https://api.semanticscholar.org/CorpusID:269363075}
}

@article{Jiang2024LongRAGER,
  title={LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs},
  author={Ziyan Jiang and Xueguang Ma and Wenhu Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.15319},
  url={https://api.semanticscholar.org/CorpusID:270688725}
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",
}

@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
    abstract = "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
}

@inproceedings{li-etal-2023-halueval,
    title = "{H}alu{E}val: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    author = "Li, Junyi  and
      Cheng, Xiaoxue  and
      Zhao, Xin  and
      Nie, Jian-Yun  and
      Wen, Ji-Rong",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.397",
    doi = "10.18653/v1/2023.emnlp-main.397",
    pages = "6449--6464",
    abstract = "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5{\%} user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.",
}

@article{chern2023factool2,
  title={FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},
  author={Chern, I-Chun and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei and others},
  journal={arXiv preprint arXiv:2307.13528},
  year={2023}
}


@article{chinese_simpleqa,
  title={Chinese simpleqa: A chinese factuality evaluation for large language models},
  author={He, Yancheng and Li, Shilong and Liu, Jiaheng and Tan, Yingshui and Wang, Weixun and Huang, Hui and Bu, Xingyuan and Guo, Hangyu and Hu, Chengwei and Zheng, Boren and others},
  journal={arXiv preprint arXiv:2411.07140},
  year={2024}
}

@article{simpleqa,
  title={Measuring short-form factuality in large language models. 2024},
  author={Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
  journal={URL https://api. semanticscholar. org/CorpusID},
  volume={273877483}
}

@article{comprehensive_hallucination,
  title={A comprehensive survey of hallucination mitigation techniques in large language models},
  author={Tonmoy, SM and Zaman, SM and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  journal={arXiv preprint arXiv:2401.01313},
  year={2024}
}

@article{chinese_hallucination,
  title={Evaluating hallucinations in chinese large language models},
  author={Cheng, Qinyuan and Sun, Tianxiang and Zhang, Wenwei and Wang, Siyin and Liu, Xiangyang and Zhang, Mozhi and He, Junliang and Huang, Mianqiu and Yin, Zhangyue and Chen, Kai and others},
  journal={arXiv preprint arXiv:2310.03368},
  year={2023}
}


@article{hallucination_aiocean,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={European conference on computer vision},
  pages={216--233},
  year={2024},
  organization={Springer}
}

@inproceedings{li2024seed,
  title={SEED-Bench: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13299--13308},
  year={2024}
}

@article{yu2023mm,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@article{zou2024dynamath,
  title={Dynamath: A dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models},
  author={Zou, Chengke and Guo, Xingang and Yang, Rui and Zhang, Junyu and Hu, Bin and Zhang, Huan},
  journal={arXiv preprint arXiv:2411.00836},
  year={2024}
}

@article{yang2023baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{he2024chinese,
  title={Chinese simpleqa: A chinese factuality evaluation for large language models},
  author={He, Yancheng and Li, Shilong and Liu, Jiaheng and Tan, Yingshui and Wang, Weixun and Huang, Hui and Bu, Xingyuan and Guo, Hangyu and Hu, Chengwei and Zheng, Boren and others},
  journal={arXiv preprint arXiv:2411.07140},
  year={2024}
}


@article{yue2024mmmupro,
  title={Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark},
  author={Yue, Xiang and Zheng, Tianyu and Ni, Yuansheng and Wang, Yubo and Zhang, Kai and Tong, Shengbang and Sun, Yuxuan and Yu, Botao and Zhang, Ge and Sun, Huan and others},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@inproceedings{cheng2024sviptr,
  title={SVIPTR: Fast and Efficient Scene Text Recognition with Vision Permutable Extractor},
  author={Cheng, Xianfu and Zhou, Weixiao and Li, Xiang and Yang, Jian and Zhang, Hang and Sun, Tao and Zhang, Wei and Mai, Yuying and Li, Tongliang and Chen, Xiaoming and others},
  booktitle={Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
  pages={365--373},
  year={2024}
}

@article{li2025llava,
  title={LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding},
  author={Li, Hongyu and Chen, Jinyu and Wei, Ziyu and Huang, Shaofei and Hui, Tianrui and Gao, Jialin and Wei, Xiaoming and Liu, Si},
  journal={arXiv preprint arXiv:2501.08282},
  year={2025}
}