\section{Related Works}
\noindent\textbf{Multimodal Benchmarks.}
Recent vision-language benchmarks have been developed to assess models' capabilities in integrating visual and textual information across various tasks~\citep{wu2024scimmir,wu2024mmra,Zhang2024CMMMUAC}, including OCR~\citep{cheng2024sviptr}, spatial awareness~\citep{li2025llava}, multimodal information retrieval~\citep{cheng2024xformparser}, and reasoning skills.
For example, MMBench \citep{liu2023mmbench} employs multiple-choice tasks in both Chinese and English, covering a wide range of domains.
MMMU \citep{yue2024mmmu} focuses on complex vision-language tasks, particularly those requiring advanced multimodal reasoning.
MMStar \citep{chen2024we} utilizes multi-task evaluations to test models' ability to fuse different modalities.
% MM-Vet~\citep{yu2023mm} focuses on visual question answering (VQA), requiring models to interpret visual data and respond to queries. MMBench~\citep{liu2023mmbench} evaluates models via multiple-choice tasks in both Chinese and English, covering diverse domains. MMStar~\citep{chen2024we} conducts multi-task evaluations to test multimodal fusion capabilities. MMMU~\citep{yue2024mmmu} and CMMMU~\citep{Zhang2024CMMMUAC} assess model performance on complex vision-language tasks, emphasizing sophisticated multimodal reasoning.
% MMRA~\citep{wu2024mmra} is designed to evaluate the models' multi-image relational association capability.
% In addition, there are several audio-understanding benchmarks. Aishell1~\citep{bu2017aishell}, Aishell2~\citep{du2018aishell}, and Librispeech~\citep{panayotov2015librispeech} are designed for automatic speech recognition, while ClothoAQA targets audio QA tasks. For automatic audio captioning and vocal sound classification, researchers have curated Clotho~\citep{drossos2020clotho} and VocalSound~\citep{gong2022vocalsound}.
% However, there is a significant lack of comprehensive understanding benchmarks to assess MLLMs' ability to simultaneously process complementary information from the  textual, audio, and visual inputs.

%  Factuality is the capability of large language models to produce contents that follow factual content, including commonsense, world knowledge, and domain facts,
% and the factual content can be substantiated by authoritative sources (e.g., Wikipedia, Textbooks).
% % The factual information can be grounded to reliable sources, such as dictionaries, Wikipedia or textbooks from different domains.
% Recent works have explored the potential of LLMs to serve as factual knowledge bases~\cite{yu2023generate,pan2023unifying}.
% Specifically,
% existing studies have primarily focused on qualitative assessments of LLM factuality~\citep{TruthfulQA,chern2023factool}, investigations into knowledge storage mechanisms \citep{meng2022locating,chen2023journey}, and analyses on knowledge-related issues~\citep{gou2023critic}. 
% Among these areas, the question of factuality in LLMs has garnered significant attention.
% Existing works focus on measuring factuality in LLMs qualitatively \citep{TruthfulQA,chern2023factool}, discussing the mechanism for storing knowledge \citep{meng2022locating,chen2023journey} and tracing the source of knowledge issues \citep{gou2023critic,kandpal2023large}. The factuality issue for LLMs receive relatively the most attention.
% For instance, an LLM might be deficient in domain-specific factual knowledge, such as medicine or law domain. Additionally, the LLM might be unaware of facts that occurred post its last update. There are also instances where the LLM, despite possessing the relevant facts, fails to reason out the correct answer. In some cases, it might even forget or be unable to recall facts it has previously learned.
% The factuality problem is closely related to several hot topics in the field of Large Language Models, including {Hallucinations} \citep{Hallucination_Survey}, {Outdated Information} \citep{WebGPT}, and {Domain-Specificity} (e.g., Law \citep{ChatLaw}, Finance \citep{BloombergGPT}). 



\noindent\textbf{Factuality Benchmarks.}
% There are many 
Factuality refers to their ability to generate content that follow facts, including commonsense, world knowledge, and domain-specific information. This capability is typically assessed by comparing model outputs to authoritative sources such as Wikipedia or academic textbooks.
Recently,
Various benchmarks have been developed to evaluate factuality in LLMs~\citep{zhong2023agieval,huang2023ceval,li2023cmmlu,BigBench,hotpotqa,TruthfulQA,codearena,execrepobench,tan2024chinesesafetyqasafetyshortform}.
For example,
MMLU \citep{mmlu} assesses multitask accuracy across 57 diverse tasks.
HaluEval \citep{li2023halueval} explores the propensity of LLMs to produce hallucinations or false information.
 SimpleQA~\citep{Wei2024MeasuringSF} and Chinese SimpleQA~\citep{he2024chinesesimpleqachinesefactuality} have been proposed to measure the short-form factuality in LLMs.
% SimpleQA \citep{Wei2024MeasuringSF} and its Chinese counterpart \citep{he2024chinesesimpleqachinesefactuality} focus on measuring short-form factuality.

%  Factuality is the capability of large language models to produce contents that follow factual content, including commonsense, world knowledge, and domain facts,
% and the factual content can be substantiated by authoritative sources (e.g., Wikipedia, Textbooks).
% Many factuality benchmarks have been proposed.
% For example,
% MMLU ~\citep{mmlu} is to measure the multitask accuracies on a diverse set of 57 tasks.
% Additionally,
% HaluEval \citep{li2023halueval}
% is to examine the tendency of LLMs to produce hallucinations.
% Recently, SimpleQA~\citep{Wei2024MeasuringSF} and Chinese SimpleQA~\citep{he2024chinesesimpleqachinesefactuality} have been proposed to measure the short-form factuality in LLMs.
% However, SimpleQA only focuses on the English domain. In contrast, our Chinese SimpleQA aims to comprehensively evaluate factuality in Chinese.