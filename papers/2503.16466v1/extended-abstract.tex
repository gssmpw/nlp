\documentclass{sigchi-ext}
% Please be sure that you have the dependencies (i.e., additional
% LaTeX packages) to compile this example.
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[scaled=.92]{helvet} % for proper fonts
\usepackage{graphicx} % for EPS use the graphics package instead
\usepackage{balance}  % for useful for balancing the last columns
\usepackage{booktabs} % for pretty table rules
\usepackage{ccicons}  % for Creative Commons citation icons
\usepackage{ragged2e} % for tighter hyphenation

% Some optional stuff you might like/need.
% \usepackage{marginnote} 
% \usepackage[shortlabels]{enumitem}
% \usepackage{paralist}
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP --
% \copyrightinfo{Permission to make digital or hard copies of all or
% part of this work for personal or classroom use is granted without
% fee provided that copies are not made or distributed for profit or
% commercial advantage and that copies bear this notice and the full
% citation on the first page. Copyrights for components of this work
% owned by others than ACM must be honored. Abstracting with credit is
% permitted. To copy otherwise, or republish, to post on servers or to
% redistribute to lists, requires prior specific permission and/or a
% fee. Request permissions from permissions@acm.org.\\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{SIGCHI Extended Abstracts Sample File: Note Initial
  Caps} \def\plainauthor{Elizabeth Watkins, Ramesh Manuvinakurike, Richard Beckwith,
  Emanuel Moss, Meng Shi, Giuseppe Raffa}
\def\emptyauthor{}
\def\plainkeywords{Authors' choice; of terms; separated; by
  semicolons; include commas, within terms only; required.}
\def\plaingeneralterms{Documentation, Standardization}

\title{ACE, Action and Control via Explanations: A Proposal for LLMs to Provide Human-Centered Explainability for Multimodal AI Assistants}

\numberofauthors{6}
% Notice how author names are alternately typesetted to appear ordered
% in 2-column format; i.e., the first 4 autors on the first column and
% the other 4 auhors on the second column. Actually, it's up to you to
% strictly adhere to this author notation.
\author{%
  \alignauthor{%
    \textbf{Elizabeth Anne Watkins}\\
    \affaddr{Intel Labs} \\
    \affaddr{Santa Clara, CA 95054, USA} \\
    \email{Elizabeth.Watkins@intel.com} }\alignauthor{%
    \textbf{Emanuel Moss}\\
    \affaddr{Intel Labs} \\
    \affaddr{Santa Clara, CA 95054, USA} \\
    \email{Emanuel.Moss@intel.com} } \vfil \alignauthor{%
    \textbf{Ramesh Manuvinakurike}\\
    \affaddr{Intel Labs}\\
    \affaddr{Hillsboro, OR 97124, USA}\\
    %\affaddr{Awdur SA22 8PP, UK}\\
    \email{Ramesh.Manuvinakurike
    @intel.com} }\alignauthor{%
    \textbf{Meng Shi}\\
    \affaddr{Intel Labs}\\
    \affaddr{Santa Clara, CA 95054, USA}\\
    \email{Meng.Shi@intel.com} } \vfil \alignauthor{%
    \textbf{Richard Beckwith}\\
    %\textbf{Fourth Author}\\    
    \affaddr{Intel Labs}\\
    \affaddr{Hillsboro, OR 97124, USA}\\
    \email{Richard.Beckwith@intel.com} \\
    %\email{author4@hchi.anotherco.com} 
    }
    \alignauthor{%
    \textbf{Giuseppe Raffa}\\
    \affaddr{Intel Labs}\\
    \affaddr{San Diego, CA 92109, USA}\\
    %\affaddr{Cape Town, South Africa}\\
    \email{Giuseppe.Raffa@intel.com} } }

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% \reversemarginpar%

\begin{document}

%% For the camera ready, use the commands provided by the ACM in the Permission Release Form.
\CopyrightYear{2024}
\setcopyright{rightsretained}
\conferenceinfo{CHI Workshop on Human-Centered Explainable AI '24,}{May 2024, HI, USA}
\isbn{978-1-4503-6819-3/20/04}
\doi{https://doi.org/10.1145/3334480.XXXXXXX}
%% Then override the default copyright message with the \acmcopyright command.
\copyrightinfo{\acmcopyright}


\maketitle

% Uncomment to disable hyphenation (not recommended)
% https://twitter.com/anjirokhan/status/546046683331973120
\RaggedRight{} 

% Do not change the page size or page settings.
\begin{abstract}
  %UPDATED---\today. 
  In this short paper we address issues related to building multimodal AI systems for human performance support in manufacturing domains. We make two contributions: we first identify challenges of participatory design and training of such systems, and secondly, to address such challenges, we propose the ACE paradigm: "Action and Control via Explanations". Specifically, we suggest that LLMs can be used to produce explanations in the form of human interpretable "semantic frames", which in turn enable end users to provide data the AI system needs to align its multimodal models and representations, including computer vision, automatic speech recognition, and document inputs. ACE, by using LLMs to "explain" using semantic frames, will help the human and the AI system to collaborate, together building a more accurate model of humans activities and behaviors, and ultimately more accurate predictive outputs for better task support, and better outcomes for human users performing manual tasks.
\end{abstract}

%\keywords{\plainkeywords}

% ACM Classfication

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121</concept_id>
<concept_desc>Human-centered computing~Human computer interaction (HCI)</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003125.10011752</concept_id>
<concept_desc>Human-centered computing~Haptic devices</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003122.10003334</concept_id>
<concept_desc>Human-centered computing~User studies</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Human computer interaction (HCI)}
\ccsdesc[100]{Human-centered computing~User studies}


%Print the classficiation codes
%\printccsdesc
%Please use the 2012 Classifiers and see this link to embed them in the text: \url{https://dl.acm.org/ccs/ccs_flat.cfm}


\section{Introduction}
This short position paper proposes a research
agenda examining how LLMs intersect with explainability in the participatory design of a multimodal AI system to support task performance in manufacturing, a highly specific and specialized system, domain, and context.  We examine what kinds of problems can be addressed by asking an LLM to "explain" the system, and %itself, and %it means to ask an LLM to "explain" itself in a when implemented into a multimodal system to support a task performance in a manufacturing context.  
propose that to best support both end-users and developers who are collaborating to build this system, explanations must go beyond developer-focused technical information about how the system operates, and toward an end-user focused, "mechanistic" or "functional" explanation \cite{liao2023ai} that supports humans' practical, goal-oriented use of a system.
%and their pursuit of useful outputs 
%When applied to LLMs in the context of multimodal learning for performance support, this proposal suggests that the best support for humans may look like suggesting they take a step away from natural language. This proposal goes against the tide of LLM's typical value-add, which purports to utilize naturalistic conversation to enable human-AI interaction. 



In this position paper, we first describe the industrial setting in which this development is taking place and a challenge we have observed in human-in-the-loop, participatory training of a multimodal AI system. We then describe findings in prior research and our empirical observations of end-users/collaborators as they interacted with an AI assistant, %End-users/collaborators interacted with a multimodal system that did not incorporate LLMs, 
and described what explanatory information they would like to receive %from an explanation interface 
as they provided training to the system. We apply insights drawn from these observations to the era of LLMs. %and what a system integrating LLMs with other modes, modules, and explanations might look like. 

Specifically, in the context of multimodal learning for performance support, this proposal suggests prompting users, via explanations, to deploy %less `naturalistic' language in favor of 
a specific linguistic conversational format of ``semantic frames." This format, we suggest, better supports human interaction with -- and training of -- the AI assistant. Ultimately,  this may be a more useful form of explanation than commonplace wisdom in XAI, which tends to focus on providing system explanations,  might otherwise suggest. We term this proposed system ACE: Action and Control via Explanations. 

\section{XAI and Human-AI Collaboration}

Explanations are highly context-specific. They are units of information that cannot stand on their own but are given meaning through the interactions between user and system (and by extension, developers), and the context in which that interaction takes place~\cite{dourish2004we}.  Their effectiveness depends on \textit{what} is being explained \textit{to whom} \cite{ehsan2021explainable} for \textit{what purpose}.

Prior work on explainable AI has identified that different stakeholders have different "interests, goals, expectations, needs, and demands" \cite{langer2021we} for AI systems. End users who are not AI experts have described that their interests are not in better "understanding" precisely how a system functions. Rather, end users ask for information that can help them become better "collaborators" with these systems, i.e. for guidance around how to change their behaviors to provide better inputs to AI systems in order to obtain better or more useful, more accurate outputs \cite{kim2023help}. %Recognizing the creator-consumer gap is critical, because the early days of the explainable AI field positioned developers as the primary consumers of explanations
As AI systems come to play a larger role in people's lives, it is critically important that human-centered explanations support human-AI interaction and enable successful outcomes across deployment domains. 

\section{Setting}
%\cite{zhao2023exploring}

%Explainability for what / for whom / contextual information 
%- THIS is the thing over which users should be able to have control -- the provisional understanding 

This project centers around an in-development system called MARIE (\textbf{M}ultimodal \textbf{A}ctivity \textbf{R}ecognition in an \textbf{I}ndustrial \textbf{E}nvironment). This first implementation of MARIE is intended to provide performance support to end users who  perform intricate cleaning tasks inside our manufacturing facilities. %These facilities are known as "clean rooms" as the chip-manufacturing must be as clean as possible. 
The stakes of this work are high, as any debris left on manufacturing components can have detrimental and costly consequences. MARIE is intended to help technicians ensure their cleaning processes are complete, to reduce losses due to debris on parts. MARIE recognizes where a technician is in their process and offers tips, reminders, and guidance, supporting training for novices and providing support for experts. 

MARIE's development process is intended to address a known bottle-neck in AI development: data representing this kind of proprietary, highly specific information is difficult to collect and label. MARIE's solution is to use participatory design and engage domain-expert end-users as collaborators. These experts in the cleaning process provide crucial data to ``teach" MARIE about what they do via demonstration and concurrent conversational exchanges with the system.  
%by talking out loud in "thinkaloud" process as they work. 
Their voice is captured by microphones and processed using automatic speech recognition (ASR), and the visual context is captured by cameras and processed using computer vision (CV) models such as action recognition and object recognition. These inputs are combined with previously analyzed in-domain process information, e.g., "spec" documents containing step-by-step instructions. %that facility leadership has written about how a process should be performed. 
%Together, this multimodal system combines speech, images, and text to "train" MARIE in the best way to clean parts. 
MARIE brings together all of these inputs to attain understanding of the cleaning process and provide performance support to both novices for training, as well as expert technicians as they juggle tasks in the facility. 

\section{Contributions: A Challenge of Multimodal 
\\Systems, and a Proposal for Structuring LLM 
\\Explanations}

\begin{marginfigure}[-5pc]
  \begin{minipage}{\marginparwidth}
    \centering
    \includegraphics[width=0.9\marginparwidth]{figures/ace_llm.png}
    \caption{Shows the LLM-based architecture of the system. The system consumes inputs from multimodal }~\label{fig:marginfig}
  \end{minipage}
\end{marginfigure}

%In this workshop paper we make two primary contributions. First, we identify a challenge for multimodal AI systems deployed for performance support in a manufacturing setting. Second, we propose ACE: Action and Control Via Explanations, to conceptualize explanations as a mode of support for human-AI collaboration. This takes a meaningful step beyond the normative approach of XAI, which is focused on improving cognitive understanding of end-users interacting with a system.
%Marking that my readthrough ends here until Friday [-Manny-]



\textbf{A Challenge of Multimodal Systems} A key component of MARIE's ``training" process, and what will enable MARIE to understand human actions well enough to provide accurate tips and guidance for performance support, is that MARIE must be able to align predictions made using disparate modes of information input. These inputs include vision data, audio data, and written data. Here a  challenge emerges: MARIE must ``understand" what activity (as ``seen" by the vision models, including action recognition and object recognition) is being performed, and align these vision model predictions with ASR predictions and data from the spec. This challenge can be observed in several mismatches between the naturalistic way that humans train MARIE through conversational exchange, and the information that MARIE needs to appropriately infer actions from multimodal data:
%needed for speech processes 

%and object recognition models 
%This is where we propose that explainability can perform more than one function and help to reconcile a setting where multiple desiderata need to be satisfied: the users' needs for explanation, and the developers' needs for data labels. 

\textit{Missing information/clarification:} technicians commonly describe their actions in ways that are missing context --- context which MARIE needs. For example, an utterance like ``I'm cleaning this part" %needle valves" 
is missing key information about tools. Not only does the generic noun fail to specifcy the part being cleaned but also there is no indication of what the part is being cleaned with. Semantic parsing of the utterance can tell us that the ``action" being performed is  (clean) but not the item affected by the action-- the ``receiver", we know only (part). %needle valve), nor whegther a ``tool" is being used for the action. Other potentially relevant action details (e.g., is a tool being used, in what manner, etc.) are also missing. 

%Misalignment: E.g., Disassembly steps- There's always time misalignment because the steps are really doing and what they're speaking about 

%suggested revision of the above para:
\textit{Misalignment:} %E.g., Disassembly steps: 
there is typically temporal misalignment between speech and action, due to the fluidity with which humans tend to plan, think, and speak about their own behaviors. The actions that technicians physically perform, and the actions they describe themselves doing, are rarely synchronously aligned. For example, technicians often describe that they are ``about to use the brush" while they are still physically conducting the preceding step. While humans can easily understand conducting a current action while describing a future action, this produces misalignment and misunderstanding for the multimodal MARIE.

%They can still retain the power of LLMs unlike supervised approaches


\textit{Alternative process execution:} The spec document often gives freedom to technicians to execute cleaning in different sequences. Certain actions might be performed at any point and a strict process execution order is not imposed. However, there remain certain actions that require strict execution after satisfying certain pre-conditions. Learning these alternative processes remains a challenge. %Given these contextual and multimodal challenges, expert inputs are required to resolve missing semantic content, temporal misalignments,  properly identify alternative steps chosen by technicians, and appropriately identify ``common sense" helpful actions which may not necessarily be detailed in the spec document.

%do we need to explain TGS? do we need to explain our prior paper?
 %In spite of what the spec document might say, the process execution allows room for certain alternative path towards a process completion.


\textbf{Semantic Frames for Actionable LLM Explanations}
%Original The semantic frames developed in this work capture abstractions of natural language  \cite{fillmore2006frame} used in this work assumes a natural language sentence consisting of an action and corresponding entities that help accomplish the action or indicate temporal aspects of the same (e.g., [Action: Turn] [Receiver: the nut] using [Tool: a screw driver] [Extent: until snug] ). These semantic frames are often observable visually while the technicians execute a process. These semantic frames can then be used for task guidance (e.g., question-answering, helping with tools, informing the extent of action, etc.) by including these as a part of dialogue system architecture.
In early empirical investigations of how technicians perceive the utility of explanations in MARIE, they expressed a desire for information that could help them to craft inputs to MARIE which would help MARIE to ``learn better." %These investigations were performed as part of a participatory design process, where developers collaborate with domain-expert end-users to gather data needed to train the system. 
In our prior research in this space we proposed a prototype and showed the applicability of semantic frames for task guidance\cite{manuvinakurike2022human}. The current contributions builds on this work and draws on the field of human-centered explainability, to propose that explanations be deployed using ``semantic frames" structure, to enable humans to act in the most productive way to train MARIE. Using ``semantic frames" means that an LLM-based conversational agent would ``explain" MARIE's outputs by asking the end-user for needed data at the same time - using the ``semantic frame" as a container of meaning, which MARIE needs the human to fill. This could look, for example, like explaining why a predicted tip is incorrect, in the form of explicitly telling human users which components of a conversational input are needed. In this way, the explanation becomes ``actionable" by guiding humans to input a non-naturalistic form of language which better supplies the information the AI system needs for it to align its many modules to reach sufficient ``understanding." These %non-naturalistic 
linguistic inputs help the human and the AI system to collaborate, together building a more accurate model of human actions, and ultimately more accurate inferences, better task support, and better outcomes for human users.

\begin{marginfigure}[-5pc]
  \begin{minipage}{\marginparwidth}
    \centering
    \includegraphics[width=0.9\marginparwidth]{figures/semantic_frame_2_egs.png}
    \caption{Semantic frames captured from task guidance scenarios.}~\label{fig:marginfig}
  \end{minipage}
\end{marginfigure}

Our system is multimodal, yet there is no native joining of vision and sound into a unified channel. We use language about the visual channel to link the modalities, which can be referred to as the symbol grounding problem \cite{harnad1990symbol}. The semantic frames we propose are abstractions of natural language  \cite{fillmore2006frame}, which ensure that a conversation includes specific components. These semantic frames can then be used for task guidance (e.g., question-answering, helping with tools, informing the extent of action, etc.) by including these as a part of dialogue system architecture. We create an ontology of actions with a semantic frame \cite{fillmore2001frame} to represent each action. Each semantic frame has both an action name “head word” and a number of roles related to that specific action (e.g., tool, location, or purpose). In our setting, these components consist of an action, and corresponding entities that help accomplish the action, or indicate temporal aspects of the action (e.g., [Action: Turn] [Receiver: the nut] using [Tool: a screw driver] [Extent: until snug]). Starting with relatively empty frames (one for each expected action), task experts fill the roles based on verbal interaction as they do the task and these are temporally aligned with actions to ground the reference.  Grounding the reference in interaction with experts unifies sound and vision into a single stream. Completed semantic frames allow the system to infer which action is referred to by an utterance.



%These semantic frames are often observable visually while the technicians execute a process. 


For this participatory, human-in-the-loop training project, in which human technicians ``teach" an LLM system to recognize what they are doing on the factory floor, explanations must explain moments in which the system has incomplete semantic grounding infer an action or object, i.e., where the technician is in the protocol, or what object she holds. The technician can provide additional information that will complete the ``semantic frame" in which the task is occurring. Semantic frames can infer ``obvious" missing info that can be inferred logically depending on the process: For instance, ``pushing swabs now" means they're cleaning a specific item ("pushing" can only be done for the one item which is hollow). 

We can consider an illustrative example of how this might work in practice. For the purposes of this example, we can follow the procedure that's described in Figure 2, the act of changing a tire. (We've chosen to use this as our example, rather than provide details of the procedure that MARIE was initially designed for, in order to avoid the possibility of inadvertently disclosing protected IP.) Let's say that someone is beginning to change a tire. They have set up MARIE to get tips and reminders, and hold up a car jack and say to MARIE, "I am going to use this on the car." This spoken description lacks referent terms - the sentence doesn't provide MARIE with key data about what "this" means - the object that's being held, or which step the person is on in their task. At this point, MARIE makes a mistake. The system might mistake the step that the person is currently performing and guide them to the incorrect "next" step; the system might mistake what the person is holding and erroneously provide tips on how to handle the hallucinated object - for example, recommending that the person turn the lug wrench, while they are still holding the car jack. Noticing the error, the person then reasonably asks for an explanation in real time (as we have seen in empirical observations when MARIE has an error in its output, or the technician disagrees with MARIE's recommendation) - "no, that's not the next step, why did you think it was?" or "no, that's not what I'm holding. Why did you say that?" MARIE then provides an LLM-generated response based on the combined best available predictions from each of the contributing modules - with the semantic frame containing the referent objects that help MARIE reach "understanding" across modalities. In our example, MARIE might say "I see that you are holding a lug wrench, so I recommended turning the lug wrench to loosen the lug nuts." The intended effect of this explanation is to %exhibit to the person a complete container of meaning, i.e. a "semantic frame" which can display which data point is incorrect or missing - so that the person can either pinpoint exactly which referent object is missing or misidentified, and, by being 
prompt the person with a sentence containing the erroneous referent terms, so that they can respond with the appropriate corrections, i.e. "I am not holding a lug wrench, I am holding a jack that I will use to jack up the car." The power of the semantic frame is in its application to elicit and sync referring and referent terms. These terms can be used to ground visual and language modalities, providing a set of references that can be found across models of object recognition, action recognition, and language. This grounding increases the probability that MARIE will accurately understand what object a person is holding and what steps come next.

Some limitations of this structure relate to cognitive overload or burden: providing too much information in an explanation can  impede or overwhelm decision-making. Too much of a cognitive load from an AI explanation can damage a user's confidence in the system \cite{hudon2021explainable}. The mitigation strategy of semantic frames, as a component of ACE, or Action and Control via Explantions, is to deliberately design explanations to integrate smoothly with user's workflow: users are provided with the explanation only when they ask for it, and the explanation itself - structured as a semantic frame - is focused on information which enables user action.  

The motivation behind ACE is that explanations could be more useful, both for humans being asked to provide data to ultimately achieve task performance support, and for developers building this system, if the inputs are "semantic frames." Although the semantic frame may sacrifice a little bit of naturalness of conversation, it provides transparency into the system's intelligence and reduces the user's cognitive load and improves system performance, as it clearly conveys the system's understanding and reasoning \cite{murad2023s}. So we propose semantic frames as a useful container for deriving meaning and aligning a multimodal system around a "ground truth."

%We could run a short explanations about LLMs explaining themselves with MARIE, Assembly other related task guidance data. 

%TKTK Suggestion for what LLM explainability might look like / 

%\section{Conclusion} In sum, this paper has made two contributions: identifying a challenge of multimodal AI systems in the manufacturing space, and a proposal for how LLM-based explanations can enable successful human-AI collaboration. 

%what is needed to close the gap between what is currently furnished by such explanations and what stakeholders need, such as support to detect hallucinations and errors and mitigate their negative effects. This research is planned to take place within an industrial setting. The web of stakeholders reflect this setting: system developers, end-users who are technicians working in our fabrication facilities, and senior leadership within those facilities. Prior work had identified that end-users, across levels of expertise, desire explanations which can support action,

%the implications of such "explanations" for stakeholders 




%\section{ACM Copyrights \& Permission Policy}
%Accepted extended abstracts and papers will be distributed in the Conference Publications. They will also be placed in the ACM Digital Library, where they will remain accessible to thousands of researchers and practitioners worldwide. To view the ACM's copyright and permissions policy, see:
%\url{http://www.acm.org/publications/policies/copyright_policy}.

%\marginpar{%
%  \vspace{-45pt} \fbox{%
%    \begin{minipage}{0.925\marginparwidth}
%      \textbf{Good Utilization of the Side Bar} \\
%      \vspace{1pc} \textbf{Preparation:} Do not change the margin
%      dimensions and do not flow the margin text to the
%      next page. \\
%      \vspace{1pc} \textbf{Materials:} The margin box must not intrude
%      or overflow into the header or the footer, or the gutter space
%      between the margin paragraph and the main left column. The text
%      in this text box should remain the same size as the body
%      text. Use the \texttt{{\textbackslash}vspace{}} command to set
%      the margin
%      note's position. \\
%      \vspace{1pc} \textbf{Images \& Figures:} Practically anything
%      can be put in the margin if it fits. Use the
%      \texttt{{\textbackslash}marginparwidth} constant to set the
%      width of the figure, table, minipage, or whatever you are trying
%      to fit in this skinny space.
%    \end{minipage}}\label{sec:sidebar} }

%\section{Page Size}
%All SIGCHI submissions should be US letter (8.5 $\times$ 11 inches). US Letter is the standard option used by this \LaTeX\ template.

%\section{Text Formatting}
%Please use an 8.5-point Verdana font, or other sans serifs font as close as possible in appearance to Verdana in which these guidelines have been set. Arial 9-point font is a reasonable substitute for Verdana as it has a similar x-height. Please use serif or non-proportional fonts only for special purposes, such as distinguishing \texttt{source code} text.

%\subsubsection{Text styles}
%The \LaTeX\ template facilitates text formatting for normal (for body text); heading 1, heading 2, heading 3; bullet list; numbered list; caption; annotation (for notes in the narrow left margin); and references (for bibliographic entries). Additionally, here is an example of footnoted\footnote{Use footnotes sparingly, if at all.} text. As stated in the footnote, footnotes should rarely be used.

%\begin{figure}
%  \includegraphics[width=0.9\columnwidth]{figures/sigchi-logo}
%  \caption{Insert a caption below each figure.}~\label{fig:sample}
%\end{figure}

%\subsection{Language, style, and content}
%The written and spoken language of SIGCHI is English. Spelling and punctuation may use any dialect of English (e.g., British, Canadian, US, etc.) provided this is done consistently. Hyphenation is optional. To ensure suitability for an international audience, please pay attention to the following:

%\begin{table}
%  \centering
%  \begin{tabular}{l r r r}
%    % \toprule
%    & & \multicolumn{2}{c}{\small{\textbf{Test Conditions}}} \\
%    \cmidrule(r){3-4}
%    {\small\textit{Name}}
%    & {\small \textit{First}}
%      & {\small \textit{Second}}
%    & {\small \textit{Final}} \\
%    \midrule
%    Marsden & 223.0 & 44 & 432,321 \\
%    Nass & 22.2 & 16 & 234,333 \\
%    Borriello & 22.9 & 11 & 93,123 \\
%    Karat & 34.9 & 2200 & 103,322 \\
%    % \bottomrule
%  \end{tabular}
%  \caption{Table captions should be placed below the table. We
%    recommend table lines be 1 point, 25\% black. Minimize use of
%    table grid lines.}~\label{tab:table1}
%\end{table}

%\begin{itemize}\compresslist%
%\item Write in a straightforward style. Use simple sentence
%  structure. Try to avoid long sentences and complex sentence
%  structures. Use semicolons carefully.
%\item Use common and basic vocabulary (e.g., use the word ``unusual''
%  rather than the word ``arcane'').
%\item Briefly define or explain all technical terms. The terminology
%  common to your practice/discipline may be different in other design
%  practices/disciplines.
%\item Spell out all acronyms the first time they are used in your
%  text. For example, ``World Wide Web (WWW)''.
%\item Explain local references (e.g., not everyone knows all city
%  names in a particular country).
%\item Explain ``insider'' comments. Ensure that your whole audience
%  understands any reference whose meaning you do not describe (e.g.,
%  do not assume that everyone has used a Macintosh or a particular
%  application).
%\item Explain colloquial language and puns. Understanding phrases like
%  ``red herring'' requires a cultural knowledge of English. Humor and
%  irony are difficult to translate.
%\item Use unambiguous forms for culturally localized concepts, such as
%  times, dates, currencies, and numbers (e.g., ``1-5- 97'' or
%  ``5/1/97'' may mean 5 January or 1 May, and ``seven o'clock'' may
%  mean 7:00 am or 19:00). For currencies, indicate equivalences:
%  ``Participants were paid {\fontfamily{txr}\selectfont \textwon}
%  25,000, or roughly US \$22.''
%\item Be careful with the use of gender-specific pronouns (he, she)
%  and other gender-specific words (chairman, manpower,
%  man-months). Use inclusive language (e.g., she or he, they, chair,
%  staff, staff-hours, person-years) that is gender-neutral. If
%  necessary, you may be able to use ``he'' and ``she'' in alternating
%  sentences, so that the two genders occur equally
%  often~\cite{Schwartz:1995:GBF}.
%\item If possible, use the full (extended) alphabetic character set
%  for names of persons, institutions, and places (e.g.,
%  Gr{\o}nb{\ae}k, Lafreni\'ere, S\'anchez, Nguy{\~{\^{e}}}n,
%  Universit{\"a}t, Wei{\ss}enbach, Z{\"u}llighoven, \r{A}rhus, etc.).
%  These characters are already included in most versions and variants
%  of Times, Helvetica, and Arial fonts.
%\end{itemize}

% \begin{figure}
%   \includegraphics[width=.9\columnwidth]{figures/ea-figure2}
%   \caption{If your figure has a light background, you can set its
%     outline to light gray, like this, to make a box around
%     it.}\label{fig:bats}
% \end{figure}



%\section{Figures}
%The examples on this and following pages should help you get a feel for how screen-shots and other figures should be placed in the template. Your document may use color figures (see Figures~\ref{fig:sample}), which are included in the page limit; the figures must be usable when printed in black and white. You can use the \texttt{\marginpar} command to insert figures in the (left) margin of the document (see Figure~\ref{fig:marginfig}). Finally, be sure to make images large enough so the important details are legible and clear (see Figure~\ref{fig:cats}). All figures should include alt text (figure description) for improved accessibility – see the Accessibility section.

%\section{Tables}
%You man use tables inline with the text (see Table~\ref{tab:table1}) or within the margin as shown in Table~\ref{tab:table2}. Try to minimize the use of lines (especially vertical lines). \LaTeX\ will set the table font and captions sizes correctly; the latter must remain unchanged.

%\section{Accessibility}
%The Executive Council of SIGCHI has committed to making SIGCHI
%conferences more inclusive for researchers, practitioners, and
%educators with disabilities. As a part of this goal, the all authors
%are expected to work on improving the accessibility of their
%submissions. Specifically, we encourage authors to carry out the
%following five steps:
%\begin{itemize}\compresslist%
%\item Add alternative text (figure description) to all figures
%\item Mark table headings
%\item Generate a tagged PDF
%\item Verify the default language
%\item Set the tab order to ``Use Document Structure''
%\end{itemize}

%For links to detailed instructions and resources, please see:
%\url{http://chi2020.acm.org/authors/papers/guide-to-an-accessible-submission/}

%Unfortunately good tools do not yet exist to create tagged PDF files from Latex. \LaTeX\ users will need to carry out all of the above steps in the PDF directly using Adobe Acrobat, after the PDF has been generated.


%\begin{figure*}
%  \centering
%  \includegraphics[width=1.3\columnwidth]{figures/map}
%  \caption{In this image, the map maximizes use of space. You can make
%    figures as wide as you need, up to a maximum of the full width of
%    both columns. Note that \LaTeX\ tends to render large figures on a
%    dedicated page. Image: \ccbynd~ayman on Flickr.}~\label{fig:cats}
%\end{figure*}

%\section{Producing and Testing PDF Files}
%We recommend that you produce a PDF version of your submission well
%before the final deadline.

%\marginpar{\vspace{-23pc}So long as you don't type outside the right
%  margin or bleed into the gutter, it's okay to put annotations over
%  here on the left, too; this annotation is near Hawaii. You'll have
%  to manually align the margin paragraphs to your \LaTeX\ floats using
%  the \texttt{{\textbackslash}vspace{}} command.}

%\begin{margintable}[1pc]
%  \begin{minipage}{\marginparwidth}
%    \centering
%    \begin{tabular}{r r l}
%      & {\small \textbf{First}}
%      & {\small \textbf{Location}} \\
%      \toprule
%      Child & 22.5 & Melbourne \\
%      Adult & 22.0 & Bogot\'a \\
%      \midrule
%      Gene & 22.0 & Palo Alto \\
%      John & 34.5 & Minneapolis \\
%      \bottomrule
%    \end{tabular}
%    \caption{A simple narrow table in the left margin
%      space.}~\label{tab:table2}
%  \end{minipage}
%\end{margintable}
%Test your PDF file by viewing or printing it with the same software we will use when we receive it, Adobe Acrobat Reader Version 10. This is widely available at no cost. Note that most reviewers will use a North American/European version of Acrobat reader, so please check your PDF accordingly.

%\section{Acknowledgements}
%We thank all the volunteers, publications support, staff, and authors who wrote and provided helpful comments on previous versions of this document. As well authors 1, 2, and 3 gratefully acknowledge the grant from NSF (\#1234--2222--ABC). Author 4 for example may want to acknowledge a supervisor/manager from their original employer. This whole paragraph is just for example. Some of the references cited in this paper are included for illustrative purposes only.

%\section{References Format}
%Your references should be published materials accessible to the
%public. Internal technical reports may be cited only if they are
%easily accessible and may be obtained by any reader for a nominal
%fee. Proprietary information may not be cited. Private communications
%should be acknowledged in the main text, not referenced (e.g.,
%[Golovchinsky, personal communication]). References must be the same
%font size as other body text. References should be in alphabetical
%order by last name of first author. Use a numbered list of references
%at the end of the article, ordered alphabetically by last name of
%first author, and referenced by numbers in brackets. For papers from
%conference proceedings, include the title of the paper and the name of
%the conference. Do not include the location of the conference or the
%exact date; do include the page numbers if available. 

%References should be in ACM citation format:
%\url{http://www.acm.org/publications/submissions/latex_style}.  This includes citations to Internet resources~\cite{CHINOSAUR:venue,cavender:writing,psy:gangnam} according to ACM format, although it is often appropriate to include URLs directly in the text, as above. Example reference formatting for individual journal articles~\cite{ethics}, articles in conference proceedings~\cite{Klemmer:2002:WSC:503376.503378}, books~\cite{Schwartz:1995:GBF}, theses~\cite{sutherland:sketchpad}, book chapters~\cite{winner:politics}, an entire journal issue~\cite{kaye:puc}, websites~\cite{acm_categories,cavender:writing}, tweets~\cite{CHINOSAUR:venue}, patents~\cite{heilig:sensorama}, games~\cite{supermetroid:snes}, and online videos~\cite{psy:gangnam} is given here.  See the examples of citations at the end of this document and in the accompanying \texttt{BibTeX} document. This formatting is a edited version of the format automatically generated by the ACM Digital Library (\url{http://dl.acm.org}) as ``ACM Ref''. DOI and/or URL links are optional but encouraged as are full first names. Note that the Hyperlink style used throughout this document uses blue links; however, URLs in the references section may optionally appear in black.

\balance{} 

\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
