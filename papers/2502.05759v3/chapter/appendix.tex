\section{Detailed Experimental Setup}
\label{app:setup}
In this section, we elaborate on our experimental setup, which consists of five parts: baseline methods, datasets, evaluation metrics, GLUE benchmarks, and hyperparameter configuration. All experiments were conducted on a single NVIDIA A100 (80GB) GPU. Editing time for all methods was measured using LLMs in half-precision mode. To better simulate real-world applications, we used the instruction tuning versions of LLMs.

\subsection{Baseline Methods}
We utilized the code from AlphaEdit and MALMEN to evaluate the performance of baseline methods. The baseline methods used in this paper are as follows:
\begin{itemize}
    \item \textbf{FT-L} \cite{modifying} is a knowledge editing approach that focuses on fine-tuning specific layers of the LLM through autoregressive loss. We implemented this baseline method using the hyperparameter settings from the original paper.
    \item \textbf{MEND} \cite{mend} is an efficient editing method based on hypernetworks. It trains a hypernetwork to learn patterns in knowledge editing by mapping low-rank decomposed fine-tuning gradients to LLM parameter updates. This approach enables efficient and localized knowledge editing. We implemented this baseline method using the hyperparameter settings from the original paper, completing training over the entire training set. Additionally, we introduce MEND* as a baseline. To address the mismatch between the initial hypernetwork and post-edited LLM in lifelong editing scenarios, we periodically retrain the hypernetwork using post-edited parameters. We adopted a strategy of retraining the hypernetwork every three editing batches.
    \item \textbf{ROME} \cite{rome} is a method for updating specific factual associations in LLM parameters. It identifies key neuron activations in MLP layers through perturbation-based knowledge localization, then modifies MLP layer weights by computing Lagrange remainders to edit knowledge. Since ROME doesn't support massive editing, we followed the original paper's configuration and evaluated it through multiple batches of single editing.
    \item \textbf{MEMIT} \cite{memit} is a method supporting large-batch knowledge updates. Building upon ROME's modeling approach, MEMIT extends it by using least squares approximation to directly manipulate parameters at specific layers, enabling multi-layer updates. This allows MEMIT to simultaneously update hundreds or thousands of knowledge facts. We evaluated MEMIT's performance in lifelong editing using the original configuration from its paper.
    \item \textbf{MALMEN} \cite{malmen} is a hypernetwork-based method designed for massive editing. To aggregate parameter shifts across large batches of knowledge, MALMEN employs a least squares approach, deriving optimal parameter shifts by solving normal equations. This algorithm effectively addresses knowledge conflicts in massive editing scenarios. We implemented this baseline method using the original paper's hyperparameter configuration and completed training across the entire training set. Similar to MEND, we also introduce MALMEN* as a baseline.
    \item \textbf{DAFNet} \cite{dafnet} is a model editing method specifically designed for sequential editing. It features a dynamic auxiliary fusion network that enhances semantic interactions between knowledge triples in the sequence, enabling continuous mistake rectification. Through this auxiliary network, DAFNet improves the performance of hypernetwork approaches in sequential editing tasks. We implemented this baseline method using the original paper's hyperparameter configuration and completed training across ZsRE and CounterFact datasets.
    \item \textbf{PRUNE} \cite{prune} is an editing method focused on sequential editing scenarios.  By imposing conditional restraints on edited matrices, PRUNE limits the interference of new knowledge on previously stored model knowledge,  thereby addressing the problem of model performance decline during multiple sequential edits. We implemented this baseline method using the hyperparameter configuration from their original paper.
    \item \textbf{RECT} \cite{rect} is an editing method designed to minimize the impact of editing on LLM's general capabilities. It investigates the role of regularization in lifelong editing and prevents editing overfitting by regularizing weight updates during the editing process. This enables RECT to achieve high editing performance while maintaining the LLM's general capabilities. We implemented this baseline method using the hyperparameter configuration from their original paper.
    \item \textbf{AlphaEdit} \cite{alphaedit} is an editing method aimed at mitigating knowledge disruption in LLM lifelong editing. By introducing the concept of null space, AlphaEdit projects parameter updates onto a knowledge-preserving null space before applying them, thus reducing interference between different knowledge updates. AlphaEdit has been proven to achieve SOTA performance across multiple metrics while maintaining strong transferability. We implemented this baseline method using the hyperparameter configuration from their original paper.
\end{itemize}

\subsection{Datasets}
Next, we introduce the datasets used in this paper.
\begin{itemize}
    \item \textbf{ZsRE} (Zero-shot Relation Extraction) \cite{zsre} dataset serves as a benchmark dataset in the field of language model editing research. The dataset's structure incorporates three distinct components for each entry: a primary question and its corresponding answer intended for editing purposes, multiple paraphrased variations of the original question created using back-translation techniques, and locality questions that are semantically unrelated to the original query. This comprehensive structure enables researchers to evaluate model editing performance across three critical dimensions: accuracy in incorporating new information, robustness when faced with differently worded but semantically equivalent queries, and precision in maintaining unrelated knowledge without interference. For locate-then-edit methods, we use the version from MEMIT; for hypernetwork-based methods, we use the version from MEND, where ZsRE is divided into training and test sets for hypernetwork training and editing performance evaluation respectively.
    \item \textbf{CounterFact} \cite{rome} represents an advanced dataset specifically designed to evaluate language models'  ability to handle contradictory factual information. The dataset's distinctive feature lies in its use of false statements that require correction, making it particularly challenging since models typically provide incorrect answers before editing. Each entry in the dataset contains three elements: an original false statement requiring editing, semantically equivalent rephrased versions of the statement, and unrelated statements for locality purposes. For locate-then-edit methods, we use the version from MEMIT; for hypernetwork-based methods, we also use the version from MEMIT and divide it into training and test sets, each containing approximately 10,000 knowledge instances.
    \item \textbf{FEVER} (Fact Extraction and VERification) \cite{fever} dataset is a comprehensive dataset for fact-checking tasks, constructed through systematic modification of Wikipedia content. The dataset contains claims that were created by altering original Wikipedia sentences and then independently verified without reference to their source material. In its structure, FEVER implements a three-category classification system: claims can be marked as Supported, Refuted, or NotEnoughInfo. For claims classified as either Supported or Refuted, the dataset includes supporting evidence sentences that justify the classification decision. When used in editing tasks, the dataset is often simplified to a binary classification problem, where the editing targets are equally distributed between two possible labels (1 and 0), representing the veracity of the claims. For locate-then-edit methods, we extract the subject from queries to adapt to $(s,r,o)$ modeling; for hypernetwork-based methods, we use the version from MEND, where FEVER is divided into training and test sets.
\end{itemize}

\subsection{Metrics}
\label{app:metric}
\subsubsection{Zsre Metrics}
Following previous research \cite{rome, mend}, we evaluate various model editing methods using standard metrics on the ZsRE dataset. Specifically, given an LLM $f_{\mathcal{W}}$, an editing knowledge pair $(x, y)$, equivalent  knowledge $x_e$, and unrelated knowledge pairs $(x_{loc}, y_{loc})$, we examine the following three metrics:

\textbf{Efficacy.} This metric measures the success rate of editing knowledge  $x$ in $f_{\mathcal{W}}$. It compares the top-1 logits output  $y'=f_{\mathcal{W}}(x)$ with the target output $y$ when inputting $x$ into $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left\{y=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f_\mathcal{W}}(y'\left|x\right.)\right\}.
\end{equation}

\textbf{Generalization.} This metric measures the success rate of editing equivalent knowledge $x_e$ in $f_{\mathcal{W}}$. It evaluates whether the LLM has truly learned the intrinsic relationships of the knowledge and can extend to other equivalent knowledge. We compare the top-1 logits output $y'=f_{\mathcal{W}}(x_e)$ with the target output $y$ when inputting $x_e$ into $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left\{y=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f_\mathcal{W}}(y'\left|x_e\right.)\right\}.
\end{equation}

\textbf{Specificity.} This metric measures the retention rate of unrelated knowledge $x_{loc}$ after editing, examining whether the knowledge editing maintains locality and only modifies the target knowledge. We compare the top-1 logits output $y'=f_{\mathcal{W}}(x_{loc})$ with the original output $y_{loc}$ when inputting $x_{loc}$ into $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left\{y_{loc}=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f_\mathcal{W}}(y'\left|x_{loc}\right.)\right\}.
\end{equation}

\subsubsection{CounterFact Metrics}
Similarly, following previous research \cite{rome,memit}, we evaluate various model editing methods using standard metrics on the CounterFact dataset. We follow ROME's original setup, comparing the probabilities of different answers in the logits for measurement. Specifically, given an LLM $f_{\mathcal{W}}$, an editing knowledge pair $(x, y)$, original knowledge pair $(x, y_0)$, equivalent knowledge $x_e$, and unrelated knowledge pairs $(x_{loc}, y_{loc})$, we examine the following three metrics, calculated following ROME and MEMIT:

\textbf{Efficacy.} This metric measures the success rate of editing knowledge $x$ in $f_{\mathcal{W}}$. We compare whether the probability of the target output $y$ is higher than the probability of the original answer $y_0$ in the logits when inputting $x$ into $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left[\mathbb{P}_{f_{\mathcal{W}}}\left[y\left|x\right.\right]>\mathbb{P}_{f_{\mathcal{W}}}\left[y_0\left|x\right.\right]\right].
\end{equation}

\textbf{Generalization.} This metric measures the success rate of editing equivalent knowledge $x_e$ in $f_{\mathcal{W}}$. We compare whether the probability of the target output $y$ is higher than the probability of the original answer $y_0$ in the logits when inputting $x_e$ into  $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left[\mathbb{P}_{f_{\mathcal{W}}}\left[y\left|x_e\right.\right]>\mathbb{P}_{f_{\mathcal{W}}}\left[y_0\left|x_e\right.\right]\right].
\end{equation}

\textbf{Specificity.} This metric measures the retention rate of unrelated knowledge $x_{loc}$ after editing. We compare whether the probability of the original answer $y_{loc}$ is higher than the probability of the edited output $y$ in the logits when inputting $x_{loc}$ into  $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left[\mathbb{P}_{f_{\mathcal{W}}}\left[y_{loc}\left|x_{loc}\right.\right]>\mathbb{P}_{f_{\mathcal{W}}}\left[y\left|x_{loc}\right.\right]\right].
\end{equation}

\subsubsection{FEVER Metrics}
We also evaluate various model editing methods using standard metrics on the FEVER dataset. Specifically, given an LLM $f_{\mathcal{W}}$, an editing knowledge pair $(x, y)$, equivalent knowledge $x_e$, and unrelated knowledge pairs $(x_{loc}, y_{loc})$, we examine the following three metrics:

\textbf{Efficacy.} This metric measures the success rate of editing knowledge $x$ in $f_{\mathcal{W}}$. It compares whether the top-1 logits output $y'=f_{\mathcal{W}}(x)$ matches the target output $y$ when inputting $x$ into $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left\{y=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f_\mathcal{W}}(y'\left|x\right.)\right\}.
\end{equation}

\textbf{Generalization.} This metric measures the success rate of editing equivalent knowledge $x_e$ in $f_{\mathcal{W}}$. Since we need to examine whether a knowledge edit is truly successful, we must verify if the LLM has genuinely learned the intrinsic relationships of the knowledge and can extend to other equivalent knowledge. We compare whether the top-1 logits output $y'=f_{\mathcal{W}}(x_e)$ matches the target output $y$ when inputting $x_e$ into $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left\{y=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f_\mathcal{W}}(y'\left|x_e\right.)\right\}.
\end{equation}

\textbf{Specificity.} This metric measures the retention rate of unrelated knowledge $x_{loc}$ after editing, examining whether the knowledge editing maintains locality and only modifies the target knowledge. We compare whether the top-1 logits output $y'=f_{\mathcal{W}}(x_{loc})$ matches the original output $y_{loc}$ when inputting $x_{loc}$ into $f_\mathcal{W}$:
\begin{equation}
\mathbb{E}\left\{y_{loc}=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f_\mathcal{W}}(y'\left|x_{loc}\right.)\right\}.
\end{equation}

\subsection{GLUE Benchmark}
GLUE (General Language Understanding Evaluation) \cite{glue} benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. We selected 6 metrics from this benchmark to evaluate how well different methods maintain general language capabilities.
\begin{itemize}
    \item \textbf{Stanford Sentiment Treebank (SST)} \cite{sst} is a dataset consisting of movie review sentences with their associated sentiment labels. This binary classification task requires models to categorize the sentiment expressed in each individual sentence.
    \item \textbf{Massive Multi-task Language Understanding (MMLU)} \cite{mmlu} is a comprehensive benchmark designed to assess language models' capabilities across multiple domains. It specifically evaluates model performance in zero-shot and few-shot learning scenarios.
    \item \textbf{Microsoft Research Paraphrase Corpus (MRPC)} \cite{mrpc} serves as a benchmark for evaluating semantic  similarity. The task challenges models to identify whether two given sentences convey the same meaning.
    \item \textbf{Recognizing Textual Entailment (RTE)} \cite{rte} examines logical relationships between sentences. The task requires determining whether a given premise sentence logically implies a hypothesis sentence.
    \item \textbf{Corpus of Linguistic Acceptability (CoLA)} \cite{cola} focuses on grammatical judgment. This single-sentence classification task uses sentences extracted from linguistics publications, requiring models to distinguish between grammatically acceptable and unacceptable sentences.
    \item \textbf{Natural Language Inference (NLI)} \cite{nli} evaluates natural language understanding capabilities. The benchmark requires models to analyze sentence pairs and determine their logical relationships.
\end{itemize}

\subsection{Hyperparameter Configuration}
Now we describe the hyperparameter configurations in our experiments.

For the hyperparameters in RLEdit training and editing, we set the memory backtracking decay factor $\mu$ to 0.95, the backtracking depth $k$ to 10, the regularization coefficient $\eta$ to 1e-4 and the discount factor $\gamma$ to 1 in the total reward formula. Additionally, the initial learning rate was set to 1e-6, while the meta-learning rate was set to 1e-5. The specific hyperparameter configurations for different models and datasets are shown in Table \ref{tab:hyperparameter},  where rank refers to the rank of linear transformation in hypernetwork, loc\_coef refers to the weight coefficient $\lambda_{loc}$ of the locality loss function $\mathcal{L}_\textit{loc}$.

\begin{table}[ht]
\centering
% \begin{small}
\caption{The specific hyperparameter configurations for different models and datasets.}
\label{tab:hyperparameter}
% \resizebox{\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule[1.5pt]
\textbf{Datasets} & \textbf{Models} & \textbf{Layer} & \textbf{Rank} & \textbf{loc\_coef $\lambda_{loc}$} \\
\midrule
\multirow{3}{*}{ZsRE} & Llama-3-8B & gate[11-15], up[18-24] & 1024 & 0.6 \\  
 & Gemma-2-9B & gate[32-40], up[32-40] & 512 & 0.6 \\  
 & Mistral-7B & down[17, 18] & 1024 & 0.8 \\ \midrule
\multirow{3}{*}{CounterFact} & Llama-3-8B & gate[22-30], up[22-30] & 512 & 0.6 \\  
 & Gemma-2-9B & gate[32-40], up[32-40] & 1024 & 0.6 \\ 
 & Mistral-7B & down[17, 18, 19] & 1024 & 0.8 \\ \midrule
\multirow{3}{*}{FEVER} & Llama-3-8B & gate[22-30], up[22-30] & 1024 & 0.6 \\  
 & Gemma-2-9B & gate[32-40], up[32-40] & 1024 & 0.6 \\ 
 & Mistral-7B & down[17, 18] & 1024 & 0.6 \\
 \bottomrule[1pt]
\end{tabular}
% }
% \end{small}
\end{table}


\newpage


\section{Related Work}
\label{app:related}
\textbf{Parameter-modifying Editing.} Parameter-modifying methods modify LLM parameters through various approaches to achieve knowledge editing. One approach is the locate-then-edit method. Knowledge-neuron \cite{knowledge-neuron} pioneered the use of causal analysis to locate knowledge in LLMs. ROME \cite{rome} adopted this idea, using causal trace to identify factual associations and edit knowledge in MLP layers. MEMIT \cite{memit} extended this approach to mass-editing. PMET \cite{pmet} specifically investigated the role of MHSA in model editing. ECE \cite{explainable} incorporates LLM explainability into the editing process and clusters similar knowledge based on explanation results. Another approach is meta-learning. KE \cite{editing-factual-knowledge} first proposed using hypernetwork for knowledge editing. MEND \cite{mend} applied low-rank decomposition to LLM fine-tuning gradients, enabling faster and more effective training in hypernetworks. MALMEN \cite{malmen} aggregated parameter shifts using normal equations, extending hypernetworks to mass-editing.

\textbf{Parameter-preserving Editing} Parameter-preserving methods guide LLM outputs by incorporating external modules or additional parameters. SERAC \cite{serac} stores edits in additional memory and uses classifiers for edit matching. T-Patcher \cite{transformerpatcher} edits knowledge by introducing additional neurons. IKE \cite{icl} improves LLM's in-context learning capabilities using memory retrieval-based examples. OneEdit \cite{oneedit} proposed a neural-symbolic prototype system for collaborative knowledge editing using natural language.

\textbf{Lifelong Model Editing.} To meet the requirements of lifelong model editing, researchers have studied sequential editing methods. T-Patcher \cite{transformerpatcher} achieves sequential editing by continuously introducing additional correction neurons. GRACE \cite{grace} enables sequential knowledge editing through dynamic updates of external codebook modules. WISE \cite{wise} uses a knowledge-sharding mechanism to optimize the balance of various metrics in sequential editing. RECT \cite{rect} studies the impact of regularization on sequential editing, using smaller parameter shifts to maintain stability. PRUNE \cite{prune} reduces disturbance to original knowledge by limiting singular values of the edit update matrix. O-Edit \cite{o-edit} orthogonalizes knowledge update directions to reduce mutual interference. AlphaEdit \cite{alphaedit} maintains knowledge stability by projecting existing knowledge into null space. NSE \cite{nse} employs neuron-level sorting to selectively identify and update influential neurons, while utilizing weights rewinding to prevent model failures during sequential editing.






\section{More Experimental Results}
\label{app:results}
In this section, we present additional experimental results.

\subsection{Ablation Study}
\label{app:ablation}
To assess the contribution of each component in RLEdit, we conducted ablation studies on 3 models and 3 datasets by removing the RL training framework, memory backtracking, and regularization respectively. Table \ref{tab:ablation} shows the contribution of each component to RLEdit under the 20$\times$100 task.

\begin{table*}[ht]
\caption{Ablation Study Results for RLEdit.}
\label{tab:ablation}
\resizebox{\textwidth}{!}{%
% \begin{small}
\begin{tabular}{c|cccccccccc}
\toprule[1.5pt]
\raisebox{-1.5ex}{\textbf{Model}} & \multicolumn{1}{c}{} & \multicolumn{3}{c|}{\textbf{CounterFact}} & \multicolumn{3}{c|}{\textbf{ZsRE}} & \multicolumn{3}{c}{\textbf{FEVER}} \\ \cmidrule{3-11} 
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c}{\multirow{-2}{*}{\diagbox{\textbf{Method}}{\textbf{Dataset}}}} & \textbf{Eff.$\uparrow$} & \textbf{Gen.$\uparrow$} & \multicolumn{1}{c|}{\textbf{Spe.$\uparrow$}} & \textbf{Eff.$\uparrow$} & \textbf{Gen.$\uparrow$} & \multicolumn{1}{c|}{\textbf{Spe.$\uparrow$}} & \textbf{Eff.$\uparrow$} & \textbf{Gen.$\uparrow$} & \multicolumn{1}{c}{\textbf{Spe.$\uparrow$}} \\ \midrule[1pt]
\multirow{5}{*}{\rotatebox{90}{Llama-3}} & \multicolumn{1}{c|}{\textbf{RLEdit}} & {91.75} & {62.40} & \multicolumn{1}{c|}{52.38} & {88.65} & {83.91} & \multicolumn{1}{c|}{47.61} & 94.46 & 91.56 & \multicolumn{1}{c}{69.01} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o RL training framework} & {50.30\textcolor{blue}{\scriptsize{$\downarrow$ 41.45}}} & {49.55\textcolor{blue}{\scriptsize{$\downarrow$ 12.85}}} & \multicolumn{1}{c|}{49.63\textcolor{blue}{\scriptsize{$\downarrow$ 2.75}}} & 0.62\textcolor{blue}{\scriptsize{$\downarrow$ 88.03}} & 0.53\textcolor{blue}{\scriptsize{$\downarrow$ 83.38}} & \multicolumn{1}{c|}{2.36\textcolor{blue}{\scriptsize{$\downarrow$ 45.25}}} & {21.28\textcolor{blue}{\scriptsize{$\downarrow$ 73.18}}} & {21.15\textcolor{blue}{\scriptsize{$\downarrow$ 70.41}}} & \multicolumn{1}{c}{13.63\textcolor{blue}{\scriptsize{$\downarrow$ 55.38}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o memory backtracking} & {89.84\textcolor{blue}{\scriptsize{$\downarrow$ 1.91}}} & {60.42\textcolor{blue}{\scriptsize{$\downarrow$ 1.98}}} & \multicolumn{1}{c|}{50.23\textcolor{blue}{\scriptsize{$\downarrow$ 2.15}}} & {88.62\textcolor{blue}{\scriptsize{$\downarrow$ 0.03}}} & {81.23\textcolor{blue}{\scriptsize{$\downarrow$ 2.68}}} & \multicolumn{1}{c|}{44.43\textcolor{blue}{\scriptsize{$\downarrow$ 3.18}}} & {93.02\textcolor{blue}{\scriptsize{$\downarrow$ 1.44}}} & {87.29\textcolor{blue}{\scriptsize{$\downarrow$ 4.27}}} & \multicolumn{1}{c}{68.87\textcolor{blue}{\scriptsize{$\downarrow$ 0.14}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o regularization} & {92.12\textcolor{red}{\scriptsize{$\uparrow$ 0.37}}} & {60.99\textcolor{blue}{\scriptsize{$\downarrow$ 1.41}}} & \multicolumn{1}{c|}{52.45\textcolor{red}{\scriptsize{$\uparrow$ 0.07}}} & {88.63\textcolor{blue}{\scriptsize{$\downarrow$ 0.02}}} & {83.12\textcolor{blue}{\scriptsize{$\downarrow$ 0.79}}} & \multicolumn{1}{c|}{48.43\textcolor{red}{\scriptsize{$\uparrow$ 0.82}}} & {94.01\textcolor{blue}{\scriptsize{$\downarrow$ 0.45}}} & {91.58\textcolor{red}{\scriptsize{$\uparrow$ 0.02}}} & \multicolumn{1}{c}{68.68\textcolor{blue}{\scriptsize{$\downarrow$ 0.33}}} \\
\midrule[1pt]
\midrule[1pt]
\multirow{5}{*}{\rotatebox{90}{Gemma-2}} & \multicolumn{1}{c|}{\textbf{RLEdit}} & {90.11} & {61.34} & \multicolumn{1}{c|}{48.57} & {89.22} & {79.85} & \multicolumn{1}{c|}{35.57} & 95.13 & 91.70 & \multicolumn{1}{c}{71.83} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o RL training framework} & {18.13\textcolor{blue}{\scriptsize{$\downarrow$ 71.98}}} & {20.73\textcolor{blue}{\scriptsize{$\downarrow$ 40.61}}} & \multicolumn{1}{c|}{80.49\textcolor{red}{\scriptsize{$\uparrow$ 31.92}}} & 12.98\textcolor{blue}{\scriptsize{$\downarrow$ 76.24}} & 10.49\textcolor{blue}{\scriptsize{$\downarrow$ 69.36}} & \multicolumn{1}{c|}{9.12\textcolor{blue}{\scriptsize{$\downarrow$ 26.45}}} & 0.00\textcolor{blue}{\scriptsize{$\downarrow$ 95.13}} & 0.00\textcolor{blue}{\scriptsize{$\downarrow$ 91.70}} & \multicolumn{1}{c}{0.00\textcolor{blue}{\scriptsize{$\downarrow$ 71.83}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o memory backtracking} & {89.37\textcolor{blue}{\scriptsize{$\downarrow$ 0.74}}} & {60.93\textcolor{blue}{\scriptsize{$\downarrow$ 0.41}}} & \multicolumn{1}{c|}{48.68\textcolor{red}{\scriptsize{$\uparrow$ 0.11}}} & {87.13\textcolor{blue}{\scriptsize{$\downarrow$ 2.09}}} & {78.29\textcolor{blue}{\scriptsize{$\downarrow$ 1.56}}} & \multicolumn{1}{c|}{35.27\textcolor{blue}{\scriptsize{$\downarrow$ 0.30}}} & {95.10\textcolor{blue}{\scriptsize{$\downarrow$ 0.03}}} & {90.98\textcolor{blue}{\scriptsize{$\downarrow$ 0.72}}} & \multicolumn{1}{c}{69.28\textcolor{blue}{\scriptsize{$\downarrow$ 2.55}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o regularization} & {89.87\textcolor{blue}{\scriptsize{$\downarrow$ 0.24}}} & {62.03\textcolor{red}{\scriptsize{$\uparrow$ 0.69}}} & \multicolumn{1}{c|}{48.74\textcolor{red}{\scriptsize{$\uparrow$ 0.17}}} & {89.21\textcolor{blue}{\scriptsize{$\downarrow$ 0.01}}} & {77.25\textcolor{blue}{\scriptsize{$\downarrow$ 2.60}}} & \multicolumn{1}{c|}{33.60\textcolor{blue}{\scriptsize{$\downarrow$ 1.97}}} & {94.95\textcolor{blue}{\scriptsize{$\downarrow$ 0.18}}} & {92.33\textcolor{red}{\scriptsize{$\uparrow$ 0.63}}} & \multicolumn{1}{c}{73.98\textcolor{red}{\scriptsize{$\uparrow$ 2.15}}} \\
\midrule[1pt]
\midrule[1pt]
\multirow{5}{*}{\rotatebox{90}{Mistral}} & \multicolumn{1}{c|}{\textbf{RLEdit}} & {84.24} & {63.93} & \multicolumn{1}{c|}{60.79} & {84.60} & {78.00} & \multicolumn{1}{c|}{50.18} & 97.78 & 96.34 & \multicolumn{1}{c}{83.71} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o RL training framework} & {50.95\textcolor{blue}{\scriptsize{$\downarrow$ 33.29}}} & {49.93\textcolor{blue}{\scriptsize{$\downarrow$ 14.00}}} & \multicolumn{1}{c|}{51.11\textcolor{blue}{\scriptsize{$\downarrow$ 9.68}}} & 6.66\textcolor{blue}{\scriptsize{$\downarrow$ 77.94}} & 6.58\textcolor{blue}{\scriptsize{$\downarrow$ 71.42}} & \multicolumn{1}{c|}{2.10\textcolor{blue}{\scriptsize{$\downarrow$ 48.08}}} & {46.38\textcolor{blue}{\scriptsize{$\downarrow$ 51.40}}} & {45.26\textcolor{blue}{\scriptsize{$\downarrow$ 51.08}}} & \multicolumn{1}{c}{30.16\textcolor{blue}{\scriptsize{$\downarrow$ 53.55}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o memory backtracking} & {82.14\textcolor{blue}{\scriptsize{$\downarrow$ 2.10}}} & {60.51\textcolor{blue}{\scriptsize{$\downarrow$ 3.42}}} & \multicolumn{1}{c|}{58.99\textcolor{blue}{\scriptsize{$\downarrow$ 1.80}}} & {84.61\textcolor{red}{\scriptsize{$\uparrow$ 0.01}}} & {76.89\textcolor{blue}{\scriptsize{$\downarrow$ 1.11}}} & \multicolumn{1}{c|}{48.47\textcolor{blue}{\scriptsize{$\downarrow$ 1.71}}} & {97.10\textcolor{blue}{\scriptsize{$\downarrow$ 0.68}}} & {94.29\textcolor{blue}{\scriptsize{$\downarrow$ 2.05}}} & \multicolumn{1}{c}{81.99\textcolor{blue}{\scriptsize{$\downarrow$ 1.72}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{w/o regularization} & {84.31\textcolor{red}{\scriptsize{$\uparrow$ 0.07}}} & {63.89\textcolor{blue}{\scriptsize{$\downarrow$ 0.04}}} & \multicolumn{1}{c|}{59.88\textcolor{blue}{\scriptsize{$\downarrow$ 0.91}}} & {85.17\textcolor{red}{\scriptsize{$\uparrow$ 0.57}}} & {77.73\textcolor{blue}{\scriptsize{$\downarrow$ 0.27}}} & \multicolumn{1}{c|}{48.21\textcolor{blue}{\scriptsize{$\downarrow$ 1.97}}} & {97.79\textcolor{red}{\scriptsize{$\uparrow$ 0.01}}} & {96.28\textcolor{blue}{\scriptsize{$\downarrow$ 0.06}}} & \multicolumn{1}{c}{82.92\textcolor{blue}{\scriptsize{$\downarrow$ 0.79}}} \\
\bottomrule[1.5pt]
\end{tabular}
% \end{small}%
}
\end{table*}

As observed, the RL training framework in RLEdit is crucial for adapting hypernetworks to lifelong editing. After ablating the RL training framework, RLEdit's performance metrics significantly decrease, essentially losing its lifelong editing capability. Memory backtracking effectively enhances long-sequence editing performance, as evidenced by the decline in Efficacy and Generalization metrics when it is ablated. Regularization primarily serves to maintain LLM's general capabilities after sequential editing, as ablating regularization results in a decrease in the post-edited LLM's general capabilities.

\subsection{More Results of Editing with Varying Numbers of Knowledge}
To evaluate the generalization capabilities of RLEdit across various editing tasks and sequence lengths, we conducted additional experiments. Specifically, we examined the editing performance on tasks of dimensions 20$\times$100, 50$\times$100, 100$\times$100, and 150$\times$100 using Llama-3-8B on three datasets\footnote{Since RLEdit and other hypernetwork-based methods require at least half of the dataset as training data, and given the limited size of the CounterFact dataset, we could only test knowledge editing up to 10,000 samples in the table.}, as shown in Table \ref{tab:app}.

\begin{table*}[pht]
\caption{Lifelong editing results under different numbers of edited knowledge samples. \ding{171} denotes locate-then-edit methods while {\color[HTML]{CB0000} \ding{170}} denotes hypernetwork-based methods. The best results are highlighted in bold, while the second-best results are underlined.}
\label{tab:app}
% \resizebox{\textwidth}{!}{%
% \begin{small}
\begin{adjustbox}{max width=\textwidth}
\begin{sc}
\begin{tabular}{c|cccccccccc}
\toprule[1.5pt]
\multirow{2.5}{*}{\textbf{Task}} & \multicolumn{1}{c}{} & \multicolumn{3}{c|}{\textbf{CounterFact}} & \multicolumn{3}{c|}{\textbf{ZsRE}} & \multicolumn{3}{c}{\textbf{FEVER}} \\ \cmidrule{3-11} 
\textbf{} & \multicolumn{1}{c}{\multirow{-2}{*}{\diagbox{\textbf{Method}}{\textbf{Dataset}}}} & \textbf{Eff.$\uparrow$} & \textbf{Gen.$\uparrow$} & \multicolumn{1}{c|}{\textbf{Spe.$\uparrow$}} & \textbf{Eff.$\uparrow$} & \textbf{Gen.$\uparrow$} & \multicolumn{1}{c|}{\textbf{Spe.$\uparrow$}} & \textbf{Eff.$\uparrow$} & \textbf{Gen.$\uparrow$} & \multicolumn{1}{c}{\textbf{Spe.$\uparrow$}} \\ \midrule[1pt]
\multirow{11}{*}{\rotatebox{90}{{20\ \ding{53}\ 100\ =\ \textbf{2000}}}} & \multicolumn{1}{c|}{FT} & {83.33\std{0.37}} & \underline{67.79\std{0.40}} & \multicolumn{1}{c|}{{\color[HTML]{1F2329} {46.63\std{0.37}}}} & {30.54\std{0.27}} & {30.29\std{0.27}} & \multicolumn{1}{c|}{{15.49\std{0.18}}} & {7.35\std{0.18}} & {6.00\std{0.16}} & \multicolumn{1}{c}{{24.10\std{0.15}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEND\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & {49.20\std{0.42}} & {50.10\std{0.33}} & \multicolumn{1}{c|}{{50.05\std{0.23}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & \underline{37.75\std{0.21}} & \underline{37.78\std{0.30}} & \multicolumn{1}{c}{\underline{29.10\std{0.27}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{ROME\textsuperscript{\ding{171}}} & {64.45\std{0.37}} & {61.42\std{0.44}} & \multicolumn{1}{c|}{49.46\std{0.40}} & {2.00\std{0.11}} & {1.68\std{0.15}} & \multicolumn{1}{c|}{{0.68\std{0.07}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEMIT\textsuperscript{\ding{171}}} & {65.05\std{0.48}} & {64.68\std{0.43}} & \multicolumn{1}{c|}{{52.33\std{0.39}}} & {57.72\std{0.37}} & {52.48\std{0.37}} & \multicolumn{1}{c|}{{25.78\std{0.22}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{PRUNE\textsuperscript{\ding{171}}} & {68.25\std{0.28}} & {64.75\std{0.01}} & \multicolumn{1}{c|}{{49.82\std{0.24}}} & {24.77\std{0.37}} & {23.87\std{0.03}} & \multicolumn{1}{c|}{{20.69\std{0.43}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{RECT\textsuperscript{\ding{171}}} & 64.00\std{0.48} & 61.20\std{0.43} & \multicolumn{1}{c|}{\underline{60.88\std{0.37}}} & 86.02\std{0.24} & 81.81\std{0.27} & \multicolumn{1}{c|}{32.04\std{0.23}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{AlphaEdit\textsuperscript{\ding{171}}} & \textbf{98.90\std{0.10}} & \textbf{94.22\std{0.19}} & \multicolumn{1}{c|}{\textbf{67.88\std{0.29}}} & \textbf{94.47\std{0.13}} & \textbf{91.13\std{0.19}} & \multicolumn{1}{c|}{\underline{32.55\std{0.22}}} & -- & -- & \multicolumn{1}{c}{--} \\
\cmidrule{2-11}
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{\textbf{RLEdit}\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & \underline{{91.75\std{0.21}}} & {62.40\std{0.18}} & \multicolumn{1}{c|}{{52.38\std{0.19}}} & \underline{{88.65\std{0.19}}} & \underline{{83.91\std{0.24}}} & \multicolumn{1}{c|}{\textbf{{47.61\std{0.23}}}} & \textbf{94.46\std{0.18}} & \textbf{91.56\std{0.29}} & \multicolumn{1}{c}{\textbf{69.01\std{0.29}}} \\
\midrule[1pt]
\midrule[1pt]
\multirow{11}{*}{\rotatebox{90}{{50\ \ding{53}\ 100\ =\ \textbf{5000}}}} & \multicolumn{1}{c|}{FT} & {82.10\std{0.19}} & \underline{66.08\std{0.29}} & \multicolumn{1}{c|}{{\color[HTML]{1F2329}{40.35\std{0.36}}}} & {21.05\std{0.24}} & {20.80\std{0.24}} & \multicolumn{1}{c|}{{9.69\std{0.14}}} & {8.40\std{18.72}} & {6.01\std{16.38}} & \multicolumn{1}{c}{{23.69\std{0.15}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEND\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & {49.71\std{0.32}} & {49.63\std{0.33}} & \multicolumn{1}{c|}{{50.23\std{0.49}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & \underline{33.49\std{0.31}} & \underline{28.26\std{0.22}} & \multicolumn{1}{c}{\underline{26.77\std{0.35}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{ROME\textsuperscript{\ding{171}}} & {48.62\std{0.50}} & {49.78\std{0.48}} & \multicolumn{1}{c|}{{51.65\std{0.46}}} & {1.25\std{0.11}} & {1.30\std{0.11}} & \multicolumn{1}{c|}{{1.65\std{0.06}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEMIT\textsuperscript{\ding{171}}} & {64.21\std{0.23}} & {60.07\std{0.43}} & \multicolumn{1}{c|}{{46.64\std{0.37}}} & {0.07\std{0.01}} & {0.07\std{0.01}} & \multicolumn{1}{c|}{{31.67\std{0.22}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{} & \multicolumn{1}{c|}{PRUNE\textsuperscript{\ding{171}}} & {56.07\std{0.34}} & {54.58\std{0.39}} & \multicolumn{1}{c|}{{50.32\std{0.28}}} & {0.09\std{0.01}} & {0.08\std{0.01}} & \multicolumn{1}{c|}{{1.35\std{0.04}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{RECT\textsuperscript{\ding{171}}} & 52.64\std{0.50} & 50.02\std{0.46} & \multicolumn{1}{c|}{\underline{53.59\std{0.42}}} & 0.07\std{0.01} & 0.07\std{0.01} & \multicolumn{1}{c|}{1.34\std{0.03}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{AlphaEdit\textsuperscript{\ding{171}}} & \textbf{94.66\std{0.15}} & \textbf{92.35\std{0.22}} & \multicolumn{1}{c|}{\textbf{62.00\std{0.31}}} & \textbf{93.76\std{0.15}} & \textbf{88.65\std{0.21}} & \multicolumn{1}{c|}{\underline{31.71\std{0.22}}} & -- & -- & \multicolumn{1}{c}{--} \\
\cmidrule{2-11}
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{\textbf{RLEdit}\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & \underline{85.54\std{0.43}} & {60.96\std{0.27}} & \multicolumn{1}{c|}{{48.60\std{0.29}}} & \underline{85.46\std{0.39}} & \underline{{80.68\std{0.44}}} & \multicolumn{1}{c|}{\textbf{{43.35\std{0.21}}}} & \textbf{94.37\std{0.18}} & \textbf{91.84\std{0.22}} & \multicolumn{1}{c}{\textbf{67.58\std{0.27}}} \\
\midrule[1pt]
\midrule[1pt]
\multirow{11}{*}{\rotatebox{90}{{100\ \ding{53}\ 100\ =\ \textbf{10000}}}} & \multicolumn{1}{c|}{FT} & {79.99\std{0.20}} & {62.02\std{0.29}} & \multicolumn{1}{c|}{{\color[HTML]{1F2329} {38.86\std{0.36}}}} & {14.80\std{0.22}} & {14.50\std{0.22}} & \multicolumn{1}{c|}{{5.22\std{0.10}}} & 23.02\std{0.28} & 8.38\std{0.19} & \multicolumn{1}{c}{{22.16\std{0.16}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEND\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & {47.24\std{0.32}} & {47.19\std{0.23}} & \multicolumn{1}{c|}{\textbf{53.10\std{0.35}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & \underline{27.24\std{0.27}} & \underline{28.93\std{0.32}} & \multicolumn{1}{c}{\underline{27.56\std{0.34}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{ROME\textsuperscript{\ding{171}}} & {46.43\std{0.39}} & {40.99\std{0.31}} & \multicolumn{1}{c|}{{46.68\std{0.23}}} & {1.87\std{0.12}} & {1.84\std{0.12}} & \multicolumn{1}{c|}{{1.65\std{0.06}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEMIT\textsuperscript{\ding{171}}} & {47.53\std{0.50}} & {43.59\std{0.49}} & \multicolumn{1}{c|}{{51.30\std{0.48}}} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{PRUNE\textsuperscript{\ding{171}}} & {48.29\std{0.23}} & {48.01\std{0.37}} & \multicolumn{1}{c|}{{50.91\std{0.43}}} & {0.09\std{0.01}} & {0.06\std{0.02}} & \multicolumn{1}{c|}{{1.26\std{0.09}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{RECT\textsuperscript{\ding{171}}} & 49.95\std{0.29} & 50.11\std{0.32} & \multicolumn{1}{c|}{46.27\std{0.25}} & 0.08\std{0.01} & 0.08\std{0.01} & \multicolumn{1}{c|}{1.36\std{0.04}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{AlphaEdit\textsuperscript{\ding{171}}} & \underline{80.17\std{0.21}} & \textbf{75.34\std{0.45}} & \multicolumn{1}{c|}{\underline{52.78\std{0.39}}} & \textbf{89.60\std{0.17}} & \textbf{86.75\std{0.23}} & \multicolumn{1}{c|}{\underline{30.96\std{0.22}}} & -- & -- & \multicolumn{1}{c}{--} \\
\cmidrule{2-11}
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{\textbf{RLEdit}\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & \textbf{80.69\std{0.23}} & \underline{62.31\std{0.27}} & \multicolumn{1}{c|}{{45.16\std{0.31}}} & \underline{{86.45\std{0.32}}} & \underline{{83.07\std{0.19}}} & \multicolumn{1}{c|}{\textbf{{41.96\std{0.34}}}} & \textbf{95.06\std{0.21}} & \textbf{92.26\std{0.18}} & \multicolumn{1}{c}{\textbf{69.19\std{0.30}}} \\
\midrule[1pt]
\midrule[1pt]
\multirow{11}{*}{\rotatebox{90}{{150\ \ding{53}\ 100\ =\ \textbf{15000}}}} & \multicolumn{1}{c|}{FT} & \textbf{89.79\std{0.43}} & \textbf{70.05\std{0.32}} & \multicolumn{1}{c|}{{\color[HTML]{1F2329} {{36.64\std{0.22}}}}} & {13.95\std{0.20}} & {13.49\std{0.20}} & \multicolumn{1}{c|}{{3.93\std{0.08}}} & \underline{44.84\std{0.43}} & \underline{32.19\std{0.42}} & \multicolumn{1}{c}{\underline{33.54\std{0.41}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEND\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & {--} & {--} & \multicolumn{1}{c|}{--} & 0.00\std{0.00}& 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 21.70\std{0.43}& 20.30\std{0.24} & \multicolumn{1}{c}{{13.86\std{0.25}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{ROME\textsuperscript{\ding{171}}} & {46.51\std{0.50}} & {47.10\std{0.47}} & \multicolumn{1}{c|}{\textbf{53.36\std{0.45}}} & {1.47\std{0.11}} & {1.43\std{0.11}} & \multicolumn{1}{c|}{{0.72\std{0.06}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEMIT\textsuperscript{\ding{171}}} & {52.07\std{0.45}} & {50.23\std{0.48}} & \multicolumn{1}{c|}{\underline{48.89\std{0.47}}} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{PRUNE\textsuperscript{\ding{171}}} & {52.19\std{0.24}} & {52.87\std{0.39}} & \multicolumn{1}{c|}{{44.43\std{0.27}}} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{RECT\textsuperscript{\ding{171}}} & 54.49\std{0.30} & 53.24\std{0.31} & \multicolumn{1}{c|}{46.22\std{0.27}} & 0.02\std{0.01} & 0.03\std{0.01} & \multicolumn{1}{c|}{1.02\std{0.21}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{AlphaEdit\textsuperscript{\ding{171}}} & \underline{79.35\std{0.35}} & \underline{69.28\std{0.17}} & \multicolumn{1}{c|}{{42.01\std{0.35}}} & \underline{84.43\std{0.25}} & \underline{78.28\std{0.30}} & \multicolumn{1}{c|}{\underline{28.48\std{0.21}}} & -- & -- & \multicolumn{1}{c}{--} \\
\cmidrule{2-11}
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{\textbf{RLEdit}\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & -- & {--} & \multicolumn{1}{c|}{--} & \textbf{{87.81\std{0.43}}} & \textbf{{85.13\std{0.22}}} & \multicolumn{1}{c|}{\textbf{{40.98\std{0.37}}}} & \textbf{95.11\std{0.26}} & \textbf{91.85\std{0.24}} & \multicolumn{1}{c}{\textbf{68.59\std{0.29}}} \\
\midrule[1pt]
\midrule[1pt]
\multirow{11}{*}{\rotatebox{90}{{200\ \ding{53}\ 100\ =\ \textbf{20000}}}} & \multicolumn{1}{c|}{FT} & \textbf{86.23\std{0.49}} & \textbf{69.77\std{0.31}} & \multicolumn{1}{c|}{{\color[HTML]{1F2329} {{38.94\std{0.33}}}}} & {13.21\std{0.30}} & {11.11\std{0.27}} & \multicolumn{1}{c|}{{4.24\std{0.15}}} & \underline{55.89\std{0.37}} & \underline{50.07\std{0.52}} & \multicolumn{1}{c}{\underline{49.79\std{0.30}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEND\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & {--} & {--} & \multicolumn{1}{c|}{--} & 0.00\std{0.00}& 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 11.08\std{0.27}& 8.01\std{0.11} & \multicolumn{1}{c}{{15.38\std{0.53}}} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{ROME\textsuperscript{\ding{171}}} & {44.43\std{0.39}} & {46.21\std{0.27}} & \multicolumn{1}{c|}{\textbf{50.77\std{0.33}}} & {1.56\std{0.21}} & {1.34\std{0.19}} & \multicolumn{1}{c|}{{1.04\std{0.08}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{MEMIT\textsuperscript{\ding{171}}} & {49.98\std{0.29}} & {46.55\std{0.30}} & \multicolumn{1}{c|}{{45.23\std{0.33}}} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{PRUNE\textsuperscript{\ding{171}}} & {50.21\std{0.22}} & {47.43\std{0.32}} & \multicolumn{1}{c|}{\underline{47.11\std{0.22}}} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{RECT\textsuperscript{\ding{171}}} & 43.32\std{0.29} & 42.21\std{0.37} & \multicolumn{1}{c|}{42.01\std{0.20}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & -- & -- & \multicolumn{1}{c}{--} \\
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{AlphaEdit\textsuperscript{\ding{171}}} & \underline{74.22\std{0.57}} & \underline{63.94\std{0.46}} & \multicolumn{1}{c|}{{39.12\std{0.45}}} & \underline{77.74\std{0.42}} & \underline{70.01\std{0.34}} & \multicolumn{1}{c|}{\underline{27.96\std{0.28}}} & -- & -- & \multicolumn{1}{c}{--} \\
\cmidrule{2-11}
\raisebox{-1.5ex}{\textbf{}} & \multicolumn{1}{c|}{\textbf{RLEdit}\textsuperscript{\color[HTML]{CB0000} \ding{170}}} & -- & {--} & \multicolumn{1}{c|}{--} & \textbf{{91.38\std{0.43}}} & \textbf{{89.93\std{0.29}}} & \multicolumn{1}{c|}{\textbf{{41.98\std{0.30}}}} & \textbf{94.03\std{0.37}} & \textbf{90.67\std{0.42}} & \multicolumn{1}{c}{\textbf{68.71\std{0.41 }}} \\
\bottomrule[1.5pt]
\end{tabular}
\end{sc}
% \end{small}%
% }
\end{adjustbox}
\end{table*}

Comparisons with baseline methods demonstrate that RLEdit maintains relatively stable editing performance across knowledge sequences of any length, while most baselines lose their effectiveness beyond 5,000 samples.

\subsection{General Capability Test on CounterFact}
To comprehensively analyze RLEdit's effectiveness in maintaining LLM's general capabilities, to complement the downstream task evaluations on ZsRE presented in Section \ref{section:4.5}, we conducted GLUE task testing on CounterFact for all methods. All experiments were performed on LLama-3-8B, as illustrated in Figure \ref{fig:downstream-cf}."
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/app-downstream.pdf}
    \caption{General Capability Test on CounterFact.}
    \label{fig:downstream-cf}
\end{figure}

The results demonstrate that on CounterFact, LLMs edited with RLEdit for 3,000 knowledge samples maintained general capabilities comparable to the pre-trained model, whereas most other methods showed significant degradation in performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/app-5k.pdf}
    \caption{RLEdit's final metric scores when editing 5,000 knowledge instances under 7 different configurations.}
    \label{fig:5k}
\end{figure}

\subsection{Impact of Different Edit Frequencies and Batch Sizes on RLEdit}
In lifelong editing, the frequency of edits and batch size are crucial parameters that significantly impact editing success rates. In this section, we investigate how these parameters affect RLEdit's performance. Using the ZsRE dataset, we edited 5,000 knowledge samples on Llama-3-8B with seven different configurations: 10$\times$500, 25$\times$200, 50$\times$100, 100$\times$50, 200$\times$25, 500$\times$10, and 1000$\times$5. Figure \ref{fig:5k} shows how various metrics vary across these configurations.

From Figure \ref{fig:5k}, we observe that RLEdit demonstrates excellent editing performance across all configurations. However, we find that under extreme conditions with very large batch sizes, the hypernetwork training in RLEdit consumes substantial GPU memory, leading to high resource utilization. Similarly, under extreme conditions with very high edit frequencies, the hypernetwork training becomes unstable and often fails to converge. These observations suggest that appropriate edit frequencies and batch sizes should be selected to achieve more effective and stable editing results.


\section{Algorithms}
\label{app:algorithm}
In Sections \ref{section:3.2} and Section \ref{section:3.3}, we introduced the hypernetwork training and editing procedures of RLEdit. The corresponding pseudocode for RLEdit's editing algorithms is presented in Algorithm \ref{alg:edit}.

\begin{algorithm}[tbh]
   \caption{RLEdit Editing}
   \label{alg:edit}
\begin{algorithmic}
   \STATE {\bfseries Input:} Knowledge sequence $(x_i,y_i)_{i=1}^n\in \mathcal{D}$, pre-trained LLM $f_{\mathcal{W}_0}$, post-trained hypernetwork $\mathcal{H}$ with parameter $\theta'$, hyper-parameters $k$, $\gamma$, $\lambda_{loc}$ and $\eta$
   \STATE {\bfseries Output:} Post-edited LLM with parameters $\mathcal{W}_n$
   \FOR{$t=1$ {\bfseries to} $n$}
   \STATE $\mathcal{L}_t\leftarrow -\log{p_{\mathcal{W}_{t-1}}\left(y_t\left|x_t\right.\right)}$
   \STATE Back-propagate $\mathcal{L}_t$ and cache $\nabla_{\mathcal{W}_{t-1}}$
   \STATE $\tilde{\nabla}_{\mathcal{W}_t}\leftarrow \mathcal{H}(\nabla_{\mathcal{W}_{t-1}})$
   \STATE $\mathcal{W}_t\leftarrow \mathcal{W}_{t-1}+\tilde{\nabla}_{\mathcal{W}_t}$
   \ENDFOR
   \STATE \textbf{return} $\mathcal{W}_n$
\end{algorithmic}
\end{algorithm}

\newpage

\section{Detailed Proof}
\label{app:proof}
In Section 3.1, we briefly introduced how hypernetwork-based lifelong editing can be modeled as an MDP. Here we provide detailed proof that hypernetwork-based lifelong editing is indeed an MDP.

In Section 3.1.1, we stated that hypernetwork-based lifelong editing is a Markov process (MP). An MP consists of several elements: (1) a well-defined state space, (2) stationary state transitions, and (3) satisfaction of probability axioms. For (1), we model the state as $(\mathcal{W}, (x, y))$ in hypernetwork-based lifelong editing, or specifically, the fine-tuning gradient $\nabla_\mathcal{W}$. This state is defined in a continuous parameter space that can take tensors of fixed dimensions. Each state has a concrete mathematical representation. For (2), we modify LLM parameters through hypernetwork-generated parameter updates, corresponding to state transitions in MDP; when we add Gaussian noise from a fixed distribution to the hypernetwork output, our transition function's probability distribution becomes stationary. For (3), Gaussian noise satisfies probability axioms (probabilities are non-negative and sum to 1), thus state transitions also satisfy these axioms. In conclusion, hypernetwork-based lifelong editing perfectly aligns with MP modeling and its characteristics.

We now prove that this MP is an MDP. Compared to MP, MDP introduces concepts of action space, state transition function, and reward function. Regarding action space, hypernetwork-based lifelong editing's actions are continuous parameter change matrices with dimensions matching the parameter matrices, which are fixed; since actions are controlled through hypernetwork parameters, they satisfy controllability; parameter change matrices in finite-dimensional real space possess complete metric structure, thus are well-defined. For the state transition function, we have already proven it is well-defined and satisfies probability axioms. The non-deterministic component in the state transition function is Gaussian noise, whose distribution remains unchanged over time, proving state transition stationarity. Regarding the reward function, Section \ref{section:3.1} explicitly detailed its computation method. Since rewards are derived from losses that have bounds, the reward function is bounded; furthermore, the computation method remains constant over time, demonstrating stability. Therefore, hypernetwork-based lifelong editing process constitutes a standard MDP.


\section{More Discussion}
\label{app:discussion}
RLEdit training achieves impressive results with relatively few trajectory samples. This suggests that the lifelong editing task may be simpler than classic RL tasks, while also demonstrating RLEdit's strong generalization capability. This insight inspired us to explore simpler few-shot knowledge editing paradigms.

In our experiments, we found that different choices of LLM editing layers significantly impact editing effectiveness. While locate-then-edit methods \cite{memit} typically choose to edit the first few layers of LLM due to their optimal editing performance, we discovered that RLEdit performs best when editing the middle and final layers of LLM. We believe this insight motivates further research on lifelong editing from the perspective of sequential knowledge localization. Additionally, we will investigate the use of advanced approaches of LLM interpretability \cite{he2024cracking,zhou2024role} to identify and extract crucial layers within the model, enabling targeted editing and refinement for improved performance and adaptability. 

Furthermore, in the future, we aim to apply the reinforcement learning paradigm to a broader range of model editing methods, such as locate-and-edit \cite{anyedit} and memory-based \cite{wise} approaches. Additionally, to address the challenges of updating multi-hop and related knowledge, we plan to enhance RLEdit by integrating external knowledge bases or knowledge graphs \cite{entity,surveygraphrag,differentiable}, which is crucial for further advancements. We will continue to investigate the hallucination and security issues \cite{injectharm, correct} brought by model editing, striving to enhance the reliability and safety of this technology to achieve more responsible AI development.

\newpage

\section{Dataset Visualization through Examples}
To help readers better understand the lifelong editing task and our implementation approach, we provide 3 examples from the ZsRE, FEVER, and CounterFact datasets, as illustrated in Figures \ref{fig:zsre}, \ref{fig:fever}, and \ref{fig:cf}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/app-zsre.pdf}
    \caption{A sample of ZsRE dataset.}
    \label{fig:zsre}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/app-fever.pdf}
    \caption{A sample of FEVER dataset.}
    \label{fig:fever}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/app-cf.pdf}
    \caption{A sample of CounterFact dataset.}
    \label{fig:cf}
\end{figure}
