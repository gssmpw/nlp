\section{Experiments}
\label{section:4}

We conduct extensive experiments to evaluate both the effectiveness and efficiency of our approach. Additionally, we perform ablation studies to analyze the contribution of each component in RLEdit, which can be found in Appendix \ref{app:ablation}.

\subsection{Experimental Settings}
We begin with a brief overview of the LLMs, datasets, evaluation metrics, and baseline methods used in our experiments. More detailed information can be found in Appendix \ref{app:setup}.

\textbf{Base LLMs.} We conduct experiments on three 8B-scale autoregressive LLMs: Llama-3-8B\footnote{\href{https://llama.meta.com/llama3}{https://llama.meta.com/llama3}}, Gemma-2-9B \cite{gemma2}, and Mistral-7B-v0.3 \cite{mistral7b}.

\textbf{Datasets \& Evaluation Metrics.} We evaluate RLEdit on three widely-used datasets: ZsRE \cite{zsre}, FEVER \cite{fever}, and CounterFact \cite{rome}. Following previous evaluation standards \cite{mend, rome, memit}, we use three metrics to assess editing success for each dataset: Efficacy, Generalization, and Specificity, as described in Appendix \ref{app:metric}.

\textbf{Baseline Methods.} We compare RLEdit against multiple editing methods, including FT \cite{modifying}, ROME \cite{rome}, MEND \cite{mend}, MALMEN \cite{malmen}, DAFNet \cite{dafnet}, MEMIT \cite{memit}, PRUNE \cite{prune}, RECT \cite{rect}, and AlphaEdit \cite{alphaedit}.


\subsection{Performance on Knowledge Update and Preservation}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/exp-radar.pdf}
    \captionsetup{skip=8pt}
    \caption{Performance comparison of editing methods across multiple metrics: Efficacy, Generalization, Specificity, General Capability, and Efficiency. (a)-(d) show results for 20$\times$100, 50$\times$100, 100$\times$100, and 150$\times$100 configurations. For General Capability, we measure that using the average of six GLUE metrics in Section \ref{section:4.5}; for Efficiency, we calculate the ratio between the total editing time of each method and that of the fastest method. Best viewed in color.}
    \label{fig:rader}
\end{figure}
To evaluate the performance on both updating of target knowledge and preservation of unrelated information in lifelong editing tasks, we conducted sequential editing experiments comparing RLEdit against baseline methods on three LLMs, measuring performance across all metrics. We randomly sampled 8,000 knowledge samples from ZsRE and FEVER respectively, performing edits over 400 batches with 20 knowledge samples per batch (denoted as a 400$\times$20 configuration throughout this paper). Upon completion of all batch edits, we evaluate all knowledge metrics on the post-edited LLM. Results are presented in Table \ref{tab:2}. The ``Time'' column in Table \ref{tab:2} indicates the average editing time per knowledge sample, including the time spent on covariance matrices computing or hypernetwork training.

In Table \ref{tab:2}, conventional single-step methods (\eg, ROME, MEND, MEMIT) demonstrate poor performance, with Efficacy and Generalization metrics falling below 50\%, revealing their inability to handle knowledge conflicts and forgetting issues in lifelong editing scenarios. Other sequential editing methods (\eg, PRUNE, RECT, DAFNet) show varying limitations across datasets, failing to maintain satisfactory performance across all metrics. In contrast, RLEdit achieves superior results across all tested LLMs and datasets, maintaining strong performance in all three metrics. Specifically, RLEdit demonstrates average improvements of 66\% in Efficacy, 65\% in Generalization, and 40\% in Specificity compared to baseline methods, while requiring only 4\% of the time compared to most of them.

\subsection{Editing with Varying Numbers of Knowledge}
To verify RLEdit's versatility across different lifelong editing scenarios, we investigated its performance under multiple configurations. We conducted extensive experiments on Llama-3-8B using 20$\times$100, 50$\times$100, 100$\times$100, and 150$\times$100 configurations on ZsRE dataset. The results are presented in Figure \ref{fig:rader}. We also performed experiments to investigate the impact of varying knowledge batch sizes on performance. Detailed results can be found in Appendix \ref{app:results}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/exp-time.pdf}
    \caption{Per-sample editing time comparison across methods. Best viewed in color.}
    \label{fig:time}

\end{figure}

Figure \ref{fig:rader} shows that as the number of edits increases, baseline methods show progressive performance degradation, with catastrophic forgetting occurring beyond certain thresholds. RLEdit, however, consistently demonstrates superior performance across all configurations, outperforming baseline methods on most evaluation metrics. RLEdit represents the first method to successfully scale lifelong editing beyond 10,000 knowledge samples. Unlike existing approaches, RLEdit maintains stable performance as knowledge quantity increases, suggesting potential applicability to even longer sequences. These results demonstrate RLEdit's effectiveness for long-term lifelong editing tasks.


\subsection{Editing Efficiency}
\label{section:4.4}
To evaluate RLEdit's efficiency, we compared the average editing time per knowledge sample across most methods. Tests were conducted on 3 LLMs using ZsRE dataset under 20$\times$100 configuration while keeping all other variables constant (such as the number of LLM editing layers) to ensure fair comparison. For locate-then-edit methods, the time included covariance matrix computation, parameter update calculation, and edit execution. For hypernetwork-based methods, we measured hypernetwork training, parameter update calculation, and edit execution time. Since covariance matrices and trained hypernetworks can be reused, we also analyzed editing time excluding these initial setup costs. Results are presented in Figure \ref{fig:time}. 

Hypernetwork-based methods demonstrate superior speed compared to locate-then-edit approaches, primarily due to better generalization capabilities. Locate-then-edit methods require complex matrix operations for each edit, resulting in slower execution. RLEdit requires only 0.145 seconds per sample for training, outperforming other hypernetwork-based methods like MEND, MALMEN, and DAFNet. This efficiency derives from RLEdit's sequence-based approach for training, enabling better generalization with reduced training data. RLEdit achieves superior editing results while requiring only 1.14\% of the time needed by locate-then-edit methods, demonstrating its exceptional efficiency and promising potential in long-term lifelong editing scenarios.


\subsection{General Capability Tests}
\label{section:4.5}
To evaluate how lifelong editing affects LLMs' general capabilities, we assessed downstream performance using 6 tests from GLUE \cite{glue}: SST \cite{sst}, MMLU \cite{mmlu}, MRPC \cite{mrpc}, CoLA \cite{cola}, RTE \cite{rte}, and NLI \cite{nli}.  We measured F1 Scores on Llama-3-8B using ZsRE dataset under 30$\times$100 configuration, and the results are shown in Figure \ref{fig:downstream}. Complete tests are available in Appendix \ref{app:results}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/exp-downstream.pdf}
    \caption{LLM capability assessment using F1 scores on six GLUE tasks. (a)-(f) shows results for SST, MMLU, MRPC, CoLA, RTE, and NLI respectively. Best viewed in color.}
    \label{fig:downstream}
\end{figure}

Baseline methods show progressive degradation of general capabilities as edited knowledge samples increase, with some methods leading to complete performance collapse. This degradation likely stems from cumulative editing errors, caused by causal trace instabilities or imprecise hypernetwork training. In contrast, RLEdit maintains consistent performance across all tests, with LLMs edited for 3,000 knowledge samples performing comparable to their pre-trained versions. This stability, achieved through regularization, demonstrates RLEdit's ability to both effectively edit target knowledge and preserve the model's general capabilities.


\subsection{Improving MEND/MALMEN}
\label{sec:4.6}
While existing hypernetwork-based editing methods focus on single-edit scenarios, RLEdit's RL framework and training methodology specifically address lifelong editing challenges, which could integrate with other hypernetwork-based methods as a plug-and-play module. To demonstrate this portability, we conducted 20$\times$100 editing experiments across 3 LLMs and 3 datasets, comparing MEND, MEND*, MEND+RLEdit, MALMEN, MALMEN*, and MALMEN+RLEdit. Results are shown in Figure \ref{fig:plug}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/exp-plug.pdf}
    \caption{Performance comparison of hypernetwork-based methods (MEND and MALMEN) with and without RLEdit framework integration, including variants with hypernetwork retraining before each edit. Results presented in the figure show the weighted average of metrics across CounterFact and ZsRE, where Efficacy and Generalization are assigned weights of 1, and Specificity is assigned a weight of 0.5. Best viewed in color.}
    \label{fig:plug}
\end{figure}

The results demonstrate RLEdit's portability across several methods. After incorporating RLEdit's training methodology, MEND and MALMEN showed average improvements of 22.89\% and 51.86\% respectively across all metrics, despite their initial suboptimal performance in lifelong editing tasks. This confirms RLEdit's effectiveness as a plug-and-play module for enabling lifelong editing capabilities in both existing and future hypernetwork-based methods.
