\section{Method}
In this section, we elaborate \textbf{RLEdit}, a method that achieves efficient and effective lifelong editing through training a hypernetwork adapted to knowledge sequences. We begin by introducing how to establish a reinforcement learning (RL) paradigm for hypernetworks training in lifelong editing (Section \ref{section:3.1}), which includes the key component of RLEdit: reward function design. We then describe RLEdit's training strategy in Section \ref{section:3.2}. Finally, in Section \ref{section:3.3}, we detail the practical implementation and considerations of RLEdit.


\subsection{Modeling Lifelong Editing as an RL Task}
\label{section:3.1}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/method.pdf}
    \caption{Overview of lifelong editing with RLEdit. (a) illustrates the training process of RLEdit's hypernetwork, while (b) demonstrates how the trained hypernetwork performs lifelong editing. Best viewed in color.}
    \label{fig:method}
\end{figure*}

Building an RL paradigm for hypernetworks training in lifelong editing aims to capture dynamic changes in LLM and adaptively generate updates that fit the current parameter state, as mentioned in Section \ref{sec:1}. Two key aspects enable this: (1) modeling the hypernetwork training process in lifelong editing as a Markov Decision Process (MDP) \cite{mdp,mdp-plus}, which RL excels at solving \cite{rl}, and (2) carefully designing a reward function tailored to the lifelong editing task.

\subsubsection{MDP Formulation}
The training process of hypernetwork $\mathcal{H}$ in lifelong editing can be formalized as the following procedure:
\begin{enumerate}
    \item At the $t$-th edit (\ie{}, time step $t$), new knowledge sample $(x_t,y_t)$ is input to the LLM with parameters $\mathcal{W}_{t-1}$ to collect fine-tuning gradient $\nabla_{\mathcal{W}_{t-1}}$;
    \item $\nabla_{\mathcal{W}_{t-1}}$ is fed into $\mathcal{H}$ to get parameter update $\tilde{\nabla}_{\mathcal{W}_t}$;
    \item $\tilde{\nabla}_{\mathcal{W}_t}$ is used to update $\mathcal{W}_{t-1}$ to produce $\mathcal{W}_t$;
    \item The loss of $(x_t,y_t)$ on the LLM with parameters $\mathcal{W}_t$ is computed to update the hypernetwork.
\end{enumerate}
This process iterates until all knowledge in the training set is traversed. We can observe that the above process only depends on $\mathcal{W}_{t-1}$ and $(x_t, y_t)$ at time step $t$, \ie, it is independent of states from time step $0$ to $t-2$. Therefore, it naturally satisfies the Markov property. Formally:
\begin{equation}
\begin{aligned}
& Pr\!\left[\mathcal{H}(\nabla_{\mathcal{W}_{t-1}}){=}\tilde{\nabla}_{\mathcal{W}_{t}}\!\left|\right.\mathcal{H}(\nabla_{\mathcal{W}_{t-2}}){=}\tilde{\nabla}_{\mathcal{W}_{t-1}},\right. \\
& \left.\dots,\mathcal{H}(\nabla_{\mathcal{W}_{0}}){=}\tilde{\nabla}_{\mathcal{W}_{1}}\right] \\
= & Pr\!\left[\mathcal{H}(\nabla_{\mathcal{W}_{t-1}}){=}\tilde{\nabla}_{\mathcal{W}_{t}}\!\left|\right.\mathcal{H}(\nabla_{\mathcal{W}_{t-2}}){=}\tilde{\nabla}_{\mathcal{W}_{t-1}}\right].
\end{aligned}
\end{equation}

\begin{table}[t]
% \begin{small}
\caption{The correspondence between MDP elements and hypernetwork-based lifelong editing components.}
% \begin{sc}
  \label{tab:1}
  \centering
  \begin{tabular}{cc}
    \toprule
    \textbf{Markov Decision Process} & \textbf{Lifelong Editing} \\
    \midrule
    Agent & $\mathcal{H}$ \\
    Environment & $f_\mathcal{W}$ \\
    Policy $\pi$ & $\theta$ \\
    Action $\mathcal{A}$ & $\tilde{\nabla}_\mathcal{W}$ \\
    State $\mathcal{S}$ & $\left(\mathcal{W}, (x, y)\right)$ or $\nabla_{\mathcal{W}}$ \\
    Reward $\mathcal{R}$ & $-\mathcal{L}$ \\
    \bottomrule
  \end{tabular}
% \end{sc}
% \end{small}
\end{table}
% \vskip -0.3in


To further extend the above Markov process into an MDP, we introduce several concepts: state $s\in\mathcal{S}$, action $a\in\mathcal{A}$, reward $r\in\mathcal{R}$, and policy $\pi$, which are formalized in Table \ref{tab:1}. Here, $\theta$ donates the parameters of $\mathcal{H}$ and $\mathcal{L}$ represents the loss on LLM for $(x, y)$. Given $n$ new knowledge samples $(x_i, y_i)_{i=1}^n$ in the training set, the interactions between $s$, $a$, and $r$ over $n$ time steps constitute a trajectory of the MDP:
\begin{equation}
\begin{aligned}
& \left\{\!\left(\mathcal{W}_0,\left(x_1,y_1\right)\right), {\tilde{\nabla}_{\mathcal{W}_1}}, -\mathcal{L}_1, \dots,\right. \\
& \left.\left(\mathcal{W}_{n-1},\left(x_n,y_n\right)\right), {\tilde{\nabla}_{\mathcal{W}_n}}, -\mathcal{L}_n\!\right\} \\
\mapsto & \left\{s_1, a_1, r_1, \dots, s_n, a_n, r_n\right\}.
\end{aligned}
\end{equation}

More specifically, in each time step, agent $\mathcal{H}$ employs parameters $\theta$ as policy $\pi$ to generate action $\tilde{\nabla}_\mathcal{W}$ based on system state $\left\{\mathcal{W},\left(x,y\right)\right\}$. This action is then applied to environment $f_\mathcal{W}$, resulting in a state transition:
\begin{equation}
\pi (s_t)=a_t,\ s_t+a_t\rightarrow s_{t+1}.
\end{equation}
Meanwhile, the reward function $R$ computes the reward $r_i$ for this interaction:
\begin{equation}
R(s_t,a_t)=r_t.
\end{equation}
This process is illustrated in Figure \ref{fig:method}(a). Through this modeling approach, we have successfully formulated the training process of hypernetworks in lifelong editing as an MDP. For more detailed proof, please refer to Appendix \ref{app:proof}.

\subsubsection{Design of the Reward Function}
Next, we focus on the design of the reward function $R$ mentioned above, which is crucial for RLEdit to achieve both efficient and effective editing. First, let's recall the fundamental objective of RL: to determine an optimal policy $\pi$ for the agent, where $\pi$ specifies which action $a\sim\pi(s)$ the agent should execute in state $s$ \cite{rl}. The reward function serves as a quantitative measure of policy performance. Specifically, the optimization objective of an RL system is to maximize the total reward $J$ over the entire trajectory. In this work, to align the RL optimization objective with the lifelong editing objective, we design a multi-component reward function for RLEdit as follows.

\textbf{Basic Component.} We first incorporate two fundamental objectives of model editing as the basic components of the reward function: target knowledge updating and unrelated knowledge preservation. Formally:
\begin{equation}
\begin{aligned}
\mathcal{L}_{e} & =-\log p_\mathcal{W}\left(y_e \mid x_e \right), \\
\mathcal{L}_{\mathit{loc}} &=\mathrm{KL} \left[ p_{\mathcal{W}_0}(\cdot|x_{\mathit{loc}}) \, \|\, p_{\mathcal{W}}(\cdot|x_{\mathit{loc}}) \right],
\end{aligned}
\end{equation}
where $\mathcal{L}_e$ and $\mathcal{L}_{\mathit{loc}}$ measure the effectiveness of target knowledge updating and unrelated knowledge preservation, respectively; $(x_e,y_e)$ represents the equivalence neighborhood of target knowledge, and $x_{\mathit{loc}}$ represents unrelated knowledge, both derived from the previously mentioned input knowledge sample $(x,y)$; $\mathcal{W}_0$ denotes the pre-trained parameters of the LLM. Following prior hypernetwork-based editing methods \cite{mend, malmen, dafnet}, we introduce a coefficient $\lambda_{\mathit{loc}}$ to balance the trade-off between these two terms:
\begin{equation}
\mathcal{L}_{\mathit{base}}=\mathcal{L}_e+\lambda_{\mathit{loc}}\mathcal{L}_{\mathit{loc}}.
\label{equ:1}
\end{equation}

\textbf{Memory Backtracking Component.} In lifelong editing, subsequent edits may interfere with previous ones, potentially degrading the effectiveness of earlier modifications. To address this issue and avoid focusing solely on current knowledge while ignoring previously edited knowledge, we propose a memory backtracking component $\mathcal{L}_{back}$.  Specifically, at time step $t$, we not only compute the basic loss of current knowledge $(x_t,y_t)$ on the current LLM $f_{\mathcal{W}_{t-1}}$ according to Equation \ref{equ:1}, but also calculate the loss of previous $k$ knowledge samples $(x_i,y_i)_{i=t-k}^{t-1}$ on $f_{\mathcal{W}_{t-1}}$ as the backtracking term using the same equation. In essence, at each time step, we simultaneously consider the basic losses of both current knowledge and the previous $k$ pieces of knowledge on the LLM. Considering that previous knowledge has already  been edited and only needs review rather than re-editing, we introduce a decay factor $\mu$, which assigns weights to the losses from previous knowledge through exponential decay based on temporal distance:
\begin{equation}
\mathcal{L}_{\mathit{back}_t}=\sum_{i=t-k}^{t-1}{\mu^{t-i}\left(\mathcal{L}_{e_i,\mathcal{W}_{t-1}}+\lambda_{\mathit{loc}}\mathcal{L}_{\mathit{loc}_i,\mathcal{W}_{t-1}}\right)}.
\end{equation}
Ablation studies in Section \ref{section:4} demonstrate that the generalization improvement from the memory backtracking component is one of the key factors enabling RLEdit to maintain effectiveness through more than 10,000 edits.

\textbf{Regularization Component.} Finally, to constrain the magnitude of updates $\tilde{\nabla}_\mathcal{W}$ generated by the hypernetwork at each time step, we introduce the $\ell_2$ norm $\|\tilde{\nabla}_\mathcal{W}\|^2$ of $\tilde{\nabla}_\mathcal{W}$ as a regularization term. The ablation study in Section \ref{section:4} shows two benefits of this regularization: First, limiting the magnitude of $\tilde{\nabla}_\mathcal{W}$ minimizes the disruption to the original parameter distribution, thereby preserving the LLM's general capabilities; Second, it enhances training stability, ensuring the convergence of the hypernetwork.

Based on the above three components, the reward $r_t$ at time step $t$ is formulated as:
\begin{equation}
r_t=-(\mathcal{L}_{\mathit{base}_t}+\mathcal{L}_{\mathit{back}_t}+\eta\|\tilde{\nabla}_{W_t}\|^2),
\label{equ:2}
\end{equation}
where $\eta$ serves as the regularization coefficient. With this reward function, the hypernetwork-based lifelong editing task has been completely formulated as an RL problem.


\subsection{Training Process of RLEdit}
\label{section:3.2}
The most straightforward training approach would be to optimize the policy (\ie, hypernetwork parameters $\theta$) using gradient ascent after obtaining rewards via Equation \ref{equ:2} at each time step. This online reinforcement learning algorithm, however, becomes computationally expensive when applied at each time step, given that training sets typically contain substantial knowledge to ensure generalization. Moreover, this approach may cause the hypernetwork to overfit specific knowledge samples, thereby reducing its effectiveness in generating general parameter updates. We therefore optimize collectively after traversing the dataset and collecting all rewards along the trajectory. More formally:
\begin{align}
\theta'=&\mathop{\arg\max}\limits_{\theta}J, \\
J{=}\!\sum_{i=1}^{n}{\gamma^i r_i}{=}\!-\!\!\sum_{i=1}^n\gamma^i(&\mathcal{L}_{base_i}{+}\mathcal{L}_{back_i}{+}\eta\|\tilde{\nabla}_{W_i}\|^2),
\label{equ:3}
\end{align}
where $J$ denotes the total reward over the entire trajectory, $\theta'$ represents the optimized hypernetwork parameters, and $\gamma$ represents the discount factor.

This training approach significantly accelerates hypernetwork convergence. We experimentally demonstrate that RLEdit achieves a 100-fold efficiency improvement over current lifelong editing methods primarily due to this strategy. After updates of the hypernetwork using $J$ in Equation \ref{equ:3}, it gradually learns how to edit LLMs with varying parameter states. Moreover, after capturing the intrinsic relationships between knowledge samples, our offline update approach enables policy optimization from a higher-level sequential perspective. Ablation studies in Section \ref{section:4} indicate that this update approach substantially enhances the hypernetwork's effectiveness in lifelong editing.

\begin{algorithm}[t]
   \caption{RLEdit Hypernetwork Training}
   \label{alg:train}
\begin{algorithmic}
   \STATE {\bfseries Input:} Pre-trained LLM $f_{\mathcal{W}_0}$, hypernetwork $\mathcal{H}$ with initial parameter $\theta$, hyperparameters $k$, $\gamma$, $\lambda_{\mathit{loc}}$ and $\eta$
   \STATE {\bfseries Output:} Optimized hypernetwork parameter $\theta'$
   \REPEAT
   \STATE Randomly sample $(x_i,y_i,x_{e_i},y_{e_i},x_{\mathit{loc}_i})_{i=1}^n$
   \FOR{$t=1$ {\bfseries to} $n$}
   \STATE $\mathcal{L}_t\leftarrow -\log{p_{\mathcal{W}_{t-1}}\left(y_t\left|x_t\right.\right)}$
   \STATE Back-propagate $\mathcal{L}_t$ and cache $\nabla_{\mathcal{W}_{t-1}}$
   \STATE $\tilde{\nabla}_{\mathcal{W}_t}\leftarrow \mathcal{H}(\nabla_{\mathcal{W}_{t-1}})$
   \STATE $\mathcal{W}_t\leftarrow \mathcal{W}_{t-1}+\tilde{\nabla}_{\mathcal{W}_t}$
   \FOR{$i=t-k$ {\bfseries to} $t$}
   \STATE $\mathcal{L}_{e_i}\leftarrow -\log{p_{\mathcal{W}_{t}}\left(y_{e_i}\left|x_{e_i}\right.\right)}$
   \STATE $\mathcal{L}_{{loc}_i}\leftarrow \textsc{KL}\left(p_{\mathcal{W}_0}(\cdot\left|x_{\mathit{loc}_i}\right.)\|p_{\mathcal{W}_{t}}(\cdot\left|x_{\mathit{loc}_i}\right.)\right)$
   \ENDFOR
   \STATE $\mathcal{L}_t\leftarrow \sum_{i=t-k}^{t}\left(\mathcal{L}_{e_i}+\lambda_{\mathit{loc}}\mathcal{L}_{\mathit{loc}_{i}}\right)$
   \STATE $r_t\leftarrow -(\mathcal{L}_t+\eta\|\tilde{\nabla}_{\mathcal{W}_t}\|^2)$
   \ENDFOR
   \STATE $J\leftarrow \sum_{t=1}^n\gamma^t{r_t}$
   \STATE Back-propagate $J$ and update $\theta$
   \UNTIL{hypernetwork convergence}\\
   \STATE \textbf{return} $\theta'$
\end{algorithmic}
\end{algorithm}


In summary, the training of RLEdit hypernetwork follows this process. At each time step $t$: (1) We collect gradients $\nabla_{\mathcal{W}_{t-1}}$ from new knowledge sample $(x_t,y_t)$ on LLM $f_{\mathcal{W}_{t-1}}$ through one step of parameter-frozen fine-tuning; (2) Through hypernetwork $\mathcal{H}$, we map $\nabla_{\mathcal{W}_{t-1}}$ to LLM parameter updates $\tilde{\nabla}_{\mathcal{W}_t}$, adding it to $\mathcal{W}_{t-1}$ to obtain $\mathcal{W}_{t}$; (3) Calculate reward $r_t$ on $f_{\mathcal{W}_{t}}$ using Equation \ref{equ:2}; (4) Repeat steps (1)-(3) until traversing the entire training set, obtaining total reward $J$ via Equation \ref{equ:3}; (5) Optimize hypernetwork parameters $\theta$ using $J$ through stochastic gradient descent; (6) Repeat steps (1)-(5) until hypernetwork convergence. This training process yields the final hypernetwork parameters $\theta'$ that demonstrate strong adaptability to lifelong editing tasks. The pseudo-code is provided in Algorithm \ref{alg:train}. 


\begin{table*}[t]
\caption{Comparison of RLEdit and baseline methods on lifelong editing tasks. The upper section represents fine-tuning and locate-then-edit methods, while the lower represents hypernetwork-based methods. MEND* and MALMEN* represent methods where the hypernetworks of MEND and MALMEN are retrained for each new knowledge editing.}

\label{tab:2}
\resizebox{\textwidth}{!}{%
% \begin{small}
% \begin{sc}
\begin{tabular}{c|cccccccccc}
\toprule[1.5pt]
\multicolumn{1}{l|}{\multirow{4}{*}{\textbf{Methods}}} & \multicolumn{10}{c}{\textbf{FEVER}} \\ \cmidrule{2-11} 
\multicolumn{1}{l|}{} & \multicolumn{3}{c|}{\textbf{LLaMA-3-8B}} & \multicolumn{3}{c|}{\textbf{Gemma-2-9B}} & \multicolumn{3}{c|}{\textbf{Mistral-7B-v0.3}} &  \\ \cmidrule{2-10}
\multicolumn{1}{l|}{} & \textbf{Eff.} & \textbf{Gen.} & \multicolumn{1}{c|}{\textbf{Spe.}} & \textbf{Eff.} & \textbf{Gen.} & \multicolumn{1}{c|}{\textbf{Spe.}} & \textbf{Eff.} & \textbf{Gen.} & \multicolumn{1}{c|}{\textbf{Spe.}} & \multirow{-2}{*}{\textbf{Time}$\downarrow$} \\  \midrule[0.8pt]
\multicolumn{1}{c|}{FT} & {1.80\std{0.10}} & {6.39\std{0.17}} & \multicolumn{1}{c|}{{26.33\std{0.13}}} & {34.23\std{0.31}} & {29.22\std{0.30}} & \multicolumn{1}{c|}{{34.23\std{0.31}}} & 17.08\std{0.24} & 21.57\std{0.30} & \multicolumn{1}{c|}{37.47\std{0.32}} & \underline{0.3358s} \\
\multicolumn{1}{c|}{ROME} & {38.56\std{0.28}} & {44.53\std{0.29}} & \multicolumn{1}{c|}{{9.29\std{0.17}}} & {30.07\std{0.26}} & {25.13\std{0.11}} & \multicolumn{1}{c|}{{10.79\std{0.24}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 2.1662s \\
\multicolumn{1}{c|}{MEMIT} & {0.18\std{0.02}} & {0.01\std{0.01}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & {11.12\std{0.26}} & {10.09\std{0.25}} & \multicolumn{1}{c|}{8.04\std{0.21}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 6.3423s \\ 
\multicolumn{1}{c|}{PRUNE} & {56.64\std{0.24}} & {43.31\std{0.18}} & \multicolumn{1}{c|}{{0.85\std{0.05}}} & {13.34\std{0.25}} & {11.17\std{0.27}} & \multicolumn{1}{c|}{{9.43\std{0.17}}} & 5.27\std{0.34} & 3.11\std{0.16} & \multicolumn{1}{c|}{5.89\std{0.23}} & 6.3356s \\ 
\multicolumn{1}{c|}{RECT} & 60.95\std{0.27} & 52.40\std{0.26} & \multicolumn{1}{c|}{1.75\std{0.07}} & 59.81\std{0.23} & 54.89\std{0.15} & \multicolumn{1}{c|}{0.05\std{0.01}} & 0.55\std{0.04} & 0.05\std{0.01} & \multicolumn{1}{c|}{0.00\std{0.00}} & 6.0486s \\ 
\multicolumn{1}{c|}{AlphaEdit} & \underline{94.22\std{0.25}} & \textbf{94.14\std{0.18}} & \multicolumn{1}{c|}{25.57\std{0.14}} & 94.37\std{0.26} & 88.34\std{0.13} & \multicolumn{1}{c|}{31.21\std{0.54}} & \underline{32.74\std{0.42}} & \underline{30.03\std{0.29}} & \multicolumn{1}{c|}{8.44\std{0.45}} & 6.2307s \\ \midrule[0.3pt] 
\multicolumn{1}{c|}{MEND} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & 50.93\std{0.43} & 50.76\std{0.27} & \multicolumn{1}{c|}{{0.36\std{0.04}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 0.9175s \\ 
\multicolumn{1}{c|}{MEND*} & {10.37\std{0.19}} & {9.34\std{0.22}} & \multicolumn{1}{c|}{{4.88\std{0.24}}} & {50.87\std{0.24}} & {52.01\std{0.11}} & \multicolumn{1}{c|}{{0.39\std{0.04}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 6.1280s \\
\multicolumn{1}{c|}{MALMEN} & {0.01\std{0.01}} & {{0.01\std{0.01}}} & \multicolumn{1}{c|}{{{0.14\std{0.03}}}} & \underline{94.56\std{0.09}} & \underline{91.94\std{0.16}} & \multicolumn{1}{c|}{{68.65\std{0.33}}} & 16.67\std{0.21} & 16.61\std{0.18} & \multicolumn{1}{c|}{12.16\std{0.16}} & 1.9858s \\ 
\multicolumn{1}{c|}{MALMEN*} & {5.74\std{0.20}} & {5.29\std{0.13}} & \multicolumn{1}{c|}{{1.26\std{0.18}}} & {88.32\std{0.33}} & {84.29\std{0.33}} & \multicolumn{1}{c|}{\textbf{69.70\std{0.27}}} & 17.73\std{0.24} & 13.30\std{0.27} & \multicolumn{1}{c|}{13.85\std{0.21}} & 9.3358s \\
\multicolumn{1}{c|}{DAFNet} & {31.27\std{0.47}} & {28.82\std{0.43}} & \multicolumn{1}{c|}{{\underline{66.55\std{0.41}}}} & {20.78\std{0.31}} & {19.99\std{0.35}} & \multicolumn{1}{c|}{{53.10\std{0.56}}} & 4.86\std{0.14} & 4.21\std{0.19} & \multicolumn{1}{c|}{\underline{41.71\std{0.48}}} & 8.2553s \\ \midrule[0.8pt] 
\multicolumn{1}{c|}{\textbf{RLEdit}} & \textbf{{95.34\std{0.34}}} & \underline{93.58\std{0.38}} & \multicolumn{1}{c|}{\textbf{70.36\std{0.29}}} & \textbf{{95.44\std{0.21}}} & \textbf{{92.83\std{0.30}}} & \multicolumn{1}{c|}{\underline{{68.76\std{0.18}}}} & \textbf{88.99\std{0.22}} & \textbf{88.25\std{0.25}} & \multicolumn{1}{c|}{\textbf{73.64\std{0.11}}} & \textbf{0.2238s} \\ 
\midrule[1pt]
\midrule[1pt]
\multicolumn{1}{l|}{\multirow{4}{*}{\textbf{Methods}}} & \multicolumn{10}{c}{\textbf{ZsRE}} \\ \cmidrule{2-11} 
\multicolumn{1}{l|}{} & \multicolumn{3}{c|}{\textbf{LLaMA-3-8B}} & \multicolumn{3}{c|}{\textbf{Gemma-2-9B}} & \multicolumn{3}{c|}{\textbf{Mistral-7B-v0.3}} &  \\ \cmidrule{2-10}
\multicolumn{1}{l|}{} & \textbf{Eff.} & \textbf{Gen.} & \multicolumn{1}{c|}{\textbf{Spe.}} & \textbf{Eff.} & \textbf{Gen.} & \multicolumn{1}{c|}{\textbf{Spe.}} & \textbf{Eff.} & \textbf{Gen.} & \multicolumn{1}{c|}{\textbf{Spe.}} & \multirow{-2}{*}{\textbf{Time}$\downarrow$} \\ \midrule[0.8pt]
\multicolumn{1}{c|}{FT} & {17.10\std{0.22}} & {16.73\std{0.22}} & \multicolumn{1}{c|}{{8.27\std{0.13}}} & {12.90\std{0.20}} & {13.09\std{0.20}} & \multicolumn{1}{c|}{{0.07\std{0.02}}} & \underline{32.84\std{0.30}} & \underline{33.78\std{0.30}} & \multicolumn{1}{c|}{\textbf{42.19\std{0.31}}} & \underline{0.3366s} \\
\multicolumn{1}{c|}{ROME} & {0.54\std{0.04}} & {0.57\std{0.04}} & \multicolumn{1}{c|}{{0.40\std{0.02}}} & {3.45\std{0.32}} & {3.33\std{0.11}} & \multicolumn{1}{c|}{{8.24\std{0.29}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 2.5621s \\
\multicolumn{1}{c|}{MEMIT} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.13\std{0.02}}} & {5.23\std{0.21}} & {3.11\std{0.12}} & \multicolumn{1}{c|}{{5.98\std{0.12}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.13\std{0.02}} & 6.0677s \\ 
\multicolumn{1}{c|}{PRUNE} & {12.27\std{0.43}} & {12.01\std{0.23}} & \multicolumn{1}{c|}{{9.88\std{0.29}}} & {10.21\std{0.27}} & {8.88\std{0.29}} & \multicolumn{1}{c|}{{11.95\std{0.55}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 6.1588s \\ 
\multicolumn{1}{c|}{RECT} & 11.05\std{0.41} & 8.12\std{0.15} & \multicolumn{1}{c|}{28.12\std{0.13}} & 12.45\std{0.45} & 10.11\std{0.51} & \multicolumn{1}{c|}{26.09\std{0.44}} & 8.18\std{0.33} & 8.04\std{0.46} & \multicolumn{1}{c|}{11.32\std{0.49}} & 6.6558s \\ 
\multicolumn{1}{c|}{AlphaEdit} & \underline{86.83\std{0.23}} & \underline{81.48\std{0.28}} & \multicolumn{1}{c|}{29.09\std{0.22}} & \underline{81.18\std{0.33}} & \underline{73.24\std{0.46}} & \multicolumn{1}{c|}{30.34\std{0.19}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 6.1831s \\ \midrule[0.3pt] 
\multicolumn{1}{c|}{MEND} & {0.00\std{0.00}} & {0.00\std{0.00}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 0.9686s \\ 
\multicolumn{1}{c|}{MEND*} & {0.00\std{0.00}} & {\color[HTML]{1F2329} {0.00\std{0.00}}} & \multicolumn{1}{c|}{{0.00\std{0.00}}} & {8.84\std{0.24}} & {8.45\std{0.21}} & \multicolumn{1}{c|}{{10.21\std{0.19}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 5.9265s \\
\multicolumn{1}{c|}{MALMEN} & {9.87\std{0.12}} & {{9.00\std{0.09}}} & \multicolumn{1}{c|}{{2.11\std{0.15}}} & {46.60\std{0.33}} & {42.50\std{0.32}} & \multicolumn{1}{c|}{{19.66\std{0.35}}} & 0.00\std{0.00} & 0.00\std{0.00} & \multicolumn{1}{c|}{0.00\std{0.00}} & 2.2779s \\ 
\multicolumn{1}{c|}{MALMEN*} & {12.23\std{0.11}} & {11.08\std{0.22}} & \multicolumn{1}{c|}{{2.43\std{0.09}}} & {56.97\std{0.24}} & {44.28\std{0.29}} & \multicolumn{1}{c|}{{15.02\std{0.21}}} & 0.01\std{0.01} & 0.02\std{0.01} & \multicolumn{1}{c|}{1.25\std{0.04}} & 9.4277s \\
\multicolumn{1}{c|}{DAFNet} & {21.99\std{0.47}} & {11.17\std{0.43}} & \multicolumn{1}{c|}{\underline{32.21\std{0.39}}} & {5.94\std{0.24}} & {5.68\std{0.33}} & \multicolumn{1}{c|}{\underline{36.29\std{0.45}}} & 1.25\std{0.08} & 2.12\std{0.12} & \multicolumn{1}{c|}{25.27\std{0.54}} & 8.2383s \\ \midrule[0.8pt] 
\multicolumn{1}{c|}{\textbf{RLEdit}} & \textbf{{89.42\std{0.34}}} & \textbf{87.32\std{0.23}} & \multicolumn{1}{c|}{\textbf{44.78\std{0.50}}} & \textbf{{84.37\std{0.22}}} & \textbf{{79.82\std{0.26}}} & \multicolumn{1}{c|}{\textbf{{37.15\std{0.41}}}} & \textbf{71.12\std{0.31}} & \textbf{67.42\std{0.27}} & \multicolumn{1}{c|}{\underline{27.43\std{0.44}}} & \textbf{0.2224s} \\ 
\bottomrule[1.5pt]
\end{tabular}
% \end{sc}
% \end{small}%
}
\end{table*}

\subsection{Applying RLEdit for Lifelong Editing}
\label{section:3.3}
After completing the training process in Section \ref{section:3.2}, given knowledge $(x,y)$ to be edited, RLEdit only needs to perform parameter-frozen fine-tuning on LLM $f_{\mathcal{W}}$ to collect gradients $\nabla_{\mathcal{W}}$, feed them into the hypernetwork to generate $\tilde{\nabla}_{\mathcal{W}}$, and add it to $\mathcal{W}$, as shown in Figure \ref{fig:method}(b).

In this process, the hypernetwork can typically be constructed with a simple 4-layer MLP, yet effectively handles lifelong editing tasks with varying numbers of knowledge samples. We attribute this excellent performance to the enhanced generalization capability brought by RL.

\textbf{Boosting Current Methods with RLEdit.} Since RLEdit's superior efficiency and effectiveness primarily stem from its RL paradigm, it can serve as a plug-and-play module to integrate with most existing hypernetwork-based approaches. Specifically, hypernetwork-based methods consist of three fundamental elements: (1) hypernetwork architecture, (2) selection of hypernetwork inputs, and (3) loss function for training the hypernetwork. For the first two elements, we can directly adopt their corresponding components from RLEdit. For the third element, we can use it to replace the basic term in RLEdit's reward function (Equation \ref{equ:2}). Our experimental results in Section \ref{sec:4.6} demonstrate these optimizations, further validating the benefits of formulating hypernetwork training in lifelong editing as an RL paradigm.

