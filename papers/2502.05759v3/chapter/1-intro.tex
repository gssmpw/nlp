\section{Introduction}
\label{sec:1}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=1.02\columnwidth]{figures/intro.pdf}}
\caption{Comparison of different lifelong editing paradigms. Here, $\mathcal{K}_a$ and $\mathcal{K}_b$ represent two consecutive new knowledge samples, while $\Delta_a$ and $\Delta_b$ are their corresponding parameter updates ($\mathcal{K}$ and $\Delta$ with other subscripts are paired data used for training the hypernetwork). (a) requires independent calculations for each knowledge sample, resulting in low efficiency. (b) is efficient but the hypernetwork cannot be generalized to the post-edited LLM. (c) ensures both efficiency and effectiveness, even after more than 20,000 edits. The time shows both initial setup time (including covariance matrix computation or hypernetwork training) and editing time. Best viewed in color.}
\label{fig:intro}
\end{center}
\vskip -0.25in
\end{figure}

Although large language models (LLMs) have achieved significant success in various downstream tasks \cite{gpt3, survey-llm},  their performance is hindered by the outdated or erroneous knowledge they may store, leading to an ongoing demand for continuous updates to model parameters \cite{mind-the-gap, editing-factual-knowledge}. A straightforward solution is lifelong model editing, which enables sequential knowledge updates in LLMs without compromising their core performance \cite{grace, wise, alphaedit}.  For each knowledge $\mathcal{K}$, conventional lifelong editing methods individually calculate an optimal update, denoted as $\Delta$, to adjust the LLM parameters $\mathcal{W}$ through matrix operation \cite{rome, memit}, as illustrated in Figure \ref{fig:intro} (a). Since the complexity and approximations involved in repetitive matrix operation make conventional methods computationally expensive and error-prone,  recently, hypernetwork-based methods \cite{mend, malmen} have emerged as a more efficient and elegant solution. 
By modeling the $\mathcal{K} \rightarrow \Delta$ process with a hypernetwork, as shown in Figure \ref{fig:intro} (b), the hypernetwork can efficiently map knowledge $\mathcal{K}$ to the required update $\Delta$ once trained on these $\mathcal{K}\mbox{-}\Delta$ pairs. This approach eliminates the need for expensive and repetitive matrix operations, thus enabling a more concise and practical model editing process.

However, current hypernetwork-based methods struggle to handle long-term lifelong editing tasks (\eg tasks involving more than 100 edits). To ensure convergence during training, all $\mathcal{K}\mbox{-}\Delta$ pairs used to train the hypernetwork must be collected from the same LLM with identical parameters. Consequently, a single hypernetwork can only model the $\mathcal{K} \rightarrow \Delta$ process for a fixed LLM, limiting its applicability across evolving models in lifelong editing scenarios. As illustrated in Figure \ref{fig:intro} (b), while locate-then-edit methods like MEMIT \cite{memit} and AlphaEdit \cite{alphaedit} can effectively support up to 5,000 edits, hypernetwork-based approaches tend to fail after only around 100 edits. This limitation severely restricts the broader use and further advancement of hypernetwork-based editing approaches.

This leads to a natural question: \textit{``How to keep the hypernetwork effective for lifelong editing?''} To address this, the hypernetwork must: 1) capture dynamic changes in the LLM, and 2) adaptively provide $\Delta$ based on the current LLM. This inspires us to turn to Reinforcement Learning (RL) \cite{reinforce_learn}, which aims to: 1) capture dynamic changes in the environment, and 2) adaptively provide actions based on the current state. 
Additionally, RL is typically applied in the context of the Markov Decision Process (MDP) \cite{mdp}. Lifelong editing naturally fits into this framework, as each edit depends solely on the new knowledge and the current LLM state, while the existing loss function and hypernetwork parameters can be modeled as reward and policy respectively. 

Building on these observations, we propose \textbf{RLEdit}, which applies RL to construct and train the hypernetwork. Specifically, RLEdit treats the hypernetwork as the agent, defines $\Delta$ as the action, and quantifies the performance of LLM as the reward function. We employ an offline policy update approach, enabling RLEdit to recognize the current state of LLMs and adaptively adjust its output $\Delta$, while retaining the efficiency of current hypernetwork-based methods. To meet the requirement that post-edited LLMs must retain previously edited knowledge in lifelong editing, we optimize the reward function by incorporating the difference between the current update and the cumulative sum of previous updates. This modification serves two key purposes: it accelerates the RL-based training process, and it reduces interference between successive updates.

To further validate RLEdit's capabilities, we conducted extensive experiments on several LLMs. When compared to existing parameter-modifying methods (\eg, RECT \cite{rect}, DAFNet \cite{dafnet}, AlphaEdit \cite{alphaedit}), RLEdit consistently outperforms these approaches across most evaluation metrics, requiring only 2\% of the computation time (including the hypernetwork training overhead) on average. As shown in Figure \ref{fig:intro} (c), RLEdit maintains satisfactory performance even after more than 20,000 edits, with each edit taking only 0.17s. Additionally, as the first approach to model lifelong editing as an RL problem, our ablation studies underscore the critical role of the RL framework in RLEdit's performance, paving the way for broader applications of lifelong editing and further advancements in hypernetwork-based methods.