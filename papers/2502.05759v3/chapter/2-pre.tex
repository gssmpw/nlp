\section{Preliminary}

\subsection{Lifelong Model Editing}
\label{section:2.1}
Lifelong model editing requires continuous and sequential model editing on the same LLM, with the total number of edits potentially reaching thousands or even tens of thousands. It requires the post-edited LLM to remember the knowledge from recent edits, retain previously edited knowledge, and maintain comprehensive performance. Let $f_{\mathcal{W}_0}: X\rightarrow Y$ be the initially pre-trained LLM with parameters $\mathcal{W}_0$, which maps the input set X to the output set $Y$. In the lifelong editing task, we have an editing dataset $\mathcal{D}_n=\{(X,  Y)|(x_1, y_1), \dots, (x_n, y_n)\}$, where $X=(x_1, \dots, x_n)$ is the input stream of knowledge to be edited, and $Y=(y_1, \dots,  y_n)$ is the target output. For the entire input stream $X$, the pre-trained model outputs $f_{\mathcal{W}_0}(X)=Y'$, where $Y'=(y_1', \dots, y_n')$ represents the ground truth. Through sequential editing, we aim to modify the model parameters to achieve $f_{\mathcal{W}_n}(X)=Y$. Therefore, we introduce the model editor ($\mathrm{ME}$). During model editing, the LLM sequentially reads input-output pairs $(x, y)$ from dataset $\mathcal{D}$. When editing the $t$-th knowledge, $\textsc{ME}$ modifies the LLM parameters according to the following formula:
\begin{equation}
    f_{\mathcal{W}_{t}} = \mathrm{ME}(f_{\mathcal{W}_{t-1}}, x_{t}, y_{t}), \ \ \ t= 1, \dots, n.
\end{equation}


\subsection{Hypernetwork-based Editing Methods}
One approach in model editing is hypernetwork-based editing methods, which involve training an editing hypernetwork to generate LLM parameter updates. Hypernetwork-based methods recognize that LLM fine-tuning gradients contain rich information and use the loss $\mathcal{L}$ of the edited LLM on both edited and unrelated knowledge as the starting point for hypernetwork training. It aims to train a hypernetwork to map fine-tuning gradients to LLM parameter updates. To address the computational burden of $d\times d$ weight matrices, \citet{mend} decomposes each layer's gradient matrix into a rank-1 product form $\nabla_{\mathcal{W}_l}\mathcal{L}=\delta_{l+1} u_l^\top$, where $\delta_{l+1}$ is the gradient of the loss with respect to pre-activations of layer $l+1$, and $u_l$ is the input to layer $l$. Through low-rank decomposition, the hypernetwork $\mathcal{H}$ can learn a $d\rightarrow d$ mapping:
\begin{equation}
    \mathcal{H}:\ \delta_{l+1}\times u_l^\top \rightarrow \tilde{\delta}_{l+1}\times \tilde{u}_l^\top,
\end{equation}
where $\tilde{\delta}_{l+1}$ and $\tilde{u}_l$ are pseudo-activations and pseudo-increments respectively, and the final updates applied to LLM parameters is $\tilde{\nabla}_{\mathcal{W}_l}=\tilde{\delta}_{l+1}\tilde{u}_l^\top$. 

In this paper, $\tilde{\nabla}_\mathcal{W}$ and $\Delta$ are equivalent, both representing the parameter updates required for editing. We use $\tilde{\nabla}_{\mathcal{W}}$ consistently throughout the rest of this paper.


\subsection{Reinforcement Learning}
A reinforcement learning task is usually formalized as a Markov Decision Process (MDP). An MDP can be defined as a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{R}, \pi, \gamma)$. At each time step $t$, the RL system is in a state $s_t\in \mathcal{S}$, and the agent generates an action $a_t\in \mathcal{A}$ based on its policy $\pi$ and the current state $s_t$, \ie, $a_t\sim\pi(s_t)$. The MDP then transitions to state $s_{t+1}$ and the agent receives a reward $r_t=R(s_t, a_t), r_t \in \mathcal{R}$. After all time steps, the agent optimizes its policy to maximize the total trajectory return $J(\tau)=\sum_{r_t\in \tau}\gamma^tr_t$, where $\gamma$ represents a discount factor and $\tau$ represents one episode trajectory of the agent in the MDP.