\documentclass{article}

\usepackage{multirow}
\usepackage{booktabs}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{adjustbox}

\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{flushend}

\newcommand{\theHalgorithm}{\arabic{algoithm}}

\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

% My packages
\usepackage{diagbox}
\usepackage{array}
\usepackage{pifont}
\newcommand{\std}[1]{\scriptsize{$\pm$#1}}
\newcommand{\un}[1]{\scriptsize{$$#1}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}
\usepackage{multirow}
\usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\usepackage[justification=justified,skip=3pt]{caption}

\renewcommand{\arraystretch}{0.85}

\setlength{\textfloatsep}{5pt plus 3pt minus 3pt}
\setlength{\intextsep}{5pt plus 3pt minus 3pt}
\setlength{\dbltextfloatsep}{5pt plus 3pt minus 3pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{3pt}

\makeatletter
\g@addto@macro\normalsize{%
  \abovedisplayskip 5pt plus 2pt minus 3pt%
  \belowdisplayskip \abovedisplayskip
  \abovedisplayshortskip 5pt plus2pt  minus3pt%
  \belowdisplayshortskip 5pt plus2pt minus3pt%
}

\icmltitlerunning{Reinforced Lifelong Editing for Language Models}

\begin{document}

\twocolumn[
\icmltitle{Reinforced Lifelong Editing for Language Models}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zherui Li}{equal,bupt}
\icmlauthor{Houcheng Jiang}{equal,ustc}
\icmlauthor{Hao Chen}{bupt}
\icmlauthor{Baolong Bi}{cas} \\
\icmlauthor{Zhenhong Zhou}{bupt}
\icmlauthor{Fei Sun}{cas}
\icmlauthor{Junfeng Fang}{nus}
\icmlauthor{Xiang Wang}{ustc}
\end{icmlauthorlist}

\icmlaffiliation{bupt}{Beijing University of Posts and Telecommunications}
\icmlaffiliation{ustc}{University of Science and Technology of China}
\icmlaffiliation{cas}{Chinese Academy of Sciences}
\icmlaffiliation{nus}{National University of Singapore}

\vskip 0.3in
]


\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and prevalent approaches leverage hypernetworks to generate these parameter updates. However, they face significant challenges in lifelong editing due to their incompatibility with LLM parameters that dynamically change during the editing process. To address this, we observed that hypernetwork-based lifelong editing aligns with reinforcement learning modeling and proposed \textbf{RLEdit}, an RL-based editing method. By treating editing losses as rewards and optimizing hypernetwork parameters at the full knowledge sequence level, we enable it to precisely capture LLM changes and generate appropriate parameter updates. Our extensive empirical evaluation across several LLMs demonstrates that RLEdit outperforms existing methods in lifelong editing with superior effectiveness and efficiency, achieving a \textbf{59.24\%} improvement while requiring only \textbf{2.11\%} of the time compared to most approaches. Our code is available at: \href{https://github.com/zhrli324/RLEdit}{https://github.com/zhrli324/RLEdit}.
\end{abstract}


\input{chapter/1-intro}
\input{chapter/2-pre}
\input{chapter/3-method}
\input{chapter/4-exp}
\input{chapter/5-limitation}
\input{chapter/6-con}



\section*{Impact Statement}
RLEdit significantly enhances the capabilities of lifelong model editing, making it invaluable for updating and maintaining knowledge in real-world applications. Given that the ability to directly modify model parameters introduces potential risks, such as the injection of false or harmful information, we strongly urge researchers to implement strict validation and oversight to ensure the ethical use of these techniques. The original goal of our work is positive, aiming to facilitate efficient knowledge updates in large language models. We encourage researchers to leverage this technology responsibly while maintaining appropriate safeguards for its deployment.

\nocite{}
\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{chapter/appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}