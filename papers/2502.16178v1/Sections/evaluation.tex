\section{Study Results}
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.50\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/user.png}
        \caption{Participants' Rating}
        \label{fig:participant_ratings}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.50\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/expert.png}
        \caption{Qualitative Assessment}
        \label{fig:expert_ratings}
    \end{subfigure}
    \caption{(a): Means and standard errors of baseline system and TutorUp
on a 5-point Likert measured by User-Study participants; (b) Means and standard errors of baseline system and \textit{TutorUp}
on test results on a 3-point Likert measured by experts.}
    \label{fig:combined_figures}
\end{figure}
We present evaluation results for the Survey, Test Assessment, and Interview. The full $p$-value data for the first two are in Appendix \ref{apdx:evaluation}.
\subsection{Survey Results}

We conducted a binomial test (equivalent to a Sign Test) to evaluate whether \textit{TutorUp} significantly enhances the training experience for novice tutors compared to the baseline system. Using a significance threshold of $p<0.05$, the results show that participants rated \textit{TutorUp} significantly higher across key areas, as illustrated in Fig. ~\ref{fig:expert_ratings}. Overall, all hypotheses were supported, except H4a, with $p$-values larger than $0.05$ in the binomial test. However, there was no significant difference ($p=0.90$) between \textit{TutorUp} and the baseline system in terms of ease of use, failing to support hypothesis H4a.  The reason might be that the layout of information presented in both the baseline and \textit{TutorUp} systems is quite similar. Based on the mean scores (Baseline Mean = $4.06$, \textit{TutorUp} Mean = $4.56$), both systems are generally easy to use, resulting in minimal differences.


\subsection{Test Assessment Results}

We averaged the test evaluations from two experienced tutors. Since the data from the four evaluation metrics followed a normal distribution, we performed a Paired Samples t-test on the tutors' test scores, with results shown in Fig. \ref{fig:expert_ratings}. Using a threshold of $p < 0.05$, we observed that participants performed significantly better in the \textit{Use Strategies Appropriately} aspect after completing the \textit{TutorUp} training (Mean = $2.47$, SD = $0.53$), compared to the baseline training (Mean = $2.06$, SD = $0.60$) ($p < 0.05$, supporting H5). However, for the \textit{Use Strategies Effectively} ($p = 0.09$), \textit{Students are More Engaged} ($p = 0.23$), and \textit{Strategies Accessible for Students} ($p = 0.10$) aspects, although the mean scores for \textit{TutorUp} were higher than for the baseline, the differences were not statistically significant (H6, H7a, H7b not supported). Due to time limitations, we only asked tutors to practice one scenario for each system, which may explain why the training effectiveness for strategy application is not obvious (H7a and H7b). The failure to support H6 is discussed in Section \ref{subsec:simulateEngagement}.


\subsection{Interview Results}
We summarized the ideas conveyed during the interview for the Scenario Simulation, Feedback scheme, and Overall system usage.
\subsubsection{Scenario Simulation}
We asked tutors about their opinions on scenario simulation in three perspectives: (1) scenario theme, (2) student information and student simulation, and (3) math problem. \textbf{\textit{Scenario theme: }} 
Several participants (P6, P9, P12, P14) expressed the importance and clarity of the scenario theme description. P6 noted that it is concise and clear, providing a detailed account of the issues and student context. However, some participants (P4, P7, P11) found the scenario theme descriptions useful but redundant due to overlap with the student information. \textbf{\textit{Student information:}} All participants expressed that the detailed student information was very helpful for applying strategies to specific scenarios. Most of them felt that the student simulations were quite realistic, noting that the students’ tone and dialogue aligned well with their character profiles. However, participants also pointed out certain aspects of the student design that felt unrealistic. P7 mentioned that the language used by the students did not match their age (around 10 years old), as it's unlikely for children that age to speak without grammatical errors. P2 commented that, in their home country (Mexico), children around that age are not allowed to use computers, which made the scenario feel unrealistic to them. Additionally, P9 felt that the students’ behavior was too straightforward and logical, lacking the complexity expected of real children, while P10 found the students to be too ``slow'' as they struggled with simple math problems that should have been easy for them. \textbf{\textit{Math problem:}} Several participants (P6, P8, P10, P13, P15) agreed that using math problems as a context is beneficial because they involve clear steps and solutions, making them more logically structured compared to subjects like literature. This helps users focus on the issue of student engagement. However, some participants (P3, P7, P8, P15) felt that the current problems in the system were too simple. P14 suggested incorporating other types of questions such as open-ended ones for greater variety.

\subsubsection{Immediate and Asynchronous Feedback}
\label{sucsec:immediate}

All participants expressed their overall appreciation for the feedback scheme in the \textit{TutorUp} system, describing it as personalized and useful. 
%
Regarding \textbf{\textit{immediate feedback}}, participants emphasized their preference for its real-time nature and practical applicability. As P13 stated, ``Immediate feedback provides timely suggestions that I can apply right away, creating a positive loop between theory and practice.'' P7 also noted that ``the immediate feedback helps connect the strategy to the current scenario, which aids memory and application.'' However, P6 mentioned that the immediate feedback seems to focus on the most recent dialog round, suggesting that covering a broader range of interactions might be beneficial. 
%
Regarding \textbf{\textit{asynchronous feedback}}, many participants (P1–P10) expressed an appreciation for its completeness and richness, finding it useful and comprehensive. However, some participants (P12, P16) felt it was too lengthy and suggested it should focus more on practical advice. 
%
Regarding \textbf{\textit{both}} types of feedback, P4 and P11 mentioned that instead of just being told what strategies to try, they would prefer more detailed guidance on how to implement those strategies effectively. Although many participants expressed that the strategies in the feedback were very practical, some mentioned that after applying these strategies as suggested by the system, the simulated students did not appear to become more engaged.

\subsubsection{System Usage and Improvements}
Most participants agreed that \textit{TutorUp} is straightforward, clear, and easy to use. P13 specifically described it as ``overall convenient, intuitive, with a clear layout.'' Participants also offered numerous suggestions for improving key system features. A major area for improvement, mentioned by P2, P3, P5, and P13, was adding more diverse scenarios and varied scenario designs. Some also suggested enhancing the system’s customization options, such as allowing users to edit math problems (P10) and customize the personalities of simulated students and scenarios (P3 and P5). Additionally, participants hoped for more visual interactivity, like adding student avatars and incorporating embedded links or slides into the system, as suggested by P11.





