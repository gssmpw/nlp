\documentclass[12pt]{settings/colt2025} % Anonymized submission
%\documentclass[final,12pt]{colt2025} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Optimistically Optimistic Exploration for Efficient Infinite-Horizon RL]{Optimistically Optimistic 
Exploration for Provably Efficient Infinite-Horizon Reinforcement and Imitation Learning}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\author[Moulin, Neu and Viano]{%
 \Name{Antoine Moulin} \Email{antoine.moulin@upf.edu}\\
 \addr Universitat Pompeu Fabra, Barcelona, Spain%
 \AND
 \Name{Gergely Neu} \Email{gergely.neu@gmail.com}\\
 \addr Universitat Pompeu Fabra, Barcelona, Spain
 \AND
 \Name{Luca Viano} \Email{luca.viano@epfl.ch}\\
 \addr EPFL, Lausanne, Switzerland
}

% custom
\input{settings/lmacros}
\input{settings/gmacros}
\input{settings/amacros}

\input{settings/preamble}

\newcommand{\MainAlg}{\texttt{MainAlg}}
\renewcommand{\phi}{\varphi}

\newcommand{\rewardbias}{\texttt{reward-bias}}
\newcommand{\modelbias}{\texttt{model-bias}}


% tikz figures
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{positioning}
\usetikzlibrary{automata}

% algorithm comments
\usepackage{transparent}
\newcommand{\algcomment}[1]{\textcolor{blue!70!black}{\transparent{0.7}\small{\texttt{\textbf{\#\hspace{2pt}#1}}}}}
\newcommand{\algcommentlight}[1]{\textcolor{blue!70!black}{\transparent{0.5}\footnotesize{\texttt{\textbf{\#\hspace{2pt}#1}}}}}
\newcommand{\algcommentbig}[1]{\textcolor{blue!70!black}{\footnotesize{\texttt{\textbf{/* #1~*/}}}}}
\newcommand{\algcommentbiglight}[1]{\textcolor{blue!70!black}{\transparent{0.5}\footnotesize{\texttt{\textbf{/* #1~*/}}}}\vspace{2pt}}
\newcommand{\algcolor}[1]{\textcolor{blue!70!black}{#1}}
\newcommand{\algspace}{\hspace{\algorithmicindent}}


% not sure what this is
\newlength{\minipagewidth}
\setlength{\minipagewidth}{\linewidth}
\setlength{\fboxsep}{3mm}
\addtolength{\minipagewidth}{-\fboxrule}
\addtolength{\minipagewidth}{-\fboxrule}
\addtolength{\minipagewidth}{-\fboxsep}
\addtolength{\minipagewidth}{-\fboxsep}
\newcommand{\bookbox}[1]{
\par\medskip\noindent
\framebox[\linewidth]{
\begin{minipage}{\minipagewidth}
{#1}
\end{minipage} } \par\medskip }


% for table of contents appendix
\usepackage{tocloft}
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\begin{document}

\maketitle

\begin{abstract}%
  We study the problem of reinforcement learning in infinite-horizon discounted linear Markov decision processes (MDPs), and propose the first computationally efficient algorithm achieving near-optimal regret guarantees in this setting. Our main idea is to combine two classic techniques for optimistic exploration: additive exploration bonuses applied to the reward function, and artificial transitions made to an absorbing state with maximal return. We show that, combined with a regularized approximate dynamic-programming scheme, the resulting algorithm achieves a regret of order $\tilde{\mathcal{O}} (\sqrt{d^3 (1 - \gamma)^{- 7 / 2} T})$, where $T$ is the total number of sample transitions, $\gamma \in (0,1)$ is the discount factor, and $d$ is the feature dimensionality. The results continue to hold against adversarial reward sequences, enabling application of our method to the problem of imitation learning in linear MDPs, where we achieve state-of-the-art results.
\end{abstract}

\begin{keywords}%
  Optimistic exploration, discounted MDPs, linear MDPs, imitation learning%
\end{keywords}

\input{sections/introduction.tex}
\input{sections/preliminaries.tex}
\input{sections/algorithm.tex}
\input{sections/analysis.tex}
\input{sections/applications.tex}
\input{sections/conclusion.tex}


% \newpage
% Acknowledgments---Will not appear in anonymized version
\acks{The authors wish to thank Asaf Cassel and Aviv Rosenberg for sharing further insights about their work at the 
virtual RL theory seminars, and Volkan Cevher for initial discussions about this project. 
This project has received funding from the European Research Council (ERC), under the European Union's Horizon 2020 
research and innovation programme (Grant agreement No.~950180).
This work is funded (in part) through a PhD fellowship of the Swiss Data Science Center, a joint
venture between EPFL and ETH Zurich.  Luca Viano acknowledges travel support from ELISE (GA no 951847).}


\bibliography{ref}


\clearpage
\appendix
\renewcommand{\contentsname}{Contents of Appendix}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
{
%  \hypersetup{hidelinks}
  \setlength{\cftbeforesecskip}{.8em}
  \setlength{\cftbeforesubsecskip}{.5em}
  \tableofcontents
}
\clearpage
\input{sections/omitted_pseudocodes.tex}

\clearpage
\input{sections/analysis_proofs.tex}

\clearpage
\input{sections/related_works_IL.tex}

\clearpage
\input{sections/applications_proofs.tex}

\clearpage
\input{sections/lower_bounds_appendix.tex}

\clearpage
\input{sections/technical_tools.tex}

% \crefalias{section}{appendix} % uncomment if you are using cleveref

\end{document}
