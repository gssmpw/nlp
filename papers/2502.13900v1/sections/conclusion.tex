\section{Concluding remarks}\label{sec:conclusion}

We close by discussing a few open problems and potential improvements to our results.

\paragraph{On the objective function.} We have focused on a relatively under-studied objective function: the discounted return from a fixed initial-state distribution. This is different from the objectives studied by other works such as \citet{liu2020regret,he2021nearly,zhou2021provably}, but arguably more natural when one is interested in learning algorithms that produce a single near-optimal policy at the end of an interactive learning period (which is the case in most practical applications one can think of). It is easy to see that resets to the initial state are absolutely necessary in this setting, unless one wants to make strong assumptions about the transition dynamics. A more exciting question is if our algorithm can be adapted to the significantly more challenging setting of undiscounted infinite-horizon reinforcement learning where existing methods \citep{WJLJ20,hong2024provably,he2024sample} either obtain suboptimal regret bounds or leverage oracles whose computationally efficient implementation is unknown. So far, our attempts towards tackling this problem have remained unsuccessful. We believe that significant new ideas are necessary for solving this major open problem, but also that the techniques we introduce in this paper will be part of an eventual solution.\looseness=-1

\paragraph{On second-degree optimism.} Our key technical contribution draws inspiration from two sources: the \RMAXalg algorithm of \citet{brafman2002r}, and the very recent work of \citet{cassel2024warmupfree}. While many of our technical tools are directly imported from the latter work, the concept of optimistically augmented MDPs and the connection with \RMAXalg has arguably brought about a new level of understanding that can be potentially valuable for future work. It has certainly proved useful for our setting, where the notion of ``contracted sub-MDP'' used in the analysis of \citet{cassel2024warmupfree} cannot be meaningfully interpreted and used for analysis. We hope our work can bring some fresh attention to older (but apparently still powerful) ideas from the past of RL theory such as the \RMAXalg trick.

\paragraph{On the tightness of the bounds.} We find it very likely that our performance guarantees can be improved to some extent in terms of their dependence on $H$ and $d$. In fact, we have made no attempt to optimize the scaling with respect to these parameters, and actually believe that the $H^{9/4}$ factor in the regret bound can be improved relatively easily. Specifically, we find it very likely that performing several value-function and policy updates at the end of each episode can reduce this factor---but we opted to keep the algorithm simple and the paper easy to read. We invite future researchers to verify this conjecture. Likewise, we believe that the dependence on the dimension $d$ can be improved by using more sophisticated estimators and concentration inequalities (as done in the finite-horizon setting by \citealp{he2023nearly,agarwal2023vo}), but leave working out the (possibly non-trivial) details for another paper.
