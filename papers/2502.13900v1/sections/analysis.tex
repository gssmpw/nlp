\section{Main Result and Analysis}
\label{sec:analysis}

The following theorem states our main result about the performance of \algname.
%
\begin{theorem} \label{thm:main}
    Suppose that Assumption~\ref{ass:LinMDP} holds, and that Algorithm~\ref{alg:linear-rmax-ravi-ucb} is executed with parameters specified in Appendix~\ref{app:putting-together-main} for a fixed number $K$ of episodes. Then, with probability at least $1 - \delta$,
    \begin{equation*}
        \regretK = \tilde\cO \spr{\sqrt{d^3 H^3 K} + \sqrt{d H^{9/2} K \log \spr{\abs{\cA}}}}\,.
    \end{equation*}
\end{theorem}
%
For the commonly studied case where the reward function $r$ is fixed, this result can be easily translated to a bound on the sample complexity of producing an $\varepsilon$-optimal policy as well, as stated below.
%
\begin{corollary} \label{cor:sample}
    Let $I$ be drawn uniformly from $\ev{1,2,\dots,K}$. Then, policy $\pi_I$ produced by \Cref{alg:linear-rmax-ravi-ucb} (with the same parameter tuning as in Theorem~\ref{thm:main}) satisfies $\EEs{V^{\pi^*}_{P,r} - V^{\pi_I}_{P,r}}{I} \le \varepsilon$, if the number of episodes satisfies
    \begin{equation*}
        K = \Omega\pa{\frac{H^3 d^3 + H^{9/2} d \log \abs{\cA}}{\varepsilon^2}}\,.
    \end{equation*}
\end{corollary}
%
As the length of each episode is geometrically distributed with expectation $H$, the number of interaction steps satisfies $\EE{T} = HK$ when the number of episodes $K$ is fixed. Taking this into account, both results can be restated in terms of the number of sample transitions $T$. Likewise, similar results can be proved when treating the sample size $T$ as fixed and letting $K$ be the smallest (random) number of episodes covering the sample budget.

In the remaining part of this section, we describe the main steps constituting the proof of Theorem~\ref{thm:main}. The analysis will make crucial use of the notion of optimistically augmented MDPs defined in Section~\ref{sec:OAMDP}. Specifically, we define an augmented MDP for each episode $k$ as $\cM^{\upplus}_k = \cM^+(r_k,p_k^\upplus)$ and use the shorthand $\cM_k = \cM(r_k)$ for the true MDP with reward function $r_k$. Letting $\mu_k^\upplus(\pi)$ denote the occupancy measure induced by policy $\pi$ in $\cM_k^\upplus$, the first step in our analysis is to rewrite the regret as follows:
%
\begin{align*}
    \sumkK \inp{\mu \spr{\pi^\star} - \mu \spr{\pi_k}, r_k} &= \sumkK \underbrace{\inp{\mu \spr{\pi^\star} - \mu \spr{\pi_k}, r_k - r_k^\upplus}}_{= \rewardbias_k} + \sumkK \underbrace{\inp{\mu \spr{\pi^\star} - \mu_k^\upplus \spr{\pi^\star}, r_k^\upplus}}_{= - \modelbias_k \spr{\pi^\star}} \\
    &\phantom{=}+ \underbrace{\sumkK \inp{\mu_k^\upplus \spr{\pi^\star} - \mu_k^\upplus \spr{\pi_k}, r_k^\upplus}}_{= \regretKplus} + \sumtT \underbrace{\inp{\mu_k^\upplus \spr{\pi_k} - \mu \spr{\pi_k}, r_k^\upplus}}_{= \modelbias_k \spr{\pi_t}}\,.
\end{align*}
%
Here, $\regretKplus$ corresponds to the (normalized) regret of \raviUCB in the sequence of optimistically augmented MDPs $\pa{\cM_k^\upplus}$, and the other terms account for the difference between this term and the regret in the original problem. Some of these are easy to handle by exploiting the optimistic nature of our augmentation technique, whereas others require a more careful tuning of the ascension functions and exploration bonuses.

A large part of the analysis will be based on the condition that the exploration bonuses $\scbr{\CB_k}_k$ used for performing optimistic value iteration are \emph{valid} estimates of the uncertainty we have on the model, in the sense that the following event holds
%
\begin{equation} \label{eq:validity-bonuses}
    \cE_{\text{valid}} = \scbr{\forall \spr{x, a} \in \cX \times \cA, \forall k \in \sbr{K}, \abs{\spr{\opP - \ophPk} V_k \spr{x, a}} \leq \CB_k \spr{x, a}}\,.
\end{equation}
%
For the sake of clearly presenting the main ideas of our analysis, we will work under the condition that the event $\cE_{\text{valid}}$ holds, and we will show later in the context of linear MDPs that this is indeed true with high probability. Furthermore, we will work under the condition that for any $k \in \sbr{K}$, there exists $\QMAX > 0$ such that $\norm{Q_k}_\infty \leq \QMAX$. We will show this is true under an appropriate choice of $p_k^\upplus$, and give an expression for $\QMAX$. Finally, we assume the episode lengths $\spr{L_k}$ are all less than some $\LMAX$ and refer to this event as $\cE_L$. We will show that this also holds with high probability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Controlling the bias due to the augmented rewards}

The first lemma controls the bias introduced by the second-degree optimism introduced into the reward function in terms of the ascension functions.
%
\begin{restatable}{Lem}{rewardbiasbound} \label{lem:reward-bias-bound}
    For any choice of $p_k^\upplus$, we have $\rewardbias_k \leq \RMAX \inp{\mu \spr{\pi_k}, p_k^\upplus}$.
\end{restatable}
%
The proof essentially follows from the definition of $r_k^\upplus$, and is found in Appendix~\ref{app:reward-bias-bound}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Controlling the bias due to the augmented transitions}

Next, we provide a bound on the bias due to introducing second-degree optimism in the transition function. It is easy to see that playing in the augmented MDP $\cM_k^\upplus$ always yields higher discounted return due to the presence of the heaven state $x^\upplus$. On the other hand, one can intuitively see that the difference in the bias term can be upper bounded by the amount of time spent in heaven $x^\upplus$. The following lemma formalizes this claim by providing a bound on $\modelbias_k \spr{\pi} = \inp{\mu_k^\upplus \spr{\pi} - \mu \spr{\pi}, r_k^\upplus}$ for any policy $\pi$.
%
\begin{restatable}{Lem}{modelbiasbounds} \label{lem:model-bias-bounds}
    Let $\pi$ be any policy and $p_k^\upplus$ be any ascension function at episode $k$. Then,
    %
    \begin{equation*}
        0 \leq \modelbias_k \spr{\pi} \leq \frac{\RMAX}{1 - \gamma} \inp{\mu \spr{\pi}, p_k^\upplus}\,.
    \end{equation*}
\end{restatable}
%
Both bounds are proved through a coupling argument, provided in Appendix~\ref{app:model-bias-bounds}. Applying the lower bound to $\pi^\star$ and the upper bound to $\pi_k$, we obtain
%
\begin{equation} \label{eq:model-bias-bounds}
    - \sumkK \modelbias_k \spr{\pi^\star} \leq 0\,, \text{ and}\;\; \sumkK \modelbias_k \spr{\pi_k} \leq \frac{\RMAX}{1 - \gamma} \sumkK \inp{\mu \spr{\pi_k}, p_k^\upplus}\,.
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regret analysis in the optimistically augmented MDP}

To control the main term $\regretKplus$, we adapt the analysis of \raviUCB due to \citet{MN23} with some appropriate changes. The key idea is to define an estimate $\ophPplusk$ of the optimistically augmented transition operator $\opPplusk$ associated with $\cM^\upplus_k$, via its action on a function $v\in\real^\X$:
\begin{equation*}
    \pa{\wh{P}^\upplus_k v}\pa{x,a} = \spr{1 - p_k^\upplus \spr{x, a}} \cdot \pa{\wh{P}_k v}{\pa{x,a}} + p_k^\upplus \spr{x, a}\cdot \frac{\RMAX}{1-\gamma}\,.
\end{equation*}
%
Then, it is easy to verify that the validity of the exploration bonuses (Eq.~\ref{eq:validity-bonuses}) implies that the scaled bonuses $\spr{1 - p_k^\upplus} \odot \CB_k$ satisfy the following analogous validity condition in the augmented MDP:
%
\begin{equation*}
    \abs{\spr{\opPplusk - \ophPplusk} V_k \spr{x, a}} \leq \spr{1 - p_k^\upplus \spr{x, a}} \CB_k \spr{x, a}\,.
\end{equation*}
%
With these insights, our algorithm can be seen as an instantiation of \raviUCB on the sequence of optimistically augmented MDPs $\pa{\cM_k^\upplus}$, and thus it can be analyzed by following the steps of \citet{MN23}. In particular, the following lemma (an adaptation of Lemma~4.3 of \citet{MN23}, proved here in Appendix~\ref{app:bound-regret-plus}) gives a bound on the regret in the augmented MDP.
%
\begin{restatable}{Lem}{boundregretplus} \label{lem:bound-regret-plus}
    Suppose that the bonuses $\scbr{\CB_k}$ are valid in the sense of Equation~\ref{eq:validity-bonuses} and that for any $k$, $\norm{Q_k}_\infty \leq \QMAX$. Then, the sequence of policies output by Algorithm~\ref{alg:linear-rmax-ravi-ucb} satisfies
    %
    \begin{equation} %\label{eq:bound-regret-plus}
        \regretKplus \leq \frac{E \spr{K} \log \abs{\cA}}{\eta} + 4 \VMAX E \spr{K} + \frac{2 \QMAX^2 \eta K}{\sqrt{1 - \gamma}} + 2 \sumkK \inp{\mu \spr{\pi_k}, \spr{1 - p_k^\upplus} \odot \CB_k}\,.
    \end{equation}
\end{restatable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Choosing the ascension functions}

It remains to verify that our choice of the probabilities $p_k^\upplus$ is such that the terms appearing in Lemmas~\ref{lem:reward-bias-bound} and Equation~\eqref{eq:model-bias-bounds} are small, yet the value of $\QMAX$ also remains bounded. 
%
In particular, we show in Lemma~\ref{lem:sigmoid-bound} that our choice satisfies
%
\begin{equation} \label{eq:ascension-function-bound}
    p_k^\upplus \spr{x, a} \leq 2 \alpha^2 \CB_k \spr{x, a}^2 + 2 e^{- \tau}\,.
\end{equation}
%
Furthermore, the following lemma (proved in in Appendix~\ref{app:qmax}) shows a suitable choice of $\QMAX$.
%
\begin{restatable}{Lem}{qmaxlemma} \label{lem:qmax}
    Suppose the bonuses $\scbr{\CB_k}_{k \in \sbr{K}}$ are valid in the sense of Equation~\ref{eq:validity-bonuses} and the ascension functions are chosen as in Line~\ref{line-alg:pplus} of Algorithm~\ref{alg:linear-rmax-ravi-ucb}. Then, for any $k \in \sbr{K}$, the iterate $Q_k$ satisfies $\norm{Q_k}_\infty \leq \QMAX$ with $\QMAX = \frac{\RMAX + 2 \omega / \alpha}{1 - \gamma}$.
\end{restatable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Exploration bonuses}

The final technical step is to verify the validity of the exploration bonuses and to bound their cumulative size. The following lemma addresses the latter question.
%
\begin{restatable}{Lem}{expectedbonusesbound} \label{lem:expected-bonuses-bound}
    Suppose Assumption~\ref{ass:LinMDP} and event $\cE_L$ hold and denote $T = \LMAX K$. Then, with probability at least $1 - \delta$, the policies $\scbr{\pi_k}$ and bonuses $\scbr{\CB_k}$ satisfy
    %
    \begin{equation*}
        \sumkK \inp{\mu \spr{\pi_k}, \CB_k} \leq 4 \spr{1 - \gamma} \beta B \sqrt{d T\log \spr{1 + \frac{B^2 T}{d}}} + 4 \beta B \log \spr{\frac{2 K}{\delta}}^2\,,
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        \sumkK \inp{\mu \spr{\pi_k}, \CB_k^2} \leq 8 \spr{1 - \gamma} \beta^2 B^2 d \log \spr{1 + \frac{B^2 T}{d}} + 4 \beta^2 B^2 \log \spr{\frac{2 K}{\delta}}^2\,,
    \end{equation*}
\end{restatable}
%
For the proof, see Appendix~\ref{app:expected-bonuses-bound}. Finally, it remains to show that the events $\cE_{\text{valid}}$ and $\cE_L$ hold which is done in the following lemma, whose proof can be found in Appendix~\ref{app:good-event-holds}.
%
\begin{restatable}{Lem}{goodeventholds} \label{lem:good-event-holds}
    Let $\beta = 8 \QMAX d \log \spr{c \alpha \WMAX \RMAX B^{9/2} \QMAX^4 \LMAX^{5/2} K^{7/2} d^{5/2} \delta^{-1}}$. Then, the event $\mathcal{E}_{\text{valid}} \cap \mathcal{E}_L$ holds with probability $1 - 2 \delta$.
\end{restatable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Putting everything together}

Theorem~\ref{thm:main} then follows from applying Lemmas~\ref{lem:reward-bias-bound}-\ref{lem:good-event-holds}, using Equation~\eqref{eq:ascension-function-bound}, and bounding the total number of epochs (Lemma~\ref{lemma:number-epochs-bound}). The full details are provided in Appendix~\ref{app:putting-together-main}.
