\section{Motivation for \emph{Learning from Features Alone} and related works in imitation learning}
\label{app:related_works_IL}



\paragraph{Related works in theoretical imitation learning.} A special case of our setting is imitation learning from state-only expert trajectories, which is recovered when $\phi_\cost (x,a) = \mathbf{e}_x$. 
This setting was first studied in \cite{sun2019provably} in the finite-horizon setting with general function approximation. 
There are some notable differences between their work and ours, primarily that they focus on the finite-horizon setting and learn a non-stationary policy. 
In principle, their algorithm could be applied to the infinite-horizon setting by truncating the trajectories after $\tilde{\mathcal{O}}(1-\gamma)^{-1}$ steps. 
However, this would still result in a non-stationary policy, whereas our approach outputs a stationary policy. 
Their realizability assumption on the expert policy and expert state-value function is not required in our work which leverages, instead, the linear MDP assumption. 
These assumptions are not directly comparable, even when the function classes in \cite{sun2019provably} are assumed to be linear. 
Indeed, the realizability assumption imposed in \cite{sun2019provably} would imply having access to the values of the features $\sum_a \expert(a|x) \phi(x,a)$ for each state $x \in \cX$. 
In contrast, our approach does not require this additional knowledge about the expert.

Furthermore, the guarantees on the number of expert trajectories in \cite[Theorem 3.3]{sun2019provably} adapted to the infinite-horizon setting, 
would scale as $\tilde{\mathcal{O}}((1-\gamma)^{-4} \epsilon^{-2})$ whereas we only require $\tilde{\mathcal{O}}((1-\gamma)^{-2} \epsilon^{-2})$ state-only samples from the expert occupancy measure.

Similarly, \cite{ADKLS20} develop a framework for imitation and representation learning from observation alone based on bilevel optimization but assume the realizability of the state-value function, which is not needed in our work.

The work of \citet{kidambi2021mobile} investigates the idea of exploration in state-only imitation learning. 
Unlike our work, they focus on the finite-horizon setting and on different structural assumptions on the MDP. 
Specifically, \citet{kidambi2021mobile} consider tabular MDPs, nonlinear kernel regulators, and MDPs with Gaussian transition kernels and bounded Eluder dimension, whereas our work focuses on infinite-horizon linear MDPs and observing only the feature directions visited by the expert, which is a weaker requirement than observing the states directly.
Moreover, our algorithm \FRAalg is computationally efficient, whereas the model fitting step in \cite{kidambi2021mobile} cannot be implemented efficiently for various situations, including linear MDPs \citep{jin2019provably} and KNRs \citep{Kakade:2020}.

\citet{wu2024diffusing} operate under a different set of assumptions, namely that the learner has access to a function class for the expert's score function and that the expected state norm remains bounded during learning. 
Under this setting, the authors are the first to achieve first- and second-order bounds for imitation learning, which lead to a faster rate in the case of low-variance expert policies and transitions. The authors do not quantify the MDP trajectory complexity, but it would scale suboptimally with $1/\epsilon$ because they require an expensive \emph{RL in the loop} routine that we avoid in our work.

\citet{xu2022understanding} develop an analysis for horizon-free bounds on $\tau_E$ for a special class of MDPs, where expert states can be visited only by visiting all preceding expert states.

The trajectory access to the MDP $\mathcal{M}\setminus \true$ assumed in this work should not be confused with interactive/online imitation learning, where the expert can be queried during learning \citep{Ross:2010,Ross:2011,swamy2021moments,li2022efficient,lavington2022improved,sekhari2024selective,sun2017deeply,sekhari2024contextual}. Furthermore, our trajectory access is a much weaker requirement compared to generative model access used in \citep{swamy2022minimax,Kamoutsi:2021}.

Moreover, it is important to note that we do not require any ergodicity or self-exploration properties of the dynamics, whereas such assumptions are needed in \citep{viano2022proximal,zeng2022maximum}. Additionally, uniformly good evaluation error, which is essentially possible only under generative model or ergodic dynamics assumptions, is required in \citep{wu2023inverse,zeng2022structural,zeng2023understanding}. 
Also, the use of exploration bonuses in imitation learning has also been useful for the related problem of finding the reward feasible set without using a generative model \citep{lazzati2024scale,lindner2022active}.

\begin{table}[t]
    \caption{\label{tab:literature} Comparison with related imitation learning algorithms.}
    \setlength{\tabcolsep}{4pt}  % Reduce horizontal padding
    \renewcommand{\arraystretch}{1.5}  % Increase vertical spacing
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|M{4cm}|M{1.5cm}|c|c|}
        \hline
        \textbf{Algorithm}                                         & \textbf{Setting}                                                & \textbf{F.O.}            & \textbf{Expert Traj. $(\tau_E)$}                                       & \textbf{MDP Traj. $(K)$}                                                              \\ \hline
        \multirow{3}{*}{Behavioural Cloning}                       & Function Approximation, Episodic \cite{foster2024behavior}      & \redcross                & $\cO \brr{\frac{H^2 \log \abs{\Pi}}{\epsilon^2}}$                      & \multicolumn{1}{c|}{-}                                                                \\ \cline{2-4}
                                                                   & Tabular, Episodic \cite{rajaraman2020toward}                    & \redcross                & $\widetilde{\cO} \brr{\frac{H^2 \abs{\cX}}{\epsilon}}$                 & \multicolumn{1}{c|}{-}                                                                \\ \cline{2-4}
                                                                   & Deterministic Linear Expert, Episodic \cite{rajaraman2021value} & \redcross                & $\widetilde{\cO} \brr{\frac{H^2 d}{\epsilon}}$                         & \multicolumn{1}{c|}{-}                                                                \\ \hline
        Mimic-MD \cite{rajaraman2020toward}                        & Tabular, Known $P$, Deterministic Expert, Episodic              & \redcross                & $\cO \brr{\frac{H^{3/2} \abs{\cX}}{\epsilon}}$                         & \multicolumn{1}{c|}{-}                                                                \\ \hline
        OAL \cite{Shani:2021}                                      & Episodic Tabular                                                & \redcross                & $\tilde{\cO} \brr{\frac{H^2 \abs{\cX}}{\epsilon^{2}}}$                 & $\tilde{\mathcal{O}}\brr{\frac{H^4 \abs{\X}^2 \abs{\aspace} }{\epsilon^{2}}}$         \\ \hline
        MB-TAIL \cite{xu2023provably}                              & Episodic, Tabular, Deterministic Expert                         & \redcross                & $\cO \brr{\frac{H^{3/2} \abs{\cX}}{\epsilon}}$                         & $\mathcal{O}\brr{\frac{H^3 \abs{\X}^2 \abs{\aspace} }{\epsilon^{2}}}$                 \\ \hline
        FAIL \cite{sun2019provably}                                & Episodic, $\expert \in \Pi$ and $V^\expert\in\mathcal{F}$       & \greentick$^\star$       & $\tilde{\cO} \brr{\frac{H^4 \log(\abs{\Pi} \abs{\cF} H)}{\epsilon^2}}$ & $\tilde{\mathcal{O}}\brr{\frac{H^4 \log(\abs{\Pi}\abs{\mathcal{F}} H) }{\epsilon^2}}$ \\ \hline
        Mobile \cite{kidambi2021mobile}                            & Episodic, $\true \in \mathcal{R}$ and $P \in \mathcal{P}$       & \greentick$^\star$       & $\tilde{\cO} \brr{\frac{H^2 \log (\abs{\mathcal{R}} H)}{\epsilon^2}}$  & $\tilde{\mathcal{O}}\brr{\frac{H^5 \log \abs{\mathcal{P}}}{\epsilon^2}}$              \\ \hline
        OGAIL \cite{Liu:2022}                                      & Episodic Linear Mixture MDP, $\WMAX=\sqrt{d}$                   & \greentick               & $\tilde{\cO} \brr{\frac{H^{3} d^2}{\epsilon^{2}}}$                     & $\tilde{\mathcal{O}}\brr{\frac{H^4 d^3}{\epsilon^{2}}}$                               \\ \hline
        ILARL \cite{viano2024imitation}                            & Linear MDP, $\WMAX=1$                                           & \greentick               & $\tilde{\cO} \brr{\frac{d}{(1 - \gamma)^{2} \epsilon^{2}}}$            & $\tilde{\mathcal{O}}\brr{ \frac{d^3 }{(1 - \gamma)^{8} \epsilon^{4}}}$                \\ \hline
        \FRAalg (This Work)                                        & Linear MDP                                                      & \greentick               & $\tilde{\cO} \brr{\frac{\WMAX^2}{(1 - \gamma)^{2} \epsilon^{2}}}$      & $\tilde{\mathcal{O}}\brr{ \frac{d^3 }{(1 - \gamma)^{4.5} \epsilon^{2}}}$              \\ \hline
        \textbf{Lower Bound} (This Work)                           & Linear MDP                                                      & \greentick               & $\Omega\brr{ \frac{\WMAX^2}{(1 - \gamma)^{2} \epsilon^{2}}}$           & $\Omega\brr{ \frac{d }{(1 - \gamma)^{2} \epsilon^{2}}}$                               \\ \hline
    \end{tabular}}
\end{table}

Finally, we present Table~\ref{tab:literature}, which compares our bounds with existing ones.
 We show the number of expert trajectories and MDP interactions required for $\epsilon$-suboptimal expected performance. 
 The acronym F.O.\ refers to "Features Only" and indicates whether the algorithm applies to the setting we consider here. 
 The star \greentick$^\star$ specifies that the algorithm only applies to state-only imitation learning. "Linear expert" refers to the case where an expert policy is of the form
%
\begin{align*}
    \pi \spr{x} = \max_{a \in \aspace} \phi \spr{x, a}\transpose \theta\,,
\end{align*}
%
for some vector $\theta$. Finally, in the work by \cite{kidambi2021mobile}, the bound on $K$ can be tighter than what we report in the table. We report this slightly looser version for sake of simplicity and avoiding to introduce the information gain (see \cite{kidambi2021mobile} for details).
