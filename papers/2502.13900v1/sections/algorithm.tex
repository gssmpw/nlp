\section{Algorithm}
\label{sec:algo}

Our algorithm implements the principle of \emph{optimism in the face of uncertainty} (OFU), by combining two classic ideas for optimistic exploration in reinforcement learning. We refer to these two separate mechanisms as two \emph{degrees of optimism}, with \emph{first-degree optimism} defined using the idea of exploration bonuses added to the rewards, and \emph{second-degree optimism} leveraging the notion of optimistically augmented MDPs defined in Section~\ref{sec:OAMDP}. These two incentives for exploration are respectively inspired by the upper-confidence-bound (UCB) methods popularized by \cite{azar2017minimax}, and the classic \RMAXalg algorithm of \citet{brafman2002r}. These two mechanisms are combined with the regularized approximate dynamic programming method of \citet{MN23}, called ``regularized approximate value iteration with upper confidence bounds'' (\raviUCB). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overview}

We begin by describing each element of our solution in general terms, and provide the pseudocode of the resulting algorithm (specifically tailored to linear MDPs) as Algorithm~\ref{alg:linear-rmax-ravi-ucb}. In recognition of the influence of the two algorithms mentioned above, we refer to our method as \algname.

\paragraph{Regularized dynamic programming.} If the transition kernel $P$ were known, the learner could achieve low regret by deploying the following regularized value iteration (RVI) scheme:
%
\begin{align} \label{eq:DPupdate}
    Q_{k+1} = r_k + \gamma P V_k, ~~~ V_{k+1}(x) = \max_{u\in\Delta_{\A}} \ev{\innerprod{u}{Q_{k+1}(x,\cdot)} - \frac{1}{\eta} \DDKL{u }{\pi_k(\cdot|x)}},
\end{align}
%
and update its policies as $\pi_{k+1}(x|a) \propto \pi_{k}(x|a)e^{\eta Q_{k+1} (x, a)}$ for some positive learning-rate parameter $\eta > 0$. As observed by \citet{MN23}, the regularization in the policy updates is helpful for controlling the difference between consecutive policies and occupancy measures, thus addressing a major challenge that one faces when analyzing approximate DP methods in infinite-horizon MDPs. Unfortunately, $P$ is unknown and needs to be estimated. The estimation error introduced in this process is taken care of by the two degrees of optimistic adjustments we describe next.

\paragraph{First-degree optimism.} Our method will make use of a (possibly implicitly defined) sequence of estimates of the transition operator $\wh{P}_k: \real^{\cX}\ra \real^{\cX\times\cA}$, and an associated sequence of \emph{exploration bonuses} $\CB_k: \cX\times\cA \ra \real$. We say that the sequence of bonuses is \emph{valid} if it satisfies $\abs{\pa{\bpa{\wh{P}_k - P} V_k}(x,a)} \le \CB_k(x,a)$ holds for all value-function estimates $V_k$ calculated by the algorithm, simultaneously for all $x,a$ and $k$. Using this property, a key idea in our algorithm is to use $\wh{P}_k V_k + \CB_k$ as an upper confidence bound on $P V_k$, therefore providing an optimistic estimate of the ideal value-function updates~\eqref{eq:DPupdate}.

\paragraph{Second-degree optimism.} Unfortunately, relying only on first-degree optimism as defined above may result in value estimates that grow without bounds, thus leading to unstable policy updates. In order to prevent this unbounded growth, we employ the idea of optimistically augmented MDPs (defined in Section~\ref{sec:OAMDP}), with the ascension function defined as $p_k^\upplus(x,a) = \sigma \spr{\alpha \CB_k \spr{x, a} - \omega}$, where $\sigma: z\mapsto \frac{e^{z}}{1+e^{z}}$ is the sigmoid function and $\alpha > 0$ and $\omega > 0$ are positive hyperparameters. Technically, this is implemented by defining our action-value updates for each $x,a$ as
%
\begin{equation*}
    Q_{k+1} \spr{x, a} = \spr{1 - p_k^\upplus \!\!\spr{x, a}} \spr{r_k \spr{x, a} + \CB_k \spr{x, a} + \gamma \wh{ P}_k V_k \spr{x, a}} + p_k^\upplus \!\!\spr{x, a} \frac{\RMAX}{1 - \gamma}\,.
\end{equation*}
%
This adjustment makes sure that the value estimates remain bounded, thanks to the multiplicative effect of the ascension function that effectively trades off the possibly huge values of $\CB_k + \gamma\wh{P}_k V_k$ by the constant upper bound $\RMAX/(1-\gamma)$ in highly uncertain state-action pairs. Additionally, supposing that $\CB_k$ is a valid sequence of exploration bonuses, one can verify that the inequality $Q_{k+1}  \ge r_k^\upplus + \gamma P_k^\upplus V_k$ holds elementwise---that is, the estimates $Q_k$ provide upper bounds on the ideal action-value updates~\eqref{eq:DPupdate} defined in the optimistically augmented MDP.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Technical details}

To complete the outline given above, we specify the missing technical details specific to linear MDPs.

\paragraph{Model estimation and bonus design.} For estimating the transition model and defining the exploration bonuses, we follow the classic approach of \citet{jin2019provably} (see also \citealp{Neu:2020}). Specifically, we define the least-squares model estimate $\wh{P}_k = \Phi \wh{M}_k$ as the operator that maps a function $v\in\real^\cX$ to $\phim \wh{M_k v}\in \real^{\cX\times\cA}$. The vector $\wh{M_k v}\in\real^d$ is the solution to the least-squares regression problem with features $\bc{\phi(X_t,A_t)}^{T_k - 1}_{t=1}$ and targets $\bc{v(X_t)}^{T_k}_{t=2}$, where $T_k$ denotes the beginning of episode $k$. The problem (with target $v = V_k$) admits the closed form solution given in line~\ref{line-alg:ridge} of \Cref{alg:linear-rmax-ravi-ucb}. The matrix $\Lambda_{T_k}= \sum^{T_k}_{t=1} \phi(X_t,A_t)\phi(X_t,A_t)\transpose + I$ used in the computation of the least-squares model estimate is called the \emph{empirical covariance matrix}. Finally, we define the \emph{exploration bonuses} for each $(x,a)\in\cX\times\cA$ as $\CB_k(x,a) = \beta \norm{\phi(x,a)}_{\Lambda^{-1}_{t_e}} = \beta \sqrt{\biprod{\phi(x,a)}{\Lambda^{-1}_{t_e}\phi(x,a)}}$, where $\beta>0$ will be chosen during the analysis to be large enough to guarantee bonus validity, and $t_e$ is the time step marking the beginning of the $e$th epoch (see below). The state $x^\upplus$ is given special treatment: for all actions $a$, we fix $\CB_k(x^\upplus,a) = 0$, $p^\upplus_k(x^\upplus,a) = 0$, and $Q_k(x^\upplus,a) = V_k(x^\upplus) =\frac{\RMAX}{1-\gamma}$.

\paragraph{Bookkeeping.} In order to turn the above ideas into a tractable algorithm amenable to theoretical analysis, a few additional bookkeeping steps are necessary. The most important of these is the introduction of an \emph{epoch schedule}, which is instrumental in keeping the complexity of the exploration bonuses and the policies low. Using a classic trick from \citet{APS11}, a new epoch is started every time that there is a significant reduction in the uncertainty of the model estimates (as measured by the determinant of the empirical covariance matrix of the features in the case of linear MDPs---see line~\ref{line-alg:epoch} in Algorithm~\ref{alg:linear-rmax-ravi-ucb}). When a epoch change is triggered, the exploration bonuses are recomputed and the policy is reset to a uniform policy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}

Before moving to the the analysis, we highlight some important features of our method.

\paragraph{Computational and storage complexity.} Due to its design outlined above (and detailed in \Cref{alg:linear-rmax-ravi-ucb}), \algname produces a sequence of policies that are simply parameterized by a $d$-dimensional vector and a $d^2$-dimensional covariance matrix. Specifically, this is made possible by keeping the bonus function fixed within each epoch and resetting the policy to uniform at the beginning of each new epoch. Therefore, storing the policies in memory and drawing actions in each new state can be both done efficiently. This should be contrasted with most other known algorithms making use of softmax policies, which crucially rely on clipped value-function estimates that cannot be stored or sampled from efficiently (as they require storing the entire history of parameter vectors and exploration bonuses). Examples of such methods include \cite{Cai:2020,zhong2024theoretical,sherman2023,MN23}. This not only makes the implementation of these algorithms impractical, but also results in suboptimal regret guarantees due to the excessive complexity of the policy and value-function classes. This major improvement is made possible in our algorithm by the incorporation of second-degree optimism (inspired by both \citealp{brafman2002r} and \citealp{cassel2024warmupfree}), which obviates the need for explicit clipping of the value estimates and keeps these bounded via alternative means. All other elements in our algorithm (such as estimating the value estimates via least-squares regression) are standard, and match the complexity of other efficient methods for online learning in linear MDPs \cite{jin2019provably,wang2021provably,he2023nearly}.

\paragraph{Relation with existing algorithms.} Being a combination of \RMAXalg and \raviUCB, our algorithm can recover these two extremes and several other known methods by an appropriate choice of hyperparameters. Setting $\CB_k = 0$, we recover algorithms which leverage only \emph{second-degree} optimism. For example, in the particular case of $\alpha = \infty$ and tabular features, we recover \RMAXalg up to some very minor changes \citep{brafman2002r,szita2010model}. On the other hand, using the ascension function $p^\upplus_k(x,a) = \mathds{1}\bc{r_k \spr{x, a} + \CB_k \spr{x, a} + \gamma \wh{ P}_k V_k \spr{x, a} \ge \frac{\RMAX}{1-\gamma}}$ essentially recovers the standard truncation rule applied by most related methods (under the condition that the exploration bonuses be valid). In particular, under this choice of $p^\upplus_k$, our algorithm reduces to \raviUCB. Setting the regularization parameter $\eta=\infty$ in the resulting method recovers optimistic value iteration methods such as \cite{azar2017minimax,jin2019provably}.
