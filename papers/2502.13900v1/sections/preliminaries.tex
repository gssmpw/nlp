\vspace{-2mm}
\section{Preliminaries}
\vspace{-1mm}

In this section, we first provide the general definitions that will repeatedly appear throughout the paper, and then go on to describe a set of ideas that will be heavily featured in our algorithm design and analysis. We finally describe the concrete learning setting in detail at the end of the section.

\vspace{-2mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Markov decision processes}

A Markov decision process (MDP) with reward function $r$ is defined by the tuple $\cM(r) = \spr{\cX, \cA, r, P, \gamma, \nu_0}$, where $\cX$ is the (possibly infinite) state space, $\cA$ is the finite action space, $r: \cX \times \cA \times \cX \rightarrow \sbr{0, 1}$ is the reward function assigning rewards to each state-action-next-state transition, $P: \cX \times \cA \rightarrow \Delta \spr{\cX}$ is the transition kernel, $\gamma \in \spr{0, 1}$ is the discount factor, and $\nu_0 \in \Delta \spr{\cX}$ is the initial-state distribution. For convenience, we will assume that $\cX$ is countable but note that this can be lifted at the expense of making the measure-theoretic notation much heavier.
%
The MDP $\cM(r)$ models a sequential decision-making problem between a decision-making \emph{agent} and its \emph{environment}. The interaction starts with the environment drawing the random initial state $X_0\sim\nu_0$, whereafter in each time step $t = 0, 1, 2, \dots$, the following steps are repeated: the agent observes state $X_t \in \cX$, takes action $A_t \in \cA$, and consequently the environment generates the next state $X_{t + 1} \sim P(\cdot | X_t, A_t)$, resulting in reward $R_t = r(X_t, A_t, X_{t + 1})$. With a slight abuse of notation, we denote the mean reward of a state-action pair $\spr{x, a} \in \cX \times \cA$ by $r \spr{x, a} = \EEs{r(x,a,X')}{X'\sim P(\cdot|x,a)}$.

A \emph{stationary state-feedback policy} (or, in short, \emph{policy}) is a randomized behavior rule $\pi: \cX \rightarrow \Delta \spr{\cA}$ that determines the action taken in each time step $t$ as $A_t \sim \pi(\cdot | X_t)$. The \emph{action-value function} of a policy $\pi$ in $\cM$ is defined for any state-action pair $\spr{x, a}$ as
%
\begin{align*}
    Q_{P, r}^\pi \spr{x, a} &= \bbE_{P, \pi} \sbr{\sum_{\tau=0}^\infty \gamma^\tau r \spr{X_\tau, A_\tau} \middle| (X_0,A_0) = (x,a)}\,,
\end{align*}
%
where $\bbE_{P, \pi}$ denotes the expectation with respect to the random sequence of states and actions generated by the transition kernel $P$ and the policy $\pi$. The \emph{value function} of $\pi$ at state $x$  is defined as $V_{P, r}^\pi \spr{x} = \bbE_{A \sim \pi \spr{\cdot \middle| x}} \bigl[Q_{P, r}^\pi \spr{x, A}\bigr]$. With some abuse of notation, we define the conditional expectation operator $P: \bbR^{\cX} \rightarrow \bbR^{\cX \times \cA}$ via its action $\bpa{P f} \spr{x, a} = \bbE_{X' \sim P \spr{\cdot \middle| x, a}} \sbr{f \spr{X'}}$ for any function $f \in \bbR^{\cX}$ and state-action pair $\spr{x, a}$. Its adjoint $P\transpose$ is the operator that acts on distributions $\mu \in \Delta \spr{\cX \times \cA}$ as $P\transpose \mu = \bbE_{\spr{X, A} \sim \mu} \sbr{P \spr{\cdot \middle| X, A}}$. With this notation, the value functions can be shown to satisfy the \emph{Bellman equations} written as
%
\begin{equation*}
    Q_{P, r}^\pi = r + \gamma P V_{P, r}^\pi\,.
\end{equation*}
%
For convenience, we also introduce the operator $E: \bbR^{\cX} \rightarrow \bbR^{\cX \times \cA}$ defined via $\bpa{E f} \spr{x, a} = f \spr{x}$ and whose adjoint acts on state-action distributions as $\bpa{E\transpose \mu }\spr{x} = \sum_{a \in \cA} \mu \spr{x, a}$. When interacting with an  MDP, any stationary policy $\pi$ induces a unique \emph{state-occupancy measure} denoted as $\nu\spr{\pi} \in \Delta \spr{\cX}$ and a state-action occupancy measure $\mu\spr{\pi} \in \Delta \spr{\cX\times\cA}$ defined (with an unusual but helpful abuse of notation) as
%
\begin{equation*}
    \nu\spr{\pi,\cdot} = \spr{1 - \gamma} \sum_{\tau=0}^\infty \gamma^\tau \bbP_{P, \pi} \sbr{X_\tau \in \cdot} \quad \mbox{and} \quad \mu\spr{\pi,\cdot} = \spr{1 - \gamma} \sum_{\tau=0}^\infty \gamma^\tau \bbP_{P, \pi} \sbr{(X_\tau,A_\tau) \in \cdot}.
\end{equation*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimistically augmented Markov decision processes}\label{sec:OAMDP}

A key concept in our algorithm design is that of \emph{optimistically augmented Markov decision processes} (OA-MDPs), inspired by the construction of \citet{brafman2002r}. The OA-MDP associated with $\cM(r)$ is defined on the augmented state space $\cX^\upplus = \cX \cup \ev{x^\upplus}$, where $x^\upplus$ is an artificial \emph{heaven} state appended to the original set of states. The transition dynamics are defined via a perturbation of the original transition function, governed by the \emph{ascension function} $p^\upplus:\cX^\upplus\times\cA\ra[0,1]$. In particular, the transition kernel from state-action pair $x,a$ to $x'$ is defined as
%
\begin{align*}
    P^\upplus \!\!\spr{\cdot \middle| x, a} = \bpa{1 - p^\upplus  \!\!\spr{x, a}} P \spr{\cdot \middle| x, a} + p^\upplus \!\!\spr{x, a} \II{x^\upplus\in \cdot}\,.
\end{align*}
%
In words, the sequence of states in the augmented MDP follows the dynamics of the original process, except that the process \emph{ascends} to heaven with probability $p^\upplus (X_t, A_t)$ in round $t$. The augmented reward function is the same for all triples in the original MDP $x,a,x'$, and ascension to heaven results in maximal reward $r(x,a,x^\upplus) = \RMAX$. The resulting state-action reward function is then
%
\begin{equation*}
    r^\upplus \spr{x, a} = \EEs{r \spr{x, a, X'}}{X'\sim P^\upplus \!\spr{\cdot \middle| x, a}} = \bpa{1 - p^\upplus \!\!\spr{x, a}} r \spr{x, a} + p^\upplus \!\!\spr{x, a} \RMAX\,.
\end{equation*}
%
Once the process enters $x^\upplus$, it remains there forever (\ie, $P^\upplus(\ev{x^\upplus}|x^\upplus,a) =1$ for all actions $a$) and obtains maximal reward $\RMAX$ in each round. Without loss of generality (and for notational convenience), we will assume throughout the state $x^\upplus$ also exists in the original MDP $\cM(r)$, but is not reachable either via regular transitions ($P \spr{\ev{x^\upplus} \middle| x, a} = 0$) or initialization ($\nu_0(\ev{x^\upplus})=0$). We will also follow the convention that $p^\upplus(x^\upplus,a)=0$ for all actions $a$. We will refer to the optimistically augmented MDP as $\cM^\upplus(r,p^\upplus)$, and illustrate the relation of the two processes in Figure~\ref{fig:illustration-mdps}.

Our algorithm and its analysis will feature a sequence of ascension functions denoted by $p_k^\upplus$, and the associated transition function will be denoted by $P_k^\upplus$. Within the augmented MDP induced by $p_k^\upplus$, we denote the value functions of a policy $\pi$ in $\cM^\upplus(r,p_k^\upplus)$ as $V_{P_k^\upplus, r^\upplus}^\pi$ and $Q_{P_k^\upplus, r^\upplus}^\pi$. Likewise, we will use $\nu_k^\upplus \spr{\pi}$ and $\mu_k^\upplus \spr{\pi}$ to refer to the occupancy measures of $\pi$ in $\cM^\upplus(r,p_k^\upplus)$. It is easy to see that for any policy $\pi$, the value functions satisfy $V_{P_k^\upplus, r^\upplus}^\pi \ge V_{P, r}^\pi$ and $Q_{P_k^\upplus, r^\upplus}^\pi \ge Q_{P, r}^\pi$, which explains why we call the resulting MDP ``optimistic''. Furthermore, for all non-heaven states $x$, we have $\nu_k^\upplus \spr{\pi,x} \le \nu\spr{\pi,x}$ and $\mu_k^\upplus \spr{\pi,x,a} \le \mu^\upplus \spr{\pi,x,a}$. Our analysis will heavily rely on these facts (which will be proved formally later).

%
\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.8, every node/.style={font=\small}]
        % Outer set M_t
        \draw[black, thick, line width=.6mm] (0,0) ellipse (1.4cm and 1cm);% node[below right] {\Large $\mathcal{M}_t$};
        % Label for \cM_t
        \node[black] at (-1.5,.8) {\Large $\cM$};
        
        % Central point x
        \node[circle, fill=black, inner sep=1pt, label=below:{$x$}] (x) at (0,-0.3) {};
        
        % Points inside M_t
        \node[circle, fill=black, inner sep=1pt] (p1) at (-0.8,0.3) {};
        \node[circle, fill=black, inner sep=1pt] (p2) at (0.6,-0.8) {};
        \node[circle, fill=black, inner sep=1pt] (p3) at (1,0.4) {};
        \node[circle, fill=black, inner sep=1pt] (p4) at (-1,-0.4) {};
        
        % Arrows between points in M_t
        \draw[->, thick] (x) to[bend left=20] (p1);
        \draw[->, thick] (p1) to[bend left=20] (p3);
        \draw[->, thick] (p3) to[bend left=20] (p2);
        \draw[->, thick] (p2) to[bend left=20] (x);
        \draw[->, thick] (x) to[bend left=20] (p4);
        \draw[->, thick] (p4) to[in=80, out=150, looseness=50] (p4);
        % Encircled x^+ with label
        \node[draw, circle, thick, blue!90!black, minimum size=1cm, line width=.6mm] (x_plus) at (3,-0.05) {\textcolor{blue!90!black}{$x^\upplus$}};
        \node at (3.6,-0.5) {\textcolor{blue!90!black}{$\cX^\upplus = \cX \cup \scbr{x^\upplus}$}};
        %
        \draw[blue, thick, ->] (x_plus) edge[loop above, looseness=10, out=60, in=120] (x_plus);

        % Arrows from points in M_t to x^+
        \draw[red!90!black, dashed, ->] (x) to[bend right=20] (x_plus);
        \draw[red!90!black, dashed, ->] (p1) to[bend right=20] (x_plus);
        \draw[red!90!black, dashed, ->] (p2) to[bend right=30] (x_plus);
        \draw[red!90!black, dashed, ->] (p3) to[bend left=20] (x_plus);
        \draw[red!90!black, dashed, ->] (p4) to[bend left=20] (x_plus);

        % Label for \cM_t^\upplus
        \node[red!90!black] at (2.2,.8) {\Large $\cM^\upplus$};
    \end{tikzpicture}
    \caption{Illustration of the MDP $\cM$ in black and its extension in blue. The MDP $\cM^\upplus$ contains the 
additional red dashed edges that allow ascension to heaven.}
    \label{fig:illustration-mdps}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Online learning in linear MDPs}

We consider a variation of the MDP setup described above, incorporating two modifications: \emph{i)} periodic resets of the state evolution to the initial-state distribution $\nu_0$, and \emph{ii)} the ability of the environment to change the reward function adversarially after each reset. This is a natural adaptation\footnote{See Section~\ref{sec:conclusion} for a discussion of the role of resets and related online-learning settings.} of the well-explored setting of online learning in adversarial MDPs to the discounted-reward case we study in this work. More precisely, we consider the following sequential interaction process between the learning agent and its environment. The interaction proceeds through $T$ time steps, organized into $K$ episodes (of random length) as follows. The initial state drawn as $X_0 \sim \nu_0$ and then the following steps are repeated in every consecutive round $t = 0, 1, \dots, T$: 
%
\begin{itemize}
 \item The agent observes the state $X_t \in \cX$, 
 \item the agent chooses an action $A_t \in \cA$,
 \item the environment generates the next state $X_{t+1}' \sim P \spr{\cdot \middle| X_t, A_t}$,
 \item the environment selects a reward function $r_t$,
 \item the agent receives a reward $r_t \spr{X_t, A_t} \in \sbr{0, 1}$ and observes the function $r_t$,
 \item with probability $\gamma$, the process moves to the next state $X_{t+1} = X_{t+1}'$, otherwise a new episode begins and the process is reset to the initial-state distribution as $X_{t+1}\sim \nu_0$.
\end{itemize}
%
Without significant loss of generality, we will restrict the environment to update the reward function only at the end of each episode, and use $r_k$ to refer to the reward function within episode $k$. Other than this restriction, the environment is free to choose the rewards in an adaptive (and possibly adversarial) way. The objective for the agent is to select a sequence of policies $\pi_k$ so as to minimize its \emph{pseudo-regret} over $K$ episodes with respect to an arbitrary comparator policy $\pi^\star: \cX^\upplus \rightarrow \Delta \spr{\cA}$, given by \looseness=-1
%
\begin{equation*}
    \Reg_K = \sumkK \inp{\nu_0, V_{P, r_k}^{\pi^\star} - V_{P, r_k}^{\pi_k}} = \frac{1}{1 - \gamma} \sumkK \inp{\mu \spr{\pi^\star} - \mu \spr{\pi_k}, r_k}\,,
\end{equation*}
%
where the second equality follows from the definition of occupancy measures. Since the learning agent can only learn about the transition function via interaction with the environment, it needs to address the classic dilemma of exploration versus exploitation. Clearly, this setup generalizes the more standard problem formulation where $r_k = r$ holds for all episodes $k$. In this case, $\frac{\Reg_K}{K}$ corresponds to the expected suboptimality of the average policy played by the agent.

In later sections, we will consider the following structural assumption on the transitions.
%
\begin{assumption}[Linear MDP]\label{ass:LinMDP}
    A discounted MDP $\cM = \spr{\cX, \cA, r, P, \gamma, \nu_0}$ is a \emph{linear MDP} if there exist a known feature map $\phi: \cX \times \cA \rightarrow \bbR^d$, an unknown map $m: \cX \rightarrow \bbR^d$ and an unknown vector $w \in \bbR^d$ such that for any triplet $\spr{x, a, x'} \in \cX \times \cA \times \cX$,
    %
    \begin{equation*}
        P \spr{x' \middle| x, a} = \inp{\phi \spr{x, a}, m \spr{x'}}, \quad r \spr{x, a} = \inp{\phi \spr{x, a}, w}\,.
    \end{equation*}
    %
    We will also use the operators $\phim: \mathbb{R}^d \rightarrow \real^{\X\times\A }$ such that $\pa{\Phi\theta}\pa{x,a} = \iprod{\theta}{\varphi(x,a)}$ holds for any $\theta\in\real^d$ and $M: \real^\X \rightarrow \mathbb{R}^d$ such that $\pa{Mf}_i = \int_\X f(x)\dd m_i(x')$. Thus, we can write the transition operator and the reward function as $P=\phim M$ and $r = \phim w$. Moreover, we assume that $\norm{w}\leq \WMAX$ and that for all $x,a\in\X\times\A$, the features have bounded norm, \ie $\norm{\phi(x,a)}\leq B$.
\end{assumption}

\paragraph{Further notation.} We will use $\piunif$ to denote both the uniform probability distribution over $\cA$ and the policy that plays uniformly at random at every state. $\Delta (A)$ denotes the simplex over a discrete set $A$. Given two distributions $p,q \in \Delta(\cZ)$ on the countable set $\cZ$, we denote the Kullback--Leibler divergence as $\DDKL{p}{q} = \sum_{z\in\cZ} \log \brr{\frac{p(z)}{q(z)}} p(z)$ and we use the convention that $\DDKL{p}{q}= +\infty$ whenever there exists an element $z\in \cZ$ such that $ q(z) = 0$ and $p(z) > 0$. For a distribution $p\in\Delta(\cZ)$ and a function $f\in\real^{\cZ}$, we will use the notation $\iprod{p}{f} = \EEs{f(Z)}{Z\sim p}$.
