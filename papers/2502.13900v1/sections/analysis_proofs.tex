\section{Omitted proofs from Section~\ref{sec:analysis}}
\label{app:analysis-proofs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:reward-bias-bound} (reward bias)}
\label{app:reward-bias-bound}

\rewardbiasbound*

\begin{proof}
    First note that for any action $a$, the rewards are equal, \ie $r_k \spr{x^\upplus, a} = r_k^\upplus \spr{x^\upplus, a}$. For the other states, plugging the definition of $r_k^\upplus$ gives
    %
    \begin{align*}
        \rewardbias_k &= \inp{\mu \spr{\pi^\star} - \mu \spr{\pi_k}, r_k - r_k^\upplus} \\
        &= \inp{\mu \spr{\pi^\star} - \mu \spr{\pi_k}, p_k^\upplus \odot \spr{r_k - \RMAX \bfone}} \\
        &\leq - \inp{\mu \spr{\pi_k}, p_k^\upplus \odot \spr{r_k - \RMAX \bfone}} \\
        &\leq \RMAX \inp{\mu \spr{\pi_k}, p_k^\upplus}\,,
    \end{align*}
    %
    where the first inequality follows from $r_k - \RMAX \bfone \preceq 0$ and $\mu \spr{\pi^\star} \succeq 0$, and the second inequality is due to $r_k \succeq 0$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Proof of Lemma~\ref{lem:model-bias-bounds} (model bias)}
    \label{app:model-bias-bounds}

\modelbiasbounds*

\begin{proof}
    Let us consider a process  $\spr{X_\tau, A_\tau}_{\tau \in \bbN}$ generated by the policy $\pi$ in the real MDP, \ie, such that $X_0 \sim \initial$, and for any $\tau \in \bbN$, $A_\tau \sim \pi \spr{\cdot \given X_\tau}$, and $X_{\tau+1} \sim P \spr{\cdot \given X_\tau, A_\tau}$. We denote $\spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}}_{\tau \in \bbN}$ its coupled process in the optimistic MDP at episode $k$ generated as follows. At the first stage we set $X^\upplus_{k, 0} = X_0$, then for any $\tau \geq 1$, the coupled process evolves as follows
    %
    \begin{equation*}
        X^\upplus_{k, \tau+1}, A^\upplus_{k, \tau+1} =
        \begin{cases}
            X_{\tau+1}, A_{\tau+1} \quad &\text{w.p.} \quad 1 - p^\upplus_k \spr{X_\tau, A_\tau} \quad \text{if} \quad  X^\upplus_{k, \tau}, A^\upplus_{k, \tau} = X_\tau, A_\tau \\
            x^\upplus, a \quad & \text{w.p.} \quad p^\upplus_k \spr{X_\tau, A_\tau} \quad \text{if} \quad  X^\upplus_{k, \tau}, A^\upplus_{k, \tau} = X_\tau, A_\tau \\
            x^\upplus, a \quad & \text{if} \quad X^\upplus_{k, \tau}, A^\upplus_{k, \tau} \neq X_\tau, A_\tau
        \end{cases}\,,
    \end{equation*}
    %
    where $a$ can be any action. Then, we can rewrite the bias term as
    %
    \begin{align*}
        &\modelbias_k \spr{\pi} = \spr{1 - \gamma} \bbE \sbr{\sum_{\tau=0}^\infty \gamma^\tau \spr{r_k^\upplus \spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}} - r_k^\upplus \spr{X_\tau, A_\tau}}} \\
        &\quad\quad\quad= \spr{1 - \gamma} \bbE \sbr{\sum_{\tau=0}^\infty \gamma^\tau \II{\spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}} \neq \spr{X_\tau, A_\tau}} \spr{r_k^\upplus \spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}} - r_k^\upplus \spr{X_\tau, A_\tau}}}\,.
    \end{align*}
    %
    By definition, the state-action pairs $\spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}}$ and $\spr{X_\tau, A_\tau}$ differ when the coupled process goes to heaven, \ie $X^\upplus_{k, \tau} = x^\upplus$. Noting that $r_k \spr{x^\upplus, a} = \RMAX$ for any action $a \in \cA$, we further get
    %
    \begin{align*}
        \modelbias_k \hspace{-1pt}\spr{\pi} &= \spr{1 - \gamma} \bbE \sbr{\sum_{\tau=0}^\infty \gamma^\tau \II{\spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}} \neq \spr{X_\tau, A_\tau}} \hspace{-1pt}\spr{r_k^\upplus \spr{x^\upplus, A^\upplus_{k, \tau}} - r_k^\upplus \spr{X_\tau, A_\tau}}} \\
        &= \spr{1 - \gamma} \bbE \sbr{\sum_{\tau=0}^\infty \gamma^\tau \II{\spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}} \neq \spr{X_\tau, A_\tau}} \spr{\RMAX - r_k^\upplus \spr{X_\tau, A_\tau}}}\,,
    \end{align*}
    %
    and $\modelbias_k \spr{\pi} \geq 0$ follows from $r_k^\upplus \preceq \RMAX$. For the upper bound, we can instead use $r_k \succeq 0$ and continue as follows
    %
    \begin{align*}
        \modelbias_k \spr{\pi} &\leq \spr{1 - \gamma} \RMAX \bbE \sbr{\sum_{\tau=0}^\infty \gamma^\tau \II{\spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}} \neq \spr{X_\tau, A_\tau}} } \\
        &= \spr{1 - \gamma} \gamma \RMAX \sum_{\tau=0}^\infty \gamma^\tau \bbP \sbr{\spr{X^\upplus_{k, \tau+1}, A^\upplus_{k, \tau+1}} \neq \spr{X_{\tau+1}, A_{\tau+1}}}\,,
    \end{align*}
    %
    \begin{figure}
        \begin{center}
            \begin{tikzpicture}[every node/.style={circle, draw, minimum size=1cm, font=\small},
                >={Stealth}, % Arrow style
                shorten >=1pt, shorten <=1pt, % Adjust arrow spacing
                node distance=1.3cm and 1.3cm % Adjust node distances
            ]
            
            % true process
            \node (X0) at (0,0) {$X_{k, 0}$};
            \node (X1) [right=of X0] {$X_{k, 1}$};
            \node (X2) [right=1.3cm of X1] {$X_{k, 2}$};
            \node (X3) [right=of X2] {$X_{k, 3}$};
            \node (dots) [right=of X3, draw=none] {$\cdots$};
            
            % additional transitions for utopian process
            \node (uX0) [below=of X1] {$x^\upplus$};
            \node (uX1) [below=of X2] {$x^\upplus$};
            \node (uX2) [below=of X3] {$x^\upplus$};
            \node (udots) [right=of uX2, draw=none] {$\cdots$};
            
            % transitions true process
            \draw[->] (X0) -- (X1);
            \draw[->] (X1) -- (X2);
            \draw[->] (X2) -- (X3);
            \draw[->] (X3) -- (dots);
            
            % transitions utopian process
            \draw[->, dashed] (X0) to[bend left=20] (X1);
            \draw[->, dashed] (X0) -- (uX0);
            \draw[->, dashed] (X1) to[bend left=20] (X2);
            \draw[->, dashed] (X1) -- (uX1);
            \draw[->, dashed] (X2) to[bend left=20] (X3);
            \draw[->, dashed] (X2) -- (uX2);
            \draw[->, dashed] (X3) to[bend left=20] (dots);
            
            % stuck in heaven
            \draw[->, dashed] (uX0) -- (uX1);
            \draw[->, dashed] (uX1) -- (uX2);
            \draw[->, dashed] (uX2) -- (udots);
            
            \end{tikzpicture}
        \end{center}
        %
        \caption{The thick arrows represent the transitions of the process in the original MDP, while the dashed ones correspond to the utopian one.}
        %
        \label{fig:bias-term}
    \end{figure}
    %
    \hspace{-4.5pt}where we used $\bbP \sbr{\spr{X^\upplus_{k, 0}, A^\upplus_{k, 0}} \neq \spr{X_0, A_0}} = 0$ by definition. Then, as illustrated in Figure~\ref{fig:bias-term}, two cases can happen. Either the coupled process was still in the original MDP and transitioned to heaven, either it was already in heaven. Denoting for any $\tau \geq 0$ the event $\cE_\mathrm{split} \spr{\tau} = \scbr{\spr{X^\upplus_{k, \tau}, A^\upplus_{k, \tau}} \neq \spr{X_\tau, A_\tau}}$ and $\mathcal{E}^c_\mathrm{split} \spr{\tau}$ its complementary, we have
    %
    \begin{align*}
        \bbP \sbr{\cE_\mathrm{split} \spr{\tau+1}} &= \bbP \sbr{\cE_\mathrm{split} \spr{\tau+1} \given \cE_\mathrm{split} \spr{\tau}} \bbP \sbr{\cE_\mathrm{split} \spr{\tau}} \\
        &\phantom{=}+ \bbP \sbr{\cE_\mathrm{split} \spr{\tau+1} \given \cE^c_\mathrm{split} \spr{\tau}} \bbP \sbr{\cE^c_\mathrm{split} \spr{\tau}}\,.
    \end{align*}
    %
    If the coupled process is already in the heaven state $x^\upplus$, then it stays there. Otherwise, it can transition there with probability $\bbE \sbr{p_k^\upplus \spr{X_\tau, A_\tau}}$, thus
    %
    \begin{align*}
        \bbP \sbr{\cE_\mathrm{split} \spr{\tau+1}} &= \bbP \sbr{\cE_\mathrm{split} \spr{\tau}} + \bbE \sbr{p_k^\upplus \spr{X_\tau, A_\tau}} \bbP \sbr{\cE^c_\mathrm{split} \spr{\tau}} \\
        &\leq \bbP \sbr{\cE_\mathrm{split} \spr{\tau}} + \bbE \sbr{p_k^\upplus \spr{X_\tau, A_\tau}} \\
        &\leq \sum_{u=0}^\tau \bbE \sbr{p_k^\upplus \spr{X_u, A_u}}\,,
    \end{align*}
    %
    by induction. Therefore, we get
    %
    \begin{align*}
        \modelbias_k \spr{\pi} &\leq \spr{1 - \gamma} \gamma \RMAX \sum_{\tau=0}^\infty \gamma^\tau \sum_{u=0}^\tau \bbE \sbr{p_k^\upplus \spr{X_u, A_u}} \\
        &= \spr{1 - \gamma} \gamma \RMAX \bbE \sbr{\sum_{u=0}^\infty \sum_{\tau=0}^\infty \gamma^\tau \II{u \leq \tau} p_k^\upplus \spr{X_u, A_u}} \\
        &= \spr{1 - \gamma} \gamma \RMAX \bbE \sbr{\sum_{u=0}^\infty \sum_{\tau=u}^\infty \gamma^\tau p_k^\upplus \spr{X_u, A_u}} \\
        &= \gamma \RMAX \bbE \sbr{\sum_{u=0}^\infty \gamma^u p_k^\upplus \spr{X_u, A_u}}\,,
    \end{align*}
    %
    and the conclusion follows from the definition of $\mu \spr{\pi}$ and $\gamma < 1$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \subsubsection{Alternative Proof of Lemma~\ref{lem:model-bias-bounds}}

\noindent We also provide an alternative proof based on a simulation lemma.
%
\begin{proof}
    By the flow constraints associated to $\mu_k^\upplus \spr{\pi}$ and after rearranging, we get
    %
    \begin{align*}
        \modelbias_k \spr{\pi} &= \spr{1 - \gamma} \inp{\nu_0, V_{P_k^\upplus, r_k^\upplus}^\pi - V_{P, r_k^\upplus}^\pi} \\
        &= \inp{\opE\transpose \mu_k^\upplus \spr{\pi} - \gamma \spr{\opPplusk}\transpose \mu_k^\upplus \spr{\pi}, V_{P_k^\upplus, r_k^\upplus}^\pi - V_{P, r_k^\upplus}^\pi} \\
        &= \inp{\mu_k^\upplus \spr{\pi}, \opE V_{P_k^\upplus, r_k^\upplus}^\pi - \opE V_{P, r_k^\upplus}^\pi - \gamma \opPplusk V_{P_k^\upplus, r_k^\upplus}^\pi + \gamma \opPplusk V_{P, r_k^\upplus}^\pi}\,.
    \end{align*}
    %
    Applying Lemma~\ref{lem:from-ev-to-q} to both $V_{P_k^\upplus, r_k^\upplus}^\pi$ and $V_{P, r_k^\upplus}^\pi$ and Bellman's equation for $Q_{P_k^\upplus, r_k^\upplus}^\pi$, we further have    
    %
    \begin{align*}
        \modelbias_k \spr{\pi} &= \inp{\mu_k^\upplus \spr{\pi}, Q_{P_k^\upplus, r_k^\upplus}^\pi - Q_{P, r_k^\upplus}^\pi - \gamma \opPplusk V_{P_k^\upplus, r_k^\upplus}^\pi + \gamma \opPplusk V_{P, r_k^\upplus}^\pi} \\
        &= \inp{\mu_k^\upplus \spr{\pi}, r_k^\upplus + \gamma \opPplusk V_{P, r_k^\upplus}^\pi - Q_{P, r_k^\upplus}^\pi} \,,
    \end{align*}
    %
    Plugging the definition of $P_k^\upplus$ and this time using Bellman's equation for $Q_{P, r_k^\upplus}^\pi$, we obtain
    %
    \begin{align}
        \modelbias_k \spr{\pi} &= \inp{\mu_k^\upplus \spr{\pi}, r_k^\upplus + \gamma \spr{1 - p_k^\upplus} \odot \opP V_{P, r_k^\upplus}^\pi + p_k^\upplus \odot \bfe_{x^\upplus} V_{P, r_k^\upplus}^\pi - Q_{P, r_k^\upplus}^\pi} \nonumber \\
        &= \inp{\mu_k^\upplus \spr{\pi}, p_k^\upplus \odot \spr{\bfe_{x^\upplus} V_{P, r_k^\upplus}^\pi - \gamma \opP V_{P, r_k^\upplus}^\pi}} \nonumber \\
        &= \inp{\mu_k^\upplus \spr{\pi}, p_k^\upplus \odot \spr{\frac{\RMAX}{1 - \gamma} \bfone - \gamma \opP V_{P, r_k^\upplus}^\pi}}\,, \label{eq:hope-decomp}
    \end{align}
    %
    where the last equality is due to having $\spr{\bfe_{x^\upplus} V_{P, r_k^\upplus}^\pi} \spr{x, a} = V_{P, r_k^\upplus}^\pi \spr{x^\upplus} = \frac{\RMAX}{1 - \gamma}$ for any state-action pair $\spr{x, a}$. The lower bound follows from noticing that $\opP V_{P, r_k^\upplus}^\pi \preceq \frac{1}{1 - \gamma} \bfone \preceq \frac{\RMAX}{1 - \gamma} \bfone$,
    %
    \begin{align*}
        \modelbias_k \spr{\pi} &\geq \spr{\frac{\RMAX}{1 - \gamma} - \frac{\gamma \RMAX}{1 - \gamma}} \cdot \inp{\mu_k^\upplus \spr{\pi}, p_k^\upplus} \\
        &= \RMAX \cdot \inp{\mu_k^\upplus \spr{\pi}, p_k^\upplus} \\
        &\geq 0\,.
    \end{align*}
    %
    Moving to the upper bound, from Equation~\ref{eq:hope-decomp} and $\opP V_{P, r_k^\upplus}^\pi \succeq 0$, we get
    %
    \begin{align*}
        \modelbias_k \spr{\pi} &= \inp{\mu_k^\upplus \spr{\pi}, p_k^\upplus \odot \spr{\frac{\RMAX}{1 - \gamma} \bfone - \gamma \opP V_{P, r_k^\upplus}^\pi}} \\
        &\leq \frac{\RMAX}{1 - \gamma} \inp{\mu_k^\upplus \spr{\pi}, p_k^\upplus} \\
        &\leq \frac{\RMAX}{1 - \gamma} \inp{\mu \spr{\pi}, p_k^\upplus}\,,
    \end{align*}
    %
    where the last inequality follows from Lemma~\ref{lem:mass-reduced}.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:bound-regret-plus} (optimistic regret)}
\label{app:bound-regret-plus}

In order to prove Lemma~\ref{lem:bound-regret-plus}, we first need the following result that shows that the functions $Q_k$ are optimistic estimates of an ideal sequence of dynamic-programming updates computed in the augmented MDPs. The statement is is an adaptation of Lemma~4.2 of \citet{MN23}, and its complete proof is provided below.
%
\begin{restatable}{Lem}{boundq} \label{lem:bound-q}
    Suppose that the bonuses $\CB_k$ are valid for the MDP $\cM_k$ in the sense of Equation~\ref{eq:validity-bonuses}. Then, for any $k$ and any state-action pair $\spr{x, a} \in \cX^\upplus \times \cA$, the iterates $Q_k$ satisfy
    %
    \begin{equation*} \label{eq:bound-q}
        r_k^\upplus + \gamma \opPplusk V_k \leq Q_{k+1} \leq r_k^\upplus + 2 \spr{1 - p_k^\upplus} \odot \CB_k + \gamma \opPplusk V_k\,.
    \end{equation*}
\end{restatable}

\begin{proof}
    For $x^\upplus$ and any action $a$, it is straightforward to check that both inequalities are equalities. Let $\spr{x, a} \in \cX \times \cA$. We have
    %
    \begin{align*}
        r_k^\upplus \spr{x, a} + \gamma \opPplusk V_k \spr{x, a} &= r_k^\upplus \spr{x, a} + \gamma \spr{\opPplusk - \ophPplusk} V_k \spr{x, a} + \gamma \ophPplusk V_k \spr{x, a} \\
        &\leq Q_{k+1} \spr{x, a} \\
        &\leq r_k^\upplus \spr{x, a} + 2 \spr{1 - p_k^\upplus \spr{x, a}} \CB_k \spr{x, a} + \gamma \opPplusk V_k \spr{x, a}\,,
    \end{align*}
    %
    where both inequalities use the fact that
    %
    \begin{equation*}
        \abs{\spr{\opPplusk - \ophPplusk} V_k \spr{x, a}} \leq \spr{1 - p_k^\upplus \spr{x, a}} \CB_k \spr{x, a}\,,
    \end{equation*}
    %
    which is implied by the event $\mathcal{E}_{\mathrm{valid}}$ in Equation~\ref{eq:validity-bonuses}.
\end{proof}

\boundregretplus*

\begin{proof}
    We decompose $\regretKplus$ as follows
    %
    \begin{align*}
        \regretKplus &= \sumkK \Bigl(\underbrace{\spr{\inp{\mu_k^\upplus \spr{\pi^\star}, r_k^\upplus} - \spr{1 - \gamma} \inp{\initial, V_k}}}_{= \Delta_k^\star} + \underbrace{\spr{1 - \gamma} \inp{\initial, V_k} - \inp{\mu_k^\upplus \spr{\pi_k}, r_k^\upplus}}_{= \Delta_k}\Bigr)\,,
    \end{align*}
    %
    where we defined $\Delta_k^\star$ and $\Delta_k$. We start with the first term. Using the flow constraint with $\mu_k^\upplus \spr{\pi^\star}$,
    %
    \begin{align*}
        \Delta_k^\star &= \inp{\mu_k^\upplus \spr{\pi^\star}, r_k^\upplus} - \inp{\opE\transpose \mu_k^\upplus \spr{\pi^\star} - \gamma \spr{\opPplusk}\transpose \mu_k^\upplus \spr{\pi^\star}, V_k} \\
        &= \inp{\mu_k^\upplus \spr{\pi^\star}, r_k^\upplus + \gamma \opPplusk V_k - \opE V_{k+1}} + \inp{\mu_k^\upplus \spr{\pi^\star}, \opE V_{k+1} - \opE V_k}\,.
    \end{align*}
    %
    Using the lower bound on $Q_{k+1}$ from Lemma~\ref{lem:bound-q}, we have
    %
    \begin{equation*}
        \Delta_k^\star \leq \inp{\mu_k^\upplus \spr{\pi^\star}, Q_{k+1} - \opE V_{k+1}} + \inp{\mu_k^\upplus \spr{\pi^\star}, \opE V_{k+1} - \opE V_k}\,,
    \end{equation*}
    %
    where the term in $x = x^\upplus$ is equal to zero. Summing over $k \in \sbr{K} = \bigcup_{e \in \sbr{1, E \spr{K}}} \sbr{k_e, k_{e+1} - 1}$,
    %
    \begin{equation*}
        \sumkK \Delta_k^\star \leq \sum_{e=1}^{E \spr{K}} \inp{\mu_{k_e}^\upplus \spr{\pi^\star}, \sum_{k \in \cK_e} \spr{Q_{k+1} - \opE V_{k+1}}} + \sum_{e=1}^{E \spr{K}} \inp{\opE\transpose \mu_{k_e}^\upplus \spr{\pi^\star}, V_{k_{e+1}} - V_{k_e}}_{\cX}\,,
    \end{equation*}
    %
    where the sum within each epoch of the second term telescoped. By \citealp[Lemma~C.1]{MN23}, we have for any state $x \in \cX$
    %
    \begin{align*}
        \sum_{k \in \cK_e} V_{k+1} \spr{x} &= \max_{p \in \Delta \spr{\cA}} \inp{p, \sum_{k \in \cK_e} Q_{k+1} \spr{x, \cdot}} - \frac1\eta \KL \spr{p \| \piunif} \\
        &\geq \inp{\pi^\star \spr{\cdot \given x}, \sum_{k \in \cK_e} Q_{k+1} \spr{x, \cdot}} - \frac1\eta \KL \spr{\pi^\star \spr{\cdot \given x} \| \piunif}\,,
    \end{align*}
    %
    where we used $\pi_{k_e} = \piunif$ in the first equality and denoted $\cK_e$ the set of episodes in epoch $e$. Multiplying the previous inequality by $\nu_{k_e}^\upplus \spr{\pi^\star,x}$, summing over $x \in \cX$, and noting that $\mu_{k_e}^\upplus \spr{\pi^\star} = \nu_{k_e}^\upplus \spr{\pi^\star} \odot \pi^\star$, we obtain
    %
    \begin{align*}
        \sum_{e=1}^{E \spr{K}} \inp{\mu_{k_e}^\upplus \spr{\pi^\star}, \sum_{k \in \cK_e} \spr{Q_{k+1} - \opE V_{k+1}}} &\leq \frac1\eta \sum_{e=1}^{E \spr{K}} \inp{\nu_{k_e}^\upplus \spr{\pi^\star}, \KL \spr{\pi^\star \| \piunif}} \\
        &\leq \frac{E \spr{K} \log \abs{\cA}}{\eta}\,.
    \end{align*}
    %
    The second term can be bounded with Hölder's inequality,
    %
    \begin{equation*}
        \sum_{e=1}^{E \spr{K}} \inp{\opE\transpose \mu_{k_e}^\upplus \spr{\pi^\star}, V_{k_{e+1}} - V_{k_e}} \leq \sum_{e=1}^{E \spr{K}} \norm{\nu_{k_e}^\upplus \spr{\pi^\star}}_1 \norm{V_{k_{e+1}} - V_{k_e}}_\infty \leq 2 E \spr{K} \VMAX\,,
    \end{equation*}
    %
    where we used $\nu_{k_e}^\upplus \spr{\pi^\star} \in \Delta \spr{\cX^\upplus}$ and $\norm{V_k}_\infty \leq \VMAX$ which follows from $\norm{Q_k}_\infty \leq \QMAX$. Therefore, we get
    %
    \begin{equation*}
        \sumkK \Delta_k^\star \leq \frac{E \spr{K} \log \abs{\cA}}{\eta} + 2 E \spr{K} \VMAX\,.
    \end{equation*}

    \noindent Moving to $\Delta_k$, we apply the flow constraints to $\mu_k^\upplus \spr{\pi_k}$ to get
    %
    \begin{align*}
        \Delta_k &= \inp{\opE\transpose \mu_k^\upplus \spr{\pi_k} - \gamma \spr{\opPplusk}\transpose \mu_k^\upplus \spr{\pi_k}, V_k} - \inp{\mu_k^\upplus \spr{\pi_k}, r_k^\upplus} \\
        &= \inp{\mu_k^\upplus \spr{\pi_k}, \opE V_k} - \inp{\mu_k^\upplus \spr{\pi_k}, r_k^\upplus + \gamma \opPplusk V_k} \\
        &\leq \inp{\mu_k^\upplus \spr{\pi_k}, \opE V_k - Q_{k+1}} + 2 \inp{\mu_k^\upplus \spr{\pi_k}, \spr{1 - p_k^\upplus} \odot \CB_k} \\
        &\leq \inp{\mu_k^\upplus \spr{\pi_k}, \opE V_k - Q_{k+1}} + 2 \inp{\mu \spr{\pi_k}, \spr{1 - p_k^\upplus} \odot \CB_k}\,,
    \end{align*}
    %
    where the first inequality follows from the upper bound on $Q_{k+1}$ in Lemma~\ref{lem:bound-q} and the term in $x = x^\upplus$ being equal to zero, and the second inequality is due to Lemma~\ref{lem:mass-reduced}. Next, noticing $\inp{\mu_k^\upplus \spr{\pi_{k+1}}, Q_{k+1}} = \inp{\nu_k^\upplus \spr{\pi_{k+1}}, V_{k+1} + \frac1\eta \KL \spr{\pi_{k+1} \| \pi_k}}$,
    %
    \begin{align*}
        \inp{\mu_k^\upplus \spr{\pi_k}, \opE V_k - Q_{k+1}} &= \inp{\nu_k^\upplus \spr{\pi_k}, V_k} - \inp{\mu_k^\upplus \spr{\pi_{k+1}}, Q_{k+1}} \\
        &\phantom{=}+ \inp{\mu_k^\upplus \spr{\pi_{k+1}}, Q_{k+1}} - \inp{\mu_k^\upplus \spr{\pi_k}, Q_{k+1}} \\
        &= \inp{\nu_k^\upplus \spr{\pi_k}, V_k} - \inp{\nu_k^\upplus \spr{\pi_{k+1}}, V_{k+1}} \\
        &\phantom{=}- \frac1\eta \inp{\nu_k^\upplus \spr{\pi_{k+1}}, \KL \spr{\pi_{k+1} \| \pi_k}}  \\
        &\phantom{=}+ \inp{\mu_k^\upplus \spr{\pi_{k+1}} - \mu_k^\upplus \spr{\pi_k}, Q_{k+1}}\,.
    \end{align*}
    %
    We sum over $k$ and look at the different terms separately. First, we get
    %
    \begin{align*}
        \sumkK \spr{\inp{\nu_k^\upplus \spr{\pi_k}, V_k} - \inp{\nu_k^\upplus \spr{\pi_{k+1}}, V_{k+1}}} &= \sum_{e=1}^{E \spr{K}} \spr{\inp{\nu_{k_e}^\upplus \spr{\pi_{k_e}}, V_{k_e}} - \inp{\nu_{k_e}^\upplus \spr{\pi_{k_{e+1}}}, V_{k_{e+1}}}} \\
        &\leq 2 \VMAX E \spr{K}\,.
    \end{align*}
    %
    For the third term, we have
    %
    \begin{align*}
        \sumkK \inp{\mu_k^\upplus \spr{\pi_{k+1}} - \mu_k^\upplus \spr{\pi_k}, Q_{k+1}} &= \sum_{e=1}^{E \spr{K}} \sum_{k \in \cK_e} \inp{\mu_{k_e}^\upplus \spr{\pi_{k+1}} - \mu_{k_e}^\upplus \spr{\pi_k}, Q_{k+1}}\,.
    \end{align*}
    %
    Successively applying Hölder's inequality, Pinsker's inequality and Lemma~\ref{lem:ineq-kl-entropy},
    %
    \begin{align*}
        \inp{\mu_{k_e}^\upplus \spr{\pi_{k+1}} - \mu_{k_e}^\upplus \spr{\pi_k}, Q_{k+1}} &\leq \QMAX \norm{\mu_{k_e}^\upplus \spr{\pi_{k+1}} - \mu_{k_e}^\upplus \spr{\pi_k}}_1 \\
        &\leq \QMAX \sqrt{2 \KL \spr{\mu_{k_e}^\upplus \spr{\pi_{k+1}} \| \mu_{k_e}^\upplus \spr{\pi_k}}} \\
        &\leq \QMAX \sqrt{\frac{2}{1 - \gamma} \inp{\nu_{k_e}^\upplus \spr{\pi_{k+1}}, \KL \spr{\pi_{k+1} \| \pi_k}}}\,.
    \end{align*}
    %
    For any $x \in \cX$, the KL divergence between $\pi_{k+1}$ and $\pi_k$ in state $x$ can be bounded as
    %
    \begin{align*}
        &\KL \spr{\pi_{k+1} \middle\| \pi_k} \spr{x} \\
        &\quad\quad\quad= \sum_{a \in \cA} \pi_{k+1} \spr{a \given x} \spr{\eta Q_{k+1} \spr{x, a} - \log \spr{\sum_{b \in \cA} \pi_k \spr{b \given x} \exp \sbr{\eta Q_{k+1} \spr{x, b}}}} \\
        &\quad\quad\quad= \eta \sum_{a \in \cA} \pi_{k+1} \spr{a \given x} Q_{k+1} \spr{x, a} - \log \spr{\sum_{b \in \cA} \pi_k \spr{b \given x} \exp \sbr{\eta Q_{k+1} \spr{x, b}}} \\
        &\quad\quad\quad\leq \eta \sum_{a \in \cA} \sbr{\pi_{k+1} \spr{a \given x} - \pi_k \spr{a \given x}} Q_{k+1} \spr{x, a} \\
        &\quad\quad\quad\leq \eta \QMAX \norm{\pi_{k+1} \spr{\cdot \given x} - \pi_k \spr{\cdot \given x}}_1 \\
        &\quad\quad\quad\leq \eta \QMAX \sqrt{2 \KL \spr{\pi_{k+1} \middle\| \pi_k} \spr{x}}\,,
    \end{align*}
    %
    where the first inequality follows from Jensen's and the convexity of $- \log$, the second inequality is by Hölder's and the boundedness of $Q_k$, and the last inequality is due to Pinkser's. Dividing by $\sqrt{\KL \spr{\pi_{k+1} \middle\| \pi_k} \spr{x}}$ and squaring the inequality, we get
    %
    \begin{equation*}
        \KL \spr{\pi_{k+1} \middle\| \pi_k} \spr{x} \leq 2 \eta^2 \QMAX^2\,.
    \end{equation*}
    %
    Plugging this back into the previous inequality and summing over $k \in \sbr{K}$, we get
    %
    \begin{equation*}
        \sumkK \inp{\mu_k^\upplus \spr{\pi_{k+1}} - \mu_k^\upplus \spr{\pi_k}, Q_{k+1}} \leq \frac{2 \QMAX^2 \eta K}{\sqrt{1 - \gamma}}\,.
    \end{equation*}
    %
    The remaining term is nonpositive. The sum of the $\Delta_k$ terms is thus bounded by
    %
    \begin{equation*}
        \sumkK \Delta_k \leq 2 \VMAX E \spr{K} + \frac{2 \QMAX^2 \eta K}{\sqrt{1 - \gamma}} + 2 \sumkK \inp{\mu \spr{\pi_k}, \spr{1 - p_k^\upplus} \odot \CB_k}\,.
    \end{equation*}
    %
    Finally, combining the bounds on $\sumkK \Delta_k^\star$ and $\sumkK \Delta_k$, we get
    \begin{equation*}
        \regretKplus \leq \frac{E \spr{K} \log \abs{\cA}}{\eta} + 4 \VMAX E \spr{K} + \frac{2 \QMAX^2 \eta K}{\sqrt{1 - \gamma}} + 2 \sumkK \inp{\mu \spr{\pi_k}, \spr{1 - p_k^\upplus} \odot \CB_k}\,.
    \end{equation*}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:qmax} (choice of $\QMAX$)}
\label{app:qmax}

\qmaxlemma*

\begin{proof}
    We want to find $\QMAX$ such that for any $k$, $\norm{Q_k}_\infty \leq \QMAX$. Since $V_k$ is a log-sum-exp of $Q_k$, we have $\norm{V_k}_\infty \leq \norm{Q_k}_\infty$. Next, we proceed by induction to find a suitable value of $\QMAX$. Let $k \in \bbN^\star$ and assume $\norm{Q_k}_\infty \leq \QMAX$. For any $\spr{x, a}$,
    %
    \begin{align*}
        \abs{Q_{k+1} \spr{x, a}} &\leq r_k^\upplus \spr{x, a} + \spr{1 - p_k^\upplus \spr{x, a}} \CB_k \spr{x, a} + \gamma \abs{\ophPplusk V_k \spr{x, a}} \\
        &\leq r_k^\upplus \spr{x, a} + 2 \spr{1 - p_k^\upplus \spr{x, a}} \CB_k \spr{x, a} + \gamma \opPplusk V_k \spr{x, a} \\
        &\leq \RMAX + 2 \spr{1 - p_k^\upplus \spr{x, a}} \CB_k \spr{x, a} + \gamma \QMAX\,,
    \end{align*}
    %
    where the second inequality follows from the validity of the bonuses (and corresponds to the upper bound on $Q_{k+1}$ from Lemma~\ref{lem:bound-q}), and the third inequality is due to the inductive assumption and the boundedness of the rewards. Plugging the definition of the probabilities $p_k^\upplus$, we further get
    %
    \begin{align*}
        \abs{Q_{k+1} \spr{x, a}} &\leq \RMAX + 2 \sigma \spr{\omega - \alpha \CB_k \spr{x, a}} \CB_k \spr{x, a} + \gamma \QMAX \\
        &\leq \RMAX + \gamma \QMAX + 2 \sup_{z \geq 0} \scbr{\sigma \spr{\omega - \alpha z} z} \\
        &\leq \RMAX + \gamma \QMAX + \frac{2 \omega}{\alpha}\,,
    \end{align*}
    %
    the last inequality is simply a property of the sigmoid function and is showm in Lemma~\ref{lem:sigmoid-bound2}. For the induction to work at time $k + 1$, we need to set $\QMAX$ such that $\QMAX = \RMAX + \gamma \QMAX + \frac{2 \omega}{\alpha}$, that is
    %
    \begin{equation*}
        \QMAX = \frac{\RMAX + 2 \omega / \alpha}{1 - \gamma}\,.
    \end{equation*}
    %
    The initial case is also true since $\norm{Q_1}_\infty \leq \frac{\RMAX}{1 - \gamma} \leq \QMAX$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:expected-bonuses-bound} (bound on bonuses)}
\label{app:expected-bonuses-bound}

We now control the sum of bonuses. For any episode $k$, we will denote $\cT_k$ the set of timesteps in episode $k$.

\expectedbonusesbound*

\noindent To prove Lemma~\ref{lem:expected-bonuses-bound}, we need the following result.
%
\begin{lemma} \label{lem:expectation-to-highprob}
    Suppose $\mathcal{E}_L$ holds. Let $\scbr{f_k}_{k \in \sbr{K}} \subset \bbR^{\cX \times \cA}$ be a sequence of functions with values in $\sbr{0, M}$ almost surely. Then, with probability at least $1 - \delta$ the schedule and policies produced by Algorithm~\ref{alg:linear-rmax-ravi-ucb} satisfy
    %
    \begin{equation*}
        \sumkK \inp{\mu \spr{\pi_k}, f_k} \leq 2 \spr{1 - \gamma} \sumkK \sum_{t \in \cT_k} f_k \spr{X_t, A_t} + 4 M \log \spr{\frac{2 K}{\delta}}^2\,.
    \end{equation*}
\end{lemma}
%
\begin{proof}
    We denote $\cF_{k-1}$ the $\sigma$-field generated by the history up to the end of episode $k-1$. We have,
    %
    \begin{align*}
        \inp{\mu \spr{\pi_k}, f_k} &= \spr{1 - \gamma} \bbE \sbr{\sum_{\tau = 0}^\infty \gamma^\tau f_k \spr{X_\tau, A_\tau} \given \cF_{k-1}} \\
        &= \spr{1 - \gamma} \bbE \sbr{\sum_{\tau = 0}^\infty \II{\tau < L_k} f_k \spr{X_\tau, A_\tau} \given \cF_{k-1}} \\
        &= \spr{1 - \gamma} \bbE \sbr{\sum_{\tau = 0}^{L_k - 1} f_k \spr{X_\tau, A_\tau} \given \cF_{k-1}} \\
        &= \spr{1 - \gamma} \bbE \sbr{\sum_{\tau \in \cT_k} f_k \spr{X_\tau, A_\tau} \given \cF_{k-1}}\,.
    \end{align*}
    %
    Plugging it back in the previous display,
    %
    \begin{align*}
        \sumkK \inp{\mu \spr{\pi_k}, f_k} &= \spr{1 - \gamma} \sumkK \bbE \sbr{\sum_{t \in \cT_k} f_k \spr{X_t, A_t} \given \cF_{k-1}}\,.
    \end{align*}
    %
    Since we assume $\cE_L$ holds, for any $k$ we have that $\sum_{t \in \cT_k} f_k \spr{X_t, A_t}$ takes values in $\sbr{0, M \LMAX}$ almost surely. Using the concentration inequality from Lemma~\ref{lem:concentration-ineq-cond-exp}, we get
    %
    \begin{align*}
        \sumkK \inp{\mu \spr{\pi_k}, f_k} &\leq 2 \spr{1 - \gamma} \sumkK \sum_{t \in \cT_k} f_k \spr{X_t, A_t} + 4 \spr{1 - \gamma} M \LMAX \log \spr{\frac{2 K}{\delta}} \\
        &\leq 2 \spr{1 - \gamma} \sumkK \sum_{t \in \cT_k} f_k \spr{X_t, A_t} + 4 M \log \spr{\frac{2 K}{\delta}}^2\,,
    \end{align*}
    %
    where we used that $\LMAX = \frac{\log \spr{K / \delta}}{1 - \gamma}$.
\end{proof}
%

We now prove Lemma~\ref{lem:expected-bonuses-bound}. With a slight abuse of notation, we use the convention that for any epoch $e$ and any $t$ in epoch $e$, the bonuses at time step $t$ are $\CB_t = \CB_{t_e}$. Noting that the bonuses $\CB_t$ take values in $\sbr{0, \beta B}$, we apply Lemma~\ref{lem:expectation-to-highprob} to get
%
\begin{equation*}
    \sumkK \inp{\mu \spr{\pi_k}, \CB_k} \leq 2 \spr{1 - \gamma} \sumtT \CB_t \spr{X_t, A_t} + 4 \beta B \log \spr{\frac{2 K}{\delta}}^2\,,
\end{equation*}
%
where we used $\CB_t \succeq 0$, and $T_{K+1} \leq T$ which follows from the event $\cE_L$. Likewise, applying Lemma~\ref{lem:expectation-to-highprob} to $\CB_t^2 \in \sbr{0, \beta^2 B^2}$, we obtain a similar bound for the term $\sumkK \inp{\mu \spr{\pi_k}, \CB_k^2}$,
%
\begin{equation*}
    \sumkK \inp{\mu \spr{\pi_k}, \CB_k^2} \leq 2 \spr{1 - \gamma} \sumtT \CB_t \spr{X_t, A_t}^2 + 4 \beta^2 B^2 \log \spr{\frac{2 K}{\delta}}^2\,.
\end{equation*}
%
By Cauchy-Schwartz's inequality, we have $\sumtT \CB_t \spr{X_t, A_t} \leq \sqrt{T} \sqrt{\sumtT \CB_t \spr{X_t, A_t}^2}$, so we can focus on the latter sum. By definition of the bonuses for linear MDPs, we have
%
\begin{align*}
    \sumtT \CB_t \spr{X_t, A_t}^2 &= \beta^2 \sum_{e=1}^{E \spr{K}} \sum_{k \in \cK_e} \sum_{t \in \cT_k} \norm{\phi \spr{X_t, A_t}}_{\Lambda_{t_e}^{-1}}^2\,.
\end{align*}
%
Since the covariance matrix only contains the data until the beginning of the epoch, there is a delay with $\phi \spr{X_t, A_t}$ which is further ahead. To compensate for this, note that for any $t \in \sbr{t_e, t_{e + 1}-1}$, we have $\det \Lambda_t \leq 2 \det \Lambda_{t_e}$ due to the update condition in Algorithm~\ref{alg:linear-rmax-ravi-ucb}, so by Lemma~\ref{lem:det-elliptical-bound}
%
\begin{align*}
    \norm{\phi \spr{X_t, A_t}}_{\Lambda_{t_e}^{-1}}^2 \leq \frac{\det \spr{\Lambda_{t_e}^{-1}}}{\det \spr{\Lambda_t^{-1}}} \norm{\phi \spr{X_t, A_t}}_{\Lambda_t^{-1}}^2 \leq 2 \norm{\phi \spr{X_t, A_t}}_{\Lambda_t^{-1}}^2\,.
\end{align*}
%
We plug this back into the previous inequality and apply Lemma~\ref{lem:bound-elliptical-potential} to obtain\footnote{Note that this is where the linear dependency in $B$ appears, but this can be removed by setting $\lambda = 1 / B^2$.}
%
\begin{equation*}
    \sumtT \CB_t \spr{X_t, A_t}^2 \leq 2 \beta^2 \sumtT \norm{\phi \spr{X_t, A_t}}_{\Lambda_t^{-1}}^2 \leq 4 \beta^2 B^2 \log \spr{\frac{\det \Lambda_T}{\det \Lambda_0}}\,.
\end{equation*}
%
Using the definition of $\Lambda_0, \Lambda_T$, the trace-determinant inequality, and the assumption $\norm{\phi \spr{\cdot, \cdot}}_2 \leq B$, we finally get
%
\begin{align*}
    \sumtT \CB_t \spr{X_t, A_t}^2 &\leq 4 \beta^2 B^2 d \log \spr{\frac{d + \sumtT \norm{\phi \spr{X_t, A_t}}_2^2}{d}} \\
    &\leq 4 \beta^2 B^2 d \log \spr{1 + \frac{B^2 T}{d}}\,.
\end{align*}
%
The conclusion follows from plugging this back into the inequalities of interest.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:good-event-holds} (good event holds)}
\label{app:good-event-holds}

Before stating the proof of Lemma~\ref{lem:good-event-holds}, we need to define some auxiliary quantities and state two intermediate results. First recall that $\scbr{L_k}_{k=1}^K$ denote the number of steps between consecutive resets and that for any $k \geq 2$, $L_k = T_k - T_{k-1}$, and $L_1 = T_1$. We need to prove the episodes are not too long, \ie $\cE_L = \scbr{\forall k \in \sbr{K}, L_k \leq \LMAX}$ holds with high probability, where $\LMAX = H \log \spr{K / \delta}$. This is done in Lemma~\ref{lem:lmax}. Then, we define the event $\cE_{V, \mathrm{alg}}$ on the iterates generated by Algorithm~\ref{alg:linear-rmax-ravi-ucb}
%
\begin{equation*}
    \cE_{V, \mathrm{alg}} = \scbr{\forall k \in \sbr{K}, \norm{M V_k - \wh{M V_k}}_{\Lambda_{T_k}} \leq \beta}\,,
\end{equation*}
%
where $\wh{M V_k} = \Lambda_{T_k}^{-1} \sum_{\spr{x, a, x'} \in \cD_{T_k}} \phi \spr{x, a} V_k \spr{x'}$. To prove $\cE_{V, \mathrm{alg}}$ holds with high probability, we need to resort to a standard uniform covering argument first introduced by \citealp{jin2019provably}. To do so, let us denote with $p^\upplus_{\Lambda, \beta, \alpha} = \sigma (\alpha \beta \norm{\phi\spr{\cdot,\cdot}}_{\Lambda} - w) = 1 - \sigma \spr{- \alpha \beta \norm{\phi \spr{\cdot, \cdot}}_{\Lambda} + \omega}$ an ascension function parametrized by the matrix $\Lambda$, the scalar $\beta$ and the sigmoid slope $\alpha$. Then, we define the following class of functions on $\cX \times \cA$
%
\begin{align*}
    \cQ &= \bigg\{Q: \X\times\mathcal{A} \rightarrow \mathbb{R} \quad \text{s.t.} \\
    &Q = \spr{1 - p^\upplus_{\Lambda, \beta, \alpha}} \odot \spr{\Phi \theta + \beta \norm{\phi \spr{\cdot, \cdot}}_{\Lambda}} + p^\upplus_{\Lambda, \beta, \alpha} \cdot \frac{\RMAX}{1-\gamma}, \\
    &\beta = \widetilde{\mathcal{O}}\brr{Q_{\max} d}, ~~~~\alpha = 2 \omega, ~~~\lambda_{\max}(\Lambda) \leq 1, ~~~\lambda_{\min}(\Lambda) \geq \frac{1}{2K B L_{\max}}, \\
    &\norm{\theta} \leq \WMAX + \QMAX \LMAX K B, \norm{Q}_\infty \leq \QMAX \bigg\} \cup \scbr{\mathbf{0}}\,,
\end{align*}
%
where $\QMAX = H \spr{\RMAX + \frac{2 \omega}{\alpha}}$ and we included the function $0$ to make sure $Q_1 \in \cQ$. Furthermore, denote for any $\eta > 0$ the function $f_\eta: \bbR^{\cX \times \cA} \rightarrow \bbR^\cX$ defined for $Q \in \bbR^{\cX \times \cA}$ as $f_\eta \spr{Q} = \frac1\eta \log \sum_{a \in \cA} \exp \spr{\eta Q \spr{\cdot, a}}$. We then define the following function class in $\bbR^\cX$
%
\begin{equation} \label{eq:function-class-v}
    \cV = \bigg\{ V: \cX \rightarrow \bbR \quad \text{s.t.} \quad\exists \scbr{Q_\ell}_{\ell=1}^K, \scbr{\bar{Q}_\ell}_{\ell=1}^K \subset \cQ, V = f_\eta \circ \spr{\sum^K_{\ell=1} Q_\ell} - f_\eta \circ \spr{\sum^K_{\ell=1} \bar{Q}_\ell} \bigg\},
\end{equation}
%
as well as the event
%
\begin{equation*}
    \cE_\cV = \scbr{\forall V \in \cV, \forall k \in \sbr{K}, \norm{M V - \wh{M V}}_{\Lambda_{T_k}} \leq \beta}\,,
\end{equation*}
where $\wh{M V} = \Lambda_{T_k}^{-1} \sum_{\spr{x, a, x'} \in \cD_{T_k}} \phi \spr{x, a}  V \spr{x'}$. Finally, we define the event that the iterates of Algorithm~\ref{alg:linear-rmax-ravi-ucb} are in the function class $\cV$
%
\begin{align*}
    \cE_{\mathrm{in}} = \scbr{\forall k \in \sbr{K}, V_k \in \cV}\,.
\end{align*}
%
What remains is to show that the iterates of the algorithm belong to $\cV$, and that the event $\cE_\cV$ holds with high probability. This is done in Lemmas~\ref{lem:iterates-in-class} and \ref{lem:uniform-event-holds}, respectively. We can now prove Lemma~\ref{lem:good-event-holds}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\goodeventholds*

\begin{proof}
    For any episode $k \in \sbr{K}$ and state-action pair $\spr{x, a} \in \cX \times \cA$, we have by Cauchy-Schwartz's inequality
    %
    \begin{equation*}
        \abs{P V_k \spr{x, a} - \wh{P V_k} \spr{x, a}} \leq \norm{M V_k - \wh{M V_k}}_{\Lambda_{T_k}} \norm{\phi \spr{x, a}}_{\Lambda_{T_k}^{-1}}\,.
    \end{equation*}
    %
    This inequality shows that the event $\cE_{V, \mathrm{alg}}$ implies the event $\cE_{\mathrm{valid}}$, \ie
    %
    \begin{align*}
        \bbP \sbr{\cE_{\mathrm{valid}} \cap \cE_L} &= \bbP \sbr{\cE_{\mathrm{valid}} \given \cE_L} \bbP \sbr{\cE_L} \\
        &\geq \bbP \sbr{\cE_{V, \mathrm{alg}} \given \cE_L} \bbP \sbr{\cE_L} \\
        &\geq \bbP \sbr{\cE_{V, \mathrm{alg}} \cap \cE_{\mathrm{in}} \given \cE_L} \spr{1 - \delta}\,,
    \end{align*}
    %
    where in the last inequality we used the monotonicity of $\bbP$ and Lemma~\ref{lem:lmax}. Then, conditioned on the event $\cE_{\mathrm{in}}$ that the iterates are in the function class $\cV$, the event $\cE_\cV$ implies $\cE_{V, \mathrm{alg}}$, that is
    %
    \begin{align*}
        \bbP \sbr{\cE_{V, \mathrm{alg}} \cap \cE_{\mathrm{in}} \given \cE_L} &= \bbP \sbr{\cE_{V, \mathrm{alg}} \given \cE_{\mathrm{in}} \cap \cE_L} \bbP \sbr{\cE_{\mathrm{in}} \given \cE_L} \\
        &\geq \bbP \sbr{\cE_\cV \given \cE_{\mathrm{in}} \cap \cE_L} \bbP \sbr{\cE_{\mathrm{in}} \given \cE_L} \\
        &= \bbP \sbr{\cE_\cV \cap \cE_{\mathrm{in}} \given \cE_L}\,.
    \end{align*}
    %
    Finally, by Lemma~\ref{lem:iterates-in-class} we have $\bbP \sbr{\cE_{\mathrm{in}} \given \cE_\cV, \cE_L} = 1$ thus
    %
    \begin{align*}
        \bbP \sbr{\cE_\cV \cap \cE_{\mathrm{in}} \given \cE_L} &= \bbP \sbr{\cE_{\mathrm{in}} \given \cE_\cV, \cE_L} \bbP \sbr{\cE_\cV \given \cE_L} \\
        &= \bbP \sbr{\cE_\cV \given \cE_L} \\
        &\geq 1 - \delta\,,
    \end{align*}
    %
    where the last inequality follows from Lemma~\ref{lem:uniform-event-holds}. In conclusion, we get
    %
    \begin{equation*}
        \bbP \sbr{\cE_{\mathrm{valid}} \cap \cE_L} \geq \spr{1 - \delta}^2 \geq 1 - 2 \delta\,.
    \end{equation*}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We now show the episodes are not too long.
%
\begin{lemma} \label{lem:lmax}
    Let $\delta \in \spr{0, 1}$ and define $\LMAX = H \log \spr{K / \delta}$. Then, the event $\cE_L$ holds with probability at least $1 - \delta$.
\end{lemma}

\begin{proof}
    For any $k$ and by definition of the cumulative density function of the geometric distribution with parameter $1 - \gamma$, we have that $\bbP \sbr{L_k \leq \LMAX} = 1 - \gamma^\LMAX$. Therefore, $\bbP \sbr{L_k \leq \LMAX} \geq 1 - \delta / K$ for $\LMAX \geq \frac{\log \spr{\frac{\delta}{K}}}{\log \spr{1 / \gamma}}$. Lower bounding the denominator as $\log \spr{1 / \gamma} \geq  1 - \gamma$, we have that for $\LMAX = \frac{\log \spr{K / \delta}}{1 - \gamma}$ and a union bound over $k \in \sbr{K}$, we have that  $\bbP \sbr{\cE_L} \geq 1 - \delta$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma} \label{lem:iterates-in-class}
    Assume the events $\cE_\cV$ and $\cE_L$ hold. Then, for all $k \in \sbr{K}$, it holds that $V_k \in \cV$, \ie $\cE_{\mathrm{in}}$ holds.
\end{lemma}

\begin{proof}
    The bound is proven by induction over $k \in [K]$. The base case holds by initialization since $Q_0 = \mathbf{0}$ is in $\cQ$. For the induction step, we assume that for all $\ell \in \sbr{k}$, $Q_\ell \in \mathcal{Q}, V_\ell \in \mathcal{V}$ and we show that $Q_{k+1} \in \cQ$ and $V_{k+1} \in \cV$.
    
    By definition of the function classes $\cQ$ and $\cV$ it holds that $\norm{Q_k}_{\infty}, \norm{V_k}_\infty \leq Q_{\max}$. $\cE_\cV$ together with the induction assumption imply that the bonuses are valid at time $k$, meaning that the derivations from Lemma~\ref{lem:qmax} guarantee that $\norm{Q_{k+1}}_\infty \leq \QMAX$. Moreover, denote $\theta_{k+1}$ the vector used to represent $Q_{k+1}$, defined as
    %
    \begin{align*}
        \theta_{k+1} = w_k + \gamma \wh{M V_k} = w_k + \gamma \Lambda_{T_k}^{-1} \sum_{\spr{x, a, x'} \in \cD_{T_k}} \phi \spr{x, a} V_k \spr{x'}\,.
    \end{align*}
    %
    It remains to show that $\theta_{k+1}$ satisfies the norm constraint defined in $\cQ$. By the triangular inequality and plugging the various assumptions, we have
    %
    \begin{align*}
        \norm{\theta_{k+1}} &\leq \norm{w_{k}} + \gamma \norm{(\Lambda_{T_k})^{-1}\sum_{(x,a,x')\in \mathcal{D}_{T_k}} \phi(x,a) V_k (x')} \\
        &\leq \WMAX + \gamma \lambda_{\max}((\Lambda_{T_k})^{-1}) \abs{\cD_{T_k}} \norm{V_k}_\infty \max_{x, a} \norm{\phi \spr{x, a}}_2 \\
        &\leq \WMAX + K L_{\max}Q_{\max} B\,,
    \end{align*}
    %
    where we also used $\gamma < 1$ in the last inequality. This proves that $Q_{k+1} \in \cQ$. Therefore, we have that $Q_\ell \in \cQ$ for $\ell \in \sbr{k+1}$. We now show that $V_{k+1} \in \cV$. Let $x \in \cX$ and $k_e$ be the initial index of the epoch $e$ such that $k \in \mathcal{K}_e$. By \citealp[Lemma~C.1]{MN23}, the sum of $V$ iterates is equal to a log-sum-exp function of the sum of $Q$ iterates. Thus,
    %
    \begin{align*}
        V_{k+1} \spr{x} &= \sum_{i=k_e}^{k+1} V_i \spr{x} - \sum_{j=k_e}^k V_j \spr{x} \\
        &= f_\eta \spr{\sum_{i=k_e}^{k+1} Q_i} \spr{x} - f_\eta \spr{\sum_{j=k_e}^k Q_j} \spr{x}\,.
    \end{align*}
    %
    Since $\mathbf{0} \in \cQ$ and $Q_\ell \in \cQ$ for $\ell \in \sbr{k+1}$, we can pad with zeros the two sums inside the exponentials and conclude that $V_{k+1} \spr{x}$ can be written as the difference between two log-sum-exp functions of the sum of $K$ functions in $\cQ$. Thus $V_{k+1} \in \cV$ and this concludes the induction.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma} \label{lem:uniform-event-holds}
    Assume the event $\cE_L$ holds, and set $\beta$ as
    %
    \begin{equation*}
        \beta = 8 Q_{\max} d \log(c \alpha \WMAX \RMAX B^{9/2} Q^4_{\max}L^{5/2}_{\max}K^{7/2} d^{5/2} \delta^{-1})\,.
    \end{equation*}
    %
    where $c = 60 \cdot 26$. Then, the event $\cE_\cV$ holds with probability $1 - \delta$.
\end{lemma}

\begin{proof}
    Under the event $\cE_L$, invoking standard concentration results for Linear MDPs (see Lemmas D.3 and D.4 in \cite{jin2019provably}), we have that with probability $1-\delta$ it holds that
    %
    \begin{align*}
        &\norm{MV - (\Lambda_{T_k})^{-1}\sum_{(x,a,x')\in \mathcal{D}_{T_k}} \phi(x,a) V(x')}_{\Lambda_{T_k}} \\ &~~~~~~~\leq Q_{\max} \sqrt{2 d \log\brr{\frac{1 + K L_{\max}B}{\delta}} + 4 \log \mathcal{N}_\epsilon + 8 K^2 L^2_{\max}B^2 \epsilon^2}\,,
    \end{align*}
    %
    where $\mathcal{N}_\epsilon$ is the $\epsilon $-covering number of the class $\mathcal{V}$. In particular, for $\epsilon = (K L_{\max} B)^{-1}$, we can invoke Lemma~\ref{lem:covering-number-v} to obtain
    %
    \begin{align*}
        \log \mathcal{N}_{\epsilon} &\leq 4d^2 \log \brr{4(\WMAX B + Q_{\max}L_{\max}K B^2 + 3\sqrt{d}+ \beta B +\RMAX)\sqrt{K^5L^3_{\max}}\alpha\beta B^{5/2}} \\
        &\leq 4d^2 \log \brr{20(3 \WMAX \beta Q_{\max}L_{\max}KB^2\sqrt{d}\RMAX)\sqrt{K^5L^3_{\max}}\alpha\beta B^{5/2}} \\
        &\leq 4d^2 \log \brr{60 \WMAX \RMAX \beta^2 \alpha B^{9/2} Q^2_{\max}L^{5/2}_{\max}K^{7/2}\sqrt{d}}\,.
    \end{align*}
    %
    Plugging in, we have that
    %
    \begin{align*}
        &\norm{MV - (\Lambda_{T_k})^{-1}\sum_{(x,a,x')\in \mathcal{D}_{T_k}} \phi(x,a) V(x')}_{\Lambda_{T_k}} \\ 
        &~~~~~~~\leq Q_{\max} \sqrt{2 d \log\brr{\frac{1 + K L_{\max} B}{\delta}} + 16 d^2 \log \brr{60 \beta^2 \alpha B^{9/2}Q^2_{\max}L^{5/2}_{\max}K^{7/2}\sqrt{d}} + 8} \\
        &~~~~~~~\leq Q_{\max} \sqrt{26 d^2 \log \brr{\frac{60 \WMAX \RMAX \beta^2 \alpha B^{9/2} Q^2_{\max}L^{5/2}_{\max}K^{7/2}\sqrt{d}}{\delta}}} \\ 
        &~~~~~~~ = \sqrt{26 Q^2_{\max} d^2 \log \brr{\frac{60 \WMAX \RMAX \beta^2 \alpha  B^{9/2} Q^2_{\max}L^{5/2}_{\max}K^{7/2}\sqrt{d}}{\delta}}} \\
    \end{align*}
    %
    At this point, to find a value for $\beta$ such that
    \begin{equation*}
        \beta^2 \geq 26 Q^2_{\max} d^2 \log \brr{\frac{60 \WMAX \RMAX \beta^2 \alpha B^{9/2} Q^2_{\max}L^{5/2}_{\max}K^{7/2}\sqrt{d}}{\delta}}\,,
    \end{equation*}
    we invoke Lemma~\ref{lemma:beta_bound} with $z = 26 Q^2_{\max} d^2$ and $R = \frac{60 \WMAX \RMAX \alpha B^{9/2}Q^2_{\max}L^{5/2}_{\max}K^{7/2}\sqrt{d}}{\delta}$ which gives that the desired inequality holds for all $\beta \in \mathbb{R}$ such that
    %
    \begin{equation*}
        \beta^2 \geq 52 Q^2_{\max} d^2 \log(c \alpha \WMAX \RMAX B^{9/2} Q^4_{\max}L^{5/2}_{\max}K^{7/2} d^{5/2} \delta^{-1})\,,
    \end{equation*}
    %
    where $c = 60 \cdot 26$. Therefore, we select
    \begin{equation*}
        \beta = 8 Q_{\max} d \log(c \alpha \WMAX \RMAX B^{9/2} Q^4_{\max}L^{5/2}_{\max}K^{7/2} d^{5/2} \delta^{-1})\,.
    \end{equation*}
\end{proof}

\begin{remark}
    For the proof of Lemma~\ref{lem:uniform-event-holds}, we need to compute a bound on the covering number of the function class $\cV$. We find this is done in a neat and more direct way than previous analysis \cite{zhong2024theoretical,sherman2023rate,cassel2024warmupfree} that needed to introduce a policy class for the iterates $\bc{\pi_k}^K_{k=1}$ generated by \Cref{alg:linear-rmax-ravi-ucb} as an intermediate step.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Proof of Lemma~\ref{lem:covering-number-v} (covering number)}
\label{app:covering-number-v}

\begin{lemma} \label{lem:covering-number-v}
    Let us consider the function class $\cV$ defined in Equation~\eqref{eq:function-class-v} and an $\epsilon$-covering set $\cR \spr{\cV}$ such that for any $V \in \cV$, there exists $V' \in \cR \spr{\cV}$ such that $\norm{V - V'}_\infty \leq \frac{1}{K L_{\max} B}$. The covering number of the class $\cV$ can be bounded as follows
    %
    \begin{equation*}
        \log \cN_{\frac1K} \leq 4 d^2 \log \spr{4 \spr{\WMAX B + \QMAX \LMAX K B^2 + 3 \sqrt{d} + \beta B + \RMAX H} \sqrt{K^5 \LMAX^3} \alpha \beta B^{5/2}}\,.
    \end{equation*}
\end{lemma}

\begin{proof}
    We will use the following intermediate class of log sum exp state value functions
    %
    \begin{equation*}
        \tilde{\cV} = \scbr{V: \cX \rightarrow \bbR \quad\text{s.t.}\quad \forall x, V \spr{x} = \frac1\eta \log \sum_{a \in \cA}\exp \spr{\eta \sum_{\ell=1}^K Q_\ell \spr{x, a}}, Q_\ell \in \cQ}\,.
    \end{equation*}
    %
    Consider any $V, V' \in \cV$, and notice that for any $x \in \cX$,
    %
    \begin{equation*}
        \abs{V \spr{x} - V' \spr{x}} \leq \abs{\bar{V} \spr{x} - \bar{V}' \spr{x}} + \abs{\tilde{V} \spr{x} - \tilde{V}' \spr{x}}.
    \end{equation*}
    %
    with $\bar{V}, \tilde{V} \in \tilde{\cV}$ such that $V \spr{x} = \bar{V} \spr{x} - \tilde{V} \spr{x}$ for all $x \in \cX$ and with $\bar{V}', \tilde{V}' \in \tilde{\cV}$ such that $V' \spr{x} = \bar{V}' \spr{x} - \tilde{V}' \spr{x}$ for all $x \in \cX$. Therefore, the above bound guarantees that an $\epsilon / 2$ covering set on the function class $\tilde{\cV}$ implies an $\epsilon$ covering for the class $\cV$. Hence, in the following we focus on computing a $\epsilon / 2$ covering number for $\tilde{\cV}$. By definition of $\bar{V}, \bar{V}'$ and Lemma~\ref{lem:lse-lipschitz}, we have
    %
    \begin{align*}
        \abs{\bar{V} \spr{x} - \bar{V}' \spr{x}} &= \abs{\frac1\eta \log \sum_{a \in \cA} \exp \brr{\eta \sum^K_{\ell=1} \bar{Q}_\ell \spr{x, a}} - \frac1\eta \log \sum_{a \in \cA} \exp \spr{\eta \sum^K_{\ell=1} \bar{Q}_\ell' \spr{x, a}}} \\
        &\leq \max_{a \in \A} \abs{ \sum^K_{\ell=1} \bar{Q}_\ell \spr{x, a} -  \sum^K_{\ell=1} \bar{Q}'_\ell \spr{x, a}} \\
        &\leq K \max_{\ell \in [K]} \norm{ \bar{Q}_\ell - \bar{Q}'_\ell }_{\infty}\,.
    \end{align*}
    %
    For any $\ell \in \sbr{K}$, we denote $\Lambda_\ell$, $\theta_\ell$ the parameters of the function $\bar{Q}_\ell$ and $\Lambda'_\ell$, $\theta'_\ell$ the parameters of the function $\bar{Q}'_\ell$. We now prove that $\bar{Q}_\ell$,$\bar{Q}'_\ell$ are Lipschitz functions. Let us denote $Q_{\theta, \Lambda}$ and $Q_{\theta', \Lambda'}$ two functions in $\cQ$ for different parameters $\theta, \Lambda, \theta', \Lambda'$. For any state-action pair $\spr{x, a}$, the difference between the two functions can be written as
    %
    \begin{align*}
        &Q_{\theta, \Lambda} \spr{x, a} - Q_{\theta', \Lambda'} \spr{x, a} = \spr{\phi \spr{x, a}\transpose \theta + \beta \norm{\phi \spr{x, a}}_\Lambda - \RMAX H} \cdot \sigma \spr{- \alpha \beta \norm{\phi \spr{x, a}}_\Lambda + \omega} \\
        &\phantom{=}\quad\quad\quad\quad\quad- \spr{\phi \spr{x, a}\transpose \theta' + \beta \norm{\phi \spr{x, a}}_{\Lambda'} - \RMAX H} \cdot \sigma \spr{- \alpha \beta \norm{\phi \spr{x, a}}_{\Lambda'} + \omega}\,.
    \end{align*}
    %
    Next, our goal is to show that the function
    %
    \begin{equation*}
        f \spr{\theta, \Lambda; x, a} := \spr{\phi(x,a)\transpose \theta + \beta \norm{\phi (x, a)}_{\Lambda} - \RMAX H} \cdot \sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda} + \omega)
    \end{equation*}
    %
    is Lipshitz in both parameters $\theta, \Lambda$. The Lipshitzness with respect to $\beta$ does not need to be established since it is kept fixed throughout the learning process. We show this showing that the gradients are bounded. In particular,
    %
    \begin{equation*}
        \norm{\nabla_\theta f(\theta,\Lambda;x,a)} = \norm{\phi(x,a)} \cdot \sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda} + \omega) \leq \norm{\phi(x,a)}  \leq B\,.
    \end{equation*}
    %
    For the Lipshitzness with respect to $\Lambda$, we have that
    %
    \begin{align*}
        &f(\theta,\Lambda;x,a) - f(\theta,\Lambda';x,a) \\
        &= \spr{\phi(x,a)\transpose \theta + \beta \norm{\phi(x,a)}_{\Lambda} 
        - \RMAX H}\cdot \sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda} + \omega) \\
        &\phantom{=}- \spr{\phi(x,a)\transpose \theta + \beta \norm{\phi(x,a)}_{\Lambda'} 
        - \RMAX H}\cdot \sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda'} + \omega) \\
        &= \spr{\phi(x,a)\transpose \theta + \beta \norm{\phi(x,a)}_{\Lambda} 
        - \RMAX H}\cdot \Bigl(\sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda} + \omega) \\
        &\phantom{=}- \sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda'} + \omega) \Bigr) + \sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda'} + \omega) \brr{\beta \norm{\phi(x,a)}_{\Lambda} - \beta \norm{\phi(x,a)}_{\Lambda'}}
    \end{align*}
    %
    Then, using the fact that $\sigma$ is $1$-Lipshitz, we have that
    %
    \begin{align*}
        &\abs{f(\theta,\Lambda;x,a) - f(\theta,\Lambda';x,a)} \\
        &\quad \leq \alpha \beta \abs{\phi(x,a)\transpose \theta + \beta \norm{\phi(x,a)}_{\Lambda} - \RMAX H} \cdot \abs{\norm{\phi(x,a)}_{\Lambda} - \norm{\phi(x,a)}_{\Lambda'}} \\
        &\quad\quad + \sigma(- \alpha\beta \norm{\phi(x,a)}_{\Lambda'} + \omega) \abs{\beta \norm{\phi(x,a)}_{\Lambda} - \beta \norm{\phi(x,a)}_{\Lambda'}} \\
        &\leq \alpha\beta \abs{\phi(x,a)\transpose \theta + \beta \norm{\phi(x,a)}_{\Lambda} 
        - \RMAX H} \cdot \abs{\norm{\phi(x,a)}_{\Lambda} 
        - \norm{\phi(x,a)}_{\Lambda'}}
        \\&\phantom{=}+  \beta \abs{\norm{\phi(x,a)}_{\Lambda} - \norm{\phi(x,a)}_{\Lambda'}} \\
        &\leq \alpha\beta (\norm{\theta}B + \beta B + \RMAX H + 1) 
        \abs{\norm{\phi(x,a)}_{\Lambda} - \norm{\phi(x,a)}_{\Lambda'}}.
    \end{align*}
    %
    where we used the fact that $\sigma(x) \leq 1$, for all $x\in \mathbb{R}$ and $\alpha \geq 1$ in the last inequality. 
    Using that $\norm{\phi (x,a)}_{\Lambda} = \norm{\Lambda^{1/2} \phi(x,a)}$ and the triangular inequality we have that
    %
    \begin{align*}
        f(\theta,\Lambda;x,a) - f(\theta,\Lambda';x,a) &\leq 
        \alpha\beta (\norm{\theta}B + \beta B +\RMAX H + 1) 
        \abs{\norm{\Lambda^{1/2}\phi(x,a)} - \norm{(\Lambda')^{1/2}\phi(x,a)}} \\
        &\leq \alpha\beta (\norm{\theta}B + \beta B +\RMAX H + 1) \norm{(\Lambda^{1/2} - (\Lambda')^{1/2})\phi(x,a)} \\
        &\leq \alpha \beta B (\norm{\theta} B + \beta B +\RMAX H + 1) \norm{\Lambda^{1/2} - (\Lambda')^{1/2}} 
    \end{align*}
    %
    where the last inequality holds for $\norm{\phi (x,a)} \leq B$. Finally, using the definition of the class $\mathcal{Q}$, we have that the matrices $\Lambda$ and $\Lambda'$ are positive definite in particular $\lambda_{\min}(\Lambda) \geq \frac{1}{2KBL_{\max}}$ and $\lambda_{\min}(\Lambda') \geq \frac{1}{2KBL_{\max}}$. 
    Therefore by \cite[Lemma 17]{cassel2024warmupfree}, 
    it holds that $\norm{\Lambda^{1/2} - (\Lambda')^{1/2}} \leq \frac{1}{2\sqrt{\lambda_{\min}}}\norm{\Lambda - \Lambda'} = \sqrt{\frac{B K L_{\max}}{2}} \norm{\Lambda - \Lambda'}$. 
    Therefore, all in all we have that
    %
    \begin{align*}
        f(\theta,\Lambda;x,a) - f(\theta,\Lambda';x,a)
        &\leq \sqrt{K L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B
        + \beta B +\RMAX H + 1) \norm{\Lambda - \Lambda'} \\
        &\leq \sqrt{K L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B
        + \beta B +\RMAX H + 1) \norm{\Lambda - \Lambda'}_F,
    \end{align*}
    %
    where $\norm{\cdot}_F$ denote the Frobenious norm of a matrix. Hence, we have that
    %
    \begin{align*}
        Q_{\theta,\Lambda}(x,a) - Q_{\theta',\Lambda'}(x,a) \leq 
        \sqrt{K L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1)
         \norm{\Lambda - \Lambda'}_F + B \norm{\theta - \theta'}.
    \end{align*}
    %
    At this point, if we have a $\epsilon_{\Lambda}$-covering set for the set
    %
    \begin{equation*}
        \mathbf{\Lambda} = \bc{\Lambda \in \mathbb{R}^{d\times d} : \lambda_{\max}(\Lambda) \leq 1, ~~~\lambda_{\min}(\Lambda) \geq \frac{1}{2BKL_{\max}}}
    \end{equation*}
    %
    and an $\epsilon_\theta$-covering set for the set
    %
    \begin{equation*}
        \mathbf{\Theta} = \bc{\theta \in \mathbb{R}^d: \norm{\theta} \leq \WMAX + Q_{\max}L_{\max}K B}
    \end{equation*}
    %
    we would have that
    %
    \begin{align*}
        \abs{V(x) - V'(x)} &\leq 2\sqrt{ K^3 L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1) \epsilon_F + 2 B K \epsilon_\theta \\
        &\leq 2 \sqrt{K^3L_{\max}}\alpha \beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1) \brr{\epsilon_F + \epsilon_\theta},
    \end{align*}
    %
    where in the last inequality we assumed that $\beta \geq 1$ and $B\geq 1$.
    Therefore, to have an $\epsilon$-covering set for $\mathcal{V}$, we need to construct an $\epsilon_{\Lambda}$-covering set for $\mathbf{\Lambda}$, where
    %
    \begin{equation*}
        \epsilon_\Lambda = \frac{\epsilon}{4 \sqrt{K^3L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1)}
    \end{equation*}
    %
    and an $\epsilon_\theta = \frac{\epsilon}{4 \sqrt{K^3L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1)}$-covering set for $\mathbf{\Theta}$. Then, using the fact that the $\epsilon$-covering number for the Euclidean ball of radius $R$ in $d$ dimension is given by $(1 + 2 R/\epsilon)^d$, we obtain
    %
    \begin{equation*}
        \log \mathcal{N}_{\epsilon_{\theta}}(\mathbf{\Theta}) \leq d \log \brr{1 + 8\frac{(\WMAX + Q_{\max}L_{\max}K B )\sqrt{K^3L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1)}{\epsilon}}
    \end{equation*}
    %
    Moreover, noticing that for all matrices $\Lambda \in \mathbf{\Lambda}$ it holds that $\norm{\Lambda}_F \leq \sqrt{d} \lambda_{\max}(\Lambda) \leq \sqrt{d}$, we need to cover the Frobenious norm ball with radius $\sqrt{d}$. Recalling that the Frobenious norm of a matrix is equivalent to the euclidean norm of the vectorization of the matrix, this equivalent to cover the euclidean ball in $\mathbb{R}^{d^2}$ with radius $\sqrt{d}$.
    %
    \begin{equation*}
        \log \mathcal{N}_{\epsilon_{\Lambda}}(\mathbf{\Lambda}) 
        \leq d^2 \log \brr{1 + 8\sqrt{d}\frac{\sqrt{K^3L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1)}{\epsilon}}.
    \end{equation*}
    %
    Therefore, using the fact that
    %
    \begin{align*}
        &\log \mathcal{N}_{\epsilon}(\mathcal{V})=  
        \log \mathcal{N}_{\epsilon_{\Lambda}}(\mathbf{\Lambda}) + 
        \log \mathcal{N}_{\epsilon_{\theta}}(\mathbf{\Theta}) \\
        \\ & \leq d \log \brr{1 + 8\frac{(\WMAX + Q_{\max}L_{\max}K B )\sqrt{K^3L_{\max}}\alpha\beta B^{3/2} 
        (\norm{\theta} B + \beta B +\RMAX H + 1)}{\epsilon}} \\&\phantom{=}+ 
        d^2 \log \brr{1 + 8\sqrt{d}\frac{\sqrt{K^3L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + \beta B +\RMAX H + 1)}{\epsilon}} \\
        \\ & \leq 2d^2\log  \brr{1 + 8\frac{(\WMAX + Q_{\max}L_{\max}K B + \sqrt{d})\sqrt{K^3L_{\max}}\alpha\beta B^{3/2} (\norm{\theta} B + 
        \beta B +\RMAX H + 1)}{\epsilon}} \\
        & \leq 2d^2 \log \brr{1 + 8\frac{(\WMAX B + Q_{\max}L_{\max}K B^2 + \sqrt{d}+ \beta B +\RMAX H +1)^2\sqrt{K^3L_{\max}}\alpha\beta B^{3/2}}{\epsilon}} \\
        & \leq 2d^2 \log \brr{16\frac{(\WMAX B + Q_{\max}L_{\max}K B^2 + 3\sqrt{d}+ \beta B +\RMAX H)^2\sqrt{K^3L_{\max}}\alpha\beta B^{3/2}}{\epsilon}} \\
        & \leq 4d^2 \log \brr{4\frac{(\WMAX B + Q_{\max}L_{\max}K B^2  + 3\sqrt{d}+ \beta B +\RMAX H)\sqrt{K^3L_{\max}}\alpha\beta B^{3/2}}{\epsilon}} \\
        & = 4d^2 \log \brr{4(\WMAX B + Q_{\max}L_{\max}K B^2 + 3\sqrt{d}+ \beta B +\RMAX H)\sqrt{K^5L^3_{\max}}\alpha\beta B^{5/2}}.
    \end{align*}
    %
    where we used $d > 1$ and the last step uses the fact that we are looking for a $\epsilon = \frac{1}{K L_{\max} B}$ covering set. Finally,
    %
    \begin{equation*}
        \log \mathcal{N}_{\epsilon} \leq 4d^2 \log \brr{4(\WMAX B +  Q_{\max}L_{\max}K B^2 + 3\sqrt{d}+ \beta B +\RMAX H)\sqrt{K^5L^3_{\max}}\alpha\beta B^{5/2}}\,.
    \end{equation*}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Putting everything together (proof of Theorem~\ref{thm:main})}
\label{app:putting-together-main}

\begin{theorem} \label{thm:main-full}
    Run Algorithm~\ref{alg:linear-rmax-ravi-ucb} with parameters $\omega = \log K$, $\alpha = 2 \log K$,
    \begin{align*}
        \eta = \sqrt{\frac{5 d \log \spr{1 + B^2 T / d} \log \abs{\cA}}{8 \RMAX^2 H^{5 / 2} K}}, \quad\text{and } \beta = C H \RMAX d \log \spr{B H \WMAX \RMAX d K \delta^{-1}}\,,
    \end{align*}
    %
    for some absolute constant $C > 0$ and $\delta \in \spr{0, 1}$. Then, with probability at least $1 - \delta$, we have
    %
    \begin{align*}
        \regretK &= \tilde\cO \spr{\sqrt{d^3 H^3 K} + \sqrt{d H^{9/2} K \log \spr{\abs{\cA}}}} \\
        &= \tilde\cO \spr{\sqrt{d^3 H^2 T} + \sqrt{d H^{7/2} T \log \spr{\abs{\cA}}}}\,.
    \end{align*}
\end{theorem}

\begin{proof}
    We are now ready to prove Theorem~\ref{thm:main}. Combining Lemma~\ref{lem:reward-bias-bound}, and the bounds in Equations~\eqref{eq:model-bias-bounds} and Lemma~\ref{lem:bound-regret-plus} we first get
    %
    \begin{align*}
        \frac1H \regretK &\leq 2 \RMAX H \sumkK \inp{\mu \spr{\pi_k}, p_k^\upplus} + 4 \QMAX E \spr{K} + \frac{E \spr{K} \log \abs{\cA}}{\eta} \\
        &\phantom{=}+ 2 \eta \QMAX^2 \sqrt{H} K + 2 \sumkK \inp{\mu \spr{\pi_k}, \spr{1 - p_k^\upplus} \odot \CB_k}\,.
    \end{align*}
    %
    Using the bound on the ascension functions provided in Inequality~\ref{eq:ascension-function-bound} and $1 - p_k^\upplus \preceq 1$, we further have
    %
    \begin{align*}
        \frac1H \regretK &\leq 4 \RMAX H \alpha^2 \sumkK \inp{\mu \spr{\pi_k}, \CB_k^2} + 4 \RMAX e^{- \omega} H K + 4 \QMAX E \spr{K} \\
        &\phantom{=}+ \frac{E \spr{K} \log \abs{\cA}}{\eta} + 2 \eta \QMAX^2 \sqrt{H} K + 2 \sumkK \inp{\mu \spr{\pi_k}, \CB_k}\,.
    \end{align*}
    %
    Lemma~\ref{lem:expected-bonuses-bound} can be used to bound the bonuses
    %
    \begin{align*}
        \frac1H \regretK &\leq 32 \RMAX \alpha^2 \beta^2 B^2 d \log \spr{1 + \frac{B^2 T}{d}} + 16 \RMAX H \alpha^2 \beta^2 B^2 \log \spr{\frac{2 K}{\delta}}^2 \\
        &\phantom{=}+ 4 \RMAX e^{- \omega} H K + 4 \QMAX E \spr{K} + \frac{E \spr{K} \log \abs{\cA}}{\eta} + 2 \eta \QMAX^2 \sqrt{H} K \\
        &\phantom{=}+ \frac{8 \beta B}{H} \sqrt{d T \log \spr{1 + \frac{B^2 T}{d}}} + 8 \beta B \log \spr{\frac{2 K}{\delta}}^2\,.
    \end{align*}
    %
    Following Lemmas~\ref{lem:qmax} and \ref{lem:lmax}, we plug the values of $\QMAX = H \spr{\RMAX + \frac{2 \omega}{\alpha}}$ and $\LMAX = H \log \spr{K / \delta}$,
    %
    \begin{align*}
        \frac1H \regretK &\leq 32 \RMAX \alpha^2 \beta^2 B^2 d \log \spr{1 + \frac{B^2 T}{d}} + 16 \RMAX H \alpha^2 \beta^2 B^2 \log \spr{\frac{2 K}{\delta}}^2 \\
        &\phantom{=}+ 4 \RMAX e^{- \omega} H K + 4 H \spr{\RMAX + \frac{2 \omega}{\alpha}} E \spr{K} + \frac{E \spr{K} \log \abs{\cA}}{\eta} \\
        &\phantom{=}+ 2 \eta \spr{\RMAX + \frac{2 \omega}{\alpha}}^2 H^{5/2} K + \frac{8 \beta B}{H} \sqrt{d T \log \spr{1 + \frac{B^2 T}{d}}} \\
        &\phantom{=}+ 8 \beta B \log \spr{\frac{2 K}{\delta}}^2\,.
    \end{align*}
    %
    By Lemma~\ref{lemma:number-epochs-bound}, we can bound $E \spr{K} \leq 5 d \log \spr{1 + \frac{B^2 T}{d}}$
    %
    \begin{align*}
        \frac1H \regretK &\leq 32 \RMAX \alpha^2 \beta^2 B^2 d \log \spr{1 + \frac{B^2 T}{d}} + 16 \RMAX \alpha^2 \beta^2 B^2 H \log \spr{\frac{2 K}{\delta}}^2 \\
        &\phantom{=}+ 4 \RMAX e^{- \omega} H K + 20 H d \spr{\RMAX + \frac{2 \omega}{\alpha}} \log \spr{1 + \frac{B^2 T}{d}} \\
        &\phantom{=}+ \frac{5 d}{\eta} \log \spr{1 + \frac{B^2 T}{d}} \log \abs{\cA} + 2 \eta \spr{\RMAX + \frac{2 \omega}{\alpha}}^2 H^{5/2} K \\
        &\phantom{=}+ \frac{8 \beta B}{H} \sqrt{d T \log \spr{1 + \frac{B^2 T}{d}}} + 8 \beta B \log \spr{\frac{2 K}{\delta}}^2\,.
    \end{align*}
    %
    It remains to choose the parameters. We start by setting $\alpha = 2 \omega$ and use $\RMAX \geq 1$ to get
    %
    \begin{align*}
        \frac1H \regretK &\leq 128 \RMAX \omega^2 \beta^2 B^2 d \log \spr{1 + \frac{B^2 T}{d}} + 64 \RMAX \omega^2 \beta^2 B^2 H \log \spr{\frac{2 K}{\delta}}^2 \\
        &\phantom{=}+ 4 \RMAX e^{- \omega} H K + 40 H d \RMAX \log \spr{1 + \frac{B^2 T}{d}} \\
        &\phantom{=}+ \frac{5 d}{\eta} \log \spr{1 + \frac{B^2 T}{d}} \log \abs{\cA} + 8 \eta \RMAX^2 H^{5/2} K \\
        &\phantom{=}+ \frac{8 \beta B}{H} \sqrt{d T \log \spr{1 + \frac{B^2 T}{d}}} + 8 \beta B \log \spr{\frac{2 K}{\delta}}^2\,.
    \end{align*}
    %
    Then, we set $\omega = \log K$
    %
    \begin{align*}
        \frac1H \regretK &\leq 128 \RMAX \beta^2 B^2 d \log \spr{K}^2 \log \spr{1 + \frac{B^2 T}{d}} + 64 \RMAX \beta^2 B^2 H \log \spr{K}^2 \log \spr{\frac{2 K}{\delta}}^2 \\
        &\phantom{=}+ 4 \RMAX H + 40 H d \RMAX \log \spr{1 + \frac{B^2 T}{d}} \\
        &\phantom{=}+ \frac{5 d}{\eta} \log \spr{1 + \frac{B^2 T}{d}} \log \abs{\cA} + 8 \eta \RMAX^2 H^{5/2} K \\
        &\phantom{=}+ \frac{8 \beta B}{H} \sqrt{d T \log \spr{1 + \frac{B^2 T}{d}}} + 8 \beta B \log \spr{\frac{2 K}{\delta}}^2\,.
    \end{align*}
    %
    We choose the learning rate as $\eta = \sqrt{\frac{5 d \log \spr{1 + B^2 T / d} \log \abs{\cA}}{8 \RMAX^2 H^{5 / 2} K}}$ and we obtain
    %
    \begin{align*}
        \frac1H \regretK &\leq 128 \RMAX \beta^2 B^2 d \log \spr{K}^2 \log \spr{1 + \frac{B^2 T}{d}} + 64 \RMAX \beta^2 B^2 H \log \spr{K}^2 \log \spr{\frac{2 K}{\delta}}^2 \\
        &\phantom{=}+ 4 \RMAX H + 40 H d \RMAX \log \spr{1 + \frac{B^2 T}{d}} \\
        &\phantom{=}+ 4 \sqrt{10 \RMAX^2 H^{5/2} d \log \spr{1 + \frac{B^2 T}{d}} \log \spr{\abs{\cA}} K} \\
        &\phantom{=}+ \frac{8 \beta B}{H} \sqrt{d T \log \spr{1 + \frac{B^2 T}{d}}} + 8 \beta B \log \spr{\frac{2 K}{\delta}}^2\,.
    \end{align*}
    %
    Finally, following Lemma~\ref{lem:good-event-holds} we set $\beta = C H \RMAX d \log \spr{B H \WMAX \RMAX d K \delta^{-1}}$ where $C > 0$ is an absolute constant and we get
    %
    \begin{align*}
        \frac1H \regretK &\leq 128 C^2 \RMAX^3 B^2 d^3 H^2 \log \spr{K}^2 \log \spr{1 + \frac{B^2 T}{d}} \log \spr{B H \WMAX \RMAX d K \delta^{-1}}^2 \\
        &\phantom{=}+ 64 C^2 \RMAX^3 d^2 B^2 H^3 \log \spr{K}^2 \log \spr{\frac{2 K}{\delta}}^2 \log \spr{B H \WMAX \RMAX d K \delta^{-1}}^2 \\
        &\phantom{=}+ 4 \RMAX H + 40 H d \RMAX \log \spr{1 + \frac{B^2 T}{d}} \\
        &\phantom{=}+ 4 \sqrt{10 \RMAX^2 H^{5/2} d \log \spr{1 + \frac{B^2 T}{d}} \log \spr{\abs{\cA}} K} \\
        &\phantom{=}+ 8 C \RMAX B d \sqrt{d T \log \spr{1 + \frac{B^2 T}{d}}} \log \spr{B H \WMAX \RMAX d K \delta^{-1}} \\
        &\phantom{=}+ 8 C \RMAX d B H^2 \log \spr{\frac{2 K}{\delta}}^2 \log \spr{B H \WMAX \RMAX d K \delta^{-1}}\,.
    \end{align*}
    %
    After multiplying by $H$, we get
    %
    \begin{align*}
        \regretK &= \tilde\cO \spr{\sqrt{d^3 H^3 K} + \sqrt{d H^{9/2} K \log \spr{\abs{\cA}}}} \\
        &= \tilde\cO \spr{\sqrt{d^3 H^2 T} + \sqrt{d H^{7/2} T \log \spr{\abs{\cA}}}}\,.
    \end{align*}
\end{proof}
