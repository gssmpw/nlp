\section{Lower bounds for imitation learning}
\label{app:lower}
In this section, we prove lower bounds for both $K$ and $\tau_E$ for all algorithms following Protocol~\ref{prot:interaction} given hereafter.
%
\begin{protocol}[!h]
    \caption{Imitation learning from features alone in Linear MDPs. \label{prot:interaction}}
    \centering
    \begin{algorithmic}[1]
        \STATE The learner adopts a learning algorithm $\mathrm{Alg}$ that receives as input \\
        (1) a features dataset $\cD_\expert = \scbr{\phi_\cost \spr{X^i_E, A^i_E}}_{i=1}^{\tau_E}$ where for any $i \in \sbr{\tau_E}$, $X^i_E, A^i_E \sim \mu \spr{\expert}$, \\
        (2) read access to $\phi_P \spr{x, a}$ for all $x, a \in \cX \times \cA$, \\
        (3) trajectory access to $\cM \setminus \true$, and \\
        (4) the reward class $\cR$ such that $\true \in \cR$.
        
        \STATE $\mathrm{Alg}$ samples $K$ trajectories from $\cM \setminus \true$ and outputs $\pi^{\mathrm{out}}$ s.t. $\bbE \sbr{\inp{\initial, V_{\true}^\expert - V_{\true}^{\pi^{\mathrm{out}}}}} \leq \varepsilon$.
    \end{algorithmic}
\end{protocol}

We prove an $\Omega \spr{\varepsilon^{-2}}$ lower bound for both cases, demonstrating that Algorithm~\ref{alg:fra} is rate optimal. First, we state the lower bound $K$ that holds even with perfect knowledge of the expert feature expectation vector $\lambda \spr{\expert}$, a strictly easier setting compared the one under which \Cref{thm:FraUpper} is proven.
%
\begin{restatable}{theorem}{LowerK} \textbf{(Lower Bound on $K$)} \label{thm:LowerK}
    For any algorithm $\mathrm{Alg}$, there exists an MDP $\cM$ and an expert policy $\expert$ such that $\mathrm{Alg}$, taking as input $ \phim_r\transpose \mu_\cM \spr{\expert}$, requires $K = \Omega \spr{\frac{d}{\spr{1 - \gamma}^2 \varepsilon^2}}$ to guarantee $\bbE_{\mathrm{Alg}} \sbr{\inp{\initial, V_\cM^{\expert} - V_\cM^{\pi^{\mathrm{out}}}}} = \cO \spr{\varepsilon}$.
\end{restatable}

\noindent Next, we establish a lower bound on the required number of expert demonstration $\tau_E$. The result holds even with perfect knowledge of the transition dynamics (\ie for $K = \infty$).
%
\begin{restatable}{theorem}{LowerTauE} \textbf{(Lower Bound on $\tau_E$)} \label{thm:LowerTauE} \label{thm:lower-bound-2states_expert}
    Let $\gamma \geq \frac12$. For any algorithm $\mathrm{Alg}$, there exists an MDP $\cM$ and an expert policy $\expert$ such that $\mathrm{Alg}$ taking as input the transitions dynamics and an expert dataset of size $\tau_E$ requires $\tau_E = \Omega \spr{\frac{\WMAX^2}{\spr{1 - \gamma}^2 \varepsilon^2}}$ to guarantee $\bbE_{\mathrm{Alg}} \sbr{\inp{\initial, V_\cM^{\expert} - V_\cM^{\pi^{\mathrm{out}}}}} = \cO \spr{\varepsilon}$.
\end{restatable}
%
\noindent The proofs are provided in the following sections.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{thm:LowerK} (lower bound on the number of interactions)}

We start with the proof of the lower bound on $K$. We consider a class of possibly randomized algorithms that output a policy $\pi^{\mathrm{out}}$ given a dataset of expert features $\cD_\expert$ and $K$ trajectories collected by the learner in the MDP $\cM$.

\noindent \emph{Proof Idea}. To construct a lower bound, we consider the case of imitation learning from states alone (\ie $\phim_r \spr{x, a} = \bfe_x$), and $\lambda \spr{\expert}$ represents the state expert occupancy measure. We consider the case of a two-state MDP, where $\cX = \scbr{x_0, x_1}$, and the learner knows the \emph{good} state $x_0$ that maximizes the expert's occupancy measure due to having access to $\lambda \spr{\expert}$. The leaner's objective is to maximize the time spent in this good state. All actions in the \emph{bad} state $x_1$ share the same transition kernel. Therefore, the agent's decisions in the state $x_0$ is the only factor that influences the outcome. An action labeled as $a^\star$ is available in the state $x_0$. The transition kernel $P\spr{x_0 \given x_0, a}$ is identical for all actions $a \neq a^\star$, while for $a^\star$, it is defined as $P \spr{x_0 \given x_0, a} + \epsilon$. We then consider a family of $\abs{\cA}$ MDPs, where each MDP assigns the role of $a^\star$ to a different action. We will formally demonstrate that for any algorithm in $\mathrm{Alg}$, there exists at least one MDP within this family where achieving $\bbE_{\mathrm{Alg}} \sbr{\inp{\initial, V_\cM^{\expert} - V_\cM^{\pi^{\mathrm{out}}}}} = \cO \spr{\varepsilon}$ requires $K = \Omega \spr{\frac{\abs{\cA}}{\spr{1 - \gamma}^2 \varepsilon^2}}$. Finally, the bound for an arbitrary dimension $d$ is obtained noticing that this MDP can be written as a linear MDP with features dimension $d = 2 + 2 \abs{\cA}$.

\bigskip
\begin{proof}
    For any policy $\pi$, we denote $\lambda_\cM \spr{\pi} = \phim_r^\trans \mu_\cM \spr{\pi}$ the expected feature vector of the policy $\pi$ in the MDP $\cM$. We consider a deterministic algorithm $\mathrm{Alg}$ that maps $\lambda_\cM \spr{\expert}$ and $K$ environment trajectories to a policy. The extension to randomized algorithms can be done by an application of Fubini's theorem (see \cite{bubeck2012regret}). The hard instance we consider for the lower bound is an MDP $\cM$ with two states, $x_0$ and $x_1$, and $\abs{\cA}$ actions per state. For any action $a$, the reward function is given by $\true \spr{x_0, a} = 1$, and $\true \spr{x_1, a} = 0$. We will refer to state $x_0$ as the ``good'' state and to state $x_1$ as the ``bad'' state.  In state $x_1$, the transition kernel induced by any action $a$ is the same, \ie $P \spr{x_1 \given x_1, a} = 1 - \delta_1$, and $P \spr{x_0 \given x_1, a} = \delta_1$ for some $\delta_1 \in \spr{0, 1}$. Let $\delta_0 \in \spr{0, 1}$ and $\epsilon \in \spr{0, \delta_0}$. In state $x_0$, there is an action $a^\star$ with a slightly different transition kernel
    %
    \begin{align*}
        P \spr{x_1 \given x_0, a^\star} = \delta_0 - \epsilon, \quad P \spr{x_0 \given x_0, a^\star} = 1 - \delta_0 + \epsilon\,,
    \end{align*}
    %
    whereas for any action $a \neq a^\star$, we set
    %
    \begin{align*}
        P \spr{x_1 \given x_0, a} = \delta_0, \quad P \spr{x_0 \given x_0, a} = 1 - \delta_0\,.
    \end{align*}
    %
    We set the unknown expert policy $\expert$ such that it always select action $a^\star$ in both states, \ie $\expert \spr{a^\star \given x_0} = \expert \spr{a^\star \given x_1} = 1$. Setting $\nu_0 \spr{x_0} = 1$, we can write the flow constraints and get
    %
    \begin{align*}
        &\nu \spr{\expert, x_0} = 1 - \gamma + \gamma \spr{1 - \delta_0 + \epsilon} \nu \spr{\expert, x_0} + \gamma \delta_1 \nu \spr{\expert, x_1}\,, \\
        &\nu \spr{\expert, x_1} = \gamma \spr{1 - \delta_1} \nu \spr{\expert, x_1} + \gamma \spr{\delta_0 - \epsilon} \nu \spr{\expert, x_0}\,.
    \end{align*}
    %
    The second equation gives $\nu \spr{\expert, x_1} = \frac{\gamma \spr{\delta_0 - \epsilon}}{1 - \gamma \spr{1 - \delta_1}} \nu \spr{\expert, x_0}$, which we can plug back into the first equation to obtain
    %
    \begin{align*}
        \nu \spr{\expert, x_0} = 1 - \gamma + \spr{\gamma \spr{1 - \delta_0 + \epsilon} + \frac{\gamma^2 \delta_1 \spr{\delta_0 - \epsilon}}{1 - \gamma \spr{1 - \delta_1}}} \nu \spr{\expert, x_0} \,,
    \end{align*}
    %
    which we can rearrange to get
    %
    \begin{align*}
        \nu \spr{\expert, x_0} = \frac{1 - \gamma + \gamma \delta_1}{1 - \gamma + \gamma \delta_1 + \gamma \delta_0 - \gamma \epsilon}\,.
    \end{align*}
    %
    Using the normalization constraint $\nu \spr{\expert, x_0} + \nu \spr{\expert, x_1} = 1$, we also get
    %
    \begin{equation*}
        \nu \spr{\expert, x_1} = \frac{\gamma \delta_0 - \gamma \epsilon}{1 - \gamma + \gamma \delta_1 + \gamma \delta_0 - \gamma \epsilon}\,.
    \end{equation*}
    %
    %Note that the expert state occupancy is the same for any environment $\cM_i$, and that for any $\cM \in \cH$, $\lambda_\cM \spr{\expert} = \nu_\cM \spr{\expert}$.
    Furthermore, let $\pi_{\mathrm{bad}}$ be a ``bad'' policy that always plays an action $a \neq a^\star$. The same calculation with $\epsilon = 0$ shows that the state occupancy measure for the policy $\pi_{\mathrm{bad}}$ is given by
    %
    \begin{align*}
        \nu \spr{\pi_{\mathrm{bad}}, x_0} &= \frac{1 - \gamma + \gamma \delta_1}{1 - \gamma + \gamma \delta_1 + \gamma \delta_0}\,, \\
        \nu \spr{\pi_{\mathrm{bad}}, x_1} &= \frac{\gamma \delta_0}{1 - \gamma + \gamma \delta_1 + \gamma \delta_0}\,.
    \end{align*}
    %
    Let $\tilde{\pi}$ be any policy. Noting that for any $x$, $V^\expert \spr{x} = Q^\expert \spr{x, a^\star}$, we can use the performance difference lemma and get
    %
    \begin{align*}
        \inp{\mu \spr{\expert} - \mu \spr{\tilde{\pi}}, \true} &= \bbE_{\spr{x, a} \sim \mu \spr{\tilde{\pi}}} \sbr{V^\expert \spr{x} - Q^\expert \spr{x, a}} \\
        &= \bbE_{\spr{x, a} \sim \mu \spr{\tilde{\pi}}} \sbr{Q^\expert \spr{x, a^\star} - Q^\expert \spr{x, a}}\,.
    \end{align*}
    %
    All actions share the same transition kernel in $x_1$ thus for any action $a$, $Q^\expert \spr{x_1, a^\star} = Q^\expert \spr{x_1, a}$ and we have
    %
    \begin{align*}
        \inp{\mu \spr{\expert} - \mu \spr{\tilde{\pi}}, \true} &= \nu \spr{\tilde{\pi}, x_0} \sum_{a \in \cA \setminus \scbr{a^\star}} \tilde{\pi} \spr{a \given x_0} \spr{Q^\expert \spr{x_0, a^\star}  - Q^\expert \spr{x_0, a}}\,.
    \end{align*}
    %
    Next, we need to compute the difference of Q-values. Using the Bellman equations for $\expert$ in state $x_0$, we have
    %
    \begin{align}
        \forall a \neq a^\star, &Q^\expert \spr{x_0, a} = 1 + \gamma \delta_0 Q^\expert \spr{x_1, a^\star} + \gamma \spr{1 - \delta_0} Q^\expert \spr{x_0, a^\star} \label{eq:bellman-eq-x0-a} \\
        &Q^\expert \spr{x_0, a^\star} = 1 + \gamma \spr{\delta_0 - \epsilon} Q^\expert \spr{x_1, a^\star} + \gamma \spr{1 - \delta_0 + \epsilon} Q^\expert \spr{x_0, a^\star}\,. \label{eq:bellman-eq-x0-astar}
    \end{align}
    %
    Solving the second equation for $Q^\expert \spr{x_0, a^\star}$ gives
    %
    \begin{align}
        Q^\expert \spr{x_0, a^\star} = \frac{1}{1 - \gamma \spr{1 - \delta_0 + \epsilon}} \spr{1 + \gamma \spr{\delta_0 - \epsilon} Q^\expert \spr{x_1, a^\star}} \label{eq:q-x0-astar-inter}\,.
    \end{align}
    %
    By the Bellman equation in state $x_1$ and action $a^\star$, we further have
    %
    \begin{align*}
        Q^\expert \spr{x_1, a^\star} = 0 + \gamma \delta_1 Q^\expert \spr{x_0, a^\star} + \gamma \spr{1 - \delta_1} Q^\expert \spr{x_1, a^\star}\,,
    \end{align*}
    %
    which implies that
    %
    \begin{align}
        Q^\expert \spr{x_1, a^\star} = \frac{\gamma \delta_1}{1 - \gamma(1 - \delta_1)} Q^\expert \spr{x_0, a^\star}\,. \label{eq:q-x1-astar-inter}
    \end{align}
    %
    Replacing \eqref{eq:q-x1-astar-inter} into \eqref{eq:q-x0-astar-inter}, we get
    %
    \begin{align*}
        Q^\expert \spr{x_0, a^\star} &= \frac{1}{1 - \gamma \spr{1 - \delta_0 + \epsilon}} + \frac{\gamma^2 \delta_1 \spr{\delta_0 - \epsilon}}{\spr{1 - \gamma \spr{1 - \delta_0 + \epsilon}} \spr{1 - \gamma \spr{1 - \delta_1}}} Q^\expert \spr{x_0, a^\star}\,.
    \end{align*}
    %
    Rearranging the terms gives
    %
    \begin{align}
        Q^\expert \spr{x_0, a^\star} &= \spr{1 - \frac{\gamma^2 \delta_1 \spr{\delta_0 - \epsilon}}{\spr{1 - \gamma \spr{1 - \delta_0 + \epsilon}} \spr{1 - \gamma \spr{1 - \delta_1}}}}^{-1} \frac{1}{1 - \gamma \spr{1 - \delta_0 + \epsilon}} \nonumber \\
        &= \frac{1 - \gamma \spr{1 - \delta_1}}{\spr{1 - \gamma \spr{1 - \delta_0 + \epsilon}} \spr{1 - \gamma \spr{1 - \delta_1}} - \gamma^2 \delta_1 \spr{\delta_0 - \epsilon}}\,. \label{eq:q-x0-astar}
    \end{align}
    %
    Plugging Equation~\eqref{eq:q-x0-astar} into Equation~\eqref{eq:q-x1-astar-inter}, we can deduce the value of the expert at $\spr{x_1, a^\star}$
    %
    \begin{equation*}
        Q^\expert \spr{x_1, a^\star} = \frac{\gamma \delta_1}{\spr{1 - \gamma \spr{1 - \delta_0 + \epsilon}} \spr{1 - \gamma \spr{1 - \delta_1}} - \gamma^2 \delta_1 \spr{\delta_0 - \epsilon}}\,.
    \end{equation*}
    %
    Looking at the difference $Q^\expert \spr{x_0, a^\star} - Q^\expert \spr{x_0, a}$, we can take the difference of Equations~\eqref{eq:bellman-eq-x0-astar} and \eqref{eq:bellman-eq-x0-a} to get
    %
    \begin{align*}
        Q^\expert \spr{x_0, a^\star} - Q^\expert \spr{x_0, a} &= \gamma \epsilon \spr{Q^\expert \spr{x_0, a^\star} - Q^\expert \spr{x_1, a^\star}} \\
        &= \frac{\gamma \epsilon \spr{1 - \gamma}}{\underbrace{\spr{1 - \gamma \spr{1 - \delta_0 + \epsilon}} \spr{1 - \gamma \spr{1 - \delta_1}} - \gamma^2 \delta_1 \spr{\delta_0 - \epsilon}}_{\spr{\diamondsuit}}}\,.
    \end{align*}
    %
    Next, we upper bound the denominator as follows
    %
    \begin{align*}
        \spr{\diamondsuit} &= 1 - \gamma \spr{1 - \delta_0 + \epsilon} - \gamma \spr{1 - \delta_1} \\
        &\phantom{=}+ \gamma^2 \spr{1 - \delta_0 + \epsilon - \delta_1 + \delta_0 \delta_1 - \epsilon \delta_1 - \delta_0 \delta_1 + \epsilon \delta_1} \\
        &= 1 - \gamma \spr{1 - \delta_0 + \epsilon} - \gamma \spr{1 - \delta_1} + \gamma^2 \spr{1 - \delta_0 - \delta_1 + \epsilon} \\
        &= 1 - \gamma + \gamma \delta_0 \spr{1 - \gamma} + \gamma \delta_1 \spr{1 - \gamma} - \gamma \spr{1 - \gamma} - \gamma \epsilon \spr{1 - \gamma} \\
        &= \spr{1 - \gamma}^2 + \gamma \delta_0 \spr{1 - \gamma} + \gamma \delta_1 \spr{1 - \gamma} - \gamma \epsilon \spr{1 - \gamma} \\
        &\leq \spr{1- \gamma}^2 + \gamma \delta_0 \spr{1 - \gamma} + \gamma \delta_1 \spr{1 - \gamma}\,,
    \end{align*}
    %
    where the inequality follows from $\gamma \epsilon \spr{1 - \gamma} > 0$. Setting $\delta_1 = \delta_0 = \frac{1 - \gamma}{\gamma}$, we obtain
    %
    \begin{equation*}
        \spr{\diamondsuit} \leq 3 \spr{1 - \gamma}^2\,,
    \end{equation*}
    %
    and it holds that
    %
    \begin{align*}
        Q^\expert \spr{x_0, a^\star} - Q^\expert \spr{x_0, a} \geq \frac{\gamma \epsilon}{3 \spr{1 - \gamma}}\,.
    \end{align*}
    %
    Moreover, the choice of $\delta_0$ and $\delta_1$ implies that $\nu \spr{\pi_{\mathrm{bad}}, x_0} = \frac23$. By definition of the transitions, note that always playing $a \neq a^\star$ like $\pi_{\mathrm{bad}}$ does minimizes the probability of being in state $x_0$. Thus, for any policy $\tilde{\pi}$, $\nu \spr{\tilde{\pi}, x_0} \geq \nu \spr{\pi_{\mathrm{bad}}, x_0}$, and we have
    %
    \begin{align*}
        \inp{\mu \spr{\expert} - \mu \spr{\tilde{\pi}}, \true} &\geq \nu \spr{\tilde{\pi}, x_0} \sum_{a \in \cA \setminus \scbr{a^\star}} \tilde{\pi} \spr{a \given x_0} \frac{\gamma \epsilon}{3 \spr{1 - \gamma}} \\
        &\geq \nu \spr{\pi_{\mathrm{bad}}, x_0} \spr{1 - \tilde{\pi} \spr{a^\star \given x_0}} \frac{\gamma \epsilon}{3 \spr{1 - \gamma}} \\
        &= 2 \spr{1 - \tilde{\pi} \spr{a^\star \given x_0}} \frac{\gamma \epsilon}{9(1 - \gamma)} \\
        &\geq \frac{\spr{1 - \tilde{\pi} \spr{a^\star \given x_0}} \epsilon}{9 \spr{1 - \gamma}}\,.
    \end{align*}
    %
    where the last inequality follows from $\gamma \geq 1/2$. We now consider the policy $\tilde{\pi} = \bar\pi$ produced by a learning algorithm $\mathrm{Alg}$ interacting with the MDP described above (with $\epsilon > 0$). We also consider  $\underline{\pi}$ the output of the same learning algorithm $\mathrm{Alg}$ when interacting with the MDP $\underline{\cM}$, a copy of $\cM$ with $\epsilon = 0$ (note that in $\underline{\cM}$, all actions are identical in \emph{both} states $x_0$ and $x_1$, so there is nothing to learn). In $\cM$, all actions are identical in state $x_1$, thus we can assume both policies are the same in state $x_1$, \ie $\bar\pi \spr{\cdot \given x_1} = \underline{\pi} \spr{\cdot \given x_1} = \bfe_{a^\star}$, and focus exclusively on learning in state $x_0$. By Pinkser's inequality, we have that
    %
    \begin{align*}
        \bar\pi \spr{a^\star \given x_0} - \underline{\pi} \spr{a^\star \given x_0} \leq \sqrt{2 \KL \spr{\underline{\pi} \spr{\cdot \given x_0} \| \bar\pi \spr{\cdot \given x_0}}}\,,
    \end{align*}
    %
    and the previous inequality becomes
    %
    \begin{align*}
        \inp{\mu \spr{\expert} -  \mu \spr{\bar\pi}, \true} \geq \frac{\epsilon}{9 \spr{1 - \gamma}} \spr{1 - \underline{\pi} \spr{a^\star \given x_0} - \sqrt{2 \KL \spr{\underline{\pi} \spr{\cdot \given x_0} \| \bar\pi \spr{\cdot \given x_0}}}}\,.
    \end{align*}
    %
    Denote $A = \abs{\cA}$ and let $\cH = \scbr{\cM_i}_{i=1}^A$ be a collection of MDPs instances where for any $i = 1, \dots, A$, the MDP $\cM_i$ is a copy of $\cM$ where the $i$th action is equal to $a^\star$, \ie $a_i = a^\star$. We denote $P_i$ the corresponding transitions. For any $i \in [\![1, A]\!]$, we denote $\bar\pi^i$ the policy output by the learning algorithm $\mathrm{Alg}$ after interacting with the instance $\cM_i$, and $\experti$ be the expert policy for the instance $\cM_i$, \ie the policy that always plays $a_i$. We denote $\mu_i \spr{\pi}$ the occupancy measure of any policy $\pi$ in the MDP $\cM_i$. Then, notice that the previous derivations apply for any MDP in $\cH$. Thus, summing over $i \in [\![1, A]\!]$ and noting that $\underline{\pi} \spr{\cdot \given x_0}$ is a probability distribution over $\cA$, we get
    %
    \begin{align}
        \sum_{i = 1}^A \inp{\mu_i \spr{\experti} - \mu_i \spr{\bar\pi^i}, \true} \geq \frac{\epsilon}{9 \spr{1 - \gamma}} \spr{A - 1 - \sum_{i = 1}^A \sqrt{2 \KL \spr{\underline{\pi} \spr{\cdot \given x_0} \| \bar\pi^i \spr{\cdot \given x_0}}}}\,. \label{eq:sum_lower_bound}
    \end{align}
    %
    For any $i \in \sbr{A}$ and $T \in \bbN^\star$, denote $\bbP_i^T$ the probability distribution over sets $\cD_i^T = \scbr{x_0, A_t^i, X_t^i}_{t \in \sbr{T}}$ of $T$ transitions starting from $x_0$ induced by the interaction between the algorithm $\mathrm{Alg}$ and the MDP $\cM_i$. Likewise, we denote $\underline{\bbP}^T$ the probability distribution corresponding to $\underline{\cM}$. Then, by the data processing inequality for the KL divergence, for any $i \in \sbr{A}$, it holds that
    %
    \begin{equation*}
        \KL \spr{\underline{\pi} \spr{\cdot \given x_0} \| \bar\pi^i \spr{\cdot \given x_0}} \leq \KL \spr{\underline{\bbP}^T \| \bbP_i^T}\,.
    \end{equation*}
    %
    Denoting $\underline{\bbE}$ the expectation with respect to $\underline{\bbP}^T$, we can use the Markov property of the environment and continue as follows
    %
    \begin{align*}
        \KL \spr{\underline{\bbP}^T \| \bbP_i^T} &= \underline{\bbE} \sbr{\log \spr{\frac{\prod_{t=1}^T \underline{P} \spr{\underline{X}_t \given x_0, \underline{A}_t} \underline{\bbP}^T \spr{\underline{A}_t \given \underline{X}_1, \underline{A}_1, \dots, \underline{X}_{t-1}}}{\prod_{t=1}^T P_i \spr{\underline{X}_t \given x_0, \underline{A}_t} \bbP_i^T \spr{\underline{A}_t \given \underline{X}_1, \underline{A}_1, \dots, \underline{X}_{t-1}}}}} \\
        &= \underline{\bbE} \sbr{\log \spr{\frac{\prod_{t=1}^T \underline{P} \spr{\underline{X}_t \given x_0, \underline{A}_t}}{\prod_{t=1}^T P_i \spr{\underline{X}_t \given x_0, \underline{A}_t}}}} \\
        &= \underline{\bbE} \sbr{\sumtT \log \spr{\frac{\underline{P} \spr{\underline{X}_t \given x_0, \underline{A}_t}}{P_i \spr{\underline{X}_t \given x_0, \underline{A}_t}}}}\,,
    \end{align*}
    %
    where the probabilities on the actions are equal due to running the same algorithm $\mathrm{Alg}$ with the same history up to time $t-1$. Next, we have
    %
    \begin{align*}
        \KL \spr{\underline{\bbP}^T \| \bbP_i^T} &= \sumtT \sum_{\spr{x, a} \in \cX \times \cA} \underline{\bbP}^T \sbr{\spr{\underline{X}_t, \underline{A}_t} = \spr{x, a}} \log \spr{\frac{\underline{P} \spr{x \given x_0, a}}{P_i \spr{x \given x_0, a}}} \\
        &= \sumtT \sum_{x \in \cX} \underline{\bbP}^T \sbr{\spr{\underline{X}_t, \underline{A}_t} = \spr{x, a_i}} \log \spr{\frac{\underline{P} \spr{x \given x_0, a_i}}{P_i \spr{x \given x_0, a_i}}}\,,
    \end{align*}
    %
    where we used that the transitions $\underline{P}$ and $P_i$ are the same for any action $a \neq a_i$. By definition of the transitions, we further have
    %
    \begin{align*}
        \KL \spr{\underline{\bbP}^T \| \bbP_i^T} &= \sumtT \underline{\bbP}^T \sbr{\spr{\underline{X}_t, \underline{A}_t} = \spr{x_0, a_i}}  \log \spr{\frac{1 - \delta_0}{1 - \delta_0 + \epsilon}} \\
        &\phantom{=}+ \sumtT \underline{\bbP}^T \sbr{\spr{\underline{X}_t, \underline{A}_t} = \spr{x_1, a_i}} \log \spr{\frac{\delta_0}{\delta_0 - \epsilon}}\,.
    \end{align*}
    %
    Next, by definition of $\underline{\bbP}^T$, we have
    %
    \begin{align*}
        \KL \spr{\underline{\bbP}^T \| \bbP_i^T} &= \sumtT \underline{\bbP}^T \sbr{\underline{A}_t = a_i} \underline{P} \spr{x_0 \given x_0, a_i} \log \spr{\frac{1 - \delta_0}{1 - \delta_0 + \epsilon}} \\
        &\phantom{=}+ \sumtT \underline{\bbP}^T \sbr{\underline{A}_t = a_i} \underline{P} \spr{x_1 \given x_0, a_i} \log \spr{\frac{\delta_0}{\delta_0 - \epsilon}} \\
        &= \underline{\bbE} \sbr{\sumtT \mathds{1} \scbr{\underline{A}_t = a_i}} \spr{\spr{1 - \delta_0} \log \spr{\frac{1 - \delta_0}{1 - \delta_0 + \epsilon}} + \delta_0 \log \spr{\frac{\delta_0}{\delta_0 - \epsilon}}}\,.
    \end{align*}
    %
    By \citealp[Lemma 20]{AJO08}, we can bound the KL divergence as follows
    %
    \begin{align*}
        \KL \spr{\underline{\bbP}^T \| \bbP_i^T} &\leq \frac{\epsilon^2}{\delta_0 \log(2)} \underline{\bbE} \sbr{\sumtT \mathds{1} \scbr{\underline{A}_t = a_i}} \\
        &\leq \frac{\epsilon^2}{\spr{1 - \gamma} \log \spr{2}} \underline{\bbE} \sbr{\sumtT \mathds{1} \scbr{\underline{A}_t = a_i}}\,,
    \end{align*}
    %
    where the last inequality is due to the choice of $\delta_0 = \frac{1 - \gamma}{\gamma}$ and $\gamma < 1$. Plugging this into Equation~\eqref{eq:sum_lower_bound} and dividing by $A$, we have
    %
    \begin{align*}
        \frac1A \sum_{i = 1}^A \inp{\mu_i \spr{\experti} - \mu_i \spr{\bar\pi^i}, \true} &\geq \frac{\epsilon}{9 \spr{1 - \gamma}} \spr{1 - \frac1A - \frac{\epsilon}{A} \sum_{i=1}^A \sqrt{\frac{\underline{\bbE} \sbr{\sumtT \mathds{1} \scbr{\underline{A}_t = a_i}}}{\spr{1 - \gamma} \log \spr{2}}}}\,.
    \end{align*}
    %
    By Jensen's inequality, we further get
    %
    \begin{align*}
        \frac1A \sum_{i = 1}^A \inp{\mu_i \spr{\experti} - \mu_i \spr{\bar\pi^i}, \true} &\geq \frac{\epsilon}{9 \spr{1 - \gamma}} \spr{1 - \frac1A - \epsilon \sqrt{\frac{\underline{\bbE} \sbr{\sum_{i=1}^A \sumtT \mathds{1} \scbr{\underline{A}_t = a_i}}}{A \spr{1 - \gamma} \log \spr{2}}}} \\
        &\geq \frac{1}{9 \spr{1 - \gamma}} \spr{\frac{\epsilon}{2} - \epsilon^2 \sqrt{\frac{T}{A \spr{1 - \gamma} \log \spr{2}}}}\,,
    \end{align*}
    %
    where the second inequality follows from $\sum_{i=1}^A \mathds{1} \scbr{\underline{A}_t = a_i} = 1$ almost surely for any $t$ and $1 - \frac1A \geq \frac12$. Note that the value of $\epsilon$ maximizing the lower bound is given by $\epsilon^\star = \frac14 \sqrt{\frac{A \spr{1 - \gamma} \log \spr{2}}{T}}$. To satisfy the constraint $\epsilon^\star \in \spr{0, \delta_0}$ with $\delta_0 = \frac{1 - \gamma}{\gamma}$, assume we have $T \geq \frac{\gamma^2 A \log \spr{2}}{16 \spr{1 - \gamma}}$. We plug the value of $\epsilon^\star$ in the previous inequality to get
    %
    \begin{align*}
        \frac1A \sum_{i = 1}^A \inp{\mu_i \spr{\experti} - \mu_i \spr{\bar\pi^i}, \true} &\geq \frac{1}{16 \cdot 9 \spr{1 - \gamma}} \sqrt{\frac{A \spr{1 - \gamma} \log \spr{2}}{T}} \\
        &= \frac{1}{144} \sqrt{\frac{A \log \spr{2}}{\spr{1 - \gamma} T}}\,,
    \end{align*}
    %
    The average can be upper bounded by the maximum, thus
    %
    \begin{align*}
        \max_{i = 1, \dots, A} \inp{\nu_0, V_{\cM_i}^{\experti} - V_{\cM_i}^{\bar\pi^i}} &= \frac{1}{1 - \gamma} \max_{i = 1, \dots, A} \inp{\mu_i \spr{\experti} - \mu_i \spr{\bar\pi^i}, \true} \\
        &\geq \frac{1}{144} \sqrt{\frac{A \log \spr{2}}{\spr{1 - \gamma}^3 T}}\,.
    \end{align*}
    %
    What remains is to set the number of samples $T$ to make the lower bound small enough to make $\max_{i = 1, \dots, A} \inp{\nu_0, V_{\cM_i}^{\experti} - V_{\cM_i}^{\bar\pi^i}} = \cO \spr{\varepsilon}$ possible, \ie we need to have $T = \Omega \spr{\frac{A}{\spr{1 - \gamma}^3 \varepsilon^2}}$ samples. Therefore, we need $T = \Omega \spr{\frac{A}{\spr{1 - \gamma}^3 \varepsilon^2}}$ samples to learn a $\cO \spr{\varepsilon}$-suboptimal policy in the MDP that achieves the maximum. In order to derive a lower bound on the episodes number $K$ we can divide the sample complexity lower bound for $T$ by the the expected number of transitions per episode which is $\spr{1-\gamma}^{-1}$. This gives $K = \Omega \spr{\frac{A}{\spr{1 - \gamma}^2 \varepsilon^2}}$. We can conclude by noting that our construction used in the lower bound is a linear MDP with dimensionality $d = 2 + 2 \abs{\cA}$, thus we have $K = \Omega \spr{\frac{d}{\spr{1 - \gamma}^2 \varepsilon^2}}$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of \Cref{thm:LowerTauE} (lower bound on the number of expert transitions)}
\label{app:lower_tau_E}

\emph{Proof Idea:} The construction of the lower bound consists in relating the problem to that of distinguishing two Bernoullis distributions with close means. For that, we consider two MDPs $\cM_0$ and $\cM_1$ that only differ in their reward function. They have two states $\cX = \scbr{x_0, x_1}$ and $\abs{\cA}$ actions available at each state. The initial distribution $\initial$ is chosen to be the uniform distribution over $\cX$. In state $x_1$, any action $a$ induces the same transition kernel: $P \spr{x_0 \given x_1, a} = \delta$. In state $x_0$, any action $a$ except some action $a^\star$ is such that $P \spr{x_1 | x_0, a} = \delta$. However, the special action $a^\star$ allows to stay in the state $x_0$ with a slightly higher probability, \ie $P \spr{x_1 \given x_0, a^\star} = \delta - \epsilon$. Then, the reward function in $\cM_0$ is defined as $\true^0 \spr{x_0, \cdot} = \WMAX$, and $\true^0 \spr{x_0, \cdot} = 0$, while in $\cM_1$, it is defined as $\true^1 \spr{x_0, \cdot} = 0$, $\true^1 \spr{x_1, \cdot} = \WMAX$. Finally, we define an expert $\pi_E^0$ for $\cM_0$ as the policy that always play the action $a^\star$, and an expert $\pi_E^1$ for $\cM_1$ that always play some action $a \neq a^\star$. We then show that the expert occupancy measures satisfy $\nu \spr{\pi_E^0, x_0} = 1/2 + \Delta$, for some small $\Delta > 0$, while $\nu \spr{\pi_E^1, x_0} = \frac12$. The remaining step is to reduce this problem to a lower bound on the regret of a two-arm Bernoulli bandits instance with means $\spr{1/2, 1/2}$ and $\spr{1/2 + \Delta, 1/2 - \Delta}$. The proof is formally presented hereafter.

\LowerTauE*

\begin{proof}
    As mentioned earlier, it is sufficient to consider deterministic algorithms that map histories to policies. The lower bound for randomized algorithms follows by an application of Fubini's theorem (see \citealp{bubeck2012regret}). We consider two MDPs $\cH = \scbr{\cM_0, \cM_1}$ with the same state space $\cX = \scbr{x_0, x_1}$ and $\abs{\cA}$ actions available in each state. The initial distribution $\initial$ is chosen to be the uniform distribution over $\cX$, \ie $\initial \spr{x_0} = \initial \spr{x_1} = \frac12$. The transitions are the same in both MDPs: in state $x_1$, each action $a \in \cA$ induces the following transition kernel
    %
    \begin{equation*}
        P \spr{x_0 \given x_1, a} = \delta, \quad P \spr{x_1 \given x_1, a} = 1 - \delta\,
    \end{equation*}
    %
    while in state $x_0$, there is an action $a^\star$ giving a slightly higher probability on staying in state $x_0$, \ie
    %
    \begin{align*}
        &P \spr{x_0 \given x_0, a^\star} = 1 - \delta + \epsilon, \quad P \spr{x_1 \given x_0, a^\star} = \delta - \epsilon \\
        \forall a \neq a^\star, &P \spr{x_0 \given x_0, a} = 1 - \delta, \quad P \spr{x_1 \given x_0, a} = \delta\,.
    \end{align*}
    %
    The reward functions, $\true^0$ and $\true^1$, are different. In $\cM_0$, the ``good'' state is $x_0$, \ie for any action $a \in \cA$, we set $\true^0 \spr{x_0, a} = \WMAX$, $\true^0 \spr{x_1, a} = 0$, and in $\cM_1$, the ``good'' state is $x_1$, \ie $\true^1 \spr{x_0, a} = 0$, and $\true^1 \spr{x_1, a} = \WMAX$. Note that $\WMAX = \RMAX$ due to using the features $\phi_r \spr{x, a} = \bfe_x$ for any state-action pair $x, a$.
     
    Then, we define one expert policy for each MDP. In $\cM_0$, the expert $\pi_E^0$ is the policy that always plays $a^\star$ and in $\cM_1$, the expert $\pi_E^1$ is the policy that always plays an action $a \neq a^\star$. Therefore, the state occupancy measure of expert $\pi_E^0$ in MDP $\cM_0$ has the highest mass in state $x_0$, while $\pi_E^1$ put equal mass on both states. Indeed, writing the flow constraints for both experts, we have
    %
    \begin{align*}
        \begin{pmatrix}
            1 - \gamma + \gamma \delta - \gamma \epsilon & - \gamma \delta \\
            - \gamma \spr{\delta - \epsilon} & 1 - \gamma + \gamma \delta
        \end{pmatrix}
        \nu \spr{\pi_E^0} &= \nu_0\,, \\
        \begin{pmatrix}
            1 - \gamma + \gamma \delta & - \gamma \delta \\
            - \gamma \delta & 1 - \gamma + \gamma \delta
        \end{pmatrix}
        \nu \spr{\pi_E^1} &= \nu_0\,.
    \end{align*}
    %
    Solving these linear systems using, \eg, Cramer's rule, we obtain
    %
    \begin{align*}
        \nu \spr{\pi_E^0, x_0} &= \frac{1 - \gamma + 2 \gamma \delta}{2 \spr{1 - \gamma - \gamma \epsilon + 2 \gamma \delta}}, &\nu \spr{\pi_E^0, x_1} &= \frac{1 - \gamma - 2 \gamma \epsilon + 2 \gamma \delta}{2 \spr{1 - \gamma - \gamma \epsilon + 2 \gamma \delta}}\,, \\
        \nu \spr{\pi_E^1, x_0} &= \frac12, &\nu \spr{\pi_E^1, x_1} &= \frac12\,.
    \end{align*}
    %
    For $i \in \scbr{0, 1}$, let $\bar\pi^i$ be the policy output by $\mathrm{Alg}$ when given a dataset $\cD_{\pi_E^i}$ as input and let $V_i^{\bar\pi^i}$ be the value function of policy $\bar\pi^i$ corresponding to the reward function $\true^i$ from the MDP $\cM_i$. By definition of $\true^i$, we can write
    %
    \begin{align}
        \frac12 \sum_{i \in \scbr{0, 1}} \inp{\initial, V_i^{\pi_E^i} - V_i^{\bar\pi^i}} &= \frac{1}{2 \spr{1 - \gamma}} \sum_{i \in \scbr{1, 2}} \inp{\mu \spr{\pi_E^i} - \mu \spr{\bar\pi^i}, \true^i} \nonumber \\
        &= \frac{\WMAX}{2 \spr{1 - \gamma}} \spr{\nu \spr{\pi_E^0, x_0} - \nu \spr{\bar\pi^0, x_0} + \nu \spr{\pi_E^1, x_1} - \nu \spr{\bar\pi^1, x_1}}\,. \label{eq:appE2-average-return}
    \end{align}
    %
    Thus, we need to compute the difference between state occupancy measures. Let $\tilde\pi$ be an arbitrary policy and denote $\alpha \in \sbr{0, 1}$ the probability of playing action $a^\star$ in state $x_0$, \ie $\tilde\pi \spr{a^\star \given x_0} = \alpha$. Writing down the flow constraints again, we can show that
    %
    \begin{equation*}
        \nu \spr{\tilde\pi, x_0} = \frac{1 - \gamma + 2 \gamma \delta}{2 \spr{1 - \gamma - \gamma \alpha \epsilon + 2 \gamma \delta}}, \quad \nu \spr{\tilde\pi, x_1} = \frac{1 - \gamma - 2 \gamma \alpha \epsilon + 2 \gamma \delta}{2 \spr{1 - \gamma - \gamma \alpha \epsilon + 2 \gamma \delta}}\,.
    \end{equation*}
    %
    Looking at the difference with $\pi_E^0$ in state $x_0$, we have
    %
    \begin{align*}
        \nu \spr{\pi_E^0, x_0} - \nu \spr{\tilde\pi, x_0} &= \frac{1 - \gamma + 2 \gamma \delta}{2 \spr{1 - \gamma - \gamma \epsilon + 2 \gamma \delta}} - \frac{1 - \gamma + 2 \gamma \delta}{2 \spr{1 - \gamma - \gamma \alpha \epsilon + 2 \gamma \delta}} \\
        &= \frac{\spr{1 - \gamma + 2 \gamma \delta} \spr{\spr{- \gamma \alpha \epsilon} - \spr{- \gamma \epsilon}}}{2 \spr{1 - \gamma - \gamma \epsilon + 2 \gamma \delta} \spr{1 - \gamma - \gamma \alpha \epsilon + 2 \gamma \delta}} \\
        &= \frac{\spr{1 - \gamma + 2 \gamma \delta} \gamma \epsilon \spr{1 - \alpha}}{2 \spr{1 - \gamma - \gamma \epsilon + 2 \gamma \delta} \spr{1 - \gamma - \gamma \alpha \epsilon + 2 \gamma \delta}}\,.
    \end{align*}
    %
    Setting $\delta = \frac{1 - \gamma}{\gamma}$ and noting $\epsilon \geq 0$, $\gamma \geq \frac12$, we can lower bound the difference as follows
    %
    \begin{align}
        \nu \spr{\pi_E^0, x_0} - \nu \spr{\tilde\pi, x_0} &= \frac{3 \spr{1 - \gamma} \gamma \epsilon \spr{1 - \alpha}}{2 \spr{3 \spr{1 - \gamma} - \gamma \epsilon} \spr{3 \spr{1 - \gamma} - \gamma \alpha \epsilon}} \nonumber \\
        &\geq \frac{\epsilon \spr{1 - \alpha}}{12 \spr{1 - \gamma}}\,. \label{eq:appE2-diffx0}
    \end{align}
    %
    Likewise, the difference between $\nu \spr{\pi_E^1}$ and $\nu \spr{\tilde\pi}$ in state $x_1$ is given by
    %
    \begin{align*}
        \nu \spr{\pi_E^1, x_1} - \nu \spr{\tilde\pi, x_1} &= \frac12 - \frac{1 - \gamma - 2 \gamma \alpha \epsilon + 2 \gamma \delta}{2 \spr{1 - \gamma - \gamma \alpha \epsilon + 2 \gamma \delta}} \\
        &= \frac{\gamma \alpha \epsilon}{2 \spr{1 - \gamma - \gamma \alpha \epsilon + 2 \gamma \delta}}\,.
    \end{align*}
    %
    Using the definition of $\delta$, and again $\epsilon \geq 0$, $\gamma \geq \frac12$, we get
    %
    \begin{align}
        \nu \spr{\pi_E^1, x_1} - \nu \spr{\tilde\pi, x_1} &= \frac{\gamma \alpha \epsilon}{2 \spr{3 \spr{1 - \gamma} - \gamma \alpha \epsilon}} \nonumber \\
        &\geq \frac{\epsilon \alpha}{12 \spr{1 - \gamma}}\,. \label{eq:appE2-diffx1}
    \end{align}
    %
    Plugging Inequalities~\eqref{eq:appE2-diffx0} and~\eqref{eq:appE2-diffx1} into Equation~\eqref{eq:appE2-average-return} with $\alpha = \bar\pi^0 \spr{a^\star \given x_0}$ and $\alpha = \bar\pi^1 \spr{a^\star \given x_0}$ respectively, we get
    %
    \begin{align*}
        \frac12 \sum_{i \in \scbr{0, 1}} \inp{\initial, V_i^{\pi_E^i} - V_i^{\bar\pi^i}} &\geq \frac{\epsilon \WMAX}{24 \spr{1 - \gamma}^2}\spr{1 - \bar\pi^0 \spr{a^\star \given x_0} + \bar\pi^1 \spr{a^\star \given x_0}} \\
        &= \frac{\epsilon \WMAX}{24 \spr{1 - \gamma}^2} \spr{\sum_{a \neq a^\star} \bar\pi^0 \spr{a \given x_0} + \bar\pi^1 \spr{a^\star \given x_0}}\,.
    \end{align*}
    %
    Next, we can lower bound the right hand side using the Bretagnolle-Huber inequality (see \citealp{bretagnolle1979estimation}, and \citealp[Theorem 14.2]{lattimore2020bandit}), which gives
    %
    \begin{equation} \label{eq:partial_lower_bound}
        \frac12 \sum_{i \in \scbr{0, 1}} \inp{\initial, V_i^{\pi_E^i} - V_i^{\bar\pi^i}} \geq \frac{\epsilon \WMAX}{24 \spr{1 - \gamma}^2} \exp \spr{- \KL \spr{\bar\pi^0 \spr{\cdot \given x_0} \| \bar\pi^1 \spr{\cdot \given x_0}}}\,.
    \end{equation}
    %
    Then, using the data processing inequality and using the fact that the learning algorithm produces $\bar\pi^i$ as a deterministic function of the dataset $\cD_{\pi_E^i}$ for $i = 0, 1$, we have that
    %
    \begin{equation*}
        \KL \spr{\bar\pi^0 \spr{\cdot \given x_0} \| \bar\pi^1 \spr{\cdot \given x_0}} \leq \KL \spr{\bbP_0^{\tau_E} \| \bbP_1^{\tau_E}}\,,
    \end{equation*}
    %
    where, for $i \in \scbr{0, 1}$, we denoted $\bbP_i^{\tau_E}$ the probability distribution over datasets of size $\tau_E$ induced by the interaction between the expert $\pi_E^i$ and the environment (analog to what is done in the proof of Theorem~\ref{thm:LowerK}). Next, we denote $\kl \spr{p, q}$ and $\chi^2 \spr{p, q}$ the KL and chi-squared divergences between bernoulli distributions of means $p$ and $p'$, \ie
    %
    \begin{align*}
        \kl \spr{p, q} &= p \log \spr{\frac{p}{q}} + \spr{1 - p} \log \spr{\frac{1 - p}{1 - q}} \\
        \chi^2 \spr{p, q} &= \frac{\spr{p - q}^2}{q \spr{1 - q}}\,.
    \end{align*}
    %
    By definition of the KL, we have
    %
    \begin{align*}
        \KL \spr{\bbP_0^{\tau_E} \| \bbP_1^{\tau_E}} &= \tau_E \cdot \kl \spr{\frac{3 \spr{1 - \gamma}}{2 \spr{3 \spr{1 - \gamma} - \gamma \epsilon}}, \frac12} \\
        &\leq \tau_E \cdot \chi^2 \spr{\frac{3 \spr{1 - \gamma}}{2 \spr{3 \spr{1 - \gamma} - \gamma \epsilon}}, \frac12} \\
        &= \tau_E \cdot \chi^2 \spr{\frac12 + \frac{\gamma \epsilon}{3 \spr{1 - \gamma} - \gamma \epsilon}, \frac12} \\
        &= \frac{4 \tau_E \gamma^2 \epsilon^2}{\spr{3 \spr{1 - \gamma} - \gamma \epsilon}^2} \\
        &\leq \frac{\tau_E \gamma^2 \epsilon^2}{\spr{1 - \gamma}^2}\,,
    \end{align*}
    %
    where the first inequality follows from the concavity of the logarithm function, and the second inequality uses the fact that $\epsilon \leq \delta = \frac{1-\gamma}{\gamma}$. Thus, plugging in this last inequality into Equation~\eqref{eq:partial_lower_bound}, we obtain
    %
    \begin{align*}
        \frac12 \sum_{i \in \scbr{0, 1}} \inp{\initial, V_i^{\pi_E^i} - V_i^{\bar\pi^i}} &\geq \frac{\epsilon \WMAX}{24 \spr{1 - \gamma}^2} \exp \spr{- \frac{\tau_E \gamma^2 \epsilon^2}{\spr{1 - \gamma}^2}} \\
        &\geq \frac{\epsilon \WMAX}{24 \spr{1 - \gamma}^2} \exp \spr{- \frac{\tau_E \epsilon^2}{\spr{1 - \gamma}^2}}\,,
    \end{align*}
    %
    where we used $\gamma < 1$ in the second inequality. Introducing $\epsilon' = \epsilon \spr{1 - \gamma}^{-1}$, we can rewrite the previous inequality as
    %
    \begin{equation*}
        \frac12 \sum_{i \in \scbr{0, 1}} \inp{\initial, V_i^{\pi_E^i} - V_i^{\bar\pi^i}} \geq \frac{\WMAX \epsilon'}{24 \spr{1 - \gamma}} \exp \spr{- \tau_E \spr{\epsilon'}^2}\,.
    \end{equation*}
    %
    It remains to make the lower bound small enough. To bound the average suboptimality gap by $\frac{\WMAX \epsilon'}{24 e \spr{1 - \gamma}}$ and have $\frac12 \sum_{i \in \scbr{0, 1}} \inp{\initial, V_i^{\pi_E^i} - V_i^{\bar\pi^i}} \leq \frac{\WMAX \epsilon'}{24 e \spr{1 - \gamma}}$, we need at least $\tau_E \geq \frac{1}{\spr{\epsilon'}^2}$ expert transitions. Therefore, to achieve
    %
    \begin{equation*}
        \frac12 \sum_{i \in \scbr{0, 1}} \inp{\initial, V_i^{\pi_E^i} - V_i^{\bar\pi^i}} \leq \varepsilon\,,
    \end{equation*}
    %
    for some $\varepsilon > 0$, we need to choose $\epsilon' = \nicefrac{24 e \spr{1 - \gamma} \varepsilon}{\WMAX}$, which means that every algorithm needs at least $\tau_E \geq \frac{\WMAX^2}{24^2 e^2 \spr{1 - \gamma}^2 \varepsilon^2}$ to guarantee a suboptimality gap of order $\varepsilon$.
\end{proof}
