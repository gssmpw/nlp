\section{Introduction}

Since the breakthrough work of \citet{jin2019provably}, the class of linear Markov decision processes (MDPs) has become a standard model for theoretical analysis of reinforcement learning (RL) algorithms under linear function approximation. This work demonstrated the possibility of constructing computationally and statistically efficient methods for large-scale RL, and pioneered an analysis technique that influenced the entire field of RL theory. Hundreds of follow-up papers have studied variations of this model, studying extensions such as learning with adversarial rewards \citep{Neu:2021,zhong2024theoretical,sherman2023,dai2023, sherman2023rate,cassel2024warmupfree,liu2023towards}, without rewards \citep{Wang:2020b,wagenmaker2022reward,hu2022towards}, or with unknown features \cite{agarwal2020flambe,uehara2021representation,zhang2022efficient,mhammedi2024efficient,modi2024model}. The linearity constraint itself has been relaxed in a variety of ways \cite{zanette2020learning,Cai:2020,du2021bilinear,weisz2024online,golowich2024linear,wu2024computationally}. However, practically all of these developments retained one major limitation of the original work of \citet{jin2019provably}: it only applies to finite-horizon MDPs. Generalizations to the more challenging (and practically much more popular) infinite-horizon MDP models have so far remained very limited, yielding only highly impractical methods or suboptimal performance guarantees \citep{WJLJ20}. In this paper, we propose an efficient algorithm that successfully addresses this long-standing open problem.

We consider the problem of learning a nearly optimal policy in $\gamma$-discounted MDPs \citep{Puterman:1994}, under the linear MDP assumption first proposed by \citet{jin2019provably} (see also \citealp{Yang:2019}). We consider an interaction protocol where a learning agent interacts with the environment in a sequence of $K$ episodes of geometrically distributed length, and aims to pick a sequence of policies such that its regret against the best fixed policy is as small as possible. Our algorithm achieves a regret bound of order $H\sqrt{d^3T} + H^{7/4} \sqrt{d T \log |\cA|}$, where $d$ is the feature dimensionality, $H = \frac{1}{1-\gamma}$ is the effective horizon, $\cA$ is the action space, and $T$ is the number of interactions. This implies a bound on the sample complexity of learning an $\varepsilon$-optimal policy of the order $\frac{H^3 d^3 + H^{7/2} d \log \abs{\cA}}{\varepsilon^2}$. The algorithm returns a single softmax policy that is fully described in terms of a $d$-dimensional parameter vector and a $d^2$-dimensional feature-covariance matrix. This constitutes the first sample-complexity result of the optimal order $1/\varepsilon^2$ achieved by a computationally efficient algorithm. The regret guarantees are also shown to hold if the reward function changes adversarially over time, and we additionally provide an extension of our method for the setting of imitation learning.

On the technical side, our main contribution is the development of a new optimistic exploration mechanism that combines two classic ideas from two different eras of RL theory. First, following the recipe of \citet{jin2019provably}, we make use of additive UCB-style exploration bonuses which have been successfuly used for several decades in both bandit problems \citep{LR85,ACF02,A02,dani08stoch,APS11} and reinforcement learning \citep{kaelbling1996reinforcement,Strehl:2008,jaksch10ucrl,azar2017minimax}. Second (and more importantly), we adapt another classic (but apparently recently less well-known) idea underlying the \RMAXalg algorithm of \citet{brafman2002r} (see also \citealp{szita2010model} and Chapter~8 in \citealp{kakade2003sample}). Roughly speaking, this technique amounts to replacing the standard empirical model estimate with a fixed optimistic estimate in state-action pairs that are very poorly explored. This addresses the notorious problem of empirical estimates in linear MDPs that they tend to have extremely high variance in under-explored states, which can only be offset with very large additive exploration bonuses. Our \RMAXalg-style scheme counteracts these large bonuses by effectively swapping out the possibly over-optimistic estimates obtained via additive bonuses with more reasonably sized estimates. Besides \citet{brafman2002r}, our algorithm design and analysis is also strongly inspired by the recent work of \citet{cassel2024warmupfree} who proposed a slightly limited variant of the same exploration mechanism for finite-horizon MDPs.
