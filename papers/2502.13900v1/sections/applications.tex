
\section{Application: Imitation Learning from features alone}
\label{sec:application}

In this section, we show an application of the results presented in Section~\ref{sec:analysis}, making crucial use of 
the fact that our main result in Theorem~\ref{thm:main} allows adaptively chosen reward functions. 

\subsection{Setting and motivation}

We consider a linear MDP with an unknown reward function $\true$ defined in terms of the feature map 
$\phi_r:\cX\times\cA\ra\real^{d_r}$ as $\true(x,a) = \innerprod{\phi_r(x,a)}{w_{\mathrm{true}}}$. The transition 
function is defined using the feature map $\phi_P: \cX\times\cA\ra\real^{d_P}$ via $P(x'|x,a)=\innerprod{\phi_P(x,a)}{ 
m(x')}$. It is easy to see that this is a linear MDP in terms of the concatenated feature map of dimension $d = d_r + 
d_P$. We consider a learning problem that we call \emph{imitation learning from features alone}, where a learner 
receives as input a data set of feature vectors $\mathcal{D}_\expert = \bc{\phi_{\cost}(X^i_E,A^i_E)}^{\tau_E}_{i=1}$ 
where $X^i_E,A^i_E \sim \mu(\expert)$ generated by an \emph{expert policy} $\expert$ by interacting with the MDP 
$\cM(\true)$. The learner is tasked with producing an $\varepsilon$-suboptimal policy $\pi^{\mathrm{out}}$ that 
satisfies $\mathbb{E}\bigl[\biprod{\initial}{V_{\true}^{\expert} - V_{\true}^{\pi^{\mathrm{out}}}}\bigr] \leq 
\varepsilon$. The learner has no knowledge of the reward function $\true$ apart from knowing a function class including 
it, but has access to the feature maps and can interact with the MDP.

This framework captures the important special case where the rewards depend only on the state but not the rewards, 
by choosing a feature map $\varphi_r$ that only depends on the states. In this case, the data set taken as input is 
significantly less informative than a full record of states and actions $\mathcal{D}^{\mathrm{x,a}}_\expert = 
\bc{(X^i_E, A^i_E)}^{\tau_E}_{i=1}$. However, observing expert actions has been found restrictive in various practical 
scenarios, which gives a strong motivation to study the setting described above.
% \paragraph{Motivations and comparison with standard imitation learning.} In standard imitation learning, the state-action pairs of the expert dataset are observed directly. 
% In contrast, in our work, the learner only observes the values of the 
% features $\phi_\cost (x,a)$ evaluated at the expert's state-action pairs. Specifically, the learner has access only to the dataset $\mathcal{D}_\expert$ described above as opposed to the potentially more informative dataset $\mathcal{D}^{\mathrm{x,a}}_\expert = \bc{(X^i_E, A^i_E)}^{\tau_E}_{i=1}$. %\footnote{The two dataset are equivalent in tabular MDPs but \emph{imitation learning from features alone} is more general. For example, notice that imitation learning from states alone is a special case.}
% 
% Our setting is important because observing expert actions has been found restrictive in various practical scenarios. 
For instance in robotics, a features only dataset describing a robotic manipulation task 
can be collected easily 
via cameras and sensors \citep{torabi2018generative,zhu2020off,yang2019imitation,torabi2019recent}.
On the other hand, collecting actions on top of features is more challenging as it 
requires knowledge of the internal dynamics of the observed robots. 
Another example of imitation learning from features alone is learning to drive from a video 
which does not show the driver's actions but only the movements of the car induced by those actions.
Finally, notice that imitation learning from states only, studied for example in \cite{sun2019provably}, is a particular 
case of our setting for $\phi_r(x,a) = \mathbf{e}_x$. In this case, the expert dataset consists of states sampled from 
the expert state occupancy measure.

\subsection{Algorithm and sample complexity guarantees} 
In the following, we propose a provably efficient algorithm for imitation learning from features alone.
Our algorithm design and analysis is driven by the following decomposition of the regret, defined as $\regretIL = \sum^K_{k=1}\innerprod{\initial}{V^{\expert}_{P,\true} - V^{\pi_k}_{P,\true}}$, in terms of an appropriately chosen sequence of reward functions $r_1,\dots,r_K$:
%
\begin{equation*}
    (1 - \gamma) \regretIL = \sum^K_{k=1} \innerprod{r_k}{\mu\brr{\expert} - \mu\brr{\pi_k}} + \sum^K_{k=1} \innerprod{\Phi\transpose\mu(\pi_k) - \Phi\transpose\mu(\expert)}{w_k - w_{\mathrm{true}}}.
\end{equation*}
%
The first term in this decomposition corresponds to the regret of our online learning algorithm for adversarial MDPs, and can be controlled by invoking \algname on the sequence of rewards $\pa{r_k}$. The second term in the decomposition corresponds to the regret of another online learning algorithm picking a sequence of reward functions, aiming to minimize the sequence of linear loss functions $\Phi\transpose\mu(\pi_k) - \Phi\transpose\mu(\expert)$ (or at least do as well as the fixed comparator $w_{\mathrm{true}}$). This objective can be achieved by running a standard online learning method such as projected online gradient descent (OGD, \citealp{Zin03}), using a sequence of  unbiased loss estimates that can be computed efficiently using the observed feature vectors. 
The full algorithm is specified as \Cref{alg:fra} in Appendix~\ref{app:pseudocodes} and it is shown to satisfy the following guarantees.
%
\begin{restatable}{theorem}{FraUpper} \label{thm:FraUpper}
  Algorithm~\ref{alg:fra}, when run for $K = \tilde{\mathcal{O}}\brr{d^{3} H^{9/2} \varepsilon^{-2}\log (\abs{\aspace})}$ iterations with an expert dataset of size $\tau_E = \widetilde{\mathcal{O}}\brr{ \WMAX^2 H^2\varepsilon^{-2}}$, outputs an $\varepsilon$-suboptimal policy.
\end{restatable}
%
Notably, the above is the first known bound for this setting that achieves 
a scaling $\varepsilon^{-2}$ with the precision parameter for both the number of MDP interactions $K$ and expert samples $\tau_E$. 
%Moreover these guarantees are achieved without requiring direct observation of the state-action pairs visited by the expert policy (only needing observation of the features). 
We provide a detailed comparison with existing imitation learning theory works in Appendix~\ref{app:related_works_IL}, 
whereas the complete technical details supporting the above theorem and a 
matching worst-case lower bound  is provided in Appendix~\ref{app:proof_IL}.

\subsection{Lower bounds}
The upper bound of Theorem~\ref{thm:FraUpper} depends both on the number of interaction steps $K$ and the expert 
samples $\tau_E$. It is natural to ask if these dependences can be improved in the setting we consider. We address both 
of these questions in the negative in a set of lower bounds described below.


% In the next paragraphs, 
% we argue that bounds for $\tau_E$ and $K$ are not improvable in general providing 
% minimax lower bounds for this quantities.

\paragraph{Lower bound on the number of MDP interactions $K$.}
Theorem~\ref{thm:LowerK} in Appendix~\ref{app:lower} proves 
that for any imitation learning from features alone algorithm, even in the setting $\tau_E = \infty$,
there exists an MDP and an expert policy where at least $K=\Omega\brr{\frac{d H^2}{\varepsilon^2}}$ interactions 
are needed to output a $\varepsilon$-optimal policy.
This lower bound shows that the upper bound provided for $K$ in Theorem~\ref{thm:FraUpper}
achieves the optimal scaling in $\varepsilon$ and can be improved at most by a factor $ d^2 H^{5/2}$.
More importantly, this lower bound marks a clear separation between standard and features only imitation learning: 
purely offline learning ($K=0$) is impossible in imitation learning from features alone
while it is possible in standard imitation learning 
where the experts actions are observed (see, e.g., \citealp{foster2024behavior}).

In the construction of the lower bound, we consider a two state MDP with a reward 
function that depends only on the state variable and we set $\phi_r(x,a) = \mathbf{e}_x$ which prevents 
observing expert actions. 
In this MDP, for $\tau_E=\infty$, the learner observes the expert state occupancy 
measure exactly and 
therefore, the ``good'' state that achieves the maximum of the expert state occupancy measure.
However, since the learner does not know the MDP dynamics, interactions with the MDP 
are needed to find out the action that allows to maximize the learner state occupancy measure 
in the ``good'' state.
Following standard techniques in MDP and bandits lower bound, we can ensure that the amount 
of MDP interactions is at least $\Omega\brr{\varepsilon^{-2}}$.

\paragraph{Lower bound on the number of expert samples $\tau_E$.}
Theorem~\ref{thm:LowerTauE} in Appendix~\ref{app:lower} proves 
that for any algorithm for imitation learning from features alone, even for $K=\infty$,
there exists an MDP and an expert policy where learning an $\varepsilon$-optimal policy requires at least 
$\tau_E =\Omega\brr{\WMAX^2 H^2\varepsilon^{-2}}$ expert samples.
This lower bound proves that Algorithm~\ref{alg:fra} scales optimally with all the problem parameters 
and in the precision $\varepsilon$. Moreover, this lower bound highlights again a clear distinction with standard 
imitation learning.
On the one hand, standard imitation learning can be reduced to a supervised 
classification problem when the optimal policy
is deterministic and the actions are observed in the dataset. As a consequence,
the classic lower bound for supervised classification of order $\mathcal{O}\brr{\varepsilon^{-1}}$ \citep{shalev2014understanding}
holds and it is matched by a purely offline behavioural cloning \citep{rajaraman2020toward}.
On the other hand, in our lower bound construction we choose again $\phi_r(x,a) =\mathbf{e}_x$
to make the expert actions unobservable for the learner and we can prove a larger lower bound of order $\mathcal{O}\brr{\varepsilon^{-2}}$
which holds even if the expert policy is deterministic.

For the proof, our construction is again a two state MDP (a ``good'' state with high reward and a ``bad'' state with 
lower reward). The expert policy is chosen to be the optimal one.
The transition dynamics and the initial distribution are chosen in a way that the expert 
state occupancy meausure is only marginally higher in the ``good'' state. That is, the 
expert occupancy measure equals roughly $1/2 + \varepsilon$ in the ``good'' state 
and  $1/2 - \varepsilon$ in the ``bad'' state.
By standard arguments, we then conclude that the learner needs at least $\Omega\brr{\varepsilon^{-2}}$
samples from the expert to identify the ``good'' state.





