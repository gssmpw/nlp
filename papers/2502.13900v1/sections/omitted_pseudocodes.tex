\section{Omitted pseudocodes}
\label{app:pseudocodes}
This section includes the pseudocode for \algname.
Each of the steps is explained in details in \Cref{sec:algo}.
\begin{algorithm}[!h]
\caption{\algname for Linear MDPs.}
  \begin{algorithmic}[1]
  \label{alg:linear-rmax-ravi-ucb}
      \STATE {\bfseries Inputs:} Number of resets $K$, learning rate $\eta > 0$, exploration coefficient $\beta > 0$, threshold $\omega > 0$, slope sigmoid $\alpha > 0$.
      \STATE {\bfseries Initialize:} $X_1 \sim \nu_0$, $\pi_1 = \piunif$, $Q_1 = 0$, $\cD_1 = \emptyset$, $\Lambda_1 = I$, $t = 1$, $e = 0$.
      \FOR{$k = 1, \dots, K$}
        
      \STATE \vspace{3pt}
      \algcomment{interact with the environment}
      \STATE The adversary adaptively chooses $r_k$, i.e.  $r_k = \textsc{RewardUpdate} \spr{\scbr{\pi_\ell}^k_{\ell=1}, \scbr{r_\ell}^{k-1}_{\ell=1}}$.
      \WHILE{\TRUE}
      \STATE Observe the state $X_t$ and play an action $A_t \sim \pi_k \spr{\cdot \given X_t}$.
      \STATE Receive reward $r_k \spr{X_t, A_t}$ and observe the function $r_k$.
      \STATE With probability $1 - \gamma$, \reset to initial distribution: $X_{t+1} \sim \nu_0$ set $T_k = t$ and \textbf{break} .
      \STATE Otherwise observe the next state $X_{t+1} \sim P \spr{\cdot \given X_t, A_t}$.
      \STATE Update $\Lambda_{t + 1} = \Lambda_t + \phi \spr{X_t, A_t} \phi \spr{X_t, A_t}\transpose$.
      \STATE $\cD_{t+1} = \cD_t \cup \scbr{\spr{X_t, A_t, X_{t+1}}}$.
      \STATE $t = t + 1$.
      \ENDWHILE
      \STATE \algcomment{initialize new epoch}
      \IF{$t = T_1$ \OR $\det \Lambda_{T_k} \geq 2 \det \Lambda_{t_e}$} \label{line-alg:epoch}
      \STATE $e = e + 1$.
      \STATE Set $k_e = k$ and $t_e = t$.
      \STATE Reset the policy $\pi_k = \piunif$.
      \ENDIF
      \STATE For any $\spr{x, a} \in \cX \times \cA$, $\CB_k \spr{x, a} = \beta \norm{\phi \spr{x, a}}_{\Lambda_{t_e}^{-1}}$,\;\;and\; $\CB_k\spr{x^\upplus, a} = 0$. \label{line-alg:bonuses}
      \STATE For any $\spr{x, a} \in \cX \times \cA$, $p_k^\upplus \spr{x, a} = \sigma \spr{\alpha \CB_k \spr{x, a} - \omega}$,\;\;and\; $p_k^\upplus \spr{x^\upplus, a} = 0$. \label{line-alg:pplus}
      \STATE \algcomment{optimistic regularized value iteration}
      \STATE $r_k^\upplus = \spr{1 - p_k^\upplus} \odot r_k + p_k^\upplus \cdot \RMAX$.
      % \STATE $\wh{P}_k = \Lambda_t^{-1} \sum_{\spr{x, a, x'} \in \cD_t} \phi \spr{x, a} \bfe_{x'}$.
      % \STATE $\wh{P}_k^\upplus = \sbr{1 - p_k^\upplus} \wh{P}_k + p_k^\upplus \bfe_{x^\upplus}$.
      % \STATE $Q_{k + 1} = r_k^\upplus + \sbr{1 - p_k^\upplus} \odot \CB_k + \gamma \wh{P}_k^\upplus V_k$.
      \STATE $\wh{M V_k} = \Lambda_{T_k}^{-1} \sum_{\spr{x, a, x'} \in \cD_{T_k}} \phi \spr{x, a} V_k \spr{x'}$. \label{line-alg:ridge}
      \STATE $\wh{P_k^\upplus V_k} = \spr{1 - p_k^\upplus} \odot \phim \wh{M V_k} + p_k^\upplus \cdot V_k \spr{x^\upplus}$,\;\;and\; $\wh{P_k^\upplus V_k} \spr{x^\upplus, \cdot} = \frac{\RMAX}{1 - \gamma}$. \label{line-alg:transf}
      \STATE $Q_{k + 1} = r_k^\upplus + \spr{1 - p_k^\upplus} \odot \CB_k + \gamma \wh{P_k^\upplus V_k}$.
      \STATE $V_{k+1} \spr{x} = \frac1\eta \log \spr{\sum_a \pi_{k} \spr{a \given x} e^{\eta Q_{k+1} \spr{x, a}}}$.
      \STATE $\pi_{k+1} = \pi_{k} \odot e^{\eta \spr{Q_{k+1} - E V_{k+1}}}$.
      \ENDFOR
      \STATE {\bfseries Output:} $\pi_I$, with $I \sim \cU \spr{\sbr{K}}$.
  \end{algorithmic}
\end{algorithm}

\noindent Next, we include the pseudocode for our imitation learning algorithms built on \algname. At line~\ref{algline:expert_estimation}, the learner computes an estimate of the expert features expectation computing an elementwise empirical average of the features in the dataset $\cD_\expert$. Such an estimate is leveraged in the online gradient descent (OGD) update given by the function at lines~~\ref{algline:OGDstart}-\ref{algline:OGDend}. This function instantiates the general $\textsc{RewardUpdate}$ routine given in \Cref{alg:linear-rmax-ravi-ucb}. That is, after each policy update in \algname, the reward player estimates the feature expectation of the current policy $\pi_k$ as the plug in estimator $\phi_\cost \spr{X_k, A_k}$ with $X_k, A_k$ sampled from the occupancy measure $\mu \spr{\pi_k}$. Notice that for the reinforcement learning applications, the adversarial reward sequence is generated online observing the policies. Therefore, for this application it is important that the guarantees in \Cref{thm:main} holds against adaptive adversaries.
%
\begin{algorithm}[t]
  \caption{\FRAalg (Feature Rmax Adversarial Imitation Learning) \label{alg:fra}}
  \centering
  \begin{algorithmic}[1]
    \STATE {\bfseries Inputs:} \\
    (1) a features dataset $\cD_\expert = \scbr{\phi_\cost \spr{X^i_E, A^i_E}}^{\tau_E}_{i=1}$ where for any $i \in \sbr{\tau_E}$, $X^i_E, A^i_E \sim \mu \spr{\expert}$, \\
    (2) read access to $\phi_P \spr{x, a}$ for all $x, a \in \cX \times \cA$, \\
    (3) trajectory access to $\cM \setminus \true$, and \\
    (4) the reward weights class $\cW$ such that $w_{\mathrm{true}} \in \cW$ and $\norm{w} \leq \WMAX$ for all $w \in \cW$.
    \STATE Set $K, \eta, \beta, \omega, \alpha$ as in \Cref{thm:main}.
    \STATE Set $\eta_r = \nicefrac{\WMAX}{B \sqrt{K}}$.
    \STATE Estimate $\widehat{\lambda \spr{\expert}} = \frac{1}{\abs{\cD_\expert}} \sum^{\tau_E}_{i=1} \phi_\cost \spr{X_E^i, A_E^i}$. \label{algline:expert_estimation}
    \STATE \textbf{Function} \label{algline:OGDstart}{$\textsc{Ogd}$}{$\spr{\mu \spr{\pi_k}, w_{k-1}}$}
    \STATE Sample $X_k, A_k \sim \mu \spr{\pi_k}$.
    \STATE $\quad \quad \text{\textbf{return}} \quad w_k = \Pi_{\cW} \spr{w_{k-1} + \eta_r \spr{\widehat{\lambda \spr{\expert}} - \phi_r \spr{X_k, A_k}}}$. \label{algline:OGDend}
    \STATE \textbf{Output:}  \algname $\spr{K, \eta, \beta, \omega, \alpha, \textsc{RewardUpdate} = \textsc{Ogd}}$.
  \end{algorithmic}
\end{algorithm}
