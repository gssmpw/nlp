

\section{\dataset{}: A Consistency-Filtered Conflict Knowledge QA Dataset} \label{sec:benchmark}



In this section, we introduce the \textbf{Co}nsistency-filtered \textbf{Conflict} knowledge \textbf{QA} (\dataset{}) dataset, specifically designed to analyze LLMs' behavior in real-world knowledge conflict scenarios. We begin by highlighting the key differences between \dataset{} and existing works, followed by an overview of our knowledge conflict construction method, and conclude with a detailed description of the dataset construction process.


\textbf{Motivations of \dataset{}.}
Previous studies typically simulate knowledge conflicts by synthesizing counterfactual contexts that contradict accurate parametric knowledge~\cite{longpre2021entity,si2022prompting,xie2023adaptive}, thereby forcing LLMs to accept these pseudo facts provided in the input contexts. In contrast, our approach collects knowledge conflicts from the hallucinations and inaccurate memories of LLMs, rather than relying on these synthesized counterfactuals. Additionally, we ensure data quality by applying a self-consistency based data filtering method and utilizing a GPT-4o-mini-based conflict verification mechanism, distinguishing our method from previous works~\cite{yuan2024discerning,kortukovstudying}.



\textbf{Knowledge Conflict Construction.}
We propose a consistency-based knowledge conflict filtering method to identify and collect queries that induce conflicts between the parametric memory of LLMs and external information, thereby constructing a real-world knowledge conflict benchmark. Our framework consists of the following two steps.

\textit{Parametric Knowledge Elicitation.} 
We adopt a closed-book QA setup and implement a self-consistency mechanism to capture the parametric knowledge of LLMs. Specifically, we first prompt the LLMs $n$ times with the same query and identify the response generated most frequently (i.e., the majority answer) as the model's dominant parametric knowledge for that query. Queries where the majority answer appears fewer than $\frac{n}{2}$ times are filtered out to ensure the quality of the elicited parametric knowledge. In Appendix~\ref{append:data_freq}, we show that retaining queries with higher answering consistency can benefit knowledge conflict construction.



\textit{Conflict Data Selection.} Next, we select instances where the parametric knowledge of LLMs conflicts with the contextual answer. Specifically, we exclude cases where the answer is either fully correct or only partially correct. This ensures that \dataset{} contains queries that can lead to clear and substantial knowledge conflicts. Specifically, we use GPT-4o-mini to identify these conflicts, with the prompt used for this process detailed in Appendix~\ref{append:prompts}. To ensure high precision, we manually spot-check a subset of the detected conflicts and compare them against human annotations (Appendix~\ref{append:human_eval}).


\textbf{\dataset{} Dataset Construction.}
We construct the \dataset{} benchmark using the above conflict-filtering framework. This benchmark is designed to assess the context-faithfulness of LLMs in KAG scenarios.

\begin{table}[t!]
    \centering
    \input{tables/our_bench_describe}
  \caption{Statistics of \dataset{} dataset. \textbf{Full Size*} represents the deduplicated dataset size.}
  \label{tab:our_bench_stats}
\end{table}

Specifically, the \dataset{} dataset is built upon the 2019 MrQA Shared Task~\cite{fisch2019mrqa}. The shared task covers a variety of QA tasks: Natural Questions (NQ)~\cite{kwiatkowski2019natural}, SQuAD~\cite{rajpurkar2016squad}, NewsQA~\cite{trischler2016newsqa}, TriviaQA~\cite{joshi2017triviaqa}, SearchQA~\cite{dunn2017searchqa}, and HotpotQA~\cite{yang2018hotpotqa}. The dataset statistics are shown in Table~\ref{tab:our_bench_stats}.




