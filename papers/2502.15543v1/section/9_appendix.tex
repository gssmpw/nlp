\appendix
\begin{table}[t!]
  \centering
  \input{tables/our_bench_each_step}
 \caption{Number of instances at each stage in the \dataset{} construction pipeline.}
 \label{tab:our_bench_stats_each_step}
\end{table}
\section{Appendix}
\subsection{License}
We present the licenses of the datasets used in this study: Natural Questions (CC BY-SA 3.0 license), NewsQA (MIT License), SearchQA and TriviaQA (Apache License 2.0), HotpotQA and SQuAD (CC BY-SA 4.0 license).

All these licenses and agreements permit the use of their data for academic purposes.

\subsection{Details of Data Constructing}
\label{append:prompts}
In this section, we detail the two main steps in constructing \dataset{}. The dataset sizes at each stage of the pipeline are shown in Table~\ref{tab:our_bench_stats_each_step}.


\textbf{Parametric Knowledge Elicitation.} First, we elicit the LLM's parametric knowledge by prompting it in a closed-book setting (i.e., without any context). To ensure the reliability of the elicited knowledge, we apply a consistency-based filtering method. Specifically, for each query, the LLM is prompted five times, and the frequency of each response is recorded. The response with the highest frequency is identified as the majority answer. Queries where the majority answer appears fewer than three times are discarded, in order to filter out inconsistent responses and enhance data quality. The following prompt is used to instruct the LLM:
\begin{tcolorbox}
[title=Prompt for eliciting parametric knowledge,colback=blue!10,colframe=blue!50!black,arc=1mm,boxrule=1pt,left=1mm,right=1mm,top=1mm,bottom=1mm]
Answer the question \textcolor{blue}{\{\textit{brevity\_instruction}\}} and provide supporting evidence.

Question: \textcolor{blue}{\{\textit{question}\}}
\end{tcolorbox}
\noindent The ``\textit{brevity\_instruction}'' is used to guide the LLM to generate responses in a more concise form.

\textbf{Conflict Data Selection.} Next, we filter the data to retain only instances where the LLM's parametric knowledge directly conflicts with the contextual answer. Specifically, we categorize the data obtained from the previous step into two groups, conflicting and non-conflicting instances, based on the detailed results of conflict detection. All non-conflicting instances are discarded. GPT-4o-mini is then used to detect the presence of a conflict, using the following prompt:

\begin{tcolorbox}
[title=Prompt for identifying conflict knowledge,colback=blue!10,colframe=blue!50!black,arc=1mm,boxrule=1pt,left=1mm,right=1mm,top=1mm,bottom=1mm]
\small
You are tasked with evaluating the correctness of a model-generated answer based on the given information. 

\small
Context: \textcolor{blue}{\{\textit{context}\}}

Question: \textcolor{blue}{\{\textit{question}\}}

Contextual Answer: \textcolor{blue}{\{\textit{contextual\_answer}\}}

Model-Generated Answer: \textcolor{blue}{\{\textit{Model-Generated\_answer}\}}

\textcolor{blue}{[\textit{Detailed task description...}]}

Output Format:

Evaluate result: (Correct / Partially Correct / Incorrect) 
\end{tcolorbox}




\subsection{Assessing the Reliability of GPT-4o-mini in Knowledge Conflict Identification}
\label{append:human_eval}
In this subsection, we conduct the human evaluation to assess the reliability of GPT-4o-mini in identifying knowledge conflicts, which is a critical task in our data construction process to guarantee the data quality.

We randomly sampled 100 examples from each of the six subsets of \dataset{}, yielding a total of 600 samples. Six senior computational linguistics researchers were then asked to evaluate whether a knowledge conflict was present in each example. For each instance, the evaluators were provided with the question, the contextual answer, the model-generated response, and the corresponding supporting evidence. The results were classified into three categories: No Conflict, Somewhat Conflict, and High Conflict. The detailed annotation instructions are as follows:

\begin{tcolorbox}
[title=Annotation Instruction,colback=blue!10,colframe=blue!50!black,arc=1mm,boxrule=1pt,left=1mm,right=1mm,top=1mm,bottom=1mm]
\small
You are tasked with determining whether the parametric knowledge of LLMs conflicts with the given context to facilitate the study of knowledge conflicts in large language models.

Each data instance contains the following fields: 

Question: \textcolor{blue}{\{\textit{question}\}}


Answers: \textcolor{blue}{\{\textit{answers}\}}


Context: \textcolor{blue}{\{\textit{context}\}}

Parametric\_knowledge: \textcolor{blue}{\{\textit{LLMs' parametric\_knowledge }\}} 

The annotation process consists of two steps. 

\textbf{Step 1}: Compare the model-generated answer with the ground truth answers, based on the given question and context, to determine whether the model’s parametric knowledge conflicts with the context.

\textbf{Step 2}: Classify the results into one of three categories: 

\textcolor{blue}{\{\textit{No Conflict}\}} if the model-generated answer is consistent with the ground truth answers and context, 

\textcolor{blue}{\{\textit{Somewhat Conflict}\}}  if it is partially inconsistent

\textcolor{blue}{\{\textit{High Conflict}\}} if it significantly contradicts the ground truth answers or context.
\end{tcolorbox}


The evaluation results, shown in Table~\ref{tab:append_human_eval}, reveal a high level of agreement between the human annotators and GPT-4o-mini. Over 85\% of the examples reach consensus among the annotators, with an average agreement rate of 85.6\% across all subsets. These findings underscore the reliability of GPT-4o-mini as an effective tool for identifying knowledge conflicts.




\begin{table}[t]
  \centering
  \input{tables/append_human_eval}
 \caption{Agreement between human annotators and GPT-4o-mini across different subsets of our \dataset{} benchmark.}
 \label{tab:append_human_eval}
\end{table}



\subsection{Evaluating the Effectiveness of Our Consistency-Based Filtering Method}
\label{append:data_freq}

In this subsection, we evaluate the effectiveness of our consistency-based knowledge conflict filtering method. As described in Appendix~\ref{append:prompts}, for each query, we prompt the model five times and record the most frequently generated answer along with its occurrence frequency. Based on this frequency, we divide the data into sub-datasets, where all queries within each sub-dataset share the same answer frequency. We then apply ``Conflict Data Selection'' to each sub-dataset, retaining only instances where knowledge conflicts occur. Finally, we evaluate ConR and MemR on these sub-datasets.

As shown in Figure~\ref{fig:diff_freq}, a clear trend emerges: as answer frequency increases, ConR consistently decreases, while MemR increases. This pattern indicates that as answer frequency rises, the model becomes increasingly reliant on its internal knowledge. Notably, for data with an answer frequency of 1, MemR is only 3\%, indicating minimal dependence on internal knowledge. Retaining only high-answer-frequency data improves the quality of \dataset{}. This data construction approach distinguishes our methodology from previous studies~\cite{longpre2021entity,xie2023adaptive}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.4\textwidth]{figs/diff_freq.pdf}
  \caption{Performance comparison of ConR and MemR across sub-datasets grouped by the answer frequency of LLMs.}
  \label{fig:diff_freq}
\end{figure}





\subsection{Additional Implementation Details of Our Experiments}
\label{append:implementation}
This subsection outlines the training prompt, describes more details of the training data, and provides details of the experimental setup used in our experiments.

\textbf{Training Prompts.}
We adopt a simple QA-format training prompt following~\citet{zhou2023context} for all methods except \attrprompt{} and \oiprompt{}.
\begin{tcolorbox}
[title=Base Prompt ,colback=blue!10,colframe=blue!50!black,arc=1mm,boxrule=1pt,left=1mm,right=1mm,top=1mm,bottom=1mm]
% \small
\textcolor{blue}{\{\textit{context}\}} 
Q: \textcolor{blue}{\{\textit{question}\}} ? 
A: \textcolor{blue}{\{\textit{answer}\}}.
\end{tcolorbox}


\textbf{Training Datasets.} During \method{}, we randomly sample 32,580 instances from the training set of the MRQA 2019 benchmark~\cite{fisch2019mrqa} to construct our training data.



\textbf{Experimental Setup.} In this work, all models are trained for 2,100 steps with a total batch size of 32 and a learning rate of 1e-4. To enhance training efficiency, we implemented \method{} with LoRA~\cite{hu2021lora}, setting both the rank $\text{r}$ and scaling factor $\text{alpha}$ to 64. For \method{}, we set $\alpha$ to 0.1 (Eq.~\ref{eq:selct_layers}), which determines the minimum activation ratio difference required for a layer to be pruned. Additionally, we adopt a dynamic $\gamma$ in $\mathcal{L}_{\text{KC}}$ (Eq.~\ref{eq:kc_loss}), which linearly transitions from an initial margin ($\gamma_{0}=1$) to a final margin ($\gamma^*=5$) as training progresses. This adaptive strategy gradually reduces the model's reliance on internal parametric knowledge, encouraging it to rely more on external knowledge provided by the KAG system.


\subsection{Implementation Details of Baselines}
\label{append:baseline}
This subsection describes the implementation details of all baseline methods.

We adopt two prompt-based baselines: the attributed prompt ($\text{Attr}_{\text{prompt}}$) and a combination of opinion-based and instruction-based prompts ($\text{O\&I}_{\text{prompt}}$). The corresponding prompt templates are as follows:

\begin{tcolorbox}
[title=Attr based prompt ,colback=blue!10,colframe=blue!50!black,arc=1mm,boxrule=1pt,left=1mm,right=1mm,top=1mm,bottom=1mm]
% \small
\textcolor{blue}{\{\textit{context}\}} Q: \textcolor{blue}{\{\textit{question}\}} based on the given text? A: \textcolor{blue}{\{\textit{answer}\}}.
\end{tcolorbox}

\begin{tcolorbox}
[title=O\&I based prompt ,colback=blue!10,colframe=blue!50!black,arc=1mm,boxrule=1pt,left=1mm,right=1mm,top=1mm,bottom=1mm]

Bob said ``\textcolor{blue}{\{\textit{context}\}}'' Q: \textcolor{blue}{\{\textit{question}\}} in Bob's opinion? A: \textcolor{blue}{\{\textit{answer}\}}.
\end{tcolorbox}
For the SFT baseline, we incorporate context during training, similar to \method{}, while keeping the remaining experimental settings identical. To construct preference pairs for DPO training, we use contextually aligned answers from the dataset as ``preferred responses'' to ensure the consistency with the provided context. The ``rejected responses'' are generated by identifying parametric knowledge conflicts through our data construction methodology (Sec.~\ref{sec:benchmark}).

For KAFT, we employ a hybrid dataset containing both counterfactual and factual data. Specifically, we integrate the counterfactual data developed by \citet{xie2023adaptive}, leveraging their advanced data construction framework.

By maintaining equivalent dataset sizes and ensuring comparable data quality across all baselines, we provide a rigorous and fair comparison with our proposed \method{}.




\subsection{Extending \method{} to More LLMs}
\label{append:diff_model_performance}


\begin{figure}[t!]
  \centering
  \input{figs/diff_models_llama}
  % \includegraphics[width=0.48\textwidth]{figs/diff_model_double.pdf}
 \caption{Average ConR and MemR across different models implemented by LLMs of LLaMA series, before and after applying \method{}.
 }
 \label{fig:diff_model_double_llama}
\end{figure}

\begin{figure}[t]
  \centering
  \input{figs/diff_models_qwen}
  % \includegraphics[width=0.48\textwidth]{figs/diff_model_double.pdf}
 \caption{Average ConR and MemR across different models implemented by LLMs of Qwen series, before and after applying \method{}.
 }
 \label{fig:diff_model_double_qwen}
\end{figure}






We extend \method{} to a diverse range of LLMs, encompassing multiple model families and sizes. 

Specifically, our evaluation includes LLaMA3-8B-Instruct, LLaMA3.2-1B-Instruct, LLaMA3.2-3B-Instruct, Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and Qwen2.5-14B-Instruct. The results on ConR and MemR are summarized in Figures~\ref{fig:diff_model_double_llama} and \ref{fig:diff_model_double_qwen}, while Table~\ref{tab:append:all_model_res} presents the average performance of all models on \dataset{} and ConFiQA. Additionally, Table~\ref{tab:diff_model_param} provides detailed parameter information and specifies the layers selected for pruning for each model. This comprehensive evaluation demonstrates the versatility and scalability of \method{} across a wide spectrum of model architectures and sizes.

\begin{table}[!t]
  \input{tables/diff_model_param}
  \caption{The total number of parameters for various models before and after applying \method{}. \textcolor{gray}{\small$(\cdot)\%$} represents the proportion relative to the original model, and the last column lists the layers selected for pruning.}
   \label{tab:diff_model_param}
\end{table}

These experimental results illustrate several key insights: 1) Larger models tend to rely more on parametric memory. As model size increases in both the LLaMA and Qwen families, MemR also grows, indicating a tendency to overlook external knowledge in favor of internal parameters. \method{} counteracts this behavior, decreasing larger models' MemR score to even below that of smaller models. 2) \method{} consistently benefits all evaluated models. Across both LLaMA and Qwen model families, \method{} outperforms Vanilla-KAG by boosting accuracy and context faithfulness, underscoring its broad applicability and effectiveness. 3) Not all parameters in KAG models are essential. Pruning parametric knowledge not only reduces computation costs but also fosters better generalization without sacrificing accuracy, highlighting the potential of building a parameter-efficient LLM within the KAG framework.




\begin{table*}[!t]
  \input{tables/all_model_res}
  \caption{Average performance of LLMs on \dataset{} and ConFiQA before and after applying \method{}.}
   \label{tab:append:all_model_res}
\end{table*}

\subsection{Neuron Activations in Different LLMs}\label{app:activation}
We present the neuron activations for the LLaMA family models, including LLaMA-3.2-1B-Instruct, LLaMA-3.2-3B-Instruct, LLaMA-3-8B-Instruct, and LLaMA-3.1-8B-Instruct, as well as the Qwen family models, including Qwen-2.5-0.5B-Instruct, Qwen-2.5-1.5B-Instruct, Qwen-2.5-3B-Instruct, Qwen-2.5-7B-Instruct, and Qwen-2.5-14B-Instruct, in Figures~\ref{fig:act_llama} and \ref{fig:act_qwen}, respectively. 
% 我们发现qwen系列模型


\begin{figure*}[t]
  \centering
  \input{append_fig/act_llama_all}

 \caption{Neuron activations across different layers of the LLaMA series models. We present the inhibition ratio $\Delta R$ under two conditions: with contextual knowledge input (w/ context) and without it (w/o context).}
 \label{fig:act_llama}
\end{figure*}

\begin{figure*}[t]
  \centering
  \input{append_fig/act_qwen_all}

 \caption{Neuron activations across different layers of the Qwen series models. We present the inhibition ratio $\Delta R$ under two conditions: with contextual knowledge input (w/ context) and without it (w/o context). }
 \label{fig:act_qwen}
\end{figure*}
