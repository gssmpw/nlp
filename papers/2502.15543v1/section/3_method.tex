\section{Methodology}

As illustrated in Figure~\ref{fig:our_method},  this section introduces our \textbf{P}arametr\textbf{I}c \textbf{P}runing-based \textbf{K}nowledge-\textbf{A}ugmented \textbf{G}eneration (\method{}) model, which helps LLMs to utilize external knowledge $c$ to answer the question $q$. Specifically, \method{} prunes parameters to uninstall internal knowledge of LLMs (Sec.~\ref{sec:param_pruning}) and installs a plug-and-play KAG adaptation module to facilitate the knowledge usage (Sec.~\ref{sec:training}).


\subsection{Uninstalling Internal Knowledge via Parametric Pruning}
\label{sec:param_pruning}
To mitigate knowledge conflicts, \method{} first uninstalls the internal knowledge of LLMs by pruning parameters using a neuron activation-based approach. Specifically, we first introduce the concept of neuron activation and then explain how internal knowledge is uninstalled by pruning knowledge-related parameters in transformer layers that become inhibited after knowledge augmentation.




\subsubsection{Preliminary of Neuron Activation} 
This section provides a technical foundation for our parametric pruning method by examining neuron activation mechanisms in transformer-based LLMs.

Modern LLMs typically comprise multiple stacked decoder blocks, each containing a multi-head attention (MHA) mechanism and a feed-forward network (FFN). The FFN sub-layer is widely viewed as a key-value memory mechanism, storing the majority of the parametric knowledge~\cite{geva2020transformer} through two parameter matrices $\bm{K}$, $\bm{V} \in \mathbb{R}^{d_m \times d}$, where $d_m$ denotes the intermediate hidden dimension. For the $i$-th token $x_i \in X$ in the input sequence, the FFN sub-layer processes its representation $\bm{x}_i \in \mathbb{R}^{d}$ from the last layer through linear transformations. Formally, the computation in the $l$-th FFN sub-layer can be expressed as a key-value memory:
\begin{equation}\small
    \label{eq:ffn_describe}
    \text{FFN}(\bm{x}_i^l) = (\sigma(\bm{K}^l \bm{x}_i^l ))^\mathrm{T}\bm{V}^l,
\end{equation}
where $\sigma$ is the activation function. Consequently, $\bm{K}^l$ and $\bm{V}^l$ can be expressed as sets of $d_m$ parameter vectors: $\bm{K}^l = \{\bm{k}_1^l, \dots, \bm{k}_{d_m}^l\}$ and $\bm{V}^l = \{\bm{v}_1^l, \dots, \bm{v}_{d_m}^l\}$. This decomposition allows the FFN computation to be rewritten as:
\begin{equation}\small
    \label{eq:ffn_describe_decomposed}
    \text{FFN}(\bm{x}_i^l) = \sum_{j=1}^{d_m} \sigma(\bm{x}_i^l \cdot \bm{k}_j^l)\bm{v}_j^l = \sum_{j=1}^{d_m}a_{ij}^l\bm{v}_j^l,
\end{equation}
where, $\bm{k}_j^l, \bm{v}_j^l \in \mathbb{R}^d$ correspond to the $j$-th row of the weight matrices $\bm{K}^l$ and $\bm{V}^l$, respectively. The term $a_{ij}^l = \sigma(\bm{x}_i^l \cdot \bm{k}_j^l)$ represents the \textit{activation coefficient} associated with the neuron $\bm{v}_j^l$. A neuron is considered \textit{activated} if $a_{ij}^l > 0$, as a zero value nullifies its contribution in subsequent computations~\cite{mu2024revealing}.

\begin{figure}[!t]
    \centering

    \includegraphics[width=0.48\textwidth]{figs/method.pdf}
  \caption{An overview of \method{}: (1) Pruning (\S\ref{sec:param_pruning}), which uninstalls the internal knowledge of LLMs through parameter pruning; (2) Adaption (\S\ref{sec:training}), which enables LLMs to effectively leverage external sources.
  }
  \label{fig:our_method}
\end{figure}

Following~\citet{fan2025slam}, we compute the sequence-level activation coefficient $A^l_j(X)$ for the $j$-th neuron in the $l$-th FFN layer by averaging $a_{ij}^l$ over all tokens $X=\{x_1, \dots, x_T\}$:
\begin{equation}\small
    \label{eq:activation_numbers}
    A^l_j (X) =  \frac{1}{T} \sum_{i=1}^{T} \mathbb{I}[a_{ij}^l].
\end{equation}
where $\mathbb{I}[a_{ij}^l]$ is $1$ if $a_{ij}^l>0$ and 0 otherwise. For clarity, we normalize the number of activated neurons into the neuron activation ratio $R^l$, given by the following equation:
\begin{equation}\small
    \label{eq:activation_ratio}
    R^l (X) = \frac{\sum_{j=1}^{d_{m}} A^l_j (X)}{d_m}.
\end{equation}
This ratio reflects how actively the neurons in layer $l$ contribute to processing the input sequence $X$.


\subsubsection{Parametric Pruning via Knowledge Augmented Neuron Inhibition} 
To identify layers that store a higher proportion of parametric memory, which may contribute to knowledge conflicts, we propose a layer selection algorithm based on neuron inhibition.

Specifically, for a query $q$ and external context $c$, we calculate the neuron inhibition ratio $\Delta R^l$ by measuring the change in activated neuron ratios before and after knowledge augmentation with $c$:


\begin{equation}\small
    \label{eq:difference}
    \Delta R^l = R^l (q) -  R^l (c \oplus q),
\end{equation}
where $\oplus$ denotes the concatenation operation. A larger $\Delta R^l$ indicates greater inhibition of neurons after incorporating external knowledge $c$. We then define $\mathcal{H}_\text{Pruning}$ as the set of layers selected for pruning, aiming to uninstall internal knowledge:

\begin{equation}\small
    \label{eq:selct_layers}
    \mathcal{H}_\text{Pruning} = 
    \left\{
    h_l \,\middle|\,
    \Delta R^l \geq \alpha
    \right\}.
\end{equation}
Here, $\alpha$ represents the threshold that specifies the minimum activation ratio difference required for a layer to be pruned. After identifying the layers for pruning $\mathcal{H}_\text{Pruning}$, we prune the FFN sub-layers in these layers, as they store the majority of the knowledge in LLMs~\cite{geva2022transformer,geva2020transformer,meng2022locating,meng2022mass}.

\subsection{Knowledge-Augmented Adaptation through Preference Optimization}
\label{sec:training}

After pruning, we further incorporate a plug-and-play KAG adaptation module~\cite{hu2021lora} and optimize it to recalibrate the model's knowledge utilization preferences, enabling more effective usages of external knowledge:
\begin{equation}\small 
\label{eq:final_loss}
    \mathcal{L} = \lambda_1 * \mathcal{L}_{\text{KAT}} + \lambda_2 *\mathcal{L}_{\text{KPO}},
\end{equation}
where $\lambda_1$ and $\lambda_2$ are hyperparameters that control the balance between the two objectives. The Knowledge-Augmented Training ($\mathcal{L}_{\text{KAT}}$) and Knowledge Preference Optimization ($\mathcal{L}_{\text{KPO}}$) objectives guide the pruned model to both generate accurate answers and calibrate its knowledge usage preference towards external knowledge.


\textbf{Knowledge-Augmented Finetuning.} Following~\citet{lin2023ra}, we maximize the likelihood of generating the ground truth answer $y^*$ based on both query $q$ and external knowledge $c$:
\begin{equation}\small
    \label{eq:sft_loss_w_context}
    \mathcal{L}_{\text{KAT}} = \sum_{q \in Q} -\log P(y^* \mid q, c),
\end{equation}
where $Q$ is the set of all training queries. This objective trains the pruned model to leverage both internal and external knowledge to answer the question accurately.

\textbf{Knowledge Preference Optimization.} To further refine the model's reliance on external versus internal knowledge, we apply a max-margin loss~\cite{david1963method} to optimize the likelihood of generating ground truth answers that depend more on external knowledge:
\begin{equation}\small
\label{eq:kc_loss}
    \mathcal{L}_{\text{KPO}} = \sum_{q \in Q} \left[ \gamma -\log P(y^* \mid q, c) +\log P(y^* \mid q)   \right]_+,
\end{equation}
where $\gamma$ is a margin parameter that controls the preference constraint, and the $[\cdot]+$ function ensures non-negativity. This objective further finetunes the pruned model to shift its reliance towards external knowledge, improving the reliability and faithfulness of its responses.


