
\section{Experiment Results}

In this section, we first present the overall performance of \method{}. We then conduct ablation studies to evaluate the contribution of each individual component in \method{}. Finally, we analyze the knowledge usage of \method{} and examine the neuron inhibition in LLMs when provided with external knowledge.

\subsection{Overall Performance}

This experiment evaluates \method{} on \dataset{} to assess its overall performance. Additionally, we test \method{} on the ConFiQA dataset, which represents an out-of-domain setting.

As shown in Table~\ref{tab:main_res_id}, \method{} significantly outperforms baseline models on \dataset{}, demonstrating its effectiveness in producing more accurate and contextually faithful responses. Compared to the vanilla KAG model, \method{} achieves an average improvement of 52.36\% in EM and about 5\% in other evaluation metrics, highlighting its effectiveness in helping LLMs mitigate knowledge conflicts and use external knowledge.
The evaluation results show that these prompt-based methods, such as \attrprompt{} and \oiprompt{}, are effective in reducing the model's reliance on parametric knowledge, but they degrade the performance of the vanilla KAG model in terms of answer correctness. In contrast, fine-tuning-based approaches, including SFT, KAFT, and DPO, provide a more effective method to guide LLMs in better utilizing external knowledge. With \method{}, LLMs benefit from our ``uninstallation-and-adaptation'' mechanism, which enhances their ability to leverage contextual knowledge during training, demonstrating consistent improvements compared to baseline methods. Notably, \method{} reduces the model size by 13.2\% compared to vanilla LLMs, demonstrating that smaller LMs equipped with external knowledge also have the potential to outperform larger models~\cite{asai2024reliable}.

To further validate the generalization capability of \method{}, we perform evaluations on ConFiQA. This dataset, which includes counterfactuals, was not used in training \method{}. As shown in Table~\ref{tab:main_res_ood}, \method{} outperforms alternative approaches, such as SFT and DPO, in term of both correctness and context-faithfulness, demonstrating its strong generalization ability. This improvement can be attributed to the effectiveness of \method{} in enhancing the ability of LLMs to utilize contextual knowledge instead of memorizing contextual knowledge. \method{} lags behind KAFT by an average of 1.66\% in ConR, which may be due to the additional data augmentation techniques designed in KAFT to enhance the SFT method.




\begin{table}[!t]
    \centering
    \input{tables/ablation_componet}
    \caption{Performance of \method{} on \dataset{} for the ablation study. We first evaluates the effectiveness of knowledge uninstallation module (w/o Uninstall) and the adaptation module (w/o Adaption). And then we present the performance of \method{} using different pruning strategies, including multi-head attention (w/ MHA), full layer pruning (w/ Layer), and parameter-level pruning (w/ Param). }
     \label{tab:component_ablation}
\end{table}

\subsection{Ablation Study}
\label{subsec:abations}
The ablation studies are conducted to investigate the contribution of different modules in \method{} and to assess the performance of \method{} using various parameter pruning strategies.

As shown in Table~\ref{tab:component_ablation}, we first compare \method{} with \method{} w/o Uninstall and \method{} w/o Adaption models, in order to analyze the roles of different modules in PIP-KAG. The results indicate that removing the knowledge uninstallation or adaptation modules causes a performance drop of approximately 0.6\% in ConR, highlighting their importance in helping LLMs effectively leverage external knowledge. Specifically, compared to PIP-KAG, PIP-KAG w/o Uninstall shows a 1.16\% decrease in $\text{MR}$ score, while PIP-KAG w/o Adaption exhibits a smaller decrease of 0.15\% in $\text{MR}$ score. These findings demonstrate the effectiveness of \method{} in calibrating the behavior of LLMs with respect to their reliance on parametric knowledge and in mitigating knowledge conflicts.

Next, we evaluate the performance of \method{} using three structured pruning strategies and one unstructured pruning method. The structured pruning strategies are as follows: (1) removal of FFN sub-layers (\method{}), (2) elimination of MHA sub-layers (\method{} w/ MHA), and (3) pruning of entire transformer layers (\method{} w/ Layer). Additionally, we compare an advanced unstructured pruning approach (\method{} w/ Param)~\cite{lee2018snip}, which identifies important parameters using a gradient-based importance scoring method and then prunes the less important parameters with the predefined pruning ratio.
Among all the methods, \method{} achieves the best performance when pruning FFN sub-layers, showing its effectiveness in reducing reliance on internal memory. The primary reason may lie in the fact that FFN layers play a critical role in knowledge storage, which aligns with findings in previous studies~\cite{geva2022transformer, dai2021knowledge}.


