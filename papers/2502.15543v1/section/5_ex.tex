\section{Experimental Methodology}
In this section, we describe the datasets, evaluation metrics, baselines and implementation details used in our experiments.


\textbf{Datasets.}
For our experiments, we use the \dataset{} dataset for both training and evaluation. In addition, we utilize the ConFiQA~\cite{bi2024context} dataset during evaluation, which serves as an out-of-domain test scenario to assess the generalization ability of different models. ConFiQA is designed to evaluate the context faithfulness of LLMs in adhering to counterfactual contexts. It consists of three subsets: Question-Answering, Multi-hop Reasoning, and Multi-Conflicts, each containing 6,000 instances.


\textbf{Evaluation.}
Following previous work~\cite{longpre2021entity, zhou2023context}, we employ multiple evaluation metrics to assess the generated responses from two aspects: correctness and context faithfulness. To ensure more accurate evaluations, we normalize both responses and answers according to the method described by \citet{li2024rag}.

For accuracy assessment, we use $\text{EM}(\uparrow)$, which measures whether the generated responses exactly match the ground truth answers.
To evaluate context faithfulness, we adopt two metrics: the recall of context (\text{ConR}$\uparrow$) and the recall of memory (\text{MemR}$\downarrow$). Specifically, \text{ConR} assesses whether the generated responses align with the provided context, while \text{MemR} assesses the alignment with parametric memories. Additionally, we adopt the memorization ratio $\text{MR}(\downarrow) = \frac{\text{MemR}}{\text{MemR} + \text{ConR}}$, which captures the tendency to rely on internal memory.

\begin{table*}[t!]
    \input{tables/main_res_id}
    \caption{Performance on the \dataset{} dataset. The highest scores are highlighted in \textbf{bold}, while the second-highest scores are \uline{underlined}. ``Param.'' refers to the total number of model parameters.}
     \label{tab:main_res_id}
\end{table*}

\begin{table*}[t!]
    \input{tables/main_res_ood}
    \caption{Performance on the testing sets of ConFiQA.}
     \label{tab:main_res_ood}
\end{table*}

\textbf{Baselines.}
We evaluate \method{} against five baselines, which are categorized into three groups: (1) Prompt-based approaches, including the attributed prompt (\attrprompt) and the combined opinion-based and instruction-based prompt (\oiprompt) from \citet{zhou2023context}; (2)  Fine-tuning methods, consisting of standard Supervised Fine-Tuning (SFT) and Knowledge Aware Fine-Tuning (KAFT)~\cite{li2022large}. KAFT enhances context faithfulness through counterfactual data augmentation; and (3) the Context-DPO~\cite{bi2024context} utilizes DPO method~\cite{rafailov2023direct} to strengthen context-grounded responses while penalizing those relying on parametric memory.

\textbf{Implementation Details.}
In our experiments, we use LLaMA3-8B-Instruct as the backbone for all methods. To train \method{}, we utilize \dataset{} to identify inaccurate parametric knowledge. The pruning threshold $\alpha$ is set to 0.05. The hyperparameters, $\lambda_1$ and $\lambda_2$, are set to 0.5, which are used to balance $\mathcal{L}_{\text{KAT}}$ and $\mathcal{L}_{\text{KPO}}$ in Eq.~\ref{eq:final_loss}. Additionally, we exclude the last three layers when selecting layers for pruning, as previous studies have shown that removing these layers significantly impacts model performance~\cite{lad2024remarkable,chen2024compressing,zhang2024finercut}. More implementation details are provided in Appendix~\ref{append:implementation}.



