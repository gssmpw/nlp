
\subsection{Effectiveness of \method{} in Leveraging Contextual Knowledge} \label{sec:analysis}

In Figure~\ref{fig:analysis}, we further evaluate the ability of different LLMs to utilize contextual knowledge. We compare the performance of three models: the vanilla LLM, the Uninstall model (\method{} w/o Adaptation), and our \method{}.

\begin{figure}[!t]
    \centering
    % \input{figs/analysis}
    \input{figs/analysis_fig}
    % \includegraphics[width=0.48\textwidth]{figs/diff_model_double.pdf}
  \caption{Evaluation of knowledge utilization of different models. We assess the response similarity with parametric knowledge and contextual knowledge (Figure~\ref{fig:sim_2_pm_ans} and Figure~\ref{fig:sim_2_context_ans}), and compute the Perplexity (PPL) score when reproducing the ground truth answer (Figure~\ref{fig:ppl_wo_context} and Figure~\ref{fig:ppl_w_context}). The ``Uninstall'' model refers to \method{} w/o Adaption, which only incorporates the knowledge uninstallation.
  }
  \label{fig:analysis}
\end{figure}

First, we compute the semantic similarity between the outputs of different models and two knowledge sources--parametric knowledge from the model (Figure~\ref{fig:sim_2_pm_ans}) and contextual answers (Figure~\ref{fig:sim_2_context_ans})--to analyze their knowledge preference. Additionally, the performance of vanilla KAG model is provided as a reference. As shown in Figure~\ref{fig:sim_2_pm_ans}, compared to vanilla LLM, the Uninstall model exhibits the lowest similarity with parametric knowledge among all models, indicating that the knowledge uninstallation process effectively reduces the LLM's reliance on internal memory. Figure~\ref{fig:sim_2_context_ans} further compares the similarity between LLM responses and the contextual answers. \method{} achieves a significantly higher similarity score with contextual answers than other models, demonstrating its ability to effectively guide LLMs in leveraging external knowledge through knowledge-augmented adaptation.

To further investigate the knowledge utilization of different models, we ask each model to reproduce the ground truth answers and calculate the Perplexity (PPL) score both without (Figure~\ref{fig:ppl_wo_context}) and with (Figure~\ref{fig:ppl_w_context}) contextual knowledge. A lower PPL score indicates that the model is more confident to produce contextual answers. When external knowledge is not provided, the Uninstall model shows a significant increase in PPL, while the vanilla LLM maintains a relatively low PPL in the absence of context, showcasing the effectiveness of knowledge uninstallation in freeing the LLM's internal knowledge storage. In contrast, \method{} reaches an ``Inf'' PPL score without contextual knowledge provided but demonstrates a significant reduction in PPL score when external knowledge is provided. This highlights the effectiveness of our knowledge-augmented adaptation module in optimizing the LLM's reliance on external context for knowledge utilization.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.93\linewidth]{figs/act_llama_step2.pdf}
    \caption{Neuron activation across different layers. We present the absolute inhibition ratio $|\Delta R|$ under two conditions: when the input incorporates context knowledge (w/ context) and when it does not (w/o context).}
    \label{fig:llama3-8b-neuron_activation}
\end{figure}


\subsection{Neuron Activation in LLMs} 
As shown in Figure~\ref{fig:llama3-8b-neuron_activation}, we visualize the ratio of activated neurons and the absolute inhibition ratio $
|\Delta R|$ (Eq.~\ref{eq:difference}) in LLaMA3-8B-Instruct. The neuron activation ratios of different LLMs are provided in Appendix~\ref{app:activation}.

The results reveal a significant reduction in overall neuron activation levels when external context is provided. This reduction likely suggests that certain neurons associated with parametric knowledge become inhibited in the presence of external knowledge. We further observe that these inhibited neurons are predominantly concentrated in the upper layers of the model, which aligns with prior findings that factual knowledge is predominantly stored in the upper layers of transformer-based models~\cite{geva2020transformer,wang2024knowledge}. 
While parametric knowledge plays a crucial role in generating responses, it may introduce risks when it is outdated or conflicts with external information provided by KAG, potentially degrading the KAG performance. This work explores a pruning-based approach to mitigate the impact of parametric memory by removing these neurons that are inactive after feeding contextual knowledge, offering a new perspective on mitigating knowledge conflicts.
