\section{Related work}
Knowledge-Augmented Generation (KAG) has proven effective by enhancing LLMs with external knowledge sources, such as Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval, ram2023context, shi2023replug, yao2022react} and Tool-Augmented Generation~\cite{schick2023toolformer,qin2024tool}. These approaches update the internal memory of LLMs by incorporating external knowledge as context, thereby facilitating a wide range of knowledge-intensive tasks~\cite{zheng2023can, lin2023ra, li2024rag}. However, the integration of external knowledge can lead to knowledge conflicts when it contradicts the modelâ€™s parametric knowledge, undermining the reliability of KAG systems~\cite{chen2022rich, longpre2021entity, yu2023characterizing, xie2023adaptive,bi2024factuality}.

Numerous studies focus on understanding and evaluating knowledge conflicts by constructing specialized benchmarks. Recent works~\cite{longpre2021entity, jin2024tug} synthesize conflicts by substituting entities in the context, demonstrating that LLMs tend to over-rely on their internal memory. In contrast, other researchers argue that LLMs may excessively rely on external evidence, even when it is counterfactual context synthesized by LLMs~\cite{xie2023adaptive,tan2024blinded}. Additionally, existing research~\cite{kortukovstudying, xie2023adaptive} indicates that LLMs often prioritize contextual cues aligned with their existing memory when confronted with multiple conflicting sources. 

Another line of research addresses knowledge conflicts by improving the external knowledge integration ability of LLMs. Some work focuses on prompt engineering, where context-aware instructions explicitly guide models to prioritize external knowledge over parametric memory~\cite{zhou2023context, wang2023resolving}. Another approach explores training-based adaptations~\cite{xue2023improving,bi2024context}, such as fine-tuning models on knowledge-augmented datasets to enhance contextual grounding answering ability of LLMs during the training phase~\cite{li2022large, fang2023getting,mo2024mitigating,neeman2022disentqa}. Additionally, contrastive decoding strategies amplify differences between context-aware and default outputs~\cite{shi2023trusting,bi2024decoding,jin2024tug}, explicitly reinforcing external knowledge reliance to improve the faithfulness. Different from these works, this paper explores the potential of parameter pruning within the KAG framework to reduce knowledge conflicts while building a smaller model to achieve comparable performance to larger ones with the help of external knowledge~\cite{asai2024reliable,borgeaud2022improving}.
