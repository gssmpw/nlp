
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{./figs/overview.pdf}
    \vspace{-20pt}
    \caption{Overview of watermark insertion and extraction. The Code LLM owner watermarks the code before distributing the snippets to end users. The third-party arbitrator leverages zero-knowledge proofs to verify the ownership without requiring the owner to reveal the encoded watermark.}
    \label{fig:overview}
    \vspace{-15pt}
\end{figure}

The AI-empowered code-generation LLMs, such as GitHub Copilot~\cite{copilot}, Qwen2.5-Coder~\cite{hui2024qwen2}, and Code LLaMA~\cite{roziere2023code}, generate high-quality code via user instructions. They assist software engineers with agile development and reduce production costs~\cite{tan2023copilot,cai2024f}. 
Developing such powerful models requires substantially more effort compared to natural languages, e.g., designing specialized tokenization modules~\cite{li2022competition,roziere2023code} and acquiring high-quality code training data~\cite{lu2021codexglue,puri2021codenet}. Nevertheless, AI-generated code may be used for malicious purposes and raise ethical and legal concerns, e.g., plagiarizing code that violates academic integrity~\cite{cyphert2023generative,tan2024rethinking} and contributing vulnerable code to open-source repositories~\cite{panichella2024vulnerabilities,garg2024coupling}, etc.

Watermarking provides a promising solution to regulate LLM-generated content by embedding invisible signatures onto the code~\cite{huo2024token,liu2024adaptive}. Prior watermarking solutions fall into two approaches: (i) inference-based watermarking and (ii) neural-based watermarking.  Inference-based watermarking~\cite{lee2023wrote,ning2024mcgmark} encodes watermarks by splitting vocabulary into green/red lists on high-entropy tokens and decoding the next token only from the green list. Such methods do not consider the syntactic constraints, which can corrupt the code functionality.
Neural-based watermarking SrcMarker~\cite{yang2024srcmarker} employs a neural network to encode watermarks on both syntactic and variable name feature space for more robust watermarks. The shallow network architecture trained from scratch limits SrcMarker's code feature extraction ability to provide higher watermarking strength and results in reduced detectability.

%Rule-based watermarking maintains a syntactic transformation table~\cite{li2023protecting} to encode watermarks, which adversaries may statistically remove by performing code transformations.

Apart from the watermarking systems' detectability-fidelity-robustness imbalance, existing solutions face practical usability challenges. After disclosing the encoded signatures for third-party verification, code owners need to re-encode new ones to reuse the same code. Due to the code's low-entropy nature, high-quality watermarks that are detectable, fidelity-preserving, and robust are limited. Encoding new signatures may corrupt the code's usability. 

%It addresses two challenges that prior work failed to tackle: 
%(i) Prior code watermarking approaches emphasize on a single dimension of watermarked code functionality-preserving, watermark detectability, and watermark robustness. However, there lacks an approach that optimizes those objectives as a whole while providing high-capacity watermarks; (ii) 



\sys{} leverages an ML/Crypto codesign approach to tackle these challenges and ground the usability of the code watermarking framework. 
It adopts the Seq-to-Seq CodeT5~\cite{wang2021codet5} architecture, pre-trained on millions of high-quality code snippets, as the watermark insertion backbone to extract better code features to fuse with watermarks and improve watermark detectability. A transformer decoder is used for watermark extraction. The watermark encoder and decoder are trained end-to-end to (i) preserve functionality by minimizing the code feature loss between original and watermarked code after syntactic and variable rename transformations; and (ii) ensure detectability and robustness by minimizing the message extraction loss between the encoded signature and the extracted message from both the watermarked and the adversarially modified code.  As such, \sys{} strengthens the detectability-fidelity-robustness tri-objective for better watermarking performance. 


As shown in Figure~\ref{fig:overview}, the trained watermark encoder embeds the owner's signature to the LLM-generated code and distributes the watermarked code to users. If a code snippet is suspected to be LLM-generated, users can submit the code to a third-party arbitrator for  inspection and request the LLM owner input their signature to a zero-knowledge
proof (ZKP) circuit for public verification. The ML/crypto codesign system enables efficient watermark source verification while keeping signatures private.

In brief, our contributions are summarized as follows:

\begin{itemize}
    \item Developing an end-to-end code watermarking framework that balances the detectability-fidelity-robustness tri-objective for high-quality code watermarking. 

    \item Leveraging the first-of-its-kind ML/Crypto codesign to enable secure watermark verification via zero-knowledge proofs. It verifies the code snippet source without revealing the encoded signatures. 
    
    \item Performing evaluations on extensive code benchmarks, demonstrating \sys{} (i) achieves  0.97 detection AUROC while preserving the code functionality and showing resilience against attacks and (ii) efficient in securely verifying the snippet within 120ms using zero-knowledge proofs.
\end{itemize}