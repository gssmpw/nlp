
\input{tabs/main_result} 

We conduct comprehensive experiments to demonstrate: (i) \sys{} maintains balanced detectability-fidelity-robustness triangle in Section~\ref{subsec:wm_perform} and Section~\ref{subsec:robustness}; (ii) \sys{} incurs minimal secure watermark verification overhead via zero-knowledge proofs in Section~\ref{subsec:zkp_perform}.  


\subsection{Experiment Setup}

\textbf{Dataset and Evaluation Metrics} We use HumanEval~\cite{chen2021evaluating}, MBPP~\cite{austin2021program}, and EvalPlus (including both HumanEval+ and MBPP+)~\cite{evalplus} as the target benchmark to evaluate \sys's performance. All of the datasets have instruction prompts for code generation, human-written canonical solutions, and test cases for functionality evaluation. 

%CodeSearchNet~\cite{husain2019codesearchnet} and MBPP~\cite{austin2021program} as the target watermarking dataset. CodeSearchNet-Python~\cite{husain2019codesearchnet} collects the open-source non-fork repositories from GitHub and cleans the dataset for executable functions. The MBPP dataset is a Python programming problems dataset with problem descriptions, code solutions, and test cases. The dataset statistics are in Table~\ref{tab:dataset}.

We assess the watermarked code performance from the following aspects: (i) \textbf{Detectability}: classification metrics (AUROC for area under the receiver operating characteristic curve, TPR for true positive rates, and FPR for false position rates) over watermarked and non-watermarked codes' z-score; (ii) \textbf{Fidelity}: the pass rate (Pass\%)~\cite{chen2021evaluating} of watermarked code. 


\textbf{Baselines}  We compare \sys{} with state-of-the-art natural language watermarking baselines: (1) \textbf{KGW}~\cite{kirchenbauer2023watermark} is an inference-based watermarking scheme for natural language. It encodes watermarks at the LLM decoding stage by splitting the vocabulary into green/red lists and guides the decoding to primarily select tokens from the green list; (2) \textbf{REMARK-LLM}~\cite{zhang2024remark} is a neural-based watermarking scheme for LLM-generated texts. We leverage CodeT5 as the watermark insertion backbone and take both the original code and watermarking signature as input. A watermark extraction module is used to decode the message, which is trained end-to-end with the insertion module to encourage close semantics and successful message extraction.
State-of-the-art code watermarking baselines: (3) \textbf{SWEET}~\cite{lee2023wrote} is an inference-based watermarking scheme for LLM-generated code. It optimizes KGW's decoding by setting an entropy threshold and encodes watermarks only toward high-entropy token decoding; (4) \textbf{SrcMarker}~\cite{yang2024srcmarker}: is a neural-based watermarking scheme for LLM-generated code. It employs shadow transformers for watermark insertion/extraction, where the watermark insertion generates the syntactic and variable rename transformation probabilities. 

\sys{} is pre-trained on CodeSearchNet~\cite{husain2019codesearchnet}, which collects the open-source non-fork repositories from GitHub and cleans the dataset for executable functions. 
For fair comparisons, we pre-train SrcMarker~\cite{yang2024srcmarker} and REMARK-LLM~\cite{zhang2024remark} on the same dataset and report their respective detectability and fidelity performance. 
As SrcMarker does not support watermarking Python code, we train SrcMarker with the same Python syntactic transformations as \sys{} in Appendix~\ref{ap:trans}.
SWEET~\cite{lee2023wrote} encodes watermarks at the inference stage. We thus report Pass\% of the watermarked code whose original one is functional. For both KGW~\cite{kirchenbauer2023watermark} and SWEET~\cite{lee2023wrote}, we use Qwen2.5-Coder-14B~\cite{hui2024qwen2} as the code generation model.

\textbf{Implementation Details} We include more \sys's implementation details in Appendix~\ref{ap:hyper}.



\subsection{Watermark Detectability and Fidelity Performance}~\label{subsec:wm_perform}
The watermarking performance of \sys{} and baselines on HumanEval~\cite{chen2021evaluating}, MBPP~\cite{austin2021program}, and EvalPlus~\cite{evalplus} in Table~\ref{tab:table_main}.
We highlight \sys{} is able to provide high detectability while maintaining code functionality invariant.  


\textbf{Compared to KGW~\cite{kirchenbauer2023reliability}} KGW watermarks LLM-generated code by promoting token decoding from the green list of the vocabulary. We relax the green/red list split ratio $\gamma$ to 0.5 and set the constant $\delta$ added on green tokens' probabilities to 3 to guide watermark insertion on green lists while ensuring Code LLM generates compilable code. However, as some of the low-entropy tokens are sensitive to alterations, restricting the watermark insertion results in an average of 48.69\% pass rate drop to achieve an average of 0.78 AUROC for watermark detection.

\textbf{Compared to REMARK-LLM~\cite{zhang2024remark}} REMARK-LLM  primarily replaces words with their synonyms or changes the sentence syntax for watermark insertion. It was able to learn code semantics by leveraging CodeT5~\cite{wang2021codet5} as the watermark insertion backbone and pre-training on large code datasets. However, it transforms code without syntactic constraints, making the watermarked code non-compilable. As such, while reaching significant detectability, REMARK-LLM has low pass rates that corrupted the watermarked codes' functionality.

\textbf{Compared to SWEET~\cite{lee2023wrote}} SWEET improves over KGW by restricting watermark decoding on high-entropy tokens to maintain both detectability and fidelity. However, encoding watermarks at inference time loses the global view of the code and fewer syntactic transformations can be made to encode the watermark. As such, it weakens the watermarking strength and results in an average of 11.34\% lower AUROC for watermark detection compared to \sys{}. While SWEET avoids watermarking on high-entropy tokens, such watermark insertions without considering code syntactic constraints still result in 10.24\% pass rate drop compared to \sys{}. 


\textbf{Compared to SrcMarker~\cite{yang2023towards}} SrcMarker uses shallow transformers trained from scratch for watermark insertion, which limits its code feature extraction ability.  
\sys{}, on the other hand, leverages CodeT5~\cite{wang2021codet5} as the backbone for better watermark insertion and results in an average of 6.59\% higher AUROC scores and 26.61\% higher TPR among all benchmarks than SrcMarker. Besides, both \sys{} and SrcMarker perform transformations adhering to syntactic constraints and watermarks code with less than 5\% pass rate drop. 

% Leveraging  results in  CodeT5+~\cite{wang2023codet5plus} extracts better code features compared to the watermark insertion modules used in SrcMarker~\cite{yang2023towards}, which improve the infusion with the watermark features from the message extractor $\textbf{R}_m$. As such, \sys{} enhances the detectability of code watermarks.

\subsection{Zero Knowledge Watermark Verification Overhead}~\label{subsec:zkp_perform}
We benchmark our ZK watermark extraction and verification by describing algorithm \ref{alg:extract} in a computational graph that can be easily translated into a zero-knowledge circuit. We highlight that our approach demonstrates virtually no loss in utility. The computation is done using the EZKL framework \cite{ezkl} in conjunction with the Halo2 proof system \cite{halo2_repo}, resulting in a small zk-SNARK proof that any third-party arbitrator can easily verify. Our approach ensures that no information is leaked about the parameters of the linear decoder $\bf{R}_e$ and the watermarking methodology, including the original watermark $M$. The verification overhead is in Table~\ref{tab:zkp}. Generating the zk-SNARK proof for a watermark of 4 bits only takes the prover \Prv \textbf{6.79 seconds}, while only requiring a maximum of approximately \textbf{2.79 GB} of RAM. While this is significantly slower than standard inference, we highlight that this is a fully privacy-preserving solution, and, more importantly, proof generation only has to be done once. This process results in a proof $\pi$ of size \textbf{18.75 KB}, which can be transmitted to as many verifiers as necessary. We benchmarked the verification over 25 examples. This proof can be verified by any verifier \Vrf in an average of \textbf{120 milliseconds}, while only requiring a maximum of approximately \textbf{227.88 MB} of RAM. All the verifier needs to verify a proof is the proof $\pi$ and the verifier key $\mathcal{VK}$, which is only \textbf{511 KB} in our setup. This results in a required communication cost of less than a megabyte. As the proof generation time is amortized due to it only being performed once, \sys{}'s watermark extraction and verification scheme is an extremely communication and runtime-efficient solution that ensures the security of watermarks that are applied to LLM-generated code samples.

\begin{table}[!ht]
    \centering
   
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|c c c c c}
    \toprule
            & Frequency  & Comm. Size & RAM  & Time\\\hline
         Proof Generation & Once & Proof Size: 18.75 KB & 2.79 GB & 6.79s \\
         WM Verification & Every WM & \Vrf Key Size: 511 KB & 227.88 MB & 120ms \\
    \bottomrule
    \end{tabular}}
    \vspace{-5pt}
    \caption{Zero-knowledge Watermark Verification Overhead }
    \vspace{-10pt}
    \label{tab:zkp}
\end{table}

%The only relevant prior work to \sys{}'s ZK watermark verification is ZKROWNN \cite{sheybani2023zkrownn}, which aims to protect watermarks embedded within the deep neural networks themselves, rather than protecting watermarks embedded in data generated by a machine learning model, which is the primary goal of \sys{}. ZKROWNN operates in settings where the watermarked neural network is \textit{public} and also does not require a full feed-forward operation to extract the watermark, as it follows the DeepSigns \cite{darvish2019deepsigns} watermarking methodology. In \sys{}, we require a full feed-forward operation on a model with \textit{private} parameters, making it a much more complex task. Due to these key differences in the intended operational environments, we do not believe that comparing \sys{} to ZKROWNN would be representative of either work.

% To achieve efficient proof generation, we build upon the state-of-the-art framework for ZK machine learning, EZKL \cite{}. Within this framework, we build custom zk-SNARKs with the Halo2 proof system backend \cite{halo2_repo} to ensure that \sys{}'s watermark verification is secure and adds minimal overhead for verifiers. \textcolor{red}{nojan: talk about quantizing to a point where there is still no loss of utility, then things are translated into a computational graph that can be easily arithmetized to zero-knowledge constraints. also maybe add a table or graph that shows how metrics change with num of bits if that isn't set}

%\textcolor{blue}{Proving time: 6.79 seconds, Proof size: 18.75 KB, Verification time: 120 milliseconds}

%\textcolor{red}{@Nojan, add zkp verification results here?}

\subsection{Robustness Evaluations}~\label{subsec:robustness}
As in Section~\ref{sec:threat}, we assume the adversary is an end-user leveraging LLM-generated code for malicious purposes. They avoid being caught the code is machine-generated by removing the encoded signatures. Following SWEET~\cite{lee2023wrote}, we consider two attacks: (1) \textbf{Variable-rename Attack (VA)}: the adversary randomly renames the variable with another word of similar meaning from the vocabulary; (2) \textbf{Refactor Attack (RA)}: the adversary refactors the watermarked code with open-source code LLM. We instruct 
Qwen2.5-Coder-32B-Instruct~\cite{hui2024qwen2} to refactor the code.
The attack performance is evaluated on the MBPP dataset~\cite{austin2021program} with results in Figure~\ref{fig:attack}. We compare \sys{} with code watermarking baselines SrcMarker~\cite{yang2023towards} and SWEET~\cite{lee2023wrote}. 


As seen, \sys{} keeps over 0.93 AUROC under variable-rename attack even after 50\% of the variable names are replaced, whereas SrcMarker and SWEET demonstrate 0.07 and 0.21 lower AUROC. Similarly, \sys{} keeps 0.73 AUROC under refactor attack, demonstrating its resilience toward attacks. 
The robustness primarily comes from two aspects: (i) leveraging CodeT5~\cite{wang2021codet5} with enhanced code feature extraction in $\mathbf{S}_e$ enables robust message recovery and (ii) adversarial training helps the watermark extraction module learns potential malicious transformations and decodes accurate watermark signatures. 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\columnwidth]{./figs/attack.pdf}
    \vspace{-5pt}
    \caption{Robustness evaluation results under Variable-rename Attack (VA) and Refactor Attack (RA).}
    \vspace{-5pt}
    \label{fig:attack}
\end{figure}


\subsection{Ablation Study and Analysis}~\label{subsec:ablation}
This subsection provides details of how different components would impact \sys{} performance and analysis of \sys's capacities. More analysis is in Appendix~\ref{ap:analysis}.

\textbf{Impact of Training Loss Weights} We analyze how different loss weight choices would impact the watermark performance in Table~\ref{tab:loss_weight}. We pre-train \sys{} on the CodeSearchNet~\cite{husain2019codesearchnet} and evaluate the model performance on MBPP~\cite{austin2021program}. As seen, when weighing more on $w_d$ than $w_f$, \sys{} provides higher AUROC and TPR metrics for verifying watermarked code. While a higher $w_r$ results in better robustness against attacks, it results in slightly lower TPR in watermark detection.

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{c|cccc}
    \toprule
     ($w_f$, $w_d$, $w_r$) &  Pass\% & AUROC & TPR & FPR  \\ \hline
     (0.8, 0.1, 0.1)    &  97.74\% & 0.95 & 0.93 & 0.07\\
     (0.1, 0.8, 0.1)    & \textbf{97.74\%} & \textbf{0.97} & \textbf{0.98} & \textbf{0.05}\\
     (0.1, 0.1, 0.8)    &  97.74\% & 0.97 & 0.95 & 0.05\\
     %(1, 1, 1) &  98.15\% & 0.97 & 0.99 & 0.07\\
    \bottomrule
    \end{tabular}
    \caption{Impact of \sys's training loss weights on the MBPP benchmark~\cite{austin2021program} performance.}
    \label{tab:loss_weight}
\end{table}

\textbf{Impact of Decoder $\textbf{S}_{d1}$ and $\textbf{S}_{d2}$} We analyze the impact of training with multiple decoders in Table~\ref{tab:decoder}. We pre-train \sys{} on the CodeSearchNet~\cite{husain2019codesearchnet}, with either $\textbf{S}_{d1}$  or $\textbf{S}_{d2}$, and evaluate the performance on MBPP~\cite{austin2021program}. As seen, encoding watermarks with sole syntactic transformations ($\textbf{S}_{d1}$) or variable name transformations ($\textbf{S}_{d2}$) results in close pass rates but degraded detectability, as less information can be embedded onto the code for watermark insertion. 
Besides, encoding only syntactic transformations results in higher detectability. It primarily because such transformations carry more information for the watermark feature insertion. 

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cc|cccc}
    \toprule
     $\textbf{S}_{d1}$ & $\textbf{S}_{d2}$ &  Pass\% & AUROC & TPR & FPR  \\ \hline
     \cmark & \xmark    & 98.05\% & 0.81 & 0.36 & 0.06\\
     \xmark & \cmark    & \textbf{98.77\%} & 0.73 & 0.24 & \textbf{0.06} \\
     \cmark & \cmark    &  98.15\% & \textbf{0.97} & \textbf{0.99} & 0.07\\
    \bottomrule
    \end{tabular}
    \caption{Impact of \sys's decoders on the MBPP benchmark~\cite{austin2021program} performance.}
    \label{tab:decoder}
\end{table}


\textbf{Watermarked Examples} We show the watermarking examples in Table~\ref{tab:example}, in which the upper code is the original code and the lower one is the watermarked one. As seen, \sys{} will transform the variable names and syntactic structure with meaningful content while maintaining the functionality invariant. For example, Listing~\ref{lst:original}'s two if statements (line 5 and line 6) are merged into one (line 5) in Listing~\ref{lst:wm}. The addition statement (line 4 in Listing~\ref{lst:original} and Listing~\ref{lst:wm}) is also changed from \texttt{\_sum = \_sum + arr[i]} to \texttt{sum += arr[i]}. The variable name is updated from \texttt{\_sum} to \texttt{sum}. Additional examples are in Appendix~\ref{ap:example}.

\begin{figure}[!ht]
    \centering
    \vspace{-20pt}
    \begin{minipage}{0.4\textwidth}
\centering
 \begin{lstlisting}[basicstyle=\ttfamily\tiny,label=lst:original,caption=Original code]
def check_last (arr,n,p):
    _sum = 0
    for i in range(n): 
        _sum = _sum + arr[i] 
        if p == 1: 
            if _sum % 2 == 0: 
                return "ODD"
            else: 
                return "EVEN"
    return "EVEN"
\end{lstlisting}
    \end{minipage}
\\
    \begin{minipage}{0.4\textwidth}
    \vspace{-10pt}
        \centering
        \begin{lstlisting}[basicstyle=\ttfamily\tiny,label=lst:wm,caption=Watermarked code]
def check_last (arr,n,onomies):     
    sum = 0
    for i in range(n): 
        sum += arr[i] 
    if (onomies == 1 and sum % 2 == 0): 
        return "ODD"
    return "EVEN"
        \end{lstlisting}
    \end{minipage}
    \vspace{-5pt}
    \caption{Watermarked example randomly selected from HumanEval~\cite{chen2021codex}. The upper code shows the original code and the lower code shows the watermarked code, where all watermarks are successfully extracted.}
    \vspace{-10pt}
    \label{tab:example}
\end{figure}

\textbf{Watermark Insertion Overhead} The time taken for watermark insertion is in Table~\ref{tab:overhead}, which is the average overhead for encoding signatures onto 50 examples from MBPP~\cite{austin2021program}. \sys{} embeds watermarks 90\% faster than SWEET's inference-based approach, which requires entropy calculation for every token and split vocabulary into green/red lists for high-entropy tokens. Compared to SrcMarker, while \sys{} introduces more complex architectures, the additional watermark insertion overhead is less than 0.01s per sample. As such, \sys's watermark insertion is efficient. 

\begin{table}[!ht]
    \centering
    \small
    \begin{tabular}{c|ccc}
    \toprule
       Method  & SWEET & SrcMarker & \sys{} \\\hline
       Time (s)  & 0.234 & 0.021 & 0.027 \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Watermark insertion overhead for different code watermarking frameworks.}
    \vspace{-5pt}
    \label{tab:overhead}
\end{table} 