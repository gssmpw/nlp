In this section, we introduce the related work and background for code LLMs and watermarking code generated by LLMs.

\subsection{LLM-generated Code Watermarking}
Code large language models are widely applied in the programming language domain to assist software development, e.g., code generation~\cite{copilot,roziere2023code,luo2023wizardcoder}, code translation~\cite{pan2023understanding,roziere2021leveraging}, code summarization~\cite{ahmed2022few,gao2023makes}, automated testing~\cite{yu2023llm,kang2023large}. 
The goal of code LLMs is to learn a probabilistic model $p(c|x)$ that predicts a code snippet $c$ given a context $x$, which can include natural language descriptions, partial code, or other relevant information. Code LLMs are typically trained on large corpora of code data using self-supervised learning objectives to maximize the likelihood of the observed code sequences. Formally, given a dataset $D = {c_1, c_2, ..., c_N}$ of code snippets, the training objective is to minimize the negative log-likelihood, where $p(c_i)$ is modeled by the large language models.

\begin{equation}
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N}\log p(c_i)
\end{equation}


Similar to natural language watermarking, watermarking code data can generally be categorized into three types~\cite{zhang2023remark}: (i) rule-based watermarking, (ii) inference-based watermarking, and, (iii) neural-based watermarking.  
The rule-based watermarking ACW~\cite{li2024resilient} maintains a transformation table and uses the code style transfer as the watermarks. However, the methodology requires additional engineering for new programming languages and exhibits low transferability. The inference-based watermarking SWEET~\cite{lee2023wrote} encodes signatures during the LLM inference stages. By watermarking on the high-entropy tokens, SWEET~\cite{lee2023wrote} maintains the correctness and the executability of the watermarked code. However, it comes at the cost of the lower watermarking strength the framework can provide. The neural-based watermarking SrcMarks~\cite{yang2023towards} leverages an end-to-end learning technique to integrate the watermarking signatures into the codes while maintaining AST-invariant.
However, it falls short of the watermarking capacity it can provide and requires re-training for new programming languages.  

In this paper, we improve upon prior work and build an efficient, high-capacity, and robust end-to-end neural-based watermarking framework that demonstrates high watermarking capacity, cross-programming language transferability, and robustness.

\subsection{Zero-knowledge Proofs}

(ZKPs) are a cryptographic primitive that allows a prover to prove knowledge of a secret value $w$ to a verifier. In a standard ZKP scheme, the prover convinces convinces a verifier that $w$ is a valid input such that $y=\mathcal{C}(x, w)$, in which $C$ is an arbitrary computation and $x$ and $y$ are public inputs and outputs, respectively. Zero-knowledge proof generation can be performed in an interactive or non-interactive manner, depending on the application. One of the main drawbacks of interactive schemes is it limits proofs to \textit{designated-verifier} settings, meaning proof generation, which is the most computationally heavy process in ZKP workflows must be repeated for every new verifier. Non-interactive ZKPs allow for the \textit{publicly verifiable} setting, meaning once a proof is generated attesting correct computation or valid data, it can be verified by any third party. ZKROWNN~\cite{sheybani2023zkrownn} shows the feasibility of non-interactive ZKPs for watermark verification in deep neural networks, requiring only KBs of communication for a user to verify a proof. 

\textcolor{red}{@Nojan, I've copied your texts, do you want to add more?}