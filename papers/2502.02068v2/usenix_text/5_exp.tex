\subsection{Experiment Setup}

\paragraph{Baselines} We compare \sys{} with state-of-the-art code watermarking baselines: (1) \textbf{SWEET}~\cite{lee2023wrote} is an inference-based watermarking scheme for LLM-generated codes; (2) \textbf{SrcMarker}~\cite{yang2024srcmarker}: is a neural-based watermarking scheme for LLM-generated codes; (3) \textbf{REMARK-LLM}~\cite{zhang2024remark} is a neural-based watermarking scheme for LLM-generated texts. 

For fair comparisons, in SrcMarker, we employ the default architecture but use the available transformations in \sys{} to watermark Python code. 
We train REMARK-LLM on the same code dataset as \sys. 
We decode SWEET \textcolor{red}{@Ruisi, add SWEET decode details}
% The decoding of the \textbf{KGW}~\cite{kirchenbauer2023watermark}, \textbf{EXP}~\cite{kuditipudi2023robust}, and \textbf{SWEET} are performed on the CodeLLaMA-13B~\cite{roziere2023code}; The training of the \textbf{AWT}~\cite{abdelnabi21oakland} and \textbf{REMARK-LLM}~\cite{zhang2023remark} are performed on the CodeSearchNet~\cite{husain2019codesearchnet} with the original objectives.


\paragraph{Datasets} We use CodeSearchNet~\cite{husain2019codesearchnet} and MBPP~\cite{austin2021program} as the target watermarking dataset. CodeSearchNet-Python~\cite{husain2019codesearchnet} collects the open-source non-fork repositories from GitHub and cleans the dataset for executable functions. The MBPP dataset is a Python programming problems dataset with problem descriptions, code solutions, and test cases. The dataset statistics are in Table~\ref{tab:dataset}.


\begin{table}[h]
\centering
\small
\begin{tabular}{ccc}
\toprule
Dataset & Training Sample  & Test Sample \\ \midrule
CodeSearchNet-Python~\cite{husain2019codesearchnet} &  &  \\ 
MBPP~\cite{austin2021program} & - & 1000 \\
\bottomrule
\end{tabular}
\caption{Dataset Statistics.\label{tab:dataset}}
\end{table}

\textcolor{red}{@Neusha: add csn-python dataset statistics.}



\paragraph{Metrics} We assess the watermarking performance from the following aspects: (i) \textbf{Watermark Effectiveness}: the inserted signatures shall be successfully extracted as evaluated by the watermark extraction rates (WER); (ii) \textbf{Fidelity}: the watermarked code quality shall not be compromised as evaluated by the pass rates (Pass\%)~\cite{chen2021evaluating} and code semantic coherence by CodeBLEU; (iii) \textbf{Efficiency}: the watermark insertion shall incur minimal time overheads; (iv) \textbf{Robustness}: the watermarked codes shall be resilient toward the watermark removal attacks from the adversaries; (v) \textbf{Stealthiness}: the watermarked codes shall be indistinguishable toward the watermark detection attacks by the adversaries. 

We also evaluate the security verification performance: \textcolor{red}{@Nojan, add zkp metrics here}

\paragraph{Hyperparameters} 

\subsection{Watermarking Performance}
This subsection demonstrates the \sys's capability in maintaining the watermarking effectiveness, fidelity, efficiency, integrity and robustness.  

\subsubsection{Watermarking Effectiveness and Fidelity}
We benchmark \sys's watermarking effectiveness and fidelity against the baselines in Table~\ref{tab:fidelity}. As seen,


\begin{table*}[!ht]
  \centering
 \resizebox{\textwidth}{!}{%
  \begin{tabular}{c|c|ccc|ccc|ccc}
    \toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Methods}  &\multicolumn{3}{c|}{4 bits} & \multicolumn{3}{c|}{8 bits} & \multicolumn{3}{c}{12 bits}\\
  &    &  WER(\%) $\uparrow$ & Pass(\%) $\uparrow$ &CodeBLEU $\uparrow$ &  WER(\%) $\uparrow$ & Pass(\%) $\uparrow$ & CodeBLEU $\uparrow$ &  WER(\%) $\uparrow$ & Pass(\%) $\uparrow$ & CodeBLEU $\uparrow$ \\
    \midrule
 \multirow{4}{*}{CSN-Python}
&  \sys & \\
&  SrcMarker &\\
& SWEET & \\
&  REMARK-LLM &\\
   \midrule 
\multirow{4}{*}{MBPP}
&  \sys & \\
&  SrcMarker &\\
& SWEET & \\
&  REMARK-LLM &\\
    \bottomrule  
  \end{tabular}
  }
  \caption{
  \label{tab:fidelity}}
\end{table*}

\subsubsection{Watermarking Efficiency}
We evaluate the average time and average memory required for watermarking 50 pieces of code randomly selected from the MBPP~\cite{austin2021program} dataset in Table~\ref{tab:efficiency}. 

\begin{table}[!ht]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|cccc}
    \toprule
    Methods & \sys & SrcMarker & SWEET & REMARK-LLM   \\\hline
     Time (ms) & & \\  
     Memory (GiB) & & \\  
    \bottomrule
    \end{tabular}}
    \caption{Watermarking efficiency of different methods.}
    \label{tab:efficiency}
\end{table}

\subsubsection{Watermarking Integrity}
To keep the watermarking integrity, \sys{} shall only provide ownership proof for the watermarked code and not validate the non-watermarked ones. The watermark extraction rates on non-watermarked codes are in Table~\ref{tab:integraity}.

\begin{table}[!ht]
    \centering
    \small
    \begin{tabular}{c|cc}
    \toprule
    Dataset & CodeSearchNet-Python & MBPP \\\hline
    WER &  \\
    \bottomrule
    \end{tabular}
    \caption{Watermarking integrity evaluation.}
    \label{tab:integraity}
\end{table}


\subsubsection{Watermarking Robustness} 
We evaluate \sys{} and the baseline's robustness under watermark removal attacks in Table~\ref{}. We design the following attacks aiming to remove the signatures~\cite{yang2024srcmarker,lee2023wrote}: (i) Variable re-naming attack, where the variables are randomly renamed to another string; (2) Re-watermark attack, where the adversary has access to \sys's architecture but the training data and hyperparameters are beyond his/her reach. The adversary encodes malicious watermarks on the watermarked code pieces for signature removal; (3) Code paraphrase attack, where the code is rewrote by Code Llama with prompt ``Can you rewrite the code'' for signature removal. 

\subsubsection{Watermarking Stealthiness}
The stealthiness of the watermarked code is evaluated by executing the following watermark detection attacks: (i) Syntax Distribution: the adversary detects if the code piece is watermarked or not by analyzing the code syntax distributions; (ii) Machine Learning-based Detection: the adversary trains a codeBERT model~\cite{feng2020codebert} to determine if the suspect code is watermarked or not.


zj

\subsection{Analysis and Ablation Study}
This subsection provides additional analysis on \sys's code watermarking performance and ablation studies on how different components impact \sys. Here we use t

\subsubsection{Watermarking Examples}
We show the original code and the corresponding code watermarked by \sys{} in Table~\ref{}. The differences are highlighted in yellow. 


\subsubsection{The Effectiveness of Backbone Models} 
We study how different variable-renaming backbones would impact the 


\subsubsection{Masking percentage and Gumbel temperatures}