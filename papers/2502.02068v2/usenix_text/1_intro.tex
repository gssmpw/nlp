% What is the problem
The AI-powered code-generation Large Language Models (LLMs), such as GitHub Copilot~\cite{copilot}, OpenAI CodeX~\cite{chen2021evaluating}, and Code LLaMA~\cite{roziere2023code}, generate accurate and bug-free codes via human-language instructions. The LLMs demonstrate state-of-the-art performance on various coding-related tasks, including code generation~\cite{copilot,roziere2023code,luo2023wizardcoder}, code translation~\cite{pan2023understanding,roziere2021leveraging}, code summarization~\cite{ahmed2022few,gao2023makes}, etc. They assist engineers with agile development and reduce production costs~\cite{tan2023copilot}. 
Developing such powerful models requires substantially more effort compared to natural languages, e.g., designing specialized tokenization modules, better training algorithms to improve reasoning ability, and acquiring high-quality code training data, constituting valuable IP to protect.  

Watermarking serves as a promising approach to trace the distribution of the LLM-generated code and protect the copyrights of the LLM owners. Before the code is sent to end-users, the LLM owner encodes signatures onto the code snippet. To verify the code snippet copyrights, the LLM owners disclose their encoded signatures and the watermarking network parameters to the third-party arbitrators for fair watermark extraction and legal verification~\cite{zhang2024remark}. 

% What has been done
Adding watermarks in LLM-generated code can be methodologically categorized into~\cite{zhang2024remark}: (i) rule-based watermarking~\cite{li2023protecting} that maintains a code transformation table, in which each signature bit represents a type of transformation. Owners extract available transformations can be applied to the original code and map them to the watermarked one via watermark signature instructions. However, such methods may not be robust against watermark removal attacks, in which adversaries can systemically refactor the code for signature removal; (ii) inference-based watermarking~\cite{lee2023wrote} that splits vocabulary into green/red lists on low-entropy tokens and restricts the LLM decoding to predict the next tokens from the green list. They process the watermark insertion per token instead of per segment. As such, the watermark insertion could undermine the code functionality; (iii) neural-based watermarking SrcMarks~\cite{yang2023towards}, which leverages an end-to-end learning technique to integrate the watermarking signatures into the codes while maintaining code functionalities. Such methods, however, demonstrated lower watermarking capacity for strong ownership proof. 

% What are the challenges
Hence, prior work failed to tackle several challenges. First of all, 
at the watermark insertion stage, the framework shall keep high performance over the required criteria in watermarking, e.g., functionality-preserving, robustness, efficiency, and high-capacity. 
Besides, during the watermark verification stage, the encoded signatures are exposed to third-party arbitrators for ownership proof. The owners need to encode another set of signatures for code data reuse~\cite{sheybani2023zkrownn}. Due to the low-entropy nature of code data~\cite{lee2023wrote}, finding another set of signatures and re-transforming the original code could undermine the code's functionality. Hence, secure verification approaches are required to prove ownership without leaking confidential watermarking information. 

% What is our solution
This paper presents \sys, a secure watermarking framework for code generated by LLMs. The watermark insertion module takes the LLM-generated code and the watermark message as input. 
Due to the perturbation-sensitive nature of code, \sys{} disentangles code transformations and variable renamings in code watermarking~\cite{yang2024srcmarker} to ensure the signatures are successfully encoded while preserving the functionality. The insertion module uses a CodeT5~\cite{wang2021codet5} Seq-to-Seq model as the backbone and outputs the watermarked code with embedded signatures. The watermark extraction module leverages a transformer-based decoder to decode the signature from the watermarked code and enable third-party arbitrators to verify the LLM owner's ownership. The two parts are jointly trained to 

In brief, our contributions are summarized as follows:
\begin{itemize}
    \item We introduce \sys, the first-of-its-kind secure watermarking framework for LLM-generated code data. It maintains the  
    
    \item \sys{} highlights several key features: (i) In the watermark insertion stage, \sys{} maintains the code functionality, demonstrates high watermarking capacity, and shows robustness against potential attacks; (ii) During watermark extraction, \sys{} employs crypto-based zero-knowledge proofs for secure and publicly verifiable watermark extraction.
    
    \item  We perform extensive evaluations of \sys{} on multiple code benchmarks: (i) \sys{} can embed XX 
     
\end{itemize}


\textbf{Paper Organization} The rest of the paper is organized as follows: 