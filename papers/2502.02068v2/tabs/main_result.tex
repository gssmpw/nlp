\begin{table*}[th]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}crlcccccccccccccc@{}}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Method}}} &
   &
  \multicolumn{4}{c}{\textbf{\textsc{HumanEval}~\cite{chen2021evaluating}}} &
  \textbf{} &
  \multicolumn{4}{c}{\textbf{\textsc{MBPP}~\cite{austin2021program}}} &
   &
  \multicolumn{4}{c}{\textbf{\textsc{EvalPlus}~\cite{evalplus}}} 
 
  \\ \cmidrule(lr){4-7} \cmidrule(lr){9-12} \cmidrule(lr){14-17}
\multicolumn{2}{c}{}                                                  &  & Pass\% & AUROC & TPR & FPR &  &  Pass\% & AUROC & TPR & FPR &  & Pass\% & AUROC & TPR & FPR \\ \midrule
\multicolumn{17}{l}{\textbf{\textit{Natural Language}}}     \\
& KWG &  & 42.62\%& 0.82 & 0.56 & \underline{0.03} & & 57.30\% & 0.78 & 0.43 & \textbf{0.03}  & & 54.02\% & 0.73 & 0.35 & \underline{0.05} \\
& REMARK-LLM & & \textcolor{failed}{0\%} & \textcolor{failed}{0.97} & \textcolor{failed}{0.88} & \textcolor{failed}{0.04} & & \textcolor{failed}{0\%}  & \textcolor{failed}{0.98} & \textcolor{failed}{0.89} & \textcolor{failed}{0.05}  & & \textcolor{failed}{0\%} & \textcolor{failed}{0.98} & \textcolor{failed}{0.96} &  \textcolor{failed}{0.05} \\ \midrule
\multicolumn{17}{l}{\textbf{\textit{Code}}}     \\
& SWEET &  & 82.53\% & 0.87 & 0.59 & \textbf{0.02} & & 90.00\% & 0.86 & 0.47 & 0.05  & & 84.90\% &  0.86 &  0.52 & \textbf{0.04}\\ 
& SrcMarker && \underline{95.12\%} & \underline{0.90}&  \underline{0.76} & 0.07 && \textbf{97.95\%} &  \underline{0.91} &   \underline{0.76} & 0.06 && \textbf{95.57\%} & \underline{0.92} & \underline{0.81} & 0.07  \\
& \sys{} && \textbf{95.12\%} & \textbf{0.97} & \textbf{0.98} & 0.06 && \underline{97.64\%} & \textbf{0.97} & \textbf{0.99} & \underline{0.05} && \underline{95.39\%} & \textbf{0.97} & \textbf{0.98} & 0.06  \\\bottomrule

\end{tabular}%

}
\vspace{-10pt}
\caption{ \sys{} performance on watermarking HumanEval~\cite{chen2021evaluating}, MBPP~\cite{austin2021program}, and DS-1000~\cite{Lai2022DS1000} datasets when comparing with natural language watermarking KWG~\cite{kirchenbauer2023watermark} and REMARK-LLM~\cite{zhang2024remark}; code watermarking SWEET~\cite{lee2023wrote} and SrcMarker~\cite{yang2023towards}.  The best metric values are highlighted in \textbf{bold} text, the second best metric values are \underline{underlined}, and
\textcolor{failed}{grey} means failed watermark insertion (0\% pass rate).}
\vspace{-10pt}
\label{tab:table_main}
\end{table*}

%Detection-based baselines classify human-written source code and LLM-generated source code. Human-written source code is a solution code in HumanEval, MBPP dataset. Prompt(which is a function name and docstring in HumanEval, and sentence instruction in MBPP) of each dataset is fed to LLM to generate a source code. Vanilla watermarking and \myalgo embed a watermark to generated code and perform a watermark detection between human-written source code and watermark-embedded LLM-generated source code. 