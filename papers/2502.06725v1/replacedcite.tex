\section{Related Works}
Due to the overwhelming complexities of drone navigation in dynamic environments, many studies have been trying to solve the general applicability of algorithms either by using machine learning or classical methods. For example, in study ____, the policy was learnt with high uncertainty and tested in a noisy real environment. The results showed effective drone navigation in reality, but the method is applicable in 2D space only and not tested in dynamic conditions. 

In study ____, DRL is utilised to avoid moving and stationary obstacles consisting of images and several scalars with Joint Neural Network (JNN). Liu et al., in research ____ use a motion planning framework that integrates visibility path searching to generate collision-free paths and RL to generate low-level motion commands. Moreover, CPU-based trained DRL for UAV to UAV tracking is introduced in ____. The model was trained using PPO and showed considerable results. Another PPO-based learnt RL agent in study ____ was used in a racing environment as a path planner and utilized conventional Proportional–Integral–Derivative (PID) controller for controlling the motion. Moreover, Song et al. research ____ generates smooth collision-free trajectories while taking care of an unseen environment by incorporating vision with DRL. While these mentioned studies show promising capabilities for motion generation and obstacle avoidance, they have predominantly been tested in simulation environments only with less challenging scenarios.

Furthermore, in study ____ highly efficient path planner was devised for cluttered environments incorporating some extent of dynamical feasibility by adjusting time allocation based upon spatial-temporal joint optimisation. Moreover, study ____ introduced a search-based method by using a multi-stage training approach and testing the agent on real air-ground unmanned system. In ____, DRL was trained for random obstacles in dense urban environment by utilising Double Deep-Q network (DDQP) while enhancing the training stability of algorithms. In addition, study ____ improves safe navigation in obstacle avoidance by leveraging map parameterisation and low-cost planning whereas the results are validated in real-life experiments. Peter et. al in study ____ trained models using Twin-Delayed Deep Deterministic (TD3) policy gradient algorithm and real life experiments showed remarkable landing performance of drones for dynamically moving platforms in the presence of downwash and wind turbulence. The extension of these works using PPO has also been successfully implemented for multi-agents ____. However, these approaches focus purely on path planning without considering obstacles and have only been tested on tiny-sized drones, therefore, overall restricting the high velocity potential of medium-sized drones. Lastly, another study ____ introduced OmniRace, a control interface based on drone velocity manipulation for drones of all sizes. However, its motion planning depends entirely on human input, which limits both navigation accuracy and autonomy.

In order to address the mentioned gaps, this research presents a novel DRL-based motion planning framework that enables velocity-based trajectory generation for any size drone due to its nature of being a model-free learning agent. Unlike previous studies that rely on static environment, our approach is designed for dynamic scenarios, ensuring robust adaptabiltiy to rapidly changing surroundings. The training environment used is Gym PyBullet ____ and carefully customised and tuned to incorporate real-world complexities for a seamless transition from simulation to reality. Furthermore, this framework accounts for dynamically evolving obstacles, allowing the drone to make intelligent decisions in real time while balancing agility and safety as required.