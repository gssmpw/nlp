@INPROCEEDINGS{panerati2021learning,
      title={Learning to Fly---a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control}, 
      author={Jacopo Panerati and Hehui Zheng and SiQi Zhou and James Xu and Amanda Prorok and Angela P. Schoellig},
      booktitle={Proc. 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
      year={2021},
      volume={},
      number={},
      pages={7512-7519},
}

@INPROCEEDINGS{SmoothTrajectoryCollision10069287,
  author={Song, Sirui and Saunders, Kirk and Yue, Ye and Liu, Jundong},
  booktitle={2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Smooth Trajectory Collision Avoidance through Deep Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={914-919},
  keywords={Deep learning;Technological innovation;Navigation;Reinforcement learning;Robustness;Trajectory;Organ transplantation;Deep reinforcement learning;collision avoidance;UAV;smoothness;rewards},
  doi={10.1109/ICMLA55696.2022.00152}
}


@Article{DRLendtoendbiomimetics7040197,
AUTHOR = {Zhao, Jiang and Liu, Han and Sun, Jiaming and Wu, Kun and Cai, Zhihao and Ma, Yan and Wang, Yingxun},
TITLE = {Deep Reinforcement Learning-Based End-to-End Control for UAV Dynamic Target Tracking},
JOURNAL = {Biomimetics},
VOLUME = {7},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {197},
URL = {https://www.mdpi.com/2313-7673/7/4/197},
PubMedID = {36412725},
ISSN = {2313-7673},
ABSTRACT = {Uncertainty of target motion, limited perception ability of onboard cameras, and constrained control have brought new challenges to unmanned aerial vehicle (UAV) dynamic target tracking control. In virtue of the powerful fitting ability and learning ability of the neural network, this paper proposes a new deep reinforcement learning (DRL)-based end-to-end control method for UAV dynamic target tracking. Firstly, a DRL-based framework using onboard camera image is established, which simplifies the traditional modularization paradigm. Secondly, neural network architecture, reward functions, and soft actor-critic (SAC)-based speed command perception algorithm are designed to train the policy network. The output of the policy network is denormalized and directly used as speed control command, which realizes the UAV dynamic target tracking. Finally, the feasibility of the proposed end-to-end control method is demonstrated by numerical simulation. The results show that the proposed DRL-based framework is feasible to simplify the traditional modularization paradigm. The UAV can track the dynamic target with rapidly changing of speed and direction.},
DOI = {10.3390/biomimetics7040197}
}

@article{AirsimKrishnan2021,
  author    = {Srivatsan Krishnan and Behzad Boroujerdian and William Fu and Aleksandra Faust and Vijay Janapa Reddi},
  title     = {Air Learning: a deep reinforcement learning gym for autonomous aerial robot visual navigation},
  journal   = {Machine Learning},
  volume    = {110},
  number    = {9},
  pages     = {2501--2540},
  year      = {2021},
  month     = {Sep},
  doi       = {10.1007/s10994-021-06006-6},
  url       = {https://doi.org/10.1007/s10994-021-06006-6},
  issn      = {1573-0565}
}

@INPROCEEDINGS{Sim2real10553074,
  author={Joshi, Bhaskar and Kapur, Dhruv and Kandath, Harikumar},
  booktitle={2024 10th International Conference on Automation, Robotics and Applications (ICARA)}, 
  title={Sim-to-Real Deep Reinforcement Learning Based Obstacle Avoidance for UAVs Under Measurement Uncertainty}, 
  year={2024},
  volume={},
  number={},
  pages={278-284},
  keywords={Training;Navigation;Noise;Measurement uncertainty;Autonomous aerial vehicles;Deep reinforcement learning;Trajectory;Autonomous navigation;deep reinforcement learning;measurement noise;obstacle avoidance;proximal policy optimization;unmanned aerial vehicle},
  doi={10.1109/ICARA60736.2024.10553074}
}


@mis{tornadodronebioinspireddrlbaseddrone,
      title={TornadoDrone: Bio-inspired DRL-based Drone Landing on 6D Platform with Wind Force Disturbances}, 
      author={Robinroy Peter and Lavanya Ratnabala and Demetros Aschu and Aleksey Fedoseev and Dzmitry Tsetserukou},
      year={2024},
      eprint={2406.16164},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2406.16164}, 
}


@INPROCEEDINGS{landeraiadaptivelandingbehavior,
  author={Peter, Robinroy and Ratnabala, Lavanya and Aschu, Demetros and Fedoseev, Aleksey and Tsetserukou, Dzmitry},
  booktitle={2024 International Conference on Unmanned Aircraft Systems (ICUAS)}, 
  title={Lander.AI: DRL-based Autonomous Drone Landing on Moving 3D Surface in the Presence of Aerodynamic Disturbances}, 
  year={2024},
  volume={},
  number={},
  pages={295-300},
  keywords={Three-dimensional displays;Navigation;System performance;Aerodynamics;Robustness;Safety;PD control;Autonomous Drone Landing;Deep Reinforcement Learning;Aerodynamic Disturbance},
  doi={10.1109/ICUAS60882.2024.10556835}}


@misc{swarmpathdroneswarmnavigation,
      title={SwarmPath: Drone Swarm Navigation through Cluttered Environments Leveraging Artificial Potential Field and Impedance Control}, 
      author={Roohan Ahmed Khan and Malaika Zafar and Amber Batool and Aleksey Fedoseev and Dzmitry Tsetserukou},
      year={2024},
      eprint={2410.07848},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.07848}, 
}

@article{PPOtrackingTAN2023101497,
title = {A new approach for drone tracking with drone using Proximal Policy Optimization based distributed deep reinforcement learning},
journal = {SoftwareX},
volume = {23},
pages = {101497},
year = {2023},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2023.101497},
url = {https://www.sciencedirect.com/science/article/pii/S2352711023001930},
author = {Ziya Tan and Mehmet Karaköse},
keywords = {Distributed learning, Drone tracking, Reinforcement learning, Proximal Policy Optimization},
abstract = {In this paper, a distributed deep reinforcement learning algorithm based on Proximal Policy Optimization (PPO) is proposed for an unmanned aerial vehicle (UAV) to autonomously track another UAV. Accordingly, this paper makes three important contributions to the literature. The first one is the development of an efficient UAV tracking algorithm, the second one is the presentation of a deep reinforcement learning approach that can be adapted to the problem, and the third one is the introduction of a generalized distributed deep reinforcement learning platform that can be used in various problems such as tracking, control and mission coordination of UAVs. In order to validate the developed approaches, the PPO algorithm is simulated with the deep reinforcement learning algorithm in a distributed and non-distributed manner, a follower UAV is trained in different scenarios and the distributed and non-distributed performances of the training using CPU are obtained, scenarios using general and adaptive learning algorithms are given, and finally, the performances of the algorithms developed in the paper are presented explicitly.}
}

@inproceedings{Beautybeastinproceedings,
author = {Kaufmann, Elia and Gehrig, Mathias and Foehn, Philipp and Ranftl, Rene and Dosovitskiy, Alexey and Koltun, Vladlen and Scaramuzza, Davide},
year = {2019},
month = {05},
pages = {690-696},
booktitle = {},
title = {Beauty and the Beast: Optimal Methods Meet Learning for Drone Racing},
doi = {10.1109/ICRA.2019.8793631}
}

@article{ChampiondroneKaufmann2023,
  author    = {Elia Kaufmann and Leonard Bauersfeld and Antonio Loquercio and Matthias M{\"u}ller and Vladlen Koltun and Davide Scaramuzza},
  title     = {Champion-level drone racing using deep reinforcement learning},
  journal   = {Nature},
  volume    = {620},
  number    = {7976},
  pages     = {982--987},
  year      = {2023},
  month     = {Aug},
  doi       = {10.1038/s41586-023-06419-4},
  url       = {https://doi.org/10.1038/s41586-023-06419-4},
  issn      = {1476-4687}
}

@inproceedings{DRLracing10.1109/IROS51168.2021.9636053,
  author={Song, Yunlong and Steinweg, Mats and Kaufmann, Elia and Scaramuzza, Davide},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Autonomous Drone Racing with Deep Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1205-1212},
  keywords={Training;Scalability;Layout;Reinforcement learning;Logic gates;Trajectory;Planning},
  doi={10.1109/IROS51168.2021.9636053}}


@INPROCEEDINGS{omnirace6dhandpose,
  author={Serpiva, Valerii and Fedoseev, Aleksey and Karaf, Sausar and Abdulkarim, Ali Alridha and Tsetserukou, Dzmitry},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={OmniRace: 6D Hand Pose Estimation for Intuitive Guidance of Racing Drone}, 
  year={2024},
  volume={},
  number={},
  pages={2508-2513},
  keywords={Hands;Machine learning algorithms;Tracking;Navigation;Source coding;Pose estimation;Artificial neural networks;Real-time systems;Intelligent robots;Drones},
  doi={10.1109/IROS58592.2024.10801907}}


@INPROCEEDINGS{marlanderlocalpathplanning,
  author={Aschu, Demetros and Peter, Robinroy and Karaf, Sausar and Fedoseev, Aleksey and Tsetserukou, Dzmitry},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={MARLander: A Local Path Planning for Drone Swarms using Multiagent Deep Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={2943-2948},
  keywords={Training;Location awareness;Scalability;Deep reinforcement learning;Path planning;Safety;Planning;PD control;Drones;Logistics;Swarm of Drones;Multi-agent system;Deep Reinforcement Learning;Collision Avoidances;Planner},
  doi={10.1109/SMC54092.2024.10831294}}


@INPROCEEDINGS{LongtermDRL9259811,
  author={Ates, Ugurkan},
  booktitle={2020 Innovations in Intelligent Systems and Applications Conference (ASYU)}, 
  title={Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Drones;Task analysis;Training;Robots;Planning;Reinforcement learning;Neural networks;Deep Reinforcement Learning;Path Planning;Machine Learning;Drone Racing},
  doi={10.1109/ASYU50717.2020.9259811}
}

@INPROCEEDINGS{NavigateObstacleDRL9081749,
  author={Çetin, Ender and Barrado, Cristina and Muñoz, Guillem and Macias, Miquel and Pastor, Enric},
  booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)}, 
  title={Drone Navigation and Avoidance of Obstacles Through Deep Reinforcement Learning}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  keywords={Drones;UAV;Deep Reinforcement Learning;Q-Network;DDQN;JNN},
  doi={10.1109/DASC43569.2019.9081749}}

@misc{searchtocontrolreinforcementlearningbased,
      title={A Search-to-Control Reinforcement Learning Based Framework for Quadrotor Local Planning in Dense Environments}, 
      author={Zhaohong Liu and Wenxuan Gao and Yinshuai Sun and Peng Dong},
      year={2025},
      eprint={2408.00275},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={arXiv:2408.00275}, 
}

@misc{ArduPilot,
  author    = {{ArduPilot Community}},
  title     = {ArduPilot: Open Source Autopilot},
  year      = {2025},
  url       = {http://www.ardupilot.org/},
  note      = {Accessed: 2025-01-29}
}

@misc{realsense,
      title={Intel RealSense Stereoscopic Depth Cameras}, 
      author={Leonid Keselman and John Iselin Woodfill and Anders Grunnet-Jepsen and Achintya Bhowmik},
      year={2017},
      eprint={1705.05548},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1705.05548}, 
}

@INPROCEEDINGS{adaptiveapf,
  author={Amiryan, Javad and Jamzad, Mansour},
  booktitle={2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM)}, 
  title={Adaptive motion planning with artificial potential fields using a prior path}, 
  year={2015},
  volume={},
  number={},
  pages={731-736},
  keywords={Planning;Force;Motion segmentation;Vehicles;Path planning;Mobile robots;Artificial Potential Fields;Autonomous vehicle;Evolutionary Algorithms;Random Sampling;Reactive Motion Planning},
  doi={10.1109/ICRoM.2015.7367873}}

@article{uavapf,
author = {Jayaweera, Herath and Hanoun, Samer},
year = {2020},
month = {01},
pages = {192760-192776},
title = {A Dynamic Artificial Potential Field (D-APF) UAV Path Planning Technique for following Ground Moving Targets},
volume = {8},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2020.3032929}
}

@article{
flyingswarm,
author = {Xin Zhou  and Xiangyong Wen  and Zhepei Wang  and Yuman Gao  and Haojia Li  and Qianhao Wang  and Tiankai Yang  and Haojian Lu  and Yanjun Cao  and Chao Xu  and Fei Gao },
title = {Swarm of micro flying robots in the wild},
journal = {Science Robotics},
volume = {7},
number = {66},
pages = {eabm5954},
year = {2022},
doi = {10.1126/scirobotics.abm5954},
URL = {https://www.science.org/doi/abs/10.1126/scirobotics.abm5954},
eprint = {https://www.science.org/doi/pdf/10.1126/scirobotics.abm5954},
abstract = {Aerial robots are widely deployed, but highly cluttered environments such as dense forests remain inaccessible to drones and even more so to swarms of drones. In these scenarios, previously unknown surroundings and narrow corridors combined with requirements of swarm coordination can create challenges. To enable swarm navigation in the wild, we develop miniature but fully autonomous drones with a trajectory planner that can function in a timely and accurate manner based on limited information from onboard sensors. The planning problem satisfies various task requirements including flight efficiency, obstacle avoidance, and inter-robot collision avoidance, dynamical feasibility, swarm coordination, and so on, thus realizing an extensible planner. Furthermore, the proposed planner deforms trajectory shapes and adjusts time allocation synchronously based on spatial-temporal joint optimization. A high-quality trajectory thus can be obtained after exhaustively exploiting the solution space within only a few milliseconds, even in the most constrained environment. The planner is finally integrated into the developed palm-sized swarm platform with onboard perception, localization, and control. Benchmark comparisons validate the superior performance of the planner in trajectory quality and computing time. Various real-world field experiments demonstrate the extensibility of our system. Our approach evolves aerial robotics in three aspects: capability of cluttered environment navigation, extensibility to diverse task requirements, and coordination as a swarm without external facilities. A fully autonomous swarm composed of palm-sized drones with versatile task extensibility in the wild is realized.}}


@Article{searchbasedapp13042244,
AUTHOR = {Chen, Xiaohui and Qi, Yuhua and Yin, Yizhen and Chen, Yidong and Liu, Li and Chen, Hongbo},
TITLE = {A Multi-Stage Deep Reinforcement Learning with Search-Based Optimization for Air–Ground Unmanned System Navigation},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {2244},
URL = {https://www.mdpi.com/2076-3417/13/4/2244},
ISSN = {2076-3417},
ABSTRACT = {An important challenge for air–ground unmanned systems achieving autonomy is navigation, which is essential for them to accomplish various tasks in unknown environments. This paper proposes an end-to-end framework for solving air–ground unmanned system navigation using deep reinforcement learning (DRL) while optimizing by using a priori information from search-based path planning methods, which we call search-based optimizing DRL (SO-DRL) for the air–ground unmanned system. SO-DRL enables agents, i.e., an unmanned aerial vehicle (UAV) or an unmanned ground vehicle (UGV) to move to a given target in a completely unknown environment using only Lidar, without additional mapping or global planning. Our framework is equipped with Deep Deterministic Policy Gradient (DDPG), an actor–critic-based reinforcement learning algorithm, to input the agents’ state and laser scan measurements into the network and map them to continuous motion control. SO-DRL draws on current excellent search-based algorithms to demonstrate path planning and calculate rewards for its behavior. The demonstrated strategies are replayed in an experienced pool along with the autonomously trained strategies according to their priority. We use a multi-stage training approach based on course learning to train SO-DRL on the 3D simulator Gazebo and verify the robustness and success of the algorithm using new test environments for path planning in unknown environments. The experimental results show that SO-DRL can achieve faster algorithm convergence and a higher success rate. We piggybacked SO-DRL directly onto a real air–ground unmanned system, and SO-DRL can guide a UAV or UGV for navigation without adjusting any networks.},
DOI = {10.3390/app13042244}
}


@Article{denseurbanen17112762,
AUTHOR = {Zhu, Yanfei and Tan, Yingjie and Chen, Yongfa and Chen, Liudan and Lee, Kwang Y.},
TITLE = {UAV Path Planning Based on Random Obstacle Training and Linear Soft Update of DRL in Dense Urban Environment},
JOURNAL = {Energies},
VOLUME = {17},
YEAR = {2024},
NUMBER = {11},
ARTICLE-NUMBER = {2762},
URL = {https://www.mdpi.com/1996-1073/17/11/2762},
ISSN = {1996-1073},
ABSTRACT = {The three-dimensional (3D) path planning problem of an Unmanned Aerial Vehicle (UAV) considering the effect of environmental wind in a dense city is investigated in this paper. The mission of the UAV is to fly from its initial position to its destination while ensuring safe flight. The dense obstacle avoidance and the energy consumption in 3D space need to be considered during the mission, which are often ignored in common studies. To solve these problems, an improved Deep Reinforcement Learning (DRL) path planning algorithm based on Double Deep Q-Network (DDQN) is proposed in this paper. Among the algorithms, the random obstacle training method is first proposed to make the algorithm consider various flight scenarios more globally and comprehensively and improve the algorithm’s robustness and adaptability. Then, the linear soft update strategy is employed to realize the smooth neural network parameter update, which enhances the stability and convergence of the training. In addition, the wind disturbances are integrated into the energy consumption model and reward function, which can effectively describe the wind disturbances during the UAV mission to achieve the minimum drag flight. To prevent the neural network from interfering with training failures, the meritocracy mechanism is proposed to enhance the algorithm’s stability. The effectiveness and applicability of the proposed method are verified through simulation analysis and comparative studies. The UAV based on this algorithm has good autonomy and adaptability, which provides a new way to solve the UAV path planning problem in dense urban scenes.},
DOI = {10.3390/en17112762}
}


@article{mapparams,
	title = {{DRL}-based {Path} {Planner} and its {Application} in {Real} {Quadrotor} with {LIDAR}},
	volume = {107},
	issn = {1573-0409},
	url = {https://doi.org/10.1007/s10846-023-01819-0},
	doi = {10.1007/s10846-023-01819-0},
	abstract = {The distribution mismatching issue has been hindering the landing of deep reinforcement learning algorithms in the robot field for a long time. This paper proposes a novel DRL-based path planner and corresponding training method to realize the safe obstacle avoidance of real quadrotors. To achieve the goal, we design a randomized environment generation module to fit the reality-simulation error. Then the map information can be parameterized to make the test data statistically significant. In addition, an instruction filter is proposed to smooth the output of the policy network in the test phase. Its improvement in obstacle avoidance performance is demonstrated in the experiment section. Finally, real-time flight experiments are conducted to verify the effectiveness of our algorithm and prove that the learning-based path planner can solve practical problems in the robot field. Our framework has three advantages: (1) map parameterization, (2) low-cost planning, and (3) reality validation. The video and code are available: https://github.com/Vinson-sheep/multi\_rotor\_avoidance\_rl.},
	number = {3},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Yang, Yongsheng and Hou, Zhiwei and Chen, Hongbo and Lu, Peng},
	month = mar,
	year = {2023},
	pages = {38},
}





