% 简要Method：
% 介绍为什么hidden states差异很小，上一个模型的hidden states对下一个模型来说只是包含了几个噪声token的hidden states

% 单轮
% 首个模型的训练方式与prompt tuning没有大的区别，小区别在于：
% learnable p  refix和input all（所有模型都需要考虑的输入）调换位置，防止input condition on prefix，没法直接输入后一个模型。

% 后续模型实现方式有两种：1. 保存前置模型A已经计算好的hidden states，微调后置模型B时，离线加载A的hidden states，开始微调。2. 离线加载可能带来巨大的存储需求以及访问存储开销，采用在线重新计算A的Hidden states的方式，只需要略微调整一下B的attention mask

% 多轮
% 相互适应，必须同时训练。同样修改attention mask就可以做到

% inference算法
% 1. 简要介绍为什么位置编码不影响计算结果。因为相对位置编码和绝对位置无关。
% 2. latex算法描述
\begin{figure*}[t] 
\centerline{
\includegraphics[width=1.\textwidth]{single-turn.pdf}} 
\caption{ 
An example of fine-tuning model B in the model chain A → B. For simplicity, the unique inputs of model A and model B are omitted. \textbf{Left:} Offline fine-tuning, where the output KV hidden states of fully trained model A are stored and used as input for model B. \textbf{Middle:} Online, where the output KV hidden states of model A are recalculated in memory. \textbf{Right:} We calculate the output KV hidden states of model A in memory and fine-tune model B by adjusting the attention mask for each layer. We use the online training strategy in practical applications.
} \label{fig:single-turn} 
\end{figure*}


In this section, 
% we elaborate on streamlining the chain of models. 
we begin by highlighting a key challenge: model chains rely on text-based communication, which prevents the direct transfer of KV hidden states between models. We then explore the feasibility of fine-tuning the downstream model to process KV hidden states from the upstream model, although these hidden states often include noise tokens irrelevant to the downstream task.
% fine-tuning can enable the model to perform effectively despite this noise. 
Lastly, we propose training strategies, FTHSS, to achieve KV hidden state sharing. 
% (Fine-Tuning for Hidden State Sharing)

% These strategies include modifying inputs and attention masks to support both single- and multi-round scenarios.


\subsection{Preliminary}
Multiple models $ M_1, M_2, \ldots, M_n $ often collaborate sequentially in RAG and agent-based tasks, with each model $ M_i $ handling a specific task component. Specifically, model $M_i$ processes the output $T_{i-1}$ from the previous model, along with its unique input $x_i$, to produce output $T_i$ for the next model. This process is expressed as: $T_i = M_i(T_{i-1}, x_i)$, forming a chain of models.

Given the high cost of deploying all models in such a chain, we can adopt a prompt-tuning approach. A shared base model $M_{\theta}$ is fine-tuned to perform different tasks, with each model $M_i$ distinguished solely by its fine-tuned prompt tokens $P_i$. This approach allows us to deploy only $M_{\theta}$, dynamically adjusting prompt tokens to replicate the behavior of multiple models:
\begin{equation}
T_i = M_{\theta}(T_{i-1}, x_i, P_i).
\end{equation}

While this approach simplifies the model chain, communication between models still occurs via text. Upon receiving the output  $T_{i-1}$ from the previous model, each model $M_i$ recalculates the hidden state of $T_{i-1}$ based on its prefix $P_i$(a process known as "prefilling"), and then generates the output $T_i$ and the corresponding hidden states $O_{T_{i}}$ autoregressively(a process known as "decoding"):
% (a process known as "prefilling") (a process known as "decoding")
% \begin{equation}
% H_{T_{i-1}}, H_{x_{i}}, H_{P_{i}} = M_\theta(T_{i-1}, x_i, P_i),
% \end{equation}
% \begin{equation}
% T_i = M_\theta(H_{T_{i-1}}, H_{x_{i}}, H_{P_{i}}).
% \end{equation}
\begin{equation}
H_{T_{i-1}}, H_{x_{i}}, H_{P_{i}} = \text{Prefilling}(T_{i-1}, x_i, P_i),
\end{equation}
\begin{equation}
T_i, O_{T_{i}} = \text{Decoding}(H_{T_{i-1}}, H_{x_{i}}, H_{P_{i}}),
\end{equation}
where $T_i$ is the output text and $O_{T_{i}}$ is the output hidden states of $T_i$.

In this paper, we argue that recalculating the KV hidden state $H_{T_{i-1}}$ is unnecessary. Instead, model $M_i$ can directly use KV hidden states $O_{T_{i-1}}$ output by the previous model $M_{i-1}$ as inputs. Besides, since prompt tuning allows the deployment of multiple models on a single device, there is no communication overhead of hidden states.

% \begin{equation}
% T_i = \text{Decode}(H_{T_{i-1}}; H_{x_{i}}; H_{P_{i}}).
% \end{equation}

% \begin{equation}
% H_{T_{i-1}}^{\text{new}} = M_{\theta}(T_{i-1}; P_i),
% \end{equation}
% \begin{equation}
% T_i = M_{\theta}(H_{T_{i-1}}^{\text{new}}, x_i; P_i).
% \end{equation}

% This method allows all models in the chain to complete their tasks in a single forward pass. It eliminates redundant intermediate calculations and removes the need for separate KV cache storage, because the model can directly access the previous model's hidden states during inference.


\subsection{Fine-Tuning for Hidden State Sharing}
\label{sec:fine-tuning}
Based on the above analysis, we aim to ensure that the KV hidden states computed by the previous model can be directly interpreted by the next. 
This is feasible due to the minimal differences between $H_{T_{i}}$ and $O_{T_{i}}$. Since models fine-tuned with prompt tuning on the same base model share identical structures and parameters, they differ only in the fine-tuned prompt tokens and input data.

Specifically, the output KV hidden state of $M_{i}$ during generation of the $j+1$-th token:
% $T_{i,j+1}$ is given by:
\begin{equation}
\_, O_{T_{i,j}} = \text{Decoding}(T_{i, 1:j}, H_{T_{i-1}}, H_{P_{i}}),
\label{eq:h_1}
\end{equation}
% \begin{equation}
% T_{i,j}, O_{T_{i,j}} = \text{Decoding}(H_{i, 1:j}, H_{T_{i-1}}, H_{P_{i}}),
% \label{eq:h_1}
% \end{equation}
where $O_{T_{i,j}}$ is the hidden state of token $T_{i,j}$ output by $M_i$. We ignore the unique input for simplicity. 

When $T_{i,j}$ serves as the input of $M_{i+1}$ rather than the output of $M_{i}$, the KV hidden state must be recomputed:
% \begin{equation}
% H_{T_{i,j}}^{\text{new}} = M_\theta(T_{i, 1:j-1}; P_{i}).
% \end{equation}
% \begin{equation}
% H_{T_{i,j}} = M_\theta(T_{i, 1:j-1}),
% \label{eq:h_2}
% \end{equation}
\begin{equation}
H_{T_{i,j}} = \text{Prefilling}(T_{i, 1:j}),
\label{eq:h_2}
\end{equation}
% \begin{equation}
% H_{T_{i,j}}, H_{x_{i+1}}, H_{P_{i+1}} = \text{Prefilling}(T_{i, 1:j}, x_{i+1}, P_{i+1}),
% \label{eq:h_2}
% \end{equation}
where $H_{T_{i,j}}$ is the KV hidden state of token $T_{i,j}$ calculated by $M_{i+1}$. We omit the prefix $P_{i+1}$ as it can be appended after $T_{i,1:j-1}$.

Since the attention calculation method is the same in both prefilling and decoding stages, the difference between equations (\ref{eq:h_1}) and (\ref{eq:h_2}) is minimal, with only the prefixes and inputs differing. This suggests that the output hidden state of $M_{i}$ introduces minimal noise for $M_{i+1}$,
and fine-tuning may be a practical solution.

We propose FTHSS (Fine-Tuning for Hidden State Sharing), a fine-tuning method to minimize these differences. By fine-tuning model $M_i$ with noisy KV hidden states from model $M_{i-1}$ as input, rather than the original ones, performance can be maintained despite the noise. We are currently exploring the implementation of this process.


\subsubsection{Fine-Tuning Strategies of Single-Round}
In practical applications, model chains are deployed in two configurations: single-round, where each model is called once, and multi-round, where models may be invoked multiple times. These configurations require distinct fine-tuning strategies.

Consider a model chain consisting of  A and B in a single-round scenario, where model A precedes model B, and its output serves as B's input. The training data and processes are organized as:

% Model chains are deployed in two configurations: single-round, where each model is called once, and multi-round, where models are invoked multiple times. Each configuration requires distinct fine-tuning strategies.

% In a single-round scenario, consider a model chain with A and B, where A's output serves as B's input. The training data and processes are organized as follows:

\paragraph{Model Input}
Since model A is the first in the chain, it does not require adjustment to any preceding model's input. Thus, the fine-tuning data for model A follows standard prompt tuning. However, we refine this process by reordering the input:
\begin{itemize}
\item Model A input order: shared content tokens, learnable prompt tokens (A), unique input content tokens for A.
\end{itemize}

We place the shared content before the learnable prompt tokens. Since the shared content is used across all models in the chain, this arrangement ensures that the KV hidden states of the shared content remain unaffected by the learnable tokens, thereby preventing the introduction of noise.


Since the output of model A serves as the input for model B,  A must be fully fine-tuned before fine-tuning B. Besides, the input to model B should consist of the output KV hidden states from A, rather than the tokens generated by A.

\begin{itemize} 
\item Model B input order: shared content tokens, output KV hidden states of fine-tuned model A, learnable prompt tokens (B), and unique content tokens for B.
\end{itemize}


\paragraph{Fine-Tuning Process}
As mentioned earlier, model B must be trained after model A, using the output KV hidden states from A. The fine-tuning process for the model chain proceeds as follows:

\begin{itemize}
\item Fine-tune A to generate output A. 
\item Store the output KV hidden states from the fully fine-tuned model A. 
\item Offline load the hidden states and fine-tune B to leverage them in generating output B. 
\end{itemize}

When fine-tuning model B, the position ID should not start at 0. Since model A's hidden states already contain position information, the position IDs for model B should begin at $l+1$, where $l$ is the last position ID in model A. As the LLM in this paper employs relative position encoding (e.g., RoPE~\cite{su2024roformer}), the absolute position is not critical. Therefore, the position ID ranges $[0, 1, \dots, l]$  and  $[l+1, l+2, \dots, 2l+1]$  are equivalent for attention computation. The proof is provided in Appendix \ref{sec:position}.

\paragraph{Fine-Tuning Tricks to Save Storage}
Given that most existing LLMs are based on the Transformer architecture, they typically include numerous layers and attention heads. As Figure \ref{fig:single-turn}(a) shows, the approach described above requires storing and accessing a large number of KV hidden states, which can be impractical. To address this, we propose recomputing the output KV hidden states of model A in memory, rather than storing them offline, as illustrated in Figure \ref{fig:single-turn}(b). 

Specifically, during the training of model B, we modify the input to B as follows:
\begin{itemize} 
\item Model B input order: shared content tokens, fine-tuned prompt tokens (A), unique input content tokens for A, output tokens of A, learnable prompt tokens (B), and unique content tokens for B. 
\end{itemize}

% Notably, we incorporate the fine-tuned prompt tokens (A), along with both the input and output tokens of model A, as part of model B's input. By adjusting the attention mask, we recompute model A’s output KV hidden states in memory (highlighted in the red box in Figure \ref{fig:single-turn}(c)). Simultaneously, the learnable prompt tokens (B) are fine-tuned to generate model B’s output, utilizing the recomputed KV hidden states from model A (as shown in the blue box in Figure \ref{fig:single-turn}(c)).
Notably, We incorporate the fine-tuned prompt tokens (A),  along with both the input and output tokens of model A, as part of model B's input. By adjusting the attention mask, we calculate model A's output KV hidden states in memory (red box in Figure \ref{fig:single-turn}(c)). Simultaneously, the learnable prompt tokens (B) are fine-tuned to generate model B’s output, using the recalculated KV hidden states from model A (blue box in Figure \ref{fig:single-turn}(c)).



The above algorithm outlines the fine-tuning process for a simplified model chain A$\rightarrow$B. In practical applications, when more than two models are involved in a model chain, each model can be trained sequentially, following the order of the chain. During this process, each model's input and attention mask should be adjusted accordingly.

\begin{figure}[t]
\centerline{\includegraphics[width=0.46\textwidth]{multi-turn.pdf}}
\caption{Cascade attention mask for every layer in the multi-round scenario.
}
\label{fig:multi-turn}
\end{figure}

\subsubsection{Fine-Tuning Strategies of Multi-Round}
% In a multi-round scenario, models may be invoked sequentially multiple times. Using models A and B as examples, the model chain sequence is no longer restricted to a simple $A \rightarrow B$ pattern; instead, it can follow more complex patterns such as $A \rightarrow B \rightarrow A \rightarrow B$. In this context, model B must adapt to the output of model A, while model A must also adapt to the output of model B. This differs from a single-round scenario, where both models must be fine-tuned simultaneously.
In a multi-round scenario, models may be invoked sequentially multiple times, allowing for more complex chains, such as $A \rightarrow B \rightarrow A \rightarrow B$. In this context, model B must adapt to the output of model A, while model A must also adapt to the output of model B. This differs from a single-round scenario, since models must be fine-tuned simultaneously.

To address this challenge, we modify the inputs and attention masks for both models, as illustrated in Figure~\ref{fig:multi-turn}. Specifically, the prompt tokens for both models are positioned at the beginning of the input. When computing the loss on the output of model A, attention scores are computed while masking the prompt tokens of model B. Conversely, When computing the loss on the output of model B, the prompt tokens of model A are masked. This ensures that model A's tasks are guided solely by its own prompt tokens, while model B's tasks are directed by its respective prompt tokens.
% Furthermore, the output from model A, when passed to model B, remains conditioned on A's prompt tokens. Similarly, the output from model B, when passed to model A, remains conditioned on B's prompt tokens. This methodology facilitates mutual adaptation between the hidden states of both models. 
