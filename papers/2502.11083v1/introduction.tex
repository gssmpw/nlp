% The increasing adoption of Chain of Models (CoM) architectures has been driven by the need to efficiently tackle complex tasks using specialized capabilities of individual models. This paradigm, prominently observed in applications such as Retrieval-Augmented Generation (RAG) and agent-based frameworks, involves multiple models working in a serial, collaborative manner. Each model focuses on a specific subtask and passes its processed output to the downstream model until the overall task is completed. For instance, in RAG applications, noisy input documents may first be summarized before question answering occurs, while in Self-RAG, reflective markers determine whether retrieval is necessary before proceeding with generation. These stepwise approaches leverage the strengths of individual models and have proven effective in numerous scenarios. However, deploying multiple dedicated models in such chains imposes substantial computational overhead, posing significant challenges for practical implementation.
\begin{figure}[t] 
\centerline{
\includegraphics[width=0.5\textwidth]{demo.pdf}} 
\caption{ 
% Comparison between the "Chain of Models" (a) and the proposed FTHSS approach (b). In (a), each model passes its output in plain text sequentially, requiring KV hidden state recomputation, while in (b), FTHSS enables models to share KV hidden states directly, reducing redundant forward passes. prompt tuning allows multiple models to be deployed on one device, so the communication overhead of hidden states is 0
Comparison of "Chain of Models" (a) and FTHSS (b): In (a), models sequentially pass outputs as plain text, requiring KV recomputation. In (b), FTHSS shares KV hidden states, reducing redundant forward passes. PEFT methods allow the deployment of multiple models on a single device, with parameters changing, so there is no communication overhead for hidden states.
} 
\label{fig:demo} 
\end{figure}

In many Retrieval-Augmented Generation (RAG) and agent-based frameworks~\cite{lewis2020retrieval}, multiple Large Language Models (LLMs) often collaborate sequentially. Each model focuses on a specific sub-task and passes its output as input to the next model until the task is completed\cite{zhang2024chain}. For instance, some RAG post-retrieval optimization methods~\cite{xu2023recomp,kim2024sure} involve summarizing retrieved documents with a summarization model, and then generating answers with a question-answering model. 
% Similarly, ~\citet{jin2024disentangling} disentangle memory and reasoning ability in LLMs with two special tokens.
% Self-RAG~\cite{asai2023self} first generates reflective tokens  to assess the need for retrieval and then generate content. 
These stepwise approaches leverage the strengths of individual models and have proven effective in many scenarios. As a result, the "Chain of Models" approach has gained popularity~\cite{zhang2024chain}. 


Deploying every specialized LLM in such chains significantly increases the resources needed. To address this, researchers have explored parameter-efficient fine-tuning (PEFT) methods, such as prompt tuning~\cite{liu2021p} and LoRA~\cite{hu2021lora}. These techniques allow fine-tuning with a fraction of the parameters when training. During inference, a shared base model is deployed on a single device and handles multiple tasks with distinct parameter configurations. This approach merges "Chain of Models" workflows into a single architecture, adapting to various sub-tasks through selective parameter usage.
However, a critical bottleneck remains: in the chain, the intermediate key-value(KV) hidden states from one model cannot be directly reused by the next model due to parameter differences. As a result, communication between models in the chain relies on passing plain text, forcing the downstream model to recompute hidden states. This practice not only adds computational overhead, but also raises KV cache storage requirements for each model in the chain, further hampering efficiency.

In this paper, we argue that such recomputation is unnecessary. Even with parameter differences, the KV hidden states produced by one model should only differ marginally from those recalculated by the next. Particularly in prompt-tuning methods, the KV hidden states produced by the previous model are essentially conditioned on a few noisy tokens.
% which are not critical for downstream tasks. 
With appropriate fine-tuning, the subsequent model can effectively interpret and utilize the KV hidden states of the previous model despite these noises, as Figure~\ref{fig:demo} shows. 
% Therefore, we advocate for enabling models to directly perform tasks based on the hidden states of the prior model, eliminating the need for communication via plain text.


To realize this vision, we propose FTHSS(Fine-Tuning for Hidden State Sharing), a prompt-tuning-based method that enables models in a chain to share KV hidden states. Specifically, when fine-tuning the model in single-round scenarios, where each model is invoked only once, we use KV hidden states from the prior models as input rather than plain text. This training approach requires extensive storage and access to KV hidden states, which may potentially increase training time and storage demands. To mitigate this, we introduce an online optimization strategy. By modifying the input and attention mask for each layer, we recompute the prior model's KV hidden states in memory during training, thus avoiding the overhead of storage and access. In multi-round scenarios, where models in the chain are invoked repeatedly, each model must adapt to the KV hidden states of others, so all models in the chain are trained synchronously to ensure mutual adaptation. 
% Additionally, we adjust the input order and attention mask during training, allowing models to utilize each other's hidden states effectively.
After fine-tuning, models can dynamically switch learnable prompt tokens during inference, adapting based on task requirements, while leveraging precomputed KV cache for direct generation. And since prompt-tuning-based methods enable the deployment of multiple models on a single device, communication overhead for hidden states is effectively eliminated.

% FTHSS provides significant advantages. During inference, models can dynamically switch learnable prompt tokens based on task requirements, leveraging precomputed hidden states for direct generation. This eliminates redundant forward passes over intermediate results, reducing inference latency. In multi-round tasks, FTHSS 
% not only avoids the redundant computation of intermediate results but also eliminates the need for multiple KV Cache copies, as hidden states can be shared.

Empirical results on four tasks, including single-round and multi-round, demonstrate that FTHSS leads to a comparable performance to the chain of models, 
% while eliminating redundant intermediate computations and the necessity of storing KV caches for individual models within the chain. 
while enhancing inference efficiency.
Technical contributions of this paper can be summarized as follows:
\begin{enumerate}
\item[\textbullet]To the best of our knowledge, we are the first to streamline the chain of models by sharing KV hidden states, thereby reducing the need for recomputing intermediate results.

\item[\textbullet]We introduce a prompt-tuning-based training strategy, FTHSS, that supports KV hidden state sharing across models in both single-round and multi-round scenarios.

\item[\textbullet] Experimental results show that FTHSS maintains comparable performance while significantly reducing inference latency and eliminating redundant KV cache storage.
\end{enumerate}