\begin{table*}[t]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{ccccccc}
            \toprule
            \textbf{Task (\(\rightarrow\))} & \multicolumn{6}{c} {Context Compression \& QA} \\

            \textbf{Dataset (\(\rightarrow\))} & \multicolumn{2}{c}{HQA} & \multicolumn{2}{c}{TQA} & \multicolumn{2}{c}{NQ} \\

            \textbf{Metric (\(\rightarrow\))} & EM & F1 & EM & F1 & EM & F1  \\
            \midrule

            \multicolumn{7}{c}{\textbf{Single Model}} \\
            Native & 14.4 & 22.8 & 40.1 & 53.7 & 14.5 & 26.4\\
            Standard RAG & 24.0 & 36.2 & 47.0 & 58.3 & 28.5 & 44.8 \\
            Prompt Tuning & 26.0 & 36.2 &26.4 & 44.2 & 32.7 & 45.1 \\

            \midrule
            \multicolumn{7}{c}{\textbf{Chain of Models}} \\
            Compress\&QA & \textcolor{gray}{\textbf{30.4}} & \textcolor{gray}{\textbf{43.8}} & \textcolor{gray}{\textbf{59.7}} & \textcolor{gray}{\textbf{68.3}} & \textcolor{gray}{\textbf{35.0}} & \textcolor{gray}{\textbf{48.3}} \\        
            \midrule  

            \multicolumn{7}{c}{\textbf{Streamlining}} \\
            Distill & 28.3 & 42.1 & 54.3 & 63.9 & 21.4 & 33.1 \\
            FTHSS(Our) & \textbf{29.0} & \textbf{42.2}& \textbf{59.3} & \textbf{67.5} & \textbf{35.8} & \textbf{45.6} \\
            \bottomrule
        \end{tabular}
        }
        \caption{
        Performance on the single-round task: Compression\&QA for FTHSS and other methods. \textbf{Bold numbers} indicate the best performance, except for the original chain of models (denoted in gray). Same below.
        % We report Exact Match (EM) and token-level F1 of answer strings to measure end task performance.
        }
        \label{tab:recomp_performance}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.465\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{ccccccc}
            \toprule
            \textbf{Task (\(\rightarrow\))} & \multicolumn{4}{c} {Query Rewriting \& QA} \\

            \textbf{Dataset (\(\rightarrow\))} & \multicolumn{2}{c}{HQABM25} & \multicolumn{2}{c}{2WikiBM25} \\

            \textbf{Metric (\(\rightarrow\))} & EM & F1 & EM & F1 \\
            \midrule

            \multicolumn{5}{c}{\textbf{Single Model}} \\
            Native  & 13.4 & 19.5 & 13.8 & 21.4 \\
            Standard RAG  & 19.0 & 31.1 & 14.4 & 21.6\\
            Prompt Tuning  & 18.2 & 29.8 & 20.6 & 27.4 \\

            \midrule
            \multicolumn{5}{c}{\textbf{Chain of Models}} \\
            Rewrite\&QA & \textcolor{gray}{\textbf{27.0}} & \textcolor{gray}{\textbf{37.2}} & \textcolor{gray}{\textbf{24.4}} & \textcolor{gray}{\textbf{30.2}} \\        
            \midrule 

            \multicolumn{5}{c}{\textbf{Streamlining}} \\
            Distill  & 20.8 & 30.4 & 18.0 & 23.9 \\
            FTHSS(Our)  & \textbf{27.4} & \textbf{36.6}& \textbf{24.0} & \textbf{29.9} \\
            \bottomrule
        \end{tabular}
        }
        \caption{
            Performance on the single-round task: Query Rewrite\&QA for FTHSS and other methods.
        }
        \label{tab:rewrite_performance}
    \end{minipage}
\end{table*}


% In this section, we first outline the experimental setup. Next, we discuss and benchmark our streamlining approach's performance in single-round and multi-round settings, emphasizing its effectiveness in reducing latency and KV cache storage. Finally, we analyze the amount of training data required for our method to achieve optimal performance.

\subsection{Setup}

% llama-3-8b
% 单轮任务，最先涉及到多模型协作的是RAG任务，检索前优化，检索后优化。数据集
% 多轮，回忆+reasoning迭代任务。数据集。根据情况考虑多轮检索任务
% 介绍时要注意把model chain中的模型是什么讲清楚
% To evaluate the effectiveness of our proposed strategy for simplifying model chains, 
We conduct experiments on both single-round and multi-round tasks. These experiments aim to evaluate whether the FTHSS approach can retain the functionality of model chains while improving inference efficiency in various scenarios.

\subsubsection{Single-Round Evaluation}
\paragraph{Tasks}
Many RAG frameworks involve chains of models due to their modular nature, 
% requiring multiple models to collaborate across various modules,
making them suitable for our evaluation. Common RAG optimization methods include pre-retrieval and post-retrieval optimization. We select two tasks from each of them as benchmarks:
\begin{itemize}
\item Context Compression \& Question Answering
\item Query Rewriting \& Question Answering
\end{itemize}

The Context Compression \& QA task involves compressing retrieved content into a noise-free context for the final response. The Query Rewriting \& QA task rewrites the query to retrieve more relevant information, and then generates the final response.

For the training data of Context Compression \& QA task, we follow the data specified in ReComp\cite{xu2023recomp}, while for the training data of  Query Rewriting \& QA task, we adhered to the data outlined by ~\citet{ma2023query}.


\paragraph{Baselines}
In the experiment, we compare three types of methods: (1) direct answer from a single model (Native, Standard RAG, Prompt Tuning), (2) using a model chain to generate intermediate results, which are then used to provide the final answer (Compress\&QA, Rewrite\&QA), and (3) simplifying the model chain to perform similarly to a single model (Distill, FTHSS).
Distill refers to fine-tuning one model to generate all intermediate steps, effectively distilling the capabilities of multiple models into a single model. We use Llama-3-8B~\cite{dubey2024llama} as the base model for all models in the chain. To ensure a fair comparison, all fine-tuning techniques discussed in this paper employ prompt tuning~\cite{liu2021p}.



\paragraph{Datasets}
We use the following widely adopted datasets to validate our approach: Natural Questions (NQ)\cite{kwiatkowski2019natural}, TriviaQA (TQA)\cite{joshi2017triviaqa}, 2WikiMultiHopQA(2Wiki)~\cite{ho2020constructing} and HotpotQA (HQA)\cite{yang2018hotpotqa}. 

\subsubsection{Multi-Round Evaluation}
\paragraph{Tasks}
% In multi-round scenarios, models in a model chain are invoked multiple times. We selected "Reasoning \& Memory" as a validation task~\cite{jin2024disentangling}, which decomposes the complex inference process into two well-defined steps, both steps are executed iteratively: (1) memory recall, which recalls relevant knowledge stored in the model's memory, and (2) reasoning, which performs logical operations based on the recalled knowledge. Besides, we also use a multi-round retrieval-generation task as an evaluation task, which iteratively retrieves and generates during the generation process, involving the use of two models (a retrieval term generation model and a question-answering model) in a loop.

In multi-round scenarios, models in a chain are invoked repeatedly. We selected "Reasoning \& Memory" as a validation task~\cite{jin2024disentangling}, which decomposes the inference process into two iterative steps: (1) memory recall, retrieving relevant knowledge from the model's memory, and (2) reasoning, applying logical operations to the recalled knowledge. Additionally, we evaluate our methods on an active retrieval augmented generation task~\cite{jiang2023active}. The Active RAG task involves multiple rounds of retrieval, which actively decides what to retrieve across the course of the generation.

For the training data of Memory\&Reasoning task, we use the data from~\citet{jin2024disentangling}, while for the training data of Active RAG task, we follow the data proposed by \citet{lyu2024retrieve}.

% multi-round retrieval-generation task, which involves iterative retrieval and generation using two models (a retrieval plan generation model and a question-answering model) in a loop.


\paragraph{Baselines}
The multi-round baselines are essentially identical to the single-round approach. They are categorized into three types: (1) direct answering (Single Model), (2) using a model chain to generate intermediate results (Memory\&Reason, Plan\&Generation), and (3) simplifying the model chain (Distill, FTHSS). 
% Following the settings outlined in \citet{jin2024disentangling}, we evaluate the performance on multiple-choice questions using accuracy as the metric.  

\paragraph{Datasets}
We take the following widely adopted datasets for evaluation: StrategyQA~\cite{geva2021did}, TruthfulQA(TruthQA)~\cite{lin2021truthfulqa}, CommonsenseQA(ComQA)~\cite{talmor2018commonsenseqa}, PubHealth~\cite{zhang2023interpretable}, 2WikiMultiHopQA(2Wiki)~\cite{ho2020constructing} and HotpotQA (HQA)\cite{yang2018hotpotqa}. 

For more details, we explain each task and other hyper-parameters in the Appendix ~\ref{sec:hyper} and ~\ref{sec:task}.


\begin{table*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{cccc}
            \toprule
            \textbf{Task (\(\rightarrow\))} & \multicolumn{3}{c} {Memory \& Reasoning} \\

            \textbf{Dataset (\(\rightarrow\))} & StrategyQA & ComQA & TruthQA \\
            \textbf{Metric (\(\rightarrow\))} & Acc & Acc & Acc   \\
            \midrule

            \multicolumn{4}{c}{\textbf{Single Model}} \\
            Zero-shot &  63.0	& 57.9 & 39.0\\ 
            CoT  & 63.0 &  66.1 & 47.6 \\
            Prompt Tuning & 63.6 & 66.7  & 65.2 \\

            \midrule
            \multicolumn{4}{c}{\textbf{Chain of Models}} \\
            % Memory\&Reason & 70.1* & 75.4* & 79.8*\\      
            Memory\&Reason & \textcolor{gray}{\textbf{70.1}} & \textcolor{gray}{\textbf{71.3}} & \textcolor{gray}{\textbf{69.2}} \\      
            \midrule 

            \multicolumn{4}{c}{\textbf{Streamlining}} \\
            Distill & 65.1 & 62.3 & 65.2 \\ 
            FTHSS(Our) & \textbf{69.2} & \textbf{70.3} & \textbf{68.9} \\
            \bottomrule
        \end{tabular}
        }
        \caption{
        Performance on the multi-round task: Memory \& Reasoning for FTHSS and other methods. 
        We evaluate the performance on multiple-choice questions using accuracy as the metric.
        }
        \label{tab:memory_performance}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.47\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{ccc}
            \toprule
            \textbf{Task (\(\rightarrow\))} & \multicolumn{2}{c} {Active RAG} \\

            \textbf{Dataset (\(\rightarrow\))} & Pubhealth & 2WikiBM25 \\
            \textbf{Metric (\(\rightarrow\))} & Acc & F1   \\
            \midrule

            \multicolumn{3}{c}{\textbf{Single Model}} \\
            Native &  69.5 & 21.4  \\ 
            Standard RAG & 56.1 & 21.6 \\
            Prompt Tuning & 69.1 & 27.4 \\

            \midrule
            \multicolumn{3}{c}{\textbf{Chain of Models}} \\
            Plan\&Generation & \textcolor{gray}{\textbf{73.4}} & \textcolor{gray}{\textbf{33.6}} \\        
            \midrule 

            \multicolumn{3}{c}{\textbf{Streamlining}} \\
            Distill & 70.1 & 23.1 \\ 
            FTHSS(Our) & \textbf{72.0}& \textbf{31.9} \\
            \bottomrule
        \end{tabular}
        }
        \caption{
        Performance on the multi-round task: Plan \& Retrieval for FTHSS and other methods.
        }
        \label{tab:rpg_performance}
    \end{minipage}
\end{table*}


\subsection{Main Results}
\paragraph{FTHSS leads to a comparable performance with the chain of models in both single-round and multi-round scenarios.}
We benchmark FTHSS with other models in Table \ref{tab:recomp_performance} and \ref{tab:rewrite_performance} in single-round settings, and find that FTHSS outperforms all single models while achieving comparable performance  to the chain of models. This demonstrates that our method avoids repeated computation of intermediate KV hidden states, improving efficiency without sacrificing performance.

For instance, in the Context Compression\&QA task on the TQA dataset, FTHSS achieves an EM score just 0.4 points lower than the approach using separate models, demonstrating nearly identical performance. Importantly, the compressed context no longer requires a forward pass through the QA model. Instead, it directly leverages the KV hidden states output by the compression model, reducing redundant computations and inference time.

Table \ref{tab:memory_performance} and \ref{tab:rpg_performance} present the results of multi-round experiments, which align closely with the findings from single-round experiments. This consistency highlights that, in addition to eliminating redundant intermediate computations, our method also removes the necessity of storing KV caches for individual models within the chain.


\paragraph{The chain of models outperforms single models.}
% In tasks such as RAG, different models serve distinct roles across various modules. 
% As shown in Table \ref{tab:recomp_performance}, \ref{tab:rewrite_performance}, \ref{tab:memory_performance} and \ref{tab:rpg_performance}, methods like Compress\&QA and Query Rewrite \&QA, which generating intermediate results, outperform approaches that rely on a single model for direct answering. This highlights the strong potential of chain-of-model collaboration. Our method, FTHSS, further enhances this process by optimizing redundant computations, leading to substantial efficiency improvements.

As shown in Tables \ref{tab:recomp_performance}, \ref{tab:rewrite_performance}, \ref{tab:memory_performance}, and \ref{tab:rpg_performance}, methods like Compress\&QA and Query Rewrite\&QA, which generate intermediate results, outperform single-model approaches. This highlights the potential of chain-of-model collaboration. Our FTHSS method further optimizes this by reducing redundant computations, yielding significant efficiency gains.


\paragraph{FTHSS outperforms Distill in both single-round and multi-round scenarios.} While Distill attempts to fine-tune a single model to handle all intermediate steps, distilling multiple models’ capabilities into one, this approach presents notable challenges. It requires the model to excel across all intermediate tasks; otherwise, the final result may be compromised. As shown in Table \ref{tab:recomp_performance}, \ref{tab:rewrite_performance}, \ref{tab:memory_performance}, and \ref{tab:rpg_performance}, experimental results reveal that distilling the capabilities of multiple models into a single model leads to varying degrees of performance degradation in both single-round and multi-round tasks. This underscores the superiority of FTHSS, where each model is allowed to specialize in its strengths, resulting in improved overall performance.




\subsection{Inference Efficiency Improvements}
To demonstrate the efficiency of our method, we present latency speed-ups achieved by eliminating redundant forward passes over intermediate results. We compare the inference latency of model B in FTHSS with that of the original model chain (where model A's output serves as input to model B), evaluating various intermediate result lengths. Results are averaged over 10 runs, performed on an Nvidia L20 GPU with the Llama-3-8B architecture.

Table~\ref{tab:latency} shows that for input sequences of 3,000 tokens, FTHSS reduces inference latency to less than one-third of the original model's. This improvement demonstrates that FTHSS maintains accuracy while significantly reducing latency. For sequences of 250 tokens, however, the speed-up is minimal due to GPUs' efficient parallel processing, limiting acceleration for smaller token counts.


In multi-round tasks, where each model in the chain may be repeatedly invoked, multiple copies of KV Caches are typically stored. FTHSS addresses this by enabling shared KV hidden states across models, reducing KV Cache storage to a single instance, regardless of chain length. As shown in Table \ref{tab:latency}, FTHSS significantly reduces GPU memory usage compared to a standard model chain. For the Llama-3-8B architecture, the KV Cache size for an input sequence of 1000 tokens is 137.5 MB. When multiple models are used, FTHSS saves $(n-1) \times 137.5 \text{ MB}$ of GPU memory. Thus, in multi-round tasks, FTHSS not only eliminates redundant computations, reducing latency, but also removes the need for multiple KV Cache copies, resulting in substantial memory savings.

\subsection{Further Analysis}
\begin{table}[t]
    \centering
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{cccccc}
        \toprule
        \multicolumn{6}{c}{Inference latency(s)(single-round task)} \\
        \midrule
        \textbf{tokens} & Chain of models & FTHSS \\
        \midrule
        % 50 & $0.45$ & $0.41$ \\
        250 & $0.45$ & $0.41$ \\
        500 & $0.52$ & $0.42$ \\
        1000 & $0.66$ & $0.43$ \\
        3000 & $1.44$ & $0.46$ \\
        \midrule \midrule
        \multicolumn{6}{c}{KV cache size(MB)(multi-round task)} \\
        \midrule
        \textbf{Models} & Chain of models & FTHSS \\
        \midrule
        1 & $137.5$ & $137.5$ \\
        2 & $137.5*2$ & $137.5$ \\
        3 & $137.5*3$ & $137.5$ \\
        % 4 & $137.5*4$ & $137.5$ \\
        % 5 & $137.5*5$ & $137.5$ \\
        \bottomrule
    \end{tabular}
    }
\caption{\textbf{Top:} Inference latency of model B in the chain A $\rightarrow$ B, with varying intermediate result lengths (in tokens), while output length is fixed at 16.
\textbf{Bottom:} GPU memory occupancy for KV cache under varying model counts in multi-round tasks, with total length fixed at 1000. Latencies are measured on an NVIDIA L20, with KV states stored in bfloat16.}
    \label{tab:latency}
\end{table}


% 少量样本训练能否达到同样的效果
\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{performance_metrics.pdf}
\caption{Performance comparison of three fine-tuning strategies on Context Compression \& QA task: (1) No additional fine-tuning, using noisy KV hidden states directly; (2) FTHSS (5000 samples), where the standard-prompt-tuning model is fine-tuned on 5,000 examples; and (3) Fully FTHSS, where the base model undergoes full-dataset fine-tuning.}
\label{fig:training_scale}
\end{figure}
In practice, specialized models are already trained using methods like Prompt Tuning. To apply our approach and simplify the model chain, these models may require re-fine-tuning, which can be computationally expensive. This raises the question: can these trained models—trained on plain text instead of the KV hidden states of previous models—be used with minimal or no fine-tuning?

% Specialized models are often pre-trained using methods like Prompt Tuning. To apply our approach and simplify the model chain, these models may require costly re-fine-tuning. This raises the question: can models trained on plain text, rather than KV hidden states, be used with minimal or no fine-tuning?



% In Figure \ref{fig:training_scale}, we present the results of directly feeding the hidden states from an upstream model into a standard trained model, as well as models trained with 5,000 samples, and those trained with the entire dataset on the Context Compression \& QA task. The results indicate that even an untrained model can produce answers in the correct format. Remarkably, on the TQA dataset, the F1 scores of the untrained model are comparable to those of trained models. However, given that hidden states inevitably contain noise, performance degrades on more challenging datasets like HotpotQA and NQ. When the model is fine-tuned with a small number of samples to adapt to the noise, its performance improves. Notably, fine-tuning with only a subset of samples is sufficient to adapt the model to the noise, enabling the simplification of the model chain into a single model without compromising performance.

% In Figure \ref{fig:training_scale}, we compare three approaches: (1) the standard prompt-tuning model without additional fine-tuning(Standard), which directly passes noisy hidden states to the next model in the chain; (2)continue fine-tuning the standard-trained model on 5,000 examples using FTHSS(5000 samples); and (3) fully fine-tuning the base model on the entire dataset with FTHSS (Fully FTHSS). These experiments, conducted on the Context Compression \& QA task, show that even standard fine-tuned models can generate correctly formatted answers. On the TQA dataset, the F1 score of the standard model is close to the fully fine-tuned model using FTHSS. However, performance degrades on more challenging datasets like HotpotQA and NQ due to inherent noise in hidden states. Besides, we find fine-tuning on a small subset of samples significantly improves performance. This indicates that fine-tuning standard fine-tuned models on small datasets with FTHSS is sufficient to handle noise, making full dataset re-fine-tuning unnecessary.


Figure \ref{fig:training_scale} compares three approaches: (1) the standard prompt-tuned model without additional re-fine-tuning (Standard), which attempts to interpret the noisy KV hidden states of the previous model directly; (2) continuing fine-tuning the standard prompt-tuned model on 5,000 examples using FTHSS (5000 samples); and (3) fully fine-tuning the base model on the entire dataset with FTHSS (Fully FTHSS). Experiments on the Context Compression\&QA task show that even the standard fine-tuned model generates mostly correct answers. On the TQA dataset, the F1 score of the standard model is close to that of the fully fine-tuned model using FTHSS. However, performance drops on more complex datasets like HotpotQA and NQ due to noise in the KV hidden states. Additionally, fine-tuning models on a small dataset significantly improves performance. This suggests that fine-tuning standard prompt-tuned models on a small dataset using FTHSS is sufficient to mitigate noise, making full-dataset re-fine-tuning unnecessary.


% In Section 3.2, we observe that directly feeding the hidden states from one model to the next mainly introduces unnecessary attention to a few noisy tokens. Previous work, such as attention sink~\cite{xiao2023efficient}, has already demonstrated the sparsity properties of attention. Therefore, adding a few noisy tokens does not necessarily affect the final result. In fact, using a standard prompt-tuned model without additional fine-tuning may still achieve strong performance in easy tasks.

% As discussed in Section ~\ref{sec:fine-tuning}, feeding hidden states from one model to the next primarily introduces unnecessary attention to a few noisy tokens. Previous work, such as attention sink~\cite{xiao2023efficient}, has shown that attention exhibits sparsity properties, meaning that a few noisy tokens do not significantly impact the final output. Consequently, using a standard prompt-tuned model without further fine-tuning can still yield strong performance on simpler tasks.

As discussed in Section \ref{sec:fine-tuning}, passing KV hidden states between models mainly introduces  unnecessary attention to noisy tokens. Previous work, such as attention sink~\cite{xiao2023efficient}, has shown that attention exhibits sparsity properties, meaning that a few noisy tokens do not significantly impact the final output. Consequently, using a standard prompt-tuned model without further fine-tuning can still yield strong performance on simpler tasks.


