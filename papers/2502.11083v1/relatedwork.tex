\subsection{Chain of Models}
% Chain of Models connects specialized models sequentially, enabling incremental sub-task processing to solve complex tasks. Each model’s output becomes the next model’s input, forming an end-to-end pipeline. While this approach utilizes individual model strengths, it also introduces significant computational overhead. This method was first introduced in ~\cite{zhang2024chain}, where it was proposed as a structured way to leverage multiple models for complex problem-solving.

The Chain of Models approach sequentially links specialized models, using the output of one as the input for the next~\cite{zhang2024chain}. This method allows for incremental processing of sub-tasks, and has been widely adopted across various domains. For example, Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval} improves the performance of question-answering (QA) tasks by combining retrieval and generation models. Additionally, the Chain of Models framework has proven highly effective for mathematical reasoning\cite{sun2023corex, dong2024effiqa, lei2024macm} and long-text generation~\cite{xi2025omnithink, wang2024autopatentmultiagentframeworkautomatic}.

% For instance, RAG consists of two core steps: retrieval and generation. A single base model is typically inadequate for both tasks, necessitating a Chain of Models. While effective, this method increases memory and computational costs, making efficiency optimization a key research focus.

While leveraging specialized models improves performance, it also increases deployment costs. One optimization strategy is to consolidate multiple models into a single, unified model through distillation. For instance, GritLM~\cite{muennighoff2024generative} enables task-switching through instruction modifications, combining retrieval and generation. OneGen~\cite{zhang2024onegen} introduces retrieval tokens, allowing LLMs to handle both tasks in a single forward pass. RankRAG~\cite{yu2024rankrag} integrates ranking and generation into a single retrained model. However, these methods require the distilled model to perform well in multiple tasks, which remains a significant challenge. The FTHSS method proposed in this paper diverges from the distillation paradigm, and it still leverages the strengths of multiple models while reducing the demand for computing resources.

\subsection{KV Cache Compression and Sharing}

Large Language Models (LLMs) face significant bottlenecks due to high memory and computational demands, with the key-value (KV) cache being a major contributor. The KV cache stores the keys and values for each Transformer layer during generation to avoid redundant computations. During deployment, the KV cache can occupy over 30\% of GPU memory~\cite{kwon2023efficient}. 

Some straightforward approaches address this issue by compressing context length~\cite{ge2024incontextautoencodercontextcompression, jiang2023llmlingua,li2023compressing} or employing sparse attention matrices ~\cite{xiao2023efficient, han2023lm}. More recently, methods focusing on KV cache reuse have been proposed. YOCO~\cite{sun2024you} utilizes a cross-decoder mechanism with cross-attention to reuse cached values, allowing the model to store KV pairs only once while maintaining global attention capabilities. LCKV~\cite{wu2024layer} and KVSharer~\cite{yang2024kvsharerefficientinferencelayerwise} enable KV cache sharing across layers within the same model. While these methods effectively enhance model efficiency by reusing and sharing KV caches at different layers of a single model, FTHSS extends this concept to multiple models.

