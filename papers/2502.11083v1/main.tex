% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}
% \newcommand{\comment}[1]{\State \textcolor{blue}{\#\ #1}} % 自定义注释样式


% \usepackage{CJK}
% \usepackage{balance}
\usepackage{changes}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{tcolorbox}

\usepackage{array}
\usepackage{lipsum} % 用于生成示例文本，可以根据需要去掉
\usepackage{arydshln}
% \usepackage[table]{xcolor}
\usepackage{float}
\definecolor{lightgray}{gray}{0.9}
\definecolor{lightgray}{gray}{0.9}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{Streamlining the Collaborative Chain of Models \\ into A Single Forward Pass in Generation-Based Tasks}

\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}



% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
% \author{Anonymous}
\author{Yuanjie Lyu, Chao Zhang,  Yuhao Chen, Yong Chen, Tong Xu\thanks{Corresponding author.}\\
\affaddr{University of Science and Technology of China} \\
\email{\{s1583050085\}@gmail.com,}\\
\email{\{tongxu\}@ustc.edu.cn} \\
}


\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\thefootnote}{\arabic{footnote}}
\begin{abstract}
% In Retrieval-Augmented Generation (RAG) and Agent-based frameworks, the "Chain of Models" approach is widely used, where multiple specialized models work sequentially to solve distinct sub-tasks. While this leverages the strengths of individual models, it also increases resource demands to deploy each model. Recent advancements attempt to address this by applying prompt tuning, which allows a shared base model to adapt to different tasks with minimal parameter changes. However, a key challenge remains: intermediate outputs, passed between models as plain text, require recomputation of hidden states (i.e., Key and Value(KV) states in Transformer) during inference. In this paper, we introduce FTHSS, a novel prompt-tuning method that allows models in a chain to share KV hidden states, eliminating redundant forward passes and reducing KV cache storage. By adapting input and attention masks during training, FTHSS allows models to effectively utilize KV hidden states from prior models in both single- and multi-round scenarios. Empirical results of 4 tasks demonstrate that FTHSS matches the performance of traditional model chains while enhancing inference efficiency.
In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the "Chain of Models" approach is widely used, where multiple specialized models work sequentially on distinct sub-tasks. This approach is effective but increases resource demands as each model must be deployed separately. Recent advancements attempt to address this by applying prompt tuning, which allows a shared base model to adapt to multiple tasks with minimal parameter changes. However, a key challenge remains: intermediate outputs, passed between models as plain text, require recomputation of hidden states (i.e., Key and Value (KV) states in Transformers) during inference. In this paper, we introduce FTHSS, a novel prompt-tuning method that enables models to share KV hidden states, eliminating redundant forward passes and reducing KV cache storage. By modifying input and attention masks during training, FTHSS allows models to effectively utilize KV hidden states from prior models in both single- and multi-round scenarios. Empirical results on four tasks show that FTHSS matches the performance of traditional model chains while improving inference efficiency.
% \footnote{Code: 
% \href{https://anonymous.4open.science/r/FTHSS-8367/}{https://anonymous.4open.science/r/FTHSS-8367/}.
% }
\footnote{Code: 
\href{https://github.com/haruhi-sudo/FTHSS}{https://github.com/haruhi-sudo/FTHSS}.
}
% by eliminating redundant computations and reducing KV cache storage.
% parameter-efficient fine-tuning (PEFT) methods, like 
\end{abstract}

\section{Introduction}
\input{introduction}

\section{Related Work}
\input{relatedwork}

\section{Methodology}
\input{method}

\section{Experiments}
\input{experiments}

\section{Conclusion}
\input{conclusion}

\section{Limitations}
\input{limitations}

% \balance
\bibliography{anthology}
% \bibliographystyle{acl_natbib}

% \clearpage
\appendix
\input{appendix}

\end{document}