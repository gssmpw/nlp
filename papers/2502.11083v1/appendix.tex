\begin{table}[t]
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Dataset name} & \textbf{Train/Test} \\
        \midrule
        \multicolumn{2}{c}{\textbf{Context Compression\&QA task}} \\
        Natural Questions(NQ) & 39,466/3,610 \\
        TriviaQA(TQA) & 47,531/11,313 \\
        HotpotQA(HQA) & 26,556/500 \\
        \midrule
        \multicolumn{2}{c}{\textbf{Query Rewrite\&QA task}} \\
        Training Data~\cite{ma2023query} & 37,520/-\\ 
        2WikiMultiHop(2Wiki) & -/500 \\
        HotpotQA(HQA) & -/500 \\
        \midrule
        \multicolumn{2}{c}{\textbf{Memory\&Reasoning task}} \\
        Training Data~\cite{jin2024disentangling} & 10,925/- \\
        StrategyQA & -/687 \\
        TruthfulQA(TruthQA) & -/164 \\
        CommonsenseQA(ComQA) & -/1,221 \\
        \midrule
        \multicolumn{2}{c}{\textbf{Active RAG task}} \\
       Training Data~\cite{lyu2024retrieve} & 47,689/- \\
        2WikiMultiHop(2Wiki) & -/500 \\
        Pubhealth & -/987 \\
        \bottomrule
    \end{tabular}
    \caption{Dataset statistics.}
    \label{tab:datasets}
\end{table} 


\section{Hyperparameters and Datasets}
\label{sec:hyper}
% 检索器， batch size，learning rate，优化器
% 数据集统计信息

\paragraph{Hyperparameters.} We fine-tune all parameters of our models for up to 3 epochs on 4 Nvidia A6000 GPUs. Our learning rate is 2e-4, and the gradient accumulation step is set to 8. We use
3\% of steps for linear warm-up of the learning rate and decay it linearly to 0 over training. To save
memory, we use DeepSpeed ZeRo-2~\cite{rajbhandari2020zero,rasley2020deepspeed} optimization, gradient checkpointing, and BF16 mixed precision training. During training, we use a maximum sequence length of 1224 for every sample, 100 learnable prompt tokens, and finetune using the Adam optimizer~\cite{kingma2014adam} with no weight decay. Our training script is based on HuggingFace accelerate~\cite{accelerate} libraries.

All base models in this paper are Llama-3-8B-Base unless otherwise specified. All PEFT fine-tuning methods are based on Prompt tuning, with the number of learnable prompt tokens set to 100. For methods that do not involve model training (e.g., Native, Standard RAG, and CoT), we utilize Llama-3-8B-Instruct, as its instruction-following capability is essential for these approaches.


\paragraph{Datasets.} The statistical details of the training and test datasets used in the experiments are provided in Table \ref{tab:datasets}. In the Context Compression\&QA task, the training phase utilizes the augmented NQ, TQA, and HQA datasets from recomp~\cite{xu2023recomp}. These datasets were created by using ChatGPT to semantically compress retrieved documents into concise summaries, generating synthetic training data. For model evaluation, we use the full test sets of NQ and TQA, along with a subset of the HQA development set, as validation benchmarks to ensure a comprehensive and reliable assessment of model performance. In the Query Rewrite\&QA task, we use the dataset from ~\citet{ma2023query} for training and evaluate the model on the multi-hop question datasets HQA and 2Wiki. In the Activate RAG task, we use the dataset from ~\citet{lyu2024retrieve} for training and evaluate the model on the short-form QA dataset PubHealth and the multi-hop QA dataset 2Wiki.


As for the retrieved documents, by default, we use the top one document ranked by Contriever-MS MARCO~\cite{izacard2021unsupervised} on Wikipedia corpus from Dec. 20, 2018, which is done to ensure a fair comparison among all baseline models. In the Query Rewrite\&QA and Active RAG tasks,  we use the top one document ranked by the BM25 retrieval algorithm. Improving the retriever is not the primary focus of this work; therefore, the retriever selection criterion is to maintain consistency with the papers that proposed these tasks.


\section{Task Explanation}
\label{sec:task}
% 几个任务的例子
\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{p{17cm}}
        \hline
        \textbf{Input}: How many episodes are there in dragon ball z?(\textcolor{gray}{\textit{NQ}})\\
        \hline
        \textbf{Input of the compression model}: \textcolor{gray}{<retrieved>April 5, 2009, the series premiered in Japan airing in Fuji TV. \"Dragon Ball Z Kai\" reduced the episode count to 159 episodes (167 episodes internationally), from the original footage of 291 episodes. Damaged frames were removed, resulting in some minor shots being remade from scratch in order to fix cropping, and others to address continuity issues. The majority of the international versions, including Funimation Entertainment's English dub, are titled \"Dragon Ball Z Kai\". premiered on Fuji TV on February 7, 1996 and ran until November 19, 1997 for 64 episodes. Unlike the first two anime series, it is not. Edition,\" which collects three individual volumes into a single large volume. However, in 2013 Viz began publishing new 3-in-1 volumes collecting the entire manga series, including what they previously released as \"Dragon Ball Z\", under the \"Dragon Ball\" name. The \"Dragon Ball Z\" films comprise a total of 15 entries as of 2015...</retrieved>} \\
        \textbf{Output of the compression model}:  \textcolor[rgb]{1,0.65,0}{<compressed>P\"Dragon Ball Z Kai\" has a total of 159 episodes (167 internationally), reducing the original count of 291 episodes. The last aired episode of \"Dragon Ball Kai\" in Japan was the 97th episode, with the 98th episode being released direct-to-video on August 2, 2011.</compressed>} \\
        \textbf{Output of the question answering model}:  \textcolor[rgb]{0.9,0,0.1}{<final\_answer>291 episodes</final\_answer>}\\
        \hline
    \end{tabular}
    }
    \caption{An Example for NQ datasets in Compression\&QA task. Model chain: compression model$\rightarrow$question answering model.}
    \label{tab:nq data examples}
\end{table*}

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{p{17cm}}
        \hline
        \textbf{Input}: Which one was established first, Grouplogic or Inbios?(\textcolor{gray}{\textit{2Wiki}})\\
        \hline
        \textbf{Output of the query rewriting model}:  \textcolor[rgb]{1,0.65,0}{<query\_rewrite>Grouplogic establishment year; Inbios establishment year</query\_rewrite>} \\
        \textbf{Input of the question answering model}: \textcolor{gray}{<retrieved>GroupLogic, Inc., founded in 1988 and headquartered in Arlington, Virginia, USA, is an enterprise software company that develops, sells and supports software for moving and storing data including activEcho, mobilEcho, ArchiveConnect, MassTransit and ExtremeZ- IP...}
        \\
        \textcolor{gray}{InBios International, Inc. is a medical diagnostic company based in Seattle that specializes in the detection of infectious diseases. The company was founded in 1996, and since its inception has developed several technologies useful in designing rapid and ELISA based immunodiagnostic assays. In April 2011, InBios International announced the clearance of its IgM test for dengue fever, DENV Detect IgM Capture ELISA...</retrieved>} \\
        \textbf{Output of the question answering model}: \textcolor[rgb]{0.9,0,0.1}{<Final answer>291 episodes</Final answer>}\\
        \hline
            \end{tabular}
        }
    \caption{An Example for 2Wiki datasets in Query Rewriting\&QA task. Model chain: query rewriting model$\rightarrow$question answering model.}
    \label{tab:2wiki data examples}
\end{table*}


\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{p{17cm}}
        \hline
        \textbf{Input}: Question: Sammy wanted to go to where the people were.  Where might he go? Options: A.race track B.populated areas C.the desert D.apartment E.roadblock
        (\textcolor{gray}{\textit{ComQA}})\\
        \hline
        \textbf{Output}: \textcolor[rgb]{0.9,0,0.1}{<reason\_start>Let's reason with knowledge: The objective is to determine a location where there are many people, as per Sammy's desire to be where the people are.  Each option needs to be evaluated based on the likelihood of it being a place with many people.<reason\_end>}\textcolor[rgb]{1,0.65,0}{<memory\_start>Let's recall the knowledge: A race track can be very crowded during events. However, it may not always be populated, so it lacks consistency in being populated.  Populated areas are locations where there are a high number of people living or congregating, such as cities and towns. This option aligns closely with the goal of being where people are consistently present.  Deserts are typically sparsely populated because of harsh living conditions, making them unlikely places for finding many people.  While an apartment building may contain a number of residents, it is not as widely or consistently populated as broader populated areas like cities.  A roadblock is usually not a gathering place for people. It might temporarily have a group or line of vehicles, but it is not known for large groups of people congregating for extended periods.<memory\_end>}\textcolor[rgb]{0.9,0,0.1}{<reason\_start>Let's reason with knowledge: Based on the assessments, option B (populated areas) is the most appropriate choice as it consistently fulfills the objective of being a place where there is a high number of people present. [The answer is]: B <reason\_end>}\\       
        \hline
            \end{tabular}
        }
    \caption{An Example for CommonsenceQA datasets in Memory\&Reasoning task. The reasoning and memory models are iteratively called multiple times during inference.}
    \label{tab:comqa data examples}
\end{table*}

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{p{17cm}}
        \hline
        \textbf{Input}: Which movie came out first Escape to Witch Mountain or Pete's Dragon?(\textcolor{gray}{\textit{HotpotQA}})\\
        \hline
        \textbf{Output}: \textcolor[rgb]{0.9,0,0.1}{<plan\_start>Escape to Witch Mountain release<plan\_end>}\textcolor{gray}{<retrieved>Escape to Witch Mountain is a 1975 American fantasy-children's film, adapted from the 1968 science fiction novel of the same name written by Alexander H. Key</retrieved>}\textcolor[rgb]{1,0.65,0}{<answer\_start>Escape to Witch Mountain came out first,<answer\_end>}\textcolor[rgb]{0.9,0,0.1}{<plan\_start>Pete's Dragon release<plan\_end>}\textcolor{gray}{<retrieved>Pete's Dragon is a 2016 American fantasy comedy-drama adventure film directed by David Lowery, written by Lowery and Toby Halbrooks, and produced by James Whitaker. </retrieved>}\textcolor[rgb]{1,0.65,0}{<answer\_start>before Pete's Dragon. <answer\_end>}[Combine]\textcolor[rgb]{1,0.65,0}{<answer\_start>Escape to Witch Mountain<answer\_end>}\\
        \hline
            \end{tabular}
        }
    \caption{An Example for HotpotQA datasets in Active RAG task.}
    \label{tab:hotpotqa data examples}
\end{table*}

In this section, we provide detailed examples to demonstrate why the evaluation tasks used in this paper involve multiple models. Table~\ref{tab:nq data examples} illustrates the Compression\&QA task. The documents retrieved by RAG are often excessively long and contain a significant amount of noise, which can mislead the question-answering model if input directly. By first using a model to compress the documents and then providing its output as input to the question-answering model, the accuracy of the responses can be significantly improved. The model chain in the Compression\&QA task is designed based on this approach, consisting of a summarization model whose output serves as the input to the question-answering model.

Table ~\ref{tab:2wiki data examples} presents the Query Rewriting\&QA task. For complex problems such as multi-hop QA, directly using the question as a query often fails to retrieve the appropriate context. To address this, we utilize another model to rewrite the query, which is then used to retrieve more accurate contextual information, followed by inputting this refined context into the question-answering model. The Query Rewriting\&QA task also involves a model chain.

Tables~\ref{tab:comqa data examples} and ~\ref{tab:hotpotqa data examples} demonstrate the model chains in multi-round scenarios, where the models are not invoked only once but are iteratively called. In the Memory\&Reasoning task, the model first recalls the knowledge required to answer the question, then uses this recalled knowledge to reason and generate the answer. Since these two sub-tasks differ significantly, different models must be deployed to handle them separately. Furthermore, a single round is insufficient to ensure that all required knowledge is retrieved, so these two sub-tasks need to be executed alternately and repeatedly. Additionally, the Active RAG task involves multiple rounds of retrieval, where the model dynamically decides what to retrieve during the generation process (the planning phase), followed by generating the response based on the retrieved information (the answering phase). The planning and answering sub-tasks are iteratively performed, requiring two distinct models to be deployed.


\section{Inference Details}
% 对比，展示KVCache，和位置编码两个区别

Algorithm \ref{alg:multi_prompt_inference} illustrates the process of inference in a single-round task, where multiple prompt-tuning-based models share KV hidden states. The performance of a shared base model across different tasks depends on the learnable, task-specific prompt tokens. During inference, these prompt tokens are dynamically switched, as demonstrated in line 12 of the algorithm. Furthermore, sharing hidden states implies that the KV cache from the previous model can be reused directly, without the need to recompute the intermediate KV hidden states of \( Y_i \). The red-striped portion in the algorithm shows the computational savings of our approach compared to previous prompt-tuning methods.

Assuming the total length of intermediate results is $n$, the computational savings of this algorithm are $O(n^2)$, given the quadratic complexity of the transformer.

Notably, the computational savings occur during the prefilling phase, which runs in parallel. Therefore, when the length of the intermediate results is relatively short, the savings have a minimal impact on the inference latency.


% \begin{algorithm*}[t]
% \caption{Transformer Autoregressive Decoding with KV Cache}
% \label{alg:transformer_decode}
% \begin{algorithmic}[1]
% \Require Initial token $\mathbf{x}_{0} = \text{[BOS]}$, max length $L_{\max}$, layers $N$
% \Ensure Generated sequence $\mathbf{x}_{1:T}$
% \State Initialize cache $\mathcal{C} \gets \{\emptyset\}^N$ \Comment{Per-layer $(\mathbf{K}_l, \mathbf{V}_l)$ storage}
% \For{$t = 0$ \textbf{to} $L_{\max}-1$}
%     \State $\mathbf{h}^{(0)} \gets \text{Embed}(\mathbf{x}_t)$ \Comment{Current token embedding}
%     \For{$l = 1$ \textbf{to} $N$} \Comment{Layer-wise computation}
%         \State $\mathbf{q}_l = \mathbf{W}_q^l \mathbf{h}^{(l-1)}$ \Comment{Query projection}
%         \State $\mathbf{k}_l = \mathbf{W}_k^l \mathbf{h}^{(l-1)}$ \Comment{Key projection}
%         \State $\mathbf{v}_l = \mathbf{W}_v^l \mathbf{h}^{(l-1)}$ \Comment{Value projection}
        
%         \State $\mathcal{C}_l.\mathbf{K} \gets \text{Concatenate}(\mathcal{C}_l.\mathbf{K}, \mathbf{k}_l)$ \Comment{Update Key cache}
%         \State $\mathcal{C}_l.\mathbf{V} \gets \text{Concatenate}(\mathcal{C}_l.\mathbf{V}, \mathbf{v}_l)$ \Comment{Update Value cache}
        
%         \State $\text{Attn}_l = \text{Softmax}\left(\frac{\mathbf{q}_l (\mathcal{C}_l.\mathbf{K})^\top}{\sqrt{d_k}}\right) \mathcal{C}_l.\mathbf{V}$ \Comment{Cached attention}
%         \State $\mathbf{h}^{(l)} \gets \text{LayerNorm}(\mathbf{h}^{(l-1)} + \text{FFN}(\text{Attn}_l))$
%     \EndFor
%     \State $\mathbf{p}_{t+1} \gets \text{Softmax}(\mathbf{W}_o \mathbf{h}^{(N)})$ \Comment{Next-token distribution}
%     \State $\mathbf{x}_{t+1} \gets \arg\max \mathbf{p}_{t+1}$ \Comment{Greedy decoding}
%     \If{$\mathbf{x}_{t+1} = \text{[EOS]}$}
%         \State \textbf{break}
%     \EndIf
% \EndFor
% \end{algorithmic}
% \end{algorithm*}

\begin{algorithm*}[t]
\caption{The Inference Process of FTHSS in Single-Round Tasks(The red-striped portion represents operations that are necessary for the original model chain, but are optimized and removed in FTHSS).}
\label{alg:multi_prompt_inference}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Input sequence $X = (x_1, x_2, \dots, x_n)$
\STATE \textbf{Output:} Sub-task output sequences $Y_1 = (y_{11}, y_{12}, \dots)$, $Y_2$, $\dots$, $Y_t$

\STATE \textbf{Initialize:} 
\STATE \quad - Decoder-only transformer $T$ with parameters $\theta$
\STATE \quad - Task-specific soft prompt tokens $\{P_1, P_2, \dots, P_t\}$
\STATE \quad - KV Cache: $\text{Cache} \gets T(X)$ \quad (Encode input sequence)
\STATE \quad - Intermediate results: $Y_0$

\FOR{$i = 1$ \TO $t$}
\STATE \textbf{Prefilling Phase:}
\STATE \textcolor{red}{\sout{$\text{Cache} \gets \emptyset$}}
\STATE \textcolor{red}{\sout{$\text{Cache} \gets T(P_i, Y_{i-1}, \text{Cache})$}}
\STATE $\text{Cache} \gets T(P_i, \text{Cache})$ \quad (Compute and cache task-specific KV)
\STATE Initialize output sequence: $Y_i \gets [\text{\textless start\textgreater}]$

\STATE \textbf{Decoding Phase (Autoregressive):}
\FOR{$k = 1$ \TO max\_length} 
\STATE \quad 1. Current token: $y_{k-1} \gets Y_i[-1]$ \quad (Last generated token)
\STATE \quad 2. Compute embedding: $e_k \gets E(y_{k-1})$
\STATE \quad 3. Update decoder layers with KV Cache:
\STATE \quad \quad $h_k, \text{Cache} \gets T(e_k, \text{Cache})$ \quad (Reuse cached KV)
\STATE \quad 4. Compute logits: $p(y_k) \gets \text{softmax}(W_o h_k)$ \quad ($h_k$ from last layer)
\STATE \quad 5. Sample next token: $y_k \sim p(y_k)$
\STATE \quad 6. Append $y_k$ to $Y_i$
\ENDFOR
\ENDFOR

\STATE \textbf{Return:} Output sequences $Y_1, Y_2, \dots, Y_t$
\end{algorithmic}
\end{algorithm*}



% 复杂度简易证明
\section{Position ID Rearrangement}
\label{sec:position}
% 相对位置编码，绝对位置无关
If  $l$ is the last position ID of the preceding model, the position encoding of the current model should begin at $l+1$, ensuring that the accuracy of the attention computation during inference is unaffected. This is because, under Rotary Position Embedding (RoPE), the position ID ranges $[0, 1, \dots, l]$ and $[l+1, l+2, \dots, 2l+1]$ are equivalent in attention computation. The proof of this conclusion is presented below.

% Rotary Position Embedding (RoPE) encodes positional information through rotational operations.
We prove that RoPE computes attention based solely on the relative position \( m-n \) , independent of the absolute positions \( m \) or \( n \). Given a query vector \( \boldsymbol{q}_m \) at position \( m \) and a key vector \( \boldsymbol{k}_n \) at position \( n \), RoPE applies rotations:

\begin{equation}
\begin{aligned}
\boldsymbol{q}_m & = R_m \boldsymbol{q}, \quad R_m = \begin{bmatrix}
\cos m\theta & -\sin m\theta \\
\sin m\theta & \cos m\theta
\end{bmatrix}, \\
\boldsymbol{k}_n & = R_n \boldsymbol{k}, \quad R_n = \begin{bmatrix}
\cos n\theta & -\sin n\theta \\
\sin n\theta & \cos n\theta
\end{bmatrix},
\end{aligned}
\end{equation}

where \( \theta \) is a frequency parameter. The attention score is:

\begin{equation}
\text{Score}(m, n) = \boldsymbol{q}_m^\top \boldsymbol{k}_n = (\boldsymbol{q}^\top R_m^\top)(R_n \boldsymbol{k}).
\end{equation}

Since rotation matrices are orthogonal (\( R^\top R = I \)), and satisfy \( R_m^\top R_n = R_{n - m} \), the score can simplify to:

\begin{equation}
\text{Score}(m, n) = \boldsymbol{q}^\top R_{n - m} \boldsymbol{k},
\end{equation}
which depends only on \( (n - m) \). For high-dimensional vectors, RoPE divides the vector into \( d/2 \) subspaces, applying rotations independently in each subspace:

\begin{equation}
R_{m}^{(i)} = \begin{bmatrix}
\cos m\theta_i & -\sin m\theta_i \\
\sin m\theta_i & \cos m\theta_i
\end{bmatrix},
\end{equation}

yielding:

\begin{equation}
\text{Score}(m, n) = \sum_{i=1}^{d/2} \boldsymbol{q}_i^\top R_{n - m}^{(i)} \boldsymbol{k}_i.
\end{equation}

Thus, RoPE strictly encodes relative positions, eliminating absolute position dependence. This property 
% enhances its effectiveness in long-text modeling and positional generalization tasks, and it 
has been utilized in some precomputed KV cache scenarios~\cite{lu2024turborag}.
% enhances its effectiveness in long-text modeling and positional generalization tasks.