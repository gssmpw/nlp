% \abstract{The emergence of Foundation Models (e.g., Large Language Models [LLM] and Large Vision Models [LVM]) that are adept at processing and generating multi-modal data has led to a paradigm shift in the role AI can play in medicine. However, these models have imperfections that limit their potential, one of the most prominent categories of which is \emph{hallucinations}, inaccurate or fabricated information—which pose significant risks in evidence-based medicine. This duality of potentials and risks of foundation models highlights the need for rigorous evaluation and mitigation in healthcare settings. We introduce the concept of \textbf{\textit{medical hallucination}}, defined as instances where models, particularly LLMs, produce inaccurate or misleading medical information that can adversely affect clinical decision-making. By examining their unique characteristics, underlying causes, and critical implications, we categorize and exemplify medical hallucinations, highlighting their potentially life-threatening outcomes. Our contributions include a taxonomy for understanding and addressing medical hallucinations, comprehensive experiments with state-of-the-art and medically-focused models, and insights from a multi-national survey of clinicians capturing their experiences with medical hallucinations in practice. These findings display the ethical and practical imperatives for robust detection and mitigation strategies, establishing a foundation for regulatory policies prioritizing patient safety and upholding clinical integrity as AI further integrates into healthcare.\abstract}


% \abstract{The emergence of Foundation Models (e.g., Large Language Models [LLM] and Large Vision Models [LVM]) that are adept at processing and generating multi-modal data has led to a paradigm shift in the role AI can play in medicine. However, these models have imperfections that limit their potential, one of the most prominent categories of which is known as \emph{hallucination}, where inaccurate or fabricated information is generated by the model. We introduce the concept of \textbf{\textit{medical hallucination}}, defined as instances where models, particularly LLMs, produce inaccurate or misleading medical information that can adversely affect clinical decision-making. Through examining their unique characteristics, underlying causes, and critical implications, we categorize and exemplify medical hallucinations, highlighting their potentially life-threatening outcomes. Our contributions include a taxonomy for understanding and addressing medical hallucinations, comprehensive experiments using medical hallucination benchmarks and medical records annotated by doctors, and insights from a multi-national survey of clinicians capturing their experiences with medical hallucinations in practice. These experiments reveal that different LLM inference techniques, such as Chain-of-Thought (CoT), Retrieval-Augmented Generation (RAG), and Incorporation of Internet Search influence hallucination rates, highlighting the need for tailored mitigation strategies. These findings highlight the ethical and practical imperative for robust detection and mitigation strategies, establishing a foundation for regulatory policies prioritizing patient safety and upholding clinical integrity as AI further integrates into healthcare.\abstract}

% \abstract{Foundation Models (e.g., Large Language Models [LLM] and Large Vision Models [LVM]) capable of processing and generating multi-modal data have transformed AI’s role in medicine. However, a key limitation to the reliability of these models is \emph{hallucination}, where inaccurate or fabricated information can compromise clinical decisinos and patient safety. We define \textbf{\textit{medical hallucination}} as any instance in which a model generates misleading medical content. %where models, particularly LLMs, produce misleading medical information that can adversely affect clinical decision-making. 
% In this paper, we examine the unique characteristics, causes, and implications of medical hallucinations. %, we present a taxonomy of these hallucinations and show how they can lead to serious clinical consequences. %categorize and exemplify medical hallucinations, highlighting their potentially life-threatening outcomes. 
% Our contributions include (1) a taxonomy for understanding and addressing medical hallucinations, (2) benchmarking models using medical hallucination datasets and physician-annotated medical records, and (3) a multi-national clinician survey on their experiences with medical hallucinations. 
% Our results reveal that inference techniques such as Chain-of-Thought (CoT), Retrieval-Augmented Generation (RAG), and Internet Search impact hallucination rates. %, emphasizing the need for tailored mitigation strategies. These findings underscore the ethical and practical imperative for robust detection and mitigation strategies, establishing a foundation for regulatory policies that prioritize patient safety and uphold clinical integrity as AI integrates further into healthcare. 
% Our findings call for the urgent need for robust detection and mitigation strategies, including technical advances and clearer ethical and regulatory guidelines, to prioritize patient safety and uphold clinical integrity as AI integrates further into healthcare. 
% A repository organizing the paper resources, summaries, and additional information is available at \url{https://github.com/mitmedialab/medical_hallucination}.}

\abstract{Foundation Models that are capable of processing and generating multi-modal data have transformed AI’s role in medicine. However, a key limitation of their reliability is \emph{hallucination}, where inaccurate or fabricated information can impact clinical decisions and patient safety. We define \textbf{\textit{medical hallucination}} as any instance in which a model generates misleading medical content. This paper examines the unique characteristics, causes, and implications of medical hallucinations, with a particular focus on how these errors manifest themselves in real-world clinical scenarios. Our contributions include (1) a taxonomy for understanding and addressing medical hallucinations, (2) benchmarking models using medical hallucination dataset and physician-annotated LLM responses to real medical cases, providing direct insight into the clinical impact of hallucinations, and (3) a multi-national clinician survey on their experiences with medical hallucinations. Our results reveal that inference techniques such as Chain-of-Thought (CoT) and Search Augmented Generation can effectively reduce hallucination rates. However, despite these improvements, non-trivial levels of hallucination persist. These findings underscore the ethical and practical imperative for robust detection and mitigation strategies, establishing a foundation for regulatory policies that prioritize patient safety and maintain clinical integrity as AI becomes more integrated into healthcare. The feedback from clinicians highlights the urgent need for not only technical advances but also for clearer ethical and regulatory guidelines to ensure patient safety. A repository organizing the paper resources, summaries, and additional information is available at \url{https://github.com/mitmedialab/medical_hallucination}.}