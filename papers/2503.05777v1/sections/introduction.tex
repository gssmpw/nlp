% \section{Introduction}\label{sec1}

% The integration of Large Language Models (LLMs) into healthcare applications presents significant opportunities, from enhancing clinical decision support \citep{mcduff2023towards, kim2024mdagentsadaptivecollaborationllms, li2024mediq} to revolutionizing medical research \citep{singhal2023large, saab2024capabilitiesgeminimodelsmedicine} and improving healthcare quality and safety \citep{jalilian2024potential, mukherjee2024polaris}. However, this integration also brings a number of critical challenges to the forefront. A particularly concerning issue is the phenomenon of \textit{hallucination}, instances where LLMs generate plausible-sounding but factually incorrect or fabricated information \citep{Ji_2023}. Indeed, hallucinations are a concern across various domains including finance \citep{kang2023deficiencylargelanguagemodels}, legal \citep{Dahl_2024}, code generation \citep{agarwal2024codemiragehallucinationscodegenerated}, education \citep{10645894} and more \citep{sun2024benchmarkinghallucinationlargelanguage, kang2023deficiencylargelanguagemodels}. However, specifically in medical contexts, hallucinations pose particularly severe risks as incorrect medication dosages, drug interactions, or diagnostic criteria can directly lead to life-threatening outcomes \citep{szolovits2024large}. This paper focuses on LLM hallucinations in the medical context, which we refer to as \textit{medical hallucinations} throughout this work. These are instances where LLMs generate incorrect or misleading medical information that could adversely affect clinical decision-making and patient outcomes. The concept of hallucinations in LLM is not new. Still, its implications in the medical domain warrant specific attention due to the high stakes involved and the minimal margin for error. For instance, the hallucination of a medical LLM agent could lead to misinformation in historical patient status that leads to the wrong treatment protocol, which could lead to adverse effects in patient care (Fig.~\ref{fig:main}).

% By examining medical hallucination, we aim to contribute to the ongoing discourse on the reliability and safety of AI in healthcare. This paper builds upon existing research on LLM hallucinations \citep{rawte2023survey, tonmoy2024comprehensive} and extends it to the specific challenges in medical applications, where LLMs face unique hurdles: \textbf{1)} Rapid evolution of medical knowledge, leading to potential model obsolescence, \textbf{2)} The necessity for extreme accuracy in medical information, \textbf{3)} The interconnected nature of medical concepts, where a small error can propagate into significant misunderstandings, \textbf{4)} The presence of domain-specific terminology and context that require specialized interpretation. 

% Our work makes three primary contributions: \textbf{First,} we present a novel taxonomy for medical hallucinations in LLMs, providing a structured framework for categorizing AI-generated medical misinformation. \textbf{Second,} we conduct comprehensive experimental analyses across various medical subdomains, including general practice, specialized fields like oncology and cardiology, and medical education scenarios, utilizing state-of-the-art LLMs such as GPT-4 \citep{openai2024hello}, Whisper \citep{radford2022robustspeechrecognitionlargescale}, Claude-3.5 \citep{anthropic2024claude}, Gemini-1.5 \citep{geminiteam2024gemini15unlockingmultimodal}, and domain-specific models like Med-PaLM2 \citep{singhal2023expertlevelmedicalquestionanswering}, Meditron \citep{chen2023meditron} and Med-Alpaca \citep{han2023medalpaca}. \textbf{Third,} we present findings from an international survey of clinicians, providing insights into how medical professionals perceive and experience LLM hallucinations in their practice. These contributions collectively advance our understanding of medical hallucinations and their mitigation strategies, with implications extending to regulatory frameworks and best practices for AI deployment in clinical settings.

%%%%%%% NEW WRITING %%%%%%


\section{Introduction}\label{sec1}

The integration of foundation models (e.g., Large Language Models [LLM] and Large Vision Language Models [VLM]) into healthcare applications presents significant opportunities, from enhancing clinical decision support \citep{mcduff2023towards, kim2024mdagentsadaptivecollaborationllms, li2024mediq} to transforming medical research \citep{singhal2023large, saab2024capabilitiesgeminimodelsmedicine} and improving healthcare quality and safety \citep{goodman2024ai,jalilian2024potential, mukherjee2024polaris}. However, this integration also brings a number of critical challenges to the forefront. A particularly concerning issue is the phenomenon of \textit{hallucination} or \textit{confabulation}, instances where LLMs generate plausible but factually incorrect or fabricated information \citep{Ji_2023,chen2023use}. 
Hallucinations are a concern across various domains, including finance \citep{kang2023deficiencylargelanguagemodels}, legal \citep{Dahl_2024}, code generation \citep{agarwal2024codemiragehallucinationscodegenerated}, education \citep{10645894} and more \citep{sun2024benchmarkinghallucinationlargelanguage, kang2023deficiencylargelanguagemodels, wang2024llm}. Medical hallucinations pose particularly serious risks as incorrect dosages of medications, drug interactions, or diagnostic criteria can directly lead to life-threatening outcomes \citep{chen2024effect, szolovits2024large}.

Just as human clinicians can be susceptible to cognitive biases in clinical decision making \citep{ke2024mitigating, vally2023errors}, LLMs exhibit their own form of systematic errors through what we refer to as \textit{medical hallucinations}. In these cases, LLMs generate incorrect or misleading medical information that could adversely affect clinical decision making and patient outcomes. For example, an LLM might hallucinate about patient information, history, and symptoms \citep{vishwanath2024faithfulness} on a clinical note that does not align with the original note. This example corresponds to the confirmation bias of a physician in which contradictory symptoms are overlooked and eventually lead to inappropriate treatment protocols (Fig.~\ref{fig:main}). Although the concept of hallucinations in LLM is not new \citep{bai2024hallucination}, its implications in the medical domain warrant specific attention due to the high stakes involved and the minimal margin of error \citep{umapathi2023med}.

This paper builds upon existing research on LLM hallucinations \citep{rawte2023survey, tonmoy2024comprehensive, huang2023survey, ji2023survey} and extends it to specific challenges in medical applications, where LLMs face particular hurdles: 
\textbf{1)} the rapid evolution of medical information, leading to potential model obsolescence \citep{wu2024well}, 
\textbf{2)} the necessity of precision in medical information \citep{topol2019high}, 
\textbf{3)} the interconnected nature of medical concepts, where a small error can cascade \citep{bari2016medical}, and 
\textbf{4)} the presence of domain-specific jargon and context that require specialized interpretation \citep{yao2023readme}. 

Our work makes three primary contributions. \textbf{First,} we introduce a taxonomy for medical hallucination in LLM, providing a structured framework to categorize AI-generated medical misinformation. \textbf{Second,} we conduct comprehensive experimental analyses across various medical subdomains, including general practice, specialized fields such as oncology and cardiology, and medical education scenarios, utilizing state-of-the-art LLMs such as o3-mini, Gemini-2.0 Flash Thinking, and domain-specific models such as Meditron \citep{chen2023meditron} and Med-Alpaca \citep{han2023medalpaca}. \textbf{Third,} we present findings from a survey of clinicians, providing insights into how medical professionals perceive and experience hallucinations when they use LLMs for their practice or research. These contributions collectively advance our understanding of medical hallucinations and their mitigation strategies, with implications extending to regulatory frameworks and best practices for the deployment of AI in clinical settings.

% Main Figure
\begin{figure}[htpb!]
    \centering\vspace{-6mm}
    \includegraphics[width=1.0\textwidth]{imgs/new_main.pdf}
    \caption{\textbf{Overview of medical hallucinations generated by state-of-the-art LLMs.} \textbf{(a)} Medical expert-annotated hallucination rates and potential risk assessments on three medical reasoning tasks with NEJM Medical Records (see Section \ref{sec:new_sec7} for full analysis). \textbf{(b)} Representative examples of medical hallucinations from \cite{chen2024detecting, vishwanath2024faithfulness} respectively. \textbf{(c)} Geographic distribution of clinician-reported medical hallucination incidents providing a global perspective on the issue (see Subsection \ref{subsec:survey_results} for full analysis).} 
    \vspace{-3mm}
\label{fig:main}
\end{figure}

% about the "Circle Chart" 

%%%%%%%%%%%%%%%%%%%





% \subsection{Definitions}\label{subsec0}
% A \emph{hallucination} in the context of large language models (LLMs) is defined as any instance where the model generates content that is factually incorrect, nonsensical, or ungrounded in the provided input or its training data. Hallucinations occur when LLMs produce erroneous or fabricated that could lead to misinformation. This phenomenon poses significant challenges, especially in domains where accuracy is critical.

% In this work, we define a \emph{medical hallucination} as any instance where an LLM-generated medical response contains incorrect or missing elements, such as misrepresentation, incorrect reasoning, or absence of reasoning. This definition encompasses both the generation of false information not grounded in the model's training data (commonly referred to as \textit{hallucination}) and the production of incorrect information resulting from attempts to fill knowledge gaps (often termed \textit{confabulation} in neuropsychological contexts). By using this inclusive definition, we aim to capture all instances of incorrect or potentially harmful outputs from medical LLMs, regardless of their underlying cognitive mechanisms. This approach enables a comprehensive examination of the challenges posed by LLMs in medical applications, acknowledging the complex nature of these errors within the highly specialized medical domain.

% % {\color{red} {Daniel: It is probably good to clarify if/what the difference is between hallucination and confabulation. Some would say confabulation is a better term. [Checked]}}

% \subsection{Categorization}\label{subsec1.5}
% Medical hallucinations can be categorized in various ways. One fundamental distinction is between intrinsic and extrinsic hallucinations \citep{maynez2020faithfulness}. Intrinsic hallucinations occur when the model generates content that is inconsistent with the input or context, while extrinsic hallucinations involve the fabrication of information not present in the training data. Another categorization framework focuses on the severity of hallucinations, ranging from minor inaccuracies to critical errors that could significantly impact patient care \citep{asgari2024framework}. Some researchers have also proposed categorizing hallucinations based on their sources, such as knowledge gaps, reasoning errors, or data misinterpretation \citep{}.

% Building upon these existing frameworks, we propose a new categorization that focuses specifically on evidence from LLM use in medical tasks. Our approach captures the most critical types of errors in medical information processing and addresses key aspects of medical hallucinations:

% \begin{enumerate}
%     \item \textbf{Temporal Hallucinations:} Misrepresentation of medical timelines or chronological orders. This category includes errors in disease progression, treatment schedules, or historical medical events.

%     \item \textbf{Predictive Hallucinations:} Inference of patient data (e.g., lab values or clinical parameters) based on patterns or disease distributions rather than facts present in the original text. These hallucinations introduce plausible but inaccurate values by assuming the patient has characteristics common to others with similar conditions.
    
%     \item \textbf{Quantitative Hallucinations:} Incorrect numerical data or misinterpretation of quantitative information. This encompasses errors in dosages, lab values, statistical data, or epidemiological figures.
    
%     \item \textbf{Causal and Mechanistic Hallucinations:} Incorrect descriptions of cause-effect relationships or physiological mechanisms. This category includes misrepresentations of disease etiology, drug interactions, or bodily processes.
    
%     \item \textbf{Spatial and Structural Hallucinations:} Misrepresentation of anatomical structures or bodily spatial relationships. This involves errors in describing organ locations, tissue structures, or spatial aspects of medical imaging.
    
%     \item \textbf{Procedural and Protocol Hallucinations:} Misstating medical procedures or treatment protocols. This category encompasses errors in describing surgical techniques, diagnostic procedures, or clinical guidelines.

%     \item \textbf{Abbreviation Hallucinations:} Misinterpretation or incorrect expansion of medical abbreviations or acronyms. This category includes errors where abbreviations are mistakenly assigned meanings that deviate from their standard interpretations, leading to misunderstandings in medical documentation or communication. For example, "AKA" could be misinterpreted as "alcoholic ketoacidosis" instead of "above-knee amputation"
% \end{enumerate}


% \subsection{LLMs in Medicine}\label{subsec1}

% Large Language Models (LLMs) have rapidly become integral tools in the medical field, offering unprecedented capabilities in understanding and generating human-like text. Their applications range from assisting in clinical documentation to supporting diagnostic decisions and enhancing patient engagement. Here, we explore the current landscape of LLMs in medicine, highlighting their training methodologies, applications, and the implications for healthcare delivery.

% \begin{enumerate}
%     \item LLMs pre-trained with medical data
%     \item Applications of LLMs in healthcare: medical notes summarizing, medical code, diagnosis prediction: Gemini in medicine \citep{saab2024capabilitiesgeminimodelsmedicine}, 
%     \item Interactive diagnosis making/patient facing frameworks: \citep{li2024agent, huang2024tool, li2024mediq, tu2024conversationaldiagnosticai}
%     \item (maybe) trustworthiness and reliance on LLMs for medical information, fairness, etc.
%     \item Language models generating synthetic data for training in medicine.
% \end{enumerate}


% Large language models in medicine \cite{thirunavukarasu2023large}

% \subsection{The Challenge of Hallucinations}\label{subsec2}

% Despite the advancements and potential benefits of LLMs in medicine, the phenomenon of hallucinations presents a significant challenge. Hallucinations in LLMs refer to instances where the model generates content that is plausible-sounding but factually incorrect, nonsensical, or ungrounded in the input data \citep{}. In the medical domain, such hallucinations can have severe implications, potentially leading to misdiagnoses, inappropriate treatments, and compromised patient safety.

% \subsubsection{Existing Strategies}

% Augmented non-hallucinating large language models as medical information curators \cite{gilbert2024augmented}

% Detecting and Evaluating Medical Hallucinations in Large Vision Language Models \cite{chen2024detecting}

% Med-HALT: Medical Domain Hallucination Test for Large Language Models \cite{umapathi2023med}

% Towards Mitigating LLM Hallucination via Self Reflection \cite{ji2023mitigatinghallucinationlargelanguage}

% \subsection{Scope and Objectives of the Review}
% This paper aims to provide a comprehensive review of the current understanding of hallucinations in medical LLMs, including their causes, detection methods, and mitigation strategies. By examining the latest research and practical applications, we seek to:

% \begin{enumerate} \item \textbf{Characterize Medical Hallucinations:} Develop a detailed taxonomy that categorizes different types of medical hallucinations based on their origins, impact, and detectability with corresponding previous works. \item \textbf{Evaluate Detection Methods:} Review and assess the effectiveness of current techniques for detecting hallucinations in LLMs, with a focus on their applicability to the medical domain. \item \textbf{Propose Mitigation Strategies:} Offer practical solutions for reducing the incidence of medical hallucinations, including Retrieval-Augmented Generation (RAG) methods and the implementation of specialized LLM-Verifier models. \item \textbf{Impact Assessment:} Analyze the potential impact of medical hallucinations on patient care and suggest pathways for integrating AI-driven solutions into clinical practice responsibly. \end{enumerate}