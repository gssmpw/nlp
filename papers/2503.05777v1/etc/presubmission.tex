\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{lineno}
\usepackage{titling}

\setlength{\droptitle}{-2em}
\title{\Large Illusions in Medical AI: Understanding and\\ Mitigating Hallucination in Medical Large\\ Language Models}
% \author{Anonymous Authors}
\date{}

\begin{document}

\maketitle

\begin{abstract}
\begin{linenumbers}
\noindent The advent of Foundation Models, particularly Large Language Models (LLMs),
has revolutionized medical fields through sophisticated multi-modal understand-
ing and generation. However, the phenomenon of hallucinations—where models
generate confident yet erroneous or fabricated information—poses significant
challenges, especially in medical contexts where precision is critical. To this end,
our paper introduces the concept of Medical Hallucination: instances where
LLMs produce incorrect or misleading medical information that can adversely
affect clinical decision-making. We systematically differentiate medical halluci-
nations from general hallucinations by examining their unique characteristics,
underlying causes, and implications, emphasizing their critical importance in
healthcare due to potential life-threatening consequences. Our analysis identifies
key contributing factors, including data biases, limitations in training processes,
and the inherent complexity of medical knowledge. To address these challenges,
we propose practical solutions focusing on two primary strategies: (1) Retrieval-
Augmented Generation (RAG), integrating curated medical knowledge bases, and
(2) an LLM-Verifier approach, utilizing specialized models for cross-checking gen-
erated medical information. Additionally, we delineate methods for detecting and
identifying medical hallucinations, and investigate refinements in training dataset
curation, advanced model training methodologies, and multi-modal collaboration
approaches. Our findings provide a comprehensive framework for understanding,
detecting, and mitigating medical hallucinations in LLMs, aiming to enhance the
safety, reliability, and clinical applicability of AI technologies in healthcare set-
tings. This research offers tangible pathways to improve patient safety, optimize
clinical decision support systems, and foster the responsible integration of AI in
medical practice.
\end{linenumbers}
\end{abstract}

\thispagestyle{empty}
\setcounter{page}{1}
\vspace*{\fill}
\begin{center}
1
\end{center}

\end{document}