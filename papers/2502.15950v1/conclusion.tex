%todo: add more content.

\section{Conclusion and Future Work}

This work introduced the Mixture of Data Experts approximation which advances pre-training data mixture optimization. By leveraging MDE as a predictive feature in regression models, we improve the mixture ranking quality, loss prediction accuracy, and sample efficiency of regression. Our findings emphasize the value of task-aware mixture optimization, showing that incorporating end-task validation signals leads to notable improvements on downstream tasks. Two directions emerge as natural next steps for this research:

\paragraph{Iterative Bayesian optimization process}

In our work, all mixtures used to construct the regression model were generated in a single batch ahead of time. Conversely, an iterative approach could dynamically select mixtures, leveraging performance feedback to refine the selection process. The MTGP model provides a confidence measure alongside predictions, enabling the construction of an objective function that balances exploitation and exploration, such as GP-UCB \cite{gp-ucb}. This allows for an iterative Bayesian optimization process, where new mixtures are proposed based on model uncertainty and then evaluated, which could help finding the optimal mixture more accurately with fewer proxy model pretraining runs. 


\paragraph{Predicting downstream task performance}

We showed that there is a strong correlations between cross-entropy loss on suitable validation domains and downstream task generation and ranking accuracy. Our approach can be extended to regression models that predict downstream task generation accuracy directly. MDE features computed from sequence probabilities of correct or model candidate responses on related tasks have the potential to substantially aid the prediction of downstream accuracy metrics.


