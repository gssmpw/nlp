
We overview the datasets used for language model training, validation domains for generalization loss estimations, and few-shot downstream tasks. 


\paragraph{Language model training datasets}

We train Transformer language models on the SlimPajama dataset~\cite{cerebras2023slimpajama}, treating the seven top-level domains as different sources for training dataset mixtures. We split the documents into segments of at most 1024 tokens according to the Gemma~\cite{gemma2} text-only SentencePiece~\cite{kudo-richardson-2018-sentencepiece} tokenizer with a vocabulary size of 256,000 tokens. 


\paragraph{Validation domain datasets}

We use samples from the development subsets of the SlimPajama dataset as one source of validation domains for generalization loss estimation. We term these \textsc{sp}  validation domains.
Additionally, we use ARC \cite{arc}, OpenBookQA \cite{openbookqa}, and MultiRC \cite{multirc}, covering question answering, commonsense reasoning, and reading comprehension, as validation sets for generalization loss estimation.
ARC has two subsets, Easy and Challenge, which we will refer to as ARC-E and ARC-C respectively.
We use separate downstream tasks for validation and final evaluation to prevent overfitting towards specific datasets. There are a total of 11 validation domains from end tasks,\footnote{We prepare the 4 end tasks in 0-shot, 1-shot, and 5-shot formats and treat each task-format combination as a domain. We discard 5-shot MultiRC because texts are often too long to fit into the 1024 token segment size, resulting in 11 domains.} which we term \textsc{et} (from end task) validation domains. The loss on each of these \textsc{et} domains as defined through the next-token probabilities from the language model, considering  the concatenation of each prompt and gold response as a single sequence. The number of tokens per domain is in Appendix ~\ref{sec:appendix_data}.


\paragraph{Downstream evaluation datasets and settings}

We evaluate models on a test suite of $10$ downstream tasks.
For \textit{generation}, we use TriviaQA \cite{triviaqa}, NaturalQuestions \citep[NQ;][]{naturalquestions}, WebQuestions \citep[WQ;][]{webquestions}, SQuAD 2.0 \cite{squad2}, and LAMBADA \cite{lambada}, covering question answering, reading comprehension and word prediction tasks.
For \textit{ranking} (multiple-choice question) tasks, we use COPA \cite{copa}, PIQA \cite{piqa}, WiC \cite{wic}, WinoGrande \cite{winogrande}, and HellaSwag \cite{hellaswag} spanning across question answering and commonsense reasoning. We prepare all the tasks in $0$-shot, $1$-shot, and $5$-shot formats, and report exact match (EM) accuracies for generation tasks and standard accuracies for ranking tasks.

%For evaluation on the ranking tasks we compare models' generation probabilities of different candidate answers and treat the highest-scoring ones as model predictions, which are compared to the reference answers to calculate standard accuracies. \kt{commenting out because now it seems to me people probably know how ranking tasks are evaluated}


