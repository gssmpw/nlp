
\noindent
We consider baselines and methods from prior work, including:

\begin{itemize}[leftmargin=*, itemsep=2pt, parsep=0pt, topsep=0pt]
    \item \textbf{Empirical Mean (baseline)}: Average loss per domain for any mixture.
    \item \textbf{DML} \cite{dml}: Predicts mixture loss with $L_i(\lambda_{1..k}) = c_i + k_i \exp (\sum_{j=1}^k t_{ij}\lambda_j)$.
    \item \textbf{BiMix} \cite{bimix}: Models validation loss using data quantity and mixing weight, and given a fixed quantity the formula is $L_i(\lambda_i) = \frac{A_i}{\lambda_i^{\alpha_i}}$.
    \item \textbf{Gradient Boosting} \cite[GBM-RegMix;][]{regmix}: Uses ensembles of regression trees to predict mixture losses.
    \item \textbf{Linear Model} \cite{regmix}: Predicts losses via regularized weighted sum of features.
\end{itemize}

\noindent
and our models, including:

\begin{itemize}[leftmargin=*, itemsep=2pt, parsep=0pt, topsep=0pt]
    \item \textbf{MDE}: Predicts losses directly with Mixture of Data Experts.
    \item \textbf{MTGP}: Uses Multi-task Gaussian Process regressors.
    % See Appendix \ref{ref:appendix_fitting_regression_models} for details.
    \item \textbf{X-\textsc{mde}}: Denotes any model \textit{X} that uses mixture weights and MDE as features.
\end{itemize}


See Appendix~\ref{ref:appendix_fitting_regression_models} for details on hyperparameters and software packages used.