\section{Related work}
\label{sec:related}
There is an extensive body of work on data selection and mixture optimization for pretraining language models. ____ offer a comprehensive recent survey. Approaches for data selection and cleaning consider different granularities of data, such as token-level, sample-level (individual documents or sentences can be selected or weighted), and group-level (where we consider samples in large groups assumed to have common characteristics, often derived from meta-data such as the web domain (like Wikipedia) or source collection name (such as C4).

Closest to our focus is work selecting or sampling data at the level of large sample groups, often termed domains. We will limit our overview to methods optimizing such group-level data mixture sampling rates. Data mixture sampling rates can be static over the course of model training, or dynamic, forming a curriculum over sampling rates which could for example facilitate faster progress through learning easier skills first. Dynamic mixtures for pre-training have been considered in e.g. ____; we focus on static mixtures in this work.





\paragraph{Online optimization of domain mixture rates through proxies}
{\doge}____ presents an efficient method to optimize data mixture rates through a first-order bi-level optimization approach.  {\doge} showed successful optimization of the average hedlout domain loss through proxies of size 124M parameters and smaller, with compute cost 2x the cost of training a single proxy model. Our approach is simpler to implement as it does not require changes in the training algorithm for language models, and also offers the possibility to derive optimal weights for a set of different criteria while reusing the same proxy models. {\doremi}____ also proposes an online method which optimizes a loss derived from training data, and has similar computational requirements to those of {\doge}.  For comparison, we train full-scale models with mixture rates optimized through {\doge} and {\doremi} and report results in Section~\ref{sec:experiments}.


\paragraph{Regression-based optimization through proxies}

Multiple methods that fit regression models to predict the performance of unseen mixtures have been devised. Some make predictions based on the domain mixture rates as features ____, while others additionally extrapolate across number of tokens ____, or both token and model size scaling parameters ____. Our work is most similar to \textsc{RegMix}____, in that we approximate the rankings of full-sized models through extrapolation from smaller proxy model, assuming that data mixture rankings at different scales are sufficiently similar (that they are approximately rank-invariant relative to parameter and data quantity scaling). While we also confirm approximate rank-invariance for small proxies when comparing mixtures based on loss on a single training domain, we find that optimizing mixtures according to aggregate metrics on diverse domains both from the training and unseen domains greatly benefits from larger proxy models and token horizon.  We show that we can effectively optimize mixtures based on up to 25 proxy models with the addition of a highly predictive and efficient to compute source of features through our MDE approximation. We also compare our regressors to the regressors used in \textsc{BiMix} and \textsc{DML}, and show that MDE features aid prediction for multiple regression model parametric families including ones from prior work.


\paragraph{Generalization losses driving model updates}

____ proposed optimizing the worst-case gap across training domains with respect to a reference model and showed that this objective led to strong few-shot end task prediction performance. \doge____ proposed to optimize toward average loss across held-out data from training domains and showed that this resulted in a model with a lower average loss than models trained with \doremi. Other works propose to optimize toward validation loss on a single training domain____, or a single domain from a different higher-quality data collection____. We propose to define the generalization loss to optimize as an aggregate over both training domain heldout data, and validation examples from end tasks. We analyze the correlation between different losses and downstream task generation/ranking performance.

%\todo{Check details BiMix RegMix DML} 



\paragraph{Approximation to models trained on data mixtures}

____ proposed to train models independently on two sets of data $A$ and $B$, resulting in parameters $\theta_1$ and $\theta_2$, and to approximate a model trained on their union $A \cup B$ with the average of the models' parameters. This work shares the intuition of our approach that we can use models trained on data domains independently to approximate a model trained jointly.  It  considered a discrete, small set of source proportion configurations for up to three sources and applied to models not pre-trained from scratch, but continuously pre-trained from a common ancestor. As prior work has shown (e.g. ____), parameter-averaging fine-tuned models can work well due to model parameters lying in the same basin of the loss landscape____; however, parameter averaging can lead to nonsensical results for models pre-trained from scratch, as we also verify in our ablations Appendix~\ref{sec:model_merge}.

Our  MDE approximation is much more efficient for use in optimizing data mixtures, as it does not require running inference with neural models for every candidate mixture evaluation. We show how to use this approximation on its own or in combination with a regression model as a source of additional features to choose approximately optimal data mixtures from an infinite set of possible configurations.