\section{Introduction}
\label{intro}

%What is the problem?

Datasets used for pre-training language and multimodal models are often heterogeneous, with distinct sources having different quality, number of available documents, combination of modalities and styles, and relevance to end tasks of interest. Different data sources are often sampled at different rates during training, effectively up-weighting or down-weighting individual mixture components.


\begin{figure*}[!tp] % Use figure* instead of figure
    \begin{center}
        \includegraphics[width=\textwidth]{figures/MDE_diagram.png} % Use \textwidth
        \caption{Illustration of our approach. Data experts $E_i$ are trained from individual pre-training mixture domains $D_i$. The per-token $p_{\text{MDE}}$ approximations are generated as a $\lambda$-weighted average of the probabilities predicted by the individual experts. Then, for each validation domain, the \emph{MDE feature} is computed as the average of log-probability under $p_{\text{MDE}}$ across its tokens. Lastly, the mixture weights $\lambda$ and the MDE features are used to fit a regression model that maps $\lambda$ to predicted validation losses. The optimal set of weights are found by optimizing an objective function based on the regression model.}
        \label{fig:mde_illustration}
    \end{center}
    \vskip -0.2in
\end{figure*}


Prior work has shown that source sampling proportions have a large impact on the generalization performance of the model, both on cross-entropy of held-out examples from the training sources, and accuracy on downstream tasks~\citep[][\textit{inter alia}]{Hashimoto2021ModelPS, doremi, flamingo, albalak2023efficientonlinedatamixing}. 





The sampling proportions of a data mixture with $k$ source domains define $k-1$ real-valued hyper-parameters. It is infeasible to evaluate the performance of many mixtures for large language models trained on sequences of hundreds of billions of tokens and the largest models are typically trained only once with the best data mixture guess.
The problem could be viewed as a bi-level optimization process which is known to be computationally challenging, both in the worst-case~\citep{grune2024completenesspolynomialhierarchynatural, bolte2025geometriccomputationalhardnessbilevel}, and in practice due to the difficulty of evaluating gradients, which require solving a non-convex minimization in the inner loop. In practice, most large-scale pre-training efforts rely on heuristics~\cite{gao2020pile800gbdatasetdiverse}.



Approaches that optimize mixtures to improve generalization loss are based on proxy models, which are smaller in number of parameters and  tokens seen than the target model of interest.  Based on proxy models, data mixtures can be optimized through an online algorithm~\cite{DOGE,doremi}, or offline, through observing the generalization loss of multiple trained proxy models, and predicting the loss of other mixtures through regression models. Mixtures are optimized to minimize loss according to the trained regressors~\cite{regmix,dml,bimix}.



Regression models observe the generalization losses $s(\lambda_1),\ldots,s(\lambda_N)$ achieved by proxy language models $\theta_1,\ldots,\theta_N$, trained with the the corresponding data mixtures $\lambda_1,\ldots,\lambda_N$. Their goal is to predict the generalization loss for unseen mixtures $\lambda$, without training proxy models for those new mixtures.  Regression-based methods are simpler to implement, as they do not require changes in the LM training algorithm.  They also have the advantage that the  same set of trained proxy language models can be used to optimize data mixtures for multiple loss criteria. On the other hand, these approaches require an up-front cost of training multiple (usually 30 to 500) proxy models $\theta_n$.


We show how such regression models can be significantly improved through the use of a new Mixture of Data Experts approximation ({\mde}).  {\mde} is a simple predictor which requires the training of only $k$ proxy models, where $k$ is the number of source domains. Each of these models (termed \emph{data experts}) is trained on data from a single domain $D_i$. Using these expert models, for each candidate $\lambda$, we define the predictor
$f_\text{MDE}(\lambda)$ as the loss obtained by an ensemble model over the experts using mixture weights $\lambda$. 

Figure~\ref{fig:mde_illustration} illustrates the method. We define generalization losses for mixtures through aggregation of cross-entropy loss on multiple validation domains. {\mde} can be used on its own or as a source of features in regression models (one feature value for each validation domain). 

A simple theoretical analysis justifies the aggregation of predictions from data experts to approximate the outcome of actually training a language model with data mixture weights $\lambda$ and identifies directions to improve upon MDE (see Section \ref{sec:theory}).



Our results indicate that the MDE approximation leads to substantial improvement in mixture ranking quality across multiple regression models. We evaluate the contribution to linear models, gradient boosting machines (GBM), and multi-task Gaussian process models (MTGP). Ranking is improved across all regression models (e.g. Spearman's correlation improved from \textbf{0.65 to 0.95} for linear regressors, and \textbf{0.81 to 0.95} for GBM). MDE can also be used to optimize data mixtures on its own, thus requiring the training of only $k$ proxy models to achieve performance comparable to regressors from prior work at \textbf{3x less computational cost}.



We perform experiments with Transformer decoder-only language models of sizes 70M, 150M, 280M, 510M, and 1B parameters (including embedding ones), using the SlimPajama~\cite{cerebras2023slimpajama} dataset, and training models for up to 100B tokens. We show that mixture rates selected based on a regression model trained from 25 examples of validation losses from proxy models of size 280M trained to 5B tokens, lead to better generalization losses for 1B models trained on 100B tokens, compared to the mixture weights optimized for the same dataset by baselines including {\doge}~\cite{DOGE} and {\doremi}~\cite{doremi}. 



We further define a generalization loss on SlimPajama validation domains and task-relevant validation examples and optimize mixture weights based on this loss, showing that the resulting mixtures outperform heuristic baselines and prior data mixture optimization methods on average few-shot downstream task accuracies over a suite of generation and ranking tasks.




