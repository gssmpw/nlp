\section{Experimental Setup}\label{sec:exp_setup}

\subsection{Model}

For all experiments, we use a Decoder-only Transformer architecture. Specifically, for all experiments except for pretrained models settings, we use the Llama architecture~\citep{llama3modelcard}, except we remove the rotary positional encoding. 
For the inputs format, we have one example per line, and stack all example on the batch dimension. Since the examples can have variable length, we pad each line on the right to the maximum length in the batch. 
We exclusively use a character level tokenizer. For pretrained models, we replace the default tokenizer with our character tokenizer, while keeping the embedding component of the pretrained model unchanged. 

\begin{table}[ht!]
\caption{Model Parameters}
\footnotesize
\vspace{1mm}
\centering
\small
\setlength{\tabcolsep}{4pt} %
\renewcommand{\arraystretch}{0.5}
     {
        \begin{tabular}{ccccc} %
        \toprule
        Model & Self-Attn Layers & Num Heads & Embedding Dim \\
        \midrule 
        From-Scratch &  6 &  6 &  384 \\
        Llama 3 1B &  24 &  16 &  1024 \\
        Llama 3 3B &  32 &  32 &  2048 \\
        \bottomrule
        \end{tabular}
    }
\label{table:model_config}
\end{table}

\subsection{Data Formats and Data Sampling}\label{sec:data_gen}
\subsubsection{Maze-Solving}\label{sec:data_gen-maze}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.30\linewidth]{fig/maze/maze_example_hops4_N30.pdf}
    \hspace{1mm}
    \includegraphics[width=0.30\linewidth]{fig/maze/maze_example_hops13_N30.pdf}
    \hspace{1mm}
    \includegraphics[width=0.33\linewidth]{fig/maze/maze_example2.pdf}
    \caption{Maze-solving task with \( N=30 \) nodes. (Left \& Middle) Visualization of the maze task with 4 hops (ID) and 13 hops (OOD). (Right) Example of the data format: the input specifies the start and end nodes along with the graph structure, and the output lists the shortest path as hops. The labeled training dataset includes paths of up to 9 hops, with difficulty increased by adding one hop in each subsequent round.}
    \label{fig:maze_data}
\end{figure}



\begin{listing}
\begin{minted}[
framesep=2mm,
baselinestretch=0.9,
bgcolor=LightGray,
fontsize=\scriptsize,
linenos
]{python}
def create_tree_with_hops_wilson(total_nodes, num_hops):
    import networkx as nx

    # Step 1: Create the main path with num_hops
    graph = nx.path_graph(num_hops + 1) 

    # Step 2: Add extra nodes to the tree with random walk
    current_nodes = list(graph.nodes())
    new_nodes = list(range(num_hops + 1, total_nodes))

    while new_nodes:
        new_node = new_nodes.pop()
        # random walk to reach graph
        walk = [new_node]
        while walk[-1] not in current_nodes:
            # choose random node from current & new nodes
            random_node = random.choice(current_nodes + new_nodes)
            walk.append(random_node)
            if random_node in new_nodes:
                new_nodes.remove(random_node)
        # add edges
        for i in range(len(walk) - 1):
            graph.add_edge(walk[i], walk[i + 1])
        current_nodes.append(new_node) 

    # Step 3: Set the start and end nodes for the main path
    start_node = 0
    end_node = num_hops

    return graph, start_node, end_node

def format_graph(graph, start_node, end_node):
    # Assign random labels to nodes
    node_labels = assign_labels(graph.nodes(), label_range=(1, 99))
    
    # Get the shortest path (in terms of edge count) from start_node to end_node
    shortest_path = nx.shortest_path(graph, source=start_node, target=end_node)

    # Format the path as a string
    path_labels = [node_labels[node] for node in shortest_path]
    path_string = ">".join(map(str, path_labels))
    
    # Format start and end nodes
    start_label = node_labels[start_node]
    end_label = node_labels[end_node]
    start_end_str = f"{start_label}>{end_label}#"

    # Build graph_str with end_node connections at the end
    graph_str = ""
    start_node_str = ""  # Temporary storage for the start_node part
    end_node_str = ""  # Temporary storage for the end_node part
    
    # randomize the order of nodes
    random_nodes = list(graph.nodes())
    random.shuffle(random_nodes)
    for node in random_nodes:
        node_label = node_labels[node]
        # randomize the order of neighbors
        random_neighbors = list(graph.adj[node])
        random.shuffle(random_neighbors)
        neighbor_labels = [node_labels[neighbor] for neighbor in random_neighbors]
        graph_str += f"{node_label}:" + ",".join(map(str, neighbor_labels)) + "-"

    # Combine everything, placing the end_node last
    graph_str = start_node_str + graph_str + end_node_str

    return start_end_str + graph_str[:-1] + "=", path_string, node_labels
    
\end{minted}
\caption{Code for the maze format generation used}
\label{listing:code_maze_generation}
\end{listing}


\newpage
\subsection{Experimental Settings}\label{sec:appendix_setting}

\subsubsection{Hyperparameter Configurations}

In this section, we provide a detailed overview of the hyperparameter configuration used in our experiments in Table~\ref{table:hyperparam_base} and \ref{table:hyperparam_si}. To enhance memory efficiency and training speed, we employ flash attention and tf32, bfloat16. Our experiments are run using PyTorch 2.4 and CUDA 12.1. Detailed dependencies are provided in our github repository\footnote{\url{https://github.com/JackCai1206/arithmetic-self-improve/}}. We use Warmup stable decay~\citep{wen2024understanding} as the learning rate schedule. In table~\ref{table:hyperparam_base} and ~\ref{table:hyperparam_si}, the number of constant LR steps is equal to the total training steps minus the sum of warmup and decay steps. We use AdamW optimizer with betas (0.9, 0.99) and epsilon $1e-12$. Weight decay is fixed to 0.1 and we do not use dropout. 

Table~\ref{table:hyperparam_base} shows the training hyperparameters for the initial training phase on labeled data $D_0$. Table~\ref{table:hyperparam_si} shows the hyperparameters for each the self-improve training rounds on $D_{1,\dots,R}$. 

\begin{table}[ht]
\caption{Hyperparameters for initial training on labeled data}
\footnotesize
\vspace{1mm}
\centering
\small
\setlength{\tabcolsep}{4pt} %
\renewcommand{\arraystretch}{0.5}
{
\begin{tabular}{cccccc}
\toprule
Task & Batch Size & LR & Iterations & Warmup Iter & Decay Iter \\
\midrule 
Reverse Addition &  1024 &  5e-4 & 10000 & 1000 & 2000  \\
Reverse Addition (Llama 3 3B) &  128 &  1e-4 & 1200 & 120 & 600  \\
Reverse Addition (Llama 3 1B) &  128 &  1e-4 & 1200 & 120 & 600  \\
Copy/Reverse & 1024 &  5e-4 & 5000 & 500 & 1000  \\
Forward Addition & 1024 &  5e-4 & 10000 & 1000 & 1000  \\
Multiplication & 1024 &  5e-5 & 10000 & 1000 & 2000  \\
Maze (hops) & 1024 &  5e-4 & 25000 & 2500 & 3500  \\
Maze (nodes) & 512 &  5e-4 & 12000 & 1200 & 2800  \\

\bottomrule
\end{tabular}
}
\label{table:hyperparam_base}
\end{table}


\begin{table}[ht]
\caption{Hyperparameters for self-improvement rounds}
\footnotesize
\vspace{1mm}
\centering
\small
\setlength{\tabcolsep}{4pt} %
\renewcommand{\arraystretch}{0.5}
{
\begin{tabular}{cccccc}
\toprule
Input Format & Batch Size & LR & Iterations & Warmup Iter & Decay Iter \\
\midrule 
Reverse Addition &  1024 &  5e-4 & 1500 & 0 & 1500  \\
Reverse Addition (Llama 3 3B) &  128 &  1e-4 & 600 & 0 & 600  \\
Reverse Addition (Llama 3 1B) &  128 &  1e-4 & 600 & 0 & 600  \\
Copy/Reverse & 1024 &  5e-4 & 500 & 0 & 500  \\
Forward Addition & 1024 &  5e-4 & 3000 & 0 & 1000  \\
Multiplication & 1024 &  5e-5 & 3000 & 0 & 1000  \\
Maze (hops) & 1024 &  2e-4 & 5000 & 500 & 1000  \\
Maze (nodes) & 512 &  2e-4 & 4000 & 400 & 1000  \\
\bottomrule
\end{tabular}
}
\label{table:hyperparam_si}
\end{table}



\subsubsection{Self-Improvement Setting for each Task}

\paragraph{Reverse Addition.} 
The initial supervised dataset $\mathcal{D}_0$ contains 2 million examples of reverse addition, with operand lengths ranging from 1 to 16 digits. This dataset is used to train the model for 10,000 steps. In subsequent self-improvement rounds, we sample 50,000 additional training examples at each round, extending the operand length by one digit. Specifically, at self-improvement round \( r \), the self-generated data $\mathcal{D}_r$ consists of length-\( 16+r \) examples produced by the model \( M_r \). The model is fine-tuned on the combined dataset $\mathcal{D}_0 \cup \mathcal{D}_1 \cup \dots \cup \mathcal{D}_r$ for 1,500 steps, resulting in an improved model \( M_{r+1} \).



\paragraph{String Copy \& String Reverse.} 
The initial training set $\mathcal{D_0}$ consists of 2 million examples of strings of length 1 to 10. The vocabulary of the string is the digits $0$ to $9$. For each subsequent round $r$, we sample $D_{r}$ consisting of $50,000$ examples of length $10+r$ from the model $M_r$. Then we continue training $M_r$ on the combined dataset $D_1\cup\dots\cup D_r$ for 500 steps to obtain $M_{r+1}$. 


\paragraph{Forward Addition.}
The models are initially trained on a dataset $\mathcal{D}_0$ containing 2 million labeled examples of forward addition, with operand lengths ranging from 1 to 10 digits. This initial training phase spans 10,000 steps. In each subsequent self-improvement round, we generate 50,000 additional training examples, incrementally extending the operand length by one digit. Specifically, at self-improvement round \( r \), the self-generated dataset $\mathcal{D}_r$ contains length-\( 10+r \) examples produced by the model \( M_r \). The model is then fine-tuned for 3,000 steps on the combined dataset $\mathcal{D}_0 \cup \mathcal{D}_1 \cup \dots \cup \mathcal{D}_r$, resulting in an updated model \( M_{r+1} \).


\paragraph{Multiplication.}
The model is initially trained on 5 million  $n$-by-$n$ multiplication examples with $n=5$. Directly introducing $n+1$-by-$n+1$ examples results in poor performance, hence, we adopt a more fine-grained difficulty schedule. In each self-improvement round, we incrementally increase one operand by one digit, sampling $n+1$-by-$m$ and $m$-by-$n+1$ examples, where $m$ grows from 1 to $n+1$. This gradual progression allows the model to adapt incrementally to larger operand sizes, making the transition to harder examples more manageable.

For data filtering, we use the following setting:
for length filtering, we remove self-generated samples where the output length is shorter than the longest output in the batch by more than 10 tokens. This helps eliminate incorrect solutions that omit intermediate steps. For majority voting, we train five models in parallel using different random seeds and retain only those data points where at least 4 out of the 5 models produce the same output. This strategy ensures that only high-consensus, reliable data points are used for training.



\paragraph{Maze Solving - Increasing Hops.}
The model is first trained on a dataset \( \mathcal{D}_0 \) containing 5 million labeled maze-solving examples, where the number of nodes is fixed at \( N=30 \) and paths range from \( h=1 \) to \( h=9 \) hops. This initial training phase spans 25,000 steps. In subsequent self-improvement rounds, we generate 50,000 additional training examples, increasing \( h \) by 1, and fine-tune the model for 5,000 steps per round. We experiment with both unfiltered training data and majority voting, where only outputs agreed upon by all 3 models are retained.

\paragraph{Maze Solving - Increasing Nodes.}
The model is first trained on a dataset \( \mathcal{D}_0 \) containing 5 million labeled maze-solving examples, with a fixed hop count \( h=9 \) and node counts ranging from \( N=10 \) to \( N=30 \). This initial training lasts 12,000 steps. In self-improvement rounds, the number of nodes \( N \) is increased by 3 per round, generating 50,000 additional training examples at each step and fine-tuning for 4,000 steps. We compare training without filtering against majority voting, where only outputs agreed upon by all 3 models are kept.

\paragraph{Ablation Task - Pretrained Models.}
To maintain consistency in tokenization, we use character-level tokenization instead of the default tokenizer of the Llama models. We use LoRA~\citep{Hu2021LoRALA} with $r=64$ and $\alpha=128$ for Llama-1B, and $r=32$ and $\alpha=128$ for Llama-3B. 
In the initial round, we train for 1200 steps with a learning rate schedule that includes 10\% warm-up steps to a constant learning rate of \( 1\text{e-}4 \), followed by 20\% cosine decay steps to a final learning rate of \( 1\text{e-}6 \). For subsequent rounds, we train for 600 steps per round using a cosine decay learning rate schedule without warm-up, starting at \( 1\text{e-}4 \) and decaying to \( 1\text{e-}6 \).
