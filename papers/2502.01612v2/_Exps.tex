\section{Preliminaries and Experimental Setup}\label{sec:prelim}








\begin{table*}%
\centering
\caption{Examples of Tasks Considered %
}
\label{tab:task_examples}
  \vspace{-1mm}
  \centering
  \small
    \setlength{\tabcolsep}{4pt} %
    \renewcommand{\arraystretch}{1.0}
		 %
		 %
		 {
\begin{tabular}{c|l|c}
\toprule
\textbf{ Task Type} & \multicolumn{1}{c|}{\textbf{Input} (\textbf{Q}: Prompt, \textbf{A}: label) } & \textbf{Task Difficulty} \\ \hline
Reverse Addition & Q: 31558+91786= \, A: 232451  & \multirow{3}{*}{ \shortstack[c]{\vspace{1mm} \\ Max digit length of \\the two operands}}  \\ %
Forward Addition & Q: 85513+68719= \, A: 154232 &  \\ %
\shortstack[l]{Multiplication \\ \quad } & \shortstack[l]{Q: 34895*148= \\ \quad} \hspace{4mm} \shortstack[l]{A: 348950+0273932(3653542)\\ \quad +00447874=36972305} & \\ \midrule
Copy & Q: 12345= \hspace{10mm} A:12345 & \multirow{2}{*}{Length of string} \\ %
Reverse & Q: 12345= \hspace{10mm} A: 54321 &  \\ \midrule
Maze Solving &
\begin{tabularx}{0.6\linewidth}{L@{}L@{}}\includegraphics[width=0.2\textwidth]{fig/maze/maze_example_hops5_N8_v2.pdf} & \hspace{-12mm} \shortstack[l]{Finding shortest path from node {\color{blue}2} to {\color{red}19}\\ {\scriptsize($\leftarrow$ example image for illustration)}\\
\vspace{2mm} \\Q: {\color{blue}2}>{\color{red}19}\#73:70,75-97:2,70-70:73,97,59\\-75:73,30,19-2:97-30:75-59:70-19:75=\\ \vspace{3mm} \\ A: 2>97>70>73>75>19 \vspace{4mm} }\hspace{-5mm} 
\end{tabularx} 
& \shortstack[l]{ (1) Number of hops \\ between start \& end \\ \vspace{1mm} \\ (2) Number of nodes} \\ %

\bottomrule
\end{tabular}
}
\vspace{-3mm}
\end{table*}


In this section, we describe the experimental setup, including the model architecture, tasks, training methodology, evaluation criteria, and the self-improvement framework.

\paragraph{Models. }
We adopt the LLaMA architecture with six layers, six attention heads, and an embedding dimension of 384 and a total of 14M parameters. Positional embeddings are excluded, using the No Positional Encoding (NoPE) method~\citep{kazemnejad2024impact}. To evaluate applicability to large language models (LLMs), we extend our experiments to pretrained models (Llama-1B, Llama-3B) in Section~\ref{sec:pretrained}. Character-level tokenization is used across all tasks, except for the maze-solving task, where numbers (0â€“99) are tokenized as individual tokens instead of characters.

\paragraph{Tasks. }
We evaluate our approach on a diverse set of tasks, categorized into arithmetic operations, string manipulation, and maze solving. Table~\ref{tab:task_examples} provides examples for each task.

\begin{itemize}[left=10pt]
\item \textbf{Arithmetic operations:} 
\begin{enumerate} 
\item \textit{Addition} (Section~\ref{sec:reverse_addition},~\ref{sec:forward_addition}):  We consider both reverse and forward addition of two numbers of equal length. In reverse addition, both operands and the answers are reversed, so they are written with the least significant digit first. Forward addition, in contrast, follows the standard format, with the most significant digit first.%
\item \textit{Multiplication} (Section~\ref{sec:mult}): Multiplication tasks are presented in a chain-of-thought (CoT) data format~\citep{deng2024explicit}, which includes intermediate steps to guide the computation. %
\end{enumerate}
\item \textbf{String manipulation:} 
\begin{enumerate} 
\item \textit{Copy} (Section~\ref{sec:string_copy}): The task is to replicate the input sequence exactly. %
\item \textit{Reverse} (Section~\ref{sec:string_copy}): The task is to reverse the input sequence.
\end{enumerate} 
\item \textbf{Maze solving} (Section~\ref{sec:maze}): The task is to find the shortest path between a start node and an end node in a tree-structured graph. The shortest path is defined as the path with the fewest number of hops, where each hop represents a transition between two adjacent nodes.
\end{itemize}

Each task presents distinct challenges that test different aspects of model generalization. Reverse addition~\citep{lee2023teaching} has been widely adopted task for length generalization. Forward addition, by contrast, is significantly harder due to its increasing dependency in length, making it more challenging for transformers to extrapolate~\citep{zhou2023algorithms}. Copying and reversing sequences are considered fundamental operations but remain difficult for standard transformers without architectural modifications~\citep{anil2022exploring,zhou2023algorithms}. Multiplication is challenging even in-distribution~\citep{dziri2024faith}, and fine-tuning large language models with CoT reasoning has shown limited success. Finally, maze solving extends our evaluation to structured reasoning tasks, requiring models to internalize search behavior rather than relying on local token-to-token dependencies~\citep{bachmann2024pitfalls}. These tasks collectively provide a controlled yet diverse testbed for studying the effectiveness of self-improvement across different problem domains.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{fig/Framework_v2.pdf}
    \caption{Illustration of our self-improvement procedure. At each round, the training data is updated with the model's predictions on progressively harder problems. }
    \label{fig:SI_illustration}
\end{figure}

\paragraph{Task Difficulty. }
All tasks we consider admit a straightforward notion of difficulty. 

\begin{itemize}[left=10pt]
    \item \textbf{Addition: } The maximum length of the two operands.
    \item \textbf{Multiplication: } The maximum length of the two operands. Intuitively, a 5-by-5 multiplication problem is harder than that of 4-by-4. But even a 6-by-1 multiplication is considered harder than 5-by-5, because the model has never seen training data containing length more than 5.
    \item \textbf{String Copy \& Reverse: } The length of the input string. 
    \item \textbf{Maze Solving: } We define difficulty as 1) the number of hops between the start and end nodes and 2) the total number of nodes in the graph. The number of hops corresponds to the number of transitions required to reach the goal. 
\end{itemize}

We denote the difficulty level of a problem instance $x$ as an integer $\text{Difficulty}(x)$. 



\paragraph{Data Generation and Sampling. }
We generate an initial supervised training dataset $\mathcal{D}_0$ of up to a fixed difficulty level $d_0$ by uniformly sampling the difficulty level $d \leq d_0$, followed by independent sampling of the data conditioned on the difficulty. Denoting the input as $x_i$, labels as $y_i$,



\begin{align*}
    \mathcal{D}_0=\{(x_i,y_i)\}_{i=1}^{N_0},\quad\text{where 
 }\text{Difficulty}(x_i)\leq d_0. 
\end{align*}

For arithmetic tasks such as addition or multiplication, each problem instance is represented as a tuple $x_i = (a_i, b_i)$, with $\mathcal{D}_0$ containing problems of up to $d_0$-digit numbers. The digit lengths $(d_{a_i}, d_{b_i})$ are uniformly sampled from $\{1, \dots, d_0\}^2$, and the numbers $a_i$ and $b_i$ are uniformly sampled from the ranges $[10^{d_{a_i}-1}, 10^{d_{a_i}}-1]$ and $[10^{d_{b_i}-1}, 10^{d_{b_i}}-1]$, respectively.

For string manipulation tasks (e.g., copying or reversing), we uniformly sample string lengths up to $d_0$ and generate random sequences. Similarly, for maze-solving tasks, we uniformly sample the number of hops or total nodes in the maze and generate random graphs that satisfy these constraints. %
This strategy ensures balanced coverage across all difficulty levels up to $d_0$. %








\paragraph{Self-Improvement Framework. }

The self-improvement framework begins by training a model using the labeled training dataset $\mathcal{D}_0$, which gives us our base model $M_0$. %

For each subsequent round $r$ ($r = 1, 2, 3, \dots$), we increase the problem difficulty, such as the number of digits or string length for arithmetic and string manipulation tasks, or the number of hops for maze-solving tasks, to $d_r$. Using the previous model $M_{r-1}$, we generate $N_r$ new self-improve data samples $\mathcal{D}_r$ defined as:

\begin{align*}
    \mathcal{D}_r = \{(x_i, {\color{blue}M_{r-1}(x_i)})\}_{i=1}^{N_r},\quad \text{where }d_{r-1}\leq \text{Difficulty}(x_i)\leq d_r
\end{align*}
Instead of the true labels $y_i$, we obtain the predicted labels $M_{r-1}(x_i)$ from the output of the model.

At each self-improvement round \( r \), the model is trained on the combined dataset \( \mathcal{D}_0 \cup \mathcal{D}_1 \cup \dots \cup \mathcal{D}_{r-1} \), which includes the initial labeled dataset and all subsequent self-improvement datasets. To ensure sufficient training on the most recently generated data \( \mathcal{D}_{r-1} \), we up-sample it with a sampling probability of 50\%. The remaining datasets \( \mathcal{D}_0, \dots, \mathcal{D}_{r-2} \) are sampled uniformly at random. %
This iterative process allows the model to gradually tackle harder problems, leveraging its own predictions to expand the training data and improve generalization. 

\paragraph{Data Filtering. }
We employ two unsupervised data-filtering methods to refine our self-improvement dataset: 1) length filtering and 2) majority voting.  For a given self-improved dataset \( \mathcal{D}_r = \{(x_i, M_{r-1}(x_i))\}_{i=1}^{N_r} \) at round \( r \), data is filtered based on specific criteria on the model-generated outputs \( M_{r-1}(x_i) \), producing a smaller, refined dataset \( \tilde{\mathcal{D}}_r = \{(x_i, M_{r-1}(x_i))\}_{i=1}^{\tilde{N}_r} \). We given detailed motivations and description in Section~\ref{sec:data_filter}. 



\paragraph{Training and Evaluation. } Except for the experiments on pretrained Llama 3.2 models, all models are trained from scratch using the conventional next-token prediction objective. The loss is computed solely on the completion, meaning that the input prompt is masked, and only the model's predictions are included in the loss computation. Detailed settings, including hyperparameters and training schedules, are provided in the Appendix.

During inference, we use greedy decoding and exact-match accuracy as the primary metric for evaluation. A prediction is deemed correct if all tokens in the output sequence match the ground truth; any discrepancy in the generated tokens is classified as an incorrect prediction.