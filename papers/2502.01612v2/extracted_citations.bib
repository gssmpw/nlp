@article{Alemohammad2023SelfConsumingGM,
  title={Self-Consuming Generative Models Go MAD},
  author={Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Reza Babaei and Daniel LeJeune and Ali Siahkoohi and Richard Baraniuk},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.01850},
  url={https://api.semanticscholar.org/CorpusID:259341801}
}

@article{Arcaute2023CombiningGA,
  title={Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?},
  author={Gonzalo Mart{\'i}nez Ruiz de Arcaute and Lauren Watson and Pedro Reviriego and Jos{\'e} Alberto Hern{\'a}ndez and Marc Ju{\'a}rez and Rik Sarkar},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.01255},
  url={https://api.semanticscholar.org/CorpusID:257280389}
}

@article{Briesch2023LargeLM,
  title={Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop},
  author={Martin Briesch and Dominik Sobania and Franz Rothlauf},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.16822},
  url={https://api.semanticscholar.org/CorpusID:265466007}
}

@inproceedings{Cho2024PositionCI,
  title={Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure},
  author={Hanseul Cho and Jaeyoung Cha and Pranjal Awasthi and Srinadh Bhojanapalli and Anupam Gupta and Chulhee Yun},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273695226}
}

@article{Dohmatob2024ATO,
  title={A Tale of Tails: Model Collapse as a Change of Scaling Laws},
  author={Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.07043},
  url={https://api.semanticscholar.org/CorpusID:267628004}
}

@InProceedings{Hataya_2023_ICCV,
    author    = {Hataya, Ryuichiro and Bao, Han and Arai, Hiromi},
    title     = {Will Large-scale Generative Models Corrupt Future Datasets?},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {20555-20565}
}

@inproceedings{alfarano2024global,
  title={Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers},
  author={Alfarano, Alberto and Charton, Fran{\c{c}}ois and Hayat, Amaury},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}

@article{bansal2022end,
  title={End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking},
  author={Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20232--20242},
  year={2022}
}

@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

@article{charton2024patternboost,
  title={PatternBoost: Constructions in Mathematics with a Little Help from AI},
  author={Charton, Fran{\c{c}}ois and Ellenberg, Jordan S and Wagner, Adam Zsolt and Williamson, Geordie},
  journal={arXiv preprint arXiv:2411.00566},
  year={2024}
}

@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@article{duan2023interpolation,
  title={From interpolation to extrapolation: Complete length generalization for arithmetic transformers},
  author={Duan, Shaoxiong and Shi, Yining and Xu, Wei},
  journal={arXiv preprint arXiv:2310.11984},
  year={2023}
}

@article{dubois2019location,
  title={Location attention for extrapolation to longer sequences},
  author={Dubois, Yann and Dagan, Gautier and Hupkes, Dieuwke and Bruni, Elia},
  journal={arXiv preprint arXiv:1911.03872},
  year={2019}
}

@article{fan2024looped,
  title={Looped Transformers for Length Generalization},
  author={Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook},
  journal={arXiv preprint arXiv:2409.15647},
  year={2024}
}

@article{feng2024beyond,
  title={Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement},
  author={Feng, Yunzhen and Dohmatob, Elvis and Yang, Pu and Charton, Francois and Kempe, Julia},
  journal={arXiv preprint arXiv:2406.07515},
  year={2024}
}

@article{gerstgrasser2024model,
  title={Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data},
  author={Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others},
  journal={arXiv preprint arXiv:2404.01413},
  year={2024}
}

@article{gillman2024self,
  title={Self-Correcting Self-Consuming Loops for Generative Model Training},
  author={Gillman, Nate and Freeman, Michael and Aggarwal, Daksh and Hsu, Chia-Hong and Luo, Calvin and Tian, Yonglong and Sun, Chen},
  journal={arXiv preprint arXiv:2402.07087},
  year={2024}
}

@article{gulcehre2023reinforced,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998},
  year={2023}
}

@article{hase2024unreasonable,
  title={The unreasonable effectiveness of easy training data for hard tasks},
  author={Hase, Peter and Bansal, Mohit and Clark, Peter and Wiegreffe, Sarah},
  journal={arXiv preprint arXiv:2401.06751},
  year={2024}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@misc{huang2024selfimprovementlanguagemodelssharpening,
      title={Self-Improvement in Language Models: The Sharpening Mechanism}, 
      author={Audrey Huang and Adam Block and Dylan J. Foster and Dhruv Rohatgi and Cyril Zhang and Max Simchowitz and Jordan T. Ash and Akshay Krishnamurthy},
      year={2024},
      eprint={2412.01951},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.01951}, 
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2023functional,
  title={Functional interpolation for relative positions improves long context transformers},
  author={Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  journal={arXiv preprint arXiv:2310.04418},
  year={2023}
}

@article{liang2024sheep,
  title={I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm},
  author={Liang, Yiming and Zhang, Ge and Qu, Xingwei and Zheng, Tianyu and Guo, Jiawei and Du, Xinrun and Yang, Zhenzhu and Liu, Jiaheng and Lin, Chenghua and Ma, Lei and others},
  journal={arXiv preprint arXiv:2408.08072},
  year={2024}
}

@article{madaan2024selfrefine,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{newman2020eos,
  title={The EOS decision and length extrapolation},
  author={Newman, Benjamin and Hewitt, John and Liang, Percy and Manning, Christopher D},
  journal={arXiv preprint arXiv:2010.07174},
  year={2020}
}

@misc{pang2024iterativereasoningpreferenceoptimization,
      title={Iterative Reasoning Preference Optimization}, 
      author={Richard Yuanzhe Pang and Weizhe Yuan and Kyunghyun Cho and He He and Sainbayar Sukhbaatar and Jason Weston},
      year={2024},
      eprint={2404.19733},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.19733}, 
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{ruoss2023randomized,
  title={Randomized positional encodings boost length generalization of transformers},
  author={Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Csord{\'a}s, R{\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  journal={arXiv preprint arXiv:2305.16843},
  year={2023}
}

@article{sabbaghi2024explicitly,
  title={Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks},
  author={Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi},
  journal={arXiv preprint arXiv:2406.01895},
  year={2024}
}

@article{schwarzschild2021can,
  title={Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6695--6706},
  year={2021}
}

@article{shin2024weak,
  title={Weak-to-Strong Generalization Through the Data-Centric Lens},
  author={Shin, Changho and Cooper, John and Sala, Frederic},
  journal={arXiv preprint arXiv:2412.03881},
  year={2024}
}

@article{shumailov2023curse,
  title={The curse of recursion: Training on generated data makes models forget},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal={arXiv preprint arXiv:2305.17493},
  year={2023}
}

@article{shumailov2024ai,
  title={AI models collapse when trained on recursively generated data},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  journal={Nature},
  volume={631},
  number={8022},
  pages={755--759},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{singh2023beyond,
  title={Beyond human data: Scaling self-training for problem-solving with language models},
  author={Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and others},
  journal={arXiv preprint arXiv:2312.06585},
  year={2023}
}

@article{song2024mind,
  title={Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models},
  author={Song, Yuda and Zhang, Hanlin and Eisenach, Carson and Kakade, Sham and Foster, Dean and Ghai, Udaya},
  journal={arXiv preprint arXiv:2412.02674},
  year={2024}
}

@article{sun2024easy,
  title={Easy-to-hard generalization: Scalable alignment beyond human supervision},
  author={Sun, Zhiqing and Yu, Longhui and Shen, Yikang and Liu, Weiyang and Yang, Yiming and Welleck, Sean and Gan, Chuang},
  journal={arXiv preprint arXiv:2403.09472},
  year={2024}
}

@article{wang2022selfinstruct,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@inproceedings{yehudai2021local,
  title={From local structures to size generalization in graph neural networks},
  author={Yehudai, Gilad and Fetaya, Ethan and Meirom, Eli and Chechik, Gal and Maron, Haggai},
  booktitle={International Conference on Machine Learning},
  pages={11975--11986},
  year={2021},
  organization={PMLR}
}

@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}

@inproceedings{zelikman2022star,
  title={STaR: Bootstrapping Reasoning With Reasoning},
  author={E. Zelikman and Yuhuai Wu and Noah D. Goodman},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247762790}
}

@article{zhang2023chain,
  title={Chain-of-thought reasoning is a policy improvement operator},
  author={Zhang, Hugh and Parkes, David C},
  journal={arXiv preprint arXiv:2309.08589},
  year={2023}
}

@article{zhang2024transcendence,
  title={Transcendence: Generative Models Can Outperform The Experts That Train Them},
  author={Zhang, Edwin and Zhu, Vincent and Saphra, Naomi and Kleiman, Anat and Edelman, Benjamin L and Tambe, Milind and Kakade, Sham M and Malach, Eran},
  journal={arXiv preprint arXiv:2406.11741},
  year={2024}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{zhou2024transformers,
  title={Transformers can achieve length generalization but not robustly},
  author={Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.09371},
  year={2024}
}

