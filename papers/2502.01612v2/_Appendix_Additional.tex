\newpage
\section{Detailed Discussion of Related Work}\label{sec:related_work_extended}

\paragraph{Length Generalization.}
While Transformers~\citep{vaswani2017attention} have achieved remarkable success, they often struggle with length generalization—where a model trained on problems of fixed length fails to extrapolate to longer sequences~\citep{dubois2019location,hupkes2020compositionality,newman2020eos,anil2022exploring}. Addressing this limitation is crucial, as poor length generalization indicates that language models may not fully understand the underlying task. 
~\citet{zhou2023algorithms} hypothesize that Transformers are more likely to length generalize on tasks with small RASP-L complexity. They demonstrate that tasks such as reverse addition and copying have low RASP-L complexity, making them easier to length generalize, whereas forward addition poses a greater challenge. 

Several approaches have been proposed to improve length generalization, particularly in arithmetic tasks. These include modifications to positional embeddings, such as Abacus embeddings~\citep{mcleish2024transformers}, NoPE~\citep{kazemnejad2024impact}, FIRE~\citep{li2023functional}, and pairwise positional encodings~\citep{sabbaghi2024explicitly,Cho2024PositionCI}, randomized positional encodings~\citep{ruoss2023randomized,zhou2024transformers}. Other methods focus on architectural changes, such as introducing looping mechanisms~\citep{fan2024looped} or incorporating hand-crafted bias corrections in attention score matrices~\citep{duan2023interpolation}. Additionally, input modifications, such as index hinting, have been explored to enhance generalization~\citep{zhou2023algorithms,zhou2024transformers}. 
Beyond arithmetic, length generalization has also been studied in the context of size generalization in graph-based tasks~\citep{yehudai2021local}.

In contrast, our approach adheres to the standard transformer architecture without introducing modifications to architecture, positional encodings, or input structure. A key distinction lies in the training methodology. While prior approaches typically rely on fixed-length training datasets without further updates to model weights, we iteratively update model weights on self-generated datasets, enabling the model to progressively improve and extend its generalization capabilities.


Our self-improvement framework and results on forward addition (Section~\ref{sec:forward_addition}) are closely related to those of~\citet{zhang2023chain}, where self-training enables forward addition generalization from 6-digit examples to 24-digit addition. Like their approach, we iteratively apply self-training on progressively harder problems. However, a key distinction is that their method follows a two-step process in each round: first generating solutions using chain-of-thought (CoT) reasoning, then fine-tuning the model to produce direct answers without CoT. 

Our multiplication results in Section~\ref{sec:mult} have relevance with findings by~\citet{jelassi2023length}, who showed that dataset priming (adding a small number of labeled long-sequence examples) can enable length generalization for multiplication (although this is not strictly out-of-distribution). Our approach of incorporating accurate, self-generated out-of-distribution data via filtering can be seen as an automated form of dataset priming.
Furthermore, while our approach uses chain-of-thought (CoT) data for multiplication, we believe it is possible to length generalize on non-COT multiplication as well, by incorporating methods like~\citet{deng2024explicit} to help the model iteratively internalize the CoT steps.




\paragraph{Easy-to-hard Generalization.} 
Our self-improvement framework operates in a setting where human annotation is provided for easier tasks, enabling generalization to harder tasks with no additional supervision. This paradigm, often referred to as easy-to-hard generalization~\citep{schwarzschild2021can,bansal2022end,burns2023weak,hase2024unreasonable,sun2024easy}, leverages the transfer of learned policies or reward models from simpler problems to more challenging ones.
For instance, ~\citet{zhang2024transcendence} study this phenomenon in chess, showing that chess transformers can sometimes outperform all players in the training dataset. Similarly, ~\citet{sun2024easy} finds that a reward model trained on easier mathematical problems can be effectively transferred to harder problems, facilitating generalization through reinforcement learning. ~\citet{shin2024weak} identifies overlap data points—instances containing both easy and hard patterns—as a key mechanism for weak-to-strong generalization, allowing weak models to pseudolabel easier patterns while stronger models use these labels to learn harder patterns. Our work shows that a similar mechanism emerges naturally within self-improvement, where progressively increasing difficulty enables models to generate useful supervision signals for harder tasks without explicit human intervention.








\paragraph{Self-Improvement.}

When high quality training labels are not available, training on self-generated labels is an efficient way to extract more capabilities from the model. Usually, this involves generating candidate labels, pruning wrong labels through verification or filtering, and retraining with self-generated data. 
ReST~\citep{gulcehre2023reinforced} and I-SHEEP~\citep{liang2024sheep} propose self-improvement as a general purpose alternative to reinforcement learning from human feedback (RLHF), while ~\citet{yuan2024self} propose "self-rewarding" model that generates its own instruction tuning set.

The self-improvement framework has been applied to a wide range of tasks. For example, ~\citet{zhang2019your} replaces an expensive teacher distillation with self-distillation for image recognition tasks. In LLM reasoning domains, \citet{huang2022large,singh2023beyond,pang2024iterativereasoningpreferenceoptimization}, and STaR~\citep{zelikman2022star} bootstrap complex reasoning capabilities by asking models to generate rationales for unlabeled questions and training on self-generated rationals that yielded correct answers. Similarly,~\citet{zhang2023chain} shows self-improving using chain-of-thought (COT) data sampled from the model allows generalization of the integer addition task to more digits. 
For coding tasks, ~\citet{chen2023teaching} teaches LLMs to self-debug with feedback using self-generated code explanation and unit test execution results. 
In mathematics, PatternBoost~\citep{charton2024patternboost} shows that transformers can discover unsolved mathematical constructions of various problems using an algorithm that alternates between sampling constructions from the model (local search) and training on self-generated data (global learning). Similarly, \citet{alfarano2024global} generate synthetic training samples to train transformers to discover new Lyapunov functions.
Recent works also investigate theoretical and empirical aspects of self-improvement. \citet{bansal2024smaller} highlight the effectiveness of smaller models in self-improvement, while \citet{song2024mind} identify the generation-verification gap as a key factor governing the self-improvement process. \citet{huang2024selfimprovementlanguagemodelssharpening} introduce the "sharpening mechanism," where training on best-of-N responses from the model amortizes maximum likelihood inference and improves output quality.

Our work is greatly inspired by ReST~\citep{gulcehre2023reinforced} and STaR~\citep{zelikman2022star}, in which models iteratively generate predictions, filter high-quality responses, and fine-tune on the self-generated dataset.











\paragraph{Model Collapse.}
Recent research has extensively investigated the phenomenon of model collapse, where repeated training on a model's own outputs leads to performance degeneration and a loss of the true underlying data distribution~\citep{shumailov2024ai,Hataya_2023_ICCV,Arcaute2023CombiningGA,shumailov2023curse,Alemohammad2023SelfConsumingGM,Briesch2023LargeLM}.

\citet{shumailov2024ai} provide evidence that iterative training on model-generated data, without filtering, results in rapid degeneration and forgetting of the true data distribution. They emphasize the importance of preserving original data sources over time. Similarly, \citet{shumailov2023curse} show that the tails of the original content distribution diminish after repeated self-training, while \citet{zhang2023chain} highlight the error avalanching effect, where errors compound as models are trained on their own generated data.

Despite its apparent inevitability, several strategies have been proposed to mitigate model collapse. Research shows that the risk of collapse diminishes when the initial model closely approximates the true data distribution~\citep{Bertrand2023OnTS}, or when real data is retained throughout training rather than being fully replaced~\citep{gerstgrasser2024model,Dohmatob2024ATO,Briesch2023LargeLM}. Additionally, ~\citet{gillman2024self,feng2024beyond} suggest using reliable verifiers during self-training to ensure high-quality self-generated data, further reducing the likelihood of collapse.

Our approach addresses these challenges by maintaining a core labeled dataset throughout training, consisting of examples of limited length or difficulty. Synthetic data, generated incrementally by the model, is added in a controlled manner. By incorporating unsupervised filtering techniques such as length filtering and majority voting, we ensure the quality of self-generated data. Our framework builds upon prior findings by preserving clean data and selectively incorporating synthetic data.

Additionally, our results in Section~\ref{sec:error_analysis} align with findings from~\citet{rolnick2017deep}, which demonstrate that deep neural networks are robust to significant label noise in image classification tasks. Additionally,~\citet{Bayat2024ThePO} recently emphasized that memorization alone does not harm generalization; rather, the combination of memorization with spurious correlations is what undermines learning. Our results suggest that despite memorizing past mistakes, the self-improvement framework remains effective, provided that incorrect samples do not dominate the training distribution.










\newpage
\section{Additional Results}\label{sec:additional_results}


\subsection{Does the Model Truly Learn Addition?}
When the two operands of length $N$ are sampled randomly, the probability of encountering an instance with a carry chain length of $N$ decreases exponentially with $N$. Under this sampling strategy, the model rarely, if ever, see ``hard\footnote{we define hard instance of addition to be cases with multiple numbers of cascading carries~\citep{quirke2023understanding}}'' instances of addition, as shown in Figure~\ref{fig:num_carries}. To address this, we manually construct a test dataset to include at least 500 examples for each maximum cascading carry length. This ensures that the evaluation captures the model's ability to handle harder instances of addition.

We evaluate models trained on 1-10 digit addition that undergo 10 rounds of self-improvement, encountering self-generated data up to 19 digits in length. The results in Figure~\ref{fig:carry_acc} indicate that the model successfully performs additions involving up to 20 cascading carries, despite never encountering such cases during training. Reverse addition maintains near-perfect accuracy across all carry lengths, whereas forward addition performance drops to 80\% at 20 cascading carries—where the carry effect propagates across the entire sequence. Nonetheless, the model's ability to handle these challenging cases demonstrates its capacity to generalize to harder instances beyond its training distribution.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.35\linewidth]{fig/etc/num_total_carry-20.pdf}
    \hspace{1mm}
    \includegraphics[width=0.35\linewidth]{fig/etc/num_max_cascading_carry-20.pdf}
    \caption{Distribution of carry occurrences in the standard 20-digit self-improve dataset. Models rarely encounter examples with large numbers of carries during training. }
    \label{fig:num_carries}
\end{figure}



\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.37\linewidth]{fig/etc/acc_upsampled_max_cascading_carry-20.pdf}
    \hspace{1mm}
    \includegraphics[width=0.35\linewidth]{fig/etc/forward_acc_filter_max_cascading_carry-20.pdf}
    \caption{Performance of the model at round 10 (trained with self-generated data up to 19 digits). Accuracy as a function of the maximum cascading carries. (Left) Reverse addition. (Right) Forward addition. Despite limited exposure to high-carry cases in training, models achieve strong generalization to these harder instances.}
    \label{fig:carry_acc}
\end{figure}

\subsection{Additional Results on Majority Voting}
\subsubsection{Majority Voting Leverages Label Diversity}\label{sec:mv_diversity}

Self-improvement relies on the model's ability to generalize to slightly harder problems. However, this generalization is not always robust and can vary significantly across different training instances~\citep{zhou2024transformers}. Majority voting mitigates this variability by aggregating predictions across multiple independently trained models, thereby improving the reliability of self-generated labels.


\begin{figure}[ht!]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{minipage}{\linewidth} \centering
    \includegraphics[width=0.3\linewidth]{fig/mult/multiplication_round-1_seed-41_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/multiplication_round-1_seed-42_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/multiplication_round-1_seed-43_1_acc.pdf}\\
    \includegraphics[width=0.3\linewidth]{fig/mult/multiplication_round-1_seed-44_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/multiplication_round-1_seed-45_1_acc.pdf}
    \end{minipage}
    }
    \hspace{1mm}
    \caption{Test accuracy on 5 different seeds during the initial training phase. Models exhibit high variance in performance.  }
    \label{fig:mult_has_high_variance1}
\end{figure}


\begin{figure}[ht!]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{minipage}{\linewidth}    \centering
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_mult_fixed_init_seed/multiplicationfix_seed_round-1_seed-41_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_mult_fixed_init_seed/multiplicationfix_seed_round-1_seed-42_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_mult_fixed_init_seed/multiplicationfix_seed_round-1_seed-43_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_mult_fixed_init_seed/multiplicationfix_seed_round-1_seed-44_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_mult_fixed_init_seed/multiplicationfix_seed_round-1_seed-45_1_acc.pdf}
    \end{minipage}
    }
    \hspace{1mm}
    \caption{Test accuracy on models trained with the same seed data but different training seeds. Despite identical training data, models exhibit large variability. }
    \label{fig:mult_has_high_variance2}
\end{figure}

To illustrate this variability, Figure~\ref{fig:mult_has_high_variance1} shows test accuracy across five models trained with different random seeds on the initial training dataset containing up to 5-by-5 multiplication. Even when trained on identical training data, models exhibit substantial performance differences in extrapolation. Similarly, Figure~\ref{fig:mult_has_high_variance2} demonstrates that this variability persists even when models are trained from the same seed data. 



\subsubsection{Ablations for Majority Voting}\label{sec:mv_ablations}

Our majority voting method requires training multiple models in parallel. In our primary setting, we train $k$ models with different random seeds, allowing each to generate and train on its own independent self-improved dataset at every round. 

To evaluate the necessity of training multiple independent models and generating separate self-improvement datasets, we compare our approach against the following baselines:
\begin{enumerate}
    \item No majority voting, but larger self-improve data: Instead of using multiple models, we train a single model while sampling $k$ times more self-improve data per round, ensuring that the total amount of generated data matches our main setting.
    \item Shared self-improve data: We train $k$ models with different initial seeds but subsequently train all models on the same self-improved dataset.
    \item Shared initial training seed: All models are initialized from the same seed but then trained on separate self-improved datasets.
    \item Our main setting: Each model is initialized with a different seed and trained on its own independently generated self-improve dataset.
\end{enumerate}

Figure~\ref{fig:mv_baseline} presents the performance of these variations, highlighting the importance of training on independently generated self-improve datasets rather than simply increasing dataset size or sharing training trajectories across models.

\begin{table}[ht!]
    \centering
    \caption{Comparison of Data Cost Across Majority Voting Variants}
    \footnotesize
    \small
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{0.5}
    {

    \label{tab:mv_data_cost}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Method} & \textbf{Initial Training Data Cost} & \textbf{Self-Improve Data Cost (Per Round)} \\
        \midrule
        No Majority Voting, Larger Data & 1 & $k$ \\
        Shared Self-Improve Data & $k$ & 1 \\
        Shared Initial Training Seed & 1 & $k$ \\
        Full Majority Voting (Ours) & $k$ & $k$ \\
        \bottomrule
    \end{tabular}
    }
\end{table}


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.24\linewidth]{fig/mult/vanilla_more_data/multiplication_vanilla_more_7_acc.pdf}
    \includegraphics[width=0.24\linewidth]{fig/mult/majority_fixed_SI/multiplication_mv_7_acc.pdf}
    \includegraphics[width=0.24\linewidth]{fig/mult/majority_mult_fixed_init_seed/multiplication_mv_mult_fix_seed_7_acc.pdf}
    \includegraphics[width=0.24\linewidth]{fig/mult/majority/multiplication_mv_mult_7_acc.pdf}
    \caption{Ablations on majority voting. (Left) No majority voting, but larger self-improve data. (Left-Center) Majority voting with shared self-improve data. (Right-Center) Majority voting with shared initial training seed. (Right) Our primary setting with fully independent training and self-improve datasets.}
    \label{fig:mv_baseline}
\end{figure}



We set $k=5$ and report the average performance across five models. Figure~\ref{fig:mv_baseline} shows that simply increasing the amount of self-improvement data without filtering leads to poor performance. Surprisingly, using $5\times$ more self-improvement data per round performs even worse than using less data (Figure~\ref{fig:multiplication_vanilla}), consistent with our findings in Section~\ref{sec:si_data_size}.

Additionally, majority voting with shared self-improve data (second panel from the left) underperforms in OOD compared to models trained on separate self-improve datasets. This suggests that model diversity—enabled by training on different self-improve data—may be important for majority voting to be effective. 

On the other hand, comparing the right two panels in Figure~\ref{fig:mv_baseline}, where the difference lies in whether the base models were trained on different labeled data $\mathcal{D}_0$, we find minimal differences in OOD performance. This may be due to the large size of the initial training set (5M examples), which provides sufficient diversity. Furthermore, as Figure~\ref{fig:mult_has_high_variance2} shows, models trained on the same initial dataset but with different training seeds still exhibit substantial variability, suggesting that model diversity can emerge from different training trajectories alone.


\subsection{Additional Error Analysis on Reverse Addition}\label{sec:appdx_error_patterns}

\subsubsection{Patterns in Model Mistakes. } 

Figure~\ref{fig:first_wrong_location} shows that when models generate incorrect answers, the first mismatch with the ground truth typically occurs near the final digits of the sequence (i.e., near the most significant digit in reverse addition). These observations inform our systematic error simulations, which are used to analyze the error avalanche phenomenon in Section~\ref{sec:error_analysis}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\linewidth]{fig/etc/first_incorrect_position.pdf}
    \caption{he first incorrect digit in model outputs tends to occur near the most significant digit in reverse addition.}
    \label{fig:first_wrong_location}
\end{figure}


\subsection{Additional Experiments on Label Noise and Robustness}\label{sec:appdx_label_noise}

\paragraph{Robustness against Random Labels. }
To further examine the model’s resilience to errors in data, we introduce randomization into the labels during training. Correct labels are replaced with random numbers of the \textit{same length} with probabilities 1, 0.8, 0.5, 0.2, 0.1, and 0. A probability of 1 corresponds to entirely incorrect labels, while 0 indicates fully correct data.

The model is initially trained on 1-10 digit reverse addition and further trained across 8 self-improvement rounds, using self-generated data of lengths 11-18 digits. We then construct a dataset of 19-digit data with randomized labels, denoted as $\mathcal{D}^\text{rand}_9$. The model is fine-tuned on a combined dataset consisting of the original dataset $\mathcal{D}_0$, self-improved datasets $\mathcal{D}_1, \dots, \mathcal{D}_8$, and $\mathcal{D}^\text{rand}_9$.

Results in Figure~\ref{fig:random_label} show that the models can tolerate substantial random label noise, maintaining robust performance even when up to 80\% of the training data is corrupted. This demonstrates the model’s resilience to random errors in the training data and its ability to self-correct such mistakes during learning. 


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\linewidth]{fig/etc/rand_prob_with_si_19_digit.pdf}
    \hspace{1mm}
    \includegraphics[width=0.4\linewidth]{fig/etc/rand_prob_with_si_20_digit.pdf}
    \caption{Effect of training on randomized labels. The model is trained on 1-10 digit data, further fine-tuned on 11-18 digit self-generated data over 8 self-improvement rounds, and additionally fine-tuned on 19-digit data with varying probabilities of random label replacement. (Left) Accuracy on 19-digit data. (Right) Accuracy on 20-digit data. The results demonstrate that while the model can self-correct random errors, biases from self-improved data can result in worse performance compared to models trained on random-labeled data of similar accuracy.}
    \label{fig:random_label}
\end{figure}

\paragraph{Model Bias vs. Random Labels.}  
Interestingly, biases in self-generated data are more detrimental than uniformly random label noise.  As shown in Figure~\ref{fig:random_label}, models trained with self-improved data perform worse than random-labeled data of comparable accuracy, given the same dataset size and fine-tuning steps. 
This suggests that the inherent biases in self-generated data hinder generalization more than randomly introduced noise. 

