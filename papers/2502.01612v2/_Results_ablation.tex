\section{Ablations}
\subsection{OOD generalization Increases with More Self-Improvement}\label{sec:ood_increases}

\begin{finding}
    The amount of out-of-distribution (OOD) extrapolation increases with more rounds of self-improvement in the addition, copying, and multiplication tasks.
\end{finding}


Out-of-distribution (OOD) generalization is a critical measure of a model's ability to extrapolate beyond its training data. In tasks like reverse addition and copy, we observe that OOD extrapolation capabilities improve progressively as the model undergoes more rounds of self-improvement. Figure~\ref{fig:acc_ood_digits} illustrates how the number of additional OOD lengths achieving over 99\% accuracy grow with each round when the model is self-improved using only one additional digit per round. The model's OOD extrapolation capabilities expand as it is trained on longer sequences. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig/etc/extra_extrapolation.pdf}
    \caption{Number of \textbf{extra} OOD digit lengths achieving over 99\% accuracy when self-improving with one additional digit per round, on (Left) copy and (Right) reverse addition. The growing OOD capability suggests the potential to sample more digits per round as self-improvement progresses.}
    \label{fig:acc_ood_digits}
\end{figure}





\paragraph{``Safe Range'' of Sampling of Next Round Difficulty. } 
At the core of the self-improvement framework is the observation that the models perform well on samples slightly harder than those in the training set. Sampling instances that are too difficult for the current model is detrimental to the quality of self-improvement data, which causes downstream performance to break down. Here, we have shown that the ``safe range'' for sampling next-round difficulty becomes more forgiving.  Nevertheless, in real-world datasets, difficulty measures are not strictly quantifiable, and sampling by difficulty often involves noise. It is important to develop precise and controllable notions of difficulty for real-world tasks. 


\subsection{Self-Improvement can be Accelerated}\label{sec:accelerate}
We observe that for addition and string manipulation tasks, the amount of extra OOD generalization increases roughly lineraly with each additional round of self-improvement (Figure~\ref{fig:acc_ood_digits}). This observation suggests that sampling multiple difficulty levels within the safe range per round could lead to exponential improvements in performance.

It is important to note that the sampling schedule in this proof-of-concept experiment leverages information about test set accuracy to determine the extra OOD lengths to sample, which is not typically available in practical scenarios. While this approach demonstrates the potential of an accelerated self-improvement schedule, practical implementation would require using proxy metrics to estimate suitable OOD lengths for sampling.

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{fig/copy-sorting-parity/accelerated.pdf}
    \includegraphics[width=0.431\linewidth]{fig/reverse_add/accelerated.pdf}
    \caption{Maximum input length achieving over 99\% accuracy at different self-improvement rounds for (Left) Copy task and (Right) Reverse addition. The dashed linear line represents the standard schedule of sampling one additional length per round. The vertical line is when we start allowing accelerated schedule. Faster self-improvement schedules allow the model to generalize to longer inputs with fewer rounds.}
    \label{fig:fast_copy_add}
\end{figure}



\paragraph{Accelerated Self-Improvement Schedule.}
Building on the observations in Figure~\ref{fig:acc_ood_digits}, we propose an accelerated self-improvement schedule where the model samples multiple difficulty levels at each round, instead of incrementally increasing by only one additional length (for string manipulation) or digit (for reverse addition). As shown in Figure~\ref{fig:fast_copy_add}, this approach significantly speeds up performance in copying and reverse addition tasks, allowing the model to achieve high accuracy with fewer training rounds and training steps. At each round, the self-improvement dataset is uniformly sampled from all difficulty levels achieving over 99\% evaluation accuracy. All other hyperparameters remain unchanged.

\begin{wrapfigure}{r}{0.61\textwidth}
    \vspace{-9mm}
    \centering
    \includegraphics[width=0.25\textwidth]{fig/mult/accelerated/mult_acc2.pdf}
    \includegraphics[width=0.25\textwidth]{fig/mult/accelerated/multiplication_acc_mv_len_n_10_no_title_19_acc.pdf}
    \caption{Accelerated self-improvement in multiplication. (Left) Accelerated schedule for multiplication. The rows and columns represent the number of digits in the two operands of the multiplication task. The number within each cell indicates the self-improvement round in which the corresponding digit pair is included for training. (Right) Results at round 19. Controlled scheduling progressively incorporates more digit pairs in each round, accelerating the self-improvement process.}
    \label{fig:mult_accelerated}
    \vspace{-8mm}
\end{wrapfigure}

We observe similar improvements using an accelerated schedule for multiplication task as well, as depicted in Figure~\ref{fig:mult_accelerated}. Under the standard schedule, reaching 10-by-10 multiplication from 5-by-5 requires 41 self-improvement rounds, incrementally increasing one operand by 1 at a time. With the accelerated schedule, we progressively sample more operand pairs as self-improvement proceeds, reducing the required rounds to 19 while achieving perfect test performance (see Appendix Figure~\ref{fig:multiplication_accelerated_len_n10_full} for full results). The settings for multiplication follow the setting in Section~\ref{sec:harder_tasks}.


\subsection{Pretrained Models achieve Faster Acceleration}\label{sec:pretrained}
\begin{finding}
    Self-improvement is more effective for larger pretrained models, and these models achieve faster acceleration.
\end{finding}

We extend our self-improvement framework to pretrained models, specifically Llama-1B and Llama-3B~\citep{llama3modelcard}, to explore scaling effects and the impact of finetuning on larger models. Our results indicate that larger models exhibit superior extrapolation performance and benefit from accelerated self-improvement.

\begin{figure}
    \centering
    \includegraphics[width=0.49\linewidth]{fig/etc/llama_1B_reverse.png}
    \includegraphics[width=0.49\linewidth]{fig/etc/llama_3B_reverse.png}
    \caption{Reverse addition results for pretrained models. (Left) Llama-1B model. (Right) Llama-3B model. Larger models exhibit better extrapolation performance across rounds of self-improvement. }
    \label{fig:pretrained_model}
\end{figure}


\begin{wrapfigure}{r}{0.44\textwidth}
    \vspace{-6mm}
    \centering
    \includegraphics[width=0.44\textwidth]{fig/etc/accelerated_pretrained_v2.pdf}
    \caption{
    Accelerated reverse addition with pretrained models.
    Comparison of self-improvement acceleration for Llama-3B, Llama-1B, and a smaller 14M parameter model trained from scratch. Larger pretrained models demonstrate faster and more robust self-improvement.}
    \vspace{-5mm}
    \label{fig:pretrained_model_accelerate}
\end{wrapfigure}

\paragraph{Setting.}
To maintain consistency in tokenization, we use character-level tokenization instead of the default tokenizer of the Llama models. All other model components, including initial weights and positional embeddings, remain unchanged. We use LoRA~\citep{Hu2021LoRALA} with $r=64$ and $\alpha=128$ for Llama-1B, and $r=32$ and $\alpha=128$ for Llama-3B. 
In the initial round, we train for 1200 steps with a learning rate schedule that includes 10\% warm-up steps to a constant learning rate of \( 1\text{e-}4 \), followed by 20\% cosine decay steps to a final learning rate of \( 1\text{e-}6 \). For subsequent rounds, we train for 600 steps per round using a cosine decay learning rate schedule without warm-up, starting at \( 1\text{e-}4 \) and decaying to \( 1\text{e-}6 \).



\paragraph{Results.}
Figure~\ref{fig:pretrained_model} illustrates that larger models, such as Llama-3B, achieve better extrapolation performance in the reverse addition task compared to smaller models. Additionally, Figure~\ref{fig:pretrained_model_accelerate} highlights the potential for faster acceleration with larger models. By leveraging their pretraining capabilities, these models can generalize to longer sequences with fewer rounds of self-improvement. 

Figure~\ref{fig:pretrained_model_accelerate} compares self-improvement acceleration between Llama-3B, Llama-1B, and a smaller 14M parameter model trained from scratch. The results demonstrate that pretrained models significantly outperform the smaller model, both in terms of extrapolation and speed of self-improvement, showcasing the scalability of our approach.

