
\section{Length and Difficulty Generalization on Forward Addition, Multiplication, Maze}~\label{sec:harder_tasks}

\begin{finding}
    Augmenting self-improvement training with label filtering based on length and majority voting, transformer models can achieve length and difficulty generalization on forward addition, COT-multiplication and maze solving. 
\end{finding}

We extend our evaluation to a class of harder tasks, including forward addition, multiplication, and maze-solving. %
Our results demonstrate that the framework is not limited to length generalization but extends to \textbf{difficulty generalization}, where the model incrementally learns to solve increasingly difficult problems. By employing controlled sampling of problem difficulty and data filtering techniques for each round, the model successfully adapts to harder tasks, highlighting the versatility and robustness of the self-improvement approach.

\subsection{Forward Addition}\label{sec:forward_addition}
Forward addition is a straightforward yet challenging task for transformer models to length generalize effectively. ~\citet{zhou2023algorithms} hypothesize that Transformers are more likely to length generalize on tasks with small RASP-L complexity. They demonstrate that tasks such as reverse addition and copying have low RASP-L complexity, making them easier to length generalize, whereas forward addition poses a greater challenge. 
In reverse addition, each step only requires processing a fixed-size subset of the input. However, in the forward addition, the size of the relevant input required to generate correct tokens increases, making the problem more complex. 

\begin{figure}
    \centering
    \includegraphics[width=0.42\linewidth]{fig/forward_add/vanilla_mean_acc_no_shade.pdf}
    \hspace{2mm}
    \includegraphics[width=0.42\linewidth]{fig/forward_add/length_filter_mean_acc_no_shade.pdf}
    \caption{Models trained on forward addition over 10 self-improvement rounds. (Left) Without data filtering. (Right) With length-based filtering using a threshold of 2. Data filtering significantly enhances length generalization performance.}
    \label{fig:filtering_forward_add}
\end{figure}


\paragraph{Setting.}
The models are initially trained on a dataset $\mathcal{D}_0$ containing 2 million labeled examples of forward addition, with operand lengths ranging from 1 to 10 digits. This initial training phase spans 10,000 steps. In each subsequent self-improvement round, we generate 50,000 additional training examples, incrementally extending the operand length by one digit. Specifically, at self-improvement round \( r \), the self-generated dataset $\mathcal{D}_r$ contains length-\( 10+r \) examples produced by the model \( M_r \). The model is then fine-tuned for 3,000 steps on the combined dataset $\mathcal{D}_0 \cup \mathcal{D}_1 \cup \dots \cup \mathcal{D}_r$, resulting in an updated model \( M_{r+1} \).

\paragraph{Results.}

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-8mm}
    \centering
    \includegraphics[width=0.5\textwidth]{fig/forward_add/multi_rounds_with_data_filtering_overall_acc.pdf}
    \caption{Results on the forward addition task with length filtering. The model is initially trained on labeled forward addition data of lengths 1 to 10. Using the self-improvement framework over 60 rounds, with incremental increases in digit length by 1 per round, the model achieves strong generalization to lengths up to 75.}
    \vspace{-5mm}
    \label{fig:forward_add}
\end{wrapfigure}

Figure~\ref{fig:filtering_forward_add} shows the results of forward addition experiments, where the model is initially trained on labeled data of up to 10 digits and then undergoes 10 rounds of self-improvement.
Without any data filtering (Left), the model's performance begins to deteriorate after a few rounds of training, leading to a collapse in generalization. However, applying the length-based filtering approach with a threshold length of 2 results in significant improvements in length generalization performance (Right). By refining the self-improvement dataset at each round, the self-improvement framework remains robust across multiple rounds.

With continued training over 60 self-improvement rounds, the model maintains performance exceeding 98\% accuracy for sequences up to length 70 (Figure~\ref{fig:forward_add}). This demonstrates the effectiveness of length-based filtering in sustaining the self-improvement process and enabling models to generalize to much longer sequences.


\paragraph{Relevance to Prior Work.} Our self-improvement framework and results are closely related to those of~\citet{zhang2023chain}, where self-training enables forward addition generalization from 6-digit examples to 24-digit addition. Like their approach, we iteratively apply self-training on progressively harder problems. However, a key distinction is that their method follows a two-step process in each round: first generating solutions using chain-of-thought (CoT) reasoning, then fine-tuning the model to produce direct answers without CoT.





\subsection{Multiplication}\label{sec:mult}
We also extend our approach on multiplication, which is a challenging task even in-distribution, as noted in~\citet{dziri2024faith}. Fine-tuning large language models on datasets with chain-of-thought(CoT) steps has shown limited success. We adopt a data format similar to~\citep{deng2024explicit}, where the input prompt is structured as \texttt{9172*9431=}, and the label expands the multiplication into stepwise additions, such as:
\texttt{17442+067801(132331)+0075180(1398490)+00091720=13976630}.
Each step includes the intermediate results (in parentheses) representing partial products formed by multiplying the first operand with each digit of the second operand. 

\begin{figure}
    \centering
    \includegraphics[width=0.24\linewidth]{fig/mult/vanilla/multiplication_vanilla_1_acc.pdf}
    \includegraphics[width=0.24\linewidth]{fig/mult/vanilla/multiplication_vanilla_4_acc.pdf}
    \includegraphics[width=0.24\linewidth]{fig/mult/vanilla/multiplication_vanilla_7_acc.pdf}    
    \caption{Results on multiplication without filtering (round $1,4,7$). Each cell represents the accuracy on $n$-digit by $m$-digit multiplication. Red boxes indicate labeled in-distribution examples, while magenta boxes indicate evaluations after training on self-improve data. The model is initially trained on up to $5$-by-$5$ multiplication. Without filtering, generalizing to larger multiplications is challenging.}
    \label{fig:multiplication_vanilla}
    \vspace{-3mm}
\end{figure}


The data format is inherently asymmetrical. For example, an $m$-by-$n$ multiplication requires $n$ intermediate steps, where each step corresponds to multiplying the $m$-digit number by one digit of the $n$-digit number. Conversely, an $n$-by-$m$ multiplication involves $m$ intermediate steps of multiplying the $n$-digit number by each digit of the $m$-digit number. 


\begin{wrapfigure}{r}{0.51\textwidth}
    \vspace{-8mm}
    \centering
    \includegraphics[width=0.49\linewidth]{fig/mult/multiplication_round-7_vanilla_7_acc.pdf}
    \includegraphics[width=0.49\linewidth]{fig/mult/multiplication_round-7_len_filter_7_acc.pdf}
    \includegraphics[width=0.49\linewidth]{fig/mult/multiplication_round-7_mv_7_acc.pdf}
    \includegraphics[width=0.49\linewidth]{fig/mult/multiplication_round-7_mv_len_7_acc.pdf}    
    \caption{Comparison of data filtering methods at round 7. (Top-Left) no filtering (Top-Right) length filtering (Bottom-Left) majority voting (Bottom-Right) and a combination of majority voting and length filtering. Data filtering significantly improves self-improvement performance, with the combined approach achieving the best results.}
    \vspace{-12mm}
    \label{fig:multiplication_filtering_comparison}
\end{wrapfigure}


\paragraph{Setting. }
The model is initially trained on $n$-by-$n$ multiplication examples with $n=5$. Directly introducing $n+1$-by-$n+1$ examples results in poor performance, hence, we adopt a more fine-grained difficulty schedule. In each self-improvement round, we incrementally increase one operand by one digit, sampling $n+1$-by-$m$ and $m$-by-$n+1$ examples, where $m$ grows from 1 to $n+1$. This gradual progression allows the model to adapt incrementally to larger operand sizes, making the transition to harder examples more manageable.


\paragraph{Results without Data Filtering. }
Despite being trained on CoT data with explicit intermediate steps, the model exhibits limited extrapolation performance for larger operand sizes. As shown in Figure~\ref{fig:multiplication_vanilla}, the model struggles to generalize beyond its training distribution without any refinement in the generated self-improvement data. After 7 self-improvement rounds, the accuracy for $6$-by-$6$ multiplication reaches only 13.7\%.
Additionally, performance varies significantly across models trained with different random seeds (Appendix Figure~\ref{fig:mult_has_high_variance1}), intermediate steps are often dropped. resulting in outputs shorter than the correct answer (Figure~\ref{fig:num_shorter_answers} (Right)). This highlights the need for data refinement strategies to improve both the quality and consistency of the self-generated data.




\paragraph{Results with Data Filtering. }
To improve the quality of self-generated training data, we apply three data refinement techniques: length filtering, majority voting, and a combination of both. For length filtering, we remove self-generated samples where the output length is shorter than the longest output in the batch by more than 10 tokens. This helps eliminate incorrect solutions that omit intermediate steps. For majority voting, we train five models in parallel using different random seeds and retain only those data points where at least four out of the five models produce the same output. This strategy ensures that only high-consensus, reliable data points are used for training.

Figure~\ref{fig:multiplication_filtering_comparison} compares the effectiveness of these filtering methods at round 7, where models are trained on self-generated data for up to $6$-by-$6$ multiplication. All three filtering methods enhance self-improvement, with majority voting outperforming length filtering. The combined approach—applying both majority voting and length filtering—achieves near-perfect generalization to $6$-by-$6$ multiplication.

Training for additional rounds further extends this generalization. As shown in Figure~\ref{fig:multiplication_mv_len_n10}, the combined filtering strategy continues to yield near-perfect accuracy up to $9$-by-$9$ multiplication at round 31, with the potential for even further generalization in subsequent rounds. In Section~\ref{sec:accelerate}, we demonstrate that a carefully crafted self-improvement schedule can accelerate this process, achieving perfect performance on $10$-by-$10$ multiplication in just 19 rounds. Detailed results for all filtering methods are provided in Appendix Section~\ref{sec:mult_full_results}.






\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_len_n10/multiplication_majority_mult_len_filtered_n_10_1_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_len_n10/multiplication_majority_mult_len_filtered_n_10_7_acc.pdf}
    \includegraphics[width=0.3\linewidth]{fig/mult/majority_len_n10/multiplication_majority_mult_len_filtered_n_10_31_acc.pdf}
    \caption{Results on combining majority voting with length filtering (at round $1,7,31$) for the multiplication task.
    This approach achieves near-perfect length generalization up to $9 \times 9$, demonstrating the effectiveness of filtering strategies in improving self-improvement.}
    \label{fig:multiplication_mv_len_n10}
    \vspace{-3mm}
\end{figure}


\paragraph{Relevance to Prior Work. }
These results have relevance with findings by~\citet{jelassi2023length}, who showed that dataset priming (adding a small number of labeled long-sequence examples) can enable length generalization\footnote{they consider encoder-decoder architecture which differs for our decoder-only model} for multiplication (although this is not strictly out-of-distribution). Our approach of incorporating accurate, self-generated out-of-distribution data via filtering can be seen as an automated form of dataset priming.
Furthermore, while our approach uses chain-of-thought (CoT) data for multiplication, we believe it is possible to length generalize on non-COT multiplication as well, by incorporating methods like~\citet{deng2024explicit} to help the model iteratively internalize the CoT steps.



\subsection{Maze-Solving}\label{sec:maze}

While arithmetic tasks and string manipulations provide valuable testbeds for studying language model generalization, we extend our evaluation to a more complex problem: finding the shortest path in a maze. Pathfinding presents significant challenges for autoregressive models~\citep{bachmann2024pitfalls}. Our mazes can be represented by a tree graph in a 2-dimensional space and they do not have loops. Figure~\ref{fig:maze_data} provides a visualization of this task and the corresponding input and output data format. 

Each tree graph consists of \(N\) nodes, which are randomly labeled with numbers between 0 and 99. The input format follows the structure: 
\texttt{start\_node>end\_node}, followed by a graph adjacency list formatted as \texttt{random\_node:adjacent\_nodes}. Here, \texttt{adjacent\_nodes} are separated by commas (\texttt{,}), and each \texttt{random\_node} is separated by a hyphen (\texttt{-}). The target output is a sequence of hops from the start node to the end node, separated by \texttt{>}. Additional details on maze generation are provided in Appendix~\ref{sec:data_gen-maze}. 

We evaluate two generalization settings: 1) increasing the number of hops while keeping the number of nodes fixed, and 2) increasing the number of nodes while keeping the number of hops fixed. In the first setting, the input graph description remains constant in size, but the output length grows as the difficulty increases. In the second setting, the input graph expands with more nodes, while the output remains of fixed length. 





\subsubsection{Increasing the Number of Hops}
The difficulty of the maze-solving task increases with the number of hops required to traverse from the start node to the end node, which directly corresponds to the number of \texttt{>} symbols in the output. We begin by training the model on a labeled dataset containing paths of up to \( h=9 \) hops. In each self-improvement round, we increase \( h \) by one, progressively introducing longer paths.

\paragraph{Setting.}
The model is first trained on a dataset \( \mathcal{D}_0 \) containing 5 million labeled maze-solving examples, where the number of nodes is fixed at \( N=30 \) and paths range from \( h=1 \) to \( h=9 \) hops. This initial training phase spans 25,000 steps. In subsequent self-improvement rounds, we generate 50,000 additional training examples, increasing \( h \) by 1, and fine-tune the model for 5,000 steps per round. We experiment with both unfiltered training data and majority voting, where only outputs agreed upon by all 3 models are retained.

\paragraph{Results.}
As shown in Figure~\ref{fig:maze_hops_len_gen_result}, without filtering, self-generated training data degrades over successive rounds, leading to a collapse in the self-improvement process. In contrast, majority voting stabilizes data quality, allowing the model to continue generalizing successfully to paths up to 30 hops.

\begin{figure}
    \centering
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_hops/vanilla_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_hops/mv_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_hops/maze_hops_data_accuracy.pdf}
    \caption{Maze-solving task with increasing hops (\( N=30 \) nodes). Models are trained on graphs with up to 9 hops and generalized by incrementally increasing hops by 1 in each self-improvement round. Results show mean accuracy across 3 seeds. (Left) No filtering. (Middle) Majority voting. (Right) Self-improve data accuracy per round. Filtering significantly enhances data accuracy and improves generalization.}
    \label{fig:maze_hops_len_gen_result}
\end{figure}




\subsubsection{Increasing the Number of Nodes}
Another approach to increasing task difficulty is to expand the number of nodes in the graph while keeping the number of hops fixed at \( h=9 \).

\paragraph{Setting.}
The model is first trained on a dataset \( \mathcal{D}_0 \) containing 5 million labeled maze-solving examples, with a fixed hop count \( h=9 \) and node counts ranging from \( N=10 \) to \( N=30 \). This initial training lasts 12,000 steps. In self-improvement rounds, the number of nodes \( N \) is increased by 3 per round, generating 50,000 additional training examples at each step and fine-tuning for 4,000 steps. We compare training without filtering against majority voting, where only outputs agreed upon by all 3 models are kept.


\paragraph{Results.}
As shown in Figure~\ref{fig:maze_nodes_len_gen_result}, training without filtering leads to gradual performance degradation, whereas majority voting preserves high-quality data, maintaining a self-improvement accuracy above 99.7\% and enabling generalization to larger graphs with 9 hops.

While this experiment focuses on fixing one dimension (number of hops or number of nodes) and increasing the other, alternating between increasing the difficulty in both dimensions is expected to generalize the maze-solving task to handle larger graphs and longer paths simultaneously.

\begin{figure}
    \centering
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_nodes/vanilla_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_nodes/mv_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_nodes/maze_nodes_data_accuracy.pdf}
    \caption{Maze-solving task with increasing nodes (fixed at 9 hops). Models are trained on graphs with up to 30 nodes and generalized by incrementally increasing the number of nodes by 3 per round. (Left) No filtering. (Middle) Majority voting. (Right) Self-improve data accuracy. Majority voting improves generalization to larger graphs.}
    \label{fig:maze_nodes_len_gen_result}
\end{figure}





\subsubsection{Using Verifiers for Data Filtering. }
Solving the shortest path problem can be computationally expensive, but verifying the correctness of a given solution is significantly simpler. A valid path can be verified by traversing the sequence and ensuring three conditions: 1) each move is valid, meaning the path follows adjacency constraints; 2) the final destination matches the intended goal; and 3) no nodes are repeated, confirming that the solution is indeed the shortest path.

Self-improvement frameworks commonly incorporate verifiers to filter self-generated data, often leveraging trained models or reward models~\citep{zelikman2022star,singh2023beyond,hosseini2024v,lightman2023let}. While our primary focus is not on training or designing an additional verification mechanism, we investigate the effectiveness of using an external verifier as a data-filtering method.

To this end, we evaluate an oracle verifier that enforces two essential constraints: 1) move validity, ensuring that every transition in the generated solution adheres to the adjacency constraints of the maze, and 2) end validity, confirming that the final node in the solution corresponds to the correct destination. We compare the effectiveness of this oracle-based filtering against self-improvement without data filtering and majority-voting-based filtering to assess its impact on performance and stability.

\paragraph{Results. }
Figure~\ref{fig:maze_verifier} shows results for mazes with increasing hops, increasing nodes, and three different verification strategies: checking moves, checking end validity, and checking both. As expected, verification improves data quality and serves as an effective filtering technique in self-improvement. Notably, verifying move validity proves to be significantly more effective than verifying only the correctness of the end node. Interestingly, however, majority voting—a strategy that does not rely on an external verifier—performs comparably to verification-based filtering. This suggests that self-consistency mechanisms alone can be sufficient for maintaining high-quality training data. 

Additional results, including finer-grained analysis of move validity and end validity beyond exact match accuracy, are provided in Appendix~\ref{sec:maze_full_results}.

\begin{figure}
    \centering
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_hops/verifier-move-ends_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_hops/verifier_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_hops/verifier-ends_mean_acc.pdf}    
    \\
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_nodes/verifier-move-ends_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_nodes/verifier_mean_acc.pdf}
    \hspace{1mm}
    \includegraphics[width=0.32\linewidth]{fig/maze/maze_nodes/verifier-ends_mean_acc.pdf}
    \caption{Results on using verifier for data filtering. (Top) Increasing hops. (Bottom) Increasing nodes. (Left) Verifier on both moves and ends. (Middle) Verifier on moves only. (Right) Verifier on ends only. Verifier-based filtering improves self-improvement performance, with move validation proving more effective than end validation alone. Interestingly, majority voting performs on par with oracle verification, suggesting that self-consistency mechanisms can serve as effective alternatives to explicit verification.}
    \label{fig:maze_verifier}
\end{figure}










