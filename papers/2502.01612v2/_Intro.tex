\section{Introduction}\label{sec:intro}












\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{fig/result_overview_v2.pdf}
    \caption{Overview of self-improvement results. Models trained with self-improvement can tackle increasingly complex tasks that extend far beyond their initial training distributions, achieving significant generalization \textbf{without any additional supervision}.}
    \label{fig:result_overview}
    \vspace{-6mm}
\end{figure*}


Despite the remarkable success of transformer-based language models~\citep{vaswani2017attention} across a wide range of tasks, these models exhibit significant limitations in \textit{length generalization}—the ability to extrapolate to longer sequences than those seen during training. Even in simple algorithmic tasks such as arithmetic, standard transformer models trained with autoregressive objectives struggle to generalize to longer problem instances~\citep{dubois2019location,hupkes2020compositionality,newman2020eos,anil2022exploring}. 

To address this, prior work has explored various approaches, including changes to positional embeddings~\citep{ruoss2023randomized,li2023functional,mcleish2024transformers,kazemnejad2024impact,sabbaghi2024explicitly,Cho2024PositionCI,zhou2024transformers}, architectural modifications~\citep{fan2024looped,duan2023interpolation}, and data format changes such as index hinting~\citep{zhou2023algorithms,zhou2024transformers}. While effective in controlled setups, these approaches are often incompatible with how large language models (LLMs) are trained in practice, as they introduce task-specific modifications that are unclear how and to what extent they would transfer to the general purpose settings.

In this work, we attempt to overcome length generalization challenges in the standard transformer setting, by building around an interesting phenomenon that transformers exhibit, i.e., ``transcendence''~\citep{zhang2024transcendence}. Transcendence is the ability of a student model to  generalize slightly beyond the difficulty of the data provided by a teacher during training. Specifically, models trained on simple instances of a task, say $n$ digit arithmetic, can sometimes generate correct outputs for slightly harder instances, e.g., $n+1$ digit arithmetic, with some accuracy. We leverage this property by applying a \textbf{self-improvement} framework, drawing significant inspiration by STaR~\citep{zelikman2022star} and ReST~\citep{gulcehre2023reinforced}, where we alternate between collecting output predictions and finetuning using the self-generated dataset. 

Self-improvement has been widely studied in various contexts~\citep{singh2023beyond,gulcehre2023reinforced,liang2024sheep}, typically in settings where external verifiers, weak supervision, or filtering mechanisms are used to ensure data quality. We demonstrate that extreme length generalization is indeed possible under this framework, \textit{without any  modification to the base transformer architecture}. For tasks like reverse addition and string copying, self-improvement succeeds with no explicit data filtering. However, for harder problems such as multiplication and shortest-path finding in mazes, self-improvement without data filtering fails due to error accumulation. We show that simple filtering techniques—such as length filtering and majority voting—suffice to maintain data quality and enable self-improvement to extend far beyond the initial training distribution.

Our findings suggest that self-improvement is not limited to length generalization but also enables \textit{easy-to-hard generalization}, where a model trained on simpler tasks successfully learns harder tasks without additional supervision. Notably, our approach does not introduce a new self-improvement framework but instead demonstrates its effectiveness across diverse algorithmic tasks. 

Furthermore, we investigate the dynamics of self-improvement and show that: (1) controlling the weak-to-strong curriculum is crucial, as models require a structured difficulty schedule to avoid catastrophic failure, (2) self-improvement accelerates over time, as models increasingly benefit from harder examples, leading in some cases to exponential extrapolation, and (3) starting with a pretrained models singificantly accelerates self-improvement, allowing to generalize further and faster than models trained from scratch. 

Our findings provide evidence that learn self-improvement is a general purpose and scalable solution for length and easy-to-hard generalization. Our contributions can be summarized as:

\begin{enumerate}[left=5pt] %
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{0pt plus 1pt}
    \item We apply an iterative self-training framework to train transformers on the arithmetic, maze and string manipulation tasks, and successfully tackle \textbf{easy-to-hard generalization} to extreme out-of-distribution test data. 
    \item We motivate the importance of a carefully crafted self-improvement schedule and \textbf{label filtering} based on length and majority voting, which are central to consistent self-improvement.
    \item We show that the rate of self-improvement can be exponential and pretrained models can achieve faster acceleration in easy-to-hard generalization.
    \item We investigate some key failure modes of self-correction due to label noise leading to an \textbf{error avalanche}, and discuss how they can be overcome through weak verification.
\end{enumerate}





