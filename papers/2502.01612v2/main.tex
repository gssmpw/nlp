\documentclass{article}





\usepackage[preprint]{neurips_2024}



% \usepackage[finalizecache,cachedir=.]{minted}
\usepackage[frozencache,cachedir=.]{minted}




\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %

\usepackage{graphicx}
\usepackage[position=top]{subfig}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[normalem]{ulem}
\usepackage{wrapfig}

\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\renewcommand \thepart{}
\renewcommand \partname{}


\definecolor{LightGray}{gray}{0.95}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{mydarkgreen}{rgb}{0,0.45,0.08}
  \hypersetup{
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue
}
\usepackage{enumitem}


\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or * \or w \or p \or mcp \or \dagger\or \mathsection\or
   \ddager\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatletter


\title{Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges}




\author{%
  Nayoung Lee\thanks{Authors contributed equally to this paper.} \\
  University of Wisconsin-Madison\\
  \texttt{nayoung.lee@wisc.edu} \\
  \And
  Ziyang Cai\footnotemark[1] \\
  University of Wisconsin-Madison\\
  \texttt{zcai75@wisc.edu} \\
  \And
  Avi Schwarzschild \\
  Carnegie Mellon University\\
  \texttt{schwarzschild@cmu.edu} \\
  \And
  Kangwook Lee \\
  University of Wisconsin-Madison\\
  \texttt{kangwook.lee@wisc.edu} \\
  \And
  Dimitris Papailiopoulos \\
   Microsoft Research, AI Frontiers\\
   University of Wisconsin-Madison\\
  \texttt{dimitris@papail.io} \\
}




\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thm-restate}

\usepackage{cuted}
\usepackage{flushend}


\usepackage{soul}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[most]{tcolorbox}

\usepackage{tabularx}
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}

\definecolor{commentcolour}{rgb}{0.3,0.7,0.2}
\definecolor{blue}{RGB}{33, 144, 141}
\definecolor{lightblue}{RGB}{240, 248, 248} 

\newtcolorbox{finding}{
    enhanced,
    breakable,
    colback=lightblue,         %
    colframe=blue,        %
    boxrule=1.5pt,
    arc=0.25em,
    left=1em,
    right=1em,
    top=1em,
    bottom=0.75em,
    before=\vspace{1em},
    overlay unbroken and first={
        \node[
            fill=blue,
            text=white,
            font=\bfseries,
            anchor=west,
            inner xsep=0.75em,
            inner ysep=0.5em,
            rounded corners=0.25em
        ] 
        at ([xshift=0.75em]frame.north west) {Findings};
    }
}




\usepackage{soul}
\usepackage{adjustbox}
\usepackage{minted}

\usepackage{dsfont}
\newcommand{\eg}{{\it e.g.}, }
\newcommand{\ie}{{\it i.e.}, }
\newcommand\op[1]{\operatorname{#1}}

\newcommand\DP[1]{{\color{magenta}DP: #1}}
\newcommand\ny[1]{{\color{blue}NY: #1}}
\newcommand\JC[1]{{\color{teal}JC: #1}}
\newcommand\cmt[1]{\textcolor{red}{#1}}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

\renewcommand{\arraystretch}{1.1} %

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\ifdefined\usebigfont
\renewcommand{\mddefault}{bx}
\renewcommand\seriesdefault{b}
\renewcommand\bfdefault{bx} %
\usepackage{times}
\usepackage[fontsize=13pt]{scrextend}
\makeatletter
\@ifpackageloaded{geometry}{\AtBeginDocument{\newgeometry{letterpaper,left=1.56in,right=1.56in,top=1.71in,bottom=1.77in}}}{\usepackage[letterpaper,left=1.56in,right=1.56in,top=1.71in,bottom=1.77in]{geometry}}
\makeatother
\else
\fi

\usepackage{thm-restate}

\appto\appendix{\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}}



\begin{document}
\doparttoc

\maketitle


\begin{abstract}

Large language models often struggle with length generalization and solving complex problem instances beyond their training distribution. We present a self-improvement approach where models iteratively generate and learn from their own solutions, progressively tackling harder problems while maintaining a standard transformer architecture. Across diverse tasks including arithmetic, string manipulation, and maze solving, self-improving enables models to solve problems far beyond their initial training distributionâ€”for instance, generalizing from 10-digit to 100-digit addition without apparent saturation. We observe that in some cases filtering for correct self-generated examples leads to exponential improvements in out-of-distribution performance across training rounds. Additionally, starting from pretrained models significantly accelerates this self-improvement process for several tasks. Our results demonstrate how controlled weak-to-strong curricula can systematically teach a model logical extrapolation without any changes to the positional embeddings, or the model architecture. 


\end{abstract}


\input{_Intro}
\input{_RelatedWorks}
\input{_Exps}
\input{_Results_easy}
\input{_Results_datafilter}
\input{_Results_hard}
\input{_Results_ablation}
\input{_Results_analysis}
\input{_Discussion}
\input{_Conclusion}


\bibliography{ref}
\bibliographystyle{icml2023}




\newpage
\tableofcontents

\appendix
\addcontentsline{toc}{section}{Appendix} %
\part{Appendix}
\parttoc %
\input{_Appendix_Additional}
\input{_Appendix_Setup}
\input{_Appendix_FullResults} %







\end{document}