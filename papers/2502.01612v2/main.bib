@article{sun2024easy,
  title={Easy-to-hard generalization: Scalable alignment beyond human supervision},
  author={Sun, Zhiqing and Yu, Longhui and Shen, Yikang and Liu, Weiyang and Yang, Yiming and Welleck, Sean and Gan, Chuang},
  journal={arXiv preprint arXiv:2403.09472},
  year={2024}
}


@article{schwarzschild2021can,
  title={Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6695--6706},
  year={2021}
}


@article{wen2024understanding,
  title={Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective},
  author={Wen, Kaiyue and Li, Zhiyuan and Wang, Jason and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2410.05192},
  year={2024}
}

@article{feng2024beyond,
  title={Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement},
  author={Feng, Yunzhen and Dohmatob, Elvis and Yang, Pu and Charton, Francois and Kempe, Julia},
  journal={arXiv preprint arXiv:2406.07515},
  year={2024}
}

@article{charton2024patternboost,
  title={PatternBoost: Constructions in Mathematics with a Little Help from AI},
  author={Charton, Fran{\c{c}}ois and Ellenberg, Jordan S and Wagner, Adam Zsolt and Williamson, Geordie},
  journal={arXiv preprint arXiv:2411.00566},
  year={2024}
}

@article{gulcehre2023reinforced,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998},
  year={2023}
}

@article{amini2022self,
  title={Self-training: A survey},
  author={Amini, Massih-Reza and Feofanov, Vasilii and Pauletto, Loic and Hadjadj, Lies and Devijver, Emilie and Maximov, Yury},
  journal={arXiv preprint arXiv:2202.12040},
  year={2022}
}

@article{quan2024language,
  title={Language Models Can Self-Lengthen to Generate Long Texts},
  author={Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, Junyang Lin},
  journal={arXiv preprint arXiv:2410.23933},
  year={2024}
}

@article{deng2024explicit,
  title={From explicit cot to implicit cot: Learning to internalize cot step by step},
  author={Deng, Yuntian and Choi, Yejin and Shieber, Stuart},
  journal={arXiv preprint arXiv:2405.14838},
  year={2024}
}

@article{lee2023teaching,
  title={Teaching arithmetic to small transformers},
  author={Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2307.03381},
  year={2023}
}

@article{zhou2024transformers,
  title={Transformers can achieve length generalization but not robustly},
  author={Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.09371},
  year={2024}
}

@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}

@article{singh2023beyond,
  title={Beyond human data: Scaling self-training for problem-solving with language models},
  author={Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and others},
  journal={arXiv preprint arXiv:2312.06585},
  year={2023}
}

@misc{prasad2024selfconsistencypreferenceoptimization,
      title={Self-Consistency Preference Optimization}, 
      author={Archiki Prasad and Weizhe Yuan and Richard Yuanzhe Pang and Jing Xu and Maryam Fazel-Zarandi and Mohit Bansal and Sainbayar Sukhbaatar and Jason Weston and Jane Yu},
      year={2024},
      eprint={2411.04109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.04109}, 
}

@article{ivanitskiy2023structured,
  title={Structured World Representations in Maze-Solving Transformers},
  author={Ivanitskiy, Michael Igorevich and Spies, Alex F and R{\"a}uker, Tilman and Corlouer, Guillaume and Mathwin, Chris and Quirke, Lucia and Rager, Can and Shah, Rusheb and Valentine, Dan and Behn, Cecilia Diniz and others},
  journal={arXiv preprint arXiv:2312.02566},
  year={2023}
}


@article{shumailov2024ai,
  title={AI models collapse when trained on recursively generated data},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  journal={Nature},
  volume={631},
  number={8022},
  pages={755--759},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{zhang2023chain,
  title={Chain-of-thought reasoning is a policy improvement operator},
  author={Zhang, Hugh and Parkes, David C},
  journal={arXiv preprint arXiv:2309.08589},
  year={2023}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nogueira2021investigating,
  title={Investigating the limitations of transformers with simple arithmetic tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  journal={arXiv preprint arXiv:2102.13019},
  year={2021}
}

@article{charton2023can,
  title={Can transformers learn the greatest common divisor?},
  author={Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2308.15594},
  year={2023}
}

@article{jelassi2023length,
  title={Length generalization in arithmetic transformers},
  author={Jelassi, Samy and d'Ascoli, St{\'e}phane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2306.15400},
  year={2023}
}

@article{quirke2023understanding,
  title={Understanding addition in transformers},
  author={Quirke, Philip and Barez, Fazl},
  journal={arXiv preprint arXiv:2310.13121},
  year={2023}
}

@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}

@article{fan2024looped,
  title={Looped Transformers for Length Generalization},
  author={Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook},
  journal={arXiv preprint arXiv:2409.15647},
  year={2024}
}

@article{mcleish2024transformers,
  title={Transformers Can Do Arithmetic with the Right Embeddings},
  author={McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and others},
  journal={arXiv preprint arXiv:2405.17399},
  year={2024}
}

@article{shen2023positional,
  title={Positional description matters for transformers arithmetic},
  author={Shen, Ruoqi and Bubeck, S{\'e}bastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi},
  journal={arXiv preprint arXiv:2311.14737},
  year={2023}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{qu2024recursive,
  title={Recursive introspection: Teaching language model agents how to self-improve},
  author={Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
  journal={arXiv preprint arXiv:2407.18219},
  year={2024}
}

@article{peng2024regenesis,
  title={ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement},
  author={Peng, Xiangyu and Xia, Congying and Yang, Xinyi and Xiong, Caiming and Wu, Chien-Sheng and Xing, Chen},
  journal={arXiv preprint arXiv:2410.02108},
  year={2024}
}

@article{lang2024theoretical,
  title={Theoretical Analysis of Weak-to-Strong Generalization},
  author={Lang, Hunter and Sontag, David and Vijayaraghavan, Aravindan},
  journal={arXiv preprint arXiv:2405.16043},
  year={2024}
}

@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{charton2024emergent,
  title={Emergent properties with repeated examples},
  author={Charton, Fran{\c{c}}ois and Kempe, Julia},
  journal={arXiv preprint arXiv:2410.07041},
  year={2024}
}

@article{bachmann2024pitfalls,
  title={The pitfalls of next-token prediction},
  author={Bachmann, Gregor and Nagarajan, Vaishnavh},
  journal={arXiv preprint arXiv:2403.06963},
  year={2024}
}

@article{zhang2024transcendence,
  title={Transcendence: Generative Models Can Outperform The Experts That Train Them},
  author={Zhang, Edwin and Zhu, Vincent and Saphra, Naomi and Kleiman, Anat and Edelman, Benjamin L and Tambe, Milind and Kakade, Sham M and Malach, Eran},
  journal={arXiv preprint arXiv:2406.11741},
  year={2024}
}

@article{velivckovic2024softmax,
  title={softmax is not enough (for sharp out-of-distribution)},
  author={Veli{\v{c}}kovi{\'c}, Petar and Perivolaropoulos, Christos and Barbero, Federico and Pascanu, Razvan},
  journal={arXiv preprint arXiv:2410.01104},
  year={2024}
}


@inproceedings{yehudai2021local,
  title={From local structures to size generalization in graph neural networks},
  author={Yehudai, Gilad and Fetaya, Ethan and Meirom, Eli and Chechik, Gal and Maron, Haggai},
  booktitle={International Conference on Machine Learning},
  pages={11975--11986},
  year={2021},
  organization={PMLR}
}

@article{sabbaghi2024explicitly,
  title={Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks},
  author={Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi},
  journal={arXiv preprint arXiv:2406.01895},
  year={2024}
}

@article{cho2024arithmetic,
  title={Arithmetic transformers can length-generalize in both operand length and count},
  author={Cho, Hanseul and Cha, Jaeyoung and Bhojanapalli, Srinadh and Yun, Chulhee},
  journal={arXiv preprint arXiv:2410.15787},
  year={2024}
}

@inproceedings{Cho2024PositionCI,
  title={Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure},
  author={Hanseul Cho and Jaeyoung Cha and Pranjal Awasthi and Srinadh Bhojanapalli and Anupam Gupta and Chulhee Yun},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273695226}
}

@article{li2023functional,
  title={Functional interpolation for relative positions improves long context transformers},
  author={Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  journal={arXiv preprint arXiv:2310.04418},
  year={2023}
}

@article{ruoss2023randomized,
  title={Randomized positional encodings boost length generalization of transformers},
  author={Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Csord{\'a}s, R{\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  journal={arXiv preprint arXiv:2305.16843},
  year={2023}
}

@article{duan2023interpolation,
  title={From interpolation to extrapolation: Complete length generalization for arithmetic transformers},
  author={Duan, Shaoxiong and Shi, Yining and Xu, Wei},
  journal={arXiv preprint arXiv:2310.11984},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{wang2023voyager,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2305.16291},
  year={2023}
}


@article{shumailov2023curse,
  title={The curse of recursion: Training on generated data makes models forget},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal={arXiv preprint arXiv:2305.17493},
  year={2023}
}

@inproceedings{Bayat2024ThePO,
  title={The Pitfalls of Memorization: When Memorization Hurts Generalization},
  author={Reza Bayat and Mohammad Pezeshki and Elvis Dohmatob and David Lopez-Paz and Pascal Vincent},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:274610625}
}

@inproceedings{alfarano2024global,
  title={Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers},
  author={Alfarano, Alberto and Charton, Fran{\c{c}}ois and Hayat, Amaury},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{butter2024extrapolating,
  title={Extrapolating Jet Radiation with Autoregressive Transformers},
  author={Butter, Anja and Charton, Fran{\c{c}}ois and Villadamigo, Javier Mari{\~n}o and Ore, Ayodele and Plehn, Tilman and Spinner, Jonas},
  journal={arXiv preprint arXiv:2412.12074},
  year={2024}
}

@article{xin2024deepseek,
  title={DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data},
  author={Xin, Huajian and Guo, Daya and Shao, Zhihong and Ren, Zhizhou and Zhu, Qihao and Liu, Bo and Ruan, Chong and Li, Wenda and Liang, Xiaodan},
  journal={arXiv preprint arXiv:2405.14333},
  year={2024}
}


@article{bansal2024smaller,
  title={Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling},
  author={Bansal, Hritik and Hosseini, Arian and Agarwal, Rishabh and Tran, Vinh Q and Kazemi, Mehran},
  journal={arXiv preprint arXiv:2408.16737},
  year={2024}
}



@misc{huang2024selfimprovementlanguagemodelssharpening,
      title={Self-Improvement in Language Models: The Sharpening Mechanism}, 
      author={Audrey Huang and Adam Block and Dylan J. Foster and Dhruv Rohatgi and Cyril Zhang and Max Simchowitz and Jordan T. Ash and Akshay Krishnamurthy},
      year={2024},
      eprint={2412.01951},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.01951}, 
}

@inproceedings{zhang2019your,
  title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
  author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3713--3722},
  year={2019}
}

@article{yang2024weak,
  title={Weak-to-strong reasoning},
  author={Yang, Yuqing and Ma, Yan and Liu, Pengfei},
  journal={arXiv preprint arXiv:2407.13647},
  year={2024}
}

@article{bowman2022measuring,
  title={Measuring progress on scalable oversight for large language models},
  author={Bowman, Samuel R and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Askell, Amanda and Jones, Andy and Chen, Anna and others},
  journal={arXiv preprint arXiv:2211.03540},
  year={2022}
}

@article{gerstgrasser2024model,
  title={Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data},
  author={Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others},
  journal={arXiv preprint arXiv:2404.01413},
  year={2024}
}


@article{Bertrand2023OnTS,
  title={On the Stability of Iterative Retraining of Generative Models on their own Data},
  author={Quentin Bertrand and Avishek Joey Bose and Alexandre Duplessis and Marco Jiralerspong and Gauthier Gidel},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.00429},
  url={https://api.semanticscholar.org/CorpusID:263334017}
}

@article{Dohmatob2024ATO,
  title={A Tale of Tails: Model Collapse as a Change of Scaling Laws},
  author={Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.07043},
  url={https://api.semanticscholar.org/CorpusID:267628004}
}

@InProceedings{Hataya_2023_ICCV,
    author    = {Hataya, Ryuichiro and Bao, Han and Arai, Hiromi},
    title     = {Will Large-scale Generative Models Corrupt Future Datasets?},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {20555-20565}
}

@article{Arcaute2023CombiningGA,
  title={Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?},
  author={Gonzalo Mart{\'i}nez Ruiz de Arcaute and Lauren Watson and Pedro Reviriego and Jos{\'e} Alberto Hern{\'a}ndez and Marc Ju{\'a}rez and Rik Sarkar},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.01255},
  url={https://api.semanticscholar.org/CorpusID:257280389}
}

@article{Alemohammad2023SelfConsumingGM,
  title={Self-Consuming Generative Models Go MAD},
  author={Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Reza Babaei and Daniel LeJeune and Ali Siahkoohi and Richard Baraniuk},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.01850},
  url={https://api.semanticscholar.org/CorpusID:259341801}
}

@article{Briesch2023LargeLM,
  title={Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop},
  author={Martin Briesch and Dominik Sobania and Franz Rothlauf},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.16822},
  url={https://api.semanticscholar.org/CorpusID:265466007}
}

@article{Huang2023LargeLM,
  title={Large Language Models Cannot Self-Correct Reasoning Yet},
  author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.01798},
  url={https://api.semanticscholar.org/CorpusID:263609132}
}

@article{Hu2021LoRALA,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.09685},
  url={https://api.semanticscholar.org/CorpusID:235458009}
}

@inproceedings{zelikman2022star,
  title={STaR: Bootstrapping Reasoning With Reasoning},
  author={E. Zelikman and Yuhuai Wu and Noah D. Goodman},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247762790}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{song2024mind,
  title={Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models},
  author={Song, Yuda and Zhang, Hanlin and Eisenach, Carson and Kakade, Sham and Foster, Dean and Ghai, Udaya},
  journal={arXiv preprint arXiv:2412.02674},
  year={2024}
}

@article{gillman2024self,
  title={Self-Correcting Self-Consuming Loops for Generative Model Training},
  author={Gillman, Nate and Freeman, Michael and Aggarwal, Daksh and Hsu, Chia-Hong and Luo, Calvin and Tian, Yonglong and Sun, Chen},
  journal={arXiv preprint arXiv:2402.07087},
  year={2024}
}

@article{liang2024sheep,
  title={I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm},
  author={Liang, Yiming and Zhang, Ge and Qu, Xingwei and Zheng, Tianyu and Guo, Jiawei and Du, Xinrun and Yang, Zhenzhu and Liu, Jiaheng and Lin, Chenghua and Ma, Lei and others},
  journal={arXiv preprint arXiv:2408.08072},
  year={2024}
}


@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@article{hosseini2024v,
  title={V-star: Training verifiers for self-taught reasoners},
  author={Hosseini, Arian and Yuan, Xingdi and Malkin, Nikolay and Courville, Aaron and Sordoni, Alessandro and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2402.06457},
  year={2024}
}

@article{hase2024unreasonable,
  title={The unreasonable effectiveness of easy training data for hard tasks},
  author={Hase, Peter and Bansal, Mohit and Clark, Peter and Wiegreffe, Sarah},
  journal={arXiv preprint arXiv:2401.06751},
  year={2024}
}

@article{bansal2022end,
  title={End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking},
  author={Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20232--20242},
  year={2022}
}


@article{wang2022selfinstruct,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{madaan2024selfrefine,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{rolnick2017deep,
  title={Deep learning is robust to massive label noise},
  author={Rolnick, D},
  journal={arXiv preprint arXiv:1705.10694},
  year={2017}
}

@article{wood2023unified,
  title={A unified theory of diversity in ensemble learning},
  author={Wood, Danny and Mu, Tingting and Webb, Andrew M and Reeve, Henry WJ and Lujan, Mikel and Brown, Gavin},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={359},
  pages={1--49},
  year={2023}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{shin2024weak,
  title={Weak-to-Strong Generalization Through the Data-Centric Lens},
  author={Shin, Changho and Cooper, John and Sala, Frederic},
  journal={arXiv preprint arXiv:2412.03881},
  year={2024}
}

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  pages={123--140},
  year={1996},
  publisher={Springer}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{newman2020eos,
  title={The EOS decision and length extrapolation},
  author={Newman, Benjamin and Hewitt, John and Liang, Percy and Manning, Christopher D},
  journal={arXiv preprint arXiv:2010.07174},
  year={2020}
}

@article{dubois2019location,
  title={Location attention for extrapolation to longer sequences},
  author={Dubois, Yann and Dagan, Gautier and Hupkes, Dieuwke and Bruni, Elia},
  journal={arXiv preprint arXiv:1911.03872},
  year={2019}
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{llama3modelcard,

title={Llama 3 Model Card},

author={AI@Meta},

year={2024},

url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}

}

@misc{pang2024iterativereasoningpreferenceoptimization,
      title={Iterative Reasoning Preference Optimization}, 
      author={Richard Yuanzhe Pang and Weizhe Yuan and Kyunghyun Cho and He He and Sainbayar Sukhbaatar and Jason Weston},
      year={2024},
      eprint={2404.19733},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.19733}, 
}
