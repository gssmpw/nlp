\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/reverse_add/reverse_add.pdf}
    \caption{Results on the reverse addition task, where both operands and the output are represented in reverse order, with the least significant digit first. The self-improvement framework enables a model initially trained on 1-16 digit examples to generalize perfectly to over 100-digit addition. }
    \label{fig:result_reverse_add}
\end{figure}



\section{Length Generalization on Reverse Addition and String Copying/Reversing}\label{sec:addition}
\begin{finding}
    With self-improvement training, transformers can achieve state-of-the-art length generalization on reversed addition, string copying and maze solving, without further architectural modifications.
\end{finding}


In this section, we apply our self-improvement framework to relatively simple tasks such as reverse addition and string copying/reversing. These tasks serve as testbeds to demonstrate that our framework can extend model capabilities far beyond the original training distribution through multiple iterations of self-improvement, even in the absence of additional data filtering.

\subsection{Reverse Addition}\label{sec:reverse_addition}


Reversed addition, where the operands and output are written with the least significant digit first, has been shown to enhance sample efficiency and performance~\citep{lee2023teaching}. Reversed addition has become a popular setting for studying length generalization in arithmetic tasks~\citep{lee2023teaching,shen2023positional,zhou2023algorithms,zhou2024transformers,Cho2024PositionCI,mcleish2024transformers}. 
Here, we show that the self-improvement framework achieves substantial length generalization on reversed addition without any modifications to positional encodings, input formats, or the Transformer architecture.

\paragraph{Setting.} 
The initial supervised dataset $\mathcal{D}_0$ contains 2 million examples of reverse addition, with operand lengths ranging from 1 to 16 digits. This dataset is used to train the model for 10,000 steps. In subsequent self-improvement rounds, we sample 50,000 additional training examples at each round, extending the operand length by one digit. Specifically, at self-improvement round \( r \), the self-generated data $\mathcal{D}_r$ consists of length-\( 16+r \) examples produced by the model \( M_r \). The model is fine-tuned on the combined dataset $\mathcal{D}_0 \cup \mathcal{D}_1 \cup \dots \cup \mathcal{D}_r$ for 1,500 steps, resulting in an improved model \( M_{r+1} \).

\paragraph{Results.} Figure~\ref{fig:result_reverse_add} demonstrates that, starting with a model trained on 1 to 16-digit reverse addition, the self-improvement framework enables near-perfect length generalization up to 100 digits without any additional supervision. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{fig/copy-sorting-parity/copy.png}
    \includegraphics[width=0.75\linewidth]{fig//copy-sorting-parity/reverse.png}
    \caption{Results on string manipulation tasks. (Top) Copy: the model replicates the input string exactly. (Bottom) Reverse: the model outputs the input string in reverse order. The model initially trained on strings of length 1 to 10 generalizes to sequences of over 120. %
    }
    \label{fig:result_copy_reverse}
\end{figure}

\subsection{String Copy \& String Reverse}\label{sec:string_copy}



Copying and reversing a given input string is another task that is considered hard for vanilla transformers~\citep{anil2022exploring,zhou2023algorithms}. The input string consists of digits from 0 to 9. 

\paragraph{Setting.} 
The initial training set $\mathcal{D_0}$ consists of 2 million examples of strings of length 1 to 10. The vocabulary of the string is the digits $0$ to $9$. For each subsequent round $r$, we sample $D_{r}$ consisting of $50000$ examples of length $10+r$ from the model $M_r$. Then we continue training $M_r$ on the combined dataset $D_1\cup\dots\cup D_r$ for 500 steps to obtain $M_{r+1}$. 

\paragraph{Results.} 
Figure~\ref{fig:result_copy_reverse} demonstrates that starting with strings of length 1 to 10, the self-improvement framework enables the model to perfectly generalize to string lengths of over 120 after approximately 100 self-improvement rounds.


