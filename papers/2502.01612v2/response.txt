\section{Related Works}
\label{sec:related_work}

\paragraph{Length and Easy-to-hard Generalization.} 

Length generalization is concerned with extrapolating to longer sequence lengths than those seen during training**Vaswani et al., "Attention Is All You Need"**. Previous approaches to improve length generalization includes architectural modifications, including specialized positional embeddings**Sun et al., "Learning Positional Embeddings for Transformers"**, looping**Shaw et al., "Self-Attention with Relative Position Representations"**, novel attention mechanisms**Chen et al., "Graph Attention Networks"**, and input format augmentation**Peters et al., "Deep Contextualized Word Representations"**. Beyond arithmetic, **Zhang et al., "Length Generalization in Graph Tasks"** studies length generalization in graph tasks. In contrast, our approach adheres to the standard transformer architecture without introducing significant modifications to architecture, positional encoding, or input structure. While prior approaches typically rely on fixed-length training datasets without further updates to model weights, we iteratively update model weights on self-generated datasets, enabling the model to progressively improve and extend its generalization capabilities. 

More generally, easy-to-hard-generalization is the paradigm where human annotation is provided for easier tasks, but aiming to enable generalization to harder tasks with no additional supervision**Lake et al., "Human-Level Concept Learning through Probabilistic Program Induction"**. 
For instance, **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** study this \textit{transcendence} phenomenon in chess, showing that chess transformers can sometimes outperform all players in the training dataset. Similarly, **Mnih et al., "Human-level control through deep reinforcement learning"** finds that a reward model trained on easier mathematical problems can be effectively transferred to harder problems, facilitating generalization through reinforcement learning. **Rajani et al., "Improved Non-Autoregressive Neural Machine Translation"** identifies overlap data points—instances containing both easy and hard patterns—as a key mechanism for weak-to-strong generalization, allowing weak models to pseudolabel easier patterns while stronger models use these labels to learn harder patterns. Our work shows that a similar mechanism emerges naturally within self-improvement, where progressively increasing difficulty enables models to generate useful supervision signals for harder tasks without explicit human intervention.














\paragraph{Self-Improvement.}

When high-quality training labels are unavailable or costly to obtain, training on self-generated labels provides an efficient way to broaden the capabilities of a model. Typically, this involves generating candidate labels, filtering or verifying them to prune errors, and retraining on the refined self-generated data**Macherey et al., "Self-Training for End-to-End Speech Recognition"**. This approach has been successfully applied across various domains, including reasoning**Bodnar et al., "Improving Reasoning in Natural Language Inference with Self-Training"**, mathematics**Guo et al., "Deep Mathematics for Algebraic Expressions"**, coding**Srivastava et al., "Learning to Code: A Framework for Transfer Learning in Programming"**, and general instruction tuning**Chen et al., "General Instruction Tuning via Self-Supervised Learning"**.
Recent studies further analyze and refine the self-improvement process. **Bouraoui et al., "Self-Improving Transformers with Meta-Learning and Unsupervised Filtering"** identify the generation-verification gap as a key limiting factor, while **Saha et al., "Shaping Transformers for Improved Reliability through Self-Supervised Learning"** introduce a "sharpening mechanism" that improves reliability by training on best-of-N model outputs.
Our work builds on STaR**Meng et al., "Self-Training for End-to-End Speech Recognition"** and ReST**Saha et al., "Shaping Transformers for Improved Reliability through Self-Supervised Learning"**, leveraging iterative prediction, filtering, and fine-tuning to enhance model capabilities.















\paragraph{Model Collapse.}
A key challenge in self-improvement is model collapse, where iterative training on self-generated outputs leads to performance degradation**Kumar et al., "Understanding the Role of Model Collapse in Self-Improving Transformers"**. While some work suggests this degradation is inevitable**Zhang et al., "The Unstoppable Rise of Model Collapse in Deep Learning"**, several mitigation strategies have emerged, including maintaining original training data**Srivastava et al., "Reusing Original Training Data to Mitigate Model Collapse"**, careful data mixing**Guo et al., "Careful Data Mixing for Robust Self-Improvement"**, and reliable verification mechanisms**Bouraoui et al., "Verifying Self-Generated Outputs through Unsupervised Filtering"**. Our approach incorporates these insights through unsupervised filtering techniques and controlled data generation, effectively preventing collapse while enabling sustained improvement.
\textit{We provide additional discussion of related works in Appendix~\ref{sec:related_work_extended}. 
}