[
  {
    "index": 0,
    "papers": [
      {
        "key": "dubois2019location",
        "author": "Dubois, Yann and Dagan, Gautier and Hupkes, Dieuwke and Bruni, Elia",
        "title": "Location attention for extrapolation to longer sequences"
      },
      {
        "key": "hupkes2020compositionality",
        "author": "Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia",
        "title": "Compositionality decomposed: How do neural networks generalise?"
      },
      {
        "key": "newman2020eos",
        "author": "Newman, Benjamin and Hewitt, John and Liang, Percy and Manning, Christopher D",
        "title": "The EOS decision and length extrapolation"
      },
      {
        "key": "anil2022exploring",
        "author": "Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam",
        "title": "Exploring length generalization in large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "press2021train",
        "author": "Press, Ofir and Smith, Noah A and Lewis, Mike",
        "title": "Train short, test long: Attention with linear biases enables input length extrapolation"
      },
      {
        "key": "li2023functional",
        "author": "Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh",
        "title": "Functional interpolation for relative positions improves long context transformers"
      },
      {
        "key": "ruoss2023randomized",
        "author": "Ruoss, Anian and Del{\\'e}tang, Gr{\\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Csord{\\'a}s, R{\\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel",
        "title": "Randomized positional encodings boost length generalization of transformers"
      },
      {
        "key": "kazemnejad2024impact",
        "author": "Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva",
        "title": "The impact of positional encoding on length generalization in transformers"
      },
      {
        "key": "sabbaghi2024explicitly",
        "author": "Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi",
        "title": "Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks"
      },
      {
        "key": "Cho2024PositionCI",
        "author": "Hanseul Cho and Jaeyoung Cha and Pranjal Awasthi and Srinadh Bhojanapalli and Anupam Gupta and Chulhee Yun",
        "title": "Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure"
      },
      {
        "key": "zhou2024transformers",
        "author": "Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny",
        "title": "Transformers can achieve length generalization but not robustly"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "fan2024looped",
        "author": "Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook",
        "title": "Looped Transformers for Length Generalization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "duan2023interpolation",
        "author": "Duan, Shaoxiong and Shi, Yining and Xu, Wei",
        "title": "From interpolation to extrapolation: Complete length generalization for arithmetic transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhou2023algorithms",
        "author": "Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum",
        "title": "What algorithms can transformers learn? a study in length generalization"
      },
      {
        "key": "zhou2024transformers",
        "author": "Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny",
        "title": "Transformers can achieve length generalization but not robustly"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yehudai2021local",
        "author": "Yehudai, Gilad and Fetaya, Ethan and Meirom, Eli and Chechik, Gal and Maron, Haggai",
        "title": "From local structures to size generalization in graph neural networks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "schwarzschild2021can",
        "author": "Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom",
        "title": "Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks"
      },
      {
        "key": "bansal2022end",
        "author": "Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom",
        "title": "End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking"
      },
      {
        "key": "burns2023weak",
        "author": "Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others",
        "title": "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision"
      },
      {
        "key": "hase2024unreasonable",
        "author": "Hase, Peter and Bansal, Mohit and Clark, Peter and Wiegreffe, Sarah",
        "title": "The unreasonable effectiveness of easy training data for hard tasks"
      },
      {
        "key": "sun2024easy",
        "author": "Sun, Zhiqing and Yu, Longhui and Shen, Yikang and Liu, Weiyang and Yang, Yiming and Welleck, Sean and Gan, Chuang",
        "title": "Easy-to-hard generalization: Scalable alignment beyond human supervision"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2024transcendence",
        "author": "Zhang, Edwin and Zhu, Vincent and Saphra, Naomi and Kleiman, Anat and Edelman, Benjamin L and Tambe, Milind and Kakade, Sham M and Malach, Eran",
        "title": "Transcendence: Generative Models Can Outperform The Experts That Train Them"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "sun2024easy",
        "author": "Sun, Zhiqing and Yu, Longhui and Shen, Yikang and Liu, Weiyang and Yang, Yiming and Welleck, Sean and Gan, Chuang",
        "title": "Easy-to-hard generalization: Scalable alignment beyond human supervision"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shin2024weak",
        "author": "Shin, Changho and Cooper, John and Sala, Frederic",
        "title": "Weak-to-Strong Generalization Through the Data-Centric Lens"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zelikman2022star",
        "author": "E. Zelikman and Yuhuai Wu and Noah D. Goodman",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "key": "wang2022selfinstruct",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      },
      {
        "key": "huang2022large",
        "author": "Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei",
        "title": "Large language models can self-improve"
      },
      {
        "key": "singh2023beyond",
        "author": "Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and others",
        "title": "Beyond human data: Scaling self-training for problem-solving with language models"
      },
      {
        "key": "chen2023teaching",
        "author": "Chen, Xinyun and Lin, Maxwell and Sch{\\\"a}rli, Nathanael and Zhou, Denny",
        "title": "Teaching large language models to self-debug"
      },
      {
        "key": "gulcehre2023reinforced",
        "author": "Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others",
        "title": "Reinforced self-training (rest) for language modeling"
      },
      {
        "key": "madaan2024selfrefine",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      },
      {
        "key": "yuan2024self",
        "author": "Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason",
        "title": "Self-rewarding language models"
      },
      {
        "key": "liang2024sheep",
        "author": "Liang, Yiming and Zhang, Ge and Qu, Xingwei and Zheng, Tianyu and Guo, Jiawei and Du, Xinrun and Yang, Zhenzhu and Liu, Jiaheng and Lin, Chenghua and Ma, Lei and others",
        "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm"
      },
      {
        "key": "pang2024iterativereasoningpreferenceoptimization",
        "author": "Richard Yuanzhe Pang and Weizhe Yuan and Kyunghyun Cho and He He and Sainbayar Sukhbaatar and Jason Weston",
        "title": "Iterative Reasoning Preference Optimization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zelikman2022star",
        "author": "E. Zelikman and Yuhuai Wu and Noah D. Goodman",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "key": "huang2022large",
        "author": "Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei",
        "title": "Large language models can self-improve"
      },
      {
        "key": "singh2023beyond",
        "author": "Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and others",
        "title": "Beyond human data: Scaling self-training for problem-solving with language models"
      },
      {
        "key": "pang2024iterativereasoningpreferenceoptimization",
        "author": "Richard Yuanzhe Pang and Weizhe Yuan and Kyunghyun Cho and He He and Sainbayar Sukhbaatar and Jason Weston",
        "title": "Iterative Reasoning Preference Optimization"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2023chain",
        "author": "Zhang, Hugh and Parkes, David C",
        "title": "Chain-of-thought reasoning is a policy improvement operator"
      },
      {
        "key": "charton2024patternboost",
        "author": "Charton, Fran{\\c{c}}ois and Ellenberg, Jordan S and Wagner, Adam Zsolt and Williamson, Geordie",
        "title": "PatternBoost: Constructions in Mathematics with a Little Help from AI"
      },
      {
        "key": "alfarano2024global",
        "author": "Alfarano, Alberto and Charton, Fran{\\c{c}}ois and Hayat, Amaury",
        "title": "Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers"
      },
      {
        "key": "liang2024sheep",
        "author": "Liang, Yiming and Zhang, Ge and Qu, Xingwei and Zheng, Tianyu and Guo, Jiawei and Du, Xinrun and Yang, Zhenzhu and Liu, Jiaheng and Lin, Chenghua and Ma, Lei and others",
        "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "chen2023teaching",
        "author": "Chen, Xinyun and Lin, Maxwell and Sch{\\\"a}rli, Nathanael and Zhou, Denny",
        "title": "Teaching large language models to self-debug"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2022selfinstruct",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      },
      {
        "key": "yuan2024self",
        "author": "Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason",
        "title": "Self-rewarding language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "song2024mind",
        "author": "Song, Yuda and Zhang, Hanlin and Eisenach, Carson and Kakade, Sham and Foster, Dean and Ghai, Udaya",
        "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "huang2024selfimprovementlanguagemodelssharpening",
        "author": "Audrey Huang and Adam Block and Dylan J. Foster and Dhruv Rohatgi and Cyril Zhang and Max Simchowitz and Jordan T. Ash and Akshay Krishnamurthy",
        "title": "Self-Improvement in Language Models: The Sharpening Mechanism"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zelikman2022star",
        "author": "E. Zelikman and Yuhuai Wu and Noah D. Goodman",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "gulcehre2023reinforced",
        "author": "Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others",
        "title": "Reinforced self-training (rest) for language modeling"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "Hataya_2023_ICCV",
        "author": "Hataya, Ryuichiro and Bao, Han and Arai, Hiromi",
        "title": "Will Large-scale Generative Models Corrupt Future Datasets?"
      },
      {
        "key": "Arcaute2023CombiningGA",
        "author": "Gonzalo Mart{\\'i}nez Ruiz de Arcaute and Lauren Watson and Pedro Reviriego and Jos{\\'e} Alberto Hern{\\'a}ndez and Marc Ju{\\'a}rez and Rik Sarkar",
        "title": "Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?"
      },
      {
        "key": "Alemohammad2023SelfConsumingGM",
        "author": "Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Reza Babaei and Daniel LeJeune and Ali Siahkoohi and Richard Baraniuk",
        "title": "Self-Consuming Generative Models Go MAD"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "shumailov2024ai",
        "author": "Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin",
        "title": "AI models collapse when trained on recursively generated data"
      },
      {
        "key": "shumailov2023curse",
        "author": "Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross",
        "title": "The curse of recursion: Training on generated data makes models forget"
      },
      {
        "key": "zhang2023chain",
        "author": "Zhang, Hugh and Parkes, David C",
        "title": "Chain-of-thought reasoning is a policy improvement operator"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "gerstgrasser2024model",
        "author": "Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others",
        "title": "Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "gerstgrasser2024model",
        "author": "Gerstgrasser, Matthias and Schaeffer, Rylan and Dey, Apratim and Rafailov, Rafael and Sleight, Henry and Hughes, John and Korbak, Tomasz and Agrawal, Rajashree and Pai, Dhruv and Gromov, Andrey and others",
        "title": "Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data"
      },
      {
        "key": "Dohmatob2024ATO",
        "author": "Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe",
        "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws"
      },
      {
        "key": "Briesch2023LargeLM",
        "author": "Martin Briesch and Dominik Sobania and Franz Rothlauf",
        "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "gillman2024self",
        "author": "Gillman, Nate and Freeman, Michael and Aggarwal, Daksh and Hsu, Chia-Hong and Luo, Calvin and Tian, Yonglong and Sun, Chen",
        "title": "Self-Correcting Self-Consuming Loops for Generative Model Training"
      },
      {
        "key": "feng2024beyond",
        "author": "Feng, Yunzhen and Dohmatob, Elvis and Yang, Pu and Charton, Francois and Kempe, Julia",
        "title": "Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement"
      }
    ]
  }
]