\section{Related Works}
\label{sec:related_work}

\paragraph{Length and Easy-to-hard Generalization.} 

Length generalization is concerned with extrapolating to longer sequence lengths than those seen during training~\citep{dubois2019location,hupkes2020compositionality,newman2020eos,anil2022exploring}. Previous approaches to improve length generalization includes architectural modifications, including specialized positional embeddings~\citep{press2021train,li2023functional,ruoss2023randomized,kazemnejad2024impact,sabbaghi2024explicitly,Cho2024PositionCI,zhou2024transformers}, looping~\cite{fan2024looped}, novel attention mechanisms~\citep{duan2023interpolation}, and input format augmentation~\citep{zhou2023algorithms,zhou2024transformers}. Beyond arithmetic, ~\citet{yehudai2021local} studies length generalization in graph tasks. In contrast, our approach adheres to the standard transformer architecture without introducing significant modifications to architecture, positional encoding, or input structure. While prior approaches typically rely on fixed-length training datasets without further updates to model weights, we iteratively update model weights on self-generated datasets, enabling the model to progressively improve and extend its generalization capabilities. 

More generally, easy-to-hard-generalization is the paradigm where human annotation is provided for easier tasks, but aiming to enable generalization to harder tasks with no additional supervision~\citep{schwarzschild2021can,bansal2022end,burns2023weak,hase2024unreasonable,sun2024easy}. 
For instance, ~\citet{zhang2024transcendence} study this \textit{transcendence} phenomenon in chess, showing that chess transformers can sometimes outperform all players in the training dataset. Similarly, ~\citet{sun2024easy} finds that a reward model trained on easier mathematical problems can be effectively transferred to harder problems, facilitating generalization through reinforcement learning. ~\citet{shin2024weak} identifies overlap data points—instances containing both easy and hard patterns—as a key mechanism for weak-to-strong generalization, allowing weak models to pseudolabel easier patterns while stronger models use these labels to learn harder patterns. Our work shows that a similar mechanism emerges naturally within self-improvement, where progressively increasing difficulty enables models to generate useful supervision signals for harder tasks without explicit human intervention.














\paragraph{Self-Improvement.}

When high-quality training labels are unavailable or costly to obtain, training on self-generated labels provides an efficient way to broaden the capabilities of a model. Typically, this involves generating candidate labels, filtering or verifying them to prune errors, and retraining on the refined self-generated data~\citep{zelikman2022star,wang2022selfinstruct,huang2022large,singh2023beyond,chen2023teaching,gulcehre2023reinforced,madaan2024selfrefine,yuan2024self,liang2024sheep,pang2024iterativereasoningpreferenceoptimization}. This approach has been successfully applied across various domains, including reasoning~\citep{zelikman2022star,huang2022large,singh2023beyond,pang2024iterativereasoningpreferenceoptimization}, mathematics~\citep{zhang2023chain,charton2024patternboost,alfarano2024global,liang2024sheep}, coding~\citep{chen2023teaching}, and general instruction tuning~\citep{wang2022selfinstruct,yuan2024self}.
Recent studies further analyze and refine the self-improvement process. \citet{song2024mind} identify the generation-verification gap as a key limiting factor, while \citet{huang2024selfimprovementlanguagemodelssharpening} introduce a "sharpening mechanism" that improves reliability by training on best-of-N model outputs.
Our work builds on STaR~\citep{zelikman2022star} and ReST~\citep{gulcehre2023reinforced}, leveraging iterative prediction, filtering, and fine-tuning to enhance model capabilities.















\paragraph{Model Collapse.}
A key challenge in self-improvement is model collapse, where iterative training on self-generated outputs leads to performance degradation~\citep{,Hataya_2023_ICCV,Arcaute2023CombiningGA,Alemohammad2023SelfConsumingGM}. While some work suggests this degradation is inevitable~\citep{shumailov2024ai,shumailov2023curse,zhang2023chain}, several mitigation strategies have emerged, including maintaining original training data~\citep{gerstgrasser2024model}, careful data mixing~\citep{gerstgrasser2024model,Dohmatob2024ATO,Briesch2023LargeLM}, and reliable verification mechanisms~\citet{gillman2024self,feng2024beyond}. Our approach incorporates these insights through unsupervised filtering techniques and controlled data generation, effectively preventing collapse while enabling sustained improvement.
\textit{We provide additional discussion of related works in Appendix~\ref{sec:related_work_extended}. 
}%