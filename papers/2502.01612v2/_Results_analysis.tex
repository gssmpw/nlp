

\section{Analysis on Errors}\label{sec:error_analysis}
\vspace{0.5em}
\begin{finding}
During self-improvement failure cases, we see an \textbf{error avalanche} phenomenon where errors build up, eventually causing performance to crash. By simulating model errors at a particular round, we find that the model tolerates errors up to a certain point before crashing in accuracy.
\end{finding}




\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{fig/etc/error_avalanche.pdf}
    \hspace{1mm}
    \includegraphics[width=0.45\linewidth]{fig/etc/error_avalanche_seed-44.pdf}
    \caption{\textit{Error avalanche} is a common failure case for self-improvement. As inaccuracies in self-generated data accumulate, they degrade future rounds of training, leading to eventual failure. (Left) The impact of inaccuracies in $n$-digit data on $n+1$-digit generalization. (Right) Gradual performance degradation over successive self-improvement rounds}
    \label{fig:error_avalanche}
\end{figure}


\subsection{Error Avalanches in Self-Improvement}

Out-of-distribution (OOD) generalization is highly sensitive to inaccuracies in self-generated data. Figure~\ref{fig:error_avalanche} highlights a key challenge in this setting: errors in $n$-digit training data propagate to $n+1$-digit examples, degrading performance in later rounds. This is evident from data points falling below the $y = x$ line, indicating that self-improvement data is becoming progressively less reliable.

This cascading effect, known as an \textit{error avalanche}, compounds over successive self-improvement rounds, leading to a gradual collapse of the training process. As inaccuracies accumulate, the model's self-generated labels become increasingly erroneous, reducing the effectiveness of future training. Without effective data filtering or correction mechanisms, this process eventually causes the model to fail entirely.


\subsection{Simulating the Error Avalanche}
A natural question to ask at this point is,\textit{ how much error the model must accumulate to trigger an avalanche?} We investigate this question by first characterizing the model mistakes, and then injecting synthetic wrong examples in the self-improvement data.

\paragraph{Patterns in Model Mistakes.} 
We can categorize all mistakes into two bins. At each digit position, either the model drop the digit, or outputs a wrong digit. Since these two kinds of mistakes are entangled in practice, we use a string matching algorithm to compare the model output and predictions and obtain the best guess. In figure~\ref{fig:mistake_patterns}, we find that digit drops by the model are concentrated near the end of the sequence, and wrong digits are most often off by 1. 

\begin{figure}
    \centering
    \includegraphics[height=10.0625em]{fig/etc/error_distribution.pdf}
    \includegraphics[height=10.0625em]{fig/etc/error_distribution_2d.pdf}
    \includegraphics[height=10.0625em]{fig/etc/dropped_character_distribution.pdf}
    \caption{Patterns in model errors. (Left) Most incorrect digits are off by 1. (Middle) Errors cluster near the end of the sequence. (Right) Digit drop errors are strongly location-dependent.}
    \label{fig:mistake_patterns}
\end{figure}

\paragraph{Injecting Synthetic Errors.} 
Having characterized the model mistakes, we simulate them by constructing four kinds of noises:

\begin{itemize}
    \item Uniform: Replaces the label with a random number of the same length. 
    \item Perturb: Randomly modifies the last three digits by $\pm 1$.  
    \item Drop-Digits: Randomly removes 1, 2, or 3 digits from the last three positions. 
    \item Drop-Perturb: Combines "perturb" and "drop-digits" by first modifying digits and then randomly deleting some.
\end{itemize}


We inject these errors of varying noise levels in rounds 5 and 20 of the reverse addition task and track their effects after five subsequent self-improvement rounds. As shown in Figure~\ref{fig:simulated_error}, injecting sufficient noise into the training data causes performance on the next difficulty to crash. In particular, we find that 1) structured noises (digit drops and perturbations) are more harmful than uniform noise and 2) more rounds of self-improvement improve robustness against label noise. Additional results on uniform errors are provided in Appendix~\ref{sec:appdx_label_noise}.


\begin{figure}
    \centering
    \includegraphics[width=0.49\linewidth]{fig/etc/low_dim_noises_round_5.pdf}
    \includegraphics[width=0.49\linewidth]{fig/etc/low_dim_noises_round_20.pdf}
    \caption{Simulating error avalanche. Synthetic mistakes of varying noise levels are injected at the end of rounds 5 and 20. The self-improvement process continues for 5 more rounds, and the resulting accuracy is recorded. The model tolerates errors up to a certain threshold, with greater tolerance observed in later self-improvement rounds.}
    \label{fig:simulated_error}
\end{figure}


\paragraph{Models can Generalize Despite Memorizing Past Mistakes.}

\begin{wrapfigure}{r}{0.43\textwidth}
    \vspace{-3mm}
    \centering
    \includegraphics[width=0.42\textwidth]{fig/etc/rand_prob_error_correction_multiple_rounds.pdf}
    \caption{Models memorize their mistakes. Accuracy on incorrect training examples decreases with additional self-improvement rounds, indicating that repeated exposure reinforces errors instead of correcting them.}
    \vspace{-5mm}
    \label{fig:memorizing_mistakes}
\end{wrapfigure}

Since self-improvement involves recycling model predictions into training data, an important question is whether the model continues making mistakes on previously incorrect examples. To investigate this, we isolate incorrect self-generated samples and evaluate the modelâ€™s performance on them. 
As shown in Figure~\ref{fig:memorizing_mistakes}, the model struggles to rectify these errors. Accuracy on incorrect examples from the 19-digit self-generated training data decreases over additional self-improvement rounds, suggesting that repeated exposure to errors reinforces them rather than correcting them. %

However, memorizing past mistakes does not necessarily cause an error avalanche. The model under self-improvement often generalize to higher difficulties while treating the incorrect samples as outliers. For example, Figure~\ref{fig:simulated_error} shows that after 20 rounds of self-improvement, the model can tolerate a surprisingly large amount of label noise, from both uniform noise and structured noise. This suggests that while individual mistakes persist, they do not necessarily hinder overall generalization.


These findings emphasize the critical need for maintaining high-quality self-generated data to sustain effective and persistent self-improvement.


\paragraph{Relevance to Prior Work. }
Our results align with findings from~\citet{rolnick2017deep}, which demonstrate that deep neural networks are robust to significant label noise in image classification tasks. Additionally,~\citet{Bayat2024ThePO} recently emphasized that memorization alone does not harm generalization; rather, the combination of memorization with spurious correlations is what undermines learning. Our results suggest that despite memorizing past mistakes, the self-improvement framework remains effective, provided that incorrect samples do not dominate the training distribution.


























\subsection{Other analysis}\label{sec:si_data_size}

\paragraph{Effect of Self-Improvement Dataset Size. }
We investigate how the quantity of self-generated training data impacts model performance. We first train 10 base models \( M_0^{(s)} \) (\( s=1, \dots, 10 \)) on a supervised 1-10 digit reverse addition dataset \( \mathcal{D}_0^s \), each using a different random seed. These models are categorized based on their accuracy on 11-digit addition: low-performing models (less than 98\% accuracy) are represented with yellow colors, while high-performing models (more than 98\% accuracy) are depicted with blue colors. 

To study the effect of dataset size, we generate self-improvement datasets \( \mathcal{D}_1^s = \{(x_i, M_0^{(s)}(x_i))\}_{i=1}^{N_1} \) of varying sizes (\( N_1 = 10,000, 50,000, 100,000, 500,000, 1,000,000 \)). Each model is then trained on the combined dataset \( \mathcal{D}_0^s \cup \mathcal{D}_1^s \). The number of incorrect examples in each self-generated dataset is approximately \( N_1 \times (1 - (\text{11-digit accuracy of } M_0)) \).

Results in Figure~\ref{fig:si_performance_by_num_samples} show that for low-performing models, increasing the quantity of self-generated data (which is of lower quality) degrades performance on both in-distribution (11-digit) and out-of-distribution (12-digit) addition. In contrast, for high-performing models, the relationship between the number of self-generated examples and performance is less clear. The total number of 11-digit examples seen during training remains constant across experiments, with smaller datasets being repeated more often. This suggests that exposure to a greater diversity of incorrect examples can bias the model more negatively.



\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{fig/etc/num_wrong_sample_11-digit.pdf}
    \hspace{1mm}
    \includegraphics[width=0.4\linewidth]{fig/etc/num_wrong_sample_12-digit.pdf}

    \caption{Effect of self-generated training data quantity and quality on model performance. Each model is trained on \( \mathcal{D}_0 \) (1-10 digit addition) and self-generated \( \mathcal{D}_1 \) (11-digit addition), then evaluated on 11-digit (in-distribution) and 12-digit (out-of-distribution) test performance. For low-performing models, increasing the quantity of self-generated data leads to degraded performance. For high-performing models, the impact of dataset size is less clear.}
    \label{fig:si_performance_by_num_samples}
\end{figure}







