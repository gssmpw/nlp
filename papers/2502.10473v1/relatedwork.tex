\section{Related Work}
\label{sec:related_work}
% \subsection*{Offline Reinforcement Learning.}
\textbf{Offline Reinforcement Learning:}
In \emph{model-free} offline RL methods, the agent learns a policy or value function directly from the dataset. To address the distributional shift, these algorithms typically penalize the values of out-of-distribution state-action or constrain the policy closed to the behavior policy~\cite{r:17,r:21,r:22,r:82,r:67}. Additionally, uncertainty quantification techniques are used to stabilize Q-functions~\cite{r:17,r:82,r:80,r:81}. On the other side, in \emph{model-based RL} methods, the offline data is used to train predictive models of the environment that can then be used for planning or policy learning~\cite{r:92,r:24}. 
The benefits of discovering diverse solutions have been demonstrated in literature pertaining to both online and offline RL~\cite{r:140,r:141,r:142}.
However, most these methods tend to require the policy to be pessimistic, while our algorithm does not involve this constraint. Besides, our algorithm is built upon the sequence-generation framework, which is different from the above methods.
% Yu et al.~[\citeyear{r:25}] proposed optimizing a policy using an uncertainty-penalized reward. This concept can be viewed as a degenerate form of single-step portfolio optimization when similarities between different trajectories are not considered.
% \de{The benefits of discovering diverse solutions have been
% demonstrated in literature pertaining to both online and offline RL, }

% \subsection*{Sequence Modeling.}
\textbf{Transformers as Trajectory Models in Offline RL:}
% Some recent papers use Transformers in RL by viewing RL as a sequence-modeling problem.
Decision Transformer (\dt)~\cite{r:4} predicts actions by feeding target rewards and previous states, rewards, and actions. 
% Decision Transformer (\dt)~\cite{r:4} learns the distribution of trajectories, and only predicts actions by feeding target rewards and previous states, rewards, and actions. 
Trajectory Transformer (\trt)~\cite{r:3} further brings out the capability of the sequence model by repurposing beam search as a planner. 
% Newer variants of \dt{} and \trt{} have demonstrated enhanced results in offline RL by advancing sequence-modeling paradigms. 
The Bootstrapped Trajectory Transformer (BooT)~\cite{r:134} improves on~\trt{} by self-generating additional offline data from the model to enhance its training process. The Elastic Decision Transformer~(EDT)~\cite{r:133} augments~\dt{} with a maximum in-support return training objective, enabling it to dynamically adjust history length to better combine sub-optimal trajectories. The Q-learning Decision Transformer~(QDT)~\cite{r:135} uses dynamic programming to relabel the return-to-go in the training data and trains the~\dt{} with the relabeled data. 
% IN-CONTEXT EXPLORATION-EXPLOITATION FOR REINFORCEMENT LEARNING optimize the efficiency of in-context policy learning.
However, these studies primarily rely on advanced and often complex training procedures. 
In contrast, our approach focuses exclusively on modifying the inference algorithm, leaving the training process unchanged. This distinction is significant because our method is complementary and orthogonal to training-based approaches, and can potentially be combined with them to achieve additional improvements.

% In contrast, our approach is focuses exclusively on modifying the inference algorithm, leaving the training process unchanged. This distinction is significant as it simplifies training and reduces computational overhead during training. Moreover, these approaches are complementary and can potentially be combined with our inference method to achieve additional improvements.




% \subsection*{Transformer Decoding.}
\textbf{Transformer Decoding Methods:}
In recent years, the rise of large Transformer-based language models has driven the development of better decoding methods. Many of these methods are sampling-based variants of \bs~\cite{r:15,r:16}. 
A notable example of diverse decoding is Diverse Beam Search~\cite{r:136}, an alternative to \bs{} that decodes a list of diverse outputs by optimizing a diversity-augmented objective. However, these methods are primarily ad-hoc and designed to produce fluent text in large Transformer-based language models trained in supervised-learning settings, where the goal is to find the most likely output sequence from the model. In offline RL, the batch also contains suboptimal behaviors, so the objective must incorporate reward signals.
\begin{table*}[h] 
    \centering
    \begin{tabular}{lrrrrrrrr}
    % \begin{tabular}{lcccccccc}
        \toprule
        Dataset                     & DT                & QDT               & EDT               & BooT              & TT                  &\wsts (ours)               \\
                                    % &                 &                   &                   &                   & Reported                  & Reproduced          &     \\
        \midrule
        HalfCheetah-medium-expert   & $86.8 \pm 1.3$    & $N/A$             & $N/A$             & $94.0 \pm 1.0$    & $\mathbf{95.0} \pm 0.8$      & $92.4 \pm 9.5$   \\ % ra = 0.01, gamma=0.01
        HalfCheetah-medium          & $42.6 \pm 0.1$    & $42.3 \pm 0.4$    & $42.5 \pm 0.9$    & $\mathbf{50.6} \pm 0.8$    & $46.9 \pm 1.6$      & $46.0 \pm 0.2$      \\ %(30)
        HalfCheetah-medium-replay   & $36.6 \pm 0.8$    & $35.6 \pm 0.5$    & $37.8 \pm 1.5$    & $\mathbf{46.5} \pm 1.2$    & $41.9 \pm 9.7 $     & $44.5 \pm 1.3$      \\ %(30) 
        
        Hopper-medium-expert        & $107.6 \pm 1.8$   & $N/A$             & $N/A$              & $102.3 \pm 19.4$  & $110.0 \pm 10.5$    & $\mathbf{112.5} \pm 0.7$     \\ %(30) 
        Hopper-medium               & $67.6  \pm 1.0$   & $66.5 \pm 6.3$    & $63.5 \pm 5.8$    & $70.2  \pm 18.1$  & $61.1  \pm 13.9$    & $\mathbf{74.5}  \pm 6.0$     \\ %(30) 
        Hopper-medium-replay        & $82.7  \pm 7.0$   & $52.1 \pm 20.3$   & $89.0 \pm  8.3$   & $92.9  \pm 13.2$  & $91.5  \pm 13.9$    & $\mathbf{95.6}  \pm 9.7$     \\ %(30) 
        
        Walker2d-medium-expert      & $108.1 \pm 0.2$   & $N/A$             & $N/A$              & $\mathbf{110.4} \pm 2.0$   & $101.9 \pm 26.3$    & $108.9 \pm 0.8$     \\ %(30) 
        Walker2d-medium             & $74.0 \pm 1.4$    & $67.1 \pm 3.2$    & $72.8 \pm 6.2$    & $\mathbf{82.9}  \pm 11.7$  & $79.0  \pm 10.8$    & $79.8  \pm 5.4$     \\ %(36) 
        Walker2d-medium-replay      & $66.6 \pm 3.0$    & $58.2 \pm 5.1$    & $74.8 \pm 4.9$    & $87.6  \pm 9.9$   & $82.6  \pm 26.7$    & $\mathbf{87.6}  \pm 5.7$     \\ %(35)
        

        \midrule
        Average Return              & $74.7 (61.7)$     & $N/A (53.6)$      & $N/A (63.4)$        & $81.9 (71.8)$       & $78.9 (67.2) $        & $\mathbf{82.4} (71.3)$          \\
        Average Standard Deviation  & $1.84 (2.22)$     & $N/A (5.97)$      & $N/A (4.6)$         & $8.59 (9.15)$       & $12.7 (12.8)$         & $\mathbf{4.4} (4.7)  $            \\

        \bottomrule
    \end{tabular}
    \caption{The table presents baseline comparisons on D4RL tasks. Our results are evaluated based on 30 random evaluations. The results for other baselines are adopted from their reported scores. For \trt{}, the paper reported the standard error; however, as we are more interested in the standard deviation, we converted their results accordingly. 
    Since QDT and EDT do not provide results for the medium expert dataset, we have excluded these datasets and listed the results of the other algorithms in parentheses for comparison. All tests were conducted on NVIDIA GeForce RTX 3090 GPUs.}
    \label{tab:results}
\end{table*}