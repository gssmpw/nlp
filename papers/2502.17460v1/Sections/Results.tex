\section{Experimental Results}\label{sec:results}
This section presents a comprehensive evaluation of our approach on two widely used public datasets, \textbf{MIMIC-III} and \textbf{VitalDB}, accessed through the pre-processed \textbf{PulseDB} resource~\cite{pulsedb}. After describing these datasets and their integration in PulseDB, we introduce the evaluation metrics used for both predictive and clinical quality. We then discuss the fine-tuning experiments, focusing on how frozen/unfrozen backbones and extended epochs influence performance. Finally, we assess the impact of quantization, highlighting both accuracy and practical gains.

\begin{table}[b]
\vspace{-0.5cm}
    \footnotesize
    \centering
    \caption{BHS Grading System~\cite{BHS}}\label{tab:req}
    \begin{tabular}{l c c c} 
        \toprule
        Grades & $\leq 5 \, \text{mmHg}$ & $\leq 10 \, \text{mmHg}$ & $\leq 15 \, \text{mmHg}$ \\
        \midrule
        Grade A & 60\% & 85\% & 95\% \\
        Grade B & 50\% & 75\% & 90\% \\
        Grade C & 40\% & 65\% & 85\% \\ 
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Datasets and Preprocessing}\label{sec:datasets}
\textbf{PulseDB} aggregates and cleans segments derived from both MIMIC-III and VitalDB. It provides 10-second windows of synchronized \gls{ecg}, \gls{ppg}, and \gls{abp} signals, downsampled to 125\,Hz. A total of 4,941 records are extracted from MIMIC-III and 3,458 from VitalDB~\cite{pulsedb}. By leveraging PulseDB, we work with consistent data splits and standardized preprocessing.

\begin{itemize}
    \item \textbf{MIMIC-III}~\cite{mimiciii}, collected from around 30,000 intensive care unit (ICU) patients MIMIC-III includes time-series of \gls{ecg}, \gls{ppg}, and \gls{abp} measurements at 125\,Hz. PulseDB extracts high-quality 10-second windows to form our training and evaluation splits.
    \item \textbf{VitalDB}~\cite{vitaldb} contains synchronized recordings from 6,153 subjects, originally sampled at 500\,Hz. We down-sampled these signals to 125\,Hz to match MIMIC-III, ensuring uniform sampling rates. 
\end{itemize}

Unless otherwise noted, each dataset (MIMIC-III portion and VitalDB portion within PulseDB) is randomly divided into 80\% training, 10\% validation, and 10\% testing. This setup allows us to compare performance across two distinct clinical data sources under consistent preprocessing.

\subsection{Evaluation Metrics and Clinical Standards}\label{subsec:metrics}

We employ three primary statistical metrics to assess the predictive accuracy of our BP estimation models, along with two clinical standards that gauge practical viability. Let $\text{Error} = BP_{\text{target}} - BP_{\text{estimated}}$ be the difference between the ground-truth BP and the model prediction. We report the MAE, which highlights how significant the prediction errors are on average. SD, which captures the spread or consistency of the errors. And the coefficient of determination ($R^2$) measures how well the model fits the observed data. These metrics reveal both the average error magnitude and the overall fit.

\begin{itemize}
    \item \textit{\gls{bhs}:} Graded A--D based on the percentage of predictions falling within $5$, $10$, and $15$\,mmHg of ground truth~\cite{BHS}. Grade~A requires at least 60\% of errors $\le5$\,mmHg, 85\% $\le10$\,mmHg, and 95\% $\le15$\,mmHg.
    \item \textit{\gls{aami}:} Considers a model valid if its mean bias is within $\pm5$\,mmHg and the SD of errors is below 8\,mmHg~\cite{AAMI}.
\end{itemize}

\begin{table*}[ht!]
\centering
\caption{Combined Results: Fine-Tuning (FT) vs.\ Training-from-Scratch (TFS), 100 vs.\ 200 Epochs, Frozen vs.\ Unfrozen, Across MIMIC-III and VitalDB. \gls{bhs} and \gls{aami} Columns Indicate Clinical Standard Compliance.}
\label{tab:merged_100_200}
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccccccccl}
\toprule
\textbf{Dataset} & \textbf{Method} & \textbf{Frozen?} & \textbf{Epochs} & \textbf{Size} & \textbf{DBP SD} & \textbf{DBP MAE} & 
\textbf{DBP R$^2$} & \textbf{SBP SD} & \textbf{SBP MAE} & \textbf{SBP R$^2$} & \textbf{BHS} & \textbf{AAMI}
\\
\midrule
MIMIC & FT & False & 100 & Small &
4.12 & 2.58 & 0.90 & 5.97 & 4.13 & 0.93 & A & Pass \\
MIMIC & FT & False & 100 & Medium &
3.57 & 2.05 & 0.93 & 5.19 & 3.42 & 0.95 & A & Pass \\
MIMIC & FT & False & 100 & Large &
3.18 & 1.72 & 0.94 & 4.55 & 2.93 & 0.96 & A & Pass \\
\cmidrule(lr){2-13}
MIMIC & FT & False & 200 & Small &
3.63 & 2.19 & 0.93 & 5.26 & 3.58 & 0.95 & A & Pass \\
MIMIC & FT & False & 200 & Medium &
3.29 & 1.80 & 0.94 & 4.78 & 3.05 & 0.96 & A & Pass \\
MIMIC & FT & False & 200 & Large &
\textbf{3.04} & \textbf{1.57} & \textbf{0.95} & \textbf{4.35} & \textbf{2.72} & \textbf{0.96} & A & Pass \\
\cmidrule(lr){2-13}
MIMIC & TFS & False & 100 & Small &
4.55 & 2.89 & 0.88 & 6.69 & 4.67 & 0.91 & A & Pass \\
MIMIC & TFS & False & 100 & Medium &
3.96 & 2.32 & 0.91 & 5.87 & 3.91 & 0.93 & A & Pass \\
MIMIC & TFS & False & 100 & Large &
3.83 & 2.23 & 0.92 & 5.66 & 3.71 & 0.94 & A & Pass \\
\cmidrule(lr){2-13}
MIMIC & FT & True & 100 & Small &
13.13 & 10.11 & 0.03 & 22.29 & 17.92 & 0.04 & D & Fail \\
\midrule
VitalDB & FT & False & 100 & Small &
6.63 & 4.77 & 0.70 & 9.70 & 6.99 & 0.74 & C & Pass/Fail \\
VitalDB & FT & False & 100 & Medium &
5.49 & 3.67 & 0.79 & 8.19 & 5.56 & 0.81 & A & Pass/Fail \\
VitalDB & FT & False & 100 & Large &
3.90 & 2.39 & 0.90 & 5.85 & 3.77 & 0.91 & A & Pass \\
\cmidrule(lr){2-13}
VitalDB & FT & False & 200 & Large &
\textbf{3.42} & \textbf{1.92} & \textbf{0.92} & \textbf{5.12} & \textbf{3.14} & \textbf{0.93} & A & Pass \\
\cmidrule(lr){2-13}
VitalDB & TFS & False & 100 & Medium &
6.14 & 4.18 & 0.74 & 9.25 & 6.39 & 0.76 & A & Pass/Fail \\
VitalDB & TFS & False & 100 & Large &
3.90 & 2.39 & 0.90 & 5.85 & 3.77 & 0.91 & A & Pass \\
\cmidrule(lr){2-13}
VitalDB & FT & True & 100 & Large &
11.45 & 9.26 & 0.08 & 18.20 & 14.78 & 0.06 & D & Fail \\
\bottomrule
\end{tabular}
}
\vspace{-0.5cm}
\end{table*}

\subsection{Fine-tuning Experiments}\label{subsec:finetune_results}

We evaluated our fine-tuning approach under varying settings (1):(1) \textit{Frozen vs.\ Unfrozen} backbones, (2) \textit{Small/Medium/Large} model sizes, and (3) \textit{100 vs.\ 200} training epochs. For comparison, we also trained all model sizes from scratch to quantify the benefit of leveraging a pre-trained \gls{eeg} foundation. 

Table~\ref{tab:merged_100_200} \emph{consolidates} all key results for experiments utilizing 100 and 200 epochs. As shown, across all three model sizes (S, M, L), using an \emph{unfrozen} backbone consistently yields better MAE and SD values than freezing it. On MIMIC-III, the large (L) model achieves the lowest MAE (1.72~mmHg for \gls{dbp} and 2.93~mmHg for \gls{sbp}) and highest $R^2$ (0.94 for \gls{dbp}, 0.96 for \gls{sbp}) at 100 epochs. On VitalDB, fine-tuning likewise improves accuracy, with the large model reaching an MAE of 2.39~mmHg for \gls{dbp} and 3.77~mmHg for \gls{sbp} at 100 epochs. Notably, these models often satisfy \gls{aami} standards and achieve \gls{bhs} Grade~A or B.

By contrast, models trained from scratch require more epochs to converge and exhibit higher MAE and SD. For instance, on MIMIC-III, the large model trained from scratch attains an MAE of 2.23~mmHg (\gls{dbp}) and 3.71~mmHg (\gls{sbp}), compared to 1.72~mmHg and 2.93~mmHg for the fine-tuned version—demonstrating how leveraging a pre-trained backbone reduces error and speeds convergence.


To examine whether additional training refines the model further, we extended the unfrozen fine-tuning runs to 200 epochs. The medium and large models on MIMIC-III increase $R^2$ above 0.94 for \gls{dbp} and 0.96 for \gls{sbp}, corroborating the advantages of longer training. On VitalDB, the large model’s MAE drops to 1.92~mmHg (\gls{dbp}) and 3.14~mmHg (\gls{sbp}), also improving $R^2$ to 0.92 and 0.93, respectively. Depending on clinical requirements and computational budget, these incremental gains may be justified—particularly in offline training where slight improvements can enhance overall reliability.

Table~\ref{tab:merged_100_200} displays the \gls{bhs} grades and \gls{aami} standard compliance for all 100-epoch runs. Fine-tuning typically achieves \gls{bhs} Grade~A or B for both \gls{dbp} and \gls{sbp}, and meets the \gls{aami} criteria of $\pm5$~mmHg mean error and $<8$~mmHg SD. Notably, the large CEReBrO model meets Grade~A for VitalDB in both \gls{dbp} and \gls{sbp} under unfrozen fine-tuning, indicating near-clinical reliability.

Overall, these results demonstrate that \emph{unfrozen fine-tuning of a pre-trained \gls{eeg} foundation model} yields the best trade-off among accuracy, robustness, and training efficiency across two significant datasets. More importantly, these findings confirm that a pre-trained \gls{eeg} transformer can robustly adapt to \gls{ecg}/\gls{ppg}-based BP estimation, reducing errors while meeting rigorous clinical standards.



\subsection{Quantization Performance}\label{subsec:quant_restults}
We evaluate static vs.\ dynamic INT8 quantization (Section~\ref{subsec:quantization}) on our best-fine-tuned models (unfrozen, 200 epochs), examining the size-accuracy trade-off.

As shown in Table~\ref{tab:quantization_r2}, \emph{dynamic per-channel quantization with symmetric weights} yields the highest $R^2$ for both \gls{dbp} and \gls{sbp} across MIMIC-III and VitalDB, closely matching the unquantized baseline. In particular, the large model’s \gls{dbp} $R^2$ only drops from 0.9479 to 0.9476 on MIMIC-III, a negligible difference in performance.

Although static quantization yields similar compression ratios, Table~\ref{tab:model_size_comparison} reveals a negligible difference in final model size compared to dynamic quantization while exhibiting a slightly lower $R^2$ in practice, which is shown in Table~\ref{tab:quantization_r2}. Additionally, static quantization requires a separate calibration step that can complicate deployment.

Table~\ref{tab:model_size_comparison} highlights a reduction factor of approximately 3.5--3.9$\times$ across all model sizes. The smallest model shrinks to 3.85~MB, enabling truly resource-limited scenarios. Meanwhile, the large model is compressed to around 83~MB yet preserves top-tier predictive performance while also being suitable for embedded deployment on devices with suitable flash sizes. The quantized models also meet \gls{aami} standards and achieve \gls{bhs} Grade~A on both datasets, indicating minimal performance degradation compared to the unquantized baseline.


\begin{table}[b]
\centering
\vspace{-0.5cm}
\caption{Dynamic and Static Quantization test $R^2$ versus original test $R^2$ for DBP and SBP on the 200 epoch models}
\label{tab:quantization_r2}
\renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{2pt} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{llcccccc}
\toprule
\textbf{Dataset} & \textbf{Size} & \multicolumn{2}{c}{\textbf{Original}} & \multicolumn{2}{c}{\textbf{Dynamic Quantized}} & \multicolumn{2}{c}{\textbf{Static Quantized}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& & \textbf{DBP R$^2$} & \textbf{SBP R$^2$} & \textbf{DBP R$^2$} & \textbf{SBP R$^2$} & \textbf{DBP R$^2$} & \textbf{SBP R$^2$} \\
\midrule
\multirow{3}{*}{MIMIC} & Small & 0.9256 & 0.9467 & 0.9253 & 0.9465 & 0.9247 & 0.9460 \\
& Medium & 0.9389 & 0.9559 & 0.9386 & 0.9556 & 0.9374 & 0.9546 \\
& Large & 0.9479 & 0.9635 & 0.9476 & 0.9632 & 0.9466 & 0.9628 \\
\midrule
\multirow{3}{*}{VitalDB} & Small & 0.8034 & 0.8304 & 0.8019 & 0.8295 & 0.7994 & 0.8279 \\
& Medium & 0.8534 & 0.8659 & 0.8522 & 0.8652 & 0.8503 & 0.8634 \\
& Large & 0.9203 & 0.9280 & 0.9198 & 0.9276 & 0.9184 & 0.9265 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht!]
\centering
\caption{Comparison of model sizes for the original, dynamically quantized, and statically quantized models across different model sizes (measured in MB). RF stands for reduction factor.}
\label{tab:model_size_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Size} & \textbf{Original (MB)} & \textbf{Dynamic (MB)} & \textbf{Static (MB)} & \textbf{RF} \\
\midrule
Small & 13.73 & 3.85 & 3.83 & 3.57 \\
Medium & 152.55 & 39.47 & 39.43 & 3.87 \\
Large & 324.96 & 83.28 & 83.22 & 3.90 \\
\bottomrule
\end{tabular}

\vspace{-0.5cm}
\end{table}

\subsection{Comparison to SOTA}
Table~\ref{tab:comparison_combined} compares our work to \gls{soa}. Our proposed method demonstrates a clear performance advantage over existing state-of-the-art approaches on both the VitalDB and MIMIC-III datasets. On the VitalDB dataset, our model achieves a DBP MAE of 1.92 mmHg and an SBP MAE of 3.14 mmHg. These results represent a reduction of approximately 35\% in DBP error and 32\% in SBP error compared to the ResUNet+Attention approach by Jamil et al.~\cite{Jamil}, which reported MAEs of 2.95 mmHg and 4.64 mmHg for DBP and SBP respectively. Furthermore, our model outperforms other recent methods such as CiGNN~\cite{liu2024cignn}, rU-Net~\cite{runet}, and Fusion+Attention~\cite{el2021deep}, where the latter reported even higher errors (3.76 mmHg for DBP and 5.32 mmHg for SBP).
Our method achieves a DBP MAE of 1.57 mmHg and an SBP MAE of 2.72 mmHg on the MIMIC-III dataset. Notably, while the ResUNet+Attention method by Jamil et al.~\cite{Jamil} produced a slightly lower DBP error of 1.13 mmHg, its SBP error was significantly higher at 4.55 mmHg, making our SBP performance nearly 40\% better. Moreover, when compared to the hybrid ResNet-LSTM approach by Paviglianiti et al.~\cite{Paviglianiti2021ACO}, which reported errors of 2.23 mmHg (DBP) and 4.12 mmHg (SBP), our method demonstrates improvements of approximately 29\% and 34\% for DBP and SBP respectively. The performance gains over additional methods such as GWO-GBRT~\cite{liu2024continuous} and Conv-LSTM~\cite{kamanditya2024continuous} further underscore the robustness of our approach. These quantitative improvements confirm that our EEG-based Foundation BioSignal design not only rivals but, in many respects, surpasses the current state-of-the-art while also offering enhanced scalability and feasibility for edge deployment. 

\begin{table}[b]
\vspace{-0.5cm}
\centering
\caption{Comparison of MAE across VitalDB and MIMIC-III datasets (all values in mmHg)}
\label{tab:comparison_combined}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llcc}
\toprule
\textbf{Dataset} & \textbf{Method} & \textbf{DBP MAE} & \textbf{SBP MAE} \\
\midrule
\multirow{4}{*}{VitalDB} & ResUNet+Attention~\cite{Jamil} & 2.95 & 4.64 \\
& CiGNN~\cite{liu2024cignn} & 2.79 & 4.15 \\
& Fusion+Attention~\cite{el2021deep} & 3.76 & 5.32 \\
& rU-Net~\cite{runet} & 2.69 & 4.49 \\
& \textbf{This Work} & \textbf{1.92} & \textbf{3.14} \\
\midrule
\multirow{4}{*}{MIMIC-III} & ResUNet+Attention~\cite{Jamil} & \textbf{1.13} & 4.55 \\
& GWO-GBRT~\cite{liu2024continuous} & 2.79 & 4.15 \\
& Conv-LSTM~\cite{kamanditya2024continuous} & 3.29 & 4.30 \\
& ResNet+LSTM~\cite{Paviglianiti2021ACO} & 2.23 & 4.12 \\
& \textbf{This Work} & 1.57 & \textbf{2.72} \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Discussion}
Our findings confirm the hypothesis that it is possible to leverage pre-trained \gls{eeg} representations to enable accurate, cuffless blood pressure monitoring from \gls{ecg} and \gls{ppg} signals. By fine-tuning a large transformer-based backbone originally trained on \gls{eeg}, we achieve low error rates that align with clinical benchmarks such as the \gls{bhs} grading system and \gls{aami} standards. Despite these encouraging results, several areas warrant deeper exploration and refinement.

First, data heterogeneity remains a significant concern. Although we validated our approach on MIMIC-III and VitalDB, these datasets exhibit variations in patient demographics, data quality, and clinical conditions that may not fully capture the breadth of real-world scenarios. Extending the model to broader populations or leveraging domain-adaptation techniques could bolster robustness across diverse settings. Moreover, while our \gls{eeg}-to-\gls{ecg}/\gls{ppg} transfer highlights that time-series features learned from brain activity can generalize to cardiovascular signals, a more granular examination of which latent representations transfer most effectively—and whether additional modalities might further enhance performance—could inform future multi-modal foundation models.

Second, our work demonstrates that quantization cuts model size and computational demands with minimal accuracy loss, thereby opening a viable path for embedded or wearable devices. However, real-time inference on resource-constrained hardware introduces additional factors such as on-device latency, battery constraints, and environmental variability. Evaluating our quantized model on ultra-low-power microcontrollers remains essential for future developments towards fully wearable deployments. Techniques such as structured pruning or model distillation may further optimize the trade-off between model capacity and energy consumption.

Finally, although we meet clinical standards under controlled conditions, truly continuous monitoring in daily life entails challenges like motion artifacts, intermittent signal dropouts, and inconsistent sensor placements. Advanced artifact minimization strategies~\cite{ingolfsson_minimizing_2024} and sensor fusion with inertial measurement units~\cite{ingolfsson2024brainfusenet} may mitigate these real-world confounders. In tandem with demographic or patient-specific calibration, such approaches could help ensure that cuffless BP estimation remains reliable over prolonged usage. These directions illuminate how cross-biosignal transfer, combined with robust compression techniques, can evolve into a clinically meaningful framework for unobtrusive, continuous blood pressure monitoring. 










