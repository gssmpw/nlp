\section{Introduction}
\Gls{bp} is one of the most critical indicators of cardiovascular health and overall physiological status~\cite{vital_signs}. Typically reported as \gls{sbp} and \gls{dbp} pressure, normal adult values range between 90–120~mmHg (\gls{sbp}) and 60–80~mmHg (\gls{dbp})~\cite{bp_range}. Elevated \gls{bp}, or hypertension, is a major risk factor for severe \glspl{cvd}, including stroke and heart failure~\cite{leading_cause,CVDs_num}. In 2017 alone, over 17 million deaths worldwide were linked to CVDs, with high \gls{sbp} implicated in more than half of these cases~\cite{HSBP_num}. These statistics highlight the urgent need for frequent or continuous \gls{bp} monitoring to enable early detection and timely intervention.

Most clinical and at-home \gls{bp} measurements currently rely on cuff-based devices, which are non-invasive but provide only intermittent readings. Their bulkiness and requirement for patient compliance also limit their utility for continuous tracking~\cite{bradley2022cuffless, pilz2024cuff}. Consequently, there is substantial interest in leveraging additional physiological signals—particularly \gls{ecg} and \gls{ppg}—to estimate \gls{bp} without a cuff. \gls{ecg} measures electrical activity of the heart, while \gls{ppg} tracks blood volume changes. Both exhibit strong correlations with arterial blood pressure~\cite{ECG_BOOK,ppg_book}, and recent deep learning methods have successfully modeled these complex relationships~\cite{Faust2018DeepLF,Jamil}.

Despite these advances, several challenges remain. Although large-scale labeled \gls{ecg}/\gls{ppg} datasets exist~\cite{ptb-xl,mimiciii,vitaldb} heterogeneity in data quality and physiological variability across patients can degrade model generalizability. Meanwhile, foundation models have emerged in biosignal research~\cite{CEReBrO,Mehta2023CantTT}, where large neural networks are pre-trained on vast amounts of (often unlabeled) data and then fine-tuned for specific tasks. Importantly, the rich temporal and spectral diversity of EEG signals enables transformer-based encoders to learn robust and generalizable representations that can effectively transferred to other biosignals~\cite{yang2023cross,joshi2021deep}.

In this context, a crucial question arises: \textbf{Can knowledge learned from \gls{eeg} waveforms transfer effectively to other biosignals, such as \gls{ecg} and \gls{ppg}, for tasks such as \gls{bp} estimation?} Although \gls{eeg} signals originate from neural activity, they share core time-series characteristics with \gls{ecg}/\gls{ppg}, potentially enabling cross-biosignal feature transfer~\cite{yang2023cross, ingolfsson2024brainfusenet}. To our knowledge, this work is the first to experimentally validate an \gls{eeg}-based foundation model for cuffless \gls{bp} prediction using knowledge transfer from \gls{eeg} to \gls{ecg} and \gls{ppg} without requiring additional large-scale biosignal-specific pre-trainings. This initial investigation lays the groundwork for future research, wherein a dedicated \gls{ecg}/\gls{ppg} foundation model could be developed by further fine-tuning the pre-trained \gls{eeg} model on extensive \gls{ecg} data.

After demonstrating the \gls{eeg}-to-\gls{ecg}/\gls{ppg} knowledge transfer, a second critical challenge to enable wearability is minimizing the computational requirements. Wearables have strong constraints on computational resources. In this context, quantization techniques are necessary to reduce memory footprint and inference latency~\cite{nagel2021whitepaperneuralnetwork}. Thanks to their ability to compress floating-point weights to lower-precision integer representations, quantization is a key ingredient to complement the development of foundation models for wearables.

In this paper, we demonstrate a cross-biosignal transfer learning approach where a transformer model, pre-trained only on \gls{eeg}, is fine-tuned for \gls{bp} estimation from \gls{ecg} and \gls{ppg} waveforms. In particular, no additional large-scale pre-trainings on \gls{ecg}/\gls{ppg} were required. We extensively evaluate this method on the MIMIC-III and VitalDB datasets, examining the trade-offs of various fine-tuning strategies, including frozen vs.\ unfrozen backbones. Furthermore, we incorporate dynamic INT8 quantization to reduce the model size by over $3.5\times$, enabling feasible deployment on edge devices. 

In summary, the contribution of this paper is as follows:

\begin{itemize}
    \item We show that a large transformer model pre-trained \textit{only} on \gls{eeg} can be successfully adapted to estimate \gls{bp} from \gls{ecg} and \gls{ppg} signals by relying solely on an additional fine-tuning, opening new avenues for cross-biosignal foundation models.

     \item We compare frozen vs.\ unfrozen transformer backbones and training from scratch vs.\ pre-trained weights, quantifying their impact on convergence speed, predictive accuracy, and computational cost.
     
    \item Our empirical results on MIMIC-III and VitalDB achieve near state-of-the-art accuracy for diastolic BP, with a mean absolute error of 1.57 mmHg, and surpass the accuracy of prior works for systolic BP, with a mean absolute error of 2.72 mmHg (1.5 $\times$ smaller than SoA).

    \item We apply dynamic INT8 quantization to reduce the model size by over $3.5\times$ (from 13.73 MB to 3.83 MB)  with negligible performance loss, enabling resource-efficient inference for real-time \gls{bp} monitoring.
\end{itemize}