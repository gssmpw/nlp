\section{Methodology}\label{sec:method}

This section describes our approach for estimating \gls{bp} from \gls{ecg} and \gls{ppg} waveforms using a large \gls{eeg}-based foundation model. We first detail how we adapt and fine-tune the CEReBrO architecture for \gls{bp} prediction, then describe our post-training quantization steps.

\subsection{Architecture and Fine-Tuning}\label{subsec:model}

\begin{figure}[htp]
    \centering
    \includegraphics[width=7.5cm]{images/architecture}
    \caption{The modified CEReBrO Architecture~\cite{CEReBrO}, supplemented with an additional MLP-head, which is utilized for the \gls{bp} estimation task.} 
    \vspace{-0.6cm}
    \label{fig:cerebro}
\end{figure}  

Our method builds on the \textbf{CEReBrO} transformer-encoder~\cite{CEReBrO}, originally pre-trained on a large \gls{eeg} dataset (TUEG~\cite{TUEG}). CEReBrO employs a tokenization scheme that splits time-series signals into non-overlapping patches and projects them into a latent space. Alternating self-attention blocks then process these tokens by focusing on intra-channel (temporal) correlations and inter-channel (spatial) relationships. This design efficiently captures both local and long-range dependencies in multi-channel biosignals. Although CEReBrO was trained on \gls{eeg} data, its attention-based encoder can generalize to other biosignals sharing similar temporal structures. To adapt CEReBrO for \gls{bp} estimation from \gls{ecg} and \gls{ppg}, we make the following modifications:

\begin{itemize}
    \item We feed \gls{ecg} and \gls{ppg} signals as two input channels, each sampled at 125,Hz and shaped into 10-second segments ($2 \times 1250$).
    \item  We replace the original classification head with a single fully connected layer (MLP) that outputs two values: \gls{sbp} and \gls{dbp}.
\end{itemize}

CEReBrO is then also available in three sizes—\emph{small} (3.58M parameters), \emph{medium} (39.95M parameters), and \emph{large} (85.15M parameters).

We then explore two fine-tuning strategies:
\begin{itemize}
    \item \textbf{Frozen Backbone}: All transformer layers except for the first input-embedding layer and the final MLP head are frozen. This preserves most \gls{eeg}-based representations while allowing partial adaptation to \gls{ecg}/\gls{ppg}.
    \item \textbf{Unfrozen Backbone}: All transformer layers are unfrozen to allow deeper domain alignment, albeit with a risk of forgetting learned \gls{eeg} features.
\end{itemize}
To measure the benefit of leveraging a pre-trained \gls{eeg} encoder, we compare fine-tuning against training from scratch (i.e., random initialization). In each setting, we train models of three different sizes (small, medium, large) for 100 epochs and extend unfrozen-backbone runs to 200 epochs to assess longer-term convergence. We use Xavier Initialization~\cite{xavier_init} when training from scratch. Performance is evaluated on the MIMIC-III and VitalDB datasets in terms of MAE, SD, and coefficient of determination ($R^2$), as well as clinical standards (\gls{bhs} and \gls{aami}).

\subsection{Quantization}\label{subsec:quantization}
We apply post-training quantization to our fine-tuned models to enable real-time deployment on resource-constrained devices. This step reduces the memory footprint and inference latency while preserving clinically relevant accuracy.

We use PyTorch’s FX Graph Mode Quantization pipeline~\cite{pytorch2} to insert quantization and dequantization operations systematically. Quantization is widely employed to map floating-point (32-bit) values to lower numerical precision, typically to 8-bit integers. The range of a floating-point value, denoted by \(x_{\mathrm{fp}}\), is defined as follows: \([x_{\min}, x_{\max}]\). Based on this, two characteristic values can be defined, which are essential for the quantization process: \textbf{Scale} \(\Delta\), which determines the step size (real-valued), while \textbf{Zero-point} \(z\), which is an integer offset whose primary function is to ensure that the zero is mapped onto an integer. In this work, we specifically adopt two types of quantization:

\begin{itemize}
    \item \emph{Symmetric Quantization} for weights (common when weight distributions are roughly zero-mean).
    \item \emph{Asymmetric Quantization} for activations (typical when ReLU shifts values positively).
\end{itemize}
For symmetric quantization, we have the following characteristic values:
\begin{equation*}
    \Delta = \frac{\max\bigl(|x_{\min}|, |x_{\max}|\bigr)}{2^{b-1}}, 
    \quad 
    z = 0
\end{equation*}

Based on these values, forward quantization is done using this equation:
\begin{equation*}
x_{\mathrm{int}} = \mathrm{clip}\!\Bigl(
        \mathrm{round}\bigl(\tfrac{x}{\Delta}\bigr), 
        \, -2^{b-1}, \, 2^{b-1} - 1\Bigr).
\end{equation*}

And for \emph{asymmetric} quantization the characteristic values can be calculated in the following way, where $b$ is the number of bits :
\begin{equation*}
    \Delta = \frac{x_{\max} - x_{\min}}{2^b - 1}, 
    \quad 
    z = \left\lfloor - \frac{x_{\min}}{\Delta} + 0.5 \right\rfloor.
\end{equation*}
When $\Delta$ and $z$ are determined, the forward quantization step is the following:
\begin{equation*}
    x_{\mathrm{int}} = \mathrm{clip}\!\Bigl(
        \mathrm{round}\bigl(\tfrac{x_{\mathrm{fp}}}{\Delta}\bigr) + z , 
        \, 0, \, 2^b - 1\Bigr),
\end{equation*}
where $\mathrm{clip}(\cdot,0,2^b-1)$ ensures $x_{\mathrm{int}}$ to stay in the range $[0, 2^b - 1]$~\cite{quant_data}.

Quantization typically involves three stages: (1) \emph{calibration}, where representative data is passed through the model to collect scaling statistics; (2) \emph{conversion}, transforming the floating-point model into a quantized version; and (3) \emph{execution}, running inference with reduced-precision operations.

We explore both \emph{static} quantization~\cite{FU20092937}, which precomputes scaling and zero points via a calibration dataset, and \emph{dynamic} quantization~\cite{vu2008stabilizing}, which calculates them on-the-fly, eliminating the calibration phase. While static quantization can offer speed benefits if the input distribution is stable, dynamic quantization is often more flexible for variable-length or varying data distributions and avoids the need for extra calibration data.

Our target precision is INT8, balancing memory savings and model fidelity. We evaluate symmetric quantization for weights and asymmetric for activations (shifted by ReLU). Different observers—\emph{MinMaxObserver}, \emph{MovingAverageMinMaxObserver}, and \emph{HistogramObserver}—estimate the range, each trading off complexity against robustness. We also employ per-channel quantization for \gls{ecg}/\gls{ppg} inputs, giving each signal channel a separate scale and zero point.

Our experiments reveal that dynamic per-channel quantization with symmetric weights yields an optimal model size, computational speed, and accuracy trade-off. Detailed results of these experiments are presented in Section~\ref{sec:results}. This approach is critical for enabling continuous, on-device \gls{bp} estimation, where both memory and energy constraints are strict.