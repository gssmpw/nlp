
\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\usepackage{amsfonts}
\usepackage{times}
\usepackage{stmaryrd}
\usepackage{hyperref}
\usepackage{mwe}
\usepackage{graphicx} 
\usepackage{wrapfig} 
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{array}
\renewenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\qed}


\usepackage[page]{appendix}
\usepackage{etoolbox}

\renewcommand{\appendixtocname}{Appendix Contents.}


\makeatletter
\renewcommand{\appendices}{%
  \clearpage
  \renewcommand{\thesection}{\Alph{section}}
  \renewcommand{\appendixname}{}
  \renewcommand{\appendixpagename}{}
  \let\tf@toc\tf@app
  \addtocontents{app}{\protect\setcounter{tocdepth}{2}}
  \immediate\write\@auxout{%
    \string\let\string\tf@toc\string\tf@app^^J
  }
}


\newcommand{\listofappendices}{%
  \begingroup
  \renewcommand{\contentsname}{\appendixtocname}
  \let\@oldstarttoc\@starttoc
  \def\@starttoc##1{\@oldstarttoc{app}}
  \tableofcontents
  \endgroup
}
\usepackage[utf8]{inputenc}
\newcommand{\BEAS}{\begin{eqnarray*}}
\newcommand{\EEAS}{\end{eqnarray*}}
\newcommand{\BEA}{\begin{eqnarray}}
\newcommand{\EEA}{\end{eqnarray}}
\newcommand{\BEQ}{\begin{equation}}
\newcommand{\EEQ}{\end{equation}}
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
\newcommand{\BA}{\begin{array}}
\newcommand{\EA}{\end{array}}
\newcommand{\diag}{\mathop{\textnormal{ diag}}}
\newcommand{\Diag}{\mathop{\textnormal{ Diag}}}
\newcommand{\rb}{\mathbb{{R}}}
\renewcommand{\labelitemiii}{$-$}
\def\defin{\stackrel{\vartriangle}{=}}
\def \tX{ \widetilde{X}}
\def \tY{ \widetilde{Y}}
\def \ds { \displaystyle}
\def \hS{ \widehat{ \Sigma} }
\def \S{  { \Sigma} }
\def \L{  { \Lambda} }
\def \E{{\mathbb E}}
\def \P{{\mathbb P}}
\def \Z{{\mathbb Z}}
\def \T{{\mathbb T}}
\def \F{{\mathcal F}}
\def \C{{\mathbb C}}
\def \M{{\mathcal M}}
\def \H{{\mathcal H}}
\def \U{{\mathcal U}}
\def \X{{\mathcal X}}
\def \Y{{\mathcal Y}}
\def \A{{\mathcal A}}
\def \S{{\mathcal S}}
\def \hQ{{\hat{Q}}}
 \def \hfl{ \hat{f}_\lambda }
 \def \hal{ \hat{\alpha}_\lambda }
  \def \fl{ {f}_\lambda }
\def \supp{ { \textnormal{ Supp }}}
\def \card{ { \rm Card }}
 \def \P{{\mathbb P}}
 



\title{\textbf{An Uncertainty Principle\\ for Linear Recurrent Neural Networks}}
\author{
  \textbf{Alexandre Fran\c{c}ois}\\
  INRIA, Ecole Normale Sup\'erieure, PSL Research University, France\\
  \texttt{alexandre.francois@inria.fr}
  \and
  \textbf{Antonio Orvieto}\\
  MPI for Intelligent Systems, ELLIS Institute T\"ubingen, Germany\\
  \texttt{antonio@tue.ellis.eu}
  \and
  \textbf{Francis Bach}\\
  INRIA, Ecole Normale Sup\'erieure, PSL Research University, France\\
  \texttt{francis.bach@inria.fr}
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
 We consider linear recurrent neural networks, which have become a key building block of sequence modeling due to their ability for stable and effective long-range modeling. In this paper, we aim at characterizing this ability on a simple but core copy task, whose goal is to build a linear filter of order $S$ that approximates the filter that looks $K$ time steps in the past (which we refer to as the shift-$K$ filter), where $K$ is larger than $S$. Using classical signal models and quadratic cost, we fully characterize the problem by providing lower bounds of approximation, as well as explicit filters that achieve this lower bound up to constants. The optimal performance highlights an uncertainty principle: the optimal filter has to average values around the $K$-th time step in the past with a range~(width) that is proportional to $K/S$.

\end{abstract}

\input{sections/introduction}
\input{sections/lower_bound}
\input{sections/upper_bound}
\input{sections/experiments}
\input{sections/discussion}

\section*{Acknowledgements}
The authors would like to thank Laurent Baratchart and Sylvain Chevillard for their helpful discussion on rational approximations of the complex exponential. We also thank Sajad Movahedi and Felix Sarnthein for their helpful comments on this manuscript.
This work has received support from the French government, managed by the National Research Agency, under the France 2030 program with the reference "PR[AI]RIE-PSAI" (ANR-23-IACL-0008). 
Antonio Orvieto is supported by the Hector Foundation.


\bibliography{main.bib}

\newpage
\setcounter{section}{0}

\begin{appendices}

In this Appendix, we provide a detailed proof for all our theoretical results. We start in Appendix~\ref{RNN basics} with an equivalence of various representations of linear RNNs, then in Appendix~\ref{review} with a review of fundamentals of signal processing.
\listofappendices

\counterwithin{figure}{section}
\counterwithin{table}{section}


\newpage

\input{sections/appendix_a_few_Recurrent_Neural_Networks_basics}
\input{sections/fundamental_of_signal_processing}
\input{sections/appendix_lower_bound}
\input{sections/appendix_upper_bound}
\input{sections/appendix_xps}

\end{appendices}

\end{document}