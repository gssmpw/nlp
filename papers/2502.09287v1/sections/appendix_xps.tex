\section{Experiments}


In this section, we present a series of experiments designed to validate our theoretical findings in a practical setting. Specifically, we assess whether our conclusions hold when transitioning from an idealized infinite-data framework to real-world scenarios with a limited number of samples. 

Let us first introduce the linear recurrent neural network (RNN) used in our study. It is defined by the following recurrence relations:
\begin{align*}
    h_0 &= 0, \\
    x_{t+1} &= Ax_t + Bu_{t+1}, \\
    y_t &= Cx_t,
\end{align*}
where $x_t \in \mathbb{R}^{d_{\text{hidden}}}$ represents the hidden state, $u_t \in \mathbb{R}$ is the input, and $y_t \in \mathbb{R}$ is the output. The network parameters consist of $A \in \mathbb{C}^{d_{\text{hidden}} \times d_{\text{hidden}}}$, $B \in \mathbb{C}^{d_{\text{hidden}} \times 1}$, and $C \in \mathbb{C}^{d_{\text{hidden}}}$. 

Without loss of generality, we adopt a diagonal representation for the matrix $A$. The choice of its initial eigenvalues depends on the specific experiment: we either use a random initialization or employ the structured initialization given by Eq.~\eqref{Param_of_the_as}. 

In the simple experiments conducted below, the objective is to learn a single filter. Consequently, there is no need to decompose the matrix $A$ into multiple diagonal blocks. The matrix $C$ is initialized as:
\[
C_{\text{init}} = \begin{pmatrix} 1 & \dots & 1 \end{pmatrix} \in \mathbb{R}^{d_{\text{hidden}} \times 1}.
\]
and the entries of $B$ are initialized given by Eq.~\eqref{Param_of_the_bs}.\newline

The synthetic dataset consists of autoregressive sequences $X = (u_1, u_2, \dots, u_N)$ of length $N$, generated as:
\begin{equation}
    u_n = \rho u_{n-1} + \epsilon_k, \quad \epsilon_k \sim \mathcal{N}(0, 1 - \rho^2), \quad u_1 \sim U(0,1).
\end{equation}
The objective is to learn a mapping with linear recurrences $f: X \to Y$, where the target is given by:
\begin{equation}
    Y = u_{t^*}
\end{equation}
This corresponds to learning a shift of $N - t^*$ with finite samples. 


\subsection{Random initialization vs. Shift-K initialization}

In this first set of experiments, we analyze the impact of initializing the complex diagonal entries $a_s$ of the linear RNN using phases that are uniformly distributed over a segment of the unit disk, with a constant radius close to 1, as described in the parametrization in Eq.~\eqref{Param_of_the_as}. Additionally, the parameters $b_s$ are initialized following the parametrization given in Eq.~\eqref{Param_of_the_bs}. We call this initialization the shift-$K$ initialization. We compare this approach to a standard random initialization to evaluate potential benefits in terms of performance and stability.


\begin{table}[ht]
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        & \textbf{Random init.} \hspace{4,2cm} \textbf{Shift-K init.} \\ \midrule
        Batch size & [20, 50, 100] \hspace{4,4cm} [20, 50, 100] \\
        Number Samples & 130000 \hspace{5,2cm} 130000\\
        Sequence length & 1500 \hspace{5,6cm} 1500 \\
        Position of $t^*$ & 200 \hspace{5,8cm} 200 \\
        Hidden neurons & 128 \hspace{5,8cm} 128 \\
        Input / output dimension & 1 \hspace{6,2cm} 1 \\
        Learning rates & [0.01, 0.005, 0.001, 0.0001] \hspace{2,2cm}[0.01, 0.005, 0.001, 0.0001] \\
        Weight decay & $10^{-5}$ \hspace{5,7cm}$10^{-5}$ \\ 
        $\rho$ & \{0, 0.2, 0.4, 0.6, 0.8, 1\} \hspace{2,7cm} \{0, 0.2, 0.4, 0.6, 0.8, 1\} \\ \midrule
        $a_s$ param. & $a_u = e^{-\alpha/K_{\textnormal{init}}}e^{i\epsilon_u\pi}, \varepsilon \sim \mathcal{U}(-1,1)$ \hspace{1cm} $a_u = e^{-\alpha/K_{\textnormal{init}}}e^{iu\frac{\pi}{K_{\textnormal{init}}}}$\\
         $b_s$ param. & $b_u = \frac{e^{-\alpha}(e^{2\alpha}-e^{-2\alpha})}{2K_{\textnormal{init}}}\times(-1)^u$ \hspace{1,8cm} $b_u = \frac{e^{-\alpha}(e^{2\alpha}-e^{-2\alpha})}{2K_{\textnormal{init}}}\times(-1)^u$  \\
        $\alpha$ & 1 \hspace{6,2cm} 1 \\
       $K_{\textnormal{init}}$ & 1300 \hspace{5,6cm} 1300 \\ \midrule
        Number epochs & 60 \hspace{6cm} 60 \\
        \bottomrule
    \end{tabular}
    \caption{{Experimental details for Figure~\ref{fig:xps} (left)}. We use $[\dots]$ to denote hyperparameters that were scanned over with grid search and $\{\dots\}$ to denote the variable of interest for the figure. We chose the same representation for $b_s$ in both cases because we observed small impact of this parameter on the final results.}
    \label{tab:xp-compare_init}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/appendix_2_filters.pdf}

    \vspace*{-.2cm}
    
    \caption{\textit{Comparison of Filters Obtained with Different Initialization Methods. Left: Filter obtained using our proposed shift-$K$ initialization method, which exhibits a more structured and interpretable pattern. Right: Filter obtained with random initialization, which appears significantly noisier, indicating less effective memory propagation. }}
    \label{fig:appendix 2 filters}
\end{figure}

\subsection{Robustness of Shift-K initialization}

In this second set of experiments, we investigate the robustness of our initialization scheme with respect to inaccuracies in the choice of $K_{\textnormal{init}}$ when initializing $a_s$ as in Eq.~\eqref{Param_of_the_as}. In practical applications, the actual shift of the sequence is often unknown, making it impossible to initialize with the exact optimal value of~$K$. A robust initialization method should exhibit resilience to such uncertainties, allowing for performance stability within a reasonable range of $K_{\textnormal{init}}$ values.

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}l@{}}
        \toprule
        \textbf{Shift-K init.} \\ \midrule
        Batch size: [20, 50, 100] \\
        Number of Samples: 150000 \\
        Sequence length: 2250 \\
        Position of $t^*$: 250 \\
        Hidden neurons: 128 \\
        Input / output dimension: 1 \\
        Learning rates: [0.01, 0.005, 0.001, 0.0001] \\
        Weight decay: $10^{-5}$ \\ 
        $\rho$: 0.7 \\ \midrule
        $a_s$ param.: $a_u = e^{-\alpha/K_{\textnormal{init}}}e^{iu\frac{\pi}{K_{\textnormal{init}}}}$\\
        $b_s$ param.: $b_u = \frac{e^{-\alpha}(e^{2\alpha}-e^{-2\alpha})}{2K_{\textnormal{init}}}\times(-1)^u$  \\
        $\alpha$: 1 \\
        $K_{\textnormal{init}}$: \{250, 500, 1000, 2000, 4000, 8000, 16000, 32000\} \\ \midrule
        Number of epochs: 60 \\
        \bottomrule
    \end{tabular}
    \caption{{Experimental details for Figure~\ref{fig:xps} (right).}}
    \label{tab:xp-robustness}
\end{table}
