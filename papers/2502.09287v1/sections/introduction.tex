\section{Introduction}

Since their early development~\citep{rumelhart1986sequential, elman1990finding}, recurrent neural networks (RNNs) have advanced machine learning for sequential data, with milestones such as echo-state networks~\citep{jaeger2001echo} and LSTMs~\citep{hochreiter1997long}. However, two problems severely limit the application of classical RNNs in modern times: (1) GPU hardware optimized for large matrix operations struggles with efficient sequential processing, and (2) RNNs are notoriously difficult to train due to vanishing and exploding gradients~\citep{bengio1994learning, pascanu2013difficulty}. As a result, transformers~\citep{vaswani2017attention} have emerged as the dominant solution for sequence processing, offering desirable scalability properties and less challenging optimization. However, the attention mechanism powering transformers relies on computing pairwise interactions between inputs at each timestamp, resulting in a squared inference and memory complexity $O(L^2)$ in the sequence length $L$. Instead, classical RNNs require one pass through the data to recurrently update their hidden state, bringing their complexity down to $O(L)$. This property is particularly desirable in the long-context setting~(e.g., analysis of long documents or genomics).

Indeed, in the interest of efficiency, we have recently witnessed a \textit{resurgence of new RNNs} in state-of-the-art industry-size applications such as language modeling~\citep{gu2024mamba, peng2024eagle, qin2024hgrn2, de2024griffin, yang2024parallelizing}. Sparked from the S4 model~\citep{gu2022efficiently}, these new recurrences offer $O(L)$ complexity as classical RNNs, yet are parallelizable on modern hardware like attention. At the core of their efficiency is a simplified recurrence that is \textit{linear} in the hidden state:
\begin{equation}
    x_{n} = A_n x_{n-1} + B_n u_n, 
    \label{eq:1}
\end{equation}
where $u_n$ is the input data at timestamp $n$, $x_n$ is the hidden state (which is a linear combination of inputs $u_1,u_2,\dots, u_n$), and $A_n,B_n$ are input-controlled transition matrices with a special parametrization~\citep{orvieto2023resurrecting}. Compared to previous RNNs, $A_n$ and $B_n$ have\textit{ no dependency on the hidden state}---a feature which reduces expressivity~\citep{merrill2024illusion,cirone2024theoretical} but unlocks GPU-efficient processing~\citep{martin2018parallelizing, smith2023simplified}.

New linear RNNs offer improved inference complexity and competitive performance on language modeling tasks~\citep{dao2024transformers,waleffe2024empirical}, as well as state-of-the-art results on several other domains including vision~\citep{liu2024vmamba,li2025videomamba, liang2024pointmamba, xing2024segmamba}, audio generation~\citep{goel2022sashimi}, online learning~\citep{zucchet2023online}, reinforcement learning~\citep{lu2023structured} and genome analysis, where the $O(L)$ complexity can tackle long DNA sequences~\citep{nguyen2024sequence}. 


Despite the practical advantages of new linear recurrent mechanisms, we are at a very early evaluation stage in regards to assessing and understanding the capabilities and optimization properties of such systems when compared to~(1) transformers and (2) non-linear~(classic) RNNs. While several works are devoted to establishing a direct connection between transformers and linear RNNs~\citep{katharopoulos2020transformers, schlag2021linear, ali2024hidden, sieber2024understanding}, others point to fundamental and drastic differences in regards to expressivity and basic capabilities. Further, despite~\citet{orvieto2024universality,wang2024state, cirone2024theoretical} provide infinite-width theoretical guarantees for the expressivity of deep architectures based on linear RNNs, other works focusing on specific reasoning tasks of general interest in language modeling tell a different story: \cite{arora2023zoology} identified in the problem of selective \textit{copying}~(i.e., of recalling a specific value from the past, when presented the relative key) a fundamental discrepancy between attention and RNNs: building up a memory of past inputs, as opposed to direct edges, can fundamentally limit finite-width performance of linear RNN based models. This finding inspired a formal investigation by~\cite{jelassi2024repeat}, who proved that perfectly retrieving~(i.e., with zero error) inputs from distant past requires the RNN width to increase linearly with the sequence length. This in contrast to attention-based models, that can build associative mechanisms to solve such tasks with 2 layers \citep[cf.][]{olsson2022context}.

Inspired both by the practical relevance of new linear RNNs and by the need of further theoretical investigations of their basic properties, in this work we mathematically investigate arguably the most basic long-range task: recalling inputs seen $K$ timestamps before the current processing step. Such task has a close relation to the \textit{copy task} by~\cite{jelassi2024repeat}, while being simpler and with a clear challenge: successful replay as $K$ increases. As~\citet{jelassi2024repeat}, we are specifically interested in characterizing optimal performance as a function of the recall range $K$ and the memory size $S$---the dimension of the hidden state $x$.  Yet, while~\citet{jelassi2024repeat} work in the finite-vocabulary input setting standard in language modeling, assuming no particular structure in the recurrence, we take instead a signal processing approach, which allows us to characterize in detail the tradeoff between long-memory requirements~(large $K$) and optimal recall resolution under reduced memory size~($S<K$). Since the task is independent from the input value to recall, we restrict our attention to the case where in Eq.~\eqref{eq:1}, $A_n$ and $B_n$ are input-independent and hence fixed matrices: $A,B$. Further, as common in modern RNNs, we consider without loss in generality\footnote{This equivalence is often used in linear systems theory~\citep{hespanha2018linear}. Let us start from $x_n = A x_{n-1} + Bu_n$. Over the space of \( S \times S \) non-diagonal real matrices $A$, the subset of those non-diagonalizable in the complex domain has measure zero~\citep{bhatia2013matrix}. Thus, with arbitrarily small perturbations, \( A = Q \text{diag}(a) Q^{-1} \). This implies $Q^{-1} x_n = \text{diag}(a) (Q^{-1} x_{n-1}) + (Q^{-1} B) u_n$. Renaming \( x_n \leftarrow Q^{-1} x_n \) and \( B \leftarrow Q^{-1} B \) yields a diagonal complex-valued recurrence. \label{diagfootnote}} the diagonal case $A = \text{diag}(a)$. For one-dimensional input sequences and a final sum operation, if the RNN is initialized with zero memory, the scalar output sequence $(y_n)_{n \geq 0}$ can be computed through a \textit{convolution}:
\begin{equation}
    x_n \!=\! \text{diag}(a) x_{n-1} + u_n b,\quad y_n \!=\! 1^\top x_n  \ \ \implies \ \ y_n = (c \ast u)_n = \sum_{k=0}^n c_k u_{n-k} \quad \text{with} \quad c_k \!=\! \sum_{s=1}^S a^k_s b_s.
    \label{eq:filter_new}
\end{equation}
The task then consists in finding potentially complex vectors $a,b$ such that $y_n \approx u_{n-K}$. This is equivalent to requiring the sequence $(c_k)_{k \geq 0}$ to approximate the \textit{shift-$K$} filter $d = \delta_{K}$ (which is a sequence that is zero everywhere except at position $K$, that is, $d_k = 1_{k=K}$). 

In order to assess the approximation of $d = \delta_K$ by $c$ in the form of Eq.~(\ref{eq:filter_new}), we consider the idealized situation of infinite-length random stationary signals $(u_n)_{n \in \mathbb{Z}}$, and consider the expected loss function at time $n=0$, $\mathbb{E} \big[  | (c \ast u)_0 - (d \ast u)_0|^2 \big]$, where the expectation $\mathbb{E}$ is taken with respect to the distribution of the random sequence $(u_n)$. By stationarity of $(u_n)$ and the law of large number, this is equivalent to the mean-square-error over the entire sequence:
\begin{equation}
\label{loss}
\mathbb{E} \big[  | (c \ast u)_0 - (d \ast u)_0|^2 \big] = \lim_{N \to +\infty} \frac{1}{2N+1} \sum_{n = - N}^N  | (c \ast u)_n - (d \ast u)_n|^2
.\end{equation}

We study this loss function for $u$ being the white noise (problem becomes $\min_{a,b}\| c(a,b) - d \|_2^2$), and for simple auto-correlations $\E[u_k \bar{u}_{k'}] = \rho^{|k-k'|}$ for $\rho \in [0,1)$ ($\rho=0$ corresponding to white noise).

\paragraph{Contributions.} We make the following contributions:
\begin{enumerate}
    
    \item We provide a lower bound 
    on the best possible for the shift-$K$ loss above~(optimized with respect to $a$ and $b$) using tools from the approximation of rational functions~\citep{baratchart2016minimax} and Cauchy matrices~\citep{yang2003generalized}. For white noise, we obtain the lower bound $1- \frac{S}{K}$, showing that a large copy lag $K$ leads to an increase in error. This is made more precise with more general $\rho$'s, with the lower bound $\big(1 - \frac{3S}{K} \frac{1}{1-\rho} \big)_+$, showing that a small error can be obtained for autocorrelated input signals.
    
    

    \item We find an analytical solution to the shift-$K$ problem close to our lower bound (with matching behavior up to constants, and thus nearly optimal). Our solution allows us to instantiate an uncertainty principle, providing a clear intuition on resolution/memory tradeoffs~(see Fig.~\ref{figure uncertainty principle}). In addition, our closed-form solution for $a$ in Eq.~\eqref{eq:filter_new} allows us to motivate from a task-specific memorization perspective the successful S4D-Lin initialization by~\citet{gu2022parameterization} --- the simplest linear RNN initialization allowing to solve the most challenging tasks in the long-range arena~\citep{tay2020long}.
\end{enumerate}

The loss of our near-optimal solution illustrates the trade-off between recall range~($K$), memory size~($S$) and recall precision~(i.e., the concentration of the filter $c$ in Eq.~\eqref{eq:filter_new} around the spike $\delta_{K}$). Surprisingly, our finding can be formulated as an uncertainty principle: 
\begin{center}
\textit{Learning a filter~$c$ centered around a large $K$ for a fixed state-size $S$ is relatively easy,\\ yet increasing time-horizon $K$ comes at the expense of resolution~(width\footnote{For a filter designed to approximate the shift-$K$, we call width the width at the halfway height of the peak centered in $K$. The narrower the width, the better the approximation of the shift-$K$. The width can be interpreted as the resolution of the filter.} of the filter). }
\end{center}
As illustrated in Fig.~\ref{figure uncertainty principle}, perfect recall is eventually achieved at $S=K$. For $K>S$, the width of the filter around the correct location is proportional to $K/S$.

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\linewidth]{img/uncertainty_new.pdf}

    \vspace*{-0cm}
    
    \caption{\textit{Learning to shift-$K$ with linear recurrences exhibits an uncertainty principle. For fixed $S=250$, different values of $K$ induce different performances: the smaller the ratio $S/K$, the lower the peak of the filter and the larger the width. For a fixed memory size $S$, increasing the time horizon is feasible, but comes at the expense of resolution. For $K>S$, the width of the filter around the correct location is $K/S$.}}
    \label{figure uncertainty principle}
\end{figure}


\section{Notation and Main Results}
In this paper, we operate in the complex domain as usually done in the literature on SSMs~\citep{gu2022parameterization,orvieto2023resurrecting}. The reason for this choice in the literature is motivated by good performance, increased expressivity guarantees~\citep{orvieto2024universality,ran2024provable}, and most of all by the equivalence between dense linear RNNs and diagonal complex-valued RNNs$^{\ref{diagfootnote}}$.

\paragraph{Time domain.} Starting from Eq.~\eqref{eq:filter_new}, given a generic real-valued input $u=(u_n)_{n\in\mathbb{Z}}$, the output $y=(y_n)_{n\in\mathbb{Z}}$ of a linear RNN with parameters $(a,b) \in \mathbb{C}^S \times \mathbb{C}^S$ can be written as 
$y_n = (c\ast u)_n:= \sum_{k=0}^\infty c_k u_{n-k}$, where the convolution kernel $c = (c_k)_{k\in\mathbb{N}}$ is defined by $(a,b)$  as: 
\begin{equation}
\label{eq:cab}
c_k = \sum_{s=1}^S a^k_s b_s,
\end{equation}
for all $k\in\mathbb{N}$. Let $d = (d_k)_{k\in\mathbb{N}}$ be a second convolution kernel processing the input---the one we would like to approximate with our RNN (that is, $d_k = 1_{k = K}$). One can compute the expected squared norm between outputs of $d\ast u$ and $c\ast u$ at only a single $n \in \mathbb{Z}$ (as shown in Eq.~\eqref{loss}, this is also the mean-square error over the entire sequence):
\begin{align*}
     \mathbb{E} \Big[ |(d\ast u)_n-(c\ast u)_n|^2  \Big] &=   \mathbb{E} \Big[ \Big|\sum_{k=0}^\infty (c_k -d_{k}) u_{n-k} \Big|^2 \Big] =  \sum_{k,k'=0}^\infty (c_k -d_{k}) (\bar{c}_{k'} -\bar{d}_{k'})  \mathbb{E} [ u_{n-k} u_{n-{k'}}].
\end{align*}
Using stationarity of the signal $u$, $\mathbb{E} [ u_{n-k} u_{n-{k'}}] = \gamma(k-k')$ only depends on $k-k'$, and, we get our objective function, to be optimized with respect to the RNN parameters $(a,b)$:
\begin{equation}
    \mathcal{L}_{\text{time}}(c, d) = \sum_{k, k'=0}^{\infty}(c_k - d_k)(\bar{c}_{k'} - \bar{d}_{k'})\gamma(k-k')
    \label{Time domain loss},
\end{equation}
where \(\gamma(k - k')\) is the auto-correlation function that captures average temporal dependencies, weighting the contribution of errors based on time step correlations~\citep{brockwell2002introduction}. When there is no ambiguity on the filters $(c_k)_{k \in \mathbb{N}}, (d_k)_{k \in \mathbb{N}}$, we will refer to the loss $\mathcal{L}_\text{time}(c, d)$ as $\mathcal{L}_\text{time}$. We adopt the common choice \(\gamma(k) = \rho^{|k|}\) with \(\rho \in [0,1)\), also used recently by~\citet{zucchet2024recurrent}, where \(\rho = 0\) corresponds to uncorrelated white noise, 
where 
$\mathcal{L}_{\text{time}}(c, d) = \sum_{k =0}^{\infty}|c_k - d_k|^2$,
and \(\rho \to 1\) reflects strong temporal dependencies.

\paragraph{Frequency Domain.} In this work, we aim at approximating the action of the shift-$K$ filter $d = \delta_{K} := (1_{k = K})_{k\in\mathbb{N}}$. 
We find it convenient to process the loss above in frequency domain. The discrete-time Fourier transforms and Parseval's theorem allow to write the loss in Eq.~\eqref{Time domain loss} as
\begin{equation}
\mathcal{L}_\text{freq}(C, D) = \frac{1}{2\pi}\int_{-\pi}^\pi\vert C(e^{i\omega}) - D(e^{i\omega})\vert^2\Gamma(e^{i\omega})d\omega,
    \label{Frequential loss copy task}
\end{equation}
where $C(e^{i\omega}) = \sum_{s=1}^S\frac{b_s}{1-a_se^{-i\omega}}$ is a rational function of $e^{-i\omega}$, $D(e^{i\omega}) = e^{-iK\omega}$~(DFT of a shifted Dirac impulse) and $\Gamma(e^{i\omega}) = \frac{1-\rho^2}{\vert 1 - \rho e^{-i\omega}\vert^2}$, thus turning the problem to that of rational approximations on the unit circle~\citep{baratchart2016minimax}. See Appendix \ref{appendix subsection natural pair} and \ref{appendix subsection frequency loss} for more details. When there is no ambiguity on the Fourier transforms $C$ and $D$, we will refer to the loss $\mathcal{L}_\text{freq}(C, D)$ as $\mathcal{L}_\text{freq}$.
 


\paragraph{Overview.} In Section \ref{section lower bound}, we provide a lower bound on $\mathcal{L}_{\text{time}}$ suggesting that having a small state size does not necessarily imply a short memory capacity; however, the bound also shows that this comes at the cost of a degraded resolution. This provides a first connection with our uncertainty principle in the context of learning shifts with linear models and reveals a fundamental tradeoff between the time horizon of our copy task and the performance of the filter, given a fixed size of the model. Furthermore, we highlight the significant role of data autocorrelation, demonstrating that while linear models struggle to retain white noise, their performance improves substantially when dealing with autocorrelated data, which may better reflect real-world scenarios.
In Section~\ref{section upper bound}, we establish our uncertainty principle by deriving a closely matching upper bound. To do this, we consider the loss $\mathcal{L}_\text{freq}$ to carefully design a new filter that performs similar to the lower bound, up to a constant factor. This representation, providing meaningful results in practice, gives insights on the behavior of linear RNNs as they implement longer memory.

To summarize, our insights stem from two main results. The first one is a lower bound on the best possible error greater than  $1- \frac{S}{K}$ for $\rho=0$ and $\big(1 - \frac{3S}{K} \frac{1}{1-\rho} \big)_+$ for $\rho \in [0,1)$ (see Theorems~\ref{lower bound white noise} and~\ref{theorem 
autocorrelated lower bound}). The second is an upper bound (construction of an explicit filter) that matches the lower-bound up to a constant factor~(thus establishing\footnote{Our uncertainty principle, as formulated in the introduction, is first suggested by our lower bound but only formally implied by~(and immediately follows from) the combination with our matching upper bound.} our uncertainty principle), as informally described below in Theorem~\ref{thm:informal_upper} and illustrated in Fig.~\ref{fig:window}.

\begin{figure}[H]
\vspace{-1mm}
    \centering
    \includegraphics[width=.85\linewidth]{img/window.pdf}

    \vspace*{-.2cm}
    
    \caption{\textit{Shown is the behavior of $\frac{C(e^{i\omega})}{D(e^{i\omega})}$, where $C(e^{i\omega})$ is the Fourier transform of our near-optimal filter in Theorem~\ref{thm:informal_upper} and $D(e^{i\omega}) = e^{-iK\omega}$ is the Fourier transform of our Shift-$K$ filter. Perfect match between filters implies the ratio is $1$ for all $\omega$.  If instead this equality holds in a window, then the filter would effectively act as a Shift-$K$ for inputs with frequencies $\Gamma(e^{-i\omega})$ in the same window. For $S=51, K=500$, we denote $T = \frac{S-1}{2}$ and plot the ratio $\frac{C(e^{i\omega})}{D(e^{i\omega})}$ with respect to $\Omega=\frac{K\omega}{\pi}$ (to dilate the space). The asymptotic ratio $\frac{C(e^{i\omega})}{D(e^{i\omega})}$ (yellow) from Theorem \ref{convergence to window}, the same ratio for linear models with ($b_s$) given by Eq.~\eqref{Param_of_the_bs} (green), and for ($b_s$) given by linear system inversion Eq.~\eqref{bs linear system inversion} (blue) are compared. The model effectively approximates the shift-$K$ operation, within the frequency window $[-\frac{\pi T}{K}, \frac{\pi T}{K}]$, while vanishing outside this window, leading to a time resolution~(inverse of filter width) of $\frac{S}{K}$. This behavior underscores the uncertainty principle associated with the filter: for small $S/K$ ratios and uncorrelated data, the approximation holds over a narrow frequency range. As autocorrelation increases, the approximation domain shrinks, enhancing accuracy. In red, we show the perfect window~(value of 1 on $[-\frac{\pi T}{K}, \frac{\pi T}{K}]$ and~$0$ outside).}
}
    \label{fig:window}
\end{figure}

\begin{theorem}[upper bound, informal]
Let $S$ be odd and $T = \frac{S-1}{2}$. The filter defined by Eq.~\eqref{eq:cab} with $a_s = \exp\left(-\frac{\alpha}{K}\right)\exp\left(i\frac{\pi s}{K}\right)$ and  $b_s \propto (-1)^s$ for $ s \in \llbracket -T, T\rrbracket$\footnote{While in our introduction, for clarity, we considered $s\in\llbracket1,S\rrbracket$, hereafter, for ease of notation and to emphasize symmetry in our filter, we reparametrize $s\in\llbracket -T,T\rrbracket$ where $T = \frac{S-1}{2}$. The dimension of our hidden state in Eq.~\eqref{eq:filter_new} remains $S$.},   achieves an approximation error comparable to the lower bound up to a constant factor in the context of white noise data. This is because our filter accurately approximates a shift-$K$ in frequency domain over the window \(\left[- \frac{\pi T}{K}, \frac{\pi T}{K} \right]\), vanishing outside this range.
\label{thm:informal_upper}
\end{theorem}



The connection between Theorem~\ref{thm:informal_upper} and HiPPO initialization is presented in Sec.~\ref{sec:hippo}



\section{Related Works}
\label{rw}

\textbf{Attention and RNNs.} In order to reduce the $O(L^2)$ complexity burden in transformers, techniques such as patching~\citep{dosovitskiy2021image,pagnoni2024byte}, gradient checkpointing~\citep{chen2016training}, and FlashAttention~\citep{dao2022flashattention, dao2023flashattention} become crucial when training and deploying models at scale. Despite this limitation, transformers successfully power most state-of-the-art architectures we use today: beyond large language models~\citep{brown2020language, team2024gemini}, attention found widespread application in vision~\citep{dosovitskiy2021image}, graphs~\citep{ma2023graph} and DNA~\citep{dalla2024nucleotide}.  Nevertheless, the quadratic complexity of attention has remained a pressing limitation, prompting numerous efforts over the years to develop efficient approximations~\citep{wang2020linformer, choromanski2020rethinking, chen2021skyformer, lee2022fnet} that inevitably bring attention closer to RNNs~\citep{schlag2021linear,katharopoulos2020transformers}. Indeed, more recently, we have witnessed a resurgence of RNNs in state-of-the-art industry-size applications such as language modeling. Sparked by S4~\citep{gu2020hippo, gu2022efficiently}, which surpassed attention on long-range reasoning tasks~\citep{tay2020long}, we have seen a drastic increase in the usage of RNNs in deep architectures, albeit in a linear form that guarantees both $O(L)$ memory/inference complexity and fast computation on GPUs~\citep{martin2018parallelizing} while matching or surpassing transformers on downstream tasks: a prime example are state-space models~(SSMs) such as Mamba~\citep{gu2024mamba}, along with architectures based on RNNs~\citep{de2024griffin, peng2024eagle, yang2024gated}. 
\\[.35cm]
\textbf{Special initialization of SSMs.} While in natural language SSMs are relatively robust to initialization, on challenging long-range reasoning or memorization tasks careful initialization is crucial~\citep{gu2023how,orvieto2023resurrecting,trockman2024mimetic}. The used schemes stem from the HiPPO theory by~\citet{gu2020hippo}: a special initialization can provably construct features related to the coefficients of optimal polynomial approximations of the input signal. Despite this intriguing connection, already S4~\citep{gu2022efficiently}, the first SSM, deviates quite significantly from the HiPPO prescription. Latest initialization such as S4D-Lin~\citep{gu2022parameterization} or the one by~\citet{orvieto2023resurrecting} are only vaguely related to the HiPPO and present a single non-trivial property: recurrent eigenvalues~($a$ in Eq.~\eqref{eq:filter_new}) are complex-valued, with coupled phase and magnitude. Our theory provides a formal justification of this choice from a memorization perspective.
\\[.35cm]
\textbf{Theoretical guarantees for (non)-linear RNNs.} Expressivity of standard nonlinear RNNs has been extensively studied from a Turing completeness perspective~\citep{siegelmann1992computational,korsky2019computational}. Taking instead the signal processing angle,~\citet{hanson2020universal} proved that wide enough non-linear RNNs can approximate up to vanishing precision non-linear time-homogeneous systems of differential equations driven by input paths. The argument used here is based on Barron's theorem~\citep{barron1993universal} for approximation of continuous functions with neural networks with one hidden layer. Regarding instead linear RNNs such as Eq.~\eqref{eq:1}, results are more recent and have been mostly driven by deep learning developments. \citet{li2022approximation} showed that linear time-invariant RNNs~($A_n$ and $B_n$ independent of $n$, as in this paper) can approximate arbitrary convolution filters as the hidden state size $S$ grows to infinity. Further,~\citet{hanson2019universal} proved that stacking exponentially~(in the sequence length) many temporal convolution filters, chained together with ReLU activations, leads to approximation of arbitrary non-linear filters. Recent works~\citep{orvieto2024universality,wang2024state} prove the universality of linear recurrences~(one layer) when equipped with a fixed (timestamp independent) point-wise MLP acting across the recurrence output, with intriguing connections to Volterra series~\citep{boyd1985fading}. Finally, expressivity of latest models such as Mamba has been studied by~\citep{cirone2024theoretical}. Further, language-specific capabilities of new SSM and RNNs have been studied by~\citet{merrill2024illusion}~(state tracking) and~\citet{jelassi2024repeat}~(copying). 
