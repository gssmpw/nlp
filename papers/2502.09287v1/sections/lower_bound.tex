\section{Lower Bound}\label{section lower bound}

We aim to establish a lower bound on the approximation error $\mathcal{L}_\text{time}(c,d)$ where $c$ has the RNN form in Eq.~\eqref{eq:cab}. By deriving this lower bound, we provide a theoretical benchmark for evaluating the effectiveness of linear time-invariant filters in our shift-$K$ task. Importantly, we demonstrate that the derived lower bound depends on the ratio \(\frac{S}{K}\), where \(S\) represents the hidden dimension and \(K\) is the horizon of our copy task.  


To gain deeper insights into the performance of these filters, we analyze the approximation error in two scenarios: the case of white noise ($\rho=0$), and the case of autocorrelated data $(\rho>0)$.

\subsection{White Noise}

With white noise input $\mathcal{L}_\text{time}(c, d)$ has a simpler squared $\ell_2$-norm formulation:

\begin{equation}
\mathcal{L}_\text{time}(c, d)= \sum_{k=0}^{+\infty}\vert c_k - d_k\vert^2 = 1 + \sum_{k=0}^{+\infty}\vert c_k\vert^2 - 2{\textnormal{ Re}}\big(\sum_{k=0}^{+\infty}c_kd_k\big),
\label{white noise time domain loss}
\end{equation}
where we recall that $c$ has the form Eq.~\eqref{eq:cab} and that the shift-$K$ filter $d_k= 1_{k=K}$ has norm one. The following theorem shows a lower bound.


\begin{theorem}[Lower bound of the approximation error---white noise]\label{lower bound white noise}
 Let $S$ and $K$ be two  positive integers. The approximation error $\mathcal{L}_\text{time}(c, d)$ of the shift-$K$ filter $d$ by a filter $c$ of the form in Eq.~\eqref{eq:cab} is lower bounded by $1-\frac{S}{K+1}$.
\end{theorem}

\begin{proof}(Sketch, see full proof in Appendix~\ref{appendix subsection white noise loss}).
Given the form of $c$ as $c_k = \sum_{s=1}^S b_s a_s^k$, the loss in Eq.~\eqref{white noise time domain loss} has an explicit expression by summing geometric series over $k$, leading to:
\begin{equation}
    \mathcal{L}_\text{time}(c, d)= 1 + \sum_{s, s'=1}^S\frac{b_s\bar{b}_{s'}}{1-a_s\bar{a}_{s'}} - 2{\textnormal{ Re}}\Big(\sum_{s=1}^Sb_sa_s^K\Big).
    \label{good as and bs loss}
\end{equation}
 We thus want to maximize with respect to $a_s$, $b_s$, $s\in\llbracket 1, S\rrbracket$, the following quantity:
\begin{equation*}
 2{\textnormal{Re}}\big(\sum_{s=1}^Sb_sa_s^K\big) - \sum_{s,s'=1}^S\frac{b_s\bar{b}_{s'}}{1-a_s\bar{a}_{s'}},
\end{equation*}
which is equal to 
\begin{equation}
\label{Q}
\langle \bar{b}, a^K\rangle + \langle a^K, \bar{b}\rangle - \langle \bar{b}, C\bar{b}\rangle,
\end{equation}
where $C$ is an $S \times S$ matrix with entries $\displaystyle C_{ss'}=\frac{1}{1-a_s\bar{a}_{s'}}$, and $\langle \cdot,\cdot\rangle$ is the standard Hermitian product. This is a quadratic form in $b$, and thus we can maximize with respect to $b$ in closed form, leading to the performance criterion  $ \mathcal{L}_\text{time} = 1 - F_K$, with 
\begin{equation}
\label{eq:FK}
F_K = \langle a^K, C^{-1}a^K \rangle = \sum_{s,s'=1}^{S} \bar{a}_s^K (C^{-1})_{ss'} a_{s'}^K.
\end{equation}
This is a function of the $a_s$'s only since we have maximized out the $b_s$'s. This function is rational but has a complicated expression.
In order to bound it, we notice that the matrix $C$ has some ``displacement structure'' similar to Cauchy matrices~\citep{yang2003generalized},
that is, 
$$
C - \Diag( {a}) C \Diag(\bar{a})  = 1_S 1_S^\top,
$$
which leads to, after some manipulations, to a ``closed form'' expression for the inverse $C^{-1}$:
$$
(C^{-1})_{ss'} \big( \frac{1}{ \bar{a}_s a_{s'}}-1 \big) = u_s \bar{v}_{s'},
$$
with $ u = C^{-1} 1_S$ and $v = \Diag( \bar{a})^{-1}  C^{-1} \Diag(a)^{-1} 1_S \propto u$. Moreover, the vector $u$ happens to have a simple characterization through rational functions as
$$
\sum_{s=1}^S \frac{ u_{s}}{1 - z \bar{a}_{s}} = 1 - \prod_{s=1}^S \bar{a}_{s} \prod_{s=1}^S \frac{ a_{s} - z}{1 - z \bar{a}_{s}}.
$$
This allows to characterize the Fourier series of $F_K$ and get an explicit bound using properties from rational approximations on the unit circle~\citep{baratchart2016minimax}. See details in Appendix~\ref{appendix subsection white noise loss}.
\end{proof}


The lower bound established in Theorem~\ref{lower bound white noise} demonstrates that the approximation error remains close to 1 when the ratio \(S/K\) is small. This highlights the inherent difficulty of approximating shift-$K$ filter using linear recurrences in this regime. Nevertheless, by increasing the dimension of the parameters $S$, with fixed $K$, we can hope to achieve a better loss. This shows a fundamental tradeoff in the linear model's ability to solve the copy task, in the context of white noise -- connected to our uncertainty principle. Allowing auto-correlated signals gives a finer picture, this is explored in the next subsection.

\subsection{Autoregressive Autocorrelation}

In this context, we consider a non-zero correlation factor defined as \(\gamma(k) = \rho^{\vert k\vert}\) to account for the temporal structure in the data. This approach with $\rho>0$ simulates situations with real-life data, whose autocorrelation is often modeled this way.~\citep{brockwell2002introduction}. The loss function writes in this case:
\begin{equation}
    \mathcal{L}_\text{time}(c, d) = \sum_{k,k'=0}^{+\infty}(c_k-d_k)(\bar{c}_{k'}-\bar{d}_{k'})\rho^{\vert k-k'\vert},
    \label{correlated time domain loss}
\end{equation}
where \(c_k\) is defined as in Eq.~\eqref{eq:filter_new}, and \((d_k)\) is given by $d_k = 1_{k=K}$. The following theorem extends Theorem~\ref{lower bound white noise}
to all $\rho$'s. We use the notation $(y)_+=\max(y, 0)$.
  

\begin{theorem}[Lower bound of the approximation error---auto-correlated noise]\label{theorem 
autocorrelated lower bound}
Let $S$ and $K$ be two integers. The approximation error $\mathcal{L}_\text{time}(c, d)$ of the shift-$K$ filter $d$ by a filter $c$ of the form in Eq.~\eqref{eq:cab} is lower bounded by, for the autoregressive autocorrelation $\Big(1-\frac{S}{K}\frac{3}{1-\rho}\Big)_+$.
\end{theorem}

\begin{proof}(Sketch, see full proof in Appendix~\ref{proof auto}).
Let \((c_k)\) be a linear-time filter such that \(c_k = \sum_{s=1}^S b_s a_s^k\), and let \((d_k)\) be defined as $d_k =  1_{k=K}$. Let $w_s = b_s a_s / ( a_s - \rho)$, for $s \in \llbracket 1,S \rrbracket$. We can compute $\mathcal{L}_\text{time}(c, d)$ in closed form by explicit summation leading to
\[
\mathcal{L}_\text{time}(c, d)=
1 - 2(1 - \rho^2)\operatorname{Re}\Big(\sum_{s=1}^{S+1}\frac{w_s a_s^K}{1-a_s\rho}\Big) + (1-\rho^2)\sum_{s,s'=1}^{S+1}\frac{w_s \bar{w}_{s'}}{1-a_s\bar{a}_{s'}},
\]
where we have used the specificity of the auto-correlation function to create a new pole, defined as \(a_{S+1} = \rho\) and the constraint \(\sum_{s=1}^{S+1} w_s a_s^{-1} = 0\) holds for some vector $w$ to be optimized. The minimum with respect to $w$ with the constraint is greater than the unconstrained minimizer, equal to (using the fact that this is a quadratic problem):
$ \displaystyle 
H_K = 1 - (1-\rho^2)\sum_{s, s'=1}^{S+1}\frac{\bar{a}_s^K}{1-\bar{a}_s\rho}\frac{a_{s'}^K}{1-a_{s'}\rho}(C^{-1})_{ss'},
$
where $C_{ss'} = \frac{1}{1-a_s\bar{a}_{s'}}$, is a matrix of a similar form as in the proof of Theorem~\ref{lower bound white noise}. The proof then follows similarly by using explicit expressions of matrix inverses.
\end{proof}

Therefore, in the autocorrelated case, $\mathcal{L}_\text{time}$ exhibits a lower bound that depends on the ratio \(\frac{S}{K}\). The error diminishes as \(\rho\) approaches 1, indicating that memorization may become more effective in the limit of strong autocorrelation. This behavior also suggests that memorization performance is intrinsically linked to the spectral characteristics of the data. Specifically, linear filters are incapable of precisely solving the copy task for time horizons~\(K\) larger than \(S\) in the presence of poorly correlated data, as in white noise. However, reducing the spectral domain, by imposing autocorrelation in the data, concentrates the signal's energy within specific frequency bands and significantly improves performance. Next, we further investigate this behavior by designing an explicit filter.

