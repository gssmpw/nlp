\section{Lower bound}


In this section, we provide the proofs of the two lower bounds.

\subsection{White noise case (Theorem~\ref{lower bound white noise})}\label{appendix subsection white noise loss}
We start by the representation of our loss function as a quadratic form.
\begin{proposition} \label{prop white noise loss}
    In the white noise case, the correlation factor $\rho$ is null. The loss $\mathcal{L}_\text{time}(c, d)$ writes 
    \[
    \mathcal{L}_\text{time}(c, d)= 1 + \sum_{k=0}^{+\infty}\vert c_k\vert^2 - 2\textnormal{Re}\big(\sum_{k=0}^{+\infty}c_kd_k\big),
    \]
    where $c_k=\sum_{s=1}^Sa_s^kb_s$. Therefore, the loss writes 
    \[
    \mathcal{L}_\text{time}(c, d) = 1 + \sum_{s, s'}^S\frac{b_s\bar{b}_{s'}}{1-a_s\bar{a}_{s'}} - 2\textnormal{Re}\big(\sum_{s=1}^Sb_sa_s^K\big).
    \]
\end{proposition}

\begin{proof}
    On the one hand, 
    \begin{align*}
        \sum_{k=0}^{+\infty}\vert c_k\vert^2 &= \sum_{k=0}^{+\infty}\big\vert\sum_{s=1}^Sa_s^kb_s\big\vert 
        =\sum_{k=0}^{+\infty}\sum_{s=1}^S\sum_{s'=1}^Sa_s^k\bar{a}_{s'}^kb_sb_{s'} \\
        &= \sum_{s=1}^S\sum_{s'=1}^Sb_sb_{s'}\sum_{k=0}^{+\infty}a_s^k\bar{a}_{s'}^k =\sum_{s=1}^S\sum_{s'=1}^Sb_sb_{s'}\frac{1}{1-a_s\bar{a}_{s'}}.
    \end{align*}
    On the other hand,
    \begin{align*}
        \textnormal{Re}\big(\sum_{k=0}^{+\infty}c_kd_k\big) &= c_Kd_K
        =\sum_{s=1}^Sb_sa_s^K.
    \end{align*}
    Hence the result.
\end{proof}

\begin{proposition}[Performance criterion]
    Minimizing the loss in Proposition \ref{prop white noise loss} boils down to maximizing the following performance criterion
    \[
    F_K = \sum_{s,s'=1}^S\bar{a}_s^K(C^{-1})_{ss'}a_{s'}^K,
    \]
    where $C_{ss'} = \frac{1}{1-a_s\bar{a}_{s'}}$.
\end{proposition}
\begin{proof}
    The loss $\mathcal{L}_\text{time}$ writes 
    \[
    1 + \langle\bar{b}, C\bar{b}\rangle - \langle\bar{b}, a^K\rangle -\langle a^K, \bar{b}\rangle.
    \]
    We thus want to maximize with respect to $a_s$ and $b_s$ the quantity
    \[
    \langle\bar{b}, a^K\rangle +\langle a^K, \bar{b}\rangle - \langle\bar{b}, C\bar{b}\rangle.
    \]
    This is convex and quadratic with respect to $b$, and the minimizer $\bar{b}^*$ is $C^{-1}a^K$, leading to the performance criterion
    \[
    F_K = \langle a^K, C^{-1}a^K\rangle = \sum_{s, s'=1}^S\bar{a}_s^K(C^{-1})_{ss'}a_{s'}^K.
    \]
\end{proof}

We can now move to the proof of Theorem \ref{lower bound white noise}, by first analyzing properties of the matrix $C$.

\paragraph{Linear algebra preview.} We use the similarities with Cauchy matrices and their so-called displacement structure~\citep{yang2003generalized,calvetti1996solution}.

Starting from
$$
C - \Diag( {a}) C \Diag(\bar{a})  = 1_S 1_S^\top,
$$
we get by  post multiplying by $\Diag(\bar{a})^{-1}$,
$$
C\Diag(\bar{a})^{-1}  - \Diag( {a}) C  = 1_S 1_S^\top\Diag(\bar{a})^{-1}
$$
and thus, by pre and post multiplying by $C^{-1}$:
$$
\Diag(\bar{a})^{-1}C^{-1}  - C^{-1}\Diag( {a})    = C^{-1}1_S 1_S^\top\Diag(\bar{a})^{-1}C^{-1},
$$
leading to
$$
\Diag(\bar{a})^{-1}C^{-1}\Diag( {a}) ^{-1}   - C^{-1} = C^{-1}1_S 1_S^\top\Diag(\bar{a})^{-1}C^{-1}\Diag( {a})^{-1}
= u v^*,
$$
with $ u = C^{-1} 1_S$ and $v = \Diag( \bar{a})^{-1}  C^{-1} \Diag(a)^{-1} 1_S$. This leads to a closed form expression for the inverse:
$$
(C^{-1})_{ss'} \big( \frac{1}{ \bar{a}_s a_{s'}}-1 \big) = u_s \bar{v}_{s'}. 
$$
We get
$$
v-u = \big[ \Diag( \bar{a})^{-1}  C^{-1} \Diag(a)^{-1} - C^{-1} \big] 1_S
= u v^\ast 1_S = u 1_S^\top\Diag(\bar{a})^{-1}C^{-1}\Diag( {a})^{-1}1_S,
$$
which leads to $ \ds v  = u ( 1 +1_S^\top\Diag(\bar{a})^{-1}C^{-1}\Diag( {a})^{-1}1_S )$. Moreover we can write
\BEAS
1_S^\top\Diag(\bar{a})^{-1}C^{-1}\Diag( {a})^{-1}1_S 
& = & 1_S^\top ( C^{-1} + u v^\ast) 1_S=  1_S^\top ( C^{-1} + C^{-1} 1_S v^\ast) 1_S\\
& = & 1_S^\top C^{-1} 1_S \cdot (1 + v^\ast 1_S )
\\
& = &  1_S^\top C^{-1} 1_S \cdot (1 + 1_S^\top\Diag(\bar{a})^{-1}C^{-1}\Diag( {a})^{-1}1_S ) , \EEAS
which leads to
$1_S^\top\Diag(\bar{a})^{-1}C^{-1}\Diag( {a})^{-1}1_S   = \frac{ 1_S^\top C^{-1} 1_S }{1- 1_S^\top C^{-1} 1_S }$, and thus
 $$ 1 +1_S^\top\Diag(\bar{a})^{-1}C^{-1}\Diag( {a})^{-1}1_S  = \frac{1}{1-1_S^\top C^{-1} 1_S} =
\frac{1}{1 - u^\top 1_S}.$$
Moreover, we have for any $z \in \mathbb{C}$, if all $a_s$ are distinct:
$$
\sum_{s'=1}^S \frac{ u_{s'}}{1 - z \bar{a}_{s'}} = 1 - \prod_{s'=1}^S \bar{a}_{s'} \prod_{s'=1}^S \frac{ a_{s'} - z}{1 - z \bar{a}_{s'}}
$$
(the two rational functions have the same degrees, the same poles and are equal for $z=a_1,\dots,a_S$),
which leads to for $z=0$,
$$
\sum_{s'=1}^S  { u_{s'}}=1_S^\top C^{-1} 1_S = \sum_{s'=1}^S   u_{s'} = 1 - \prod_{s'=1}^S | {a}_{s'}|^2,
$$
and thus $\ds 1 - 1_S^\top C^{-1} 1 = \prod_{s'=1}^S | {a}_{s'}|^2$.

We have, if $|z|=1$,
$$
\Big|\prod_{s'=1}^S \frac{ a_{s'} - z}{1 - z \bar{a}_{s'}}\Big|
= 1,
$$
which will be used in the bound (such expressions are typically referred to as Blaschke products~\citep{baratchart2016minimax}, and are known to have unit magnitude).


\paragraph{Proof of the lower bound (by upper bounding $F_K$).}
We have, using our linear algebra preview,
$$
F_K = \langle a^K, C^{-1} a^K \rangle
= \sum_{s,s'=1}^S \bar{a}_s^K (C^{-1})_{ss'} a_{s'}^K
= \sum_{s,s'=1}^S (\bar{a}_s  a_{s'})^{K+1} \frac{u_s \bar{v}_{s'} }{1 -  \bar{a}_s a_{s'}}.
$$
We get, using our linear algebra results,
$$
F_K - F_{K+1} = 
\sum_{s,s'=1}^S (\bar{a}_s  a_{s'})^{K+1} (1 -  \bar{a}_s a_{s'})\frac{u_s \bar{v}_{s'} }{1 -  \bar{a}_s a_{s'}}
=  \frac{1}{\prod_{s'=1}^S | {a}_{s'}|^2} \Big| \sum_{s=1}^S \bar{a}_s^{K+1}  u_s \Big|^2.
$$
This leads to 
\BEAS
F_K & = &  \sum_{L=K}^{+\infty}
( F_L - F_{L+1}) = 
\sum_{L=K+1}^{+\infty} \Big| \sum_{s=1}^S \bar{a}_s^L  u_s \Big|^2   \frac{1}{\prod_{s'=1}^S | {a}_{s'}|^2} 
.\EEAS
We have:
\BEAS
\sum_{L=K+1}^{+\infty} \Big| \sum_{s=1}^S \bar{a}_s^L  u_s \Big|^2 
& \leqslant &  \frac{1}{K+1} 
\sum_{L=0}^{+\infty} L \Big| \sum_{s'=1}^S \bar{a}_{s'}^L  u_{s'} \Big|^2 \mbox{ since } 1_{L \geqslant K+1} \leqslant \frac{L}{K+1}.
 \EEAS
 We consider the sequence $\ds w_L =  \sum_{s=1}^S \bar{a}_s^L  u_s$, with Fourier series
 $$
 W(\omega) = \sum_{L=0}^{+\infty} w_L e^{-i \omega L} 
 =  \sum_{s=1}^S \frac {u_s}{1-\bar{a}_s e^{-i \omega }}   =
  1 - \prod_{s'=1}^S \bar{a}_{s'} \prod_{s'=1}^S \frac{ a_{s'} - e^{-i\omega}}{1 -  e^{-i\omega} \bar{a}_{s'}}.
 $$
 We then use Proposition \ref{proposition semi parseval} to write:
 $$
 \sum_{L = 0 }^{+\infty}
 L | w_L|^2 =   \frac{i}{2\pi} \int_0^{2\pi} W'(\omega) \overline{W(\omega)}d\omega
, $$
 leading to
 \BEAS
&&\sum_{L=K+1}^{+\infty} \Big| \sum_{s=1}^S \bar{a}_s^L  u_s \Big|^2\\
& \leqslant &  \frac{1}{K+1} 
  \frac{i}{2\pi   }  \int_0^{2\pi} 
\frac{d}{d\omega} \Big[    - \prod_{s=1}^S \bar{a}_{s} \prod_{s=1}^S \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}
 \Big]
\overline{ \Big(
 1 - \prod_{s'=1}^S \bar{a}_{s'} \prod_{s'=1}^S \frac{ a_{s'} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s'}}
 \Big)}
      d\omega
\\
& = &  \frac{1}{K+1} 
  \frac{i}{2\pi   }  \int_0^{2\pi} 
\frac{d}{d\omega} \Big[      \prod_{s=1}^S \bar{a}_{s} \prod_{s=1}^S \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}
 \Big]
\overline{ \Big(
    \prod_{s'=1}^S \bar{a}_{s'} \prod_{s'=1}^S \frac{ a_{s'} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s'}}
 \Big)}
      d\omega.
\\
      \EEAS
      We now have, by taking derivatives of the product:
     \BEAS
     \frac{d}{d\omega} \Big[       \prod_{s=1}^S \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}
 \Big] & = & 
  \prod_{s=1}^S \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}
  \sum_{s=1}^S \frac{1 - e^{i\omega} \bar{a}_{s}}{ a_{s} - e^{i\omega}} 
   \frac{d}{d\omega} \Big[      \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}\Big]
\\
 & = & 
  \prod_{s=1}^S \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}
  \sum_{s=1}^S \frac{1 - e^{i\omega} \bar{a}_{s}}{ a_{s} - e^{i\omega}} 
   \frac{d}{d\omega} \Big[     \frac{1}{\bar{a}_s} +   \frac{a_s - \frac{1}{\bar{a}_s}}{1 - e^{i\omega} \bar{a}_{s}}\Big]
\\
 & = & 
  \prod_{s=1}^S \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}
  \sum_{s=1}^S \frac{1 - e^{i\omega} \bar{a}_{s}}{ a_{s} - e^{i\omega}} 
  \Big[
(1-|a_s|^2) \frac{ -i e^{i \omega}}{ (1 - e^{i\omega} \bar{a}_{s})^2}\Big]
\\
 & = & 
  \prod_{s=1}^S \frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}
  \sum_{s=1}^S (1-|a_s|^2) \frac{-i  }{ |e^{-i\omega}  - \bar{a}_{s}|^2}
. \EEAS 
      This leads to, using the unit magnitude of $\frac{ a_{s} - e^{i\omega}}{1 - e^{i\omega} \bar{a}_{s}}$,
\BEAS
      F_K
      & \leqslant &  \frac{1}{K+1} 
  \frac{1}{2\pi   } 
   \sum_{s=1}^S ( 1- |a_s|^2) 
  \int_0^{2\pi}  \frac{1}{|a_s - e^{i\omega}|^2}
      d\omega =  \frac{S}{K+1},
\EEAS
using an explicit integration $\ds \frac{1}{2\pi   } 
    \int_0^{2\pi}  \frac{1}{|a_s - e^{i\omega}|^2}
      d\omega = \frac{1}{1-|a_s|^2}$.
    
 The approximation error $\mathcal{L}_\text{time}(c, d)$ is  thus 
$
1 - F_K$, 
which leads to the desired result.

\subsection{Autocorrelated case (Theorem~\ref{theorem 
autocorrelated lower bound})}
\label{proof auto}
We follow the same proof technique as for Theorem~\ref{lower bound white noise}, and compute first an explicit expression of the loss, this time, by introducing a new $a_s$, equal to $\rho$, with the introduction of new weights $w_s = b_s a_s / ( a_s - \rho)$ for $s \in \{1,\dots,S\}$, the weight $w_{S+1}$ being determined by the linear constraint.
\begin{lemma}
    In the autocorrelated case ($\rho \neq 0$), $\mathcal{L}_\text{time}(c, d)$ as in Eq.~\eqref{correlated time domain loss} writes 
    \begin{equation}
    1 - 2(1-\rho^2)\textnormal{Re}\big(\sum_{s=1}^{S+1}\frac{w_sa_s^k}{1-a_s\rho}\big) + (1-\rho^2)\sum_{s, s'}^{S+1}\frac{w_s\bar{w}_{s'}}{1-a_s\bar{a}_{s'}},
    \label{appendix constrained autocorrelated loss}
    \end{equation}    
    where $a_{S+1}=\rho$ and the constraint $\sum_{s=1}^{S+1}w_sa_s^{-1}=0$ holds. 
    \end{lemma}
\begin{proof}
   We aim to minimize 
    \[
    \sum_{k, k'}(c_k-d_k)(c_{k'}-d_{k'})\gamma(k-k'),
    \]
    where $\gamma(k-k')=\rho^{\vert k-k'\vert}$.
    Denoting $C(e^{i\omega}), D(e^{i\omega})$ and $\Gamma(e^{i\omega})$ the Fourier transforms of $(c_n), (d_n)$ and $(\gamma_n)$ respectively, Parseval's theorem yields 
    \[
    \sum_{k, k'}(c_k-d_k)(c_{k'}-d_{k'})\gamma(k-k') = \frac{1}{2\pi}\int_{-\pi}^\pi\big\vert C(e^{i\omega}) - D(e^{i\omega})\big\vert^2\Gamma(e^{i\omega})d\omega.
    \]

    We have $D(e^{i\omega})=e^{-iK\omega}$ (Fourier transform of a shifted Dirac at timestep K), and 
    \begin{align*}
        C(e^{i\omega}) &= \sum_{k=0}^{+\infty}\sum_{s=1}^Sb_sa_s^ke^{-i\omega k} = \sum_{s=1}^S\frac{b_s}{1-a_se^{-i\omega}},\\
        \Gamma(e^{i\omega})&=\sum_{k=-\infty}^{+\infty}\gamma(k)e^{-i\omega k} = \frac{1}{1 - \rho e^{-i\omega}}\frac{1-\rho^2}{1 - \rho e^{i\omega}}.
    \end{align*}
    The criterion becomes (with an error of $1$ if $C=0$):
\BEAS
&&\frac{1}{2\pi} \int_0^{2\pi} | D(e^{i\omega}) - C(e^{i\omega})|^2 \Gamma(e^{i\omega}) d\omega\\
& = & 
\frac{ 1-\rho^2}{2\pi} \int_0^{2\pi} \Big| D(e^{i\omega})\frac{1}{1 - \rho e^{-i\omega}} - C(e^{i\omega}) \frac{1}{1 - \rho e^{-i\omega}}\Big|^2  d\omega \\
& = & 1 - 
\frac{1-\rho^2}{2\pi} 2 {\textnormal{ Re}} \Big(\int_0^{2\pi} 
\overline{D(e^{i\omega})\frac{1}{1 - \rho e^{-i\omega}}}
C(e^{i\omega}) \frac{1}{1 - \rho e^{-i\omega}}
\Big) d\omega   \\
& & \hspace*{2cm} + \frac{1-\rho^2}{2\pi}  \int_0^{2\pi} \Big|C(e^{i\omega}) \frac{1}{1 - \rho e^{-i\omega}}\Big|^2  d\omega.
\EEAS 
We have 
$$
\frac{1}{1- a_se^{-i\omega}}\frac{1}{1 - \rho e^{-i\omega}}
= \frac{1}{a_s-\rho} \Big( \frac{a_s}{1 - a_s e^{-i\omega}} - \frac{\rho}{1 - \rho e^{-i\omega}} \Big),
$$
and thus
\BEAS
C(e^{i\omega}) \frac{1}{1 - \rho e^{-i\omega}}  & = &  \sum_{s=1}^{S}  
\frac{b_s}{a_s-\rho} \Big( \frac{a_s}{1 - a_s e^{-i\omega}} - \frac{\rho}{1 - \rho e^{-i\omega}} \Big) \\
 & = & \sum_{s=1}^{S+1} \frac{w_s  }{1- a_se ^{-i\omega}},
\EEAS
with $w_s = b_s a_s / ( a_s - \rho)$, $a_{S+1} = \rho$, and the constraint $\ds \sum_{s=1}^{S+1} w_sa_s^{-1}  = 0$.
The criterion becomes
\BEAS
& & 1 - 
(1-\rho^2)\sum_{s=1}^{S+1} 2 {\textnormal{Re}} \Big( \frac{w_s a_s^K}{1 - a_s \rho}
 \Big)   +(1-\rho^2) \sum_{s,s'=1}^{S+1} \frac{\bar{w}_s w_{s'}  }{1- a_s \bar{a}_s'},
\EEAS 
after straightforward computations.
\end{proof}

\paragraph{Proof of Theorem \ref{theorem autocorrelated lower bound}.}

The minimum with respect to $w$ in Eq.~\eqref{appendix constrained autocorrelated loss} with the constraint is greater than the unconstrained minimizer, equal to
\BEAS
H_K & =  & 1 - (1-\rho^2)\sum_{s,s'=1}^{S+1} 
\frac{ \bar{a}_s^K}{1 - \bar{a}_s \rho}\frac{ a_{s'}^K}{1 - a_{s'} \rho} (C^{-1})_{ss'},
\EEAS
where we recall that $C_{ss'} = \frac{1}{1-a_s\bar{a}_{s'}}$.

Using linear algebra properties from above with $S+1$ zeros and poles, we get
\BEAS
H_K&= & 1 -  (1-\rho^2)\sum_{s,s'=1}^{S+1}
\frac{ 1}{1 - \bar{a}_s \rho}\frac{1}{1 - a_{s'} \rho}  \frac{(\bar{a}_s a_{s'})^{K+1}u_s \bar{v}_{s'}}{1 - \bar{a}_s a_{s'}} 
\\
& = & 1 - (1-\rho^2)\frac{1}{\prod_{s=1}^{S+1} |a_s|^2 }
\sum_{s,s'=1}^{S+1} 
\frac{ 1}{1 - \bar{a}_s \rho}\frac{1}{1 - a_{s'} \rho}  \frac{(\bar{a}_s a_{s'})^{K+1}u_s \bar{u}_{s'}}{1 - \bar{a}_s a_{s'}} ,
\EEAS
where we recall that $u = C^{-1}1_S$ and $v = \text{Diag}(\bar{a})^{-1}C^{-1}\text{Diag}(a)^{-1}1_S$.

We have
\BEAS
H_{K+1} - H_K 
& = & \frac{1-\rho^2}{\prod_{s=1}^{S+1} |a_s|^2 }
\sum_{s,s'=1}^{S+1} 
\frac{ 1}{1 - \bar{a}_s \rho}\frac{1}{1 - a_{s'} \rho}   (\bar{a}_s a_{s'})^{K+1}u_s \bar{u}_{s'} 
\\
& = & \frac{1-\rho^2}{\prod_{s=1}^{S+1} |a_s|^2 }
\Big| \sum_{s=1}^{S+1}
\frac{ 1}{1 - \bar{a}_s \rho}   \bar{a}_s  ^{K+1}u_s  
\Big|^2,
 \EEAS
 leading to
 \BEAS
 H_K & = & \sum_{L=K}^{+\infty} ( H_L - H_{L+1} ) + 1 \\
 & = & 1 - \frac{1-\rho^2}{\prod_{s=1}^{S+1} |a_s|^2 }
 \sum_{L = K}^{+\infty}
 \Big| \sum_{s=1}^{S+1} 
\frac{ 1}{1 - \bar{a}_s \rho}   \bar{a}_s  ^{L} \bar{a}_su_s  
\Big|^2 \\
& \geqslant & 
1 - \frac{1}{K} \frac{1-\rho^2}{\prod_{s=1}^{S+1} |a_s|^2 }
 \sum_{L = 0 }^{+\infty}
 L\Big| \sum_{s=1}^{S+1}
\frac{ 1}{1 - \bar{a}_s \rho}   \bar{a}_s  ^{L} \bar{a}_s u_s  
\Big|^2 ,
 \EEAS
 using $1_{L \geqslant K} \leqslant \frac{L}{K}$.
 
 The sequence $\ds w_L = \sum_{s=1}^{S+1} 
\frac{ 1}{1 - \bar{a}_s \rho}   \bar{a}_s  ^{L} \bar{a}_su_s  $, has Fourier series
\BEAS
W(\omega) & = &  \sum_{L=0}^{+\infty} w_L e^{-i\omega L}
= \sum_{L=0}^{+\infty}   e^{-i\omega L}\sum_{s=1}^{S+1} 
\frac{ 1}{1 - \bar{a}_s \rho}   \bar{a}_s  ^{L}\bar{a}_s u_s \\
& = & \sum_{s=1}^{S+1}
\frac{ 1}{1 - \bar{a}_s \rho}   \frac{\bar{a}_s u_s}{1 - \bar{a}_s e^{-i\omega}} 
= \sum_{s=1}^{S+1} u_s \Big( 
\frac{ 1}{1 - \bar{a}_s \rho}   - \frac{1}{1 - \bar{a}_s e^{-i\omega}} \Big)\frac{1}{  \rho - e^{-i\omega}} \\
& = & 
\frac{1}{  \rho - e^{-i\omega}} \Big( \prod_{s=1}^{S+1} \bar{a}_s\Big) \Big( \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}
-\prod_{s=1}^{S+1} \frac{a_s - \rho }{1- \rho \bar{a}_s}
\Big)
\\
 & = & 
\frac{1}{  \rho - e^{-i\omega}} \Big(\prod_{s=1}^{S+1} \bar{a}_s \Big) \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s},
 \EEAS
 because of the link between $u,C$ and rational functions.
 
We have:
\BEAS
 1 - H_K
 & \leqslant & 
 \frac{1-\rho^2}{K} \frac{1}{\prod_{s=1}^{S+1} |a_s|^2 }
 \sum_{L = 0 }^{+\infty}
 L | w_L|^2 \\
 & = & \frac{1-\rho^2}{K} \frac{1}{\prod_{s=1}^{S+1} |a_s|^2 }
\frac{i}{2\pi} \int_0^{2\pi} W'(\omega) \overline{W(\omega)}d\omega
\ \mbox{ using properties of Fourier Series,} \\
 & = & \frac{1-\rho^2}{K}  
\frac{i}{2\pi} \int_0^{2\pi} \frac{d}{d\omega} \Big(
\frac{1}{  \rho - e^{-i\omega}}   \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}
\Big)
\overline{\frac{1}{  \rho - e^{-i\omega}}   \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}}d\omega
\\
 & = & \frac{1-\rho^2}{K}  
\frac{i}{2\pi} \int_0^{2\pi}  
\frac{-i e^{-i\omega}}{  (\rho - e^{-i\omega})^2}   \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}
\overline{\frac{1}{  \rho - e^{-i\omega}}   \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}}d\omega
\\
& & \hspace*{1cm} + 
\frac{1-\rho^2}{K}  
\frac{i}{2\pi} \int_0^{2\pi} 
\frac{1}{  \rho - e^{-i\omega}} \frac{d}{d\omega} \Big(   \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}
\Big)
\overline{\frac{1}{  \rho - e^{-i\omega}}   \prod_{s=1}^{S+1} \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}}d\omega.
\EEAS
Using the following identities,
\BEAS
 \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}
& = & \frac{1}{\bar{a}_s} + \frac{a_s - 1/ \bar{a}_s}{1-e^{-i\omega} \bar{a}_s}, \\
\frac{d}{d\omega} \Big( \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}
\Big) & = & \frac{a_s - 1/ \bar{a}_s}{(1-e^{-i\omega} \bar{a}_s)^2} \bar{a}_s (-i e^{-i\omega})
=  i e^{-i\omega}\frac{1-|a_s|^2 }{(1-e^{-i\omega} \bar{a}_s)^2}, \\
\Big| \frac{a_s - e^{-i\omega}}{1-e^{-i\omega} \bar{a}_s}
\Big|  & = & 1, \EEAS
we get
\BEAS
 1 - H_K & \leqslant & \frac{1-\rho^2}{K}  
\frac{1}{2\pi} \int_0^{2\pi}  
\frac{e^{-i\omega}}{  (\rho - e^{-i\omega})^2 (\rho - e^{i\omega})}     d\omega
\\
& & \hspace*{4cm} +
\frac{1-\rho^2}{K}  
\sum_{s=1}^{S+1}  ( 1- |a_s|^2) 
\frac{1}{2\pi} \int_0^{2\pi} 
\frac{1}{  |\rho - e^{-i\omega}|^2}  
 \frac{1}{|a_s - e^{-i\omega}|^2} d\omega
\\
& = & \frac{1-\rho^2}{K}  
\frac{1}{2\pi} \int_0^{2\pi}  
\frac{e^{-i\omega}}{  (\rho - e^{-i\omega})^2 (\rho - e^{i\omega})}     d\omega
\\
& & \hspace*{-1cm}
+
\frac{(1-\rho^2)^2}{K}  
\frac{1}{2\pi} \int_0^{2\pi} 
\frac{1}{  |\rho - e^{-i\omega}|^4}  d\omega
 +
\frac{1-\rho^2}{K}  
\sum_{s=1}^{S}  ( 1- |a_s|^2) 
\frac{1}{2\pi} \int_0^{2\pi} 
\frac{1}{  |\rho - e^{-i\omega}|^2}  
 \frac{1}{|a_s - e^{-i\omega}|^2} d\omega
\\
& = & - \frac{1}{K} \frac{1}{1-\rho^2}   +
 \frac{1}{K} \frac{1+\rho^2}{1-\rho^2}   + 
\frac{1-\rho^2}{K}  
\sum_{s=1}^{S}  ( 1- |a_s|^2) 
\frac{1}{2\pi} \int_0^{2\pi} 
\frac{1}{  |\rho - e^{-i\omega}|^2}  
 \frac{1}{|a_s - e^{-i\omega}|^2} d\omega  
 \EEAS
 by exact integration. Then, using $\ds\frac{1}{  |\rho - e^{-i\omega}|^2}   \leqslant \frac{1}{(1-\rho)^2}$, 
 and $\ds\frac{1}{2\pi} \int_0^{2\pi} 
  \frac{1}{|a_s - e^{-i\omega}|^2} d\omega  = \frac{1}{1-|a_s|^2}$, we get
  \BEAS
1-H_K
& \leqslant &  
 \frac{1}{K} \frac{\rho^2}{1-\rho^2}   + 
\frac{1+\rho}{1-\rho}\frac{S}{K}  \leqslant  \frac{1}{K} \frac{\rho}{1-\rho}   + 
\frac{2}{1-\rho}\frac{S}{K}
= \frac{1}{K} \frac{1}{1-\rho} ( \rho + 2 S).
 \EEAS
Thus, we get an approximation error greater than
$\displaystyle
\Big( 1 - \frac{1}{K} \frac{3S}{1-\rho} \Big)_+.
$ (since it is always nonnegative).
