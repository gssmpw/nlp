
\section{Recurrent Neural Networks and Diagonal forms}

\label{RNN basics}

 

Linear recurrent networks such as SSMs, in their simplest form, are causal models acting on a $d$ dimensional input sequence with $L$ elements $U\in\mathbb{R}^{d\times L}$, producing an output sequence $Y\in\mathbb{R}^{d\times L}$ through a filtering process parametrized by variables $A\in\mathbb{R}^{N\times N}$, $B\in\mathbb{R}^{N\times d}, P\in\mathbb{R}^{d\times N}$. Let $U_n\in\mathbb{R}^{d}$ denote the $n$-th timestamp data contained in $U$, a linear RNN processes the inputs as follows~\citep{gu2022parameterization,orvieto2023resurrecting}
\begin{equation}
    X_{n} = A X_{n-1} + B U_n,\qquad Y_{n} = PX_n.
    \label{eq:appendix linear_RNN}
\end{equation}

\begin{proposition}[Linear RNNs and convolution form]
    Let $A\in\mathbb{R}^{S\times S}$ such that $A$ is diagonal, $B\in\mathbb{R}^{S\times 1}, P\in\mathbb{R}^{1\times S}$, and $u = (u_n)_{n\in\mathbb{Z}}$ be a univariate input signal. The output signal $(y_n)_{n\in\mathbb{Z}}$ can write 
    \[
    y_n = \sum_{k=0}^\infty c_ku_{n-k}
    \]
    with $c_k = \sum_{s=1}^Sa_s^kb_s$.
\end{proposition}

\begin{proof}
    We have $A = \begin{pmatrix}
        a_1 & \dots & \\
        & \ddots & \\
        & \dots & a_S
    \end{pmatrix}, B = \begin{pmatrix}
        b_1\\
        \vdots\\
        b_S
    \end{pmatrix}, P = \begin{pmatrix}
        p_1 & \dots & p_S
    \end{pmatrix}$.

\begin{align*}
    X_n &= AX_{n-1} + Bu_n\\
    &= A(AX_{n-2} + Bu_{n-1}) + Bu_n  = \dots = \sum_{k=0}^nA^kBu_{n-k}\\
    &= \sum_{k=0}^n\begin{pmatrix}
        a_1^k & \dots & \\
        & \ddots & \\
        & \dots & a_S^k
    \end{pmatrix}\begin{pmatrix}
        b_1\\\vdots\\b_S
    \end{pmatrix}u_{n-k} = \sum_{k=0}^n\begin{pmatrix}
        a_1^kb_1\\\vdots \\a_S^kb_s
    \end{pmatrix}u_{n-k}.
\end{align*}
Finally,
\begin{align*}
    y_n = \begin{pmatrix}
        p_1 & \dots & p_S
    \end{pmatrix}X_n = \sum_{k=0}^n\begin{pmatrix}
        p_1 & \dots & p_S
    \end{pmatrix}\begin{pmatrix}
        a_1^kb_1\\\vdots\\a_N^kb_N
    \end{pmatrix}u_{n-k} & = \sum_{s=1}^S\sum_{k=0}^np_sa_s^kb_su_{n-k}\\
    &=\sum_{k=0}^nu_{n-k}\sum_{s=1}^Sa_s^kb_sp_s = \sum_{k=0}^nu_{n-k}c_k,
\end{align*}
with $c_k = \sum_{s=1}^Sa_s^kb_sp_s$. In this paper, we consider without loss of generality $\begin{pmatrix}
p_1 \dots p_s
\end{pmatrix} = \begin{pmatrix}
    1 \dots 1
\end{pmatrix}.$
\end{proof}