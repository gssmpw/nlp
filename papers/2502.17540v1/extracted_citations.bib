@article{DBLP:conf/aaai/TanakaNNHSS23,
  title={SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images},
  volume={37},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/26598},
  DOI={10.1609/aaai.v37i11.26598},
  number={11},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
  year={2023},
  month={Jun.},
  pages={13636-13645} }

@inproceedings{DBLP:conf/acl/0005RSMBKPNL24,
    title = "{D}oc{LLM}: A Layout-Aware Generative Language Model for Multimodal Document Understanding",
    author = "Wang, Dongsheng  and
      Raman, Natraj  and
      Sibue, Mathieu  and
      Ma, Zhiqiang  and
      Babkin, Petr  and
      Kaur, Simerjot  and
      Pei, Yulong  and
      Nourbakhsh, Armineh  and
      Liu, Xiaomo",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.463/",
    doi = "10.18653/v1/2024.acl-long.463",
    pages = "8529--8548",
}

@inproceedings{DBLP:conf/acl/LevSHJK19,
    title = "{T}alk{S}umm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks",
    author = "Lev, Guy  and
      Shmueli-Scheuer, Michal  and
      Herzig, Jonathan  and
      Jerbi, Achiya  and
      Konopnicki, David",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1204/",
    doi = "10.18653/v1/P19-1204",
    pages = "2125--2131",
}

@inproceedings{DBLP:conf/acl/LiuLYZJLH24,
    title = "{S}um{S}urvey: An Abstractive Dataset of Scientific Survey Papers for Long Document Summarization",
    author = "Liu, Ran  and
      Liu, Ming  and
      Yu, Min  and
      Zhang, He  and
      Jiang, Jianguo  and
      Li, Gang  and
      Huang, Weiqing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.574/",
    doi = "10.18653/v1/2024.findings-acl.574",
    pages = "9632--9651",
  
}

@inproceedings{DBLP:conf/coling/PuWLD24,
    title = "{S}ci{N}ews: From Scholarly Complexities to Public Narratives {--} a Dataset for Scientific News Report Generation",
    author = "Liu, Dongqi  and
      Wang, Yifan  and
      Loy, Jia  and
      Demberg, Vera",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1258/",
    pages = "14429--14444",
}

@inproceedings{DBLP:conf/cvpr/LuoSZZYY24,
  author       = {Chuwei Luo and
                  Yufan Shen and
                  Zhaoqing Zhu and
                  Qi Zheng and
                  Zhi Yu and
                  Cong Yao},
  title        = {LayoutLLM: Layout Instruction Tuning with Large Language Models for
                  Document Understanding},
  booktitle    = {{CVPR}},
  pages        = {15630--15640},
  publisher    = {{IEEE}},
  year         = {2024},
  URL = {https://openaccess.thecvf.com/content/CVPR2024/html/Luo_LayoutLLM_Layout_Instruction_Tuning_with_Large_Language_Models_for_Document_CVPR_2024_paper.html}
}

@inproceedings{DBLP:conf/emnlp/CacholaLCW20,
    title = "{TLDR}: Extreme Summarization of Scientific Documents",
    author = "Cachola, Isabel  and
      Lo, Kyle  and
      Cohan, Arman  and
      Weld, Daniel",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.428/",
    doi = "10.18653/v1/2020.findings-emnlp.428",
    pages = "4766--4777",
}

@inproceedings{DBLP:conf/emnlp/JuLKJDP21,
    title = "Leveraging Information Bottleneck for Scientific Document Summarization",
    author = "Ju, Jiaxin  and
      Liu, Ming  and
      Koh, Huan Yee  and
      Jin, Yuan  and
      Du, Lan  and
      Pan, Shirui",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.345/",
    doi = "10.18653/v1/2021.findings-emnlp.345",
    pages = "4091--4098",
}

@inproceedings{DBLP:conf/naacl/SotudehG22,
    title = "{TSTR}: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation",
    author = "Sotudeh, Sajad  and
      Goharian, Nazli",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.25/",
    doi = "10.18653/v1/2022.naacl-main.25",
    pages = "325--335",
}

@inproceedings{DBLP:conf/naacl/TakeshitaGR0P24,
    title = "{ACLS}um: A New Dataset for Aspect-based Summarization of Scientific Publications",
    author = "Takeshita, Sotaro  and
      Green, Tommaso  and
      Reinig, Ines  and
      Eckert, Kai  and
      Ponzetto, Simone",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.371/",
    doi = "10.18653/v1/2024.naacl-long.371",
    pages = "6660--6675",
}

@inproceedings{DBLP:conf/nips/AlayracDLMBHLMM22,
  author       = {Jean{-}Baptiste Alayrac and
                  Jeff Donahue and
                  Pauline Luc and
                  Antoine Miech and
                  Iain Barr and
                  Yana Hasson and
                  Karel Lenc and
                  Arthur Mensch and
                  Katherine Millican and
                  Malcolm Reynolds and
                  Roman Ring and
                  Eliza Rutherford and
                  Serkan Cabi and
                  Tengda Han and
                  Zhitao Gong and
                  Sina Samangooei and
                  Marianne Monteiro and
                  Jacob L. Menick and
                  Sebastian Borgeaud and
                  Andy Brock and
                  Aida Nematzadeh and
                  Sahand Sharifzadeh and
                  Mikolaj Binkowski and
                  Ricardo Barreira and
                  Oriol Vinyals and
                  Andrew Zisserman and
                  Kar{\'{e}}n Simonyan},
  title        = {Flamingo: a Visual Language Model for Few-Shot Learning},
  url={https://openreview.net/forum?id=EbMuimAbPbs},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@inproceedings{DBLP:conf/nips/LiuLWL23a,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34892--34916},
 publisher = {Curran Associates, Inc.},
 title = {Visual Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{DBLP:journals/corr/abs-2311-03079,
  author = {Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and XiXuan, Song and Xu, Jiazheng and Chen, Keqin and Xu, Bin and Li, Juanzi and Dong, Yuxiao and Ding, Ming and Tang, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {121475--121499},
 publisher = {Curran Associates, Inc.},
 title = {CogVLM: Visual Expert for Pretrained Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/dc06d4d2792265fb5454a6092bfd5c6a-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@inproceedings{DocFormerv2,
  author       = {Srikar Appalaraju and
                  Peng Tang and
                  Qi Dong and
                  Nishant Sankaran and
                  Yichu Zhou and
                  R. Manmatha},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {DocFormerv2: Local Features for Document Understanding},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {709--718},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i2.27828},
  doi          = {10.1609/AAAI.V38I2.27828},
  timestamp    = {Tue, 02 Apr 2024 16:32:08 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/AppalarajuTDSZM24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ERNIE,
  author       = {Qiming Peng and
                  Yinxu Pan and
                  Wenjin Wang and
                  Bin Luo and
                  Zhenyu Zhang and
                  Zhengjie Huang and
                  Yuhui Cao and
                  Weichong Yin and
                  Yongfeng Chen and
                  Yin Zhang and
                  Shikun Feng and
                  Yu Sun and
                  Hao Tian and
                  Hua Wu and
                  Haifeng Wang},
  editor       = {Yoav Goldberg and
                  Zornitsa Kozareva and
                  Yue Zhang},
  title        = {ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich
                  Document Understanding},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},
  pages        = {3744--3756},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.findings-emnlp.274},
  doi          = {10.18653/V1/2022.FINDINGS-EMNLP.274},
  timestamp    = {Wed, 13 Nov 2024 15:45:04 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/PengPWLZHCYCZFS22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ScisummNet,
  author       = {Michihiro Yasunaga and
                  Jungo Kasai and
                  Rui Zhang and
                  Alexander R. Fabbri and
                  Irene Li and
                  Dan Friedman and
                  Dragomir R. Radev},
  title        = {ScisummNet: {A} Large Annotated Corpus and Content-Impact Models for
                  Scientific Paper Summarization with Citation Networks},
  booktitle    = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2019, The Thirty-First Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii,
                  USA, January 27 - February 1, 2019},
  pages        = {7386--7393},
  publisher    = {{AAAI} Press},
  year         = {2019},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4727},
  doi          = {10.1609/AAAI.V33I01.33017386},
  timestamp    = {Mon, 04 Sep 2023 12:29:24 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/YasunagaKZFLFR19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{m3av,
  author       = {Zhe Chen and
                  Heyang Liu and
                  Wenyi Yu and
                  Guangzhi Sun and
                  Hongcheng Liu and
                  Ji Wu and
                  Chao Zhang and
                  Yu Wang and
                  Yanfeng Wang},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {M{\({^3}\)}AV: {A} Multimodal, Multigenre, and Multipurpose Audio-Visual
                  Academic Lecture Dataset},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {9041--9060},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.acl-long.489/},
  doi          = {10.18653/V1/2024.ACL-LONG.489},
  timestamp    = {Sun, 19 Jan 2025 13:22:06 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/ChenLYSLWZWW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{masry-etal-2022-chartqa,
    title = "{C}hart{QA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
    author = "Masry, Ahmed  and
      Do, Xuan Long  and
      Tan, Jia Qing  and
      Joty, Shafiq  and
      Hoque, Enamul",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.177/",
    doi = "10.18653/v1/2022.findings-acl.177",
    pages = "2263--2279",
    abstract = "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions."
}

@misc{yao2024minicpmvgpt4vlevelmllm,
      title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone}, 
      author={Yuan Yao and Tianyu Yu and Ao Zhang and Chongyi Wang and Junbo Cui and Hongji Zhu and Tianchi Cai and Haoyu Li and Weilin Zhao and Zhihui He and Qianyu Chen and Huarong Zhou and Zhensheng Zou and Haoye Zhang and Shengding Hu and Zhi Zheng and Jie Zhou and Jie Cai and Xu Han and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2408.01800},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.01800}, 
}
misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya et al.},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@inproceedings{zheng-etal-2024-multimodal,
    title = "Multimodal Table Understanding",
    author = "Zheng, Mingyu  and
      Feng, Xinwei  and
      Si, Qingyi  and
      She, Qiaoqiao  and
      Lin, Zheng  and
      Jiang, Wenbin  and
      Wang, Weiping",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.493/",
    doi = "10.18653/v1/2024.acl-long.493",
    pages = "9102--9124",
    abstract = "Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings."
}

