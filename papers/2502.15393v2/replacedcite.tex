\section{Related Work}
\label{sec:rela_work}
\subsection{Large Multimodal Model}
Recent advancements in large language models (LLMs)____ have demonstrated impressive language understanding and generation capabilities. 
This success has sparked interest in large multimodal models (LMMs)____, which typically consist of visual encoders, modality projectors, and pretrained language model decoders. 
LMMs initially made breakthroughs in image understanding tasks. 
With the construction of high-quality video-text datasets, more researchers are applying LMMs to video understanding tasks____. 
For example, models like VideoChatGPT____, VideoChat____, and Video-LLaMA____ have enhanced the video understanding capabilities of LMMs through high-quality data and fine-tuning techniques.  
These models have shown excellent performance in short video understanding. 
Recently, some efforts____ have been made to input long videos into LMMs, achieving some progress. 
For instance, MovieChat____ introduced a memory mechanism to compress long video tokens into a fixed size. 
Additionally, LongVA____ extended the context window by continuously training LLMs on long texts. 
Although they perform well in long video-QA tasks, they face challenges in generating video captions that require global descriptions. 
For a 20-minute video, they struggle to output even 300 words of description. 
This limits the application of the model in video understanding. 

\subsection{Video Captioning}
Early video captioning methods used template-based approaches____, which lacked flexibility. 
With the development of deep learning, expert models based on CNN-RNN and Transformer architectures replaced previous methods____. However, these approaches typically handle only short videos of a few seconds, and the generated captions are also brief. 
The VideoReCap____ model was the first to attempt generating descriptions for long videos recursively. 
However, its descriptions often do not exceed 100 words for a 60-minute video, inevitably missing much of the video content. 
Dense video captioning____ typically identifies different event timestamps within a video and generates corresponding captions for each event. 
However, these methods still focus on short video-short caption scenarios, with annotated captions generally limited to no more than 30 words. 
Recently, some efforts____ have attempted to combine LMMs and LLMs to construct large-scale video captioning datasets. 
For instance, Panda70M____ constructed semantically consistent videos by splitting and merging based on semantic understanding, and then used a pre-trained model to generate captions for each video. 
ShareGPT4Video____ proposed a differential video captioning strategy, leveraging GPT-4V____ to synthesize video captions by identifying differences between adjacent frames. 
However, these methods typically focus on short video-caption examples.