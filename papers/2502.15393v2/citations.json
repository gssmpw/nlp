[
  {
    "index": 0,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "{LLaMA} 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral {7B}"
      },
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others",
        "title": "Qwen2 technical report"
      },
      {
        "key": "Claude3",
        "author": "Anthropic",
        "title": "Claude 3"
      },
      {
        "key": "gpt4o",
        "author": "OpenAI",
        "title": "{GPT-4o}"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "{GPT}-4 technical report"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2024llava",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan",
        "title": "{LLaVA-OneVision}: Easy Visual Task Transfer"
      },
      {
        "key": "Dai2023InstructBLIPTG",
        "author": "Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Albert Li and Pascale Fung and Steven C. H. Hoi",
        "title": "{InstructBLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning"
      },
      {
        "key": "Ye2023mPLUGOwlME",
        "author": "Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yi Zhou and Junyan Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qiang Qi and Ji Zhang and Feiyan Huang",
        "title": "{mPLUG-Owl}: Modularization Empowers Large Language Models with Multimodality"
      },
      {
        "key": "yao2024minicpm",
        "author": "Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others",
        "title": "{MiniCPM-V}: A {GPT-4V} level {MLLM} on your phone"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2024long",
        "author": "Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei",
        "title": "Long context transfer from language to vision"
      },
      {
        "key": "xue2024longvila",
        "author": "Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others",
        "title": "{LongVILA}: Scaling Long-Context Visual Language Models for Long Videos"
      },
      {
        "key": "liu2024kangaroo",
        "author": "Liu, Jiajun and Wang, Yibing and Ma, Hanghang and Wu, Xiaoping and Ma, Xiaoqi and Wei, Xiaoming and Jiao, Jianbin and Wu, Enhua and Hu, Jie",
        "title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input"
      },
      {
        "key": "li2024llava",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan",
        "title": "{LLaVA-OneVision}: Easy Visual Task Transfer"
      },
      {
        "key": "li2023llamavid",
        "author": "Li, Yanwei and Wang, Chengyao and Jia, Jiaya",
        "title": "{LLaMA-VID}: An image is worth 2 tokens in large language models"
      },
      {
        "key": "jin2023chat-univi",
        "author": "Jin, Peng and Takanobu, Ryuichi and Zhang, Caiwan and Cao, Xiaochun and Yuan, Li",
        "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding"
      },
      {
        "key": "song2024moviechat",
        "author": "Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others",
        "title": "{MovieChat}: From dense token to sparse memory for long video understanding"
      },
      {
        "key": "xu2024pllava",
        "author": "Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and Ng, See Kiong and Feng, Jiashi",
        "title": "{PLLaVA}: Parameter-free llava extension from images to videos for video dense captioning"
      },
      {
        "key": "lin2023video",
        "author": "Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li",
        "title": "{Video-LLaVA}: Learning united visual representation by alignment before projection"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "0001RKK24",
        "author": "Muhammad Maaz and\nHanoona Abdul Rasheed and\nSalman Khan and\nFahad Khan",
        "title": "{Video-ChatGPT}: Towards Detailed Video Understanding via Large Vision and Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023videochat",
        "author": "Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu",
        "title": "{VideoChat}: Chat-centric video understanding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "cheng2024videollama",
        "author": "Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others",
        "title": "{VideoLLaMA 2}: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2024long",
        "author": "Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei",
        "title": "Long context transfer from language to vision"
      },
      {
        "key": "wei2024visual",
        "author": "Wei, Hongchen and Chen, Zhenzhong",
        "title": "Visual Context Window Extension: A New Perspective for Long Video Understanding"
      },
      {
        "key": "xue2024longvila",
        "author": "Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others",
        "title": "{LongVILA}: Scaling Long-Context Visual Language Models for Long Videos"
      },
      {
        "key": "liu2024kangaroo",
        "author": "Liu, Jiajun and Wang, Yibing and Ma, Hanghang and Wu, Xiaoping and Ma, Xiaoqi and Wei, Xiaoming and Jiao, Jianbin and Wu, Enhua and Hu, Jie",
        "title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "song2024moviechat",
        "author": "Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others",
        "title": "{MovieChat}: From dense token to sparse memory for long video understanding"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2024long",
        "author": "Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei",
        "title": "Long context transfer from language to vision"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "kojima2002natural",
        "author": "Kojima, Atsuhiro and Tamura, Takeshi and Fukunaga, Kunio",
        "title": "Natural language description of human activities from video images based on concept hierarchy of actions"
      },
      {
        "key": "guadarrama2013youtube2text",
        "author": "Guadarrama, Sergio and Krishnamoorthy, Niveda and Malkarnenkar, Girish and Venugopalan, Subhashini and Mooney, Raymond and Darrell, Trevor and Saenko, Kate",
        "title": "{Youtube2Text}: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition"
      },
      {
        "key": "krishnamoorthy2013generating",
        "author": "Krishnamoorthy, Niveda and Malkarnenkar, Girish and Mooney, Raymond and Saenko, Kate and Guadarrama, Sergio",
        "title": "Generating natural-language video descriptions using text-mined knowledge"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gao2020fused",
        "author": "Gao, Lianli and Wang, Xuanhan and Song, Jingkuan and Liu, Yang",
        "title": "Fused {GRU} with semantic-temporal attention for video captioning"
      },
      {
        "key": "hu2019hierarchical",
        "author": "Hu, Yaosi and Chen, Zhenzhong and Zha, Zheng-Jun and Wu, Feng",
        "title": "Hierarchical global-local temporal modeling for video captioning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "islam2024video",
        "author": "Islam, Md Mohaiminul and Ho, Ngan and Yang, Xitong and Nagarajan, Tushar and Torresani, Lorenzo and Bertasius, Gedas",
        "title": "{Video ReCap}: Recursive Captioning of Hour-Long Videos"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "YangNSMPLSS23",
        "author": "Antoine Yang and\nArsha Nagrani and\nPaul Hongsuck Seo and\nAntoine Miech and\nJordi Pont{-}Tuset and\nIvan Laptev and\nJosef Sivic and\nCordelia Schmid",
        "title": "{Vid2Seq}: Large-Scale Pretraining of a Visual Language Model for Dense\nVideo Captioning"
      },
      {
        "key": "KimKMC024",
        "author": "Minkuk Kim and\nHyeon Bae Kim and\nJinyoung Moon and\nJinwoo Choi and\nSeong Tae Kim",
        "title": "Do You Remember? {Dense} Video Captioning with Cross-Modal Memory Retrieval"
      },
      {
        "key": "WangZLZC021",
        "author": "Teng Wang and\nRuimao Zhang and\nZhichao Lu and\nFeng Zheng and\nRan Cheng and\nPing Luo",
        "title": "End-to-End Dense Video Captioning with Parallel Decoding"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "chen2024panda",
        "author": "Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others",
        "title": "Panda-70m: Captioning 70m videos with multiple cross-modality teachers"
      },
      {
        "key": "chen2024sharegpt4video",
        "author": "Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others",
        "title": "Sharegpt4video: Improving video understanding and generation with better captions"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "chen2024panda",
        "author": "Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others",
        "title": "Panda-70m: Captioning 70m videos with multiple cross-modality teachers"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "chen2024sharegpt4video",
        "author": "Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others",
        "title": "Sharegpt4video: Improving video understanding and generation with better captions"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "{GPT}-4 technical report"
      }
    ]
  }
]