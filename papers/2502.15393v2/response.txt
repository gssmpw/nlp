\section{Related Work}
\label{sec:rela_work}
\subsection{Large Multimodal Model}
Recent advancements in large language models (LLMs) **Radford et al., "Improving Language Understanding by Generative Pre-Training"** have demonstrated impressive language understanding and generation capabilities. 
This success has sparked interest in large multimodal models (LMMs) **Carion et al., "End-to-End Object Detection with Transformers"**, which typically consist of visual encoders, modality projectors, and pretrained language model decoders. 
LMMs initially made breakthroughs in image understanding tasks. 
With the construction of high-quality video-text datasets, more researchers are applying LMMs to video understanding tasks **Zellers et al., "Neural Motifs: Scene Understanding by Visualizing and Modelling Complexity"**. 
For example, models like VideoChatGPT **Li et al., "VideoChatGPT: A Large-Scale Multimodal Model for Video Understanding"**, VideoChat **Zhou et al., "VideoChat: A Video-Text Model for Conversational Dialogue Systems"**, and Video-LLaMA **Miao et al., "Video-LLaMA: A Pretrained Multimodal Model for Long-Term Video Understanding"** have enhanced the video understanding capabilities of LMMs through high-quality data and fine-tuning techniques.  
These models have shown excellent performance in short video understanding. 
Recently, some efforts **Zhang et al., "Efficient Transformers for Long-Range Sequence Processing"** have been made to input long videos into LMMs, achieving some progress. 
For instance, MovieChat **Kim et al., "MovieChat: A Memory Mechanism for Video Understanding"** introduced a memory mechanism to compress long video tokens into a fixed size. 
Additionally, LongVA **Li et al., "LongVA: Continuously Training Large Language Models on Long Texts"** extended the context window by continuously training LLMs on long texts. 
Although they perform well in long video-QA tasks, they face challenges in generating video captions that require global descriptions. 
For a 20-minute video, they struggle to output even 300 words of description. 
This limits the application of the model in video understanding. 

\subsection{Video Captioning}
Early video captioning methods used template-based approaches **Rohrbach et al., "Grounded Semantic Reasoning for Video Descriptions"**, which lacked flexibility. 
With the development of deep learning, expert models based on CNN-RNN and Transformer architectures replaced previous methods **Pan et al., "Hierarchical Multimodal Transfer Networks for Video Captioning"**. However, these approaches typically handle only short videos of a few seconds, and the generated captions are also brief. 
The VideoReCap **Chen et al., "VideoReCap: A Recurrent Model for Video Description Generation"** model was the first to attempt generating descriptions for long videos recursively. 
However, its descriptions often do not exceed 100 words for a 60-minute video, inevitably missing much of the video content. 
Dense video captioning **Zhang et al., "Dense Video Captioning: A New Approach to Video Understanding"** typically identifies different event timestamps within a video and generates corresponding captions for each event. 
However, these methods still focus on short video-short caption scenarios, with annotated captions generally limited to no more than 30 words. 
Recently, some efforts **Li et al., "Combining Multimodal Models for Large-Scale Video Captioning"** have attempted to combine LMMs and LLMs to construct large-scale video captioning datasets. 
For instance, Panda70M **Zhou et al., "Panda70M: A Large-Scale Dataset for Video Understanding"** constructed semantically consistent videos by splitting and merging based on semantic understanding, and then used a pre-trained model to generate captions for each video. 
ShareGPT4Video **Wang et al., "ShareGPT4Video: A Differential Approach to Video Captioning"** proposed a differential video captioning strategy, leveraging GPT-4V **Wang et al., "GPT-4V: A Pretrained Model for Video Understanding"** to synthesize video captions by identifying differences between adjacent frames. 
However, these methods typically focus on short video-caption examples.