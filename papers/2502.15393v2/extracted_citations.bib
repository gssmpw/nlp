@inproceedings{0001RKK24,
  author       = {Muhammad Maaz and
                  Hanoona Abdul Rasheed and
                  Salman Khan and
                  Fahad Khan},
  title        = {{Video-ChatGPT}: Towards Detailed Video Understanding via Large Vision and Language Models},
  booktitle    = {ACL},
  address = {Bangkok, Thailand},
  pages        = {12585--12602},
  year         = {2024}
}

@misc{Claude3,
    title={Claude 3},
    howpublished={\url{https://www.anthropic.com/news/claude-3-family}},
    author={Anthropic},
    month={March},
    year={2024}
}

@article{Dai2023InstructBLIPTG,
  title={{InstructBLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Albert Li and Pascale Fung and Steven C. H. Hoi},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.06500},
}

@inproceedings{KimKMC024,
  author       = {Minkuk Kim and
                  Hyeon Bae Kim and
                  Jinyoung Moon and
                  Jinwoo Choi and
                  Seong Tae Kim},
  title        = {Do You Remember? {Dense} Video Captioning with Cross-Modal Memory Retrieval},
  booktitle    = {CVPR},
  pages        = {13894--13904},
  address = {Seattle, WA, USA},
  year         = {2024}
}

@inproceedings{WangZLZC021,
  author       = {Teng Wang and
                  Ruimao Zhang and
                  Zhichao Lu and
                  Feng Zheng and
                  Ran Cheng and
                  Ping Luo},
  title        = {End-to-End Dense Video Captioning with Parallel Decoding},
  booktitle    = {ICCV},
  pages        = {6827--6837},
  address = {Montreal, QC, Canada},
  year         = {2021}
}

@inproceedings{YangNSMPLSS23,
  author       = {Antoine Yang and
                  Arsha Nagrani and
                  Paul Hongsuck Seo and
                  Antoine Miech and
                  Jordi Pont{-}Tuset and
                  Ivan Laptev and
                  Josef Sivic and
                  Cordelia Schmid},
  title        = {{Vid2Seq}: Large-Scale Pretraining of a Visual Language Model for Dense
                  Video Captioning},
  booktitle    = {CVPR},
  pages        = {10714--10726},
  address = {Vancouver, BC, Canada},
  year         = {2023}
}

@article{Ye2023mPLUGOwlME,
  title={{mPLUG-Owl}: Modularization Empowers Large Language Models with Multimodality},
  author={Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yi Zhou and Junyan Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qiang Qi and Ji Zhang and Feiyan Huang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.14178},
}

@article{achiam2023gpt,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{chen2024panda,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle={CVPR},
  pages={13320--13331},
  year={2024}
}

@article{chen2024sharegpt4video,
  title={Sharegpt4video: Improving video understanding and generation with better captions},
  author={Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others},
  journal={arXiv preprint arXiv:2406.04325},
  year={2024}
}

@article{cheng2024videollama,
  title={{VideoLLaMA 2}: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@article{gao2020fused,
  title={Fused {GRU} with semantic-temporal attention for video captioning},
  author={Gao, Lianli and Wang, Xuanhan and Song, Jingkuan and Liu, Yang},
  journal={Neurocomputing},
  volume={395},
  pages={222--228},
  year={2020},
}

@misc{gpt4o,
    title={{GPT-4o}},
    howpublished={\url{https://openai.com/index/hello-gpt-4o/}},
    author={OpenAI},
    month={May},
    year={2024}
}

@inproceedings{guadarrama2013youtube2text,
  title={{Youtube2Text}: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition},
  author={Guadarrama, Sergio and Krishnamoorthy, Niveda and Malkarnenkar, Girish and Venugopalan, Subhashini and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  booktitle={ICCV},
  pages={2712--2719},
  year={2013}
}

@inproceedings{hu2019hierarchical,
  title={Hierarchical global-local temporal modeling for video captioning},
  author={Hu, Yaosi and Chen, Zhenzhong and Zha, Zheng-Jun and Wu, Feng},
  booktitle={ACM MM},
  pages={774--783},
  year={2019}
}

@inproceedings{islam2024video,
  title={{Video ReCap}: Recursive Captioning of Hour-Long Videos},
  author={Islam, Md Mohaiminul and Ho, Ngan and Yang, Xitong and Nagarajan, Tushar and Torresani, Lorenzo and Bertasius, Gedas},
  booktitle={CVPR},
  pages={18198--18208},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral {7B}},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{jin2023chat-univi,
  title={Chat-univi: Unified visual representation empowers large language models with image and video understanding},
  author={Jin, Peng and Takanobu, Ryuichi and Zhang, Caiwan and Cao, Xiaochun and Yuan, Li},
  journal={arXiv preprint arXiv:2311.08046},
  year={2023}
}

@article{kojima2002natural,
  title={Natural language description of human activities from video images based on concept hierarchy of actions},
  author={Kojima, Atsuhiro and Tamura, Takeshi and Fukunaga, Kunio},
  journal={IJCV},
  volume={50},
  pages={171--184},
  year={2002},
}

@inproceedings{krishnamoorthy2013generating,
  title={Generating natural-language video descriptions using text-mined knowledge},
  author={Krishnamoorthy, Niveda and Malkarnenkar, Girish and Mooney, Raymond and Saenko, Kate and Guadarrama, Sergio},
  booktitle={AAAI},
  volume={27},
  number={1},
  pages={541--547},
  year={2013}
}

@article{li2023llamavid,
  title={{LLaMA-VID}: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  journal={arXiv preprint arXiv:2311.17043},
  year={2023}
}

@article{li2023videochat,
  title={{VideoChat}: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{li2024llava,
  title={{LLaVA-OneVision}: Easy Visual Task Transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{lin2023video,
  title={{Video-LLaVA}: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{liu2024kangaroo,
  title={Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input},
  author={Liu, Jiajun and Wang, Yibing and Ma, Hanghang and Wu, Xiaoping and Ma, Xiaoqi and Wei, Xiaoming and Jiao, Jianbin and Wu, Enhua and Hu, Jie},
  journal={arXiv preprint arXiv:2408.15542},
  year={2024}
}

@inproceedings{song2024moviechat,
  title={{MovieChat}: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle={CVPR},
  pages={18221--18232},
  year={2024}
}

@article{touvron2023llama,
  title={{LLaMA} 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wei2024visual,
  title={Visual Context Window Extension: A New Perspective for Long Video Understanding},
  author={Wei, Hongchen and Chen, Zhenzhong},
  journal={arXiv preprint arXiv:2409.20018},
  year={2024}
}

@article{xu2024pllava,
  title={{PLLaVA}: Parameter-free llava extension from images to videos for video dense captioning},
  author={Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and Ng, See Kiong and Feng, Jiashi},
  journal={arXiv preprint arXiv:2404.16994},
  year={2024}
}

@article{xue2024longvila,
  title={{LongVILA}: Scaling Long-Context Visual Language Models for Long Videos},
  author={Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others},
  journal={arXiv preprint arXiv:2408.10188},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{yao2024minicpm,
  title={{MiniCPM-V}: A {GPT-4V} level {MLLM} on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@article{zhang2024long,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

