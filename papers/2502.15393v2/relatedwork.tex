\section{Related Work}
\label{sec:rela_work}
\subsection{Large Multimodal Model}
Recent advancements in large language models (LLMs)~\cite{touvron2023llama,jiang2023mistral,yang2024qwen2,Claude3,gpt4o,achiam2023gpt} have demonstrated impressive language understanding and generation capabilities. 
This success has sparked interest in large multimodal models (LMMs)~\cite{li2024llava,Dai2023InstructBLIPTG,Ye2023mPLUGOwlME,yao2024minicpm}, which typically consist of visual encoders, modality projectors, and pretrained language model decoders. 
LMMs initially made breakthroughs in image understanding tasks. 
With the construction of high-quality video-text datasets, more researchers are applying LMMs to video understanding tasks~\cite{zhang2024long,xue2024longvila,liu2024kangaroo,li2024llava,li2023llamavid,jin2023chat-univi,song2024moviechat,xu2024pllava,lin2023video}. 
For example, models like VideoChatGPT~\cite{0001RKK24}, VideoChat~\cite{li2023videochat}, and Video-LLaMA~\cite{cheng2024videollama} have enhanced the video understanding capabilities of LMMs through high-quality data and fine-tuning techniques.  
These models have shown excellent performance in short video understanding. 
Recently, some efforts~\cite{zhang2024long,wei2024visual,xue2024longvila,liu2024kangaroo} have been made to input long videos into LMMs, achieving some progress. 
For instance, MovieChat~\cite{song2024moviechat} introduced a memory mechanism to compress long video tokens into a fixed size. 
Additionally, LongVA~\cite{zhang2024long} extended the context window by continuously training LLMs on long texts. 
Although they perform well in long video-QA tasks, they face challenges in generating video captions that require global descriptions. 
For a 20-minute video, they struggle to output even 300 words of description. 
This limits the application of the model in video understanding. 

\subsection{Video Captioning}
Early video captioning methods used template-based approaches~\cite{kojima2002natural,guadarrama2013youtube2text,krishnamoorthy2013generating}, which lacked flexibility. 
With the development of deep learning, expert models based on CNN-RNN and Transformer architectures replaced previous methods~\cite{gao2020fused,hu2019hierarchical}. However, these approaches typically handle only short videos of a few seconds, and the generated captions are also brief. 
The VideoReCap~\cite{islam2024video} model was the first to attempt generating descriptions for long videos recursively. 
However, its descriptions often do not exceed 100 words for a 60-minute video, inevitably missing much of the video content. 
Dense video captioning~\cite{YangNSMPLSS23,KimKMC024,WangZLZC021} typically identifies different event timestamps within a video and generates corresponding captions for each event. 
However, these methods still focus on short video-short caption scenarios, with annotated captions generally limited to no more than 30 words. 
Recently, some efforts~\cite{chen2024panda,chen2024sharegpt4video} have attempted to combine LMMs and LLMs to construct large-scale video captioning datasets. 
For instance, Panda70M~\cite{chen2024panda} constructed semantically consistent videos by splitting and merging based on semantic understanding, and then used a pre-trained model to generate captions for each video. 
ShareGPT4Video~\cite{chen2024sharegpt4video} proposed a differential video captioning strategy, leveraging GPT-4V~\cite{achiam2023gpt} to synthesize video captions by identifying differences between adjacent frames. 
However, these methods typically focus on short video-caption examples.