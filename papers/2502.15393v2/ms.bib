@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@article{chen2024sharegpt4video,
  title={Sharegpt4video: Improving video understanding and generation with better captions},
  author={Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others},
  journal={arXiv preprint arXiv:2406.04325},
  year={2024}
}



@article{xue2024longvila,
  title={{LongVILA}: Scaling Long-Context Visual Language Models for Long Videos},
  author={Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others},
  journal={arXiv preprint arXiv:2408.10188},
  year={2024}
}

@article{liu2024kangaroo,
  title={Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input},
  author={Liu, Jiajun and Wang, Yibing and Ma, Hanghang and Wu, Xiaoping and Ma, Xiaoqi and Wei, Xiaoming and Jiao, Jianbin and Wu, Enhua and Hu, Jie},
  journal={arXiv preprint arXiv:2408.15542},
  year={2024}
}


@article{ataallah2024minigpt4,
  title={{MiniGPT4-Video}: Advancing multimodal llms for video understanding with interleaved visual-textual tokens},
  author={Ataallah, Kirolos and Shen, Xiaoqian and Abdelrahman, Eslam and Sleiman, Essam and Zhu, Deyao and Ding, Jian and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2404.03413},
  year={2024}
}

@article{li2023llamavid,
  title={{LLaMA-VID}: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  journal={arXiv preprint arXiv:2311.17043},
  year={2023}
}

@article{jin2023chat-univi,
  title={Chat-univi: Unified visual representation empowers large language models with image and video understanding},
  author={Jin, Peng and Takanobu, Ryuichi and Zhang, Caiwan and Cao, Xiaochun and Yuan, Li},
  journal={arXiv preprint arXiv:2311.08046},
  year={2023}
}

@inproceedings{song2024moviechat,
  title={{MovieChat}: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle={CVPR},
  pages={18221--18232},
  year={2024}
}

@article{song2024moviellm,
  title={{MovieLLM}: Enhancing long video understanding with ai-generated movies},
  author={Song, Zhende and Wang, Chenchen and Sheng, Jiamu and Zhang, Chi and Yu, Gang and Fan, Jiayuan and Chen, Tao},
  journal={arXiv preprint arXiv:2403.01422},
  year={2024}
}

@article{xu2024pllava,
  title={{PLLaVA}: Parameter-free llava extension from images to videos for video dense captioning},
  author={Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and Ng, See Kiong and Feng, Jiashi},
  journal={arXiv preprint arXiv:2404.16994},
  year={2024}
}


@article{zhang2024long,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@article{bai2024longwriter,
  title={Longwriter: Unleashing 10,000+ word generation from long context llms},
  author={Bai, Yushi and Zhang, Jiajie and Lv, Xin and Zheng, Linzhi and Zhu, Siqi and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi},
  journal={arXiv preprint arXiv:2408.07055},
  year={2024}
}

@article{wei2024visual,
  title={Visual Context Window Extension: A New Perspective for Long Video Understanding},
  author={Wei, Hongchen and Chen, Zhenzhong},
  journal={arXiv preprint arXiv:2409.20018},
  year={2024}
}

@article{touvron2023llama,
  title={{LLaMA} 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral {7B}},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{wang2024qwen2,
  title={{Qwen2-VL}: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@misc{Claude3,
    title={Claude 3},
    howpublished={\url{https://www.anthropic.com/news/claude-3-family}},
    author={Anthropic},
    month={March},
    year={2024}
}
@misc{gpt4o,
    title={{GPT-4o}},
    howpublished={\url{https://openai.com/index/hello-gpt-4o/}},
    author={OpenAI},
    month={May},
    year={2024}
}

@misc{ChatGPT,
  title = {{ChatGPT}},
  howpublished = {https://openai.com/index/chatgpt/},
  author={OpenAI},
  month={November},
  year = {2022}
}

@article{achiam2023gpt,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{li2024llava,
  title={{LLaVA-OneVision}: Easy Visual Task Transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{Dai2023InstructBLIPTG,
  title={{InstructBLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Albert Li and Pascale Fung and Steven C. H. Hoi},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.06500},
}

@article{Zhu2023MiniGPT4EV,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.10592},
}

@article{Ye2023mPLUGOwlME,
  title={{mPLUG-Owl}: Modularization Empowers Large Language Models with Multimodality},
  author={Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yi Zhou and Junyan Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qiang Qi and Ji Zhang and Feiyan Huang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.14178},
}

@article{yao2024minicpm,
  title={{MiniCPM-V}: A {GPT-4V} level {MLLM} on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@inproceedings{0001RKK24,
  author       = {Muhammad Maaz and
                  Hanoona Abdul Rasheed and
                  Salman Khan and
                  Fahad Khan},
  title        = {{Video-ChatGPT}: Towards Detailed Video Understanding via Large Vision and Language Models},
  booktitle    = {ACL},
  address = {Bangkok, Thailand},
  pages        = {12585--12602},
  year         = {2024}
}

@article{zhang2024video,
  title={Video Instruction Tuning With Synthetic Data},
  author={Zhang, Yuanhan and Wu, Jinming and Li, Wei and Li, Bo and Ma, Zejun and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2410.02713},
  year={2024}
}

@article{li2023videochat,
  title={{VideoChat}: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{cheng2024videollama,
  title={{VideoLLaMA 2}: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@article{kojima2002natural,
  title={Natural language description of human activities from video images based on concept hierarchy of actions},
  author={Kojima, Atsuhiro and Tamura, Takeshi and Fukunaga, Kunio},
  journal={IJCV},
  volume={50},
  pages={171--184},
  year={2002},
}

@inproceedings{guadarrama2013youtube2text,
  title={{Youtube2Text}: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition},
  author={Guadarrama, Sergio and Krishnamoorthy, Niveda and Malkarnenkar, Girish and Venugopalan, Subhashini and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  booktitle={ICCV},
  pages={2712--2719},
  year={2013}
}

@inproceedings{krishnamoorthy2013generating,
  title={Generating natural-language video descriptions using text-mined knowledge},
  author={Krishnamoorthy, Niveda and Malkarnenkar, Girish and Mooney, Raymond and Saenko, Kate and Guadarrama, Sergio},
  booktitle={AAAI},
  volume={27},
  number={1},
  pages={541--547},
  year={2013}
}

@article{gao2020fused,
  title={Fused {GRU} with semantic-temporal attention for video captioning},
  author={Gao, Lianli and Wang, Xuanhan and Song, Jingkuan and Liu, Yang},
  journal={Neurocomputing},
  volume={395},
  pages={222--228},
  year={2020},
}

@inproceedings{hu2019hierarchical,
  title={Hierarchical global-local temporal modeling for video captioning},
  author={Hu, Yaosi and Chen, Zhenzhong and Zha, Zheng-Jun and Wu, Feng},
  booktitle={ACM MM},
  pages={774--783},
  year={2019}
}

@inproceedings{islam2024video,
  title={{Video ReCap}: Recursive Captioning of Hour-Long Videos},
  author={Islam, Md Mohaiminul and Ho, Ngan and Yang, Xitong and Nagarajan, Tushar and Torresani, Lorenzo and Bertasius, Gedas},
  booktitle={CVPR},
  pages={18198--18208},
  year={2024}
}

@inproceedings{chen2024panda,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle={CVPR},
  pages={13320--13331},
  year={2024}
}

@inproceedings{VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention is All you Need},
  booktitle    = {NeurIPS},
  pages        = {5998--6008},
  year         = {2017}
}

@article{wu2024longvideobench,
  title={LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding},
  author={Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2407.15754},
  year={2024}
}

@article{zhou2024mlvu,
  title={MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding},
  author={Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao, Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and Huang, Tiejun and Liu, Zheng},
  journal={arXiv preprint arXiv:2406.04264},
  year={2024}
}


@article{lin2023video,
  title={{Video-LLaVA}: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{chen2023videollm,
  title={{VideoLLM}: Modeling video sequence with large language models},
  author={Chen, Guo and Zheng, Yin-Dong and Wang, Jiahao and Xu, Jilan and Huang, Yifei and Pan, Junting and Wang, Yi and Wang, Yali and Qiao, Yu and Lu, Tong and others},
  journal={arXiv preprint arXiv:2305.13292},
  year={2023}
}

@inproceedings{liu2024st,
  title={{ST-LLM}: Large Language Models Are Effective Temporal Learners},
  author={Liu, Ruyang and Li, Chen and Tang, Haoran and Ge, Yixiao and Shan, Ying and Li, Ge},
  booktitle={ECCV},
  year={2024}
}


@inproceedings{jin2024chat,
  title={{Chat-UniVi}: Unified visual representation empowers large language models with image and video understanding},
  author={Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li},
  booktitle={CVPR},
  pages={13700--13710},
  year={2024}
}


@article{liu2023btadapter,
  title={One for all: Video conversation is feasible without video instruction tuning},
  author={Liu, Ruyang and Li, Chen and Ge, Yixiao and Shan, Ying and Li, Thomas H and Li, Ge},
  journal={arXiv preprint arXiv:2309.15785},
  year={2023}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
  pages={22195--22206},
  year={2024}
}

@misc{liu2024llavanext,
    title={{LLaVA-NeXT}: Improved reasoning, OCR, and world knowledge},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{glm2024chatglm,
  title={{ChatGLM}: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
  author={GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and Lai, Hanyu and others},
  journal={arXiv preprint arXiv:2406.12793},
  year={2024}
}

@misc{GPT-4o,
  author = {OpenAI},
  title = {{OpenAI}: Hello {GPT-4o}},
  year = {2024},
  url = {https://openai.com/index/hello-gpt-4o/},
}

@misc{GPT-4o-mini,
  author = {OpenAI},
  title = {{GPT-4o mini}: advancing cost-efficient intelligence},
  year = {2024},
  url = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
}

@inproceedings{VedantamZP15,
  author       = {Ramakrishna Vedantam and
                  C. Lawrence Zitnick and
                  Devi Parikh},
  title        = {{CIDEr}: Consensus-based image description evaluation},
  booktitle    = {CVPR},
  pages        = {4566--4575},
  address = {Boston, MA, USA},
  year         = {2015}
}

@inproceedings{ChenD11,
  author       = {David L. Chen and
                  William B. Dolan},
  title        = {Collecting Highly Parallel Data for Paraphrase Evaluation},
  booktitle    = {ACL},
  pages        = {190--200},
  address = {Portland, Oregon, USA},
  year         = {2011}
}

@inproceedings{ZhouXC18,
  author       = {Luowei Zhou and
                  Chenliang Xu and
                  Jason J. Corso},
  title        = {Towards Automatic Learning of Procedures From Web Instructional Videos},
  booktitle    = {AAAI},
  pages        = {7590--7598},
  address = {New Orleans, Louisiana, USA},
  year         = {2018}
}

@inproceedings{XuMYR16,
  author       = {Jun Xu and
                  Tao Mei and
                  Ting Yao and
                  Yong Rui},
  title        = {{MSR-VTT:} {A} Large Video Description Dataset for Bridging Video and Language},
  booktitle    = {CVPR},
  pages        = {5288--5296},
  address = {Las Vegas, NV, USA},
  year         = {2016}
}

@inproceedings{HeilbronEGN15,
  author       = {Fabian Caba Heilbron and
                  Victor Escorcia and
                  Bernard Ghanem and
                  Juan Carlos Niebles},
  title        = {{ActivityNet}: {A} large-scale video benchmark for human activity understanding},
  booktitle    = {CVPR},
  pages        = {961--970},
  address = {Boston, MA, USA},
  year         = {2015}
}

@article{zhang2024direct,
  title={Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward},
  author={Zhang, Ruohong and Gui, Liangke and Sun, Zhiqing and Feng, Yihao and Xu, Keyang and Zhang, Yuanhan and Fu, Di and Li, Chunyuan and Hauptmann, Alexander and Bisk, Yonatan and others},
  journal={arXiv preprint arXiv:2404.01258},
  year={2024}
}

@inproceedings{YangNSMPLSS23,
  author       = {Antoine Yang and
                  Arsha Nagrani and
                  Paul Hongsuck Seo and
                  Antoine Miech and
                  Jordi Pont{-}Tuset and
                  Ivan Laptev and
                  Josef Sivic and
                  Cordelia Schmid},
  title        = {{Vid2Seq}: Large-Scale Pretraining of a Visual Language Model for Dense
                  Video Captioning},
  booktitle    = {CVPR},
  pages        = {10714--10726},
  address = {Vancouver, BC, Canada},
  year         = {2023}
}

@inproceedings{KimKMC024,
  author       = {Minkuk Kim and
                  Hyeon Bae Kim and
                  Jinyoung Moon and
                  Jinwoo Choi and
                  Seong Tae Kim},
  title        = {Do You Remember? {Dense} Video Captioning with Cross-Modal Memory Retrieval},
  booktitle    = {CVPR},
  pages        = {13894--13904},
  address = {Seattle, WA, USA},
  year         = {2024}
}

@inproceedings{WangZLZC021,
  author       = {Teng Wang and
                  Ruimao Zhang and
                  Zhichao Lu and
                  Feng Zheng and
                  Ran Cheng and
                  Ping Luo},
  title        = {End-to-End Dense Video Captioning with Parallel Decoding},
  booktitle    = {ICCV},
  pages        = {6827--6837},
  address = {Montreal, QC, Canada},
  year         = {2021}
}

@inproceedings{HuSWALWWC22,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  booktitle    = {ICLR},
  address = {Virtual},
  year         = {2022}
}

@inproceedings{AminabadiRALLZRSZRH22,
  author       = {Reza Yazdani Aminabadi and
                  Samyam Rajbhandari and
                  Ammar Ahmad Awan and
                  Cheng Li and
                  Du Li and
                  Elton Zheng and
                  Olatunji Ruwase and
                  Shaden Smith and
                  Minjia Zhang and
                  Jeff Rasley and
                  Yuxiong He},
  title        = {DeepSpeed- Inference: Enabling Efficient Inference of Transformer
                  Models at Unprecedented Scale},
  booktitle    = {SC22},
  pages        = {46:1--46:15},
  address = {Dallas, TX, USA},
  year         = {2022}
}

@inproceedings{KrishnaHRFN17,
  author       = {Ranjay Krishna and
                  Kenji Hata and
                  Frederic Ren and
                  Li Fei{-}Fei and
                  Juan Carlos Niebles},
  title        = {Dense-Captioning Events in Videos},
  booktitle    = {ICCV},
  pages        = {706--715},
  address = {Venice, Italy},
  year         = {2017}
}


@article{fu2024video,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}


@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@inproceedings{ren2024timechat,
  title={Timechat: A time-sensitive multimodal large language model for long video understanding},
  author={Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
  booktitle={CVPR},
  pages={14313--14323},
  year={2024}
}