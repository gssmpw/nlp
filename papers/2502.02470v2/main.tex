
%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
% \usepackage{subfig}
\usepackage{subcaption}
\usepackage[numbers]{natbib}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{arxiv}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{preamble/icml2025}

% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Modular Training of Neural Networks}

\begin{document}

\twocolumn[
% \icmltitle{Training Neural Networks for Modularity
% Aids Interpretability}
% \icmltitle{nn.Modular: Training Neural Networks Modularly aids Interpretability}
\icmltitle{Modular Training of Neural Networks aids Interpretability}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Satvik Golechha}{mats}
\icmlauthor{Maheep Chaudhary}{ind}
\icmlauthor{Joan Velja}{uva}
\icmlauthor{Alessandro Abate}{ox}
\icmlauthor{Nandi Schoots}{ox}
\end{icmlauthorlist}

\icmlaffiliation{mats}{Machine Learning Alignment \& Theory Scholars (MATS) Program}
\icmlaffiliation{ox}{Department of Computer Science, University of Oxford}
\icmlaffiliation{uva}{University of Amsterdam}
\icmlaffiliation{ind}{Independent}

\icmlcorrespondingauthor{Satvik Golechha}{zsatvik@gmail.com}
\icmlcorrespondingauthor{Nandi Schoots}{nandi.schoots@kcl.ac.uk}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.



\input{content}

% \clearpage
\bibliography{main}
\bibliographystyle{preamble/icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Hyperparameters}\label{app:hyper}

We list the hyperparameters and various design choices in our experiments in Tab. \ref{tab:hyperparameters}, and refer to our codebase for more details (to be added during de-anonymization).

\begin{table}[ht]
\centering
\caption{Hyperparameter choices and other experiment details.}
\vspace{5pt}
\begin{tabular}{l l}
\toprule
\textbf{Hyperparameter/Design Choice} & \textbf{Value} \\
\midrule
Dataset & CIFAR-10 (or MNIST) \\
Batch Size & 64 \\
\midrule
Optimizer & Adam \\
Learning Rate ($\alpha$) & $1 \times 10^{-3}$ \\
Criterion & Cross-Entropy \\
Clusterability Factor & 20 \\
Number of Clusters & 4 \\
\midrule
\textbf{MLP (for MNIST)} & \\
\midrule
Input Size & $28 \times 28$ \\
Hidden Layer Sizes & 64, 64 \\
Activation Function & ReLU \\
Output Size & 10 \\
Bias & False \\
\midrule
\textbf{CNN (for CIFAR-10)} & \\
\midrule
Input Channels & 3 \\
Conv Layer 1: Out Channels & 16 \\
Conv Layer 1: Kernel Size & 3 \\
Conv Layer 1: Stride & 1 \\
Conv Layer 1: Padding & 1 \\
FC Layer 1: Input Features & $16 \times 16 \times 16$ \\
FC Layer 1: Output Features & 64 \\
FC Layer 2: Output Features & 64 \\
Output Size & 10 \\
Bias & False \\
Activation Function & ReLU \\
\midrule
\textbf{Transformer (for Modular Arithmetic)} & \\
\midrule
Learning Rate ($\alpha$) & $1 \times 10^{-3}$ \\
Weight Decay & 1.0 \\
Prime Number ($p$) & 113 \\
Model Dimension ($d_{\text{model}}$) & 128 \\
Function Name & Addition \\
Training Fraction & 0.3 \\
Number of Epochs & 200 \\
Number of Layers & 1 \\
Batch Style & Full \\
Vocabulary Size ($d_{\text{vocab}}$) & $p + 1$ \\
Context Length ($n_{\text{ctx}}$) & 3 \\
MLP Dimension ($d_{\text{mlp}}$) & $d_{\text{model}}$ \\
Number of Heads & 4 \\
Activation Function & ReLU \\
Device & CUDA \\
Use LayerNorm & False \\
\midrule
Pruning Method & Iterative weight pruning \\
Pruning Criteria & Performance-based (loss and accuracy) \\
\midrule
Effective Circuit Size Calculation & Fraction of non-zero weights \\
\bottomrule
\end{tabular}
\label{tab:hyperparameters}
\end{table}

\section{Results on MNIST}\label{app:mnist}

Here we present our results on the MNIST \citep{deng2012mnist} dataset. For the clusterability of the model with $k$ bipartite clusters, see Figure \ref{fig:mnist_clusterability}, and for the class-wise accuracies for each label with clusters turned ON and OFF, see Figure \ref{fig:mnist_classwise-accuracies}.

\begin{figure}[h]
\centering
    \includegraphics[width=0.4\textwidth,  trim=2 2 40 80, clip]{figures/mnist/clusterability_scores.pdf}
    \caption{Clusterability (enmeshment) of the model with $k$ bipartite clusters using Algorithm~\ref{alg:spectral_clustering} on MNIST.}
    \label{fig:mnist_clusterability}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth,  trim=2 2 300 100, clip]{figures/mnist/classwise_cluster_all_accuracies.pdf}
    \caption{Class-wise accuracy for each label with clusters turned ON (top) and OFF (bottom) for MNIST. Note individual clusters learning near-complete circuits for various labels.}
    \label{fig:mnist_classwise-accuracies}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth,  trim=2 2 50 25, clip]{figures/mnist/ecs_pruned_all_labels.pdf}
    \caption{Effective Circuit Size (ECS) of circuits for each label as a fraction of the whole model for both clustered and unclustered models trained on MNIST. Larger arrows denote a larger reduction in ECS.}
    \label{fig:mnist_ecs}
\end{figure}

\section{Clustered Layer Visualization}

Figure \ref{fig:cluster-visualization} shows the visualization of the fully connected layer trained on CIFAR10 of the network with bipartite clusters.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/clustered_layer.png}
    \caption{Visualizing the clustered layer $fc2$ of the model. Red and blue denote negative and positive weights respectively.}
    \label{fig:cluster-visualization}
\end{figure}

\section{Interpretability of Modular Transformer for Modular Arithmetic}\label{app:arithmetic}

In Figure \ref{fig:fourier-norms}, we show the $L2$ norms of the embeddings with the discrete Fourier transformation are very similar to the observations made in \citet{nanda2023progressmeasuresgrokkingmechanistic}, indicating that the circuits in the modular model learn the same algorithm to perform modular addition. 

However, interpreting such a model is much easier with modularity, because modular MLPs have a much simpler representation space, and modular Attention matrices imply a $n$ times simpler circuit search space as compared to a non-modular model in the presence of $n$ completely non-intersecting clusters with a clusterability value of $1$, as we observe in \ref{sec:modular-modular-addition}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/modular-arithmetic-embedding-norms.pdf}
    \caption{Norms of the Fourier embeddings for sine and cosine functions.}
    \label{fig:fourier-norms}
\end{figure}

\section{Type-1 and Type-2 Intervention Performance}

Figure \ref{fig:modvsnmod} shows Type $1$ and Type $2$ interventions illustrating the dependence of samples on individual or pairs of modules. The figure highlights a significant difference in sample dependence between modular and non-modular models, particularly in Type $1$ interventions. Approximately $30\%$ of samples in the non-modular model rely on all $4$ modules for correct predictions, compared to only about $1\%$ in the modular model.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/modular_vs_nonmodular_performance_across_layers.pdf}
    \caption{Type $1$ and Type $2$ interventions illustrating the dependence of samples on individual or pairs of modules. The figure highlights a significant difference in sample dependence between modular and non-modular models, particularly in Type $1$ interventions. Approximately $30\%$ of samples in the non-modular model rely on all $4$ modules for correct predictions, compared to only about $1\%$ in the modular model. Similarly, in Type $2$ interventions, there is a notable $10\%$ difference in module dependence on all $4$ samples between the two models.}
    \label{fig:modvsnmod}
\end{figure}

\section{Explanation of LLM Intervention Values}\label{app:explanation-LLM-values}

To calculate these values, we 
(1) only consider the subset of the test set for which the model without intervention gets hundred percent accuracy; and (2) for each module we do a type 1 intervention (i.e. only keeping that module on and turning the other three off) and keep track of the datapoints that are now handled incorrectly.
If a datapoint is handled incorrectly when only module X is turned on, 
then module X is not sufficient on its own. 
% N = 1 means that there was only one module such that when that module was on the performance went down
% N = 2 means that there are two modules for which the performance went down when 
% N = 3 means that there are three modules for which the performance went down when that 
We then count for how many datapoints there was one module (N=1) such that this module is not sufficient on its own, for how many datapoints there were two modules (N=2) such that this module is not sufficient on its own, and so on.
% Note: if there is a type 2 intervention, and there is a module X for which the performance goes down when this module is turned off, then we can say this module was necessary.
% Note: if we count after a type 1 intervention how many datapoints are handled Correctly, we can say that module is sufficient on its own

In Figure \ref{fig:llm-fractions} we find that for modular models N=1 is higher, which means there were many datapoints for which there was only one module that was not sufficient on its own, i.e. many modules (three) were able to handle the datapoint correctly when they were turned on in isolation.
For non-modular models N=4 is higher, which means that there were more datapoints such that all modules were not sufficient on their own, i.e. these datapoints required two or more modules to be turned on. 

% So we take the prediction of the samples for the crop data set for which the model gives hundred percent accuracy. We do intervening on one of the modules to calculate if a sample is being correctly or incorrectly predicated. Based on the prediction, we prepare a list for each module intervention. After getting the list of correct/incorrect samples for each module intervention. 
% We subtract these 4 lists by one such that the correct samples becomes zero and the incorrect samples attain the value one. 
% Then we discard the samples which were correct for all the modules or which are zero in all four prediction list. Then we see for each samples for how many module intervention it is “1” . If it is for all 4 it implies it is dependent on all 4, and so on.

\section{Clusterability for Pythia Models for Different Number of Clusters}
\label{app:pythia}

In Table \ref{tab:clusterability-diff-n}, we share the clusterability for pythia-70m on different values of $k$ (number of clusters).

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c}
        \hline
        \textbf{k} & \textbf{Baseline} (\( 1/k \)) & \textbf{Before Finetuning} & \textbf{After 2 Finetuning Epochs} \\
        \hline
        2 & 0.5000 & 0.6009 & 0.9833 \\
        4 & 0.2500 & 0.2403 & 0.9658 \\
        5 & 0.2000 & 0.2109 & 0.9629 \\
        6 & 0.1667 & 0.1611 & 0.9548 \\
        8 & 0.1250 & 0.1201 & 0.9463 \\
        \hline
    \end{tabular}
    \caption{Clusterability scores before and after finetuning for different numbers of clusters for Pythia-70m. Baseline is given by \( 1/k \).}
    \label{tab:clusterability-diff-n}
\end{table}

\section{Details on Other Metrics}

\subsection{Community Structure}\label{sec:community-structure}

In an undirected and unweighted graph (where a weight can only take on the values 0 and 1), the expected number of edges between two nodes $i$ and $j$ is
\begin{align*}
& E[J_{ij}] = \frac{ \sum\limits_{i=1}^{n} W_{ij} \sum\limits_{j=1}^{n} W_{ij}}{2 \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{n} W_{ij}}, \text{ the community structure is } \\
& Q = \frac{ \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{n} \big( W_{ij} -  E[J_{ij}] \big) \cdot \mathbb{I}_{(i \in C_U(u) \land j \in C_V(u))} }{2 \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{n} W_{ij}},  \\
&\text{ where }
\mathbb{I} =
\begin{cases} 
1 & \text{if } i \in C_U(u) \text{ and } j \in C_V(u), \\ 
  & \text{i.e. if $i$ and $j$ are in the same module} \\
0 & \text{otherwise} 
\end{cases}
\end{align*}

% to do: I want to say that the expected weight is different in the case of weighted matrices
The calculation of $Q$ assumes that weights are 0 or 1, the absence or existence of a weight.
Since we are interested in a modularity metric for weighted graphs (weight matrices), 
we want to use a more `continuous' metric, and we want to punish large weights more heavily than small weights. 

% \subsection{Counting Polytopes Modular Network}

% Below we will show that the partitioning of neural networks into polytopes on which the network is linear, becomes coarser when we make a model modular, i.e. the expressivity of a network goes down by adding modularity constraints. We restrict our analysis to fully connected ReLU feed-forward neural networks. 

% \begin{definition}[ReLU Network]
% A ReLU network $\mathcal{N} \colon \mathbb{R}^n \rightarrow \mathbb{R}^m$, is a composition of $L \in \mathbb{N}$ hidden layers given by: 
% \[ \chi^{(l)} = \sigma( {W^{(l)}} \chi^{(l-1)} + b^{(l)}),\]
% where $\sigma$ is an element-wise ReLU activation function, $\sigma(x_i) = \max\{0,x_i\}$. We define $\chi^{(0)} = x$ and the output layer is given by 
% \[\mathcal{N}(x) = W^{(L+1)} \chi^{(L)} + b^{(L+1)}.\] 
% The number of neurons in each layer is given by a vector $\mathbf{N} = [n_1, n_2, \ldots, n_L]$, and all activations are in the positive reals, i.e. $\chi^{(l)} \in \mathbb{R}^{n_l}_{\geq 0}$ for all $l \in \{1,\ldots,L\} = [L]$. We stress the dependence on $x$ by writing $\chi^{(l)}(x)$.
% \end{definition}

% We can describe these functions as linear models that are applied to certain regions of the input space \citep{sudjianto2020unwrapping}. Moreover, these regions partition $\mathbb{R}^n$. 

% \begin{proposition}\label{prop:network-to-partition}[\citet{sudjianto2020unwrapping}]
% For a ReLU network $\mathcal{N} \colon \mathbb{R}^n \rightarrow \mathbb{R}^m$ there is a finite partition $\Omega$ of $\mathbb{R}^n$ of cardinality $p := \# \Omega$ such that for each part $\omega \in \Omega$ there exists a piece-wise linear function $f\colon \mathbb{R}^n \rightarrow \mathbb{R}^m$, and its restriction on $\omega$, denoted $f|_\omega$, can be described by a linear function: 
% \[f|_\omega (x) = \alpha_\omega^T x + \beta_\omega.\]
% Moreover, each part is a  polytope, given by the intersection of a collection of half-spaces.
% We write the minimum set of half-space conditions that can be used to specify the entire partition as $H_1,\ldots,H_k$, where each $H_i$ is given by the set of all $x \in \mathbb{R}^n$ such that, for for $h_{i,j} \in \mathbb{R}, i\in [k], j\in [n]$: 
% \[ h_{i,1} x_1 + h_{i,2} x_2 + \ldots + h_{i,n} x_n  > c_i.\]
% \end{proposition}

% In some sense, the number of polytopes gives a measure of the granularity or refinement of a network.
% Given a network with neurons $\mathbf{N} = [n_1, n_2, \ldots, n_L]$, 
% there are at most $2^{n_1} \times 2^{n_2} \times \ldots \times 2^{n_L}$ polytopes. 

% Now suppose that the weight matrix $W^{(l)}$ in layer $\chi^{(l)}$ is modular and contains $k$ modules, meaning that we can divide $n_{l-1}$ and $n_{l}$ into $k$ sets $[n_{l-1}^1, \ldots, n_{l-1}^k]$ and $[n_{l}^1, \ldots, n_{l}^k]$ such that the only non-zero weights in $W^{(l)}$ are between $n_{l-1}^i$ and $n_{l}^i$.
% In this case the number of half-space conditions generated by $\chi^{(l-1)}$ and $\chi^{(l)}$ is no longer $2^{n_{l-1}} \times 2^{n_l}$, but it now is $2^{n_{l-1}} \times 2^{n_{l}^1} + \ldots + 2^{n_{l-1}} \times 2^{n_{l}^k}$, which is substantially fewer.
% This means that the expressivity of a network goes down by adding modularity constraints.


\end{document}