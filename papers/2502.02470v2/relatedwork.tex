\section{Related Work}
\label{sec:related}

% From Nandi's Research Proposal
\paragraph{Modularity} metrics inspired by research in neuroscience incorporate transfer entropy \citep{novelli2019deriving, ursino2020transfer} % transfer entropy
or spatial metrics \citep{liu2023seeing, liu2023growing}. % Max Tegnmark
 % the thing that would be new that we have not yet done is using neuro-evolution
Models are sometimes trained or pruned using modularity metrics \citep{patil2023neural}.
% Here we use the clusterability loss function.
\citet{tsitsulin2023graph} find that Graph Neural Networks node pooling methods do not work well to cluster graphs and introduce an unsupervised clustering method.
Their method first uses a graph convolutional network to obtain soft clusters for each node and then 
optimizes this assignment based on the first modularity metric in Section \ref{sec:clusterability}.
\citet{salha2022modularity} add a regularization term for modularity in Graph Autoencoders to show performance gains. In this work, we focus on how modularity can help us train models to be more interpretable.


\paragraph{Mixture of experts} is a form of modularity, where each module is an entire network. In our set-up each module is a component of a layer, i.e. the partitioning is at a lower level. Mixture of experts sometimes use routing policies to decide which expert to activate (e.g. based on features of the input).
Gradient routing \citep{cloud2024gradient} is a method that trains task-specialized experts by hard-coding what tasks each expert specializes on, one of their stated benefits is that this allows a user to selectively remove ability on a particular task. 
MONET \citep{park2024monet} trains monosemantic experts, which learn to route different types of inputs to specific experts during training, without human-specified task assignments.
Instead, we encourage the emergence of task-specialized layer components during training.

\paragraph{Clustering} of neural network weights is typically done either using graph properties of the weights (structural)~\citep{watanabe2018modular, filan2021clusterability, patil2023neural} or using correlations between neuron activations (functional) \citep{hod2022detecting, lange2022clustering}.
% From Nandi's paper on Machine Unlearning
MoEfication groups feedforward neurons in a pre-trained language model into clusters of `experts' and at inference only activates the most clusters neurons~\citep{zhang2022moefication}. In this paper, we consider both weight-based and gradient-based clustering, and show that they do not help with modularity across clusters.

\paragraph{Circuit Discovery} is an active field of research \citep{wang2022interpretability, olah2020zoom, elhage2021mathematical, conmy2023towards}, which aims to uncover subnetworks that perform a specific functionality. 
\citet{wortsman2020supermasks} use a randomly initialized, fixed base network and for each task find a subnetwork that achieves good performance on this task. 
These overlapping subnetworks can be thought of as circuits.
In this work, we evaluate and optimize for structural connectedness or global functionality without considering specific functionalities.
We evaluate the resulting modules by assessing the extent to which they have specialized on a class level, i.e. the extent to which they correspond to class-specific performance.
% These subnetworks may overlap, so there is no modularity, but the method does allow for efficient inferences.