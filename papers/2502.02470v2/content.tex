\begin{abstract}

An approach to improve neural network interpretability is via clusterability, i.e., splitting a model into disjoint clusters that can be studied independently.
We define a measure for clusterability and show that pre-trained models form highly enmeshed clusters via spectral graph clustering. We thus train models to be more modular using a ``clusterability loss'' function that encourages the formation of non-interacting clusters.
Using automated interpretability techniques, we show that our method can help train models that are more modular and learn different, disjoint, and smaller circuits. We investigate CNNs trained on MNIST and CIFAR, small transformers trained on modular addition, and language models.
Our approach provides a promising direction for training neural networks that learn simpler functions and are easier to interpret.

\end{abstract}

\section{Introduction}

Interpretability is an active area of research that aims to solve both high-stake deployment constraints for fairness and robustness~\citep{McClure2020EvaluatingAR} and AI safety and trustworthiness concerns~\citep{Bereska2024MechanisticIF}. Several breakthroughs in the subdomain of mechanistic interpretability, have helped us understand the inner workings of deep networks, both via circuits~\citep{wang2022interpretability, olah2020zoom, elhage2021mathematical} and representation spaces~\citep{zou2023representation, bricken2023towards}.

One approach to mechanistic interpretability is to identify features at the level of neurons, alternatively we can identify `subskills' at the level of subnetworks or circuits.
While sparse auto-encoders (SAEs) \citep{huben2024sparse} extract a model's features over different neurons, we nudge the model to divide its computation into disentangled components.

In this work, we split models into separate modules and interpret them in isolation. However, this is feasible only if the interaction, between these modules is minimal, i.e. if the clusterability is high. We introduce a metric to measure the amount of clusterability in neural network components, and attempt to make models modular and more interpretable during training by optimizing for this metric.

\begin{figure}[h]
\centering
    \includegraphics[width=0.42\textwidth,  trim=40 480 40 30, clip]{figures/fig1-modularity.pdf}
    \caption{Our training methodology promotes the formation of modular clusters of this kind in any network component.}
    \label{fig:fig1}
\end{figure}

Making models modular has two interpretability benefits:
(a) modules may specialize in different sub-skills and behaviors, which would allow us to investigate and control them independently; and (b) computationally, interpretability methods such as patching \cite{wang2022interpretability}, pruning, and training sparse auto-encoders \cite{cunningham2023sparseautoencodershighlyinterpretable} become more tractable.

We empirically show that when we train a model to be modular, the modules specialize in different ways.
For models trained on CIFAR, we find in Figure \ref{fig:classwise-accuracies} that modules specialize on different labels.
For language models we find in Figure \ref{fig:llm-fractions} that in models trained to be modular, performance often only requires a single module.

Applying mechanistic interpretability methods to large models \citep{kaplan2020scaling} and complex behaviors has been a major hurdle for interpretability~\cite{lieberum2023does, golechha2024challenges} due to complicated computational subgraphs (circuits) and superposition~\cite{henighan2023superposition}, i.e., networks representing more features than they have neurons.
We theoretically show in Section \ref{sec:theoretical-representations} that the latent space of a modular matrix has a much smaller feature-space, and empirically show for CIFAR in Section \ref{sec:circuit-size} that splitting models into separate components makes circuits smaller, making pruning and patching \citep{conmy2023towards} more tractable.

The main contributions of our work are as follows:
\begin{itemize}
    \item We introduce a measure of modularity for neural network components and define ``clusterability loss'', a simple and effective regularizer that encourages the emergence of non-interference between clusters during model training (Section \ref{sec:clusterability}). See Figure \ref{fig:fig1} for a visual description of modularity in a layer.
    \item We use automated interpretability methods to show that the clusters thus obtained make the model more interpretable by producing specialized clusters  (Section \ref{sec:cluster-specialization}).
    \item We empirically (Section \ref{sec:circuit-size}) and theoretically (Section \ref{sec:theoretical-representations}) show that the clusters reduce the search-space of circuit-style analyses by reducing effective circuit size.
\end{itemize}

We also modify and test prior clusterability methods to split a trained neural network layer into clusters and show that the clusters thus found are highly enmeshed (not modular) and do not help our interpretability goals (Section \ref{sec:spectral-clustering}), discuss how our training methodology, as described in Section \ref{subsec:training}, can help with building or fine-tuning more modular models, and discuss some mathematical implications in Section \ref{sec:theoretical}.


\section{Related Work}
\label{sec:related}

% From Nandi's Research Proposal
\paragraph{Modularity} metrics inspired by research in neuroscience incorporate transfer entropy \citep{novelli2019deriving, ursino2020transfer} % transfer entropy
or spatial metrics \citep{liu2023seeing, liu2023growing}. % Max Tegnmark
 % the thing that would be new that we have not yet done is using neuro-evolution
Models are sometimes trained or pruned using modularity metrics \citep{patil2023neural}.
% Here we use the clusterability loss function.
\citet{tsitsulin2023graph} find that Graph Neural Networks node pooling methods do not work well to cluster graphs and introduce an unsupervised clustering method.
Their method first uses a graph convolutional network to obtain soft clusters for each node and then 
optimizes this assignment based on the first modularity metric in Section \ref{sec:clusterability}.
\citet{salha2022modularity} add a regularization term for modularity in Graph Autoencoders to show performance gains. In this work, we focus on how modularity can help us train models to be more interpretable.


\paragraph{Mixture of experts} is a form of modularity, where each module is an entire network. In our set-up each module is a component of a layer, i.e. the partitioning is at a lower level. Mixture of experts sometimes use routing policies to decide which expert to activate (e.g. based on features of the input).
Gradient routing \citep{cloud2024gradient} is a method that trains task-specialized experts by hard-coding what tasks each expert specializes on, one of their stated benefits is that this allows a user to selectively remove ability on a particular task. 
MONET \citep{park2024monet} trains monosemantic experts, which learn to route different types of inputs to specific experts during training, without human-specified task assignments.
Instead, we encourage the emergence of task-specialized layer components during training.

\paragraph{Clustering} of neural network weights is typically done either using graph properties of the weights (structural)~\citep{watanabe2018modular, filan2021clusterability, patil2023neural} or using correlations between neuron activations (functional) \citep{hod2022detecting, lange2022clustering}.
% From Nandi's paper on Machine Unlearning
MoEfication groups feedforward neurons in a pre-trained language model into clusters of `experts' and at inference only activates the most clusters neurons~\citep{zhang2022moefication}. In this paper, we consider both weight-based and gradient-based clustering, and show that they do not help with modularity across clusters.

\paragraph{Circuit Discovery} is an active field of research \citep{wang2022interpretability, olah2020zoom, elhage2021mathematical, conmy2023towards}, which aims to uncover subnetworks that perform a specific functionality. 
\citet{wortsman2020supermasks} use a randomly initialized, fixed base network and for each task find a subnetwork that achieves good performance on this task. 
These overlapping subnetworks can be thought of as circuits.
In this work, we evaluate and optimize for structural connectedness or global functionality without considering specific functionalities.
We evaluate the resulting modules by assessing the extent to which they have specialized on a class level, i.e. the extent to which they correspond to class-specific performance.
% These subnetworks may overlap, so there is no modularity, but the method does allow for efficient inferences.

\section{Introducing a Measure of Modularity}
\label{sec:clusterability}

First, we discuss existing modularity metrics used in other contexts and their drawbacks in our setting. We then motivate our choice of the clusterability metric (and the clusterability loss) and discuss its benefits as an optimization metric for neural network modularity.

Non-differentiable metrics to measure modularity that rely on conditional entropy \citep{ursino2020transfer}, sampling, or discrete computation \cite{veniat2021efficientcontinuallearningmodular} have been introduced. While they capture the essence of modularity, we cannot use gradient descent to optimize for them during training.
% Some examples of non-differentiable modularity metrics.
% This is kind of sampling, but it does seem end-to-end...? https://arxiv.org/pdf/1811.05249
% This is based on the REINFORCE algorithm https://arxiv.org/pdf/2012.12631 
For unweighted graphs, `community structure' is a commonly used modularity metric \citep{newman2006modularity, salha2022modularity, tsitsulin2023graph, bhowmick2024neural}, which is based on the difference between an edge value (0 or 1) and the expected number of edges (see Appendix \ref{sec:community-structure} for an explicit formula).

Our metric, inspired by community structure, is differentiable and better tailored to weighted graphs. In particular, in our metric large weights are disproportionally punished, and when we slot in real numbers for the weights, the terms can not cancel out. 

We measure the modularity of a model component by choosing a clustering of the component and calculating the amount of ``clusterability'' in the clusters obtained, i.e., the average fraction of weights that are inside a cluster as opposed to between clusters.

% We want high connectivity within a module, but little between modules. 

Let \( W \) be the weight matrix where \( W_{ij} \) represents the edge weight between nodes \( i \) and \( j \) in the layer's input and output neurons respectively. Define \( U \) and \( V \) as the clusters for the rows and columns of \( W \), respectively. Let \( C_U(u) \) and \( C_V(v) \) denote the sets of nodes in clusters \( u \in \{1, \ldots, k\} \) and \( v \in \{1, \ldots, k\} \), respectively.
The clusterability measure \( C \) is defined as follows:

\begin{align*}
&C = \frac{\sum\limits_{i=1}^{n} \sum\limits_{j=1}^{n} W_{ij}^2 \cdot \mathbb{I}_{(i \in C_U(u) \land j \in C_V(v))}}{\sum\limits_{i=1}^{n} \sum\limits_{j=1}^{n} W_{ij}^2}, \\
&\text{ where }
% \mathbb{I}_{(i \in C_U(u) \land j \in C_V(u))} =
\mathbb{I} =
\begin{cases} 
1 & \text{if } i \in C_U(u) \text{ and } j \in C_V(u) , \\ 
  & \text{i.e. if $i$ and $j$ are in the same module} \\
0 & \text{otherwise}
\end{cases}
\end{align*}

% \vspace{0.3cm}

We use this metric to measure modularity in neural network components and train models to be more modular by jointly maximizing for clusterability by adding the clusterability loss to the usual cross-entropy loss:
\[\mathcal{L} = \mathcal{L}_{CE} - \lambda \mathcal{L}_C,\]

for an appropriate clusterability coefficient $\lambda$.

Optimizing for this modularity metric has several benefits:

% % When we measure modularity we care about the extent to which modules operate `independently'.
% If the weights between two modules are 0, then no information flows from one module to the other.
% So our measure should be such that the modularity score is high when the weights between modules are 0, this is a property that is already satisfied by the metric $Q$.

\begin{itemize}
    \item Clusterability measures the extent to which modules are disjoint components which can be studied in isolation.
    \item The metric is differentiable, which makes it possible to optimize for it using back-propagation across a model's parameters.
    \item It is easy to compute. For a model with $k$ components of dimension $n \times n$ that are trained for modularity, computing the clusterability loss takes $O(n^2)$ time and constant space.
    \item Mathematically, as discussed in Section \ref{sec:theoretical}, optimizing our metric leads to models learning simpler function spaces (polytopes) and fewer features (orthogonal directions) to perform a given task.
    \item As we show in Section \ref{subsec:evals}, optimizing for clusterability leads to different, disjoint, and smaller circuits (in the case of CIFAR-10) and other interpretability benefits.
\end{itemize}

\section{Neural Networks trained on Cross-Entropy are not Modular}
\label{sec:spectral-clustering}

First, we would like to evaluate (based on our clusterability metric) how modular neural networks trained using the cross-entropy loss are by default. Directly searching for clusters that maximize our metric leads to a combinatorial explosion, so we use a clustering algorithm to split a component into clusters. For our clustering algorithm, we modify the methodology of \citet{filan2021clusterability} and use normalized spectral clustering to split any component of a neural network into $k$ different clusters, taking into account that a network layer is bipartite.

\begin{algorithm}[t]
   \caption{Bipartite Spectral Graph Clustering (BSGC)}
   \label{alg:spectral_clustering}
   \begin{algorithmic}[1]
      \STATE \textbf{Input:} Similarity matrix $A$ ($m \times n$), number of clusters $k$
      \STATE \textbf{Output:} Bipartite clusters $\text{U, V}$ of input/output neurons
      \STATE \textbf{1.} Compute normalized similarity matrix:
      \STATE \quad $D_U, D_V \gets \text{diag}\left(\sum_i A_{i, \cdot}\right), \text{diag}\left(\sum_j A_{\cdot, j}\right)$
      \STATE \quad $\tilde{A} \gets D_U^{-1/2} A D_V^{-1/2}$
      \STATE \textbf{2.} Perform Singular Value Decomposition (SVD):
      \STATE \quad $U, \Sigma, V^T \gets \text{SVD}(\tilde{A}, k)$
      \STATE \textbf{3.} Perform KMeans clustering:
      \STATE \quad $\text{U}, \text{V} \gets \text{KMeans}(k, U), \text{KMeans}(k, V^T)$
      \STATE \textbf{Return:} Clusters $\text{U}, \text{V}$
   \end{algorithmic}
\end{algorithm}

Our clustering method, called Bipartite Spectral Graph Clustering (BSGC) is shown in Algorithm \ref{alg:spectral_clustering}. The similarity matrix for BSGC can be created by either the weights of the model or the accumulated gradients.

\paragraph{Weight-based BSGC.}
\label{subsec:weight-bsgc}

Here, we use the weight matrix of a layer as the similarity matrix between neurons of adjacent hidden layers, based on the idea that neurons with strong weights connecting them can be expected to cluster well.


\paragraph{Gradient-based BSGC.}
\label{subsec:grads-bsgc}

Analyzing the gradients of each parameter during training gives us another way to cluster models. The idea is that weights that update together are likely to be part of the same circuit and connect neurons that cluster well together. We set the similarity matrix in Algorithm~\ref{alg:spectral_clustering} to the average cosine similarity of the gradients of each parameter.

\begin{figure}[h]
\vspace{-0.2cm}
\centering
\includegraphics[width=0.45\textwidth,  trim=2 2 40 30, clip]{figures/clusterability_scores.pdf}
\vspace{-0.1cm}
    \caption{Clusterability of the model with $k$ clusters using Algorithm~\ref{alg:spectral_clustering} on CIFAR-10. Note that while gradient-based BSGC helps, the resulting clusters are still highly enmeshed and not far above random clusterability, which is $\frac{1}{k}$.}
    \label{fig:clusterability}
\end{figure}

In Figure \ref{fig:clusterability}, we see that as the number of clusters increases, the amount of clusterability decreases, and even at $k=2$ clusters, we get $C=0.6$ (random is 0.5, higher is better), which is substantial interference between clusters and hinders our interpretability goals. We show similar results for Pythia-70m \cite{biderman2023pythiasuiteanalyzinglarge} in Appendix \ref{app:pythia}.

\section{Methodology}
\label{sec:methodology}

Across a range of models and tasks, we optimize for modularity by training to maximize the clusterability (as defined in Section \ref{sec:clusterability}), and use various automated interpretability methods to evaluate the interpretability gains of the clusters thus obtained. This section details our methodology, and in Section \ref{sec:results} we share our results.

\subsection{Training for Modularity}
\label{subsec:training}

We train models to be modular by following a training pipeline that comprises the following steps:

\begin{enumerate}
    \item Train the original model to minimize cross-entropy for the first $t$ steps. This can help to form simpler features before we begin clustering and allow ``winning tickets'' (as defined in the lottery ticket hypothesis \citep{Frankle2018TheLT}) to emerge. In practice, we found that our results do not vary with $t$, and default to $t=0$ for most results.
    \item For the set $U$ of model components to cluster, use a clustering method to cluster them and calculate the clusterability loss for each component. Here, we find that gradient-based BSGC (see Algorithm \ref{alg:spectral_clustering}) leads to slightly better initializations (see Figure \ref{fig:clusterability}), but the gains do not remain after our training, which is why we use arbitrary clustering instead. Without loss of generality, we make contiguous clusters of the same size for our experiments for better visualization.
    \item Calculate the effective loss function $\mathcal{L}_{\text{eff}} = \mathcal{L}_{\text{CE}} + \lambda \displaystyle\sum_{u \in \mathcal{U}} \mathcal{L}_{\text{C}}^u$, where $\lambda$ is a hyperparameter that controls the trade-off between performance and modularity. We share results for training on $\lambda=40$, but have found results to be stable across various values of $\lambda$.
    \item Complete the rest of the training for $t$ steps to minimize $\mathcal{L_{\text{eff}}}$ to promote the clusters to be modular (see Section \ref{sec:clusterability} for more details).
\end{enumerate}

We experiment with vanilla MLPs on MNIST \citep{deng2012mnist}, and CNNs on the CIFAR-10 dataset \citep{krizhevsky2009learning}, transformers on modular addition, and fine-tuning of large language models (LLMs). We compare the performance, clusterability, and interpretability gains of our clustered models against models trained without the clusterability loss. Our results are available in Section \ref{sec:results} and all the hyperparameters we use are given in Appendix \ref{app:hyper}.

\begin{figure}[h]
\centering
    \includegraphics[width=0.45\textwidth,  trim=2 2 0 50, clip]{figures/modular-arithmetic-training.pdf}
    \caption{Clusterability against the train and test losses of a 1-layer transformer model trained modularly on modular addition. For this task, we show that a model can be trained to completely solve the task while making every model component completely clusterable.}
    \label{fig:mod-mod-training}
\end{figure}

For modular addition, we follow the training pipeline of \citet{nanda2023progressmeasuresgrokkingmechanistic} to train a 1-layer transformer model and increase the training data to reduce ``grokking'', or the delayed generalization effect they studied. Instead, we focus on training the model with the clusterability loss, and show in Figure \ref{fig:mod-mod-training} that we can train the model to completely solve the task while making every model component completely clusterable. See Appendix \ref{app:arithmetic} for an exploration of the interpretability gains from modularity in this setting.

For language models, we fine-tune our models on a subset of the Wikipedia data \cite{merity2016pointer}, while making the MLP input and output matrices to be modular. For individual layers, we define the ``maximum clusterability'' of a layer to be the maximum amount of clusterability that can be achieved without loss of performance. We measure this by training the model with the clusterability loss until the clusterability stops improving or the performance starts degrading. We show the maximum clusterability for the MLP input and output matrices of each layer in Figure \ref{fig:mlp}.

\begin{figure}[h]
\centering
    \includegraphics[width=0.45\textwidth,  trim=10 50 40 10, clip]{figures/clusterability_mlp.pdf}
    \caption{Maximum clusterability achievable for each layer's MLP input and output matrices (independently) for GPT2-small without degradation in performance. We found the MLP input matrices to have higher maximum clusterability compared to the MLP output matrices, except in the first layer.}
    \label{fig:mlp}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.45\textwidth,  trim=10 10 40 50, clip]{figures/wiki_modular_mlp_in_training.pdf}
    \caption{Combined clusterability of the MLP input of all layers in GPT2-small and the cross-entropy training loss on a subset of the Wikipedia dataset during training for modularity.}
    \label{fig:wiki-training}
\end{figure}

We show the training plots for modular training of all MLP\_in weight matrices with the cross-entropy training loss in Figure \ref{fig:wiki-training}. We checkpoint the model after the first epoch to avoid overfitting and study the model for interpretability gains, comparing it with a model fine-tuned on the same data for the same number of epochs but without the clusterability loss.

\subsection{Evaluating Interpretability Gains}
\label{subsec:evals}

We use various automated interpretability metrics to evaluate our clustered models and their interpretability gains. In this section, we define these metrics: class-wise accuracy (with and without each individual cluster), and Effective Circuit Size (ECS) for each label. (See Section \ref{sec:results} for our results on these metrics.)

\subsubsection{Interventions}
\label{subsec:interventions}

We perform two types of interventions on clusters to evaluate their contribution toward a model's computation:
\begin{enumerate}
    \item \textit{Type 1 / ON-intervention}: Here, we turn off (zero-ablate) the activations of all the other clusters, and run the model's forward pass using just the activations of a single cluster. This gives us a measure of the contribution of a cluster on its own toward predicting any given class or datapoint, which tells us whether a cluster is sufficient.
    \item \textit{Type 2 / OFF-intervention}: Here, we switch off a given cluster, and keep the activations of all the other clusters. This gives us a measure of how well all the other clusters combined can predict any given class or datapoint, which tells us whether a cluster is necessary.
\end{enumerate}

\subsubsection{Effective Circuit Size (ECS)}\label{sec:circuit-size}

We use automated pruning to recursively remove edges that do not contribute (see Automated Circuit DisCovery \citep{conmy2023towards}) to extract the `effective circuit' for each label and define the ratio of the number of parameters in it to the number of parameters in the whole model to be the effective circuit size (ECS) for that label and model. A lower effective circuit size indicates that the model has learned a smaller task-specific circuit, and reduction in ECS is a proxy for interpretability gains. Since recursively removing edges is expensive, we only show results on simpler models, and leave more efficient pruning techniques for future work.

% \subsubsection{Task Specialization in Modular Language Model (LLM) Fine-tuning}
% \label{sec:train-llm}

\section{Results}
\label{sec:results}

\subsection{Task-Specialization of Clusters in a Network Trained to be Modular}\label{sec:cluster-specialization}

\subsubsection{Clusters Specialize in Class-level Features for MLPs and CNNs }

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth,  trim=2 2 300 60, clip]{figures/classwise_cluster_all_accuracies.pdf}
    \caption{Class-wise accuracy for each label with clusters turned ON (top) and OFF (bottom). Note that individual clusters learn near-complete circuits for various labels, such as Cluster $0$ for \textit{SHIP} and Cluster $1$ for \textit{TRUCK}. The figure on the top shows that a cluster is ``sufficient'' to predict a given class, while a dip in the accuracy in figure on the bottom shows that it is ``necessary''. For instance, note that Cluster $3$ is both necessary and sufficient for completely identifying a ``deer''.}
    \label{fig:classwise-accuracies}
\end{figure*}

Figure~\ref{fig:classwise-accuracies} compares class-wise accuracy of the model on the CIFAR-10 dataset with individual clusters turned OFF and ON respectively in a clustered model with an accuracy $>95\%$ for each label. 
We find individual clusters learning near-complete circuits for various labels for both kinds of interventions described in Section \ref{subsec:interventions}. 
In other words, clusters specialize on certain labels.

In the top row we see there are a number of labels for which a cluster is able to perform well even when it is turned on in isolation. This means that the cluster is sufficient for performance on that label. 
Whenever performance plummets in the bottom row (when a specific cluster is turned off) this means that a specific cluster was necessary for performance.
For example, cluster 1 is both sufficient and necessary for classifying the label truck.


% \textcolor{red}{TODO: We don't have enough to back this up. Keep this to 1-2 lines or remove it (or future work might..).}

% Another interesting observation is the fact that a cluster learns classes that require similar semantic features to learn. For example, the first cluster, which primarily learns predicting a ``ship'', also helps with ``airplane'' and ``bird'', all of which share characteristics such as lack of color and legs and a pointy front-end (nose or beak). We hope that such semantic correlations can help, for instance, restrict related harmful behavior only to certain modules, making them easier to study and remove.

Future work may find that there are high-level semantic themes that different clusters learn. 
For example, the first cluster, which primarily learns predicting a ``ship'', also helps with ``airplane'' and ``bird'', all of which share a pointy front-end (nose or beak).

\begin{figure*}[t]
  \centering
  \subfloat[Fraction of samples vs. the number of clusters that contribute to them]{\includegraphics[width=0.45\linewidth,  trim=2 2 50 25, clip]{figures/gpt2_small_left.pdf}\label{fig:f1}}
  \hspace{0.02\linewidth}
  \subfloat[Number of samples each cluster contributes to]{\includegraphics[width=0.45\linewidth,  trim=2 2 50 25, clip]{figures/gpt2_small_right.pdf}\label{fig:f2}}
  \caption{Fraction of samples (left) that need $k$ clusters for correct prediction, for $k\in[1,2,3,4]$ for a clustered model with $4$ clusters. Note that in a modular model, most datapoints can be solved by $1-2$ clusters, while $3-4$ are required for a large fraction in a non-modular model fine-tuned on the same data. Figure (b) on the right shows the number of datapoints each cluster contributes to, indicating that a modular model reduces the number of clusters involved and keeps them all similarly useful.}
  \label{fig:llm-fractions}
\end{figure*}

\subsubsection{Task Specialization in Modular Language Model (LLM) Fine-tuning}
\label{sec:modular-llm}

We also do an ON-intervention investigation on GPT2-small fine-tuned on wiki data.
In Figure \ref{fig:f1}, we see the fraction of samples (left) that need $k$ clusters for correct prediction, for $k\in[1,2,3,4]$ for a clustered model with $4$ clusters. 
Note that in a modular model, most datapoints can be solved by $1-2$ clusters, while $3-4$ are required for a large fraction in a non-modular model fine-tuned on the same data. 
See Appendix \ref{app:explanation-LLM-values} for a more elaborate explanation of these values.

Figure \ref{fig:f2} on the right shows the number of datapoints each cluster contributes to, indicating that a modular model reduces the number of clusters involved and keeps them all similarly useful.

% \textcolor{red}{TODO: Add Pythia results to this if/when we have them.}

% \subsubsection{RAVEL}
% \label{subsec:ravel}

% \textcolor{red}{TODO: Add RAVEL results (in a table?) and write a paragraph on them.}

\subsection{Modularity Encourages Smaller Circuits}\label{sec:circuit-size}

Figure \ref{fig:ecs} compares the Effective Circuit Size (ECS) for each label for the clustered and unclustered models on CIFAR-10 (see Section \ref{sec:methodology}). We show that an unclustered model has, on average, $61.25\%$ more parameters in its effective circuits. In Appendix \ref{app:mnist}, we share similar results for the MNIST dataset \citep{deng2012mnist}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth,  trim=2 2 50 25, clip]{figures/ecs_pruned_all_labels.pdf}
    \caption{Percentage increase in Effective Circuit Size (ECS) for each label (as a fraction of the whole model) from the clustered to the unclustered models. Larger arrows denote a larger change in ECS.}
    \label{fig:ecs}
\end{figure*}
% \textcolor{red}{TODO optional: Add theoretical analysis of why ACDC is cheaper on four modules than on one large model.}

\section{Theoretical Analysis of the Effects of Modularity}\label{sec:theoretical}


\subsection{Polytopes in a Modular Network}

We show that the partitioning of neural networks into polytopes on which the network is linear \citep{sudjianto2020unwrapping} becomes coarser when we make a model modular. In other words, the simplicity of the function space of a network increases by adding modularity constraints. In this section, we restrict to fully connected ReLU feed-forward neural networks. 

\begin{definition}[ReLU Network]
A ReLU network $\mathcal{N} \colon \mathbb{R}^n \rightarrow \mathbb{R}^m$, is a composition of $L \in \mathbb{N}$ hidden layers given by $\chi^{(l)} = \sigma( {W^{(l)}} \chi^{(l-1)} + b^{(l)})$, where $\sigma$ is an element-wise ReLU activation function, $\sigma(x_i) = \max\{0,x_i\}$. We define $\chi^{(0)} = x$ and the output layer is given by 
\[\mathcal{N}(x) = W^{(L+1)} \chi^{(L)} + b^{(L+1)}.\] 
The number of neurons in each layer is given by a vector $\mathbf{N} = [n_1, n_2, \ldots, n_L]$, and all activations are in the positive reals, i.e. $\chi^{(l)} \in \mathbb{R}^{n_l}_{\geq 0}$ for all $l \in \{1,\ldots,L\} = [L]$. We stress the dependence on $x$ by writing $\chi^{(l)}(x)$.
\end{definition}

ReLU networks can be described as linear models that are applied to certain regions of the input space \citep{sudjianto2020unwrapping}. These regions partition the input space $\mathbb{R}^n$. 

\begin{proposition}\label{prop:network-to-partition}[\citet{sudjianto2020unwrapping}]
For a ReLU network $\mathcal{N} \colon \mathbb{R}^n \rightarrow \mathbb{R}^m$ there is a finite partition $\Omega$ of $\mathbb{R}^n$ of cardinality $p := \# \Omega$ such that for each part $\omega \in \Omega$ there exists a piece-wise linear function $f\colon \mathbb{R}^n \rightarrow \mathbb{R}^m$, and its restriction on $\omega$, denoted $f|_\omega$, can be described by a linear function: 
\[f|_\omega (x) = \alpha_\omega^T x + \beta_\omega.\]
Moreover, each part is a  polytope, given by the intersection of a collection of half-spaces.
We write the minimum set of half-space conditions that can be used to specify the entire partition as $H_1,\ldots,H_k$, where each $H_i$ is given by the set of all $x \in \mathbb{R}^n$ such that, for for $h_{i,j} \in \mathbb{R}, i\in [k], j\in [n]$:
\[ h_{i,1} x_1 + h_{i,2} x_2 + \ldots + h_{i,n} x_n  > c_i.\]
\end{proposition}

In some sense, the number of polytopes gives a measure of the granularity or refinement of a network.
Given a network with neurons $\mathbf{N} = [n_1, n_2, \ldots, n_L]$, 
there are at most $2^{n_1} \times 2^{n_2} \times \ldots \times 2^{n_L}$ polytopes. Now suppose that the weight matrix $W^{(l)}$ in layer $\chi^{(l)}$ is modular and contains $k$ modules, meaning that we can divide $n_{l-1}$ and $n_{l}$ into $k$ sets $[n_{l-1}^1, \ldots, n_{l-1}^k]$ and $[n_{l}^1, \ldots, n_{l}^k]$ such that the only non-zero weights in $W^{(l)}$ are between $n_{l-1}^i$ and $n_{l}^i$.
In this case the number of half-space conditions generated by $\chi^{(l-1)}$ and $\chi^{(l)}$ is no longer $2^{n_{l-1}} \times 2^{n_l}$, but it now is $2^{n_{l-1}} \times 2^{n_{l}^1} + \ldots + 2^{n_{l-1}} \times 2^{n_{l}^k}$, which is substantially fewer.
This means that modularity increases the simplicity of the function-space of a network at the cost of expressivity.

\subsection{Concept Representations in Activation Space}\label{sec:theoretical-representations}

Based on the Johnson-Lindenstrauss lemma we will show that the number of nearly orthogonal points that can be represented in a layer goes down when we make the layer modular.

\begin{proposition}[Johnson-Lindenstrauss lemma \citep{johnson1984extensions}]
Let $0< \epsilon < 1$ and let $X$ be a set of $m$ points in $\mathbb{R}^N$ and $n > \frac{8(ln(m))}{\epsilon^2}$,
then there exists a linear map $f \colon \mathbb{R}^N \rightarrow \mathbb{R}^n$ such that for all $u,v \in X$ we have
\[ (1- \epsilon) \| u - v\|  \leq \| f(u) - f(v) \|^2 \leq (1+ \epsilon) \| u - v \|^2. \]
\end{proposition}

In other words, a set of $m$ nearly orthogonal points in $\mathbb{R}^N$ can be projected onto nearly orthogonal directions in a much smaller dimensional space $\mathbb{R}^n$,
as long as $n > \frac{8(ln(m))}{\epsilon^2}$ or equivalently $ e^{\frac{ \epsilon^2 }{8} \cdot n} > m $.
This means that the number of points that can be projected orthogonally onto $\mathbb{R}^n$ is exponential in $n$.
% Q: Am I phrasing this correctly?

Given a network with neurons $\mathbf{N} = [n_1, n_2, \ldots, n_L]$ where the weight matrix $W^{(l)}$ in layer $\chi^{(l)}$ is modular and contains $k$ modules such $n_{l-1}$ and $n_{l}$ can be divided into $k$ sets $[n_{l-1}^1, \ldots, n_{l-1}^k]$ and $[n_{l}^1, \ldots, n_{l}^k]$ such that the only non-zero weights in $W^{(l)}$ are between $n_{l-1}^i$ and $n_{l}^i$.

Then the number of nearly orthogonal points that can be represented in layer $n_l$ becomes $~ e^{n_{l}^1} + \ldots + e^{n_{l}^k} $ as opposed to $~ e^{   {n_{l}^1} + \ldots + {n_{l}^k} }$. 
This means that as a result of modularizing the weight matrix $W^{(l)}$, the model learns to perform computation by using features from an exponentially smaller set of subspaces. Sparse Autoencoders (SAEs) \cite{cunningham2023sparseautoencodershighlyinterpretable} extract linear subspaces as latents for human-interpretable features, and modularity helps reduce the search space for SAEs exponentially. Training SAEs on modular models and clusters is an interesting direction for future work.

\section{Conclusion}

We show that a simple regularizer is effective at splitting a neural network layer into simpler, more interpretable clusters. We show that the average circuit size and the search space improve with more clusters without a decrease in overall performance (for the models and tasks that we investigated).

Future work could study the Pareto frontier (clusterability versus performance) as we scale to harder tasks and larger models. We are also interested in using our insights to train and align language models with modularity and see if it leads to better control for harmful behaviors. We hope that modularity can help with a number of mechanistic interpretability goals, and a scaled-up exploration into this (such as clustering attention heads and training SAEs) is an interesting direction for future work.

\clearpage

\section{Acknowledgments and Disclosure of Funding}

We would especially like to thank Dylan Cope, Daniel Filan, Sandy Tanwisuth, Siddhesh Pawar, Niels uit de Bos, Matthew Wearden, and Henning Bartsch for valuable discussions, feedback, and support. SG,NS would like to thank the ML Alignment Theory \& Scholars (MATS) Program, the organizers, funders, and staff. SG was supported by independent research grants from AI Safety Support and the Long-Term Future Fund. This research was supported by the Center for AI Safety Compute Cluster.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors.