\section{Introduction}

The increase in hate speech on online platforms is a significant concern.
Numerous studies and reports have shed light on the prevalence and impact of toxic behavior online, providing substantial evidence of its pervasive nature.
According to a study conducted by the Anti-Defamation League (ADL), two in five Americans have experienced online harassment, indicating a significant proportion of the population affected by toxic interactions~\cite{adl2017online}.
Additionally, Amnesty International reported that one in four women have experienced online abuse, highlighting the gendered dimension of online toxicity~\cite{amnesty2018toxictwitter}. 
%Moreover, the rise of online communities has accelerated the spread of hate speech~\cite{fortuna2018survey}, making it easier for derogatory terms to reach a wider audience rapidly.
%Aggression attacks have become a common occurrence on platforms like Facebook, Twitter, and Youtube~\cite{blondel2008fast,hine2017kek,phadke2020many,ribeiro2018characterizing}, especially for women~\cite{chatzakou2017hate,chess2015conspiracy,vitak2017identifying} or users who identify as minorities~\cite{relia2019race}.
%The prevalence of hate speech on social media can be attributed to various factors, like the text-centric format of the platforms, which provides users with a wider range of means to express their thoughts, opinions, and emotions with less inhibition and self-awareness as opposed to face-to-face interactions~\cite{iftikhar2023together}.
%Additionally, the echo chamber effect exacerbates the problem, as social media algorithms often reinforce users' existing beliefs, isolating them from opposing viewpoints~\cite{bakshy2015exposure,kumar2016disinformation,flaxman2016filter}.
%The harmful effects of hate speech on society are significant, making it crucial to increase awareness of its dangers.

The task of moderating hate speech on social media is complex and has received considerable attention, as seen in various recent studies~\cite{Poletto2020ResourcesAB,caselli-etal-2021-hatebert}.
%Research has delved into the impact of social media on the spread of hate speech, analyzing user behavior and the role of algorithms in amplifying or filtering such content, as well as detecting instances of hate speech~\cite{davidson2017automated}.
Nevertheless, finding viable solutions to address this problem remains a difficult task.
Human moderation, although valuable, is limited in its scalability and often relies on volunteers who must sift through vast amounts of disparaging and hateful content~\cite{jhaver2019human,paudel2023lambretta}.
Moreover, these moderators frequently face personal threats~\cite{kozyreva2023resolving, york2019moderating} during the process. Larger platforms like Facebook, Twitter, and Discord have started adopting automated content moderation techniques, including tools capable of detecting and filtering out hate speech from social media platforms~\cite{fortuna2018survey}.
In this regard, annotated datasets and benchmarking tools are crucial resources due to the numerous supervised approaches that have been proposed~\cite{Poletto2020ResourcesAB}.
However, these systems use lexicon-based detection, which relies on pre-constructed hate lexicons.
This method may not be sufficient as toxic language, including slurs and hateful words, can change over time~\cite{waseem2016hateful}.
Consequently, the effectiveness of these lexicons may decline, making them inadequate for identifying hate speech that targets communities that were not included in the original datasets used to compile these lexicons.

%One of the key factors for the evolution of language is cultural and societal change, which shapes the language people use and the taboos associated with certain terms. As societal norms shift, some once acceptable words may become widely recognized as offensive or derogatory~\cite{giles1975speech,bender2018data}.
Conversely, new slurs and hateful words may emerge as language adapts to reflect emerging prejudices or target marginalized groups~\cite{schild2020go,young2021young,gao-etal-2017-recognizing}.
Another significant factor driving the evolution of toxic language is the deliberate attempts by aggressors to evade detection.
These individuals actively modify their language by introducing spelling errors, substituting non-toxic words for toxic ones, or employing other creative techniques to circumvent content moderation systems~\cite{fortuna2018survey}.
This cat-and-mouse game between aggressors and moderation methods further emphasizes the need for robust and adaptive models, capable of recognizing not only explicit toxic language but also its subtle variations and disguised forms. Therefore, to make online spaces safer and more inclusive, it is important to keep up with evolving patterns and strategies used by aggressors.
It is important to note that the impact of toxic language extends beyond its initial usage. The words and slurs used in the past continue to resonate throughout history, and their effects can persist, perpetuating harmful stereotypes and reinforcing systemic discrimination.
Understanding how toxic language evolves and spreads is crucial in combating its harmful effects.

Our research aims to shift the paradigm of automated content moderation from reactive, one-size-fits-all measures to content-aware, adaptive solutions. A critical gap in the existing literature lies in the limitations of current hate speech detection systems, which often depend on outdated, static lexicons. As language evolves, these static approaches struggle to keep pace, leading to decreased accuracy and effectiveness in identifying toxic language in real-time. For instance, when we tested models provided by Davidson et al.~\cite{davidson2017automated} on a newer dataset of social media posts, we observed a significant drop in accuracy from 90\% to 76\%. This highlights the need for approaches that can dynamically adapt to the evolving nature of language.

\textcolor{black}{Widely used systems for hate speech detection, such as Google's Moderate Hatespeech~\cite{moderatehatespeech}, rely on transformer models like RoBERTa, which excel in this domain. To ensure robust evaluation, we benchmark our models against these established baselines. Although generative models, such as GPT-3.5, have demonstrated better generalization in zero-shot hate detection, they face limitations in precision and recall~\cite{Pendzel2023GenerativeAF}. Additionally, while augmenting training datasets with synthetically generated hate speech has shown promise in improving detection performance~\cite{Cao2020HateGANAG, Wullach2020TowardsHS}, generative models remain underutilized in mainstream hate speech detection due to their limited effectiveness in detecting hatespeech~\cite{Pendzel2023GenerativeAF} or counterspeech generation~\cite{Bar2024GenerativeAM}. These gaps highlight the need for more hybrid models.} Therefore, our work focuses on comparisons with proven baselines, emphasizing the development of a fast, scalable, and adaptive solution capable of addressing the challenges posed by evolving hate speech. A key objective of our work is to develop a solution that is fast, scalable, and adaptive, enabling seamless integration with existing approaches while addressing the evolving nature of hate speech.
%Our approach fills this gap by developing a hybrid model that embraces the dynamic nature of language, allowing it to adapt to new linguistic patterns, emerging slurs, and evolving expressions of hate speech.  

Through our research, we address the following high-level research questions:

\begin{itemize}
	\item \textbf{RQ1 -- \textit{Adaptive Improvement of Lexicons:}} How can we update hate speech lexicons so that they are better aligned with the dynamic nature of evolving language?
	
	\item \textbf{RQ2 -- \textit{Hybrid Approach to Risk Detection:}} Can we improve the accuracy of hate speech detection systems using the updated lexicons?
\end{itemize}

%Addressing RQ1 is crucial for identifying promising directions in adaptive hate speech detection algorithms, especially in the face of evolving toxic language. Furthermore, answering RQ2 can assist researchers and platforms in designing hate speech detection systems that adopt a more hybrid approach.
To answer these questions, we utilize two distinct sets of posts: a human-annotated dataset containing approximately 100k posts from Twitter from 2016 to 2017, provided by Founta et al.~\cite{founta2018large}, and another dataset comprising 76,378 random posts collected by the authors between 2021 and 2022.
Our findings for RQ1 demonstrate that by leveraging a seed set of hate speech lexicons we can find other contextually similar hate speech lexicons.
Regarding RQ2, we discover that incorporating these updated lexicons leads to improved accuracy (up to 95\%) in detecting hate speech in social media posts.
Interestingly, our investigation reveals the emergence of contemporary hate speech lexicons that exhibit greater prevalence in today's context.
In summary, our paper makes the following empirical contributions:

\begin{itemize}
\item We propose an adaptive approach using word embeddings to detect and flag toxic language considering the dynamic evolution of hate speech.
	
\item We evaluate the limitations of existing approaches to automatically detect and block toxic language, and demonstrate their reduced effectiveness in identifying hate speech that targets communities not included in the datasets used for lexicon construction.
	
\item We develop a hybrid model that can adapt to the evolving nature of hate speech, by integrating both lexicon-based and unsupervised learning techniques to improve its accuracy and effectiveness over time.

%\item We find that our model can detect emerging lexicons and intentional spelling errors that are deliberately employed to evade detection.
	
%\item Based on the findings of our research and analysis of existing approaches, we provide practical recommendations for how to design and implement effective systems for detecting and flagging toxic language.
	
\end{itemize}