\section{Dataset}
In this section, we provide an overview of our data collection pipeline.
We begin by describing our process for collecting seed lexicons and then providing details of the social media posts we use for hate speech detection.

\subsection{Collecting Seed List of Hate Words}

To begin our analysis we collect different databases of hate speech lexicons, starting from those that are made publicly available by non-profit organizations~\cite{google-profanity-words-node,hatebase,badwordslist} and those released by academia~\cite{rezvan2018quality,Bassignana2018HurtlexAM,Wiegand2018InducingAL}. 
%This initial collection effort has been based on a thorough literature review to identify additional lexicons, starting from going through existing systematic literature review in this area~\cite{Poletto2020ResourcesAB}.
Our goal for this phase is to assemble a representative list of toxic words that cover the different contexts in which hate speech can manifest itself on social media, including bullying~\cite{rezvan2018quality}, sexual harassment~\cite{razi2020lets,nagar2021holistic}, profanity~\cite{google-profanity-words-node}, and racism~\cite{hatebase}.
Table~\ref{tab:lexicons} lists six popular lexicon databases.

\begin{table}[ht]
	\begin{center}
	\small\begin{tabular}{lc}
		\toprule
		\textbf{Name/Reference} & \textbf{Size of Lexicons}\\
		\midrule
		Hatebase~\cite{hatebase} & 1,565 \\
		Google Profanity Words~\cite{google-profanity-words-node} & 958 \\
		Google Code Archive~\cite{badwordslist} & 458 \\
            Lexicon of Abusive Words~\cite{Wiegand2018InducingAL} & 1650 \\ 
		Hurtlex ~\cite{Bassignana2018HurtlexAM} & 11,008 \\ 
		Corpus for Harassment~\cite{rezvan2018quality} & 737\\
		\bottomrule \\
	\end{tabular}    
        \caption{Popular Hate Speech Lexicons}
        \label{tab:lexicons}
\end{center}
\end{table}

After identifying suitable lexicons, we preprocess our collected lexicons to make them suitable for the following word-embedding and classification phases.
Previous work with the Hatebase dataset~\cite{hine2017kek} highlights that several words that can be used in a toxic context are highly contextual, and are most of the
time used in regular, harmless settings instead.
For example the word ``India,'' which is contained
in the Hatebase lexicons~\cite{hatebase} as a slur but is used as a benign word in the overwhelming majority of cases. Including these words from our lexicons is problematic, since the subsequent steps of our approach would learn their benign use in language and end up flagging other benign words as potential hate speech (for example other country names being semantically similar to ``India'' will be added to our list of hate speech lexicons).
To avoid these issues downstream in our analysis, we carefully sanitize the dataset by removing
highly contextual words. 
We process each lexicon as follows:
\begin{itemize}
	\item Remove words that do not belong to the English language
	\item Convert all words to lowercase
	\item Remove repeated entries in different lexicons
        \item Remove generic words by cross-referencing the NLTK~\cite{nltk} stopword list and filtering out non-discriminative terms identified in the HateCheck~\cite{rottger-etal-2021-hatecheck} dataset.
\end{itemize}
The output of this phase is a comprehensive list of 749 toxic words that will be used throughout the rest of the paper. 

\subsection{Collecting Social Media Posts for Evaluation}\label{testset}
After collecting a comprehensive seed set of toxic words, we move on to collecting a corpus of social media posts to perform classification as well as build our word-embedding models.
We employ Twitter as the social media data source because of its growing public footprint including 486 million active users, averaging 700 followers each~\cite{twitter-carbon-footprint}.
To conduct our analysis, we utilize multiple publicly available datasets as well as our own collected dataset for comparison.

\descr{Ground Truth Annotations.}
We begin our analysis by using the human-annotated social media posts publicly made available by six different research datasets:

%Founta et. al.\cite{founta2018large}.
%This dataset contains approximately ~100k manually annotated posts.
%The researchers have introduced an incremental and iterative method that utilized crowdsourcing to label a vast array of posts with abusive-related tags, like ``hateful,'' ``abusive,'' ``spam,'' and ``normal.''
%According to the paper, annotating the posts was a laborious and time-intensive task that demanded substantial effort. However, it yielded a vast and resilient collection of annotated posts that could be utilized for additional analysis.
%We combine the hateful and abusive posts into an umbrella term ``hate speech,'' and discarded all the posts labeled as ``spam''.
%This provided us with 32,115 posts labeled as hate speech and 53,851 labeled as normal.


\begin{itemize}
    \item \textbf{\citeauthor{davidson2017automated}\cite{davidson2017automated}.} This dataset consists of 24,783 tweets annotated as hate speech, offensive language, or neither. 

    \item \textbf{\citeauthor{founta2018large}\cite{founta2018large}.} This dataset contains approximately ~100k manually annotated posts. We combine the hateful and abusive posts into an umbrella term ``hate speech,'' and discarded all the posts labeled as ``spam''. This provided us with 32,115 posts labeled as hate speech and 53,851 labeled as normal.

    \item \textbf{Implicit Hate Corpus~\cite{elsherief-etal-2021-latent}.} This dataset comprises over 22,056 tweets, 6,346 of these tweets contain implicit hate speech. This dataset is meticulously annotated, with posts categorized into implicit hate, explicit hate, or non-hate.

    \item \textbf{HateCheck~\cite{rottger-etal-2021-hatecheck}.} The dataset includes 3,728 test cases covering 29 categories of hate speech, along with non-hateful counterparts to test for false positives. HateCheck offers a rigorous benchmark for evaluating and improving hate speech detection systems, making it an essential tool for researchers developing more reliable detection models.

    \item \textbf{ToxicSpan~\cite{pavlopoulos-etal-2022-acl}.}The dataset comprises 10,000 social media comments, meticulously annotated to identify toxic language. This detailed annotation enables a nuanced understanding of harmful phrases within a broader context, essential for more precise moderation strategies.

    \item \textbf{ToxicGen~\cite{hartvigsen-etal-2022-toxigen}.} The dataset comprises over 274,000 synthetic hate speech examples, generated using a large-scale language model fine-tuned to produce toxic content targeting various demographic groups. These synthetic examples are crafted to cover a wide array of hate speech categories, including racism, sexism, and xenophobia, allowing researchers to train models on diverse and extensive hate speech scenarios. 
    
\end{itemize}


\descr{Complimentary Dataset for Qualitative Analysis.}
We collect our second set of social media posts through a random sampling approach, utilizing the Twitter 1\% Public Streaming API from January 2021 to December 2022.
This API serves as a valuable resource for developers, granting access to a real-time stream of approximately 1\% of the public posts.
We gathered a dataset of 76,378 posts, randomly sampled from 100 different dates, to ensure a diverse and representative collection of data.
We use this dataset to uncover new instances of derogatory language and gain insight into how toxic behavior is concealed in real-time data.

We tested our approach using both older datasets, such as the one provided by Davidson et al.~\cite{davidson2017automated}, and newer datasets, including 76,378 randomly sampled social media posts from 2021–2022. This combination allows us to rigorously evaluate the temporal adaptability of our method. By comparing the performance on older datasets with lexicons updated using newer data, we demonstrate how our approach bridges the gap between legacy systems and modern language trends. Additionally, benchmark datasets like HateCheck~\cite{rottger-etal-2021-hatecheck} and ToxicSpan~\cite{pavlopoulos-etal-2022-acl} were included to evaluate our system’s robustness in detecting nuanced and emerging forms of toxic language.

\descr{Ethics.} Since we only use publicly available data and do not interact with human subjects, our work is not considered human subjects research by our IRB.
Nonetheless, we follow standard ethics guidelines: when presenting examples, we remove any personally identifiable information such as usernames, and ensure that user anonymity is maintained and not compromised.