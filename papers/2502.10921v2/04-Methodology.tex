\section{Methodology}
In this section, we discuss our adaptive method for updating hate speech lexicons in detail as well as the machine learning approaches we use.
We adopt both traditional supervised-learning approaches and deep-learning models to compare the accuracy of detecting hate speech using the seed lexicons and the updated lexicons. Figure~\ref{fig:pipeline} provides a high-level description of our adaptive approach to hate speech detection.
Later we propose a novel hybrid approach to hate speech detection that utilizes both lexicon-based and unsupervised learning approaches.

 \begin{figure*}[t!]
 	\centering
 	% \includegraphics[width=0.9\textwidth]{Figures/P4.png}
    \includegraphics[width=0.70\textwidth]{Figures/adaptive-toxicity-pipeline.pdf}
 	\caption{Architecture of our adaptive hate speech detection system.}
 	\label{fig:pipeline}
 \end{figure*}

\subsection{Step 1: Identifying Candidate New Toxic Words}
The first step in our pipeline after data collection is to update the lexicons.
The goal of this step is to pinpoint harmful words that are utilized similarly to already established toxic words so that they are relevant to the piece of text in which we are determining hate speech.
For example, to avoid censorship users use the word ``ducking'' instead of ``fucking''~\cite{theverge2023}.
We label all the seed lexicons as $S_{lexicons}$ and the updated lexicons as $U_{lexicons}$ respectively.
The updated lexicons contain both the seed lexicons as well as the new lexicons that we find using word-embedding models. Note that word embedding models allow us to identify \textit{contextually similar words} that may or may not be synonymous.

%\shiza{Seed Lexicons -- ($S_{lexicons}$) \\
%Updated Lexicons -- ($U_{lexicons}$) \\
%Word Embedding Lexicons -- ($W_{lexicons}$)}

To this end, we adopt a \textit{similarity-based} approach to find new toxic words.
For each word appearing in our dataset, we compute its vector embedding.
We test different approaches to find similar words, i.e., Word2Vec~\cite{tensorflowWord2vec}, GloVe~\cite{glove}, and more modern word embedding techniques like BERT~\cite{bert}. %In the following list, we go through each word embedding model in more detail.

% is more detail required
%\begin{itemize}
    %\item \textbf{Word2vec:} Word2Vec, introduced by Mikolov et al.~\cite{mikolov2013efficient} transforms words into continuous vector representations, to enable mathematical operations on words. %For our research, we employ the Word2Vec algorithm to learn distributed representations of words in the corpus.
    %By training the model on the preprocessed dataset, we obtain dense vector representations that capture the semantic relationships between words.
    %Finally, we utilize the trained Word2Vec model to compute the similarity between the seed lexicons ($S_{lexicons}$) and other words in the corpus using cosine distance and update our list of lexicons.
    
    %\item \textbf{GloVe:} GloVe (Global Vectors for Word Representation) is another word embedding model introduced by Pennington et. al.~\cite{pennington2014glove} that can capture both syntactic and semantic information.
    %GloVe learns word representations by leveraging co-occurrence statistics derived from large text corpora.
    %We use the GloVe algorithm to learn word embeddings from the preprocessed posts corpus.
    %The co-occurrence statistics derived from the social media posts dataset enable GloVe to generate distributed word representations that capture the semantic relationships between words.
    %Finally, we employ the learned GloVe embeddings to calculate word similarities using cosine similarity for all the seed lexicons ($S_{lexicons}$).
    
    %\item \textbf{BERT:} Similarly, BERT (Bidirectional Encoder Representations from Transformers), by Devlin et al.~\cite{devlin2018bert}, also captures contextualized word representations but its ability to capture bidirectional context allows it to comprehend word semantics more accurately.
    %For our research, we fine-tune BERT's pre-trained model using our posts dataset.
    %This fine-tuning process allows BERT to adapt to the specific characteristics of Twitter data.
    %Subsequently, we extract informative word embeddings from BERT's contextualized representations of the social media posts corpus and calculate word similarities.
    
%\end{itemize}

By going through this process, we were able to pinpoint words that are \textit{``similar,''} meaning they are utilized within comparable situations.
We determine the similarity between the word embeddings using cosine similarity. \textcolor{black}{We used a cosine similarity threshold of $\geq 0.75$ to identify new toxic words, after empirically testing thresholds of 0.7, 0.75, and 0.8. The threshold of 0.75 struck an optimal balance, generating a diverse set of new words while minimizing redundancy in the lexicon. All flagged words were manually labeled, and 36\% were excluded as non-toxic or irrelevant.}

\noindent\textbf{Graph-based Similarity Approach:} We incorporate a graph-based method using the Louvain algorithm~\cite{blondel2008fast} to assess word similarity through embeddings. Graphs are constructed based on word embeddings, connecting words if the cosine similarity of their vector representations surpasses a predefined threshold of $\geq 0.75$~\cite{zannettou2020quantitative}.
However, this approach exhibits limitations in performance. The method tends to generate numerous false positives, primarily due to its inherent lack of context specificity found in graph-based similarity methods.

The output of this phase is a set of words that we call updated lexicons ($U_{lexicons}$) that are \textit{likely} to be toxic for our specific dataset.

\subsection{Step 2(a): Testing The Updated Lexicons Using Traditional Machine Learning Models}

To test our adaptive approach to hate speech detection we use traditional machine learning models provided by Davidson et. al.~\cite{davidson2017automated}.
We choose Linear Support Vector Machine (SVM)~\cite{cortes1995support}, Random Forest (RF)~\cite{breiman2001random}, and Logistic Regression (LR)~\cite{hosmer2013applied} as traditional classification approaches.
We use the average accuracy of the models, F1-measure, and class-specific precision and recall to evaluate our models on the test sets.
We use grid search and stratified $k$-fold cross-validation ($k=10$) to tune the hyper-parameters during the training and validation phases.

\subsection{Step 2(b): Hybrid Approach For Hate Speech Detection}

The lexicon-based approach relies solely on a predefined list of toxic terms or phrases, which may not capture the evolving nature of hate speech or account for contextual nuances.
It can struggle to identify hate speech that does not precisely match the terms in the lexicon.
On the other hand, BERT models, while effective at capturing contextual information and semantic relationships, may require significant amounts of labeled data for fine-tuning and can be computationally expensive.

By combining the two approaches, we can leverage the strengths of both.
%The lexicon-based method provides a strong foundation for identifying explicit toxic words, while BERT models enhance the detection capability by capturing contextual cues and understanding the complexities of language.
To do this we used the \textit{Lexical Substitution} method for incorporating the hate speech lexicons as features.
We used the set of lexicons to generate additional features for the input text, which are then used as input to the BERT model.
Our method involves enhancing the input embeddings with hate speech lexicons, which are then passed through the pre-trained BERT classification model to get the prediction. Specifically, we tokenize the input text using the BERT tokenizer and then generate binary features for each word or phrase in the lexicon that appears in the input text.
We also augment the input features with binary flags to indicate the presence or absence of each hate speech lexicon in the post.
We do this by first tokenizing the post using the BERT tokenizer and then adding an extra feature vector of 0s and 1s to represent the presence or absence of each hate speech lexicon.
Then, we concatenate the feature vector with the BERT embeddings and pass it through the model. We utilize BERT-based models, like BERT-base~\cite{devlin2018bert}, BERT-large~\cite{devlin2018bert}, and RoBERTa~\cite{liu2019roberta} and state-of-the-art pre trained BERT-model for hatespeech detection Detoxify~\cite{Detoxify}, BERT-HateXplain~\cite{Mathew_Saha_Yimam_Biemann_Goyal_Mukherjee_2021} and HurtBERT~\cite{hurtbert2020} in our analysis and approach development.