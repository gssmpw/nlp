\section{Results and Evaluation}

In this section, we first present the results of each of the previous models using our dataset and compare the results that these models give using the updated lexicons (RQ1).
We also present the results of our hybrid approach that predicted whether a post contains hate speech or not using a modified version of BERT (RQ2). 

\subsection{Data Preprocessing}
To test our approach we use the publicly available datasets. To evaluate our models, we employ average model accuracy, F1-measure, as well as class-specific precision and recall.
While the accuracy and F1 values offer a broad overview of the model's performance, the precision and recall scores for each class provide more specific information.

\subsection{RQ1: Evaluating Our Adaptive Approach for Lexicon Improvement}
\textcolor{black}{To evaluate the effectiveness of our adaptive lexicon approach, we used the models provided by~\citeauthor{davidson2017automated}\cite{davidson2017automated}for several reasons. First, these models are well-established baselines in the field, frequently cited and used for benchmarking new approaches to hate speech detection. Second, they rely on traditional lexicon-based methods, making them ideal candidates to demonstrate the improvements achieved by our adaptive lexicon updates. Our goal for RQ1 is to \textit{validate} that our lexicon updating method enhances the performance of existing models by aligning them with evolving language trends.
We tested the models provided by Davidson et al.~\cite{davidson2017automated} using the new dataset from Founta et al.~\cite{founta2018large}. We found that the accuracy dropped from the originally reported ~90\% during training to 76\% in our tests. This indicates that language evolves over time and that toxic lexicons must be updated to remain effective for detecting toxic language. Next, we utilized the same models with newer datasets but incorporated updated lexicons to validate our approach. We implemented and evaluated the Support Vector Machine (SVM) and Random Forest (RF) classifiers provided by Davidson et al. to detect hate speech, using the 100,000 social media posts from Founta et al.~\cite{founta2018large} as training and testing datasets.}

\begin{table}
\caption{Model performance across different word embedding lexicons for traditional models.}
\label{tab:supervisedPerformance}
\begin{center}
\small
\begin{tabular}{|l|l|lllll|}
\hline
\textbf{Features} & \textbf{Lexicon Size} & \multicolumn{1}{l|}{\textbf{Class}} & \multicolumn{1}{l|}{\textbf{Prec.}} & \multicolumn{1}{l|}{\textbf{Rec.}} & \multicolumn{1}{l|}{\textbf{F1}} & \textbf{Accr.} \\ \hline
\textbf{Linear SVM} &  &  &  &  &  &  \\ \hline
\multirow{2}{*}{$S_{lexicons}$} & \multirow{2}{*}{749} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.69} & \multicolumn{1}{l|}{0.77} & \multicolumn{1}{l|}{0.73} & \multirow{2}{*}{0.76} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.73} & \multicolumn{1}{l|}{0.65} & \multicolumn{1}{l|}{0.69} &  \\ \hline
\multirow{2}{*}{$U_{Word2Vec}$} & \multirow{2}{*}{1006} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.89} & \multicolumn{1}{l|}{0.68} & \multicolumn{1}{l|}{0.81} & \multirow{2}{*}{0.77} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.77} & \multicolumn{1}{l|}{0.99} & \multicolumn{1}{l|}{0.87} &  \\ \hline
\multirow{2}{*}{$U_{GloVe}$} & \multirow{2}{*}{1010} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.83} & \multicolumn{1}{l|}{0.77} & \multicolumn{1}{l|}{0.80} & \multirow{2}{*}{0.82} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.70} & \multicolumn{1}{l|}{0.76} & \multicolumn{1}{l|}{0.73} &  \\ \hline
\multirow{2}{*}{$U_{BERT}$} & \multirow{2}{*}{1433} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.90} & \multicolumn{1}{l|}{0.70} & \multicolumn{1}{l|}{0.79} & \multirow{2}{*}{0.82} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.74} & \multicolumn{1}{l|}{0.92} & \multicolumn{1}{l|}{0.82} &  \\ \hline
\textbf{Random Forest} &  &  &  &  &  &  \\ \hline
\multirow{2}{*}{$S_{lexicons}$} & \multirow{2}{*}{749} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.74} & \multicolumn{1}{l|}{0.74} & \multicolumn{1}{l|}{0.74} & \multirow{2}{*}{0.79} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.62} & \multicolumn{1}{l|}{0.62} & \multicolumn{1}{l|}{0.62} &  \\ \hline
\multirow{2}{*}{$U_{Word2Vec}$} & \multirow{2}{*}{1006} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.90} & \multicolumn{1}{l|}{0.70} & \multicolumn{1}{l|}{0.79} & \multirow{2}{*}{0.82} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.74} & \multicolumn{1}{l|}{0.92} & \multicolumn{1}{l|}{0.82} &  \\ \hline
\multirow{2}{*}{$U_{GloVe}$} & \multirow{2}{*}{1010} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.94} & \multicolumn{1}{l|}{0.68} & \multicolumn{1}{l|}{0.79} & \multirow{2}{*}{0.83} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.76} & \multicolumn{1}{l|}{0.96} & \multicolumn{1}{l|}{0.85} &  \\ \hline
\multirow{2}{*}{$U_{BERT}$} & \multirow{2}{*}{1433} & \multicolumn{1}{l|}{Hate} & \multicolumn{1}{l|}{0.86} & \multicolumn{1}{l|}{0.93} & \multicolumn{1}{l|}{0.89} & \multirow{2}{*}{\textbf{0.85}} \\ \cline{3-6}
 &  & \multicolumn{1}{l|}{Normal} & \multicolumn{1}{l|}{0.91} & \multicolumn{1}{l|}{0.84} & \multicolumn{1}{l|}{0.87} & \\ \hline
\end{tabular}%
\end{center}
\end{table}


\textcolor{black}{Table~\ref{tab:supervisedPerformance} presents the performance metrics of traditional machine learning models using different feature sets, which include lexicons derived from various word embedding models (Word2Vec, GloVe, and BERT). Overall, we find that the Random Forest model with lexicons updated through BERT achieves the highest accuracy at 0.85, outperforming other classifiers. When using only the seed lexicons $S_{lexicons}$, accuracy is lower compared to the updated lexicons generated by the word embedding models. Additionally, the model demonstrates strong class-specific precision and recall. For hate speech, recall (0.93) exceeds precision (0.86), while for normal content, precision (0.91) is higher than recall (0.84).}

\subsection{RQ2: Evaluating Our Hybrid Approach to Risk Detection}

In this section, we evaluate six different BERT-based models: BERT-base~\cite{devlin2018bert}, BERT-large~\cite{devlin2018bert}, RoBERTa~\cite{liu2019roberta}, and modified pre-trained BERT models for hate speech detection, including Detoxify~\cite{Detoxify}, BERT-HateXplain~\cite{Mathew_Saha_Yimam_Biemann_Goyal_Mukherjee_2021}, and HurtBERT~\cite{hurtbert2020}. These models are tested on six different test sets, as described in section~\ref{testset}.

For each BERT-based model, we evaluate performance across six different test sets. Table~\ref{tab:BERTPerformance} summarizes the performance metrics of these models using three feature sets: without lexicons ($W$), with seed lexicons ($S_{lexicons}$), and with the best-performing lexicons derived from BERT ($U_{BERT}$). Overall, we find that Detoxify and BERT-HateXplain outperform the other BERT models.

\begin{table*}[ht]
\centering
\caption{Performance of different BERT-based models for hate speech detection using different feature sets.}
\label{tab:BERTPerformance}
\small
\begin{tabular}{||c|c|c|c|c|c|c||c|c|c|c|c|c||}
\hline
\multirow{3}{*}{\textbf{TestSet}} & \multicolumn{6}{c||}{\textbf{BERT Base}} & \multicolumn{6}{c||}{\textbf{BERT Large}} \\ \cline{2-13} 
 & \multicolumn{2}{c|}{$W$} & \multicolumn{2}{c|}{$S_{lexicons}$} & \multicolumn{2}{c||}{$U_{BERT}$} & \multicolumn{2}{c|}{$W$} & \multicolumn{2}{c|}{$S_{lexicons}$} & \multicolumn{2}{c||}{$U_{BERT}$} \\ \cline{2-13}
 & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} \\ \hline
 
\citeauthor{davidson2017automated}\cite{davidson2017automated} & 0.68 & 0.69 & 0.68 & 0.69 & 0.71 & 0.78 & 0.68 & 0.69 & 0.65 & 0.72 & 0.74 & 0.78 \\ \hline
\citeauthor{founta2018large}\cite{founta2018large} & 0.68 & 0.68 & 0.73 & 0.75 & 0.72 & 0.75 & 0.68 & 0.67 & 0.71 & 0.72 & 0.81 & 0.81 \\ \hline
Implicit Hate~\cite{elsherief-etal-2021-latent} & 0.67 & 0.72 & 0.79 & 0.70 & 0.79 & 0.70 & 0.67 & 0.72 & 0.79 & 0.70 & 0.79 & 0.71 \\ \hline
HateCheck~\cite{rottger-etal-2021-hatecheck} & 0.69 & 0.78 & 0.86 & 0.80 & 0.87 & 0.80 & 0.70 & 0.78 & 0.86 & 0.80 & 0.86 & 0.80 \\ \hline
ToxicSpan~\cite{pavlopoulos-etal-2022-acl} & 0.74 & 0.83 & 0.87 & 0.87 & 0.89 & 0.84 & 0.74 & 0.83 & 0.89 & 0.87 & 0.89 & 0.85 \\ \hline
ToxiGen~\cite{hartvigsen-etal-2022-toxigen} & 0.73 & 0.81 & 0.81 & 0.85 & 0.89 & 0.85 & 0.74 & 0.85 & 0.89 & 0.85 & 0.87 & 0.86 \\ \hline

\multicolumn{13}{c}{} \\ \hline
\multirow{3}{*}{\textbf{TestSet}} & \multicolumn{6}{c||}{\textbf{RoBERTa}} & \multicolumn{6}{c||}{\textbf{Detoxify}} \\ \cline{2-13} 
 & \multicolumn{2}{c|}{$W$} & \multicolumn{2}{c|}{$S_{lexicons}$} & \multicolumn{2}{c||}{$U_{BERT}$} & \multicolumn{2}{c|}{$W$} & \multicolumn{2}{c|}{$S_{lexicons}$} & \multicolumn{2}{c||}{$U_{BERT}$} \\ \cline{2-13}
 & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} \\ \hline
 
\citeauthor{davidson2017automated}\cite{davidson2017automated} & 0.74 & 0.78 & 0.74 & 0.79 & 0.71 & 0.78 
                             & 0.81 & 0.79 & 0.85 & 0.86 & \textbf{0.84} & \textbf{0.88} \\ \hline
                             
\citeauthor{founta2018large}\cite{founta2018large} & 0.76 & 0.79 & 0.75 & 0.82 & 0.74 & 0.81
                       & 0.84 & 0.87 & 0.91 & 0.92 & \textbf{0.91} & \textbf{0.94} \\ \hline
                       
Implicit Hate~\cite{elsherief-etal-2021-latent} & 0.73 & 0.72 & 0.79 & 0.70 & 0.79 & 0.70 
                                                & 0.83 & 0.82 & 0.79 & 0.80 & \textbf{0.89} & \textbf{0.91} \\ \hline
                                                
HateCheck~\cite{rottger-etal-2021-hatecheck} & 0.73 & 0.76 & 0.79 & 0.79 & 0.79 & 0.80
                                             & 0.80 & 0.88 & 0.94 & 0.90 & 0.96 & 0.91 \\ \hline
                                             
ToxicSpan~\cite{pavlopoulos-etal-2022-acl} & 0.76 & 0.83 & 0.87 & 0.87 & 0.84 & 0.84 
                                           & 0.84 & 0.84 & 0.89 & 0.87 & 0.89 & 0.85 \\ \hline
                                           
ToxiGen~\cite{hartvigsen-etal-2022-toxigen} & 0.83 & 0.81 & 0.81 & 0.85 & 0.89 & 0.85 
                                            & 0.84 & 0.85 & 0.89 & 0.85 & 0.87 & 0.86 \\ \hline

\multicolumn{13}{c}{} \\ \hline
\multirow{3}{*}{\textbf{TestSet}} & \multicolumn{6}{c||}{\textbf{HurtBERT}} & \multicolumn{6}{c||}{\textbf{BERT-HateXplain}} \\ \cline{2-13} 
 & \multicolumn{2}{c|}{$W$} & \multicolumn{2}{c|}{$S_{lexicons}$} & \multicolumn{2}{c||}{$U_{BERT}$} & \multicolumn{2}{c|}{$W$} & \multicolumn{2}{c|}{$S_{lexicons}$} & \multicolumn{2}{c||}{$U_{BERT}$} \\ \cline{2-13}
 & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} & \textbf{F1} & \textbf{Accr.} \\ \hline
 
\citeauthor{davidson2017automated}\cite{davidson2017automated} & 0.81 & 0.81 & 0.89 & 0.89 & 0.81 & 0.85 
                             & 0.85 & 0.82 & 0.86 & 0.89 & \textbf{0.84} & 0.83 \\ \hline
                             
\citeauthor{founta2018large}\cite{founta2018large} & 0.81 & 0.82 & 0.83 & 0.85 & 0.84 & 0.85 
                       & 0.84 & 0.84 & 0.84 & 0.87 & 0.82 & 0.85 \\ \hline
                       
Implicit Hate~\cite{elsherief-etal-2021-latent} & 0.73 & 0.76 & 0.79 & 0.79 & 0.79 & 0.81 & 0.73 & 0.77 & 0.80 & 0.82 & 0.80 & 0.84 \\ \hline

HateCheck~\cite{rottger-etal-2021-hatecheck} & 0.76 & 0.78 & 0.86 & 0.91 & 0.87 & 0.91 & 0.79 & 0.81 & 0.86 & 0.92 & \textbf{0.86} & \textbf{0.93} \\ \hline

ToxicSpan~\cite{pavlopoulos-etal-2022-acl} & 0.84 & 0.83 & 0.87 & 0.87 & 0.89 & 0.93 & 0.84 & 0.83 & 0.89 & 0.87 & \textbf{0.89} & \textbf{0.95} \\ \hline

ToxiGen~\cite{hartvigsen-etal-2022-toxigen} & 0.83 & 0.86 & 0.91 & 0.95 & 0.92 & 0.95 & 0.84 & 0.85 & 0.89 & 0.95 & \textbf{0.92} & \textbf{0.96} \\ \hline


\end{tabular}
\end{table*}



