\section{Discussion}
In this section, we discuss the key implications of our findings based on our two overarching research questions. Overall, our findings open up interesting opportunities for future research and implications for the industry as a whole.

\subsection{Resilience Of Adaptive Hate Speech Detection Against Poisoning Attacks - RQ1}
Lexicon-based approaches to hate speech detection systems are prone to poisoning attacks.
In a poisoning attack, an adversary intentionally uses safe words in place of toxic words that can cause the model to produce incorrect or biased outputs.
For example, in 2016, 4Chan's /pol/ launched a deliberate attack against Google's Perspective API via the so-called ``Operation Google''~\cite{hine2017kek,operationgoogle}. This attack was designed to poison models by replacing slurs with the names of various tech companies.
For example, instead of saying a slur for a black person, you would say ``Google,'' or instead of a slur for a Jew, you would say ``Skype'' etc.
Poisoning attacks can be challenging to mitigate because they exploit vulnerabilities in the training process of machine learning models.
However, our proposed system can be used to expose it specifically because our goal is to discover how toxic behavior and aggression attacks change over time.
The basic core of our approach is to identify words that are used in a \textit{similar fashion} as known toxic and toxic ones.
We adopt a similarity-based approach thus for each word appearing in our dataset we calculate its vector embedding, extracted from the models built as part of the previous step. We compare this vector with the vector embeddings for all the words in our seed dataset. If the vector for a word has a high similarity (e.g., cosine similarity) with a known toxic word, it is very likely that this word is itself toxic â€“ this is because the two words are used in similar contexts on social media. The output of this phase is a set of words that are likely to be toxic, or used in a toxic way.

\subsection{Hybrid Approach to Hate Speech Detection - RQ2}
By combining the strengths of lexicon-based detection as well as BERT methodologies into a hybrid model we can effectively identify and analyze hate speech in various domains with improved accuracy and contextual understanding. The lexicon-based analysis component leverages pre-defined word lists and sentiment analysis techniques to identify toxic words and sentiments associated with them.
This approach provides a good foundation for detecting explicit risk indicators and capturing straightforward and easily identifiable risk factors.
It allows for quick identification of keywords and phrases commonly associated with risk, enabling efficient detection in real-time scenarios.
On the other hand, the BERT approach, which utilizes a deep learning neural network model, brings contextual understanding and semantic analysis to the hybrid system. BERT enables the model to comprehend the context and nuances of language, capturing the subtleties and complexities of risk factors that may not be explicitly expressed.
This contextual understanding helps the hybrid model to identify implicit risks, detect sarcasm, and recognize risks that might be disguised through various linguistic techniques.
The combination of these two approaches creates a comprehensive risk detection system that combines the advantages of both methods. Secondly, our model also detects implicit hate found in most text online. Unlike explicit hate speech, which uses overtly offensive words or phrases, implied hate speech is more subtle and can be embedded within seemingly innocuous language. Our hybrid model mitigates this limitation by leveraging BERT's contextual understanding.

\subsection{Limitations and Future Work}
\textcolor{black}{In our research, we propose an adaptive methodology to detect toxic language through the utilization of word embeddings. However, it is important to acknowledge that our hybrid approach, despite its numerous strengths, does possess certain limitations.
One notable limitation lies within the lexicon-based analysis employed in our methodology itself. However, our approach reduces this dependency by employing adaptive techniques, allowing for the detection of new toxic words with minimal manual input. This significantly enhances scalability compared to traditional lexicon-based methods. Future work could explore removing older lexicons, and real-time language monitoring to fully automate lexicon updates and improve adaptability to evolving language trends.
Our evaluation also primarily focuses on English-language content, which is a limitation given the global nature of online hate speech. While this allowed us to deeply analyze our approach within a single language, adapting the method to multilingual contexts is crucial for real-world applicability. Hate speech varies significantly across languages and cultures, both in content and contextual nuances. Future work will explore the use of multilingual embeddings (e.g., mBERT, XLM-R) and cross-lingual transfer learning to adapt the approach to other languages. Additionally, we aim to incorporate culturally diverse datasets and expert input to address cross-cultural variations in hate speech detection. Furthermore, it is worth mentioning that recent limitations imposed on using Twitter's APIs have impacted the availability and accessibility of data for research purposes. These limitations may pose challenges in acquiring the necessary data for training and evaluating our model, however, our approach can be mapped to other text-based social media applications especially Threads which is a Meta-owned platform similar in design to Twitter.}



