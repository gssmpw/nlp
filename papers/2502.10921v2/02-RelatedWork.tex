\section{Related Work}
In this section we review previous research on hate speech and utilizing automated approaches for hate speech detection.

\subsection{The evolving nature of Hate Speech}

Toxic behavior particularly hate speech is not static, because language evolves; new slurs are continually created and existing vocabulary can, and does, shift over time~\cite{kulkarni2015statistically}.
Aggressors come up with new slurs and hateful words that often target specific vulnerable populations, as shown in previous work~\cite{davidson2017automated,fortuna2018survey}. Researchers have also shown that existing approaches for detection cannot keep up with the evolving nature of hateful language~\cite{tahmasbi2021go}.

Current methods to automatically flag and block hateful language rely on lexicons constructed ahead of time~\cite{burnap2015cyber,founta2018large}.
Their effectiveness decreases over time, potentially making them unsuitable for identifying hate speech that targets communities not featured in the datasets used to compile the lexicons~\cite {waseem2016hateful,nobata2016abusive}. Additionally, the lexicons can quickly become outdated as toxic language evolves and new slurs and insults are introduced.
This can result in false negatives, where toxic language goes undetected, and false positives, where non-toxic language is flagged as hate speech.
Furthermore, these approaches cannot distinguish between different contexts and intents in which the same word or phrase might be used, leading to the incorrect categorization of language as toxic. 

Existing solutions also suffer from several biases~\cite{haimson2021disproportionate}.
Racial~\cite{sap2019risk}, contextual~\cite{zueva2020reducing}, and demographic~\cite{huang2020multilingual} biases have all been noted as problems with classification techniques that are unable to deal with slang and cultural differences because of their static nature.
Existing work on the temporal analysis of hate speech on Gab has made it clear that the problem is getting worse over time~\cite{mathew2020hate}, however, even this study used a static, keyword-based model of toxic behavior.
Other studies have also concluded that aggressors use sneaky methods of avoiding being flagged by either introducing a new toxic word or making an intentional spelling error, for example, Hosseini et. al~\cite{hosseini2017deceiving} explored the vulnerability of Google's Perspective API to modified versions of a text that still contain the same toxic language, but receive a significantly lower toxicity score from the API.
The authors show that an adversary can deceive the system by misspelling abusive words or by adding punctuation between the letters showing a need for more adaptive and context-aware methods for toxic language moderation.
%They also apply the attack on the sample phrases provided on the Perspective website and show that they can consistently reduce the toxicity scores to the level of the non-toxic phrases.
%The existence of such adversarial examples is very harmful to toxic detection systems and seriously undermines their usability.
%Therefore, there is a need for more adaptive and context-aware methods for toxic language moderation that can overcome these limitations and provide a safer online environment for all users. 

\subsection{Mitigation strategies}
Although there is a considerable amount of work on detecting toxic behavior, almost none of it considers that toxic behavior evolves.
This is problematic because what little work there is indicates that toxic behavior changes in response to real-world events, as they unfold.
%Existing classification solutions only go so far in detecting such toxic language. 
%To combat the dynamic nature of toxic language and enhance the detection of hate speech, computer scientists and researchers have explored various avenues.
One approach involves leveraging machine learning algorithms and natural language processing techniques to develop models capable of adapting to new forms of hate speech~\cite{gao2018neural}.
These models can learn from vast amounts of data including a diverse range of sources and monitoring real-time social media platforms. While lexicon-based approaches have been extensively used in early works~\cite{burnap2015cyber,founta2018large} recent studies highlight their continued relevance~\cite{basile2019semeval,mozafari2020bert} and current methods often incorporate lexicons as a foundational component. 
Another promising avenue is the application of deep learning techniques, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), for hate speech detection~\cite{chatzakou2017hate}
Indeed there has been numerous research in this domain including using multiple deep learning architectures~\cite{badjatiya2017deep} or fine-tuning language models for hate speech detection~\cite{howard2018universal}.
Caron et. al.~\cite{Caron2022} used transfer learning and attention-based models and Mozafari et. al.~\cite{mozafari2020bert} investigated the ability of BERT to capture hateful content within social media content by using new fine-tuning methods also based on transfer learning. Saha et. al.~\cite{saha2021} generate ``fear'' lexicons using the word2vec model using some seed lexicons to improve their list of lexicons. Other modifications of BERT include HateBERT~\cite{caselli-etal-2021-hatebert} and HurtBert~\cite{hurtbert2020} which are trained BERT models specifically designed for detecting abusive language. These models can learn intricate patterns and contextual nuances from large-scale datasets, enabling them to identify hate speech even when it employs subtle language or sarcasm. However, all these models require continuous updating and fine-tuning with fresh data, to enhance their ability to identify hate speech across different demographics~\cite{schmidt2017survey}. Understanding implicit hatespeech is also a domain that touches upon the evolving nature of the problem~\cite{elsherief-etal-2021-latent, hartvigsen-etal-2022-toxigen}.

\noindent \textbf{Remarks: }In our research, we adopt a new, holistic approach that leverages the dynamic nature of language and the tendency of malicious users to conceal their toxicity by using alternative, seemingly harmless words instead of recognized toxic terms.