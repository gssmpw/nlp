\section{Interesting Case Studies}
To gain further insights into the performance of our hybrid model, we conduct an in-depth qualitative analysis.
We found that aggressors employ various sneaky methods to \textit{conceal} slurs and hate speech, often making it challenging to detect and address.
Here are some categories that encompass these tactics:
\begin{itemize}[leftmargin=*]
    \item \textbf{Introducing new hate speech lexicons:} As online platforms implement measures to combat hate speech, aggressors adapt by using alternative terms, neologisms, or coded language to express their hateful ideas without triggering automated filters or detection systems making it difficult for outsiders or automated tools to immediately recognize the underlying hate speech. For example, the word ``shitskins'' (Example 1) and ``salads'' (Example 2) are used as hate words in the following social media posts.

    \vspace{0.1in}
    \begin{graybox}
    \textbf{Example 1: }``Ive seen videos of Muslim shitskins dividing a single person into multiple pieces.''\\
    \textbf{Example 2: }``of course I'm over the limit I'm on a night out you fucking salads''
    \end{graybox}
    \vspace{0.1in}

    \item \textbf{Spelling Errors:} Aggressors intentionally misspell words related to hate speech or use deliberate variations in spelling to bypass content filters. In Examples 3, 4, and 5 we illustrate some of the spelling errors made.

    \vspace{0.1in}
    \begin{graybox}
    \textbf{Example 3: }``Y'all \textbf{niggaz} evil af''\\
    \textbf{Example 4: }``If you'll see me holding up my middle finger to the world. \textbf{Fck} ur ribbons and ur pearls.'' \\
    \textbf{Example 5: }``This shit got me \textbf{fuckin} CRYINGG!! Cuz the \textbf{lil nigga} aint even want this stupid cut just look \@ his face''
    \end{graybox}
    \vspace{0.1in}
    
    \item \textbf{Adding Punctuation:} Another tactic employed by aggressors is the insertion of special characters or punctuation marks within offensive words or slurs to obscure or obfuscate the offensive language. For example, adding an apostrophe like ``nas.ty'' (Example 6) or an underscore like ``x\_x'' (Example 7).

    \vspace{0.1in}
    \begin{graybox}
    \textbf{Example 6: }''I just like \textbf{nas.ty} shit men`` \\
    \textbf{Example 7: }''When u pounding the \textbf{x\_x} like u don't wna``
    \end{graybox}
    \vspace{0.1in}

    \item \textbf{Implied Hate:} Aggressors often resort to implied hate, where they use veiled language (Example 8), innuendos, sarcasm (Example 9), or ambiguous statements (Example 10) to convey discriminatory or hateful ideas indirectly.
    \vspace{0.1in}
    \begin{graybox}
    \textbf{Example 8: }``To bad u couldn't box the hell out I'd be even prouder'' \\
    \textbf{Example 9: }``You are a chicken nugget and soy milk'' \\
    \textbf{Example 10: }``Latina backwards spells crazy as hell in 2 languages''
    \end{graybox}

\end{itemize}

Our findings reveal that our model exhibits a higher proficiency in identifying instances of hate speech when substitute lexicons are employed especially to bypass already in place moderation systems.

\subsection{Comparing Our Hybrid Approach with State-of-The-Art Moderate Hate Speech API}

Moderate Hate Speech API~\cite{moderatehatespeech} is a Google Cloud service that helps identify and moderate hate speech.
It can be used to moderate content in a variety of applications, including social media platforms, forums, and news websites.
For each detected hate speech token, the API returns a confidence score which indicates how likely it is that the token is hate speech. However, it is important to note that the API is not perfect. It sometimes misidentifies content as hate speech, and it can also sometimes fail to identify hate speech as reported on its website~\cite{moderatehatespeech}. We find that our model detects hate and toxicity towards vulnerable populations especially women and the black community.

We use this API to compare our hybrid model.
We use the 76,378 unlabeled posts for this purpose.
We find that our model detects 663 posts as hate speech out of 76,378 posts whereas Moderate Hate Speech API detects 678 posts as hate speech. Our model detects 65 different posts than Moderate Hate Speech API, and upon manual analysis, we find that most of the posts that our model detected contained new toxic lexicons for example ``sigma'' (Example 11), ``karen,'' ``thot''(Example 12),  etc.

\vspace{0.1in}
\begin{graybox}
\textbf{Example 11: }``typical sigma behavior''\\
\textbf{Example 12: }``i am your local thot''
\end{graybox}
\vspace{0.1in}

There were other examples where the API failed where harsher emotions or words were used for example in Example 13 ``liberal Stalinists'' is used negatively:

\vspace{0.1in}
\begin{graybox}
\textbf{Example 13: }``So, for the first time ever since 2017, America is a communist nation again. liberal Stalinists!!''
\end{graybox}
\vspace{0.1in}

There were other cases where sexual harassment towards women was missed by the API for example (Examples 14 and 15):

\vspace{0.1in}
\begin{graybox}
\textbf{Example 14: }``I just wanna be a good bun, having someone clip a leash to my collar and take me for a walk, letting anyone who asks fuck and breed me, then getting headpats and scritches after''\\
\textbf{Example 15: }``Anyone else wanna help me breed her..''
\end{graybox}
\vspace{0.1in}

On the other hand, our model performs poorly when the hate speech lexicons were not part of the initial diagnosis, for example in the following post (Example 16) the lexicons ``xenophobes'' and ``halfwits'' were not part of the toxic lexicon list and hence this post is not flagged by our model but was detected by Moderate Hate Speech API.
The Moderate Hate Speech API detects 80 different posts than our model. 

\vspace{0.1in}
\begin{graybox}
\textbf{Example 16: }``RT @username: @username My business is in services. Xenophobes and halfwits like yourself destroyed the EU side of my business.''
\end{graybox}
\vspace{0.1in}

However, we also find that Moderate Hate Speech is biased towards black people (This has also been confirmed in the documentation of this API~\cite{moderatehatespeech}), for example, the following posts (Examples 17 and 18) from our dataset are labeled as hate speech by this model, however upon manual analysis, we can see that they are clearly not hate speech.

\vspace{0.1in}
\begin{graybox}
\textbf{Example 17: }``@username: Did you know a disabled Black woman invented the walker, toilet paper holder, and sanitary belt?''\\
\textbf{Example 18: }``@username: the older black generation be saying some questionable things.''
\end{graybox}
\vspace{0.1in}

We discover that our hybrid model goes beyond existing approaches by addressing the dynamic nature of language, adapting to new vocabulary, and evolving linguistic patterns. It also helps identify toxicity towards vulnerable populations that were not mentioned in the original lexicon dataset. 