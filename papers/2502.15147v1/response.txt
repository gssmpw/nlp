\section{Related Work}
\begin{figure*}[tb]
\centering
\includegraphics[scale=0.65]{figures/mainfig.pdf}
\caption{The proposed framework.~\ourmethod~generates a set of natural language property descriptions from data, i.e., documents (1a); then estimates the compatibility between each data point and each property (1b), and perform correlation-based grouping of properties to discover latent factors (2). The compatibility between each property is efficiently computed using a distilled dense text representation model. We provide additional details and examples in~\Cref{fig:our_framework_detailed} and~\Cref{sec:our_framework_detailed}.
}
\label{fig:our_framework}
\vspace{-2mm}
\end{figure*}






\paragraph{Latent Factor Models}
Latent factor models (LFMs) assume observed data in a dataset are governed by a set of latent factors. 
Traditional algorithms for estimating latent factors from data such as **Blei et al., "Latent Dirichlet Allocation"**, **Jolliffe, "Principal Components Analysis"**, and **Hinton et al., "Autoencoders"**. 
Oftentimes, practitioners gain insight into the dataset by interpreting the learned factors, such as reading the topics in an LDA model or generating samples from the latent space of a variational autoencoder **Kingma et al., "Variational Autoencoders"**.


\paragraph{LLMs for Insight Mining}
Recently, there have been some successes in using LLM-based systems for analyzing textual documents, such as using LLMs to discover topics from documents **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, or clustering documents **Gupta and Lehal, "A Review of Text Mining Techniques and Their Applications: A Detailed Literature Study"**.
The most relevant framework to our work is TopicGPT **Chen et al., "TopicGPT: An End-to-End Framework for Open-Domain Topic Modeling with Pre-trained Language Models"**, which uses LLM to propose potential topics by sequentially iterating over the dataset and generating new topics based on prior outputs.


\paragraph{Key Differences} While drawing insprirations from the above areas,~\ourmethod~is the first work to combine LLMs' instruction-following ability with statistical models for goal-conditioned latent factor discovery.
In contrast to pure-LLM-based frameworks, our method only rely on LLMs to document interesting properties from data, which is less reliant on LLMs' reasoning ability.
By combining these LLM-proposed, goal-oriented properties with statistical-model-based latent factor models, we later show in our experiments that~\ourmethod~is the only method that can discover informative patterns from noisy data such as conversation and embodied navigation logs.
We provide further discussion on other related research areas in~\Cref{sec:further_related_work}.