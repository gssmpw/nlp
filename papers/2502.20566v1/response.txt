\section{Related works}
\textbf{Stochastic rounding.} SR is an alternative rounding approach to the canonical nearest rounding (NR) where high precision number is quantized to the nearest low precision value. While NR has the best worst error guarantees, it is biased. SR on the other hand, is an unbiased quantizer, as the probability of quantization is inversely proportional to the distance to low precision values, which makes it more suitable for training ML models to reduce accumulating errors across iterations **Jain, "Stochastic 1-bit Quantization,"** %\tao{what is this reference for and what does it do? just to confirm}
% While SR results in higher MSE error compared to nearest rounding it has unbiasedness property (when the probability of mapping is inversely proportional to the distance to quantization level) which makes it more suitable for machine learning applications where variance can be tolerated **Gupta, "Stochastic Quantization,"**. 
In particular, **Jain, "Stochastic 1-bit Quantization,"** shows that SR has better probabilistic error bounds when summing small numbers, as in deep learning applications, compared to nearest rounding. **Courbariaux, "BinaryConnect: Training Deep Neural Networks with Low-Precision Weights,"** analyze error terms due to SR in gradient descent and show that for simple machine learning tasks (e.g. regression) SR gives competitive performance. **Gupta, "Stochastic Quantization,"** apply SR for training $\leq$1B scale LLMs and argue that there is a performance discrepancy compared to mixed precision training or FP32 training.

\textbf{Efficient LLM training.} Traditional machine learning training algorithms generally rely on 32-bit representations (FP32) to avoid perturbing the training performance. 
% With the commercialization of LLMs, 
Recently, many works have been proposed to lower the training costs without accuracy drop using various data types, 
% Various data types have been employed for a more efficient training 
such as FP16 **Meller, "FP16 Acceleration of Deep Neural Networks,"**, BF16 **Choi, "Training Deep Models Fast and Cheap,"**, FP8 **Touretzky, "A Universal Numerical Representation,"** and so on.  **Gupta, "Stochastic Quantization,"** realized that FP16 operations can be utilized by keeping a FP32 master copy for accumulating the updates without accuracy degradation. **Jain, "FP16 Training of Deep Neural Networks,"** showed that Kahan summation could be utilized to account for numerical errors of BF16 computations. **Dally, "A Framework for Numerical Computation with Low-Precision Data Types,"** used multiple-component floating-point to reduce errors during BF16 computations. Contrary to previous work, our method is able to match the performance of mixed precision training without introducing any auxiliary variables.

%\youngsuk{clearly state the limitation of previous works, which will be used in intro/motivation/experiments}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/350M-val-loss.pdf}
%     \caption{Validation losses of competing methods while pre-training GPT-2 (350M). When employed with a relatively high learning rate, training with SR outperforms mixed precision strategy. If the same high learning rate is used for the mixed precision training we observe divergent training, which indicates robustness of SR to higher learning rates.}
%     \label{fig:350M val}
% \end{figure}


%\kaancr{change labels, maybe add SR with default lr}



% \begin{table}[h]
% \centering
%   %   \begin{tabular}{lcccc} \hline 
%   %   Method & Accuracy & Throughput & Memory & Robustness to high lr \\ \hline
%   %    & Highest &  Highest & Lowest & High \\
%   %   BF16,FP32 MP & High & High & Highest & Low \\
%   %   BF16  & Lowest & Highest  & Lowest & Low
%   % \end{tabular}
%     \begin{tabular}{lccc} \hline 
%          & BF16+SR(ours) & BF16,FP32 MP  & BF16  \\ \hline
%         Accuracy & {\color{teal}$\boldsymbol{\uparrow}$} & $\uparrow$ & {\color{red}$\downarrow$} \\
%         Throughput & {\color{teal}$\boldsymbol{\uparrow}$} &  $\uparrow$ &  {\color{teal}$\boldsymbol{\uparrow}$}  \\
%         Memory eff. &  {\color{teal}$\boldsymbol{\uparrow}$} & {\color{red}$\downarrow$}  & {\color{teal}$\boldsymbol{\uparrow}$} \\
%         Robustness & {\color{teal}$\boldsymbol{\uparrow}$} & {\color{red}$\downarrow$}  & {\color{red}$\downarrow$}
%     \end{tabular}
  
%   \captionof{table}
%       {
%         Performance summary of competing methods.%
%         \label{tab:throughput o2}
%       }
% \end{table}