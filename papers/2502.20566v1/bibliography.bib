@article{
defossez2022a,
title={A Simple Convergence Proof of Adam and Adagrad},
author={Alexandre D{\'e}fossez and Leon Bottou and Francis Bach and Nicolas Usunier},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ZPQhzTSWA7},
note={}
}

@article{dozat2016nadam,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@ARTICLE{Schuchman1964,
  author={Schuchman, L.},
  journal={IEEE Transactions on Communication Technology}, 
  title={Dither Signals and Their Effect on Quantization Noise}, 
  year={1964},
  volume={12},
  number={4},
  pages={162-165},
  keywords={Quantization;Noise level;Probability density function;Sufficient conditions;Noise measurement;TV;Communication system control;Statistics;Nonlinear distortion;Filtering},
  doi={10.1109/TCOM.1964.1088973}}


@inproceedings{gupta2015deep,
  title={Deep learning with limited numerical precision},
  author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  booktitle={International conference on machine learning},
  pages={1737--1746},
  year={2015},
  organization={PMLR}
}

@article{li2017training,
  title={Training quantized nets: A deeper understanding},
  author={Li, Hao and De, Soham and Xu, Zheng and Studer, Christoph and Samet, Hanan and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}



@misc{micikevicius2018mixedprecisiontraining,
      title={Mixed Precision Training}, 
      author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
      year={2018},
      eprint={1710.03740},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1710.03740}, 
}

@misc{trainium,
  title = {Amazon EC2 Trn1 Instances for High-Performance Model Training are Now Available},
  author = {Antje Barth},
  year = {2022},
  url = {https://aws.amazon.com/blogs/aws/amazon-ec2-trn1/},
}

@article{wang2018training,
  title={Training deep neural networks with 8-bit floating point numbers},
  author={Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen, Chia-Yu and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{sun2019hybrid,
  title={Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks},
  author={Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi Viji and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{xia2023influencestochasticroundofferrors,
      title={On the influence of stochastic roundoff errors and their bias on the convergence of the gradient descent method with low-precision floating-point computation}, 
      author={Lu Xia and Stefano Massei and Michiel E. Hochstenbach and Barry Koren},
      year={2023},
      eprint={2202.12276},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.12276}, 
}

@misc{kalamkar2019studybfloat16deeplearning,
      title={A Study of BFLOAT16 for Deep Learning Training}, 
      author={Dhiraj Kalamkar and Dheevatsa Mudigere and Naveen Mellempudi and Dipankar Das and Kunal Banerjee and Sasikanth Avancha and Dharma Teja Vooturi and Nataraj Jammalamadaka and Jianyu Huang and Hector Yuen and Jiyan Yang and Jongsoo Park and Alexander Heinecke and Evangelos Georganas and Sudarshan Srinivasan and Abhisek Kundu and Misha Smelyanskiy and Bharat Kaul and Pradeep Dubey},
      year={2019},
      eprint={1905.12322},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.12322}, 
}

@misc{kahan1997mindless,
  author = {William Kahan},
  title = {How Futile are Mindless Assessments of Roundoff
in Floating-Point Computation ?},
  year = {2006},
  url = {https://people.eecs.berkeley.edu/~wkahan/Mindless.pdf},
  note = {Accessed: 2024-10-08}
}

@article{connoly2021srprob,
author = {Connolly, Michael P. and Higham, Nicholas J. and Mary, Theo},
title = {Stochastic Rounding and Its Probabilistic Backward Error Analysis},
journal = {SIAM Journal on Scientific Computing},
volume = {43},
number = {1},
pages = {A566-A585},
year = {2021},
doi = {10.1137/20M1334796},

URL = { 
    
        https://doi.org/10.1137/20M1334796
    
    

},
eprint = { 
    
        https://doi.org/10.1137/20M1334796
    
    

}
}
@article{fan2024hlat,
  title={HLAT: High-quality Large Language Model Pre-trained on AWS Trainium},
  author={Fan, Haozheng and Zhou, Hao and Huang, Guangtai and Raman, Parameswaran and Fu, Xinwei and Gupta, Gaurav and Ram, Dhananjay and Wang, Yida and Huan, Jun},
  journal={arXiv preprint arXiv:2404.10630},
  year={2024}
}


@InProceedings{ozkara24a,
  title = 	 {{MADA}: Meta-Adaptive Optimizers Through Hyper-Gradient Descent},
  author =       {Ozkara, Kaan and Karakus, Can and Raman, Parameswaran and Hong, Mingyi and Sabach, Shoham and Kveton, Branislav and Cevher, Volkan},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {38983--39008},
  year = 	 {2024},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v235/ozkara24a.html},
}


@misc{chen2021quantizedadamerrorfeedback,
      title={Quantized Adam with Error Feedback}, 
      author={Congliang Chen and Li Shen and Haozhi Huang and Wei Liu},
      year={2021},
      eprint={2004.14180},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2004.14180}, 
}


@misc{rae2022scalinglanguagemodelsmethods,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Rae et al.,Jack W. },
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.11446}, 
}


@article{croci2022stochastic,
  title={Stochastic rounding: implementation, error analysis and applications},
  author={Croci, Matteo and Fasi, Massimiliano and Higham, Nicholas J and Mary, Theo and Mikaitis, Mantas},
  journal={Royal Society Open Science},
  volume={9},
  number={3},
  pages={211631},
  year={2022},
  publisher={The Royal Society}
}


@misc{zamirai2021revisitingbfloat16training,
      title={Revisiting BFloat16 Training}, 
      author={Pedram Zamirai and Jian Zhang and Christopher R. Aberger and Christopher De Sa},
      year={2021},
      eprint={2010.06192},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.06192}, 
}


@InProceedings{cattaneo24on,
  title = 	 {On the Implicit Bias of {A}dam},
  author =       {Cattaneo, Matias D. and Klusowski, Jason Matthew and Shigida, Boris},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {5862--5906},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/cattaneo24a/cattaneo24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/cattaneo24a.html},
  abstract = 	 {In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.}
}


@misc{molybog2023adaminstability,
      title={A Theory on Adam Instability in Large-Scale Machine Learning}, 
      author={Igor Molybog and Peter Albert and Moya Chen and Zachary DeVito and David Esiobu and Naman Goyal and Punit Singh Koura and Sharan Narang and Andrew Poulton and Ruan Silva and Binh Tang and Diana Liskovich and Puxin Xu and Yuchen Zhang and Melanie Kambadur and Stephen Roller and Susan Zhang},
      year={2023},
      eprint={2304.09871},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.09871}, 
}

@inproceedings{
yu2024collage,
title={Collage: Light-Weight Low-Precision Strategy for {LLM} Training},
author={Tao Yu and Gaurav Gupta and Karthick Gopalswamy and Amith R Mamidala and Hao Zhou and Jeffrey Huynh and Youngsuk Park and Ron Diamant and Anoop Deoras and Luke Huan},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=LkJ6qOMv77}
}

@inproceedings{
hou2018analysis,
title={Analysis of Quantized Models},
author={Lu Hou and Ruiliang Zhang and James T. Kwok},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryM_IoAqYX},
}

@inproceedings{hao2017trainingq,
 author = {Li, Hao and De, Soham and Xu, Zheng and Studer, Christoph and Samet, Hanan and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Training Quantized Nets: A Deeper Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/1c303b0eed3133200cf715285011b4e4-Paper.pdf},
 volume = {30},
 year = {2017}
}


@inproceedings{
smith2021on,
title={On the Origin of Implicit Regularization in Stochastic Gradient Descent},
author={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=rq_Qr0c1Hyo}
}

@misc{wikitext,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{barrett2020implicit,
  title={Implicit gradient regularization},
  author={Barrett, David GT and Dherin, Benoit},
  journal={arXiv preprint arXiv:2009.11162},
  year={2020}
}

@article{lambada,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{
foret2021sam,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=6Tm1mposlrM}
}

@misc{you2017lars,
      title={Large Batch Training of Convolutional Networks}, 
      author={Yang You and Igor Gitman and Boris Ginsburg},
      year={2017},
      eprint={1708.03888},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{
You2020Lamb,
title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Syx4wnEtvH}
}

@inproceedings{
reddi2018,
title={On the Convergence of Adam and Beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@inproceedings{KingmaB14,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{schmidt21a,
  title = 	 {Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers},
  author =       {Schmidt, Robin M and Schneider, Frank and Hennig, Philipp},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9367--9376},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/schmidt21a/schmidt21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/schmidt21a.html},
}


@InProceedings{real2020automlzero,
  title = 	 {{A}uto{ML}-Zero: Evolving Machine Learning Algorithms From Scratch},
  author =       {Real, Esteban and Liang, Chen and So, David and Le, Quoc},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8007--8019},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/real20a/real20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/real20a.html},
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{hospedales2021meta,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={9},
  pages={5149--5169},
  year={2021},
  publisher={IEEE}
}

@inproceedings{wichrowska2017learned,
  title={Learned optimizers that scale and generalize},
  author={Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W and Colmenarejo, Sergio Gomez and Denil, Misha and Freitas, Nando and Sohl-Dickstein, Jascha},
  booktitle={International conference on machine learning},
  pages={3751--3760},
  year={2017},
  organization={PMLR}
}

@article{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{
baydin2018online,
title={Online Learning Rate Adaptation with Hypergradient Descent},
author={Atilim Gunes Baydin and Robert Cornish and David Martinez Rubio and Mark Schmidt and Frank Wood},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkrsAzWAb},
}

@inproceedings{
chandra2022gradient,
title={Gradient Descent: The Ultimate Optimizer},
author={Kartik Chandra and Audrey Xie and Jonathan Ragan-Kelley and Erik Meijer},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=-Qp-3L-5ZdI}
}

@misc{chen2023symbolic,
      title={Symbolic Discovery of Optimization Algorithms}, 
      author={Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
      year={2023},
      eprint={2302.06675},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xie2023adan,
      title={Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models}, 
      author={Xingyu Xie and Pan Zhou and Huan Li and Zhouchen Lin and Shuicheng Yan},
      year={2023},
      eprint={2208.06677},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{zaheer2018adaptive,
 author = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Methods for Nonconvex Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished = {\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@misc{Karpathy2022,
  author = {Andrej Karpathy},
  title = {\text{NanoGPT}},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
  commit = {325be85d9be8c81b436728a420e85796c57dba7e}
}

@misc{karpathy2015,
  title = {The Unreasonable Effectiveness of Recurrent Neural Networks},
  howpublished = {\url{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}},
  author={Andrej Karpathy},
  year = {2015}
}

@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={International conference on machine learning},
  pages={2113--2122},
  year={2015},
  organization={PMLR}
}

@inproceedings{franceschi2017forward,
  title={Forward and reverse gradient-based hyperparameter optimization},
  author={Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
  booktitle={International Conference on Machine Learning},
  pages={1165--1173},
  year={2017},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{shoeybi2020megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08053}, 
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@misc{liu2023sophia,
      title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training}, 
      author={Hong Liu and Zhiyuan Li and David Hall and Percy Liang and Tengyu Ma},
      year={2023},
      eprint={2305.14342},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{heo2021adamp,
      title={AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights}, 
      author={Byeongho Heo and Sanghyuk Chun and Seong Joon Oh and Dongyoon Han and Sangdoo Yun and Gyuwan Kim and Youngjung Uh and Jung-Woo Ha},
      year={2021},
      eprint={2006.08217},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
Liu2020Oradam,
title={On the Variance of the Adaptive Learning Rate and Beyond},
author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgz2aEKDr}
}

@misc{chen2020padam,
      title={Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks}, 
      author={Jinghui Chen and Dongruo Zhou and Yiqi Tang and Ziyan Yang and Yuan Cao and Quanquan Gu},
      year={2020},
      eprint={1806.06763},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{
luo2018adabound,
title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
author={Liangchen Luo and Yuanhao Xiong and Yan Liu},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg3g2R9FX},
}

@inproceedings{zhuang2020adabelief,
 author = {Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18795--18806},
 publisher = {Curran Associates, Inc.},
 title = {AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inbook{almeida1999, place={Cambridge}, series={Publications of the Newton Institute}, title={Parameter Adaptation in Stochastic Optimization}, DOI={10.1017/CBO9780511569920.007}, booktitle={On-Line Learning in Neural Networks}, publisher={Cambridge University Press}, author={Almeida, Luís B. and Langlois, Thibault and Amaral, Jose D. and Plakhov, Alexander}, editor={Saad, DavidEditor}, year={1999}, pages={111–134}, collection={Publications of the Newton Institute}}

@article{metz2022velo,
  title={Velo: Training versatile learned optimizers by scaling up},
  author={Metz, Luke and Harrison, James and Freeman, C Daniel and Merchant, Amil and Beyer, Lucas and Bradbury, James and Agrawal, Naman and Poole, Ben and Mordatch, Igor and Roberts, Adam and others},
  journal={arXiv preprint arXiv:2211.09760},
  year={2022}
}

@article{metz2020tasks,
  title={Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves},
  author={Metz, Luke and Maheswaranathan, Niru and Freeman, C Daniel and Poole, Ben and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2009.11243},
  year={2020}
}

@article{almeida2021generalizable,
  title={A generalizable approach to learning optimizers},
  author={Almeida, Diogo and Winter, Clemens and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:2106.00958},
  year={2021}
}

@inproceedings{muhamed2023training,
  title={Training large-scale foundation models on emerging AI chips},
  author={Muhamed, Aashiq and Bock, Christian and Solanki, Rahul and Park, Youngsuk and Wang, Yida and Huan, Jun},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5821--5822},
  year={2023}
}
@inproceedings{park2024inference,
  title={Inference optimization of foundation models on ai accelerators},
  author={Park, Youngsuk and Budhathoki, Kailash and Chen, Liangfu and K{\"u}bler, Jonas M and Huang, Jiaji and Kleindessner, Matth{\"a}us and Huan, Jun and Cevher, Volkan and Wang, Yida and Karypis, George},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6605--6615},
  year={2024}
}
