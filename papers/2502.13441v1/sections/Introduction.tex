\section{Introduction}

In recent years, large language models (LLMs) such as GPT-4o \cite{DBLP:journals/corr/abs-2410-21276}, Gemini~
\cite{DBLP:journals/corr/abs-2312-11805}, Llama \cite{DBLP:journals/corr/abs-2302-13971}, and DeepSeek-R1 \cite{guo2025deepseek} have demonstrated remarkable capabilities, revolutionizing natural language processing and various other tasks. The success of these models can be attributed to the scaling laws \cite{DBLP:journals/corr/abs-2001-08361}, which dictate the relationship between model parameters, computational resources, and training data size. 
% \textcolor{red}{The success of models like Llama-3.1-405B \cite{DBLP:journals/corr/abs-2407-21783} is built on massive, high-quality datasets.} 
For instance, the prominent performance of Llama-3.1 with 405B parameters \cite{DBLP:journals/corr/abs-2407-21783} roots in, amongst others, the massive, high-quality datasets for pre- and post-training. However, as models continue to scale, the available real-world (public) data quickly becomes exhausted; meanwhile, 
%\textcolor{red}{creating a significant bottleneck.} 
manually crafting high-quality %, domain-specific 
data is time- and labor-intensive. Thus, data volume has become a key limiting factor for the effective scaling of new-generation models.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{latex/fig/motivation.pdf}
  \caption{Different schemes of self-improvement.}
  \label{fig:motivation}
\end{figure}

In response to this challenge, synthetic data generation and data augmentation have emerged as key methods to further improve the performance of LLMs while avoiding extensive supervision. These methods leverage the ability of LLMs to mirror real-world distributions and generate high-quality, pseudo-realistic data \cite{DBLP:conf/iclr/ZhangCSDTG23}. Following this line of research, the problem of \emph{self-improvement} naturally arises: Can we improve the performance of an LLM by fine-tuning it with synthetic data generated by itself?
%\textcolor{red}{for scaling LLMs}. 
% It has become increasingly clear that LLMs possess the ability to mirror real-world distributions, generating high-quality, pseudo-realistic data \cite{DBLP:conf/iclr/ZhangCSDTG23}. Taking this idea further, if LLMs could use synthetic data to achieve self-improvement, it would offer tremendous potential. Recently, several works\sytcomment{how many to cite} have focused on self-improvement using synthetic data. 
This problem has triggered a recent surge of research results \cite{DBLP:journals/corr/abs-2410-12896}. These methods, however, rely heavily on \emph{external seed datasets} for augmentation (e.g., \cite{DBLP:conf/emnlp/0001GHW00023,DBLP:conf/acl/WangKMLSKH23}) and/or \emph{stronger third-party models} as classifiers or reward agents (e.g., \cite{DBLP:conf/nips/Le0GSH22,DBLP:journals/corr/abs-2405-14333}); see \cref{fig:motivation}. Such dependency on external supervision signals limits their ability to achieve true self-improvement. Orthogonally, the recently proposed method Magpie~\cite{DBLP:journals/corr/abs-2406-08464} suffices to generate high-quality dialogue datasets (i.e., both responses and instructions) entirely through the model itself. 
%Whereas Magpie's success highlights that LLMs can generate both high-quality responses and instructions, 
Nonetheless, the generated data is highly randomized and primarily dedicated to the alignment of 
% (third-party) 
base LLMs. Such data may improve instruction-following abilities but will degrade fundamental capabilities like math and reasoning; see \cite[Sect.~6]{DBLP:journals/corr/abs-2406-08464}. Recent discussions \cite{DBLP:conf/icml/KambhampatiVGVS24,DBLP:journals/nature/ShumailovSZPAG24} have explicitly questioned whether genuine self-improvement is feasible, suggesting that when trained solely on self-generated data, LLMs may fail. \emph{Can LLMs achieve true self-improvement?} remains an open question in the literature.
%\textcolor{red}{Existing research is unable to provide a definitive answer to this question.}

This paper aims to provide the infrastructure to explore the self-improvement problem of LLMs:
%\textcolor{blue}{This paper provides an affirmative answer to this question by presenting 
We present {\langname} -- \emph{a fully autonomous framework for generating high-quality synthetic question-answer (QA) data that suffice to improve the reasoning capabilities of an LLM while preserving its general performance}.
% \textcolor{red}{In this paper, we propose the {\langname} framework, which efficiently generates high-quality, domain-specific datasets. {\langname} helps explore whether a model can truly self-improve using only its own outputs.}
{\langname} adopts a \emph{simple} yet \emph{effective} workflow:
\begin{enumerate*}[label=(\roman*)]
    \item It uses a \emph{bait prompt} to guide the model to generate raw questions in a specific domain, such as math word problems;
    \item It applies a \emph{self-deduplication} mechanism based on rejection sampling \cite{liu2001monte} to refine and diversify the question pool; and
    \item For each question, it performs majority voting \cite{DBLP:conf/iclr/0002WSLCNCZ23} to identify the most confident answer from the model (thus \emph{enhancing the consensus}).
\end{enumerate*}
The so-obtained QA pairs are then used to fine-tune the original LLM via, e.g., supervised fine-tuning (SFT), to improve its math-reasoning capability.

Experiments with {\langname} demonstrate evident self-improvement of LLMs consistently for three benchmarks on mathematical word problems in both 0-shot and 5-shot settings, without trading off their general capabilities. %despite no supervision from the benchmark training datasets during the SFT process. 
The improvement is especially prominent for the 0-shot case, thus improving the generalization ability of the model to real-world tasks. 
%Moreover, the attained improvement does not 
%, nor the instruction-following ability. 
Ablation studies further demonstrate the superiority of {\langname} over Magpie~\cite{DBLP:journals/corr/abs-2406-08464} in the generation of themed data: the latter tends to generate math-related dialogues, e.g., \enquote{Could you tell me what type of mathematics you like?} -- rather than proper mathematical problems. 
%Furthermore, extensive analysis confirms that this improvement does not trade off general capabilities, and it enhances both domain-specific expertise and instruction-following ability. 
% We conducted a detailed analysis using GPT-4o to compare model performance before and after self-improvement, revealing significant enhancements in mathematical reasoning ability and a decreased sensitivity to different prompt templates, improving the model's generalization to real-world tasks. 
Moreover, our experiments show that {\langname} can serve as a highly effective and efficient distillation method, surpassing the baselines using external data and stronger models.

\paragraph*{\bf Contributions}
Our main contributions include:
\begin{itemize}
    \item We present a simple yet effective framework {\langname} -- utilizing the techniques of bait prompting, diversification, and consensus enhancement -- to investigate the self-improvement problem of LLMs.
    \item We show that {\langname}-generated QA pairs suffice to improve the reasoning capabilities of an LLM with zero supervision signals while preserving its general performance, thereby providing an affirmative answer to the self-improvement problem in the domain of mathematical reasoning (math word problems).
    \item Experiments demonstrate significant improvements achieved by {\langname} compared to multiple prompting methods. %and Magpie. %(for generating themed data). 
    As a by-product, we show {\langname} facilitates more effective %and efficient 
    LLM knowledge distillation than existing approaches based on seed-dataset augmentation.
    % \item We demonstrate that model trained on {\langname} can achieve evident performance improvements in 3 different mathematical reasoning benchmarks;
    % \item To investigate this self-improvement phenomenon, we conducted comprehensive experiments on the trade-off in general capabilities, impact of domain-specific expertise, instruction-following ability, the difference between prompt engineering methods, and the modelâ€™s generalization across different tasks. Furthermore, we explored the unique advantages of the {\langname} method in distillation scenarios.
\end{itemize}





% Aligned LLMs have been shown to generate high-quality responses, and Magpie \citep{DBLP:journals/corr/abs-2406-08464} has confirmed LLMs can also generate high-quality queries.

% The concept of utilizing LLMs for instruction generation is not new. However, most existing approaches either require seed data or training on supplementary instruction datasets, as mentioned in~\cref{sec:related-work}. Such synthetic data introduces additional supervisory signals during the LLM self-improvement process, which can potentially bias the evaluation of the results. Magpie \citep{DBLP:journals/corr/abs-2406-08464} demonstrated that LLMs can autonomously generate a diverse set of high-quality instructions by only modifying chat templates. However, the instructions generated in their work lacked precise thematic guidance. Furthermore, enforcing thematic constraints through direct modifications to the system prompt often results in a significant reduction in the diversity and quality of the generated instructions, as shown in~\cref{subsec:ablation}.