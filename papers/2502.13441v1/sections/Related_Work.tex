\section{Related Work}\label{sec:related-work}

\textbf{Synthetic Data from Scratch:} Recent efforts to reduce reliance on external seed data have led to the exploration of generating data from scratch for fine-tuning LLMs. UltraChat \cite{DBLP:conf/emnlp/DingCXQHL0Z23} shows how to generate diverse, high-quality multi-turn conversations without human queries. Magpie \cite{DBLP:journals/corr/abs-2406-08464} introduces a self-synthesis method to generate large-scale alignment data by utilizing only pre-defined chat templates. GenQA \cite{DBLP:journals/corr/abs-2406-10323} aims to generate large instruction datasets with minimal human oversight by prompting LLMs to create diverse instruction examples. Note note that these methods primarily focus on \emph{creating alignment data to train the instruction-following capabilities of base models}.

\textbf{LLM Self-Improvement:} Recent methods exploring self-improvement demonstrate the potential of enhancing LLMs' capabilities through self-generated feedback. \citep{DBLP:conf/emnlp/0001GHW00023} demonstrates that LLMs can improve by sampling high-confidence answers from existing high-quality question sets. Similarly, CodeRL \cite{DBLP:conf/nips/Le0GSH22} introduces reinforcement learning to program synthesis, where the model receives feedback from unit tests and critic scores from other models, aiming to optimize performance on unseen coding tasks. StaR \cite{DBLP:conf/nips/ZelikmanWMG22} leverages small amounts of rationale examples and iteratively refines the reasoning ability through self-generated rationales. SPIN \cite{DBLP:conf/icml/ChenDYJG24} proposes a self-play fine-tuning method, where a model generates its training data from previous iterations.