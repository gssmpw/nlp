\section{The {\langname} Approach}

This section presents {\langname} -- %model-agnostic 
a framework for \underline{c}ontrolled QA self-gene\underline{r}ation via div\underline{e}r\underline{s}ification and \underline{c}onsensus \underline{en}hancemen\underline{t}. {\langname} suffices to generate high-quality domain-specific QA pairs leveraging only the model itself, with zero external data, nor assistance from third-party models.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{latex/fig/method.pdf}
  \caption{The general workflow of {\langname} in mathematical reasoning.}
  \label{fig:methods}
\end{figure*}

\cref{fig:methods} sketches the general workflow of {\langname}, which consists of three main steps:
\begin{enumerate*}[label=(\Roman*)]
    \item \emph{Bait prompting}: We use a bait prompt to instruct the original, aligned LLM to produce a set of raw questions within a specific domain;\label{step:bp}
    \item \emph{Diversification}: The raw questions may be semantically analogous to each other (as per some similarity metric), and thus we employ a rejection sampling mechanism to attain a diverse pool of representative questions through self-deduplication;\label{step:rs}
    \item \emph{Consensus enhancement}: We treat the generated questions as query prompts and feed them back to the LLM. Then, by majority vote, we obtain the final set of synthetic QA pairs.\label{step:ce}
\end{enumerate*}
We show that such QA pairs are of high quality in the sense that they suffice to improve the domain-specific capabilities (mathematical reasoning, in our case) by fine-tuning the original LLM with these QA pairs while preserving its general capabilities.

Below, we first present the technical details of Steps \ref{step:bp} to \ref{step:ce} and then provide the rationale behind the self-improvement achieved by these steps.

% The design principle of {\langname} framework is simple and straightforward. 
% We start with a \textit{bait prompt} to guide the LLM in generating questions within a specific domain. These questions are then deduplicated and enhanced to create a question pool. Next, we treat these generated questions as new query prompts and input them back into the original LLM. Through self-consistency~\citep{DBLP:conf/iclr/0002WSLCNCZ23}, we extract the high-confidence answers. The overview of our method is showed in~\cref{fig:methods}. In the remaining part of this section, we provide a detailed description of the implementation of our method.

\subsection{Question Generation (Steps \ref{step:bp} and \ref{step:rs})}
%by Rejection Sampling}

% Building on the premise that LLMs are capable of generating high-quality instructions autonomously, we propose the {\langname} framework using Self-Consistency and deduplication-enhancement techniques to create a high-quality, domain-specific question dataset. Specifically, 
We begin by utilizing a simple \emph{bait prompt} to elicit the LLM to generate a bunch of domain-specific questions, such as math word problems illustrated in \cref{fig:methods}, denoted as \emph{raw questions}. As some of them may be semantically analogous to each other, we optimize diversity of the questions in an iterative manner: Each generated question is vectorized and compared against the (embeddings of) other questions. If there exists a question that is deemed sufficiently similar (i.e., the similarity score is below a prescribed threshold), we apply the following \emph{deduplication prompt} to modify it:
\begin{align*}
    &\promptfont{\{\nblue{question}\}~is~very~similar~to~\{\maroon{question}\}, please}\\[-1mm]
    &\promptfont{modify~the~latter~to~make~it~different.}
\end{align*}%
This iterative process ensures that the question pool remains diverse and representative across the specific domain through redundancy-aware selection.
%The self-deduplication step is crucial in maintaining the quality of the generated question dataset.

Formally, the question-generation phase can be described as follows:
Let $Q = \{q_1, q_2, \ldots, q_n\}$ be the set of raw questions generated by the LLM per the bait prompt. For each question $q_i$, we embed it as a real-valued vector $v_i$ and compare it against the vector representations $\{v_1, v_2, \dots, v_{i-1}\}$ of the previously generated questions. The \emph{similarity} between the two questions is determined by the distance between their respective vector embeddings in the inner product space, e.g., the $L^2$ distance.
%\( d(v_i, v_j) \defeq (v_i \cdot v_j)/(\norm{v_i} \norm{v_j})\).
%the distance \( d(v_i, v_j) \) can be computed using a metric such as the cosine distance. 
If the distance is below a given threshold \( \theta \), then \( q_i \) with ($i > j$) is considered as a \emph{duplicate} and thus needs to be modified via the deduplication prompt, i.e.,
\begin{align}\label{eq:deduplication}
\text{If} \  d\left(v_i, v_j\right) < \theta \  \text{then}\ q_i^* = \text{Deduplicate}\left(q_i\right).\tag{$\dagger$}
\end{align}%
Such similarity-based deduplication incorporates the \emph{maximal marginal relevance} (MMR) criterion~\cite{DBLP:conf/sigir/CarbonellG98} to minimize repetition while preserving content relevance. Moreover, the iterative refining process falls into the paradigm of \emph{rejection sampling} (cf.\ e.g., \cite{liu2001monte}), which ultimately yields a diversified question pool featuring relevance and representativeness w.r.t.\ the target domain with negligible redundancy; see \cref{sec:rationale}.

\subsection{Answer Generation (Step \ref{step:ce})}
%via Consensus Enhancement}
Let $Q^* = \{q_1^*, q_2^*, \ldots, q_n^*\}$ be the deduplicated set of questions generated through the previous step. The phase of answer generation aims to synthesize the corresponding high-quality answers w.r.t.\ each $q_i^* \in Q^*$. We achieve this by means of \emph{consensus enhancement}, namely, we feed each question \( q_i^* \) back to the LLM and collect \( m \) \emph{independently} produced answers, denoted by the set \( A_i = \{a_1, a_2, \ldots, a_m\} \), where each \( a_j\) contains integrated chain-of-thought (CoT) processes~\cite{DBLP:conf/nips/Wei0SBIXCLZ22} generated for question \( q_i^* \). We then select the final answer $a_i^*$ for question $q_i^*$ using 
\emph{majority voting}~\cite{DBLP:conf/iclr/0002WSLCNCZ23}.
% (aka \emph{self-consistency}~\citep{DBLP:conf/iclr/0002WSLCNCZ23}). 
That is, we first identify the set $\bar{A}_i$ of \emph{most frequent answers}:
\begin{align*}
    \bar{A}_i \ddefeq \left\{a_j \in A_i \ \big\vert\  f\left(a_j\right) = \max_{a_k \in A_i} f\left(a_k\right)\right\}~,
\end{align*}%
where \( f(a_j) \) denotes the \emph{frequency} (i.e., the number of occurrences) of answer \( a_j \) in \( A_i \). Then, we uniformly sample an answer from $\bar{A}_i$ as the final answer $a_i^*$ paired with question \( q_i^* \). By repeating the majority voting procedure for every question, we obtain the final set of synthetic QA pairs:
\begin{align*}
    (Q^*, A^*) \eeq \left\{ \left(q_1^*,a_1^*\right), \left(q_2^*,a_2^*\right), \ldots, \left(q_n^*,a_n^*\right) \right\}~.
\end{align*}%

% The next step is to select the final answer. The selection is based on the \textbf{frequency} of the generated answers . Let \( f(a_j) \) denote the frequency of answer \( a_j \) in the set \( A_i \), i.e., the number of times \( a_j \) appears. The final answer \( a_i^* \) is chosen as follows:
% $$a_i^* = \arg\max_{a_j \in A_i} f(a_j)$$
% We define the set of most frequent answers:

% The final answer \( a_i^* \) is selected randomly from the set \( A_{\text{max}} \).

% Now, once the final answer \( a_i^* \) has been selected, we pair it with the original question \( q_i \) to form a complete question-answer pair: $(QA)_i = (q_i, a_i^*)$

% This entire process is repeated for all questions in the pool, resulting in a high-quality, domain-specific $\{QA\}$ dataset.

\subsection{Rationale for Self-Improvement}\label{sec:rationale}

Next, we provide the intuition on why self-generated QA pairs using the {\langname} framework can be used to improve the capabilities of the underlying LLM. This observation will be further justified by extensive experiments in \cref{sec:experiments}.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{latex/fig/intuition.pdf}
  \caption{The intuition of {\langname}. Let the black dots be question embeddings and distribution curve be conditional answer distribution. (1) Our diversification step modifies question samples violating the minimal distance criterion per \cref{eq:deduplication} (the middle plot). (2) the consensus enhancement step selects the majority mode answer. (the green X in the left and right plots.)}
  \label{fig:rationale}
\end{figure}

The intuition is three-fold (see \cref{fig:rationale}):
\begin{enumerate}[label=(\roman*)]
    \item \emph{Relevance by bait prompting}: The initial bait prompt restricts the considered space of questions and answers to a specific domain and hence all the generated QA pairs within the {\langname} scope are pertinent to this domain.
    \item \emph{Diversity by rejection sampling-based deduplication}: Our diversification step explores the question space while maintaining a minimal pair-wise distance 
    %\maroon{lower bound} on the distance between every question pair 
    to alleviate redundancy. This is achieved by a rejection sampling loop where question samples violating the distance criterion per \cref{eq:deduplication} are modified and, therefore, the generated questions exhibit a scattered distribution stretching over the space.
    \item \emph{Accuracy by majority voting}: Based on the observation that a complex reasoning problem typically admits multiple distinct ways of thinking yielding its unique correct answer~\citep{DBLP:conf/iclr/0002WSLCNCZ23}, our consensus enhancement step selects, for each question, the most frequent answer that may coincide with the correct one with high likelihood.
\end{enumerate}
As a consequence, fine-tuning the original LLM with the so-obtained QA pairs will strengthen its domain-specific capabilities by \emph{enforcing a reduction in the variance of answer generation for a diverse set of domain-relevant questions}.

\endinput

\subsection{RL Perspective}\label{sec:RLtheory}

Though {\langname} is conceptually straightforward, framing it through the lens of RL reveals a deeper structure underlying the process. From an RL perspective, the seemingly simple steps involved in question generation and answer generation can be viewed as components of an RL agentâ€™s policy optimization.
% By interpreting our approach as an RL method, we highlight its potential to optimize a task-specific objective through internal exploration and exploitation, aligning with RL principles of reward shaping and policy improvement.

From an RL standpoint, {\langname} exhibits fundamental structural equivalence with trial-and-error learning processes, despite superficial differences in implementation. We formally characterize this connection through the lens of Markov Decision Processes (MDPs) and advantage-weighted policy optimization.

\paragraph{RL Formulation}  
Let us define our MDP tuple \((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)\) where:  

\textbf{State space} \(\mathcal{S}\): Current question pool \(Q_t = \{q_1,...,q_n\}\) 

\textbf{Action space} \(\mathcal{A}\): Space of potential questions \(\mathcal{Q}\) and answers \(\mathcal{A}\) 

\textbf{Transition dynamics} \(\mathcal{P}\): Generation process governed by LLM policy \(\pi_\theta\) 

\textbf{Reward function} \(\mathcal{R}\): Implicit reward from answer consensus 

\textbf{Discount factor} \(\gamma\): 1 (finite horizon process)

The policy \(\pi_\theta(a|s)\) consists of two complementary components:  

1. \textbf{Question Generation Policy}: \(\pi_{quest}(q|Q_t)\)  
   $$q_{t+1} \sim \pi_{quest}(\cdot|Q_t) = \text{LLM}_{\theta}(\text{bait\_prompt} \oplus Q_t)$$  
   
2. \textbf{Answer Generation Policy}:  
   $$ A_i = \{a_1, ..., a_n\} \sim \prod_{k=1}^n \pi_{ans}(a_k|q_i) $$  
   where \( \pi_{ans}(a|q) = \text{LLM}_\theta(q) \) generates answer candidates through stochastic sampling.

% 2. \textbf{Majority Voting}:  
%    $$ a_i^* = \arg\max_{a \in A_i} \underbrace{\sum_{k=1}^n \mathbb{I}(a = a_k)}_{\text{Consensus Score}} $$  
%    If multiple answers share the maximal score:  
%    $$ a_i^* \sim \text{Uniform}(\arg\max_{a \in A_i} \sum_{k=1}^n \mathbb{I}(a = a_k)) $$


\paragraph{Dual-Phase Optimization}  
{\langname} framework alternates between:  

\textbf{1. Exploration Phase (Question Generation):}  
$$\max_{\pi_{quest}} \mathbb{E}_{q \sim \pi_{quest}}[\mathcal{D}(q|Q_t)]$$  
where diversity reward \(\mathcal{D}\) is enforced through:  
$$\mathcal{D}(q|Q_t) = \mathbb{I}[d(v_q, v_{q'}) > \theta], \quad \forall q' \in Q_t$$  
This implements an \textit{advantage-weighted} objective with implicit reward shaping.  

% \paragraph{Answer Policy Optimization}  
% The self-consistency mechanism implements an implicit advantage function:  
% $$ A(q,a) = \frac{\text{Count}(a)}{\max(1, \sum_{a'} \text{Count}(a'))} $$  
% This corresponds to on-policy reinforcement learning where the policy gradient is estimated through:  
% $$ \nabla_\theta \mathcal{J} \propto \mathbb{E}[\nabla_\theta \log \pi_{ans}(a|q) A(q,a)] $$  


\textbf{2. Exploitation Phase (Answer Generation):}  
$$\max_{\pi_{ans}} \mathbb{E}_{a_1,...,a_k \sim \pi_{ans}}[\mathbb{I}(a^* = \arg\max_{a} f(a))]$$  
where \( f(a_j) \) refers to the \textbf{frequency} (count) of answer \( a_j \) appearing in the set of generated answers \( A_i \). The advantage function \(A(s,a)\) becomes:  
$$A(q,a) = \begin{cases} 
1 & \text{if } a \in A_{\text{max}} \\
0 & \text{otherwise}
\end{cases}$$  

\paragraph{Theoretical Connection}  
The complete objective function combines both phases:  
$$\mathcal{J}(\theta) = \mathbb{E}_{\substack{q \sim \pi_{quest} \\ a \sim \pi_{ans}}}[A(q,a) - \beta \text{KL}(\pi_{quest} \| \pi_{ref})]$$  
where \(\beta\) controls the strength of KL regularization (implemented via our similarity threshold \(\theta\)), \(\pi_{ref}\) represents the baseline question distribution before deduplication.
% \begin{itemize}
% \item \(\beta\) controls the strength of KL regularization (implemented via our similarity threshold \(\theta\))  
% \item \(\pi_{ref}\) represents the baseline question distribution before deduplication 
% \end{itemize}

This formulation reveals {\langname} as implementing \textit{Consensus-Based Reinforcement Learning} (CBRL), where:  
1. Synthetic data generation corresponds to policy rollouts  
2. Answer consensus mechanism acts as a sparse reward signal  
3. Deduplication process enforces an entropy constraint  

The structural equivalence becomes apparent when considering that each iteration of question generation and filtering constitutes a policy improvement step, while answer generation through Self-Consistency performs policy evaluation through Monte Carlo sampling. This perspective unifies synthetic data generation and RL under a common framework, suggesting {\langname} implicitly optimizes a lower-bound approximation of the true RL objective.  

By interpreting {\langname} as an RL method, we highlight its potential to optimize a task-specific objective through internal exploration and exploitation, aligning with RL principles of reward shaping and policy improvement.
% This RL interpretation provides theoretical grounding for our empirical results while suggesting natural pathways for enhancement through explicit reward modeling or advantage estimation techniques - directions we leave for future work.
% \subsection{RL Perspective}\label{sec:RLtheory}

% Though {\langname} is conceptually straightforward, framing it through the lens of reinforcement learning reveals an underlying decision-making structure. Our approach can be formally characterized as a \textit{fixed-policy Markov Decision Process} with state-dependent filtering, where the LLM serves as a stationary policy and human-defined thresholds govern state transitions.

% \paragraph{RL Formulation}  
% We define our process as a tuple \((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})\) where:  

% - \textbf{State space} \(\mathcal{S}\): Current question pool \(Q_t = \{q_1,...,q_n\}\) with vector representations \(V_t = \{v_1,...,v_n\}\)
% - \textbf{Action space} \(\mathcal{A}\): Question generation through LLM sampling \(\mathcal{A} = \mathcal{Q}\)
% - \textbf{Transition dynamics} \(\mathcal{P}\):  
%   $$ Q_{t+1} = \begin{cases} 
%   Q_t \cup \{q_{new}\} & \text{if } \min_{v \in V_t} d(v, v_{new}) \geq \theta \\
%   Q_t & \text{otherwise}
%   \end{cases} $$
%   where \(q_{new} \sim \text{LLM}(Q_t)\) and \(\theta\) is a fixed similarity threshold
% - \textbf{Reward function} \(\mathcal{R}\): Implicit binary reward for state transitions:
%   $$ \mathcal{R}(s_t, a_t, s_{t+1}) = \mathbb{I}[s_{t+1} \neq s_t] $$

% \paragraph{Policy Execution}  
% The LLM acts as a fixed policy \(\pi_{LLM}\) that maps states to action distributions:
% $$ \pi_{LLM}(a|s) = P_{\text{LLM}}(q_{new}|Q_t) $$
% Our framework then implements a \textit{state-filtering mechanism} governed by the threshold \(\theta\), which determines whether generated questions enter the next state. This creates a dynamic where:

% 1. The LLM policy explores the question space through generation
% 2. The threshold \(\theta\) enforces exploitation by filtering redundant questions

% \paragraph{Theoretical Interpretation}  
% The answer consensus mechanism operates as a \textit{terminal reward function} over trajectories. For a generated QA pair \((q_i, a_i^*)\), we define the episodic reward:
% $$ \mathcal{R}_{\text{final}} = \mathbb{I}[\text{Consensus}(a_i^*) \geq \tau] $$
% where \(\tau\) is the minimum agreement threshold for answer acceptance.

% This formulation reveals {\langname} as implementing a \textit{Monte Carlo Tree Search} variant where:
% 1. Question generation represents node expansion
% 2. Deduplication acts as a pruning criterion
% 3. Answer consensus serves as a leaf node evaluation

% The fixed threshold \(\theta\) functions as an \textit{expert prior} that maintains trajectory quality without policy gradient updates. Through this lens, our method achieves controlled exploration by combining LLM-based generation with human-defined constraints, avoiding the need for explicit policy optimization while maintaining RL-compatible semantics.






