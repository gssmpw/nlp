\section{The {\langname} Approach}

This section presents {\langname} -- %model-agnostic 
a framework for \underline{c}ontrolled QA self-gene\underline{r}ation via div\underline{e}r\underline{s}ification and \underline{c}onsensus \underline{en}hancemen\underline{t}. {\langname} suffices to generate high-quality domain-specific QA pairs leveraging only the model itself, with zero external data, nor assistance from third-party models.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{latex/fig/method.pdf}
  \caption{The general workflow of {\langname} in mathematical reasoning.}
  \label{fig:methods}
\end{figure*}

\cref{fig:methods} sketches the general workflow of {\langname}, which consists of three main steps:
\begin{enumerate*}[label=(\Roman*)]
    \item \emph{Bait prompting}: We use a bait prompt to instruct the original, aligned LLM to produce a set of raw questions within a specific domain;\label{step:bp}
    \item \emph{Diversification}: The raw questions may be semantically analogous to each other (as per some similarity metric), and thus we employ a rejection sampling mechanism to attain a diverse pool of representative questions through self-deduplication;\label{step:rs}
    \item \emph{Consensus enhancement}: We treat the generated questions as query prompts and feed them back to the LLM. Then, by majority vote, we obtain the final set of synthetic QA pairs.\label{step:ce}
\end{enumerate*}
We show that such QA pairs are of high quality in the sense that they suffice to improve the domain-specific capabilities (mathematical reasoning, in our case) by fine-tuning the original LLM with these QA pairs while preserving its general capabilities.

Below, we first present the technical details of Steps \ref{step:bp} to \ref{step:ce} and then provide the rationale behind the self-improvement achieved by these steps.

% The design principle of {\langname} framework is simple and straightforward. 
% We start with a \textit{bait prompt} to guide the LLM in generating questions within a specific domain. These questions are then deduplicated and enhanced to create a question pool. Next, we treat these generated questions as new query prompts and input them back into the original LLM. Through self-consistency~\citep{DBLP:conf/iclr/0002WSLCNCZ23}, we extract the high-confidence answers. The overview of our method is showed in~\cref{fig:methods}. In the remaining part of this section, we provide a detailed description of the implementation of our method.

\subsection{Question Generation (Steps \ref{step:bp} and \ref{step:rs})}
%by Rejection Sampling}

% Building on the premise that LLMs are capable of generating high-quality instructions autonomously, we propose the {\langname} framework using Self-Consistency and deduplication-enhancement techniques to create a high-quality, domain-specific question dataset. Specifically, 
We begin by utilizing a simple \emph{bait prompt} to elicit the LLM to generate a bunch of domain-specific questions, such as math word problems illustrated in \cref{fig:methods}, denoted as \emph{raw questions}. As some of them may be semantically analogous to each other, we optimize diversity of the questions in an iterative manner: Each generated question is vectorized and compared against the (embeddings of) other questions. If there exists a question that is deemed sufficiently similar (i.e., the similarity score is below a prescribed threshold), we apply the following \emph{deduplication prompt} to modify it:
\begin{align*}
    &\promptfont{\{\nblue{question}\}~is~very~similar~to~\{\maroon{question}\}, please}\\[-1mm]
    &\promptfont{modify~the~latter~to~make~it~different.}
\end{align*}%
This iterative process ensures that the question pool remains diverse and representative across the specific domain through redundancy-aware selection.
%The self-deduplication step is crucial in maintaining the quality of the generated question dataset.

Formally, the question-generation phase can be described as follows:
Let $Q = \{q_1, q_2, \ldots, q_n\}$ be the set of raw questions generated by the LLM per the bait prompt. For each question $q_i$, we embed it as a real-valued vector $v_i$ and compare it against the vector representations $\{v_1, v_2, \dots, v_{i-1}\}$ of the previously generated questions. The \emph{similarity} between the two questions is determined by the distance between their respective vector embeddings in the inner product space, e.g., the $L^2$ distance.
%\( d(v_i, v_j) \defeq (v_i \cdot v_j)/(\norm{v_i} \norm{v_j})\).
%the distance \( d(v_i, v_j) \) can be computed using a metric such as the cosine distance. 
If the distance is below a given threshold \( \theta \), then \( q_i \) with ($i > j$) is considered as a \emph{duplicate} and thus needs to be modified via the deduplication prompt, i.e.,
\begin{align}\label{eq:deduplication}
\text{If} \  d\left(v_i, v_j\right) < \theta \  \text{then}\ q_i^* = \text{Deduplicate}\left(q_i\right).\tag{$\dagger$}
\end{align}%
Such similarity-based deduplication incorporates the \emph{maximal marginal relevance} (MMR) criterion~\cite{DBLP:conf/sigir/CarbonellG98} to minimize repetition while preserving content relevance. Moreover, the iterative refining process falls into the paradigm of \emph{rejection sampling} (cf.\ e.g., \cite{liu2001monte}), which ultimately yields a diversified question pool featuring relevance and representativeness w.r.t.\ the target domain with negligible redundancy; see \cref{sec:rationale}.

\subsection{Answer Generation (Step \ref{step:ce})}
%via Consensus Enhancement}
Let $Q^* = \{q_1^*, q_2^*, \ldots, q_n^*\}$ be the deduplicated set of questions generated through the previous step. The phase of answer generation aims to synthesize the corresponding high-quality answers w.r.t.\ each $q_i^* \in Q^*$. We achieve this by means of \emph{consensus enhancement}, namely, we feed each question \( q_i^* \) back to the LLM and collect \( m \) \emph{independently} produced answers, denoted by the set \( A_i = \{a_1, a_2, \ldots, a_m\} \), where each \( a_j\) contains integrated chain-of-thought (CoT) processes~\cite{DBLP:conf/nips/Wei0SBIXCLZ22} generated for question \( q_i^* \). We then select the final answer $a_i^*$ for question $q_i^*$ using 
\emph{majority voting}~\cite{DBLP:conf/iclr/0002WSLCNCZ23}.
% (aka \emph{self-consistency}~\citep{DBLP:conf/iclr/0002WSLCNCZ23}). 
That is, we first identify the set $\bar{A}_i$ of \emph{most frequent answers}:
\begin{align*}
    \bar{A}_i \ddefeq \left\{a_j \in A_i \ \big\vert\  f\left(a_j\right) = \max_{a_k \in A_i} f\left(a_k\right)\right\}~,
\end{align*}%
where \( f(a_j) \) denotes the \emph{frequency} (i.e., the number of occurrences) of answer \( a_j \) in \( A_i \). Then, we uniformly sample an answer from $\bar{A}_i$ as the final answer $a_i^*$ paired with question \( q_i^* \). By repeating the majority voting procedure for every question, we obtain the final set of synthetic QA pairs:
\begin{align*}
    (Q^*, A^*) \eeq \left\{ \left(q_1^*,a_1^*\right), \left(q_2^*,a_2^*\right), \ldots, \left(q_n^*,a_n^*\right) \right\}~.
\end{align*}%

% The next step is to select the final answer. The selection is based on the \textbf{frequency} of the generated answers . Let \( f(a_j) \) denote the frequency of answer \( a_j \) in the set \( A_i \), i.e., the number of times \( a_j \) appears. The final answer \( a_i^* \) is chosen as follows:
% $$a_i^* = \arg\max_{a_j \in A_i} f(a_j)$$
% We define the set of most frequent answers:

% The final answer \( a_i^* \) is selected randomly from the set \( A_{\text{max}} \).

% Now, once the final answer \( a_i^* \) has been selected, we pair it with the original question \( q_i \) to form a complete question-answer pair: $(QA)_i = (q_i, a_i^*)$

% This entire process is repeated for all questions in the pool, resulting in a high-quality, domain-specific $\{QA\}$ dataset.

\subsection{Rationale for Self-Improvement}\label{sec:rationale}

Next, we provide the intuition on why self-generated QA pairs using the {\langname} framework can be used to improve the capabilities of the underlying LLM. This observation will be further justified by extensive experiments in \cref{sec:experiments}.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{latex/fig/intuition.pdf}
  \caption{The intuition of {\langname}. Let the black dots be question embeddings and distribution curve be conditional answer distribution. (1) Our diversification step modifies question samples violating the minimal distance criterion per \cref{eq:deduplication} (the middle plot). (2) the consensus enhancement step selects the majority mode answer. (the green X in the left and right plots.)}
  \label{fig:rationale}
\end{figure}

The intuition is three-fold (see \cref{fig:rationale}):
\begin{enumerate}[label=(\roman*)]
    \item \emph{Relevance by bait prompting}: The initial bait prompt restricts the considered space of questions and answers to a specific domain and hence all the generated QA pairs within the {\langname} scope are pertinent to this domain.
    \item \emph{Diversity by rejection sampling-based deduplication}: Our diversification step explores the question space while maintaining a minimal pair-wise distance 
    %\maroon{lower bound} on the distance between every question pair 
    to alleviate redundancy. This is achieved by a rejection sampling loop where question samples violating the distance criterion per \cref{eq:deduplication} are modified and, therefore, the generated questions exhibit a scattered distribution stretching over the space.
    \item \emph{Accuracy by majority voting}: Based on the observation that a complex reasoning problem typically admits multiple distinct ways of thinking yielding its unique correct answer~\citep{DBLP:conf/iclr/0002WSLCNCZ23}, our consensus enhancement step selects, for each question, the most frequent answer that may coincide with the correct one with high likelihood.
\end{enumerate}
As a consequence, fine-tuning the original LLM with the so-obtained QA pairs will strengthen its domain-specific capabilities by \emph{enforcing a reduction in the variance of answer generation for a diverse set of domain-relevant questions}.

\endinput

\subsection{RL Perspective}\label{sec:RLtheory}

Though {\langname} is conceptually straightforward, framing it through the lens of RL reveals a deeper structure underlying the process. From an RL perspective, the seemingly simple steps involved in question generation and answer generation can be viewed as components of an RL agent’s policy optimization.
% By interpreting our approach as an RL method, we highlight its potential to optimize a task-specific objective through internal exploration and exploitation, aligning with RL principles of reward shaping and policy improvement.

From an RL standpoint, {\langname} exhibits fundamental structural equivalence with trial-and-error learning processes, despite superficial differences in implementation. We formally characterize this connection through the lens of Markov Decision Processes (MDPs) and advantage-weighted policy optimization.

\paragraph{RL Formulation}  
Let us define our MDP tuple \((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)\) where:  

\textbf{State space} \(\mathcal{S}\): Current question pool \(Q_t = \{q_1,...,q_n\}\) 

\textbf{Action space} \(\mathcal{A}\): Space of potential questions \(\mathcal{Q}\) and answers \(\mathcal{A}\) 

\textbf{Transition dynamics} \(\mathcal{P}\): Generation process governed by LLM policy \(\pi_\theta\) 

\textbf{Reward function} \(\mathcal{R}\): Implicit reward from answer consensus 

\textbf{Discount factor} \(\gamma\): 1 (finite horizon process)

The policy \(\pi_\theta(a|s)\) consists of two complementary components:  

1. \textbf{Question Generation Policy}: \(\pi_{quest}(q|Q_t)\)  
   $$q_{t+1} \sim \pi_{quest}(\cdot|Q_t) = \text{LLM}_{\theta}(\text{bait\_prompt} \oplus Q_t)$$  
   
2. \textbf{Answer Generation Policy}:  
   $$ A_i = \{a_1, ..., a_n\} \sim \prod_{k=1}^n \pi_{ans}(a_k|q_i) $$  
   where \( \pi_{ans}(a|q) = \text{LLM}_\theta(q) \) generates answer candidates through stochastic sampling.

% 2. \textbf{Majority Voting}:  
%    $$ a_i^* = \arg\max_{a \in A_i} \underbrace{\sum_{k=1}^n \mathbb{I}(a = a_k)}_{\text{Consensus Score}} $$  
%    If multiple answers share the maximal score:  
%    $$ a_i^* \sim \text{Uniform}(\arg\max_{a \in A_i} \sum_{k=1}^n \mathbb{I}(a = a_k)) $$


\paragraph{Dual-Phase Optimization}  
{\langname} framework alternates between:  

\textbf{1. Exploration Phase (Question Generation):}  
$$\max_{\pi_{quest}} \mathbb{E}_{q \sim \pi_{quest}}[\mathcal{D}(q|Q_t)]$$  
where diversity reward \(\mathcal{D}\) is enforced through:  
$$\mathcal{D}(q|Q_t) = \mathbb{I}[d(v_q, v_{q'}) > \theta], \quad \forall q' \in Q_t$$  
This implements an \textit{advantage-weighted} objective with implicit reward shaping.  

% \paragraph{Answer Policy Optimization}  
% The self-consistency mechanism implements an implicit advantage function:  
% $$ A(q,a) = \frac{\text{Count}(a)}{\max(1, \sum_{a'} \text{Count}(a'))} $$  
% This corresponds to on-policy reinforcement learning where the policy gradient is estimated through:  
% $$ \nabla_\theta \mathcal{J} \propto \mathbb{E}[\nabla_\theta \log \pi_{ans}(a|q) A(q,a)] $$  


\textbf{2. Exploitation Phase (Answer Generation):}  
$$\max_{\pi_{ans}} \mathbb{E}_{a_1,...,a_k \sim \pi_{ans}}[\mathbb{I}(a^* = \arg\max_{a} f(a))]$$  
where \( f(a_j) \) refers to the \textbf{frequency} (count) of answer \( a_j \) appearing in the set of generated answers \( A_i \). The advantage function \(A(s,a)\) becomes:  
$$A(q,a) = \begin{cases} 
1 & \text{if } a \in A_{\text{max}} \\
0 & \text{otherwise}
\end{cases}$$  

\paragraph{Theoretical Connection}  
The complete objective function combines both phases:  
$$\mathcal{J}(\theta) = \mathbb{E}_{\substack{q \sim \pi_{quest} \\ a \sim \pi_{ans}}}[A(q,a) - \beta \text{KL}(\pi_{quest} \| \pi_{ref})]$$  
where \(\beta\) controls the strength of KL regularization (implemented via our similarity threshold \(\theta\)), \(\pi_{ref}\) represents the baseline question distribution before deduplication.
% \begin{itemize}
% \item \(\beta\) controls the strength of KL regularization (implemented via our similarity threshold \(\theta\))  
% \item \(\pi_{ref}\) represents the baseline question distribution before deduplication 
% \end{itemize}

This formulation reveals {\langname} as implementing \textit{Consensus-Based Reinforcement Learning} (CBRL), where:  
1. Synthetic data generation corresponds to policy rollouts  
2. Answer consensus mechanism acts as a sparse reward signal  
3. Deduplication process enforces an entropy constraint  

The structural equivalence becomes apparent when considering that each iteration of question generation and filtering constitutes a policy improvement step, while answer generation through Self-Consistency performs policy evaluation through Monte Carlo sampling. This perspective unifies synthetic data generation and RL under a common framework, suggesting {\langname} implicitly optimizes a lower-bound approximation of the true RL objective.  

By interpreting {\langname} as an RL method, we highlight its potential to optimize a task-specific objective through internal exploration and exploitation, aligning with RL principles of reward shaping and policy improvement.
% This RL interpretation provides theoretical grounding for our empirical results while suggesting natural pathways for enhancement through explicit reward modeling or advantage estimation techniques - directions we leave for future work.
% \subsection{RL Perspective}\label{sec:RLtheory}

% Though {\langname} is conceptually straightforward, framing it through the lens of reinforcement learning reveals an underlying decision-making structure. Our approach can be formally characterized as a \textit{fixed-policy Markov Decision Process} with state-dependent filtering, where the LLM serves as a stationary policy and human-defined thresholds govern state transitions.

% \paragraph{RL Formulation}  
% We define our process as a tuple \((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})\) where:  

% - \textbf{State space} \(\mathcal{S}\): Current question pool \(Q_t = \{q_1,...,q_n\}\) with vector representations \(V_t = \{v_1,...,v_n\}\)
% - \textbf{Action space} \(\mathcal{A}\): Question generation through LLM sampling \(\mathcal{A} = \mathcal{Q}\)
% - \textbf{Transition dynamics} \(\mathcal{P}\):  
%   $$ Q_{t+1} = \begin{cases} 
%   Q_t \cup \{q_{new}\} & \text{if } \min_{v \in V_t} d(v, v_{new}) \geq \theta \\
%   Q_t & \text{otherwise}
%   \end{cases} $$
%   where \(q_{new} \sim \text{LLM}(Q_t)\) and \(\theta\) is a fixed similarity threshold
% - \textbf{Reward function} \(\mathcal{R}\): Implicit binary reward for state transitions:
%   $$ \mathcal{R}(s_t, a_t, s_{t+1}) = \mathbb{I}[s_{t+1} \neq s_t] $$

% \paragraph{Policy Execution}  
% The LLM acts as a fixed policy \(\pi_{LLM}\) that maps states to action distributions:
% $$ \pi_{LLM}(a|s) = P_{\text{LLM}}(q_{new}|Q_t) $$
% Our framework then implements a \textit{state-filtering mechanism} governed by the threshold \(\theta\), which determines whether generated questions enter the next state. This creates a dynamic where:

% 1. The LLM policy explores the question space through generation
% 2. The threshold \(\theta\) enforces exploitation by filtering redundant questions

% \paragraph{Theoretical Interpretation}  
% The answer consensus mechanism operates as a \textit{terminal reward function} over trajectories. For a generated QA pair \((q_i, a_i^*)\), we define the episodic reward:
% $$ \mathcal{R}_{\text{final}} = \mathbb{I}[\text{Consensus}(a_i^*) \geq \tau] $$
% where \(\tau\) is the minimum agreement threshold for answer acceptance.

% This formulation reveals {\langname} as implementing a \textit{Monte Carlo Tree Search} variant where:
% 1. Question generation represents node expansion
% 2. Deduplication acts as a pruning criterion
% 3. Answer consensus serves as a leaf node evaluation

% The fixed threshold \(\theta\) functions as an \textit{expert prior} that maintains trajectory quality without policy gradient updates. Through this lens, our method achieves controlled exploration by combining LLM-based generation with human-defined constraints, avoiding the need for explicit policy optimization while maintaining RL-compatible semantics.






