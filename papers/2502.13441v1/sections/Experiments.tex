\section{Experiments}\label{sec:experiments}


\subsection{Experimental Setups}

\paragraph{Benchmarks}
We adopt three benchmarks on math word problems (MWPs): (i) \textbf{GSM8K}~\citep{cobbe2021gsm8k}: 8.5K grade school math problems with step-by-step solutions;  
(ii) \textbf{ASDiv}~\citep{DBLP:conf/acl/MiaoLS20}: 2,305 diverse MWPs covering multiple difficulty levels; and (iii) \textbf{GSM-Plus}~\citep{DBLP:conf/acl/LiCZKB24}: an enhanced version of GSM8K with 12K problems incorporating robustness checks. In order to accelerate the evaluation, we use \textbf{GSM-Plus-mini} -- a subset of GSM-Plus containing 2,400 questions. It should be noted that the GSM-Plus-mini and GSM8K datasets do not overlap.

\paragraph{Baseline Models} We conduct self-improvement experiments with two different LLM models:
(i) Llama3-8B-Instruct: the instruction-tuned version of Llama3-8B~\citep{DBLP:journals/corr/abs-2407-21783}; and
(ii) Llama2-7B-Chat~\citep{DBLP:journals/corr/abs-2307-09288}: a instruction-tuned version of Llama2-7B. 

\paragraph{Generation Configurations} 
For each model, we generate MWP QA pairs following these settings:  

\textbf{Question Generation:} Bait prompt: \emph{\enquote{Generate a diverse math word problem requiring multi-step reasoning}}. We generate 50K candidate questions for Llama2-7B-Chat and 75k for Llama3-8B-Instruct, both with temperature $T=0.95$. Diversification: We use sentence embeddings generated by the \texttt{all-MiniLM-L6-v2} model from the Sentence-BERT~\citep{DBLP:conf/emnlp/ReimersG19} family; we eliminate semantically similar questions using the $L^2$ distance with threshold $\theta=0.25$. We employ 
%Facebook AI Similarity Search 
FAISS \citep{douze2024faiss} to accelerate vector computation and comparisons.

\textbf{Answer Generation:} For each question, sample 5 answers with temperature $T=0.95$, then select the most frequent answer as the final answer. We use the same answer generation settings for both models. We use the vLLM~\citep{kwon2023efficient} inference framework for both generation stages.

\textbf{GPU hours:}
It took 30.0 GPU hours to generate 75k QA pairs with Llama3-8B-Instruct and 42.9 GPU hours for the 50k pairs with Llama2-7B-Chat.

\paragraph{SFT Implementation}  
Our SFT procedure uses single-epoch training with max sequence length of %truncated at 
2,048 tokens. Optimization is performed using AdamW~\citep{DBLP:conf/iclr/LoshchilovH19} ($\beta_1=0.9, \beta_2=0.95$) under a linear learning rate schedule (initial LR = 1e-5, 3\% warm-up), and the batch size is set to 128 through 8-way parallelization on NVIDIA A100%-PCIe
-80GB GPUs with 16-step gradient accumulation. We use DeepSpeed Stage3~\citep{DBLP:conf/kdd/RasleyRRH20}
%acceleration, 
and \texttt{bfloat16} %(mixed-precision training) 
for mitigating memory constraints, and FlashAttention-2~\citep{dao2023flashattention2} for efficient attention computation. 

\paragraph{Evaluation Protocol}  
We use LM-Evaluation-Harness \citep{eval-harness} library; all datasets are evaluated under \textbf{0-shot} and \textbf{5-shot} settings. Few-shot examples are randomly selected from training sets, excluding test samples. We use two \emph{answer extractors}: one identifies the number appearing after "\#\#\#\#" and the other extracts the last number in the output. An answer is considered correct if either of the extractors retrieves the correct answer.

\subsection{Main Results}\label{sec:main_results}

The experimental results shown in \cref{tab:main_results} validate our core hypothesis: \emph{self-generated reasoning QA pairs -- boosted through diversification and consensus enhancement -- enable model improvement without external supervision signals}. 
% Across both models and all benchmarks, {\langname} achieves consistent gains through pure self-generation. 
For GSM8K, Llama2-7B-Chat shows improvements of +4.4\%$\uparrow$ (0-shot) and +2.1\%$\uparrow$ (5-shot), while Llama3-8B-Instruct achieves noticeable gains of +28.8\%$\uparrow$ (0-shot) and +1.8\%$\uparrow$ (5-shot). Similar observations apply consistently to ASDiv 
% (+4.3\%$\uparrow$ 0-shot for Llama2, +22.3\%$\uparrow$ for Llama3; +0.3\%$\uparrow$ 5-shot for Llama3) 
and GSM-Plus-mini featuring different QA distributions.

\begin{table}[t]
  \centering
  \caption{Main results comparing original models vs. {\langname} versions. Best results in \textbf{bold} (accuracy \%). %$\dagger$ indicates p < 0.05 via bootstrap test.
  }
  \label{tab:main_results}
  \begin{small}
  \setlength{\tabcolsep}{2pt}
\resizebox{\linewidth}{!}{%
  \begin{tabular}{cccccccc}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Training} & \multicolumn{3}{c}{0-shot} & \multicolumn{3}{c}{5-shot} \\
    \cmidrule(l{2pt}r{2pt}){3-5} \cmidrule(l{2pt}r{2pt}){6-8}
    & & GSM8K & ASDiv & GSM+ & GSM8K & ASDiv & GSM+ \\
    \midrule
    \multirow{2}{*}{Llama2-7B-Chat} & Original & 18.8 &41.7  &11.3 & 23.0 &\textbf{45.9}  &13.5  \\
     &\langname & \textbf{23.2} &\textbf{46.0}  &\textbf{13.0}  & \textbf{25.1} &45.2  &\textbf{14.8}  \\
    \midrule
    \multirow{2}{*}{Llama3-8B-Inst.} & Original & 34.5 & 43.6 & 23.1 & 75.8 &62.3  & 51.2 \\
    & \langname & \textbf{63.3} & \textbf{65.9} & \textbf{48.6} & \textbf{77.6} &\textbf{63.8}  & \textbf{52.8} \\
    \bottomrule
  \end{tabular}
  }
  \end{small}
\end{table}

% \begin{table*}[t]
%   \centering
%   \caption{Main results comparing original models vs. {\langname} versions. Best results in \textbf{bold} (accuracy \%). %$\dagger$ indicates p < 0.05 via bootstrap test.
%   }
%   \label{tab:main_results}
%   \begin{small}
%   \begin{tabular}{cccccccc}
%     \toprule
%     \multirow{2}{*}{Model} & \multirow{2}{*}{Training} & \multicolumn{3}{c}{0-shot} & \multicolumn{3}{c}{5-shot} \\
%     \cmidrule(lr){3-5} \cmidrule(lr){6-8}
%     & & GSM8K & ASDiv & GSM+ & GSM8K & ASDiv & GSM+ \\
%     \midrule
%     \multirow{2}{*}{Llama2-7B-Chat} & Original & 18.8 &41.7  &11.3 & 23.0 &\textbf{45.9}  &13.5  \\
%      &\langname & \textbf{23.2} &\textbf{46.0}  &\textbf{13.0}  & \textbf{25.1} &45.2  &\textbf{14.8}  \\
%     \midrule
%     \multirow{2}{*}{Llama3-8B-Instruct} & Original & 34.5 & 43.6 & 23.1 & 75.8 &62.3  & 51.2 \\
%     & \langname & \textbf{63.3} & \textbf{65.9} & \textbf{48.6} & \textbf{77.6} &\textbf{63.8}  & \textbf{52.8} \\
%     \bottomrule
%   \end{tabular}
%   \end{small}
% \end{table*}

% The \emph{self-improvement phenomenon} was observed on three distribution-differing MWP benchmarks (combining 0-shot and 5-shot settings). 
It is noteworthy that {\langname} leads to \emph{substantial improvements in the 0-shot} setting across all three datasets, with performance on certain datasets surpassing even the 5-shot counterparts for the original models. This observation highlights the potential of 0-shot learning in reducing dependency on task-specific examples, thus indicating better generalization to real-world unseen problem types.
% (+1.7\%$\uparrow$ 0-shot for Llama2, +25.5\%$\uparrow$ for Llama3; +1.3\%$\uparrow$ 5-shot for Llama2, +1.6\%$\uparrow$ for Llama3).

% These results structurally align with our RL formulation. The \textit{question generation phase} corresponds to exploration through policy rollouts ($\pi_{quest}$), where diversity maintenance via deduplication ($\theta=0.25$) enforces state-space coverage. The \textit{answer generation phase} implements exploitation through consensus-based rewards ($A(q,a) \in {0,1}$), where self-consistency filtering selects high-confidence training targets. The combined process—generating diverse questions (exploration) then reinforcing consensus answers (exploitation)—mirrors an RL agent’s policy optimization cycle under the MDP structure defined in Section 3.3.

% The amplified gains for Llama3 (+83.5\% relative improvement vs. +25.5\% for Llama2 on GSM8K) further validate the RL analogy: stronger base models better approximate the theoretical ideal of an optimal policy $\pi_{*}$, where exploration (question diversity) and exploitation (answer consensus) balance through the KL-regularized objective ($\mathcal{J}(\theta)$). This demonstrates that even simple self-supervised mechanisms, when framed as RL components, can unlock substantial latent capabilities in modern LLMs.

% In our main experiment, both models of Llama2 and Llama3 exhibit improvements after training with {\langname}, with more significant gains observed in 0-shot performance and stable improvements in 5-shot performance. Importantly, these gains are achieved without any external data from any training set of these benchmarks, demonstrating the authenticity and generalization ability of the improvement across different benchmarks. The results highlight the substantial benefits of 0-shot performance, which is crucial for LLMs in real-world applications. Ultimately, our findings confirm that for mathematical reasoning tasks, models can self-improve using high-quality, self-generated data with only simple constraints during the generation phase.

\subsection{Ablation Study}\label{subsec:ablation}

\begin{figure}[t]
    \centering
  \includegraphics[width=.72\columnwidth]{latex/fig/ablation_study.pdf}
  \caption{Accuracies w.r.t.\ the ablation study.}
  \label{fig:ablation}
\end{figure}

To justify the pivotality of {\langname}'s core components, we conduct comprehensive ablation experiments over Llama3-8B-Instruct under 5-shot GSM8K evaluation. As depicted in \cref{fig:ablation}, 
\begin{enumerate*}[label=(\roman*)]
    \item full method of {\langname} achieves accuracy of 77.6\%, outperforming all ablated variants and the baseline;
    \item removing consensus enhancement (w/o CE) reduces performance to 73.0\% (-4.6\%);
    \item excluding diversification (w/o DV) yields a more severe drop to 71.1\% (-6.53\%);
    \item using only bait prompting (BP only) results in 70.6\% (-7.0\%).
\end{enumerate*}
The results demonstrate the significance of both diversification and consensus enhancement.
%, showing that they work synergistically to enhance model performance rather than acting independently.  


\begin{figure*}[t]
\centering
\hspace{0.1cm}
\begin{subfigure}[b]{0.30\linewidth}
    \centering
    \includegraphics[height=2.8cm]{latex/fig/tsne_visualization.pdf}
    \caption{\langname}
    \label{fig:tsne-a}
\end{subfigure}
\hfil
\begin{subfigure}[b]{0.30\linewidth}
    \centering
    \includegraphics[height=2.8cm]{latex/fig/tsne_visualization_ablation.pdf}
    \caption{\langname w/o DV}
    \label{fig:tsne-b}
\end{subfigure}
\hfil
\begin{subfigure}[b]{0.30\linewidth}
    \centering
    \includegraphics[height=2.8cm]{latex/fig/tsne_visualization_magpie_math.pdf}
    \caption{Magpie-Math}
    \label{fig:tsne-c}
\end{subfigure}
\hfil
\hspace{-0.4cm}
\begin{subfigure}[b]{0.06\linewidth}
    \centering
    \raisebox{0.7cm}{  
        \includegraphics[height=2.73cm]{latex/fig/color_bar.pdf}
    }
    % \caption{Magpie-Math}
    \label{fig:tsne-c}
\end{subfigure}
\caption{T-SNE visualization of synthetic math questions. Points colored from 1 to 9 represent mathematical questions with increasing difficulty; Gray marks math-related questions (rather than actual mathematical problems).}
\label{fig:ablation tsne}
\end{figure*}

% \begin{figure*}[t]
% \centering
% \begin{subfigure}[b]{0.32\linewidth}
%     \centering
%     \includegraphics[width=4.4cm,height=3.9cm]{latex/fig/tsne_visualization.pdf}
%     \caption{\langname}
%     \label{fig:tsne-a}
% \end{subfigure}
% \hfil
% \hspace{-0.43cm}
% \begin{subfigure}[b]{0.32\linewidth}
%     \centering
%     \includegraphics[width=4.4cm,height=3.9cm]{latex/fig/tsne_visualization_ablation.pdf}
%     \caption{\langname w/o DV}
%     \label{fig:tsne-b}
% \end{subfigure}
% \hfil
% \begin{subfigure}[b]{0.32\linewidth}
%     \centering
%     \includegraphics[width=5.07cm,height=3.96cm]{latex/fig/tsne_visualization_magpie_math.pdf}
%     \caption{Magpie-Math}
%     \label{fig:tsne-c}
% \end{subfigure}
% \caption{T-SNE visualization of synthetic math questions. Points colored from 1 to 9 represent mathematical questions with increasing difficulty; Gray marks math-related questions (rather than actual mathematical problems).}
% \label{fig:ablation tsne}
% \end{figure*}

% \begin{figure*}[t]
% \centering
% \begin{subfigure}[b]{0.32\linewidth}
%     \centering
%     \includegraphics[height=4.323cm,keepaspectratio]{latex/fig/tsne_visualization.pdf}
%     \caption{\langname}
%     \label{fig:tsne-a}
% \end{subfigure}
% \hfil
% \begin{subfigure}[b]{0.32\linewidth}
%     \centering
%     \includegraphics[height=4.323cm,keepaspectratio]{latex/fig/tsne_visualization_ablation.pdf}
%     \caption{\langname w/o DV}
%     \label{fig:tsne-b}
% \end{subfigure}
% \hfil
% \begin{subfigure}[b]{0.32\linewidth}
%     \centering
%     \includegraphics[height=4.4cm,keepaspectratio]{latex/fig/tsne_visualization_magpie_math.pdf}
%     \caption{Magpie-Math}
%     \label{fig:tsne-c}
% \end{subfigure}
% \caption{T-SNE visualizations of question generation from three methods. Different point colors represent varying levels of difficulty, with gray points representing questions that are \emph{math-related dialogues yet not actual mathematical problems}.}
% \label{fig:ablation tsne}
% \end{figure*}

Notably, {\langname} surpasses the Magpie variants by substantial margins: 
\begin{enumerate*}[label=(\roman*)]
    \item +5.6\% over Magpie-Common (Magpie-C) (72.0\%);
    \item +11.0\% over Magpie-Math (Magpie-M) (66.6\%).
\end{enumerate*}

To investigate the discrepancy between {\langname} and Magpie-Math, we conduct a sampling analysis on the mathematical questions generated by {\langname}, {\langname} w/o DV, and Magpie-Math: For each method, we randomly sample 1,500 questions; Each question is then classified by difficulty using GPT-4o~\cite{DBLP:journals/corr/abs-2410-21276}, vectorized with the \texttt{all-MiniLM-L6-v2} embedding model, and projected into a two-dimensional plane using t-SNE~\cite{van2008visualizing}. The visualization in \cref{fig:ablation tsne} suggests that,
% questions generated by Magpie-Math exhibit dense clustering, revealing pattern repetition inherent to template-based generation. In contrast, our guided bait prompting produces more dispersed patterns, and incorporating deduplication achieves optimal spatial uniformity, confirming that thematic guidance and diversity control jointly enable comprehensive domain coverage.  
even without diversification, {\langname} can still generate high-quality mathematical questions, albeit with reduced diversity and difficulty (\cref{fig:tsne-b}). In contrast, the vectors for Magpie-Math problems (\cref{fig:tsne-c}) feature (i) a more agglomerate form exhibiting significantly low coverage than {\langname}; and (ii) numerous gray points signifying non-mathematical problems; they are merely instructions related to the mathematics topic, e.g., \emph{\enquote{Could you tell me what type of mathematics you like?}}. The latter aligns with the observation in~\cite[Sect.~6]{DBLP:journals/corr/abs-2406-08464} stating that Magpie-generated dialogues may degrade math and reasoning capabilities. 
%In comparison, the questions generated by {\langname} exhibit a more dispersed distribution with a broader difficulty range, as shown in \cref{fig:tsne-a}. This result demonstrates a clear advantage of {\langname} in generating \emph{domain-specific} datasets, with improved diversity and coverage.

% \begin{figure*}[t]
%   \includegraphics[width=0.32\linewidth]{latex/fig/tsne_visualization.pdf} \hfill
%   \includegraphics[width=0.32\linewidth]{latex/fig/tsne_visualization_ablation.pdf} \hfill
%   \includegraphics[width=0.32\linewidth]{latex/fig/tsne_visualization_magpie_math.pdf}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{Ablation tsne}
%   \label{fig:ablation_tsne}
% \end{figure}
% The experimental evidence solidifies two principles:  
% 1. \textbf{Self-Consistency as Answer Quality Control}: The 4.6\% accuracy gap between full and w/o SC variants validates SC's role in suppressing contradictory or low-confidence answers.  
% 2. \textbf{Deduplication as Diversity Enforcement}: The 9.5\% degradation from w/o deduplication demonstrates that duplicate questions induce overfitting to redundant patterns during SFT.  

% Crucially, even with identical training data volume (75K QA pairs), our framework outperforms Magpie-Math by 11.0\%, proving that \textit{controlled diversity}—maintaining thematic focus while maximizing problem variation—is more critical than mere data quantity for mathematical self-improvement. The t-SNE patterns further establish a quantitative correlation: methods with higher embedding dispersion consistently achieve better downstream accuracy, confirming our hypothesis that question diversity directly governs self-training efficacy.