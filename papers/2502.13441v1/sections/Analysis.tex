\section{Detailed Analysis of {\langname}}
\subsection{General-Capability Preservation}
\emph{Will {\langname} incur catastrophic forgetting of general capabilities?}
%\emph{Does the self-improvement of mathematical reasoning capabilities degrade other capabilities of the LLM?} 
We address this problem by evaluating Llama3-8B-Instruct before and after {\langname} on five non-mathematical benchmarks covering \emph{commonsense reasoning} (ARC-C \cite{DBLP:journals/corr/abs-1803-05457}, HellaSwag \cite{DBLP:conf/acl/ZellersHBFC19}), \emph{general knowledge preserving} (MMLU \cite{DBLP:conf/iclr/HendrycksBBZMSS21}), \emph{instruction following} (IFEval \cite{DBLP:journals/corr/abs-2311-07911}), and \emph{graduate-level question answering} (GPQA \cite{DBLP:journals/corr/abs-2311-12022}). We use the {\langname} checkpoint directly from \cref{sec:main_results}.

\begin{table}[t]
  \centering
  \caption{General capability before/after {\langname} (\%).}
  \label{tab:gen_cap}
  \begin{small}
  \begin{tabular}{ccccc}
    \toprule
    Benchmark &\#shots& before & after &$\Delta$\\
    \midrule
    ARC-C &0 &52.9 & 52.3&\maroon{0.6$\downarrow$} \\
    MMLU &5& 65.6 & 65.9&\green{0.3$\uparrow$} \\
    IFEval &-& 50.9 & 52.5&\green{1.6$\uparrow$} \\
    HellaSwag  &5& 77.9 & 77.2&\maroon{0.7$\downarrow$} \\
    GPQA &0&31.2 &31.5&\green{0.3$\uparrow$} \\ 
    % \midrule
    % Average&- &- &- &\green{0.12$\uparrow$} \\
    \bottomrule
  \end{tabular}
  \end{small}
\end{table}

\Cref{tab:gen_cap} shows that the {\langname}-enhanced model exhibits performance comparable to that of the original model in all five tasks. This observation reveals that domain-specific self-enhancement through {\langname} does not compromise general capabilities, a critical advantage over fine-tuning approaches using external data, which often exhibit significant capability trade-offs \cite{DBLP:journals/corr/abs-2308-08747}.

\subsection{Analysis of Corrected Questions}\label{sec:0shotcase}

Our results show significant improvements in the 0-shot setting. However, does this improvement reflect better generalization, or is it due to the lack of formatting constraints in GSM8K's 0-shot evaluation, which can lead to incorrect answer extraction? To investigate, we analyze Llama3-8B-Instruct's 0-shot results before and after applying {\langname}, focusing on questions that were incorrect before but correct after \textbf{(corrected questions)}. We use GPT-4o to classify and analyze these errors.

\begin{figure}[t]
  \centering
  \includegraphics[width=.71\linewidth]{latex/fig/ring_chart.pdf}
  \caption{Breakdown of the corrected questions after applying {\langname} in the 0-shot setting.}
  \label{fig:0shot_case}
\end{figure}

\cref{fig:0shot_case} shows the total number of corrected questions is 453. 390 (86\%) of them are due to genuine improvement in mathematical reasoning ability. These corrected questions can be further broken down into the following:
\begin{enumerate*}[label=(\roman*)]
    \item \textbf{Stepwise reasoning:} 199 questions (44\%) had errors in stepwise reasoning due to variable tracking (113), step sequence issues (47), and missing steps (39);
    \item \textbf{Mathematical concept:} 115 questions (25\%) involved fundamental math errors, with 98 attributed to calculation mistakes and 17 to unit conversion failures;
    \item \textbf{Redundant information:} 37 questions (8\%) were impacted by irrelevant information in the problem statement;
    \item \textbf{Logical structure:} 19 questions (4\%) involved errors in logical reasoning, such as issues with propositions or set operations;
    \item \textbf{Other errors:} 20 questions (4\%) were due to other miscellaneous error types.
\end{enumerate*}

Meanwhile, there are 63 (14\%) corrected questions due to a better output format. After fine-tuning with {\langname}-generated QA pairs, these questions are correctly answered without generating redundant content, indicating that {\langname}'s high-quality QA data also improves the model's instruction-following capability. 

% It is important to note that the answer extraction errors cannot be directly addressed by defining the formatting requirements in the prompt (as discussed in \cref{sec:prompt_method}). Therefore, we still consider them errors in the main experiments subject to a penalty for insufficient instruction-following capability. This is also for fairness in comparison, since all the experiments employ the same answer extractor.
%and may contain similar issues.

\subsection{Comparison with Prompt Engineering}\label{sec:prompt_method}
\emph{Can prompt techniques %in the evaluation phase 
achieve a similar performance with {\langname}?}
% We now demonstrate the effectiveness of {\langname} by comparing it against different \emph{prompt techniques} in the evaluation phase. We select the original LLaMA3-8B-Instruct model and its counterpart trained via {\langname} from  \cref{sec:main_results}. 
We address this question by comparing {\langname}-trained LLaMA3-8B-Instruct against five prompting methods:
\begin{enumerate*}[label=(\roman*)]
    \item \textbf{Standard prompt} from Llama3 official repository;\footnote{\url{https://github.com/meta-llama/llama-cookbook}}
    \item \textbf{Standard prompt with self-consistency} (SC, aka majority voting) following the settings in \cite{DBLP:conf/iclr/0002WSLCNCZ23};
    \item \textbf{Random rephrased} utilizes GPT-4o to randomly rephrase the standard prompt five times (where we select the best evaluation result). 
    Considering the answer-extractor failures discussed in \cref{sec:0shotcase}, we carefully craft \emph{each instruction} to control the output format, such as requesting the answer to \emph{be placed after "\#\#\#\#"} or \emph{at the end of the output}, ensuring that the prompt includes relevant formatting information compatible with our answer extractor when rephrased by GPT-4o;
    \item \textbf{CoT prompt} following the settings in \cite{DBLP:conf/nips/Wei0SBIXCLZ22};
    \item \textbf{Optimized prompt} by integrating CoT, the best candidate from random rephrased, and the SC process.
\end{enumerate*}

The comparison results are reported in \cref{tab:prompt_comp}. Overall, 0-shot outcomes demonstrate higher sensitivity to prompt variations compared to 5-shot configurations. For the original model, the optimized prompt achieves optimal performance, improving 0-shot accuracy by 10.6\% over standard prompts while exhibiting comparable 5-shot results. However, this result remains \emph{substantially inferior} (-18.2\%) to {\langname} using only standard prompts. Notably, when employing the same optimized prompts, the {\langname}-enhanced model further improves 0-shot performance by 6.5\%.

% Table~\ref{tab:prompt_comp} demonstrates {\langname}'s superiority in both 0-shot and 5-shot settings in GSM8K benchmark. Notably, the framework outperforms chain-of-thought (CoT) prompting by +12.8\% on GSM8K (0-shot) and self-consistency sampling by +9.3\% (5-shot), indicating that \textit{our method surpasses in-context prompting strategies}.

\begin{table}[t]
  \centering
  \caption{Comparison with prompting methods (\%).}
  \label{tab:prompt_comp}
  \begin{small}
  \begin{tabular}{lcc}
    \toprule
    Method & 0-shot & 5-shot \\
    \midrule
    Standard prompt & 34.5 & 75.8 \\
    Standard prompt + SC & 37.8 & 75.6 \\
    Random rephrased & 36.9 & 75.8 \\
    CoT prompt & 43.6 & 76.0 \\    
    Optimized prompt & 45.1 & 75.7 \\
    \midrule
    \textbf{{\langname} + standard} & \textbf{63.3} & \textbf{77.6} \\
    \textbf{{\langname} + optimized} & \textbf{69.8} & \textbf{77.1} \\
    \bottomrule
  \end{tabular}
  \end{small}
\end{table}

The observed performance gap substantiates that the improvements achieved by {\langname} \emph{cannot be replicated} through prompting techniques.
% , providing empirical validation for the authenticity of its self-improvement, underscoring {\langname}'s divergence from prompt-based approaches. 
Moreover, in random rephrased experiments (cf.\ \cref{tab:random_prompts}), {\langname} demonstrates \emph{superior robustness} across five different prompts, exhibiting consistent performance with 37.4\% higher accuracy and much lower standard deviation. This result indicates that {\langname} not only enhances \emph{domain-specific proficiency}, but also establishes \emph{prompt-agnostic generalization} in 0-shot scenarios.

\begin{table}[t]
\centering

\caption{0-shot robustness w.r.t.\ rephrased prompts (\%).}
\label{tab:random_prompts}
\adjustbox{max width=\columnwidth}{
\begin{tabular}{cccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{5}{c}{Random rephrased trials} & \multirow{2}{*}{Mean} & \multirow{2}{*}{Std $\sigma$} \\
\cmidrule(lr){2-6}
& {T1} & {T2} & {T3} & {T4} & {T5} \\
\midrule
Original & 29.9 & 19.9 & 28.6 & 36.9 & 24.4 & 27.9 & 5.69 \\
{\langname} & 64.9 & 63.3 & 64.6 & 67.8 & 66.1 & 65.3 & 1.52 \\
\bottomrule
\end{tabular}}
% \vspace{0.2cm}
% \small\raggedright
\end{table}

\subsection{Data Efficiency and Training Dynamics}
Next, we investigate \emph{the effect of self-improvement in terms of the volume of synthetic data and the number of training epochs}.

\begin{figure}[t]
  \centering
  \includegraphics[width=.86\linewidth]{latex/fig/data_amount.pdf}
  \caption{Accuracy in terms of synthetic data volume.}
  \label{fig:data_volume}
\end{figure}

\textbf{Data Volume}: We perform one epoch of SFT using Llama3-8B-Instruct on {\langname} data with data volumes of 25k, 50k, 75k, 100k, and 150k; we use the standard prompt for evaluation. As shown in \cref{fig:data_volume}, the model's performance improves consistently from 25k to 75k, but stabilizes between 75k and 150k, suggesting an upper limit to the improvement gained from increasing data volume.
% indicating that the improvement brought by {\langname} is not sensitive to data volume. This also suggests that there is a performance upper limit to the improvement gained from increasing data volume.

\textbf{Training Epochs}: We perform SFT with Llama3-8B-Instruct on 50k {\langname} data for 4 epochs. The evaluation is conducted using the standard prompt. \Cref{tab:epochs} shows that, in both settings of 0-shot and 5-shot, the model exhibits a steady performance as the number of epochs increases. 
%the model's 0-shot performance steadily improved, and there was also a noticeable improvement in the 5-shot performance.

%The above experimental results confirm that {\langname} does not require careful selection of data volume and epoch hyperparameters to achieve self-improvement. To enhance experimental efficiency, we only report the training results of the Llama3-8B-Instruct model with 75k {\langname} data for one epoch in \cref{sec:main_results}.


\begin{table}[t]
\centering
\caption{Accuracy in terms of number of epochs (\%).}
% \caption{Performance across different epochs on GSM8K (\%).}
\label{tab:epochs}
\begin{small}
\begin{tabular}{lccccc}
\toprule
\#epochs & {1} & {2} & {3} & {4}  \\
\midrule
0-shot & 50.8 & 60.4 & 61.1 & 62.6   \\
5-shot & 74.3 & 75.7 & 75.3 & 75.9   \\
\bottomrule
\end{tabular}
\end{small}
% \vspace{0.2cm}
\end{table}

\subsection{{\langname} for Model Distillation}

Next, we explore the potential of using the {\langname}-generated data to distil the knowledge of an LLM into a weaker model. Specifically, we use 50k data generated by Llama3-8B-Instruct through {\langname} to perform SFT on Llama2-7B-Chat, with settings inherited from \cref{sec:main_results}. We compare this approach with the following distillation methods:
\begin{enumerate*}[label=(\roman*)]
    \item Directly using the \textbf{GSM8K training set} without external model enhancement, which contains only 7k samples;
    \item \textbf{MetaMath} \cite{DBLP:conf/iclr/YuJSYLZKLWL24}: a method bootstraps existing math datasets by rewriting questions from multiple perspectives, generating a new dataset called MetaMathQA. For comparability, we use Llama3-8B-Instruct to generate 50k new QA pairs from GSM8K training set;
    \item \textbf{ScaleQuest} \cite{DBLP:journals/corr/abs-2410-18693}: a hybrid method combining multiple models, including Qwen2-Math-7B \cite{DBLP:journals/corr/abs-2409-12122}, DeepSeekMath7B-RL \cite{DBLP:journals/corr/abs-2402-03300}, GPT-4o, and InternLM2-7B-Reward \cite{DBLP:journals/corr/abs-2403-17297}, along with datasets from GSM8K and MATH. We randomly sample 50k QA pairs from their open-source dataset;\footnote{\url{https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math}}
    \item \textbf{MMIQC} \cite{DBLP:journals/corr/abs-2401-09003}: a method leverages GPT-4o to enhance existing GSM8K, MATH and MetaMathQA datasets. We similarly sample 50k QA pairs from their open-source data\footnote{\url{https://huggingface.co/datasets/Vivacem/MMIQC}}.
\end{enumerate*}

\begin{table}[t]
  \centering
  \caption{Comparison of distillation approaches (\%).}
  \label{tab:distill_comp}
    \setlength{\tabcolsep}{2pt}
\resizebox{\linewidth}{!}{%
  \begin{tabular}{ccccccc}
    \toprule
    Method & Teacher data &\#Data& Teacher model & Acc (5-shot)& Acc (0-shot)   \\
    \midrule
    - & GSM8K &7k&-&38.4&38.4  \\ \hline
    MetaMath & GSM8K &50k&Llama3-8B-I.& 41.7&22.0 \\
    ScaleQuest &GSM8K\&MATH&50k& Mix & 38.9&22.8  \\
    MMIQC &Mix&50k& GPT-4 & 33.7&28.3  \\
    \langname & - &50k&Llama3-8B-I.& \textbf{44.8} &\textbf{30.8} \\
    \bottomrule
  \end{tabular}
  }
\end{table}

The results shown in \cref{tab:distill_comp} demonstrate that {\langname} outperforms all other approaches that rely on external data or stronger models. This highlights that {\langname} is an efficient and effective distillation approach, requiring no external datasets, let alone complex interactions with them. Furthermore, this result also suggests that excessive reliance on external data during distillation may limit the quality of the distilled data, in other words, the model inherently features the ability to produce data of higher quality than the seed dataset, but is constrained to merely modifying or enhancing the seed data; {\langname}, in contrast, unleashes such ability to achieve self-improvement.
%the model to generate training data of superior quality without any external data.




