\begin{tabular}{|>{\raggedright\arraybackslash}m{4cm}|>{\raggedright\arraybackslash}m{3cm}|>{\raggedright\arraybackslash}m{7cm}|}
\hline
\textbf{Name} & \textbf{Notation} & \textbf{Description} \\
\hline
Input Sequence & $x$ & Input sequence that is passed to the model. \\
Output Sequence & $y$ & Expected label or output of the model. \\
\hline
Dispreferred Response & $y_l$ & Negative samples for reward model training. \\
Preferred Response & $y_w$ & Positive samples for reward model training. \\
\hline
Optimal Policy Model & $\pi^*$ & Optimal policy model. \\
Policy Model & $\pi_\theta$ & Generative model that takes the input prompt and returns a sequence of output or probability distribution. \\
Reference Policy Model & $\pi_{\text{ref}}$ & Generative model that is used as a reference to ensure the policy model is not deviated significantly. \\
\hline
Preference Dataset & $\mathcal{D}_{\text{pref}}$ & Dataset with a set of preferred and dispreferred responses to train a reward model. \\
SFT Dataset & $\mathcal{D}_{\text{sft}}$ & Dataset with a set of input and label for supervised fine-tuning. \\
\hline
Loss Function & $\mathcal{L}$ & Loss function. \\
Regularization Hyper-parameters & $\alpha, \beta_{\text{reg}}$ & Regularization Hyper-parameters for preference tuning. \\
Reward & $r$ & Reward score. \\
Target Reward Margin & $\gamma$ & The margin separating the winning and losing responses. \\
Variance & $\beta_i$ & Variance (or noise schedule) used in diffusion models. \\
\hline
\end{tabular}