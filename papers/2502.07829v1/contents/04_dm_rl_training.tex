\section{Preference Alignment on DMs}
\label{sec:preference_fine_tuning_on_t2i}
\subsection{Framework}

\begin{figure*}[th!]
    \centering
    \includegraphics[width=1\linewidth]{figures/framework_0208.png}
    \caption{Preference Alignment on DM Framework: The process begins with a prompt model encoding a prompt into an embedding, and a source image is transformed into a latent representation $z_t$. DMs fine-tuned with RLHF, DPO, and SFT, process these embeddings to generate new images or edit existing ones while preserving structural integrity, ensuring outputs align with user preferences.}
    \label{fig:total_framework}
    % \vspace{-5pt}
\end{figure*}

\begin{table*}[th!]
\centering
\resizebox{1\linewidth}{!}
{\input{tables/03_image_generation_editing_classification}}
\caption{Preference Alignment methods, which are categorized based on the approaches under study, without restricting their applicability to other domains or modalities.}
\label{tab:method_categories}
% \vspace{-10pt}
\end{table*}

Generally, the preference alignment process for generative models can be formulated as an RL problem, which includes the following components: a policy model, a reward model, an action space, and an environment. The policy model $\pi_\theta$ is a generative model taking an input as prompt $x$, and producing either a sequence of outputs or probability distributions $y$. Furthermore, we categorize all preference alignment approaches into the following taxonomy, as shown in Table~\ref{tab:method_categories}.

% In more detail, a generative model is defined as a policy model $\pi_\theta$, parameterized by $\theta$. Given a prompt $x$, the generative model generates an output $y$ as the following expression:
% \begin{align}
% \pi_\theta(y \mid x) = \prod_t \pi_\theta(y_t \mid x, y_{<t}), \nonumber
% \end{align}
% \vspace{-10pt}

% \noindent where $y_t$ represents the $t$-th token in the response, and $y_{<t}$ denotes all tokens in the response preceding $y_t$; the input $x$ is the text sequence, while $y$ is the generated
% image.
% \begin{equation}
% \resizebox{.9\linewidth}{!}{$
% \begin{aligned}
% \mathcal{L}_{\text{RL}}(\phi) = 
% \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(\cdot \mid x)} 
% \Big[ r_\phi^*(x, y) - \beta_{\text{reg}} \, \text{KL} \big(\pi(\cdot|x) || \pi_{\text{ref}}(\cdot | x)\big) \Big],
% \label{eq:rl_optimization}
% \end{aligned}$}
% \end{equation}

% % less preferred outcomes receive lower scores compared to more preferred samples
% The reward model takes input as both the text sequence $x$ and the generated
% image $y$ to compute a reward value $r_\theta(y \mid x)$, indicating the relative degree of preference. This score can be interpreted as a relative measure assigned to the $y$ given the $x$, where the scores are proportional to the preferability. The action space is constituted of continuous real-valued representations of images, such as the next hierarchical level in diffusion generative models \textcolor{red}{[DJ: I cannot understand this example.]}. The environmental distribution is defined over all possible input token sequences from generative models, as its sample space. In the context of vision tasks, the sample space includes all possible images.

% \subsubsection{Terminology and Notation}

% Table \ref{tab:notation} presents the key notations for RL used in this survey, offering a clear reference to aid in understanding the mathematical expressions and technical terms discussed throughout the paper.

\noindent \textbf{Sampling.} In RL, methods are categorized by how data is sampled and used to train or obtain rewards: online and offline human alignment. Online alignment refers to generative models acting as policies that collect data through interaction with their environments, acquiring reward values either from a predefined reward model or from the policies themselves. Online methods are further divided into on-policy learning with identical behavior and target policies and off-policy learning with different behavior and target policies. Offline alignment means that generative models use collected human demonstrations in prior for training.

\noindent \textbf{Training Strategy.} Training strategies for preference alignment in DMs include RL, DPO, and others. RL techniques, such as PPO, allow models to align with human preferences by maximizing a reward function. DPO directly optimizes the reward signal without relying on RL steps, providing a computationally efficient alternative that will be detailed later. We discuss GFlowNets-based and self-play methods in others.

\noindent \textbf{Reward Feedback.} Data collection strategies can be divided into two main genres: human annotation and AI-generated annotation. Human annotation means collecting preference data through manual evaluations and comparisons, which provide high-quality but often limited datasets. AI annotation leverages pre-trained models to generate large-scale annotations automatically, serving as a scalable and cost-effective approach. These two methods are often combined to create hybrid datasets that balance quality and scalability.

\noindent \textbf{Modalities.} The approaches reviewed are categorized based on their optimized modalities: textual, visual, and multi-modalities. Textual modalities focus on optimizing textual prompts for improved image generation. Visual modalities mean directly updating the DM. Multi-modalities approaches combine textual and visual modalities to create comprehensive frameworks that leverage cross-modal information, resulting in more cohesive and context-aware outputs.

\subsection{RLHF}
Generally, RLHF involves learning a reward function from human feedback and subsequently optimizing policies with respect to the reward~\citep{christiano2017deep}. The training process for RLHF consists of three stages. Firstly, the policy model \( \pi_\theta \) interacts with the environment, and its parameters are updated using reinforcement learning techniques. Next, pairs of output segments generated by \( \pi_\theta \) are selected and presented to human annotators for preference comparison. Finally, the model's parameters are optimized based on a reward signal \( r \), which is elicited from the human-provided comparisons.

As briefly introduced in Sec.~\ref{sec:pre_diffusion_model}, the formulation of text-to-image DMs can be described as a Markov Chain Process (MDP). There are two typical related works in RLHF for DMs, DDPO \citep{black2023training} and DPOK \citep{fan2024reinforcement}. Consider taking $(i, x_i, c)$ as the state space, and define the action as the next hierarchy $x_{i-1}$ to go to. Then, Eq.~\ref{eq:back_denoise} naturally defines a stochastic policy: the stochasticity of the policy comes from $\sqrt{\beta_i z_i}$. Accordingly, the policy follows a Gaussian distribution $\mathcal{N} \big(s_\theta^* (i, x_i, c), \beta_i \big)$ with the mean determined by $s_\theta^* (i, x_i, c)$ and variance $\beta_i$:
\begin{equation}
\begin{aligned}
\pi_\theta(x_{i-1} \mid x_i) \sim  \mathcal{N} \left(\frac{1}{\sqrt{1 - \beta_i}} \left( x_i + \beta_i s_\theta(i, x_i, c) \right), \beta_i \right). \nonumber
% \quad i = N, N-1, \dots, 1.&
\end{aligned}
\end{equation}

Given this formulation, DDPO \citep{black2023training} directly maximize the expected reward, 
$J_{\text{DDPO}} = \mathbb{E}_\theta [r(x_0, c)],$
using either the REINFORCE or PPO algorithms, without applying regularization.
\vspace{-5pt}
\begin{equation}
\begin{aligned}
\nabla_\theta J_{\text{DDPO}} = \mathbb{E} \left[ 
\sum_{t=0}^{T} \nabla_\theta \log p_\theta(x_{t-1} \mid x_t, c) \, r(x_0, c)
\right]. \nonumber
\end{aligned}
\end{equation}

In contrast, DPOK \citep{fan2024reinforcement}, optimizes the regularized reward objective as below. They proposed a clipped gradient algorithm inspired by the PPO objective and showed that incorporating regularization improves generation quality over unregularized versions.
\begin{equation}
\resizebox{.95\linewidth}{!}{$
\begin{aligned}
J_{\text{DPOK}} = \mathbb{E}_\theta \big[r(x_0, c)\big] 
- \beta \mathbb{E}_{p(z)} \big[\text{KL}(p_\theta(x_0 \mid z) \parallel p_{\text{pre}}(x_0 \mid z))\big]. \nonumber
\end{aligned}$}
\end{equation}

Moreover, the Human Preference Score (HPS) aligns T2I models with human preferences by leveraging a classifier trained on a large-scale dataset of human choices~\citep{wu2023human}. \citet{xu2024imagereward} applies Reward Feedback Learning (ReFL) for better synthesis, which trains on 137,000 expert comparisons. VersaT2I~\citep{guo2024versat2i} adopts a self-training approach with Low-Rank Adaptation (LoRA) to address multiple image quality aspects, introducing a mixture of LoRA models for improved quality consistency. The Parrot framework \citep{lee2025parrot} employs a multi-reward optimization algorithm using reinforcement learning to balance aesthetics, alignment, sentiment, and human preferences through Pareto-optimal selection, achieving superior image quality. Diffusion-MVP integrates visual rewards into text-to-image generation including Visual Market, Visual Aesthetic, and CLIP relevance~\citep{he2023learning}.  To address the depth-efficiency trade-off, Deep Reward Tuning (DRTune) optimizes reward functions in DMs by controlling early denoising steps~\citep{wu2025deep}. LRLD~\citep{zhang2025large} introduces a scalable RL framework that focuses on improving text-to-image alignment with human preferences, fairness, and object composition across a
diverse set of reward functions. 

On the other hand, recent efforts in fine-tuning DMs have expanded to \textbf{image editing}. InstructRL4Pix \citep{li2024instructrl4pix} introduces a reinforcement learning framework for image editing, utilizing attention maps and Proximal Policy Optimization to enhance feature localization and preservation. Incorporating human preferences into image editing workflows, HIVE takes advantage of human feedback to train a reward function~\citep{zhang2024hive}. To address the challenge of generative diversity, DRLDiv \citep{miao2024training} is a method that quantifies the overlap between generative and reference distributions. LfVoid exemplifies multi-modal integration by leveraging pre-trained T2I models to generate visual objectives for robotic learning tasks~\citep{gao2023can}. By editing images based on natural language instructions, it guides RL agents without in-domain data, showcasing strong performance in simulated and real-world tasks and the potential of generative models in robotics.

The above methods underscore the growing importance of human-centered optimization strategies in T2I generation, while further advancements have introduced exploration into \textbf{AI feedback} mechanisms. Specifically, LLMs and LVLMs have demonstrated to function with minimal human intervention as the evaluation strategy. 
% DreamSync takes a model-agnostic approach, enhancing both the faithfulness and aesthetic quality through the LVLM feedback~\citep{sun2023dreamsync}.  
\citet{liang2024rich} introduces RAHF, which provides automatic feedback on implausible regions and misaligned keywords, facilitating training data refinement and region inpainting. LVLM-REFL integrates LVLMs to evaluate alignment between generated images and input texts~\citep{wen2023improving}. Additionally, this method incorporates an iterative correction process during inference, applying image-editing algorithms guided by the LVLM.    

Recently, there has been a growing surge of interest in the precise refinement of \textbf{text modality}. Prompt Auto-Editing (PAE) utilizes a two-stage training process combining supervised fine-tuning and online RL for dynamic prompts control~\citep{mo2024dynamic}. This approach enables the iterative refinement of prompts of the weights and injection time steps of each word. In contrast, NegOpt optimizes negative prompts to improve image aesthetics and fidelity~\citep{ogezi2024optimizing}. 
% POAC~\citep{fan2024prompt} incorporates a Prompt Language Model that has been fine-tuned on a curated dataset which employs reinforcement learning to enhance the generation of abstract concepts. 
TextCraftor~\citep{li2024textcraftor} focuses on the text encoder, leveraging reward functions to optimize performance without paired datasets. 
% TexForce exclusively fine-tunes text encoders with the objective of optimizing text-image alignment and visual quality through task-specific rewards, ensuring seamless integration with pre-trained U-Net models~\citep{chen2025enhancing}. 
The integration of LLMs and VLMs as carriers or alternative representations of reward functions has introduced novel strategies for optimizing textual modality. OPT2I~\citep{manas2024improving} exemplifies this by employing automatic prompt optimization, where an LLM iteratively refines prompts to maximize a consistency score while preserving image quality and improving recall between generated and real data. Similarly, the RPG framework~\citep{yang2024mastering} advances text-to-image generation by using multimodal LLMs to decompose complex prompts into detailed subprompts, allocate these across image regions via chain-of-thought planning, and apply complementary regional diffusion for precise image synthesis.
DiffChat introduces an interactive image creation framework that leverages the InstructPE dataset and RL to integrate feedback on aesthetics, user preferences, and content integrity, enabling efficient generation of high-quality target prompts~\citep{wang2024diffchat}. These approaches underscore the growing importance of LLMs and VLMs in enhancing prompt optimization and aligning text-to-image models with user-driven goals.

Another common approach is direct reward backpropagation. Direct reward backpropagation simplifies implementation, especially with a pre-trained differentiable reward model, and accelerates training by directly backpropagating reward gradients. DRaFT propagates gradients directly from reward functions to update policies~\citep{clark2023directly}. It has proven effective across a variety of reward functions, significantly enhancing the aesthetic quality of images generated by Stable Diffusion.  \citet{prabhudesai2024aligningtexttoimagediffusionmodels} introduces AlignProp, reformulating denoising in text-to-image DMs as a differentiable recurrent policy. It uses randomized truncated backpropagation to mitigate over-optimisation, randomly selecting the denoising step for reward backpropagation.
Furthermore, \citet{uehara2024finetuningcontinuoustimediffusionmodels} propose ELEGANT to directly optimize entropy-enhanced rewards with neural SDEs, effectively preventing excessive reward maximization.

\subsection{DPO and its Variants}
This section explains each preference alignment method like DPO and its variants. One notable drawback of RLHF is that the RL optimization step often demands significant computational resources \citep{winata2024preference}, such as those required for PPO. To address this, DPO, recently proposed by~\citep{rafailov2024direct}, offers a promising alternative by bypassing the reward modelling stage and eliminating the need for reinforcement learning, which has garnered considerable attention. The core idea of DPO is based on the observation that, given a reward function $r(x, y)$, 
% the problem admits a closed-form solution:
% \begin{equation}
% \begin{aligned}
% \pi_r(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) 
% \exp\left(\frac{1}{\beta_{\text{reg}}} r(x, y)\right). \nonumber
% \end{aligned}
% \end{equation}
the DPO objective can be described as follows:
% \vspace{-5pt}
\begin{equation}
\resizebox{.95\linewidth}{!}{$
\begin{aligned}
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) := & - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \nonumber \\
& \Bigg[\log \sigma \Bigg( 
\beta_{\text{reg}} \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta_{\text{reg}} \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}
\Bigg)
\Bigg]. \nonumber
\end{aligned}$}
\end{equation}

\citet{wallace2024diffusion} investigates human preference learning in text-to-image DMs and introduces Diffusion-DPO, a novel approach to aligning these models with user preferences. By adapting the DPO framework, Diffusion-DPO reformulates the objective of incorporating the evidence lower bound, enabling a differentiable and efficient optimization process. HF-T2I ~\citep{lee2023aligning} proposes a reward-weighted likelihood maximization framework by collecting binary feedback. As for temporal rewards DPO, \citet{yang2024dense} introduces a dense reward perspective with temporal discounting in DPO-style objectives, prioritizing early steps in the generation process to improve alignment efficiency. PRDP~\citep{deng2024prdp} reformulates RLHF as a supervised regression task, allowing stable fine-tuning in large prompt datasets. In response to the provided text prompts, the method selects two image candidates and trains the DM to predict reward differences between them based on their denoising trajectories. The model parameters are updated based on the prediction's mean square error loss. In DMs, the storage of gradients for multiple latent image representations, which are substantially larger than word embeddings, imposes memory requirements that are frequently infeasible. In \citep{yang2024using} (D3PO), the denoising process is reframed as a multi-step MDP to reduce computational overhead and enable DPO-based fine-tuning of DMs with human feedback. Using a pre-trained model to represent the action value function  $Q$, the DPO framework is extended to allow direct parameter updates at each denoising step, eliminating the need for a reward model and significantly reducing computational costs. Diffusion-KTO introduces a novel approach for aligning text-to-image models by using only per-image binary feedback, removing the need for costly pairwise preference data \citep{li2024aligningdiffusionmodelsoptimizing}. TailorPO~\citep{ren2025refiningalignmentframeworkdiffusion} ranks intermediate noisy samples by step-wise reward and efficiently addresses gradient direction issues.

Unlike DPO, which operates on a fixed offline dataset, \textbf{Online DPO} dynamically constructs preference datasets by leveraging model outputs and various labelers, such as pre-trained reward models, LLM judges, or even the trained model itself via prompting. To address off-policy optimization and scalability challenges in existing video preference learning frameworks, \citet{zhang2024onlinevpoalignvideodiffusion} propose an online DPO algorithm (OnlineVPO), which continually refines video generation quality by incorporating feedback in real time. Moreover, Iterative Preference Optimization (IPO) extends the concept of integrating human feedback to enhance generated video quality \citep{yang2025ipoiterativepreferenceoptimization}. Specifically, IPO utilizes a critic model to evaluate and rank video outputs pairwise, similar to the approach used in DPO, thereby refining the generative process based on explicit preference signals.  

\subsection{Others}

In addition to RL and DPO, some literature uses GFlowNets~\citep{bengio2023gflownet}, also coined as the consistency model, to achieve direct reward backpropagation and alignment. Regarding the GFlowNet-based algorithm, the formulation of DM is directly connected to the definition of GFlowNet MDP, as highlighted in \citep{zhang2022unifying}. Importantly, the conditional distribution of the denoising process, $p_\theta(\mathbf{x}_{T-t-1} \, | \, \mathbf{x}_{T-t})$, aligns with the GFlowNet forward policy $P_F(s_{t+1} \, | \, s_t)$. Similarly, the conditional distribution of the diffusion process, $q(\mathbf{x}_{T-t} \, | \, \mathbf{x}_{T-t-1})$, corresponds to the GFlowNet backward policy $P_B(s_t \, | \, s_{t+1})$ \citep{lahlou2023theory}. Building on the above works, 
\citet{zhang2024improving} proposes the
DAG to post-train DMs with
black-box property functions. Further, they propose a KL-based objective for optimizing GFlowNets to offer improved sample efficiency. Moreover, RLCM proposes a framework for fine-tuning consistency models via RL, framing the iterative inference process as an MDP~\citep{oertell2024rl}. This approach speeds up training and inference while maintaining the efficiency of consistency models, enhancing performance across metrics. However, as a caveat, this property might also pose challenges in effectively generating high-reward samples beyond the training data. This implies that these approaches may not be suitable when accurate reward feedback is readily available without learning. Hence, we generally recommend using them when reward functions are unknown. Innovative self-play methods like SPIN-Diffusion~\citep{yuan2024self} remove the need for human preference data, achieving superior performance in text-to-image tasks. By engaging the model in a self-improvement process, where the diffusion
model engages in competition with its earlier versions, facilitating an iterative self-improvement
process. SPIN-Diffusion ensures efficient fine-tuning that leads to improved alignment with target distributions.

