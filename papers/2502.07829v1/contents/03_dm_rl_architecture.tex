\section{Modalities on T2I models}
\label{sec:modalities_on_t2i_models}

As shown in Figure~\ref{fig:total_framework}, the T2I models have several different modalities to update. Therefore, we categorize the reviewed works into three distinct groups for structured analysis based on their modalities: (1) textual modality, (2) image modality, and (3) multi-modalities. Table \ref{tab:method_categories} provides a multi-dimensional categorization of the surveyed papers, offering a concise and systematic reference for further exploration.

\xiaowei{it would probably make sense to explain what FT optimization is (as it is in Table 1) and how RL is related to FT optimization. Based on this discussion, you can summarize/discuss why you consider modalities and how the techniques may be different in different modalities. }

\subsection{Textual modality}

In recent years, the T2I model has attracted considerable attention on account of its remarkable ability to generate high-quality images and its versatility in various image editing applications. However, despite these advancements, the T2I model faces notable challenges in aligning with human preferences for image processing tasks. These challenges stem from the high demands, the complexity of multi-dimensional requirements, and the lack of a standardized evaluation framework, all of which contribute to its suboptimal performance in this critical domain. Recent studies have increasingly focused on optimizing the textual modality, particularly prompt models, to enhance their ability to generate high-quality prompts. These optimized prompts align more closely with user preferences and have demonstrated significant potential to improve downstream tasks such as image generation and image editing.

The first generative models were aligned using CLIP scores \citep{radford2021learningtransferablevisualmodels}. From these, a variety of score-based human preference alignment methods were developed, such as the HPS ~\citep{wu2023human}, which aligns text-to-image models with human preferences by leveraging a classifier trained on a large-scale dataset of human choices. ImageReward ~\citep{xu2024imagereward}, a human preference reward model for text-to-image synthesis, trained on 137,000 expert comparisons to accurately capture human preferences, optimizes diffusion models using ImageReward feedback and ReFL in improving text-to-image generation.  Meanwhile, PAE~\citep{mo2024dynamic} leverages dynamic fine-control prompts in a two-stage training process combining supervised fine-tuning and online reinforcement learning. This approach enables the iterative refinement of prompts and dynamic adjustment of modifier weights and injection time steps. Similarly, PROMPTIST~\citep{hao2024optimizing} integrates relevance and aesthetic scores into a reward function to optimize prompts, ensuring both improved image quality and alignment with user intent. Together, these advancements highlight the growing importance of human-centered optimization strategies in text-to-image generation. Further exploration into specific aspects of text-to-image generation has introduced methodologies like NegOpt~\citep{ogezi2024optimizing}, which optimizes negative prompts to improve image aesthetics and fidelity, and POAC~\citep{fan2024prompt}, incorporates a Prompt Language Model that has been fine-tuned on a curated dataset which employs reinforcement learning to enhance the generation of abstract concepts. 
TextCraftor~\citep{li2024textcraftor} focuses on the text encoder, leveraging reward functions to optimize performance without paired datasets, and VersaT2I~\citep{guo2024versat2i} adopts a self-training approach with Low-Rank Adaptation (LoRA) to address multiple image quality aspects, introducing a mixture of LoRA models for improved quality consistency. Additionally, novel frameworks like PRDP ~\citep{deng2024prdp} reformulate RLHF as a supervised regression task, enabling stable fine-tuning of diffusion models on large prompt datasets. It predicts reward differences between generated image pairs based on their denoising trajectories, enabling superior generation quality on complex and unseen prompts. 
% And, TexForce ~\citep{chen2025enhancing}  exclusively fine-tunes text encoders with the objective of optimizing text-image alignment and visual quality through task-specific rewards, ensuring seamless integration with pre-trained U-Net models.  
The Parrot framework \citep{lee2025parrot} employs a multi-reward optimization algorithm using reinforcement learning to balance aesthetics, alignment, sentiment, and human preferences through Pareto-optimal selection, achieving superior image quality.  Together, these approaches highlight the growing trend of leveraging advanced optimization and feedback mechanisms to enhance the capabilities of T2I diffusion models.

The integration of LLMs and VLMs as carriers or alternative representations of reward functions has introduced novel strategies for optimizing diffusion models. OPT2I~\citep{manas2024improving} exemplifies this by employing automatic prompt optimization, where an LLM iteratively refines prompts to maximize a consistency score while preserving image quality and improving recall between generated and real data. Similarly, the RPG framework~\citep{yang2024mastering} advances text-to-image generation by using multimodal LLMs to decompose complex prompts into detailed subprompts, allocate these across image regions via chain-of-thought planning, and apply complementary regional diffusion for precise image synthesis.
In addition to bridging LLM with T2I synthesis, DiffChat~\citep{wang2024diffchat} introduces an interactive framework for image creation. Leveraging the InstructPE dataset and RL to incorporate feedback on aesthetics, user preferences, and content integrity, DiffChat efficiently generates high-quality target prompts. These approaches underscore the growing importance of LLMs and VLMs in enhancing prompt optimization and aligning text-to-image models with user-driven goals.

\subsection{Image Modality }

Advancements in T2I generation have been significantly driven by methods that optimize model performance through various feedback mechanisms, focusing on improving image quality, text-image alignment, and even specific attributes like identity preservation and aesthetic appeal. 
One such method, HF-T2I ~\citep{lee2023aligning} proposes a fine-tuning framework for text-to-image generation that incorporates human feedback to improve image-text alignment. By collecting binary feedback, training a reward function based on CLIP embeddings and prompt classification, and optimizing the model through reward-weighted likelihood maximization.
A reinforcement learning-based framework, DPOK \citep{fan2024reinforcement}, further enhances T2I models by optimizing the expected reward of generated images while maintaining high image quality. Through KL regularization and policy optimization, DPOK stabilizes the fine-tuning process, showing superior performance in aligning generated images with complex prompts when compared to traditional supervised fine-tuning methods. Building upon the idea of enhancing alignment, the ID-Aligner~\citep{chen2024id} framework focuses on identity preservation within T2I generation, integrating identity consistency and aesthetic rewards into the fine-tuning process with improving the preservation of identity traits and enhancing visual appeal. 
The integration of visual rewards into the T2I generation process is also exemplified by Diffusion-MVP~\citep{he2023learning}, which is focused on generating NFT images from user-input texts. By incorporating three key visual rewards—Visual Market, Visual Aesthetic, and CLIP cross-modal relevance—the method optimizes NFT generation through fine-tuning a Stable Diffusion model within a PPO framework. 
The importance of aligning text-to-image models with human expectations is further underscored by frameworks like SeeTRUE~\citep{yarom2024you}, by utilizing methodologies such as VQ2 and VNLI for alignment assessment which introduces a benchmark for evaluating semantic alignment between text and images. Moreover, datasets like RichHF-18K~\citep{liang2024rich} play a crucial role in refining T2I models, providing detailed human feedback with annotations for implausible regions and misaligned keywords. The introduction of RAHF, a multimodal transformer model, allows for the automatic prediction of these annotations, which are then used for training data refinement and region inpainting, leading to substantial improvements in image generation quality.
To further optimize text-to-image models, RLCM~\citep{oertell2024rl} introduces a reinforcement learning framework for fine-tuning consistency models by framing the inference process as a Markov Decision Process (MDP). This method accelerates training and inference times while maintaining the efficiency advantages of consistency models, thus enhancing performance on various evaluation metrics. Innovative self-play methods like SPIN-Diffusion~\citep{yuan2024self} remove the need for human preference data, achieving superior performance in text-to-image tasks. By engaging the model in a self-improvement process, SPIN-Diffusion ensures efficient fine-tuning that leads to improved alignment with target distributions.
Finally, Deep Reward Tuning (DRTune)~\citep{wu2025deep} optimizes reward functions in diffusion models by controlling early denoising steps. This approach addresses the depth-efficiency trade-off and enables faster convergence while maintaining effective optimization, further improving text-to-image generation quality.

To further improve T2I generation, the integration of LLMs or LVLMs as reward functions has proven to be a novel and effective strategy for optimizing diffusion models. LVLM-REFL \citep{wen2023improving} enhances compositional T2I generation by using LVLMs to evaluate alignment between generated images and input texts. The framework considers key factors such as object count, attribute binding, spatial relationships, and aesthetic quality, leveraging these assessments to fine-tune diffusion models during training. Additionally, the method includes an iterative correction process during inference, using image-editing algorithms guided by the LVLM to address misalignments and refine the final output. Similarly, LRLD~\citep{zhang2025large} introduces a scalable reinforcement learning framework that focuses on improving text-to-image alignment with human preferences, fairness, and object composition. By employing multi-task fine-tuning across millions of prompts, LRLD addresses common alignment challenges in RLHF, ensuring that generated images better reflect human expectations and specific task-related objectives. In parallel, DreamSync~\citep{sun2023dreamsync} takes a model-agnostic approach, enhancing both the faithfulness and aesthetic quality of T2I models by leveraging feedback from VLMs. Unlike traditional reinforcement learning methods, DreamSync generates multiple candidate images for a given text prompt and uses visual question answering and aesthetic models to evaluate them. LLMs and LVLMs have been demonstrated to function with minimal human intervention, in conjunction with scalable optimization strategies.

On the other hand, recent efforts in fine-tuning diffusion models have expanded their applicability to domains such as image editing. InstructRL4Pix \citep{li2024instructrl4pix} introduces a reinforcement learning framework for image editing, utilizing attention maps and Proximal Policy Optimization (PPO) to enhance feature localization and preservation. Incorporating human preferences into image editing workflows, HIVE \citep{zhang2024hive} leverages human feedback to train a reward function that fine-tunes diffusion models for better alignment with user instructions. Addressing the challenge of generative diversity, DRLDiv \citep{miao2024training} employs reinforcement learning by introducing a diversity reward function that quantifies the overlap between generative and reference distributions. 

\subsection{Multi-Modalities Method}

As for the multi-modalities method, LfVoid~\citep{gao2023can} is a prime example of this integrated approach, where pre-trained T2I generative models establish visual objectives for robotic learning tasks. LfVoid generates visual objectives that guide the RL agents by utilizing natural language instructions to edit images, effectively bridging the gap between visual task specifications and autonomous agents. This methodology enables robot training without requiring in-domain data, demonstrating its superior performance in both simulated and real-world tasks, and highlighting the vast potential of large generative models in enhancing robotics applications. DDPO~\citep{black2023training} further exemplifies the fusion of RL with T2I models. The DDPO approach is an RL-based method that optimizes diffusion models for complex downstream goals, such as image quality and prompt alignment. It frames the denoising process as a multi-stage decision task and uses policy gradient algorithms to optimize models based on specific reward functions directly. This method significantly outperforms traditional reward-weighted likelihood approaches, enabling the achievement of challenging goals that are difficult to specify through typical text prompts, such as image compressibility and enhancing aesthetic quality.

