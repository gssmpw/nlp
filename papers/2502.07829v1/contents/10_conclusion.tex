\section{Conclusion}

This survey provides a comprehensive review of preference alignment methods for DMs, focusing on integrating RL-related techniques like RLHF and DPO to enhance alignment with human preferences. We categorize approaches into sampling, training strategies, data collection, and modalities, as well as highlight their applications in medical imaging, autonomous driving and robotics. Despite the temporary success, challenges such as computational efficiency and preference modelling still remain. Future research should address these issues, and continue to explore scalable algorithms, multimodal capabilities, and ethical considerations to expand the full potential of generative AI.