\section{Applications}
\label{sec:applications}

% \begin{table*}[ht]
% \centering
% \resizebox{0.9\linewidth}{!}
% {\input{tables/02_application_categories}}
% \caption{Overview of applications categorized by research objectives. \color{blue}{indicate details of condition, e.g. preference including instructions, preference data pairs...}}
% \label{tab:applications}
% \end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/paradigm_application.pdf}
    \caption{General paradigm of preference alignment for various \textbf{condition modules} on DM applications.}
    \label{fig:application_paradigm}
\end{figure*}
This section underscores the critical role of preference alignment in diffusion models across various domains, including medical imaging, robotics, and autonomous driving. As shown in Figure~\ref{fig:application_paradigm}, most applications rely on fine-tuning modules integrated into the denoising process, ensuring optimized generation conditioned on the specific requirements of each domain.
\subsection{Medical Imaging}
%The application of condition-controlled T2I DMs in medical imaging is remarkable \citep{shi2024diffusion}. These models excel in generating high-quality, high-resolution images and are used primarily in key areas such as medical image segmentation, synthesis, and reconstruction. 

% Segmentation
\textbf{Segmentation} Medical image segmentation partitions images into distinct regions of interest, playing a significant role in diagnosis and image-guided surgery. However, lesions and organs often exhibit ambiguity, making them difficult to be identified from the background. To this end, \citet{wu2022medsegdiff} proposed MedSegDiff to enhance the step-wise regional attention in DDPMs. Their approach optimizes the lesion’s feature map during the feature extraction phase in the diffusion encoder to enhance its visibility. Building on this work, \citet{wu2024medsegdiff} proposed MedSegDiff-V2, integrating diffusion processes and transformers with dual conditioning, anchor and semantic, guided by SS-Former to improve medical image segmentation. Furthermore, \citet{rahman2023ambiguous} utilized the stochastic sampling mechanism to generate a distribution of segmentation masks with cost-effective additional training.

% Synthesis
\noindent\textbf{Synthesis \& Reconstruction} The scarcity of public medical imaging datasets, combined with strict privacy regulations, underscores the urgent need for advanced image synthesis techniques. \citet{daum2024differentially} presented latent DMs designed to generate synthetic images conditioned on  medical attributes while protecting patient privacy through differentially private training. These models are pre-trained on public datasets and then fine-tuned with differential privacy using the UK Biobank dataset. Subsequently, \citet{han2024advancing} fused RLHF strategies and proposed CXRL to optimize the generation result.
% Reconstruction
Moreover, preference fine-tuned DMs are widely used to reconstruct high-quality medical imaging data by integrating feature engineering into the denoising process~\citep{xie2022measurement}, effectively addressing issues such as missing data, noise interference, and low resolution.

\subsection{Robotics}
%Preference fine-tuning on T2I DMs has revolutionized robotics, driving significant advancements in trajectory planning, motion generation, and policy learning.

%Motion generation%
\textbf{Motion Generation} DMs have enhanced robustness and adaptability in motion generation by unifying offline learning with preference alignment and physics-informed control. Offline diffusion learning, combined with online preference alignment, improves quadrupedal locomotion by ensuring stability, precision, and zero-shot generalization to real-world robots~\citep{yuan2024preferencealigneddiffusionplanner}. These advancements showcase the power of DMs to capture complex motion dynamics in robotic motion learning. By generating diverse and realistic trajectories, these models enable robots to perform robust and adaptive behaviors, paving the way for more efficient and capable robotic systems.

%Trajectory Planning%
\noindent\textbf{Trajectory Planning} DMs have significantly advanced trajectory planning in robotics by facilitating preference alignment and decision-making across complex, multi-objective scenarios. \citet{janner2022planningdiffusionflexiblebehavior} embedded the trajectory optimization process into the denoising process, enabling long-horizon planning while addressing alignment inconsistencies between conditions and generated trajectories. Preference-conditioned DMs further enhance multi-task decision-making by maximizing mutual information to improve trajectory alignment~\citep{yu2024regularized}. 
% Additionally, incorporating prior knowledge as a condition into the training process of DMs has also become an effective approach~\citep{carvalho2023motion}. 
% In all, above approaches highlight the potential of DM optimization methods in robotics, driving future innovation in dynamic and collaborative environments.

%Policy Learning
\noindent\textbf{Policy Learning \& Others} DMs are revolutionizing policy learning in robotics by tackling challenges like multi-task learning~\citep{wang2024sparse}, data scarcity~\citep{liu2024rdt}, and aligning policies with human preferences~\citep{dong2023aligndiff}. Furthermore, preference fine-tuned DMs demonstrate remarkable potential not only in advancing the design and control of soft robots~\citep{wang2023diffusebot}, but also in the training of sophisticated world simulators~\citep{yang2023learning}. These simulators leverage DMs to generate high-fidelity, diverse, and contextually accurate virtual environments, providing a robust framework for training and testing autonomous systems in a wide range of scenarios.
% \citet{wang2024sparse} develop the Sparse Diffusion Policy (SDP), integrating a Mixture of Experts (MoE) within a transformer-based diffusion framework. This design enables SDP to selectively activate experts tailored to specific tasks, achieving efficient learning while mitigating catastrophic forgetting, making it highly effective for multitask and continual learning. Building on the need for dexterous manipulation, \cite{liu2024rdt} introduce the Robotics Diffusion Transformer (RDT), a scalable DM adept at handling multi-modal action distributions. By leveraging a Physically Interpretable Unified Action Space, RDT facilitates seamless knowledge transfer and exhibits zero-shot generalization and advanced manipulation capabilities after extensive pre-training and fine-tuning. To address the scarcity of labeled robotic data, ~\cite{he2024large} propose a method combining generative pre-training on actionless human videos with fine-tuning on limited action-labeled robot data. Their framework uses a discrete DM to generate high-fidelity video tokens, enhancing low-level action learning and achieving superior generalization. Further advancing preference alignment, ~\cite{dong2023aligndiff} presente AlignDiff, which incorporates RLHF to quantify and integrate human preferences into diffusion-based policy planning. This framework excels in customizing behaviors, aligning with user-defined goals, and generalizing across diverse tasks. Together, these methods underscore the potential of diffusion-based models to advance policy learning by offering flexible, robust, and context-aware solutions tailored to the growing complexity of robotics.
% In addition, \citet{yang2023learning} introduces a world simulator capable of learning from heterogeneous real-world datasets, demonstrating its utility in robotics by facilitating controllable content creation and planning tasks. Moreover, \citet{wang2023diffusebot} develops DiffuseBot, which leverages DMs to design soft-material robots, showcasing the adaptability of these models for novel hardware configurations.

\subsection{Autonomous Driving}
% data generation %
\noindent\textbf{Data Generation} Autonomous driving models require a large amount of high-quality and complex training data. However, real data are costly to collect with restricted coverage, so synthesizing data through generative techniques becomes an effective alternative. To improve data quality, specifically spatio-temporal consistency and resolution, RLHF, SFT, and DPO have all been shown to be remarkably effective methods. Several methods have been proved to generate training data for multi-view videos~\citep{li2023drivingdiffusion}, panoramic videos~\citep{wen2024panacea}, etc. 

% In order to improve the data quality (spatio-temporal consistency and resolution), RLHF~\citep{cao2024reinforcement}, SFT~\citep{tu2024driveditfitfinetuningdiffusiontransformers} and DPO~\citep{blattmann2023align} have been shown to be remarkably effective methods.
% During the training phase for autonomous driving, various methods are employed to generate high-quality training data. For instance, DrivingDiffusion~\citep{li2023drivingdiffusion} generates spatially and temporally consistent multi-view videos from synthetic 3D layouts by leveraging cross-view attention, temporal models, and post-processing techniques like sliding windows, addressing key challenges in multi-view video generation. Similarly, Panacea~\citep{wen2024panacea} leverages latent DMs (conditioned on Bird's Eye View and prompt guidance) to create panoramic and controllable driving scene videos. The generated videos offer unparalleled flexibility in controlling global attributes such as weather and lighting, as well as fine details like object depth and road geometry, enabling precise simulation of edge cases.  Furthermore, DriveDiTFit~\citep{tu2024driveditfitfinetuningdiffusiontransformers} utilizes a gap-driven modulation approach to selectively fine-tune pre-trained Diffusion Transformers, integrating specialized embedding modules and progressive tuning strategies to efficiently produce diverse and high-quality autonomous driving datasets. Besides, \citet{cao2024reinforcement} proposes TrafficRLHF, which is a traffic modelling framework that utilized RLHF to significantly improve the efficiency and quality of data generation while maintaining the realism of traffic situations. In order to improve the generation efficiency, \cite{blattmann2023align} presents VideoLDM for training high-resolution and long-term consistent video generation models based on LDMs, further temporally fine-tune LDMs.

%decision making%
\noindent\textbf{Decision Making} Traditional auto-driving decision-making processes rely on predicting the future behavior of traffic participants, such as vehicles and pedestrians, and using deterministic or rule-based methods for path planning. However, these approaches are limited by predicting only a few possible scenarios, failing to account for the uncertainty in real-world traffics. To mitigate these issues, several methods are proposed to optimize the decision-making process by RLHF~\citep{huang2024gen}; to accelerate the decision-making process by shortening denoising process~\citep{liao2024diffusiondrive}; and to increase the security of the decision-making process by merging RLHF and SFT~\citep{liu2024ddm}.

% \citet{huang2024gen} proposes the Gen-Drive framework, which adopts a “Generation-then-Evaluation” paradigm. In this framework, a DM generates multiple feasible future scenarios, and an evaluation model selects the optimal decision among them. Specifically, they introduced a scene evaluator to assess the importance of the generated scenarios, which is then integrated into an RL framework that guides the parameter updates of the DM through reward signals. Auto-driving involves high-risk decision-making, and DMs require a multi-step denoising process during generation; excessive denoising steps can significantly reduce inference speed. To mitigate this, \citet{liao2024diffusiondrive} proposes the Truncated Diffusion Policy and Prior Multi-mode Anchors, which shorten the denoising process. Additionally, to maintain or enhance accuracy, they designed a Cascade Diffusion Decoder that improves generation quality by interacting with environmental information on a layer-by-layer basis. Furthermore, \citet{liu2024ddm} employes a reinforcement learning method on DMs by introducing the Diffusion Decision Model with Lagrangian-based safety, which treats the continuous decision-making process as a DMing task within a generative model. Specifically, the authors introduced a hybrid strategy update approach with an Actor-Critic architecture to achieve efficient updating of the DM.

% world model %
\noindent\textbf{World Model} Similar to robotics, world models hold great promise in autonomous driving, where accurately predicting diverse movements is essential for making safe and effective driving decisions~\citep{feng2025survey}. Recently, generalized diffusion-based world models have attracted considerable attention~\citep{chen2024drivinggpt}.
% Leveraging these models, some approaches integrate the world model with RL to improve decision-making~\citep{garg2024imagine}, while others incorporate digital twins to enhance data quality~\citep{xu2023generative}.
% \citet{gao2024vistageneralizabledrivingworld} propose VISTA, an excellent world model has high generalization ability, high prediction accuracy and diverse action controllability. It does also leverage Vista's own ability to build a generalizable reward mechanism for evaluating real-world actions without having to access real action data. Subsequently, \citet{garg2024imagine} present the Imagin-2-drive framework, which utilized VISTA to accurately predict future state, and designed a Diffusion Policy Actor (DPA) that can generate multiple reasonable driving behaviors. DPA is trained by DDPO to maximize the cumulative rewards on the trajectory. With this optimization approach, DPA can generate driving behaviors that are more in line with expectations and improve overall decision quality. Interestingly, \citet{xu2023generative} claim world model could be used in Digital Twin system (which is a kind of mixed reality). Through multi-dimensional communication and data interaction, it can overcome the differences between real and virtual entities and enable the car to operate in a synergistic manner in both real and virtual environments, thus enhancing the efficiency and safety of autonomous driving decisions. It is able to do unconstrained “conditionalized” traffic and driving data generation in a virtual simulator using DMs.

\subsection{Others}

The above discussion demonstrates that preference alignment on DMs exhibits remarkable potential in domains with a critical demand for large-scale training data. Beyond the above domains, in the field of biology, DMs can not only predict protein structures~\citep{wu2024protein} but also optimize their stability and functionality, expediting experimental workflows. In game simulation~\citep{menapace2024promptable}, these models generate realistic virtual worlds, simulate complex agent behaviors, and streamline gameplay mechanics, ultimately enhancing user immersion and engagement. Furthermore, in the content creation sphere, DMs facilitate the production of high-quality, preference-aligned outputs—spanning digital art, music, and other creative mediums—satisfying diverse aesthetic and stylistic requirements~\citep{lin2023magic3d}. Taken together, these applications illustrate how preference alignment on DMs can adapt across multidisciplinary domains, opening up new avenues for innovation and collaboration.
%like Biology (Protein Prediction, translation efficiency in RNA, docking score in molecules, stability in protein), Game simulation, Content art creation.



