\section{Introduction}
\par \noindent Advancements in Generative AI have greatly enhanced image generation and editing with diffusion models (DMs) ~\citep{ramesh2022hierarchical,saharia2022photorealistic}, addressing key challenges across applications such as media enhancement~\citep{zhou2024surveygenerativeaillm}, driving simulation~\citep{guan2024world}, and virtual reality~\citep{10765093}. This progress broadens visual media capabilities and optimizes the production and integration of digital content on diverse platforms. Despite these advances, images generated or edited with DMs often encounter issues such as text-image misalignment, deviations from human aesthetic preferences, and propagation of content that may include toxic or biased content. %Despite significant advancements, generated images with DMs often face text-image misalignment, deviations from human aesthetics, and content that harbors toxicity and bias.

To address these challenges, scholars have employed reinforcement learning (RL) strategies, notably using reward models (RM) to infer human preferences from expert-annotated output. A key instance is the Large Language Model (LLM), which applies reinforcement learning with human feedback (RLHF) to steer the intention towards human values and preferences \citep{achiam2023gpt}. In image generation and editing, many initiatives adopt various preference alignment strategies to produce images that better align with textual descriptions, aesthetics, and human values. However, the various forms of integration between DMs and preference alignment strategies pose challenges for novices in understanding the intersection of these domains. Therefore, this paper addresses the gap by systematically reviewing how DMs and RL integrate for image generation and editing, offering a concise overview and anticipating future developments. %\xiaowei{need to motivate preference fine-tuning, and why you consider RL in this context... Also, RL may want to be part of the title? }
%\Definition, impact, general problem and novelty
%\par \noindent \textbf{Existing challenges:}

Although there are numerous surveys on diffusion-based image generation~\citep{DBLP:journals/corr/abs-2303-07909} and image editing~\citep{DBLP:journals/corr/abs-2402-17525}, few have provided a systematic and comprehensive review that thoroughly integrates DMs with RL in the field of image generation and editing. Several studies~\citep{DBLP:journals/corr/abs-2407-13734,du2023beyond,DBLP:journals/corr/abs-2308-14328} have reviewed the integration of text-to-image models with RL; however, they tend to primarily concentrate on the applications of RL to diffusion processes or offer overarching assessments of generative AI applications. They do not systematically explore the specific integration of DMs and RL in image generation and editing, nor adequately summarize the prevailing challenges and prospective direction in this field. To address this gap, this paper provides a comprehensive review of preference alignment-based DMs, examining their theoretical foundations, developmental progress, and practical implementations in the realms of image generation and editing. Building on these insights, it identifies key challenges and outlines future research directions, offering a forward-looking perspective on potential advancements in this rapidly evolving field.

Our contributions include: i) systematically reviewing optimization techniques such as RLHF, DPO, and others, highlighting their role in the preference alignment with DMs (Sec.~\ref{sec:preference_fine_tuning_on_t2i}); ii) thoroughly exploring applications of DMs in autonomous driving, medical imaging, robotics, and more (Sec.~\ref{sec:applications}); iii) comprehensively discussing challenges of preference alignment with DMs (Sec.~\ref{sec:challenges_and_future_directions}). To our knowledge, this paper is the first survey specifically focusing on preference alignment with DMs in image generation and editing, aiming to help researchers deepen their understanding and drive further innovation.%This survey presents recent methods on preference alignment for DMs, helping researchers and practitioners deepen their understanding and drive further innovation.