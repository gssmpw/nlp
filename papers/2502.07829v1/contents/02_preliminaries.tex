\section{Preliminaries}

% This section outlines the preliminaries of preference alignment on the DM for image generation and editing, including the foundational concepts of the DM and RL. 

\subsection{Diffusion Model}
\label{sec:pre_diffusion_model}

DDPMs employ a forward diffusion process that iteratively adds Gaussian noise to the data \( x_0 \) over \( T \) time steps. This process forms a Markov chain, where the data at each time step \( x_t \) depend only on the data from the immediately preceding step \( x_{t-1} \)~\citep{ho2020denoising}. The forward process is governed by the following equation:
\begin{equation}
\begin{aligned}
q(x_{t}\mid x_{t-1})=\mathcal N(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t} I),\nonumber
\end{aligned}
\end{equation}
with noise schedules ($\beta_{1}, ..., \beta_{T} $)  determining how quickly the original data degrades. The distribution at any step  $t$ can also be computed directly as following form.
\begin{equation}
\begin{aligned}
q(x_{t}\mid x_{0}) = \mathcal N (x_{t};\sqrt{ \bar{\alpha_{t}}}x_{0},(1- \bar{\alpha_{t}})I),\nonumber
\end{aligned}
\end{equation}
During the denoising phase, the reverse process  starts from pure noise at time step  $T$, uses a learnable model to iteratively denoise and reconstruct the original signal $x_{0}$, which is formulated as below:
\begin{equation}
\begin{aligned}
p_{\theta } (x_{t-1} \mid x_{t}) = \mathcal N (x_{t-1}; \mu_{\theta }  (x_{t}, t),  {\textstyle \sum_{\theta}} (x_{t},t)),\nonumber
\end{aligned}
\end{equation}
The model is trained by minimizing a variational lower bound on the negative log-likelihood of the data.
{\small
\begin{equation}
\begin{aligned}
\mathbb {E}[-\log p_{\theta }(x_{0})] \le  \mathbb{E}_{q}\left[ -\log p(x_{T})-\sum_{ t>1}\log \frac{p_{\theta}(x_{t-1}\mid x_{t})}{q(x_{t}\mid x_{t-1})}   \right]. \nonumber
\end{aligned}
\end{equation}
}

DDIMs is a more efficient and deterministic alternative, replacing the Markov chain with a non-Markovian process~\citep{song2020denoising}. This approach enables faster generation, accurate reconstruction, and supports applications such as image and video editing. The backward denoising step can be computed as follows:
{\small
\begin{equation}
\label{eq:back_denoise}
\begin{aligned}
x_{t-1} =\sqrt{ \bar {\alpha}_{t-1}}\frac{x_{t}-\sqrt{1-\alpha_{t} }\epsilon _{\theta }(x_{t},t)  }{\sqrt{\alpha_{t}}} + \sqrt{1-\alpha_{t-1} }\epsilon _{\theta }(x_{t},t). \nonumber
\end{aligned}
\end{equation}
}

\subsection{Reinforcement Learning}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/01_training_stages.png}
%     \caption{Training Stages}
%     \label{fig:traning_stages}
% \end{figure}
% \textcolor{red}{[DJ: I have one question that the theme of our survey is about RL/DPO for image processing, so why shall we mention below the use of RL for text generations? This could be incoherent across the article.]}
The preference fine-tuning pipeline typically begins with a supervised fine-tuning (SFT) stage, where the diffusion model is trained to accurately map textual prompts to images. During SFT, the focus is on improving the modelâ€™s ability to produce high-quality images that closely reflect the prompt. Once the model is sufficiently proficient at generating images, it is further refined via reinforcement learning (RL). Proximal Policy Optimization (PPO) \citep{schulman2017proximal} is commonly employed for this step, owing to its stability and robustness during training. The following section provides a detailed overview of PPO in this context.

% \subsubsection{REINFORCE}
% \cite{williams1992simple}

\noindent \textbf{PPO.} In each update timestep, the PPO algorithm collects a batch of transition samples using a rollout policy $\pi_{\theta_{\text{old}}}(a_t | s_t)$ and optimizes a clipped surrogate objective, as follows:
{\small
\begin{align}
L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) A_t, \, \text{clip}\left( r_t(\theta), 1 - \epsilon, 1 + \epsilon \right) A_t \right) \right], \nonumber
\end{align}
}

\noindent where $r_t(\theta)$ represents the probability ratio, defined as $r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}$. 
The term $A_t$ is the advantage in the timestep $t$, estimated using Generalized Advantage Estimation (GAE)~\citep{schulman2015high}, 
and $\epsilon$ is a hyperparameter that controls the width of the clipping interval. The value function is trained by minimizing the following loss function:
{\small
\begin{align}
L^V(\theta) = \hat{\mathbb{E}}_t \big[ &\frac{1}{2} \max \big( 
\left( V_\theta - V_t^{\text{targ}} \right)^2, \nonumber
\\
&\left( \text{clip}(V_\theta, V_{\text{old}} - \epsilon, V_{\text{old}} + \epsilon) - V_t^{\text{targ}} \right)^2 
\big) \big], \nonumber
\end{align}
}

\noindent where $V_t^{\text{targ}}$ denotes the bootstrapped value function target. Additionally, an entropy bonus is often incorporated to encourage sufficient exploration such that
\begin{equation}
\begin{aligned}
L^H(\theta) &= \hat{\mathbb{E}}_t \left[ H[\pi_\theta](s_t) \right], \nonumber
\end{aligned}
\end{equation}

\noindent where $H[\pi_\theta](s_t)$ is the entropy of a policy $\pi_\theta$ at state $s_t$. Incorporating the above loss functions, the overall optimization objective of PPO is defined as:
\begin{equation}
\begin{aligned}
L^{\text{PPO}}(\theta) &= -L^{\text{CLIP}}(\theta) + \lambda_V L^V(\theta) - \lambda_H L^H(\theta), \nonumber
\end{aligned}
\end{equation}

\noindent where $\lambda_V$ and $\lambda_H$ are weighting coefficients to balance the importance of the value function loss and the entropy bonus. The algorithm alternates between collecting trajectory data using the policy and optimizing the collected data based on this loss function, until convergence.



