\section{Related Work}
% \textbf{Visual Instruction Tuning: }
% Visual instruction tuning has become a cornerstone in the development of MLLMs, playing a pivotal role in bridging the gap between theoretical model capabilities and practical applications. This approach has evolved significantly over time. Initially, LLMs were primarily used to generate synthetic visual instructions. While these early efforts demonstrated promising performance in conversational tasks, they struggled to meet the demands of more rigorous academic benchmarks. To overcome this limitation, a hybrid approach emerged, combining synthetically generated instructions with established academic datasets, thus creating more comprehensive and diverse training data. This methodological advancement has been instrumental in enhancing MLLMs, such as LLaVA \citep{liu2024visual}, InstructBLIP \citep{dai2023instructblip}, and Cambrian \citep{tong2024cambrian}, allowing these models to better interpret and respond to visual-linguistic cues. The importance of visual instruction tuning goes beyond improving task-specific performance; it also contributes to better alignment between model outputs and user expectations, thereby enhancing the practical utility of these systems for real-world applications while ensuring robust performance across academic evaluations. 
\textbf{Visual Instruction Tuning:}  
Visual instruction tuning is essential for aligning MLLMs with both practical applications and academic benchmarks. Early methods relied on synthetic visual instructions, which performed well in conversations but struggled on rigorous benchmarks. A hybrid approach later emerged, integrating synthetic data with academic datasets to improve training diversity. This advancement has enhanced models like LLaVA \citep{liu2024visual}, InstructBLIP \citep{dai2023instructblip}, and Cambrian \citep{tong2024cambrian}, enabling better visual-linguistic understanding. Beyond task performance, visual instruction tuning improves model alignment with user expectations, ensuring both practical utility and strong academic performance.

\noindent
% \textbf{Visual Instruction Selection: }
% \noindent
% Although MLLMs have achieved remarkable performance across various tasks, the rapid expansion of visual instruction datasets has led to significant redundancy, mirroring challenges faced in LLMs \citep{zhou2024lima,chen2023maybe05dataneeded,xia2024lessselectinginfluentialdata}. State-of-the-art MLLMs, such as BLIP3 \citep{xue2024xgen}, InternVL2.5 \citep{chen2025expandingperformanceboundariesopensource}, LLaVA-OneVision \citep{li2024llavaonevisioneasyvisualtask}, leverage billions of visual instruction instances to enhance their understanding capabilities. However, the sheer scale of these datasets results in substantial computational costs, often requiring hundreds to thousands of GPU hours, particularly for models built on large LLM backbones.

% \noindent
% To mitigate this issue, various data selection strategies have been proposed to evaluate and reduce redundancy, identifying high-value instances while preserving model performance. TIVE \citep{liu2024morehighvaluedataselection} detects severe redundancy in multimodal datasets and selects valuable data at both the task and instance levels based on gradient similarity, but it requires additional training on downstream tasks. SELF-FILTER \citep{chen2024visionlanguagemodelstrongfilter} introduces an auxiliary evaluation model that updates its parameters alongside training to prioritize high-value samples. COINCIDE \citep{lee2024conceptskilltransferabilitybaseddataselection} clusters data based on conceptual and skill-based representations, while InstructionGPT-4 \citep{wei2023instructiongpt4200instructionparadigmfinetuning} filters a subset of 200 instructions for MiniGPT-4 \citep{zhu2023minigpt4enhancingvisionlanguageunderstanding}, though this approach lacks scalability across different settings. ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage} builds upon LESS \citep{xia2024lessselectinginfluentialdata} to incorporate targeted instruction tuning by leveraging gradient-based specialist influence estimation. DataTailor \citep{yu2024mastering} optimizes data selection based on three key principles—informativeness, uniqueness, and representativeness—ensuring that the most relevant samples are retained.
\noindent
\textbf{Visual Instruction Selection:}  
% Despite the strong performance of MLLMs, the rapid growth of visual instruction datasets has introduced significant redundancy, similar to challenges in LLMs \citep{zhou2024lima,chen2023maybe05dataneeded,xia2024lessselectinginfluentialdata}. State-of-the-art models like BLIP3 \citep{xue2024xgen}, InternVL2.5 \citep{chen2025expandingperformanceboundariesopensource}, and LLaVA-OneVision \citep{li2024llavaonevisioneasyvisualtask} rely on billions of instructions to enhance understanding, but their massive scale leads to substantial computational costs, often requiring hundreds to thousands of GPU hours.  
Despite the strong performance of MLLMs, the rapid growth of visual instruction datasets has introduced significant redundancy, similar to challenges in LLMs \citep{zhou2024lima,chen2023maybe05dataneeded,xia2024lessselectinginfluentialdata}. State-of-the-art models like BLIP3 \citep{xue2024xgen}, InternVL2.5 \citep{chen2025expandingperformanceboundariesopensource}, and LLaVA-OneVision \citep{li2024llavaonevisioneasyvisualtask} rely on billions of instructions to enhance understanding, but their massive scale leads to substantial computational costs, often requiring hundreds to thousands of GPU hours.

% \noindent
% To tackle this issue, various data selection strategies aim to reduce redundancy while preserving model performance. TIVE \citep{liu2024morehighvaluedataselection} selects valuable data at both the task and instance levels using gradient similarity but requires additional downstream training. SELF-FILTER \citep{chen2024visionlanguagemodelstrongfilter} introduces an auxiliary evaluation model that updates its parameters alongside training to prioritize high-value samples. COINCIDE \citep{lee2024conceptskilltransferabilitybaseddataselection} clusters data based on conceptual and skill-based representations, while InstructionGPT-4 \citep{wei2023instructiongpt4200instructionparadigmfinetuning} filters 200 instructions for MiniGPT-4 \citep{zhu2023minigpt4enhancingvisionlanguageunderstanding}, though it lacks scalability. ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage} builds upon LESS \citep{xia2024lessselectinginfluentialdata} to incorporate targeted instruction tuning by leveraging gradient-based specialist influence estimation. DataTailor \citep{yu2024mastering} selects data based on informativeness, uniqueness, and representativeness to retain the most relevant samples.
\noindent To address this, various data selection strategies aim to reduce redundancy while preserving performance. TIVE \citep{liu2024morehighvaluedataselection} selects valuable data based on gradient similarity but requires additional training on downstream tasks. SELF-FILTER \citep{chen2024visionlanguagemodelstrongfilter} uses an auxiliary evaluation model to prioritize high-value samples. COINCIDE \citep{lee2024conceptskilltransferabilitybaseddataselection} clusters data by conceptual and skill-based representations, while InstructionGPT-4 \citep{wei2023instructiongpt4200instructionparadigmfinetuning} filters 200 instructions for MiniGPT-4 \citep{zhu2023minigpt4enhancingvisionlanguageunderstanding}, though it lacks scalability. ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage} extends LESS \citep{xia2024lessselectinginfluentialdata} by incorporating specialist influence estimation for instruction tuning. DataTailor \citep{yu2024mastering} selects data based on informativeness, uniqueness, and representativeness to retain the most relevant samples.