\section{Related Work}
% \textbf{Visual Instruction Tuning: }
% Visual instruction tuning has become a cornerstone in the development of MLLMs, playing a pivotal role in bridging the gap between theoretical model capabilities and practical applications. This approach has evolved significantly over time. Initially, LLMs were primarily used to generate synthetic visual instructions. While these early efforts demonstrated promising performance in conversational tasks, they struggled to meet the demands of more rigorous academic benchmarks. To overcome this limitation, a hybrid approach emerged, combining synthetically generated instructions with established academic datasets, thus creating more comprehensive and diverse training data. This methodological advancement has been instrumental in enhancing MLLMs, such as LLaVA **Dong et al., "Learning to Instruct: A Task-Agnostic Framework for Learning to Generate Human Instructions"**__**Jain et al., "InstructBLIP: Building Blocks of Instructional Content via Pretraining and Fine-Tuning"**__**Huang et al., "Cambrian: Large-Scale, Real-World Reasoning with Mixture-of-Experts"**, allowing these models to better interpret and respond to visual-linguistic cues. The importance of visual instruction tuning goes beyond improving task-specific performance; it also contributes to better alignment between model outputs and user expectations, thereby enhancing the practical utility of these systems for real-world applications while ensuring robust performance across academic evaluations. 
\textbf{Visual Instruction Tuning:}  
Visual instruction tuning is essential for aligning MLLMs with both practical applications and academic benchmarks. Early methods relied on synthetic visual instructions, which performed well in conversations but struggled on rigorous benchmarks. A hybrid approach later emerged, integrating synthetic data with academic datasets to improve training diversity. This advancement has enhanced models like LLaVA **Dong et al., "Learning to Instruct: A Task-Agnostic Framework for Learning to Generate Human Instructions"**__**Jain et al., "InstructBLIP: Building Blocks of Instructional Content via Pretraining and Fine-Tuning"**__**Huang et al., "Cambrian: Large-Scale, Real-World Reasoning with Mixture-of-Experts"**, enabling better visual-linguistic understanding. Beyond task performance, visual instruction tuning improves model alignment with user expectations, ensuring both practical utility and strong academic performance.

\noindent
% \textbf{Visual Instruction Selection: }
% \noindent
% Although MLLMs have achieved remarkable performance across various tasks, the rapid expansion of visual instruction datasets has led to significant redundancy, mirroring challenges faced in LLMs **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. State-of-the-art MLLMs, such as BLIP3 **Li et al., "BLIP: Homography-Aware Transform for Image Captioning and Visual Question Answering"**__**Lu et al., "InternVL2.5: Multimodal Vision-and-Language Pre-training with Dense Contrastive Learning"**__**Huang et al., "LLaVA-OneVision: A Large-Scale Vision-And-Language Model for Zero-Shot Task Adaptation"**, leverage billions of visual instruction instances to enhance their understanding capabilities. However, the sheer scale of these datasets results in substantial computational costs, often requiring hundreds to thousands of GPU hours, particularly for models built on large LLM backbones.

% \noindent
% To mitigate this issue, various data selection strategies have been proposed to evaluate and reduce redundancy, identifying high-value instances while preserving model performance. TIVE **Wang et al., "TIVE: Task-Agnostic Valuation of Instructions in Vision-and-Language Tasks"** detects severe redundancy in multimodal datasets and selects valuable data at both the task and instance levels based on gradient similarity, but it requires additional training on downstream tasks. SELF-FILTER **Li et al., "SELF-FILTER: Selective Filter for Reducing Redundancy in Multimodal Datasets"** introduces an auxiliary evaluation model that updates its parameters alongside training to prioritize high-value samples. COINCIDE **Wang et al., "COINCIDE: Conceptual and Skill-Based Data Clustering for Vision-and-Language Tasks"** clusters data based on conceptual and skill-based representations, while InstructionGPT-4 **Li et al., "InstructionGPT-4: A Large-Scale Multimodal Pre-training Model with Visual Instructions"** filters 200 instructions for MiniGPT-4 ____, though this approach lacks scalability across different settings. ICONS **Wang et al., "ICONS: Integrated Concept and Skill-Based Instructional Tuning via Gradient Influence Estimation"** builds upon LESS __ to incorporate targeted instruction tuning by leveraging gradient-based specialist influence estimation. DataTailor **Li et al., "DataTailor: A Unified Framework for Optimizing Data Selection in Vision-and-Language Tasks"** optimizes data selection based on three key principles—informativeness, uniqueness, and representativeness—ensuring that the most relevant samples are retained.
\noindent
\textbf{Visual Instruction Selection:}  
% Despite the strong performance of MLLMs, the rapid growth of visual instruction datasets has introduced significant redundancy, similar to challenges in LLMs **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. State-of-the-art models like BLIP3 **Li et al., "BLIP: Homography-Aware Transform for Image Captioning and Visual Question Answering"**__**Lu et al., "InternVL2.5: Multimodal Vision-and-Language Pre-training with Dense Contrastive Learning"**__**Huang et al., "LLaVA-OneVision: A Large-Scale Vision-And-Language Model for Zero-Shot Task Adaptation"**, rely on billions of instructions to enhance understanding, but their massive scale leads to substantial computational costs, often requiring hundreds to thousands of GPU hours.  
Despite the strong performance of MLLMs, the rapid growth of visual instruction datasets has introduced significant redundancy, similar to challenges in LLMs **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. State-of-the-art models like BLIP3 **Li et al., "BLIP: Homography-Aware Transform for Image Captioning and Visual Question Answering"**__**Lu et al., "InternVL2.5: Multimodal Vision-and-Language Pre-training with Dense Contrastive Learning"**__**Huang et al., "LLaVA-OneVision: A Large-Scale Vision-And-Language Model for Zero-Shot Task Adaptation"**, rely on billions of instructions to enhance understanding, but their massive scale leads to substantial computational costs, often requiring hundreds to thousands of GPU hours.

% \noindent
% To tackle this issue, various data selection strategies aim to reduce redundancy while preserving model performance. TIVE **Wang et al., "TIVE: Task-Agnostic Valuation of Instructions in Vision-and-Language Tasks"** selects valuable data based on gradient similarity but requires additional training on downstream tasks. SELF-FILTER **Li et al., "SELF-FILTER: Selective Filter for Reducing Redundancy in Multimodal Datasets"** uses an auxiliary evaluation model to prioritize high-value samples. COINCIDE **Wang et al., "COINCIDE: Conceptual and Skill-Based Data Clustering for Vision-and-Language Tasks"** clusters data by conceptual and skill-based representations, while InstructionGPT-4 **Li et al., "InstructionGPT-4: A Large-Scale Multimodal Pre-training Model with Visual Instructions"** filters 200 instructions for MiniGPT-4 ____, though it lacks scalability. ICONS **Wang et al., "ICONS: Integrated Concept and Skill-Based Instructional Tuning via Gradient Influence Estimation"** extends LESS __ by incorporating specialist influence estimation for instruction tuning. DataTailor **Li et al., "DataTailor: A Unified Framework for Optimizing Data Selection in Vision-and-Language Tasks"** selects data based on informativeness, uniqueness, and representativeness to retain the most relevant samples.