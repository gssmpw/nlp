
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{balance}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{array}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{amsmath, amssymb, algorithm, algorithmic, graphicx}
\title{\includegraphics[height=1.3em]{figs/Prismatic_Star.png}PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Author 1 \and ... \and Author n \\
        Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
% \author {
% Jinhe Bi\textsuperscript{\rm 1,}\textsuperscript{\rm 2} \quad
% Yifan Wang\textsuperscript{\rm 1} \thanks{These authors contributed equally to this work.} \quad
% Danqi Yan\textsuperscript{\rm 1} \thanks{These authors contributed equally to this work.} \quad
% Xun Xiao\textsuperscript{\rm 2} \thanks{Corresponding authors: \textit{yunpu.ma@ifi.lmu.de}, \textit{drxiaoxun@gmail.com}, \textit{bijinhe@outlook.com}} \quad
% Artur Hecker\textsuperscript{\rm 2} \quad \\
% \textbf{Volker Tresp\textsuperscript{\rm 1,}\textsuperscript{\rm 3} \quad 
% Yunpu Ma\textsuperscript{\rm 1,}\textsuperscript{\rm 3} \thanks{Corresponding authors: \textit{yunpu.ma@ifi.lmu.de}, \textit{drxiaoxun@gmail.com}, \textit{bijinhe@outlook.com}}} \\
% \textsuperscript{\rm 1} Ludwig Maximilian University of Munich \quad 
% \textsuperscript{\rm 2} Munich Research Center, Huawei Technologies \\
% \textsuperscript{\rm 3} Munich Center for Machine Learning
% }

\author {Jinhe Bi\textsuperscript{\rm 1,}\textsuperscript{\rm 2} \thanks{Email: \textit{bijinhe@outlook.com}}\quad
Yifan Wang\textsuperscript{\rm 1}\thanks{These authors contributed equally to this work.} \quad
Danqi Yan\textsuperscript{\rm 1}\footnotemark[2]  \quad
Xun Xiao\textsuperscript{\rm 2}\thanks{Corresponding authors: \textit{yunpu.ma@ifi.lmu.de}, \textit{drxiaoxun@gmail.com}} \quad  
Artur Hecker\textsuperscript{\rm 2} \quad \\
\textbf{Volker Tresp\textsuperscript{\rm 1,}\textsuperscript{\rm 3} \quad 
Yunpu Ma\textsuperscript{\rm 1,}\textsuperscript{\rm 3}\footnotemark[3]} \\
\textsuperscript{\rm 1} Ludwig Maximilian University of Munich \quad \textsuperscript{\rm 2} Munich Research Center, Huawei Technologies \\
\textsuperscript{\rm 3} Munich Center for Machine Learning
}

% \author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy, leading to excessive computational costs. Existing data selection methods predominantly rely on proxy models or loss-based metrics, both of which impose substantial computational overheads due to the necessity of model inference and backpropagation.
To address this challenge, we propose \textbf{PRISM}, a novel training-free approach for efficient multimodal data selection. Unlike existing methods, PRISM eliminates the reliance on proxy models, warm-up pretraining, and gradient-based optimization. Instead, it leverages Pearson correlation analysis to quantify the intrinsic visual encoding properties of MLLMs, computing a task-specific correlation score to identify high-value instances. This not only enables data-efficient selection, but maintains the model's original performance.
Empirical evaluations across multiple MLLMs demonstrate that PRISM reduces the overall time required for visual instruction tuning and data selection to just 30\% of conventional methods, while surpassing fully fine-tuned models across eight multimodal and three language understanding benchmarks, achieving a 101.7\% relative improvement in final performance.
\end{abstract}
\begin{figure}[t]
  \includegraphics[width=\columnwidth, trim=0mm 155mm 240mm 0mm, clip]{figs/intro.pdf}
  \caption{The radar chart illustrates the performance of PRISM, LLaVA, and TIVE across multiple benchmarks. PRISM demonstrates competitive performance while using significantly fewer training samples. The bar chart on the right highlights the data efficiency of PRISM-Instruct-250K, achieving 101.7\% relative performance with only 30\% of the data used by LLaVA-Instruct-665K and significantly outperforming TIVE-Instruct-100K. The shaded regions indicate data selection time, showing that PRISM achieves 3× faster tuning speed compared to LLaVA and TIVE, emphasizing its efficiency in multimodal instruction tuning.}
  \label{fig:experiments}
\end{figure}

\section{Introduction}

The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly transformed artificial intelligence by integrating vision and language processing capabilities \citep{liu2024improvedbaselinesvisualinstruction, zhu2023minigpt4enhancingvisionlanguageunderstanding, dai2023instructblip}. Modern MLLMs typically undergo a two-stage training process: (1) large-scale pretraining on web-scale image-text pairs to establish cross-modal alignment, followed by (2) visual instruction tuning on task-specific datasets to enhance instruction-following abilities. While instruction tuning is crucial for achieving strong downstream performance, the exponential growth of low-quality and redundant data \citep{chen2024visionlanguagemodelstrongfilter,wei2023instructiongpt4200instructionparadigmfinetuning} in curated datasets poses a major challenge. This proliferation not only increases computational costs but also leads to diminishing returns, highlighting the need for efficient data selection strategies that maximize informativeness while minimizing redundancy.

\noindent 
% As training on the full dataset becomes increasingly impractical, selecting the most informative samples is essential for maintaining strong performance while reducing computational overhead.
% Existing data selection approaches can be broadly classified into two categories: \textit{Model-Agnostic Selection}, which relies on proxy models (e.g., pretrained scorers \citep{chen2024visionlanguagemodelstrongfilter} or auxiliary MLLMs \citep{lee2024conceptskilltransferabilitybaseddataselection}), and \textit{Gradient-Based Selection}, which utilizes loss-based \citep{liu2024morehighvaluedataselection}or influence function-driven criteria \citep{wu2025iconsinfluenceconsensusvisionlanguage}. However, both paradigms suffer from inherent limitations: proxy-based methods introduce bias due to potential misalignment between the proxy and target models, whereas gradient-based techniques incur prohibitive computational costs due to the iterative nature of model training. More critically, these approaches often fail to outperform full-dataset training within practical computational constraints, limiting their real-world applicability.

\noindent As training on the full dataset becomes increasingly impractical, selecting the most informative samples is essential for maintaining strong performance while reducing computational overhead.
Existing data selection approaches can be broadly classified into two categories: Model-Agnostic Selection and Gradient-Based Selection.
Model-Agnostic Selection relies on proxy models, such as pretrained scorers \citep{chen2024visionlanguagemodelstrongfilter} or auxiliary MLLMs \citep{lee2024conceptskilltransferabilitybaseddataselection}, to estimate data importance. However, these methods often introduce bias due to potential misalignment between the proxy and target models.
Gradient-Based Selection, on the other hand, utilizes criteria derived from model training dynamics, such as loss-based \citep{liu2024morehighvaluedataselection} or influence function-driven metrics \citep{wu2025iconsinfluenceconsensusvisionlanguage}. These approaches are computationally expensive due to the iterative nature of gradient computation.
More critically, both paradigms often fail to outperform full-dataset training within practical computational constraints, limiting their real-world applicability.


% To address these limitations, we introduce PRISM, a novel training-free framework that redefines multimodal data selection by exploiting the intrinsic visual encoding properties of MLLMs. PRISM leverages the architectural synergy between vision encoders (e.g., CLIP \citep{radford2021learningtransferablevisualmodels}) and language models \citep{li2023textbooksneediiphi15, zheng2023judgingllmasajudgemtbenchchatbot}, wherein visual inputs are projected into the LLM's latent space via projectors. Our key insight is that the informational uniqueness of images is inherently captured within the LLM's intermediate token embeddings. By computing pairwise Pearson correlations of token embeddings, PRISM quantifies the representational distinctiveness of visual samples, selecting those that maximize diversity while minimizing redundancy—all without relying on proxy models, gradient computations, or additional training. This method reframes multimodal data selection by leveraging the LLM’s intrinsic representations as a \textit{quality-sensitive filter}, where high-value samples—aligned with the model’s semantic priors and exhibiting complementary feature patterns—form unique correlation structures and contribute to increased Shannon entropy, ultimately improving multimodal learning.
\noindent To address these shortcomings, we introduce PRISM, a novel training-free framework that redefines multimodal data selection by exploiting the intrinsic visual encoding properties of MLLMs. Unlike existing Model-Agnostic and Gradient-Based methods, PRISM represents a third paradigm: Intrinsic Selection. A key challenge in developing such a method is that MLLMs encode rich multimodal interactions in high-dimensional token representations, yet directly leveraging these internal structures for data selection is nontrivial. Unlike Gradient-Based approaches, which capture model learning dynamics, or Model-Agnostic methods, which rely on external scoring heuristics, our Intrinsic Selection extracts meaningful structural information \textit{without} access to model training or auxiliary predictors.

\noindent 
PRISM overcomes this challenge by leveraging the architectural synergy between vision encoders (e.g., CLIP \citep{radford2021learningtransferablevisualmodels}) and language models \citep{li2023textbooksneediiphi15, zheng2023judgingllmasajudgemtbenchchatbot}, wherein visual inputs are projected into the LLM's latent space via projectors. Our key insight is that the informational uniqueness of images is inherently captured within the LLM's intermediate token embeddings. By computing pairwise Pearson correlations of token embeddings, PRISM quantifies the representational distinctiveness of visual samples, selecting those that maximize diversity while minimizing redundancy—all without relying on proxy models, gradient computations, or additional training. This method reframes multimodal data selection by leveraging the LLM’s intrinsic representations as a quality-sensitive filter, where high-value samples—aligned with the model’s semantic priors and exhibiting complementary feature patterns—form unique correlation structures and contribute to increased Shannon entropy, ultimately improving multimodal learning.

\noindent 
We validate PRISM through extensive experiments on a diverse set of multimodal benchmarks, evaluating its efficacy against state-of-the-art data selection methods. Our results demonstrate that MLLMs fine-tuned on PRISM-selected data (\texttt{PRISM-Instruct-250K}) outperform models trained on the full dataset while reducing computational costs by 70\%. Furthermore, we conduct additional analyses on \textit{Cross-Model Generalization and Scalability} and \textit{Knowledge Retention}, demonstrating that PRISM generalizes effectively across different MLLM architectures and better preserves linguistic capabilities compared to full-dataset training.

% \noindent\textbf{Our key contributions are as follows:}
% \begin{itemize}
%     \item We introduce \textbf{PRISM}, the first training-free multimodal data selection framework that eliminates the need for proxy models, gradient computation, and iterative retraining.
%     \item We propose a novel correlation-based scoring mechanism that quantifies intrinsic feature redundancy in MLLMs' latent representations, enabling computationally efficient data selection.
%     \item We conduct extensive empirical evaluations, demonstrating that PRISM-selected data consistently outperforms full-dataset training across multiple multimodal and language understanding benchmarks while significantly reducing computational costs.

% \end{itemize}

\noindent\textbf{Our key contributions are as follows:}
\begin{itemize}
\item We introduce PRISM, a paradigm shift in multimodal data selection. As the first training-free framework, PRISM fundamentally departs from traditional selection paradigms by eliminating reliance on proxy models, gradient computation, and iterative retraining, offering an efficient yet principled alternative.
\item We propose the intrinsic selection mechanism that unlocks the latent structure of multimodal representations. By directly quantifying intrinsic feature redundancy within MLLMs’ token embeddings, PRISM enables scalable, high-fidelity data selection—achieving stronger multimodal generalization without additional training overhead.
% \item We conduct extensive empirical evaluations, demonstrating that PRISM-selected data not only surpasses full-dataset training in performance but also dramatically reduces computational costs across multiple multimodal and language understanding benchmarks. This establishes PRISM as a practical, resource-efficient solution for scalable multimodal learning.
 \item Extensive experiments show that PRISM-selected data outperforms full-dataset training while significantly reducing computational costs, making it a practical solution for scalable multimodal learning.
\end{itemize}


\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth, trim=0mm 135mm 100mm 0mm, clip]{figs/main.pdf}
  \caption{Comparison of data selection paradigms for MLLMs. Model-Agnostic Selection (left) relies on external proxy models without involving the LLM, potentially misaligning with its learned representations. Gradient-Based Selection (middle) uses the LLM’s gradients for selection but incurs high computational costs. Intrinsic Selection (PRISM) (right) directly utilizes the LLM’s token embeddings, enabling training-free, efficient, and model-aware data selection.}
  \label{fig:Visual_Instruction_Selection}
\end{figure*}

\section{Visual Instruction Selection}  

Visual instruction selection is an approach that can effectively reduce training time of visual instruction tuning by identifying high-value instruction instances. Numerous studies have explored effective methods for selecting such instruction instances while minimizing computational overheads. In this section, we first introduce two fundamental principles for visual instruction selection, which provide a framework for evaluating the effectiveness of different methods in real-world scenarios. Furthermore, we position existing developments in this research area, highlighting key advancements and their implications for optimizing multimodal instruction tuning.

\noindent 
\textbf{Principle 1: Data selection should not come at the cost of performance.}  
An effective visual instruction selection method should ensure that the model’s performance is at least not worse than a fully fine-tuned counterpart. While it is acceptable to achieve better performance with more data, it is not justifiable to compromise model quality in pursuit of dataset reduction.  

\noindent 
\textbf{Principle 2: The time required for visual instruction selection should not exceed the time saved in visual instruction tuning.}  
The primary goal of visual instruction selection is to improve efficiency by reducing the computational burden of instruction tuning. However, if the selection process itself is excessively time-consuming, it defeats the purpose by negating the computational savings gained from dataset reduction. In contrast, an ideal selection method should strike a balance between efficiency and effectiveness, ensuring that the overall training pipeline benefits from reduced resource consumption without introducing additional overhead that outweighs the savings.


\noindent
As shown in Fig. \ref{fig:Visual_Instruction_Selection}, we categorize current visual instruction selection methods mentioned before into two main types. The first is Model-Agnostic Selection, where the target MLLM remains untouched, and data quality is assessed using a proxy model. Such a proxy model can be a scoring function, such as a pre-trained scoring model \citep{chen2024visionlanguagemodelstrongfilter}, human reward models, or GPT-based scoring mechanisms \citep{wei2023instructiongpt4200instructionparadigmfinetuning}. Some approaches \citep{lee2024conceptskilltransferabilitybaseddataselection} also involve training a small MLLM to guide the selection process for the target MLLM.

\noindent  
The second category is Gradient-Based Selection, where the target MLLM is first pre-trained on a specific data partition, and data value is subsequently assessed using loss, perplexity, or gradient-based metrics.  

\noindent  
Both approaches have inherent limitations:
\begin{enumerate}[label=(\arabic*)]  % 使用编号 (1), (2), (3)
    \item \textbf{Performance degradation}—While the existing two types of selection methods effectively filter a subset of the data, they often degrade the model performance compared to full fine-tuning (see in Table \ref{tab:main}). This contradicts \emph{Principle 1}, as the objective is to construct a stronger model with fewer data, rather than a weaker model due to reduced data availability.
    
    \item \textbf{High computational cost}—Gradient-based selection is computationally prohibitive. For instance, TIVE \citep{liu2024morehighvaluedataselection} employs LoRA-based warm-up training on the target MLLM before computing gradient vectors. However, the time required for this process often surpasses the time saved in instruction tuning (see in Table \ref{tab:time}), violating \emph{Principle 2} and rendering it impractical.
    
    \item \textbf{Proxy model bias}—To mitigate computational overhead, some methods rely on proxy models, keeping the target MLLM independent during training. However, this introduces bias from the pre-trained proxy model (e.g., GPT or a human reward model) or a warm-up trained small MLLM, which may not generalize well to the target MLLM. Since proxy models and warm-up data significantly influence selection results, a general selection strategy that excludes the target MLLM will yield the same selected data across different MLLMs, despite their distinct data requirements. Consequently, such approaches fail to provide optimal data selection tailored to specific MLLMs.
\end{enumerate}

\noindent 
To overcome these challenges, we introduce PRISM, a self-PRuning Intrinsic Selection Method for training-free multimodal data selection.



\section{\includegraphics[height=1.3em]{figs/Prismatic_Star.png}PRISM}

The PRISM framework establishes a new paradigm for multimodal data selection by directly harnessing the intrinsic representation structures of MLLMs. Unlike existing methods that depend on external heuristics or model-driven proxies, PRISM leverages the model’s intrinsic encoding mechanisms to assess data informativeness. Modern MLLMs, such as LLaVA \citep{liu2024improvedbaselinesvisualinstruction}, unify visual and textual modalities through a vision encoder and projector, embedding images into the LLM’s latent space—where their uniqueness is inherently captured. This approach ultimately enhances performance while reducing training time by 70\%.
% A key insight of PRISM is that layer-wise token embeddings act as an implicit quality filter, inherently distinguishing between informative and redundant samples. By analyzing statistical dependencies within these embeddings, PRISM systematically identifies high-value data without requiring external supervision, proxy models, or gradient-based computations.
Our initial research (as in Fig. \ref{fig:findings}) revealed that layer-wise token embeddings inherently capture structural distinctions between informative and redundant samples. Inspired by this, we explored the statistical dependencies within these embeddings to systematically identify high-value data instances. These findings ultimately led to the design of PRISM, a method that selects informative samples without relying on external supervision (e.g., proxy models or gradient-based computations).

\noindent PRISM formalizes this approach in a three-stage pipeline: feature representation, correlation analysis, and self-pruning selection. As we will see in the performance evaluation, PRISM offers a scalable and computationally efficient solution to multimodal data selection.

\subsection{Feature Representation and Correlation Analysis}

\noindent Let $\mathcal{D} = \{I_1, I_2, \dots, I_N\}$ denote the image dataset for target task $\mathcal{T}$. For each image $I_i$, the vision encoder (VE) extracts and projects visual embeddings into the LLM's latent space:
\begin{equation}
v_i = \text{VE}(I_i) \in \mathbb{R}^{d_v}, \quad 
z_i = \text{Proj}(v_i) \in \mathbb{R}^d
\end{equation}
where $\text{Proj}: \mathbb{R}^{d_v} \to \mathbb{R}^d$ is a linear projector. The LLM processes $z_i$ through transformer layers, with averaged token features from layer $l$ computed as:
\begin{equation}
F_i = \frac{1}{T} \sum_{t=1}^T \text{LLM}^{(l)}(z_i)_t \in \mathbb{R}^d
\end{equation}
where $T$ denotes the number of tokens. We hypothesize that images with divergent feature correlations provide complementary information. This is quantified through Pearson analysis:
\begin{gather}
P_{ij} = \frac{\mathbb{E}[(F_i - \mu_i)(F_j - \mu_j)]}{\sigma_i\sigma_j}, \quad
C_i = \sum\nolimits_{j=1}^N P_{ij}
\end{gather}
where $\mu_i, \sigma_i$ are mean and standard deviation of $F_i$, and $C_i$ measures alignment with $\mathcal{D}$'s feature distribution.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/normalized_score_distribution.pdf}
    \caption{Correlation-based distribution of multimodal data across datasets. The selection strategy prioritizes samples that balance redundancy reduction and information diversity, ensuring that high-correlation images do not dominate while preserving a broad range of semantic variance. The highlighted region represents the top 30\% of high-value samples identified from LLaVA-665K.}
    \label{fig:findings}
\end{figure}
\subsection{Self-Pruning Selection}

\noindent Images with the lowest $C_i$ values (i.e., those in the bottom $\tau\%$ of the sorted correlation scores) are selected as high-value candidates. This selection strategy is guided by three factors:

\noindent \textbf{Reduction of Feature Redundancy:} High correlation images ($\uparrow C_i$) exhibit substantial semantic overlap, contributing diminishing returns during training.

\noindent \textbf{Information-Theoretic Diversity:} Low correlation samples ($\downarrow C_i$) maximize the Shannon entropy of the selected subset, as formally analyzed in Appendix~\ref{shannon}.

\noindent \textbf{Outlier Resilience:} Unlike variance-based selection, Pearson correlation’s scale invariance ensures robustness to embedding magnitude variations caused by projector miscalibrations.

\noindent Formally, given a threshold $\tau$ (e.g., $\tau = 30\%$), the selected subset is defined as:
\begin{equation}
    \mathcal{D}_{\text{selected}} = \{ I_i \mid C_i \leq Q_{\tau} (C) \},
\end{equation}
where $Q_{\tau}$ denotes the $\tau$-th percentile of correlation scores. Algorithm~\ref{algo:selection} is organized into three main phases, corresponding to the selection strategy's core factors:

\noindent
\textbf{Intrinsic Feature Extraction (Step 1):}  
    This phase computes visual embeddings and projects them into the LLM's latent space, obtaining layer-wise averaged token features. By capturing intrinsic semantic information, it prepares the feature space for correlation analysis, laying the foundation for \textit{Outlier Resilience}.

\noindent
    \textbf{Correlation Analysis (Step 2):}  
    The Pearson correlation matrix is computed to evaluate feature similarity across all images. Summing each row gives the total correlation score for each image, $C_i$, which quantifies its semantic redundancy. This phase directly targets the \textit{Reduction of Feature Redundancy} by identifying images with high semantic overlap.

 \noindent   \textbf{Self-Pruning Selection (Step 3):}  
    By sorting and selecting images with the lowest $C_i$ scores, this step maximizes diversity while avoiding redundant samples. It achieves \textit{Information-Theoretic Diversity} by preserving images that contribute the most to the subset's entropy, ensuring data efficiency.


\begin{algorithm}
    \caption{PRISM Data Selection}
    \label{algo:selection}
    \begin{algorithmic}[1]
        \REQUIRE Image dataset $D = \{I_1, \dots, I_N\}$, vision encoder $\text{VE}$, projector $\text{Proj}$, LLM layer $l$, threshold $\tau$
        \ENSURE Selected subset $D_{\text{selected}}$
        
        \STATE \textbf{Step 1: Intrinsic Feature Extraction}
        \FOR{each image $I_i \in D$}
            \STATE Compute visual embedding: $v_i \gets \text{VE}(I_i)$
            \STATE Project to LLM space: $z_i \gets \text{Proj}(v_i)$
            \STATE Extract layer-$l$ features:
            \begin{equation*}
                F_i \gets \frac{1}{T} \sum_{t=1}^{T} \text{LLM}^{(l)}(z_i)_t
            \end{equation*}
        \ENDFOR
        
        \STATE \textbf{Step 2: Correlation Analysis}
        \STATE Construct feature matrix $F \in \mathbb{R}^{N \times d}$
        \STATE Compute Pearson matrix $P_{ij} \gets \frac{\text{cov}(F_i, F_j)}{\sigma_{F_i} \sigma_{F_j}}$
        \STATE Score images: $C_i \gets \sum_{j=1}^{N} P_{ij}, \forall i$
        
        \STATE \textbf{Step 3: Self-Pruning Selection}
        \STATE Sort indices: $\text{argsort}(C) \gets [i_1, \dots, i_N]$ s.t. $C_{i_1} \leq \dots \leq C_{i_N}$
        \STATE Select subset: $D_{\text{selected}} \gets \{ I_{i_k} \mid k \leq \lfloor \tau N \rfloor \}$
        \RETURN $D_{\text{selected}}$
    \end{algorithmic}
\end{algorithm}

\section{Experiments}
We first present our experimental setup and evaluation benchmarks, followed by comparisons with state-of-the-art methods. Next, we analyze our method’s behavior and effectiveness across various dimensions. Additionally, we evaluate the transferability of our strategy to unseen tasks and model architectures. Finally, we conduct ablation studies to assess the contribution of each component.

\noindent 
\subsection{Experiment Setup}

\noindent
\textbf{Dataset \& Model:} We evaluate PRISM on the visual instruction tuning dataset LLaVA-665K \citep{liu2024improvedbaselinesvisualinstruction}, using LLaVA-1.5-7B \citep{liu2024improvedbaselinesvisualinstruction} as our primary base model. All experiments are conducted for one epoch following the official fine-tuning hyperparameters. To ensure a fair comparison, we maintain a consistent training environment across all evaluations.

\noindent
\textbf{Baselines:} We compare PRISM against a comprehensive set of data selection baselines, including Random Selection, Instruction Length, Perplexity \citep{liu2024morehighvaluedataselection}, GraNd \citep{paul2023deeplearningdatadiet}, EL2N \citep{paul2023deeplearningdatadiet}, InstructionGPT-4 \citep{wei2023instructiongpt4200instructionparadigmfinetuning}, SELF-FILTER \citep{chen2024visionlanguagemodelstrongfilter}, TIVE \citep{liu2024morehighvaluedataselection}, COINCIDE \citep{lee2024conceptskilltransferabilitybaseddataselection}, DataTailor \citep{yu2024mastering}, and ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage}. To ensure fair comparisons, we adopt the experimental settings and incorporate results from ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage} and TIVE \citep{liu2024morehighvaluedataselection}.

\noindent
\textbf{Benchmarks:} Following the evaluation framework of LLaVA-1.5 \citep{liu2024improvedbaselinesvisualinstruction}, we assess the effectiveness of PRISM across a diverse set of multimodal benchmarks designed to test various capabilities of MLLMs. These benchmarks are grouped into three main categories: understanding and reasoning, factual consistency and generalization, and visual conversation and core multimodal skills.

\noindent
For understanding and reasoning, we evaluate the model’s ability to perform multiple-choice tasks (MMBench \citep{liu2024mmbenchmultimodalmodelallaround}), scientific question answering (ScienceQA \citep{lu2022learn}), and multimodal reasoning (MME \citep{yin2023survey}). For factual consistency and generalization, we measure the model’s tendency for hallucination (POPE \citep{li2023evaluatingobjecthallucinationlarge}) and its zero-shot generalization ability on unseen visual queries (VizWiz \citep{gurari2018vizwizgrandchallengeanswering}). Finally, for visual conversation and core multimodal skills, we assess the model’s conversational capabilities (MM-Vet \citep{yu2024mmvetevaluatinglargemultimodal}) and its proficiency in perception, knowledge integration, and reasoning (MMMU \citep{yue2024mmmumassivemultidisciplinemultimodal}).

% \noindent
% To demonstrate the efficiency gains of PRISM, we report the tailored amount of valid data used for each benchmark, along with the corresponding reduction in training time, in Appendix 2. 

\subsection{Main Results}
We present a comprehensive evaluation of PRISM across multiple settings. First, in Table \ref{tab:main}, we compare PRISM with the baseline methods selected above on the LLaVA-1.5-7B model \citep{liu2024improvedbaselinesvisualinstruction}. This demonstrates its superior performance in multimodal data selection. Next, Table \ref{tab:arch} showcases the results of PRISM across different MLLMs, which highlights its generalizability and robustness. Finally, Table \ref{tab:text} focuses on PRISM's text-only capabilities, which provides insights into its effectiveness in uni-modal settings.

\noindent
We further analyze these results as follows:

\noindent
\textbf{Superior Multimodal Understanding.} As shown in Table \ref{tab:main}, PRISM achieves the best performance across 11 multimodal benchmarks, surpassing full-dataset fine-tuning by \textbf{1.7\%} in relative performance. Notably, PRISM excels in instruction-sensitive tasks: it outperforms full fine-tuning on MMBench (65.2 vs. 64.3) and MM-Vet (32.0 vs. 31.1), demonstrating its ability to select samples that enhance complex reasoning and visual conversation capabilities. The improvements are particularly significant compared to gradient-based methods like GraNd (62.9 vs. 65.2 on MMBench), highlighting the limitations of loss-driven selection in multimodal contexts.

\noindent
\textbf{Hallucination Mitigation.} PRISM achieves the highest scores on all POPE subsets (87.7/88.7/85.5), outperforming even specialized hallucination reduction methods like ICONS (87.5). This suggests that low-correlation samples inherently reduce the model’s tendency to generate inconsistent facts, as they avoid overfitting to spurious text-visual correlations prevalent in redundant data.

\noindent
\textbf{Balanced Efficiency and Performance.} While gradient-based methods like TIVE achieve comparable average performance (100.6\% rel.), their total time costs (selection + training) often exceed full fine-tuning due to iterative model updates. In contrast, PRISM achieves higher accuracy (\textbf{101.7\%}) and reduces total time by \textbf{70\%}. This efficiency stems from its training-free advantage: feature extraction and correlation computation are executed in a single forward pass and offline batched processing, respectively, with negligible overhead compared to full training cycles. Remarkably, PRISM simultaneously enhances spatial reasoning capabilities (\textbf{330.0} on MME-C vs. 311.9 for full fine-tuning), validating that its selection criteria preserve geometrically informative samples often lost in random or length-based pruning.
\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{ % 调节表格宽度为文本宽度，高度按比例缩放
\begin{tabular}{l*{12}{c}}
\toprule
\textbf{Method} & \textbf{SQA} & \textbf{SQA-I} & \textbf{VizWiz}& \textbf{POPE-P} & \textbf{POPE-R} & \textbf{POPE-A} & \textbf{MM-Vet} & \textbf{MMBench} & \textbf{MME-P} & \textbf{MME-C} & \textbf{MMMU} & \textbf{\cellcolor{gray!10}Rel. (\%)} \\
\midrule
Full-Finetune & 69.4 & 66.8& 50.0 & 86.1& 87.3&84.2 &31.1 &64.3 &1510.7 & 311.9& 35.4& 100\% \cellcolor{gray!10} \\
\cmidrule(lr){1-13} % 在 Full-Finetune 下面加一条分割线
Random &65.5 &64.5 &48.1  &85.1 &84.6 &83.6 &30.2 &55.5 &1492.0 &233.5 &30.5 &93.2\% \cellcolor{gray!10} \\
Length &66.8 &66.7 & 47.0 &85.4 &85.5 &84.1 &31.5 &57.0 & 1422.1& 306.0&33.1 & 96.6\%\cellcolor{gray!10} \\
EL2N &70.2 &70.6 & 44.4& 85.6 & 85.6 &85.6 &- &61.6 &1356.5 &294.7 & -&97.2\% \cellcolor{gray!10} \\
Perplexity &70.5 & 67.9& -& 83.3  & 83.3 &83.3  &- & 62.3&1393.3 & 260.7& -&95.8\% \cellcolor{gray!10} \\
GraNd &71.4 &68.4& 37.8& 82.5 & 82.5 &82.5 &- & 62.9&  1400.5&287.1& -& 94.6\%\cellcolor{gray!10} \\
TIVE &72.2 &70.6 & -& 85.6 & 85.6 &85.6 &- &63.2 &1433.0 &322.1 & -&100.6\% \cellcolor{gray!10} \\
InstructionGPT-4 & -& -& -& - & -&- &- &31.4 &463.3 &- & -&39.75\% \cellcolor{gray!10} \\
Self-Filter &- & 61.4& 53.2& 83.8 & 83.8&83.8 &- &61.4 & 1306.2& -& -&96.1\% \cellcolor{gray!10} \\
COINCIDE & -& 69.2& 46.8& 86.1 & 86.1&86.1 &- &63.1 &1495.6 &- & -&99.3\% \cellcolor{gray!10} \\
ICONS &- &70.8 & -& 87.5 & 87.5&87.5 &- &63.1 &1485.7& - & -& 101.0\%\cellcolor{gray!10} \\
DataTailor  & 71.0& -& 49.5& 85.3 & 85.3&85.3 &- &- & 1476.1& 319.2& -&99.9\% \cellcolor{gray!10} \\
\includegraphics[height=1em]{figs/Prismatic_Star.png}\cellcolor{cyan!10}PRISM (Ours) & \textbf{71.3}\cellcolor{cyan!10} &\textbf{69.1} \cellcolor{cyan!10}  & \textbf{50.1}\cellcolor{cyan!10} &\textbf{87.7} \cellcolor{cyan!10} &\textbf{88.7} \cellcolor{cyan!10} &\textbf{85.5} \cellcolor{cyan!10} & \textbf{32.0}\cellcolor{cyan!10} &\textbf{65.2} \cellcolor{cyan!10} & 1470.0\cellcolor{cyan!10} &\textbf{330.0} \cellcolor{cyan!10} & 34.7\cellcolor{cyan!10} & \textbf{101.7\%}\cellcolor{cyan!10} \\
\bottomrule
\end{tabular}
}
\caption{Evaluation of PRISM against full fine-tuning and existing data selection approaches across multiple multimodal understanding benchmarks. PRISM achieves superior performance, surpassing full fine-tuning while significantly reducing computational costs. Metrics in \textbf{bold} indicate improvements over the full fine-tuning baseline. For POPE, we report the average score across three subsets for certain baselines due to the unavailability of complete results.}
\label{tab:main}
\end{table*}


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{ % 调节表格宽度为文本宽度，高度按比例缩放
\begin{tabular}{l*{12}{c}}
\toprule
\textbf{Model} & \textbf{SQA} & \textbf{SQA-I} & \textbf{VizWiz} & \textbf{POPE-P} & \textbf{POPE-R} & \textbf{POPE-A} & \textbf{MM-Vet} & \textbf{MMBench} & \textbf{MME-P} & \textbf{MME-C} & \textbf{MMMU} & \textbf{\cellcolor{gray!10}Rel. (\%)} \\
\midrule
LLaVA-Phi2-3B & 75.3&72.7 & 41.2&87.3 & 88.6& 86.1 &35.6 &68.7 &1467.7 & 298.0&37.7 & 100\%\cellcolor{gray!10} \\

\includegraphics[height=1em]{figs/Prismatic_Star.png}\cellcolor{cyan!10}PRISM-3B & \cellcolor{cyan!10} \textbf{76.3}& \cellcolor{cyan!10} \textbf{72.8}& \cellcolor{cyan!10} 40.9 & \cellcolor{cyan!10} \textbf{87.5}& \cellcolor{cyan!10} \textbf{88.8}& \cellcolor{cyan!10} \textbf{86.5}& \cellcolor{cyan!10} 34.1& \cellcolor{cyan!10}\textbf{68.9} & \cellcolor{cyan!10} \textbf{1485.5}& \cellcolor{cyan!10} \textbf{305.0}& \cellcolor{cyan!10} 37.6& \cellcolor{cyan!10} \textbf{100.1\%}\\
\cmidrule(lr){1-13} % 在 Full-Finetune 下面加一条分割线

LLaVA-Vicuna-7B & 69.4 & 66.8& 50.0 & 86.1& 87.3&84.2 &31.1 &64.3 &1510.7 & 311.9& 35.4& 100\% \cellcolor{gray!10} \\

\includegraphics[height=1em]{figs/Prismatic_Star.png}\cellcolor{cyan!10}PRISM-7B & \textbf{71.3}\cellcolor{cyan!10} &\textbf{69.1} \cellcolor{cyan!10}  & \textbf{50.1}\cellcolor{cyan!10} &\textbf{87.7} \cellcolor{cyan!10} &\textbf{88.7} \cellcolor{cyan!10} &\textbf{85.5} \cellcolor{cyan!10} & \textbf{32.0}\cellcolor{cyan!10} &\textbf{65.2} \cellcolor{cyan!10} & 1470.0\cellcolor{cyan!10} &\textbf{330.0} \cellcolor{cyan!10} & 34.7\cellcolor{cyan!10} & \textbf{101.7\%}\cellcolor{cyan!10} \\
\cmidrule(lr){1-13} % 在 Full-Finetune 下面加一条分割线

LLaVA-Vicuna-13B & 74.4&71.6 &53.6 &87.4 & 88.0& 85.6&36.1  & 67.7&1531.3 &295.4 & 35.1& 100\%\cellcolor{gray!10} \\

\includegraphics[height=1em]{figs/Prismatic_Star.png}\cellcolor{cyan!10}PRISM-13B  & \textbf{74.5}\cellcolor{cyan!10} &\textbf{71.8} \cellcolor{cyan!10}  & 53.1\cellcolor{cyan!10} &\textbf{87.7} \cellcolor{cyan!10} &\textbf{88.4} \cellcolor{cyan!10} &\textbf{85.7} \cellcolor{cyan!10} & \textbf{36.4}\cellcolor{cyan!10} &65.8 \cellcolor{cyan!10} & \textbf{1538.5}\cellcolor{cyan!10} &\textbf{307.5} \cellcolor{cyan!10} & \textbf{35.7}\cellcolor{cyan!10} & \textbf{100.4\%}\cellcolor{cyan!10} \\

\bottomrule
\end{tabular}
}
\caption{Performance on Cross-Model Generalization and Scalability with PRISM.}
\label{tab:arch}
\end{table*}



\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{ % 调整表格宽度为单栏宽度
\begin{tabular}{l*{4}{c}}
\toprule
\textbf{Model} & \textbf{Hellaswag} & \textbf{MMLU} & \textbf{MMLU-PRO} & \textbf{Rel. (\%)} \\
\midrule
LLaVA-Phi2-3B &66.0 & 50.5&9.1 & 100\%\\

\includegraphics[height=1em]{figs/Prismatic_Star.png}PRISM-3B &\textbf{67.4} &\textbf{52.7} &8.6 & \textbf{100.3\%}\\
\cmidrule(lr){1-5} % 在 PRISM-3B 下面加一条分割线

LLaVA-Vicuna-7B & 66.5& 35.0& 17.8&100\% \\

\includegraphics[height=1em]{figs/Prismatic_Star.png}PRISM-7B & \textbf{66.5}&\textbf{41.1} &15.7 & \textbf{101.9\%}\\
\cmidrule(lr){1-5} % 在 PRISM-7B 下面加一条分割线

LLaVA-Vicuna-13B &69.5 &36.2 & 6.8&100\% \\

\includegraphics[height=1em]{figs/Prismatic_Star.png}PRISM-13B &\textbf{69.6} &\textbf{39.5} & \textbf{12.4}&\textbf{130.6\%} \\

\bottomrule
\end{tabular}
}
\caption{Results on language benchmarks.}
\label{tab:text}
\end{table}




\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{ % 调整表格宽度为单栏宽度
\begin{tabular}{l*{3}{c}}
\toprule
\textbf{Method} & \textbf{Data Selection} & \textbf{Visual Instruction Tuning} & \textbf{Overall} \\
\midrule
Full-Finetune & - & 94 (Hours)& 94 \\
\cmidrule(lr){1-4} % 在 Full-Finetune 下面加一条分割线

TIVE & 87 (Hours) &14 (Hours) & 101 (\textcolor{red}{+7.5\%})\\

\includegraphics[height=1em]{figs/Prismatic_Star.png}PRISM &1.5 (Hours) & 28 (Hours)& 29.5 (\textcolor{green}{-71\%})\\
\bottomrule
\end{tabular}
}
\caption{Wall-clock runtime (measured as A100 80G GPU) for total computation cost.}
\label{tab:time}
\end{table}


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{ % 调节表格宽度为文本宽度，高度按比例缩放
\begin{tabular}{l*{12}{c}}
\toprule
\textbf{Method} & \textbf{SQA} & \textbf{SQA-I} & \textbf{VizWiz} & \textbf{POPE-P} & \textbf{POPE-R} & \textbf{POPE-A} & \textbf{MM-Vet} & \textbf{MMBench} & \textbf{MME-P} & \textbf{MME-C} & \textbf{MMMU} & \textbf{Rel. (\%)} \\
\midrule
Deep Layer & 71.2& 69.1& 51.6& 86.6& 88.0&84.2 &31.1 & 62.9&1477.0 &254.0 &34.5 & 97.2\%\\

Middle Layer &70.9 &69.1 &47.7 &86.5 &87.8 &84.2 &31.9 &65.0 &1517.1 &276.0 & 34.9&97.9\% \\

\raisebox{-0.2em}{\includegraphics[height=0.8em]{figs/12483567.png}} Shallow Layer & \textbf{71.3}\cellcolor{cyan!10} &\textbf{69.1} \cellcolor{cyan!10}  & 50.1\cellcolor{cyan!10} &\textbf{87.7} \cellcolor{cyan!10} &\textbf{88.7} \cellcolor{cyan!10} &\textbf{85.5} \cellcolor{cyan!10} & \textbf{32.0}\cellcolor{cyan!10} &\textbf{65.2} \cellcolor{cyan!10} & 1470.0\cellcolor{cyan!10} &\textbf{330.0} \cellcolor{cyan!10} & 34.7\cellcolor{cyan!10} & \textbf{100.0\%}\cellcolor{cyan!10} \\
\cmidrule(lr){1-13} 

High Correlation &70.6&68.0 &48.1 &85.8 &87.6 &83.9 &30.7 &64.0 &1428.5 &275.3 &33.5 &96.3\%  \\

Moderate Correlation & 71.0&69.7 &48.3 &85.9 &86.7 &84.0 &30.0 &64.2 &1509.0 &286.0 & 34.1&97.3\% \\

\raisebox{-0.2em}{\includegraphics[height=0.8em]{figs/12483567.png}} Low Correlation & \textbf{71.3}\cellcolor{cyan!10} &69.1 \cellcolor{cyan!10}  & \textbf{50.1}\cellcolor{cyan!10} &\textbf{87.7} \cellcolor{cyan!10} &\textbf{88.7} \cellcolor{cyan!10} &\textbf{85.5} \cellcolor{cyan!10} & \textbf{32.0}\cellcolor{cyan!10} &\textbf{65.2} \cellcolor{cyan!10} & 1470.0\cellcolor{cyan!10} &\textbf{330.0} \cellcolor{cyan!10} & \textbf{34.7}\cellcolor{cyan!10} & \textbf{100.0\%}\cellcolor{cyan!10} \\
\cmidrule(lr){1-13} 

Last Token & 69.9&67.3 &49.4 &87.4 &88.3 &85.0 &31.6 &62.6 &1471.0 &272.0 &35.3 & 97.4\%\\

\raisebox{-0.2em}{\includegraphics[height=0.8em]{figs/12483567.png}} Avg Pooling & \textbf{71.3}\cellcolor{cyan!10} &\textbf{69.1} \cellcolor{cyan!10}  & \textbf{50.1}\cellcolor{cyan!10} &\textbf{87.7} \cellcolor{cyan!10} &\textbf{88.7} \cellcolor{cyan!10} &\textbf{85.5} \cellcolor{cyan!10} & \textbf{32.0}\cellcolor{cyan!10} &\textbf{65.2} \cellcolor{cyan!10} & 1470.0\cellcolor{cyan!10} &\textbf{330.0} \cellcolor{cyan!10} & 34.7\cellcolor{cyan!10} & \textbf{100.0\%}\cellcolor{cyan!10} \\
\bottomrule
\end{tabular}
}
\caption{Ablation study results on LLM layer selection, correlation-based scoring, and token aggregation for image representation.}
\label{tab:ablation}
\end{table*}









\subsection{Model Behavior Analysis}

% \textbf{Cross-Model Generalization and Scalability.} To evaluate the transferability of PRISM, we conduct generalization experiments across different types of architectures and scales. While our subset was initially selected using LLaVA-1.5-7B \citep{liu2024improvedbaselinesvisualinstruction}—which employs CLIP ViT-L/14 336px \citep{radford2021learningtransferablevisualmodels} as the vision encoder and Vicuna v1.5 7B \citep{zheng2023judgingllmasajudgemtbenchchatbot} as the language model—we investigate whether PRISM generalizes to other model combinations and scales. Following the methodology of LLaVA Steering \citep{bi2025llavasteeringvisualinstruction}, we evaluate two additional configurations: (1) LLaVA-Phi2-3B, which combines SigLIP-SO400M-Patch14-384 \citep{zhai2023sigmoidlosslanguageimage} as the vision encoder and Phi-2 2.7B \citep{li2023textbooksneediiphi15} as the language model, and (2) LLaVA-Vicuna-13B, which uses CLIP ViT-L/14 336px and Vicuna v1.5 13B. These experiments test whether our selection criteria identify universally valuable training examples rather than model-specific patterns.
\textbf{Cross-Model Generalization and Scalability.}  
PRISM is designed to identify high-value data that remains effective across different model architectures and scales. To validate this, we assess whether data selected using one model setup can benefit others. While our subset was initially selected with LLaVA-1.5-7B, we further evaluate its effectiveness on two additional model configurations. The detailed architectures of these models are summarized in Appendix~\ref{tab:model_arch}. The results approve that PRISM captures generally useful training samples rather than those tailored to a specific model.

\noindent
As shown in Table \ref{tab:arch}, PRISM demonstrates strong cross-architecture and cross-scale generalization capabilities. The subset selected using the 7B model achieves competitive performance across different model sizes and architectures, suggesting that our method captures fundamental visual-language understanding capabilities that are transferable and scalable. This highlights the robustness of PRISM in identifying high-value data points that generalize well across diverse multimodal model configurations.

\noindent
\textbf{Language Knowledge Retention.} While visual instruction tuning significantly enhances performance on vision-centric tasks, it often leads to a degradation in the model's ability to handle text-only tasks \citep{zhang2024wingslearningmultimodalllms}. To assess the text-only performance, we evaluate PRISM on a range of benchmarks accordingly, including interdisciplinary knowledge assessments such as MMLU \citep{hendrycks2021measuringmassivemultitasklanguage} and MMLU-PRO \citep{wang2024mmluprorobustchallengingmultitask}, as well as reasoning tasks like HellaSwag \citep{zellers2019hellaswagmachinereallyfinish}. These benchmarks are designed to test the model's ability to retain and utilize its original language understanding capabilities after multimodal fine-tuning.

\noindent
The results in Table \ref{tab:text} further demonstrate that PRISM in some cases can even improve the model's performance on text-only tasks, suggesting that our data selection method effectively mitigates the knowledge forgetting problem commonly associated with visual instruction tuning. This highlights the dual benefit of PRISM: enhancing multimodal task performance while maintaining the model's foundational language capabilities.

\noindent
\subsection{Ablation Study}
To validate the design choices of PRISM, we conduct systematic ablation studies on three key components: LLM layer selection, correlation-based scoring, and token aggregation for image representation. 

\noindent\textbf{Influence of LLM Layer Selection.} We first investigate how different transformer layers impact PRISM’s performance by extracting features from three representative layers: \emph{Shallow Layer 1}, which captures low-level visual patterns such as edges and textures; \emph{Middle Layer 16}, which balances visual and semantic features; and \emph{Deep Layer 32}, which encodes high-level semantic abstractions. As shown in Table~\ref{tab:ablation}, PRISM achieves the highest performance when using shallow layer features, outperforming deeper layers by 2.8\%. This result indicates that early-layer embeddings sufficiently capture the necessary information for redundancy detection, while deeper layers may overfit to task-specific semantics, leading to reduced generalizability. 

\noindent\textbf{Impact of Correlation-based Selection.} We evaluate PRISM’s correlation-based selection strategy by partitioning the dataset into three groups based on their Pearson correlation scores: \emph{Low-Correlation Group}, \emph{Medium-Correlation Group}, and \emph{High-Correlation Group}. The results in Table~\ref{tab:ablation} demonstrate that selecting low-correlation samples leads to the highest performance, outperforming the high-correlation group by 3.7\%. This supports our hypothesis that prioritizing samples with minimal correlation maximizes informational diversity, whereas high-correlation samples tend to be redundant, diminishing their contribution to multimodal learning. Our findings underscore the effectiveness of leveraging feature correlation as a criterion for efficient data selection (see Appendix~\ref{shannon} for further analysis).

\noindent\textbf{Effect of Token Aggregation Strategy.} Finally, we examine how different token aggregation methods influence image feature modeling. We compare two approaches: \emph{Average Token}, which computes a global average over all transformer tokens, and \emph{Last Image Token}, which uses only the final image token in the sequence. As shown in Table~\ref{tab:ablation}, the average token method achieves the best performance, surpassing the last image token by 2.6\%. This result aligns with PRISM’s design principle that averaging token representations captures holistic visual semantics, whereas relying on the last token may introduce positional biases or task-specific artifacts. These findings validate our choice of average pooling as a more robust and generalizable strategy for training-free multimodal data selection.







\section{Related Work}
% \textbf{Visual Instruction Tuning: }
% Visual instruction tuning has become a cornerstone in the development of MLLMs, playing a pivotal role in bridging the gap between theoretical model capabilities and practical applications. This approach has evolved significantly over time. Initially, LLMs were primarily used to generate synthetic visual instructions. While these early efforts demonstrated promising performance in conversational tasks, they struggled to meet the demands of more rigorous academic benchmarks. To overcome this limitation, a hybrid approach emerged, combining synthetically generated instructions with established academic datasets, thus creating more comprehensive and diverse training data. This methodological advancement has been instrumental in enhancing MLLMs, such as LLaVA \citep{liu2024visual}, InstructBLIP \citep{dai2023instructblip}, and Cambrian \citep{tong2024cambrian}, allowing these models to better interpret and respond to visual-linguistic cues. The importance of visual instruction tuning goes beyond improving task-specific performance; it also contributes to better alignment between model outputs and user expectations, thereby enhancing the practical utility of these systems for real-world applications while ensuring robust performance across academic evaluations. 
\textbf{Visual Instruction Tuning:}  
Visual instruction tuning is essential for aligning MLLMs with both practical applications and academic benchmarks. Early methods relied on synthetic visual instructions, which performed well in conversations but struggled on rigorous benchmarks. A hybrid approach later emerged, integrating synthetic data with academic datasets to improve training diversity. This advancement has enhanced models like LLaVA \citep{liu2024visual}, InstructBLIP \citep{dai2023instructblip}, and Cambrian \citep{tong2024cambrian}, enabling better visual-linguistic understanding. Beyond task performance, visual instruction tuning improves model alignment with user expectations, ensuring both practical utility and strong academic performance.

\noindent
% \textbf{Visual Instruction Selection: }
% \noindent
% Although MLLMs have achieved remarkable performance across various tasks, the rapid expansion of visual instruction datasets has led to significant redundancy, mirroring challenges faced in LLMs \citep{zhou2024lima,chen2023maybe05dataneeded,xia2024lessselectinginfluentialdata}. State-of-the-art MLLMs, such as BLIP3 \citep{xue2024xgen}, InternVL2.5 \citep{chen2025expandingperformanceboundariesopensource}, LLaVA-OneVision \citep{li2024llavaonevisioneasyvisualtask}, leverage billions of visual instruction instances to enhance their understanding capabilities. However, the sheer scale of these datasets results in substantial computational costs, often requiring hundreds to thousands of GPU hours, particularly for models built on large LLM backbones.

% \noindent
% To mitigate this issue, various data selection strategies have been proposed to evaluate and reduce redundancy, identifying high-value instances while preserving model performance. TIVE \citep{liu2024morehighvaluedataselection} detects severe redundancy in multimodal datasets and selects valuable data at both the task and instance levels based on gradient similarity, but it requires additional training on downstream tasks. SELF-FILTER \citep{chen2024visionlanguagemodelstrongfilter} introduces an auxiliary evaluation model that updates its parameters alongside training to prioritize high-value samples. COINCIDE \citep{lee2024conceptskilltransferabilitybaseddataselection} clusters data based on conceptual and skill-based representations, while InstructionGPT-4 \citep{wei2023instructiongpt4200instructionparadigmfinetuning} filters a subset of 200 instructions for MiniGPT-4 \citep{zhu2023minigpt4enhancingvisionlanguageunderstanding}, though this approach lacks scalability across different settings. ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage} builds upon LESS \citep{xia2024lessselectinginfluentialdata} to incorporate targeted instruction tuning by leveraging gradient-based specialist influence estimation. DataTailor \citep{yu2024mastering} optimizes data selection based on three key principles—informativeness, uniqueness, and representativeness—ensuring that the most relevant samples are retained.
\noindent
\textbf{Visual Instruction Selection:}  
% Despite the strong performance of MLLMs, the rapid growth of visual instruction datasets has introduced significant redundancy, similar to challenges in LLMs \citep{zhou2024lima,chen2023maybe05dataneeded,xia2024lessselectinginfluentialdata}. State-of-the-art models like BLIP3 \citep{xue2024xgen}, InternVL2.5 \citep{chen2025expandingperformanceboundariesopensource}, and LLaVA-OneVision \citep{li2024llavaonevisioneasyvisualtask} rely on billions of instructions to enhance understanding, but their massive scale leads to substantial computational costs, often requiring hundreds to thousands of GPU hours.  
Despite the strong performance of MLLMs, the rapid growth of visual instruction datasets has introduced significant redundancy, similar to challenges in LLMs \citep{zhou2024lima,chen2023maybe05dataneeded,xia2024lessselectinginfluentialdata}. State-of-the-art models like BLIP3 \citep{xue2024xgen}, InternVL2.5 \citep{chen2025expandingperformanceboundariesopensource}, and LLaVA-OneVision \citep{li2024llavaonevisioneasyvisualtask} rely on billions of instructions to enhance understanding, but their massive scale leads to substantial computational costs, often requiring hundreds to thousands of GPU hours.

% \noindent
% To tackle this issue, various data selection strategies aim to reduce redundancy while preserving model performance. TIVE \citep{liu2024morehighvaluedataselection} selects valuable data at both the task and instance levels using gradient similarity but requires additional downstream training. SELF-FILTER \citep{chen2024visionlanguagemodelstrongfilter} introduces an auxiliary evaluation model that updates its parameters alongside training to prioritize high-value samples. COINCIDE \citep{lee2024conceptskilltransferabilitybaseddataselection} clusters data based on conceptual and skill-based representations, while InstructionGPT-4 \citep{wei2023instructiongpt4200instructionparadigmfinetuning} filters 200 instructions for MiniGPT-4 \citep{zhu2023minigpt4enhancingvisionlanguageunderstanding}, though it lacks scalability. ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage} builds upon LESS \citep{xia2024lessselectinginfluentialdata} to incorporate targeted instruction tuning by leveraging gradient-based specialist influence estimation. DataTailor \citep{yu2024mastering} selects data based on informativeness, uniqueness, and representativeness to retain the most relevant samples.
\noindent To address this, various data selection strategies aim to reduce redundancy while preserving performance. TIVE \citep{liu2024morehighvaluedataselection} selects valuable data based on gradient similarity but requires additional training on downstream tasks. SELF-FILTER \citep{chen2024visionlanguagemodelstrongfilter} uses an auxiliary evaluation model to prioritize high-value samples. COINCIDE \citep{lee2024conceptskilltransferabilitybaseddataselection} clusters data by conceptual and skill-based representations, while InstructionGPT-4 \citep{wei2023instructiongpt4200instructionparadigmfinetuning} filters 200 instructions for MiniGPT-4 \citep{zhu2023minigpt4enhancingvisionlanguageunderstanding}, though it lacks scalability. ICONS \citep{wu2025iconsinfluenceconsensusvisionlanguage} extends LESS \citep{xia2024lessselectinginfluentialdata} by incorporating specialist influence estimation for instruction tuning. DataTailor \citep{yu2024mastering} selects data based on informativeness, uniqueness, and representativeness to retain the most relevant samples.
\section{Conclusion}
% PRISM redefines multimodal data selection by leveraging MLLMs' intrinsic cross-modal alignment, identifying high-value samples through Pearson correlations of token embeddings. Without proxy models or training, PRISM achieves superior performance with 70\% cost reduction across benchmarks while preserving linguistic capabilities. This work establishes a new paradigm for efficient multimodal learning.
PRISM leverages MLLMs' intrinsic cross-modal alignment to select high-value samples using Pearson correlations of token embeddings, requiring no proxy models or training. It achieves 70\% cost reduction while maintaining performance, setting a new standard for efficient multimodal learning.

\section*{Limitations}

A limitation of this work is the static nature of the data selection strategy, which only handles text and image modalities. Extending this approach to include video and sound could introduce challenges due to the temporal and sequential properties of these modalities.

\noindent
Additionally, our method does not incorporate dynamic data selection during training. Adapting the selection process over time could improve model efficiency by focusing on the most relevant data at each stage, particularly for large and diverse datasets.

\bibliographystyle{acl_natbib}
\bibliography{custom}

\newpage

\appendix
\section{Dataset Details}
\label{sec:appendix}

We present the detailed composition of the PRISM-Instruct-250K dataset, which spans multiple visual question answering (VQA), image understanding, and text-based tasks. This diverse selection ensures a comprehensive representation of multimodal learning challenges. The table below shows the distribution of samples across different data sources. 
\begin{table}[h]
\centering
\small
\begin{tabular}{l r}
\toprule
\textbf{Dataset} & \textbf{Number of Samples} \\
\midrule
LLaVA & 53,591 \\
VQAv2 & 27,567 \\
OKVQA & 2,997 \\
A-OKVQA & 22,032 \\
RefCOCO & 16,933 \\
VG & 28,777 \\
GQA & 24,023 \\
OCRVQA & 26,638 \\
TextCaps & 7,311 \\
Text-Only & 40,688 \\
\midrule
\textbf{Total} & 250,557 \\
\bottomrule
\end{tabular}
\caption{Sample distribution of the PRISM-selected Instruct-250K dataset.}
\label{tab:prism_data_distribution}
\end{table}

\section{Model Architectures}
In our experiments, we assess PRISM's transferability across various model architectures and scales, following the methodology outlined in \citep{bi2025llavasteeringvisualinstruction}. The models tested include LLaVA-Vicuna-7B, LLaVA-Phi2-3B, and LLaVA-Vicuna-13B. Each model consists of a vision encoder, a projector, and a language model. The table below summarizes their configurations.

\begin{table}[h]
\centering
\small
\caption{Architectural configurations of the models used in our experiments.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Vision Encoder} & \textbf{Language Model} \\
\midrule
LLaVA-Vicuna-7B  & CLIP ViT-L/14 336px & Vicuna v1.5 7B \\
LLaVA-Phi2-3B    & SigLIP-SO400M-Patch14-384 & Phi-2 2.7B \\
LLaVA-Vicuna-13B & CLIP ViT-L/14 336px & Vicuna v1.5 13B \\
\bottomrule
\end{tabular}%
}
\label{tab:model_arch}
\end{table}


\section{Shannon Entropy and Feature Diversity}
\label{shannon}
Let $F = \{ f_1, f_2, \dots, f_N \}$ denote the set of feature vectors extracted from the dataset $D$, where each $f_i \in \mathbb{R}^d$ corresponds to the averaged token embedding of image $I_i$. The Shannon entropy $H(F)$ of the feature set is defined as:

\begin{equation}
    H(F) = -\sum_{i=1}^{N} p(f_i) \log p(f_i),
\end{equation}

where $p(f_i)$ is the probability density of feature $f_i$. In practice, $p(f_i)$ can be approximated using kernel density estimation or other non-parametric methods. However, directly maximizing $H(F)$ is computationally infeasible for large datasets. Instead, we use the Pearson correlation matrix $P$ as a proxy for feature diversity.

The Pearson correlation matrix $P$ captures pairwise linear dependencies between feature vectors. For a given feature $f_i$, the correlation score is given by:

\begin{equation}
    C_i = \sum_{j=1}^{N} P_{ij},
\end{equation}

which quantifies its overall alignment with the dataset. Low $C_i$ values indicate that $f_i$ is weakly correlated with other features, suggesting that it contributes unique information to the dataset.

Let $F_{\text{selected}} = \{ f_i \mid C_i \leq Q_\tau( \{ C_j \}_{j=1}^{N} ) \}$ denote the subset of features selected by PRISM, where $Q_\tau$ is the $\tau$-th percentile of correlation scores. The entropy of $F_{\text{selected}}$ can be approximated as:

\begin{equation}
    H(F_{\text{selected}}) \approx -\sum_{f_i \in F_{\text{selected}}} p(f_i) \log p(f_i).
\end{equation}

By selecting features with minimal $C_i$, we implicitly minimize the pairwise dependencies within $F_{\text{selected}}$, thereby maximizing the entropy $H(F_{\text{selected}})$. This is because low-correlation features are less likely to share redundant information, leading to a more diverse and informative subset.

We now formalize this intuition with the following theorem:

\textbf{Theorem 1 (Entropy Maximization via Low-Correlation Selection)}: Let $F$ be a set of feature vectors with correlation matrix $P$. For any subset $F_{\text{selected}} \subseteq F$, the Shannon entropy $H(F_{\text{selected}})$ is maximized when $F_{\text{selected}}$ consists of features with minimal row-wise sums:

\begin{equation}
    C_i = \sum_{j=1}^{N} P_{ij}.
\end{equation}

\textbf{Proof}: The proof follows from the properties of Shannon entropy and the definition of Pearson correlation. Let  
$F_{\text{selected}} = \{ f_i \mid C_i \leq Q_\tau( \{ C_j \}_{j=1}^{N} ) \}$. By construction, $F_{\text{selected}}$ contains features that are minimally correlated with the rest of the dataset. This implies that the pairwise dependencies within $F_{\text{selected}}$ are reduced, leading to a higher entropy $H(F_{\text{selected}})$.  

Formally, for any two subsets $F_1$ and $F_2$ with $H(F_1) > H(F_2)$, the features in $F_1$ exhibit lower pairwise correlations on average. Thus, selecting features with minimal $C_i$ ensures that $H(F_{\text{selected}})$ is increased.

The above theorem provides a theoretical foundation for PRISM’s low-correlation selection strategy. By prioritizing features with minimal $C_i$, PRISM ensures that the selected subset $F_{\text{selected}}$ is both diverse and informative, aligning with the principles of information-theoretic feature selection. 



\end{document}