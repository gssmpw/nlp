\documentclass{article}

\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{natbib}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}

\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{multirow}

% custom packages
\usepackage{dsfont}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
pdftitle={},
pdfsubject={},
pdfkeywords={},
pdfborder=0 0 0,
pdfpagemode=UseNone,
colorlinks=true,
linkcolor=mydarkblue,
citecolor=mydarkblue,
filecolor=mydarkblue,
urlcolor=mydarkblue,
}

\input{math_commands.tex}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{problem}{Problem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks}
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: 
%\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
%}

\author{
  Linus Aronsson \\
  Chalmers University of Technology \\
  \texttt{linaro@chalmers.se} \\
  %% examples of more authors
   \And
  Morteza Haghir Chehreghani \\
  Chalmers University of Technology \\
  \texttt{morteza.chehreghani@chalmers.se} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle

\begin{abstract}
Signed networks, where edges are labeled as positive or negative to indicate friendly or antagonistic interactions, offer a natural framework for studying polarization, trust, and conflict in social systems. Detecting meaningful group structures in these networks is crucial for understanding online discourse, political division, and trust dynamics. A key challenge is to identify groups that are cohesive internally yet antagonistic externally, while allowing for neutral or unaligned vertices. In this paper, we address this problem by identifying $k$ polarized communities that are large, dense, and balanced in size. We develop an approach based on Frank-Wolfe optimization, leading to a local search procedure with provable convergence guarantees. Our method is both scalable and efficient, outperforming state-of-the-art baselines in solution quality while remaining competitive in terms of computational efficiency.
\end{abstract}

% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
\label{Introduction}

Signed networks extend traditional graph representations by associating each edge with a positive or negative number, indicating friendly or antagonistic relationships. Originating from studies on social dynamics in the 1950s \citep{10.1307/mmj/1028989917}, signed networks introduce fundamental differences in graph structure that make many algorithms designed for unsigned networks inapplicable \citep{DBLP:journals/csur/TangCAL16, DBLP:conf/cikm/BonchiGGOR19, DBLP:conf/nips/TzengOG20}. These challenges have fueled extensive research in recent years, leading to advances in signed network embeddings, signed clustering, and signed link prediction. We refer to the survey by \citep{DBLP:journals/csur/TangCAL16} for a comprehensive review of these methods. Most relevant to this paper is the problem of signed clustering, which we split into two categories: (i) \emph{signed network partitioning} (SNP), and (ii) \emph{polarized community discovery} (PCD). The latter is the problem studied in this paper.

The goal of signed clustering is to identify $k$ conflicting groups (clusters) where intra-cluster similarity is maximized (predominantly positive) and inter-cluster similarity is minimized (predominantly negative). This problem has numerous real-world applications \citep{DBLP:journals/csur/TangCAL16}, particularly in social networks, where vertices represent individuals and edges capture friendly or antagonistic relationships (e.g., shared or opposing political views). Detecting conflicting groups in such networks is crucial for analyzing polarization \citep{polarization, polarizationtwitter, DBLP:conf/www/XiaoOG20}, echo chambers \citep{echo, 10.1093/poq/nfw006}, and the spread of misinformation \citep{DBLP:journals/sigkdd/ShuSWTL17, alma997565503602341, DBLP:conf/aaai/YangSWG0019}.

In the SNP problem, the $k$ groups must form a partition of the vertices, meaning every vertex must be included. Spectral methods based on the signed Laplacian have been widely used to tackle this problem \citep{DBLP:conf/sdm/KunegisSLLLA10, DBLP:conf/cikm/ChiangWD12, DBLP:conf/icml/MercadoT019, DBLP:conf/aistats/CucuringuDGT19}. Alternatively, formulating SNP explicitly as an optimization problem leads to the well-studied \emph{correlation clustering} (CC) problem \citep{DBLP:journals/ml/BansalBC04}, which is known to be APX-hard. Consequently, numerous approximation algorithms have been developed \citep{DBLP:journals/ml/BansalBC04, CharikarGW05, DemaineEFI06, AilonCN08}, with local search methods standing out for their strong performance in both clustering quality and computational efficiency \citep{DBLP:conf/aaai/ThielCD19, Chehreghani22_shift, DBLP:journals/tmlr/AronssonC24, aronsson2024informationtheoreticactivecorrelationclustering}. % A distinctive feature of CC is that the number of clusters is determined automatically, although a variant where $k$ is specified as input has also been explored \citep{DBLP:journals/toc/GiotisG06,DBLP:journals/jmlr/ChehreghaniBB12}.

The problem formulation of PCD is identical to that of SNP, \emph{except that the $k$ clusters are not required to form a partition of the vertices, allowing some vertices to remain unassigned.} The goal is therefore to only find the \emph{dense} subgraphs of polarized communities. This accounts for cases where certain vertices are neutral w.r.t. the underlying conflicting group structure. For example, in a social network with a heated political debate, many users may not engage in the dispute, and their interactions might not align with any specific faction. There is a substantial body of work addressing this problem, but most approaches focus on identifying only two communities \citep{DBLP:conf/cikm/BonchiGGOR19, DBLP:conf/www/XiaoOG20, DBLP:conf/www/OrdozgoitiMG20, DBLP:journals/pvldb/FazzoneLDTB22, DBLP:conf/www/NiuS23, DBLP:journals/ml/GulloMT24}. As a result, they do not easily generalize to arbitrary $k$. To our knowledge, only two works specifically tackle PCD for arbitrary $k$. \citep{DBLP:conf/kdd/ChuWPWZC16} formulated the task as a constrained quadratic optimization problem and proposes an efficient algorithm that iteratively refines small subgraphs, avoiding the costly computation of the full adjacency matrix. \citep{DBLP:conf/nips/TzengOG20} introduced a spectral method based on maximizing a discrete Rayleigh quotient, which extends the seminal work of \citep{DBLP:conf/cikm/BonchiGGOR19} to accommodate arbitrary $k$. Their method is known to produce highly imbalanced communities in terms of size \citep{DBLP:journals/ml/GulloMT24}.

The main contributions of this paper are as follows:
(i) We propose a novel formulation for the PCD problem that encourages more balanced communities, addressing a key limitation of \citep{DBLP:conf/nips/TzengOG20}.
(ii) We demonstrate that this formulation is particularly well-suited for optimization via the block-coordinate Frank-Wolfe algorithm \citep{fw, DBLP:conf/icml/Lacoste-JulienJSP13}.
(iii) We establish a simple yet effective equivalent local search method with a provable convergence rate.
(iv) We propose techniques that allow the local search method to scale to large networks.
(v) Finally, through extensive experiments, we show that our approach consistently outperforms state-of-the-art baselines.

%We develop scalable techniques that enable this local search algorithm to handle large networks efficiently.

%\citep{DBLP:conf/kdd/ChuWPWZC16, DBLP:conf/cikm/BonchiGGOR19, DBLP:conf/nips/TzengOG20, DBLP:conf/www/XiaoOG20, DBLP:conf/www/OrdozgoitiMG20, DBLP:journals/pvldb/FazzoneLDTB22, DBLP:conf/www/NiuS23, DBLP:journals/ml/GulloMT24}

\section{Problem Formulation} \label{section:problem formulation}

We start by introducing the relevant notation, followed by an introduction to correlation clustering (CC), which is connected to our problem. Finally, we present the problem of polarized community discovery (PCD). %, which is the focus of this paper.

\subsection{Notation}

Consider a signed network $G = (V, E)$, where $V$ is the set of objects and $E$ the set of edges. The weight of an edge $(i, j) \in E$ is represented by the element $A_{i,j} \in \{-1,0,+1\}$ of an adjacency matrix $A$. The matrix $A$ is symmetric with zeros on the diagonal, which means $A_{i,j} = A_{j,i}$ and $A_{i,i} = 0$. We use $A_{i,:}$ and $A_{:,j}$ to denote row $i$ and column $j$ of $A$, respectively. While we restrict all similarities to be in $\{-1,0,+1\}$ (for clarity), all methods presented in the paper extend to arbitrary similarities in $\mathbb{R}$. We can decompose the adjacency matrix as $A = A^+ - A^-$ where $A^+ = \max(A, 0)$ and $A^- = \max(-A, 0)$. A clustering with $k$ clusters is denoted $S_{[k]} = \{S_1,\dots,S_k\}$, where each $S_m \subseteq V$ is the set of objects assigned to cluster $m \in [k] = \{1,\dots,k\}$. Let $N^+_{\text{intra}} = \sum_{m \in [k]} \sum_{i,j \in S_m} A^+_{i,j}$ and $N^-_{\text{intra}} = \sum_{m \in [k]} \sum_{i,j \in S_m} A^-_{i,j}$ be the sum of positive and absolute negative intra-cluster similarities, respectively. Furthermore, let $N^+_{\text{inter}} = \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{i \in S_m} \sum_{j \in S_p} A^+_{i,j}$ and $N^-_{\text{inter}} = \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{i \in S_m} \sum_{j \in S_p} A^-_{i,j}$ be the sum of positive and absolute negative inter-cluster similarities, respectively.

%Consider a signed network $G = (V, E)$, where $V$ is the set of objects and $E$ the set of edges. Each edge $(i, j) \in E$ has a weight $A_{i,j} \in \{-1,0,+1\}$, given by the symmetric adjacency matrix $A$ with zeros on the diagonal, i.e., $A_{i,j} = A_{j,i}$ and $A_{i,i} = 0$. We denote row $i$ and column $j$ of $A$ as $A_{i,:}$ and $A_{:,j}$, respectively. While we restrict similarities to $\{-1,0,+1\}$ for clarity, all methods in this paper extend to arbitrary similarities in $\mathbb{R}$. The adjacency matrix can be decomposed as $A = A^+ - A^-$, where $A^+ = \max(A, 0)$ and $A^- = \max(-A, 0)$. A clustering with $k$ clusters is represented as $S_{[k]} = \{S_1,\dots,S_k\}$, where each cluster $S_m \subseteq V$ contains objects assigned to cluster $m \in [k] = \{1,\dots,k\}$. The sum of positive and absolute negative intra-cluster similarities are given by $N^+_{\text{intra}} = \sum_{m \in [k]} \sum_{i,j \in S_m} A^+_{i,j}$ and $N^-_{\text{intra}} = \sum_{m \in [k]} \sum_{i,j \in S_m} A^-_{i,j}$, respectively, while the sum of positive and absolute negative inter-cluster similarities are $N^+_{\text{inter}} = \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{i \in S_m} \sum_{j \in S_p} A^+_{i,j}$ and $N^-_{\text{inter}} = \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{i \in S_m} \sum_{j \in S_p} A^-_{i,j}$.


\subsection{Correlation Clustering}

We begin by noting that for CC, unlike PCD to be discussed in the next subsection, a clustering $S_{[k]}$ is a partition of $V$, meaning $V = \bigcup_{m \in [k]} S_m$ and each $S_m$ is disjoint. A notable feature of CC is its ability to automatically determine the number of clusters based on the similarities encoded in $A$. However, a variant of this problem has also been studied where the number of clusters, $k$, is specified as an input \citep{DBLP:journals/toc/GiotisG06}. Automatic detection of the number of clusters could be a desirable property of a clustering algorithm. However, constraining the number of clusters to $k$ can act as a form of regularization, and has been shown to produce higher-quality clusters in many scenarios \citep{DBLP:journals/jmlr/ChehreghaniBB12}. Given this, the $k$-CC problem can be defined as shown below.
%
\begin{problem}[$k$-CC] \label{problem-kcc}
Find a clustering $S_{[k]}$ that maximizes 
\begin{equation} \label{eq:fullcc}
   N^+_{\text{intra}} - N^-_{\text{intra}} + N^-_{\text{inter}} - N^+_{\text{inter}}.
\end{equation}
\end{problem}
%
In other words, we want to find a clustering that maximizes the sum of intra-cluster similarities and minimizes the sum of inter-cluster similarities. The following proposition presents alternative objectives equivalent to maximizing Eq. \ref{eq:fullcc}. While this is known in the CC literature \citep{ethz-a-010077098}, we include a complete summary here to better contextualize our problem.

\begin{restatable}{proposition}{propcc} \label{prop:propcc}
Problem \ref{problem-kcc} is equivalent to finding a clustering $S_{[k]}$ that maximizes any one of the four objectives below\footnote{By equivalent, we mean they share all local maxima, including the global maximum.% In other words, the objectives differ by only constant terms.
}.
%
\begin{equation} \label{eq:maxagree}
    N^+_{\text{intra}} + N^-_{\text{inter}}
\end{equation}
\begin{equation} \label{eq:mindisagree}
    -N^-_{\text{intra}} - N^+_{\text{inter}}
\end{equation}
\begin{equation} \label{eq:maxcorr}
    N^+_{\text{intra}} - N^-_{\text{intra}}
\end{equation}
\begin{equation} \label{eq:mincut}
    N^-_{\text{inter}} - N^+_{\text{inter}}
\end{equation}
%\begin{equation} \label{eq:fullcc}
%   N^+_{\text{intra}} - N^-_{\text{intra}} + N^-_{\text{inter}} - N^+_{\text{inter}}
%\end{equation}
%
Furthermore, maximizing any other combination of the four terms is not equivalent to Problem \ref{problem-kcc}. 
\end{restatable}

All proofs can be found in Appendix \ref{appendix:proofs}. CC is typically formulated as either maximizing agreements (Eq. \ref{eq:maxagree}) or minimizing disagreements (Eq. \ref{eq:mindisagree}) \citep{DBLP:conf/kdd/BonchiGL14}. It can also be framed in terms of maximizing intra-cluster similarities, referred to as \emph{max correlation} (Eq. \ref{eq:maxcorr}), or minimizing inter-cluster similarities (Eq. \ref{eq:mincut}). 
%, often called \emph{minimum cut}\footnote{Minimum cut is traditionally applied to unsigned networks \citep{Chehreghani22_shift}.} (Eq. \ref{eq:mincut}). 
Finally, we can combine all these notions into one single objective (i.e., Eq. \ref{eq:fullcc}). 
% 
CC is an NP-hard problem, leading to the development of numerous approximation algorithms. Existing approximation algorithms maximize one of the five expressions above, leading to differences in clustering performance, computational complexity and theoretical performance guarantees.  

\subsection{Polarized Community Discovery} \label{section:pcd}

The problem of PCD is similar to CC in that the goal is to identify $k$ clusters $S_1, \dots, S_k$, where the similarity within each cluster is large (and positive) and the similarity between different clusters is small (and negative). However, unlike CC, we also allow objects to remain neutral, introducing an additional \emph{neutral} set $S_0$. This means an object can either be assigned to one of the non-neutral clusters $S_1, \dots, S_k$ or designated as neutral by assigning it to $S_0$. Assigning an object to $S_0$ effectively excludes it from influencing the objective function. Specifically, assigning an object $i \in V$ to $S_0$ is equivalent to setting both row $A_{i,:}$ and column $A_{:,i}$ of the matrix $A$ to zero. Consequently, a clustering $S_{[k]}$ is no longer a partition of $V$ and we have $S_0 = V \setminus \bigcup_{m \in [k]} S_m$ (although all clusters are still disjoint). In our view, the goal of PCD is to extract $k$ non-neutral clusters that are (i) large, (ii) balanced, and (iii) dense. Dense clusters imply that each object should strongly align with its assigned cluster—being highly similar to most objects within its cluster and markedly dissimilar to those in other clusters. Objects lacking a clear association, such as those similar to objects in multiple clusters or those that are inherently neutral (e.g., low-degree objects), should be labeled as neutral by assigning them to $S_0$. We notice that a natural trade-off exists between the size of the non-neutral clusters and their density, as very small clusters can achieve high density trivially.

We begin this section by explaining why Eq.~\ref{eq:fullcc}, which incorporates all relevant terms, must be considered when neutral objects are allowed. Much prior work on PCD also optimize all terms, but often without providing a detailed justification for this choice. The next proposition provides such an intuition.
%
\begin{restatable}{proposition}{notequiv} \label{prop:notequiv}
A clustering $S_{[k]}$ with neutral objects $S_0 = V \setminus \bigcup_{m \in [k]} S_m$ that maximizes one of the objectives in Eqs.~\ref{eq:fullcc}-\ref{eq:mincut} is not guaranteed to maximize any of the other objectives\footnote{Unless $k = 2$, in which case Eq. \ref{eq:maxagree} and Eq. \ref{eq:fullcc} are equivalent as established in \citep{DBLP:conf/cikm/BonchiGGOR19}.}.
\end{restatable}
%
From Proposition \ref{prop:notequiv}, we conclude that each term in Eq. \ref{eq:fullcc} provides unique information when neutral objects are allowed, unlike the standard CC problem, where the different objectives are equivalent, as outlined by Proposition \ref{prop:propcc}. This makes Eq. \ref{eq:fullcc} the most reasonable objective for optimization in this context, as it effectively balances all contributing terms. Moreover, since each term captures unique aspects of the PCD problem, it may be beneficial to \emph{weight them differently} to achieve an optimal trade-off. Furthermore, \citep{DBLP:conf/cikm/BonchiGGOR19} showed that for $k = 2$, an optimal solution to Eq. \ref{eq:fullcc} consists of no neutral objects, i.e., $S_0 = \emptyset$. While we cannot directly extend this conclusion to $k > 2$ (see proof of Proposition \ref{prop:notequiv}), it is evident that even for $k > 2$, an object may be assigned to a non-neutral cluster as long as it marginally improves the objective in Eq.~\ref{eq:fullcc}, even if it does not maintain the density of the graph induced by non-neutral clusters $S_{[k]}$. In other words, low-degree objects that should ideally remain neutral may be included in non-neutral clusters.


%However, see the next proposition. %For $k > 2$, this claim no longer holds, as demonstrated by the next proposition%as noted in \citep{DBLP:conf/cikm/BonchiGGOR19}, when $k = 2$, there exists a clustering $S_{[k]}$ with no objects assigned to the neutral set, i.e., $S_0 = \emptyset$, that maximizes Eq. \ref{eq:fullcc}. For $k > 2$, this claim no longer holds, as demonstrated by the next proposition.

%\begin{proposition} \label{prop:neutral} When $k = 2$, there exists a clustering $S_{[k]}$ in which no objects are assigned to the neutral set, i.e., $S_0 = \emptyset$, that maximizes Eq. \ref{eq:fullcc} \citep{DBLP:conf/cikm/BonchiGGOR19}. In contrast, for $k > 2$, the clustering $S_{[k]}$ that maximizes Eq. \ref{eq:fullcc} may include neutral objects that strictly improve the objective\footnote{A neutral object that strictly improves the objective implies that assigning it to a non-neutral cluster would result in a strictly lower value for the objective.}. \end{proposition}
%
%\begin{proposition} \label{prop:neutral} %Given a clustering $S_{[k]}$ with neutral objects $S_0 = V \setminus \bigcup_{m \in [k]} S_m$, if $k = 2$, one can always find a clustering $\tilde{S}_{[k]}$ with neutral objects $\tilde{S}0 = V \setminus \bigcup{m \in [k]} \tilde{S}m$ where $\tilde{S}0 \subseteq S_0$, such that $f(\tilde{S}{[k]}) \geq f(S{[k]})$ \citep{DBLP:conf/cikm/BonchiGGOR19}. However, if $k > 2$, this is no longer guaranteed. %\end{proposition}


Based on the above discussion, we conclude that (i) it may be beneficial to weight the terms in Eq.~\ref{eq:fullcc} differently and (ii) we must encourage the presence of neutral objects by penalizing large/sparse non-neutral clusters. In prior work, these concerns are typically addressed by weighting inter-cluster terms with a parameter $\alpha \in \mathbb{R}$ and normalizing Eq. \ref{eq:fullcc} by the number of non-neutral objects, i.e., 
%
\begin{equation} \label{eq:polarity}
\frac{(N^+_{\text{intra}} - N^-_{\text{intra}}) + \alpha(N^-_{\text{inter}} - N^+_{\text{inter}})}{\sum_{m \in [k]} |S_m|}.
\end{equation}
%
If $\alpha = 1/(k-1)$, Eq. \ref{eq:polarity} is commonly referred to as \emph{polarity} in prior work and is a well-established objective for PCD \citep{DBLP:conf/cikm/BonchiGGOR19, DBLP:conf/nips/TzengOG20}. However, as highlighted in \citep{DBLP:journals/ml/GulloMT24}, maximizing polarity often results in highly imbalanced clustering solutions. In particular, clustering solutions with the same polarity can differ significantly in terms of cluster balance. A concrete example illustrating this issue is provided in Appendix \ref{appendix:polarity}. 

In this paper, we propose an alternative objective that, instead of normalizing by the number of non-neutral objects, incorporates a regularization term by subtracting the sum of squared sizes of the non-neutral clusters\footnote{For simplicity, we follow prior work by only weighting the inter-cluster terms.}:
%
\begin{equation} \label{eq:ours}
(N^+_{\text{intra}} - N^-_{\text{intra}}) + \alpha(N^-_{\text{inter}} - N^+_{\text{inter}})  - \beta \sum_{m \in [k]} |S_m|^2.
\end{equation}
%
The third term in Eq. \ref{eq:ours} has been previously applied to the minimum cut objective for unsigned networks \citep{Chehreghani22_shift} where in practice, however, they use a different penalty: they shift the pairwise similarities so that the sum of the rows and columns of the similarity matrix becomes zero.
%The additive penalty term in Eq. \ref{eq:ours} has been previously explored in the context of full network partitioning. In particular, \citep{Chehreghani22_shift} introduced it as a penalty term to the minimum cut objective for unsigned networks to promote more balanced partitions. 
In our context (i.e., for the PCD problem), this objective achieves two goals simultaneously: it penalizes the formation of (i) large/sparse and (ii) imbalanced non-neutral clusters. The second property is easy to see, as for a clustering with $k$ clusters and $n$ objects, the term $\sum_{m \in [k]} |S_m|^2$ is minimized when each cluster is assigned $\frac{n}{k}$ objects 
%\footnote{If $\frac{n}{k}$ is not an integer, the term is minimized when the cluster sizes are as close to $\frac{n}{k}$ as possible.}
(i.e., the clusters are perfectly balanced). The parameter $\beta \in \mathbb{R}$ allows us to easily trade-off the size of non-neutral clusters and their density, which is a desirable property in this context as discussed in the beginning of this section. As we will demonstrate in the subsequent sections, maximizing our objective with varying values of $\beta$ allows us to generate a range of clustering solutions with different cluster sizes and corresponding densities. This enables the selection of a solution that offers the most desirable trade-off, and is absent in the existing methods that are based on Eq. \ref{eq:polarity}. Finally, we introduce regularization as an additive term rather than a normalization for three key reasons: (i) It allows for a flexible trade-off between cluster size and density. (ii) It enables the development of efficient optimization procedures with strong convergence guarantees. (iii) As we will demonstrate, it results in superior performance in practice. We are now ready to formally state our problem.
%
\begin{problem}[$k$-PCD] \label{problem-kpcd}
Find a clustering $S_{[k]}$ with neutral objects $S_0 = V \setminus \bigcup_{m \in [k]} S_m$ that maximizes Eq. \ref{eq:ours}.
\end{problem}
%
We start by highlighting the computational complexity of the problem with the following theorem.
%
\begin{restatable}{theorem}{nphard}\label{prop:nphard}
Problem \ref{problem-kpcd} (i.e., $k$-PCD) is NP-hard.
\end{restatable}
%
Theorem \ref{prop:nphard} underscores the necessity of approximate methods to solve Problem \ref{problem-kpcd}. % The next section is dedicated to this.

%&In this paper, we illustrate how Frank-Wolfe optimization \citep{fw} can be used effectively for this purpose, which we discuss in the next section.

\section{Algorithms} \label{section:algorithms}

In this section, we demonstrate how our problem can be solved using Frank-Wolfe (FW) optimization \citep{fw}. Specifically, we consider a variant called block-coordinate FW, which we begin by describing in the next subsection. After this, we establish its equivalence to a straightforward and provably efficient local search procedure. Next, we analyze the convergence rate of this approach. Following that, we propose practical enhancements to improve scalability, enabling the method to handle large problems. Finally, we provide a detailed analysis of the impact of $\alpha$ and $\beta$.


\subsection{Block-Coordinate Frank-Wolfe Optimization}

\begin{algorithm}[tb]
   \caption{Block-coordinate Frank-Wolfe Optimization}
   \label{alg:blockfw}
\begin{algorithmic}[1]
   \STATE Initialize $\vx_{[n]}^{(0)} \in \mathcal{D}^{(1)} \times \mathcal{D}^{(2)} \times \dots \times \mathcal{D}^{(n)}$.
   \FOR{$t \coloneqq 0,\dots,T$}
        \STATE Select a random block $i \in [n]$
        \STATE $\vx_i^{\ast} \coloneqq \argmax_{\vx_i \in \mathcal{D}^{(i)}} \vx_i \cdot \nabla_i f(\vx_{[n]}^{(t)})$
        \STATE Let $\gamma \coloneqq \frac{2n}{t + 2n}$ or optimize $\gamma$ by line-search
        \STATE $\vx^{(t+1)}_i \coloneqq (1-\gamma)\vx^{(t)}_i + \gamma\vx_i^{\ast}$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

The Frank–Wolfe (FW) algorithm is one of the earliest methods for nonlinear constrained optimization \citep{fw}. In recent years, it has regained popularity, particularly in machine learning, due to its scalability \citep{DBLP:conf/icml/Jaggi13}. In this paper, we use a variant of this method called block-coordinate FW \citep{DBLP:conf/icml/Lacoste-JulienJSP13}. This method yields a significantly faster optimization procedure while enjoying similar theoretical guarantees. Block-coordinate FW is applied to problems where the feasible domain can be split into blocks $\mathcal{D} = \mathcal{D}^{(1)} \times \dots \times \mathcal{D}^{(n)} \subseteq \mathbb{R}^d$, where each $\mathcal{D}^{(i)} \subseteq \mathbb{R}^{d_i}$ is convex and compact and we have $d = \sum_{i=1}^{n}d_i$. Let $\vx_{[n]}$ denote the concatenation of the variables $\vx_i \in \mathcal{D}^{(i)}$ from all blocks $i \in [n]$. The optimization problem is then
%
\begin{equation} \label{eq:fw}
    \max_{\vx_{[n]} \in \mathcal{D}^{(1)} \times \dots \times \mathcal{D}^{(n)}} f(\vx_{[n]}),
\end{equation}
%
where $f$ is a differentiable function with an $L$-Lipschitz continuous gradient. This approach is particularly effective when optimizing $f$ w.r.t. the variables in a single block (while keeping other blocks fixed) is simple and efficient. This turns out to be the case for our problem, as will be discussed in the remainder of this section. The method is outlined in Alg. \ref{alg:blockfw}, where $\nabla_i f(\vx_{[n]})$ represents the gradient of $f(\vx_{[n]})$ with respect to block $\vx_i$. When the problem involves only a single block ($n = 1$), Alg. \ref{alg:blockfw} reduces to the standard FW algorithm. We now show how our problem can be turned into an instance of Eq. \ref{eq:fw}. In the following proposition, we show an alternative way of writing our objective in Eq. \ref{eq:ours}.
%
\begin{restatable}{proposition}{shift} \label{prop:shift}
Our objective in Eq. \ref{eq:ours} can be written as
%
\begin{equation} \label{eq:shift}
    \sum_{m \in [k]} \sum_{i,j \in S_m} (A_{i,j} - \beta) - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j}.
\end{equation}
%
\end{restatable}
%
We observe that the regularization term in Eq. \ref{eq:ours} is equivalent to shifting the intra-cluster similarities by $-\beta$. This reformulation proves highly useful for the remainder of this section. We now describe how our problem is well-suited for block-coordinate FW. In our context, each object $i \in [n]$ defines a block. We represent the cluster membership of object $i$ using $\vx_i \in \{\ve_0, \dots, \ve_k\}$, where $\ve_m$ (for $m \in \{0, \dots, k\}$) are the standard basis vectors. Each $\vx_i$ is a vector of dimension $k+1$, with index zero indicating membership in the neutral set $S_0$. Specifically, if $x_{i0} = 1$, object $i$ is assigned to $S_0$. Using this notation, we can now define our objective as follows.
%
%A related result was established by \citep{Chehreghani22_shift}, who demonstrated that optimizing the minimum cut objective regularized with $-\beta \sum_{m \in [k]} |S_m|^2$ is equivalent to optimizing the max correlation objective (Eq. \ref{eq:maxcorr}) with similarities shifted by $\beta$, specifically in the context of full network partitioning of unsigned networks. 
\begin{equation} \label{eq:relaxedpcd}
%\begin{split}
    f(\vx_{[n]}) = \sum_{(i,j) \in E} \sum_{m \in [k]} x_{im} x_{jm} (A_{i,j} - \beta) - \alpha \sum_{(i,j) \in E} \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} x_{im} x_{jp} A_{i,j} .
%\end{split}
\end{equation}
%
Note that we do not include any terms involving $x_{i0}$, thereby excluding contributions from neutral objects, as intended. The objective in Eq. \ref{eq:relaxedpcd} remains discrete and is therefore unsuitable for FW optimization. To address this, we relax the problem to make it continuous by allowing soft cluster memberships. Specifically, each $\vx_i \in \Delta^{k+1}$, where $\Delta^{k+1} = \{\vx \in \mathbb{R}^{k+1} \mid x_m \geq 0, \sum_{m=0}^k x_m = 1\}$ represents the simplex of dimension $k$. With this relaxation, we can now reformulate the optimization problem as follows.
%
\begin{equation} \label{eq:opt}
    \max_{\vx_i \in \Delta^{k+1}, \forall i \in [n]} f(\vx_{[n]})
\end{equation}
%
Equation \ref{eq:opt} is a specific instance of the block-coordinate FW formulation described in Eq. \ref{eq:fw} (where $f$ is non-concave). Consequently, we can apply Alg. \ref{alg:blockfw} to solve this problem.

\subsection{Equivalence to a Local Search Approach}

\begin{algorithm}[tb]
   \caption{Local Search for PCD}
   \label{alg:ls}
\begin{algorithmic}[1]
    \STATE Randomly assign each object $i \in [n]$ to one of the clusters in $S_{[k]}$, or make it neutral by adding it to $S_0$
    \WHILE{not converged}
    \STATE Select object $i \in [n]$ uniformly at random
    \STATE Assign object $i$ to a cluster (or make it neutral) which maximally increases our objective in Eq. \ref{eq:ours}.
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

We now show that optimizing Eq. \ref{eq:opt} using Alg. \ref{alg:blockfw} is equivalent to the local search procedure in Alg. \ref{alg:ls}. Let matrix $G \in \mathbb{R}^{n \times (k+1)}$, where element $G_{i,m} \triangleq [\nabla_i f(\vx^{(t)}_{[n]})]_m$ is the gradient of $f(\vx_{[n]})$ w.r.t. variable $m$ of block $i$ evaluated at $\vx^{(t)}_{[n]}$ (solution at step $t$ of Alg. \ref{alg:blockfw}). Given this, we present the following theorem.
%
\begin{restatable}{theorem}{discrete}\label{prop:discrete}
If $\vx_{[n]}^{(0)}$ in Alg \ref{alg:blockfw} is discrete, the following holds.
(a) For our problem (Eq. \ref{eq:opt}), the solution $\vx^{\ast}_i$ (line 4 of Alg. \ref{alg:blockfw}) is the basis vector $\ve_{p}$, where $p = \argmax_{m \in \{0,\dots,k\}} G_{i,m}$ and the optimal value of the step size on line 6 is $\gamma = 1$.
(b) Our objective function in Eq. \ref{eq:relaxedpcd} satisfies $(\vx^{\ast}_i - \vx^{(t)}_i) \cdot G_{i,:} = f(\vx^{\ast}_{[n]}) - f(\vx_{[n]}^{(t)})$, where $\vx^{\ast}_{[n]}$ is $\vx^{(t)}_{[n]}$ with block $i$ modified to $\vx^{\ast}_i$.
%(c) For our problem (Eq. \ref{eq:opt}), the optimal value of the step size on line 6 is $\gamma = 1$.
\end{restatable}
%
%\begin{proposition} \label{theorem:multilinea} 
%The function $f(\vx_{[n]})$ as defined in Eq. \ref{eq:relaxedpcd} is multilinear in each block $\vx_i$. 
%\end{proposition}
%\begin{proof}
%Let $f(\vx_i)$ denote the function $f(\vx_{[n]})$ when we treat all variables in all other blocks not equal to $i$ as constants. We have
%
%\begin{equation} \label{eq:multilinear}
%\begin{split}
%    &f(\vx_i) = -\sum_{m \in [k]} x_{im}\beta + 2\sum_{j \in [n] \setminus \{i\}}\sum_{m \in [k]} x_{im} x_{jm} (A_{i,j} - \beta) \\
%    & - 2\alpha\sum_{j \in [n] \setminus \{i\}}  \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} x_{im} x_{jp} A_{i,j} + C,
%\end{split}
%\end{equation}
%
%where the constant $C$ correspond to terms that do not involve $\vx_i$. We can rearrange the sums and obtain $f(\vx_i) = x_{i0} c_{i0} + \sum_{m \in [k]} x_{im} c_{im} + C$, where $c_{i0} = 0$ and $c_{im} = -\beta + 2\sum_{j \in [n] \setminus \{i\}} (x_{jm} (A_{i,j} - \beta) - \alpha\sum_{p \in [k] \setminus \{m\}} x_{jp} A_{i,j})$ for $m \in [k]$. Since each $c_{im}$ does not depend on $x_{im}$, we see that $f(\vx_i)$ is linear w.r.t. each $x_{im}$, which we wanted to show.
%\end{proof}
%

From part (a) of Theorem \ref{prop:discrete}, the current solution, $\vx^{(t)}_{[n]}$, remains discrete (i.e., hard cluster assignments) at every step of Alg. \ref{alg:blockfw} for all $i \in [n]$. Moreover, each step of Alg. \ref{alg:blockfw} consists of placing object $i$ in the cluster $m \in \{0,\dots,k\}$ with maximal gradient $G_{i,m}$. By part (b) of Theorem \ref{prop:discrete}, this is equivalent to placing object $i$ in the cluster that maximally improves our objective in Eq. \ref{eq:ours}. Based on this, we conclude the following corollary.

%From Theorem \ref{prop:grad}, we understand the process of optimizing Eq. \ref{eq:opt} using Alg. \ref{alg:blockfw} as follows: First, select a random object $i \in [n]$ and compute its gradient, $G_{i,:} = \nabla_i f(\vx^{(t)}_{[n]})$. The value of $G_{i,m}$ represents the contribution of object $i$ to the objective function in Eq. \ref{eq:relaxedpcd} if it is assigned to cluster $m$. Next, assign object $i$ to the cluster $m \in \{0, \dots, k\}$ that maximizes $G_{i,m}$. Since the contribution to the neutral cluster is always $G_{i,0} = 0$, it follows that object $i$ is assigned to $S_0$ if and only if its contributions to all non-neutral clusters are negative. Moreover, this process ensures that the current solution, $\vx^{(t)}_i$, remains discrete (i.e., hard cluster assignments) at every step of Alg. \ref{alg:blockfw} for all $i \in [n]$. Based on this, we conclude the following corollary.
%
\begin{restatable}{corollary}{equivalent}\label{cor:equivalent}
From Theorem \ref{prop:discrete}, if $\vx_{[n]}^{(0)}$ is discrete, solving the optimization problem in Eq. \ref{eq:opt} using Alg. \ref{alg:blockfw} is equivalent to executing the local search procedure described in Alg. \ref{alg:ls}.
\end{restatable}
%
Given this, we now present results for the convergence rate of Alg. \ref{alg:ls}.

\subsection{Convergence Analysis}

Following the prior work on the analysis of general FW algorithms \citep{DBLP:conf/icml/Jaggi13, DBLP:conf/icml/Lacoste-JulienJSP13}, we begin by providing the following definitions.
%
\begin{definition}[FW duality gap] \label{def:dg}
    The FW duality gap is defined as \citep{DBLP:conf/icml/Jaggi13}
    %
    \begin{equation}
        g(\vx_{[n]}) = \max_{\vs_{[n]} \in \mathcal{D}} (\vs_{[n]} - \vx_{[n]}) \cdot \nabla f(\vx_{[n]}),
    \end{equation}
    %
    which is zero if and only if $\vx_{[n]}$ is a stationary point. Furthermore, let $\tilde{g}_t = \min_{0\leq l \leq t-1} g(\vx^{(l)}_{[n]})$ be the smallest duality gap observed in Alg. \ref{alg:blockfw} up until step $t$.
\end{definition}
%
\begin{definition}[Convergence rate] \label{def:cr}
     We say the convergence rate of Alg. \ref{alg:blockfw} is at least $O(1/r_t)$ if $\mathbb{E}[\tilde{g}_t] \leq O(1/r_t)$, where $r_t$ is some expression involving only $t$ and the expectation is w.r.t. the random selection of blocks on line 3. If $n = 1$ the bound is deterministic.
\end{definition}
%
%\begin{definition}[$\epsilon$-convergence]
%      We say Alg. \ref{alg:blockfw} converges in at most $O(1/r_\epsilon)$ steps, if $\mathbb{E}[\tilde{g}_t]$ becomes $\epsilon$-close to zero in at most $O(1/r_\epsilon)$ steps, where $r_\epsilon$ is some expression involving only $\epsilon$. Again, if $n = 1$ the bound is deterministic.
%\end{definition}
%
The FW algorithm has been shown to converge to a stationary point of $f$ under various settings, with well-established convergence rates. We summarize a few known results below. The standard FW algorithm ($n = 1$) achieves a deterministic convergence rate of $O(1/t)$ for concave $f$ \citep{fw} and $O(1/\sqrt{t})$ for non-concave $f$ \citep{DBLP:journals/corr/Lacoste-Julien16, DBLP:conf/allerton/ReddiSPS16}. For the block variant, \citep{DBLP:conf/icml/Lacoste-JulienJSP13} prove a convergence rate of $O(1/t)$ for concave $f$ in expectation. For non-concave $f$, \citep{DBLP:conf/aaai/ThielCD19} prove a convergence rate of $O(1/t)$ in expectation, under the assumption that $f(\vx_{[n]})$ is multilinear in each block $\vx_i$ %(meaning it is linear in each variable in $\vx_i$ separately, when all other blocks are fixed), 
including correlation clustering. 
%\citep{DBLP:conf/aaai/ThielCD19} have established a convergence rate of $O(1/t)$ for Problem \ref{problem-kcc} ($k$-CC) using local search for correlation clustering. %procedure similar to Alg.~\ref{alg:ls}. 
We here extend the analysis of \citep{DBLP:conf/aaai/ThielCD19} to Problem \ref{problem-kpcd} ($k$-PCD) using Alg.~\ref{alg:ls}, described in Theorem \ref{theorem:convergence}. Note that their analysis cannot be applied directly to our objective function in Eq. \ref{eq:relaxedpcd} as this objective does not satisfy the multilinearity property.   %,  noting that their analysis cannot be applied directly to our setting. 
%
\begin{restatable}{theorem}{convergence}\label{theorem:convergence}
The convergence rate of Alg.~\ref{alg:ls} is at least $nh_0/t = O(1/t)$, where $h_0 = \sum_{(i,j) \in E} |A_{i,j}|$.
\end{restatable}
%
The $O(1/t)$ convergence rate presented in Theorem~\ref{theorem:convergence} should be compared with the deterministic convergence rate of $O(1/\sqrt{t})$ for general non-concave functions $f$ under the standard FW method ($n = 1$) \citep{DBLP:journals/corr/Lacoste-Julien16, DBLP:conf/allerton/ReddiSPS16}. %From Theorem \ref{theorem:convergence}, we have the following corollary.
%
%\begin{corollary} \label{cor:eps}
%From Theorem \ref{theorem:convergence}, we conclude that Alg. \ref{alg:ls} converges in at most $nh_0/\epsilon = O(1/\epsilon)$ steps.
%\end{corollary}
%
\subsection{Improving the Computational Complexity}

\begin{algorithm}[tb]
   \caption{Local Search for PCD (efficient)}
   \label{alg:ls2}
\begin{algorithmic}[1]
    \STATE Randomly assign each object $i \in [n]$ to one of the clusters in $S_{[k]}$, or make it neutral by adding it to $S_0$
    \STATE Initialize $X$ based on initial clustering $S_{[k]}$
    \STATE $M \coloneqq 2AX$
    \WHILE{not converged}
        \STATE Select object $i \in [n]$ uniformly at random
        \STATE $\hat{p} \coloneqq \text{current cluster of } i$
        \STATE $\eta_i \coloneqq \sum_{p} M_{i, p}$
        \STATE $G_{i,p} \coloneqq -\beta + (1+\alpha) M_{i,p} - 2\beta|S_p| + 2\beta\1_{[i \in S_p]} - \alpha\eta_i, \forall p \in [k]$
        \STATE $G_{i,0} \coloneqq 0$
        %\STATE If $\hat{p} \neq 0$, $G_{i,\hat{p}} \coloneqq G_{i,\hat{p}} + 2\beta$
        \STATE $p^{\ast} \coloneqq \argmax_{p \in \{0,\dots, k\}} G_{i,p}$
        %\STATE If $G_{i,p^{\ast}} < 0$, let $p^{\ast} \coloneqq 0$ (neutral)
        \STATE If $p^{\ast} = \hat{p}$, then skip to next iteration
        \STATE Assign object $i$ to cluster $S_{p^{\ast}}$
        \IF{$\hat{p} \in [k]$ (current cluster non-neutral)}
            %\STATE $X_{i,\hat{m}} \coloneqq 0$
            \STATE $M_{:,\hat{p}} \coloneqq M_{:,\hat{p}} - 2A_{:,i}$
        \ENDIF
        \IF{$p^{\ast} \neq 0$ (new cluster not neutral)}
            %\STATE $X_{i,m^{\ast}} \coloneqq 1$
            \STATE $M_{:,p^{\ast}} \coloneqq M_{:,p^{\ast}} + 2A_{:,i}$
        \ENDIF
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

%regarding stopping criteria. See section B.4 of https://arxiv.org/pdf/1207.4747

%\begin{algorithm}[tb]
%   \caption{Local Search for PCD (efficient)}
%   \label{alg:ls2}
%\begin{algorithmic}[1]
%    \STATE Randomly assign each object $i \in [n]$ to one of the clusters in $S_{[k]}$, or make it neutral by adding it to $S_0$
%    \STATE $M \coloneqq -\beta + 2AX - 2\beta\mathbf{1}_n(\mathbf{1}_n^TX) + 2\beta X - 2\alpha A((1-X) \odot X\mathbf{1}_k\mathbf{1}_k^T)$
%    \WHILE{not converged}
%        \STATE Select object $i \in [n]$ uniformly at random
%        \STATE $\hat{m} \coloneqq \text{current cluster of } i$
%        \STATE $m^{\ast} \coloneqq \argmax_{m \in [k]} M_{im}$
%        \STATE If $M_{i,m^{\ast}} < 0$, let $m^{\ast} \coloneqq 0$ (neutral)
%        \STATE If $m^{\ast} = \hat{m}$, then skip to next iteration
%        \STATE Assign object $i$ to cluster $S_{m^{\ast}}$
%        \IF{$\hat{m} \in [k]$ (current cluster non-neutral)}
%            %\STATE $X_{i,\hat{m}} \coloneqq 0$
%            \STATE $M_{:,\hat{m}} \coloneqq M_{:,\hat{m}} - 2A_{:,i} + 2\beta$
%            \STATE $M_{i,\hat{m}} \coloneqq M_{i,\hat{m}} - 2\beta$
%            \STATE $M_{:, -\hat{m}} \coloneqq M_{:, -\hat{m}} + 2\alpha A_{:,i}$
%        \ENDIF
%        \IF{$m^{\ast} \neq 0$ (new cluster not neutral)}
%            %\STATE $X_{i,m^{\ast}} \coloneqq 1$
%            \STATE $M_{:,m^{\ast}} \coloneqq M_{:,m^{\ast}} + 2A_{:,i} - 2\beta$
%            \STATE $M_{i,m^{\ast}} \coloneqq M_{i,m^{\ast}} + 2\beta$
%            \STATE $M_{:, -m^{\ast}} \coloneqq M_{:, -m^{\ast}} - 2\alpha A_{:,i}$
%        \ENDIF
%    \ENDWHILE
%\end{algorithmic}
%\end{algorithm}

In the previous section, we demonstrated that Alg. \ref{alg:ls} is guaranteed to converge at the linear rate $O(1/t)$, making it highly efficient. In this section, we propose an alternative version of Alg. \ref{alg:ls}, designed to enhance the efficiency of each step $t$ while maintaining full equivalence in functionality. This ensures that the convergence analysis from the previous section still remains valid. Firstly, a naive implementation of Alg. \ref{alg:ls} has a complexity of $O(Tk^2n^2)$, as each iteration requires $O(k^2n^2)$ to compute the full objective in Eq. \ref{eq:shift} for every candidate cluster in order to determine the best cluster for the current object $i$. Since the number of iterations $T$ until convergence is typically larger than $n$, this approach can become computationally expensive.

Part (b) of Theorem \ref{prop:discrete} offers an alternative: \emph{instead of evaluating the full objective, we can compute the gradient $G_{i,:}$, which involves only terms related to object $i$}. Let $M_{i,m} \triangleq 2\sum_{j \in S_m} A_{i,j}$, $\eta_i \triangleq \sum_{m\in[k]} M_{i,m}$ and $\beta_{im} = 2\beta|S_m| - 2\beta\1_{[i\in S_m]}$. Given this, we present the following theorem.
%
\begin{restatable}{theorem}{grad}\label{prop:grad} 
Let $S_{[k]}$ be the current clustering of our local search procedure, with neutral objects $S_0 = V \setminus \bigcup_{m \in [k]} S_m$. The gradient can then be expressed as follows.
%
\begin{equation} \label{eq:gradient}
G_{i,m} = -\beta + (1+\alpha) M_{i,m} - \beta_{im} - \alpha\eta_i
\end{equation}
%
for all $m \in [k]$ and $G_{i,0} = 0$.
\end{restatable}

A naive calculation of the full gradient $G_{i,:}$ for block $i$ is $O(k^2n)$. However, the particular form of the gradient presented in Eq. \ref{eq:gradient} makes it $O(kn)$. See the proof of Theorem \ref{prop:grad} for further insight on this. From Theorem \ref{prop:discrete}, the gradient $G_{i,m}$ represents the impact on the full objective in Eq. \ref{eq:shift} if object $i$ is placed in cluster $m$. Thus, because $G_{i,0} = 0$, we observe that an object $i$ is made neutral if its contribution to all non-neutral clusters is currently negative. Moreover, the total complexity is now reduced to $O(Tkn)$, which is a significant improvement over the naive approach with complexity of $O(Tk^2n^2)$.

We present a third approach, outlined in Alg. \ref{alg:ls2}, where we define a matrix $X \in \{0,1\}^{n \times k}$, with $X_{i,m} = 1$ if object $i$ belongs to cluster $m \in [k]$, and zero otherwise. Neutral objects $i \in S_0$ have rows $X_{i,:}$ of zeros. The procedure precomputes the matrix $M = 2AX$, where $M_{i,m}$ is the total similarity of object $i$ to cluster $m$. Precomputing $M$ takes $O(kn^2)$, but allows gradient computation in $O(k)$ (line 8). We then have to update $M_{i,m}$ accordingly (lines 15 and 18), which is $O(n)$, reducing the per-iteration complexity to $O(n + k)$. Thus, the total complexity is $O(kn^2 + T(n + k))$, which improves on the $O(Tnk)$ approach because, (i) computing $M$ involves a sparse matrix product, which is highly efficient in practice, and (ii) since  $T > n$, reducing per-iteration cost leads to significant practical gains.

%Finally, we note that several enhancements to the block-coordinate FW procedure in Alg. \ref{alg:blockfw} have been explored, including: (1) batch selection of blocks to improve computational efficiency, (2) variance reduction techniques, and (3) prioritized sampling of blocks based on how much they would increase the objective \citep{DBLP:conf/icml/Lacoste-JulienJSP13, DBLP:conf/aaai/ThielCD19}. These methods offer promising opportunities to further improve our algorithms, which we leave as an interesting direction for future research in this context.

\subsection{Impact of $\alpha$ and $\beta$}

We now analyze the impact of $\alpha$ and $\beta$ in Eq. \ref{eq:ours}. We begin by stating the following proposition.


\begin{restatable}{proposition}{betaprop} \label{prop:beta} 
(a) There exists a $\xi_1 < 0$ such that for any $\beta \leq \xi_1$, there is a clustering solution maximizing Eq. \ref{eq:ours} where all the objects are assigned to a single non-neutral cluster.
(b) Conversely, there exists a $\xi_2 > 0$ such that for any $\beta \geq \xi_2$, there is a clustering solution maximizing Eq. \ref{eq:ours} where all the objects are neutral.
\end{restatable}

From Proposition \ref{prop:beta}, we understand the extreme cases of $\beta$: (a) a small negative $\beta$ results in a maximally imbalanced non-neutral clustering (i.e., all objects in one non-neutral cluster), while (b) a large positive $\beta$ makes all objects neutral. For intermediate $\beta \in [\xi_1,\xi_2]$, we analyze the gradient in Eq. \ref{eq:gradient}. Increasing $\beta$ strictly reduces the contribution of object $i$ to each cluster $m \in [k]$, but since the term $-2\beta |S_m|$ scales with cluster size, larger clusters become less favorable, promoting balance. If $\beta$ is large enough, it forces $G_{i,m} < 0$ for all $m \in [k]$, making neutrality optimal for object $i$. Note that this is more likely for low-degree objects, implying that high-degree objects (with clear cluster assignment) are more likely to remain non-neutral, resulting in dense non-neutral clusters. Consequently, increasing $\beta$ leads to smaller (i.e., more neutral objects) and denser non-neutral clusters, while maintaining balanced, as desired.


The parameter $\alpha$ has been studied in prior work \citep{DBLP:conf/kdd/ChuWPWZC16, DBLP:conf/nips/TzengOG20}. From Eq. \ref{eq:ours}, $\alpha$ balances maximizing intra-similarities and minimizing inter-similarities, which translates to a trade-off between cohesion within clusters and separation between them. A heuristic choice of $\alpha = 1/(k-1)$ was proposed in \citep{DBLP:conf/nips/TzengOG20}, based on the observation that the number of intra-similarities scale linearly with $k$, while the number of inter-similarities grow quadratically. This choice prevents inter-similarities from dominating the objective. Finally, the term $-\alpha \eta_i$ indicates that $\alpha$ influences whether object $i$ becomes neutral, underscoring the need to account for inter-similarities in the objective (as suggested in Section \ref{section:pcd}).


\section{Experiments}

\begin{table*}[t!]
    \caption{Results for different methods and real-world datasets w.r.t. \emph{balance-aware polarity}. $|E|$ is number of non-zero edges. Dashes indicate the method exceeded memory capacity. LSPCD (ours) yields the best results in most of the cases.}
    \label{table:table1}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccc}
        \toprule
        & & \texttt{W8} & \texttt{BTC} & \texttt{WikiV} & \texttt{REF} & \texttt{SD} & \texttt{WikiC} & \texttt{EP} & \texttt{WikiP} \\
        \midrule
        & $|V|$ & 790 & 5.9K & 7.1K & 10.9K & 82.1K & 116.7K & 131.6K & 138.6K \\
        & $|E|$ & 116K & 214.5K & 1M & 251.4K & 500.5K & 2M & 711.2K & 715.9K \\
        %& $|E_-|/|E|$ & 0.2 & 0.2 & 0.2 & 0.1 & 0.2 & 0.6 & 0.2 & 0.1 \\
%\midrule
%$k = 2$  & SCG-B & 4.9 & 19.7 & 0.7 & 1.7 & 1.5 & 47.3 & 3.8 & 0.9 \\
% & SCG-MO & 0.5 & 0.6 & 0.4 & 1.3 & 0.7 & 39.6 & 2.4 & 0.4 \\
% & SCG-MA & 0.4 & 2.4 & 0.4 & 1.3 & 0.8 & 44.7 & 2.5 & 0.6 \\
% & SCG-R & 3.4 & 1.8 & 4.6 & 2.1 & 1.1 & 31.9 & 6.6 & 3.0 \\
% & KOCG-top-$1$ & 10.4 & 1.0 & 3.8 & 4.8 & 1.3 & 3.5 & 3.3 & 1.8 \\
% & KOCG-top-$r$ & 11.3 & 3.3 & 2.2 & 11.7 & 2.2 & 2.9 & 10.1 & 1.1 \\
% & LS & 20.6 & \textbf{20.8} & \textbf{26.9} & \textbf{64.4} & \textbf{33.5} & \textbf{106.9} & \textbf{57.3} & \textbf{17.4} \\
% & BNC-$(k+1)$ & -0.4 & -0.9 & -0.6 & -1.0 & --- & --- & --- & --- \\
% & BNC-$k$ & 0.7 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
% & SPONGE-$(k+1)$ & 19.7 & 0.6 & 0.3 & 0.6 & --- & --- & --- & --- \\
% & SPONGE-$k$ & \textbf{22.4} & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
\midrule
$k = 4$  & LSPCD (ours) & 6.9 & 3.6 & \textbf{5.9} & \textbf{3.9} & \textbf{6.7} & \textbf{26.3} & \textbf{14.9} & \textbf{5.4} \\
& SCG-MA & 5.8 & 0.3 & 0.8 & 0.3 & 1.9 & 0.7 & 0.1 & 0.7 \\
& SCG-MO & 4.9 & 0.3 & 0.4 & 0.4 & 0.2 & 6.6 & 0.1 & 0.0 \\
& SCG-B & 0.3 & 0.1 & 0.2 & 0.1 & 0.5 & 18.5 & 0.2 & 0.1 \\
& SCG-R & 1.3 & 0.9 & 0.9 & 0.6 & 2.4 & 2.0 & 0.3 & 1.0 \\
& KOCG-top-$1$ & 5.8 & \textbf{4.2} & 1.2 & 2.6 & 0.9 & 0.4 & 4.0 & 0.9 \\
& KOCG-top-$r$ & 5.4 & 2.7 & 2.6 & 1.6 & 1.0 & 2.8 & 5.9 & 1.2 \\
& BNC-$(k+1)$ & -0.1 & -0.8 & -0.4 & -1.0 & --- & --- & --- & --- \\
& BNC-$k$ & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
& SPONGE-$(k+1)$ & \textbf{22.9} & 0.0 & 0.4 & 0.3 & --- & --- & --- & --- \\
& SPONGE-$k$ & 18.6 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
\midrule
$k = 6$  & LSPCD (ours) & 4.4 & \textbf{2.2} & \textbf{3.6} & 2.2 & \textbf{3.8} & \textbf{16.4} & \textbf{6.8} & 3.5 \\
& SCG-MA & 3.1 & 0.1 & 0.2 & 0.1 & 0.0 & 0.6 & 0.4 & 0.0 \\
& SCG-MO & 3.6 & 0.1 & 0.2 & 0.1 & 0.1 & 4.2 & 0.1 & 0.0 \\
& SCG-B & 0.3 & 0.2 & 0.2 & 0.1 & 0.2 & 1.8 & 0.2 & 0.1 \\
& SCG-R & 0.3 & 0.3 & 0.4 & 0.2 & 0.4 & 2.3 & 0.1 & 0.0 \\
& KOCG-top-$1$ & 6.1 & 1.7 & 2.5 & \textbf{3.3} & 1.7 & 0.4 & 2.7 & \textbf{3.7} \\
& KOCG-top-$r$ & 5.0 & 1.2 & 1.8 & 2.5 & 1.3 & 1.1 & 2.8 & 0.7 \\
& BNC-$(k+1)$ & -0.1 & -0.1 & -0.8 & -0.4 & --- & --- & --- & --- \\
& BNC-$k$ & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
& SPONGE-$(k+1)$ & \textbf{17.7} & 0.0 & 0.4 & 0.6 & --- & --- & --- & --- \\
& SPONGE-$k$ & 16.0 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[t!]
    \caption{Results for different methods and real-world datasets w.r.t. \emph{polarity}. LSPCD (ours) yields the best or close to the best results. $|E|$ is number of non-zero edges. Dashes indicate the method exceeded memory capacity. LSPCD (ours) yields the best results in most of the cases.}
    \label{table:table2}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccc}
        \toprule
        & & \texttt{W8} & \texttt{BTC} & \texttt{WikiV} & \texttt{REF} & \texttt{SD} & \texttt{WikiC} & \texttt{EP} & \texttt{WikiP} \\
        \midrule
        & $|V|$ & 790 & 5.9K & 7.1K & 10.9K & 82.1K & 116.7K & 131.6K & 138.6K \\
        & $|E|$ & 116K & 214.5K & 1M & 251.4K & 500.5K & 2M & 711.2K & 715.9K \\
        %& $|E_-|/|E|$ & 0.2 & 0.2 & 0.2 & 0.1 & 0.2 & 0.6 & 0.2 & 0.1 \\
%\midrule
%$k = 2$  & SCG-B & 200.6 & 21.6 & 37.6 & 116.3 & 61.0 & 129.3 & \textbf{156.4} & 46.5 \\
% & SCG-MO & \textbf{236.6} & \textbf{29.5} & \textbf{71.7} & \textbf{174.1} & \textbf{79.7} & 175.7 & 128.7 & \textbf{88.4} \\
% & SCG-MA & \textbf{236.6} & 28.8 & 71.5 & 172.2 & 77.5 & 155.2 & 128.3 & 82.8 \\
% & SCG-R & 214.6 & 14.2 & 54.7 & 120.9 & 29.7 & 101.1 & 72.3 & 36.1 \\
% & KOCG-top-$1$ & 13.0 & 1.0 & 7.6 & 11.6 & 2.0 & 5.9 & 8.2 & 3.0 \\
% & KOCG-top-$r$ & 13.0 & 3.8 & 2.3 & 15.4 & 2.6 & 3.4 & 14.0 & 1.3 \\
% & LS & 223.4 & 29.0 & 62.3 & 146.1 & 75.9 & \textbf{190.8} & 127.8 & 82.0 \\
% & BNC-$(k+1)$ & -0.7 & -10.8 & -1.1 & -1.0 & --- & --- & --- & --- \\
% & BNC-$k$ & 184.6 & 5.3 & 15.8 & 41.5 & --- & --- & --- & --- \\
% & SPONGE-$(k+1)$ & 88.0 & 1.0 & 1.0 & 1.0 & --- & --- & --- & --- \\
% & SPONGE-$k$ & 191.4 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
\midrule
$k = 4$ & LSPCD (ours) & \textbf{218.5} & 23.3 & 52.6 & \textbf{139.2} & \textbf{61.1} & 113.6 & 111.5 & \textbf{71.6} \\
 & SCG-MA & 205.1 & 25.1 & 52.9 & 94.5 & 35.5 & 104.9 & 127.4 & 56.5 \\
 & SCG-MO & 213.2 & \textbf{25.3} & \textbf{53.1} & 82.1 & 38.5 & \textbf{117.9} & \textbf{129.0} & 39.7 \\
 & SCG-B & 211.6 & 12.4 & 24.8 & 116.2 & 48.3 & 49.8 & 94.4 & 45.7 \\
 & SCG-R & 214.6 & 8.0 & 19.5 & 118.7 & 10.7 & 41.1 & 65.1 & 33.7 \\
 & KOCG-top-$1$ & 9.1 & 8.4 & 4.5 & 15.0 & 2.6 & 4.5 & 8.9 & 3.1 \\
 & KOCG-top-$r$ & 7.4 & 5.0 & 3.3 & 3.7 & 3.0 & 3.8 & 11.0 & 4.4 \\
 & BNC-$(k+1)$ & -0.2 & -9.4 & -1.1 & -1.0 & --- & --- & --- & --- \\
 & BNC-$k$ & 185.3 & 5.2 & 15.8 & 41.5 & --- & --- & --- & --- \\
 & SPONGE-$(k+1)$ & 53.8 & 1.1 & 1.0 & 1.0 & --- & --- & --- & --- \\
 & SPONGE-$k$ & 71.2 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
\midrule
$k = 6$ & LSPCD (ours) & \textbf{217.3} & \textbf{20.0} & 46.2 & \textbf{137.6} & \textbf{57.1} & 96.1 & 103.4 & \textbf{58.7} \\
 & SCG-MA & 207.3 & 14.6 & 45.5 & 84.9 & 37.8 & 102.6 & 88.8 & 57.5 \\
 & SCG-MO & 205.8 & 15.2 & \textbf{47.0} & 55.6 & 34.6 & \textbf{111.6} & \textbf{129.2} & 41.8 \\
 & SCG-B & 211.6 & 9.3 & 23.3 & 116.2 & 47.7 & 46.1 & 94.5 & 46.0 \\
 & SCG-R & 201.2 & 6.9 & 10.4 & 50.3 & 7.9 & 18.3 & 43.3 & 3.3 \\
 & KOCG-top-$1$ & 7.9 & 4.1 & 4.5 & 8.6 & 3.6 & 4.9 & 6.0 & 10.1 \\
 & KOCG-top-$r$ & 9.1 & 3.6 & 3.1 & 4.0 & 3.3 & 1.5 & 6.8 & 3.6 \\
 & BNC-$(k+1)$ & -0.2 & -4.2 & -1.1 & -0.8 & --- & --- & --- & --- \\
 & BNC-$k$ & 185.2 & 5.2 & 15.8 & 41.5 & --- & --- & --- & --- \\
 & SPONGE-$(k+1)$ & 47.8 & 1.3 & 1.0 & 1.0 & --- & --- & --- & --- \\
 & SPONGE-$k$ & 57.9 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


In this section, we describe our experimental studies, where extensive additional results are presented in Appendix \ref{appendix:experiments}. We use eight publically available datasets, which are commonly used in previous work for PCD. In Appendix \ref{appendix:datasets}, we provide details about each dataset. We compare our efficient local search method for PCD, called LSPCD, against a number of baselines introduced below. 
%We introduced an efficient local search approach for PCD (LSPCD). We compare our method to several baselines, which we introduce below. 

\textbf{Baselines}. (i) SCG \citep{DBLP:conf/nips/TzengOG20} is a spectral method that identifies $k$ non-neutral clusters by maximizing polarity (Eq. \ref{eq:polarity}) with $\alpha = 1/(k-1)$. It solves a continuous relaxation and applies one of four rounding techniques, resulting in SCG-MA, SCG-R, SCG-MO, and SCG-B. We refer to \citep{DBLP:conf/nips/TzengOG20} for details. (ii) KOCG \citep{DBLP:conf/kdd/ChuWPWZC16} optimizes a similar objective and formulates it as a constrained quadratic optimization problem (this optimization approach is very different from ours). It outputs a set of local minima. For comparison, we select KOCG-top-$1$ (the best local minimum) and KOCG-top-$r$, where $r$ is chosen such that the number of non-neutral objects is closest to SCG-MA, following \citep{DBLP:conf/nips/TzengOG20}. (iii) BNC \citep{DBLP:conf/cikm/ChiangWD12} and SPONGE \citep{DBLP:conf/aistats/CucuringuDGT19} are spectral methods designed for SNP that do not explicitly handle neutral objects. As in \citep{DBLP:conf/nips/TzengOG20}, we apply two heuristics with these methods: (a) we treat all $k$ clusters as non-neutral, and (b) we run the methods with $k+1$ clusters and then designate the largest cluster as neutral. These variants are denoted BNC-$k$ / SPONGE-$k$ and BNC-$(k+1)$ / SPONGE-$(k+1)$, respectively. We use publicly available implementations of these baselines, with further details, including hyperparameters, provided in Appendix \ref{appendix:baselines}.  

%\textbf{Metrics}. (i) We follow previous work and use \emph{polarity} to measure the quality of a clustering \citep{DBLP:conf/cikm/BonchiGGOR19, DBLP:conf/nips/TzengOG20}. It is defined as in Eq. \ref{eq:polarity} with $\alpha = 1/(k-1)$. However, we note that SCG \citep{DBLP:conf/nips/TzengOG20} explicitly aims to maximize polarity, which we do not, since we consider a different objective. (ii) We also consider a variant which we called \emph{balance-aware polarity} which is the polarity multiplied with $S_m/S_l$ where $S_m$ and $S_l$ correspond to the largest and smallest non-neutral clusters, respectively. (iii) We consider \emph{mean average cohesion} (MAC) and \emph{mean average opposition} (MAO) defined as $\text{MAC} = \frac{1}{k}\sum_{m \in [k]}\frac{1}{|S_m||S_m-1|}\sum_{i,j \in S_m} A^+_{i,j}$  and $\text{MAO} = \frac{1}{k(k-1)}\sum_{m \in [k]}\sum_{p \in [k] \setminus m} \frac{1}{|S_m||S_p|} \sum_{i \in S_m} \sum_{j \in S_p} A^-_{i,j}$.

\textbf{Metrics}. (i) Following prior work, we use \emph{polarity} to evaluate the quality of different methods \citep{DBLP:conf/cikm/BonchiGGOR19, DBLP:conf/nips/TzengOG20}, defined as in Eq. \ref{eq:polarity} with $\alpha = 1/(k-1)$. We note that SCG \citep{DBLP:conf/nips/TzengOG20} explicitly optimizes for polarity, unlike our approach, which gives it an inherent advantage in polarity-based comparisons. (ii) We introduce \emph{balance-aware polarity} (BA-Polarity), which multiplies polarity by a balance factor $|S_m|/|S_l|$, where $|S_m|$ and $|S_l|$ are the sizes of the smallest and largest non-neutral clusters, respectively. In Appendix \ref{appendix:results}, we provide a detailed presentation of the solutions found by each method. This includes the number of non-neutral objects identified, the balance of the non-neutral clusters, the runtime, and more.


%(iii) We also consider \emph{mean average cohesion} (MAC) defined as
%%
%%\begin{equation}
%$\text{MAC} = \frac{1}{k} \sum_{m \in [k]} \frac{1}{|S_m|(|S_m| - 1)} \sum_{i,j \in S_m} A^+_{i,j}$
%%\end{equation}
%%
%and \emph{mean average opposition} (MAO) as
%%
%%\begin{equation}
%$\text{MAO} = \frac{1}{k(k-1)} \sum_{\substack{m \in [k] \\ p \in [k] \setminus \{m\}}} \frac{1}{|S_m||S_p|} \sum_{\substack{i \in S_m \\ j \in S_p}} A^-_{i,j}.$ 
%%\end{equation}
%%
%MAC and MAO quantify the density of positive similarities within clusters and negative similarities between clusters. 



\textbf{Results}. Tables \ref{table:table1} and \ref{table:table2} present results for different methods and datasets with $k = 4$ and $k = 6$, evaluating BA-Polarity and polarity, respectively. The spectral clustering methods, BNC and SPONGE, exceeded memory limits on large datasets (caused by $k$-means), indicated by dashes. Unlike most prior work, which primarily focuses on $k = 2$, we consider more than two clusters; however, for completeness, results for $k = 2$ are provided in Appendix \ref{appendix:results}, where similar trends are observed. We report the mean over five runs with different seeds, with standard deviations included in Appendix \ref{appendix:results}. For our method, we select the $\beta$ value that maximizes \emph{polarity}, testing 10 values per dataset, while we fix $\alpha = 1/(k-1)$ for all methods unless stated otherwise. Results show that our method is highly competitive—often the best—in polarity across all datasets, despite SCG explicitly optimizing for polarity while our method does not. Moreover, our method consistently outperforms all baselines in BA-Polarity. While SCG remains competitive in polarity, our method surpasses it in BA-Polarity across all datasets. In contrast, KOCG and SPONGE occasionally achieve higher BA-Polarity but at the cost of extremely low polarity (see Table \ref{table:table2}) due to overly balanced clustering solutions (arguably to an excessive degree). This demonstrates that our method finds high-polarity solutions while maintaining balance, effectively addressing SCG’s tendency to produce highly imbalanced clusters. Additionally, our method does not impose strict balance constraints, which is beneficial since real-world clusterings are rarely perfectly balanced. Instead, it identifies high-polarity solutions with reasonable balance, making it more practical for real-world applications (further discussed in Appendix \ref{appendix:results}).

\begin{figure}[t!] %htb!
\centering 
%\subfigure[Epinions, $k=2$, $\beta=0.1$]{\label{subfig:s1}\includegraphics[width=0.49\linewidth]{imgs/alpha.png}}
%\subfigure[Wikipol, $k=6$, $\alpha=\frac{1}{k-1}$]{\label{subfig:s2}\includegraphics[width=0.49\linewidth]{imgs/beta1.png}}
%\subfigure[]
\includegraphics[width=0.49\linewidth]{imgs/beta2.png}
%\subfigure[]
\includegraphics[width=0.49\linewidth]{imgs/beta3.png}
\caption{Impact of $\beta$ on the WikiPol dataset with $k = 6$.}
\label{fig:f1}
\end{figure}



%Tables \ref{table:table1} and \ref{table:table2} present results for different methods and datasets with $k = 4$ and $k = 6$, evaluating BA-Polarity and polarity, respectively. The spectral clustering methods, BNC and SPONGE, exceeded memory limitations on large datasets (caused by $k$-means), indicated by dashes. Unlike most previous work, which primarily considers $k = 2$, we focus on finding more than two clusters. However, for completeness, results for $k = 2$ are provided in Appendix \ref{appendix:results}, where we observe the same trends (excluded here due to space constraints). We report the mean over five runs with different seeds (the standard deviation is presented in Appendix \ref{appendix:results}). For our method, we select the $\beta$ value that maximizes \emph{polarity}, testing 10 values per dataset, while $\alpha$ is fixed at $1/(k-1)$ for all methods unless stated otherwise. Results show that our method is highly competitive—often the best—in polarity across all datasets, despite SCG explicitly optimizing for polarity while our method does not. Moreover, our method consistently outperforms all baselines in BA-Polarity. While SCG is competitive with our method in polarity, our method surpasses it in BA-Polarity across all datasets. In contrast, KOCG and SPONGE occasionally achieve higher BA-Polarity but at the expense of extremely low polarity (see Table \ref{table:table2}). This occurs because they produce clustering solutions that are highly balanced—arguably to an excessive degree. This highlights our method’s ability to find high-polarity solutions while maintaining balance, effectively addressing SCG’s tendency to produce highly imbalanced clusters. Furthermore, our method does not impose strict balance constraints, which is beneficial since real-world clusterings are often not perfectly balanced. Instead, it identifies high-polarity solutions that maintain a reasonable balance, aligning with practical scenarios (this is further highlighted in the appendix).



%Figure \ref{fig:f1} illustrates the impact of $\alpha$ and $\beta$. We run our method with five random seeds, reporting the mean and standard deviation. % (shaded region). 
%As shown in Figure \ref{subfig:s1}, increasing $\alpha$ naturally balances intra-cluster cohesion and inter-cluster opposition. The size proportion, defined as the fraction of non-neutral objects in $V$, remains constant as $\alpha$ varies. In contrast, Figure \ref{subfig:s2} shows that increasing $\beta$ monotonically reduces the number of non-neutral objects, leading to denser clusters, as indicated by improved MAC and MAO scores. Notably, balance remains stable across different $\beta$ values, unlike the baseline SCG-MA. 

Figure \ref{fig:f1} shows the effect of varying $\beta$. Very small or large $\beta$ values lead to poorer polarity, as they produce clustering solutions with too many or too few of non-neutral objects, respectively. In contrast, intermediate $\beta$ values yield competitive polarity while consistently outperforming baselines in BA-Polarity. Further insights on the impact of $\beta$ and $\alpha$ are provided in Appendix \ref{appendix:results}, along with other results.



%Figure \ref{fig:f1} illustrates the impact of $\alpha$ and $\beta$. We run our method with five random seeds, reporting the mean and standard deviation (shaded region). As shown in Figure \ref{subfig:s1}, increasing $\alpha$ naturally balances intra-cluster cohesion and inter-cluster opposition. The size proportion, defined as the fraction of non-neutral objects in $V$, remains constant as $\alpha$ varies. In contrast, Figure \ref{subfig:s2} shows that increasing $\beta$ monotonically reduces the number of non-neutral objects, leading to denser clusters, as indicated by improved MAC and MAO scores. Notably, balance remains stable across different $\beta$ values, unlike the baseline SCG-MA. Figures \ref{subfig:s3}-\ref{subfig:s4} show that our method achieves polarity comparable to SCG while significantly outperforming baselines in BA-Polarity, demonstrating its ability to find high-polarity solutions with improved balance. Furthermore, our method does not impose strict balance constraints, which is beneficial since real-world clusterings are often not perfectly balanced. Instead, it identifies high-polarity solutions that maintain a reasonable balance, aligning with practical scenarios. Finally, Tables \ref{table:table1}-\ref{table:table2} confirm that this conclusion holds across all datasets for $k = 4$ and $k = 6$. Dashes indicate that the corresponding method exceeded memory limits (caused by $k$-means on large datasets). Table \ref{table:table1} presents results for BA-Polarity, while Table \ref{table:table2} reports polarity. For our method (LSPCD), we report the average over five runs and select the value of $\beta$ which maximizes the corresponding metric (we try $10$ different values of $\beta$ for each dataset). Detailed results, including standard deviations, investigations with other values of $k$, and eight additional metrics (including runtime), are reported in Appendix \ref{appendix:results}.


%\begin{table*}[h]
%    \caption{}
%    \label{sample-table}
%    \vskip 0.15in
%    \begin{center}
%    \begin{small}
%    \begin{sc}
%    %\renewcommand{\arraystretch}{0.6}
%    \begin{tabular}{clcccccccc}
%        \toprule
%        & & \texttt{W8} & \texttt{BTC} & \texttt{WikiV} & \texttt{REF} & \texttt{SD} & \texttt{WikiC} & \texttt{EP} & \texttt{WikiP} \\
%        \midrule
%        & $|V|$ & 790 & 5.9K & 7.1K & 10.9K & 82.1K & 116.7K & 131.6K & 138.6K \\
%        & $|E|$ & 116K & 214.5K & 1M & 251.4K & 500.5K & 2M & 711.2K & 715.9K \\
%        %& $|E_-|/|E|$ & 0.2 & 0.2 & 0.2 & 0.1 & 0.2 & 0.6 & 0.2 & 0.1 \\
%\midrule
%\multicolumn{10}{c}{BA-Polarity} \\
%%\midrule
%% $k = 2$ & LS (ours) & 20.6 & \textbf{20.8} & \textbf{26.9} & \textbf{64.4} & \textbf{33.5} & \textbf{106.9} & \textbf{57.3} & \textbf{17.4} \\
%% & SCG & 4.9 & 19.7 & 4.6 & 4.8 & 1.5 & 47.3 & 6.6 & 3.0 \\
%% & KOCG & 11.3 & 3.3 & 3.8 & 11.7 & 2.2 & 3.5 & 10.1 & 1.8 \\
%% & BNC & 0.7 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
%% & SPONGE & \textbf{22.4} & 0.6 & 0.3 & 0.6 & --- & --- & --- & --- \\
%\midrule
%$k = 4$  & LS (ours) & 7.5 & \textbf{5.0} & \textbf{9.5} & \textbf{5.5} & \textbf{11.2} & \textbf{34.5} & \textbf{14.9} & \textbf{8.4} \\
% & SCG & 5.8 & 0.9 & 0.9 & 0.6 & 2.4 & 18.5 & 0.3 & 1.0 \\
% & KOCG & 5.8 & 4.2 & 2.6 & 2.6 & 1.0 & 2.8 & 5.9 & 1.2 \\
% & BNC & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
% & SPONGE & \textbf{22.9} & 0.0 & 0.4 & 0.3 & --- & --- & --- & --- \\
%\midrule
%$k = 6$ & LS (ours) & 4.9 & \textbf{3.4} & \textbf{4.3} & 3.1 & \textbf{6.3} & \textbf{20.6} & \textbf{10.6} & \textbf{6.1} \\
% & SCG & 3.6 & 0.3 & 0.4 & 0.2 & 0.4 & 4.2 & 0.4 & 0.1 \\
% & KOCG & 6.1 & 1.7 & 2.5 & \textbf{3.3} & 1.7 & 1.1 & 2.8 & 3.7 \\
% & BNC & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
% & SPONGE & \textbf{17.7} & 0.0 & 0.4 & 0.6 & --- & --- & --- & --- \\
% \midrule
%\multicolumn{10}{c}{Polarity} \\
%%\midrule
%%$k = 2$  & LS (ours) & 223.4 & 29.0 & 62.3 & 146.1 & 75.9 & \textbf{190.8} & 127.8 & 82.0 \\
%%& SCG & \textbf{236.6} & \textbf{29.5} & \textbf{71.7} & \textbf{174.1} & \textbf{79.7} & 175.7 & \textbf{156.4} & \textbf{88.4} \\
%% & KOCG & 13.0 & 3.8 & 7.6 & 15.4 & 2.6 & 5.9 & 14.0 & 3.0 \\
%% & BNC & 184.6 & 5.3 & 15.8 & 41.5 & --- & --- & --- & --- \\
%% & SPONGE & 191.4 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
%\midrule
%$k = 4$   & LS (ours) & \textbf{218.5} & 23.3 & 52.6 & \textbf{139.2} & \textbf{61.1} & 113.6 & 111.5 & \textbf{71.6} \\
%& SCG & 214.6 & \textbf{25.3} & \textbf{53.1}& 118.7 & 48.3 & \textbf{117.9} & \textbf{129.0} & 56.5\\
% & KOCG & 9.1 & 8.4 & 4.5 & 15.0 & 3.0 & 4.5 & 11.0 & 4.4 \\
% %& BNC & 185.3 & 5.2 & 15.8 & 41.5 & --- & --- & --- & --- \\
% %& SPONGE & 71.2 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
%\midrule
%$k = 6$  & LS (ours) & \textbf{217.3} & \textbf{20.0} & 46.2 & \textbf{137.6} & \textbf{57.1} & 96.1 & 103.4 & \textbf{58.7} \\
% & SCG & 211.6 & 15.2 & \textbf{47.0} & 116.2 & 47.7 & \textbf{111.6} &  \textbf{129.2} & 57.5 \\
% & KOCG & 9.1 & 4.1 & 4.5 & 8.6 & 3.6 & 4.9 & 6.8 & 10.1 \\
%% & BNC & 185.2 & 5.2 & 15.8 & 41.5 & --- & --- & --- & --- \\
% %& SPONGE & 57.9 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
%        \bottomrule
%    \end{tabular}
%    \end{sc}
%    \end{small}
%    \end{center}
%    \vskip -0.1in
%\end{table*}




\section{Conclusion}

We proposed a novel formulation for the polarized community discovery problem, emphasizing (i) large, (ii) dense, and (iii) balanced clustering solutions. We developed an efficient and effective local search method and established its connection to block-coordinate Frank-Wolfe optimization, proving a linear convergence rate of $O(1/t)$. Our extensive experimental results demonstrate that our method achieves high-polarity clustering solutions while maintaining balance, and outperforms the other methods.

\section*{Acknowledgments}

This work was partially supported by the Wallenberg AI, Autonomous Systems and
Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. 


%Bibliography
%\bibliographystyle{unsrt}  
\bibliographystyle{unsrtnat}  
\bibliography{references}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs} \label{appendix:proofs}

\propcc*
\begin{proof}

We begin by defining the following quantities, which are constants w.r.t. different clustering solutions for the $k$-CC problem.
\begin{equation}
c_{\text{sim}} \triangleq \sum_{(i,j) \in E} A_{i,j}.
\end{equation}

\begin{equation}
c_{\text{abs}} \triangleq \sum_{(i, j) \in E} |A_{i,j}|.
\end{equation}

The five objectives can be written as follows.

\begin{equation} \label{eq:full2}
\begin{aligned}
     f^{\text{full}} &\triangleq N^+_{\text{intra}} - N^-_{\text{intra}} + N^-_{\text{inter}} - N^+_{\text{inter}} = \sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j} - \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j} \\
    f^{\text{MaxAgree}} &\triangleq N^+_{\text{intra}} + N^-_{\text{inter}} \\
    &= \sum_{m \in [k]} \sum_{i,j \in S_m} A^+_{i,j} - \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A^-_{i,j}  \\
    &= \frac{1}{2}\sum_{m \in [k]} \sum_{i,j \in S_m} (|A_{i,j}| + A_{i,j}) - \frac{1}{2}\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} (|A_{i,j}| - A_{i,j}) \\
    f^{\text{MinDisagree}} &\triangleq N^-_{\text{intra}} + N^+_{\text{inter}} \\
    &= \sum_{m \in [k]} \sum_{i,j \in S_m} A^-_{i,j} + \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A^+_{i,j}  \\
    &= \frac{1}{2}\sum_{m \in [k]} \sum_{i,j \in S_m} (|A_{i,j}| - A_{i,j}) + \frac{1}{2}\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} (|A_{i,j}| + A_{i,j})\\
    f^{\text{MaxCorr}} &\triangleq N^+_{\text{intra}} - N^-_{\text{intra}} = \sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j} \\
    f^{\text{MinCut}} &\triangleq -N^-_{\text{inter}} + N^+_{\text{inter}} = \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j}
    \end{aligned}
\end{equation}
%\begin{equation} \label{eq:fullcc}
%   N^+_{\text{intra}} - N^-_{\text{intra}} + N^-_{\text{inter}} - N^+_{\text{inter}}
%\end{equation}
%

Given this, we observe the following connection between the objectives.

\begin{equation}
\begin{aligned}
    f^{\text{MaxAgree}} &= \frac{1}{2}\sum_{m \in [k]} \sum_{i,j \in S_m} (|A_{i,j}| + A_{i,j}) - \frac{1}{2}\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} (|A_{i,j}| - A_{i,j}) \\
    &= \frac{1}{2}f^{\text{full}} + \frac{1}{2}c_{\text{abs}} \\
    &= -f^{\text{MinDisagree}} + c_{\text{abs}} \\
    &= \frac{1}{2} f^{\text{MaxCorr}} - \frac{1}{2} f^{\text{MinCut}} + \frac{1}{2}c_{\text{abs}} \\
    &= f^{\text{MaxCorr}} - \frac{1}{2}c_{\text{sim}}  + \frac{1}{2}c_{\text{abs}} \\
    &= -f^{\text{MinCut}} + \frac{1}{2}c_{\text{sim}}  + \frac{1}{2}c_{\text{abs}}
\end{aligned}
\end{equation}

The above establishes that they are all equal up to constants. We prove the last statement of the proposition by counterexample. We consider the graph $V=\{1,2,3\}$ with edge weights $A_{1,2}=+1$, $A_{2,3}=+1$, $A_{1,3}=-1$. The possible clustering solutions (partitions) are:
\begin{equation*}
S^{(1)} = \{\{1,2,3\}\},\quad
S^{(2)} = \{\{1,2\},\,\{3\}\},\quad
S^{(3)} = \{\{1,3\},\,\{2\}\},\quad
S^{(4)} = \{\{2,3\},\,\{1\}\},\quad
S^{(5)} = \{\{1\},\,\{2\},\,\{3\}\}.
\end{equation*}

In Table~\ref{prop1-table}, we list all linear combinations of the terms \(N_{\text{intra}}^{+}, -N_{\text{intra}}^{-}, N_{\text{inter}}^{-}, -N_{\text{inter}}^{+}\) evaluated on each of the five clustering solutions. Expectedly (from the first part of the proposition), the five objectives \(f^{\text{full}}, f^{\text{MaxAgree}}, f^{\text{MinDisagree}}, f^{\text{MaxCorr}}, f^{\text{MinCut}}\) all produce the same ranking of these solutions. In contrast, every other combination of the terms ranks at least one solution differently compared to these five. This proves the last statement of the proposition.

\begin{table}[h]
\caption{All sums of $N_{\text{intra}}^{+},\,-N_{\text{intra}}^{-},\,N_{\text{inter}}^{-},\,-N_{\text{inter}}^{+}$ and their values on the five partitions $S^{(m)}$. In parentheses we indicate the known name when the combination corresponds to one of the five standard correlation-clustering objectives (or its negative).}
\label{prop1-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{r|lccccc}
\toprule
\textbf{Idx} & \textbf{Combination} 
 & $S^{(1)}$ & $S^{(2)}$ & $S^{(3)}$ & $S^{(4)}$ & $S^{(5)}$ \\
\midrule
1 & $N_{\text{intra}}^{+}$ 
   & 2 & 1 & 0 & 1 & 0 \\
2 & $-\,N_{\text{intra}}^{-}$ 
   & -1 & 0 & -1 & 0 & 0 \\
3 & $N_{\text{inter}}^{-}$ 
   & 0 & 1 & 0 & 1 & 1 \\
4 & $-\,N_{\text{inter}}^{+}$ 
   & 0 & -1 & -2 & -1 & -2 \\
5 & $N_{\text{intra}}^{+} - N_{\text{intra}}^{-}$ ($f^{\text{MaxCorr}}$) 
   & 1 & 1 & -1 & 1 & 0 \\
6 & $N_{\text{intra}}^{+} + N_{\text{inter}}^{-}$ ($f^{\text{MaxAgree}}$) 
   & 2 & 2 & 0 & 2 & 1 \\
7 & $N_{\text{intra}}^{+} - N_{\text{inter}}^{+}$ 
   & 2 & 0 & -2 & 0 & -2 \\
8 & $-\,N_{\text{intra}}^{-} + N_{\text{inter}}^{-}$ 
   & -1 & 1 & -1 & 1 & 1 \\
9 & $-\,N_{\text{intra}}^{-} - N_{\text{inter}}^{+}$ ($-\,f^{\text{MinDisagree}}$)
   & -1 & -1 & -3 & -1 & -2 \\
10 & $N_{\text{inter}}^{-} - N_{\text{inter}}^{+}$ ($-f^{\text{MinCut}}$) 
   & 0 & 0 & -2 & 0 & -1 \\
11 & $N_{\text{intra}}^{+} - N_{\text{intra}}^{-} + N_{\text{inter}}^{-}$ 
   & 1 & 2 & -1 & 2 & 1 \\
12 & $N_{\text{intra}}^{+} - N_{\text{intra}}^{-} - N_{\text{inter}}^{+}$ 
   & 1 & 0 & -3 & 0 & -2 \\
13 & $N_{\text{intra}}^{+} + N_{\text{inter}}^{-} - N_{\text{inter}}^{+}$ 
   & 2 & 1 & -2 & 1 & -1 \\
14 & $-\,N_{\text{intra}}^{-} + N_{\text{inter}}^{-} - N_{\text{inter}}^{+}$ 
   & -1 & 0 & -3 & 0 & -1 \\
15 & $N_{\text{intra}}^{+} - N_{\text{intra}}^{-} + N_{\text{inter}}^{-} - N_{\text{inter}}^{+}$ ($f^{\text{full}}$)
   & 1 & 1 & -3 & 1 & -1 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\end{proof}

\notequiv*

\begin{proof}
If an object transitions from neutral to non-neutral, it may introduce agreements (positive intra-cluster or negative inter-cluster similarities) and/or disagreements (negative intra-cluster or positive inter-cluster similarities). An exception is objects with zero degree (zero similarity to all others), which can be assigned as neutral or non-neutral without affecting any of the five objectives. Thus, we only consider non-zero degree objects in the remainder of the proof.

The \emph{max agreement} objective (Eq. \ref{eq:maxagree}) considers only agreements. Making an object non-neutral either increases or maintains the objective but never decreases it, ensuring all objects become non-neutral. Conversely, the \emph{min disagreement} objective (Eq. \ref{eq:mindisagree}) considers only disagreements. Making an object non-neutral either decreases or maintains the objective but never improves it, ensuring all objects remain neutral.

Now, consider a clustering with $k$ non-neutral clusters, where all intra-cluster similarities are $+1$ and all inter-cluster similarities are $-1$. If an unassigned object $i \in V$ has similarity $+1$ to all others, the \emph{max correlation} objective (Eq. \ref{eq:maxcorr}) assigns it to the largest non-neutral cluster, while the \emph{minimum cut} objective (Eq. \ref{eq:mincut}) keeps it neutral. For $k > 2$, the full objective (Eq. \ref{eq:fullcc}) places $i$ in the largest non-neutral cluster if its size exceeds the sum of all others; otherwise, it remains neutral. Conversely, if object $i$ has similarity $-1$ to all others, \emph{max correlation} keeps it neutral, whereas \emph{minimum cut} assigns it to the smallest non-neutral cluster. For $k > 2$, the full objective may assign $i$ as neutral or non-neutral depending on cluster sizes.

Therefore, we conclude that \emph{max agreement} and \emph{min disagreement} differ fundamentally, always assigning all objects as non-neutral or neutral, respectively. Furthermore, from the two counterexamples above, \emph{max correlation}, \emph{minimum cut}, and the full objective are not equivalent and none of them guarantee that all objects are either neutral or non-neutral in all cases (meaning they are all different from \emph{max agreement} and \emph{min disagreement} in general).

For $k = 2$, the full objective always increases (or remains constant) when an object is made non-neutral, aligning it with \emph{max agreement}. To see this, consider a clustering with $k = 2$ non-neutral clusters, and let $M_{i,m} = \sum_{j \in S_m} A_{i,j}$ be the total similarity of object $i$ to cluster $m$. The impact on the objective when assigning $i$ to $m$ is $M_{i,m} - M_{i,p}$, where $p$ is the other cluster. Since this difference is always positive when $i$ is placed in its most similar cluster, assigning $i$ as non-neutral always improves the objective. Then, since all objects are non-neutral, the problem is equivalent to the $k$-CC problem where we know \emph{max agreement} and the full objective are equivalent (from Proposition \ref{prop:propcc}). For $k > 2$, this reasoning no longer holds, as contributions from other clusters can outweigh the within-cluster similarity to the most similar cluster (i.e., making the total contribution negative), potentially making neutrality optimal. However, we note that in our final objective (Eq. \ref{eq:ours}), when $\alpha$ and $\beta$ are involved, all terms will contribute with unique information even for $k = 2$.

\end{proof}

\nphard*

\begin{proof}
Fix $\alpha, \beta \in \mathbb{R}$ to any values. Assume that we know which objects in $V$ should be assigned to the neutral set $S_0$ in the optimal solution to the $k$-PCD problem. Then, let $V^{\prime} = V \setminus S_0$ and let $E^{\prime}$ be the set of edges between objects in $V^{\prime}$. Since no object in $V^{\prime}$ should be neutral, the problem reduces to finding a partition of $V^{\prime}$ that maximizes Eq. \ref{eq:ours}. We rewrite our objective in Eq. \ref{eq:ours} as

\begin{equation} \label{eq:oursappendix}
\begin{aligned}
    (N^+_{\text{intra}} - N^-_{\text{intra}}) + \alpha(N^-_{\text{inter}} - N^+_{\text{inter}}) - \beta \sum_{m \in [k]} |S_m|^2 &= \sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j} - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j} - \beta \sum_{m \in [k]} |S_m|^2 \\ 
    &= \sum_{m \in [k]} \sum_{i,j \in S_m} (A_{i,j} - \beta) - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j}.
\end{aligned}
\end{equation}

The second equality follows from Proposition \ref{prop:shift}. Defining $c_{\text{sim}} \triangleq \sum_{(i,j) \in E^{\prime}} A_{i,j}$, we obtain:

\begin{equation}
    \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j} = c_{\text{sim}} - \sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j}.
\end{equation}

Substituting this into Eq. \ref{eq:oursappendix} and simplifying:

\begin{equation} \label{eq:oursappendix2}
\begin{aligned}
    \sum_{m \in [k]} \sum_{i,j \in S_m} (A_{i,j} - \beta) - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j} &= \sum_{m \in [k]} \sum_{i,j \in S_m} (A_{i,j} - \beta) - \alpha(c_{\text{sim}} - \sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j}) \\
    &= (1+\alpha)\sum_{m \in [k]} \sum_{i,j \in S_m} (A_{i,j} - \beta) - \alpha c_{\text{sim}}.
\end{aligned}
\end{equation}

Defining $A^{\prime}_{i,j} = (1+\alpha)(A_{i,j} - \beta)$, we observe that since $c_{\text{sim}}$ is a constant across clustering solutions, the problem reduces to finding a partition of $V^{\prime}$ that maximizes $\sum_{m \in [k]} \sum_{i,j \in S_m} A^{\prime}_{i,j}$. This is equivalent to the \emph{max correlation} objective (Eq. \ref{eq:maxcorr}) applied to the transformed adjacency matrix $A^{\prime}$. By Proposition \ref{prop:propcc}, this objective is equivalent to the $k$-CC problem (Problem \ref{problem-kcc}). Thus, solving the $k$-PCD problem requires solving the $k$-CC problem on the instance $G^{\prime} = (V^{\prime}, E^{\prime})$, meaning $k$-PCD is at least as hard as $k$-CC. Since correlation clustering is NP-hard \citep{DBLP:journals/ml/BansalBC04, DBLP:journals/toc/GiotisG06}, we conclude that $k$-PCD is also NP-hard.    
\end{proof}

\shift*

\begin{proof}
    We have
    \begin{equation} \label{eq:shiftnew}
    \begin{aligned}
    &(N^+_{\text{intra}} - N^-_{\text{intra}}) + \alpha(N^-_{\text{inter}} - N^+_{\text{inter}}) - \beta \sum_{m \in [k]} |S_m|^2 =\\
    &=\sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j} - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j} - \beta \sum_{m \in [k]} |S_m|^2  \\
    &= \sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j} - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j} - \beta \sum_{m \in [k]}\sum_{i,j \in S_m} 1 \\
    &= \sum_{m \in [k]} \sum_{i,j \in S_m} A_{i,j} - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j} - \sum_{m \in [k]}\sum_{i,j \in S_m} \beta \\
    &= \sum_{m \in [k]} \sum_{i,j \in S_m} (A_{i,j} - \beta) - \alpha\sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} \sum_{\substack{i \in S_m \\ j\in S_p}} A_{i,j}.
    \end{aligned}
\end{equation}

The second equality (line 3) holds because the number of pairs of objects inside cluster $m$ is $|S_m|^2$. A similar regularization is established in \citep{Chehreghani22_shift} for the minimum cut objective, where it is shown that optimizing this minimum cut objective regularized with $-\beta \sum_{m \in [k]} |S_m|^2$ is equivalent to optimizing the max correlation objective (Eq. \ref{eq:maxcorr}) with similarities shifted by $\beta$. However, their result specifically considers the full network partitioning of unsigned networks, where the initial pairwise similarities are assumed non-negative. Moreover, they use a different regularization in practice: they shift the pairwise similarities so that the sum of the rows and columns of the similarity matrix becomes zero.  
\end{proof}

\discrete*

\begin{proof}

We begin by writing our objective function $f(\vx_{[n]})$ in Eq. \ref{eq:relaxedpcd} as follows.

\begin{equation} \label{eq:relaxedpcdappendix}
\begin{split}
    f(\vx_{[n]}) &= \sum_{(i,j) \in E}\sum_{m \in [k]} x_{im} x_{jm} (A_{i,j} - \beta) - \alpha \sum_{(i,j) \in E} \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} x_{im} x_{jp} A_{i,j} \\
    &= -\sum_{i \in [n]} \sum_{m \in [k]} x^2_{im} \beta 
    + \sum_{\substack{(i,j) \in E \\ i \neq j}}\sum_{m \in [k]} x_{im} x_{jm} (A_{i,j} - \beta) - \alpha \sum_{(i,j) \in E} \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} x_{im} x_{jp} A_{i,j} \\
    &= -\sum_{i \in [n]} \sum_{m \in [k]} x_{im} \beta 
    + \sum_{\substack{(i,j) \in E \\ i \neq j}}\sum_{m \in [k]} x_{im} x_{jm} (A_{i,j} - \beta) - \alpha \sum_{(i,j) \in E} \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} x_{im} x_{jp} A_{i,j}.
\end{split}
\end{equation}

In the second equality, we separate out the terms for $i = j$ and use that $A_{i,i} = 0$. In the third equality, we consider that $\vx_{[n]}$ is a discrete solution. This makes the first term linear instead of being quadratic w.r.t. $x_{im}$, which is a crucial step in proving the theorem. Let $f(\vx_i)$ denote $f(\vx_{[n]})$ when treating all blocks other than $\vx_i$ as constants. Then,

\begin{equation} \label{eq:multilinear}
\begin{split}
    f(\vx_i) &= -\sum_{m \in [k]} x_{im}\beta + 2\sum_{j \in [n] \setminus \{i\}}\sum_{m \in [k]} x_{im} x_{jm} (A_{i,j} - \beta) - 2\alpha\sum_{j \in [n] \setminus \{i\}}  \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} x_{im} x_{jp} A_{i,j} + C \\
    &= \sum_{m \in [k]} x_{im} \underbrace{\Big(-\beta + 2\sum_{j \in [n] \setminus \{i\}} \big( x_{jm} (A_{i,j} - \beta) - \alpha \sum_{p \in [k] \setminus \{m\}} x_{jp} A_{i,j} \big) \Big)}_{c_{im}} + C,
\end{split}
\end{equation}

where $C$ denotes terms independent of $\vx_i$. Define $\vc_i \in \mathbb{R}^{k+1}$ with elements

\begin{equation} \label{eq:cim}
    c_{im} \triangleq -\beta + 2\sum_{j \in [n] \setminus \{i\}} \big( x_{jm} (A_{i,j} - \beta) - \alpha \sum_{p \in [k] \setminus \{m\}} x_{jp} A_{i,j} \big), \quad \text{for } m \in [k], \quad c_{i0} \triangleq 0.
\end{equation}

Then, we obtain

\begin{equation} \label{eq:multilinear2}
    f(\vx_i) = \sum_{m \in \{0,\dots,k\}} x_{im} c_{im} + C = \vx_i^T \vc_i + C.
\end{equation}

Eq. \ref{eq:multilinear2} clearly illustrates that the contribution of the neutral component (index zero) of each $x_{im}$ is not included in the total objective (since $c_{i0} = 0$). From Eq. \ref{eq:multilinear2}, the gradient of $f(\vx_{[n]})$ w.r.t. $\vx_{i}$ is

\begin{equation} \label{eq:gradientappendix}
    \nabla_i f(\vx_{[n]}) = \vc_i.
\end{equation}

Let $\vc^{(t)}_i = \nabla_i f(\vx^{(t)}_{[n]})$ be the gradient of $f(\vx_{[n]})$ evaluated at the current solution $\vx_{[n]}^{(t)}$ (defined as in Eq. \ref{eq:cim}). The optimization problem on line 4 of Algorithm \ref{alg:blockfw} is

\begin{equation} \label{eq:lp}
    \vx_i^{\ast} = \argmax_{\vx_i \in \Delta^{k+1}} \vx_i^T \vc_i^{(t)}.
\end{equation}

Since Eq. \ref{eq:lp} is a linear program over the simplex $\Delta^{k+1}$, the optimal solution is obtained by setting $x_{im}^{\ast} = 1$ for $m = \argmax_{m \in \{0,\dots,k\}} c^{(t)}_{im}$ and $x_{ip}^{\ast} = 0$ for all $p \neq m$. This proves the first statement of part (a) of the theorem.

Next, we note that the difference $f(\vx^{\ast}_{[n]}) - f(\vx^{(t)}_{[n]})$ simplifies to $f(\vx^{\ast}_i) - f(\vx^{(t)}_i)$ (where $f(\vx_i)$ is defined in Eq. \ref{eq:multilinear}), since only the terms involving the variables in block $i$ change between $\vx^{\ast}_{[n]}$ and $\vx^{(t)}_{[n]}$. Therefore, we can derive the following.

\begin{equation} \label{eq:prtb}
\begin{aligned}
    f(\vx^{\ast}_{[n]}) - f(\vx^{(t)}_{[n]}) &= f(\vx^{\ast}_i) - f(\vx^{(t)}_i) \\
    &= ((\vx_i^{\ast})^T\vc_i^{\ast} + C) - ((\vx_i^{(t)})^T\vc_i^{(t)} + C) \\
    &= ((\vx_i^{\ast})^T\vc_i^{(t)} + C) - ((\vx_i^{(t)})^T\vc_i^{(t)} + C) \\
    &= ((\vx_i^{\ast})^T - (\vx_i^{(t)})^T)\vc_i^{(t)} \\
    &= (\vx_i^{\ast} - \vx_i^{(t)}) \cdot \nabla_i f(\vx^{(t)}_{[n]})
\end{aligned}
\end{equation}

Here, $\vc_i^{\ast}$ is defined as in Eq. \ref{eq:cim} w.r.t. $\vx^{\ast}_{[n]}$. Since $\vx^{\ast}_{[n]}$ and $\vx^{(t)}_{[n]}$ differ only in block $i$, and neither $\vc_i^{\ast}$ nor $\vc_i^{(t)}$ depend on the variables in block $i$, it follows that $\vc_i^{\ast} = \vc_i^{(t)}$, justifying the third equality. In Eq. \ref{eq:relaxedpcdappendix}, we assume that $\vx_{[n]}$ is discrete. To ensure this property holds throughout, we require that both $\vx^{\ast}_{[n]}$ and $\vx_{[n]}^{(t)}$ remain discrete for all $t \in \{0,\dots,T\}$. 

First, by assumption in the theorem, $\vx_{[n]}^{(0)}$ is discrete. From part (a), we know that $\vx_i^{\ast}$ is discrete, implying $\vx^{\ast}_{[n]}$ is discrete as long as $\vx_{[n]}^{(t)}$ is discrete. Furthermore, from Eq. \ref{eq:prtb}, the optimal solution $\vx_i^{\ast}$ in line 4 of Algorithm \ref{alg:blockfw} maximally increases the objective, which ensures the optimal step size in line 6 is $\gamma = 1$ (proving the second statement of part (a)). Consequently, $\vx_i^{(t+1)}$ remains discrete. By induction, this guarantees that $\vx^{(t)}_{[n]}$ is discrete for all $t$, ensuring Eq. \ref{eq:prtb} holds for all $t \in \{0,\dots,T\}$. This completes the proof of part (b) of the theorem.
    
\end{proof}

\convergence*

\begin{proof}
From Definition \ref{def:dg}, we have that, in our case, the FW duality gap is defined as
 \begin{equation}
    g(\vx_{[n]}) \triangleq \max_{\vs_i \in \Delta^{k+1}, \forall i \in [n]} (\vs_{[n]} - \vx_{[n]}) \cdot \nabla f(\vx_{[n]}).
 \end{equation}

Then, we recall that

\begin{equation} \label{eq:smallestgap}
    \tilde{g}_t = \min_{0\leq l \leq t-1} g(\vx^{(l)}_{[n]})
\end{equation}

is the smallest duality gap observed in Alg. \ref{alg:blockfw} up until step $t$. As established by \citep{DBLP:conf/icml/Lacoste-JulienJSP13} for general domains, the FW duality gap can be decomposed as follows.

\begin{equation}
    \begin{aligned}
        g(\vx_{[n]}) &\triangleq \max_{\vs_i \in \Delta^{k+1}, \forall i \in [n]} (\vs_{[n]} - \vx_{[n]}) \cdot \nabla f(\vx_{[n]}) \\
        &= \max_{\vs_i \in \Delta^{k+1}, \forall i \in [n]} \sum_{i \in [n]} (\vs_{i} - \vx_{i}) \cdot \nabla_i f(\vx_{[n]})  \\
        &= \sum_{i \in [n]} \underbrace{\max_{\vs_i \in \Delta^{k+1}} (\vs_{i} - \vx_{i}) \cdot \nabla_i f(\vx_{[n]})}_{\triangleq g_i(\vx_{[n]})} 
    \end{aligned}
\end{equation}

Let $g_i(\vx_{[n]}) \triangleq \max_{\vs_i \in \Delta^{k+1}} (\vs_{i} - \vx_{i}) \cdot \nabla_i f(\vx_{[n]})$ be the duality gap related to block $i$. We have that the FW duality gap is the sum of the gaps from each block: $g(\vx_{[n]}) = \sum_{i \in [n]}g_i(\vx_{[n]})$.

From Definition \ref{def:cr} (convergence rate), in order to prove the stated convergence rate, we need to show that $\mathbb{E}[\tilde{g}_t] \leq nh_0/t$. The structure of our proof is similar to the proof of Theorem 2 in \citep{DBLP:conf/aaai/ThielCD19}. However, here we adapt it to our problem and make the proof more rigorous (including correction of a mistake in the proof by \citep{DBLP:conf/aaai/ThielCD19}). A key difference is that our objective in Eq. \ref{eq:relaxedpcd} is not multilinear in the blocks $i$. Then, as shown in Eq. \ref{eq:relaxedpcdappendix} of Theorem \ref{prop:discrete}, the first quadratic term can be transformed into a linear one by assuming a discrete solution (which we showed holds at every step $t$).

In Alg. \ref{alg:blockfw}, a block $i \in [n]$ is chosen uniformly at random (on line 3). Therefore, we have

\begin{equation} \label{eq:uar}
\begin{aligned}
    \mathbb{E}[g_i(\vx^{(t)}_{[n]})|\vx^{(t)}_{[n]}] &= \sum_{i \in [n]} P(i \text{ is selected}) g_i(\vx^{(t)}_{[n]}) \\
    &= \sum_{i \in [n]} \frac{1}{n}g_i(\vx^{(t)}_{[n]}) \\
    &= \frac{1}{n} \sum_{i \in [n]} \max_{\vs^{(t)}_i \in \Delta^{k+1}} (\vs_{i} - \vx^{(t)}_{i}) \cdot \nabla_i f(\vx^{(t)}_{[n]}) \\
    &= \frac{1}{n} g(\vx_{[n]}^{(t)}).
\end{aligned}
\end{equation}

We now take an expectation w.r.t. $\vx^{(t)}_{[n]}$ on both sides and obtain


\begin{equation} \label{eq:uarexp}
\begin{aligned}
    \mathbb{E}[\mathbb{E}[g_i(\vx^{(t)}_{[n]})|\vx^{(t)}_{[n]}]] &= \frac{1}{n} \mathbb{E}[g(\vx_{[n]}^{(t)})] \\
    &= \mathbb{E}[g_i(\vx_{[n]}^{(t)})],
\end{aligned}
\end{equation}

where the last equality follows from the Law of Total Expectation (i.e., that $\mathbb{E}_Y[\mathbb{E}_X{[X|Y]}] = \mathbb{E}_X{[X}]$, where $X$ and $Y$ are random variables). We therefore have that $\frac{1}{n} \mathbb{E}[g(\vx_{[n]}^{(t)})] = \mathbb{E}[g_i(\vx_{[n]}^{(t)})]$, where the expectation is w.r.t. all randomly chosen blocks $i$ before step $t$. Now, from Theorem \ref{prop:discrete} we have that our objective satisfies

\begin{equation}
    f(\vx^{(t+1)}_{[n]}) - f(\vx^{(t)}_{[n]}) = g_i(\vx_{[n]}^{(t)}) =  \max_{\vs^{(t)}_i \in \Delta^{k+1}} (\vs_{i} - \vx^{(t)}_{i}) \cdot \nabla_i f(\vx^{(t)}_{[n]}).
\end{equation}

Then, we have

\begin{equation} \label{eq:ff}
\begin{aligned}
    \frac{1}{n} \sum_{t = 0}^{T-1} \mathbb{E}[g(\vx^{(t)})] &= \sum_{t = 0}^{T-1} \mathbb{E}[g_i(\vx_{[n]}^{(t)})] \\
    &= \sum_{t = 0}^{T-1} \mathbb{E}[f(\vx^{(t+1)}_{[n]}) - f(\vx^{(t)}_{[n]})] \\
    &= \mathbb{E}[f(\vx^{(T)}_{[n]})] - f(\vx^{(0)}_{[n]}) \\
    &\leq OPT - f(\vx_{[n]}^{(0)}),
\end{aligned}
\end{equation}

where the third equality is due to the \emph{telescoping rule} and $OPT$ is the objective value of the optimal clustering solution to Problem \ref{problem-kcc} ($k$-PCD). On the other hand, we have

\begin{equation}
    \frac{1}{n} \sum_{t = 0}^{T-1} \mathbb{E}[g(\vx^{(t)})] \geq \frac{T}{n}\mathbb{E}[\tilde{g}_T],
\end{equation}

where $\tilde{g}_t$ is defined as in Eq. \ref{eq:smallestgap} (the smallest gap observed until step $t$). Therefore,

\begin{equation} \label{eq:finalconv}
\begin{aligned}
    \frac{T}{n}\mathbb{E}[\tilde{g}_T] &\leq OPT - f(\vx^{(0)}) \\
    \Rightarrow \mathbb{E}[\tilde{g}_T] &\leq \frac{n(OPT - f(\vx^{(0)}))}{T}.
\end{aligned}
\end{equation}

The value of $OPT$ depends on the particular instance. In order to obtain an instance-independent bound, we use that $OPT - f(\vx^{(0)}) \leq \sum_{(i,j) \in E} |A_{i,j}|$ resulting in 

\begin{equation} \label{eq:finalconv2}
    \mathbb{E}[\tilde{g}_T] \leq \frac{n\sum_{(i,j) \in E} |A_{i,j}|}{T}.
\end{equation}

which we aimed to show since it holds for any $T$.
\end{proof}

\equivalent*

\begin{proof}
From part (a) of Theorem \ref{prop:discrete}, the current solution, $\vx^{(t)}_{[n]}$, remains discrete (i.e., hard cluster assignments) at every step of Alg. \ref{alg:blockfw} for all $i \in [n]$. Moreover, each step of Alg. \ref{alg:blockfw} consists of placing object $i$ in the cluster $m \in \{0,\dots,k\}$ with maximal gradient $G_{i,m}$. By part (b) of Theorem \ref{prop:discrete}, this is equivalent to placing object $i$ in the cluster that maximally improves our objective in Eq. \ref{eq:ours}.
\end{proof}

\grad*

\begin{proof}
From Theorem \ref{prop:discrete}, we recall that since the current solution $\vx_{[n]}^{(t)}$ always remains discrete, our objective can be written as

\begin{equation} \label{eq:relaxedpcdappendix2}
    f(\vx^{(t)}_{[n]}) = -\sum_{i \in [n]} \sum_{m \in [k]} x^{(t)}_{im} \beta 
    + \sum_{\substack{(i,j) \in E \\ i \neq j}}\sum_{m \in [k]} x^{(t)}_{im} x^{(t)}_{jm} (A_{i,j} - \beta) - \alpha \sum_{(i,j) \in E} \sum_{m \in [k]} \sum_{p \in [k] \setminus \{m\}} x^{(t)}_{im} x^{(t)}_{jp} A_{i,j}.
\end{equation}

We let $f(\vx^{(t)}_i)$ denote $f(\vx^{(t)}_{[n]})$ when treating all blocks other than $\vx^{(t)}_i$ as constants. Then,

\begin{equation} \label{eq:multilinear99}
    f(\vx^{(t)}_i) = \sum_{m \in [k]} x^{(t)}_{im} \big(\underbrace{-\beta + 2\sum_{j \in [n] \setminus \{i\}} x^{(t)}_{jm} (A_{i,j} - \beta) - 2\alpha \sum_{j \in [n] \setminus \{i\}}\sum_{p \in [k] \setminus \{m\}} x^{(t)}_{jp} A_{i,j}}_{c_{im}}\big) + C.
\end{equation}

Therefore, we have

\begin{equation}
    G_{i,m} \triangleq [\nabla_i f(\vx^{(t)}_{[n]})]_m = c_{im}, \quad \text{for } m \in [k].
\end{equation}

This holds because neither $c_{im}$ nor $C$ depend on $x^{(t)}_{im}$. Furthermore, since $x_{i0}$ does not show up in Eq. \ref{eq:multilinear99}, everything in Eq. \ref{eq:multilinear99} is a constant w.r.t. $x_{i0}$. We therefore have

\begin{equation}
    G_{i,0} \triangleq [\nabla_i f(\vx^{(t)}_{[n]})]_0 = 0.
\end{equation}

By noting that $\vx^{(t)}$ is discrete, we can rewrite $c_{im}$ for $m \in [k]$ as follows.

\begin{equation} \label{eq:gradderiv}
    \begin{aligned}
        c_{im} &= -\beta + 2\sum_{j \in [n] \setminus \{i\}} x^{(t)}_{jm} (A_{i,j} - \beta) - 2\alpha \sum_{j \in [n] \setminus \{i\}}\sum_{p \in [k] \setminus \{m\}} x^{(t)}_{jp} A_{i,j} \\
        &= -\beta + 2\sum_{j \in [n] \setminus \{i\}} x^{(t)}_{jm} A_{i,j} - 2\sum_{j \in [n] \setminus \{i\}}\beta - 2\alpha \sum_{j \in [n] \setminus \{i\}}\sum_{p \in [k] \setminus \{m\}} x^{(t)}_{jp} A_{i,j} \\
        &= -\beta + 2\sum_{j \in S_m \setminus \{i\}} A_{i,j} - 2\sum_{j \in S_m \setminus \{i\}}\beta - 2\alpha \sum_{p \in [k] \setminus \{m\}} \sum_{j \in S_p \setminus \{i\}} A_{i,j} \\
        &= -\beta + 2\sum_{j \in S_m \setminus \{i\}} A_{i,j} - 2|S_m|\beta + 2\beta \1_{[i \in S_m]} - 2\alpha \sum_{p \in [k] \setminus \{m\}} \sum_{j \in S_p \setminus \{i\}} A_{i,j}.
    \end{aligned}
\end{equation}

In the last equality we use $-2\sum_{j \in S_m \setminus \{i\}}\beta = -2|S_m|\beta + 2\beta \1_{[i \in S_m]}$. We note that computing the final expression in Eq. \ref{eq:gradderiv} is $O(k^2n)$, due to the last term. However, by noting that $\sum_{p \in [k] \setminus \{m\}} \sum_{j \in S_p \setminus \{i\}} A_{i,j} = \sum_{j \notin S_0} A_{i,j} - \sum_{j \in S_m \setminus \{i\}} A_{i,j}$ we can derive the following.

\begin{equation} \label{eq:gradderiv2}
    \begin{aligned}
        c_{im} &= -\beta + 2\sum_{j \in S_m \setminus \{i\}} A_{i,j} - 2|S_m|\beta + 2\beta \1_{[i \in S_m]} - 2\alpha \sum_{p \in [k] \setminus \{m\}} \sum_{j \in S_p \setminus \{i\}} A_{i,j} \\
        &= -\beta + 2\sum_{j \in S_m \setminus \{i\}} A_{i,j} - 2|S_m|\beta + 2\beta \1_{[i \in S_m]} - 2\alpha \big(\sum_{j \notin S_0} A_{i,j} - \sum_{j \in S_m \setminus \{i\}} A_{i,j}\big) \\
        &= -\beta + 2\sum_{j \in S_m \setminus \{i\}} (A_{i,j} +\alpha A_{i,j}) - 2|S_m|\beta + 2\beta \1_{[i \in S_m]} - 2\alpha\sum_{j \notin S_0} A_{i,j} \\
        &= -\beta + 2(1+\alpha)\sum_{j \in S_m \setminus \{i\}} A_{i,j} - 2|S_m|\beta + 2\beta \1_{[i \in S_m]} - 2\alpha\sum_{j \notin S_0} A_{i,j} 
    \end{aligned}
\end{equation}

Now, recall the defintions $M_{i,m} \triangleq 2\sum_{j \in S_m} A_{i,j}$, $\eta_i \triangleq \sum_{m\in[k]} M_{i,m}$ and $\beta_{im} = 2\beta|S_m| - 2\beta\1_{[i\in S_m]}$. Then, we note that $2\sum_{j \notin S_0} A_{i,j} = \sum_{m\in[k]} M_{i,m}$ (sum of all similarities from object $i$ to all non-neutral objects). Then, we obtain the final expression, i.e.,

\begin{equation} \label{eq:gradfinal}
    c_{im} = -\beta + (1+\alpha)M_{i,m} - \beta_{im} - \alpha \eta_i, \quad \text{for } m \in [k],
\end{equation}

The expression in Eq. \ref{eq:gradfinal} can be computed in $O(kn)$ since it just amounts to computing $M_{i,m}$ for all $m \in [k]$ (i.e., the total similarity from $i$ to all non-neutral cluters $m \in [k]$).

\end{proof}

\betaprop*

\begin{proof}
By examining the gradient in Eq. \ref{eq:gradfinal}, we observe that the dominant term involving $\beta$ is $-\beta|S_m|$. Consequently, making $\beta$ large and negative \emph{increases} the incentive to assign objects to non-neutral clusters. Moreover, since $-\beta|S_m|$ scales with cluster size, the local search procedure will favor placing an object $i$ in the largest non-neutral cluster. If $\beta$ is sufficiently large and negative, this term will completely dominate the objective, ensuring that no object is assigned to the neutral set (as the contribution to all non-neutral clusters remains positive). Ultimately, all objects will be placed in the largest non-neutral cluster.  

Similarly, if $\beta$ is made very large and positive, $-\beta|S_m|$ will eventually dominate the objective, making the contribution to every non-neutral cluster negative for all objects. As a result, all objects will be assigned to the neutral set.  
\end{proof}



\section{Experiments: More Details and Further Results} \label{appendix:experiments}
%In this section, we provide additional details about the datasets and baselines. In addition, we include more experimental results further demonstrating the effectiveness of our method.
\subsection{Datasets} \label{appendix:datasets}

Following \citep{DBLP:conf/nips/TzengOG20}, we consider the following widely studied real-world signed networks. \textbf{WoW-EP8} (\texttt{W8}) \citep{DBLP:conf/www/KristofGT20} represents interactions among authors in the 8th EU Parliament legislature, where edge signs indicate collaboration or competition. \textbf{Bitcoin} (\texttt{BTC}) \citep{snapnets} is a trust-distrust network of users trading on the Bitcoin OTC platform. \textbf{WikiVot} (\texttt{WikiV}) \citep{snapnets} records positive and negative votes for Wikipedia admin elections. \textbf{Referendum} (\texttt{REF}) \citep{DBLP:conf/nldb/LaiPRR18} captures tweets about the 2016 Italian constitutional referendum, with edge signs indicating whether users share the same stance. \textbf{Slashdot} (\texttt{SD}) \citep{snapnets} is a friend-foe network from the Slashdot Zoo feature. \textbf{WikiCon} (\texttt{WikiC}) \citep{DBLP:conf/www/Kunegis13} tracks positive and negative interactions between users editing English Wikipedia. \textbf{Epinions} (\texttt{EP}) \citep{snapnets} represents the trust-distrust relationships in the Epinions online social network. \textbf{WikiPol} (\texttt{WikiP}) \citep{DBLP:conf/www/ManiuAC11} captures interactions among users editing Wikipedia pages on political topics.


\subsection{Baselines} \label{appendix:baselines}

For SCG, we use the public implementation from \citep{scg}. For KOCG, we use the public implementation from \citep{kocg} with default hyperparameters: $\alpha = 1/(k-1)$, $\beta = 50$ (note that the purpose of this $\beta$ differs from the one used in our paper), and $\ell = 5000$. For the spectral methods SPONGE and BNC, we use the public implementations from \citep{spectralmethods}. Following \citep{DBLP:conf/nips/TzengOG20}, for SPONGE, we evaluate both the \emph{unnormalized} and \emph{symmetric normalized} versions and report results for the best-performing method.


\subsection{Complete Results on All Datasets} \label{appendix:results}

Tables \ref{table:table3}-\ref{table:table4} report results for polarity and BA-Polarity with $k = 2$, $k = 4$, and $k = 6$. The conclusions for $k = 2$ are consistent with those for $k = 4$ and $k = 6$, discussed in the main paper. According to these results, our method consistently achieves the highest scores or performs competitively with the best scores.

Tables \ref{wow8-table}-\ref{wikipol-table} present detailed analyses for all datasets across 12 different aspects, including polarity and BA-polarity. %No single metric alone determines which method is superior; rather, they 
They provide insights into the differences between the clustering solutions computed by each method. In the context of PCD, evaluating clustering quality is inherently subjective, as different aspects capture different aspects of the solution. Generally, there is a trade-off among these aspects, and the objective is to achieve a good balance between them. Below, we define the aspects used in our analysis. We have defined these aspects such that a larger number is better (apart from runtime).

Let $N = \sum_{m \in [k]} |S_m|$ denote the number of non-neutral objects, and let $N_{\text{nz}} = N^+_{\text{intra}} +  N^-_{\text{intra}} + N^-_{\text{inter}} + N^+_{\text{inter}}$ represent the number of non-zero similarities between non-neutral objects. 

\begin{itemize}
    \item \texttt{SIZE} = $N$: The total number of non-neutral objects. 
    \item \texttt{BAL} = $S_m/S_l$: The balance factor, where $S_m$ and $S_l$ are the largest and smallest non-neutral clusters, respectively. Its range it $[0,1]$, with $1$ indicating perfect balance.
    \item \texttt{K}: The number of non-empty non-neutral clusters.
    \item \texttt{MAC}: \emph{Mean Average Cohesion}, quantifying the density of positive intra-cluster similarities, defined as 
        \begin{equation*}
            \texttt{MAC} = \frac{1}{k} \sum_{m \in [k]} \frac{1}{|S_m|(|S_m| - 1)} \sum_{i,j \in S_m} A^+_{i,j}.
        \end{equation*}
        Its range is $[0,1]$, where higher values indicate stronger cohesion within clusters.
    \item \texttt{MAO}: \emph{Mean Average Opposition}, measuring the density of negative inter-cluster similarities, defined as 
        \begin{equation*}
            \texttt{MAO} =\frac{1}{k(k-1)} \sum_{\substack{m \in [k] \\ p \in [k] \setminus \{m\}}} \frac{1}{|S_m||S_p|} \sum_{\substack{i \in S_m \\ j \in S_p}} A^-_{i,j}.
        \end{equation*}
        Its range is $[0,1]$, where higher values indicate stronger opposition between clusters.
    \item \texttt{CC+}: Measures the fraction of intra-cluster similarities that are positive minus those that are negative, defined as
        \begin{equation*}
            \texttt{CC+} = \frac{N^+_{\text{intra}} -  N^-_{\text{intra}}}{N^+_{\text{intra}} + N^-_{\text{intra}}}.
        \end{equation*}
        Its range is $[-1,1]$, where $-1$ indicates that all non-zero intra-cluster similarities are negative, and $+1$ indicates that all are positive.
    \item \texttt{CC-}: Measures the fraction of inter-cluster similarities that are negative minus those that are positive, defined as
        \begin{equation*}
            \texttt{CC-} = \frac{N^-_{\text{inter}} -  N^+_{\text{inter}}}{N^-_{\text{inter}} + N^+_{\text{inter}}}.
        \end{equation*}
        Its range is $[-1,1]$, where $-1$ indicates that all non-zero inter-cluster similarities are positive, and $+1$ indicates that all are negative.
    \item \texttt{DENS}: The proportion of non-zero similarities among non-neutral objects, defined as
        \begin{equation*}
            \texttt{DENS} = \frac{N_{\text{nz}}}{N(N-1)}.
        \end{equation*}
        Its range is $[0,1]$, with higher values indicating denser connectivity.
    \item \texttt{ISO}: \emph{Isolation}, measuring the separation between non-neutral and neutral objects, defined as
        \begin{equation*}
            \texttt{ISO} = \frac{N_{\text{nz}}}{N_{\text{nz}} + \sum_{i\in S_0}\sum_{j \notin S_0} |A_{i,j}|}.
        \end{equation*}
        Its range is $[0,1]$, where \texttt{ISO} = $1$ means non-neutral objects are fully isolated from neutral ones, meaning no non-zero edges exist between them (which is ideal).
    \item \texttt{POL}: \emph{Polarity}, as defined in the main paper.
    \item \texttt{BA-POL}: \emph{Balance-aware polarity}, as defined in the main paper.
    \item \texttt{RT}: Runtime of the corresponding method in seconds.
\end{itemize}

From Tables \ref{wow8-table}-\ref{wikipol-table}, we derive several key insights. Firstly, our method exhibits low standard deviation, indicating robustness to the initial random solution. It consistently finds high-polarity solutions while maintaining better balance than its main competitor, SCG. Unlike some baselines such as KOCG and SPONGE, our method does not enforce excessive balance, ensuring solutions remain both high in polarity and reasonably balanced.

In terms of runtime, our method is efficient and competitive with the baselines. It is also consistently ranked among the best in both \texttt{DENS} and \texttt{ISO}. Unlike SCG, our method always identifies $k$ non-empty non-neutral clusters, which we argue is a significant limitation of SCG. 

While SCG generally achieves higher MAC values, this is largely due to its tendency to produce highly imbalanced solutions, often with singleton clusters. Since small or singleton clusters trivially yield high average cohesion (values close to 1), they can disproportionately inflate the overall MAC score.

Finally, our method performs comparably or better than SCG in \texttt{CC+} and significantly outperforms it in \texttt{CC-} in most cases. This highlights another limitation of SCG: it often includes more positive similarities between clusters than negative ones, as reflected by negative \texttt{CC-} values. Some baselines produce either overly large or overly small non-neutral clusters (based on \texttt{SIZE}), whereas our method consistently finds solutions with a reasonable number of non-neutral objects (which consequently leads to a good balance of the other aspects), similar to SCG. However, we note that we can easily adjust the number of non-neutral objects by adjusting $\beta$, as discussed in the paper.

\begin{figure}[t!] %htb!
\centering 
\subfigure[Epinions, $k=2$, $\beta=0.1$]{\label{subfig:s1}\includegraphics[width=0.49\linewidth]{imgs/alpha.png}}
\subfigure[Wikipol, $k=6$, $\alpha=\frac{1}{k-1}$]{\label{subfig:s2}\includegraphics[width=0.49\linewidth]{imgs/beta1.png}}
%\subfigure[]
%{\label{subfig:s3}\includegraphics[width=0.49\linewidth]{imgs/beta2.png}}
%\subfigure[]
%{\label{subfig:s4}\includegraphics[width=0.49\linewidth]{imgs/beta3.png}}
\caption{Investigation of the impact of $\alpha$ and $\beta$.}
\label{fig:f2}
\end{figure}

In Figure \ref{fig:f2}, we illustrate the impact of varying $\alpha$ and $\beta$. According to Figure \ref{subfig:s1},   increasing $\alpha$ naturally balances intra-cluster cohesion and inter-cluster opposition. The size proportion, defined as the fraction of non-neutral objects in $V$, remains constant as $\alpha$ varies. Figure \ref{subfig:s2} shows that increasing $\beta$ monotonically reduces the number of non-neutral objects, leading to denser clusters, as indicated by improved MAC and MAO scores. Notably, balance remains stable across different $\beta$ values, unlike the baseline SCG-MA.


%%
%%\begin{equation}
%$\text{MAC} = \frac{1}{k} \sum_{m \in [k]} \frac{1}{|S_m|(|S_m| - 1)} \sum_{i,j \in S_m} A^+_{i,j}$
%%\end{equation}
%%
%and \emph{mean average opposition} (MAO) as
%%
%%\begin{equation}
%$\text{MAO} = \frac{1}{k(k-1)} \sum_{\substack{m \in [k] \\ p \in [k] \setminus \{m\}}} \frac{1}{|S_m||S_p|} \sum_{\substack{i \in S_m \\ j \in S_p}} A^-_{i,j}.$ 
%%\end{equation}
%%
%MAC and MAO quantify the density of positive similarities within clusters and negative similarities between clusters. 



%\begin{table*}[htb!]
%    \caption{Results for all methods/datasets for the \emph{balance-aware polarity} metric.}
%    \label{table:table3}
%    \vskip 0.15in
%    \begin{center}
%    \begin{small}
%    \begin{sc}
%    %\renewcommand{\arraystretch}{0.6}
%    \begin{tabular}{clcccccccc}
%        \toprule
%        & & \texttt{W8} & \texttt{BTC} & \texttt{WikiV} & \texttt{REF} & \texttt{SD} & \texttt{WikiC} & \texttt{EP} & \texttt{WikiP} \\
%        \midrule
%        & $|V|$ & 790 & 5.9K & 7.1K & 10.9K & 82.1K & 116.7K & 131.6K & 138.6K \\
%        & $|E|$ & 116K & 214.5K & 1M & 251.4K & 500.5K & 2M & 711.2K & 715.9K \\
%        & $|E_-|/|E|$ & 0.2 & 0.2 & 0.2 & 0.1 & 0.2 & 0.6 & 0.2 & 0.1 \\
%\midrule
%$k = 2$ & LSPCD (ours) & 20.6 & \textbf{20.8} & \textbf{26.9} & \textbf{64.4} & \textbf{33.5} & \textbf{106.9} & \textbf{57.3} & \textbf{17.4} \\
% & SCG-MA & 0.4 & 2.4 & 0.4 & 1.3 & 0.8 & 44.7 & 2.5 & 0.6 \\
% & SCG-MO & 0.5 & 0.6 & 0.4 & 1.3 & 0.7 & 39.6 & 2.4 & 0.4 \\
% & SCG-B & 4.9 & 19.7 & 0.7 & 1.7 & 1.5 & 47.3 & 3.8 & 0.9 \\
% & SCG-R & 3.4 & 1.8 & 4.6 & 2.1 & 1.1 & 31.9 & 6.6 & 3.0 \\
% & KOCG-top-$1$ & 10.4 & 1.0 & 3.8 & 4.8 & 1.3 & 3.5 & 3.3 & 1.8 \\
% & KOCG-top-$r$ & 11.3 & 3.3 & 2.2 & 11.7 & 2.2 & 2.9 & 10.1 & 1.1 \\
% & BNC-$(k+1)$ & -0.4 & -0.9 & -0.6 & -1.0 & --- & --- & --- & --- \\
% & BNC-$k$ & 0.7 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
% & SPONGE-$(k+1)$ & 19.7 & 0.6 & 0.3 & 0.6 & --- & --- & --- & --- \\
% & SPONGE-$k$ & \textbf{22.4} & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
%\midrule
%$k = 4$ & LSPCD (ours) & 7.5 & \textbf{5.0} & \textbf{9.5} & \textbf{5.5} & \textbf{11.2} & \textbf{34.5} & \textbf{14.9} & \textbf{8.4} \\
% & SCG-MA & 5.8 & 0.3 & 0.8 & 0.3 & 1.9 & 0.7 & 0.1 & 0.7 \\
% & SCG-MO & 4.9 & 0.3 & 0.4 & 0.4 & 0.2 & 6.6 & 0.1 & 0.0 \\
% & SCG-B & 0.3 & 0.1 & 0.2 & 0.1 & 0.5 & 18.5 & 0.2 & 0.1 \\
% & SCG-R & 1.3 & 0.9 & 0.9 & 0.6 & 2.4 & 2.0 & 0.3 & 1.0 \\
% & KOCG-top-$1$ & 5.8 & 4.2 & 1.2 & 2.6 & 0.9 & 0.4 & 4.0 & 0.9 \\
% & KOCG-top-$r$ & 5.4 & 2.7 & 2.6 & 1.6 & 1.0 & 2.8 & 5.9 & 1.2 \\
% & BNC-$(k+1)$ & -0.1 & -0.8 & -0.4 & -1.0 & --- & --- & --- & --- \\
% & BNC-$k$ & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
% & SPONGE-$(k+1)$ & \textbf{22.9} & 0.0 & 0.4 & 0.3 & --- & --- & --- & --- \\
% & SPONGE-$k$ & 18.6 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
%\midrule
%$k = 6$ & LSPCD (ours) & 4.9 & \textbf{3.4} & \textbf{4.3} & 3.1 & \textbf{6.3} & \textbf{20.6} & \textbf{10.6} & \textbf{6.1} \\
% & SCG-MA & 3.1 & 0.1 & 0.2 & 0.1 & 0.0 & 0.6 & 0.4 & 0.0 \\
% & SCG-MO & 3.6 & 0.1 & 0.2 & 0.1 & 0.1 & 4.2 & 0.1 & 0.0 \\
% & SCG-B & 0.3 & 0.2 & 0.2 & 0.1 & 0.2 & 1.8 & 0.2 & 0.1 \\
% & SCG-R & 0.3 & 0.3 & 0.4 & 0.2 & 0.4 & 2.3 & 0.1 & 0.0 \\
% & KOCG-top-$1$ & 6.1 & 1.7 & 2.5 & \textbf{3.3} & 1.7 & 0.4 & 2.7 & 3.7 \\
% & KOCG-top-$r$ & 5.0 & 1.2 & 1.8 & 2.5 & 1.3 & 1.1 & 2.8 & 0.7 \\
% & BNC-$(k+1)$ & -0.1 & -0.1 & -0.8 & -0.4 & --- & --- & --- & --- \\
% & BNC-$k$ & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
% & SPONGE-$(k+1)$ & \textbf{17.7} & 0.0 & 0.4 & 0.6 & --- & --- & --- & --- \\
% & SPONGE-$k$ & 16.0 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
%        \bottomrule
%    \end{tabular}
%    \end{sc}
%    \end{small}
%    \end{center}
%    \vskip -0.1in
%\end{table*}

\begin{table*}[h!]
   \caption{Results for different methods and datasets w.r.t. the \emph{balance-aware polarity} metric. $|E|$ is the number of non-zero edges and $|E_-|$ is the number of negative edges. We observe that our method (LSPCD) often yields the highest scores.}
   \label{table:table3}
   \vskip 0.15in
   \begin{center}
   \begin{small}
   \begin{sc}
   \renewcommand{\arraystretch}{0.6}
   \begin{tabular}{clcccccccc}
       \toprule
       & & \texttt{W8} & \texttt{BTC} & \texttt{WikiV} & \texttt{REF} & \texttt{SD} & \texttt{WikiC} & \texttt{EP} & \texttt{WikiP} \\
       \midrule
       & $|V|$ & 790 & 5.9K & 7.1K & 10.9K & 82.1K & 116.7K & 131.6K & 138.6K \\
       & $|E|$ & 116K & 214.5K & 1M & 251.4K & 500.5K & 2M & 711.2K & 715.9K \\
       & $|E_-|/|E|$ & 0.2 & 0.2 & 0.2 & 0.1 & 0.2 & 0.6 & 0.2 & 0.1 \\
\midrule
$k = 2$  & LSPCD (ours) & 19.4 & 10.9 & \textbf{13.9} & \textbf{61.8} & \textbf{9.7} & \textbf{102.2} & \textbf{56.2} & \textbf{12.4} \\
& SCG-MA & 0.4 & 2.4 & 0.4 & 1.3 & 0.8 & 44.7 & 2.5 & 0.6 \\
& SCG-MO & 0.5 & 0.6 & 0.4 & 1.3 & 0.7 & 39.6 & 2.4 & 0.4 \\
& SCG-B & 4.9 & \textbf{19.7} & 0.7 & 1.7 & 1.5 & 47.3 & 3.8 & 0.9 \\
& SCG-R & 3.4 & 1.8 & 4.6 & 2.1 & 1.1 & 31.9 & 6.6 & 3.0 \\
& KOCG-top-$1$ & 10.4 & 1.0 & 3.8 & 4.8 & 1.3 & 3.5 & 3.3 & 1.8 \\
& KOCG-top-$r$ & 11.3 & 3.3 & 2.2 & 11.7 & 2.2 & 2.9 & 10.1 & 1.1 \\
& BNC-$(k+1)$ & -0.4 & -0.9 & -0.6 & -1.0 & --- & --- & --- & --- \\
& BNC-$k$ & 0.7 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
& SPONGE-$(k+1)$ & 19.7 & 0.6 & 0.3 & 0.6 & --- & --- & --- & --- \\
& SPONGE-$k$ & \textbf{22.4} & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
\midrule
$k = 4$  & LSPCD (ours) & 6.9 & 3.6 & \textbf{5.9} & \textbf{3.9} & \textbf{6.7} & \textbf{26.3} & \textbf{14.9} & \textbf{5.4} \\
& SCG-MA & 5.8 & 0.3 & 0.8 & 0.3 & 1.9 & 0.7 & 0.1 & 0.7 \\
& SCG-MO & 4.9 & 0.3 & 0.4 & 0.4 & 0.2 & 6.6 & 0.1 & 0.0 \\
& SCG-B & 0.3 & 0.1 & 0.2 & 0.1 & 0.5 & 18.5 & 0.2 & 0.1 \\
& SCG-R & 1.3 & 0.9 & 0.9 & 0.6 & 2.4 & 2.0 & 0.3 & 1.0 \\
& KOCG-top-$1$ & 5.8 & \textbf{4.2} & 1.2 & 2.6 & 0.9 & 0.4 & 4.0 & 0.9 \\
& KOCG-top-$r$ & 5.4 & 2.7 & 2.6 & 1.6 & 1.0 & 2.8 & 5.9 & 1.2 \\
& BNC-$(k+1)$ & -0.1 & -0.8 & -0.4 & -1.0 & --- & --- & --- & --- \\
& BNC-$k$ & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
& SPONGE-$(k+1)$ & \textbf{22.9} & 0.0 & 0.4 & 0.3 & --- & --- & --- & --- \\
& SPONGE-$k$ & 18.6 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
\midrule
$k = 6$  & LSPCD (ours) & 4.4 & \textbf{2.2} & \textbf{3.6} & 2.2 & \textbf{3.8} & \textbf{16.4} & \textbf{6.8} & 3.5 \\
& SCG-MA & 3.1 & 0.1 & 0.2 & 0.1 & 0.0 & 0.6 & 0.4 & 0.0 \\
& SCG-MO & 3.6 & 0.1 & 0.2 & 0.1 & 0.1 & 4.2 & 0.1 & 0.0 \\
& SCG-B & 0.3 & 0.2 & 0.2 & 0.1 & 0.2 & 1.8 & 0.2 & 0.1 \\
& SCG-R & 0.3 & 0.3 & 0.4 & 0.2 & 0.4 & 2.3 & 0.1 & 0.0 \\
& KOCG-top-$1$ & 6.1 & 1.7 & 2.5 & \textbf{3.3} & 1.7 & 0.4 & 2.7 & \textbf{3.7} \\
& KOCG-top-$r$ & 5.0 & 1.2 & 1.8 & 2.5 & 1.3 & 1.1 & 2.8 & 0.7 \\
& BNC-$(k+1)$ & -0.1 & -0.1 & -0.8 & -0.4 & --- & --- & --- & --- \\
& BNC-$k$ & 0.5 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
& SPONGE-$(k+1)$ & \textbf{17.7} & 0.0 & 0.4 & 0.6 & --- & --- & --- & --- \\
& SPONGE-$k$ & 16.0 & 0.0 & 0.0 & 0.0 & --- & --- & --- & --- \\
       \bottomrule
   \end{tabular}
   \end{sc}
   \end{small}
   \end{center}
   \vskip -0.1in
\end{table*}

\begin{table*}[h!]
    \caption{Results for different methods and datasets w.r.t. the \emph{polarity} metric. $|E|$ is the number of non-zero edges and $|E_-|$ is the number of negative edges. Our method (LSPCD) yields the best or competitive results, despite the fact that it, unlike SCG methods, does not directly optimize polarity.}
    \label{table:table4}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccc}
        \toprule
        & & \texttt{W8} & \texttt{BTC} & \texttt{WikiV} & \texttt{REF} & \texttt{SD} & \texttt{WikiC} & \texttt{EP} & \texttt{WikiP} \\
        \midrule
        & $|V|$ & 790 & 5.9K & 7.1K & 10.9K & 82.1K & 116.7K & 131.6K & 138.6K \\
        & $|E|$ & 116K & 214.5K & 1M & 251.4K & 500.5K & 2M & 711.2K & 715.9K \\
        & $|E_-|/|E|$ & 0.2 & 0.2 & 0.2 & 0.1 & 0.2 & 0.6 & 0.2 & 0.1 \\
\midrule
$k = 2$  & LSPCD (ours) & 223.4 & 29.0 & 62.3 & 146.1 & 75.9 & \textbf{190.8} & 127.8 & 82.0 \\
 & SCG-MA & \textbf{236.6} & 28.8 & 71.5 & 172.2 & 77.5 & 155.2 & 128.3 & 82.8 \\
 & SCG-MO & \textbf{236.6} & \textbf{29.5} & \textbf{71.7} & \textbf{174.1} & \textbf{79.7} & 175.7 & 128.7 & \textbf{88.4} \\
 & SCG-B & 200.6 & 21.6 & 37.6 & 116.3 & 61.0 & 129.3 & \textbf{156.4} & 46.5 \\
 & SCG-R & 214.6 & 14.2 & 54.7 & 120.9 & 29.7 & 101.1 & 72.3 & 36.1 \\
 & KOCG-top-$1$ & 13.0 & 1.0 & 7.6 & 11.6 & 2.0 & 5.9 & 8.2 & 3.0 \\
 & KOCG-top-$r$ & 13.0 & 3.8 & 2.3 & 15.4 & 2.6 & 3.4 & 14.0 & 1.3 \\
 & BNC-$(k+1)$ & -0.7 & -10.8 & -1.1 & -1.0 & --- & --- & --- & --- \\
 & BNC-$k$ & 184.6 & 5.3 & 15.8 & 41.5 & --- & --- & --- & --- \\
 & SPONGE-$(k+1)$ & 88.0 & 1.0 & 1.0 & 1.0 & --- & --- & --- & --- \\
 & SPONGE-$k$ & 191.4 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
\midrule
$k = 4$ & LSPCD (ours) & \textbf{218.5} & 23.3 & 52.6 & \textbf{139.2} & \textbf{61.1} & 113.6 & 111.5 & \textbf{71.6} \\
 & SCG-MA & 205.1 & 25.1 & 52.9 & 94.5 & 35.5 & 104.9 & 127.4 & 56.5 \\
 & SCG-MO & 213.2 & \textbf{25.3} & \textbf{53.1} & 82.1 & 38.5 & \textbf{117.9} & \textbf{129.0} & 39.7 \\
 & SCG-B & 211.6 & 12.4 & 24.8 & 116.2 & 48.3 & 49.8 & 94.4 & 45.7 \\
 & SCG-R & 214.6 & 8.0 & 19.5 & 118.7 & 10.7 & 41.1 & 65.1 & 33.7 \\
 & KOCG-top-$1$ & 9.1 & 8.4 & 4.5 & 15.0 & 2.6 & 4.5 & 8.9 & 3.1 \\
 & KOCG-top-$r$ & 7.4 & 5.0 & 3.3 & 3.7 & 3.0 & 3.8 & 11.0 & 4.4 \\
 & BNC-$(k+1)$ & -0.2 & -9.4 & -1.1 & -1.0 & --- & --- & --- & --- \\
 & BNC-$k$ & 185.3 & 5.2 & 15.8 & 41.5 & --- & --- & --- & --- \\
 & SPONGE-$(k+1)$ & 53.8 & 1.1 & 1.0 & 1.0 & --- & --- & --- & --- \\
 & SPONGE-$k$ & 71.2 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
\midrule
$k = 6$ & LSPCD (ours) & \textbf{217.3} & \textbf{20.0} & 46.2 & \textbf{137.6} & \textbf{57.1} & 96.1 & 103.4 & \textbf{58.7} \\
 & SCG-MA & 207.3 & 14.6 & 45.5 & 84.9 & 37.8 & 102.6 & 88.8 & 57.5 \\
 & SCG-MO & 205.8 & 15.2 & \textbf{47.0} & 55.6 & 34.6 & \textbf{111.6} & \textbf{129.2} & 41.8 \\
 & SCG-B & 211.6 & 9.3 & 23.3 & 116.2 & 47.7 & 46.1 & 94.5 & 46.0 \\
 & SCG-R & 201.2 & 6.9 & 10.4 & 50.3 & 7.9 & 18.3 & 43.3 & 3.3 \\
 & KOCG-top-$1$ & 7.9 & 4.1 & 4.5 & 8.6 & 3.6 & 4.9 & 6.0 & 10.1 \\
 & KOCG-top-$r$ & 9.1 & 3.6 & 3.1 & 4.0 & 3.3 & 1.5 & 6.8 & 3.6 \\
 & BNC-$(k+1)$ & -0.2 & -4.2 & -1.1 & -0.8 & --- & --- & --- & --- \\
 & BNC-$k$ & 185.2 & 5.2 & 15.8 & 41.5 & --- & --- & --- & --- \\
 & SPONGE-$(k+1)$ & 47.8 & 1.3 & 1.0 & 1.0 & --- & --- & --- & --- \\
 & SPONGE-$k$ & 57.9 & 5.1 & 15.8 & 41.5 & --- & --- & --- & --- \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}

\begin{table*}[h]
    \caption{Detailed results on the \textbf{WoW-EP8} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{wow8-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$  & LSPCD (avg) & 586 & 0.087 & 2 & 0.261 & 0.098 & 0.757 & 0.509 & 0.511 & 0.769 & 223.406 & 19.409 & 0.249 \\ 
 & LSPCD (std) & 0 & 0.0 & 0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.034 \\ 
 & SCG-MA & 527 & 0.002 & 1 & 0.52 & 0.0 & 0.762 & 0.0 & 0.59 & 0.725 & 236.55 & 0.448 & 1.025 \\ 
 & SCG-MO & 517 & 0.002 & 1 & 0.527 & 0.0 & 0.769 & 0.0 & 0.596 & 0.708 & 236.592 & 0.457 & 1.026 \\ 
 & SCG-B & 583 & 0.025 & 2 & 0.274 & 0.094 & 0.697 & -0.369 & 0.514 & 0.767 & 200.604 & 4.918 & 4.657 \\ 
 & SCG-R & 513 & 0.016 & 2 & 0.272 & 0.104 & 0.761 & 0.451 & 0.552 & 0.654 & 214.616 & 3.386 & 0.517 \\ 
 & KOCG-top-$1$ & 16 & 0.8 & 2 & 0.986 & 0.889 & 0.965 & 0.778 & 1.0 & 0.019 & 13.0 & 10.4 & --- \\ 
 & KOCG-top-$r$ & 527 & 0.869 & 2 & 0.467 & 0.11 & 0.658 & -0.598 & 0.559 & 0.691 & 12.964 & 11.269 & --- \\ 
 & BNC-$(k+1)$ & 3 & 0.667 & 2 & 0.5 & 0.0 & -1.0 & 0.0 & 0.333 & 0.007 & -0.667 & -0.444 & 0.99 \\ 
 & BNC-$k$ & 790 & 0.004 & 2 & 0.152 & 0.049 & 0.628 & 1.0 & 0.372 & 1.0 & 184.63 & 0.702 & 0.537 \\ 
 & SPONGE-$(k+1)$ & 375 & 0.224 & 2 & 0.204 & 0.095 & 0.738 & 0.318 & 0.343 & 0.346 & 87.957 & 19.705 & 0.581 \\ 
 & SPONGE-$k$ & 790 & 0.117 & 2 & 0.191 & 0.093 & 0.696 & 0.15 & 0.372 & 1.0 & 191.38 & 22.404 & 0.572 \\ 
\midrule
 $k = 4$  & LSPCD (avg) & 590 & 0.031 & 4 & 0.156 & 0.082 & 0.76 & 0.426 & 0.506 & 0.772 & 218.458 & 6.862 & 0.317 \\ 
 & LSPCD (std) & 1 & 0.0 & 0 & 0.003 & 0.002 & 0.001 & 0.016 & 0.0 & 0.002 & 0.142 & 0.008 & 0.044 \\ 
 & SCG-MA & 599 & 0.028 & 4 & 0.58 & 0.13 & 0.762 & -0.32 & 0.527 & 0.822 & 205.137 & 5.828 & 1.198 \\ 
 & SCG-MO & 568 & 0.023 & 4 & 0.827 & 0.141 & 0.77 & -0.349 & 0.55 & 0.776 & 213.211 & 4.939 & 1.176 \\ 
 & SCG-B & 615 & 0.002 & 1 & 0.421 & 0.0 & 0.693 & 0.0 & 0.498 & 0.822 & 211.561 & 0.343 & 6.233 \\ 
 & SCG-R & 503 & 0.006 & 4 & 0.211 & 0.056 & 0.762 & 0.747 & 0.564 & 0.644 & 214.623 & 1.301 & 1.131 \\ 
 & KOCG-top-$1$ & 31 & 0.636 & 4 & 0.962 & 0.668 & 0.944 & 0.339 & 0.978 & 0.039 & 9.054 & 5.761 & --- \\ 
 & KOCG-top-$r$ & 599 & 0.735 & 4 & 0.436 & 0.099 & 0.692 & -0.618 & 0.516 & 0.806 & 7.393 & 5.434 & --- \\ 
 & BNC-$(k+1)$ & 8 & 0.4 & 4 & 0.5 & 0.0 & -1.0 & 0.0 & 0.036 & 0.003 & -0.25 & -0.1 & 0.62 \\ 
 & BNC-$k$ & 790 & 0.003 & 4 & 0.327 & 0.03 & 0.632 & 1.0 & 0.372 & 1.0 & 185.305 & 0.473 & 0.594 \\ 
 & SPONGE-$(k+1)$ & 485 & 0.426 & 4 & 0.448 & 0.06 & 0.886 & -0.53 & 0.33 & 0.433 & 53.823 & 22.911 & 0.593 \\ 
 & SPONGE-$k$ & 790 & 0.261 & 4 & 0.383 & 0.08 & 0.803 & -0.499 & 0.372 & 1.0 & 71.162 & 18.585 & 0.61 \\ 
\midrule
 $k = 6$ & LSPCD (avg) & 591 & 0.02 & 6 & 0.142 & 0.071 & 0.761 & 0.414 & 0.505 & 0.773 & 217.344 & 4.419 & 0.269 \\ 
 & LSPCD (std) & 0 & 0.0 & 0 & 0.006 & 0.002 & 0.0 & 0.009 & 0.0 & 0.001 & 0.142 & 0.003 & 0.015 \\ 
 & SCG-MA & 598 & 0.015 & 6 & 0.785 & 0.113 & 0.763 & -0.339 & 0.527 & 0.819 & 207.299 & 3.141 & 1.226 \\ 
 & SCG-MO & 591 & 0.017 & 6 & 0.756 & 0.171 & 0.77 & -0.321 & 0.534 & 0.811 & 205.796 & 3.576 & 1.378 \\ 
 & SCG-B & 615 & 0.002 & 1 & 0.421 & 0.0 & 0.693 & 0.0 & 0.498 & 0.822 & 211.561 & 0.343 & 7.512 \\ 
 & SCG-R & 744 & 0.001 & 5 & 0.323 & 0.178 & 0.669 & 0.313 & 0.41 & 0.978 & 201.172 & 0.275 & 1.295 \\ 
 & KOCG-top-$1$ & 42 & 0.778 & 6 & 0.992 & 0.605 & 0.984 & 0.303 & 0.934 & 0.053 & 7.905 & 6.148 & --- \\ 
 & KOCG-top-$r$ & 598 & 0.554 & 6 & 0.472 & 0.095 & 0.73 & -0.637 & 0.522 & 0.812 & 9.109 & 5.045 & --- \\ 
 & BNC-$(k+1)$ & 10 & 0.5 & 6 & 0.5 & 0.0 & -1.0 & 0.0 & 0.022 & 0.002 & -0.2 & -0.1 & 0.977 \\ 
 & BNC-$k$ & 790 & 0.003 & 6 & 0.552 & 0.016 & 0.632 & 1.0 & 0.372 & 1.0 & 185.198 & 0.473 & 0.61 \\ 
 & SPONGE-$(k+1)$ & 572 & 0.372 & 6 & 0.537 & 0.065 & 0.893 & -0.511 & 0.326 & 0.536 & 47.762 & 17.749 & 0.607 \\ 
 & SPONGE-$k$ & 790 & 0.276 & 6 & 0.53 & 0.075 & 0.834 & -0.529 & 0.372 & 1.0 & 57.868 & 15.99 & 0.599 \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h]
    \caption{Detailed results on the \textbf{Bitcoin} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{bitcoin-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$  & LSPCD (avg) & 155 & 0.377 & 2 & 0.2 & 0.143 & 0.94 & 0.969 & 0.199 & 0.211 & 29.022 & 10.947 & 2.203 \\ 
 & LSPCD (std) & 0 & 0.0 & 0 & 0.0 & 0.0 & 0.003 & 0.001 & 0.001 & 0.001 & 0.013 & 0.005 & 0.161 \\ 
 & SCG-MA & 179 & 0.084 & 2 & 0.298 & 0.068 & 0.906 & 0.873 & 0.179 & 0.216 & 28.838 & 2.418 & 0.122 \\ 
 & SCG-MO & 138 & 0.022 & 2 & 0.114 & 0.147 & 0.91 & 0.778 & 0.237 & 0.184 & 29.522 & 0.646 & 0.123 \\ 
 & SCG-B & 40 & 0.909 & 2 & 0.248 & 0.87 & 0.956 & 1.0 & 0.56 & 0.201 & 21.65 & 19.682 & 2.243 \\ 
 & SCG-R & 842 & 0.124 & 2 & 0.022 & 0.009 & 0.908 & 0.812 & 0.019 & 0.394 & 14.24 & 1.763 & 1.024 \\ 
 & KOCG-top-$1$ & 2 & 1.0 & 2 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.004 & 1.0 & 1.0 & --- \\ 
 & KOCG-top-$r$ & 179 & 0.885 & 2 & 0.063 & 0.055 & 0.266 & 0.182 & 0.094 & 0.165 & 3.754 & 3.324 & --- \\ 
 & BNC-$(k+1)$ & 50 & 0.083 & 2 & 0.058 & 0.0 & -0.516 & 0.0 & 0.425 & 0.581 & -10.76 & -0.897 & 0.341 \\ 
 & BNC-$k$ & 5881 & 0.008 & 2 & 0.059 & 0.001 & 0.721 & 0.694 & 0.001 & 1.0 & 5.268 & 0.043 & 0.202 \\ 
 & SPONGE-$(k+1)$ & 6 & 0.6 & 2 & 0.667 & 0.0 & 1.0 & 0.0 & 0.2 & 1.0 & 1.0 & 0.6 & 1.116 \\ 
 & SPONGE-$k$ & 5881 & 0.001 & 2 & 0.501 & 0.0 & 0.697 & 0.0 & 0.001 & 1.0 & 5.092 & 0.003 & 2.138 \\ 
\midrule
 $k = 4$  & LSPCD (avg) & 217 & 0.154 & 4 & 0.182 & 0.12 & 0.929 & 0.815 & 0.143 & 0.256 & 23.333 & 3.586 & 2.117 \\ 
 & LSPCD (std) & 13 & 0.028 & 0 & 0.027 & 0.029 & 0.004 & 0.123 & 0.011 & 0.009 & 0.765 & 0.534 & 0.408 \\ 
 & SCG-MA & 176 & 0.014 & 4 & 0.488 & 0.07 & 0.914 & -0.44 & 0.172 & 0.2 & 25.121 & 0.349 & 0.302 \\ 
 & SCG-MO & 180 & 0.014 & 4 & 0.487 & 0.07 & 0.91 & -0.431 & 0.169 & 0.205 & 25.252 & 0.341 & 0.348 \\ 
 & SCG-B & 216 & 0.011 & 4 & 0.473 & 0.052 & 0.865 & 0.296 & 0.076 & 0.176 & 12.401 & 0.142 & 6.377 \\ 
 & SCG-R & 450 & 0.11 & 4 & 0.036 & 0.008 & 0.92 & -0.627 & 0.033 & 0.238 & 8.033 & 0.886 & 1.237 \\ 
 & KOCG-top-$1$ & 26 & 0.5 & 4 & 0.859 & 0.621 & 1.0 & 0.653 & 0.738 & 0.112 & 8.41 & 4.205 & --- \\ 
 & KOCG-top-$r$ & 176 & 0.54 & 4 & 0.136 & 0.041 & 0.856 & -0.246 & 0.113 & 0.157 & 5.034 & 2.717 & --- \\ 
 & BNC-$(k+1)$ & 58 & 0.083 & 4 & 0.112 & 0.0 & -0.516 & 0.0 & 0.32 & 0.576 & -9.414 & -0.784 & 0.185 \\ 
 & BNC-$k$ & 5881 & 0.001 & 4 & 0.029 & 0.0 & 0.721 & 0.67 & 0.001 & 1.0 & 5.208 & 0.004 & 0.178 \\ 
 & SPONGE-$(k+1)$ & 71 & 0.045 & 4 & 0.754 & 0.0 & 1.0 & 0.0 & 0.016 & 0.443 & 1.099 & 0.05 & 3.364 \\ 
 & SPONGE-$k$ & 5881 & 0.001 & 4 & 0.75 & 0.0 & 0.697 & 0.0 & 0.001 & 1.0 & 5.092 & 0.003 & 2.797 \\ 
\midrule
 $k = 6$  & LSPCD (avg) & 194 & 0.114 & 6 & 0.251 & 0.143 & 0.948 & 0.646 & 0.155 & 0.231 & 20.031 & 2.201 & 2.73 \\ 
 & LSPCD (std) & 23 & 0.044 & 0 & 0.046 & 0.053 & 0.007 & 0.304 & 0.019 & 0.015 & 1.827 & 0.582 & 0.468 \\ 
 & SCG-MA & 430 & 0.009 & 6 & 0.536 & 0.021 & 0.931 & -0.355 & 0.055 & 0.301 & 14.568 & 0.125 & 0.448 \\ 
 & SCG-MO & 412 & 0.009 & 6 & 0.571 & 0.028 & 0.929 & -0.337 & 0.058 & 0.3 & 15.165 & 0.14 & 0.477 \\ 
 & SCG-B & 326 & 0.017 & 6 & 0.313 & 0.009 & 0.866 & -0.421 & 0.053 & 0.222 & 9.321 & 0.16 & 10.509 \\ 
 & SCG-R & 860 & 0.038 & 6 & 0.038 & 0.006 & 0.941 & -0.529 & 0.017 & 0.367 & 6.861 & 0.258 & 2.125 \\ 
 & KOCG-top-$1$ & 28 & 0.429 & 6 & 0.867 & 0.338 & 1.0 & 0.197 & 0.537 & 0.055 & 4.071 & 1.745 & --- \\ 
 & KOCG-top-$r$ & 430 & 0.333 & 6 & 0.077 & 0.013 & 0.88 & -0.405 & 0.043 & 0.26 & 3.601 & 1.2 & --- \\ 
 & BNC-$(k+1)$ & 224 & 0.018 & 6 & 0.075 & 0.001 & -0.622 & 0.958 & 0.033 & 0.394 & -4.239 & -0.077 & 0.286 \\ 
 & BNC-$k$ & 5881 & 0.001 & 6 & 0.075 & 0.0 & 0.722 & 0.657 & 0.001 & 1.0 & 5.197 & 0.003 & 0.194 \\ 
 & SPONGE-$(k+1)$ & 222 & 0.016 & 6 & 0.622 & 0.0 & 1.0 & 0.0 & 0.006 & 0.401 & 1.252 & 0.02 & 1.959 \\ 
 & SPONGE-$k$ & 5881 & 0.001 & 6 & 0.563 & 0.0 & 0.696 & -1.0 & 0.001 & 1.0 & 5.085 & 0.003 & 2.473 \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h]
    \caption{Detailed results on the \textbf{WikiVot} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{wikivot-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$  & LSPCD (avg) & 1278 & 0.224 & 2 & 0.038 & 0.015 & 0.831 & 0.673 & 0.06 & 0.548 & 62.322 & 13.948 & 2.457 \\  
 & LSPCD (std) & 4 & 0.001 & 0 & 0.0 & 0.0 & 0.0 & 0.003 & 0.0 & 0.001 & 0.003 & 0.056 & 0.276 \\ 
 & SCG-MA & 813 & 0.005 & 2 & 0.048 & 0.079 & 0.846 & 0.671 & 0.104 & 0.436 & 71.476 & 0.353 & 1.537 \\ 
 & SCG-MO & 748 & 0.005 & 2 & 0.052 & 0.082 & 0.854 & 0.671 & 0.113 & 0.411 & 71.733 & 0.385 & 1.432 \\ 
 & SCG-B & 414 & 0.02 & 2 & 0.054 & 0.033 & 0.756 & 0.776 & 0.12 & 0.221 & 37.589 & 0.737 & 7.501 \\ 
 & SCG-R & 1100 & 0.085 & 2 & 0.032 & 0.013 & 0.83 & 0.618 & 0.061 & 0.444 & 54.693 & 4.63 & 0.781 \\ 
 & KOCG-top-$1$ & 10 & 0.5 & 2 & 0.905 & 0.857 & 1.0 & 1.0 & 0.844 & 0.012 & 7.6 & 3.8 & --- \\ 
 & KOCG-top-$r$ & 813 & 0.964 & 2 & 0.047 & 0.022 & 0.427 & -0.337 & 0.066 & 0.297 & 2.312 & 2.229 & --- \\ 
 & BNC-$(k+1)$ & 9 & 0.571 & 2 & 0.0 & 0.0 & -1.0 & 0.0 & 0.139 & 1.0 & -1.111 & -0.635 & 0.721 \\ 
 & BNC-$k$ & 7115 & 0.002 & 2 & 0.002 & 0.0 & 0.558 & 0.0 & 0.004 & 1.0 & 15.794 & 0.024 & 0.49 \\ 
 & SPONGE-$(k+1)$ & 10 & 0.333 & 2 & 0.571 & 0.0 & 1.0 & 0.0 & 0.111 & 1.0 & 1.0 & 0.333 & 1.602 \\ 
 & SPONGE-$k$ & 7115 & 0.002 & 2 & 0.057 & 0.0 & 0.558 & 0.0 & 0.004 & 1.0 & 15.794 & 0.024 & 0.977 \\ 
\midrule
 $k = 4$  & LSPCD (avg) & 1089 & 0.115 & 4 & 0.073 & 0.013 & 0.856 & -0.045 & 0.072 & 0.489 & 52.605 & 5.869 & 4.381 \\ 
 & LSPCD (std) & 149 & 0.029 & 0 & 0.026 & 0.002 & 0.004 & 0.427 & 0.011 & 0.04 & 6.003 & 0.86 & 1.69 \\ 
 & SCG-MA & 1142 & 0.015 & 4 & 0.081 & 0.018 & 0.849 & -0.618 & 0.069 & 0.506 & 52.945 & 0.78 & 2.042 \\ 
 & SCG-MO & 1059 & 0.007 & 4 & 0.089 & 0.022 & 0.858 & -0.692 & 0.073 & 0.474 & 53.07 & 0.356 & 1.986 \\ 
 & SCG-B & 790 & 0.01 & 4 & 0.091 & 0.014 & 0.774 & -0.718 & 0.077 & 0.342 & 24.782 & 0.243 & 18.286 \\ 
 & SCG-R & 1524 & 0.044 & 4 & 0.031 & 0.008 & 0.813 & -0.68 & 0.043 & 0.549 & 19.524 & 0.861 & 3.074 \\ 
 & KOCG-top-$1$ & 33 & 0.267 & 4 & 0.845 & 0.086 & 0.933 & -0.609 & 0.576 & 0.03 & 4.525 & 1.207 & --- \\ 
 & KOCG-top-$r$ & 1142 & 0.803 & 4 & 0.055 & 0.011 & 0.719 & -0.618 & 0.059 & 0.44 & 3.288 & 2.639 & --- \\ 
 & BNC-$(k+1)$ & 15 & 0.333 & 4 & 0.0 & 0.0 & -1.0 & 0.0 & 0.076 & 1.0 & -1.067 & -0.356 & 0.527 \\ 
 & BNC-$k$ & 7115 & 0.0 & 4 & 0.001 & 0.0 & 0.558 & 0.0 & 0.004 & 1.0 & 15.794 & 0.007 & 0.533 \\ 
 & SPONGE-$(k+1)$ & 12 & 0.429 & 4 & 0.8 & 0.0 & 1.0 & 0.0 & 0.091 & 1.0 & 1.0 & 0.429 & 2.327 \\ 
 & SPONGE-$k$ & 7115 & 0.001 & 4 & 0.156 & 0.0 & 0.558 & 0.0 & 0.004 & 1.0 & 15.794 & 0.011 & 1.522 \\ 
\midrule
 $k = 6$  & LSPCD (avg) & 534 & 0.078 & 6 & 0.143 & 0.029 & 0.896 & -0.314 & 0.133 & 0.287 & 46.179 & 3.586 & 5.292 \\ 
 & LSPCD (std) & 46 & 0.008 & 0 & 0.015 & 0.002 & 0.004 & 0.07 & 0.012 & 0.008 & 2.177 & 0.201 & 0.931 \\ 
 & SCG-MA & 1355 & 0.004 & 6 & 0.064 & 0.023 & 0.849 & -0.647 & 0.056 & 0.564 & 45.494 & 0.168 & 2.24 \\ 
 & SCG-MO & 1226 & 0.004 & 6 & 0.073 & 0.024 & 0.859 & -0.683 & 0.063 & 0.526 & 47.013 & 0.189 & 2.178 \\ 
 & SCG-B & 941 & 0.01 & 6 & 0.121 & 0.018 & 0.78 & -0.735 & 0.065 & 0.369 & 23.332 & 0.229 & 29.072 \\ 
 & SCG-R & 1501 & 0.043 & 6 & 0.039 & 0.008 & 0.817 & -0.734 & 0.044 & 0.542 & 10.433 & 0.444 & 3.475 \\ 
 & KOCG-top-$1$ & 40 & 0.556 & 6 & 0.894 & 0.227 & 0.981 & -0.188 & 0.564 & 0.033 & 4.52 & 2.511 & --- \\ 
 & KOCG-top-$r$ & 1355 & 0.561 & 6 & 0.051 & 0.009 & 0.73 & -0.62 & 0.05 & 0.506 & 3.132 & 1.757 & --- \\ 
 & BNC-$(k+1)$ & 13 & 0.75 & 6 & 0.0 & 0.0 & -1.0 & 0.0 & 0.09 & 1.0 & -1.077 & -0.808 & 0.966 \\ 
 & BNC-$k$ & 7115 & 0.0 & 6 & 0.001 & 0.0 & 0.558 & 0.0 & 0.004 & 1.0 & 15.794 & 0.007 & 0.546 \\ 
 & SPONGE-$(k+1)$ & 20 & 0.429 & 6 & 0.644 & 0.0 & 1.0 & 0.0 & 0.053 & 1.0 & 1.0 & 0.429 & 1.791 \\ 
 & SPONGE-$k$ & 7115 & 0.0 & 6 & 0.434 & 0.0 & 0.558 & 0.0 & 0.004 & 1.0 & 15.794 & 0.007 & 1.66 \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h]
    \caption{Detailed results on the \textbf{Referendum} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{referendum-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$  & LSPCD (avg) & 915 & 0.423 & 2 & 0.279 & 0.014 & 1.0 & 0.114 & 0.17 & 0.353 & 146.109 & 61.846 & 3.376 \\  
 & LSPCD (std) & 1 & 0.002 & 0 & 0.001 & 0.0 & 0.0 & 0.017 & 0.001 & 0.001 & 0.092 & 0.233 & 0.19 \\ 
 & SCG-MA & 824 & 0.007 & 2 & 0.455 & 0.247 & 1.0 & 0.558 & 0.211 & 0.409 & 172.206 & 1.26 & 1.863 \\ 
 & SCG-MO & 673 & 0.007 & 2 & 0.546 & 0.3 & 1.0 & 0.571 & 0.261 & 0.352 & 174.083 & 1.299 & 1.108 \\ 
 & SCG-B & 1158 & 0.015 & 2 & 0.176 & 0.068 & 1.0 & 0.58 & 0.101 & 0.396 & 116.252 & 1.729 & 23.313 \\ 
 & SCG-R & 1550 & 0.018 & 2 & 0.095 & 0.04 & 1.0 & 0.529 & 0.079 & 0.492 & 120.85 & 2.14 & 4.657 \\ 
 & KOCG-top-$1$ & 15 & 0.417 & 2 & 0.973 & 0.659 & 1.0 & 1.0 & 0.829 & 0.007 & 11.6 & 4.833 & --- \\ 
 & KOCG-top-$r$ & 824 & 0.761 & 2 & 0.057 & 0.018 & 0.705 & -0.317 & 0.065 & 0.169 & 15.425 & 11.741 & --- \\ 
 & BNC-$(k+1)$ & 4 & 1.0 & 2 & 0.0 & 0.0 & -1.0 & 0.0 & 0.333 & 0.286 & -1.0 & -1.0 & 1.929 \\ 
 & BNC-$k$ & 10884 & 0.0 & 2 & 0.002 & 0.0 & 0.898 & -1.0 & 0.004 & 1.0 & 41.495 & 0.011 & 1.114 \\ 
 & SPONGE-$(k+1)$ & 6 & 0.6 & 2 & 0.667 & 0.0 & 1.0 & 0.0 & 0.2 & 1.0 & 1.0 & 0.6 & 6.754 \\ 
 & SPONGE-$k$ & 10884 & 0.0 & 2 & 0.502 & 0.0 & 0.898 & 0.0 & 0.004 & 1.0 & 41.495 & 0.011 & 6.889 \\ 
\midrule
 $k = 4$ & LSPCD (avg) & 1065 & 0.028 & 4 & 0.196 & 0.043 & 1.0 & 0.056 & 0.145 & 0.394 & 139.163 & 3.915 & 3.724 \\  
 & LSPCD (std) & 1 & 0.0 & 0 & 0.0 & 0.0 & 0.0 & 0.001 & 0.0 & 0.0 & 0.037 & 0.006 & 0.392 \\ 
 & SCG-MA & 1713 & 0.004 & 4 & 0.124 & 0.048 & 1.0 & -0.693 & 0.081 & 0.512 & 94.544 & 0.348 & 6.809 \\ 
 & SCG-MO & 1658 & 0.004 & 4 & 0.142 & 0.054 & 1.0 & -0.767 & 0.084 & 0.502 & 82.139 & 0.355 & 3.863 \\ 
 & SCG-B & 1142 & 0.001 & 1 & 0.102 & 0.0 & 1.0 & 0.0 & 0.102 & 0.398 & 116.233 & 0.102 & 60.02 \\ 
 & SCG-R & 1514 & 0.005 & 4 & 0.174 & 0.019 & 1.0 & 0.432 & 0.08 & 0.479 & 118.706 & 0.559 & 2.545 \\ 
 & KOCG-top-$1$ & 53 & 0.172 & 4 & 0.85 & 0.297 & 1.0 & -0.363 & 0.615 & 0.024 & 14.956 & 2.579 & --- \\ 
 & KOCG-top-$r$ & 1713 & 0.426 & 4 & 0.065 & 0.003 & 0.885 & -0.862 & 0.052 & 0.363 & 3.711 & 1.581 & --- \\ 
 & BNC-$(k+1)$ & 8 & 1.0 & 4 & 0.0 & 0.0 & -1.0 & 0.0 & 0.143 & 0.25 & -1.0 & -1.0 & 1.125 \\ 
 & BNC-$k$ & 10884 & 0.0 & 4 & 0.001 & 0.0 & 0.898 & -0.429 & 0.004 & 1.0 & 41.495 & 0.011 & 1.129 \\ 
 & SPONGE-$(k+1)$ & 18 & 0.333 & 4 & 0.452 & 0.0 & 1.0 & 0.0 & 0.059 & 1.0 & 1.0 & 0.333 & 5.042 \\ 
 & SPONGE-$k$ & 10884 & 0.0 & 4 & 0.156 & 0.0 & 0.898 & 0.0 & 0.004 & 1.0 & 41.495 & 0.019 & 6.327 \\ 
\midrule
 $k = 6$  & LSPCD (avg) & 1021 & 0.016 & 6 & 0.176 & 0.028 & 1.0 & 0.04 & 0.15 & 0.379 & 137.627 & 2.211 & 5.461 \\ 
 & LSPCD (std) & 1 & 0.001 & 0 & 0.001 & 0.0 & 0.0 & 0.001 & 0.0 & 0.0 & 0.131 & 0.081 & 2.813 \\ 
 & SCG-MA & 1945 & 0.001 & 5 & 0.107 & 0.033 & 1.0 & -0.771 & 0.069 & 0.56 & 84.933 & 0.104 & 8.225 \\ 
 & SCG-MO & 2469 & 0.001 & 5 & 0.16 & 0.003 & 1.0 & -0.853 & 0.049 & 0.629 & 55.571 & 0.071 & 6.925 \\ 
 & SCG-B & 1142 & 0.001 & 1 & 0.102 & 0.0 & 1.0 & 0.0 & 0.102 & 0.398 & 116.233 & 0.102 & 98.669 \\ 
 & SCG-R & 1660 & 0.005 & 6 & 0.08 & 0.038 & 0.986 & -0.756 & 0.052 & 0.356 & 50.258 & 0.247 & 5.075 \\ 
 & KOCG-top-$1$ & 81 & 0.381 & 6 & 0.923 & 0.088 & 1.0 & -0.673 & 0.536 & 0.032 & 8.622 & 3.285 & --- \\ 
 & KOCG-top-$r$ & 1945 & 0.63 & 6 & 0.061 & 0.003 & 0.917 & -0.876 & 0.053 & 0.442 & 4.037 & 2.543 & --- \\ 
 & BNC-$(k+1)$ & 12 & 0.5 & 6 & 0.222 & 0.0 & -0.714 & 0.0 & 0.106 & 0.25 & -0.833 & -0.417 & 1.923 \\ 
 & BNC-$k$ & 10884 & 0.0 & 6 & 0.056 & 0.0 & 0.898 & -0.2 & 0.004 & 1.0 & 41.495 & 0.011 & 1.155 \\ 
 & SPONGE-$(k+1)$ & 18 & 0.6 & 6 & 0.667 & 0.0 & 1.0 & 0.0 & 0.059 & 1.0 & 1.0 & 0.6 & 11.664 \\ 
 & SPONGE-$k$ & 10884 & 0.0 & 6 & 0.501 & 0.0 & 0.898 & 0.0 & 0.004 & 1.0 & 41.495 & 0.011 & 9.346 \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h]
    \caption{Detailed results on the \textbf{Slashdot} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{slashdot-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$  & LSPCD (avg) & 235 & 0.128 & 2 & 0.207 & 0.055 & 0.969 & 0.836 & 0.337 & 0.167 & 75.903 & 9.686 & 29.957 \\ 
 & LSPCD (std) & 0 & 0.002 & 0 & 0.0 & 0.001 & 0.0 & 0.003 & 0.001 & 0.0 & 0.095 & 0.133 & 3.151 \\ 
 & SCG-MA & 307 & 0.01 & 2 & 0.63 & 0.123 & 0.968 & 0.923 & 0.262 & 0.152 & 77.485 & 0.76 & 3.316 \\ 
 & SCG-MO & 234 & 0.009 & 2 & 0.674 & 0.137 & 0.973 & 0.882 & 0.352 & 0.145 & 79.692 & 0.681 & 2.654 \\ 
 & SCG-B & 289 & 0.025 & 2 & 0.145 & 0.056 & 0.98 & -0.005 & 0.221 & 0.205 & 60.962 & 1.503 & 287.233 \\ 
 & SCG-R & 3033 & 0.036 & 2 & 0.007 & 0.007 & 0.872 & 0.635 & 0.011 & 0.216 & 29.706 & 1.065 & 25.778 \\ 
 & KOCG-top-$1$ & 3 & 0.667 & 2 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.005 & 2.0 & 1.333 & --- \\ 
 & KOCG-top-$r$ & 307 & 0.828 & 2 & 0.028 & 0.03 & 0.159 & 0.182 & 0.05 & 0.037 & 2.612 & 2.164 & --- \\ 
\midrule
 $k = 4$ & LSPCD (avg) & 380 & 0.11 & 4 & 0.212 & 0.087 & 0.966 & 0.492 & 0.192 & 0.189 & 61.089 & 6.738 & 37.751 \\  
 & LSPCD (std) & 2 & 0.005 & 0 & 0.01 & 0.003 & 0.0 & 0.004 & 0.002 & 0.001 & 0.251 & 0.261 & 3.013 \\ 
 & SCG-MA & 2552 & 0.054 & 4 & 0.159 & 0.012 & 0.862 & -0.431 & 0.026 & 0.269 & 35.53 & 1.907 & 16.032 \\ 
 & SCG-MO & 2111 & 0.004 & 4 & 0.181 & 0.051 & 0.876 & -0.657 & 0.03 & 0.24 & 38.534 & 0.153 & 21.868 \\ 
 & SCG-B & 410 & 0.011 & 4 & 0.287 & 0.101 & 0.973 & -0.491 & 0.128 & 0.199 & 48.306 & 0.51 & 814.654 \\ 
 & SCG-R & 3853 & 0.223 & 4 & 0.01 & 0.002 & 0.877 & -0.34 & 0.008 & 0.227 & 10.749 & 2.396 & 27.234 \\ 
 & KOCG-top-$1$ & 23 & 0.364 & 4 & 0.453 & 0.172 & 1.0 & 0.643 & 0.206 & 0.009 & 2.609 & 0.949 & --- \\ 
 & KOCG-top-$r$ & 2552 & 0.34 & 4 & 0.013 & 0.003 & 0.627 & -0.477 & 0.012 & 0.16 & 2.973 & 1.012 & --- \\ 
\midrule
 $k = 6$  & LSPCD (avg) & 272 & 0.066 & 6 & 0.306 & 0.083 & 0.982 & 0.423 & 0.251 & 0.156 & 57.075 & 3.759 & 64.95 \\  
 & LSPCD (std) & 30 & 0.009 & 0 & 0.041 & 0.013 & 0.001 & 0.134 & 0.032 & 0.014 & 2.428 & 0.393 & 24.494 \\ 
 & SCG-MA & 2343 & 0.001 & 5 & 0.35 & 0.026 & 0.868 & -0.701 & 0.028 & 0.256 & 37.849 & 0.02 & 32.081 \\ 
 & SCG-MO & 2504 & 0.003 & 6 & 0.212 & 0.063 & 0.876 & -0.421 & 0.026 & 0.265 & 34.649 & 0.098 & 26.278 \\ 
 & SCG-B & 420 & 0.004 & 3 & 0.254 & 0.005 & 0.971 & -0.481 & 0.124 & 0.191 & 47.676 & 0.168 & 1408.849 \\ 
 & SCG-R & 9661 & 0.045 & 6 & 0.02 & 0.002 & 0.814 & -0.43 & 0.004 & 0.433 & 7.906 & 0.359 & 73.943 \\ 
 & KOCG-top-$1$ & 48 & 0.467 & 6 & 0.65 & 0.079 & 0.978 & -0.166 & 0.216 & 0.016 & 3.583 & 1.672 & --- \\ 
 & KOCG-top-$r$ & 2343 & 0.408 & 6 & 0.021 & 0.003 & 0.722 & -0.54 & 0.014 & 0.164 & 3.28 & 1.34 & --- \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h]
    \caption{Detailed results on the \textbf{WikiCon} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{wikicon-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$  & LSPCD (avg) & 1876 & 0.536 & 2 & 0.055 & 0.128 & 0.871 & 0.997 & 0.108 & 0.242 & 190.8 & 102.249 & 72.368 \\  
 & LSPCD (std) & 0 & 0.0 & 0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.019 & 0.066 & 17.051 \\ 
 & SCG-MA & 8903 & 0.288 & 2 & 0.008 & 0.026 & 0.81 & 0.998 & 0.019 & 0.473 & 155.215 & 44.697 & 69.196 \\ 
 & SCG-MO & 2442 & 0.226 & 2 & 0.036 & 0.094 & 0.839 & 0.999 & 0.08 & 0.22 & 175.654 & 39.641 & 19.316 \\ 
 & SCG-B & 502 & 0.366 & 2 & 0.117 & 0.387 & 0.816 & 0.926 & 0.295 & 0.142 & 129.335 & 47.318 & 1314.819 \\ 
 & SCG-R & 12669 & 0.316 & 2 & 0.004 & 0.011 & 0.798 & 0.997 & 0.009 & 0.441 & 101.138 & 31.924 & 169.997 \\ 
 & KOCG-top-$1$ & 14 & 0.6 & 2 & 0.803 & 0.289 & 0.85 & 0.368 & 0.648 & 0.002 & 5.857 & 3.514 & --- \\ 
 & KOCG-top-$r$ & 8903 & 0.849 & 2 & 0.007 & 0.007 & 0.0 & 0.056 & 0.014 & 0.327 & 3.417 & 2.901 & --- \\ 
\midrule
 $k = 4$ & LSPCD (avg) & 2288 & 0.232 & 4 & 0.033 & 0.051 & 0.869 & 0.936 & 0.086 & 0.23 & 113.637 & 26.258 & 88.288 \\ 
 & LSPCD (std) & 40 & 0.02 & 0 & 0.008 & 0.005 & 0.002 & 0.058 & 0.001 & 0.009 & 2.613 & 1.623 & 20.523 \\ 
 & SCG-MA & 4852 & 0.007 & 4 & 0.042 & 0.104 & 0.82 & 0.577 & 0.027 & 0.241 & 104.937 & 0.707 & 139.274 \\ 
 & SCG-MO & 1943 & 0.056 & 4 & 0.063 & 0.117 & 0.848 & 0.533 & 0.086 & 0.163 & 117.935 & 6.581 & 69.637 \\ 
 & SCG-B & 1700 & 0.372 & 4 & 0.12 & 0.032 & 0.768 & 0.268 & 0.07 & 0.174 & 49.824 & 18.514 & 3792.395 \\ 
 & SCG-R & 7174 & 0.048 & 4 & 0.006 & 0.012 & 0.836 & 0.308 & 0.015 & 0.293 & 41.125 & 1.984 & 131.226 \\ 
 & KOCG-top-$1$ & 57 & 0.085 & 4 & 0.708 & 0.502 & 0.75 & 0.891 & 0.253 & 0.027 & 4.456 & 0.379 & --- \\ 
 & KOCG-top-$r$ & 4852 & 0.742 & 4 & 0.014 & 0.011 & 0.213 & -0.075 & 0.024 & 0.199 & 3.821 & 2.833 & --- \\ 
\midrule
 $k = 6$ & LSPCD (avg) & 2394 & 0.172 & 6 & 0.049 & 0.039 & 0.873 & 0.847 & 0.08 & 0.233 & 96.085 & 16.382 & 69.593 \\ 
 & LSPCD (std) & 207 & 0.031 & 0 & 0.025 & 0.007 & 0.004 & 0.112 & 0.006 & 0.019 & 4.787 & 2.244 & 9.767 \\ 
 & SCG-MA & 4827 & 0.006 & 6 & 0.009 & 0.044 & 0.821 & 0.622 & 0.028 & 0.243 & 102.611 & 0.578 & 145.295 \\ 
 & SCG-MO & 2016 & 0.037 & 6 & 0.06 & 0.06 & 0.848 & 0.685 & 0.079 & 0.159 & 111.578 & 4.151 & 76.205 \\ 
 & SCG-B & 1924 & 0.039 & 6 & 0.084 & 0.015 & 0.771 & 0.291 & 0.061 & 0.174 & 46.069 & 1.779 & 6125.694 \\ 
 & SCG-R & 12909 & 0.128 & 6 & 0.011 & 0.004 & 0.788 & 0.135 & 0.009 & 0.463 & 18.278 & 2.331 & 175.294 \\ 
 & KOCG-top-$1$ & 50 & 0.074 & 6 & 0.765 & 0.476 & 0.962 & 0.633 & 0.505 & 0.007 & 4.904 & 0.363 & --- \\ 
 & KOCG-top-$r$ & 4827 & 0.698 & 6 & 0.016 & 0.009 & 0.286 & -0.209 & 0.023 & 0.2 & 1.522 & 1.062 & --- \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h]
    \caption{Detailed results on the \textbf{Epinions} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{epinions-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$ & LSPCD (avg) & 2188 & 0.44 & 2 & 0.12 & 0.013 & 0.907 & 0.74 & 0.066 & 0.351 & 127.784 & 56.221 & 59.119 \\ 
 & LSPCD (std) & 4 & 0.004 & 0 & 0.001 & 0.0 & 0.002 & 0.002 & 0.0 & 0.0 & 0.181 & 0.42 & 3.692 \\ 
 & SCG-MA & 1234 & 0.02 & 2 & 0.088 & 0.114 & 0.906 & 0.739 & 0.116 & 0.246 & 128.316 & 2.541 & 34.752 \\ 
 & SCG-MO & 1017 & 0.019 & 2 & 0.099 & 0.138 & 0.91 & 0.713 & 0.14 & 0.22 & 128.722 & 2.446 & 25.471 \\ 
 & SCG-B & 253 & 0.024 & 2 & 0.419 & 0.205 & 0.999 & 1.0 & 0.621 & 0.501 & 156.379 & 3.768 & 822.236 \\ 
 & SCG-R & 4396 & 0.091 & 2 & 0.01 & 0.007 & 0.891 & 0.766 & 0.019 & 0.363 & 72.282 & 6.561 & 12.119 \\ 
 & KOCG-top-$1$ & 12 & 0.4 & 2 & 0.708 & 0.815 & 1.0 & 0.833 & 0.803 & 0.007 & 8.167 & 3.267 & --- \\ 
 & KOCG-top-$r$ & 1234 & 0.719 & 2 & 0.054 & 0.022 & 0.5 & -0.245 & 0.064 & 0.16 & 14.036 & 10.092 & --- \\ 
\midrule
 $k = 4$ & LSPCD (avg) & 2120 & 0.136 & 4 & 0.124 & 0.016 & 0.932 & 0.408 & 0.065 & 0.341 & 111.544 & 14.851 & 65.489 \\ 
 & LSPCD (std) & 129 & 0.042 & 0 & 0.021 & 0.003 & 0.002 & 0.312 & 0.004 & 0.006 & 7.5 & 3.548 & 2.958 \\ 
 & SCG-MA & 1576 & 0.001 & 3 & 0.416 & 0.001 & 0.928 & -0.714 & 0.09 & 0.285 & 127.432 & 0.107 & 42.784 \\ 
 & SCG-MO & 1373 & 0.001 & 3 & 0.438 & 0.001 & 0.934 & -0.635 & 0.103 & 0.264 & 128.951 & 0.129 & 34.407 \\ 
 & SCG-B & 868 & 0.002 & 3 & 0.405 & 0.0 & 0.926 & -0.411 & 0.119 & 0.226 & 94.43 & 0.187 & 2169.558 \\ 
 & SCG-R & 1872 & 0.005 & 4 & 0.152 & 0.033 & 0.928 & -0.801 & 0.044 & 0.23 & 65.124 & 0.294 & 49.068 \\ 
 & KOCG-top-$1$ & 28 & 0.455 & 4 & 0.865 & 0.62 & 0.953 & 0.582 & 0.81 & 0.011 & 8.905 & 4.048 & --- \\ 
 & KOCG-top-$r$ & 1576 & 0.533 & 4 & 0.071 & 0.01 & 0.768 & -0.63 & 0.06 & 0.202 & 11.001 & 5.869 & --- \\ 
\midrule
 $k = 6$ & LSPCD (avg) & 2660 & 0.066 & 6 & 0.088 & 0.014 & 0.929 & 0.324 & 0.05 & 0.373 & 103.375 & 6.835 & 107.579 \\ 
 & LSPCD (std) & 153 & 0.009 & 0 & 0.009 & 0.004 & 0.002 & 0.142 & 0.004 & 0.007 & 3.637 & 0.857 & 20.806 \\ 
 & SCG-MA & 2564 & 0.005 & 6 & 0.301 & 0.048 & 0.935 & -0.713 & 0.05 & 0.34 & 88.759 & 0.448 & 57.185 \\ 
 & SCG-MO & 1373 & 0.001 & 3 & 0.438 & 0.001 & 0.934 & -0.635 & 0.103 & 0.264 & 129.22 & 0.129 & 37.194 \\ 
 & SCG-B & 868 & 0.002 & 3 & 0.405 & 0.0 & 0.926 & -0.411 & 0.119 & 0.226 & 94.476 & 0.187 & 3696.279 \\ 
 & SCG-R & 1365 & 0.003 & 6 & 0.128 & 0.036 & 0.946 & -0.898 & 0.054 & 0.203 & 43.324 & 0.138 & 53.993 \\ 
 & KOCG-top-$1$ & 34 & 0.444 & 6 & 0.9 & 0.43 & 1.0 & 0.496 & 0.576 & 0.012 & 5.965 & 2.651 & --- \\ 
 & KOCG-top-$r$ & 2564 & 0.405 & 6 & 0.043 & 0.006 & 0.779 & -0.654 & 0.035 & 0.262 & 6.802 & 2.753 & --- \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h]
    \caption{Detailed results on the \textbf{WikiPol} dataset from various aspects. LSPCD (avg) and LSPCD (std) respectively indicate the mean and standard deviation across five runs of our method with different seeds.}
    \label{wikipol-table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    %\renewcommand{\arraystretch}{0.6}
    \begin{tabular}{clcccccccccccc}
        \toprule
        & & \texttt{SIZE} & \texttt{BAL} & \texttt{K} & \texttt{MAC} & \texttt{MAO} & \texttt{CC+} & \texttt{CC-} & \texttt{DENS} & \texttt{ISO} & \texttt{POL} & \texttt{BA-POL} & \texttt{RT}\\
\midrule
 $k = 2$ & LSPCD (avg) & 599 & 0.151 & 2 & 0.093 & 0.034 & 0.917 & 0.87 & 0.15 & 0.109 & 81.985 & 12.417 & 57.022 \\ 
 & LSPCD (std) & 2 & 0.001 & 0 & 0.001 & 0.0 & 0.003 & 0.011 & 0.0 & 0.0 & 0.037 & 0.042 & 7.851 \\ 
 & SCG-MA & 1251 & 0.007 & 2 & 0.035 & 0.054 & 0.924 & 0.928 & 0.072 & 0.172 & 82.822 & 0.599 & 11.484 \\ 
 & SCG-MO & 648 & 0.005 & 2 & 0.071 & 0.079 & 0.928 & 1.0 & 0.147 & 0.121 & 88.441 & 0.41 & 4.041 \\ 
 & SCG-B & 609 & 0.02 & 2 & 0.041 & 0.013 & 0.963 & -0.238 & 0.081 & 0.112 & 46.525 & 0.932 & 773.37 \\ 
 & SCG-R & 7400 & 0.082 & 2 & 0.003 & 0.001 & 0.91 & 0.63 & 0.005 & 0.305 & 36.119 & 2.968 & 76.435 \\ 
 & KOCG-top-$1$ & 6 & 0.6 & 2 & 0.75 & 0.625 & 1.0 & 1.0 & 0.6 & 0.003 & 3.0 & 1.8 & --- \\ 
 & KOCG-top-$r$ & 1251 & 0.859 & 2 & 0.024 & 0.012 & 0.322 & -0.284 & 0.035 & 0.097 & 1.258 & 1.081 & --- \\ 
\midrule
 $k = 4$ & LSPCD (avg) & 450 & 0.075 & 4 & 0.147 & 0.068 & 0.938 & 0.546 & 0.184 & 0.086 & 71.628 & 5.389 & 83.214 \\ 
 & LSPCD (std) & 23 & 0.002 & 0 & 0.063 & 0.032 & 0.002 & 0.249 & 0.014 & 0.003 & 2.015 & 0.189 & 26.749 \\ 
 & SCG-MA & 2140 & 0.013 & 4 & 0.093 & 0.014 & 0.917 & -0.613 & 0.038 & 0.217 & 56.471 & 0.728 & 49.769 \\ 
 & SCG-MO & 2783 & 0.0 & 3 & 0.283 & 0.001 & 0.895 & -0.775 & 0.026 & 0.242 & 39.698 & 0.019 & 44.39 \\ 
 & SCG-B & 727 & 0.002 & 2 & 0.203 & 0.001 & 0.967 & -0.899 & 0.07 & 0.125 & 45.661 & 0.076 & 2225.353 \\ 
 & SCG-R & 7740 & 0.029 & 4 & 0.002 & 0.001 & 0.916 & 0.599 & 0.005 & 0.302 & 33.723 & 0.966 & 91.037 \\ 
 & KOCG-top-$1$ & 26 & 0.286 & 4 & 0.558 & 0.119 & 0.949 & 0.182 & 0.255 & 0.005 & 3.051 & 0.872 & --- \\ 
 & KOCG-top-$r$ & 2140 & 0.27 & 4 & 0.02 & 0.002 & 0.808 & -0.609 & 0.01 & 0.092 & 4.409 & 1.192 & --- \\ 
\midrule
 $k = 6$ & LSPCD (avg) & 825 & 0.06 & 6 & 0.191 & 0.026 & 0.94 & -0.28 & 0.093 & 0.123 & 58.694 & 3.532 & 228.675 \\ 
 & LSPCD (std) & 72 & 0.001 & 0 & 0.023 & 0.014 & 0.005 & 0.183 & 0.011 & 0.008 & 1.548 & 0.079 & 192.894 \\ 
 & SCG-MA & 2176 & 0.001 & 4 & 0.259 & 0.001 & 0.919 & -0.73 & 0.037 & 0.22 & 57.546 & 0.046 & 54.104 \\ 
 & SCG-MO & 2783 & 0.0 & 3 & 0.283 & 0.001 & 0.895 & -0.775 & 0.026 & 0.242 & 41.846 & 0.02 & 48.571 \\ 
 & SCG-B & 727 & 0.002 & 2 & 0.203 & 0.001 & 0.967 & -0.899 & 0.07 & 0.125 & 45.986 & 0.077 & 3973.384 \\ 
 & SCG-R & 95033 & 0.002 & 6 & 0.006 & 0.0 & 0.884 & -0.686 & 0.003 & 0.901 & 3.329 & 0.007 & 331.066 \\ 
 & KOCG-top-$1$ & 83 & 0.364 & 6 & 0.756 & 0.029 & 0.967 & -0.658 & 0.268 & 0.015 & 10.135 & 3.685 & --- \\ 
 & KOCG-top-$r$ & 2176 & 0.192 & 6 & 0.032 & 0.002 & 0.867 & -0.578 & 0.006 & 0.077 & 3.585 & 0.687 & --- \\ 
\bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}

%\subsection{Synthetic Datasets with Ground-Truth Communities} \label{appendix:synthetic}

\section{Limitations of Polarity} \label{appendix:polarity}

To illustrate the limitation of polarity, we refer to Example 2 from \citep{DBLP:journals/ml/GulloMT24}, which considers a signed graph with 12 objects: 
$\{\texttt{A}, \texttt{B}, \texttt{C}, \texttt{D}, \texttt{E}, \texttt{F}, \texttt{G}, \texttt{H}, \texttt{I}, \texttt{J}, \texttt{K}, \texttt{L}\}$.
The sign of each similarity can be found in Figure 3 of \citep{DBLP:journals/ml/GulloMT24}. The study evaluates the following three clustering solutions:

\begin{itemize}
    \item $S^{(1)} = \{\{\texttt{A}, \texttt{B}, \texttt{C}, \texttt{D}\}, \{\texttt{E}, \texttt{F}, \texttt{G}, \texttt{H}\}\}$
    \item $S^{(2)} = \{\{\texttt{A}, \texttt{B}, \texttt{C}, \texttt{D}\}, \{\texttt{E}, \texttt{F}, \texttt{G}, \texttt{H}, \texttt{I}, \texttt{J}, \texttt{K}, \texttt{L}\}\}$
    \item $S^{(3)} = \{\emptyset, \{\texttt{E}, \texttt{F}, \texttt{G}, \texttt{H}, \texttt{I}, \texttt{J}, \texttt{K}, \texttt{L}\}\}$
\end{itemize}

The polarity values for these solutions are: 
$\text{Polarity}(S^{(1)}) = (20 + 10)/8 = 3.75$,  
$\text{Polarity}(S^{(2)}) = (38 + 6)/12 = 3.67$,  
and $\text{Polarity}(S^{(3)}) = (30 + 0)/8 = 3.75$. 

Although $S^{(1)}$ and $S^{(3)}$ achieve the same polarity score, $S^{(1)}$ is significantly more balanced, making it the more reasonable choice. In contrast, evaluating our objective (Eq.~\ref{eq:ours}) for the same solutions, we obtain 
$S^{(1)}: (20 + 10) - (4^2 + 4^2) = -2$,  
$S^{(2)}: (38 + 6) - (4^2 + 8^2) = -36$,  
and $S^{(3)}: (30 + 0) - 8^2 = -34$. 

Our objective function still identifies $S^{(2)}$ as the worst solution (consistent with polarity), but it strongly favors $S^{(1)}$ over $S^{(3)}$ due to its better balance. Here, we assume $\alpha = 1/(k-1)$ (which is 1 since $k = 2$) for consistency with polarity, and $\beta = 1$.


\end{document}
