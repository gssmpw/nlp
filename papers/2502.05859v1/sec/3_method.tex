\section{Method}
\label{sec:method}


% In this section, we describe the details of SphereFusion, as shown in Fig. \ref{fig:pipeline}, which extracts features from the image in the equirectangular projection by 2D convolution and the spherical mesh by mesh convolution, and fuses these features in the spherical domain to estimates the panorama depth map.
In this section, we describe the details of SphereFusion, as shown in Fig. \ref{fig:pipeline}.
We first describe how to represent the panorama image in Section \ref{sec:represent} and extract features in the spherical projection in Section \ref{sec:mesh}. 
Then, we show the pipeline of our SphereFusion in Section \ref{sec:pipeline}.
Finally, we present the loss function in Section \ref{sec:loss}.


\begin{figure*} [t]
	\begin{center}

        \subfloat[Panorama Image]{\includegraphics[width=0.22\linewidth]{fig/pano_image.png}}
		\hspace{0.1em}
		\subfloat[Equirectangular Projection]{\includegraphics[width=0.30\linewidth]{fig/equi_proj.png}}	
        \hspace{0.1em}
		\subfloat[Spherical Projection]{\includegraphics[width=0.2\linewidth]{fig/spherical_proj.png}}	
  
	\end{center}
    \vspace{-1.5em}
	\caption{
        The ideal representation of a panorama image is the sphere, but it is impractical. The equirectangular projection is the most popular method, but it suffers from distortion at the poles and discontinuity at the borders. The spherical mesh can approximate the sphere, and their difference becomes smaller with higher MR.
	}
    \vspace{-2.0em}
	\label{fig:projections}
\end{figure*}


\begin{figure} [t]
	\begin{center}
		\subfloat[Mesh Convolution]{\includegraphics[width=0.8\linewidth]{fig/mesh_conv.png}}
		\vspace{0.1em}
		\subfloat[Mesh Pooling]{\includegraphics[width=0.8\linewidth]{fig/mesh_pool_unpool.png}}	
  
	\end{center}
    \vspace{-2.0em}
	\caption{
		Mesh Operations includes Mesh Convolution and the Mesh Pooling/Unpooling \cite{yan2022spheredepth}, which relies on the relationship between triangles of the spherical mesh. 
	}
    \vspace{-1.5em}
	\label{fig:mesh_op}
\end{figure}


\subsection{Panorama Representation}
\label{sec:represent}

Finding a suitable way to represent the panorama image is the key to high-quality panorama depth estimation. 
The equirectangular projection \cite{zioulis2018omnidepth} suffers from distortion on the poles and discontinuity at the borders, but it can directly use 2D convolution to extract features from images. 
The cubemap and tangent projection have small distortion but need a special mechanism to fuse different patches \cite{li2022omnifusion,shen2022panoformer,peng2022high}. 
The spherical projection \cite{yan2022spheredepth} is an ideal way to represent the panorama image, but mesh convolution is hard to extract texture features compared with 2D image convolution.
In light of these, we simultaneously employ the equirectangular and spherical projections. 

The equirectangular projection uses a 2D image plane to represent the panorama image, as Fig. \ref{fig:projections}(b) shows, where the image plane is built on the latitude and longitude of the sphere surface. Given a pixel $p=(u,v)$ on the image plane, we can calculate its position on the sphere surface by Eq. \ref{eq:to_equi}, where $W$ is the image width and $H$ is the image height. 

\begin{align}
	\label{eq:to_equi}
	\begin{cases}
		longitude &= (2u/W - 1 ) \times \pi \\
		latitude &=( v/H - 0.5 ) \times \pi
	\end{cases}
\end{align}


The spherical projection is based on the icosahedron spherical mesh  \cite{eder2020tangent}, which can approximate the sphere by a higher mesh resolution (MR), where MR represents the times of loop subdivision \cite{hu2021subdivision} is applied on the icosahedron spherical mesh and determines the number of triangles in the spherical mesh by $20 \times 4^{MR}$. 
One triangle in the spherical mesh represents one pixel in equirectangular projection, as Fig. \ref{fig:projections}(c) shows, Tangent \cite{eder2020tangent} points out that a panorama image in the equirectangular projection with higher image resolution needs a spherical mesh with higher MR.
In our implementation, we use the triangle center $(x,y,z)$ to represent the whole triangle and can calculate its position on the sphere surface by Eq. \ref{eq:to_mesh}.


\begin{align}
	\label{eq:to_mesh}
	\begin{cases}
		longitude &= atan(y,x) \\
		latitude &= atan(z,\sqrt{x^2+y^2})
	\end{cases}
\end{align}


According to Eq. \ref{eq:to_equi} and Eq. \ref{eq:to_mesh}, we define the E2S ( equirectangular projection to spherical projection ) and S2E ( spherical projection to equirectangular projection ).



\subsection{Mesh Operations}
\label{sec:mesh}

As we utilize the spherical projection to represent the panorama image, we need mesh convolution and the mesh pooling/unpooling to extract features, which is inspired by SubdivNet \cite{hu2021subdivision} and SphereDepth \cite{yan2022spheredepth}.

Mesh convolution relies on the FAF ( face adjacent face ), which describes the topological relationship between triangles of the spherical mesh, as Fig. \ref{fig:mesh_op}(a) shows. Each triangle in the mesh has three neighbors and can extract features by linear interpolation by Eq. \ref{eq:mesh_conv}, where $w_i (i=0,1,2,3)$ are the weight parameters, $b_0$ is the bias parameter, $f_0$ is the feature of the center triangle, $f_1$, $f_2$, $f_3$ are the feature of adjacent triangles, $f_n$ is the extracted features.

\begin{align}
	\label{eq:mesh_conv}
	f_{n}= \sum_{i=0}^{3}w_if_{i}+b_0
\end{align}

The mesh pooling and unpooling are fundamental components of constructing an encoder-decoder structure. 
Similar to the image pooling, mesh pooling merges four triangles into one, as shown in Fig. \ref{fig:mesh_op}(b) and calculates the feature of the new $f_{n}$ triangle through features of $f_0$, $f_1$,$f_2$, and $f_3$ by the mesh max pooling.
The mesh unpooling is the opposite of the mesh pooling, which splits one triangle into four triangles by loop subdivision.

\begin{figure*} [t]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{fig/fuse.PNG}
	\end{center}
    \vspace{-1.5em}
	\caption{
		We implement BiFuse \cite{wang2020bifuse}, UniFuse \cite{jiang2021unifuse}, and our GateFuse to fuse features from spherical projection $F_{sp}$ and equirectangular projection $F_{eq}$. Unlike BiFuse and UniFuse select features from $F_{eq}$ and fuse them to $F_{sp}$, GateFuse selects features from $F_{sp}$ and $F_{eq}$.
	}
	\label{fig:fuse}
    \vspace{-1.5em}
\end{figure*}


\subsection{Our Framework}
\label{sec:pipeline}

\subsubsection{The Network Encoder}
Given a panorama image, we represent it by the spherical and equirectangular projection and employ the mesh encoder and image encoder to extract features, respectively.
The network structure of the mesh encoder follows the ResNet \cite{he2016deep}. Considering the extremely high computational complexity of mesh operations \cite{hu2021subdivision,yan2022spheredepth}, the mesh encoder uses the simplest ResNet18, and generates five scales of spherical features $F_{sp}$, of which the channels are 64, 64, 128, 256, and 512. SphereFusion randomly initializes the mesh encoder and trains it from scratch.
The image encoder directly uses the ResNet50 to extract image features $F_{eq}$, of which the channels are 64, 256, 512, 1024, and 2048. 
As 2D image convolution can not capture reliable features on distorted regions \cite{zioulis2018omnidepth}, we add a lightweight image encoder to extend the receptive field. Meanwhile, the image decoder aligns the number of channels of $F_{eq}$ to $F_{sp}$ to facilitate the subsequent feature fusion. SphereFusion initializes the image encoder by a pre-trained weight and randomly initializes the image decoder.





\subsubsection{The Fusion Module}

After extracting features from the spherical projection $F_{sp}$ and the equirectangular projection $F_{eq}$, SphereFusion uses the GateFuse module to fuse these features in the spherical domain. 
The core idea of the GateFuse module is to enhance the $F_{sp}$ by $F_{eq}$ through a reset gate and a forget gate inspired by GRU \cite{cho2014learning}.
For extracted features in each scale, GateFuse first transforms $F_{eq}$ to the spherical projection through the E2S module, then concatenates these features to estimate a reset gate value $r$ and a forget gate value $z$, where $r$ selects features from $F_{sp}$ and $z$ selects features from $F_{eq}$. Finally, GateFuse adds these features to get fused features $F_{fused}$.
Compared with BiFuse \cite{wang2020bifuse} and UniFuse \cite{jiang2021unifuse}, which use concatenate features to estimate a mask to select reliable features from $F_{eq}$, GateFuse simultaneously selects reliable features from $F_{sp}$ and $F_{eq}$, instead of simply trusting one type of features.
To better compare different fusion modules, we implement BiFuse, UniFuse, and our GateFuse and visualize these modules in Fig. \ref{fig:fuse}.



\subsubsection{The Network Decoder}
With fused features, we construct a mesh decoder to estimate the panorama depth. 
Following the network structure of UNet \cite{ronneberger2015u}, SphereFusion uses skip-connection to concatenate fused features $F_{fused}$ with features from the mesh encoder $F_{sp}$, and uses the mesh unpooling to reconstruct a high-resolution panorama depth map. 
Specifically, the mesh decoder has six layers, and the number of channels in each layer are 1024, 512, 64, 32, 32, and 32, respectively. 
Meanwhile, each layer contains one mesh unpooling layer to gradually increase the MR of the spherical mesh from 2 to 7 for 360D and 3 to 8 for Matterport3D and Stanford2D3D. The mesh decoder outputs multi-resolution panorama depth maps to speed up the training procedure.





\subsubsection{Inference Efficiency}
Unlike 2D image convolution, which can directly find adjacent pixels by coordinate, the mesh operation needs to compute the FAF of each triangle to identify nearby triangles in each layer, which will become more complex with higher MR. 
However, as the mesh pooling/unpooling changes the spherical mesh, mesh convolution layers between two mesh pooling/unpooling layers use a spherical mesh with the same MR. 
Meanwhile, the mesh decoder and the mesh encoder at the same scale use a spherical mesh with the same MR. 
Based on these findings, SphereFusion first identifies all active spherical meshes, then merges these spherical meshes with the same MR, and then calculates the corresponding FAF once.
During training and testing, SphereFusion stores these connectivity information in cache memory without recalculating FAF for each layer, which can significantly improve inference efficiency. 



\subsection{Loss Function}
\label{sec:loss}

Following recent works \cite{pintore2021slicenet,wang2020bifuse}, we use the BerHu loss \cite{laina2016deeper} during training as Eq. \ref{eq:loss} shows, where $y$ is the ground truth depth and $\hat{y}$ is the predicted depth, and the threshold $T$ is set to 0.2 in all our experiments.

\begin{align}
	\label{eq:loss}
	\mathcal{L}(y,\hat{y}) = 
	\begin{cases}
		|y-\hat{y}|, &|y-\hat{y}|<T \\
		\frac{(y-\hat{y})^2+T^2}{2T}, &|y-\hat{y}|\ge T
	\end{cases}
\end{align}

To accelerate the training, SphereFusion predicts depth maps with multiple scales and extendsEq. \ref{eq:loss} to multi-scale, as Eq. \ref{eq:multi_loss} shows, where $s$ is the scale, $s_i$ is the weight, $V_i$ is the valid pixel, $N_i$ is the number of the valid pixel.

\begin{align}
	\label{eq:multi_loss}
	Loss = \sum_{i<s}{s_i \frac{\sum_{p \in V_i}{\mathcal{L}(y,\hat{y})}}{N_i}  }
\end{align}
