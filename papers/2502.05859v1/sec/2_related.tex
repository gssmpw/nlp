\section{Related Work}
\label{sec:related}

% This section demonstrates related work in perspective depth estimation and panorama depth estimation.

\subsection{Perspective Depth Estimation}
Over the past few decades, the perspective 2D image from the classical pinhole camera model has attracted much interest in inferring depth estimation for 3D sensing applications. 
% The traditional method
The traditional methods, such as Make3D \cite{saxena2008make3d} and Pop-up \cite{hoiem2005automatic}, leverage the prior depth distribution and regress the pixel-wise depth via a Markov random field (MRF). 
% deep learning-based methods
Due to the success of deep learning, researchers have also utilized its capability of multi-level feature extraction to improve the depth quality. As the first trial, Eigen \cite{eigen2014depth} and Laina \cite{laina2016deeper} design end-to-end convolutional neural networks to regress the depth map from an RGB image. 
Furthermore, several recent studies \cite{bhat2021adabins,yuan2022new} take advantage of the visual transformer (ViT) \cite{vit2021}, which captures the relationship between each pixel patch to regress the depth map with a global context. 
% other technical skills from the typical computer vision tasks
To improve the model generalization and depth consistency, Midas \cite{ranftl2020towards} focuses on the training strategy of mixing different datasets and tuning the loss function. 
% introduce other constraints to improve the depth quality.
Other studies, such as \cite{eigen2015predicting,alhashim2018high,yin2021virtual,patil2022p3depth}, introduce some constraints like the surface normal or the semantic information to co-optimize the depth map.
% unsupervised learning
However, a key challenge in training models for depth estimation is the lack of high-quality depth labels, which motivates the studies of unsupervised learning \cite{zhou2017unsupervised,godard2019digging,bian2021unsupervised,zhao2020towards,watson2021temporal}. SfMLearner \cite{zhou2017unsupervised} leverages photometric consistency to estimate the relative pose and depth simultaneously. MonoDepth2 \cite{godard2019digging} filters out the outliers that violate photometric consistency to deal with occlusions. In addition, SfMLearner-SC \cite{bian2021unsupervised} explores the depth consistency between video frames to restrict the depth of each pixel.
Although depth estimation for 2D perspective images has achieved considerable performance, they cannot be directly applied to the panorama image. 
The main reason is that the panorama image needs a particular type of projection, which usually causes distortion or discontinuity, where network for perspective vision are hard to capture reliable features.

\begin{figure*} [t]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{fig/pipeline.PNG}
	\end{center}
    \vspace{-1.5em}
	\caption{
        Given a panorama image in the equirectangular projection and the spherical projection, SphereFusion simultaneously extracts features by a 2D image encoder and a mesh encoder, which follows the ResNet structure \cite{he2016deep}, then fuses these features by the Gate Fusion module in the spherical projection, and finally estimates the depth map through the mesh decoder.
	}
	\label{fig:pipeline}
    \vspace{-1.5em}
\end{figure*}

\subsection{Panorama Depth Estimation}

Unlike the perspective image, which only provides a limited FOV, the panorama image has a 360$^\circ$ FOV. 
Compared to the development of perspective depth estimation, panorama depth estimation is still in its infancy stage. 
% Most of the recent studies \cite{zioulis2018omnidepth,wang2020bifuse,sun2021hohonet,shen2022panoformer,junayed2022himode,shen2022neural} rely on special projection methods, such as the equirectangular, cubemap, and tangent projection, to transform the panorama image to the 2D perspective image and estimate the depth using the perspective neural network structures.
Most of the recent studies \cite{zioulis2018omnidepth,wang2020bifuse,sun2021hohonet,shen2022panoformer,junayed2022himode,shen2022neural} rely on special projection methods to transform the panorama image to the 2D perspective image and estimate the depth using the perspective neural network structures.
The equirectangular projection is the most common projection, allowing all the surrounding information to be observed from a single 2D image. 
OmniDepth \cite{zioulis2018omnidepth} proposes the first end-to-end network based on the equirectangular projection for panorama depth estimation along with a large synthetic dataset. 
SliceNet \cite{pintore2021slicenet} uses sliced feature maps to estimate the depth map through LSTM. 
The cubemap projection transforms the panorama image into six perspective images to estimate depth maps and then fuse them through a specially designed module \cite{cheng2018cube,wang2020bifuse}.
The tangent projection approximates the sphere through more perspective images.
OmniFusion \cite{li2022omnifusion}, PanoFormer \cite{shen2022panoformer}, and 360MonoDepth \cite{rey2022360monodepth} samples perspective images from the spherical surface \cite{eder2020tangent} and applies the existing convolutional or ViT models on each tangent patch. 
Despite the success of these methods, the main problem of those projections is introducing distortion and discontinuity in certain areas of the panorama image. 
Although several subsequent studies have attempted to improve the depth quality of these regions, such as designing special convolutional kernels \cite{yu2017flat2sphere, de2018eliminating,su2019kernel,eder2019mapped,tateno2018distortion,wang2020360sd,cheng2020omnidirectional}, applying the deformable convolution \cite{chen2021distortion,fernandez2020corners}, and multi-task learning \cite{eder2019pano,zeng2020joint,jin2020geometric}, we argue that the influence of distortion and discontinuity cannot be removed entirely.
In addition to changing the convolution kernel, BiFuse \cite{wang2020bifuse} and UniFuse \cite{jiang2021unifuse} combine the equirectangular projection and the cubemap projection.

% spherical projection
To eliminate the drawbacks of the above two projection methods, some recent studies attempt to process the panorama image in the spherical domain.
% SpherePHD \cite{lee2020spherephd} uses the icosahedral spherical mesh to represent the panorama image and extract semantic maps. SphereDepth \cite{yan2022spheredepth} modifies the mesh operation from the SubdivNet \cite{hu2021subdivision}, which is more efficient than MeshCNN \cite{hanocka2019meshcnn} and MeshNet \cite{feng2019meshnet} to estimate the panorama depth in the spherical domain.
SpherePHD \cite{lee2020spherephd} uses the icosahedral spherical mesh to represent the panorama image and extract semantic maps. SphereDepth \cite{yan2022spheredepth} modifies the mesh operation from the SubdivNet \cite{hu2021subdivision}, which is more efficient than MeshCNN \cite{hanocka2019meshcnn} and MeshNet \cite{feng2019meshnet}.
In addition, S2CNN \cite{cohen2018spherical} uses the spheric harmonics function to build networks but is unsuitable for dense estimation. US2CNN \cite{jiang2019spherical} builds grids and manually assigns weight to build the network. 

