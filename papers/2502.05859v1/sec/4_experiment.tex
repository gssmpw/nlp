\section{Experiments}
\label{sec:experiment}


\subsection{Datasets}

% We conduct experiments on three datasets, including 360D, Matterport3D and Stanford2D3D.

% \textbf{360D} is a synthetic panorama dataset provided by OmniDepth \cite{zioulis2018omnidepth} and contains 35977 panorama images with a resolution of $256 \times 512$. 
\textbf{360D} \cite{zioulis2018omnidepth} is a synthetic panorama dataset and contains 35977 panorama images with a resolution of $256 \times 512$. 

% \noindent \textbf{Matterport3D} \cite{chang2017matterport3d} is a large real-world dataset, which has 10800 panorama images with raw depth maps from sensors. We resize all panorama images and depth maps into $512 \times 1024$ during training and testing.
\noindent \textbf{Matterport3D} \cite{chang2017matterport3d} is a large real-world dataset, which has 10800 panorama images. We resize all panorama images and depth maps into $512 \times 1024$ during training and testing.

% \noindent \textbf{Stanford2D3D} \cite{armeni2017joint} is a real-world indoor dataset that contains 1413 panoramas with ground truth depth maps rendered from 3D models. We resize all panorama images and depth maps into $512 \times 1024$. 
\noindent \textbf{Stanford2D3D} \cite{armeni2017joint} is a real-world indoor dataset that contains 1413 panoramas. We resize all panorama images and depth maps into $512 \times 1024$. 



\subsection{Implementation}

We implement our method by Jittor \cite{hu2020jittor}.
On 360D, we train the network with 30 epochs, setting the batch size to 4 and the initial learning rate to 0.0002. 
On Matterport3D and Stanford2D3D, we train the network with 30 epochs, setting the batch size to 2 and the initial learning rate to 0.0001. 
On all datasets, we train on one Nvidia Tesla V100 and halve the learning rate after every ten epochs. 
Since Matterport3D and Stanford2D3D utilize the same sensors to capture panorama images, and the size of Stanford2D3D is relatively small, we combine their training data but test each dataset separately.




\subsection{Quantitative Evaluation}

During the quantitative evaluation,  we follow common evaluation metrics \cite{wang2020bifuse} and use MAE, MRE, RMSE, RMSE (log), and $\delta$ and compare with state-of-the-art panorama depth estimation methods \cite{wang2020bifuse,jiang2021unifuse,pintore2021slicenet,shen2022panoformer,li2022omnifusion,yan2022spheredepth,sun2021hohonet}.
To deal with outliers, we ignore pixels whose depth is outside of the range $0.1 \sim 10$ meters for 360D \cite{zioulis2018omnidepth} and $0.1 \sim 16$ for Stanford2D3D \cite{armeni2017joint} and Matterport3D \cite{chang2017matterport3d}. 
Table \ref{tab:sota_results} shows evaluation results on three datasets, including the quality of the depth map and the inference efficiency.



\begin{table*} [ht]
	\begin{center}
		\caption{Quantitative evaluation results of depth maps, where `S2D3D' is the short for Standard2D3D and `M3D' is the short for Matterport3D. `---' means no data is available from the original paper. 
		We mark out top three methods for better comparison. 
		}
		\label{tab:sota_results}

        \vspace{-0.50em}
  
		\resizebox{\linewidth}{!}{\begin{tabular}{l|lcccc|ccc|c}
			\hline\noalign{\smallskip}
			Dataset & Method & MRE$\downarrow$ & MAE$\downarrow$ & RMSE$\downarrow$ & RMSE(log)$\downarrow$ &$\delta_1\uparrow$ &$\delta_2\uparrow$ &$\delta_3\uparrow$ & Time(s)$\downarrow$ \\
			
			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
			
			\multirow{11}{*}{S2D3D}
			& FCRN \cite{laina2016deeper} & 0.1837 & 0.3428 & 0.5774 & 0.1100 & 0.7230 & 0.9207 & 0.9731 & ---\\ 
			& OmniDepth \cite{zioulis2018omnidepth} & 0.1996 & 0.3743 & 0.6152 & 0.1212 & 0.6877 & 0.8891 & 0.9578 & ---\\
			& BiFuse  \cite{wang2020bifuse}  & 0.1209 & 0.2343 & 0.4142 & 0.0787 & 0.8660 & 0.9580 & 0.9860 & 0.7825\\ 
			& SliceNet$^*$ \cite{pintore2021slicenet} & $\textbf{0.0998}^3$ & $\textbf{0.1737}^2$ & 0.3728 & 0.0765 & 0.9038 & 0.9623 & 0.9843 & 0.0668 \\
			& UniFuse \cite{jiang2021unifuse}       & 0.1114 & 0.2082 & 0.3691 & $\textbf{0.0721}^3$   & 0.8711     & 0.9664     & 0.9882 & $\textbf{0.0247}^2$    \\
			& HohoNet \cite{sun2021hohonet} & 0.1014 & $\textbf{0.2027}^3$ & 0.3834 & $\textbf{0.0668}^2$ & $\textbf{0.9054}^3$ & 0.9693 & 0.9771 & $\textbf{0.0400}^3$ \\ 
			& PanoFormer \cite{shen2022panoformer} & --- & --- & $\textbf{0.3083}^1$ &   ---  & $\textbf{0.9394}^1$     & $\textbf{0.9838}^1$     & $\textbf{0.9941}^1$  & 0.1253   \\
			& OmniFusion \cite{li2022omnifusion} & $\textbf{0.0950}^2$ &  ---  & $\textbf{0.3474}^3$ & 0.1599   & 0.8988     & $\textbf{0.9769}^2$     & $\textbf{0.9924}^2$  & 1.5885   \\
			& SphereDepth \cite{yan2022spheredepth} & 0.1158 & 0.2323 & 0.4512 & 0.0754 & 0.8666 & 0.9642 & 0.9863  & 0.0612 \\
			& \textbf{SphereFusion}     & $\textbf{0.0899}^1$ & $\textbf{0.1654}^1$ & $\textbf{0.3194}^2$ & $\textbf{0.0611}^1$   & $\textbf{0.9257}^2$     & $\textbf{0.9755}^3$     & $\textbf{0.9904}^3$ & $\textbf{0.0174}^1$  \\
			
			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
			
			\multirow{11}{*}{M3D} 
			& FCRN  \cite{laina2016deeper}    & 0.2409 & 0.4008 & 0.6704 & 0.1244 & 0.7703 & 0.9174 & 0.9617 & ---\\ 
			& OmniDepth \cite{zioulis2018omnidepth} & 0.2901 & 0.4838 & 0.7643 & 0.1450 & 0.6830 & 0.8794 & 0.9429 & ---\\ 
			& BiFuse \cite{wang2020bifuse}  & 0.2048 & 0.3470 & 0.6259 & 0.1134 & 0.8452 & 0.9319 & 0.9632 & 0.7825\\ 
			& SliceNet \cite{pintore2021slicenet} & 0.1764 & 0.3296 & 0.6133 & 0.1045 & 0.8716 & 0.9483 & 0.9716 & 0.0668 \\
			& UniFuse \cite{jiang2021unifuse} & $\textbf{0.1063}^2$ & \textbf{0.2814} & 0.4941 & \textbf{0.0701}   & $\textbf{0.8897}^3$     & $\textbf{0.9623}^3$     & 0.9831  & $\textbf{0.0247}^2$    \\
			& HohoNet \cite{sun2021hohonet} & 0.1488 & $\textbf{0.2862}^3$ & 0.5138 & 0.0871 & 0.8786 & 0.9519 & 0.9771 & $\textbf{0.0400}^3$ \\ 
			& PanoFormer \cite{shen2022panoformer} & --- & --- & $\textbf{0.3635}^1$ &  ---  & $\textbf{0.9184}^2$     & $\textbf{0.9804}^1$  &  $\textbf{0.9916}^2$   & 0.1253   \\
			& OmniFusion \cite{li2022omnifusion} & $\textbf{0.0900}^1$ & --- & $\textbf{0.4261}^2$ & 0.1483   & $\textbf{0.9189}^1$     & $\textbf{0.9797}^2$     & $\textbf{0.9931}^1$  & 1.5885   \\
			& SphereDepth \cite{yan2022spheredepth} & 0.1205 & 0.3311 & 0.5922 & $\textbf{0.0806}^3$ & 0.8620 & 0.9519 & 0.9770  & 0.0612 \\
			& \textbf{SphereFusion} & $\textbf{0.1145}^3$ & $\textbf{0.2852}^2$ & $\textbf{0.4885}^3$ & $\textbf{0.0733}^2$   & 0.8701     & 0.9613     & $\textbf{0.9838}^3$ & $\textbf{0.0174}^1$  \\
			
			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
			
			\multirow{9}{*}{360D}
			& FCRN \cite{laina2016deeper} & 0.0699 & 0.1381 & 0.2833 & 0.0473 & 0.9532 & 0.9905 & 0.9966 & --- \\
			& OmniDepth \cite{zioulis2018omnidepth} & 0.0931 & 0.1706 & 0.3171 & 0.0725 & 0.9092 & 0.9702 & 0.9851 & --- \\
			& BiFuse \cite{wang2020bifuse}  & 0.0615 & 0.1143 & 0.2440 & 0.0428 & 0.9699 & 0.9927 & 0.9969 & 0.6971\\ 
			& SliceNet \cite{pintore2021slicenet} & 0.0467 & $\textbf{0.1134}^3$ & $\textbf{0.1323}^1$ & $\textbf{0.0212}^1$ & 0.9788 & 0.9952 & 0.9969& 0.0595 \\
			& UniFuse \cite{jiang2021unifuse} & $\textbf{0.0466}^3$ & $\textbf{0.0996}^2$ & 0.1968 & $\textbf{0.0315}^3$ & 0.9835 & 0.9965 & 0.9987 & $\textbf{0.0221}^2$    \\
			& PanoFormer \cite{shen2022panoformer} & --- & --- & $\textbf{0.1429}^2$ & --- & $\textbf{0.9876}^1$ & $\textbf{0.9975}^1$ & $\textbf{0.9991}^1$ & 0.1116   \\
			& OmniFusion \cite{li2022omnifusion} & $\textbf{0.0430}^2$ & --- & $\textbf{0.1808}^3$ & 0.0735 & $\textbf{0.9859}^3$ & $\textbf{0.9969}^3$ & $\textbf{0.9989}^3$ & 1.4151   \\
			& SphereDepth \cite{yan2022spheredepth} & 0.0550 & 0.1145 & 0.2364 & 0.0369 & 0.9743 & 0.9944 & 0.9978 & $\textbf{0.0545}^3$ \\
			& \textbf{SphereFusion} & $\textbf{0.0417}^1$ & $\textbf{0.0894}^1$ & 0.1813 & $\textbf{0.0286}^2$ & $\textbf{0.9869}^2$ & $\textbf{0.9970}^2$ & $\textbf{0.9989}^2$ & $\textbf{0.0155}^1$  \\
			
			\hline
			
		\end{tabular}}
	\end{center}
	\scriptsize{$^*$We recalculate all metrics using open-source models.}

    \vspace{-2.0em}
 
\end{table*}

% NOTE: move to sup
% \begin{align}
% 	\label{eq:metrics}
% 	\begin{cases}
% 		MAE = \sum_{i\in V}{|g_i-p_i|}   \\
% 		MRE = \sum_{i\in V}{{|g_i-p_i|}/{g_i}} \\
% 		RMSE = ({\sum_{i\in V}(g_i-p_i)^2}/{N})^{0.5} \\
% 		RMSE(log) = ({{\sum_{i\in V}(log(g_i)-log(p_i))^2}/{N}})^{0.5} \\
% 		\delta_n = ({\sum_{i\in V}max({g_i}/{p_i},{p_i}/{g_i})<1.25^n})/{N} \\
% 	\end{cases}
% \end{align}


On 360D \cite{zioulis2018omnidepth}, our method achieves the best results on MRE and MAE and ranks second on RMSE(log), $\delta_1$, $\delta_2$, and $\delta_3$.
On Matterport3D \cite{chang2017matterport3d}, our method ranks second on MRE, MAE, and RMSE(log). 
As for Stanford2D3D \cite{armeni2017joint}, our method achieves the lowest MRE, MRE, and RMSE(log) values while ranking second in RMSE and $\delta_1$. 
Overall, our method achieves competitive performance with the state-of-the-art methods. 
Notably, compared to SphereDepth \cite{yan2022spheredepth}, which only utilizes the mesh operation, SphereFusion significantly improves the quality of the depth map by the fusion strategy.
Moreover, SphereFusion achieves comparable results with only a simple ResNet structure compared to OmniFusion \cite{li2022omnifusion} and PanoFormer \cite{shen2022panoformer}, which use the ViT as the encoder, demonstrating the importance of choosing the proper projection.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!ht]
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	

	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/360D/18_rgb.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/360D/18_gt.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/360D/pano_depth/18_pr.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/360D/unifuse/6_pr.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/360D/pano_fusion/18_pr.png}}
	
    \vspace{-1.0em}
		
	\subfloat[RGB]{\includegraphics[width=.18\linewidth]{fig/360D/51_rgb.png}}
	\hspace{0.1em}
	\subfloat[GT]{\includegraphics[width=.18\linewidth]{fig/360D/51_gt.png}}
	\hspace{0.1em}
	\subfloat[SphereDepth]{\includegraphics[width=.18\linewidth]{fig/360D/pano_depth/51_pr.png}}
	\hspace{0.1em}
	\subfloat[UniFuse]{\includegraphics[width=.18\linewidth]{fig/360D/unifuse/17_pr.png}}
	\hspace{0.1em}
	\subfloat[ours]{\includegraphics[width=.18\linewidth]{fig/360D/pano_fusion/51_pr.png}}
	
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/360D/18_cloud_gt.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/360D/18_cloud_pano_depth.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/360D/18_cloud_unifuse.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/360D/18_cloud_pano_fusion.png}}
	
    \vspace{-1.0em}

	\subfloat[GT]{\includegraphics[width=.23\linewidth]{fig/360D/51_cloud_gt.png}}
	\hspace{0.1em}
	\subfloat[SphereDepth]{\includegraphics[width=.23\linewidth]{fig/360D/51_cloud_pano_depth.png}}
	\hspace{0.1em}
	\subfloat[UniFuse]{\includegraphics[width=.23\linewidth]{fig/360D/51_cloud_unifuse.png}}
	\hspace{0.1em}
	\subfloat[ours]{\includegraphics[width=.23\linewidth]{fig/360D/51_cloud_pano_fusion.png}}	

    \vspace{-1.0em}
    
	\caption{
        We select two scenes from 360D \cite{zioulis2018omnidepth} and visualize depth maps and  point clouds.
	}
	\label{fig:3d60_depth_cloud}

    \vspace{-1.3em}
 
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}  [!h]
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/Mat3D/46_rgb.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/Mat3D/46_gt.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/Mat3D/slicenet/46_pr.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/Mat3D/panoformer/46_pr.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/Mat3D/pano_fusion/46_pr.png}}
    
    \vspace{-1.0em}

	\subfloat[RGB]{\includegraphics[width=.18\linewidth]{fig/Mat3D/14_rgb.png}}
	\hspace{0.1em}
	\subfloat[GT]{\includegraphics[width=.18\linewidth]{fig/Mat3D/14_gt.png}}
	\hspace{0.1em}
	\subfloat[SliceNet]{\includegraphics[width=.18\linewidth]{fig/Mat3D/slicenet/14_pr.png}}
	\hspace{0.1em}
	\subfloat[PanoFormer]{\includegraphics[width=.18\linewidth]{fig/Mat3D/panoformer/14_pr.png}}
	\hspace{0.1em}
	\subfloat[ours]{\includegraphics[width=.18\linewidth]{fig/Mat3D/pano_fusion/14_pr.png}}

	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/Mat3D/14_cloud_gt.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/Mat3D/14_cloud_slicenet.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/Mat3D/14_cloud_panoformer.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/Mat3D/14_cloud_pano_fusion.png}}
	
    \vspace{-1.0em}

	\subfloat[GT]{\includegraphics[width=.23\linewidth]{fig/Mat3D/46_cloud_gt.png}}
	\hspace{0.1em}
	\subfloat[SliceNet]{\includegraphics[width=.23\linewidth]{fig/Mat3D/46_cloud_slicenet.png}}
	\hspace{0.1em}
	\subfloat[PanoFormer]{\includegraphics[width=.23\linewidth]{fig/Mat3D/46_cloud_panoformer.png}}
	\hspace{0.1em}
	\subfloat[ours]{\includegraphics[width=.23\linewidth]{fig/Mat3D/46_cloud_pano_fusion.png}}

    \vspace{-1.0em}
	\caption{
        We select two scenes from Matterport3D \cite{chang2017matterport3d} and estimate depth maps by SliceNet \cite{pintore2021slicenet}, PanoFormer \cite{shen2022panoformer}, and our method SphereFusion. 
	}
	\label{fig:mat3d_depth_cloud}
    \vspace{-1.3em}
    
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [t]
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/2D3D/4_rgb.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/2D3D/4_gt.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/2D3D/omnifusion/4_pr.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/2D3D/panoformer/4_pr.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/2D3D/pano_fusion/4_pr.png}}
 
	\vspace{-1.0em}
	
	\subfloat[RGB]{\includegraphics[width=.18\linewidth]{fig/2D3D/7_rgb.png}}
	\hspace{0.1em}
	\subfloat[GT]{\includegraphics[width=.18\linewidth]{fig/2D3D/7_gt.png}}
	\hspace{0.1em}
	\subfloat[OmniFusion]{\includegraphics[width=.18\linewidth]{fig/2D3D/omnifusion/7_pr.png}}
	\hspace{0.1em}
	\subfloat[PanoFormer]{\includegraphics[width=.18\linewidth]{fig/2D3D/panoformer/7_pr.png}}
	\hspace{0.1em}
	\subfloat[ours]{\includegraphics[width=.18\linewidth]{fig/2D3D/pano_fusion/7_pr.png}}
	
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/2D3D/4_cloud_gt.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/2D3D/4_cloud_omnifusion.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/2D3D/4_cloud_panoformer.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.23\linewidth]{fig/2D3D/4_cloud_pano_fusion.png}}
	
    \vspace{-1.0em}
	
	\subfloat[GT]{\includegraphics[width=.23\linewidth]{fig/2D3D/7_cloud_gt.png}}
	\hspace{0.1em}	
	\subfloat[OmniFusion]{\includegraphics[width=.23\linewidth]{fig/2D3D/7_cloud_omnifusion.png}}
	\hspace{0.1em}
	\subfloat[PanoFormer]{\includegraphics[width=.23\linewidth]{fig/2D3D/7_cloud_panoformer.png}}
	\hspace{0.1em}
	\subfloat[ours]{\includegraphics[width=.23\linewidth]{fig/2D3D/7_cloud_pano_fusion.png}}
	\vspace{-1.0em}
	\caption{
        We select two scenes from Stanford2D3D \cite{armeni2017joint} and show depth maps and corresponding point clouds.
	}
	\label{fig:2d3d_depth_cloud}
    \vspace{-1.5em}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In addition to the quality of panorama depth maps, we also compare the inference efficiency of different methods. 
We average the inference time by predicting the depth map of 100 panorama images with a resolution of $512 \times 1024$ on a single RTX 3090 to obtain a reliable inference time. 
SphereFusion is the most efficient method, requiring only 0.0174 seconds per image during inference.
Compared with SphereDepth \cite{yan2022spheredepth}, our method achieves higher efficiency by using a lighter mesh network and the cache strategy to store the FAF during inference. 
Although OmniFusion \cite{li2022omnifusion} and PanoFormer \cite{shen2022panoformer} achieve higher depth map quality, they require longer inference time. 

In summary, SphereFusion benefits from choosing the suitable projection and can obtain comparable reconstruction results with more efficient inference efficiency using only a lightweight network model.

\begin{table*} [ht]
	\begin{center}
		\caption{ Ablation studies on network encoder and different fusion strategies. }
		\label{tab:pipeline}
        \vspace{-0.5em}
        
		\addtolength{\tabcolsep}{-0.2pt}
		\resizebox{\linewidth}{!}{\begin{tabular}{ccc|cccc|ccc}
			\hline\noalign{\smallskip}
			2D Encoder & Mesh Encoder & Fusion Module & MRE$\downarrow$ & MAE$\downarrow$ & RMSE$\downarrow$ & RMSE(log)$\downarrow$ & $\delta_1\uparrow$ & $\delta_2\uparrow$ & $\delta_3\uparrow$ \\
			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
			
			$\checkmark$ & $\times$ & $\times$ & 0.0461 & 0.0969 & 0.2081 & 0.0315 & 0.9833 & 0.9964 & 0.9986 \\
			$\times$ & $\checkmark$ & $\times$  & 0.0572 & 0.1180 & 0.2372 & 0.0374 & 0.9755 & 0.9956 & 0.9985 \\
			$\checkmark$ & $\checkmark$ & BiFuse \cite{wang2020bifuse} & \textbf{0.0415} & \textbf{0.0888} & 0.1824 & 0.0288 & 0.9861 & 0.9969 & 0.9988 \\
			$\checkmark$ & $\checkmark$ & UniFuse \cite{jiang2021unifuse} & 0.0427 & 0.0915 & 0.1837 & 0.0290 & 0.9868 & 0.9969 & 0.9988 \\
			$\checkmark$ & $\checkmark$ & GateFuse (ours)   & 0.0417 & 0.0894 & \textbf{0.1813} & \textbf{0.0286} & \textbf{0.9869} & \textbf{0.9970} & \textbf{0.9989} \\
			
			\hline
		\end{tabular}}
	\end{center}
    \vspace{-2.5em}
\end{table*}


\subsection{Qualitative Evaluation}

For the qualitative evaluation, we visualize depth maps of different methods in Fig. \ref{fig:3d60_depth_cloud}, Fig. \ref{fig:mat3d_depth_cloud}, and Fig. \ref{fig:2d3d_depth_cloud}.
Furthermore, we convert them to point clouds to compare different methods in the 3D space and visualize point clouds by Meshlab \cite{meshlab} with the same rendering settings.


On 360D \cite{zioulis2018omnidepth}, we compare our method with SphereDepth \cite{yan2022spheredepth} and UniFuse \cite{jiang2021unifuse}, and visualize depth maps and corresponding point clouds in Fig. \ref{fig:3d60_depth_cloud}.
UniFuse has better reconstruction results in the middle areas but struggles around the polar regions, such as the lights on the ceiling. SphereDepth reconstructs the ceiling region but suffers from losing details, such as edges of the door and the wall. SphereFusion combines the strengths of two projections and can reconstruct details and polar regions at the same time.


On Matterport3D \cite{chang2017matterport3d}, we compare our method with SliceNet \cite{pintore2021slicenet} and PanoFormer \cite{shen2022panoformer}, and Fig. \ref{fig:mat3d_depth_cloud} shows  results.
SliceNet suffers from poles, as it only uses the equirectangular projection and extracts features by the 2D image encoder.
PanoFormer and SphereFusion achieve better results using the tangent and spherical projections.


On Stanford2D3D \cite{armeni2017joint}, we compare our method with OmniFusion \cite{yan2022spheredepth} and PanoFormer \cite{jiang2021unifuse}.
Fig. \ref{fig:2d3d_depth_cloud}
shows depth maps and corresponding point clouds.  
OmniFusion and PanoFormer use the tangent projection and attempt to reduce discontinuities using more complex feature fusion mechanisms. 
However, OmniFusion fails to merge different tangent patches and has noticeable gaps in the point cloud, while PanoFormer is smoother and loses some details. 
Although OmniFusion and PanoFormer achieve better reconstruction results than SphereFusion, our method only uses a simple encoder based on ResNet, demonstrating the importance of choosing the proper projection.


Overall, SphereFusion utilizes the spherical projection to avoid distortion and discontinuities and the equirectangular projection to extract visual features, achieving comparable results with state-of-the-art methods with a lighter network and higher inference efficiency.


\subsection{Ablation Studies}

We conduct several ablation studies to study the influence of different components of the SphereFusion. We first compare the network encoder to show the importance of using two encoders to extract features from the panorama image. We then study how to fuse features from the spherical and equirectangular projection. Throughout all ablation experiments, we use the 360D dataset.

\subsubsection{Network Encoder}
To evaluate the contribution of each encoder, we build two networks to estimate panorama depth, where each network only uses one type of encoder.
Table \ref{tab:pipeline} shows the performance of different network structures. 
The network that only uses the mesh encoder obtains the worst results, which cannot reconstruct details of the panorama image only through the mesh operation, as SphereDepth \cite{yan2022spheredepth} does.
The network that only uses the 2D image encoder ranks second in Table \ref{tab:pipeline}, which can achieve higher performance but cannot deal with distortion and discontinuity.
SphereFusion outperforms others and achieves the best results, proving that combining the 2D image encoder and the mesh encoder can obtain higher-quality depth maps.


\subsubsection{Fusion Strategy}
The fusion strategy fuses features from different panorama projections.
To compare different fusion strategies, we implement BiFuse \cite{wang2020bifuse}, UniFuse \cite{jiang2021unifuse}, and our GateFuse to fuse features in the spherical projection.
Table \ref{tab:pipeline} shows the results of different fusion strategies.
UniFuse obtains the worst results, and BiFuse ranks second.
Our GateFuse achieves the best results on RMSE, RMSE(log), $\delta$, and ranks second on MAE and MRE.
We visualize depth maps from different fusion strategies for better comparison in Fig. \ref{fig:viz_fusion}, where BiFuse and UniFuse fail to reconstruct details.


\begin{figure} [h]
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/compare/186_rgb.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/compare/186_gt.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/compare/186_pr_bifuse.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/compare/186_pr_unifuse.png}}
	\hspace{0.1em}
	\subfloat[]{\includegraphics[width=.18\linewidth]{fig/compare/186_pr.png}}
	\hspace{0.1em}
 
	\vspace{-1.0em}

	\subfloat[RGB]{\includegraphics[width=.18\linewidth]{fig/compare/222_rgb.png}}
	\hspace{0.1em}
	\subfloat[GT]{\includegraphics[width=.18\linewidth]{fig/compare/222_gt.png}}
	\hspace{0.1em}
	\subfloat[BiFuse]{\includegraphics[width=.18\linewidth]{fig/compare/222_pr_bifuse.png}}
	\hspace{0.1em}
	\subfloat[UniFuse]{\includegraphics[width=.18\linewidth]{fig/compare/222_pr_unifuse.png}}
	\hspace{0.1em}
	\subfloat[ours]{\includegraphics[width=.18\linewidth]{fig/compare/222_pr.png}}
	\hspace{0.1em}

    \vspace{-1.0em}

	\caption{
		We select two scenes from 360D \cite{zioulis2018omnidepth} and compare depth maps from different fusion strategies.
		Our GateFuse can reconstruct more details, and we mark out these regions with red boxes.
	}
	\label{fig:viz_fusion}

    \vspace{-1.5em}
 
\end{figure}


\subsection{Limitations}

We propose SphereFuion for panorama depth estimation by using the 2D image convolution and the mesh convolution, which achieves competitive results with a lighter network and the highest inference efficiency.
However, the cache strategy requires additional GPU memory to store FAF information during training and testing.
Furthermore, SphereFusion requires huge GPU memory during training and does not support ultra-high resolution panorama images, such as $1024 \times 2048$ \cite{rey2022360monodepth}. 
We still need more in-depth research to improve the mesh operation and the quality of panorama depth estimation. 

