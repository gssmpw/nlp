\section{Introduction}
\label{sec:intro}

Depth estimation is an important task in computer vision that helps to understand the 3D environment. In particular, the panorama image has a 360$^\circ$ field of view (FOV) and can reconstruct the entire surrounding environment in one shot \cite{zioulis2018omnidepth,wang2020bifuse}. With the development of consumer-level panorama cameras, such as Ricoh Theta, Samsung Gear360, and Insta360 ONE, it becomes an intriguing task to estimate the depth map from the panorama image \cite{pintore2021slicenet,sun2021hohonet,yan2022spheredepth,shen2022panoformer,li2022omnifusion}, and many related datasets have been generated to facilitate the research of panorama depth estimation \cite{zioulis2018omnidepth,armeni2017joint,chang2017matterport3d,zheng2020structured3d,albanis2021pano3d}.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{fig/time_viz.png}
	\end{center}

    \vspace{-1.5em}
 
	\caption{
		% Comparison with previous state-of-the-art methods BiFuse \cite{wang2020bifuse}, UniFuse \cite{jiang2021unifuse}, SliceNet \cite{pintore2021slicenet}, PanoFormer \cite{shen2022panoformer}, OminiFusion \cite{li2022omnifusion}, SphereDepth \cite{yan2022spheredepth}, HohoNet \cite{sun2021hohonet} on Stanford2D3D \cite{armeni2017joint} with resolution of $512 \times 1024$. The horizontal axis is the FPS (frame per second), and the vertical axis is $ \delta(1.25) (\%)$, which counts the percentage of the absolute relative difference between the prediction and the ground truth that is less than 1.25. The higher FPS and higher $ \delta(1.25) (\%)$ mean better.
        Comparison with BiFuse \cite{wang2020bifuse}, UniFuse \cite{jiang2021unifuse}, SliceNet \cite{pintore2021slicenet}, PanoFormer \cite{shen2022panoformer}, OminiFusion \cite{li2022omnifusion}, SphereDepth \cite{yan2022spheredepth}, HohoNet \cite{sun2021hohonet} on Stanford2D3D \cite{armeni2017joint} with resolution of $512 \times 1024$. The horizontal axis is the FPS, and the vertical axis is $ \delta(1.25) (\%)$, which counts the percentage of the absolute relative difference between the prediction and the ground truth that is less than 1.25. The higher FPS and higher $ \delta(1.25) (\%)$ mean better.
	}
	\label{fig:time_viz}

    \vspace{-2.0em}
 
\end{figure}

However, it is challenging to find a proper way to represent the panorama image. 
The most popular equirectangular projection faces huge distortion around the poles and poor discontinuity near the borders. The cubemap projection \cite{skupin2017standardization} and the tangent projection \cite{eder2020tangent,shen2022panoformer,li2022omnifusion} project the panorama image to several planes to avoid distortion but introduce serious discontinuity problems and have to rely on a well-designed fusion strategy to merge them. The spherical projection \cite{yan2022spheredepth} can deal with distortion and discontinuity by approximating the sphere, but it is hard to capture details of the panorama image and cannot directly handle high-resolution panorama images.


Based on the characteristics of these projections, different strategies are used to extract features from panorama images. 
The equirectangular projection represents the panorama image in one image plane and can directly utilize 2D image convolution to extract features. Nevertheless, the equirectangular projection faces huge distortion and needs specially designed 2D convolution kernels to extract reliable features \cite{zioulis2018omnidepth,tateno2018distortion,chen2021distortion}.
Like the equirectangular projection, the cubemap projection and tangent projection employ multiple planes to represent the panorama image and use 2D image convolution to extract features. However, they require a suitable mechanism to fuse features from different planes and maintain global consistency \cite{cheng2018cube,li2022omnifusion,shen2022panoformer,peng2022high}. 
% On the other hand, the spherical projection is an ideal way to represent the panorama image, and utilizes mesh operation to extract features. Nonetheless, the mesh operation struggles to capture details and exhibits inefficiency when dealing with complex topologies 
\cite{yan2022spheredepth,hu2021subdivision,feng2019meshnet,hanocka2019meshcnn}.
On the other hand, the spherical projection is an ideal way to represent the panorama image, and utilizes mesh operation to extract features. Nonetheless, the mesh operation struggles to capture details and exhibits inefficiency \cite{yan2022spheredepth,hu2021subdivision,feng2019meshnet,hanocka2019meshcnn}.
% Meanwhile, BiFuse \cite{wang2020bifuse} and UniFuse \cite{jiang2021unifuse} try to combine the strengths of different projections via projecting features from the cubemap projection to the equirectangular projection. Although they achieve better depth estimation results, they still suffer from distortion around the poles.
Meanwhile, BiFuse \cite{wang2020bifuse} and UniFuse \cite{jiang2021unifuse} try to combine the strengths of different projections. Although they achieve better depth estimation results, they still suffer from distortion around the poles.


To this end, this paper proposes SphereFusion, which unites equirectangular and spherical projections.
SphereFusion first represents the panorama image by equirectangular and spherical projections, then extracts two types of features, and finally fuses them through a fusion module to estimate the panorama depth in the spherical projection.
% SphereFusion first represents the panorama image by equirectangular and spherical projections, then extracts two types of features using 2D convolution and mesh convolution, and finally fuses them through a tailormade fusion module to estimate the panorama depth in the spherical projection.
To balance the accuracy and efficiency, SphereFusion implements a lightweight encoder to extract features \cite{he2016deep} and utilizes a cache strategy to reduce the computation complexity of the mesh operation.
Fig. \ref{fig:time_viz} compares the efficiency and quality of the depth map with state-of-the-art methods \cite{wang2020bifuse,jiang2021unifuse,pintore2021slicenet,shen2022panoformer,li2022omnifusion,yan2022spheredepth,sun2021hohonet}.  
SphereFusion obtains high-quality depth maps and achieves around 60 FPS during inference on the panorama image with a resolution of $512 \times 1024$.
Our contributions are summarized as follows.

\begin{enumerate}

\item We propose a panorama depth estimation method, SphereFusion, to estimate the depth map in the spherical projection, which uses the features from the equirectangular projection to improve the details.
	
\item We design a feature fusion module, GateFuse, which selects reliable features from two projections to improve the quality of the depth map.
	
\item Experiments demonstrate that SphereFusion achieves competitive results on three public panorama datasets and can produce point clouds with less noise and higher completeness. Besides, SphereFusion achieves 60 FPS of inference speed on an NVIDIA RTX 3090, outperforming existing methods.

\end{enumerate}