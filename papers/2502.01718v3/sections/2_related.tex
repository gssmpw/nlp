\section{Related Works}
\label{sec:related_works}
\subsection{LLM for Code Generation}
\label{subsec:llm_for_coding}
Large language models (LLMs) have demonstrated significant potential in code generation. Due to the unique nature of coding tasks, specialized coding models such as Code Llama~\citep{codellama} and Qwen Coder~\citep{hui2024qwen2,qwen2} were developed shortly after the emergence of general-purpose LLMs. These models typically undergo a two-phase training process: pre-training and fine-tuning. During pre-training, they are exposed to extensive coding corpora sourced from various internet platforms, including raw text, GitHub repositories, and pull requests. This is followed by supervised fine-tuning, which enhances their instruction-following capabilities.
To assess the performance of these models in code generation, several benchmarks have been established, including MBPP~\citep{austin2021program}, HumanEval~\citep{codex}, EvalPlus~\citep{evalplus, evalperf}, Big Code Bench~\citep{zhuo2024bigcodebench}, and Live Code Bench~\citep{jain2024livecodebench}. These benchmarks usually include a series of prompts or problems for the LLMs to solve, and they also contain test cases to assess the correctness of the generated code.


\subsection{Synthesizing Test Cases}
Automatic test generation is a widely used approach for verifying the correctness of LLM-generated programs. Prior work has commonly employed the same LLM that generates the programs to also generate test cases, selecting the most consistent program from multiple sampled outputs in a self-consistency manner~\citep{Chen2022CodeTCG,Huang2023EnhancingLL,Jiao2024PreferenceOF}. However, these generated test cases often suffer from significant hallucinations. To address this issue, Algo~\citep{Zhang2023ALGOSA} introduced the use of an oracle program solution to improve test case quality. While similar in spirit to our test case filtering approach, Algo constructs its oracle solution by exhaustively enumerating all possible combinations of relevant variables, whereas we leverage a stronger coder LLM to generate the oracle solution. Beyond using test cases as verification signals, Clover~\citep{Sun2023CloverCV} enhances program verification by performing consistency checks between code, docstrings, and formal annotations, incorporating formal verification tools alongside LLMs.

\subsection{Reward Models}
\label{subsec:reward_models}
Reward models play a crucial role in aligning LLMs by assigning scalar values to response pairs based on specific evaluation criteria, such as human preference~\citep{instructgpt} and accuracy~\citep{Zhang2025TheLO}. They are widely used in reinforcement learning with human feedback (RLHF) to refine model behavior and in Best-of-N sampling to enhance test-time performance.
However, while general-purpose reward models are effective for assessing human preference, they often struggle with specialized domains like mathematics and coding due to the complexity of these tasks. For instance, even top-ranked reward models from Reward Bench~\citep{Lambert2024RewardBenchER}, such as Skywork-RM~\citep{skywork}, have difficulty providing reliable rewards for these domains. To address this issue, task-specific reward models have been developed, such as Qwen-2.5-Math-PRM~\citep{Zhang2025TheLO} for mathematical reasoning. However, coding reward models have remained largely absent due to the lack of reliable training signalsâ€”an issue that our proposed \RM{} aims to address.


\subsection{Reinforcement Learning for LLM}
\label{subsec:rl_for_llm}
Reinforcement Learning from Human Feedback (RLHF)\citep{instructgpt} has been widely adopted to enhance the capabilities of large language models (LLMs) in various tasks, including conversational interactions and mathematical reasoning\citep{yang2024qwen25mathtechnicalreportmathematical}. Reinforcement learning (RL) algorithms such as PPO\citep{Schulman2017ProximalPO}, GRPO\citep{Shao2024DeepSeekMathPT}, and Reinforcement++\citep{reinforce++} have been employed to fine-tune models using reward signals derived from either learned reward models\citep{Shao2024DeepSeekMathPT} or predefined rule-based heuristics~\citep{DeepseekR1, autocode}.

Given that coding is an inherently verifiable task, recent studies have explored RL techniques that leverage direct execution accuracy as a reward signal. PPOCoder~\citep{Shojaee2023ExecutionbasedCG} and CodeRL~\citep{Le2022CodeRLMC} demonstrated the effectiveness of PPO-based RL for coding tasks, while RLEF~\citep{Gehring2024RLEFGC} extended this approach to multi-turn settings by incorporating execution feedback at each step. StepCoder~\citep{Dou2024StepCoderIC} refined the reward mechanism by assigning rewards at a more granular level, considering only successfully executed lines of code. Additionally, DSTC~\citep{Liu2024DSTCDP} explored the application of Direct Preference Optimization (DPO) to code generation by using self-generated test cases and programs.

Despite these advancements, most prior RL-based approaches for coding have been constrained by the use of pre-annotated datasets such as APPS~\citep{APPS}, which consists of only 5,000 examples, with most problems having a single test case. This limited data availability poses challenges to scalable RL training. Furthermore, the potential of reward models for coding remains largely unexplored. In this work, we address these limitations by automatically synthesizing test cases and leveraging trained reward models for reinforcement learning, demonstrating the scalability and effectiveness of our approach.


% Since the introduction of Reinforcement Learning from Human Feedback (RLHF)\citep{instructgpt}, it has been extensively applied to enhance LLM capabilities in tasks such as conversational interactions and mathematical reasoning\citep{yang2024qwen25mathtechnicalreportmathematical}. Popular reinforcement learning algorithms, including PPO~\citep{Schulman2017ProximalPO}, GRPO~\citep{Shao2024DeepSeekMathPT}, and Reinforcement++~\citep{reinforce++}, have been used to fine-tune models according to reward signals generated either by reward models~\citep{Shao2024DeepSeekMathPT} or pre-defined rule-based rewards~\citep{DeepseekR1, autocode}.
% Since coding is inherently a verifiable task, reinforcement learning in coding has been explored by many recent work with the direct execution accuracy as a reward. PPOCoder~\citep{Shojaee2023ExecutionbasedCG} and CodeRL~\citep{Le2022CodeRLMC} showed the potential of RL for coding using PPO. RLEF~\citep{Gehring2024RLEFGC} further explored the RL in the multi-turn scenario by providing the execution feedback in each turn. StepCoder~\citep{Dou2024StepCoderIC} used a more fine-grained reward for only the executed lines of the code. And DSTC~\citep{Liu2024DSTCDP} also tried applying DPO to the coding model with self-generated tests and programs. However, most of them do RL on pre-annotated datasets like APPS~\citep{APPS}, which contains only 5,000 examples, with most problems having just a single test case, making it insufficient for scalable RL-based training. What's more, the potential of reward models for coding remains unexplored in these works. In our paper, we tackle these issues by automatically synthesize test cases and conduct RL using our trained reward models, proving the scalability of our method.

% Despite the fact that coding is inherently a verifiable task, reinforcement learning has seen limited application in code generation due to challenges in defining meaningful and scalable reward signals. The most relevant prior work, CodeRL, leverages the  to generate rewards based on test case evaluations. However, APPS . Our work is the first to automate high-quality test case synthesis in code domain.
