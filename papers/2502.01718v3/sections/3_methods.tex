\section{Methodology}
\label{sec:methodology}

In this section, we will introduce the overall methodology of \coder. We begin with formulations of the problems we are investigating, including reward model training and reinforcement learning for LLMs. We then elaborate on how we synthesize the test cases and construct the \dataset{}. Finally, we explain how we perform the reinforcement learning using our \RM trained on the \dataset{}.

\subsection{Problem Formulation}
\label{subsec:problem_formulation}
\paragraph{Reward Model Training}
Let $x$ denote the coding question and $\mathbf{y} = \{y_1, \cdots, y_t\}$ denote the program solution, where $y_i$ represents the $i$-th token of the program solution and $(\mathbf{x},\mathbf{y}) \in D$. Assuming $\theta$ represents the parameters of the model, then $n$ responses $(\mathbf{y}^1, ..., \mathbf{y}^n)$ will be sampled from the model $\pi_{\theta}$ given the input $\mathbf{x}$. Let $(s_1, ..., s_n)$ be the target rewards, i.e. the test case pass rates in our scenario, then we define the Bradley-Terry loss~\citep{Bradley1952RankAO} for every pair of responses $\mathbf{y}^i$ and $\mathbf{y}^j$ with scores of $s_i$ and $s_j$ when we are training a reward model $R_\phi$ as follows:
\begin{equation*}
\begin{split}
    &\mathcal{L}_\phi(\mathbf{x},s_i,s_j) \\
    & = \mathbbm{1}[s_i > s_j]\log{\sigma(R_\phi(\mathbf{x}, \mathbf{y}^i) - R_\phi(\mathbf{x}, \mathbf{y}^j))}
\end{split}
\end{equation*}
where $\mathbbm{1}[\cdot]=1$ if the expression inside the brackets is true, otherwise, it's 0. The final loss function for the reward training is:
\begin{equation}
\label{eq:rm_loss}
\mathcal{L}(\phi) = -\frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j=1}^n \mathcal{L}_\phi(\mathbf{x},s_i,s_j)
\end{equation}
That means the reward model is trained to assign higher values to preferred responses and lower values to non-preferred ones, maximizing the difference between these ratings.

\paragraph{Best-of-N Sampling}
After we get the trained reward model $R_\phi$, one way to quickly test the performance of the reward model is Best-of-N sampling, which is usually used as a test-time scaling approach. We will simply select the best response according to the predicted value of $R_\phi$. That is $\mathbf{y}^* = \arg\max_{\mathbf{y}^i \in {\mathbf{y}^1,...,\mathbf{y}^N}} R_\phi(\mathbf{x},\mathbf{y}^i)$. 

\paragraph{Reinforcement Learning}
We can finally conduct reinforcement learning for the original policy model $\pi_{\theta}$ after we get a well-trained reward model $R_\phi$. Proximal Policy Optimization (PPO) is an actor-critic RL algorithm that is widely used for LLM's RL process. Let $\pi_{\theta_{old}}$ be the reference model and $\pi_{\theta}$ be the current policy model that we are updating frequently during the RL training. We denote $r_t(\theta)$ as the probability ratio of the current policy model over the old policy model on the $t$-th generated token:
\begin{equation}
    r_t(\theta) = \frac{\pi_{\theta}(y_t|\mathbf{x}, \mathbf{y}_{<t})}{\pi_{\theta_{old}}(y_t|\mathbf{x}, \mathbf{y}_{<t})}
\end{equation}
Then the PPO algorithms optimize the LLM by the following surrogate objective:
\begin{equation*}
\begin{split}
    &\mathcal{L}_{PPO}(\theta) = \\
    &-\frac{1}{|\mathbf{y}|}\sum_{t=1}^{|\mathbf{y}|}\min \left [r_t\left (\theta\right )A_t, \text{clip}\left (r_t\left (\theta\right ), 1-\epsilon,1+\epsilon\right )A_t \right ]
\end{split}
\end{equation*}
where $\mathbf{y} \sim \pi_{\theta_{old}}(\cdot|x)$, and $A_t$ is the advantage computed through the Generalized Advantage Estimation (GAE)~\citep{Schulman2015HighDimensionalCC} via the rewards generated by $R_\phi$ and the learned value function $V_\psi$. The PPO training objective will force the policy model $\pi$ to increase the probability of generating tokens with higher $A_t$ and decrease the probability ratio of generating tokens with lower $A_t$ until the clipped bounds $1+\epsilon$ and $1-\epsilon$ are reached respectively.

However, PPO usually requires training an additional value model $V_\psi$ and thus makes the training inefficient. Recently, there are some other works like Reinforecement++~\citep{reinforce++} that eliminate the need for value model but instead compute advantage only using the rewards generated by $R_\phi$ and the KL-divergence of the tokens after the $t$-th tokens. This makes the RL process more efficient and has also proved to be more stable.