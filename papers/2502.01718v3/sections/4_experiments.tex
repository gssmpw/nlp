\section{Experiments}
\label{sec:experiments}
\input{tables/main_result}

\subsection{Reward Model Training Setup}
We mainly use Qwen2.5-Coder-7B-Instruct~\footnote{\href{https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct}{Qwen/Qwen2.5-Coder-7B-Instruct}} as the backbone of the reward model and sample 16 responses from it for each question in \dataset{}. Finally, following the rule defined in ~\autoref{eq:pref_pair_sel}, around 300K preference pairs were created out of 46,618 distinct questions (37.34\% of the total questions) that have at least one pair satisfying the condition, and other questions are not used.
\input{tables/rl_result}

Our reward model is trained using LlamaFactory~\citep{zheng2024llamafactory}. We apply full fine-tuning with DeepSpeed stage 3. We train for 1 epoch using a cosine learning rate schedule, starting at 1e-5 with a warmup ratio of 0.1 to gradually increase the learning rate in the initial training phase. Training batch size is set to 128. We enable bf16 precision to reduce memory overhead without compromising model fidelity. The training takes 24 hours on 8 x A100 GPUs.

\subsection{Reinforcement Learning Setup}

We perform RL training from three policy models: Qwen2.5-7B-Instruct~\footnote{\href{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}{Qwen/Qwen2.5-7B-Instruct}} and Qwen2.5-Coder-7B-Base~\footnote{\href{https://huggingface.co/Qwen/Qwen2.5-Coder-7B}{Qwen/Qwen2.5-Coder-7B}} and Qwen2.5-Coder-7B-Instruct. Two types of reward can be used, i.e. the trained reward model \RM-7B and the rule-based reward, i.e. pass rate over the test cases in \dataset{}. During training, we set the pass rate to be a binary reward, which is 1.0 when all test cases passed, otherwise 0. This is similar to the verfiable reward used in Tulu3~\citep{tulu3} and DeepSeek-R1~\citep{DeepseekR1}. Similar to DeepSeek-R1~\citep{DeepseekR1}, we also experiment with RL from the base model because SFT may cause the search space of the model to be stuck in the local minimum. Since coding is also a highly verifiable task like math, we include the Qwen2.5-Coder-7B-Base in our experiments.

We have trained different policy model backbones with different rewards, resulting in 6 RL models in total. All the RL-tuning are based on \texttt{OpenRLHF}~\citep{Hu2024OpenRLHFAE}. We adopt the Reinforcement++~\citep{reinforce++} algorithm instead of PPO to improve the training efficiency without training the value model. It's also proved to be more stable than PPO and GRPO. We train our model on a subsampled hard version of \dataset{}, where we keep the 25\% of the questions with lower average pass rates and higher variance. This is to ensure the question is hard and that the sampled programs are diverse enough. For the training hyperparameters, we set the rollout batch size to 256, and 8 programs are sampled from per question. The training batch size is 128 with a learning rate of 5e-7. All the models are trained for 1 episode and finished in 6 hours on 8 x H100 GPUs.


\subsection{Evaluation Setup}
We evaluate our method on three established code-focused benchmarks: EvalPlus~\citep{evalplus, evalperf}, Big Code Bench~\citep{zhuo2024bigcodebench} and Live Code Bench~\citep{jain2024livecodebench}. These benchmarks collectively cover a diverse array of coding tasks, enabling us to assess both the correctness and quality of generated code. For Best-of-N sampling experiments, we adopt top-p sampling with a temperature of 1.0 to generate multiple candidate solutions per question. We select the response with the highest reward for evaluation. For RL experiments, we use the benchmark's default setting, which is greedy sampling most of the time.

\subsection{Main Results}
\label{subsec:main_results}

Here we show the experimental results of reward model and RL-trained model. 
\paragraph{RM Results}
We conduct Best-of-N experiments on 3 inference models, specifically Mistral-Instruct-V0.3-7B\citep{mistral7binstructv03}, Llama-3.1-Instruct-8B~\citep{grattafiori2024llama3herdmodels}, and Qwen2.5-Coder-7B-Insutrct~\citep{hui2024qwen2, qwen2}. We additionally report the average score across all generated samples and also the oracle score (pass@N) for better comparison.

According to ~\autoref{tab:main_bon_results}, \RM can consistently boost the performance of inference models by a large margin compared to the greedy decoding results. On weaker models like Mistral~\citep{mistral7binstructv03} and Llama-3.1~\citep{zheng2024llamafactory}, the overall improvements are greater than 10 points. These improvements can be attributed to our reward modelâ€™s ability to identify high-quality completions among multiple candidates, thereby reducing the impact of suboptimal sampling on the final output. Notably, these gains become more pronounced on benchmarks where the gap between greedy decoding and oracle performance (i.e., the best possible completion among all samples) is larger. In such cases, the variance among sampled completions is relatively high, providing greater opportunities for the reward model to pinpoint and elevate top-tier responses.

Greedy decoding systematically outperforms the average sampled performance, reflecting the strong code generation capability of these inference models. Consequently, while most reward models achieve best-of-N results above the average, we consider a reward model effective only if it surpasses the performance of greedy decoding.

\paragraph{RL Results}
We perform RL training over 3 different initial policy models in~\autoref{tab:rl_results} with model-based and rule-based rewards. When starting from Qwen2.5-Instruct-7B, we can see the RL tuning can consistently improve the performance, especially on HumanEval and MBPP. Even for the Plus version with more and harder test cases, the RL-tuned model also has more than $3$ points of improvement. 

When starting from the Qwen2.5-Coder-Instruct-7B itself, we can still observe improvements, especially when using the rule-based reward. For example, we get more than $3.4$ improvement on BigCodeBench-Full-Hard. Using the reward model for RL can also bring $3.2$ improvement on MBPP. This highlights the charm of self-improvement given the reward model backbone is the same with the initial policy model. We compare our method with other RL-based models like RLEF~\citep{rlef}, PPOCoder~\citep{PPOCoder}, StepCoder~\citep{stepcoder}, DSTC~\citep{dstc}, etc. We show that our 7B model is able to beat these competitors across the evaluation benchmarks.

Another experiment we conduct is to perform RL training directly from base model Qwen2.5-Coder-7B-base. We show significant improvement, especially through test-case pass rewards on HumanEval, MBPP, and BigCodeBench-I. These results are achieved by only training for 80 steps. We believe further scaling up the training will lead to much larger gains.

\paragraph{Comparison with Other RMs}
\input{tables/competitor_analysis}
We compare our \RM with 3 top-ranked RM on the RewardBench, including InternLM2-RM-8B~\citep{cai2024internlm2}, Skywork-Llama-3.1-8B, and Skywork-Gemma-27B~\citep{skywork}, where results are reported in~\autoref{tab:comparison_with_other_rm}. We can see that these general-purpose RM can hardly improve and sometimes decrease the performance through Best-of-N sampling compared to greedy sampling, showcasing the incapability in identifying the correct generated programs. On the other hand, our \RM surpasses all other publicly released reward models in our evaluation and consistently gets positive gains. These findings further underscore our assumption that previous RM training lacks of reliable signals for codes and prove that our RMs can generate reliable and state-of-the-art reward signals in code generation tasks.

\subsection{Ablation Studies}

\paragraph{Test Case Quality Matters}
\input{tables/ablation_test_case_filtering}
\input{tables/ablation_different_base}
We also conduct experiments to investigate how filtering the test cases with a proxy model can affect the results. As shown in ~\autoref{tab:consistency_ablation}, training RM on data after the filtering improve the performance significantly, especially for those hard code questions like MBPP-Plus and BigCodeBench-Hard (C/I). We believe this is because the test case filtering can ensure the remaining ones are consistent with each other and thus point to the same implicit program, which improves the quality of the rewards.

\paragraph{RM Backbone Matters}
Our results in ~\autoref{tab:backbone_ablation} clearly show that changing the backbone of the reward model from Llama-3.1 to Qwen2.5 can significantly improve the Best-of-16 performance. This is because the Qwen2.5-Coder models have been pre-trained on way more code-related data compared to the Llama-3.1 models, and thus more knowledgeable when tuning it into a reward model.



\paragraph{Does R1-style Tuning Work?}
Inspired by the recent DeepSeek-R1~\citep{DeepseekR1}, we also conduct the RL directly from the base model without any SFT. It turns out we get huge improvements when using rule-based rewards. For example, we get $25.0$ points of improvements on HumanEval-Plus after training only 6 hours from the Base Model, which is way more efficient that the large-scale SFT. What's more, the \coder$_{Rule}$ improve the BigCodeBench-Instruct-Full's performance from $40.2$ to $43.2$, nearly the same performance with DeepSeek-R1-Distill-Qwen-32B ($43.9$) which was directly distilled from the DeepSeek-R1 Model. This further consolidates the finding of DeepSeek-Zero. However, we do find that using reward models for RL tuning can lead to worse results. We attribute this to the potential reward hacking during the tuning. 




% \subsection{Case Study}