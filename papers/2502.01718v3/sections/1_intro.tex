\section{Introduction}
\label{sec:intro}
In recent years, code generation models have advanced significantly with compute scaling~\citep{kaplan2020scaling} and training data quality improvement~\citep{huang2024opencoder,lozhkov2024starcoder,guo2024deepseek}. The state-of-the-art coder models, including Code-Llama~\citep{codellama}, Qwen2.5-Coder~\citep{Hui2024Qwen25CoderTR}, DeepSeek-Coder~\citep{Guo2024DeepSeekCoderWT} and so on, have shown unprecedented performance across a wide range of coding tasks like program synthesis~\citep{codex}, program repair~\citep{opencodeinterpreter}, optimization~\citep{shypula2023learning}, test generation~\citep{steenhoek2023reinforcement}, SQL~\citep{spider}, issue fix~\citep{swebench}. These models are all pre-trained and further supervised fine-tuned (SFT) on large-scale coding data from web resources like Common Crawl or Github.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/overall.pdf}
    \caption{Overall Workflow of our model: we start from the seed code dataset to create well-formatted questions and corresponding test cases. Then we adopt strong models to filter the noisy test cases. Finally, we adopt these test cases to harvest positive and negative program pairs for reward model training and RL.}
    \vspace{-3ex}
    \label{fig:overview}
\end{figure}
Though strong performance has been achieved through SFT~\citep{luo2023wizardcoder,magicoder}, very few models have explored the potential of reinforcement learning (RL)~\citep{Ouyang2022TrainingLM}, which has proven effective in other domains such as mathematical reasoning like DeepSeek-R1~\citep{Shao2024DeepSeekMathPT}. We argue that this absence of RL-based training in coder models is primarily due to two key challenges:\vspace{1ex}\\
\textbf{(1) Lack of reliable reward signals for code generation}. In tasks such as mathematical problem-solving, rewards can be easily derived from rule-based string matches with reference answers~\citep{DeepseekR1} or large-scale human annotations~\citep{instructgpt}. In contrast, evaluating code quality typically requires executing test cases to measure the pass rate, making reward signal design more complex. This also explains why existing reward models like Skywork~\citep{skywork} can hardly generalize to the coding domain (see ~\autoref{subsec:main_results}). \\
\textbf{(2) Scarcity of large-scale coding datasets with reliable test cases.} Most existing coding datasets like APPS~\citep{APPS,codex} heavily rely on costly human expert annotations for test cases, which limits their scalability for training purposes. The largest data is TACO~\citep{taco} with 25K examples, which are crawled from the popular coding competition websites, which were already heavily exploited during the pre-training phase.   
 

Therefore, we curate \dataset{}, on which we trained our reward models: \RM-7B and \RM-32B. Comprehensive experiments of best-of-N sampling show that \RM can significantly boost existing LLM's performance on coding benchmarks. For example, \RM-7B can improve the performance of Llama-3.1-8B-Instruct by an average of $8.4$ points across the 4 coding benchmarks, i.e. HumanEval~\citep{evalplus}, MBPP~\citep{evalplus}, BigCodeBench~\citep{zhuo2024bigcodebench} and LiveCodeBench~\citep{jain2024livecodebench}. Even for the stronger coder model Qwen2.5-Coder-7B-Instruct, our "7B+7B" combination still gets an average of $2.6$ improvements. \RM-32B is even more powerful, which pushes the former two numbers to $10.7$ and $4.7$ respectively, showcasing the effectiveness of \RM.

Furthermore, we adopt \RM-7B and test case pass rate separately to do reinforcement learning with reinforce++~\citep{reinforce++} over coder models. Experiments show $2.1$ and $0.7$ points of average improvement when starting from Qwen2.5-7B-Ins and the Qwen2.5-Coder-7B-Ins respectively, making the latter even more powerful than GPT-4-Turbo on benchmarks like MBPP. Inspired by the recent DeepSeek-R1~\citep{DeepseekR1}, we also perform RL training directly from the Qwen2.5-Coder-7B-base model and saw a surprising 25\% improvement on HumanEval-plus and 6\% improvement on MBPP-plus~\citep{evalplus} with merely 80 optimization steps (48 H100 GPU hours). These improvements are also generalizable to other more difficult benchmarks. 

% further boost Qwen2.5-Coder-7B-Inst's performance on MBPP, LiveCodeBench (V4), and BigCodeBench-Instruct (hard) by 2.38\%, 4.10\%, and 2.70\% respectively. What's more, by using \RM-7B as reward models for training Qwen2.5-7B-Inst with reinforcement learning, the performance on HumanEval+, MBPP+ are boosted by 4.27\% and 3.44\% respectively, which eventually resulted in our state-of-the-art coder model, \coder{}. 

% To our knowledge, this is the first work to propose a fully automated pipeline for synthesizing large-scale reliable tests used for the reward model training and reinforcement learning in the coding scenario. 
To our knowledge, this is the first work to perform reward model training and reinforcement learning for code generation using a fully automated pipeline that synthesizes large-scale reliable tests.
We believe our \dataset{} will unlock the potential of RL training for code generation models and help the community to further push the boundaries of LLM's coding abilities.