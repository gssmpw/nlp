\section{\dataset{}}
\label{subsec:dataset}
To be able to train a reward model specifically designed for code generation, the first thing is to synthesize reliable test cases for each coding problem and use them as training signals. In this section, we explain the whole procedure of constructing \dataset{} step by step. We show the overall statistics in~\autoref{tab:dataset}. 
% We obtained the initial entries from the three datasets with a total of 124K entries for synthesis. After consistency filtering, we lower down the 46K entries. From these entries, we curate over 300K code pairs. 

\paragraph{Test Case Synthesis from Seed Dataset}
\label{subsecp:test_case_synthesis}
We start from existing coding datasets with provided question $\mathbf{x}$ and corresponding program $\mathbf{y}$. Specifically, we combine Magicoder-Evol-Instruct\footnote{\href{https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K}{ise-uiuc/Magicoder-Evol-Instruct-110K}}, Magicoder-OSS-Instruct-75K\footnote{\href{https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K}{ise-uiuc/Magicoder-OSS-Instruct-75K}}, and StackPyFunction\footnote{\href{https://huggingface.co/datasets/bigcode/stack-dedup-python-fns}{bigcode/stack-dedup-python-fns}} as our seed dataset. We only keep the questions written in Python that contain either a function or a class, resulting in a total of 124K entries. We find that these datasets contain highly noisy questions that could not be easily evaluated using test cases. Therefore, we feed every question-solution pair ($\mathbf{x}$, $\mathbf{y}$) into a GPT-4o-mini~\citep{gpt4o} to propose a refined LeetCode-style question $\mathbf{x_r}$ with highly structured instructions. Meanwhile, we also prompt it to `imagine' around 20 test cases $(t_1, ..., t_m)$ for each refined coding question $\mathbf{x_r}$ based on its understanding of the expected behavior of the desired program. See prompt template used in~\autoref{appendix:seed_code_prompt}. Please note that we do not use the program solution $\mathbf{y}$ from the existing datasets at all in our final curated \dataset{}. These datasets are purely used as seeds to help LLM formulate well-structured coding problems.

\paragraph{Test Case Filtering}
\label{subsecp:test_case_consistency}
These `imagined' test cases generated from the LLM contain severe hallucinations. To filter out those hallucinated test cases, we facilitated a stronger coder model Qwen2.5-Coder-32B-Instruct~\citep{Hui2024Qwen25CoderTR} as a proxy to perform quality control. Specifically, we prompt it for each $\mathbf{x_r}$ to generate a program $\mathbf{y}^{\prime}$ and then run these programs over the test cases to approximate their quality. We removed all test cases $t_i$ where the generated solution program $\mathbf{y}^{\prime}$ could not pass. Furthermore, we removed questions with fewer than 5 tests after filtering, as these questions might be overly ambiguous. With the above filtering, we constructed the \dataset{} with 87.1K distinct coding questions and 1.38M cleaned test cases, as represented by $(\mathbf{x_r}, (t_1,...,t_{m_c}))$, where $m_c$ represents the number of test cases after filtering.

\input{tables/rm_dataset_info}

% \begin{figure}[h]
%     \centering
%     \includegraphics[scale=0.75]{figures/pair_creation.png}
%     \label{fig:enter-label}
%     \caption{\textcolor{red}{Please refine this figure. This looks very ugly. Fonts are really large.}}
% \end{figure}

\paragraph{Preference Pairs Construction}
\label{subsecp:pref_pair_construction}
We propose to use the Bradley-Terry model to train the reward model as defined in~\autoref{eq:rm_loss}. Therefore, we need to construct (question, [positive program, negative program]) data from \dataset{}. Specifically, we sample programs $(\mathbf{y}^1,...,\mathbf{y}^n)$ from existing models (e.g. Llama-3.1~\citep{grattafiori2024llama3herdmodels}) w.r.t $\mathbf{x}_r$ and utilize the test-case pass rate to distinguish positive and negative programs. Since the pass rate $s_i$ for the sampled program $\mathbf{y}^i$ can be any number between $[0,1]$, a minor difference in pass rate may not represent that one program is more accurate than another. Therefore, instead of using $\mathbbm{1}[s_i>s_j]$ to select the preference pairs, we have thus modified the selection rules to be:
\begin{equation}
\label{eq:pref_pair_sel}
    \mathbbm{1}[s_i>s_j+0.4,s_i>0.8,s_j>0]
\end{equation}
This is to ensure the preferred program has at least a $0.8$ pass rate to make sure it represents a more correct program. Also, we find many sampled programs with 0 pass rates can be caused by some small syntax errors or some Python packaging missing errors during evaluation, we chose to not include them as the preference pair to make sure our constructed datasets represent only the preference-based on the valid pass rate. We also ensure the sampled programs all come from the backbone of $R_\phi$ so the reward model is trained in an on-policy way. After that, we train our reward model $R_\phi$ by fully fine-tuning an instruct coding model. Specifically, We extract the last token's final hidden representations and pass it through a linear model head that generates a single scalar output, which is optimized via the loss function defined in \autoref{eq:rm_loss}.