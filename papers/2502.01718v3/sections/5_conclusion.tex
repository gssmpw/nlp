\section{Conclusion}
We are first work to automate large-scale test-case synthesis and adopt them to train coder language models. Without relying on the most advanced model, our data collection pipeline can still produce very high-quality verifiable code data, which empowers the training of reward model and coder model through reinforcement learning. Though our work demonstrates huge improvement in Best-of-N experiments, the improvement on RL training is less prominent. We believe future work should further our reward model training to improve its robustness to further the results. 