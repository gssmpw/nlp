% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)

\usepackage{booktabs}
\usepackage{graphicx} 
\usepackage{graphics}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{pifont}
\usepackage{makecell}
% \usepackage{nidanfloat}
\usepackage{resizegather}
% \usepackage{fixltx2e}
\usepackage{etoolbox}
\usepackage{booktabs,makecell,tabularx} 
% \usepackage{resizegather}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{multirow}
% \usepackage{hhline}
% \usepackage[export]{adjustbox}
\usepackage{booktabs,array}
% \usepackage[ruled,noend]{algorithm2e}
% \usepackage{bm}
\usepackage{color, colortbl}
\usepackage{xcolor}
\usepackage{url}
\usepackage{hyperref}
% \definecolor{Gray}{gray}{0.9}
% \usepackage{arydshln} 
% \usepackage{resizegather}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xspace}
% \usepackage{fdsymbol}

\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,    % typewriter-style font
  columns=fullflexible,    % allow full width usage
  breaklines=true,         % automatic line breaking
  breakatwhitespace=false, % break at any character (not just spaces)
  showstringspaces=false,  % donâ€™t visually mark spaces in strings
  language={},             % no specific language (plain text)
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\dongfu}[1]{\textcolor{blue}{[DongfuJiang: #1]}}
\newcommand{\huaye}[1]{\textcolor{purple}{[HuayeZeng: #1]}}
\newcommand{\haozhe}[1]{\textcolor{orange}{[HaozheWang: #1]}}
\newcommand{\wenhu}[1]{\textcolor{red}{[WenhuChen: #1]}}
\newcommand{\RM}{\textsc{AceCode-RM}\xspace}
\newcommand{\coder}{\textsc{AceCoder}\xspace}
\newcommand{\dataset}{\textsc{AceCode-87K}\xspace}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{AceCoder: Pushing the frontier of Code Language Models Through Reward Model and Reinforcement Learning}
\title{\coder{}: Acing Coder RL via Automated Test-Case Synthesis}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Huaye Zeng$^\dagger$\thanks{Equal Contribution}~, Dongfu Jiang$^\dagger$\samethanks~, Haozhe Wang$^\ddagger$, Ping Nie$^\mathsection$, Xiaotong Chen$^\clubsuit$, Wenhu Chen$^\dagger$\\
    $^\dagger$University of Waterloo, $^\ddagger$HKUST, $^\mathsection$Independent Researcher, $^\clubsuit$Netmind.AI\\
    %\texttt{email@domain, ping.nie@pku.edu.cn, xiaotong.chen@netmind.ai}
    \texttt{\{w33zeng,dongfu.jiang,wenhuchen\}@uwaterloo.ca} \\
    \\
    \url{https://tiger-ai-lab.github.io/AceCoder}
    }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}    
%Recent, code models have demonstrated strong performance in tasks such as code generation and bug fixing. 
Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models. 
\end{abstract}

\input{sections/1_intro}

\input{sections/3_methods}

\input{sections/3_1_dataset}

\input{sections/4_experiments}

\input{sections/2_related}

\input{sections/5_conclusion}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

\appendix
\input{sections/appendix}

\end{document}
