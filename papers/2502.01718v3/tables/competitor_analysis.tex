\begin{table*}[!t]
\centering
\caption{\RM's performance against other open-sourced reward models in terms of Best-of-16 sampling for Llama-3.1-8B-Inst. We can see the top-ranked RM on Reward Bench get little improvements compared to ours.}
\label{tab:comparison_with_other_rm}
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccc}
\toprule
Method \& RM & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c}{MBPP} & \multicolumn{2}{c}{BigCodeBench-C} & \multicolumn{2}{c}{BigCodeBench-I} & LiveCodeBench & Average \\
 & - & Plus & - & Plus & Full & Hard & Full & Hard & V4 & \\
 \midrule
Greedy & 68.9 & 62.2 & 67.2 & 54.8 & 38.5 & 12.8 & 31.8 & \textbf{13.5} & 18.0 & 40.9 \\
Average & 50.1 & 42.2 & 57.9 & 47.2 & 22.0 & 10.6 & 18.2 & 12.0 & 14.9 & 30.6 \\
\midrule
InternLM2-RM-8B & 57.9 & 55.5 & 66.7 & 54.0 & 38.7 & 8.8 & 29.8 & 8.8 & 15.1 & 37.3 \\
Skywork-Gemma-27B & 73.8 & 67.1 & 64.3 & 53.4 & 40.1 & 14.9 & 32.5 & 12.8 & 23.6 & 42.5 \\
Skywork-Llama-3.1-8B & 67.7 & 61.6 & 69.6 & 56.9 & 40.6 & 10.8 & 31.8 & 12.2 & 18.8 & 41.1 \\
\rowcolor{LightCyan}
$\Delta$ (max(other RM)-greedy) & +4.9 & +4.9 & +2.4 & +2.1 & +2.1 & +2.0 & +0.6 & -0.7 & +5.6 & +2.6 \\
\midrule
\RM-7B & \textbf{77.4} & \textbf{70.7} & \textbf{76.5} & \textbf{64.3} & \textbf{45.8} & \textbf{20.3} & \textbf{36.4} & 12.2 & \textbf{26.1} & \textbf{47.7} \\
\rowcolor{LightCyan}
$\Delta$ (RM-greedy) & +8.5 & +8.5 & +9.3 & +9.5 & +7.3 & +7.4 & +4.6 & -1.4 & +8.1 & +6.8 \\
\bottomrule
\end{tabular}
}
\end{table*}