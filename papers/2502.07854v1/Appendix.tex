\section{Appendix}

\subsection{Base Architecture}
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\linewidth]{images/Base.pdf}
\caption{Wavelet scalogram-based architecture used for heat demand forecast as adopted from \cite{chatterjee2022heat}.}
\label{Fig6_1:originialarch}
\end{figure}

\subsection{Experimental Parameters} \label{exps}
Extension to the experimental setup discussed in Section \ref{exp}.
\subsubsection{Data Preprocessing}
The demand data for various DMAs is obtained by aggregating individual smart meter readings within each DMA and is differenced to achieve stationarity. The data undergoes preprocessing to remove statistical outliers, as well as to address missing values or instances of negative consumption. The dataset is segmented into $h = 24$ hour intervals, spanning from midnight to midnight. Relevant features are identified through correlational analysis and seasonal decomposition. Due to the observed daily and weekly consumption patterns, two lagged consumption features—24 hours (previous day) and 168 hours (same day in the previous week)—are used as demand features. For weather features, the hourly maximum temperature and feels-like temperature for the forecast day are selected. Time-based features, such as the hour of the day and day of the week, are cyclically encoded using sine and cosine functions.

\subsubsection{Training parameters}
All models are trained using the ADAM optimizer with \acrshort{mse} as the loss function to convergence, with an early stopping criterion to prevent overfitting. Hyperparameters are further fine-tuned using grid search. A batch size of $256$ and a learning rate of $0.01$ are applied consistently across all models during training. For the model $F^{'}$ visualised in Figure \ref{Fig1:block_diagram} the positional encoding of tokens is managed by a learnable layer, chosen based on prior experiments comparing it with sinusoidal encoding.

\subsection{Model Parameters} \label{mp}
The number of trainable parameters in the LSTM model, the baseline wavelet model $F$, and the proposed model $F^{'}$ is given below:

\begin{itemize}
    \item LSTM: 32,536
    \item  Scalogram - Base model $F$: 155,339,512 
    \item  Wavelet Scalogram - with Cross Attention $F^{'}$:  5,789,152 

\end{itemize}


