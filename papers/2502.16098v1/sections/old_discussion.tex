\subsection{Task Offloading to \bma{} Through Distributed Cognition}


% [visual cognition, compare with bme, seeing ai, ocr, facebook ai, bring spatial awareness etc.]

% [human's own cognition process]

% [bme bringing another human's cognition, compare with bme], switch back and forth, hybrid, human+ai work together, two heads

% {\color{blue}
% [white cane vs bma]: real time vs pause, a tech problem, cannot replace white cane as o\&m skills
% }

% [hand off, when AI/human bring their knowledge, 
% fine tune the environment for individuals, how AI can analyze the environment better to understand the users' goal (360 panoramic view)]

% [understand the context, error state, e.g. remove eggshell rather than identify it; not just CV in seeing what is in the scene, but what to do]


%%%%%%%%%%%
% [bma's visual cognition]
% [human's own cognition process]
% bma is transition, human make it cog extension with adaptation, indication and opportunity that bma is a good start (thermostat, different prompts)
Our findings reveal that participants and \bma{} collaboratively work as interconnected components within the cognitive system, reflecting the principles of distributed cognition. Participants offload the burden of visual perception to \bma, enabling them to more effectively navigate and interact with their environment. This tool acts as a prosthetic for visual processing, transforming complex visual information into comprehensible audible text, which PVI can easily process. 


Unlike traditional prosthetics like the white cane, which extend PVI’s tactile senses, \bma{} is an active cognitive extension. It interprets visual data in real time, providing dynamic and creative assistance in various contexts. 
For example, it can read allergy-free labels on food packaging (Section~\ref{cooking}), identify colors and patterns for outfit matching (Section~\ref{fashion}), and describe scenes to enhance spatial awareness (Section~\ref{scene}). 
With its capability to interpret visual content in a detailed and accessible manner, \bma{} effectively simulates visual cognition for PVI.





% [individual adaptation:] how to accommodate challenges and limitations, tradeoff
% gap between what pvi expect and what they get 
Despite its advantages, \bma{} also presents several limitations and challenges.
To overcome these obstacles, participants have developed adaptive strategies.
They utilize their memory, inferential reasoning, and experiential knowledge alongside \bma{} to create a more robust cognitive extension within the distributed cognition system. 
For instance, when \bma{} recognized a thermostat but failed to give detailed information about how to adjust a thermostat, participants posed targeted questions to get more specific information or called for human assistance (Section~\ref{cooking}). 
In cases of AI hallucinations, participants rely on their spatial knowledge or seek human confirmation (Section~\ref{scene}). These adaptations illustrate the dynamic nature in distributed cognition, where participants actively reshape their interactions with technology to enhance cognitive support.
% 
% These instances indicate that opportunity that \bma is not perfect but a good start [related to distributed cognition]


% Moreover, participants creatively adapt \bma's visual interpretation capabilities to their intent in more creative and personalized contexts. 
Furthermore, participants creatively adapt \bma's visual interpretation capabilities to their personal needs and contexts. For instance, they combine \bma's outputs with their O\&M skills to enhance navigation (Section~\ref{navigation}) and supplement \bma's visual cues with their auditory, tactile, and olfactory senses to ensure safety when interacting with horses (Section~\ref{animal}). 
Participants also adjust their use of \bma{} when assessing fashion choices (Section~\ref{fashion}) or interpreting a pet’s expressions (Section~\ref{animal}).

These examples demonstrate how users actively shape their cognitive support systems, reinforcing the idea that distributed cognition is a dynamic process. By harnessing their own cognitive strengths and adjusting their interaction patterns with \bma, participants create a more flexible and robust approach to accessing visual information. 



% bma is transition, human make it cog extension with adaptation, indication and opportunity that bma is a good start (thermostat, different prompts)


% [artifact] address the need for visual input, how people can simulate bma's capability to visual cognition, simulation of bma in dcog system

% bma simulates pvi's visual cognition in dcog system



%%%%%%%% compared with white cane 
% Compared with white cane, 
% prosthetic are taking the visual load, managing it in alternative way, new prosthetic do it in another way different from white cane, bma is more dynamic, expensive, creative


% cane: extension of hand, limited, bma can extend it to far-away objects, both prosthetic

% o\&m skills in navigation, tactile information

%%%%%%%%%%%%%%%%%%%
% compare with ai-powered vqa
% seeing ai, ocr-based, facebook ai, bring spatial awareness etc, cannot provide conversations
% bma is external memory


\paragraph{Comparison with Non-LMM AI-Powered Systems}


Compared to non-LMM AI-powered systems like Seeing AI~\cite{SeeingAI2020} and Facebook AI, participants noted that \bma{} can process both graphic elements and text, offering more comprehensive and vivid interpretations of visual content (Section~\ref{cooking}). This capability enhances the distributed cognition system by incorporating a form of visual cognition that is superior to conventional systems.


Additionally, \bma{} facilitates interactive conversations with its users, a functionality absent in OCR-based visual interpretation systems. This conversational ability introduces an interactive cognitive process, where \bma{} not only decodes visual information but also engages in a dialogic interaction. This interaction enhances cognitive engagement, allowing users to clarify, question, and explore visual content in a more dynamic manner. 





% compare with human-assisted vqa
% [bme bringing another human's cognition, compare with bme], switch back and forth, hybrid, human+ai work together, two heads, helpful to get from diverse thoughts
% bma is NOT perfect but can provide some subjective info 

% aira - domain expert objective, bma (domain expert -- visual graphics) is playing more roles rather than giving visual description (fashion) 
% human is varied, not consistent, simulate human's conversation 

\paragraph{Comparison with Human-Assisted VQA Systems}
In human-assisted VQA systems, another human’s cognition is involved to support PVI. In these systems, participants offload visual processing and some aspects of reasoning to human assistants. 
For instance, human assistants can identify the colors of outfits; however, their capability may be limited by factors such as colorblindness (Section~\ref{fashion}) or they may get overwhelmed by an abundance of visual elements and overlook critical details (Sections~\ref{scene} and \ref{physical}). Moreover, services like Aira instruct their human assistants to provide only objective information~\cite{lee2020emerging,xie2023two}, which may limit the depth of cognitive support provided. 

In contrast, \bma{} offers consistent visual interpretations and goes beyond mere description by incorporating subjective judgments, like fashion suggestions. 
This capability allows \bma{} to not only support basic visual tasks but also integrate contextual understanding into its interactions. 
%
By doing so, \bma{} functions as an extension of the PVI's cognitive processes, adapting to various contexts and individual needs in ways that human assistants may not. Such a system enriches the distributed cognition framework by providing a more adaptive cognitive agent. This adaptability enables PVI to engage with their environments more seamlessly. 



% summary, bma as domain exeprt, subjective...



%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%resonate with goal

%break-down

% https://dl.acm.org/doi/abs/10.1145/3563657.3595977

% Our findings highlight the role of distributed cognition in the interaction between PVI and AI-powered assistive technologies like \bma{}. The distributed cognition framework, which views cognitive processes as interactions among individuals, artifacts, and socio-cultural elements \cite{hutchins1995cognition, hollan2000distributed}, provides a valuable lens for understanding how PVI adapt to and integrate these technologies into their daily lives.

% \subsubsection{Enhancing Spatial Awareness and Social Navigation}
% One of the key findings of our study is that AI-powered assistive technologies like \bma{} can significantly enhance PVI's spatial awareness and ability to navigate complex social situations. By providing detailed scene descriptions, these technologies often reveal spatial details that are not initially apparent to PVI, allowing them to gain a better understanding of their surroundings and social contexts.

% PVI can leverage this enhanced spatial awareness to assess and align with social norms, such as determining the appropriate attire for a given event or ensuring that their appearance matches that of their peers. This aligns with the distributed cognition framework's emphasis on the coordination between internal and external representations \cite{rogers2022hci, hollan2000distributed} and highlights the importance of designing assistive technologies that support users' sense-making processes in social situations.

% \subsubsection{Offloading Cognitive Tasks and Enhancing Engagement}
% Our findings also demonstrate how AI-powered assistive technologies can act as cognitive extensions for PVI, enabling them to offload cognitive tasks and enhance their engagement in various activities. In parenting roles, for example, PVI can use these technologies to describe and verify the appearance of their children, focusing on attributes like hair texture, skin complexion, and clothing styles and colors. This allows them to make informed decisions about their children's appearance and engage more meaningfully in family activities, such as reading books or participating in puzzles.

% By offloading the cognitive load associated with visual distinction, AI-powered assistive technologies enable PVI to participate more fully in family activities and social interactions. However, our findings also highlight the limitations of these technologies in providing goal-oriented information, underscoring the need for designers to consider the appropriate level of detail and context-awareness when developing assistive technologies for PVI.

% \subsubsection{Transforming Social Interactions}
% The detailed visual interpretation capabilities of AI-powered assistive technologies can also enable PVI to engage in more meaningful and authentic social interactions. By providing access to visual details that might otherwise be overlooked or taken for granted by sighted individuals, these technologies allow PVI to personalize their interactions and contribute more fully to social activities.

% For example, PVI can use these technologies to ensure that gifts are appropriately prepared and presented, or to read and respond to personal messages in a more genuine and emotionally resonant manner. Moreover, AI-powered assistive technologies can facilitate decision-making and autonomy within physical social engagement, allowing PVI to make informed choices based on the same visual information available to sighted individuals.

% These findings demonstrate how AI-powered assistive technologies can enhance PVI's social interactions by providing detailed visual interpretations and enabling them to perform tasks independently, fostering more genuine and joyful connections with others. This aligns with previous research that has explored how people utilize external cognitive resources to tackle complex challenges and facilitate communication and coordination within a distributed cognitive framework \cite{halverson2002activity, baumer2011comparing, wu2008collaborating, rogers2006distributed, perry2003distributed}.

% \subsubsection{Balancing AI Interpretations with Subjective Inputs}

% While AI-powered assistive technologies provide valuable information in various contexts, our findings show that PVI often adapt the outputs of these technologies to align with their own perceptions and combine them with other sensory information to ensure accuracy and safety.

% PVI may emphasize personal judgment over the subjective interpretations provided by these technologies, preferring the flexibility to override assumptions or integrate AI-based interpretations with their own sensory inputs. This strategy allows PVI to maintain a sense of control and autonomy in their interactions with the world, while still benefiting from the insights provided by AI-powered assistive technologies.

% These findings highlight the importance of designing AI-powered assistive technologies that allow for user customization and adaptability, enabling PVI to tailor the distribution of cognitive tasks to their individual requirements and abilities, and to balance AI interpretations with their subjective inputs.






% \subsection{Exploring Motivations for \bma{} Usage through Self-Determination Theory}
% % SDT: why participants use bma, why does it matter for the theory, SDT looks into pvi's use of assistive technology (fulfill user's needs) 

% % competence > tools + skills > BeMyAI is a great tool; it increases competence, which boost their confidence to be independent. Independence/agency/dignity is the primary reason blind people use assistive technology

% % sooyeon's independence paper

% Based on our findings, we identified several aspects that reflect the underlying motivations for using \bma{} and align with the principles of SDT. These aspects illustrate how \bma{} meets participants' psychological needs for autonomy, competence, and relatedness, which are crucial for fostering motivation and psychological well-being.


% % autonomy
% \textit{As for autonomy,} we found that \bma{} does not override the users' ability to apply their own judgment, instead granting them control over personal choices. Rather than making decisions for them, \bma{} provides users with the information needed to make informed choices.
% For example, participants preferred to rely on their own judgment over AI's suggestions for outfit matching (Section~\ref{fashion}). Similarly, P5 valued \bma’s ability to interpret her pet's status but chose to adapt the AI-generated information to fit her perceptions (Section~\ref{animal}).
% These instances emphasize that while \bma{} enhances autonomy by providing visual interpretations, it also supports users' autonomy to retain control over their personal decision-making.


% Furthermore, \bma{} boosts the autonomy of its users by addressing navigational needs. The tool proves effective in aiding localization and orientation, essential for PVI's independence. Participants demonstrated how \bma{} assists in navigation tasks by reading signages and identifying surroundings (Section~\ref{navigation}). 
% % 
% This capability enables users to navigate more independently without relying on constant human assistance. As a result, they gain more confidence and control over their environment, emphasizing the tool's role in enhancing autonomy.


% % competence
% \textit{As for competence,} \bma{} increases PVI's competence by providing them access to complex visual content that would otherwise be inaccessible. This tool interprets and transforms intricate visual data into accessible, audible text, allowing users to engage in various contexts of their daily lives. 
% %
% \bma{} has helped participants understand detailed graphics on food packaging, enabling safer dietary choices (Section~\ref{cooking}). 
% % 
% It has also provided descriptions of objects and bystanders, enriching environmental interactions and spatial awareness (Section~\ref{scene}). 
% % 
% Additionally, \bma{} has conveyed visual and textual details on Christmas cards, facilitating more genuine social exchanges (Section~\ref{physical}). 
% % 
% By converting visual information into formats that are usable for PVI, \bma{} supports and improves their ability to perform tasks competently. 


% % relatedness
% \textit{As for relatedness,} \bma{} enhances relatedness for PVI by enabling richer social interactions in digital, physical, familial, and human-animal contexts. In the digital realm, \bma{} helps users describe images, check image quality, and generate alt text, making them both recipients and active contributors of visual content on social media (Section~\ref{digital}). 
% % This functionality boosts their confidence and elevates their social media engagement.
% % 
% \bma{} facilitates communication between blind and sighted individuals by providing accurate visual descriptions, reducing the need for sighted people to perfectly describe images (Section~\ref{digital}). 
% % 
% In familial settings, it supports parenting roles (e.g., dressing their children) and family activities (e.g., puzzle assembling) (Section~\ref{physical}). 
% % 
% Additionally, \bma{} enhances interactions with animals, allowing for a deeper understanding and safer engagement by identifying their positions and behaviors accurately (Section~\ref{animal}). 
% % 
% In each of these scenarios, \bma{} not only supports but also amplifies the social, familial, and human-animal connectivity of users, demonstrating its profound impact on fostering relatedness within and beyond the visually impaired community.

% \vspace{3mm}

% % conclusion


% In this study, we have demonstrated how \bma{} aligns with the principles of SDT by enhancing autonomy, competence, and relatedness among users with visual impairments. Traditionally, SDT has been applied within the domains of gaming -- focusing on fun~\cite{deterding2016contextual,johnson2015all,ryan2006motivational,aufheimer2023examination}, and health -- emphasizing wellness~\cite{eilert2020osteoarthritis,guldenpfennig2019autonomy,muriana2021affecting}, in HCI research. We extend SDT by applying it to under-investigated areas, specifically safety and engagement within accessibility.

% \bma{} contributes to user safety by providing environmental descriptions that inform navigational risks like objects and bystanders (Section~\ref{scene}), increasing users' self-reliance and ability to navigate securely. In human-animal interactions, \bma{} offers critical visual interpretations like positions and directions of horses, which helps ascertain safety-related aspects and avoid dangerous situations (Section~\ref{animal}).


% Regarding engagement, \bma{} enriches participant interactions across digital, physical, and familial contexts (Section~\ref{digital} and~\ref{physical}). It bolsters social engagement by facilitating accurate communication of visual content, thereby making digital platforms more accessible and enjoyable. In familial settings, it supports essential activities, enhancing the quality of interactions and shared experiences. 

% These enhancements across the domains of safety and engagement not only fulfill the immediate functional needs of users but also profoundly contribute to their psychological needs for competence, autonomy, and relatedness as outlined by SDT. By addressing these crucial areas, this study broadens the scope of SDT, showing its relevance and utility in improving accessibility in technology use.