\section{Introduction}

% People with visual impairments (PVI) rely on both AI and human assistance to understand their visual surroundings. 
People with visual impairments (PVI) face challenges in perceiving their surroundings due to the absence of visual cues. \rev{Traditional} AI-powered \rev{systems like} Seeing AI~\cite{SeeingAI2020} \rev{and} human-assisted \rev{services like} remote sighted assistance~\cite{BeMyEyes2020} \rev{have helped PVI interpret their environment and complete} daily tasks. \rev{Recent advances} in computer vision (CV) and natural language processing (NLP) \rev{have enabled AI systems to identify} objects and text \rev{in scenes while responding to photo-based queries from PVI~\cite{ahmetovic2020recog, mukhiddinov2022automatic, hong2022blind, morrison2023understanding, gonzalez2024investigating}.
%
The emergence of} large multimodal models (LMMs)~\cite{yu2023mm}, \rev{particularly} GPT-4~\cite{achiam2023gpt}, \rev{has transformed} visual question-answering (VQA) \rev{capabilities.} Researchers have begun \rev{exploring how these powerful LMMs might benefit} PVI~\cite{zhao2024vialm, yang2024viassist}. \rev{At the forefront of this development is} \textit{\bma{}} (\sbma{})~\cite{bma_usecase}, the first \rev{publicly-available} LMM-based system \rev{designed specifically to help PVI with} visual interpretation and question answering\rev{. Built on} OpenAI's GPT-4 models~\cite{achiam2023gpt}, \sbma{} offers \rev{capabilities that surpass those of} similar applications.

Prior work~\cite{adnin2024look} has investigated \rev{how PVI use and access generative AI tools,} focusing on information access, acquisition, and content creation through platforms like \bma. Our work \rev{broadens this investigation beyond} content-oriented interactions to \rev{examine} real-life scenarios \rev{involving} visual descriptions, task performance, social interactions, and navigation. By \rev{analyzing} the capabilities and limitations of LMM-based VQA systems, particularly \bma{}, we identify gaps between \rev{current} technological capabilities and \rev{PVI's} practical needs and expectations. 
This study offers timely insights into rapidly evolving LMMs \rev{while seeking broader understanding that will remain valuable as foundational knowledge even as specific technologies advance.} 

\rev{
More specifically, we explore} the following \textit{research questions}:
\begin{itemize}
    \item \textit{What are the capabilities and limitations of LMM-based assistance in the daily lives of people with visual impairments?}
    % 
    \item \textit{How do people with visual impairments mitigate these limitations?}
\end{itemize}


To answer these questions, we conducted an exploratory study \rev{using two complementary data sources. First, we interviewed} 14 visually impaired users \rev{about their experiences with \sbma. This was our primary data source. Second, we analyzed image descriptions generated by this tool that were shared by both our interview participants and users on} social media platforms (X, Facebook, Reddit, and blogposts). This \rev{secondary data source allowed us to understand PVI's lived experiences with this AI tool while capturing concrete examples of their interactions in various real-life scenarios.} 


\rev{Our study revealed} that \sbma's context-aware capabilities \rev{help participants better understand their} spatial \rev{surroundings, support} social \rev{interactions, interpret} stylistic \rev{elements, and convey} human identities. \rev{Yet} several limitations \rev{undermine} these benefits\rev{: the AI sometimes hallucinates} non-existent details, \rev{makes} subjective interpretations \rev{about} human-animal interactions and fashion \rev{choices, and misidentifies} people's age or gender. \rev{When encountering these limitations, participants draw on their spatial memory and auditory cues, apply personal judgment, or turn to human assistants.} Moreover, \rev{\sbma{} struggles to grasp users' intentions, provide} actionable support\rev{, or offer} real-time feedback. Participants compensate for this by actively guiding it through prompting, seeking human \rev{assistance}, or depending on their orientation and mobility skills.


Informed by these findings, we discuss strategies to improve handoff between PVI, the AI tool, and remote sighted assistants. We propose streamlined interactions to reduce redundancy and envision new paradigms to achieve more accurate identity recognition, improve subjective interpretations, and mitigate AI hallucinations. 
% 
We also discuss the benefits of multi-agent systems, including both human-human and human-AI collaborations, and explore future possibilities of AI-AI cooperation to aid PVI in tasks requiring specialized knowledge. 


\rev{
Our research examines how state-of-the-art AI, particularly} LMM-based systems like \sbma, \rev{creates new opportunities for PVI through advanced human-like language and vision capabilities. Rather than contrasting this tool with other VQA systems, we characterized this as a new genre of AI-driven prosthetic and identified how its early-stage usage could better align with established assistive approaches such as remote sighted assistance~\cite{lee2020emerging,kamikubo2020support,granquist2021evaluation}. 
Through detailed analysis of an AI tool's capabilities and limitations, based on rich narratives from PVI about their first-hand experiences, we provide insights into how LMM-based systems are reshaping accessibility tools. These insights help us understand how to enhance both the context awareness and intent-oriented capabilities of LMMs, laying the groundwork for future advances in intelligent, interactive, and personalized assistive technology that better serve PVI's needs.
}




