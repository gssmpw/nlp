\section{Background and Related Work}


% intro to DCog: collaborative, human-powered prosthetic are DCog (RSA)
% adaptive artifact that is human-like, intelligent, is also DCog (bma)

In this section, we review the literature on AI-powered and human-assisted visual interpretation and question answering systems, as well as information needed in visual interpretations for PVI.


\subsection{AI-Powered Visual Interpretation and Question Answering Systems}


The advancements in deep learning models for computer vision and natural language processing have significantly enhanced the capabilities of AI-powered visual assistive systems\rev{~\cite{ahmetovic2020recog, mukhiddinov2022automatic}}. These systems leverage photos taken by PVI to identify objects or text within the scene, as well as respond to queries about the contents of the images\rev{~\cite{hong2022blind, morrison2023understanding}. Such applications are now widely adopted, exemplified by Microsoft's Seeing AI~\cite{SeeingAI2020}, providing support to PVI in various scenarios~\cite{gonzalez2024investigating}.}

% \sout{The advancements in deep learning models for CV and NLP technologies have significantly enhanced the capabilities of AI-powered visual assistive systems. These systems leverage photos taken by PVI to identify objects or text within the scene, as well as respond to queries about the contents of the images. For instance, Ahmetovic et al.~\cite{ahmetovic2020recog} developed a mobile app utilizing deep network models to guide PVI in capturing images and recognizing objects. Mukhiddinov et al.~\cite{mukhiddinov2022automatic} devised a fire detection and notification app for PVI using convolutional neural networks. Hong et al.~\cite{hong2022blind} created an iOS app enabling PVI to gather training images for personalized object recognition. Morrison et al.~\cite{morrison2023understanding} designed an application to teach AI to identify individualized items, aiding PVI in locating personal belongings. Gonzalez et al.~\cite{gonzalez2024investigating} introduced a scene description app utilizing Microsoft's Azure AI Vision image description API. Additionally, PVI commonly employ commercial AI-powered applications like Microsoft's Seeing AI~\cite{SeeingAI2020} to recognize objects through device cameras.}

Recent advancements in LMMs, such as GPT-4~\cite{achiam2023gpt}, have demonstrated exceptional performance in multimodal tasks~\cite{yu2023mm}, \rev{prompting exploration into their potential for assisting PVI. State-of-the-art LMMs have been leveraged to create assistive systems capable of evaluating image quality, suggesting retakes, answering queries about captured images~\cite{zhao2024vialm, yang2024viassist}, and even integrate multiple functions to assist PVI in tasks such as navigation~\cite{zhang2025enhancing} and text input~\cite{10.1145/3613904.3642939}. In the commercial domain, Be My Eyes~\cite{BeMyEyes2020} introduced the \bma{} feature powered by GPT-4~\cite{achiam2023gpt}.}

% \sout{More recently, Large Multimodal Model (LMM)~\cite{yu2023mm}, represented by GPT-4~\cite{achiam2023gpt}, have showcased remarkable proficiency in multimodal tasks. In response, researchers have begun exploring the potential of LMM in assisting PVI. Zhao et al.~\cite{zhao2024vialm} investigated the feasibility of leveraging state-of-the-art LMM to aid PVI and established a corresponding benchmark. Yang et al.~\cite{yang2024viassist} utilized LMM to develop an assistive system for answering PVI's questions about captured images, including assessing picture quality and suggesting retakes. In the realm of commercial applications, Be My Eyes~\cite{BeMyEyes2020} and OpenAI collaborated to introduce the \bma{} feature~\cite{bma_usecase}, powered by GPT-4~\cite{achiam2023gpt}, aimed at replacing human volunteers.}

% \rev{While prior research has examined the use of AI-powered visual assistive systems like Seeing AI for tasks such as reading~\cite{granquist2021evaluation} and daily routine support~\cite{kupferstein2020understanding}, these studies predate the integration of advanced LMM technologies and thus may not fully reflect the capabilities or user experience of cutting-edge systems. Although some accounts, such as Bendelâ€™s documentation of experiences with GPT-4-based \bma{}~\cite{bendel2024can}, offer insights, they remain anecdotal and lack generalizability. This study aims to bridge this gap by conducting an in-depth investigation into the daily usage of \bma{} among 14 PVI users, offering a comprehensive understanding of its real-world applications and impact.}


Prior work has examined the use of AI-powered visual assistive systems by PVI in their daily lives\rev{~\cite{granquist2021evaluation,kupferstein2020understanding,gonzalez2024investigating}}. However, these studies did not incorporate the latest LMM technologies, thus may not fully represent the user experience of cutting-edge AI-powered systems. Bendel~\cite{bendel2024can} documented his experience with GPT\mbox{-}4\mbox{-}based \bma, yet his account remains subjective. Therefore, in-depth investigation into PVI's daily utilization of state-of-the-art AI-powered systems is imperative. 
This paper addresses this gap by exploring how 14 visually impaired users incorporate \bma, a state-of-the-art LMM-based VQA system, into their daily routines. 


% Prior works have examined the use of AI-powered visual assistive systems by PVI in their in their daily lives. For instance, Granquist et al.~\cite{granquist2021evaluation} examined the use of Seeing AI for reading tasks among PVI participants, while Kupferstein et al.~\cite{kupferstein2020understanding} conducted a diary study to investigate PVI users' incorporation of Seeing AI into their daily routines. Recently, Gonzalez et al.~\cite{gonzalez2024investigating} undertook a diary study to explore the use cases of AI-powered scene description applications. 
% However, these studies did not incorporate the latest LMM technologies, thus may not fully represent the user experience of cutting-edge AI-powered systems. Bendel~\cite{bendel2024can} documented his experience with GPT-4-based \bma{}, yet his account remains subjective. Therefore, extensive and comprehensive research into PVI's daily utilization of state-of-the-art AI-powered systems is imperative. This paper addresses this gap by investigating the daily usage of \bma{} among fourteen PVI users.


% adnin2024look
% \textcolor{red}{
% Based on prior work~\cite{adnin2024look}, this study goes beyond usability and accessibility issues to identify previously unexplored limitations of LMMs. We explore higher-level insights that inform potential future directions of LMMs even if the specific implementations evolve, and map these insights into broader Human-AI and AI-AI interactions. 
% }

% https://maitraye.github.io/files/papers/BLV_GenAI_ASSETS24.pdf





\subsection{Human-Assisted Visual Interpretation and Question Answering Systems}

Human-assisted VQA systems offer prosthetic support for PVI by facilitating connections to sighted people through remote assistance. These systems utilize image-based and video-based modalities to \rev{meet varied} needs and situations.


Image-based human-assisted VQA systems allow PVI to submit photos along with their queries and receive responses after some time. An example of this is Vizwiz, where PVI can upload images accompanied by audio-recorded questions and receive text-based answers through crowdsourced assistance~\cite{bigham2010vizwiz_nearly}. This method has been successfully applied in tasks such as reading text, identifying colors, locating objects, obtaining fashion advice, \rev{and supporting social interactions}~\cite{bigham2010vizwiz_nearly, bigham2010vizwiz, burton2012crowdsourcing,gurari2018vizwiz}. 
% Additionally, Vizwiz Social extends this functionality by linking PVI with their social networks, enabling support from friends and family members~\cite{gurari2018vizwiz}. 
However, the single-photo, single-query limitation of image-based VQA systems~\cite{bigham2010vizwiz_nearly} makes it less suitable for addressing complex or contextually deep inquiries~\cite{lasecki2013answering}.



Conversely, video-based human-assisted VQA systems facilitate real-time, interactive support, allowing PVI to receive immediate assistance tailored to their specific environmental context. This approach enables the visual interpretation of real-time scenes and supports a dynamic, back-and-forth VQA process, which is essential for addressing more specific and complex contextual inquiries effectively.
The evolution of this technology has progressed from wearable digital cameras ~\cite{hunaiti2006remote,garaj2003system,baranski2015field} and webcams~\cite{bujacz2008remote,scheggi2014remote,chaudary2017tele} to the utilization of mobile video applications~\cite{holmes2015iphone, BeMyEyes2020, Aira2020,xie2022dis,xie2024bubblecam} and real-time screen sharing technologies~\cite{lasecki2011real,lasecki2013answering,xie2023two}. 
% 
Services like Be My Eyes~\cite{BeMyEyes2020}, which connects PVI with untrained volunteers, and Aira~\cite{Aira2020}, which connects PVI with trained professional assistants, exemplify the application of video-based VQA in scenarios that require immediate feedback. These services prove effective in navigation~\cite{kamikubo2020support,xie2022dis,c4vtochi,iui,yu2024human}, 
shopping~\cite{c4vtochi,xie2023two,iui}, 
and social interaction~\cite{lee2020emerging,Caroll2020Human,lee2018conversations}. 
\rev{
In this study, we revealed specific situations where participants used human-assisted VQA systems to address limitations in \bma's assistance.
}





\subsection{Information Needed in Visual Interpretations for Visually Impaired Users}

% Either AI-powered or human-assisted visual interpretation faces the same challenge: determining what specific information is necessary for visual interpretation to meet the needs of PVI. Existing research primarily focuses on identifying what information PVI seek in descriptions of online images. For instance, the Web Content Accessibility Guidelines~\cite{caldwell2008web} offer advice on creating alternative text (alt text) for images. However, these guidelines tend to follow a one-size-fits-all approach, which can be difficult to apply effectively across different contexts. Similarly, AI-based image description tools (e.g., Seeing AI~\cite{SeeingAI2020}) also adhere to this one-size-fits-all design model.

% Previous studies~\cite{gurari2018vizwiz, gurari2020captioning} have adopted sighted volunteers to decide how images should be described for PVI. Recently, more research~\cite{stangl2020person,stangl2021going,bennett2021s} has shifted toward user-centered approaches, gathering input directly from PVI to understand their preferences for image descriptions. For example, Stangl et al.~\cite{stangl2020person} explored PVI users' preferences for descriptions across different online platforms (e.g., news sites, social media, eCommerce). Their findings indicated that the source of an image significantly impacts PVIâ€™s information needs, with common requirements across platforms (e.g., identifying a personâ€™s gender or naming objects), as well as source-specific details (e.g., a personâ€™s hair color on dating websites). In a follow-up study~\cite{stangl2021going}, they discovered that PVI's image description preferences are influenced not only by the image source but also by their information goals. Additionally, while subjective interpretation may be desired in certain cases, such content is not typically included in descriptions. From an ethical perspective, Bennett et al.~\cite{bennett2021s} examined screen reader users' preferences for describing personal and others' appearances, specifically addressing when and how aspects such as race, gender, and disability should be conveyed.

% Recent research also explores the varied needs for describing information from videos. For instance, Jiang et al.~\cite{jiang2024s} investigated PVI preferences for video descriptions across different types of online content (e.g., how-to videos, educational videos, music videos). Based on their proposed design framework, they offered several recommendations for improving video accessibility. For example, descriptions for entertainment videos, like music videos, should focus more on subjects (people and animals), while how-to videos should prioritize actions and tools over the subjects. Natalie et al.~\cite{natalie2024audio} conducted interviews with PVI to better understand their customization needs for video descriptions in various aspects such as length, emphasis, speed, and voice. 

% All of these studies focus on asynchronous visual interpretation systems. When it comes to synchronous systems, such as real-time video interpretation, Lee et al.~\cite{lee2020emerging} identified specific needs for scene descriptions in remote sighted assistance (RSA). The required information in RSA descriptions includes both objective details (e.g., text, spatial information, scenery) and subjective aspects (e.g., opinions on clothing). In this study, we will examine how Be My AI processes visual information and provide design insights based on the information needed by PVI.

\rev{Both} AI-powered \rev{and} human-assisted visual interpretation face the challenge \rev{of identifying what specific information is necessary to meet PVI needs}. Existing research primarily focuses on \rev{what information PVI seek in online image descriptions}. The Web Content Accessibility Guidelines~\cite{caldwell2008web} \rev{advise on creating alternative text (alt text), but their one-size-fits-all approach limits context-specific applicability}. Similarly, AI-based \rev{tools like Seeing AI~\cite{SeeingAI2020} adhere to this uniform design model.}

\rev{Earlier studies~\cite{gurari2018vizwiz, gurari2020captioning} relied on} sighted volunteers \rev{to decide image descriptions for PVI, while recent research~\cite{stangl2020person,stangl2021going,bennett2021s,chen2024role} emphasizes} user-centered approaches. For example, Stangl et al.\cite{stangl2020person} \rev{found PVI preferences for descriptions vary by platform} (e.g., news, social media, eCommerce)\rev{, with both common needs (e.g., identifying gender, naming objects) and platform-specific details (e.g.,} hair color on dating websites). \rev{Follow-up work~\cite{stangl2021going} showed preferences also depend on users' goals, with subjective details rarely included.} Bennett et al.~\cite{bennett2021s} examined screen reader users' \rev{views on describing} race, gender, and disability \rev{in appearance descriptions.}
\rev{For video content,} Jiang et al.~\cite{jiang2024s} \rev{examined PVI preferences across genres (e.g., how-to, music videos), recommending that entertainment descriptions focus on subjects, while how-to videos prioritize actions and tools.} Natalie et al.~\cite{natalie2024audio} \rev{studied PVI needs for customization in video descriptions, such as length, speed, and voice. For synchronous systems like real-time video interpretation,} Lee et al.~\cite{lee2020emerging} identified \rev{PVI needs} in remote sighted assistance\rev{, including} both objective details (e.g., text, spatial information\rev{) and subjective input} (e.g., opinions on clothing).
In this study, we will examine how the LMM-based system processes visual information and provide design insights based on the information needed by PVI.



\rev{
% image Interpretations: 
% [1] table 2 in https://dl.acm.org/doi/pdf/10.1145/3313831.3376404?casa_token=cZV0Jj4xOEQAAAAA:DUGrnBq-QrkQdqRKALU7MBuhin-liR-tyGsr0jEUvcznLAMBTFTTPbocpCy2Em-Vqz0rYs5sTLlo
% [2] https://dl.acm.org/doi/pdf/10.1145/3441852.3471233?casa_token=4gFpHDQlYvUAAAAA:hWjcbkbuhYmJ-3x6EaNoewOQbIXG93mzA7w1agDMzwKhimO_ycUgKxl-k5_2ntWpFLZ_71W3PgOf
% [3] https://dl.acm.org/doi/pdf/10.1145/3411764.3445498?casa_token=TB9lu5AmUfsAAAAA:Hx9BiXQde88FZn_oDlHEsPKTnXDae4z5FYINoFrKG43L7Qg1M3NYnXTizwQe8M5ade9Gf2Ipw_cm

% video Interpretations: 
% [1] sooyeon's paper (used in discussion 5.2)
% https://dl.acm.org/doi/pdf/10.1145/3313831.3376591?casa_token=ATq0EXcGE2oAAAAA:pgbAj9B_z6skIcxG3IaBpFYLBQBCLpq85ZnuXoP6jb2LMFNAgXVCn0W-VFU__84OYIkWF0vSZHg1

% incremental in belowing contexts
% 1. scene description: description (what is it)
% 2. navigation: description (what is it) + directional info (where is it, how to get to the destination)
% 3. task performance: description + directional info + domain knowledge (what to do, how to do it)
% 4. social engagement:  description + directional info to navigate in social space + discreet info (don't want others to know they are using rsa)


% [2] table 4 in https://dl.acm.org/doi/pdf/10.1145/3313831.3376823

}










