\section{Method: Data Collection and Analysis}


Our exploratory study used qualitative methods to examine how PVI use \bma, an emerging LMM technology, in their daily lives.
This section details our data collection and analysis processes.





\subsection{Data Collection}





\rev{
We gathered data from two sources: interviews as the primary data source and \sbma-generated image descriptions as the secondary data source. 
Interview data captured accounts of PVI's lived experiences with \bma, enabling participants to articulate personal interactions, challenges, and perceptions of the technology. 
The image descriptions enriched and supplemented interview data by providing concrete examples of how PVI used this tool across various contexts, from daily chores to professional tasks and travel. 


We collected the two data sources sequentially. 
First, we interviewed $14$ visually impaired users, focusing on \sbma's emerging practices in their daily lives. 
At the end of each interview, we invited participants to share their image descriptions. 
However, most participants didn't have examples to share or didn't want to share due to the demanding process or personal content sensitivity (e.g., images capturing their faces). Respecting and prioritizing participant privacy, we ensured voluntary sharing of image descriptions. Consequently, 4 participants shared their descriptions voluntarily.
% 
Meanwhile, we accessed and analyzed publicly available experiences shared by PVI on reliable online platforms. 



The combination of text-based interview data and text/visual image descriptions provided richer insights into PVI's experiences with emerging LMM technologies than either source alone would have offered. We detail our data collection process in the following section. 
}






\subsubsection{Primary Data Source: Interview Data}
We recruited 14 visually impaired participants (10 blind, 4 low-vision; 4 males, 10 females) through our prior contacts and snowball sampling.  
All participants actively used \bma{} and were primarily aged 35-40. Their occupations included three students and nine employed individuals, with two participants unemployed. 
Table~\ref{user_demographic_info} presents their demographics. 
Each participant received a \$30 gift card per session for their time and effort. 




%%%%%%%%%%%
\begin{table*}[]
\small
\caption{Participants' demographics.}
\label{user_demographic_info}
\begin{tabular}{p{0.3cm}p{0.8cm}p{0.8cm}p{4.8cm}p{2.8cm}p{3cm}p{2.1cm}}
\toprule
\textbf{ID} & \textbf{Gender} & \textbf{Age Group} & \textbf{Condition of Vision Impairment} & \textbf{Age of Onset} & \textbf{Occupation Type} & \textbf{Be My AI Usage Frequency} \\ \toprule
P1 & F & 45-50 & Totally blind, retinopathy of prematurity & Since birth & IT consultant &  3 or 4 times a day \\ \hline
P2 & F & 35-40 & Low vision, cone-rod dystrophy & Since birth & Program director in a nonprofit &  a few times a week \\ \hline
P3 & F & 30-35 & Totally blind, Leber's congenital amaurosis & Since birth & Elementary school teacher & 5 times a week \\ \hline
P4 & M & 25-30 & Totally blind, Pale optic nerves & More than 12 yrs ago & Criminal law employee & 2 to 3 times a week \\ \hline
P5 & F & 40-45 & Totally blind, retinopathy of prematurity & Since birth & Manager of digital accessibility & 2 times a day \\ \hline
P6 & F & 25-30 & Totally blind, microcephaly and detached retina & Since birth & Student & once a week \\ \hline
P7 & F & 35-40 & Low vision, retinopathy of prematurity & Since birth & Part-time employee &  2 times a week \\ \hline
P8 & M & 40-45 & Totally blind, detached retina & Since birth & Insurance & a few times a day \\ \hline
P9 & F & 30-35 & Totally blind, retinopathy of prematurity & Since birth & In-between jobs & a few times a week \\ \hline
P10 & F & 30-35 & Low vision, retinitis pigmentosa & Since birth & Student &  3 or 4 times a day \\ \hline
P11 & M & 35-40 & Totally blind, retinopathy of prematurity & Since birth & Stay-at-home parent &  3 or 4 times a day \\ \hline
P12 & M & 20-25 & Low vision, Leber's hereditary optic neuropathy & Since 14 yrs old & Student &  2 times a week \\ \hline
P13 & F & 40-45 & Totally blind, retinitis pigmentosa & Low vision since infancy, totally blind since 2021 & Human service employee &  5 times a week \\ \hline
P14 & F & 35-40 & Totally blind, retinopathy of prematurity & Since a few months old & Assistive technology specialist &  5 times a week 
\\ \bottomrule
\end{tabular}
\Description[Blind participants' demographic information about gender, age, condition of vision impairment, occupation type, and Be My AI usage frequency]{Ten blind participants are female, and four blind participants are male. Their age group ranges from 25 to 50, and their occupations include student, IT consultant, stay-at-home parent, teacher, employees in nonprofit and insurance, and assistive technology specialist. Their frequency of Be My AI usage varies from 2 times a week to 4 times a day.}
\end{table*}




\paragraph{Procedure} We conducted individual semi-structured interviews via Zoom lasting 50-76 minutes, with one or two researchers present per session. All interviews were recorded with participant consent. 

The interviews followed four main phases. 
% 
\textit{First}, we invited participants to share their personal use cases for \bma. 
To facilitate recall, we referenced Be My Eyes' common use case list~\cite{bma_usecase}, prompting participants to discuss similar experiences. 
% 
Follow-up questions were posed to further investigate their experiences with each identified use case. 
% 
\textit{Second}, participants evaluated the quality of the tool's visual interpretations, focusing on accuracy, detail level, error, and appropriateness of identity descriptions. 
% 
\textit{Third}, we explored how participants used \sbma{} among their various assistive tools, including human-assisted and AI-powered VQA systems, to understand its distinct advantages and limitations. 
% 
\textit{Finally}, we collected demographic information and inquired about participants' willingness to share copies of its visual interpretations for research analysis.








\subsubsection{Secondary Data Source: Image Descriptions Generated by \bma}
Image descriptions collected from interview participants and social media platforms served as a secondary data source to complement our interview findings. These descriptions included either text-only copies, or original images sent to \sbma{} with their corresponding descriptions. Some descriptions also included follow-up questions and responses between users and the system.


\paragraph{From Participants}


Participants were given the flexibility to select their preferred documentation method, either copying text or taking screenshots. 
% 
In total, we gathered 22 image descriptions from 4 participants, as most participants either lacked examples or were reluctant to share due to privacy concerns or the effort required in documentation.
Of these descriptions, 4 were original images alongside their descriptions, while the majority consisted of text copies -- a format participants found more manageable. 
Additionally, 6 of the 22 descriptions contained follow-up questions where participants sought clarification or additional details from the system. 





\paragraph{From Social Media Platforms}

We collected image descriptions from 4 social media platforms: X\footnote{\url{http://x.com/}}, Facebook\footnote{\url{https://www.facebook.com/}}, Reddit\footnote{\url{https://www.reddit.com/}}, and blogposts. The platforms were chosen to cover both image-sharing sites (X, Facebook) and discussion forums (Reddit, blogposts).



We used the search terms ``BeMyAI'' and ``\#bemyai'' to find relevant posts on X, Facebook, and blogposts. For Reddit, we searched for ``AI'' and ``BeMyAI'' within the r/Blind community. 



We gathered posts that met the following criteria: (i) written in English, (ii) published between September 25, 2023, the official release date of \bma, and March 31, 2024, and (iii) contained image descriptions generated by the tool. 
% 
To avoid redundancy, if identical content appeared across multiple platforms, only the earliest published post was recorded. 
If one post included multiple descriptions, each description was documented as a separate entry. 
If there were multiple posts published around the same time frame about the same topic (e.g., an X or Reddit thread), they were treated as a single entry. 



We verified the authenticity of posts by confirming that they were from either the official Be My Eyes account or from users who identified themselves as visually impaired in their profiles. These established platforms serve as reliable sources where PVI regularly share their experiences. 


In total, we collected 28 \sbma-generated image descriptions from 4 social media platforms. Of these, 23 included original images sent to \sbma{} \rev{and 5 were text-only interactions.} Fifteen descriptions contained follow-up questions that participants sent to the system for additional details or clarification. 
By platform, we collected 
(i) X: 17 image descriptions (16 with original images, 6 with follow-up questions), 
(ii) Facebook: 3 image descriptions (all with original images), 
(iii) Reddit: 3 image descriptions (1 with follow-up questions), and 
(iv) blogposts: 5 image descriptions (4 with original images, 8 with follow-up questions). 
The complete dataset is available at \href{https://bit.ly/4hqo5ve}{\textit{\textcolor{black}{https://bit.ly/4hqo5ve}}}.

\rev{This dataset, although limited in size, captures crucial insights into early user experiences with \bma. By collecting descriptions shortly after its release, we documented initial user interactions and system performance during its early phase. 
}





\subsection{Data Analysis}


We used a bottom-up approach \rev{to analyze the interview data}. The first author conducted inductive thematic analysis~\cite{braun2006using} by developing initial codes through open coding, then iteratively collating and grouping these codes into themes and categories. All authors reviewed and finalized these themes during weekly meetings (see Table~\ref{codebook} in Appendix). 



\rev{
We used a top-down approach to analyze the image descriptions. 
The first author examined each image description alongside its contextual elements: the user's follow-up questions, \sbma's responses, and the circumstances of image capture. Using the codebook derived from our interview analysis, we then analyzed these descriptions deductively. 



The following example illustrates our data analysis process from inductive to deductive approach.
During interviews, participants mentioned instances where \sbma{} initially provided general scene descriptions rather than their desired specific content, requiring participants to guide it through follow-up questions. We labeled this interaction as ``Goal Understanding Dialogue.'' 
When analyzing image descriptions, we identified similar patterns (Figure~\ref{eggshells}) and classified them under this pre-established ``Goal Understanding Dialogue'' category. 



Through this process, we first identified the system's capabilities and limitations through interview data, then complement these interview findings with specific examples from the image descriptions and subsequent conversations between users and the system.

}


