\section{Findings}
\label{findings}

In this section, we first explore \sbma's context-aware capabilities across diverse settings, assessing its effectiveness and constraints in interpreting physical environments, social and stylistic cues, and people's identity. 
% 
Next, we examine its intent-oriented capabilities by evaluating both the strengths and limitations of the system's ability to understand and act on users' intentions. 
% 
For each aspect, our findings unfold in two parts. In \textit{Capabilities of \bma}, we examine how PVI use the system and reveal its capabilities. In \textit{Example}, we illustrate specific scenarios that present both the challenges users faced and their adaptive strategies. 



\subsection{Context Awareness}

This section examines how \bma{} enhances user experiences across diverse environments and interactions. We analyze its roles in enhancing spatial awareness in physical settings, interpreting social and stylistic context, and conveying human identities. 
% 
Our analysis considers both the system's strengths in providing context-rich descriptions and its limitations due to technical constraints and subjective interpretations. 





\subsubsection{Physical Environments}
\label{physical_environments}


Participants used \sbma{} to enhance their perception of indoor and outdoor environments through detailed scene descriptions. 
They explored various settings, from theaters (P2) and room layouts (P3, P7, P9, P13) to holiday decorations (P5) and street views (P11, P12). While the system provides structured visual information that expands spatial awareness, its effectiveness occasionally diminishes due to AI hallucinations or requires participants' guidance for accurate scanning. 




\paragraph{Capabilities of \bma} Seven participants (P2, P3, P7, P9, P11, P13, P14) reported that the AI tool's comprehensive scene descriptions enhanced their spatial awareness. 
These descriptions often uncover spatial details previously unnoticed by users, as noted by P11, \textit{``It gives me unexpected information about things that I didn't even know were there.''} 
% 


% AI hallucinations 
However, this enhanced awareness is compromised by AI hallucinations, instances where AI incorrectly identifies nonexistent objects. 
Such errors can disrupt the user's context understanding, leading to confusion or misinterpretation of the environment. To verify \sbma's descriptions, participants either seek human assistance or rely on their pre-existing mental models of the environment. 



Participants successfully guide \sbma{} using auditory cues, particularly when locating dropped objects. They combine their hearing and spatial memory to adjust camera angles, which enables the tool to scan more accurately. 
% 
This integration of participants' auditory inputs guides and refines the system performance, leading to more accurate and useful descriptions.




\paragraph{Example 1: AI Hallucinations of Adding Non-existent Details}


Be My AI provides detailed descriptions of objects, characterizing their colors, sizes (e.g., ``small,'' ``medium,'' ``large,'' and ``tall''), shapes (e.g., ``round,'' ``square,'' ``fluffy,'' ``wispy,'' ``dense,'' and ``open lattice structure''), and spatial orientations (e.g., ``on the left,'' ``to the right,'' ``in the foreground,'' ``in the middle,'' and ``in the background''). 
% 
However, four participants (P3, P5, P11, P14) encountered AI hallucinations where the tool fabricated details in their environments. 



Participants developed two strategies to verify the accuracy: consulting human assistants and drawing on their spatial memory.
P3's experience illustrates the value of human verification. When \bma{} reported a \textit{``phantom''} object behind her, she first investigated personally, finding nothing. A human assistant then confirmed the absence of any object. 

\begin{quote}
    \textit{``Apparently, Be My AI said there was an object behind me, and there really wasn't. So, then, when I went and asked somebody, `You know what is behind me?' and they said there wasn't anything behind you in the picture.''} (P3)
\end{quote}


P5's experience demonstrates how spatial memory helps identify inaccuracies. When the system incorrectly described objects next to her dogs in her home, her familiarity with the space enabled her to recognize these errors independently. This ability to leverage environmental knowledge allows participants to detect and disregard AI hallucinations effectively. 




%%%%%%%%%%%%
\paragraph{Example 2: Need for User Support in Locating Dropped Objects}

Participants (P4, P12, P13) used \sbma's sequential \textit{``top to bottom, left to right''} descriptions to locate dropped items like earbuds and hair ties. 
% 
These descriptions provided precise locations, such as when P13 learned of \textit{``a picture of a carpet with a hair tie in the upper right hand corner,''} and when P12 discovered \textit{``the earphones are directly in front of you, between your feet.''}
Such sequential descriptions allow participants to pinpoint lost objects with greater accuracy. 



To optimize the tool's scanning effectiveness, participants first used their auditory perception and spatial memory to approximate an object's location before positioning the camera. 
This adjustment allowed the tool to focus on the intended search area, rather than random searching. 
% 
For instance, P4 utilized his \textit{``listening skills''} to detect the location of a fallen object before directing the camera to that specific spot. Similarly, P12 enhanced the camera's view of dropped earbuds by stepping back from his seat to capture a better angle of the floor. 
% 
These examples illustrate how participants synergize their understanding of the environment with the system's technological capabilities to manage tasks that require spatial awareness.








\subsubsection{Social and Stylistic Contexts}
\label{social_stylistic}


Participants leveraged \sbma{} for both social interactions and style-related decisions. 
In social settings, nine participants (P2, P4, P5, P8-11, P13, P14) used it to take leisure pictures of animals like cats, dogs, birds, and horses, to better understand and monitor animal behavior and status. 
% 
In stylistic applications, it aids in identifying clothing colors (P2, P7, P9, P13), recognizing patterns (P2, P9), coordinate outfits by matching clothes with shoes and jewelry (P2, P4, P5, P6, P12), and assessing makeup (P5, P10).
% 
This section examines the system's subjective interpretations within these contexts, highlighting its utility in enriching usersâ€™ experiences and the challenges in providing accurate, context-aware descriptions.





\paragraph{Capabilities of \bma}

\sbma{} usually concludes its descriptions by injecting subjective interpretations, which enriches users' understanding of depicted scenarios. 
These interpretations encompass people's or animal's emotional states (e.g., ``a cheerful expression on his face,'' ``smiling slightly,'' ``friendly and approachable,'' ``curious expression''), 
body language (e.g., ``his arms are open as if he is engaging in a conversation or greeting the other man''),
and the ambiance (e.g., ``peaceful and natural,'' ``serene and peaceful atmosphere,'' ``cozy and cheerful holiday vibe,'' ``warm and inviting atmosphere,'' ``the atmosphere seems to be lively and festive''). 




These subjective interpretations help participants better engage in social interactions and understand animal behavior. 
% 
Participants (P5, P9-11) specifically used the system to grasp nuances in animals' facial expressions, activities, and body language (Figure~\ref{human_animal}). P10 highlighted its utility during dog walks: \textit{``Sometimes it's hard for me to know if the dog is peeing or what the dog is doing.''}

\begin{figure}[t!]
\centering
\includegraphics[width=0.4\textwidth]{images/human_animal_1.png}
\caption{\bma's description of a dog, including subjective interpretations of the dog's emotions. This screenshot was provided by P9.}
\Description{Interface of Be My AI. Top of the screen is a photo of a dog. Below the photo is the description of the dog, including subjective interpretations of the dog's emotions. This screenshot was provided by P9. The full description is as follows. The picture features a happy, cream-colored Labrador Retriever sitting in front of a vibrant backdrop of Christmas presents. The dog is wearing a festive red scarf and has a joyful expression with its tongue out. Surrounding the dog are numerous gift boxes wrapped in colorful paper with patterns of snowflakes, Christmas trees, reindeer, and traditional holiday plaid. Some presents have large bows on them. Behind the dog, to the left, is a large teddy bear with a red bow, and to the right, a black bear plush toy dressed in a plaid outfit. The setting gives off a cozy and cheerful holiday vibe.}
\label{human_animal}
\end{figure}
%%%%%%%%%%%%%% fashion
Furthermore, the subjectivity extends to fashion suggestions, with \bma{} recommending stylistically coordinated outfits based on colors and patterns, as well as assessing makeup. 
For instance, P5 employed it to check the color, placement, and overall balance of her makeup, P2 and P9 consulted it to coordinate tops and bottoms, and P12 used it to select a tie that complemented his shirt. 


However, participants noted that BMA's visual interpretations sometimes included excessive subjective elements. 
This subjective input, unverified by human, could potentially undermine the system's ability to provide accurate context awareness.  
As a result, participants preferred their own subjective interpretations or feedback from sighted assistants in contexts involving human-animal interactions and fashion choices. 




%%%%%%%%%%%%%%%%
\paragraph{Example 1: Subjective Interpretations in Human-Animal Interactions}


Participants (P2, P5) identified limitations in how \sbma{} infers animal emotions in its image descriptions, raising concerns about the accuracy of these subjective interpretations. 
For instance, P5 highlighted instances where the tool describes a dog \textit{``appears to be relaxed''} or \textit{``appears to be happy.''} 
Likewise, P2 noted its tendency to include subjective commentary, as in \textit{``That's a white cat curled up on a fuzzy blanket. She looks peaceful and happy and rested.''} 



These interpretations go beyond observable visual elements to make emotional inferences that may not reflect reality. 
% 
As a result, participants expressed a preference for objective, fact-based descriptions. 
P2 articulated a desire for less editorializing, saying, \textit{``Maybe I don't want it to editorialize, you know, maybe I just literally only want the facts of it.''}



Participants (P2, P5) stressed the importance of maintaining human agency in interpreting animal behaviors, preferring their own judgment rather than the system's subjective interpretations. 
P5 particularly valued the ability to modify \sbma's descriptions of her dog's expressions, maintaining control over her interpretation of pet behaviors. 


\begin{quote}
    \textit{``Some blind people think, `How does it know that the dogs are happy? Why does it assume?' Some people don't like that it's making assumptions about the picture. I like having access to that information, but I like to be able to change it if I want.''} (P5)
\end{quote}


In summary, \sbma's subjective inferences can enrich descriptions, yet they risk introducing inaccuracies that undermine the system's reliability. Users therefore value the ability to override these interpretations, preserving their agency in understanding the context. 




\paragraph{Example 2: Subjective Interpretations in Fashion Help}


\sbma's subjective interpretations extend beyond human-animal interactions to fashion help. 
Participants (P6, P7, P10) utilized it to describe colors and patterns but expressed concerns about its subjective fashion suggestions. 
% 
They preferred to make their own style choices or seek human assistance for outfit matching, highlighting the human subjectivity in fashion decisions.



P6 questioned the AI's capacity to authentically replicate human judgment, saying, \textit{``It's interesting how AI is being taught to simulate kind of the human factor of things.''}
% 
She cited experiences where AI-generated responses appeared \textit{``strange''} and \textit{``complete nonsense,''} contrasting these with the nuanced understanding that humans provide.  



\begin{quote}
    \textit{``No, no, no, no, I would never use it to do anything that required human subjectivity... I just don't trust AI with a task that is supposed to be subjective like that, particularly visual like that. Have you ever seen AI weirdness?... I think that just goes to show why I'm not gonna trust AI with my fashion yet.''} (P6)
\end{quote}


P10 explicitly preferred human feedback, stating, \textit{``I'm still more confident asking a sighted person to provide me with the feedback.''} 
This preference underscores how participants maintain their agency by relying on human judgment for fashion decisions rather than deferring to \sbma's suggestions. 







%%%%%%%%%%%%
\subsubsection{Identity Accuracy and Sensitivity}
\label{identity}


Nine participants (P2, P3, P5-7, P9-11, P13) evaluated the system's ability to describe people's identities in images of their families, friends, and social media posts. 
Our analysis examines how the tool handles identity attributes like age and gender, focusing on both its capabilities and limitations in providing sensitive and accurate descriptions. 




\paragraph{Capabilities of \bma}

\sbma{} describes people's identity in terms of gender (e.g., ``woman,'' ``man''), age (e.g., ``old,'' ``late twenties or early thirties''), appearance (e.g., ``long, dark brown hair,'' ``wavy brown hair,'' ``His hair falls past his ears, with a slightly messy but stylish look'') and ethnicity (e.g., ``East Asian''). 
% 
Participants appreciated such detailed descriptions for providing deeper insight into people's visual characteristics. P2 explained: \textit{``I enjoyed hearing, I knew my friend was East Asian when I saw her picture. It was kind of cool to see that because... as a blind person, you don't know that all the time. So, I'd like access to it.''}


However, eight participants (P2, P3, P5-7, P9, P10, P13) encountered challenges with the system's accuracy and sensitivity in describing identity attributes, particularly gender and age. 
% 
These errors reveal the tool's limited ability to interpret contextual cues that extend beyond visual appearance, such as cultural, situational, and personal contexts. 



\paragraph{Example: Inaccurate Identification of People's Gender and Age}


Three participants (P3, P10, P13) reported inaccuracies in \sbma's gender identification. 
P13 described how it misidentified gender by mistaking a hidden ponytail for short hair: \textit{``just looked like short-cropped hair.''} 
% 
Similarly, P10 revealed that the system struggled with gender identification for individuals whose physical attributes do not conform to typical gender norms, noting, \textit{``the person had a short hair, and was a female''}. 
In response, it defaulted to neutral language, \textit{``\bma{} didn't tell me it was a woman or a man, just said a person.''}
% 
These examples illustrate the system's reliance on stereotypical indicators and reveal its challenges in interpreting non-visible details.



Additionally, two participants (P5, P9) identified problems with age description. P9 reported that \bma{} overestimated her daughter's age by one year. 
% 
Likewise, P5 observed sensitivity concerns when the system labeled someone as \textit{``an old woman''} based solely on grey hair. 
% 
To address these age-related inaccuracies, P5 suggested that the tool should adopt more objective, factual descriptions rather than subjective or potentially stigmatizing labels, such as \textit{``a woman with grey hair, instead of an older woman.''}
% 
These examples indicate the system's difficulties with precise age estimation and the potential inaccuracy when AI makes assumptions based on appearance alone.



In summary, these examples illustrate the tool's limited context awareness and its challenge in accurately interpreting people's identities, such as gender and age, due to the reliance on stereotypical visual cues. These misjudgements are rooted in the system's inability to integrate and interpret broader, non-visible contextual elements like cultural norms and personal styling choices. 








%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{Intent-Oriented Capabilities}

In this section, we explore \sbma's limitations \rev{in} comprehending users' goals, providing actionable support to fulfill these objectives, and offering real-time feedback. 
% 
Through analysis of specific examples, we elucidate the extent of the system's intent-oriented capabilities and highlight areas where it falls short in adapting to user needs. 



\begin{figure}[t!]
\centering
\includegraphics[width=0.45\textwidth]{images/cooking_eggshell.png}
\caption{On the left is the original image sent to \bma. On the right is \bma's description of eggs in a frying pan, followed by a question checking for the presence of eggshells. This example was originally drawn from X.}
\Description{On the left is the image of a frying pan on the stove-top. Inside the pan there are 4 eggs, one of whose yolk is broken. On the right is a photo of an user using Be My AI on a smart phone.}
\label{eggshells}
\end{figure}


\subsubsection{Agentic Interaction}
\label{agentic_interaction}

While \bma{} can process visual information, it often struggles to infer users' specific intentions from images. Users compensate for these limitations by guiding the system through targeted prompting. 


\paragraph{Capabilities of \bma}



BMA's ``ask more'' function enables users to explore image details that were not covered in initial descriptions. Through this feature, users can probe for specific details that match their personal needs or interests.
% 
Eleven participants (P1-3, P5, P6, P8-12, P14) used the ``ask more'' function to 
match outfits (P2, P5, P12),
check makeup (P5, P10), 
suggest cooking recipes (P12), 
assist with household appliances (P5, P12), 
examine text or objects (P6, P8, P11, P12, P14),
gain more details about people's facial expressions, attire, or actions (P2, P3, P5, P8, P9, P10, P11), 
check animal status (P3, P5, P11, P14), 
and edit the descriptions for social media posts (P2, P3). 



Seven participants (P1-3, P5, P8-10) valued the flexibility of posing follow-up questions, enhancing their independence by reducing reliance on human assistance. As P3 noted, \textit{``I can ask follow-up questions, so I have a good way to sort of figure out what's in the image independently, which is something I was not able to do before this app came out.''} 


However, participants identified a common issue that the system is unable to discern users' goals in initial response, often resulting in generalized descriptions without knowing which aspects to emphasize. P12 elaborated on this challenge, stating, \textit{``\bma{} loves to make general descriptions, and it doesn't know what to focus on.''} 
To overcome this limitation, users guide the system with targeted prompts that clarify their specific intentions. 





\paragraph{Example 1: Check for the Presence of Eggshells}







In Figure~\ref{eggshells}, BMA's initial response provides a detailed descriptions of what is included in a frying pan. 
The user, likely influenced by prior experiences of inadvertently leaving unwanted items in the pan while cooking, specifically intended to check for such elements. 
% 
However, the system did not infer that the user was looking for unwanted elements like eggshells, rather than seeking a general description of the scene.
% 
To clarify her objective, the user posed a follow-up question, thus directing \sbma{} to recognize and prioritize her specific concerns and intentions. 



In this example, although the AI tool can describe the visual elements accurately and comprehensively (from the condition of the eggs to the surroundings), it failed to identify unusual visual elements that are challenging for PVI to detect but are important to their understanding of the scene. 
% 
User prompting helps refocus the system's attention on these unusual elements, thereby enhancing its ability to interpret and respond to user-specific intents more effectively. 



%%%%%%%%%%
\paragraph{Example 2: Adjust Rotary Control Appliances}
Six participants highlighted the tool's inadequacy in comprehending their goal of adjusting rotary control appliances like washers (P1) and thermostats (P2, P4, P5, P13, P14). This task requires \bma{} to interpret the current settings on these appliances as users modify dials for time, mode, or temperature.
% 
However, the system often offered broad descriptions of visual elements without honing in on the user's specific objectives. 


P13's experience exemplifies this limitation. While the system could recognize a thermostat on the wall, it failed to provide critical information such as the current temperature setting or instructions for adjusting the temperature. 



To overcome the limitation, participants guided the system to better understand their goal through prompting. 
% 
For instance, P5 asked precise questions like \textit{``What is the arrow pointed at right now on the current setting?''} Such specific queries helped redirect \sbma's focus from general scene descriptions to the exact details needed for appliance adjustment. 





\subsubsection{Consistency and Follow-Through}
\label{consistency}


In Section~\ref{agentic_interaction}, we described how users needed to explicitly guide \bma{} when it failed to understand their intentions. 
% 
This section examines the system's difficulties maintaining consistency and following through on tasks even after acknowledging users' goals. 



\paragraph{Capabilities of \bma}


\sbma{} often fails to proactively suggest next steps or support users until they achieve their objectives. 
% 
Effective consistency requires maintaining focus on the user's objective throughout the interaction. \rev{This includes} offering progressively specific and relevant assistance, and ensuring all responses contribute to the user's intended outcome.


Due to these limitations, participants turn to human assistants. \rev{Human assistants} can interpret the context of tasks and goals more dynamically, \rev{providing} conversational guidance and adapting to users' actions to facilitate goal completion. 




%%%%%%%%%%%%
\paragraph{Example 1: Inadequacy in Identifying Central Puzzle Piece}

P14 used \sbma{} to differentiate between various puzzles by describing the images on the boxes like a scene of cats or bears. 
% 
When tasked with identifying the centerpiece of a nine-piece puzzle using the box image, \sbma{} failed to provide the necessary detail for this precise task. 
% 
Initially, the system correctly read text from the puzzle box indicating which piece belonged in the center. 
However, when P14 requested more specific guidance to locate the centerpiece, \sbma{} only noted that it was square -- a characteristic shared by multiple pieces. This response was too vague for successful puzzle assembly. 



\begin{quote}
    \textit{``When \bma{} read me the text of the box on the back of the puzzle that said, you know, this particular piece should be in the center of the puzzle. As a follow-up question I asked, `Could I have more information about the piece in the center?' And it said, `This piece is a square piece,' but I mean, there were many different square pieces, so I could not tell from that.''} (P14)
\end{quote}
% 
This example illustrates the system's limited ability to translate visual understanding into actionable guidance. While it could process the image and comprehend users' general goals, it could not provide the detailed information needed for successful goal achievement. 
As a result, P14 turned to a family member for assistance. 


\begin{figure}[t!]
\centering
\includegraphics[width=0.6\columnwidth]{images/description-of-a-conference-room.png}
\caption{\bma's description of a conference room, with the original image cropped. This example was drawn from X.}
\Description{Screenshot of the Be My AI interface. In the top is the description of a conference room. In the bottom left is the Take Picture button. In the bottom right is the Ask More button. The full description is as follows. The picture shows an indoor conference room with a group of people seated at round tables. The attendees appear to be focused on something at the front of the room, which is not visible in the photo. The room has a wooden floor and a ceiling with white beams, from which stage lights are hanging. The tables are covered with grey tablecloths, and there are some papers and bottles on them. There's a screen visible in the background showing some kind of presentation or logo, but it's not clear what it is. The lighting in the room is bright, and the overall atmosphere seems professional.}
\label{incomplete_info}
\end{figure}
% \footnotetext[2]{\url{https://x.com/Chr1sLew1s/status/1732730506557477052}}

\paragraph{Example 2: Lack of Instruction for Camera Adjustment}
\label{adjust_camera}

A common challenge for participants (P3, P5, P6, P8, P10-12, P14) was aligning the camera properly to capture clear views of their intended areas. 
\bma{} can identify image quality issues, notifying users when pictures were \textit{``cut off''} (P5), \textit{``blurry''} (P6), or incomplete (Figure~\ref{incomplete_info}).  
% 
However, it is unable to provide further actionable guidance on adjusting the camera to improve image clarity. 
% 
This limitation manifested in scenarios where \sbma{} recognized both the user's goal of capturing specific areas and the image deficiencies but could not suggest practical solutions. 
As P6 described, \textit{``It was very frustrating because it said the picture is blurry. Can you please put the label in the frame? But I didn't know how to put the label on the frame.''} 








%%%%%%%%%%%%%
Eight participants (P1-3, P7, P8, P12-14) \rev{resolved this challenge by turning to human assistants, who can offer} adaptive support to achieve their goals, \rev{especially in} tasks that require continuous feedback. P13 shared an example where a human assistant not only understood her goal but also guided her in adjusting the camera and instantly reminded her to turn on the light to achieve her objective.

\begin{quote}
    \textit{``You know, having that ability to communicate and say, `Hey, this is what I'm looking for.' Or one time, I was looking for something and I had the lights off, and they're like, `You need to turn the lights on.' And I go, `Okay,' as opposed to, you know, if I tried using AI for that, it would just say `dark room.'''} (P13)
\end{quote}


This example highlights human assistants' ability to adapt their communication based on the situation and the user's implied needs, providing solutions that directly support achieving the user's goals.
% 
\rev{However}, \bma{} struggles to provide practical assistance despite understanding basic requests. 







\subsubsection{Real-Time Feedback}
\label{realtime_feedback}

We investigate how participants used \sbma{} to support navigation tasks, focusing on location awareness and orientation. 
Our analysis reveals the system's constraints in delivering real-time feedback and comprehensive navigational information from static images, and its inability to facilitate immediate interactions with surroundings.  



\paragraph{Capabilities of \bma}

Participants (P2, P5, P10) employed \sbma{} to aid in localization and orientation while navigating to their destinations. 
% 
For instance, P2 utilized it to identify gate numbers at the airport, P5 employed it to read signage directing toward the airport's transportation area, and P10 used it to recognize her surroundings when disoriented in her neighborhood.  
% 
Despite these benefits, participants encountered challenges when using \sbma{} for navigation. The primary limitations stemmed from the limited camera view and practical mobility issues. 




\paragraph{Example 1: Limited Navigational Information in Static Images}


Six participants (P2, P5, P6, P10, P11, P13) reported that \sbma's reliance on static images rather than real-time videos makes it difficult to capture comprehensive navigational information, such as obstacles and signage, in a single shot. 
% 
As P6 described, users \textit{``have to stand there and keep taking pictures and taking pictures,''} verify the captured content, assess its utility for navigation, and adjust the angle for additional shots. This iterative process can be time-consuming, \textit{``It'll be too much of a task that's supposed to take maybe 10 minutes would probably take like 30.''}



P10 elaborated on the challenge and risk of simultaneously taking pictures and navigating, particularly when attempting to identify and navigate around obstacles. The uncertainty of capturing all potential hazards was a significant concern. 



\begin{quote}
    \textit{``It's still hard to know how to capture, you know, all the obstacles. I think that's the issue. Like, how to know that I captured the right obstacle on my path? I mean, it depends [on] what I'm able to capture with the camera. You know, that's the tricky part for somebody without vision to capture the obstacle.''} (P10)
\end{quote}


Given these limitations, participants preferred human assistance through video-based interactions over \bma{} for real-time navigation. 
Video interactions not only benefit from human adaptability in adjusting guidance (Section~\ref{adjust_camera}) but also offer crucial real-time feedback unavailable in static images. 
Participants (P2, P6, P13) suggested that real-time video interpretation capabilities would significantly enhance the system by eliminating the need for repeated picture-taking. 



\begin{quote}
    \textit{``If you could hold the camera and it could do it in real time, versus having to stop, take a [picture], then assess it... If that were the case, then you could move on to doing things like uploading videos and getting it to describe actual videos and things like that, versus just still images. That would be great. You know, then you could describe more.''} (P2)
\end{quote}
% 
Such real-time video analysis would enable continuous camera movement and immediate feedback, streamlining the navigation process without pausing and reviewing individual images. 



%%%%%%%%%%%%%%
\paragraph{Example 2: Irreplaceable Role of Orientation \& Mobility Skills in Navigation}


Besides the value of real-time visual interpretations from external resources like human assistants, participants (P5, P10, P13, P14) emphasized the indispensable role of real-time feedback through their Orientation and Mobility (O\&M) skills for safe navigation.
% 
P5 and P13 pointed out that even human assistance, though adaptive in guiding PVI away from navigational hazards, cannot substitute for essential O\&M skills required for tasks like street crossing.
Consequently, participants are more cautious about relying on emerging AI tools for navigation.
% 
P14 reinforced this perspective, saying, \textit{``it's not a replacement for our mobility skills or just any skills in general. It can aid and augment the skills but it's not a replacement for them. It shouldn't be.''} 



%%%%%%%
Participants (P5, P13) further delineated vital information provided by O\&M skills or discerned through O\&M tools like white canes or guide dogs, which current AI or human assistance cannot replace. 
First, immediate surroundings. \bma{} can identify obstacles \textit{``that are a little far away''} (P5) but may miss immediate surrounding hazards. 
Second, distance and proximity measurements of obstacles. Although \sbma{} can indicate the presence of obstacles, it lacks the capability to measure their distance.
Third, directional details. Essential navigational information, such as the direction of stairs (ascending or descending) or the presence of railings, are not always detectable through the system. 
% 
With the O\&M skills and tools, users can instantly adapt their movements based on direct interaction with their environment. This level of responsiveness is currently unattainable with AI tools or human assistance. 


\begin{quote}
    \textit{``[\bma] says, you know, `stairs in front,' and it's like, `Okay, that's great, but where are they? How far are they? Are they going up? Are they going down? Is there a railing?' which would be information that the dog or the cane could tell you. So, I would say use it as a tool along with, but definitely not by itself.''} (P13)
\end{quote}


In summary, neither AI systems nor human assistants can replace the essential, real-time feedback and adaptive capabilities offered by O\&M skills and tools. These elements are vital for ensuring safe navigation by allowing users to directly interact with their environment.
