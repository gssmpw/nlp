\section{Discussion}
\label{sec:discussion}


\begin{figure*}[t!]
\centering
\includegraphics[width=0.85\textwidth]{images/discussion_eggshell_user+app_combined.pdf}
\caption{The top shows the status quo of handoff between the user and \bma. The bottom illustrates our proposed simplified interaction.}
\Description{The top figure illustrates the current state of interactions between the user and Be My AI, where the user submits a photo and Be My AI returns a response in the first time of interaction. Subsequently, the user poses a follow-up question to Be My AI and Be My AI returns a more detailed description in the second time of interaction. The bottom figure is our proposed simplified interaction, where Be My AI learns from previous interactions and returns a detailed description during the first interaction, without user asking follow-up question.}
\label{fig.discussion_eggshell_user+app}
\end{figure*}




In this section, we examine the current state of handoff between users, \bma, and remote sighted assistants, and propose new paradigms to address the challenges identified in our findings. 
Next, we explore how multi-agent systems, both human-human and human-AI interactions, assist visually impaired users, and envision the transition toward AI-AI collaborations for tasks requiring specialized knowledge. Finally, we discuss the potential advantages of real-time video processing in the next generation of AI-powered VQA systems.






\subsection{Handoff Between Users, \bma, and Remote Sighted Assistants}
\label{handoff}

In this study, we illustrated the advantages of the latest LMM-based VQA system in (i) enhancing spatial awareness through detailed scene descriptions of objects' colors, sizes, shapes, and spatial orientations
(Section~\ref{physical_environments}), (ii) enriching users' understanding in social and stylistic contexts by detailing emotional states of people or animals, their body language, ambiance
(Section~\ref{social_stylistic}), and identity recognition (Section~\ref{identity}), 
and (iii) facilitating navigation by interpreting signages (Section~\ref{realtime_feedback}). 


Informed by our findings, despite these various benefits, there are challenges that the system alone cannot overcome.  
\bma{} still requires human intervention, either from the blind user or the remote sighted assistant (RSA), to guide or validate its outputs.  
% 
Users seek confirmation from RSAs or depend on their own spatial memory to overcome AI hallucinations, where \sbma{} inaccurately adds non-existent details to scenes. Users also rely on auditory cues and spatial memory to locate dropped objects and direct the system toward the intended search areas (Section~\ref{physical_environments}).
% 
Moreover, users actively prompt the system to understand their specific objectives, such as checking for eggshells in a frying pan or adjusting appliance dials (Section~\ref{agentic_interaction}). 
% 
There are also instances where users require assistance from RSAs when the system fails to provide adequate support to fulfill users' objectives, such as identifying the centerpiece of puzzles or adjusting the camera angle (Section~\ref{consistency}).
% 
Human assistance or users' O\&M skills are necessary to receive real-time feedback for safe and smooth navigation (Section~\ref{realtime_feedback}). 



Furthermore, our findings revealed that the system might produce inaccurate or controversial interpretations. Users express skepticism towards \sbma's subjective interpretations of animals' emotions and fashion suggestions (Section~\ref{social_stylistic}), and have encountered inaccuracies in \sbma's identification of people's gender and age (Section~\ref{identity}). These instances underline potential areas where human judgment is necessary to corroborate or correct the system's descriptions.





Next, we discuss the handoff~\cite{mulligan2020concept} between users, \bma, and RSAs to mitigate the aforementioned challenges. 




% \begin{figure}[]
% \centering
% \includegraphics[width=0.95\textwidth]{images/discussion_eggshell_user+app.png}
% \caption{Status quo of handoff between the user and \bma.}
% \label{fig.discussion_eggshell_user+app}
% \end{figure}


% \begin{figure}[]
% \centering
% \includegraphics[width=0.95\textwidth]{images/discussion_eggshell_user+app-single-interaction.png}
% \caption{Status quo of handoff between the user and \bma.}
% \label{fig.discussion_eggshell_user+app}
% \end{figure}






%%%%%%%%%%%%%
\subsubsection{Status Quo of Interactions Between Users and \bma}


Through BMA's ``ask more'' function, users are able to request additional details about the images that were not covered in initial descriptions. 
% 
This functionality facilitates a shift in interaction dynamics between users and \bma, even if the system may not accurately understand or answer users' questions in the first attempt. 
In these interactions, users are not merely passive recipients of AI-generated outputs, they actively guide the AI tool with specific prompts to better align AI's responses with their objectives. 


Our findings reported one instance where the AI tool fails to grasp the user's intent to check the presence for eggshells in the beginning (Figure~\ref{fig.discussion_eggshell_user+app} top). First, the user submits an image of eggs in the pan. Respond to the image, the system describes the quantity and object (``Inside the frying pan, there are three eggs'') and states of the yolks and whites (``whites separated'', ``yolk has broken'', ``mixing with the egg white''). Next, the user clarifies her inquiry by asking, ``are there any shells in my eggs?''
This prompts the system to understand the user's goal, reevaluate the image, subsequently confirming the presence (``Yes, there is a small piece of eggshell in the frying pan'') and location of an eggshell to help her remove it (``near the bottom left of the broken egg yolk''). 


This interaction exemplifies the status quo of handoff, where the user and \bma{} engage in a back-and-forth dialogue to refine the descriptions based on the ``ask more'' function and the user's precise prompts. 
% 
\rev{While this iterative process allows the system to eventually understand the users' intent without RSAs' intervention, it places cognitive burden on users who must carefully craft and iteratively refine their prompts. The cognitive load increases as users mentally track what information they've already received, analyze gaps between their needs and the system's responses, and develop increasingly specific queries. 
}






\rev{To reduce users' cognitive load, we propose enabling the system to adopt a mechanism that combines multi-source data input with long-term and short-term memory capabilities~\cite{zhong2024memorybank}. With explicit user consent, future versions of LMM-based VQA systems could integrate data from users' mobile devices (e.g., location information, time data) alongside historical interaction data within the system (e.g., contexts and follow-up questions) to recognize user preferences and common inquiries, and infer their needs, thereby generating responses more effectively in similar contexts. Long-term memory serves as a repository for capturing generalized user preferences, behavior patterns, and aggregated insights across multiple users. This long-term memory is particularly effective for improving system intelligence by identifying common user needs and optimizing general responses~\cite{priyadarshini2023human}. Meanwhile, short-term memory can focus on task-specific optimization within a single interaction session. It retains context from the immediate conversation, such as recent user inputs and system responses, to enhance relevance and coherence in real time. Short-term memory operates dynamically, clearing retained data once the session ends or the task is completed, thereby ensuring privacy and preventing unnecessary data retention.}


% However, this interaction could be further simplified by training \bma{} to learn from previous dialogues. By recognizing user preferences and common inquiries, such as identifying unusual elements in the scenario (eggshells in cooking eggs), we envision that the system could proactively address user concerns more efficiently and thus reduce the need for multiple clarifying prompts (Figure~\ref{fig.discussion_eggshell_user+app} bottom). 
\rev{For example, when identifying unusual elements during cooking (e.g., eggshells in cooking eggs), \bma{} could utilize the user's immediate input while referencing short-term memory from the current session or recent similar interactions. Additionally, by leveraging long-term memory, the system can learn from the user's past questions and query patterns to better match their habits and preferences, i.e., user's typical needs. Furthermore, multi-source data input, such as time or location information, can assist the system in inferring the user's current context, for instance, recognizing that the user is preparing a specific meal at a particular time or place, which allows the system to provide more relevant and context-aware assistance. This approach enables \bma{} to proactively anticipate user intent and deliver targeted responses, reducing the need for multiple clarifying prompts (Figure~\ref{fig.discussion_eggshell_user+app} bottom).} 

\rev{Cognitive Load Theory (CLT) suggests that well-designed interactions can significantly reduce users' extraneous load while enhancing the effective management of germane load~\cite{sweller1988cognitive, chandler1991cognitive}. Following the principles of CLT, we recommend using the above design to enable LMM-based VQA systems to minimize unnecessary clarifying prompts, thereby reducing users' cognitive load. 

}






\begin{figure*}[h!]
\centering
\includegraphics[width=0.8\textwidth]{images/discussion_eggshell_user+app+rsa.png}
\caption{Handoff between the user, \bma, and RSA for identity interpretations.}
\Description{The user submits a photo of people to Be My AI. Be My AI recognizes the requirement for identity interpretations and direct the photo to a remote sighted assistant. The remote sighted assistant returns a description of people's identities to Be My AI. Be My AI learns from the assistant's responses.}
\label{fig.discussion_eggshell_user+app+rsa}
\end{figure*}



%%%%%%%%%%%%%

\subsubsection{AI Deferral Learning for Identity Interpretations} 
\label{sec:deferral_learning}
% 


Our findings elucidated \sbma's capabilities and limitations in interpreting identity attributes. Although the system can describe aspects like gender, age, appearance, and ethnicity, it may make errors due to its reliance on stereotypical indicators and its inability to interpret non-visible details (Section~\ref{identity}).
% 
However, Stangl et al.'s work~\cite{stangl2020person} pointed out that PVI \rev{seek identity interpretations from AI assistants} across various contents, including \rev{browsing} social networking sites where our participants reported using \bma.


% human's ability 
This reveals a tension between PVI's interests in knowing about \rev{identity} attributes and the AI's challenges in providing reliable information\rev{~\cite{hanley2021computer}}. The conflict arises because attributes such as age and gender are not purely perceptual and cannot be accurately identified by visual cues alone. \rev{However}, RSAs \rev{can draw on contextual clues, past interactions, and cultural knowledge to make more nuanced observations about these human traits.} These social strategies are not typically accessible to AI systems. 
% \rev{This suggests a need for human-AI collaboration: certain tasks are best handled by AI assistants, others by human assistants, and still others through coordinated effort between both~\cite{gonzalez2024investigating}.}


% To mitigate these issues, we consider the potential benefits of AI \rev{deferral} learning that involves handoff between the user, \bma, and RSA. The process is illustrated in Figure~\ref{fig.discussion_eggshell_user+app+rsa}.

% To facilitate effective handoff between AI and human assistants,
To mitigate these issues, \rev{we propose adopting a deferral learning architecture~\cite{mozannar2020consistent, raghu2019algorithmic}, where an AI model learns when to defer decisions to humans and when to make decisions by itself. As detailed by Han et al.~\cite{han2024uncovering} and illustrated in Figure \ref{fig.discussion_eggshell_user+app+rsa}, this architecture creates a three-stage information flow:


\begin{itemize}
    \item \textbf{Stage 1}: It begins when users submit image-based queries to \bma. At this stage, the system uses a detection mechanism to identify sensitive contents, focusing particularly on those involving human physical traits. Current large-language models have already incorporated such mechanisms~\cite{perez2022red, bai2022training}; however, they still struggle to interpret human identity with consistent accuracy~\cite{hanley2021computer}.    
    
    % 
    \item \textbf{Stage 2}: Rather than declining sensitive requests outright, \bma{} redirects these queries to RSAs. This maintains the system's helpfulness while ensuring accurate responses. 
    % 
    \item \textbf{Stage 3}: RSAs provide descriptions by leveraging contextual understanding, such as analyzing the users' current environment and cultural background.
    % Additionally, RSAs employ a variety of social strategies by integrating contextual information, historical interactions, and cultural insights to deduce user attributes and preferences, ensuring that responses are not only relevant but also culturally and contextually appropriate. 
    % RSAs provide detailed descriptions by leveraging contextual understanding and social awareness.    
    %\textcolor{red}{RSAs can employ a variety of social strategies to deduce these attributes by leveraging context, prior interactions, and cultural insights.} 
\end{itemize}




In contrast to prior work that addresses stereotypical identity interpretation through purely computational approaches~\cite{wang2019balanced,wang2020towards,ramaswamy2021fair}, our proposed AI deferral learning takes a hybrid human-AI approach that combines AI capabilities with human expertise. 
% 
While previous AI-only solutions have made progress in reducing bias, they still struggle with identity interpretation~\cite{hanley2021computer}.
The challenges arise not only from technical issues but also from the ontological and epistemological limitations of social categories (e.g., the inherent instability of identity categories), as well as from social context and salience (e.g., describing a photograph of the Kennedy assassination merely as ``ten people, car'').
%
Our system leverages RSAs who have got more experience and probably more success in identifying people's identity through their human perception abilities and real-world experience. 
RSAs can interpret subtle contextual cues, understand cultural nuances, and adapt to diverse presentation styles that may challenge AI systems. 
% For example, RSAs can understand cultural markers of identity, and perceive age and ethnicity across different cultural contexts. 
Through the AI deferral learning architecture, AI assistants can learn continuously from human assistants' responses and improve its ability to handle similar situations. The three-way interactions between users, AI assistants, and RSAs generate rich contextual data that can enhance the AI system's identity detection mechanisms.} 

% Such process could also be adapted for requests involving the interpretation of social contexts \rev{like assessing animals' emotions (Section~\ref{social_stylistic})}. It can facilitate \bma{} continuously improve through observation and learning from human expertise. 

% When equipped with memory capabilities and given user consent, the AI assistant can build a knowledge base of individuals whom users frequently encounter. Moreover, the three-way interactions between users, AI assistants, and RSAs generate rich contextual data that can enhance the AI system's identity detection mechanisms.



% compared to prior works attempted to address the issues of stereotypical identity interpretation, how does the proposed AI referral learning could perform better?
% \textcolor{red}{[add prior work on identity interpretation (e.g., CV)]}

% \textcolor{red}{referral, when handoff to human,  challenges of ai referral learning is how to get the dataset. here bma get the dataset for training from rsa.}


% \rev{
% In contrast to prior work that addresses stereotypical identity interpretation through purely computational approaches~\cite{wang2019balanced,wang2020towards,ramaswamy2021fair}, our proposed AI referral learning takes a hybrid human-AI approach that combines AI capabilities with human expertise. 
% % 
% While previous AI-only solutions have made progress in reducing bias, they still struggle with identity interpretation~\cite{hanley2021computer}.
% The challenges arise not only from technical issues but also from the ontological and epistemological limitations of social categories (e.g., the inherent instability of identity categories), as well as from social context and salience (e.g., describing a photograph of the Kennedy assassination merely as ``ten people, car'').
% %
% Our system leverages RSAs who have got more experience and probably more success in identifying people's identity through their human perception abilities and real-world experience. 
% RSAs can interpret subtle contextual cues, understand cultural nuances, and adapt to diverse presentation styles that may challenge AI systems. For example, RSAs can understand cultural markers of identity, and perceive age and ethnicity across different cultural contexts. 
% Through the AI referral learning framework, \bma{} can learn from RSAs' nuanced interpretations in these scenarios, gradually improving its ability to handle similar situations. The system logs RSAs' responses and uses them as training examples, allowing \bma{} to develop more context aware in identity recognition. 
% }
% % 
% Such referral learning processes could also be adapted for requests involving the interpretation of social contexts. It can facilitate \bma{} continuously improve through observation and learning from human expertise. 


\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\textwidth]{images/discussion_eggshell_user+app+rsa-check.png}
\caption{Handoff between the user, \bma, and RSA for fact-checking.}
\Description{The user submit a photo to Be My AI, Be My AI returns a description with possible AI hallucination. Then, the user directs the photo to a remote sighted assistant, who returns a corrected description to the user.}
\label{fig.discussion_eggshell_user+app+rsa-check}
\end{figure*}


%%%%%%%%%%%%%
\subsubsection{Fact-Checking for AI Hallucination Problem}
% ai hallucinations - informed by our findings, cannot be addressed, ask humans 



Our findings highlighted that the AI-generated detailed descriptions helped users understand their physical surroundings. However, there were instances where AI systems hallucinated, i.e. incorrectly added non-existent details to the descriptions, which led to confusion (Section~\ref{physical_environments}). 
% 
\rev{
In fact, hallucinations is a known problem for large language models upon which \bma{} is built~\cite{gonzalez2024investigating}. Current approaches to address this problem include Chain-of-thought (CoT) prompting~\cite{wei2022chain}, self-consistency~\cite{wang2022self}, and retrieval-augmented generation (RAG)~\cite{lewis2020retrieval}.
% 

In CoT prompting~\cite{wei2022chain}, users ask an AI model to show its reasoning steps, like solving a math problem step by step rather than simply giving the final answer. It is similar to the ``think aloud'' protocol in HCI. 
% 
Self-consistency~\cite{wang2022self} is an extension of CoT prompting. Instead of generating just one chain of thought, the model is asked to generate multiple different reasoning paths for the same task. Each reasoning path might arrive at a different answer. The model then takes a ``majority vote'' among these different answers to determine the final response.
% 
In RAG~\cite{lewis2020retrieval}, AI models are provided with relevant information retrieved from a vector storage as ``context'' to reduce factual errors in their responses.
}

\rev{
In light of these techniques, users adopt various strategies that mirror CoT prompting, self-consistency, and RAG. We outline some potential strategies below.
% with a visual (Figure~\ref{fig.discussion_eggshell_user+app+rsa-check}).
\begin{itemize}
    \item \textbf{Part-Whole Prompting}: This strategy parallels the Chain of Thought (CoT) prompting. A user sends an image to \bma{} and requests an initial overall description, followed by a systematic breakdown that justifies this description. For example, users might first ask for a description of the image as a ``whole'', then request to divide the image into smaller ``parts'', like a $3 \times 3$ grid, and describe each grid individually. If the descriptions of the individual parts align coherently, it increases the likelihood that the overall description is accurate. This approach would require processing more information; however, it will provide users with greater confidence in the AI's response, as it enables them to verify consistency between the whole and its constituent parts.  
    % 

    \item \textbf{Prompting from Multiple Perspectives}: This strategy resembles the self-consistency technique. A user sends an image to \bma{} and requests multiple descriptions from different perspectives. For example, users might ask for one description that focuses on the background and another that emphasizes foreground objects. Users can also request descriptions from the viewpoint of objects within the image (e.g., ``How would a person sitting on a chair see this scene?'' and ``How would a person sitting on the floor see this scene?''). While gathering descriptions from multiple perspectives may increase the likelihood of hallucination, it can also help identify common elements that appear consistently across different viewpoints, potentially indicating true features of the image.    
     % 
    \item \textbf{Prompting with Human Knowledge}: This strategy resembles the RAG approach. A user sends an image to \bma{}, provides their current understanding of the image and its context, and requests a description that complements their knowledge. For example, in Figure~\ref{eggshells}, users can specify that someone took the picture in a kitchen environment and that it should show a frying pan containing eggs. Users possess this knowledge through their familiarity with physical environments, self-exploration, spatial memory, and touch~\cite{gonzalez2024investigating}. The user-provided knowledge will help the AI model ground its response in an accurate context~\cite{liu2024coquest}.     
    % 
    \item \textbf{Pairing with Remote Human Assistants}: While the previous three strategies rely on multiple prompting and response aggregation to identify facts, this approach leverages the traditional remote sighted assistance framework.
    This strategy (shown in Figure~\ref{fig.discussion_eggshell_user+app+rsa-check}) differs from the deferral learning framework (Section~\ref{sec:deferral_learning}) in that users forward the AI responses to human assistants, rather than the AI assistant deferring to humans for the response.
    In this strategy, a user first sends an image to \bma{} to receive a description. When users suspect inaccuracies through triangulation~\cite{gonzalez2024investigating}, such as descriptions that conflict with their spatial memory or common sense (e.g., implausible objects like a palm tree in a cold region), they can request a RSA to fact-check the description. The RSA then verifies the description and sends corrected information back to the user. This verification process is likely easier and faster for a RSA than composing a description from scratch, as the RSA's work involves checking rather than creating content.
\end{itemize}
}

\rev{
In summary, AI hallucination presents both challenges and opportunities. By addressing these issues, future work will strengthen the way users, AI models, and human assistants interact with each other.
}















%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{Towards Multi-Agent Systems for Assisting Visually Impaired Users}


%%%%%%%%%%%%
This section examines the transition from human-human interactions to human-AI and AI-AI systems in supporting PVI. We explore how these multi-agent systems, which involve the collaborative efforts of multiple agents (AI or human), are designed to adaptively meet the diverse needs of PVI. 


Lee et al.~\cite{lee2020emerging} identified four contexts in which a professional human-assisted VQA system (Aira) offer support to PVI. The type of information required by PVI is incremental in these contexts. 
First, \textit{scene description} and \textit{object identification} acquire information about ``what is it.''
Second, \textit{navigation} requires description about PVI's surroundings and obstacles (``what is it'') and directional information (``where is it'' and ``how to get to the destination'').
Third, \textit{task performance} like putting on lipstick, cooking, and teaching a class. This context requires description (``what is it'') and domain knowledge on ``how to do it.'' 
% directional information (``where is the intended object'' and ``how to get it''), as well as
Forth, \textit{social engagement} like helping PVI in public spaces or interacting with other people. This needs description (``what is it''), directional information to navigate in social space, and discreet communication (PVI prefer not to disclose their use of VQA systems).


Our study reported how participants used \bma{} for tasks like matching outfits and assessing makeup, fitting under the category of \textit{task performance}. Some participants raised concerns about the accuracy of \sbma's interpretations and suggestions, indicating their preference for human subjectivity in this context. 
% 
Contrasting with this, Lee et al.'s work~\cite{lee2020emerging} highlighted that remote sighted assistants (RSAs), even those professionals RSAs from Aira, sometimes lack the specialized information or domain knowledge required in task performance, thereby they need to collaborate with other RSAs to find solutions. 


Furthering this investigation, Xie et al.~\cite{xie2023two} paired two RSAs to assist one visually impaired user in synchronous sessions, validating the need for RSAs to complement each other's description in task performance like aiding the user in applying makeup and matching outfits. They also explored the challenges in this human-human collaboration, revealing collaboration breakdowns between two opinionated RSAs. 
% 
To address these issues, they proposed a collaboration modality in which one ``silent'' RSA supports the other RSA by researching but not directly communicating. This approach suggested that two RSAs in this multi-agent system should not deliver information simultaneously but have a clear division of labor, designating who takes the lead, to avoid overwhelming PVI with information. 


% AI-RSA handoff (section 5.1) to AI-AI (GPT + expert AI)
Transitioning from human-human to human-AI collaboration, the handoff between the user, \bma{} and RSA (Section~\ref{handoff}) opens up new opportunities for multi-agent systems. 
% 
Our proposed modality of human-AI collaboration integrates the scalable, on-demand capabilities of AI-based visual assistance with the contextual understanding and adaptability of RSAs. 
This multi-agent system involves the AI system recognizing its own limitations and seamlessly handing off tasks to a RSA when appropriate. This collaboration aligns with prior work~\cite{xie2023two}, where AI (\bma) and human (RSA) maintain a clear division of labor, minimizing cumbersome back-and-forth and reducing potential confusion for PVI. 



Looking ahead, we envision the potential for AI-AI collaboration as part of the future multi-agent systems to assist PVI, especially for task performance. 
A domain-specific AI expert can be trained to handle more specialized tasks such as matching outfits, performing mathematical computations, or answering chemistry-related questions. \bma, as the core AI system, can provide general visual descriptions (``what is it'') and delegate more specialized tasks requiring domain knowledge to the domain-specific AI expert. 
% 
This approach is in line with the human-AI collaboration (Section~\ref{handoff}) by ensuring effective handoffs when necessary. By leveraging AI agents with more specialized capabilities, this multi-agent system can better adapt to PVI's needs. 



However, similar to concerns around human-human and human-AI interactions, these AI-AI collaborations must be carefully designed with clear protocols and handoff points for transitioning tasks between AI agents. It is important to make these transitions as seamless and transparent as possible to PVI, thereby avoiding any complexity or confusion. 




%%%%%%%%%%%
%%%%%%%%%%%
\subsection{Towards Real-Time Video Processing in LMM-based VQA Systems}


One of the most significant advantages of \bma{} and other LMM-based assistive tools is their ability to provide contextually relevant and personalized assistance to users. By leveraging machine learning and natural language understanding, these systems can understand and respond to a wide range of user queries. This level of contextual awareness represents a significant advancement over pre-LMM-based assistive technologies, which often fail to adapt to the diverse needs and preferences of individual users.


However, our findings also identified several challenges and limitations associated with the reliance on static images by current LMM-based assistive tools. 
Participants in our study reported frustration with the need to take multiple pictures to capture the desired information, a process they found time-consuming and cognitively demanding (Section~\ref{realtime_feedback}). This iterative process hinders efficiency and also poses safety risks, as participants struggled with taking images while navigating around obstacles.


% \rev{[real-time video process can also facilitate the transition from conveying ``what'' to ``how'' questions]}
To mitigate these issues, integrating real-time video processing capabilities into future LMM-based VQA systems could offer significant benefits. 
% 
Our findings suggest that the dynamic nature of video serves as a foundation for subsequent guidance (Section~\ref{realtime_feedback}), which is currently provided by human assistants through video-based remote sighted assistance.
% 
Shifting to real-time video processing would allow LMM-based VQA systems to transition from identifying objects (answering ``what is it'') to offering practical advice (addressing ``how to do it''), such as how to adjust the camera angle or how to navigate to a destination.
% 
By continuously analyzing the user's surroundings through real-time video feeds, these systems can dynamically interpret changes and provide immediate feedback, thus eliminating the need for static image captures. This capability would enhance the user experience by offering seamless navigation aid in real time. 


The feasibility of real-time video processing is supported by existing technologies demonstrated in commercial products and research prototypes. For instance, systems that utilize sophisticated algorithms for real-time object segmentation in video streams~\cite{wang2021swiftnet} have shown significant potential in other domains. Building on these techniques for video analysis could significantly extend the capabilities of future LMM-based VQA systems. 


Transitioning from static image analysis to real-time video processing can alleviate the burden of iteratively taking pictures and adjusting angles experienced by users. It can also enhance the utility and safety of LMM-based VQA systems, particularly during navigation. 
% 
This progression, driven by ongoing advancements in machine learning and computer vision, is essential for the development of more adaptive and responsive assistive technologies that align with the dynamic nature of real-world environments.




% Our findings highlight a limitation in the capabilities of \bma{} when it comes to providing actionable, goal-oriented guidance to PVI (Section~\ref{appliances}). While \bma{} is good at conveying ``what'' information with most of time accurately describing the visual content of a scene or object, such as identifying a thermostat on the wall. 
% 
% It often struggles with providing ``how'' information, guiding users on the specific actions required to interact with or operate elements in their environment, such as how to adjust the theromstat. This ``what'' and ``how'' divide poses a major challenge to the effectiveness and usability of \bma{}, as PVI rely on it not only for understanding their surroundings but also for completing tasks and achieving their goals.



% To address this limitation and design more goal-oriented AI-powered assistant prosthetics, we propose the following key design implications. 
% % 
% Future AI-powered assistive technologies should be designed with a focus on action-oriented reasoning and task-specific guidance. Although this could be achieved through further user inquiries~\cite{truhn2023large}, integrating knowledge bases~\cite{zhu2014reasoning} and event/behavior reasoning engines~\cite{chen2008using} to enable contextual inference of actions and intentions, and associating visual elements' feedback with reasoning, would greatly reduce the cognitive burden on PVI and enhance the user experience. By leveraging this knowledge, assistive technologies can provide more relevant and actionable guidance to PVI, helping them effectively navigate and interact with their environment to complete desired tasks. Our findings emphasize the importance of human-centered design principles, particularly in the design of assistive technologies, which should be reinforced through a goal-oriented technical roadmap that adapts to users' needs, preferences, and external environments~\cite{fischer2001user,amershi2014power}. By emphasizing action goal-oriented reasoning~\cite{huffman1993goal,letier2002agent}, future AI-powered prosthetics will be optimized, further benefiting PVI.






% reference about real-time image processing:
% By continuously analyzing the user's surroundings and providing relevant information without the need for explicit image capture, these systems could offer a more seamless and efficient user experience. The integration of \textcolor{red}{real-time image processing} into AI-powered VQA systems aligns with the growing availability of commercial products and research prototypes that leverage advanced object detection and text recognition technologies. For example, Microsoft's Seeing AI \cite{SeeingAI2020} and various currency recognition systems \cite{liu2008camera, parlouar2009assistive, paisios2012exchanging} demonstrate the feasibility and potential impact of real-time image processing in assistive technology. By building upon these existing approaches and incorporating state-of-the-art deep learning techniques for object detection \cite{girshick2014rich, girshick2015fast, ren2016faster, krizhevsky2017imagenet} and text recognition \cite{ma2018arbitrary, he2017deep, zhou2017east, yao2016scene, liu2017deep, lyu2018multi}, future AI-powered VQA systems could provide even more robust and reliable assistance to users.







% \subsection{Subjectivity Interpretations}
% % positive, identity
% know gender, race, humans have problem in identifying it, humans also guess but not make public mistake. not purely perceptual. discussion: neutral, conflict. tension between prior work and this one. only objective info and pvi will use social skills. intent in physical social gathering - limitations of prior work, digital interactions
% % https://dl.acm.org/doi/10.1145/3313831.3376404



% % negative, social cues
% [Discussion:] ai, human agency, undermine pvi as people, they can understand the social meanings. compare to human agents, subjectivity comes from ai no verification 








% \subsection{Design Implications for AI-Powered Assistant Prosthetics} %Albert
% % Design Implications: How AI-Powered Visual Question Answering Should Look Like
% As demonstrated by our research, \bma{} has shown significant potential in empowering PVI by providing a more intuitive, user-friendly, and context-aware assistive experience. However, to further enhance the usability and effectiveness of AI-powered VQA systems like \bma{}, several design implications should be considered.
% %lmm: large multi model 
% %
% %Suggest features that could improve \bma's usability, like real-time image processing to reduce the need for multiple pictures.
% %
% %Recommend design changes that might help \bma become more goal-oriented and contextually aware, such as advanced machine learning models trained on diverse environments.



% \subsubsection{Towards Goal-Oriented AI-Powered Assistant Prosthetics}

% %bma good at what information (``what'' is about the visual content) but not ``how'' information
% %
% %(Section~\ref{appliances}) recognize thermostat (what), but not adjust dial (how)

% Our findings highlight a limitation in the capabilities of \bma{} when it comes to providing actionable, goal-oriented guidance to PVI (Section~\ref{appliances}). While \bma{} is good at conveying ``what'' information with most of time accurately describing the visual content of a scene or object, such as identifying a thermostat on the wall. 
% % 
% It often struggles with providing ``how'' information, guiding users on the specific actions required to interact with or operate elements in their environment, such as how to adjust the theromstat. This ``what'' and ``how'' divide poses a major challenge to the effectiveness and usability of \bma{}, as PVI rely on it not only for understanding their surroundings but also for completing tasks and achieving their goals.

% To address this limitation and design more goal-oriented AI-powered assistant prosthetics, we propose the following key design implications. 
% % 
% Future AI-powered assistive technologies should be designed with a focus on action-oriented reasoning and task-specific guidance. Although this could be achieved through further user inquiries~\cite{truhn2023large}, integrating knowledge bases~\cite{zhu2014reasoning} and event/behavior reasoning engines~\cite{chen2008using} to enable contextual inference of actions and intentions, and associating visual elements' feedback with reasoning, would greatly reduce the cognitive burden on PVI and enhance the user experience. By leveraging this knowledge, assistive technologies can provide more relevant and actionable guidance to PVI, helping them effectively navigate and interact with their environment to complete desired tasks. Our findings emphasize the importance of human-centered design principles, particularly in the design of assistive technologies, which should be reinforced through a goal-oriented technical roadmap that adapts to users' needs, preferences, and external environments~\cite{fischer2001user,amershi2014power}. By emphasizing action goal-oriented reasoning~\cite{huffman1993goal,letier2002agent}, future AI-powered prosthetics will be optimized, further benefiting PVI.
 


% \subsubsection{Towards Real-Time Processing AI-Powered Assistant}
% One of the most significant advantages of \bma{} and other LMM-based assistive tools is their ability to provide contextually relevant and personalized assistance to users. By leveraging the power of machine learning and the ability for understanding of natural language, these systems can understand and respond to a wide range of user queries. This level of contextual awareness represents a significant advancement over traditional assistive technologies, which often struggle to adapt to the diverse needs and preferences of individual users.

% However, our findings also identify several challenges and limitations of current LMM-based assistive tools, particularly in terms of their reliance on user-generated images. Participants in our study reported frustration with the need to take multiple pictures to capture the desired information, which can be time-consuming and cognitively demanding (Section~\ref{navigation}). To address this issue, we envision the integration of real-time image processing capabilities into future AI-powered VQA systems. By continuously analyzing the user's surroundings and providing relevant information without the need for explicit image capture, these systems could offer a more seamless and efficient user experience. The integration of real-time image processing into AI-powered VQA systems aligns with the growing availability of commercial products and research prototypes that leverage advanced object detection and text recognition technologies. For example, Microsoft's Seeing AI \cite{SeeingAI2020} and various currency recognition systems \cite{liu2008camera, parlouar2009assistive, paisios2012exchanging} demonstrate the feasibility and potential impact of real-time image processing in assistive technology. By building upon these existing approaches and incorporating state-of-the-art deep learning techniques for object detection \cite{girshick2014rich, girshick2015fast, ren2016faster, krizhevsky2017imagenet} and text recognition \cite{ma2018arbitrary, he2017deep, zhou2017east, yao2016scene, liu2017deep, lyu2018multi}, future AI-powered VQA systems could provide even more robust and reliable assistance to PVI.



% \subsubsection{Towards Reliable AI-Powered Assistant Prosthetics}



% Our findings highlight the significant potential of AI-Powered assistive technologies like \bma{} in enhancing the perception and understanding of surroundings for PVI. However, our study also reveals a notable drawback of AI-powered assistant prosthetics, namely AI hallucinations. These errors, where the artifact inaccurately identifies objects that aren't present, can lead to confusion and mistrust among users. Participants in our study reported instances where \bma{} erroneously added non-existent details to scenes (Section~\ref{scene}). 
% It can be argued that the presence of AI hallucinations poses a major challenge to the reliability and robustness of AI-powered assistive prosthetics. If users cannot trust the information provided by these systems, their effectiveness as cognitive extensions is severely compromised. This issue is particularly critical for PVI, who rely on these technologies to navigate and make sense of their environment.

% To address the problem of AI hallucinations, our participants applied various strategies, such as consulting human assistants for verification or relying on their own knowledge to identify inaccuracies. While these strategies showcase PVI's adaptability and problem-solving skills, they also highlight the need for more reliable and robust AI-powered assistive technologies.

% One potential approach to mitigating AI hallucinations is to incorporate uncertainty estimation and communication into the design of these technologies. By quantifying and conveying the confidence level of predictions, AI-powered assistive systems can help users assess the reliability of the information provided. This approach has been explored in the context of other AI-based systems, such as medical diagnosis~\cite{leibig2017leveraging,begoli2019need} and autonomous vehicles~\cite{michelmore2018evaluating}.

% Another strategy is to develop AI-powered assistive technologies that can learn and adapt to user feedback over time. By allowing PVI or human assistants to correct errors and provide input to improve system performance, which is not only able to continuously improve system's reliability and robustness and also help aware users about possible AI hallucinations. This approach aligns with the principles of interactive machine learning, emphasizing the importance of human-in-the-loop learning for AI systems~\cite{amershi2014power,retzlaff2024human}.

% In addition to these technical solutions, involving PVI in the design and evaluation of AI-powered assistive technologies is crucial. By engaging users as co-designers and co-evaluators, researchers and designers can gain a better understanding of the challenges and requirements of PVI, leading to the development of more reliable and robust systems. This participatory design approach has been widely advocated in the assistive technology domain~\cite{frauenberger2015pursuit,lee2004trust,zhang2023redefining}.




% \subsubsection{Envisioning Multimodal AI-Powered Assistant Prosthetics} 

% %integrate multimodal input, participants can use gestures, touch, haptic feedback to interact with \bma. 

% %llm

% Our research demonstrates the superior ability and performance of \bma{} compared to previous AI-powered systems when handling various tasks. In tasks such as object recognition, processing complex information, and interpreting graphical elements, \bma{} not only completes the tasks but also provides extended explanations and further task collaboration capabilities. 
% This undoubtedly greatly expands the interaction scenarios and user experience of \bma{}. 

% However, as mentioned earlier, the current \bma{} faces challenges, including higher interaction costs due to multiple photo captures and AI hallucinations.
% % previously mentioned, we found that the current \bma{} still has some challenges and limitations in terms of interaction experience, such as the additional interaction costs caused by the need for multiple photo captures and AI hallucinations.
% % 
% Apart from integrating additional advanced technologies to expand \bma{}'s capabilities, we also suggest adopting multimodal interaction. This approach, a key design insight from our research, underscores the value of diverse interactions and feedback in AI-powered assistive tools.
% % apart from integrating other advanced technologies to expand \bma{}'s capabilities, we suggest introducing the concept of multimodal interaction, which is another key design implication derived from our research, emphasizing the importance of multimodal interaction and feedback in artificial intelligence assistive tools. 
% % 
% Although \bma{} primarily relies on voice-based input and output, integrating multimodal interaction methods, such as touch, gestures, and haptic feedback, will greatly enhance the usability and accessibility of LMM-based assistive functions. This perspective of combining multimodal interaction has been widely studied in some previous literature \cite{turk2014multimodal, reeves2004guidelines}.
% % oviat2017handbook
% For example, touch-based gestures can enable users to navigate the system interface more effectively, while haptic feedback can provide additional spatial and contextual information to supplement voice output. These multimodal interactions also allow \bma{} to adapt and personalize to better meet the diverse needs of PVI. Future research should further explore how to design and implement these multimodal interaction technologies and assess their long-term impact on PVI user experience and quality of life.

% %Our research demonstrates the superior ability and performance of \bma{} compared to previous systems and tools when handling various tasks. For instance, in tasks such as object recognition, processing complex information, and interpreting graphical elements, \bma{} not only accomplishes the tasks but also provides \textcolor{red}{multimodal input,} extended explanations, and further task collaboration capabilities. This undoubtedly greatly expands the interaction scenarios and user experience of \bma. Traditional applications and tools, such as those using conventional OCR technology and rapid reading applications like ``Seeing AI,'' are considered effective. \textcolor{red}{[cite:] However, users find it challenging to process a complete task flow using such tools, as their functions are designed for single tasks.} In other words, traditional applications and tools can only complete a specific step in a task flow and cannot connect the context of the entire task. Leveraging the LMM's ability to understand multimodal data, interact using natural language, and be context-sensitive, \bma{} can comprehend more practical needs and provide personalized assistance to users based on their requirements. Furthermore, \bma{} demonstrates its potential to replace previous assistive methods and technologies. \textcolor{red}{This is particularly evident in a series of scenarios where \bma{} substitutes RSA services, such as when participants use \bma{} for reading and control tasks. These tasks, which are considered replaceable by \bma{}, are common needs of PVI in RSA services. }

% %Another key design implication that emerged from our research is the importance of multimodal interaction and feedback in AI-powered assistive tools. While \bma{} primarily relies on voice-based input and output, \textcolor{red}{participants expressed interest in exploring other interaction modalities, such as touch, gestures, and haptic feedback.=> integrate other interaction modalities, such as touch, gestures, and haptic feedback, into \bma} The incorporation of multimodal interaction techniques, which have been extensively studied in the HCI literature \cite{turk2014multimodal, reeves2004guidelines, oviatt2017handbook}, could greatly enhance the usability and accessibility of LMM-based assistive tools. For example, touch-based gestures could enable users to navigate the system's interface more efficiently, while haptic feedback could provide additional spatial and contextual information to supplement voice output.

%\subsubsection{future ai assistant prosthetic}
%from 5.4, about design process? 
%
%integrate new tech: generative ai 
%next iteration




% \subsubsection{Opportunities in Human-AI Collaboration}
% % Evolution of Human-AI Collaboration
% % Future AI Assistant Prosthetic - From Temporal Dimension to Consider Design Implications
% Again, while our findings demonstrate the significant potential of AI-powered assistive technologies in enhancing PVI's capabilities across various contexts, participants also highlighted some limitations of these technologies. However, we argue that these limitations may be part of an adjustment period as PVI learn to interact with and adapt to AI-powered assistive technologies.


% This observation aligns with the findings of~\citet{10.1145/3563657.3595977}, who described how users, when initially using AI-powered assistive technologies, may encounter responses that do not match their expectations. Despite this, most participants were able to quickly learn and adapt to the system, reaching a point where they could collaborate with \bma{} to complete tasks and perceive it as an effective technology.

% This contradictory state may give rise to a form of Human-AI ``confrontation.'' However, this ``confrontation'' appears to diminish as the system undergoes iterative upgrades and PVI update their understanding of the system's capabilities. From a phenomenological perspective, Ihde's theory~\cite{ihde1991instrumental} suggests that the science of tools evolves alongside people's cognition, implying that the relationship between PVI and AI-powered assistive technologies is not static but rather a dynamic process of mutual adaptation and growth.


% As PVI become more familiar with the capabilities and limitations of AI-powered assistive technologies, they develop strategies to leverage these technologies effectively and compensate for the shortcomings. This process of adaptation and learning is a critical aspect of the distributed cognition framework, which emphasizes the role of artifacts and the environment in shaping cognitive processes \cite{hollan2000distributed}.


% Moreover, the iterative nature of AI-powered assistive technologies, with ongoing updates and improvements, allows for a continuous refinement of the Human-AI interaction. As these technologies become more sophisticated and attuned to the needs and preferences of PVI, the initial ``confrontation'' may give way to a more seamless and synergistic collaboration between PVI and AI-powered assistive technologies.

% This perspective highlights the importance of considering the temporal dimension of design implications in the context of AI-powered assistive technologies. Rather than viewing the limitations of these technologies as static barriers, we should recognize the potential for PVI to adapt and develop new cognitive strategies in response to these limitations, and for the technologies themselves to evolve in response to user feedback and needs.


%%%%%%%%%%
% \subsubsection{Ethical Considerations and User Involvement in the Design of AI-Powered Assistive Technologies}
% As we continue to explore the potential of AI-powered assistive technologies for PVI, it is crucial to involve the target users in the design and evaluation process \cite{10.1145/3025453.3025899}. Our research with \bma{} underscores the importance of actively engaging with PVI throughout the design process to ensure that these systems are tailored to their specific needs, preferences, and contexts of use. By involving users as co-designers and co-evaluators, we can create assistive tools that are not only technologically advanced but also truly empowering and inclusive.

% Moreover, the development of AI-powered assistive technologies should be guided by a commitment to ethical and responsible innovation~\cite{floridi2018ai4people,jobin2019global}. As LMM-based systems become increasingly sophisticated and integrated into users' daily lives, it is essential to consider issues of privacy, security, and fairness. Researchers and designers should ensure these technologies are transparent, accountable, and aligned with the values and goals of the communities they serve~\cite{10.1145/3351095.3372873}.

% In conclusion, our research with \bma{} highlights the immense potential of LMM-based assistive tools for PVI while also revealing key design implications for future AI-powered VQA systems. By integrating real-time image processing, supporting multimodal interaction and feedback, and prioritizing user-centered design principles, we can create assistive technologies that are more usable, effective, and empowering. As the field of AI-powered assistive technology continues to evolve, it is essential for HCI researchers and practitioners to collaborate with PVI's communities to ensure that these innovations are developed in an inclusive, ethical, and responsible manner.



%\subsection{\textcolor{red}{Most assistive technologies die after their introduction}}
%
%\textcolor{red}{[combine with design implications, with sub-headings]}

%Assistive technologies for PVI have undergone a rapid evolution in recent years~\cite{10.1145/3597638.3608412}, with the introduction of LLM-based applications like \bma{} marking a significant shift in the landscape. \textcolor{red}{Traditional assistive technologies, such as VQA}~\cite{bigham2010vizwiz_nearly,BeMyEyes2020}, screen readers and braille displays~\cite{muhsin2024review,alves2009assistive}, have long been the primary methods for blind individuals to access the content and navigate the world. However, assistive technology is undergoing a transition from human-driven to AI-driven~\cite{xu2023transitioning, 10.1145/3234695.3239330}, and the emergence of a series of technology-dominated assistive services has challenged the dominance of these traditional technologies, offering a more intuitive, user-friendly, and versatile experience for blind users. Our research further reveals that \bma{} has elevated AI-led assistive technology to new heights, which inevitably leads one to imagine the possibility of a future where assistive technology is fully AI-dominated.
%
%Despite the initial promise and potential of many traditional assistive technologies, a significant number of them gradually lost their appeal after the emergence of applications like \bma{}, and may eventually die out after users fully adopt \bma{}. This phenomenon can be attributed to several factors, including the lack of comprehensive functionality, high complexity of operation, social difficulties and limited usage scenarios~\cite{gori2016devices,manjari2020survey,tapu2020wearable}. These technologies failed to meet the evolving needs and expectations of blind users in one or more aspects, while \bma{} seems to offer a promising solution that integrates and expands the capabilities of various traditional assistive technologies.
%
%
%While \bma{} is not perfect at present, the introduction of \bma{} represents a significant milestone in the development of assistive technologies for blind individuals. These applications offer new possibilities and empower PVI in unprecedented ways. Building on this foundation, it is crucial to approach their development and adoption from a user-centered perspective, ensuring that they are accessible, inclusive, and responsive to the diverse needs of the blind community~\cite{10.1145/3025453.3025895,federici2012assistive}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paragraph can be removed.
%The rise and fall of traditional assistive technologies, coupled with the emergence of LLM-based applications \bma{}, offer valuable lessons for the future development and adoption of assistive tools for PVI. First and foremost, user-centered design and continuous adaptation must be at the heart of assistive technology development to ensure that these tools truly meet the evolving needs and expectations of blind users. Second, the success of LLM-based applications highlights the importance of leveraging advanced technologies, such as natural language processing and machine learning, to create more intuitive and versatile assistive tools. Finally, the challenges and concerns surrounding the adoption of LLM-based applications underscore the need for ongoing research, collaboration, and dialogue among researchers, developers, and blind users to ensure that these technologies are developed and deployed in an accessible, inclusive, and responsible manner.


%The introduction of \bma{} represents a significant milestone in the evolution of assistive technologies for blind people. While these applications offer new possibilities and empower blind individuals in unprecedented ways, it is crucial to approach their development and adoption with a user-centered perspective, ensuring that they are accessible, inclusive, and responsive to the diverse needs of the blind community. As the field of assistive technology continues to evolve, researchers and developers must remain committed to creating tools that truly enhance the lives of blind individuals and promote their full participation in the digital world. %The decline of traditional assistive technologies in the face of LLM-based applications serves as a reminder of the importance of continuous innovation, adaptation, and user-centered design in the quest to create a more accessible and inclusive world for all.



