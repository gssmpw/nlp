\section{Related Work}
Retrievers often fetch noisy content, reducing output accuracy, while overly long contexts further hinder model efficiency. To address these challenges, some researchers utilize re-ranking methods to prioritize more relevant sentences. 
RichZhang, "Ranking-based Retrieval for Open-Domain Question Answering" uses a generative list-wise ranker to generate and rank candidate documents, ensuring the answer is comprehensive and aligns with the modelâ€™s preferences.  Liang et al., "Bridge: A Novel Mechanism to Optimize the Connection between Retriever and LLM in Retrieval-Augmented Generation" proposes a novel bridge mechanism to optimize the connection between retrievers and LLMs in retrieval-augmented generation, improving performance in question-answering and personalized generation tasks.
However, reranking sentences may disrupt the original logical structure of the document and generate unfaithful clues.


Other researchers utilize abstractive or extractive summarization models to identify query-relevant answer clues.  Lewis et al., "Pre-training enables online learning of entity-centric question answering" propose leveraging LLMs as abstractive filters to compress retrieved text by targeting the most relevant sentences.  See et al., "MASS: Masked Sequence to Coordinated Action Prompt Tuning for Abstractive Summarization" apply the information bottleneck principle to filter noise, striving to strike a balance between conciseness and correctness. Despite its potential benefits, this method is associated with high computational complexity during the training process, posing additional challenges for practical implementation.  Wang et al., "DynaML: Dynamic Multitask Learning for Efficient Text Generation" explore extractive filters to select the most relevant sentences. While these methods help eliminate irrelevant information, they also face the risk of over-compression, which may lead to a reduction in output accuracy.  In another approach, Feng et al., "Selective Context: Reducing Redundancy in Long Document for Efficient Language Model Inference" introduce the concept of Selective Context, which eliminates redundant content based on self-information metrics to enhance the efficiency of LLM inference. However, this technique may compromise the semantic coherence of the context.