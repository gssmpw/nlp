@inproceedings{ke-etal-2024-bridging,
    title = "Bridging the Preference Gap between Retrievers and {LLM}s",
    author = "Ke, Zixuan  and
      Kong, Weize  and
      Li, Cheng  and
      Zhang, Mingyang  and
      Mei, Qiaozhu  and
      Bendersky, Michael",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.562/",
    doi = "10.18653/v1/2024.acl-long.562",
    pages = "10438--10451",
}

@inproceedings{li2023compressing,
    title = "Compressing Context to Enhance Inference Efficiency of Large Language Models",
    author = "Li, Yucheng  and
      Dong, Bo  and
      Guerin, Frank  and
      Lin, Chenghua",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.391/",
    doi = "10.18653/v1/2023.emnlp-main.391",
    pages = "6342--6353",
}

@inproceedings{wang-etal-2025-richrag,
    title = "{R}ich{RAG}: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation",
    author = "Wang, Shuting  and
      Yu, Xin  and
      Wang, Mang  and
      Chen, Weipeng  and
      Zhu, Yutao  and
      Dou, Zhicheng",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.750/",
    pages = "11317--11333",
    abstract = "Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator`s preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM`s preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users."
}

@article{wang2023learning,
  title={Learning to filter context for retrieval-augmented generation},
  author={Wang, Zhiruo and Araki, Jun and Jiang, Zhengbao and Parvez, Md Rizwan and Neubig, Graham},
  journal={arXiv preprint arXiv:2311.08377},
  year={2023}
}

@inproceedings{xu2024recomp,
  title={RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation},
  author={Xu, Fangyuan and Shi, Weijia and Choi, Eunsol},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{zhu2024information,
    title = "An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation",
    author = "Zhu, Kun  and
      Feng, Xiaocheng  and
      Du, Xiyuan  and
      Gu, Yuxuan  and
      Yu, Weijiang  and
      Wang, Haotian  and
      Chen, Qianglong  and
      Chu, Zheng  and
      Chen, Jingchang  and
      Qin, Bing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.59/",
    doi = "10.18653/v1/2024.acl-long.59",
    pages = "1044--1069",
    abstract = "Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with 2.5{\%} compression rate."
}

