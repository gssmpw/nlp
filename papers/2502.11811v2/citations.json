[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang-etal-2025-richrag",
        "author": "Wang, Shuting  and\nYu, Xin  and\nWang, Mang  and\nChen, Weipeng  and\nZhu, Yutao  and\nDou, Zhicheng",
        "title": "{R}ich{RAG}: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ke-etal-2024-bridging",
        "author": "Ke, Zixuan  and\nKong, Weize  and\nLi, Cheng  and\nZhang, Mingyang  and\nMei, Qiaozhu  and\nBendersky, Michael",
        "title": "Bridging the Preference Gap between Retrievers and {LLM}s"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xu2024recomp",
        "author": "Xu, Fangyuan and Shi, Weijia and Choi, Eunsol",
        "title": "RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhu2024information",
        "author": "Zhu, Kun  and\nFeng, Xiaocheng  and\nDu, Xiyuan  and\nGu, Yuxuan  and\nYu, Weijiang  and\nWang, Haotian  and\nChen, Qianglong  and\nChu, Zheng  and\nChen, Jingchang  and\nQin, Bing",
        "title": "An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xu2024recomp",
        "author": "Xu, Fangyuan and Shi, Weijia and Choi, Eunsol",
        "title": "RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation"
      },
      {
        "key": "wang2023learning",
        "author": "Wang, Zhiruo and Araki, Jun and Jiang, Zhengbao and Parvez, Md Rizwan and Neubig, Graham",
        "title": "Learning to filter context for retrieval-augmented generation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2023compressing",
        "author": "Li, Yucheng  and\nDong, Bo  and\nGuerin, Frank  and\nLin, Chenghua",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models"
      }
    ]
  }
]