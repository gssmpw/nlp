\section{Related Work}
\label{sec:Related Work}
Most efforts in accelerating ViT model inference are concentrated on model optimization. 
Optimization methods, such as knowledge distillation\cite{hao2022learning}, pruning\cite{pan2021iared,tang2022patch,xu2022evovit,yu2022width,zheng2022savit}, quantization\cite{li2023psaqvit}, neural architecture 
search\cite{you2022shiftaddnas}, 
and designing lightweight networks\cite{yang2022lite} have been developed to optimize the inference phase of ViT models. These methods aim to reduce the computational complexity and memory demands of the model, leading to faster inference. 
However, these models and techniques only work in local device environments. 
They cannot be directly applied to the collaborative inference setting and do not fully exploit the potential of resource-rich cloud servers to accelerate inference further. 

Cloud-device collaborative inference enables the partitioned deployment of the model between the device and cloud in CNN architectures\cite{kang2017neurosurgeona,song2018insitu,jeong2018computation,hu2019dynamic,zeng2019boomerang,li2020edge,laskaridis2020spinna,ren2021finegrained,yang2022cnnpc,zhang2021cloudedgeb}.
Fundamentally, the benefit of cloud-device collaboration for inference acceleration relies on data reduction during the inference phase, leading to a shorter communication latency.  Unlike the typical CNN architecture, the unique transformer architecture lacks this property, rendering the existing cloud-device collaboration efforts inapplicable. 
In our work, we introduce the first cloud-device collaborative inference framework for emerging ViTs.