\section{Related Work}
\textbf{Large Multimodel Models}
has emerged as a powerful paradigm for integrating language and non-language modalities, such as images~\cite{radford2021learning}, audio~\cite{ao2021speecht5}, and video~\cite{zellers2021merlot}, to perform complex reasoning tasks. 
Recent advancements have been driven by the fusion of pretrained LLMs with multimodal encoders~\cite{peng2023kosmos,tsimpoukelli2021multimodal,alayrac2022flamingo}, enabling the models to process and generate cross-modal content effectively. 
Works such as Flamingo~\cite{alayrac2022flamingo} and BLIP-2~\cite{li2023blip} demonstrated the potential of aligning vision and language modalities through carefully designed bridging modules. 
However, these models often fall short in richness or alignment with the reasoning capabilities of LLMs~\cite{bubeck2023sparks,bommasani2021opportunities}. 
To address this, techniques have been proposed, such as contrastive pretraining~\cite{radford2021learning,yuan2021multimodal} and feature fusion mechanisms~\cite{lu2019vilbert}.
Yet, efficiently capturing diverse modal interactions across different tasks remains a bottleneck~\cite{baltruvsaitis2018multimodal}, highlighting the need for more adaptive mechanisms in multimodal reasoning.\looseness-1


\textbf{Mixture-of-Experts} has become a prominent architectural choice to enhance the scalability and efficiency of large-scale neural networks~\cite{shazeer2017outrageously}.
By dynamically selecting a subset of specialized expert modules for each input~\cite{li2023simple}, MoE reduces computational overhead while maintaining high expressive power~\cite{shazeer2017outrageously,zoph2022designing}.
In the context of LLMs, MoE has been shown to improve both training efficiency and generalization across tasks~\cite{artetxe2019massively}.
Works such as Switch Transformers~\cite{fedus2022switch} and GShard~\cite{lepikhin2020gshard} have demonstrated the effectiveness of MoE in scaling up model capacity without prohibitive increases in training costs.
In multimodal settings, MoE has been explored to address the modality alignment problem~\cite{goyal2021coordination}, where different experts handle distinct modalities or specific tasks. 
However, the optimal utilization of experts heavily relies on the effectiveness of routing mechanisms, which remains an active area of research.\looseness-1

\textbf{Routers and Routing Strategies} 
are the cornerstone of any MoE-based architecture, responsible for determining which experts are activated for each input~\cite{li2024your}.
Traditional routers, such as softmax gating functions~\cite{shazeer2017outrageously}, compute a weighted combination of experts based on input embeddings.
Despite their simplicity, these routing strategies often face challenges in achieving optimal expert assignment~\cite{lepikhin2020gshard,zoph2022designing}, particularly in unseen or highly diverse test scenarios.
Recent works have proposed advanced routing strategies, including routing via reinforcement learning~\cite{rosenbaum2017routing}, early-exit~\cite{li2023towards}, and task-specific  allocation~\cite{shi2024eagle}.
However, these approaches typically focus on training-time optimization, leaving test-time adaptability largely unexplored.
\ours introduces an efficient method to refine routing weights dynamically during inference, ensuring better alignment with task-specific requirements and improving overall model robustness across diverse multimodal benchmarks.\looseness-1


\textbf{Test-Time Optimization} has been explored by adapting models dynamically during inference to improve generalization.
For example, ~\cite{wang2022continual} propose test-time adaptation, which fine-tunes model parameters on test data distributions using entropy minimization or self-supervised learning.
Similarly, ~\cite{sun2020testtimetrainingselfsupervisiongeneralization} introduce test-time training, where models are updated via auxiliary tasks (e.g., rotation prediction) during inference.
However, these methods require modifying the base model's parameters, leading to significant computational overhead and potential instability when deployed on resource-constrained systems.
Unlike prior test-time optimization methods that update model weights, \ours solely optimizes the routing weights of a frozen MoE model without retraining any model parameters.