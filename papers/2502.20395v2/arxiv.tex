\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{hyperref}
\usepackage{xcolor}

\hypersetup{
    pdftitle={R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts},
    pdfauthor={Zhongyang Li, Ziyue Li, Tianyi Zhou},
    pdfsubject={arXiv preprint}, 
    pdfkeywords={Machine Learning, Multimodal Learning, Mixture-of-Experts},
    bookmarksnumbered=true,
    bookmarksopen=true,
    colorlinks=true,
    citecolor=violet,
    linkcolor=brown,
    urlcolor=blue
}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{xcolor}
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}      
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{times}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{float}
\usepackage{stfloats} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{rotating}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{multicol}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{arydshln}
\usepackage{placeins}
\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}
\newcommand{\ours}{R2-T2\xspace}
\newcommand{\tianyi}[1]{\textcolor{magenta}{Tianyi: #1}}
\newcommand{\ziyue}[1]{\textcolor{orange}{Ziyue: #1}}
\newcommand{\zhongyang}[1]{\textcolor{blue}{Zhongyang: #1}}

\icmltitlerunning{\ours: Re-Routing in Test-Time for Multimodal Mixture-of-Experts}

\begin{document}

\twocolumn[
\icmltitle{\ours: Re-Routing in Test-Time for Multimodal Mixture-of-Experts}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhongyang Li}{jhu}
\icmlauthor{Ziyue Li}{umd}
\icmlauthor{Tianyi Zhou}{umd}
\end{icmlauthorlist}

\vspace{-0.2em}
\begin{center}
\textsuperscript{1}Johns Hopkins University; \textsuperscript{2}University of Maryland, College Park\\
\texttt{zli300@jh.edu, \{litzy619,tianyi\}@umd.edu}
\end{center}




\vspace{-1em}
\begin{center}
\textcolor{cyan}{Project: \url{https://github.com/tianyi-lab/R2-T2}}
\end{center}


\icmlcorrespondingauthor{Tianyi Zhou}{tianyi@umd.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\begin{abstract}
In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. 
This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. 
The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. 
However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method ``\textbf{R}e-\textbf{R}outing in \textbf{T}est-\textbf{T}ime (\ours)'' that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three \ours strategies with different optimization objectives and neighbor-search spaces. \ours consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.%\looseness-1 

\end{abstract}

\section{Introduction}

\begin{figure}[htbp]
\vspace{-0.5em}
\hspace{-0.5em}
\includegraphics[width=\linewidth]{fig/radar.png}
\vspace{-1em}
\caption{\ours applied to MoAI-7B compared against 7/8/13B VLMs on 9 benchmarks. \ours significantly enhances performance of the 7B base MoE model, surpassing a recent 13B VLM.}
\label{fig:radar}
\vspace{-1.5em}
\end{figure}
Mixture-of-Experts (MoE) have achieved remarkable success in scaling up the size and capacity of large language and multimodal models (LLMs and LMMs)~\cite{shazeer2017outrageously} without (significantly) increasing the inference cost. Specifically, it allows us to increase the total number of experts, which provides finer-grained expertise and skills, yet selecting a constant number of experts for each input~\cite{lepikhin2020gshard}. In MoE, the sparse selection of experts is achieved through a router, which determines the weight of each candidate expert based on the input so only experts with nonzero weights are selected~\cite{fedus2022switch}. MoE then aggregates the outputs of the selected experts according to their weights. Hence, the router and its produced routing weights play important roles in MoE's inference cost and output quality.\looseness-1

As the most widely studied LMM, many vision language models (VLM) adopt an architecture composed of a vision encoder and an LLM~\cite{zhu2023minigpt}, which are both pre-trained and then aligned by further finetuning so the LLM can include the vision encoder's output in its input as additional tokens. The alignment is usually obtained through a lightweight projection layer or Q-former (a Transformer model) converting the vision encoder's output to LLM tokens. Despite the broad usage of this architecture, the capability of a vision encoder is usually much more limited than the LLMs (i.e., the ``\textit{modality imbalance}'')~\cite{schrodi2024two}. So the visual features cannot cover all the information required by different reasoning tasks performed by LLMs. Moreover, the alignment module may lead to an information bottleneck from the visual perception to the reasoning~\cite{yao2024deco}. 

\begin{figure*}[ht]
\centering
\setlength{\abovecaptionskip}{10pt}
% \resizebox{\linewidth}{!}{%
 \includegraphics[width=0.98\textwidth]{fig/example.png}
% }
\vspace{-1em}
 \caption{An example of how \ours optimizes the routing weights. Given the test sample, it finds $k$NN in the reference set of correctly predicted samples with similar questions. In the example, the test sample requires reasoning about positional relationships.
 \ours identifies relevant kNN samples, adjusting the top-1 expert from $\mathbf{I}_{\textsc{lang}}$ (aligning visual features with language) to $\mathbf{I}_{\textsc{aux}}$ (aligning visual features with auxiliary computer vision features).
 This expert shift is crucial in correcting the initial wrong answer.
 }
\label{fig:example}
\vspace{-1em}
\end{figure*}
Recent advances in LMMs replace a single vision encoder with a mixture of encoders~\cite{lin2024moma, lee2025moai, zong2024mova, shi2024eagle}, which turns out to be an effective and low-cost approach to mitigate \textit{modality imbalance and alignment bottleneck}. In multimodal MoE, each expert is an encoder or a mixer of sensory inputs that focuses on a specific type of features, e.g., object classes, text in images, spatial relations, dense captions, segmentation, etc., so the LLM can select the information acquired by any given downstream task from the concatenated or fused features from the MoE, through a router that is trained in an end-to-end manner to produce the weights of all the experts adaptive to the input task. 

Although multimodal MoE achieves remarkable success in enhancing the performance of existing LMMs, the choice of experts or the routing weights for individual instances are not always optimal due to the limitations of the router's design and the diversity of potential downstream tasks compared to the tasks used to train the router. The suboptimality of routing substantially limits the performance and generalization of multimodal MoE on unseen tasks.
As illustrated in Figure~\ref{fig:example}, the base model initially selects a sub-optimal expert (e.g., $\mathbf{I}_{\textsc{lang}}$) for a spatial reasoning task, leading to incorrect predictions.
This has been verified on recent multimodal MoE models. As shown in Table~\ref{tab:method_comparison_mova}, compared to the original routing weights of base models, the optimal (oracle) routing weights improve the accuracy by $\geq 10\%$ on most evaluated LMM benchmarks. To avoid the expensive cost of re-training a router on a much larger dataset, in this paper, we investigate \textit{how to improve the routing weights in test-time without training any model parameters.} 

Since routing weights encode the choices of experts with essential knowledge and key skills acquired by the input task, and motivated by the assumption that knowledge and skills are usually transferable across different tasks, we posit that the routing weights of successful tasks can provide critical clues for optimizing the routing weights of a new task. 
Specifically, we leverage the similarity in a task embedding space, which may reflect the knowledge or skill sharing between tasks, and modify the routing weight vector of a test task by imitating its nearby successful tasks. While the task embedding space, optimization objective, and the number of update steps can vary and their design choices may result in different performances, this innovative mechanism of \textbf{optimizing routing weights or ``re-routing'' in test-time (\ours)} focuses on correcting the mistakes made by the routers in existing multimodal MoE, e.g., extracting object detection features for a task mainly depending on the text information in an input image, and thus turns various failed cases into success. Rather than finetuning the whole model, \ours is training-free and aims to maximize the potential of MoE in the reasoning tasks by LMMs. 

Following the above idea, we explored several novel strategies for test-time routing weight optimization. They all modify the routing weights of a test task/sample based on a representative set of tasks/samples on which the multimodal MoE achieves correct or high-quality outputs. 
While the oracle routing weights are achieved by minimizing the test sample's loss, for a practical approach, we propose to replace the oracle loss with a surrogate, i.e., a weighted average of losses of nearby reference samples, and apply multiple steps of ``\textbf{neighborhood gradient descent (NGD)}'' to minimize the surrogate. 
In addition, we investigate kernel regression and mode finding, which do not require gradient descent. The former moves the routing weights to a kernel-weighted sum of nearby reference tasks' routing weights in a task embedding space, while the latter moves the routing weights to the nearest mode on the distribution of reference tasks' routing weights.
Evaluating these strategies on two recent multimodal MoE models across eight challenging benchmarks, we find that \ours significantly outperforms models twice its size, as shown in Figure~\ref{fig:radar}.
Our analysis reveals that NGD progressively refines routing, increasing correct predictions while mitigating the original router's over-reliance on a single expert.
Case studies confirm that test-time re-routing enhances domain-specific reasoning, demonstrating \ours's ability to adapt multimodal MoE models without additional training, unlocking greater generalization and robustness.

Our main contributions can be summarized below:
\begin{itemize}[leftmargin=1em, itemsep=0.1em]
\vspace{-0.5em}
    \item We proposed a novel problem of \ours that bridges a significant performance gap on multimodal MoE.
    \item We developed three practical \ours strategies that shed several critical insights into expert re-routing. 
    \item Our \ours considerably advances the performance of multimodal MoE on several recent benchmarks of challenging tasks for LMMs. 
\end{itemize}

\section{Related Work}

\textbf{Large Multimodel Models}
has emerged as a powerful paradigm for integrating language and non-language modalities, such as images~\cite{radford2021learning}, audio~\cite{ao2021speecht5}, and video~\cite{zellers2021merlot}, to perform complex reasoning tasks. 
Recent advancements have been driven by the fusion of pretrained LLMs with multimodal encoders~\cite{peng2023kosmos,tsimpoukelli2021multimodal,alayrac2022flamingo}, enabling the models to process and generate cross-modal content effectively. 
Works such as Flamingo~\cite{alayrac2022flamingo} and BLIP-2~\cite{li2023blip} demonstrated the potential of aligning vision and language modalities through carefully designed bridging modules. 
However, these models often fall short in richness or alignment with the reasoning capabilities of LLMs~\cite{bubeck2023sparks,bommasani2021opportunities}. 
To address this, techniques have been proposed, such as contrastive pretraining~\cite{radford2021learning,yuan2021multimodal} and feature fusion mechanisms~\cite{lu2019vilbert}.
Yet, efficiently capturing diverse modal interactions across different tasks remains a bottleneck~\cite{baltruvsaitis2018multimodal}, highlighting the need for more adaptive mechanisms in multimodal reasoning.\looseness-1


\textbf{Mixture-of-Experts} has become a prominent architectural choice to enhance the scalability and efficiency of large-scale neural networks~\cite{shazeer2017outrageously}.
By dynamically selecting a subset of specialized expert modules for each input~\cite{li2023simple}, MoE reduces computational overhead while maintaining high expressive power~\cite{shazeer2017outrageously,zoph2022designing}.
In the context of LLMs, MoE has been shown to improve both training efficiency and generalization across tasks~\cite{artetxe2019massively}.
Works such as Switch Transformers~\cite{fedus2022switch} and GShard~\cite{lepikhin2020gshard} have demonstrated the effectiveness of MoE in scaling up model capacity without prohibitive increases in training costs.
In multimodal settings, MoE has been explored to address the modality alignment problem~\cite{goyal2021coordination}, where different experts handle distinct modalities or specific tasks. 
However, the optimal utilization of experts heavily relies on the effectiveness of routing mechanisms, which remains an active area of research.\looseness-1

\textbf{Routers and Routing Strategies} 
are the cornerstone of any MoE-based architecture, responsible for determining which experts are activated for each input~\cite{li2024your}.
Traditional routers, such as softmax gating functions~\cite{shazeer2017outrageously}, compute a weighted combination of experts based on input embeddings.
Despite their simplicity, these routing strategies often face challenges in achieving optimal expert assignment~\cite{lepikhin2020gshard,zoph2022designing}, particularly in unseen or highly diverse test scenarios.
Recent works have proposed advanced routing strategies, including routing via reinforcement learning~\cite{rosenbaum2017routing}, early-exit~\cite{li2023towards}, and task-specific  allocation~\cite{shi2024eagle}.
However, these approaches typically focus on training-time optimization, leaving test-time adaptability largely unexplored.
\ours introduces an efficient method to refine routing weights dynamically during inference, ensuring better alignment with task-specific requirements and improving overall model robustness across diverse multimodal benchmarks.\looseness-1


\textbf{Test-Time Optimization} has been explored by adapting models dynamically during inference to improve generalization.
For example, ~\cite{wang2022continual} propose test-time adaptation, which fine-tunes model parameters on test data distributions using entropy minimization or self-supervised learning.
Similarly, ~\cite{sun2020testtimetrainingselfsupervisiongeneralization} introduce test-time training, where models are updated via auxiliary tasks (e.g., rotation prediction) during inference.
However, these methods require modifying the base model's parameters, leading to significant computational overhead and potential instability when deployed on resource-constrained systems.
Unlike prior test-time optimization methods that update model weights, \ours solely optimizes the routing weights of a frozen MoE model without retraining any model parameters.

\section{Test-Time Re-Routing}

\begin{figure*}[tbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/method.png}
}
\vspace{-2.5em}
\caption{Illustration of \ours' test-time re-routing mechanism with three strategies. (a) Neighborhood Gradient Descent: Optimizes $r$ using gradients derived from neighbors' loss functions ($\nabla_{r} l_1$, $\nabla_{r} l_2$, and $\nabla_{r} l_3$ for the 3 nearest neighbors), weighted by their similarity to the test sample. (b) Kernel Regression: Estimates $r$ as a weighted average of neighbors' routing weights ($\hat{r}$), and further optimizes it through binary search between $\hat{r}$ and initial weights $r$ to find the optimal coefficient $
\alpha$. (c) Mode Finding: Iteratively updates $r$ through weighted interpolation between currecnt weights and the local average $\bar{r}$ in routing weight space, shifting towards the densest region.}
\vspace{-1em}
\label{fig:method}
\end{figure*}

MoE trains a router to reweight experts for each input. However, such an end-to-end trained router may not always produce optimal weights for challenging or out-of-distribution samples at test-time, whereas sub-optimal weights can drastically degrade the performance of MoE on diverse downstream tasks. The importance of routing weights has been broadly demonstrated on eight benchmarks in our experiments: The large performance gap between the base model (using the router's routing weights) and the oracle (using the optimal routing weights) in Table~\ref{tab:method_comparison_mova} implies the potential merits of optimizing the routing weights in the test-time.

To address this problem, Test-Time Re-Routing (\ours) introduces a dynamic test-time re-routing mechanism that adapts the routing weights for each test sample based on similar samples in a \textbf{reference set}—a set of samples on which the MoE's outputs are correct or preferred. 
Specifically, given a reference set of $n$ samples $\{(x_i,y_i)\}_{i=1}^n$ and their corresponding routing weights $\{r_i\}_{i=1}^n$, on which the model makes correct prediction (i.e., $f(x_i,r_i)=y_i$), for a new test sample $x$, the goal of \ours is to find a better routing weight vector $r$ for $x$ that leads to a more accurate and higher-quality output $f(x,r)$. 


In the following, we will introduce three core strategies, illustrated in Figure~\ref{fig:method}, to optimize $r$ based on the neighbors of $x$ in the reference set, i.e., $\mathcal N(x)$, according to a similarity metric.
These strategies are developed with different optimization objectives (e.g., loss surrogate, regression, mode finetuning, etc.) and neighbor-search spaces (e.g., routing weights, task embedding, etc.). While the first is gradient-based, the other two are gradient-free, offering more flexible options for different setups and computational budgets.  


\subsection{Gradient Descent}

The gradient descent method uses the gradient of an objective function $L(r)$ to update $r$ for multiple steps until convergence or when certain stopping criteria have been fulfilled. In every step, we apply
\begin{equation}
    r\leftarrow r-\lambda \nabla_r L(r),
\end{equation}
where $\lambda$ is a learning rate determined by a scheduler. We discuss the two choices of $L(r)$ in the following. 

\textbf{Oracle (upper bound)} assumes that we know the ground truth label $y$ for $x$, which is a cheating setting that can provide an upper bound of the gradient descent method. In this setting,
\begin{equation}
    L(r) = \ell[f(x,r), y],
\end{equation}
where $\ell[\cdot, \cdot]$ is the loss function (e.g., cross-entropy or L2 loss) measuring the discrepancy between the model output $f(x,r)$ and the ground truth $y$. 
Although this is not applicable in real scenarios, it serves as a performance ceiling to reveal the degradation caused by sub-optimal routing weights and evaluate the effectiveness of other methods.

\textbf{Neighborhood Gradient Descent (NGD)} is a practical approach that uses the loss functions of the nearest neighbors of $x$ in the reference set to estimate the gradient of $r$, i.e.,
\begin{equation}
    L(r)=\frac{\sum_{i\in \mathcal N(x)}K(x_i, x)\times \ell[f(x_i,r), y_i]}{\sum_{i\in \mathcal N(x)} K(x_i, x)}
\end{equation}
By incorporating loss information from the neighborhood of $x$, NGD enables a label-free, test-time adaptation mechanism. This effectively aligns $r$ with the successful routing patterns in the reference set. This ensures that $r$ exploits the routing for relevant reference examples without requiring access to the oracle loss.

\subsection{Kernel Regression}

Kernel regression predicts $r$ by the weighted average of the neighbors' routing weights $\{r_i\}_{i\in \mathcal N(x)}$, i.e.,
\begin{equation}
    \hat r\triangleq \frac{\sum_{i\in \mathcal N(x)}K(x_i, x)\cdot r_i}{\sum_{i\in \mathcal N(x)} K(x_i, x)},
\end{equation}
where $K(\cdot, \cdot)$ is a kernel function, e.g., Gaussian kernel, Matern kernel, etc. In the experiments, we found that directly setting $r\leftarrow \hat r$ already brings non-trivial improvement. 

However, $\hat r$ does not take the router-produced initial $r$ into account and may not fully capture the nuanced dependencies required for optimal performance.
To further optimize $r$, we conduct a binary search on the straight line between $r$ and $\hat r$:\looseness-1
\begin{equation}
    r\leftarrow \alpha r + (1-\alpha)\hat r.
\end{equation}
The search goal is to find the optimal $\alpha$ minimizing the objective $L(r)$, i.e.,
\begin{equation}
    \alpha^*\in \arg\min_\alpha L(\alpha r + (1-\alpha)\hat r).
\end{equation}
This refinement step balances the kernel regression estimate with the router's original routing weights. It includes $\hat r$ as a special case (when $\alpha=0$) and can further enhance the accuracy and robustness of the model's predictions.

\subsection{Mode Finding (Meanshift)}

Mode finding aims to move $r$ towards the highest density region of the distribution $p(r)$ for the reference routing weights $\{r_i\}_{i=1}^n$. It applies the following update for multiple steps until convergence. 
\begin{equation}
    r\leftarrow \alpha r + (1-\alpha) \bar r,
\end{equation}
where $\alpha$ controls the step size and $\bar r$ the weighted average routing weights defined below (different from $\hat r$).
\begin{equation}
    \bar r\triangleq \frac{\sum_{i\in \mathcal N(r)}K(r_i, r)\cdot r_i}{\sum_{i\in \mathcal N(r)} K(r_i, r)}.
\end{equation}
Unlike kernel regression, mode finding identifies the densest region in the routing weight space (so the kernel $K(\cdot,\cdot)$ and neighborhood $\mathcal N(\cdot)$ are applied to $r$ instead of $x$), representing the most consistent configurations among nearby reference samples. This makes it effective for capturing the dominating patterns in the local distribution of routing weights.\looseness-1

\subsection{Neighborhood and Embedding Space}\label{sec:neighborhood}
\textbf{Neighborhood}
The choices of neighborhood definition and the embedding space in which to apply the kernels are important to the final performance. For the former, we can use either $k$NN or $\epsilon$-ball, i.e.,
\begin{align}
    &\mathcal N(x)\triangleq \arg\min_{A\subseteq 2^n, |A|\leq k} \sum_{i\in A} d(x_i, x),\\
    &\mathcal N(x)\triangleq \{i\in [n]: d(x_i, x)\leq \epsilon\},
\end{align}
\textbf{Embedding}
Instead of directly applying an existing kernel function $K(\cdot, \cdot)$ and a distance metric $d(\cdot, \cdot)$ to the raw inputs $x_i$ and $x$, we can replace $x$ and $x_i$ with their embedding $E(x)$ and $E(x_i)$, where $E(\cdot)$ is a pre-trained embedding model applied to the task description of each sample. \looseness-1   


\begin{table}[htbp]
\centering
\vspace{-0.15in}
\caption{Summary of reference and evaluation benchmarks. If the reference dataset contains more than 5,000 samples, we randomly select 5,000 to ensure balanced evaluation.}
\label{tab:trainset}
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{l|lc|lc}
\toprule
\textbf{Task Type} & \textbf{Reference} & \textbf{Size} & \textbf{Evaluation} & \textbf{Size} \\
\hline
\multirow{4}{*}{\makecell[l]{General \\ Visual \\ Understanding}} 
& VQA-V2 & 5,000 & MMBench & 2,374 \\
& Visual7W & 5,000 & MME-P & 2,114 \\
& COCO-QA & 5,000 & CVBench\textsuperscript{2D/3D} & 2,638 \\
& CLEVR & 5,000 & GQA & 1,590 \\
\midrule
\multirow{3}{*}{\makecell[l]{Knowledge- \\ Based \\ Reasoning}} 
& A-OKVQA & 5,000 & SQA-IMG & 2,017 \\
& TQA & 5,000 & AI2D & 3,087 \\
& MathVista & 5,000 & PhysBench & 2,093 \\
\midrule
\multirow{2}{*}{\makecell[l]{Optical Character \\ Recognition}} 
& ST-VQA & 5,000 & TextVQA & 5,734 \\
& DocVQA & 5,000 &  &  \\
\bottomrule
\end{tabular}}
\vspace{-0.1in}
\end{table}

\section{Experiments}



\subsection{Experimental Setting}
\label{sec:exp_setup}

\textbf{Models}
We evaluate two multimodal MoE models: \textbf{MoAI}~\cite{lee2025moai} and \textbf{MoVA}~\cite{zong2024mova}, each leveraging specialized experts for vision-language tasks. MoAI has six experts: (1) \textbf{Visual Experts} process auxiliary CV features ($\mathbf{I}_{\textsc{aux}}$), align visuals with language ($\mathbf{I}_{\textsc{lang}}$), and capture spatial relationships ($\mathbf{I}_{\textsc{self}}$); (2) \textbf{Language Experts} integrate external knowledge ($\mathbf{L}_{\textsc{aux}}$), link language to visuals ($\mathbf{L}_{\textsc{img}}$), and maintain coherence ($\mathbf{L}_{\textsc{self}}$). Further details about MoAI experts are provided in Appendix~\ref{app:model_detail}. 
MoVA includes seven experts, incorporating SAM~\cite{zou2024segment} to enhance the vision encoder with specialized knowledge.



\textbf{Reference datasets and evaluation benchmarks}
Our evaluation covers three task categories: general visual understanding, knowledge-based reasoning, and optical character recognition. Table~\ref{tab:trainset} summarizes the reference datasets and evaluation benchmarks, including their dataset sizes. See Appendix~\ref{app:benchmark} for details.





\begin{table*}[t]
\centering

\vspace{-12pt}
\caption{Comparison of three \ours methods ($k$NN with $k=5$) applied to MoVA and MoAI (base models), with Accuracy (\%) reported\footnotemark[1]. Oracle has access to the ground truths and provides an upper bound. \textbf{NGD significantly improves base models and performs the best.}}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
Method & \textbf{MMBench} & \textbf{MME-P} & \textbf{SQA-IMG} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{GQA} & \textbf{CVBench$^{2D}$} & \textbf{CVBench$^{3D}$} & \textbf{PhysBench} \\

\bottomrule
\toprule
MoVA (base model) & 74.3 & 1579.2 & 74.4 & 74.9 & 76.4 & 64.8 & 61.6 & 62.3 & 32.6 \\
\midrule
Mode Finding & 75.2 & 1587.1 & 74.9 & 75.8 & 77.3 & 65.7 & 62.5 & 63.2 & 33.5 \\
Kernel Regression & 77.9 & 1610.6 & 76.4 & 78.5 & 79.9 & 68.3 & 65.2 & 65.9 & 35.7 \\
NGD & \textbf{81.2} & \textbf{1645.3} & \textbf{79.1} & \textbf{81.8} & \textbf{83.2} & \textbf{71.5} & \textbf{68.3} & \textbf{68.9} & \textbf{37.8} \\
\midrule
Oracle (upper bound) & 87.6 & 1735.4 & 87.3 & 88.4 & 89.5 & 76.2 & 72.5 & 73.2 & 47.5 \\
\bottomrule
\toprule
MoAI (base model) & 79.3 & 1714.0 & 83.5 & 78.6 & 67.8 & 70.2 & 71.2 & 59.3 & 39.1 \\
\midrule
Mode Finding & 80.8 & 1725.2 & 84.1 & 79.8 & 66.5 & 71.4 & 70.0 & 60.1 & 40.2 \\
Kernel Regression & 83.7 & 1756.7 & 86.2 & 82.6 & 71.2 & 74.5 & 74.6 & 64.5 & 42.8 \\
NGD & \textbf{85.2} & \textbf{1785.5} & \textbf{88.3} & \textbf{85.0} & \textbf{73.5} & \textbf{77.0} & \textbf{77.9} & \textbf{69.2} & \textbf{44.7} \\
\midrule
Oracle (upper bound) & 92.1 & 1860.2 & 93.8 & 91.2 & 79.6 & 83.2 & 84.0 & 76.8 & 54.5 \\
\bottomrule
\end{tabular}%
}
\vspace{-12pt}
\label{tab:method_comparison_mova}
\end{table*}




\textbf{Evaluations}
We adopt standard evaluation protocols for each benchmark. For MME-P, performance is assessed using two metrics:(1) Accuracy, measuring the correctness of a single question per image, and (2) Accuracy+, requiring both questions per image to be answered correctly. The final score is the sum of these two metrics, with a maximum of 2,000~\cite{fu2024mmecomprehensiveevaluationbenchmark}. For other benchmarks, accuracy is the primary metric~\cite{yin2023survey}. We compute the mean score across benchmarks as $\frac{1}{\text{\#benchmark}}\left(S_{\text{total}} + S_{\text{mmp-e}}\right),$
where $S_{\text{total}}$ is the sum of all benchmark scores except MME-P, and $S_{\text{mmp-e}}$ is the normalized MME-P score.



\textbf{Baselines}
\ours introduces test-time re-routing, a problem not addressed in prior work. To assess its effectiveness, we compare it against multiple \ours variants and base models. Additionally, we benchmark \ours against state-of-the-art VLMs across scales, as shown in Table~\ref{tab:vlm_comparison}.

We use fixed hyperparameters across all benchmarks without per-task tuning, determined via experiments on small-scale benchmarks independent of our evaluation datasets. See Appendix~\ref{app:hyperparams} for details.



\subsection{Main Results}
\label{sec:main_res}
\textbf{Comparsion of different \ours methods}
Tables \ref{tab:method_comparison_mova} summarizes the performance of \ours methods on the MoVA and MoAI models across eight benchmarks.
Among all evaluated methods, $k$NN Neighborhood Gradient Descent (NGD) emerges as the most effective, delivering significant improvements over the pretrained base models.
For MoAI-7B, \ours enhances performance significantly, achieving +6.9\% on MMBench, a +66.1-point increase on MME-P, and a +6.8\% gain on TextVQA. Similarly, on MoVA-7B, it yields notable improvements of +5.9\% on MMBench, +71.5 points on MME-P, and +5.7\% on TextVQA. These consistent gains across diverse benchmarks highlight the ability of \ours to optimize routing weights effectively, enabling better utilization of expert modules for improved model performance.  
Notably, $k$NN NGD achieves results close to the Oracle upper bound, which relies on ground truth labels during test-time and is thus infeasible in practice. Our method, without accessing labels, captures 70–80\% of the potential improvement, demonstrating its effectiveness.
\footnotetext[1]{Except MME-P's score, the sum of two accuracy metrics.\looseness-1}

\begin{table*}[htbp]
\centering
\vspace{-12pt}
\caption{Comparison of \ours ($k$NN, NGD) with state-of-the-art vision-language models on nine benchmarks (higher the better).}
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
VLM & \textbf{MMBench} & \textbf{MME-P} & \textbf{SQA-IMG} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{GQA} & \textbf{CVBench$^{2D}$} & \textbf{CVBench$^{3D}$} & \textbf{PhysBench} \\
\hline
\rowcolor{gray!20} \multicolumn{10}{l}{\textbf{7B Models}} \\
InstructBLIP-7B~\cite{dai2023instructblip} & 36.0 & - & 60.5 & - & 50.1 & 56.7 & - & - & 23.8 \\
Qwen-VL-7B~\cite{bai2023qwen} & 38.2 & - & 67.1 & 62.3 & 63.8 & 59.4 & - & - & - \\
Qwen-VL-Chat-7B~\cite{bai2023qwen} & 60.6 & 1488.0 & 68.2 & 57.7 & 61.5 & - & - & - & \textbf{35.6} \\
mPLUG-Owl-7B~\cite{ye2023mplug} & 46.6 & 967.0 & - & - & - & 58.9 & - & - & - \\
mPLUG-Owl2-7B~\cite{ye2024mplug} & 64.5 & 1450.0 & 68.7 & - & 58.2 & 62.9 & - & - & - \\
ShareGPT4V-7B~\cite{chen2025sharegpt4v} & \textbf{68.8} & \textbf{1567.4} & \textbf{68.4} & 67.3 & 65.8 & \textbf{63.4} & 60.2 & 57.5 & 31.3 \\
\hline
\rowcolor{gray!20} \multicolumn{10}{l}{\textbf{8B Models}} \\
Mini-Gemini-HD-8B~\cite{li2024mini} & 72.7 & 1606.0 & \textbf{75.1} & \textbf{73.5} & \textbf{70.2} & 64.5 & 62.2 & 63.0 & 34.7 \\
LLaVA-NeXT-8B~\cite{liu2024llava} & 72.1 & 1603.7 & 72.8 & 71.6 & 64.6 & \textbf{65.2} & 62.2 & \textbf{65.3} & - \\
Cambrian1-8B~\cite{tong2024cambrian} & \textbf{75.9} & \textbf{1647.1} & 74.4 & 73.0 & 68.7 & 64.6 & \textbf{72.3} & 65.0 & \textbf{24.6} \\
\hline
\rowcolor{gray!20} \multicolumn{10}{l}{\textbf{13B Models}} \\
BLIP2-13B~\cite{li2023blip} & 28.8 & 1294.0 & 61.0 & - & 42.5 & - & - & - & 38.6 \\
InstructBLIP-13B~\cite{dai2023instructblip} & 39.1 & 1213.0 & 63.1 & - & 50.7 & - & - & - & 29.9 \\
Mini-Gemini-HD-13B~\cite{li2024mini} & 68.6 & 1597.0 & 71.9 & 70.1 & 70.2 & 63.7 & 53.6 & 67.3 & - \\
LLaVA-NeXT-13B~\cite{liu2024llava} & 70.0 & 1575.0 & 73.5 & 70.0 & 67.1 & \textbf{65.4} & 62.7 & 65.7 & \textbf{40.5} \\
Cambrian1-13B~\cite{tong2024cambrian} & \textbf{75.7} & \textbf{1610.4} & \textbf{79.3} & \textbf{73.6} & \textbf{72.8} & 64.3 & \textbf{72.5} & \textbf{71.8} & - \\
\hline
\rowcolor{gray!20} \multicolumn{10}{l}{\textbf{34B Models}} \\
Mini-Gemini-HD-34B~\cite{li2024mini} & 80.6 & 1659.0 & 77.7 & 80.5 & 74.1 & 65.8 & 71.5 & 79.2 & - \\
LLaVA-NeXT-34B~\cite{liu2024llava} & 79.3 & 1633.2 & 81.8 & 74.9 & 69.5 & \textbf{67.1} & 73.0 & 74.8 & - \\
Cambrian1-34B~\cite{tong2024cambrian} & \textbf{81.4} & \textbf{1689.3} & \textbf{85.6} & \textbf{79.7} & \textbf{76.7} & 65.8 & \textbf{74.0} & \textbf{79.7} & \textbf{30.2} \\
\hline
\rowcolor{gray!20} \multicolumn{10}{l}{\textbf{Ours}} \\
MoVA-7B & 74.3 & 1579.2 & 74.4 & 74.9 & 76.4 & 64.8 & 61.6 & 62.3 & 32.6 \\
\ours (MoVA-7B) & \textbf{81.2} & \textbf{1645.3} & \textbf{79.1} & \textbf{81.8} & \textbf{83.2} & \textbf{71.5} & \textbf{68.3} & \textbf{68.9} & \textbf{37.8} \\
\midrule
MoAI-7B & 79.3 & 1714 & 83.5 & 78.6 & 67.8 & 70.2 & 71.2 & 59.3 & 39.1 \\
\ours (MoAI-7B) & \textbf{85.2} & \textbf{1785.5} & \textbf{88.3} & \textbf{85.0} & \textbf{73.5} & \textbf{77.0} & \textbf{77.9} & \textbf{69.2} & \textbf{44.7} \\
\bottomrule
\end{tabular}%
}
\label{tab:vlm_comparison}
\end{table*}


\textbf{Comparison with state-of-the-art VLMs}
In Table~\ref{tab:vlm_comparison}, we compare our approach with state-of-the-art VLMs of various sizes (7B, 8B, 13B, 34B) across benchmarks. 
When applied to the pretrained MoVA-7B—which initially lags behind larger models—\ours achieves substantial performance gains and outperforms 7/8/13/34 competitors across most benchmarks through effective test-time re-routing.
In addition, applying \ours to MoAI-7B results in a significant performance boost, establishing it as highly competitive against larger models. Notably, for PhysBench, which contains both video and image tests, our results reflect only the image-only evaluation. R2-T2(MoAI-7B) ranks second in the image-only leaderboard of PhysBench.
These results highlight the effectiveness of \ours in unlocking the potential of smaller models, enabling them to match or even surpass the performance of significantly larger VLMs.

\textbf{Inference efficiency trade-off}
While \ours introduces additional operations beyond the base model’s inference pipeline, it achieves near-oracle performance with moderate computational overhead (Table \ref{tab:time_overhead_mmb}). To ensure hardware-independent comparison, we measure computational costs in FLOPs. The base model requires 9.9T FLOPs per case. Mode finding adds only 1.8T FLOPs, leading to a 1.5\% accuracy gain. Kernel regression and \ours require 6–7× more FLOPs due to loss computations over five neighbors, yet \ours ($k$NN, NGD) achieves the highest accuracy improvement (+5.9\%) while maintaining competitive efficiency.



\begin{table}[htbp]
\centering
\vspace{-12pt}
\caption{FLOPs of different methods ($k$NN with $k$ = 5) on MMBench using MoAI-7B as the base model.}
\label{tab:time_overhead_mmb}
\resizebox{0.9\columnwidth}{!}{
\setlength{\tabcolsep}{3pt} 
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccc}
\toprule
Method & \makecell{\textbf{Inference} \\ \textbf{steps}} & \makecell{\textbf{FLOPs (T)} \\ \textbf{per case}} & \makecell{\textbf{Accuracy} \\ \textbf{(\%)}} \\
\midrule
Base Model (MoAI-7B) & 1 & 9.9 & 79.3 \\
\hline
Mode Finding & 10 & 10.7 & 80.8 \\
Kernel Regression & 10 & 61.9 & 83.7 \\
\ours ($k$NN, NGD) & 10 & 67.5 & 85.2 \\
\hline
Oracle (upper bound) & 10 & 11.8 & 89.8 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[!ht]
\centering
\vspace{-16pt}
\caption{Ablation study of \ours ($k$NN, NGD) with different \textbf{choices of neighborhood} on MoAI.}
\resizebox{0.31\textwidth}{!}{
\begin{tabular}{lcc|lcc}
\toprule
\multicolumn{3}{c|}{$\epsilon$-ball} & \multicolumn{3}{c}{$k$NN} \\
Parameter & Avg. &  & Parameter & Avg.\\
\midrule
$\epsilon=0.2$  & 76.5 & & $k=3$  & 78.6 \\
$\epsilon=0.4$  & 77.9 & & $k=5$  & \textbf{80.7} \\
$\epsilon=0.6$  & \textbf{78.9} & & $k=10$ & 79.4 \\
$\epsilon=0.8$  & 77.7 & & $k=20$ & 76.6 \\
\bottomrule
\end{tabular}}
\vspace{-16pt}
\label{tab:neighborhood_comparison}
\end{table}

\begin{table*}[!ht]
\centering
\vspace{-12pt}
\captionsetup{font=small,skip=4pt}
\small
\renewcommand{\arraystretch}{1.1} 

% 第一个表格：Kernel Ablation
\begin{minipage}[t]{0.32\textwidth}
\centering
\captionof{table}{Ablation study of \textbf{\ours} ($k$NN, NGD) with \textbf{kernel choices} on MoAl.}
\vspace{4pt}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{@{}l c@{}}
\toprule
Kernel & Avg. \\
\midrule
Linear~\cite{cortes1995support} & 76.3 \\
Polynomial~\cite{cortes1995support} & 77.7 \\
Matern~\cite{williams2006gaussian} & 78.7 \\
Gaussian~\cite{williams2006gaussian} & \textbf{80.7} \\
\bottomrule
\end{tabular}}
\label{tab:kernel_ablation}
\end{minipage}
\hfill
\begin{minipage}[t]{0.33\textwidth}
\centering
\captionof{table}{Ablation study of \textbf{\ours} ($k$NN, NGD) with \textbf{embedding models} on MoAI.}
\vspace{4pt}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{@{}l c@{}}
\toprule
Embedding Model & Avg. \\
\midrule
Sentence-Bert~\cite{reimers2019sentence} & 77.5 \\
Stella-En-1.5B-V5~\cite{kusupati2022matryoshka} & 78.5 \\
Gte-Qwen2-7B~\cite{li2023towards} & 78.7 \\
NV-Embed-V2~\cite{lee2024nv} & \textbf{80.7} \\
\bottomrule
\end{tabular}}
\label{tab:embedding_ablation}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\centering
\captionof{table}{Ablation study of \textbf{\ours} ($k$NN, NGD) with \textbf{NGD steps} on MoAI.}
\vspace{4pt}
\resizebox{0.31\columnwidth}{!}{
\begin{tabular}{@{}l c@{}}
\toprule
\#Step & Avg. \\
\midrule
5 & 76.6 \\
10 & \textbf{80.7} \\
20 & 80.5 \\
50 & \textbf{80.7} \\
\bottomrule
\end{tabular}}
\label{tab:gradient_steps}
\end{minipage}
\end{table*}

\subsection{Ablation Study}
\label{sec:ab}
We analyze how each component contributes to the performance and robustness of $k$NN NGD, with all studies conducted on MoAI. Results are averaged across 8 test benchmarks detailed in Section~\ref{sec:exp_setup}, with individual results and further analysis provided in Appendix~\ref{app:ab}.

\textbf{Neighborhood selection} compare two strategies: 
$\epsilon$-ball (radius $\epsilon$ = 0.2 to 0.8) and $k$NN ($k$ = 3 to 20), as shown in Table~\ref{tab:neighborhood_comparison}.
The results demonstrate that $k$NN with $k=5$ consistently achieves better performance across most tasks, outperforming both smaller neighborhoods that may lack sufficient context and larger ones that could introduce noise. While $\epsilon$-ball shows stable performance across different radius, it suffers from inherent limitations: a fixed radius threshold may yield too few neighbors in sparse regions or excessive neighbors in dense regions, leading to inconsistent performance. The $k$NN approach provides more reliable and generally superior results. This suggests that maintaining a fixed number of neighbors not only ensures consistent computational cost but also provides sufficient information for effective test-time re-routing. 


\textbf{Kernel choice}
is critical for determining how similarity is modeled in high-dimensional spaces, which directly affects gradient updates in NGD.
In Table~\ref{tab:kernel_ablation}, we compare four different kernel functions.
The results consistently show that the Gaussian kernel outperforms other kernel functions across all tasks, with up to a 4.4\% accuracy improvement over the linear kernel. Its superior performance may due to its ability to effectively capture similarity relationships in high-dimensional embedding spaces while being less affected by the curse of dimensionality~\cite{cristianini2000introduction}. 

\textbf{Embedding model}
directly impacts the neighborhood quality, which in turn influences the gradient updates.
In Table~\ref{tab:embedding_ablation}, we compare four embedding models.
The results show that NV-Embed-V2 achieves consistent improvements of 3.2\% over Sentence-Bert, indicating its ability to provide more discriminative feature representations that better capture semantic relationships between samples.


\textbf{Gradient descent steps} significantly affect both convergence and performance.
Experiments with 5, 10, 20, and 50 steps assess the trade-off between cost and accuracy.
As seen in Table~\ref{tab:gradient_steps}, increasing the step count from 5 to 10 significantly improves performance (76.6 → 80.7), indicating that more iterations enhance optimization. Beyond 10 steps, performance saturates (80.5 at 20 steps, 80.7 at 50), suggesting diminishing returns. 
Thus, 10 steps offer the best balance between performance and efficiency.  

\begin{figure*}[t]
\centering
\vspace{-8pt}
\setlength{\abovecaptionskip}{-16pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/final.png}
} 
\vspace{-12pt}
 \caption{Top-1 expert transitions to correct/incorrect preditions on CVBench\textsuperscript{2D/3D} after re-routing. The primary transitions to correct predictions in (a) include $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{L}_{\textsc{IMG}}$,  $\mathbf{L}_{\textsc{AUX}}$ and $\mathbf{L}_{\textsc{AUX}}$. The primary transitions to incorrect predictions in (b) include $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{I}_{\textsc{AUX}}$, $\mathbf{L}_{\textsc{IMG}}$ and $\mathbf{L}_{\textsc{AUX}}$.
 \textbf{\ours considerably mitigates the \textit{modality imbalance} of the base model.}\looseness-1
 } 
\label{fig:final}
\vspace{-12pt}
\end{figure*}

\begin{figure}[!htbp]
\centering
\setlength{\abovecaptionskip}{4pt}
\resizebox{1.0\linewidth}{!}{%
\includegraphics{fig/transition_diagram.png}
}
\vspace{-2.5em}
\caption{Transition between correct and incorrect predictions on CVBench\textsuperscript{2D/3D} during NGD steps of \ours from Step 0 to 10. \textbf{NGD keeps turning more incorrect predictions to correct.}}
\label{fig:transition}
\vspace{-2.em}
\end{figure}

\subsection{Case Studies}
\label{sec:case_study}
\textbf{Accuracy Transition Analysis}
Figure~\ref{fig:transition} illustrates the transition of predictions as NGD progresses over ten steps.
During Step 0 to Step 4, 17.22\% of incorrect predictions are corrected, and by Step 10, a total of 28.12\% of incorrect predictions have been converted to correct ones.
Meanwhile, only 2.31\% correct predictions become incorrect throughout the optimization process.
As the optimization converges in later steps, the routing weight changes become smaller, reducing the number of prediction shifts.


\textbf{Expert Shift Patterns}
Figure~\ref{fig:final} illustrates top-1 expert transitions before and after re-routing, where (a) shows transitions leading to correct predictions and (b) those leading to incorrect predictions. The original router over-relies on $\mathbf{I}_{\textsc{lang}}$, limiting model adaptability. After re-routing, many samples shift from $\mathbf{I}_{\textsc{lang}}$ to $\mathbf{L}_{\textsc{img}}$, $\mathbf{I}_{\textsc{aux}}$, and $\mathbf{L}_{\textsc{aux}}$, leading to improved accuracy. This indicates that the pretrained router excessively favors $\mathbf{I}_{\textsc{lang}}$, preventing optimal expert utilization.
Notably, samples that were initially correct before re-routing exhibited a more balanced expert distribution, whereas those initially incorrect depended heavily on $\mathbf{I}_{\textsc{lang}}$. After re-routing, expert distributions in both cases become more balanced, showing that \ours effectively diversifies expert selection.
Furthermore, transition patterns differ between correctly and incorrectly predicted samples. In correct cases (Figure~\ref{fig:final} (a)), re-routing typically shifts $\mathbf{I}_{\textsc{lang}}$ to $\mathbf{L}_{\textsc{img}}$. 
In incorrect cases (Figure~\ref{fig:final} (b)), transitions often involve $\mathbf{I}_{\textsc{lang}}$ to $\mathbf{L}_{\textsc{aux}}$. This may indicate occasional mismatches in the re-weighted routing. Crucially, the number of cases shifting from correct to incorrect is significantly lower than those transitioning from incorrect to correct. The overall improvements outweigh potential misclassifications, validating \ours as an effective optimization strategy.

\textbf{Example Case: Spatial Reasoning Improvement }
Figure~\ref{fig:example} demonstrates how \ours rectifies a spatial reasoning failure. The test question asks, ``where is the chair located with respect to the tennis racket?''.
Initially, the model selects $\mathbf{I}_{\textsc{lang}}$ (language-aligned visual expert), which prioritizes textual alignment but fails to capture positional relationships.
\ours addresses this by retrieving nearest neighbors from the reference set with similar spatial queries.
By dynamically adjusting routing weights, \ours elevates $\mathbf{I}_{\textsc{aux}}$ to the top-1 position.
$\mathbf{I}_{\textsc{aux}}$ integrates features from open-world object detection~\cite{lee2025moai, NEURIPS2023_e6d58fc6}, enabling a more precise interpretation of spatial layouts.

Additional transition pattern cases and details are provided in Appendix~\ref{ap:case_study} and ~\ref{app:expert_transitions} for further insights.

\section{Conclusions}
We introduce \ours, a novel test-time re-routing method that enhances multimodal Mixture-of-Experts (MoE) models without additional training. By dynamically adjusting routing weights based on reference samples, \ours corrects suboptimal expert selection, improving model generalization. We propose and evaluate three strategies—Neighborhood Gradient Descent, Kernel Regression, and Mode Finding—demonstrating their effectiveness across multiple multimodal benchmarks. \ours consistently outperforms the base MoE model and rivals oracle-based optimization methods, highlighting the potential of test-time adaptation for more efficient and adaptive expert utilization. 

\bibliography{example_paper}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\section{MoAI Experts}
\label{app:model_detail}

MoAI integrates specialized computer vision models and expert modules to achieve comprehensive scene understanding:

\textbf{External CV Models:} Four computer vision models provide complementary capabilities: (1) panoptic segmentation~\cite{cheng2022masked} for object identification and localization, (2) open-world object detection~\cite{minderer2024scaling} for recognizing diverse objects beyond predefined categories, (3) scene graph generation~\cite{yang2022panoptic} for understanding object relationships, and (4) optical character recognition (OCR)~\cite{du2021pp} for text understanding. These models provide auxiliary information that enhances MoAI's visual perception.

\textbf{Cross-Modal Capabilities:} The expert modules are designed to facilitate effective cross-modal interactions:

\begin{itemize}
    \item Visual Experts: $\mathbf{I}_{\textsc{aux}}$ connects visual features with structured CV outputs through cross-attention, $\mathbf{I}_{\textsc{lang}}$ aligns visual representations with language semantics, while $\mathbf{I}_{\textsc{self}}$ maintains spatial awareness through self-attention.
    \item Language Experts: $\mathbf{L}_{\textsc{aux}}$ integrates verbalized CV outputs with language understanding, $\mathbf{L}_{\textsc{img}}$ grounds language in visual context, and $\mathbf{L}_{\textsc{self}}$ ensures coherent text generation.
\end{itemize}

The combination of specialized CV models and cross-modal experts enables MoAI to bridge the gap between detailed visual perception and high-level language understanding. This architecture is particularly effective for tasks requiring both fine-grained visual analysis and natural language reasoning.



\section{Evaluation Benchmarks and Reference Datasets}
\label{app:benchmark}

We conduct evaluations using a diverse set of reference datasets and task-specific benchmarks.
For general visual understanding, we use four reference datasets: \textbf{VQA-V2}~\cite{goyal2017making}, \textbf{Visual7W}~\cite{zhu2016visual7w}, \textbf{CLEVR}~\cite{johnson2017clevr}, and \textbf{COCO-QA}~\cite{lu2016hierarchical}. For knowledge-based reasoning, which requires leveraging external knowledge, we include \textbf{A-OKVQA}~\cite{schwenk2022okvqa}, \textbf{TQA}~\cite{kembhavi2017you} and \textbf{MathVista}~\cite{lu2023mathvista}. For optical character recognition (OCR), we employ \textbf{ST-VQA}~\cite{biten2019scene}, \textbf{DocVQA}~\cite{mathew2021docvqa}. 
To ensure a balanced evaluation, we randomly sample 5,000 instances from datasets exceeding this size.

Correspondingly, we evaluate on task-specific benchmarks. For general visual understanding, these include \textbf{MMBench}~\cite{liu2025mmbench},
\textbf{MME-P}~\cite{fu2024mmecomprehensiveevaluationbenchmark},
\textbf{CVBench\textsuperscript{2D/3D}}~\cite{tong2024cambrian},
and \textbf{GQA}~\cite{hudson2019gqa}.
For knowledge-based reasoning, we evaluate on \textbf{SQA-IMG}~\cite{lu2022learn}
\textbf{AI2D}~\cite{kembhavi2016diagram} and \textbf{PhysBench}~\cite{chow2025physbench}.
\textbf{TextVQA}~\cite{singh2019towards} is evaluated for OCR.
 



\subsection*{Reference Datasets}
\begin{itemize}
    \item \textbf{VQA-V2}~\cite{goyal2017making}: Focuses on \textbf{open-ended visual question answering}, requiring models to answer questions about images. Tasks include object recognition, attribute identification, and scene understanding. Contains 1.1M questions across 200K+ COCO images, with balanced annotations to reduce language bias.
    
    \item \textbf{Visual7W}~\cite{zhu2016visual7w} Specializes in \textbf{7-type visual QA} (``what,'' ``where,'' ``when,'' ``who,'' ``why,'' ``how,'' and ``which''), emphasizing grounding answers in image regions (e.g., ``Where is the cat?'' with bounding box annotations). It includes 327K QA pairs, challenging models on spatial reasoning and causal explanations.

    
    \item \textbf{CLEVR}~\cite{johnson2017clevr}: A synthetic benchmark for \textbf{compositional visual reasoning}. Tasks involve counting objects, comparing attributes, and logical operations (e.g., ``Are there more red cubes than blue spheres?''). Contains 100K rendered 3D images and 853K questions, designed to test systematic generalization.
    
    \item \textbf{COCO-QA}~\cite{lu2016hierarchical}: Automatically generates QA pairs from COCO image captions for \textbf{basic visual understanding}. Questions fall into four categories: object, number, color, and location (e.g., ``What color is the car?''). Includes 117K QA pairs, serving as a lightweight evaluation for object-centric reasoning.
    
    \item \textbf{A-OKVQA}~\cite{schwenk2022okvqa}: Requires \textbf{commonsense and external knowledge} for visual QA (e.g., ``Why is the person wearing a helmet?''). Distinguishes between direct perception (``What is this?'') and knowledge-augmented reasoning. Contains 25K questions with crowdsourced explanations.
    
    \item \textbf{TQA}~\cite{kembhavi2017you}: A multimodal machine comprehension dataset designed to \textbf{test reasoning over middle school science curricula.} It contains 1,076 lessons with 26,260 questions, combining text, diagrams, and images. Questions require parsing complex scientific concepts and reasoning across multiple modalities, making it more challenging than traditional QA datasets. The dataset is split into training, validation, and test sets, with no content overlap, ensuring robust evaluation of models' ability to integrate and reason over multimodal information.
    
    \item \textbf{MathVista}~\cite{lu2023mathvista}: A \textbf{multimodal math reasoning benchmark} combining visual understanding (diagrams/plots) and textual problem-solving. Contains 6,141 problems testing abilities like geometric reasoning, equation parsing, and chart interpretation. Highlights the stark gap between human performance (91.6\% on text-only tasks) and state-of-the-art AI models (58.9\%), particularly in visual-textual integration and multi-step reasoning.
    
    \item \textbf{ST-VQA}~\cite{biten2019scene}: Evaluates \textbf{scene text understanding in visual QA}. Questions require reading text in images (e.g., ``What is the store name?''). Includes 23K questions across diverse scenarios (signboards, documents, etc.), with strict answer normalization.
    
    \item \textbf{DocVQA}~\cite{mathew2021docvqa}: Focuses on \textbf{document image understanding}. Tasks include extracting information from tables, forms, and invoices (e.g., ``What is the invoice number?''). Contains 50K questions on 12K document images, testing OCR and layout understanding.
\end{itemize}

\subsection*{Evaluation Benchmarks}
\begin{itemize}
    \item \textbf{MMBench}~\cite{liu2025mmbench}: A comprehensive benchmark for \textbf{multimodal understanding and generation}. Tasks span image captioning, visual entailment, and fine-grained attribute QA. Includes 2,374 pairs with hierarchical evaluation dimensions (perception, reasoning, knowledge).
    
    \item \textbf{MME-P}~\cite{fu2024mmecomprehensiveevaluationbenchmark}: Evaluates \textbf{multimodal event understanding} through paired questions (e.g., before/after event prediction). Contains 2,114 pairs covering temporal, causal, and counterfactual reasoning in video/text contexts.
    
    \item \textbf{CVBench 2D/3D}~\cite{tong2024cambrian}: A unified benchmark for \textbf{2D and 3D vision tasks}. 2D tasks include depth estimation and object detection (1,438 pairs), while 3D tasks focus on point cloud registration and mesh reconstruction (1,200 pairs).
    
    \item \textbf{GQA}~\cite{hudson2019gqa}: Tests \textbf{compositional reasoning over real-world images}. Questions use functional programs (e.g., ``Select then compare'') to ensure compositional validity. Includes 1,590 pairs with explicit scene graph grounding for error analysis.

    \item \textbf{SQA-IMG}~\cite{lu2022learn}: A \textbf{science QA benchmark with diagrammatic reasoning}. Questions combine textbook diagrams and textual context (e.g., ``Which process is shown in the diagram?''). Contains 2,017 pairs spanning biology, physics, and chemistry.

    \item \textbf{AI2D}~\cite{kembhavi2016diagram}: Focuses on \textbf{diagram interpretation for K-12 science}. Tasks include diagram labeling, relation extraction, and multi-step inference (e.g., ``What happens after step 3?''). Contains 3,087 pairs with annotated diagram primitives (arrows, labels).


    \item \textbf{TextVQA}~\cite{singh2019towards}: Requires \textbf{text-aware visual QA} (e.g., answering ``What brand?'' from text in images). Contains 5,734 pairs with a focus on OCR-VQA integration, using real-world images with scene text.

    \item \textbf{PhysBench}~\cite{chow2025physbench}: Requires \textbf{physical world understanding} (e.g., reasoning about object properties and dynamics). Contains 10,002 video-image-text entries(2093 image-only entries) evaluating VLMs on physical properties, relationships, scenes, and dynamics understanding.

\end{itemize}


\section{Hyperparameter Choices}
\label{app:hyperparams}

To ensure a robust and fair evaluation, we use a fixed set of hyperparameters across all benchmarks. This approach maintains consistency, prevents task-specific optimizations, and allows for an unbiased comparison of performance.

The selected hyperparameters are as follows: cosine annealing schedule with a learning rate ranging from $1 \times 10^{-2}$ to $1 \times 10^{-5}$, neighborhood selection is performed using $k$NN with $k=5$, the number of NGD steps is fixed at 10, the Gaussian kernel is used for kernel-based methods, and NV-Embed-V2 is adopted as the embedding model. These values are applied uniformly across all evaluated tasks.

\paragraph{Hyperparameter Selection Strategy}
Rather than tuning hyperparameters separately for each benchmark, we determined these values through controlled experiments on Qbench~\cite{wu2023q} that do not overlap with our evaluation benchmarks. This ensures that hyperparameter selection is independent of the test sets, minimizing the risk of overfitting while maintaining general applicability.

Additionally, our ablation studies (Section~\ref{sec:ab}) confirm the effectiveness of these choices. Variations in key hyperparameters, such as NGD steps and neighborhood size, show that our selected values strike a balance between performance and efficiency, supporting their suitability across diverse benchmarks.











\section{Additional Analysis}

\subsection{Ablation Study}
\label{app:ab}

We perform an ablation study to assess the impact of key hyperparameters on \ours's performance. Table~\ref{tab:ab_lr_ablation} evaluates different learning rate schedules for Gradient Descent, comparing cosine annealing, step decay, and fixed schedules. Full results for the ablation studies discussed in Section~\ref{sec:ab} are presented in Tables~\ref{tab:ab_neighborhood_comparison}-\ref{fig:ab_step}.

\paragraph{Comparison of different learning rate}

In Table~\ref{tab:ab_lr_ablation}, we investigate how different learning rate schedules affect the performance of Gradient Descent. We compare
cosine annealing schedule against two fixed ($1e\text{-}3$ and $1e\text{-}4$) and a step decay schedule. The cosine annealing schedule
consistently outperforms all baseline approaches across all benchmarks, achieving improvements of up to 12.7 percentage points over the fixed learning rate ($1e\text{-}3$) baseline. These findings suggest that carefully designed learning rate schedules are essential for maximizing the potential of \ours.


\begin{table*}[htbp]
\centering
\caption{Ablation study of \ours ($k$NN, NGD) with different \textbf{learning rate schedules}.}
\label{tab:ab_lr_ablation}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Schedule & \textbf{MMBench} & \textbf{MME-P} & \textbf{SQA-IMG} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{GQA} & \textbf{CVBench$^{2D}$} & \textbf{CVBench$^{3D}$} \\
\midrule
Fixed ($1e\text{-}3$) & 71.8 & 1671.2 & 74.3 & 70.9 & 60.4 & 63.1 & 66.8 & 57.2 \\
Fixed ($1e\text{-}4$) & 75.2 & 1692.5 & 77.8 & 74.5 & 63.9 & 66.5 & 69.9 & 63.3 \\
Step Decay & 82.9 & 1745.4 & 84.2 & 81.8 & 70.5 & 73.8 & 73.5 & 67.2 \\
Cosine & \textbf{85.2} & \textbf{1785.5} & \textbf{88.3} & \textbf{85.0} & \textbf{73.5} & \textbf{77.0} & \textbf{77.9} & \textbf{69.2} \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Ablation study of \ours ($k$NN, NGD) with different \textbf{choices of neighborhood} on MoAI.}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{llccccccccc}
\toprule
Neighbors &Parameter & \textbf{MMBench} & \textbf{MME-P} & \textbf{SQA-IMG} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{GQA} & \textbf{CVBench$^{2D}$} & \textbf{CVBench$^{3D}$} \\
\midrule
% \multicolumn{9}{c}{$\epsilon$-ball ($\epsilon$)} \\
\multirow{4}{*}{$\epsilon$-ball}
&$\epsilon=$ 0.2  & 82.4 & 1733.9 & 84.8 & 81.3 & 69.9 & 73.1 & 67.1 & 66.5 \\
&$\epsilon=$ 0.4  & 83.9 & 1758.4 & 86.0 & 83.0 & 71.5 & 74.8 & 68.5 & 67.3 \\
&$\epsilon=$ 0.6  & \textbf{85.4} & \textbf{1778.8} & \textbf{87.2} & \textbf{83.8} & \textbf{72.4} & \textbf{75.9} & \textbf{69.6} & \textbf{68.0} \\
&$\epsilon=$ 0.8  & 83.7 & 1756.5 & 85.9 & 82.5 & 71.2 & 74.5 & 68.3 & 67.4 \\

\midrule

\multirow{4}{*}{$k$NN}
&$k=$ 3 & 83.2 & 1740.9 & 86.1 & 83.1 & 71.3 & 75.1 & 75.8 & 67.4 \\
&$k=$ 5 & \textbf{85.2} & \textbf{1785.5} & \textbf{88.3} & \textbf{85.0} & \textbf{73.5} & \textbf{77.0} & \textbf{77.9} & \textbf{69.2} \\
&$k=$ 10 & 84.0 & 1761.3 & 86.8 & 83.5 & 72.8 & 75.3 & 76.6 & 68.1 \\
&$k=$ 20 & 80.7 & 1693.6 & 83.6 & 80.7 & 70.5 & 73.2 & 73.9 & 65.7 \\
\bottomrule
\end{tabular}%
}
\label{tab:ab_neighborhood_comparison}
\end{table*}


\begin{table*}[htbp]
\centering
\caption{Ablation study of \ours ($k$NN, NGD) with different \textbf{choices of kernels} on MoAI.}
\label{tab:ab_kernel_ablation}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Kernel & \textbf{MMBench} & \textbf{MME-P} & \textbf{SQA-IMG} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{GQA} & \textbf{CVBench$^{2D}$} & \textbf{CVBench$^{3D}$} \\
\midrule
Linear  & 82.1 & 1722.3 & 84.2 & 80.8 & 69.5 & 72.8 & 72.7 & 62.1 \\
Polynomial & 83.2 & 1745.5 & 85.1 & 81.9 & 70.4 & 73.9 & 74.5 & 65.2 \\
Matern & 83.9 & 1752.8 & 85.8 & 82.5 & 71.2 & 74.6 & 76.3 & 67.8 \\
Gaussian & \textbf{85.2} & \textbf{1785.5} & \textbf{88.3} & \textbf{85.0} & \textbf{73.5} & \textbf{77.0} & \textbf{77.9} & \textbf{69.2} \\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[htbp]
\centering
\caption{Ablation study of \ours ($k$NN, NGD) with different \textbf{embedding models} on MoAI.}
\label{tab:ab_embedding_ablation}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Embedding Model & \textbf{MMBench} & \textbf{MME-P} & \textbf{SQA-IMG} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{GQA} & \textbf{CVBench$^{2D}$} & \textbf{CVBench$^{3D}$} \\
\midrule
Sentence-Bert & 82.8 & 1748.2 & 84.2 & 80.3 & 70.2 & 73.8 & 75.6 & 66.0 \\
Stella-En-1.5B-V5 & 83.6 & 1752.5 & 85.4 & 82.1 & 70.8 & 74.3 & 76.3 & 67.5 \\
Gte-Qwen2-7B-instruct & 84.0 & 1757.0 & 86.0 & 82.7 & 71.3 & 74.8 & 76.1 & 67.0 \\
NV-Embed-V2 & \textbf{85.2} & \textbf{1785.5} & \textbf{88.3} & \textbf{85.0} & \textbf{73.5} & \textbf{77.0} & \textbf{77.9} & \textbf{69.2} \\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[htbp]
\centering
\caption{Ablation study of \ours ($k$NN, NGD) with different \textbf{number of NGD steps}.}
\label{tab:ab_gradient_steps}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\#Step & \textbf{MMBench} & \textbf{MME-P} & \textbf{SQA-IMG} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{GQA} & \textbf{CVBench$^{2D}$} & \textbf{CVBench$^{3D}$} \\
\midrule
5 & 81.3 & 1705.8 & 84.2 & 80.9 & 69.2 & 73.5 & 72.2 & 66.1 \\
7 & 83.8 & 1745.2 & 86.5 & 83.2 & 71.8 & 75.2 & 76.0 & 67.6 \\
10 (ours) & 85.2 & 1785.5 & 88.3 & \textbf{85.0} & 73.5 & 77.0 & \textbf{77.9} & 69.2 \\
20 & 85.0 & 1777.8 & \textbf{88.5} & 84.6 & 73.7 & 76.8 & 77.7 & 69.0 \\
50 & \textbf{85.3} & \textbf{1792.0} & 88.2 & 84.8 & \textbf{73.4} & \textbf{77.1} & 77.6 & \textbf{69.3} \\
\bottomrule
\end{tabular}
}
\label{fig:ab_step}
\end{table*}

\clearpage

\subsection{Case study}
\label{ap:case_study}

\paragraph{Case Study: Transition from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{L}_{\textsc{AUX}}$}

Figures~\ref{fig:plane} and \ref{fig:plate} illustrate cases where the initial routing incorrectly prioritizes $\mathbf{I}_{\textsc{LANG}}$, which aligns visual features with language but lacks object-specific recognition capabilities. This results in misidentifications: in the first case, the model misinterprets the plane number, yielding ``728FW'' instead of the correct ``728TFW''; in the second case, it incorrectly predicts ``FRENCH'' as the license plate’s state instead of the correct ``California.''

To correct these errors, \ours retrieves three highly relevant reference samples using $k$NN based on question similarity. Each reference set contains samples with similar question structures, providing a more suitable routing adjustment. After incorporating insights from these references, the routing shifts towards $\mathbf{L}_{\textsc{AUX}}$, which enhances object-specific recognition and scene understanding. This re-routing process enables the model to produce the correct answers ``728TFW'' and ``California,'' demonstrating the effectiveness of \ours in dynamically refining expert selection.


\paragraph{Case Study: Transition from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{I}_{\textsc{AUX}}$}

We show one case for this transition in Figure~\ref{fig:example} and analyze in Section~\ref{sec:case_study}. 
Figure~\ref{fig:hat} illustrates another case where the initial routing incorrectly prioritizes $\mathbf{I}_{\textsc{LANG}}$, which aligns visual features with language but lacks object-specific recognition capabilities. As a result, the model miscounts the number of hats in the image, selecting answer ``(C) 2'' instead of the correct ``(D) 1.''

To correct this, \ours retrieves three highly relevant reference samples using $k$NN based on question similarity. These samples contain similar counting-related queries, allowing for a more effective routing adjustment. After integrating insights from these references, the routing shifts towards $\mathbf{I}_{\textsc{AUX}}$, which specializes in fine-grained object recognition. This re-routing enables the model to correctly identify and count the hats, selecting the correct answer ``(D) 1.'' This case demonstrates the ability of \ours to refine expert selection dynamically, improving numerical reasoning in visual question-answering tasks.

\paragraph{Case Study: Transition from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{L}_{\textsc{IMG}}$}

Figures~\ref{fig:speaker} and \ref{fig:shirt} illustrate cases where the initial routing incorrectly prioritizes $\mathbf{I}_{\textsc{LANG}}$, which aligns visual features with language but lacks fine-grained perceptual understanding. This misalignment leads to incorrect predictions: in the first case, the model incorrectly identifies ``DVD Player'' instead of the correct answer ``Speaker'' when asked which device is not illuminated; in the second case, it incorrectly answers ``No'' instead of ``Yes'' when asked if the shirt is soft and white.

To correct these errors, \ours retrieves three relevant reference samples using $k$NN based on question similarity. These samples involve similar queries related to illumination and color perception, guiding a more suitable routing adjustment. After incorporating insights from these references, the routing shifts towards $\mathbf{L}_{\textsc{IMG}}$, which specializes in fine-grained visual details. This adjustment enables the model to correctly identify the non-illuminated device and recognize the shirt’s color and texture, leading to the correct answers ``Speaker'' and ``Yes.''

These cases demonstrate \ours' ability to dynamically refine expert selection, improving visual perception in multimodal reasoning tasks by leveraging contextual cues from reference samples.

\begin{figure*}[tbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/hat.png}
}
 \caption{Example for transition from $\mathbf{I}_{\textsc{lang}}$ to $\mathbf{I}_{\textsc{aux}}$ using \ours. The model initially gives incorrect answer ``(C)2" by relying on $\mathbf{I}_{\textsc{lang}}$. After $k$NN retrieval with similar questions about counting hats , it re-routes to $\mathbf{I}_{\textsc{aux}}$ and correctly answers ``(D) 1" for the number of hats in the image.}
\label{fig:hat}
\end{figure*}

\begin{figure*}[htbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/plane.png}
}
 \caption{Example of routing transition from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{L}_{\textsc{AUX}}$ using \ours. Initially, the model selects $\mathbf{I}_{\textsc{LANG}}$, misidentifying the plane number. By retrieving $k$NN with similar queries, \ours shifts the routing weights towards $\mathbf{L}_{\textsc{AUX}}$, leading to the correct answer.}
\label{fig:plane}
\end{figure*}

\begin{figure*}[tbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/plate.png}
}
 \caption{Example for transition from $\mathbf{I}_{\textsc{lang}}$ to $\mathbf{L}_{\textsc{aux}}$ using \ours. The model initially gives incorrect answer ``FRENCH" by relying on $\mathbf{I}_{\textsc{lang}}$. After $k$NN retrieval with similar questions, it re-routes to LAUX and correctly identifies ``California" as the plate's state.}
\label{fig:plate}
\end{figure*}

\begin{figure*}[tbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/speaker.png}
}
 \caption{Example of routing transition from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{L}_{\textsc{IMG}}$ using \ours. Initially, the model selects $\mathbf{I}_{\textsc{LANG}}$, leading to the incorrect prediction ``DVD Player'' when asked which device is not illuminated. By retrieving $k$NN samples with similar illumination-related queries, \ours shifts the routing weights towards $\mathbf{L}_{\textsc{IMG}}$, enabling the correct answer ``Speaker.''}
\label{fig:speaker}
\end{figure*}

\begin{figure*}[tbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/shirt.png}
}
 \caption{Example of routing transition from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{L}_{\textsc{IMG}}$ using \ours. Initially, the model selects $\mathbf{I}_{\textsc{LANG}}$, leading to the incorrect prediction ``No'' when asked if the shirt is soft and white. By retrieving $k$NN samples with similar color-based queries, \ours shifts the routing weights towards $\mathbf{L}_{\textsc{IMG}}$, allowing the model to correctly answer ``Yes.''}
\label{fig:shirt}
\end{figure*}


\clearpage

\section{Expert Transition Analysis}
\label{app:expert_transitions}

To better understand the impact of test-time re-routing, we analyze expert transitions across different prediction scenarios. Figures~\ref{fig:wrong2right}-\ref{fig:wrong2wrong} illustrate how top-1 expert selections shift before and after re-routing on CVBench\textsuperscript{2D/3D}.

\begin{figure*}[htbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/wrong2right.png}
}
 \caption{Top-1 expert transitions from incorrect to correct predictions on CVBench\textsuperscript{2D/3D} after re-routing. For transitions to incorrect predictions, the main patterns include transitions from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{I}_{\textsc{IMG}}$, $\mathbf{L}_{\textsc{AUX}}$ and $\mathbf{I}_{\textsc{AUX}}$}
\label{fig:wrong2right}
\end{figure*}


\begin{figure*}[htbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/right2wrong.png}
}
 \caption{Top-1 expert transitions from correct to incorrect predictions on CVBench\textsuperscript{2D/3D} after re-routing. The visualization shows primary transitions from $\mathbf{I}_{\textsc{LANG}}$ to $\mathbf{I}_{\textsc{AUX}}$ and $\mathbf{L}_{\textsc{IMG}}$, demonstrating how correct predictions can shift to incorrect outcomes through these pathways.}
\label{fig:right2wrong}
\end{figure*}

\begin{figure*}[htbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/right2right.png}
}
 \caption{Top-1 expert transitions from correct to correct predictions on CVBench\textsuperscript{2D/3D} after re-routing. The main transition patterns demonstrate consistent routing from $\mathbf{I}_{\textsc{LANG}}$ through $\mathbf{I}_{\textsc{AUX}}$ to $\mathbf{L}_{\textsc{IMG}}$ and $\mathbf{I}_{\textsc{AUX}}$, showing stable pathways for maintaining correct predictions.}
\label{fig:right2right}
\end{figure*}

\begin{figure*}[htbp]
\centering
\setlength{\abovecaptionskip}{10pt}
\resizebox{\linewidth}{!}{%
\includegraphics{fig/wrong2wrong.png}
}
 \caption{Top-1 expert transitions from incorrect to incorrect predictions on CVBench\textsuperscript{2D/3D} after re-routing. The visualization reveals persistent incorrect prediction patterns, with transitions primarily flowing from $\mathbf{I}_{\textsc{LANG}}$ through $\mathbf{I}_{\textsc{AUX}}$ to $\mathbf{L}_{\textsc{IMG}}$ and $\mathbf{I}_{\textsc{AUX}}$, with additional $\mathbf{I}_{\textsc{SELF}}$ routing observed.}
\label{fig:wrong2wrong}
\end{figure*}

\end{document}
