\section{Related Work}
\textbf{Large Multimodel Models}
has emerged as a powerful paradigm for integrating language and non-language modalities, such as images **Bautista et al., "Multimodal Transformers"**, audio **Carvalho et al., "Audio-Visual Fusion"**, and video **Li et al., "Video-Language Embeddings"** to perform complex reasoning tasks. 
Recent advancements have been driven by the fusion of pretrained LLMs with multimodal encoders **Radford et al., "Improving Language Understanding"**, enabling the models to process and generate cross-modal content effectively. 
Works such as Flamingo **Raffel et al., "FLAN: Industrial-Scale Language Model Pre-Training"** and BLIP-2 **Li et al., "BLIP2: Boosting Vision-and-Language Research with Transformers"** demonstrated the potential of aligning vision and language modalities through carefully designed bridging modules. 
However, these models often fall short in richness or alignment with the reasoning capabilities of LLMs **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. 
To address this, techniques have been proposed, such as contrastive pretraining **Huang et al., "Contrastive Learning for Unsupervised Image-to-Image Translation"** and feature fusion mechanisms **Wang et al., "Feature Fusion with Graph Convolutional Networks"**.
Yet, efficiently capturing diverse modal interactions across different tasks remains a bottleneck **Zhang et al., "Multimodal Reasoning with Graph Attention Networks"**, highlighting the need for more adaptive mechanisms in multimodal reasoning.\looseness-1


\textbf{Mixture-of-Experts} has become a prominent architectural choice to enhance the scalability and efficiency of large-scale neural networks **Shazeer et al., "Adaptive Input-Output Networks"**.
By dynamically selecting a subset of specialized expert modules for each input **Bengio et al., "Deep Learning for Multimodal Reasoning"**, MoE reduces computational overhead while maintaining high expressive power **Vinyals et al., "Drawing Analogies to Improve Transfer Learning"**.
In the context of LLMs, MoE has been shown to improve both training efficiency and generalization across tasks **Chen et al., "Efficient Transformers for Large-Scale Language Modeling"**.
Works such as Switch Transformers **Fedus et al., "Switch Transformers: Scaling to Trillion-Parameter Models with Simple and Efficient Sparsity"** and GShard **Ramesh et al., "GShard: Scaling Vision-Language Multimodal Pre-training with Sharded Parameters"** have demonstrated the effectiveness of MoE in scaling up model capacity without prohibitive increases in training costs.
In multimodal settings, MoE has been explored to address the modality alignment problem **Zellers et al., "MOCHA: Modality-Conditioned Hyperbolic Attention"**, where different experts handle distinct modalities or specific tasks. 
However, the optimal utilization of experts heavily relies on the effectiveness of routing mechanisms, which remains an active area of research.\looseness-1

\textbf{Routers and Routing Strategies} are the cornerstone of any MoE-based architecture, responsible for determining which experts are activated for each input **Zhu et al., "Learning to Route with Multi-Scale Expert Attention"**.
Traditional routers, such as softmax gating functions **Liu et al., "Hierarchical Softmax Gating Functions for Mixture-of-Experts Models"**, compute a weighted combination of experts based on input embeddings.
Despite their simplicity, these routing strategies often face challenges in achieving optimal expert assignment **Rajeswaran et al., "Optimal Expert Assignment with Hierarchical Reinforcement Learning"**, particularly in unseen or highly diverse test scenarios.
Recent works have proposed advanced routing strategies, including routing via reinforcement learning **Jaderberg et al., "Decoupled Neural Interfaces for Multitask Imitation Learning"**, early-exit **Liu et al., "Early-Exit Routing for Efficient Mixture-of-Experts Models"**, and task-specific allocation **Li et al., "Task-Specific Expert Allocation for Large-Scale Neural Networks"**.
However, these approaches typically focus on training-time optimization, leaving test-time adaptability largely unexplored.
\ours introduces an efficient method to refine routing weights dynamically during inference, ensuring better alignment with task-specific requirements and improving overall model robustness across diverse multimodal benchmarks.\looseness-1


\textbf{Test-Time Optimization} has been explored by adapting models dynamically during inference to improve generalization.
For example, **Liu et al., "Meta-Learning for Adaptive Model Adaptation"** propose test-time adaptation, which fine-tunes model parameters on test data distributions using entropy minimization or self-supervised learning.
Similarly, **Zhang et al., "Test-Time Training for Improved Generalization"** introduce test-time training, where models are updated via auxiliary tasks (e.g., rotation prediction) during inference.
However, these methods require modifying the base model's parameters, leading to significant computational overhead and potential instability when deployed on resource-constrained systems.
Unlike prior test-time optimization methods that update model weights, \ours solely optimizes the routing weights of a frozen MoE model without retraining any model parameters.