\section{Related Work}
\textbf{Large Multimodel Models}
has emerged as a powerful paradigm for integrating language and non-language modalities, such as images____, audio____, and video____, to perform complex reasoning tasks. 
Recent advancements have been driven by the fusion of pretrained LLMs with multimodal encoders____, enabling the models to process and generate cross-modal content effectively. 
Works such as Flamingo____ and BLIP-2____ demonstrated the potential of aligning vision and language modalities through carefully designed bridging modules. 
However, these models often fall short in richness or alignment with the reasoning capabilities of LLMs____. 
To address this, techniques have been proposed, such as contrastive pretraining____ and feature fusion mechanisms____.
Yet, efficiently capturing diverse modal interactions across different tasks remains a bottleneck____, highlighting the need for more adaptive mechanisms in multimodal reasoning.\looseness-1


\textbf{Mixture-of-Experts} has become a prominent architectural choice to enhance the scalability and efficiency of large-scale neural networks____.
By dynamically selecting a subset of specialized expert modules for each input____, MoE reduces computational overhead while maintaining high expressive power____.
In the context of LLMs, MoE has been shown to improve both training efficiency and generalization across tasks____.
Works such as Switch Transformers____ and GShard____ have demonstrated the effectiveness of MoE in scaling up model capacity without prohibitive increases in training costs.
In multimodal settings, MoE has been explored to address the modality alignment problem____, where different experts handle distinct modalities or specific tasks. 
However, the optimal utilization of experts heavily relies on the effectiveness of routing mechanisms, which remains an active area of research.\looseness-1

\textbf{Routers and Routing Strategies} 
are the cornerstone of any MoE-based architecture, responsible for determining which experts are activated for each input____.
Traditional routers, such as softmax gating functions____, compute a weighted combination of experts based on input embeddings.
Despite their simplicity, these routing strategies often face challenges in achieving optimal expert assignment____, particularly in unseen or highly diverse test scenarios.
Recent works have proposed advanced routing strategies, including routing via reinforcement learning____, early-exit____, and task-specific  allocation____.
However, these approaches typically focus on training-time optimization, leaving test-time adaptability largely unexplored.
\ours introduces an efficient method to refine routing weights dynamically during inference, ensuring better alignment with task-specific requirements and improving overall model robustness across diverse multimodal benchmarks.\looseness-1


\textbf{Test-Time Optimization} has been explored by adapting models dynamically during inference to improve generalization.
For example, ____ propose test-time adaptation, which fine-tunes model parameters on test data distributions using entropy minimization or self-supervised learning.
Similarly, ____ introduce test-time training, where models are updated via auxiliary tasks (e.g., rotation prediction) during inference.
However, these methods require modifying the base model's parameters, leading to significant computational overhead and potential instability when deployed on resource-constrained systems.
Unlike prior test-time optimization methods that update model weights, \ours solely optimizes the routing weights of a frozen MoE model without retraining any model parameters.