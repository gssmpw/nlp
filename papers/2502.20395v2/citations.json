[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ao2021speecht5",
        "author": "Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and others",
        "title": "Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zellers2021merlot",
        "author": "Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin",
        "title": "Merlot: Multimodal neural script knowledge models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "peng2023kosmos",
        "author": "Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu",
        "title": "Kosmos-2: Grounding multimodal large language models to the world"
      },
      {
        "key": "tsimpoukelli2021multimodal",
        "author": "Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix",
        "title": "Multimodal few-shot learning with frozen language models"
      },
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "bubeck2023sparks",
        "author": "Bubeck, S{\\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others",
        "title": "Sparks of artificial general intelligence: Early experiments with gpt-4"
      },
      {
        "key": "bommasani2021opportunities",
        "author": "Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others",
        "title": "On the opportunities and risks of foundation models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "yuan2021multimodal",
        "author": "Yuan, Xin and Lin, Zhe and Kuen, Jason and Zhang, Jianming and Wang, Yilin and Maire, Michael and Kale, Ajinkya and Faieta, Baldo",
        "title": "Multimodal contrastive training for visual representation learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lu2019vilbert",
        "author": "Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan",
        "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "baltruvsaitis2018multimodal",
        "author": "Baltru{\\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe",
        "title": "Multimodal machine learning: A survey and taxonomy"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "shazeer2017outrageously",
        "author": "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff",
        "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2023simple",
        "author": "Li, Ziyue and Ren, Kan and Jiang, Xinyang and Shen, Yifei and Zhang, Haipeng and Li, Dongsheng",
        "title": "Simple: Specialized model-sample matching for domain generalization"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "shazeer2017outrageously",
        "author": "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff",
        "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
      },
      {
        "key": "zoph2022designing",
        "author": "Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William",
        "title": "Designing effective sparse expert models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "artetxe2019massively",
        "author": "Artetxe, Mikel and Schwenk, Holger",
        "title": "Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lepikhin2020gshard",
        "author": "Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng",
        "title": "Gshard: Scaling giant models with conditional computation and automatic sharding"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "goyal2021coordination",
        "author": "Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua",
        "title": "Coordination among neural modules through a shared global workspace"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2024your",
        "author": "Li, Ziyue and Zhou, Tianyi",
        "title": "Your mixture-of-experts llm is secretly an embedding model for free"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "shazeer2017outrageously",
        "author": "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff",
        "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "lepikhin2020gshard",
        "author": "Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng",
        "title": "Gshard: Scaling giant models with conditional computation and automatic sharding"
      },
      {
        "key": "zoph2022designing",
        "author": "Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William",
        "title": "Designing effective sparse expert models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "rosenbaum2017routing",
        "author": "Rosenbaum, Clemens and Klinger, Tim and Riemer, Matthew",
        "title": "Routing networks: Adaptive selection of non-linear functions for multi-task learning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "li2023towards",
        "author": "Li, Ziyue and Ren, Kan and Yang, Yifan and Jiang, Xinyang and Yang, Yuqing and Li, Dongsheng",
        "title": "Towards inference efficient deep ensemble learning"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "shi2024eagle",
        "author": "Shi, Min and Liu, Fuxiao and Wang, Shihao and Liao, Shijia and Radhakrishnan, Subhashree and Huang, De-An and Yin, Hongxu and Sapra, Karan and Yacoob, Yaser and Shi, Humphrey and others",
        "title": "Eagle: Exploring the design space for multimodal llms with mixture of encoders"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "wang2022continual",
        "author": "Wang, Qin and Fink, Olga and Van Gool, Luc and Dai, Dengxin",
        "title": "Continual test-time domain adaptation"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "sun2020testtimetrainingselfsupervisiongeneralization",
        "author": "Yu Sun and Xiaolong Wang and Zhuang Liu and John Miller and Alexei A. Efros and Moritz Hardt",
        "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts"
      }
    ]
  }
]