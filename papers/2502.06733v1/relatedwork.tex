\section{Related work}
\vspace{-3mm}
\textbf{Training Data Re-weighting/Selection for LLMs.}
Several recent studies \citep{xie2023doremi,chen2023skill,fan2023doge,thakkar2023self} have explored various reweighting techniques to enhance the generalization and efficiency of language models pretraining. For instance, \cite{xie2023doremi} and \cite{fan2023doge} optimize the composition of pretraining corpora to achieve better performance across pretraining domains or for out-of-domain generalization. \citet{chen2023skill} introduce a framework for ordered skill learning, optimizing data selection based on how effectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although effective, these techniques operate at the group level, whereas our work explores reweighting at the instance level, offering finer control over how individual samples are treated based on their loss values. Furthermore, we demonstrate that combining domain-level methods such as DoReMi \citep{xie2023doremi} or DoGE \citep{fan2023doge} with our instance-level reweighting methods results in improved performance across multiple domains.
Instance-level reweighting has been used in post-training settings of LLMs \citep{chen2024take,jiang2024importance}. \citet{jiang2024importance} boost the self-improvement abilities of LLMs by employing sample reweighting to filter out self-generated data that have correct answers but exhibit high distribution shifts. \cite{chen2024take} reweight individual samples during continual training/instruction-tuning to focus on medium-loss samples. In contrast, our work systematically studies the effects of various sample-level, loss-based reweighting strategies on the efficiency and effectiveness of LLMs \emph{pretraining}.
The approach in \cite{fan2023irreducible} offers a curriculum learning framework that prioritizes samples with a higher learnability score, which is precomputed using another auxiliary model similar to DoReMi and DoGE. While we do not explicitly address curriculum learning in this work, our re-weighting mechanisms naturally allow for implementing a form of loss-based curriculum learning algorithms without the need to train and store additional proxy models as in \cite{fan2023irreducible}. 

\textbf{Sample-level Re-weighting as generic ML solution.} 
Sample-level reweighting has been extensively explored in other machine learning areas. For image classification, \cite{loshchilov2015online}, \cite{jiang2019accelerating}, and \cite{katharopoulos2018not} introduce pioneering approaches that prioritize samples based on loss values or gradient norms with the goal of accelerating training speed. Although these methods emphasize the importance of selecting high-loss samples during training, they typically require additional forward/backward passes on each training sample and are primarily designed for multi-epoch training, limiting their applicability with the vast pretraining corpus used for LLMs. Using bilevel optimization, \citet{grangier2023adaptive} adapt the data distribution during training to focus more on relevant samples for a target data distribution. Similarly, \citet{ren2018learning} employ a meta-learning approach to adjust sample weights based on validation set performance, which excels in noisy and imbalanced datasets. In this work, we introduce and investigate various lightweight, loss-based reweighting techniques that add little to no computational overhead compared to uniform sampling and do not require any nested optimization routines, often arising with bilevel optimization and/or meta-learning.
Sample-level reweighting has also been explored in the context of adversarial machine learning \citep{zhang2020geometry,liu2021probabilistic,zeng2021adversarial,sow2023doubly}, domain adaptation \citep{jiang2007instance,fang2020rethinking}, data augmentation \citep{yi2021reweighting}, and imbalanced classification \citep{qi2021online,ren2018learning}. Our reweighting mechanisms also have the potential to be studied under these contexts, which we leave as future work.