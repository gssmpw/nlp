@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{
    fan2023doge,
    title={{DOGE}: Domain Reweighting with Generalization Estimation},
    author={Simin Fan and Matteo Pagliardini and Martin Jaggi},
    booktitle={Second Agent Learning in Open-Endedness Workshop},
    year={2023},
    url={https://openreview.net/forum?id=qiKqsqwYXm}
}

@article{jiang2024importance,
  title={Importance Weighting Can Help Large Language Models Self-Improve},
  author={Jiang, Chunyang and Chan, Chi-min and Xue, Wei and Liu, Qifeng and Guo, Yike},
  journal={arXiv preprint arXiv:2408.09849},
  year={2024}
}

@article{chen2024take,
  title={Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization},
  author={Chen, Xuxi and Wang, Zhendong and Sow, Daouda and Yang, Junjie and Chen, Tianlong and Liang, Yingbin and Zhou, Mingyuan and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2402.14270},
  year={2024}
}

@article{thakkar2023self,
  title={Self-influence guided data reweighting for language model pre-training},
  author={Thakkar, Megh and Bolukbasi, Tolga and Ganapathy, Sriram and Vashishth, Shikhar and Chandar, Sarath and Talukdar, Partha},
  journal={arXiv preprint arXiv:2311.00913},
  year={2023}
}

@article{grangier2023adaptive,
  title={Adaptive training distributions with scalable online bilevel optimization},
  author={Grangier, David and Ablin, Pierre and Hannun, Awni},
  journal={arXiv preprint arXiv:2311.11973},
  year={2023}
}

@inproceedings{ren2018learning,
  title={Learning to reweight examples for robust deep learning},
  author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  booktitle={International conference on machine learning},
  pages={4334--4343},
  year={2018},
  organization={PMLR}
}

@article{sow2023doubly,
  title={Doubly Robust Instance-Reweighted Adversarial Training},
  author={Sow, Daouda and Lin, Sen and Wang, Zhangyang and Liang, Yingbin},
  journal={arXiv preprint arXiv:2308.00311},
  year={2023}
}

@article{fang2020rethinking,
  title={Rethinking importance weighting for deep learning under distribution shift},
  author={Fang, Tongtong and Lu, Nan and Niu, Gang and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11996--12007},
  year={2020}
}

@article{yi2021reweighting,
  title={Reweighting augmented samples by minimizing the maximal expected loss},
  author={Yi, Mingyang and Hou, Lu and Shang, Lifeng and Jiang, Xin and Liu, Qun and Ma, Zhi-Ming},
  journal={arXiv preprint arXiv:2103.08933},
  year={2021}
}

@article{zhang2020geometry,
  title={Geometry-aware instance-reweighted adversarial training},
  author={Zhang, Jingfeng and Zhu, Jianing and Niu, Gang and Han, Bo and Sugiyama, Masashi and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2010.01736},
  year={2020}
}

@article{liu2021probabilistic,
  title={Probabilistic margins for instance reweighting in adversarial training},
  author={Liu, Feng and Han, Bo and Liu, Tongliang and Gong, Chen and Niu, Gang and Zhou, Mingyuan and Sugiyama, Masashi and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23258--23269},
  year={2021}
}

@inproceedings{zeng2021adversarial,
  title={Are adversarial examples created equal? A learnable weighted minimax risk for robustness under non-uniform attacks},
  author={Zeng, Huimin and Zhu, Chen and Goldstein, Tom and Huang, Furong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={10815--10823},
  year={2021}
}



@inproceedings{jiang2007instance,
    title = "Instance Weighting for Domain Adaptation in {NLP}",
    author = "Jiang, Jing  and
      Zhai, ChengXiang",
    editor = "Zaenen, Annie  and
      van den Bosch, Antal",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    pages = "264--271"
}

@inproceedings{qian2019robust,
  title={Robust optimization over multiple domains},
  author={Qian, Qi and Zhu, Shenghuo and Tang, Jiasheng and Jin, Rong and Sun, Baigui and Li, Hao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={4739--4746},
  year={2019}
}

@article{longpre2023pretrainer,
  title={A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity},
  author={Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and others},
  journal={arXiv preprint arXiv:2305.13169},
  year={2023}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@article{kumar2023stochastic,
  title={Stochastic re-weighted gradient descent via distributionally robust optimization},
  author={Kumar, Ramnath and Majmundar, Kushal and Nagaraj, Dheeraj and Suggala, Arun Sai},
  journal={arXiv preprint arXiv:2306.09222},
  year={2023}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}



@inproceedings{
jhunjhunwala2023fedexp,
title={FedExP: Speeding Up Federated Averaging via Extrapolation},
author={Divyansh Jhunjhunwala and Shiqiang Wang and Gauri Joshi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@article{dar2021farewell,
  title={A farewell to the bias-variance tradeoff? an overview of the theory of overparameterized machine learning},
  author={Dar, Yehuda and Muthukumar, Vidya and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:2109.02355},
  year={2021}
}

@article{radhakrishnan2020overparameterized,
  title={Overparameterized neural networks implement associative memory},
  author={Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler, Caroline},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={44},
  pages={27162--27170},
  year={2020},
  publisher={National Acad Sciences}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@misc{cerebras2023slimpajama,
author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
month = {June},
year = 2023,
howpublished = {\url{https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}},
url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}

@article{wang2021adversarial,
  title={Adversarial attack generation empowered by min-max optimization},
  author={Wang, Jingkang and Zhang, Tianyun and Liu, Sijia and Chen, Pin-Yu and Xu, Jiacen and Fardad, Makan and Li, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16020--16033},
  year={2021}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{lee2023platypus,
  title={Platypus: Quick, cheap, and powerful refinement of llms},
  author={Lee, Ariel N and Hunter, Cole J and Ruiz, Nataniel},
  journal={arXiv preprint arXiv:2308.07317},
  year={2023}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{xie2023doremi,
  title={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={arXiv preprint arXiv:2305.10429},
  year={2023}
}

@inproceedings{
    chen2023skill,
    title={Skill-it! A data-driven skills framework for understanding and training language models},
    author={Mayee F Chen and Nicholas Roberts and Kush Bhatia and Jue WANG and Ce Zhang and Frederic Sala and Christopher Re},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=IoizwO1NLf}
}

@article{loshchilov2015online,
  title={Online batch selection for faster training of neural networks},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1511.06343},
  year={2015}
}

@inproceedings{katharopoulos2018not,
  title={Not all samples are created equal: Deep learning with importance sampling},
  author={Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={2525--2534},
  year={2018},
  organization={PMLR}
}

@article{jiang2019accelerating,
  title={Accelerating deep learning by focusing on the biggest losers},
  author={Jiang, Angela H and Wong, Daniel L-K and Zhou, Giulio and Andersen, David G and Dean, Jeffrey and Ganger, Gregory R and Joshi, Gauri and Kaminksy, Michael and Kozuch, Michael and Lipton, Zachary C and others},
  journal={arXiv preprint arXiv:1910.00762},
  year={2019}
}

@article{fan2023irreducible,
  title={Irreducible Curriculum for Language Model Pretraining},
  author={Fan, Simin and Jaggi, Martin},
  journal={arXiv preprint arXiv:2310.15389},
  year={2023}
}

@article{villalobos2022will,
  title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  journal={arXiv preprint arXiv:2211.04325},
  year={2022}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{sankaranarayanan2018learning,
  title={Learning from synthetic data: Addressing domain shift for semantic segmentation},
  author={Sankaranarayanan, Swami and Balaji, Yogesh and Jain, Arpit and Lim, Ser Nam and Chellappa, Rama},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3752--3761},
  year={2018}
}

@article{belgodere2023auditing,
  title={Auditing and Generating Synthetic Data with Controllable Trust Trade-offs},
  author={Belgodere, Brian and Dognin, Pierre and Ivankay, Adam and Melnyk, Igor and Mroueh, Youssef and Mojsilovic, Aleksandra and Navartil, Jiri and Nitsure, Apoorva and Padhi, Inkit and Rigotti, Mattia and others},
  journal={arXiv preprint arXiv:2304.10819},
  year={2023}
}

@article{he2022synthetic,
  title={Is synthetic data from generative models ready for image recognition?},
  author={He, Ruifei and Sun, Shuyang and Yu, Xin and Xue, Chuhui and Zhang, Wenqing and Torr, Philip and Bai, Song and Qi, Xiaojuan},
  journal={arXiv preprint arXiv:2210.07574},
  year={2022}
}

@article{campos2021curriculum,
  title={Curriculum learning for language modeling},
  author={Campos, Daniel},
  journal={arXiv preprint arXiv:2108.02170},
  year={2021}
}

@article{wang2017stochastic,
  title={Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions},
  author={Wang, Mengdi and Fang, Ethan X and Liu, Han},
  journal={Mathematical Programming},
  volume={161},
  pages={419--449},
  year={2017},
  publisher={Springer}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}



@inproceedings{mirzasoleiman2020coresets,
  title={Coresets for data-efficient training of machine learning models},
  author={Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure},
  booktitle={International Conference on Machine Learning},
  pages={6950--6960},
  year={2020},
  organization={PMLR}
}

@inproceedings{killamsetty2021glister,
  title={Glister: Generalization based data subset selection for efficient and robust learning},
  author={Killamsetty, Krishnateja and Sivasubramanian, Durga and Ramakrishnan, Ganesh and Iyer, Rishabh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={8110--8118},
  year={2021}
}

@inproceedings{killamsetty2021grad,
  title={Grad-match: Gradient matching based data subset selection for efficient deep model training},
  author={Killamsetty, Krishnateja and Durga, Sivasubramanian and Ramakrishnan, Ganesh and De, Abir and Iyer, Rishabh},
  booktitle={International Conference on Machine Learning},
  pages={5464--5474},
  year={2021},
  organization={PMLR}
}

@article{sener2017active,
  title={Active learning for convolutional neural networks: A core-set approach},
  author={Sener, Ozan and Savarese, Silvio},
  journal={arXiv preprint arXiv:1708.00489},
  year={2017}
}

@article{fu2013active,
  title={Active learning with optimal instance subset selection},
  author={Fu, Yifan and Zhu, Xingquan and Elmagarmid, Ahmed K},
  journal={IEEE Transactions on Cybernetics},
  volume={43},
  number={2},
  pages={464--475},
  year={2013},
  publisher={IEEE}
}

@article{mohiuddin2022data,
  title={Data selection curriculum for neural machine translation},
  author={Mohiuddin, Tasnim and Koehn, Philipp and Chaudhary, Vishrav and Cross, James and Bhosale, Shruti and Joty, Shafiq},
  journal={arXiv preprint arXiv:2203.13867},
  year={2022}
}

@article{attendu2023nlu,
  title={NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks},
  author={Attendu, Jean-Michel and Corbeil, Jean-Philippe},
  journal={arXiv preprint arXiv:2306.03208},
  year={2023}
}

@article{marion2023less,
  title={When less is more: Investigating data pruning for pretraining llms at scale},
  author={Marion, Max and {\"U}st{\"u}n, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara},
  journal={arXiv preprint arXiv:2309.04564},
  year={2023}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{zhang2023llmaaa,
  title={Llmaaa: Making large language models as active annotators},
  author={Zhang, Ruoyu and Li, Yanzeng and Ma, Yongliang and Zhou, Ming and Zou, Lei},
  journal={arXiv preprint arXiv:2310.19596},
  year={2023}
}

@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{li2023textbooks,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@misc{javaheripi2023phi,
  title={Phi-2: The surprising power of small language models},
  author={Javaheripi, Mojan and Bubeck, S{\'e}bastien and Abdin, Marah and Aneja, Jyoti and Bubeck, Sebastien and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Del Giorno, Allie and Eldan, Ronen and Gopi, Sivakanth and others},
  year={2023}
}

@article{mitra2023orca,
  title={Orca 2: Teaching small language models how to reason},
  author={Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and others},
  journal={arXiv preprint arXiv:2311.11045},
  year={2023}
}

@article{qi2021online,
  title={An online method for a class of distributionally robust optimization with non-convex objectives},
  author={Qi, Qi and Guo, Zhishuai and Xu, Yi and Jin, Rong and Yang, Tianbao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10067--10080},
  year={2021}
}

@article{
qi2023attentionalbiased,
title={Attentional-Biased Stochastic Gradient Descent},
author={Qi Qi and Yi Xu and Wotao Yin and Rong Jin and Tianbao Yang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=B0WYWvVA2r},
note={}
}

@article{qi2022stochastic,
  title={Stochastic constrained dro with a complexity independent of sample size},
  author={Qi, Qi and Lyu, Jiameng and Bai, Er Wei and Yang, Tianbao and others},
  journal={arXiv preprint arXiv:2210.05740},
  year={2022}
}

@article{qiu2023not,
  title={Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization},
  author={Qiu, Zi-Hao and Hu, Quanqi and Yuan, Zhuoning and Zhou, Denny and Zhang, Lijun and Yang, Tianbao},
  journal={arXiv preprint arXiv:2305.11965},
  year={2023}
}

@inproceedings{sebbouh2021almost,
  title={Almost sure convergence rates for stochastic gradient descent and stochastic heavy ball},
  author={Sebbouh, Othmane and Gower, Robert M and Defazio, Aaron},
  booktitle={Conference on Learning Theory},
  pages={3935--3971},
  year={2021},
  organization={PMLR}
}


%%%%% Papers for data curation 
@misc{dubey2024_llama3,
  doi = {10.48550/ARXIV.2407.21783},
  url = {https://arxiv.org/abs/2407.21783},
  author = {Dubey,  Abhimanyu and Jauhri,  Abhinav and others},
  keywords = {Artificial Intelligence (cs.AI),  Computation and Language (cs.CL),  Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {The Llama 3 Herd of Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}


@InProceedings{pmlr-v235-wettig24a,
  title = 	 {{Q}u{R}ating: Selecting High-Quality Data for Training Language Models},
  author =       {Wettig, Alexander and Gupta, Aatmik and Malik, Saumya and Chen, Danqi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {52915--52971},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wettig24a/wettig24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/wettig24a.html}
}

@misc{penedo2024finewebdatasetsdecantingweb,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      eprint={2406.17557},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17557},
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}


@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{gordon-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning",
    author = "Gordon, Andrew  and
      Kozareva, Zornitsa  and
      Roemmele, Melissa",
booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "394--398",
}

@inproceedings{welbl-etal-2017-crowdsourcing,
    title = "Crowdsourcing Multiple Choice Science Questions",
    author = "Welbl, Johannes  and
      Liu, Nelson F.  and
      Gardner, Matt",
    booktitle = "Proceedings of the 3rd Workshop on Noisy User-generated Text",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    pages = "94--106",
}



@article{liu2020logiqa,
  title={Logiqa: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.08124},
  year={2020}
}

@inproceedings{pilehvar-camacho-collados-2019-wic,
    title = "{W}i{C}: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations",
    author = "Pilehvar, Mohammad Taher  and
      Camacho-Collados, Jose",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1267--1273",
}

@article{liu2023logiqa,
  title={Logiqa 2.0—an improved dataset for logical reasoning in natural language understanding},
  author={Liu, Hanmeng and Liu, Jian and Cui, Leyang and Teng, Zhiyang and Duan, Nan and Zhou, Ming and Zhang, Yue},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2023},
  publisher={IEEE}
}



@article{polo2024tinybenchmarks,
  title={tinyBenchmarks: evaluating LLMs with fewer examples}, 
  author={Felipe Maia Polo and Lucas Weber and Leshem Choshen and Yuekai Sun and Gongjun Xu and Mikhail Yurochkin},
  year={2024},
  eprint={2402.14992},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
  }

