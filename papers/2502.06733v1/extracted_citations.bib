@article{chen2024take,
  title={Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization},
  author={Chen, Xuxi and Wang, Zhendong and Sow, Daouda and Yang, Junjie and Chen, Tianlong and Liang, Yingbin and Zhou, Mingyuan and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2402.14270},
  year={2024}
}

@article{fan2023irreducible,
  title={Irreducible Curriculum for Language Model Pretraining},
  author={Fan, Simin and Jaggi, Martin},
  journal={arXiv preprint arXiv:2310.15389},
  year={2023}
}

@article{fang2020rethinking,
  title={Rethinking importance weighting for deep learning under distribution shift},
  author={Fang, Tongtong and Lu, Nan and Niu, Gang and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11996--12007},
  year={2020}
}

@article{grangier2023adaptive,
  title={Adaptive training distributions with scalable online bilevel optimization},
  author={Grangier, David and Ablin, Pierre and Hannun, Awni},
  journal={arXiv preprint arXiv:2311.11973},
  year={2023}
}

@inproceedings{jiang2007instance,
    title = "Instance Weighting for Domain Adaptation in {NLP}",
    author = "Jiang, Jing  and
      Zhai, ChengXiang",
    editor = "Zaenen, Annie  and
      van den Bosch, Antal",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    pages = "264--271"
}

@article{jiang2019accelerating,
  title={Accelerating deep learning by focusing on the biggest losers},
  author={Jiang, Angela H and Wong, Daniel L-K and Zhou, Giulio and Andersen, David G and Dean, Jeffrey and Ganger, Gregory R and Joshi, Gauri and Kaminksy, Michael and Kozuch, Michael and Lipton, Zachary C and others},
  journal={arXiv preprint arXiv:1910.00762},
  year={2019}
}

@article{jiang2024importance,
  title={Importance Weighting Can Help Large Language Models Self-Improve},
  author={Jiang, Chunyang and Chan, Chi-min and Xue, Wei and Liu, Qifeng and Guo, Yike},
  journal={arXiv preprint arXiv:2408.09849},
  year={2024}
}

@inproceedings{katharopoulos2018not,
  title={Not all samples are created equal: Deep learning with importance sampling},
  author={Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={2525--2534},
  year={2018},
  organization={PMLR}
}

@article{liu2021probabilistic,
  title={Probabilistic margins for instance reweighting in adversarial training},
  author={Liu, Feng and Han, Bo and Liu, Tongliang and Gong, Chen and Niu, Gang and Zhou, Mingyuan and Sugiyama, Masashi and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23258--23269},
  year={2021}
}

@article{loshchilov2015online,
  title={Online batch selection for faster training of neural networks},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1511.06343},
  year={2015}
}

@article{qi2021online,
  title={An online method for a class of distributionally robust optimization with non-convex objectives},
  author={Qi, Qi and Guo, Zhishuai and Xu, Yi and Jin, Rong and Yang, Tianbao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10067--10080},
  year={2021}
}

@inproceedings{ren2018learning,
  title={Learning to reweight examples for robust deep learning},
  author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  booktitle={International conference on machine learning},
  pages={4334--4343},
  year={2018},
  organization={PMLR}
}

@article{sow2023doubly,
  title={Doubly Robust Instance-Reweighted Adversarial Training},
  author={Sow, Daouda and Lin, Sen and Wang, Zhangyang and Liang, Yingbin},
  journal={arXiv preprint arXiv:2308.00311},
  year={2023}
}

@article{thakkar2023self,
  title={Self-influence guided data reweighting for language model pre-training},
  author={Thakkar, Megh and Bolukbasi, Tolga and Ganapathy, Sriram and Vashishth, Shikhar and Chandar, Sarath and Talukdar, Partha},
  journal={arXiv preprint arXiv:2311.00913},
  year={2023}
}

@article{xie2023doremi,
  title={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={arXiv preprint arXiv:2305.10429},
  year={2023}
}

@article{yi2021reweighting,
  title={Reweighting augmented samples by minimizing the maximal expected loss},
  author={Yi, Mingyang and Hou, Lu and Shang, Lifeng and Jiang, Xin and Liu, Qun and Ma, Zhi-Ming},
  journal={arXiv preprint arXiv:2103.08933},
  year={2021}
}

@inproceedings{zeng2021adversarial,
  title={Are adversarial examples created equal? A learnable weighted minimax risk for robustness under non-uniform attacks},
  author={Zeng, Huimin and Zhu, Chen and Goldstein, Tom and Huang, Furong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={10815--10823},
  year={2021}
}

@article{zhang2020geometry,
  title={Geometry-aware instance-reweighted adversarial training},
  author={Zhang, Jingfeng and Zhu, Jianing and Niu, Gang and Han, Bo and Sugiyama, Masashi and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2010.01736},
  year={2020}
}

