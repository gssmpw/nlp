% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
%\usepackage{arydshln} This package changes the display the other tables


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{titlesec}
% {command}{left spacing}{before spacing}{after spacing}[right]
% Spacing: how to read {12pt plus 4pt minus 2pt}
% 12pt is what we would like the spacing to be.
% plus 4pt means that TeX can stretch it by at most 4pt.
% minus 2pt means that TeX can shrink it by at most 2pt.
% This is one example of the concept of, 'glue', in TeX.
\titlespacing*\section{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing*\subsection{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing*\subsubsection{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 2pt}


\newcommand{\todo}[1]{\textcolor{pink}{\textbf{TODO: }#1}}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  \textbf{Qiuhai Zeng\textsuperscript{1}\thanks{These authors contributed equally to this work.}} \quad
  \textbf{Claire Jin\textsuperscript{2}\footnotemark[1]} \quad
  \textbf{Xinyue Wang\textsuperscript{1}} \quad 
  \textbf{Yuhan Zheng\textsuperscript{3}} \quad
  \textbf{Qunhua Li\textsuperscript{1}\thanks{Corresponding author: qunhua.li@psu.edu.}}\\
  \\
  \textsuperscript{1}Pennsylvania State University,
  \textsuperscript{2}Carnegie Mellon University,
  \textsuperscript{3}International Monetary Fund\\
  \small{
    \texttt{\{qjz5084, xpw5228, qunhua.li\}}\href{mailto:}{\textcolor{black}{@psu.edu}}, \ 
    \href{mailto:}{\textcolor{black}{\texttt{claireji@andrew.cmu.edu}}}, \
    \href{mailto:}{\textcolor{black}{\texttt{helenzheng099@gmail.com}}}
  }
}




%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) have demonstrated 
potential for data science tasks via code generation. However, the exploratory nature of data science, alongside the stochastic and opaque outputs of LLMs, raise concerns about their reliability. While prior work focuses on benchmarking LLM accuracy, reproducibility remains underexplored, despite being critical to establishing trust in LLM-driven analysis. \\
We propose a novel analyst-inspector framework to automatically evaluate and enforce the reproducibility of LLM-generated data science workflows — the first rigorous approach to the best of our knowledge. Defining reproducibility as the sufficiency and completeness of workflows for reproducing functionally equivalent code, this framework enforces computational reproducibility principles, ensuring transparent, well-documented LLM workflows while minimizing reliance on implicit model assumptions. \\
Using this framework, we systematically 
evaluate five state-of-the-art LLMs on 1,032 data analysis tasks across three diverse benchmark datasets. We also introduce two novel reproducibility-enhancing prompting strategies. 
Our results show that higher reproducibility strongly correlates with improved accuracy and reproducibility-enhancing prompts are effective, demonstrating structured prompting’s potential to enhance automated data science workflows and enable transparent, robust AI-driven analysis. Our code is publicly available\footnote{\href{https://github.com/qunhualilab/LLM-DS-Reproducibility}{https://github.com/qunhualilab/LLM-DS-Reproducibility}}.

\end{abstract}

\setlength{\belowcaptionskip}{-15pt}
\setlength{\belowdisplayskip}{4pt} 
\setlength{\abovedisplayskip}{4pt} 

\section{Introduction}
The advancement of large language model (LLM) analytical and quantitative reasoning abilities, such as solving math problems, writing code, and analyzing tabular data 
has sparked an interest in their potential for data science \citep{wang2024large, zhu2024large, gu2024blade}. 
Recent work on LLMs in data science has explored methods to improve their accuracy, from prompting strategies to multi-agent frameworks  to fine-tuning models \citep{liu2024llms,  majumder2024datadrivendiscoverylargegenerative, zhu2024large}. Various benchmarks have also been developed to assess LLM performance across core data analysis tasks, including table understanding \citep{yu2018spider, hazoom2021text, zhao2022multihiertt}, hypothesis testing \citep{liu2024llms, zhu2024large}, and multi-step workflows \citep{majumder2024discoverybench}.

Unlike mathematics, where questions typically have single correct answers, data science is inherently open-ended and exploratory \citep{yu2020veridical}. 
Prior studies have shown significant variation across defensible approaches and outcomes among human expert analysts performing the same task \citep{breznau2022observing, menkveld2024nonstandard, botvinik2020variability}. Given this variability, 
ensuring reproducibility in analysis workflows is crucial for independent verification of results using the same dataset and analysis pipeline \citep{davidson2008provenance,leek2015reproducible, NAP25303}. This practice fosters confidence in findings, facilitates knowledge sharing and reuse, and is widely advocated in scientific guidelines \citep{lee2022investigating, NAP25104}. However, current methods for assessing computational reproducibility are predominantly manual and lack standardized, quantifiable approaches.


\begin{figure*}[t]
  \includegraphics[width=\linewidth]{human_in_loop_overview.png} 
  \caption { 
A \textbf{human-in-the-loop pipeline} for \textbf{AI-generated data analysis}, demonstrating our reproducibility assessment framework. Given a data analysis task, an AI Analyst generates a workflow (analysis steps), code, and conclusion.  To ensure reproducibility and robustness, an \textbf{independent AI Inspector} executes the workflow to generate new code and a new conclusion. If these mismatch the originals, the AI Analyst must revise or redo its solution. Once reproducibility is confirmed, a human analyst assesses the \textit{workflow} for correctness. Since reproducibility implies the workflow accurately and sufficiently details the analytical steps, the analyst can focus on \textbf{high-level validation} rather than verifying code manually. If the workflow is sound, the analysis proceeds; otherwise, it is revised. This process \textbf{improves both trust and efficiency }in AI-assisted data analysis.}
  \label{fig:hci-pipeline}
\end{figure*}

The rise of LLMs as data science tools introduces new challenges to reproducibility. While LLMs excel at code automation and generating natural language reports \citep{Mayfield_2024, rasheed2025large}, their stochastic outputs and model-specific variations can lead to inconsistencies in analysis results. Consequently, it remains unclear whether analyses generated by one LLM can be reliably reproduced by another LLM or a human analyst. Therefore, rigorously assessing the reproducibility of LLM-generated analyses is imperative to ensure that incorporating these tools into real-world data analysis enhances the transparency and reliability of research and data-driven decision-making. However, to the best of our knowledge, no prior work has addressed this issue.


We therefore introduce a novel analyst-inspector framework (Figure \ref{fig:hci-pipeline}) for automatically evaluating and enforcing the reproducibility of LLM-generated statistical analyses, drawing inspiration from the concept of scientific reproducibility described above. This framework is grounded in the classical statistical concepts of completeness and sufficiency, providing a rigorous foundation for assessing the computational reproducibility of LLM solutions for data science.

Using this framework, we perform a systematic evaluation on five state-of-the-art LLMs
and investigate the following critical questions:\\
\textit{\textbf{Q1:} 
   To what extent can current LLMs produce reproducible analyses?
   } \\
\textit{\textbf{Q2:} 
   What is the relationship between reproducibility and accuracy in LLM-generated analyses, and does improving reproducibility enhance accuracy?
   } \\   
\textit{\textbf{Q3:} How do prompting techniques and analysis types influence the reproducibility and accuracy of LLM-generated analyses? } \\
\textit{\textbf{Q4:} How can prompting strategies be designed to improve reproducibility of out-of-the-box models? } 


We summarize our contributions as follows:
\begin{enumerate}
\setlength{\itemindent}{0em}
\itemsep-0.3em 
    \item A rigorous framework for systematically evaluating reproducibility of AI-generated data analyses - the first of its kind, to the best of our knowledge. 
    \item A comprehensive comparison of reproducibility and accuracy of data analyses by open and closed source LLMs, and comparison with human performance.
    \item Novel Reproducibility-of-Thought (RoT) and Reproducibility-Reflexion (RReflexion) prompting strategies to enhance the quality of LLM-generated data science.
    %\item A curated dataset of human-generated code solutions and workflows.
\end{enumerate}

\section{Related Work}

Recent advancements in LLM code generation have garnered significant attention, exemplified by tools like GitHub Copilot \citep{chen2021evaluatinglargelanguagemodels} and CodeGeeX \citep{Zheng2023CodeGeeX}, which leverage code LLMs to streamline software development. 
%As modern data analysis relies heavily on code, 
These advancements have spurred growing interests in LLM-driven code generation for data tasks \citep{zhu2024large, gu2024blade}, driving progress in automating data manipulation, visualization, and statistical analysis \citep{hong2024datainterpreter, guo2024ds, Li2024AutomatedSM}.

While prior work examines both general-purpose models (e.g., GPT-4 \citep{openai2024gpt4}, Llama 3 \citep{grattafiori2024llama3}, Claude 3 \citep{TheC3}) and code-specialized variants (e.g., Code LLaMA \citep{rozière2024codellama}, DeepSeek-Coder \citep{guo2024deepseek}), empirical evaluations in data analysis code generation remain dominated by studies of general-purpose models, particularly the GPT family \citep{hong2024datainterpreter, gu2024blade, Li2024AutomatedSM, guo2024ds}.

\subsection{Methodologies and Frameworks}

Recent work has explored a number of techniques to enhance LLM capabilities for data science tasks. For example, several works \citep{majumder2024discoverybench, liu2024llms, zhu2024large} test existing general-purpose prompting frameworks like chain-of-thought \cite{wei2022chain} and self-reflection \cite{shinn2024reflexion}. %Several works 
Some propose more complex, data-science-specific paradigms --- for example a hierarchical graph modeling framework \citep{hong2024datainterpreter} and a multi-agent framework for iterative revisions \citep{guo2024ds} --- which are also implemented via prompting. Performance effects of fine-tuning LLMs on data science questions have also been benchmarked \cite{zhu2024large}.

\subsection{LLM-Assisted Data Science} 
As LLM-generated data analyses are increasingly integrated into real-world pipelines, recent research has focused on understanding their practical use, particularly how human analysts engage with the LLM assistants.
TalkToEBM \citep{bordt2024datascience} investigates how LLMs can provide interpretable model summarization and graph explanation, while CliniDSBench \citep{wang2024large} 
presents an interactive LLM-based platform to streamline medical data analysis coding. \citet{nascimento2024llm4ds} evaluates popular interfaced LLM coding tools as data science programming tools.
\citet{nejjar2024llms} performs a systematic study of commercially available LLM coding tools used in real-world research for 
data analysis and visualization, finding that some tools had wrong and misleading analysis results due to difficulty picking up important details. 

\subsection{Datasets and Benchmarks}
Several benchmarks have been introduced to assess LLM performance on data science and data reasoning tasks. DS-1000 \citep{lai2022ds1000} and ExeDS \citep{huang2022execution} explore generation of code for building statistical models and making visualizations, 
while additional benchmarks assess LLM data understanding without extensive code generation (e.g. tabular data understanding) \cite{zhao2022multihiertt, yu2018spider, hazoom2021text}. 
More recently, DiscoveryBench, StatQA, DAEval, and QRData, \citep{majumder2024discoverybench, zhu2024large, hu2024infiagent, liu2024llms} build on prior works by targeting models’ abilities to code multi-step, data-driven workflows using appropriate statistical reasoning and knowledge.

\subsection{Reproducibility in LLMs}

Existing research on the reproducibility of LLM outputs focuses on quantifying their consistency as evaluators \citep{lee2024evaluatingconsistency} and question-answering tools \citep{lee2024evaluatingconsistenciesllm}. Additionally, concerns about the reproducibility of studies investigating LLMs have emerged due to their opacity and experimental design standardization\citep{vaugrante2024looming}. However, to the best of our knowledge, no prior work has addressed reproducibility of LLM-generated data analyses. Thus, our work fills a critical gap by ensuring scientific rigor of LLM-assisted data analysis, which is becoming an integral part of modern research.

\section{LLM Reproducibility} \label{sec:LLMRepr}
Our approach to evaluating and enforcing reproducibility in LLM-generated analyses is inspired by the scientific community's best practices: workflows must include all necessary details (e.g., data processing, model choices, parameters) alongside publicly released code to enable independent replication \citep{NAP25303}. To hold LLM-generated analyses these standards,
our approach introduces an independent LLM inspector who, given only the workflow and minimal context (e.g., data filenames), attempts to reproduce the results. Successful reproduction confirms the workflow provides sufficient detail for computational reproducibility.
This approach is particularly relevant to LLM-assisted data analysis, where human oversight is crucial but detailed code review is time-consuming. 
Reproducible LLM solutions allow human inspectors to validate workflows directly, bypassing exhaustive code verification (Figure~\ref{fig:hci-pipeline}). %and balancing rigor with efficiency.


\subsection{Reproducibility Framework}

Our formal reproducibility framework (Figure \ref{framework}) is defined as follows.
Let \( D \) represent a data science task, consisting of input data, contextual information, and a data science question. 
Let \( A \) be an analyst agent that generates analysis solutions according to a probabilistic distribution \( f_A \).
The function \( f_A \) is induced by the model's architecture and learned parameters, from which its statistical reasoning and code generation capabilities emerge.

When responding to task \( D \), analyst \( A \) produces a solution tuple $(W_A, C_A)$, where 

 $\bullet$ \( W_A \sim f_A(W \mid D)\) denotes the workflow, encapsulating $A$'s reasoning and analysis plan. 
 
$\bullet$ \( C_A \sim f_A(C \mid D, W_A)\) denotes the corresponding code implementation following $W_A$.

To assess the reproducibility of $A$'s solution, we introduce an independent inspector agent \( I \) that evaluates whether the workflow \( W_A \) allows $I$ to reproduce the analysis conclusion by generating a new code implementation \( C_I \) from $W_A$, i.e., 
\[
C_I \sim f_I(C \mid W_A),
\]  
where \( f_I \) is $I$'s probabilistic distribution for generating code implementations given a workflow. We then define the criterion for a reproducible solution as follows.

\textbf{Definition:} {\it Given a task \( D \), workflow \( W_A \) is reproducible if and only if \( C_A \equiv C_I \), where \( \equiv \) holds if \( C_A \) and \( C_I \) produce the same result with respect to \( D \). 
If $W_A$ is reproducible, then its corresponding code implementation \( C_A \) is also reproducible. Thus the analysis $(W_A, C_A)$ is reproducible.} 

In our framework, \( C_A \equiv C_I \) is instantiated as functional equivalence of code, %meaning that 
that is, the outcomes derived from their deterministic code execution, \( O_A=o(C_A) \) and \( O_I=o(C_I) \), are consistent, even if \( C_A \) and \( C_I \) differ texturally. This criterion represents the highest level of code similarity, i.e. consistency in program functionality, as discussed in \citep{zakerinasrabadi2023systematicliteraturereviewsource}.  

\subsection{Reproducibility is Sufficient and Complete}

Our reproducibility framework ensures that a workflow encapsulates all necessary information to generate functionally equivalent code while avoiding unnecessary complexity.

To see this, first apply our framework to a self-check scenario where the original agent \( A \) also serves as the inspector. During inspection, \( A \) regenerates code from its original workflow \( W_A \), producing $
C_A^* \sim f_A(C \mid W_A).
$ 
If \( C_A \equiv C_A^* \), then probabilistically,   
\[
f_A(C \mid D, W_A) = f_A(C \mid W_A),
\]  
which implies that, given \( W_A \), \( C \) is conditionally independent of \( D \), i.e. $C \perp D \mid W_A$. This means \( D \) contributes no additional information to $W_A$ for generating \( C \), demonstrating that our framework ensures that \( W_A \) sufficiently encapsulates all necessary information for reproducing functionally equivalent code within the same agent.

To achieve reproducibility, our framework strengthens this sufficiency by introducing an independent inspection agent $I$ with potentially different $f_I$. This imposes a stricter reproducibility requirement:
\[
f_A(C \mid D, W_A) = f_I(C \mid W_A).
\]
This condition ensures $W_A$ does not rely on implicit assumptions specific to $A$, making reproducibility independent of agent internal knowledge — a common barrier to successful replication \citep{NAP25303}.

From a theoretical perspective, our criterion aligns with the principles of sufficiency and completeness in statistics \citep{casella2024statistical}, \\
\textbf{Sufficiency}: $W_A$ captures all relevant information from $D$ required to generate $C_A$, ensuring the inspector does not need additional task-specific knowledge.\\
\textbf{Completeness}: $W_A$ excludes extraneous details that could introduce inconsistencies or ambiguity.

By enforcing both sufficiency and completeness, our reproducibility criterion ensures that a workflow is self-contained, independent of agent-specific biases, and suitable for independent verification while avoiding unnecessary complexity.


\subsection{Reproduciblity Enhancing Prompts} \label{sec:ReprEnhPrompt}

To address \textbf{\textit{Q2}} and \textbf{\textit{Q4}}, we propose two prompting strategies to elicit reproducibility in LLM-generated solutions, Reproducibility-of-Thought (RoT) and Reproducibility-Reflexion (RReflexion). (1) RoT builds on Chain-of-Thought (CoT) \cite{wei2022chain} prompting with 
the explicitly reproducibility-oriented instruction: ``Make sure a person can replicate the action input by only looking at the workflow, and the action input reflects every step of the workflow.'' By building directly on CoT, RoT will allow us to assess the effect of reproducibility instruction in a controlled manner. (2) RReflexion, inspired by \citep{shinn2024reflexion}, is an iterative strategy utilizing our framework's inspector reproducibility evaluation as an external feedback signal to the CoT analyst. This emulates the situation depicted in Figure ~\ref{fig:hci-pipeline}, where the reproducibility check fails, and the AI analyst is asked to redo or revise the analysis. These techniques are further discussed in Section \ref{sec:LLMAgents}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{framework_overview.png}
  \caption{Analyst-Inspector framework for assessing LLM data analysis reproducibility. 
  }
\label{framework}
\end{figure}

\section{Experiment Design} 
\subsection{Datasets}
To evaluate the performance of LLMs in statistical analysis, we compiled a diverse set of 1,032 question-answer  (QA) pairs (details in Table~\ref{tab:dataset}) sourced from three 
data science benchmarks: DiscoveryBench \citep{majumder2024discoverybench}, QRData \citep{liu2024llms}, and StatQA \citep{zhu2024large}. 
(1) The DiscoveryBench dataset comprises 264 tasks from real-world scientific workflows and 903 synthetic tasks, covering 6 domains. 
We focused on the 239 real-world tasks in the `test' set. Many of these tasks require multi-step data processing and exploratory analysis to determine the appropriate analysis approach.
(2) The QRData benchmark tests advanced quantitative reasoning using 411 questions paired with data sheets from textbooks, online learning materials, and academic papers.
For our evaluation, we removed 18 flawed multiple-choice questions as detailed in Appendix \ref{sec:Cat}.
(3) The StatQA dataset, designed for statistical analysis, contains 11,623 tasks spanning five categories: descriptive statistics, correlation analysis, contingency table tests, distribution compliance tests, and variance tests. From its `mini-StatQA subset', consisting of 1,163 tasks, we randomly selected 80 tasks per category for balanced representation and to manage computational cost.


These benchmarks were selected to assess diverse aspects of data analysis and statistical reasoning. An example from each benchmark dataset is provided in Appendix Table \ref{tab:CatExamples}. We classified all QA pairs into three categories: `Numerical', `Categorical', and `Textual' (Appendix \ref{sec:Cat}).  


\begin{table}
  \centering
  \begin{tabular}{lccc|c}
    \hline
    \textbf{Dataset} & \textbf{Num} & \textbf{Cat} & \textbf{Txt} & \textbf{Total} \\
    \hline
    DiscoveryBench   & 90                 & 86                   & 63               & 239           \\
    QRData           & 163                & 230                  & 0                & 393           \\
    StatQA           & 75                 & 249                  & 76               & 400           \\
    \hline
    \textbf{Total}   & 328                & 565                  & 139              & 1032          \\
    \hline
  \end{tabular}
  \caption{
  QA pair type distributions by dataset. 
  }
  \label{tab:dataset}
\end{table}


\subsection{LLM Prompting Strategies and Models} \label{sec:LLMAgents}
On the aforementioned data, we evaluated the following LLMs in combination with each of the below prompting strategies.

\noindent\textit{Proprietary LLMs:} GPT-4o (GPT-4o-2024-11-20) \citep{openai2024gpt4}, Claude-3.5-sonnet (v2) \citep{TheC3}, o3-mini (o3-mini-2025-01-31) \citep{openai2025o3mini}

\noindent\textit{Open-source LLMs:} Llama-3.3-70B \citep{grattafiori2024llama3}, DeepSeek-R1-70B \citep{guo2025deepseek}

\noindent\textit{Prompting strategies} (prompts in Appendix \ref{sec:PromptTemp}):

\textbf{Chain-of-Thought} (CoT) follows the approach introduced in \citet{wei2022chain}, using the line ``let's think step-by-step'' to elicit reasoning.

\textbf{Reproducibility-of-Thought} (RoT), described in Section \ref{sec:ReprEnhPrompt}, instructs the LLM to incorporate reproducibility in its CoT reasoning while generating the workflow and code.

\textbf{Reproducibility-Reflexion (RReflexion)} incorporates the inspector's reproducibility assessment as external feedback for re-analysis via CoT reasoning when the initial analysis was irreproducible. 

\textbf{ReAct}, introduced by \citet{yao2022react}, iteratively alternates between workflow design, code generation, and code execution to conduct multi-turn data analysis.



In all the above prompts, we provide essential context, including detailed dataset descriptions and metadata, and any available relevant domain knowledge. All prompting strategies were equipped with a code execution tool. Maximum number of LLM calls and code executions per sample for each prompting strategy are in Appendix Table \ref{tab:max_llm_code}. 

\begin{figure*}[htbp]
  \includegraphics[width=\linewidth]{acc_analysis.pdf} 
  \caption {Accuracy and reproducibility of LLMs across datasets for different agent types. In each dataset, for every agent type, the average metric scores calculated from different LLMs are connected by the solid lines.
}
\label{AccRepr}
\end{figure*}

\subsection{Evaluation and Performance Analysis} \label{sec:EvalMetrics}
We assessed performance through accuracy and reproducibility. To compute accuracy, analysis correctness was determined by comparing the LLM analysis conclusions to the benchmark ground truth. 
For numerical answers, the LLM conclusion agrees with the ground truth if the deviation between LLM conclusion and the ground truth is within a predefined error threshold (Appendix \ref{sec:EvalPrompt}). As the majority of LLM conclusions and ground truth answers are presented in natural language, we employ an evaluator LLM (GPT-4o-2024-11-20) to assess agreement with the ground truth.
Reproducibility is evaluated via the procedure detailed in Section \ref{sec:LLMRepr} with GPT-4o-2024-11-20 as the inspector agent. 
Prompts for the answer agreement evaluation and reproducibility inspection are detailed in Appendix \ref{sec:EvalPrompt}. To ensure the trustworthiness of LLM accuracy evaluations and reproducibility inspections, a human expert manually verified the LLM assessment for a randomly selected subset of 350 samples (details in Appendix \ref{sec:Alignment}). 


For Llama-3.3, GPT-4o, Claude-3.5-sonnet, and o3-mini, we used a regex parser to extract the workflow, code, and conclusion from the LLM output. For DeepSeek-R1-70B, the parser extracted the code and conclusion, while all reasoning text preceding the code was treated as the workflow. 
For CoT and RoT, which generate a single iteration of workflow and code, reproducibility was assessed on that output. For RReflexion and ReAct, which are iterative, only the final workflow and code were evaluated. 
Solutions with inexecutable code were considered inaccurate and irreproducible.

To assess the impact of LLM choice, prompting strategy and task specification (i.e. benchmark dataset, statistical question categories, QA type) on analysis accuracy and reproducibility, 
we employ the following regression model, where $Y$ represents either accuracy or reproducibility:
\begin{equation*}
Y= \beta_0 + \beta_L LLM + \beta_p Prompt + \beta_d Task
\end{equation*}


\subsection{Human Data Collection}

Data analysis tasks are inherently exploratory and open-ended, leading to variability in solutions --- a well-documented phenomenon in prior studies of human analysts \citep{botvinik2020variability, breznau2022observing}. Thus, to fairly assess the overall performance of LLMs, we establish a human analyst baseline by collecting expert solutions on the DiscoveryBench dataset. Two experienced postgraduate data analysts independently solved half of the tasks following the instructions in Appendix Table \ref{InstructionPrompt}. Each analyst spent approximately 120 hours developing comprehensive workflows and code solutions. 
We assessed these solutions using the methodology outlined in Section \ref{sec:EvalMetrics}.

\section{Results}

\subsection{Overall Performance of LLMs}

We present accuracy, reproducibility, and the proportion of executable code for solutions generated by each LLM and prompting strategy combination across the three benchmark datasets in Appendix Tables \ref{tab:discoverybench} and \ref{tab:qrdata-statqa}.

Comparing performance across the three benchmark datasets, all LLMs performed best on StatQA (mean accuracy 75.91\%), followed by QRData (54.9\%), with the lowest performance on DiscoveryBench (28.72\%). The differences in accuracy (Figure \ref{AccRepr}) and executable code proportion (Appendix Figure \ref{CodeNotRun}) across the datasets were particularly pronounced, while variations in reproducibility (Figure \ref{AccRepr}) were less striking, but followed the same trend. This pattern reflected the varying level of task complexity across the datasets (easiest to hardest: StatQA, QRData, DiscoveryBench). 

Among the five LLMs we assessed, o3-mini achieved the highest overall accuracy on the two more challenging datasets, DiscoveryBench (mean accuracy 32.64\%) and QRData (60.18\%), and was among the top performers in StatQA (77.56\%). Claude-3.5-sonnet ranked second in accuracy on the two harder datasets and attained the highest accuracy on StatQA (80.56\%).
GPT-4o exhibited inconsistent performance across benchmarks, ranking among the best on StatQA, lowest on QRData, and mid-range on DiscoveryBench (Figure \ref{AccRepr}). Llama-3.3 maintained moderate performance across all benchmarks. DeepSeek-R1-70B performed the weakest overall, having the lowest accuracy in DiscoveryBench and StatQA and a mid-tier ranking in QRData.

In terms of reproducibility, o3-mini consistently outperformed  all other LLMs across datasets (Figure \ref{AccRepr}), with a substantial margin on the two  harder datasets 
(DiscoveryBench: 55.23-71.55\%, QRData: 56.74-90.33\%, StatQA: 81-94.25\%). GPT-4o and DeepSeek-R1-70B formed the middle tier, exhibiting comparable reproducibility on the two harder datasets, while GPT-4o matched o3-mini on StatQA. At the lower end, Claude-3.5-sonnet consistently showed the lowest reproducibility across most prompting strategies and datasets (DiscoveryBench: 12.55-28.03\%, QRData: 27.23-45.04\%, StatQA: 47.75-53.5\%). Llama-3.3 performed similarly poorly as Claude-3.5-sonnet on the two harder datasets but reached a reproducibility rate comparable to DeepSeek-R1-70B on StatQA.

We also observed significant differences in the proportion of executable code across models (Appendix Figure \ref{CodeNotRun}). Specifically, o3-mini consistently generated the highest proportion of executable code across all datasets, while DeepSeek-R1-70B produced the lowest.  
The other three LLMs formed the middle tier, exhibiting similar performance with some variations across different datasets. 


\subsection{Impact of Prompting Strategies}
\label{results:prompting}

Compared with CoT, both RoT and RReflexion resulted in substantial improvements in reproducibility rate (Figure \ref{AccRepr} and Appendix \ref{sec:AppendixA}). For example, on DiscoveryBench, switching from CoT to RoT with GPT‑4o raised overall reproducibility from 42.68\% to 48.54\%, while RReflexion pushed it even further to 58.58\%. Similarly, for o3‑mini on DiscoveryBench, reproducibility increased from 55.23\% (CoT) to 60.25\% (RoT), peaking at 71.55\% with RReflexion. Regression analysis, as described in Section \ref{sec:EvalMetrics}, confirmed that RReflexion significantly enhanced reproducibility (coefficients in Appendix Table \ref{tab:coef}). In particular, across all datasets, the RReflexion and o3‑mini combination consistently achieved the highest reproducibility rate. 

The RoT and RReflexion strategies not only improved reproducibility, but also enhanced accuracy across most LLM-dataset combinations. For instance, on QRData, GPT‑4o achieved accuracies 48.85\% (CoT), 50.89\% (RoT), and 51.15\% (RReflexion), while o3‑mini scored 58.52\%, 62.09\%, and 59.54\%, respectively. Notably, the accuracy gains over CoT were more pronounced on the simpler benchmarks, QRData and StatQA, than on the more complex DiscoveryBench. This suggests that while encouraging consistency between the generated code and the steps in the workflow boosts accuracy, it does not fully compensate for reasoning gaps in more complex tasks.
Therefore, enhancing the statistical reasoning that informs the workflow remains crucial for achieving higher accuracy in these cases.


Consistent with prior work \citep{yao2022react}, we found that the ReAct prompting strategy often improved accuracy over CoT. This benefit was especially noticeable for Claude-3.5-sonnet and DeepSeek-R1-70B (Figure \ref{AccRepr}). 
However, ReAct sometimes lowered reproducibility. For example, on DiscoveryBench Llama‑3.3's reproducibility dropped from 23.85\% (CoT) to just 12.97\% (ReAct). This is likely due to incomplete workflows in ReAct's later iterations, as the LLM sometimes referenced previously generated workflows despite explicit instruction to produce a complete workflow (prompts in Appendix \ref{sec:PromptTemp}). 
Notably, ReAct reduced code execution errors, likely by allowing the LLM to iteratively process code execution results and refine the code,
resulting in the highest proportion of executable code samples (Appendix Figure \ref{CodeNotRun}). 

\subsection{Reproducibility-Accuracy Relationship}
\begin{figure}[htbp]
  \includegraphics[width=\linewidth]{reproducibility_analysis.pdf} 
  \caption {Comparison of accuracy across datasets for CoT agent type using LLM models at varying reproducibility levels. For accuracy calculations at R=0, samples containing inexecutable code are excluded. The p value of paired one-sided t-test is less than 0.001.\\
  }
  \label{AccAtRepr}
\end{figure}

As briefly discussed in Section ~\ref{results:prompting}, our results suggest that reproducibility enhances accuracy.
Across all datasets and LLMs, we observed that reproducible workflow and code solutions (R = 1) achieved significantly higher accuracy compared to those deemed irreproducible (R = 0) (Figure \ref{AccAtRepr}, Appendix Figure \ref{AccAtReprAgents}). This trend suggests a positive correlation: when a workflow supports the independent generations of functionally equivalent codes, it is more likely to produce accurate results. 
The observed improvements of RoT over CoT demonstrate a causal link between explicit reproducibility instructions and improved accuracy. Additionally, a significant (two sample paired t-test p-value of 0.007) 4.15\% average accuracy gain in samples reanalyzed by RReflexion after incorporating the inspector reproducibility feedback further underscores reproducibility as a strong proxy for solution quality and accuracy, addressing \textit{\textbf{Q2}}.



\subsection{Performance Across Question Types}
\begin{figure}[htbp]
  \includegraphics[width=\linewidth]{category_analysis.pdf} 
  \caption {Accuracy and reproducibility of LLMs across different statistical question categories in StatQA.
 }
 \label{StatQACtgr}
\end{figure}

To investigate how different statistical question types affected performance (\textbf{\textit{Q3}}), we analyzed model performance based on statistical question categories defined by StatQA within the StatQA dataset (results in Appendix Tables \ref{CACTT}, \ref{DSDCT}, and \ref{VT}). Our analysis revealed distinct performance patterns across different statistical tasks (Figure \ref{StatQACtgr}). Descriptive Statistics (DS) showed the strongest overall performance, with all models achieving high accuracy (80.00-92.50\%) and reproducibility (71.25-98.75\%), particularly for o3-mini (90.00-98.75\%). 
Distribution Compliance Tests (DCT) and Variance Tests (VT) demonstrated similarly high accuracy (77.50-93.75\%), though reproducibility varied widely between models (o3-mini: 72.50-95.00\%, Claude-3.5-sonnet: 31.25-40.00\%). Contingency Table Tests (CTT) and Correlation Analysis (CA) showed more moderate accuracy (42.50-83.75\%), with notable differences in reproducibility across models, ranging from Claude-3.5-sonnet's 36.25-51.25\% to o3-mini's 80.00-93.75\%.

We also classified data analysis QA pairs into three types: numerical, categorical, and textual, and compared performance across types (Appendix Figure \ref{Ctgr}). This categorization is motivated by prior work suggesting that nature of a task can influence both human and model responses \citep{liu2024llms}. 
Our regression analysis (Appendix Table \ref{tab:coef}) showed that compared to numerical tasks, categorical and textual tasks had significantly higher accuracy, but both had significantly lower reproducibility.

\subsection{Human vs LLM Comparison}

Comparing human expert solutions with LLM solutions on DiscoveryBench, we observed a notable performance gap in overall accuracy (Appendix Table \ref{tab:discoverybench}). Human experts achieved an overall accuracy of 66.53\% and a reproducibility rate of 66.53\%, albeit at the cost of approximately 240 hours of total effort. In contrast, while LLMs generally lagged behind human experts in overall accuracy (23.01\%-35.56\%), certain configurations exhibited strong reproducibility. For example, o3-mini with RReflexion achieved a reproducibility rate of 71.6\%, surpassing that of the human experts. Moreover, the best LLM performance was often obtained using advanced prompting strategies (i.e., RoT, RReflexion, or ReAct), suggesting that future enhancements in prompt design and reproducibility practices could help narrow the gap between automated and manual approaches.

\section{Conclusion}
We propose a novel analyst-inspector framework to automatically evaluate and enforce reproducibility of LLM-generated data analysis solutions. We systematically evaluated open-source and proprietary models across diverse benchmarks, contextualizing their performance against human analysts. Our findings highlight reproducibility as critical to the quality and trustworthiness of LLM-generated analyses, addressing four critical questions that inform their integration into real-world data analysis pipelines.

We find that reproducibility directly enhances accuracy, validating this relationship through our RoT prompting strategy, which builds on CoT by prioritizing reproducibility. Through our RReflexion prompting strategy, we show that enabling the analyst agent to integrate the inspector agent’s reproducibility feedback into analysis revisions further improves reproducibility and accuracy.
Among the five LLMs assessed, o3-mini and Claude-3.5-sonnet produced the most accurate analyses, with o3-mini achieving the highest reproducibility. We identify prompting strategies that optimize accuracy, reproducibility, and code execution rates in different analytical contexts. Moreover, we identify specific statistical tasks and question-answer 
 types where LLMs underperform, highlighting a need for improved analytical and statistical reasoning.

Building on these findings, we recommend future research to focus on enhancing the statistical knowledge, code generation reliability, and analytical reasoning abilities of LLMs. Research into agentic systems to evaluate workflow correctness and directly check workflow-code alignment can also provide significant value. Finally, our analyst-inspector framework offers a blueprint for applying reproducibility in other technical domains beyond data science to foster transparency and trust.

\section{Acknowledgements}
This work is partially supported by NIH R01 GM109453 to QL. 

\section{Limitations}
Our study has three primary limitations.

(1) Despite our extensive evaluation of various prompting strategies, we did not investigate the potential benefits of fine-tuning LLMs.

(2) Although we employed an independent inspector agent to mitigate inherent biases in LLM evaluations of reproducibility, and manually verified its correctness, this approach cannot guarantee complete elimination of bias.

(3) While the benchmark datasets we selected cover a broad range of data science tasks, they may not fully capture the complexity and breadth of real-world data analysis scenarios. 

We also note that data analysis tasks are sometimes open-ended. Our evaluations in this study rely solely on predefined benchmark answers as the ground truth when measuring accuracy. Though we also collected and assessed human expert solutions to these benchmark tasks as a baseline for fair judgement of LLM performance, we acknowledge that evaluating against multiple correct ground truths would better capture the inherent flexibility of data science problems.

\section{Ethics Statement}

\paragraph{Existing Benchmark Licenses.} 
Our work builds upon publicly available datasets and models, ensuring compliance with their respective licenses. Below, we detail the licensing terms for the benchmarks we used and our own contributions.
\begin{itemize}
    \item \textbf{DiscoveryBench} \cite{majumder2024discoverybench}: Licensed under ODC-BY, permitting redistribution and modification with proper attribution.
    \item \textbf{QRData} \cite{liu2024llms}: Licensed under CC BY-NC 4.0, allowing non-commercial use with attribution.
    \item \textbf{StatQA} \citep{zhu2024large}: Licensed under GPL-3.0, requiring derivative works to adopt the same license.
\end{itemize}
Our use of these datasets adheres to the terms specified by their respective licenses, and we use them strictly for research purposes.

\paragraph{Codebase License.} We release our code under the MIT License, granting users permission to use, modify, and distribute the code with proper attribution. This choice aligns with our goal of advancing reliability of LLM-generated data analysis and reproducibility of scientific research.

\paragraph{Potential Risks.}
Our evaluation indicates that current LLMs are not yet fully reliable in generating reproducible and accurate data analyses. We suggest that users carefully review any LLM-generated solutions before using them to automate data analysis tasks. Additionally, while reproducible analyses are more likely to be correct, it is possible that both the analyst and inspector agents are incorrect in consistent ways. Thus, our framework is meant for human-in-the-loop systems as detailed in our motivation and in Figure \ref{fig:hci-pipeline}.

\bibliography{main}

\appendix

\section{Data Cleaning and QA Types} \label{sec:Cat}
The questions removed from the QRData benchmark were 18 causal relationship questions with one of the following two structures:

``Question: Which cause-and-effect relationship is more likely?

Answer options:

A: [item] causes [item] 

B: [item] causes [item] 

C:  The causal relation is double sided between [item] and [item]

D: No causal relationship exists''

``
Question: Which cause-and-effect relationship is more likely?

Answer options:

A: [item] causes [item] 

B: [item] causes [item] 

C: No causal relationship exists''

where all [item]'s are the same, rendering the answer options meaningless and duplicated.

`Numerical' responses provide quantitative values; `Categorical' responses consist of discrete labels or classifications; `Textual' responses involve free-form language that delivers narrative descriptions, interpretations, or explanations. Appendix Table \ref{tab:CatExamples} provides examples for each QA type. For StatQA, if the hypothesis test responses contradict each other, the QA type is classified as `Textual'. 

\section{Prompt Templates} \label{sec:PromptTemp}
Templates for different prompting strategies are provided in Appendix Tables \ref{CoTPrompt}, \ref{ReActPrompt}, \ref{ContinueReActPrompt}, and \ref{Continue2ReActPrompt}:
\begin{itemize}
\setlength{\itemindent}{0em}
    \item Appendix Table \ref{CoTPrompt}: CoT, RoT, and RReflexion templates
    \item Appendix Tables \ref{ReActPrompt}, \ref{ContinueReActPrompt}, \ref{Continue2ReActPrompt}: ReAct template
\end{itemize}

\section{Evaluation Prompts} \label{sec:EvalPrompt}
Evaluation prompt templates are provided in Appendix Tables \ref{AccuracyPrompt}, \ref{WorkflowCodePrompt}, \ref{ConclusionPrompt}, and \ref{ReprPrompt}:
\begin{itemize}
\setlength{\itemindent}{0em}
    \item Appendix Table \ref{AccuracyPrompt}: Accuracy assessment 
    \item Appendix Table \ref{WorkflowCodePrompt}: Workflow-to-code conversion
    \item Appendix Table \ref{ConclusionPrompt}: Conclusion extraction from code execution output
    \item Appendix Table \ref{ReprPrompt}: Reproducibility assessment
\end{itemize}


\section{Assessing Alignment of LLM Evaluator and Human Judgments} \label{sec:Alignment}
To validate the reliability of our automated evaluation framework, we conducted a systematic human validation study comparing manual scoring against LLM-generated scores.

For accuracy assessment, we randomly selected 25 examples from each QA type (numerical, categorical, and textual) across all three datasets (DiscoveryBench, QRData, and StatQA) from the GPT-4o CoT results. A human expert then independently assessed the accuracy of the LLM's predicted answers. Overall, among 200 examples, we found a 98.5\% agreement between the human evaluations and the LLM scoring, underscoring the high reliability of our automated accuracy assessment.

For reproducibility assessment, we randomly selected 50 examples from each dataset in the GPT-4o CoT results, resulting in a total of 150 samples. A human expert manually evaluated these examples using our established criteria in Section \ref{sec:LLMRepr}. The evaluation showed a 98.7\% agreement between the human assessments and the LLM scoring, confirming that our automated reproducibility assessment closely aligned with human judgment.

These high consistency rates (98.5\% for accuracy and 98.7\% for reproducibility) confirmed the reliability of our automated evaluation and demonstrated its suitability for large-scale assessments of LLM performance in data science tasks.

\section{Description of Results Tables and Figures} \label{sec:AppendixA}
Appendix Tables \ref{tab:discoverybench} and \ref{tab:qrdata-statqa} were used to generate Figures \ref{AccRepr} and \ref{AccAtRepr}, along with Appendix Figures \ref{CodeNotRun} and \ref{AccAtReprAgents}. Appendix Tables \ref{CACTT}, \ref{DSDCT}, and \ref{VT} were used to generate figure \ref{StatQACtgr}. Appendix Tables \ref{CACTT}, \ref{DSDCT}, and \ref{VT} showed the results of LLMs on different categories of StatQA tasks. Appendix Tables \ref{CatNum} and \ref{TXT} showed the results of LLMs on Numerical, Categorical and Textural QA types.

\section{Human Experts}

The two human expert data analysts who provided workflow and code solutions in this study were postgraduates (PhD, MS) with extensive experience in data analysis and fluency in English. They were recruited through the authors' personal connections. They were not paid, but instead credited as co-authors of this work. They were provided the instructions in Appendix Table \ref{InstructionPrompt} and were informed how their analysis solutions would be used.

\section{Computational Resources and Experimental Setup}

We conducted our experiments on a mix of locally hosted (open-source) and API-accessed (closed-source) LLMs. The locally hosted models included Llama-3.3-70B and DeepSeek-R1-70B, both of which were run on a cluster of four NVIDIA A6000 GPUs. The total computational budget was approximately 200 GPU hours for Llama-3.3-70B and 300 GPU hours for DeepSeek-R1-70B.  

For consistency, we set temperature to 0 for all models except o3-mini, which required a temperature of 1 to ensure compatibility with the LangChain AzureChatOpenAI package. The hyperparameters for each model were as follows:

\begin{itemize}
    \item \textbf{Llama-3.3-70B}: \texttt{repetition\_penalty = 1.18}, \texttt{num\_ctx = 8192}, \texttt{num\_predict = 2048}
    \item \textbf{DeepSeek-R1-70B}: \texttt{num\_ctx = 8196 \(\times\) 2}, \texttt{num\_predict = 6000}
    \item \textbf{GPT-4o}:  \texttt{max\_tokens = 2048}
    \item \textbf{Claude 3.5 Sonnet}: \texttt{max\_tokens = 2048}
    \item \textbf{o3-mini}:  \texttt{reasoning\_effort = low}, \texttt{max\_completion\_tokens = 4000}
\end{itemize}

We did not perform extensive hyperparameter tuning beyond selecting reasonable values based on prior literature and preliminary trials. Instead, we focused on evaluating model performance on structured data analysis tasks under fixed conditions to ensure a fair comparison across LLMs.

The LLM snapshots we used for the OpenAI models were 2025-01-31 for o3-mini and 2024-11-20 for GPT-4o.
The Azure API versions used were 2024-12-01-preview for o3-mini and 2024-10-01-preview for GPT-4o.

\section{Use of AI Assistants}

We used GitHub Copilot for code completion and Anthropic's Claude web interface to assist with debugging. Additionally, we used OpenAI's ChatGPT web interface to refine the clarity and smoothness of our writing. 

All AI-generated content, whether in code or text, was carefully reviewed, edited, and validated by the authors to ensure accuracy and alignment with our research objectives. No AI-generated content was used without human verification, and all final decisions regarding implementation and manuscript writing remained with the authors.

\begin{table*}[htbp]
    \centering
    \begin{tabular}{@{}p{0.17\linewidth} p{0.35\linewidth} p{0.25\linewidth} p{0.11\linewidth}@{}}
        \toprule
        \textbf{Dataset} & \textbf{Question} & \textbf{Conclusion} & \textbf{Type} \\ \midrule
        DiscoveryBench & What is the relationship of amber finds and number of monuments between 3400-3000 BCE? & Between 3400-3000 BCE, there is a high number of amber finds and a large number of monuments. & Textual \\[1ex]
        \midrule
        QRData & Which cause-and-effect relationship is more likely? A. Lumbago causes R S1 radiculopathy B. R S1 radiculopathy causes Lumbago & B & Categorical \\[1ex]
        \midrule
        StatQA & What is the kurtosis of the distribution of the variable representing Base Special Defense? & 2.39175 & Numerical \\ \bottomrule
    \end{tabular}
    \caption{Examples of different QA types.}
    \label{tab:CatExamples}
\end{table*}

\begin{table*}[htbp]
\centering
\begin{tabular}{lcc}
\toprule
Prompt           & \begin{tabular}[c]{@{}c@{}}Max LLM\\ Call\end{tabular} & \begin{tabular}[c]{@{}r@{}}Max Code\\ Execution\end{tabular}  \\
\midrule
CoT            & 2            & 1                  \\
RoT            & 2            & 1                  \\
RReflexion      & 3            & 1                  \\
ReAct          & 4            & 3                  \\
\bottomrule
\end{tabular}
\caption{Maximum number of LLM calls and code executions across different prompting strategies.}
\label{tab:max_llm_code}
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
Could you help work out the workflow (analysis steps) and code solutions on DiscoveryBench questions?\\

Below are the instructions:\\
 
To prepare solutions for given questions in a standardized format. Each solution should include:\\
1. A workflow explaining the steps and logic of the code.\\
2. Python code that computes the answer.\\[1ex]
 
Additional Requirements:\\
1.  The code should be written in a way that, when provided to another person, they can write a workflow with the same level of detail and functionality as your workflow.\\
2. The workflow should be detailed and clear enough that, when provided to another person, they can write functionally identical code solely based on the workflow.\\
    \hline
  \end{tabular}
  \caption{Instructions given to human experts for solving data analysis problems in DiscoveryBench dataset.}
  \label{InstructionPrompt}
\end{table*}


\begin{table*}[htbp]
\centering
\begin{tabular}{ll|rrr|rr}
\toprule
Prompt           & Model                & \begin{tabular}[c]{@{}r@{}}Overall\\ Accuracy\end{tabular} & \begin{tabular}[c]{@{}r@{}}Overall\\ Reproducibility\end{tabular} & \begin{tabular}[c]{@{}r@{}}Code\\ Not Run\end{tabular} & \begin{tabular}[c]{@{}r@{}}Accuracy\\ (R=1)\end{tabular} & \begin{tabular}[c]{@{}r@{}}Accuracy\\ (R=0)\end{tabular} \\
\midrule
\multirow{5}{*}{CoT} 
    & Llama-3.3         & 28.87  & 23.85  & 23.85 & 61.4  & 27.2 \\
    & DeepSeek-R1-70B   & 23.01  & 42.68  & 33.05 & 42.16 & 20.69 \\
    & GPT-4o            & 25.94  & 42.68  & 16.74 & 39.22 & 22.68 \\
    & Claude-3.5-sonnet & 28.45  & 21.34  & 25.52 & 56.86 & 30.71 \\
    & o3-mini           & 31.80  & 55.23  & 13.81 & 37.12 & 36.49 \\
\cmidrule{1-7}
\multirow{5}{*}{RoT} 
    & Llama-3.3         & 30.54  & 25.52  & 27.2  & 57.38 & 33.63 \\
    & DeepSeek-R1-70B   & 24.27  & 43.10  & 31.80 & 42.72 & 23.33 \\
    & GPT-4o            & 27.2   & 48.54  & 22.18 & 40.52 & 25.71 \\
    & Claude-3.5-sonnet & 28.45  & 28.03  & 21.76 & 53.73 & 26.67 \\
    & o3-mini           & 33.05  & 60.25  & 15.90 & 43.06 & 29.82 \\
\cmidrule{1-7}
\multirow{5}{*}{RRefl.} 
    & Llama-3.3         & 28.03  & 30.54  & 22.18 & 54.79 & 23.89 \\
    & DeepSeek-R1-70B   & 23.01  & 51.05  & 28.87 & 38.52 & 16.67 \\
    & GPT-4o            & 26.36  & 58.58  & 18.83 & 37.14 & 20.37 \\
    & Claude-3.5-sonnet & 30.54  & 27.20  & 19.67 & 53.85 & 29.92 \\
    & o3-mini           & 30.96  & 71.55  & 13.81 & 37.43 & 28.57 \\
\cmidrule{1-7}
\multirow{5}{*}{ReAct} 
    & Llama-3.3         & 28.87  & 12.97  & 8.37  & 64.52 & 26.06 \\
    & DeepSeek-R1-70B   & 28.03  & 42.26  & 15.90 & 51.49 & 15 \\
    & GPT-4o            & 26.78  & 37.24  & 9.62  & 38.20 & 23.62 \\
    & Claude-3.5-sonnet & 35.56  & 12.55  & 11.30 & 66.67 & 35.71 \\
    & o3-mini           & 34.73  & 57.74  & 3.35  & 42.75 & 25.81 \\
\cmidrule{1-7}
\multicolumn{2}{l|}{Human experts} 
    & 66.53  & 66.53  & 0     & 71.07 & 57.50 \\
\bottomrule
\end{tabular}
\caption{Model performance comparison on the DiscoveryBench dataset. Samples with inexecutable code are excluded when calculating Accuracy (R=0).}
\label{tab:discoverybench}
\end{table*}


\begin{table*}[htbp]
\centering
\begin{tabular}{ll|rrr|rr}
\toprule
Prompt           & Model                & \begin{tabular}[c]{@{}r@{}}Overall\\ Accuracy\end{tabular} & \begin{tabular}[c]{@{}r@{}}Overall\\ Reproducibility\end{tabular} & \begin{tabular}[c]{@{}r@{}}Code\\ Not Run\end{tabular} & \begin{tabular}[c]{@{}r@{}}Accuracy\\ (R=1)\end{tabular} & \begin{tabular}[c]{@{}r@{}}Accuracy\\ (R=0)\end{tabular} \\
\midrule
\multicolumn{7}{c}{QRData} \\
\midrule
\multirow{5}{*}{CoT} 
    & Llama-3.3         & 52.67  & 30.28  & 8.14  & 68.91 & 51.65 \\
    & DeepSeek-R1-70B   & 53.69  & 55.47  & 12.47 & 63.76 & 57.14 \\
    & GPT-4o            & 48.85  & 48.85  & 10.69 & 59.38 & 49.06 \\
    & Claude-3.5-sonnet & 54.45  & 31.3   & 6.62  & 59.35 & 57.79 \\
    & o3-mini           & 58.52  & 75.06  & 2.54  & 58.31 & 65.91 \\
\cmidrule{1-7}
\multirow{5}{*}{RoT} 
    & Llama-3.3         & 54.2   & 37.15  & 6.36  & 71.23 & 49.1  \\
    & DeepSeek-R1-70B   & 51.65  & 55.47  & 16.03 & 65.60 & 53.57 \\
    & GPT-4o            & 50.89  & 51.4   & 7.38  & 64.85 & 42.59 \\
    & Claude-3.5-sonnet & 56.23  & 38.93  & 7.38  & 56.21 & 63.98 \\
    & o3-mini           & 62.09  & 81.68  & 1.53  & 63.55 & 60.61 \\
\cmidrule{1-7}
\multirow{5}{*}{RRefl.} 
    & Llama-3.3         & 52.93  & 49.87  & 6.36  & 66.84 & 44.77 \\
    & DeepSeek-R1-70B   & 55.73  & 66.41  & 10.43 & 61.69 & 63.74 \\
    & GPT-4o            & 51.15  & 67.43  & 5.09  & 58.87 & 41.67 \\
    & Claude-3.5-sonnet & 58.02  & 45.04  & 3.31  & 59.89 & 60.1  \\
    & o3-mini           & 59.54  & 90.33  & 1.27  & 59.72 & 66.67 \\
\cmidrule{1-7}
\multirow{5}{*}{ReAct} 
    & Llama-3.3         & 54.2   & 30.53  & 6.62  & 62.5  & 55.87 \\
    & DeepSeek-R1-70B   & 56.23  & 46.31  & 7.89  & 68.68 & 53.33 \\
    & GPT-4o            & 51.15  & 49.36  & 4.33  & 61.86 & 44.51 \\
    & Claude-3.5-sonnet & 55.22  & 27.23  & 2.8   & 53.27 & 58.18 \\
    & o3-mini           & 60.56  & 56.74  & 0.25  & 64.13 & 56.21 \\
\midrule
\multicolumn{7}{c}{StatQA} \\
\midrule
\multirow{5}{*}{CoT} 
    & Llama-3.3         & 71.25  & 63.75  & 8.5   & 81.18 & 70.27 \\
    & DeepSeek-R1-70B   & 69     & 66.75  & 16    & 84.64 & 72.46 \\
    & GPT-4o            & 79     & 78.5   & 4     & 83.76 & 75.71 \\
    & Claude-3.5-sonnet & 78.25  & 48     & 11    & 89.58 & 85.98 \\
    & o3-mini           & 77.5   & 81     & 3.5   & 80.56 & 79.03 \\
\cmidrule{1-7}
\multirow{5}{*}{RoT} 
    & Llama-3.3         & 72.5   & 67.75  & 7.5   & 80.07 & 73.74 \\
    & DeepSeek-R1-70B   & 63.5   & 64.25  & 18.25 & 83.66 & 55.71 \\
    & GPT-4o            & 79.25  & 81.75  & 3.75  & 85.02 & 67.24 \\
    & Claude-3.5-sonnet & 80.25  & 53.5   & 8     & 90.65 & 82.47 \\
    & o3-mini           & 78.5   & 84.25  & 3     & 81.01 & 80.39 \\
\cmidrule{1-7}
\multirow{5}{*}{RRefl.} 
    & Llama-3.3         & 72.75  & 76.5   & 7.25  & 81.7  & 63.08 \\
    & DeepSeek-R1-70B   & 74.25  & 77.75  & 9     & 83.92 & 67.92 \\
    & GPT-4o            & 82     & 90.25  & 2     & 83.66 & 83.87 \\
    & Claude-3.5-sonnet & 79     & 51.5   & 9.25  & 88.35 & 85.35 \\
    & o3-mini           & 79     & 94.25  & 1.5   & 80.37 & 76.47 \\
\cmidrule{1-7}
\multirow{5}{*}{ReAct} 
    & Llama-3.3         & 73.25  & 63.25  & 3.5   & 79.45 & 69.17 \\
    & DeepSeek-R1-70B   & 72.5   & 68.25  & 5.5   & 80.95 & 65.71 \\
    & GPT-4o            & 76.5   & 80.25  & 2.25  & 81.93 & 61.43 \\
    & Claude-3.5-sonnet & 84.75  & 47.75  & 1.25  & 90.05 & 81.86 \\
    & o3-mini           & 75.25  & 83.5   & 0.75  & 77.25 & 68.25 \\
\bottomrule
\end{tabular}
\caption{Model performance comparison on the QRData and StatQA datasets. Samples with inexecutable code are excluded when calculating Accuracy (R=0).}
\label{tab:qrdata-statqa}
\end{table*}

% The dashed line package changed the display of the other tables, so I changed it.
\begin{table*}[htbp]
  \centering
  \begin{tabular}{lcc|cc|cc}
    \toprule
                      & \multicolumn{2}{c}{All Datasets} & \multicolumn{2}{c}{StatQA} & \multicolumn{2}{c}{All Datasets} \\
                      & \multicolumn{2}{c}{(Prompt, LLM, Dataset)} & \multicolumn{2}{c}{(Prompt, LLM, Category)} & \multicolumn{2}{c}{(Prompt, LLM, Type)} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
                      & Accuracy & Repr.   & Accuracy & Repr.   & Accuracy & Repr.   \\
    \cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    RoT               & 0.75     & 3.79    & -0.35    & 2.70    & 0.16     & 3.20    \\
    RRefl.         & 1.47     & 12.23\textsuperscript{***}   & 1.05     & 10.45\textsuperscript{***}   & 0.42     & 7.99\textsuperscript{**}    \\
    ReAct             & 2.16     & -3.25   & -0.80    & 1.00    & 0.83     & -3.29   \\
    \cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    DeepSeek-R1-70B   & -2.10    & 13.98\textsuperscript{***}   & 1.31     & 1.44    & 1.55     & 13.08\textsuperscript{***}   \\
    GPT-4o            & 0.42     & 18.57\textsuperscript{***}   & 5.63\textsuperscript{***}     & 14.88\textsuperscript{***}   & 0.91     & 17.04\textsuperscript{***}   \\
    Claude-3.5-sonnet & 4.09\textsuperscript{**}     & -6.63\textsuperscript{**}   & 10.38\textsuperscript{***}    & -17.63\textsuperscript{***}  & 5.71\textsuperscript{***}     & -8.72\textsuperscript{**}   \\
    o3-mini           & 5.12\textsuperscript{***}     & 31.64\textsuperscript{***}   & 4.56\textsuperscript{**}     & 17.94\textsuperscript{***}   & 4.91\textsuperscript{***}     & 29.27\textsuperscript{***}   \\
    \cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    QRData            & 26.18\textsuperscript{***}    & 12.10\textsuperscript{***}   &          &         &          &         \\
    StatQA            & 47.19\textsuperscript{***}    & 31.49\textsuperscript{***}   &          &         &          &         \\
    \cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    CTT               &          &         & 9.94\textsuperscript{***}     & 1.69    &          &         \\
    DS                &          &         & 27.38\textsuperscript{***}    & 16.50\textsuperscript{***}   &          &         \\
    DCT               &          &         & 28.69\textsuperscript{***}    & -1.38   &          &         \\
    VT                &          &         & 29.00\textsuperscript{***}    & -2.38   &          &         \\
    \cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    Categorical       &          &         &          &         & 1.63\textsuperscript{*}     & -4.27\textsuperscript{*}   \\
    Textual           &          &         &          &         & 2.06\textsuperscript{**}     & -15.09\textsuperscript{***}  \\
    \bottomrule
  \end{tabular}
  \caption{Coefficients of linear regression models across three settings. Each column represents a response variable, and each row represents a dependent variable. Empty cells indicate that the variable is not present in the corresponding model. *: $0.01\leq p<0.05$, **: $0.001 \leq p< 0.01$, ***: p<0.001. CoT is the baseline of the prompts. Llama-3.3 is the baseline of LLMs. DiscoveryBench is the baseline of datasets. CA is the baseline for statistical question categories. Numerical is the baseline for QA types.} 
  \label{tab:coef}
\end{table*}

\begin{figure*}[htbp]
  \includegraphics[width=\linewidth]{code_not_run.pdf} 
  \caption {Proportion of executable code across datasets for various LLMs and agents. 
  }
  \label{CodeNotRun}
\end{figure*}

\begin{figure*}[htbp]
  \includegraphics[width=\linewidth]{reproducibility_agents_analysis.pdf} 
  \caption {Comparison of accuracy across datasets for RoT, RReflexion, and ReAct prompting strategies using LLM models at varying reproducibility levels. For accuracy calculations at R=0, samples containing inexecutable code are excluded. The p values of paired one-sided t-tests are all less than 0.001.}
  \label{AccAtReprAgents}
\end{figure*}

\begin{table*}[htbp]
\centering
\begin{tabular}{c l l c c}
\hline
Category & Model & Prompt & \begin{tabular}[c]{@{}c@{}}Overall\\ Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Overall\\ Reproducibility\end{tabular} \\
\hline
\multirow{20}{*}{\rotatebox{90}{Correlation Analysis}} 
 & Llama-3.3         & CoT         & 48.75 & 57.50 \\
 & Llama-3.3         & RoT         & 55.00 & 61.25 \\
 & Llama-3.3         & RRefl.   & 51.25 & 72.50 \\
 & Llama-3.3         & ReAct       & 42.50 & 67.50 \\
 & DeepSeek-R1-70B   & CoT         & 60.00 & 60.00 \\
 & DeepSeek-R1-70B   & RoT         & 47.50 & 57.50 \\
 & DeepSeek-R1-70B   & RRefl.   & 63.75 & 75.00 \\
 & DeepSeek-R1-70B   & ReAct       & 55.00 & 63.75 \\
 & GPT-4o            & CoT         & 63.75 & 77.50 \\
 & GPT-4o            & RoT         & 61.25 & 78.75 \\
 & GPT-4o            & RRefl.   & 67.50 & 90.00 \\
 & GPT-4o            & ReAct       & 55.00 & 80.00 \\
 & Claude-3.5-sonnet & CoT         & 65.00 & 36.25 \\
 & Claude-3.5-sonnet & RoT         & 68.75 & 51.25 \\
 & Claude-3.5-sonnet & RRefl.   & 62.50 & 45.00 \\
 & Claude-3.5-sonnet & ReAct       & 70.00 & 46.25 \\
 & o3-mini           & CoT         & 62.50 & 80.00 \\
 & o3-mini           & RoT         & 67.50 & 80.00 \\
 & o3-mini           & RRefl.   & 61.25 & 93.75 \\
 & o3-mini           & ReAct       & 53.75 & 91.25 \\
\hline
\multirow{20}{*}{\rotatebox{90}{Contingency Table Test}}
 & Llama-3.3         & CoT         & 61.25 & 57.50 \\
 & Llama-3.3         & RoT         & 62.50 & 63.75 \\
 & Llama-3.3         & RRefl.   & 58.75 & 67.50 \\
 & Llama-3.3         & ReAct       & 63.75 & 61.25 \\
 & DeepSeek-R1-70B   & CoT         & 70.00 & 65.00 \\
 & DeepSeek-R1-70B   & RoT         & 66.25 & 61.25 \\
 & DeepSeek-R1-70B   & RRefl.   & 71.25 & 71.25 \\
 & DeepSeek-R1-70B   & ReAct       & 70.00 & 62.50 \\
 & GPT-4o            & CoT         & 65.00 & 76.25 \\
 & GPT-4o            & RoT         & 66.25 & 78.75 \\
 & GPT-4o            & RRefl.   & 70.00 & 87.50 \\
 & GPT-4o            & ReAct       & 62.50 & 75.00 \\
 & Claude-3.5-sonnet & CoT         & 81.25 & 58.75 \\
 & Claude-3.5-sonnet & RoT         & 82.50 & 60.00 \\
 & Claude-3.5-sonnet & RRefl.   & 83.75 & 61.25 \\
 & Claude-3.5-sonnet & ReAct       & 80.00 & 55.00 \\
 & o3-mini           & CoT         & 67.50 & 80.00 \\
 & o3-mini           & RoT         & 66.25 & 85.00 \\
 & o3-mini           & RRefl.   & 67.50 & 90.00 \\
 & o3-mini           & ReAct       & 65.00 & 81.25 \\
\hline
\end{tabular}
\caption{Results for Correlation Analysis and Contingency Table Test in StatQA}
\label{CACTT}
\end{table*}


\begin{table*}[htbp]
\centering
\begin{tabular}{c l l c c}
\hline
Category & Model & Prompt & \begin{tabular}[c]{@{}c@{}}Overall\\ Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Overall\\ Reproducibility\end{tabular} \\
\hline
\multirow{20}{*}{\rotatebox{90}{Descriptive Statistics}}
 & Llama-3.3         & CoT         & 82.50 & 85.00 \\
 & Llama-3.3         & RoT         & 82.50 & 82.50 \\
 & Llama-3.3         & RRefl.   & 85.00 & 90.00 \\
 & Llama-3.3         & ReAct       & 83.75 & 72.50 \\
 & DeepSeek-R1-70B   & CoT         & 80.00 & 82.50 \\
 & DeepSeek-R1-70B   & RoT         & 80.00 & 86.25 \\
 & DeepSeek-R1-70B   & RRefl.   & 83.75 & 91.25 \\
 & DeepSeek-R1-70B   & ReAct       & 86.25 & 82.50 \\
 & GPT-4o            & CoT         & 90.00 & 81.25 \\
 & GPT-4o            & RoT         & 88.75 & 85.00 \\
 & GPT-4o            & RRefl.   & 88.75 & 92.50 \\
 & GPT-4o            & ReAct       & 86.25 & 93.75 \\
 & Claude-3.5-sonnet & CoT         & 92.50 & 73.75 \\
 & Claude-3.5-sonnet & RoT         & 91.25 & 78.75 \\
 & Claude-3.5-sonnet & RRefl.   & 86.25 & 76.25 \\
 & Claude-3.5-sonnet & ReAct       & 92.50 & 71.25 \\
 & o3-mini           & CoT         & 87.50 & 90.00 \\
 & o3-mini           & RoT         & 87.50 & 91.25 \\
 & o3-mini           & RRefl.   & 87.50 & 98.75 \\
 & o3-mini           & ReAct       & 87.50 & 90.00 \\
\hline
\multirow{20}{*}{\rotatebox{90}{Distribution Compliance Test}}
 & Llama-3.3         & CoT         & 87.50 & 61.25 \\
 & Llama-3.3         & RoT         & 86.25 & 72.50 \\
 & Llama-3.3         & RRefl.   & 87.50 & 77.50 \\
 & Llama-3.3         & ReAct       & 90.00 & 52.50 \\
 & DeepSeek-R1-70B   & CoT         & 83.75 & 53.75 \\
 & DeepSeek-R1-70B   & RoT         & 82.50 & 51.25 \\
 & DeepSeek-R1-70B   & RRefl.   & 87.50 & 75.00 \\
 & DeepSeek-R1-70B   & ReAct       & 77.50 & 62.50 \\
 & GPT-4o            & CoT         & 87.50 & 82.50 \\
 & GPT-4o            & RoT         & 88.75 & 82.50 \\
 & GPT-4o            & RRefl.   & 90.00 & 91.25 \\
 & GPT-4o            & ReAct       & 90.00 & 88.75 \\
 & Claude-3.5-sonnet & CoT         & 88.75 & 33.75 \\
 & Claude-3.5-sonnet & RoT         & 87.50 & 37.50 \\
 & Claude-3.5-sonnet & RRefl.   & 90.00 & 36.25 \\
 & Claude-3.5-sonnet & ReAct       & 91.25 & 35.00 \\
 & o3-mini           & CoT         & 91.25 & 77.50 \\
 & o3-mini           & RoT         & 88.75 & 88.75 \\
 & o3-mini           & RRefl.   & 93.75 & 95.00 \\
 & o3-mini           & ReAct       & 86.25 & 82.50 \\
\hline
\end{tabular}
\caption{Results for Descriptive Statistics and Distribution Compliance Test in StatQA}
\label{DSDCT}
\end{table*}

\begin{table*}[htbp]
\centering
\begin{tabular}{c l l c c}
\hline
Category & Model & Prompt & \begin{tabular}[c]{@{}c@{}}Overall\\ Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Overall\\ Reproducibility\end{tabular} \\
\hline
\multirow{20}{*}{\rotatebox{90}{Variance Test}}
 & Llama-3.3         & CoT         & 85.00 & 57.50 \\
 & Llama-3.3         & RoT         & 82.50 & 58.75 \\
 & Llama-3.3         & RRefl.   & 86.25 & 75.00 \\
 & Llama-3.3         & ReAct       & 92.50 & 62.50 \\
 & DeepSeek-R1-70B   & CoT         & 83.75 & 72.50 \\
 & DeepSeek-R1-70B   & RoT         & 83.75 & 65.00 \\
 & DeepSeek-R1-70B   & RRefl.   & 83.75 & 76.25 \\
 & DeepSeek-R1-70B   & ReAct       & 85.00 & 70.00 \\
 & GPT-4o            & CoT         & 90.00 & 75.00 \\
 & GPT-4o            & RoT         & 92.50 & 83.75 \\
 & GPT-4o            & RRefl.   & 93.75 & 90.00 \\
 & GPT-4o            & ReAct       & 90.00 & 63.75 \\
 & Claude-3.5-sonnet & CoT         & 92.50 & 37.50 \\
 & Claude-3.5-sonnet & RoT         & 92.50 & 40.00 \\
 & Claude-3.5-sonnet & RRefl.   & 91.25 & 38.75 \\
 & Claude-3.5-sonnet & ReAct       & 92.50 & 31.25 \\
 & o3-mini           & CoT         & 86.25 & 77.50 \\
 & o3-mini           & RoT         & 86.25 & 76.25 \\
 & o3-mini           & RRefl.   & 87.50 & 93.75 \\
 & o3-mini           & ReAct       & 85.00 & 72.50 \\
\hline
\end{tabular}
\caption{Results for Variance Test in StatQA}
\label{VT}
\end{table*}


\begin{figure*}[htbp]
  \includegraphics[width=0.95\linewidth]{NCT_analysis.pdf} 
  \caption {Accuracy and reproducibility of LLMs for different QA types across all three benchmark datasets. 
  }
  \label{Ctgr}
\end{figure*}

\begin{table*}[htbp]
\centering
\begin{tabular}{c l l c c}
\hline
QA Type & Model & Prompt & \begin{tabular}[c]{@{}c@{}}Overall\\ Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Overall\\ Reproducibility\end{tabular} \\
\hline
\multirow{20}{*}{\rotatebox{90}{Numerical}}
 & Llama-3.3         & CoT         & 54.27 & 41.46 \\
 & Llama-3.3         & RoT         & 54.88 & 43.29 \\
 & Llama-3.3         & RRefl.   & 55.18 & 49.09 \\
 & Llama-3.3         & ReAct       & 57.32 & 34.15 \\
 & DeepSeek-R1-70B   & CoT         & 54.27 & 72.87 \\
 & DeepSeek-R1-70B   & RoT         & 54.88 & 71.95 \\
 & DeepSeek-R1-70B   & RRefl.   & 55.79 & 74.70 \\
 & DeepSeek-R1-70B   & ReAct       & 58.54 & 68.60 \\
 & GPT-4o            & CoT         & 59.76 & 60.98 \\
 & GPT-4o            & RoT         & 59.76 & 60.98 \\
 & GPT-4o            & RRefl.   & 59.45 & 67.99 \\
 & GPT-4o            & ReAct       & 60.67 & 61.89 \\
 & Claude-3.5-sonnet & CoT         & 62.20 & 42.38 \\
 & Claude-3.5-sonnet & RoT         & 61.89 & 54.57 \\
 & Claude-3.5-sonnet & RRefl.   & 61.28 & 50.91 \\
 & Claude-3.5-sonnet & ReAct       & 61.89 & 32.32 \\
 & o3-mini           & CoT         & 60.06 & 75.00 \\
 & o3-mini           & RoT         & 61.59 & 82.62 \\
 & o3-mini           & RRefl.   & 60.37 & 84.76 \\
 & o3-mini           & ReAct       & 61.28 & 77.13 \\
\hline
\multirow{20}{*}{\rotatebox{90}{Categorical}}
 & Llama-3.3         & CoT         & 58.23 & 45.31 \\
 & Llama-3.3         & RoT         & 59.47 & 52.04 \\
 & Llama-3.3         & RRefl.   & 56.46 & 51.15 \\
 & Llama-3.3         & ReAct       & 57.52 & 43.54 \\
 & DeepSeek-R1-70B   & CoT         & 61.24 & 51.86 \\
 & DeepSeek-R1-70B   & RoT         & 59.29 & 51.15 \\
 & DeepSeek-R1-70B   & RRefl.   & 63.54 & 58.05 \\
 & DeepSeek-R1-70B   & ReAct       & 59.29 & 46.19 \\
 & GPT-4o            & CoT         & 56.81 & 61.95 \\
 & GPT-4o            & RoT         & 56.46 & 66.02 \\
 & GPT-4o            & RRefl.   & 56.64 & 71.15 \\
 & GPT-4o            & ReAct       & 53.98 & 60.35 \\
 & Claude-3.5-sonnet & CoT         & 62.65 & 36.81 \\
 & Claude-3.5-sonnet & RoT         & 65.13 & 40.71 \\
 & Claude-3.5-sonnet & RRefl.   & 63.19 & 37.88 \\
 & Claude-3.5-sonnet & ReAct       & 64.96 & 37.52 \\
 & o3-mini           & CoT         & 63.01 & 76.28 \\
 & o3-mini           & RoT         & 65.13 & 80.88 \\
 & o3-mini           & RRefl.   & 63.36 & 87.96 \\
 & o3-mini           & ReAct       & 61.59 & 65.31 \\
\hline
\end{tabular}
\caption{Results for Categorical and Numerical QA types across all three datasets.}
\label{CatNum}
\end{table*}

\begin{table*}[htbp]
\centering
\begin{tabular}{c l l c c}
\hline
QA Type & Model & Prompt & \begin{tabular}[c]{@{}c@{}}Overall\\ Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Overall\\ Reproducibility\end{tabular} \\
\hline
\multirow{20}{*}{\rotatebox{90}{Textual}}
 & Llama-3.3         & CoT         & 58.27 & 35.97 \\
 & Llama-3.3         & RoT         & 58.99 & 38.85 \\
 & Llama-3.3         & RRefl.   & 57.55 & 50.36 \\
 & Llama-3.3         & ReAct       & 60.43 & 40.29 \\
 & DeepSeek-R1-70B   & CoT         & 59.71 & 46.04 \\
 & DeepSeek-R1-70B   & RoT         & 55.40 & 43.88 \\
 & DeepSeek-R1-70B   & RRefl.   & 63.31 & 51.08 \\
 & DeepSeek-R1-70B   & ReAct       & 61.87 & 46.04 \\
 & GPT-4o            & CoT         & 58.27 & 49.64 \\
 & GPT-4o            & RoT         & 58.27 & 56.12 \\
 & GPT-4o            & RRefl.   & 60.43 & 66.19 \\
 & GPT-4o            & ReAct       & 58.99 & 46.76 \\
 & Claude-3.5-sonnet & CoT         & 63.31 & 22.30 \\
 & Claude-3.5-sonnet & RoT         & 60.43 & 24.46 \\
 & Claude-3.5-sonnet & RRefl.   & 62.59 & 23.74 \\
 & Claude-3.5-sonnet & ReAct       & 67.63 & 17.27 \\
 & o3-mini           & CoT         & 62.59 & 60.43 \\
 & o3-mini           & RoT         & 65.47 & 59.71 \\
 & o3-mini           & RRefl.   & 61.87 & 74.10 \\
 & o3-mini           & ReAct       & 61.15 & 52.52 \\
\hline
\end{tabular}
\caption{Results for Textual QA types across all three datasets.}
\label{TXT}
\end{table*}




\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
    You are a statistician trying to answer a question based on one or more datasets.\\ [1ex]

    You have access to the following tools: \\
    python\_repl\_ast: A Python shell. Use this to execute python commands. Input should be a valid python command. When using this tool, sometimes output is abbreviated - make sure it does not look abbreviated before using it in your answer. \\

    In your output, please strictly follow the format outlined below, maintaining the specified order and structure.\\

    Question: the input question you must answer\\
    Workflow: a plan to tackle the problem\\
    Action: the action to take, should be one of [python\_repl\_ast]\\
    Action Input: the input to the action in the format of (\verb|```python\s.*?```|). Use print to show the results.\\
    Observation: the result of performing the action with the action input (please do not generate)\\
    Final Answer: an answer to the original question\\ [1ex]

    NOTE: You will need to generate the complete action input in one code snippet to solve the query in one attempt. \\
    If no observation is provided, you need to generate the workflow, action, and action input. You don't need to provide the final answer. \\
    If an observation is provided, you should generate the answer starting with "Final Answer:" \{agent\_instruction\}\\ [1ex]

    Begin!\\ [1ex]

    You need to load all datasets in Python using the specified paths: \\
    \{file\_paths\}\\ [1ex]

    Dataset descriptions: \\
    \{descriptions\}\\ [1ex]

    Question: \\
    \{question\}\\
    \{conversation\}\{reflexion\}\\
    \hline
  \end{tabular}
  \caption{\label{CoTPrompt}
    Prompt templates for Chain-of-Thought (CoT), Reproducibility-of-Thought (RoT), and Reproducibility-Reflexion (RReflexion). For the CoT prompt, $\text{\{agent\_instruction\}}$ is replaced with "\textbackslash nLet's think step by step." The RoT prompt adds the instruction: "Make sure a person can replicate the action input by only looking at the workflow, and the action input reflects every step of the workflow." For the RReflexion prompt, $\text{\{reflexion\}}$ is replaced with "\textbackslash nThe above workflow and action input are not aligned for reproducibility. Please rewrite the workflow, action, and action input. Generate the most likely executable code (action input) and ensure reproducibility of the code through the workflow."
  }
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
    You are a statistician trying to answer a question based on one or more datasets. \\ [1ex]

You have access to the following tools: \\
python\_repl\_ast: A Python shell. Use this to execute python commands. Input should be a valid python command. When using this tool, sometimes output is abbreviated - make sure it does not look abbreviated before using it in your answer.\\[1ex]

Below is the structure of the agent-environment interaction. Your task is to generate only the agent's responses. \\
Question: the input question you must answer \\
<Agent> \\
Workflow: a plan to tackle the problem \\
Action: the action to take, should be one of [python\_repl\_ast] \\
Action Input: the input to the action in the format of (\verb|```python\s.*?```|). Use print to show the results. \\
<Environment> \\
Observation: the result of performing the action with the action input (please do not generate) \\
<Agent> \\
Workflow: a plan to tackle the problem \\
Action: the action to take, should be one of [\{tool\_names\}] \\
Action Input: the input to the action in the format of (\verb|```python\s.*?```|). Use print to show the results. \\
<Environment> \\
Observation: the result of performing the action with the action input (please do not generate) \\
...  \\
<Agent> \\
Final Answer: an answer to the original question \\
Task done! \\[1ex]

NOTE: You will need to generate the complete action input in one code snippet. We will execute the code for you and provide the observation to you. \\
If no observation is provided or the observation is insufficient to answer the question, you need to (re)generate the complete workflow, action, and action input. Do not generate final answer and 'Task done'. \\
If the observation is sufficient to answer the question, generate the final answer starting with 'Final Answer:' and indicate completion by ending with 'Task done!'. Do not generate workflow any more. \\
Let's think step by step.\\[1ex]

Example \\
Question:  \\
What is the linear coefficient that describes the positive relationship between the rate of maximum body length evolution and spatial variation in speciation rates, where the former emerges as the most influential factor? \\
<Agent> \\
Workflow: First, I need to load the dataset 'body-size-evolution-in-south-american-freshwater-fishes.csv' into a Pandas DataFrame, and print the first few rows of the data. \\
Action: python\_repl\_ast \\
Action Input: \verb|```python|\begin{verbatim}
import pandas as pd
df = pd.read_csv('body-size-evolution-in-south-american-freshwater-fishes.csv')
df.head()
\end{verbatim}
\verb|```|\\
    \hline
  \end{tabular}
  \caption{\label{ReActPrompt}
    Prompt template for ReAct.
  }
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
    <Environment> \\
    Observation: \\
     HYBAS\_ID       long       lat  BAMM\_speciation  BAMM\_extinction  BAMM\_NetDiv  ...   aet    Elevation  sgr  soil\_div     area  diversity \\
0  6050000010 -76.477422  7.742693         0.137392         0.026807     0.110585  ...  1387   330.150088  166  0.482402  72363.7         68\\
1  6050000740 -74.628725  9.803586         0.117235         0.025796     0.091438  ...  1082    69.475294   23  0.457436  17944.3         35\\
2  6050068100 -75.295995  8.448815         0.119381         0.023826     0.095555  ...  1312   143.032178   74  0.378793  17105.5         44\\
3  6050068110 -74.608408  8.922863         0.132477         0.027777     0.104700  ...  1445    14.724138    3  0.468328    610.1         48\\
4  6050070260 -75.591588  5.770093         0.120127         0.022940     0.097187  ...  1371  1378.729945  421  0.158870  61901.9         81\\
\text{[5 rows x 21 columns]} \\
<Agent>\\
Workflow: 1. Import Libraries: Import `LinearRegression` from `sklearn` and `numpy`.\\
2. Prepare Data: Extract predictor 'BAMM\_speciation' and response 'BAMM\_NetDiv'.\\
3. Create and Fit Model: Instantiate `LinearRegression()` and fit the model using `model.fit(X, y)`.\\
4. Retrieve Coefficient: Get the slope using `model.coef\_[0]`.\\
5. Output Result: Print the coefficient with `print(coefficient)`.\\
Action: python\_repl\_ast\\
Action Input: \verb|```python|\begin{verbatim}
from sklearn.linear_model import LinearRegression
import numpy as np

# Prepare the data for regression
X = df[['BAMM_speciation']].values.reshape(-1, 1)  # Predictor
y = df['BAMM_NetDiv'].values  # Response

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Get the coefficient
coefficient = model.coef_[0]
print(coefficient)
\end{verbatim}
\verb|```|\\
<Environment>\\
Observation: 0.5175306498596297\\
<Agent>\\
Final Answer:\\
The linear coefficient that describes the positive relationship between the rate of maximum body length evolution ('BAMM\_speciation') and spatial variation in speciation rates ('BAMM\_NetDiv') is approximately 0.518.\\
Task done!\\[1ex]

    \hline
  \end{tabular}
  \caption{\label{ContinueReActPrompt}
    [Continue] Prompt template for ReAct.
  }
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
Begin!\\[1ex]

You need to load all datasets in python using the specified paths: \\
\{file\_paths\}\\[1ex]

Dataset descriptions:\\ 
{descriptions}\\[1ex]

Question: \\
\{question\}\\
<Agent>\\
\{conversation\}\\
    \hline
  \end{tabular}
  \caption{\label{Continue2ReActPrompt}
    [Continue] Prompt template for ReAct.
  }
\end{table*}



\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
    Evaluate the correctness (0 for incorrect, 1 for correct) of the predicted answer to the question: \{question\}\\
\{dataset\_specific\_prompt\}\\
Predicted answer: \{predicted\_answer\}\\[1ex]

Ground truth answer: \{true\_answer\}\\[1ex]

Please reply in this format: \\
"Thoughts: \\[1ex]

The accuracy score is:"\\
    \hline
  \end{tabular}
  \caption{\label{AccuracyPrompt}
    Prompt for evaluating the accuracy of an answer. The $\text{\{dataset\_specific\_prompt}\}$ is used to define dataset-specific requirements. For questions with a numerical conclusion, accuracy is considered achieved if the deviation between LLM’s response and the benchmark conclusion falls below a benchmark-specific error threshold: 3\% threshold for QRData following \citep{liu2024llms} and 1\% threshold for DiscoveryBench and StatQA. If the conclusions in StatQA tasks contradict each other, any conclusion is considered correct.
  }
\end{table*}



\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
    Question: \{question\}\\
These are the dataset paths: \{file\_paths\}\\[1ex]

Develop a Python script that precisely converts the provided narrative summary into executable code. Ensure that each component of the analysis process is correctly implemented, closely following the steps outlined in the summary. Maintain consistency by using the exact variable names specified in the narrative. Below is the code summary to translate:\\
\{workflow\}\\
    \hline
  \end{tabular}
  \caption{\label{WorkflowCodePrompt}
    Prompt for workflow-to-code conversion. 
  }
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
    You are a data scientist answering a question based on the code and code output.\\[1ex]

    Question: \{question\}\\[1ex]
    
    Code:\\
    \{code\}\\[1ex]
    
    Code output: \{code\_output\}\\[1ex]
    
    Please generate the answer based only on the code output in this format: \\
    "Thought:\\[1ex]
    
    Conclusion:"\\
    \hline
  \end{tabular}
  \caption{\label{ConclusionPrompt}
    Prompt for extracting conclusion from code execution output. 
  }
\end{table*}


\begin{table*}
  \centering
  \begin{tabular}{p{0.95\textwidth}}
    \hline
    Your task is to determine whether Code 1 and Code 2 arrive at the same conclusion regarding the question: \{question\}\\[1ex]

Code 1:\\
\{code\_1\}\\[1ex]

Code 1 execution output:\\
\{code\_1\_output\}\\[1ex]

Code 1 conclusion:\\
\{code\_1\_conclusion\}\\[1ex]

Code 2:\\
\{code\_2\}\\[1ex]

Code 2 execution output:\\
\{code\_2\_output\}\\[1ex]

Code 2 conclusion:\\
\{code\_2\_conclusion\}\\[1ex]

If the output of the two code snippets provide the same values for the same statistics and lead to the same conclusion to the question, please score 1. Otherwise, score 0. Note that if code 2 runs into error, please score 0.
Please reply in this format: \\
"Thoughts: \\[1ex]

The similarity score is:"\\
    \hline
  \end{tabular}
  \caption{\label{ReprPrompt}
    Prompt for assessing reproducibility. 
  }
\end{table*}

\end{document}
