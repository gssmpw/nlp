\section{Related Work}
Recent advancements in LLM code generation have garnered significant attention, exemplified by tools like GitHub Copilot \citep{chen2021evaluatinglargelanguagemodels} and CodeGeeX \citep{Zheng2023CodeGeeX}, which leverage code LLMs to streamline software development. 
%As modern data analysis relies heavily on code, 
These advancements have spurred growing interests in LLM-driven code generation for data tasks \citep{zhu2024large, gu2024blade}, driving progress in automating data manipulation, visualization, and statistical analysis \citep{hong2024datainterpreter, guo2024ds, Li2024AutomatedSM}.

While prior work examines both general-purpose models (e.g., GPT-4 \citep{openai2024gpt4}, Llama 3 \citep{grattafiori2024llama3}, Claude 3 \citep{TheC3}) and code-specialized variants (e.g., Code LLaMA \citep{rozière2024codellama}, DeepSeek-Coder \citep{guo2024deepseek}), empirical evaluations in data analysis code generation remain dominated by studies of general-purpose models, particularly the GPT family \citep{hong2024datainterpreter, gu2024blade, Li2024AutomatedSM, guo2024ds}.

\subsection{Methodologies and Frameworks}

Recent work has explored a number of techniques to enhance LLM capabilities for data science tasks. For example, several works \citep{majumder2024discoverybench, liu2024llms, zhu2024large} test existing general-purpose prompting frameworks like chain-of-thought \cite{wei2022chain} and self-reflection \cite{shinn2024reflexion}. %Several works 
Some propose more complex, data-science-specific paradigms --- for example a hierarchical graph modeling framework \citep{hong2024datainterpreter} and a multi-agent framework for iterative revisions \citep{guo2024ds} --- which are also implemented via prompting. Performance effects of fine-tuning LLMs on data science questions have also been benchmarked \cite{zhu2024large}.

\subsection{LLM-Assisted Data Science} 
As LLM-generated data analyses are increasingly integrated into real-world pipelines, recent research has focused on understanding their practical use, particularly how human analysts engage with the LLM assistants.
TalkToEBM \citep{bordt2024datascience} investigates how LLMs can provide interpretable model summarization and graph explanation, while CliniDSBench \citep{wang2024large} 
presents an interactive LLM-based platform to streamline medical data analysis coding. \citet{nascimento2024llm4ds} evaluates popular interfaced LLM coding tools as data science programming tools.
\citet{nejjar2024llms} performs a systematic study of commercially available LLM coding tools used in real-world research for 
data analysis and visualization, finding that some tools had wrong and misleading analysis results due to difficulty picking up important details. 

\subsection{Datasets and Benchmarks}
Several benchmarks have been introduced to assess LLM performance on data science and data reasoning tasks. DS-1000 \citep{lai2022ds1000} and ExeDS \citep{huang2022execution} explore generation of code for building statistical models and making visualizations, 
while additional benchmarks assess LLM data understanding without extensive code generation (e.g. tabular data understanding) \cite{zhao2022multihiertt, yu2018spider, hazoom2021text}. 
More recently, DiscoveryBench, StatQA, DAEval, and QRData, \citep{majumder2024discoverybench, zhu2024large, hu2024infiagent, liu2024llms} build on prior works by targeting models’ abilities to code multi-step, data-driven workflows using appropriate statistical reasoning and knowledge.

\subsection{Reproducibility in LLMs}

Existing research on the reproducibility of LLM outputs focuses on quantifying their consistency as evaluators \citep{lee2024evaluatingconsistency} and question-answering tools \citep{lee2024evaluatingconsistenciesllm}. Additionally, concerns about the reproducibility of studies investigating LLMs have emerged due to their opacity and experimental design standardization\citep{vaugrante2024looming}. However, to the best of our knowledge, no prior work has addressed reproducibility of LLM-generated data analyses. Thus, our work fills a critical gap by ensuring scientific rigor of LLM-assisted data analysis, which is becoming an integral part of modern research.