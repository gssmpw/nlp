%Testing learning-enabled CPS significantly differs from testing traditional software systems due to several key distinctions. Traditional software testing relies on well-defined specifications and coverage metrics, such as statement coverage, function coverage, and branch coverage, to guide the testing process and measure its effectiveness. However, defining coverage metrics for machine learning models within a learning-enabled CPS is a significant challenge. Determining the concept of ``full coverage'' testing for a neural network is intricate, and current coverage-driven approaches are considerably limited~\cite{harel2020neuron,li2019structural,yang2022revisiting,usman2022overview} (\textbf{Coverage Metrics Challenge}). Traditional software testing often employs an ``oracle'' – a benchmark for expected outcomes to compare against actual results. In learning-enabled CPS, however, such oracles might be absent, especially when it comes to assessing the quality of labels in training data for machine learning models. Unlike the manual test cases typical of conventional software testing, evaluating the label quality in learning-enabled CPS demands a thorough examination of the datasets used for training. These datasets, often vast in volume, can be multi-modal and high-dimensional. Ensuring the accuracy and consistency of labels within them presents unique and intricate challenges (\textbf{Oracle Challenge}). Furthermore, in traditional software, behavior is generally fixed once deployed unless manually updated or patched. In contrast, a learning-enabled CPS may continuously learn and adapt its behavior over time based on new data, making the testing and validation process ongoing and dynamically changing (\textbf{Behavior Change Challenge}). 
Recently, there has been a notable increase in research that concentrates on the formal verification of machine learning models. Various techniques have been employed, such as geometric reachability through NNV star sets~\cite{tran2020verification}, mixed-integer programming feasibility via Sherlock~\cite{dutta2019sherlock}, SMT solvers like Reluplex~\cite{katz2017reluplex}, and model-free optimization methods like Branch and Bound~\cite{bunel2018unified}. While this body of work primarily aims at ensuring the robustness and safety of individual models, it commonly falls short in several areas. One significant limitation is the tendency to treat robustness as a function of a predefined geometric input space. Within this space, researchers typically search for a 'radius' where the model's predictions remain stable or are bounded by linear constraints. Additionally, while recent neural network verification competitions~\cite{bak2021second} have started to explore practical issues, such as the prevention of aircraft collisions~\cite{owen2019acas}, these efforts often lack alignment with formally specified safety requirements, like those defined in Signal Temporal Logic~\cite{donze2010robust}. Moreover, the target systems under study frequently comprise single, relatively simplistic models. These lack the complexity, both in terms of input spaces and architecture, that one encounters in industry-scale, multi-module, multi-modal learning-enabled Cyber-Physical Systems (CPS). Such industry-relevant CPS integrate various functionalities like perception, planning, and prediction and utilize a range of sensors including cameras, Lidar, and radar~\cite{lou2022testing,prakash2021multi}.

%There has been a surge in research focusing on the formal verification of machine learning models using 
%geometric reachability (NNV star set~\cite{tran2020verification}), mixed-interger programming feasibility e(Sherlock~\cite{dutta2019sherlock}), SMT solver (Reluplex~\cite{katz2017reluplex}) and model-free optimization (Branch and Bound~\cite{bunel2018unified}).
%~\cite{katz2017reluplex,ehlers2017formal,zhang2018efficient,katz2019marabou}. 
%This line of research primarily targets the robustness and safety of the models themselves. However, a common limitation in these studies is the treatment of robustness as a predefined geometric input space, wherein a search is conducted to identify a radius where the model's predictions remain stable or bounded by linear constraints. In recent neural network verification competitions~\cite{bak2021second}, efforts have been made to examine properties like the prevention of aircraft collisions~\cite{owen2019acas}. 
%However, these initiatives frequently fall short in aligning with formal specifications essential for system safety, such as Signal Temporal Logic~\cite{donze2010robust}. Moreover, the target systems—often consisting of single models—are overly simplistic in both their input spaces and architectural complexities, especially when compared to industry-scale, multi-module, multi-modal learning-enabled CPS including perception, planning and prediction with cameras, Lidar and radar sensors~\cite{lou2022testing,prakash2021multi}.

Recent research in the system domain has explored in-situ reasoning for closed-loop systems. In one example~\cite{tuncali2018reasoning}, barrier certificates serve as differentiable invariant functions that, in conjunction with a non-linear SMT solver, provide safety guarantees. These guarantees state that from a set of possible initial conditions, a set of unsafe states will not be reached. However, identifying these states remains challenging, and questions about the scalability and applicability of this approach to multi-module learning-enabled systems remain unanswered. 

Simultaneously, the testing landscape for learning-enabled CPS has seen considerable research activity. One primary focus area has been the verification of system properties, specifically concerning accuracy~\cite{deng2022declarative} and safety~\cite{tian2022mosat,li2020av}. Parallel to this, another research direction has honed in on evaluating system robustness. This includes metrics like resilience to data corruption~\cite{tian2018deeptest}, adversarial robustness~\cite{deng2020analysis}, and the system's ability to generalize when faced with shifts in data distribution~\cite{cai2020real}.
%Recent research has delved extensively into the testing of various facets of learning-enabled CPS. One prominent direction of this research is dedicated to verifying the accuracy~\cite{deng2022declarative} and safety~\cite{tian2022mosat,li2020av} properties inherent to these systems. Concurrently, another research trajectory is centered on evaluating the robustness of such systems, taking into account factors like corruption robustness~\cite{tian2018deeptest}, adversarial resilience~\cite{deng2020analysis}, and generalization under distribution shifts~\cite{cai2020real}. 
However, a significant portion of the current testing methodologies employs fitness functions that encompass multiple objectives or coverage metrics. These approaches lack any concrete formal guarantees concerning formal specifications. While there are recent studies \cite{sun2022lawbreaker,zhou2023specification} that incorporate formal specifications, specifically in the form of STL, these specifications tend to be narrowly tailored to specific manually defined scenarios, such as lane changes or navigating intersections. As a result, they fall short in offering a comprehensive safety guarantee (e.g., ensuring no crashes). Furthermore, the test cases derived from these methodologies don't provide a stringent guarantee regarding the extent to which the specified properties are met. From an industrial perspective, particularly in sectors deploying learning-enabled CPS, existing testing methodologies are largely probabilistic in nature. What the industry is increasingly calling for are tangible formal guarantees. For example, in the context of self-driving cars, while it may be unrealistic to claim that an autonomous vehicle will never be involved in an accident, there should be a robust safety assurance that the vehicle will not be a contributing factor to the causation of any accident~\cite{shalev2017formal}.

Certainly, testing remains the most pragmatically adopted method for ensuring the reliability and safety of systems in industry, particularly for complex learning-enabled CPS. If testing methodologies can be designed to rigorously evaluate systems against formal specifications, then they would offer a meaningful form of formal guarantee. This approach would occupy a middle ground between two existing paradigms: model checking and runtime verification. On one end, formal verification (model checking~\cite{kochdumper2023fully}) aims to exhaustively verify properties against all possible behaviors of the system, a process that is often computationally infeasible for complex, large-scale systems like learning-enabled CPS and  more critically, verifying individual learning-enabled components (LEC) and attempting
to compose guarantees of individual components
into system-level guarantees is not available~\cite{seshia2017compositional}. On the other end, runtime verification~\cite{zapridou2020runtime} monitors properties against observed runtime traces, offering no guarantees about unobserved behaviors and often requiring a large number of samples for confidence. Incorporating formal specifications into a test-based methodology could offer a more feasible yet comprehensive alternative. It would not only be more aligned with industrial practices but would also be more computationally tractable than exhaustive model checking, while still offering broader assurances than runtime verification.

In this paper, we outline a research roadmap to enhance testing methodologies for learning-enabled Cyber-Physical Systems. Leveraging large-language models, we aim to generate rigorous test cases, extract formal specifications from this test data, and then test against these specifications. This approach offers a more practical yet robust alternative to existing methods, providing a form of formal guarantee that aligns with industrial needs.


%Linear-time temporal logic guided greybox fuzzing
%Given a linear-time temporal logic (LTL) property $\phi$, a B\"uchi automata $\mathrm{A}\neg\phi$ can be constructed which accepte the violation of the property $\phi$.  But the approach requires to identify program locations where the atomic propositions in the LTL property may be affected, which is not possible in learning-based CPS where corresponding locations do not exist (e.g., speed control resides in multiple neural networks from perception, prediction to planning for self-drving cars).


%The amount of data
%required to guarantee a probability of $10^(-9)$ fatality per hour of driving is proportional to its inverse, $10^9$ hours of data
%(see details in the sequel), which is roughly in the order of thirty billion miles~\cite{shalev2017formal}. 



%1. Do not hit someone from behind.
%2. Do not cut-in recklessly.
%3. Right-of-way is given, not taken.
%4. Be careful of areas with limited visibility
%5. If you can avoid an accident without causing another one, you must do it~\cite{shalev2017formal}


%verification where one checks whether all system behaviors satisfy
%', LTL is also used for lightweight verification (monitoring, runtime verification [5])
%where the satisfaction of a formula by one or more individual behaviors is checked.
%For both uses, monitoring and verification alike, we point out three fundamental features
%of this framework, the first two related to the nature of the systems and behaviors
%considered while the third is related to the very notion of property and logical truth.
%1. Discrete qualitative time: behaviors are defined as sequences of states or events,
%which are often interpreted in a purely qualitative manner, that is, without considering
%the metric distance between subsequent time instances;
%2. Discrete state space: the sequences are defined over %discrete and often finite domains
%emphasizing control rather than data;
%3. Yes/No answer: the satisfaction of a temporal formula by a behavior is considered
%as membership in a set, with no quantitative degree of satisfaction. They present an efficeint dense-time algorithm for computing both Space (predicate over signals) and time (duration of events) robustness for piecewise-linear signals~\cite{donze2010robust}.

%Fainekos and Pappas~\cite{fainekos2009robustness} define the notion of robustness degree
%as a real number associated with a property-behavior pair, based on, roughly speaking,
%the distance between the behavior and the (boundary of) the set of all behaviors that satisfy
%the property. This measure is more positive when the behavior is deeper inside the
%set of satisfying behaviors and more negative the further is the behavior outside that set. Behavior-oriented approach to modify signal to satisfy or violate the property.

%\begin{bmatrix}
%\frac{\partial f}{\partial x_1} \\
%\frac{\partial f}{\partial x_2} \\
%\vdots \\
%\frac{\partial f}{\partial x_n}
%\end{bmatrix} 

%\begin{bmatrix}
%\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}%{\partial x_2} & \dots & \frac{\partial f_1}{\partial x_n} \\
%\frac{\partial f_2}{\partial x_1} & \dots & \dots & \frac{\partial f_2}{\partial x_n} \\
%\vdots & \vdots & \ddots & \vdots \\
%\frac{\partial f_m}{\partial x_1} & \dots & \dots & \frac{\partial f_m}{\partial x_n}
%\end{bmatrix}