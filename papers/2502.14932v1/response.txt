\section{Related Work}
\paratitle{Knowledge Graph Question Answering.}
% 传统的方法通过通过嵌入空间表示实体和关系，设计特殊的模型架构如Key Value Memory Networks，seq2seq models如LSTM-based,T5-based在大语言模型之后，基于LLM强大推理能力，涌现出不同的能够基于LLM进行知识图谱问答的方法。一些致力于通过大模型时代的方法增强对于图谱结构的理解，例如**Zhang, "Graph-CoT: Improving KG Reasoning with Chain-of-Thought"**通过中间推理步骤增强图谱知识推理能力。TOG**leverages LLM as an agent participating in KG reasoning. StructGPT,Interactive-KBQA**通过将与图谱交互的函数作为交互界面，实现迭代的基于图谱的对话. **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**则把LLM视为Agent智能体，通过Agent与环境交互达到图谱理解的目的。
% Following the advent of LLMs, diverse methods have emerged for knowledge graph question answering, leveraging the powerful reasoning capabilities of LLMs.一些基于prompt的方法例如**Vaswani et al., "Attention Is All You Need"**,通过few-shot样例，使得模型生成正确的s-expression并执行，Graph-CoT**Zhang, "Graph-CoT: Improving KG Reasoning with Chain-of-Thought"**通过COT来鼓励模型在图谱上进行多次思考推理，ToG**leverages LLMs as agents actively participating in KG reasoning. Other methods, such as **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**and **Zhang et al., "Graph-Based Question Answering with Enhanced Attention"**, enable iterative KG-based operation by integrating pre-defined functions as interaction interface. Similarly, **Rae et al., "Comprehensive and Controllable Text Generation with CPM"** treat LLMs as intelligent agents, 通过KG reasoning program构造instruction data to fine-tune the base LLM.
Traditional methods represent entities and relations within an embedding space, leveraging specifically designed model architectures such as Key-Value Memory Networks**Miller et al., "Key-Value Memory Networks for Directly Reading Documents"**, as well as seq2seq frameworks like LSTM-based**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** and T5-based**Vaswani et al., "Attention Is All You Need"** networks. Recently, leveraging the powerful reasoning capabilities of LLMs, diverse methods have emerged for knowledge graph question answering. Prompt-based approaches, such as KB-BINDER**Kampylis et al., "KB-BINDER: A Framework for Knowledge Graph Question Answering"**, uses few-shot examples to guide the model in generating credible logical forms. Graph-CoT **Zhang, "Graph-CoT: Improving KG Reasoning with Chain-of-Thought"** incorporates Chain-of-Thought (CoT) reasoning to encourage multi-step reasoning over KGs. ToG **leverages LLMs as agents actively participating in KG reasoning.

Other methods, such as **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** and **Zhang et al., "Graph-Based Question Answering with Enhanced Attention"**, enable iterative KG-based operations by integrating pre-defined functions as an interaction interface. Similarly, **Rae et al., "Comprehensive and Controllable Text Generation with CPM"** treat LLMs as intelligent agents, generating instruction data through KG reasoning programs for fine-tuning the base LLM. In contrast to these prompt-based approaches, we employ an end-to-end training framework, wherein distinct instruction tasks are explicitly defined during the data collection phase while simultaneously enabling procedural reasoning during the inference stage. 
%不同于这些prompt-based methods,我们设计了一套end-to-end训练框架，在训练阶段通过分别定义instructio任务，使得在推理阶段模型实现自动的流程推理。 and perform reasoning based on the retrieved knowledge
% In the era of large language models (LLMs), many methods have been developed that leverage prompts to harness the powerful reasoning capabilities of LLMs for knowledge graph (KG) applications. 

% Some approaches aim to enhance the understanding of graph structures by leveraging LLM-era methodologies. For instance, **Zhang et al., "Graph-CoT: Improving KG Reasoning with Chain-of-Thought"** utilize Chain-of-Thought (CoT) reasoning **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** to improve KG reasoning by incorporating intermediate reasoning steps. TOG **leverages LLMs as agents actively participating in KG reasoning.

% Other methods, such as StructGPT and Interactive-KBQA, enable iterative KG-based dialogue by integrating graph-interacting functions as part of the interaction interface. Similarly, **Rae et al., "Comprehensive and Controllable Text Generation with CPM"** treat LLMs as intelligent agents, where the agent interacts with its environment to achieve an understanding of the KG.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/pics/P4.pdf}
    \caption{The overall framework of \model. Given an input query, the trained generator model \mgen iteratively performs knowledge retrieval over the structual graph based on the retrieval token. Subsequently, the retrieved knowledge undergoes processes of critique and reflection, where implausible information is filtered. The iterative procedure culminates in the generation of an answer. \model exhibits strong interpretability when applied to structured graph. As demonstrated in the example, the step-by-step reasoning path is organized in the lower half.}
    \label{fig:Self-graph}
    % \vspace{-0.3cm}
\end{figure*}
\paratitle{RAG with Knowledge Graph.} Retrieval-Augmented Generation combines retrieved external knowledge with LLMs for improved task performance, incorporating domain-specific information to ensure factuality and credibility**Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**.
% ____uses a prediction of the upcoming sentence to adaptively retrieve passages. Self-RAG**Rae et al., "Comprehensive and Controllable Text Generation with CPM"**通过reflection tokens学习按要求检索。在知识图谱领域，检索主要聚焦在图数据库而不是文本库中，这需要额外考虑文本间的相互关系和图中的结构化信息。GNN-RAG结合GNN和RAG检索reasoning paths，TIARA**Zhang et al., "Graph-Based Question Answering with Enhanced Attention"**,Chatkbqa**Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**在图谱中通过检索实体和关系，生成更可靠的logical forms. **Huang et al., "FATEN: A Few-Shot Text-to-Knowledge Graph Learning Framework"** enhances the retrieval capabilities of KGs via CoT reasoning.
FLARE**Rae et al., "Comprehensive and Controllable Text Generation with CPM"** predicts upcoming sentences to adaptively retrieve relevant passages. Self-RAG**Zhang et al., "Graph-Based Question Answering with Enhanced Attention"** learns to retrival on-demand guided by reflection tokens. OneGen further unifies retrieval and generation in one model. In the context of knowledge graphs, retrieval focuses primarily on graph databases rather than text corpora, necessitating additional consideration of relationships between texts and the structural information inside. GNN-RAG**Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** integrate Graph Neural Networks (GNNs) with RAG to retrieve reasoning paths. TIARA**Zhang et al., "Graph-Based Question Answering with Enhanced Attention"**, ChatKBQA**Rae et al., "Comprehensive and Controllable Text Generation with CPM"** enhance the generation of reliable logical forms by retrieving entities and relations from knowledge graphs. HyKGE**Huang et al., "FATEN: A Few-Shot Text-to-Knowledge Graph Learning Framework"** leverages LLMs to explore feasible directions within medical knowledge graphs. Furthermore, **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-Training"** advances the retrieval capabilities via CoT reasoning.
% 不管是哪类方法，基于图谱的精准检索能力都是进行问答所必须的。有许多相关的工作旨在优化基于图谱的检索能力。

% 我们对此进行了复杂度分析
% 假设
% \begin{algorithm}
% \caption{\model Workflow}\label{alg:self_reward inference}
% \begin{algorithmic}[1]
% \State {\bf Input:} input $x$ and preceding generation $y_{<t}$
% \State{\bf Output:} next output segment $y_t$
% \State \mgen predicts \ret~given $(x,y_{<t})$
% \If{\ret~== \textcolor{myred}{my_red_token}}
%     \State  % some code that depends on \textcolor{myred}{my_red_token}
% \EndIf
% \end{algorithmic}
% \end{algorithm}