% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{GrailQA,
author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449992},
doi = {10.1145/3442381.3449992},
abstract = {Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.1},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3477â€“3488},
numpages = {12},
keywords = {Knowledge Base, Question Answering, Semantic Parsing},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}


@InProceedings{SPARQL,
author="P{\'e}rez, Jorge
and Arenas, Marcelo
and Gutierrez, Claudio",
editor="Cruz, Isabel
and Decker, Stefan
and Allemang, Dean
and Preist, Chris
and Schwabe, Daniel
and Mika, Peter
and Uschold, Mike
and Aroyo, Lora M.",
title="Semantics and Complexity of SPARQL",
booktitle="The Semantic Web - ISWC 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="30--43",
abstract="SPARQL is the W3C candidate recommendation query language for RDF. In this paper we address systematically the formal study of SPARQL, concentrating in its graph pattern facility. We consider for this study simple RDF graphs without special semantics for literals and a simplified version of filters which encompasses all the main issues. We provide a compositional semantics, prove there are normal forms, prove complexity bounds, among others that the evaluation of SPARQL patterns is PSPACE-complete, compare our semantics to an alternative operational semantics, give simple and natural conditions when both semantics coincide and discuss optimization procedures.",
isbn="978-3-540-49055-5"
}

@inproceedings{ji-etal-2024-retrieval,
    title = "Retrieval and Reasoning on {KG}s: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering",
    author = "Ji, Yixin  and
      Wu, Kaixin  and
      Li, Juntao  and
      Chen, Wei  and
      Zhong, Mingjie  and
      Jia, Xu  and
      Zhang, Min",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.446/",
    doi = "10.18653/v1/2024.findings-emnlp.446",
    pages = "7598--7610",
    abstract = "Despite Large Language Models (LLMs) have performed impressively in various Natural Language Processing (NLP) tasks, their inherent hallucination phenomena severely challenge their credibility in complex reasoning. Combining explainable Knowledge Graphs (KGs) with LLMs is a promising path to address this issue. However, structured KGs are difficult to utilize, and how to make LLMs understand and incorporate them is a challenging topic. We thereby reorganize a more efficient structure of KGs, while designing the KG-related instruction tuning and continual pre-training strategies to enable LLMs to learn and internalize this form of representation effectively. Moreover, we construct subgraphs to further enhance the retrieval capabilities of KGs via CoT reasoning. Extensive experiments on two KGQA datasets demonstrate that our model achieves convincing performance compared to strong baselines."
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{sun2023think,
  title={Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph},
  author={Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Shum, Heung-Yeung and Guo, Jian},
  journal={arXiv preprint arXiv:2307.07697},
  year={2023}
}
@inproceedings{jiang-etal-2023-structgpt,
    title = "{S}truct{GPT}: A General Framework for Large Language Model to Reason over Structured Data",
    author = "Jiang, Jinhao  and
      Zhou, Kun  and
      Dong, Zican  and
      Ye, Keming  and
      Zhao, Xin  and
      Wen, Ji-Rong",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.574",
    doi = "10.18653/v1/2023.emnlp-main.574",
    pages = "9237--9251",
}
@inproceedings{gu-etal-2023-dont,
    title = "Don`t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",
    author = "Gu, Yu  and
      Deng, Xiang  and
      Su, Yu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.270/",
    doi = "10.18653/v1/2023.acl-long.270",
    pages = "4928--4949",
}
@article{jiang2024kg,
  title={Kg-agent: An efficient autonomous agent framework for complex reasoning over knowledge graph},
  author={Jiang, Jinhao and Zhou, Kun and Zhao, Wayne Xin and Song, Yang and Zhu, Chen and Zhu, Hengshu and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2402.11163},
  year={2024}
}
@inproceedings{kb-binder,
    title = "Few-shot In-context Learning on Knowledge Base Question Answering",
    author = "Li, Tianle  and
      Ma, Xueguang  and
      Zhuang, Alex  and
      Gu, Yu  and
      Su, Yu  and
      Chen, Wenhu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.385/",
    doi = "10.18653/v1/2023.acl-long.385",
    pages = "6966--6980",
}
@inproceedings{luo-etal-2024-chatkbqa,
    title = "{C}hat{KBQA}: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
    author = "Luo, Haoran  and
      E, Haihong  and
      Tang, Zichen  and
      Peng, Shiyao  and
      Guo, Yikai  and
      Zhang, Wentai  and
      Ma, Chenghao  and
      Dong, Guanting  and
      Song, Meina  and
      Lin, Wei  and
      Zhu, Yifan  and
      Luu, Anh Tuan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.122/",
    doi = "10.18653/v1/2024.findings-acl.122",
    pages = "2039--2056",
}
@article{liang2024kag,
  title={Kag: Boosting llms in professional domains via knowledge augmented generation},
  author={Liang, Lei and Sun, Mengshu and Gui, Zhengke and Zhu, Zhongshu and Jiang, Zhouyu and Zhong, Ling and Qu, Yuan and Zhao, Peilong and Bo, Zhongpu and Yang, Jin and others},
  journal={arXiv preprint arXiv:2409.13731},
  year={2024}
}
@article{peng2024graph,
  title={Graph retrieval-augmented generation: A survey},
  author={Peng, Boci and Zhu, Yun and Liu, Yongchao and Bo, Xiaohe and Shi, Haizhou and Hong, Chuntao and Zhang, Yan and Tang, Siliang},
  journal={arXiv preprint arXiv:2408.08921},
  year={2024}
}
@inproceedings{jin-etal-2024-graph,
    title = "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
    author = "Jin, Bowen  and
      Xie, Chulin  and
      Zhang, Jiawei  and
      Roy, Kashob Kumar  and
      Zhang, Yu  and
      Li, Zheng  and
      Li, Ruirui  and
      Tang, Xianfeng  and
      Wang, Suhang  and
      Meng, Yu  and
      Han, Jiawei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.11/",
    doi = "10.18653/v1/2024.findings-acl.11",
    pages = "163--184",
}
@inproceedings{xiong-etal-2024-interactive,
    title = "Interactive-{KBQA}: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models",
    author = "Xiong, Guanming  and
      Bao, Junwei  and
      Zhao, Wen",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.569/",
    doi = "10.18653/v1/2024.acl-long.569",
    pages = "10561--10582",
}
@inproceedings{zhang-etal-2022-subgraph,
    title = "Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering",
    author = "Zhang, Jing  and
      Zhang, Xiaokang  and
      Yu, Jifan  and
      Tang, Jian  and
      Tang, Jie  and
      Li, Cuiping  and
      Chen, Hong",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.396/",
    doi = "10.18653/v1/2022.acl-long.396",
    pages = "5773--5784",
}
@article{BM25,
author = {Robertson, Stephen and Zaragoza, Hugo},
title = {The Probabilistic Relevance Framework: BM25 and Beyond},
year = {2009},
issue_date = {April 2009},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {3},
number = {4},
issn = {1554-0669},
url = {https://doi.org/10.1561/1500000019},
doi = {10.1561/1500000019},
abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970â€”1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
journal = {Found. Trends Inf. Retr.},
month = {apr},
pages = {333â€“389},
numpages = {57}
}

@inproceedings{DPR,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@inproceedings{SimCSE,
    title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
    author = "Gao, Tianyu  and
      Yao, Xingcheng  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.552",
    doi = "10.18653/v1/2021.emnlp-main.552",
    pages = "6894--6910",
}

@article{Contriever,
title={Unsupervised Dense Information Retrieval with Contrastive Learning},
author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=jKN1pXi7b0},
note={}
}
@article{luo2023reasoning,
  title={Reasoning on graphs: Faithful and interpretable large language model reasoning},
  author={Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
  journal={arXiv preprint arXiv:2310.01061},
  year={2023}
}
@inproceedings{sun-etal-2018-open,
    title = "Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text",
    author = "Sun, Haitian  and
      Dhingra, Bhuwan  and
      Zaheer, Manzil  and
      Mazaitis, Kathryn  and
      Salakhutdinov, Ruslan  and
      Cohen, William",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1455/",
    doi = "10.18653/v1/D18-1455",
    pages = "4231--4242",
}
@inproceedings{shu-etal-2022-tiara,
    title = "{TIARA}: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base",
    author = {Shu, Yiheng  and
      Yu, Zhiwei  and
      Li, Yuhan  and
      Karlsson, B{\"o}rje  and
      Ma, Tingting  and
      Qu, Yuzhong  and
      Lin, Chin-Yew},
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.555/",
    doi = "10.18653/v1/2022.emnlp-main.555",
    pages = "8108--8121",
}
@inproceedings{miller-etal-2016-key,
    title = "Key-Value Memory Networks for Directly Reading Documents",
    author = "Miller, Alexander  and
      Fisch, Adam  and
      Dodge, Jesse  and
      Karimi, Amir-Hossein  and
      Bordes, Antoine  and
      Weston, Jason",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1147/",
    doi = "10.18653/v1/D16-1147",
    pages = "1400--1409"
}
@inproceedings{das-etal-2017-question,
    title = "Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks",
    author = "Das, Rajarshi  and
      Zaheer, Manzil  and
      Reddy, Siva  and
      McCallum, Andrew",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2057/",
    doi = "10.18653/v1/P17-2057",
    pages = "358--365",
}
@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://dl.acm.org/doi/pdf/10.5555/3524938.3525306},
}
@inproceedings{jiang-etal-2023-active,
    title = "Active Retrieval Augmented Generation",
    author = "Jiang, Zhengbao  and
      Xu, Frank  and
      Gao, Luyu  and
      Sun, Zhiqing  and
      Liu, Qian  and
      Dwivedi-Yu, Jane  and
      Yang, Yiming  and
      Callan, Jamie  and
      Neubig, Graham",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.495/",
    doi = "10.18653/v1/2023.emnlp-main.495",
    pages = "7969--7992",
    abstract = "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method."
}
@article{asai2023selfrag,
      author    = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
      title     = {{Self-RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
      year      = {2023},
     journal    = {arXiv preprint arXiv:2310.11511},
     url        = {https://arxiv.org/abs/2310.11511}
    }
@article{mavromatis2024gnn,
  title={GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning},
  author={Mavromatis, Costas and Karypis, George},
  journal={arXiv preprint arXiv:2405.20139},
  year={2024}
}
@inproceedings{wang2021relational,
  title     = {Relational message passing for knowledge graph completion},
  author    = {Wang, Hongwei and Ren, Hongyu and Leskovec, Jure},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages     = {1697--1707},
  year      = {2021}
}
@inproceedings{query-rewrite,
    title = "Query Rewriting in Retrieval-Augmented Large Language Models",
    author = "Ma, Xinbei  and
      Gong, Yeyun  and
      He, Pengcheng  and
      Zhao, Hai  and
      Duan, Nan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.322",
    doi = "10.18653/v1/2023.emnlp-main.322",
    pages = "5303--5315",
}
@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{feng2023alphazero,
  title={Alphazero-like tree-search can guide large language model decoding and training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:2309.17179},
  year={2023}
}
@inproceedings{liu-etal-2024-knowledge-graph,
    title = "Knowledge Graph-Enhanced Large Language Models via Path Selection",
    author = "Liu, Haochen  and
      Wang, Song  and
      Zhu, Yaochen  and
      Dong, Yushun  and
      Li, Jundong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.376/",
    doi = "10.18653/v1/2024.findings-acl.376",
    pages = "6311--6321",
    abstract = "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP."
}
@inproceedings{Freebase,
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376746},
doi = {10.1145/1376616.1376746},
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1247â€“1250},
numpages = {4},
keywords = {semantic network, collaborative systems, tuple store},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}
@inproceedings{wang-etal-2024-improving-text,
    title = "Improving Text Embeddings with Large Language Models",
    author = "Wang, Liang  and
      Yang, Nan  and
      Huang, Xiaolong  and
      Yang, Linjun  and
      Majumder, Rangan  and
      Wei, Furu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.642/",
    doi = "10.18653/v1/2024.acl-long.642",
    pages = "11897--11916",
    abstract = "In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks."
}
@inproceedings{zhang-etal-2024-onegen,
    title = "{O}ne{G}en: Efficient One-Pass Unified Generation and Retrieval for {LLM}s",
    author = "Zhang, Jintian  and
      Peng, Cheng  and
      Sun, Mengshu  and
      Chen, Xiang  and
      Liang, Lei  and
      Zhang, Zhiqiang  and
      Zhou, Jun  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.237/",
    doi = "10.18653/v1/2024.findings-emnlp.237",
    pages = "4088--4119",
    abstract = "Despite the recent advancements in Large Language Models (LLMs), which have significantly enhanced the generative capabilities for various NLP tasks, LLMs still face limitations in directly handling retrieval tasks. However, many practical applications demand the seamless integration of both retrieval and generation. This paper introduces a novel and efficient One-pass Generation and retrieval framework (OneGen), designed to improve LLMs' performance on tasks that require both generation and retrieval. The proposed framework bridges the traditionally separate training approaches for generation and retrieval by incorporating retrieval tokens generated autoregressively. This enables a single LLM to handle both tasks simultaneously in a unified forward pass. We conduct experiments on two distinct types of composite tasks, RAG and Entity Linking, to validate the pluggability, effectiveness, and efficiency of OneGen in training and inference. Furthermore, our results show that integrating generation and retrieval within the same context preserves the generative capabilities of LLMs while improving retrieval performance. To the best of our knowledge, OneGen is the first to enable LLMs to conduct vector retrieval during the generation."
}
@inproceedings{STAGG,
    title = "The Value of Semantic Parse Labeling for Knowledge Base Question Answering",
    author = "Yih, Wen-tau  and
      Richardson, Matthew  and
      Meek, Chris  and
      Chang, Ming-Wei  and
      Suh, Jina",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-2033",
    doi = "10.18653/v1/P16-2033",
    pages = "201--206",
}
@inproceedings{CWQ,
    title = "The Web as a Knowledge-Base for Answering Complex Questions",
    author = "Talmor, Alon  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1059",
    doi = "10.18653/v1/N18-1059",
    pages = "641--651",
    abstract = "Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.",
}
@inproceedings{DECAF,
title={Dec{AF}: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases},
author={Donghan Yu and Sheng Zhang and Patrick Ng and Henghui Zhu and Alexander Hanbo Li and Jun Wang and Yiqun Hu and William Yang Wang and Zhiguo Wang and Bing Xiang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XHc5zRPxqV9}
}
@inproceedings{RnG-KBQA,
    title = "{RNG}-{KBQA}: Generation Augmented Iterative Ranking for Knowledge Base Question Answering",
    author = "Ye, Xi  and
      Yavuz, Semih  and
      Hashimoto, Kazuma  and
      Zhou, Yingbo  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.417",
    doi = "10.18653/v1/2022.acl-long.417",
    pages = "6032--6043",
    abstract = "Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GrailQA and WebQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.",
}
@inproceedings{sun2019pullnet,
  title     = {PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text},
  author    = {Sun, Haitian and Bedrax-Weiss, Tania and Cohen, William},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages     = {2380--2390},
  year      = {2019}
}
@inproceedings{UniKGQA,
title={Uni{KGQA}: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph},
author={Jinhao Jiang and Kun Zhou and Xin Zhao and Ji-Rong Wen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Z63RvyAZ2Vh}
}
@inproceedings{gu-su-2022-arcaneqa,
    title = "{A}rcane{QA}: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering",
    author = "Gu, Yu  and
      Su, Yu",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.148/",
    pages = "1718--1731",
    abstract = "Question answering on knowledge bases (KBQA) poses a unique challenge for semantic parsing research due to two intertwined challenges: large search space and ambiguities in schema linking. Conventional ranking-based KBQA models, which rely on a candidate enumeration step to reduce the search space, struggle with flexibility in predicting complicated queries and have impractical running time. In this paper, we present ArcaneQA, a novel generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking. Experimental results on multiple popular KBQA datasets demonstrate the highly competitive performance of ArcaneQA in both effectiveness and efficiency."
}
@article{wu2024comparative,
  title={A Comparative Study on Reasoning Patterns of OpenAI's o1 Model},
  author={Wu, Siwei and Peng, Zhongyuan and Du, Xinrun and Zheng, Tuney and Liu, Minghao and Wu, Jialong and Ma, Jiachen and Li, Yizhi and Yang, Jian and Zhou, Wangchunshu and others},
  journal={arXiv preprint arXiv:2410.13639},
  year={2024}
}
@inproceedings{zheng-etal-2024-llamafactory,
    title = "{L}lama{F}actory: Unified Efficient Fine-Tuning of 100+ Language Models",
    author = "Zheng, Yaowei  and
      Zhang, Richong  and
      Zhang, Junhao  and
      Ye, Yanhan  and
      Luo, Zheyan",
    editor = "Cao, Yixin  and
      Feng, Yang  and
      Xiong, Deyi",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-demos.38/",
    doi = "10.18653/v1/2024.acl-demos.38",
    pages = "400--410",
    abstract = "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks."
}
@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2020},
  url="https://dl.acm.org/doi/10.5555/3433701.3433727"
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{zhang2024alter,
  title={Alter: Augmentation for large-table-based reasoning},
  author={Zhang, Han and Ma, Yuheng and Yang, Hanfang},
  journal={arXiv preprint arXiv:2407.03061},
  year={2024}
}
@article{codeKG,
author = {Bi, Zhen and Chen, Jing and Jiang, Yinuo and Xiong, Feiyu and Guo, Wei and Chen, Huajun and Zhang, Ningyu},
title = {CodeKGC: Code Language Model for Generative Knowledge Graph Construction},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3641850},
doi = {10.1145/3641850},
abstract = {Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provide intermediate steps, thereby improving knowledge extraction abilities. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.1},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {45},
numpages = {16},
keywords = {Knowledge graph construction, code, language model}
}
@article{jiang2024hykge,
  title={HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses},
  author={Jiang, Xinke and Zhang, Ruizhe and Xu, Yongxin and Qiu, Rihong and Fang, Yue and Wang, Zhiyuan and Tang, Jinyi and Ding, Hongxin and Chu, Xu and Zhao, Junfeng and others},
  journal={arXiv preprint arXiv:2312.15883},
  year={2024}
}
@inproceedings{zhang2018variational,
  title     = {Variational reasoning for question answering with knowledge graph},
  author    = {Zhang, Yuyu and Dai, Hanjun and Kozareva, Zornitsa and Smola, Alexander and Song, Le},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {32},
  number    = {1},
  year      = {2018}
}
@article{xie2023self,
  title={Self-evaluation guided beam search for reasoning},
  author={Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={41618--41650},
  year={2023}
}
@article{guan2025rstar,
  title={rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking},
  author={Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2501.04519},
  year={2025}
}
@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{li-etal-2022-c3kg,
    title = "{C}$^3${KG}: A {C}hinese Commonsense Conversation Knowledge Graph",
    author = "Li, Dawei  and
      Li, Yanran  and
      Zhang, Jiayi  and
      Li, Ke  and
      Wei, Chen  and
      Cui, Jianwei  and
      Wang, Bin",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.107/",
    doi = "10.18653/v1/2022.findings-acl.107",
    pages = "1369--1383",
    abstract = "Existing commonsense knowledge bases often organize tuples in an isolated manner, which is deficient for commonsense conversational models to plan the next steps. To fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense knowledge and dialog flow information. To show the potential of our graph, we develop a graph-conversation matching approach, and benchmark two graph-grounded conversational tasks. All the resources in this work will be released to foster future research."
}