\section{Reflection Tokens}
\label{sec:appendix}
\paragraph{Definitions of Reflection Tokens.} This section provides detailed definitions of the four types of reflection tokens used in \model.

\paragraph{Retrieval Token (\ret)} indicates whether the output can be fully verified by the provided evidence and historical information, or if it requires additional external retrieval. There are three possible scenarios:\\
- If the output can be verified using the evidence and history, the Retrieval Token should be \texttt{[No Retrieval]}.\\
- If additional information based on relations is required, the Retrieval Token should be \texttt{[Relation Retrieval]}.\\
- If additional information based on entities is needed, the Retrieval Token should be \texttt{[Entity Retrieval]}.

\paragraph{Relevance Token (\crel)} indicates whether the knowledge retrieved is relevant to the query or contributes to answering it. This is evaluated on a scale from \texttt{[Fully Relevant]} to \texttt{[Partially Relevant]} and \texttt{[Irrelevant]}. Here, "knowledge" refers to relations or entities.

\paragraph{Rationality Token (\cre)} indicates whether the reasoning process (from the topic entity to the answer) is logical and coherent. This is evaluated on a scale from \texttt{[Fully Reasonable]} to \texttt{[Partially Reasonable]} and \texttt{[Unreasonable]}.

\paragraph{Utility Token (\cuse)} indicates whether the answer is a useful response to the query, using a five-point scale from \texttt{[Utility:1]} (the least useful) to \texttt{[Utility:5]} (the most useful).

\section{Details of Datasets}
\label{app:datasets}
This section provides information about the two benchmark datasets used in our experiment.
\paragraph {WebQSP} (WebQuestionsSP)~\citep{STAGG} is a widely-used KGQA dataset. It is developed to evaluate the importance of gathering semantic parses compared to answers alone for a set of questions. WebQSP consists of 4,737 KBQA questions, with 34 logical form skeletons and 2,461 entities involved. There are 628 relations specified within the dataset, which is divided into a training set of 3,098 questions and a testing set of 1,639 questions. This dataset utilizes Freebase as its knowledge base and is tailored for developing systems that can process and answer natural language questions using structured data.
\paragraph {CWQ} (ComplexWebQuestions)~\citep{CWQ} is another commonly used KGQA dataset. It is designed to answer complex questions requiring reasoning over multiple web snippets, which contains a large set of complex questions in natural language and is versatile in its applications. CWQ is considerably larger with 34,689 questions, underpinned by 174 logical form skeletons. It encompasses a more extensive set of entities amounting to 11,422 and includes 845 relations. The training set comprises 27,639 questions, supplemented by a validation set of 3,519 questions and a test set of 3,531 questions. CWQ also leverages Freebase as its knowledge base and is designed for complex question-answering tasks that require the interpretation and synthesis of information from various sources.

\section{Baselines}
\label{app:baseline}
We compare ARG with 13 baseline methods, which can be grouped into 3 categories: 1) Semantic Parsing(SP)-based methods, 2) Information Retrieval(IR)-based methods, and 3) LLM-based methods. In this section, details of baselines are described as follows.
\paragraph {SP-based methods.} 

DECAF~\cite{DECAF} is a framework that jointly generates both logical forms and direct answers, and then combines the merits of them to get the final answers. Moreover, it is based on simple free-text retrieval without relying on any entity linking tools, which eases its adaptation to different datasets.

TIARA~\cite{shu-etal-2022-tiara} is a KBQA model which addresses the issues of coverage and generalization settings by applying multi-grained retrieval to help the PLM focus on the most relevant KB context, viz., entities, exemplary logical forms, and schema items.

ArcaneQA~\cite{gu-su-2022-arcaneqa} is a generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking.

ChatKBQA~\cite{luo-etal-2024-chatkbqa} is a generate-then-retrieve KBQA framework, which proposes first generating the logical form with fine-tuned LLMs, then retrieving and replacing entities and relations with an unsupervised retrieval method, to improve both generation and retrieval more directly.

\paragraph {IR-based methods.}

GrafNet~\cite{sun-etal-2018-open} is a model for extracting answers from a question-specific subgraph containing text and KB entities and relations, which is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting.

PullNet~\cite{sun2019pullnet} is an integrated framework for learning what to retrieve (from the KB and/or corpus) and  reasoning with this heterogeneous information to find the best answer. It uses an iterative process to construct a question-specific subgraph that contains information relevant to the question.

Subgraph Retrieval~\cite{zhang-etal-2022-subgraph} is a trainable model, decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive experiments demonstrate that it achieves significantly better retrieval and QA performance than existing retrieval methods.

UniKGQA~\cite{UniKGQA} is an approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of UniKGQA on the multi-hop KGQA task.

\paragraph {LLM-based methods}

StructGPT~\cite{jiang-etal-2023-structgpt} is an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured data. In this framework, the specialized interfaces collect relevant evidence from structured data (i.e., reading), and LLMs concentrate on the reasoning task based on the collected information (i.e., reasoning).

KG-Agent~\cite{jiang2024kg} is an autonomous LLM-based agent framework, which enables a small LLM to actively make decisions until finishing the reasoning process over knowledge graphs (KGs). It has improved the reasoning ability of LLMs over KGs to answer complex questions.

RoG~\cite{luo2023reasoning} (Reasoning on Graphs) is a method that synergizes LLMs with KGs to enable faithful and interpretable reasoning. It not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference.

% ToG~\cite{sun2023think} (Think-on-Graph) is an approach introduced to further implement the paradigm of “LLM$\otimes$ KG”, in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. In our experiments, we compare our ARG with two kinds of ToG baselines, which use ChatGPT and GPT-4 as the LLM respectively.

\section{Prompt}
\label{app:prompt}
In this section, we present the instructions used to prompt GPT models for collecting self-reflection tokens, including \textit{Relevance}, \textit{Rationality}, and \textit{Utility}. Notably, data for retrieval on demand is not required, as the reasoning path itself provides directional guidance to the model for conducting retrieval. Figure~\ref{fig:pro-rrel} and Figure~\ref{fig:pro-erel} present the instructions for the \textit{Relevance} token, while Figure~\ref{fig:pro-reason} and Figure~\ref{fig:pro-uti} provide the instructions for the \textit{Rationality} token and \textit{Utility} token, respectively.

\section{Details of Score Calculations}
\label{app:score}
We obtain the value of each tree node by computing a confidence score. For each special reflection token $\hat{t}$ generated along the reasoning path at depth-$d$, the confidence score is derived by applying the softmax function to its log probability.
\begin{equation}
    s_d (\hat{t})=\frac{exp\left(p_d(\hat{t})\right)}{\sum_{i=1}^Gexp\left(p_d(t_i)\right)}.
\end{equation}
Here $s_d(\hat{t})$ represents the score of the special token at depth $d$, where $G$ denotes the number of categories for tokens of the same type. The aggregate score for each node is computed using the following equation:
\begin{equation}
    S(d) =S_{seq}(d) + k_1 S_{rel}(d) * S_{rat}(d) + k_2 *S_{uti},
\end{equation}
where $S_{seq}$ denotes the cumulative probability. For a valid reasoning path $w$, the final score is computed by backtracking the score along the pathway. In the experiments, we set $k_1 =1$ and $k_2 =0.5$. For $S_{rel}$, $S_{rat}$ and $S_{uti}$, we compute the score as follows:
\begin{equation}
\begin{aligned}
    S_{rel}(d) &= s_d(\text{\textcolor{myred}{\texttt{[Fully Relevant]}}}) \\
    &+ 0.5 \times s_d(\textcolor{myred}{{\texttt{[Partially Relevant]}}}), \\
    S_{rat}(d) &= s_d(\text{\textcolor{myred}{\texttt{[Fully Reasonable]}}}) \\
    &+ 0.5 \times s_d(\textcolor{myred}{{\texttt{[Partially Reasonable]}}}), \\
    S_{uti}&= \sum_{i=1}^5 s_{D}(\textcolor{myred}{\texttt{[Utility]}} =i). \\
    \end{aligned}
\end{equation}

% graphs
\begin{figure*}[htbp]

\begin{tcolorbox}[left=1mm,right=1mm,top=0.mm, bottom=0mm,colback=white]
\begin{lstlisting}
/*Query:*/ what organization did martin luther king lead

/*Output:*/ @[Retrieve Relation]@{<paragraph>organization.organization_founder.organizations_founded;religion.religious_organization.leaders;organization.organization.founders;organization.organization.leadership;organization.leadership.organization</paragraph>}organization.organization_founder.organizations_founded+[Fully Relevant]+religion.religious_organization.leaders+[Partially Relevant]+organization.organization.founders[Unrelevant]organization.organization.leadership+[Partially Relevant]+organization.leadership.organization+[Partially Relevant]+@[Retrieve Entity]@{<paragraph>(Martin Luther King, Jr., organization.organization_founder.organizations_founded, Montgomery Improvement Association);(Martin Luther King, Jr., organization.organization_founder.organizations_founded, Southern Christian Leadership Conference)</paragraph>}Montgomery Improvement Association+[Fully Relevant]+Southern Christian Leadership Conference+[Fully Relevant]+=[Partially Reasonable]=@[No Retrieval]@Answer: Southern Christian Leadership Conference;Montgomery Improvement Association^[Utility:5]^
\end{lstlisting}
\end{tcolorbox}
\caption{An example of \model training data.}
\label{app:train_data}
\end{figure*}


\begin{figure*}[htbp]
\begin{tcolorbox}[left=1mm,right=1mm,top=0.mm, bottom=0mm,colback=white]
\begin{lstlisting}
You will receive a query, topic entity, evidence and optional preceding sentences containing history information. The evidence contains graph relationships possibly useful to answering the query. Your task is evaluate each relationship's contribution to answering the query and provide a relevance score for each relation, output your explanations for the score.
The score of relevance range from [Fully Relevant], [Partially Relevant] to [Unrelevant]:
- If the relationship directly contains information directly about the query or can answer the query with information in preceding sentences, return [Fully Relevant].
- If the relationship do not directly answer the query, but includes information possibly point to the answer, return [Partially Relevant].
- If the relationship contains irrelevant information about the query, return [Unrelevant].
\end{lstlisting}
\end{tcolorbox}
\caption{Instructions for \crel (for relations).}
\label{fig:pro-rrel}
\end{figure*}

\begin{figure*}[htbp]
\begin{tcolorbox}[left=1mm,right=1mm,top=0.mm, bottom=0mm,colback=white]
\begin{lstlisting}
You will receive a query, evidence and optional preceding historical information for the task. The evidence and preceding information include associated retrieved knowledge graph triplets presented as (head entity, relation, tail entity). 
Your task is to assign a relevance score to the query for each tail entity in the evidence. Additionally, you are required to provide explanations for the scores assigned.
The relevance scores should fall into one of the following categories: [Fully Relevant], [Partially Relevant], or [Unrelevant]. 
\end{lstlisting}
\end{tcolorbox}
\caption{Instructions for \crel (for entities).}
\label{fig:pro-erel}
\end{figure*}

\begin{figure*}[htbp]
\begin{tcolorbox}[left=1mm,right=1mm,top=0.mm, bottom=0mm,colback=white]
\begin{lstlisting}
You will receive a query, output and a reasoning path. The reasoning path contains the current reasoning process starting from the topic entitiy to the answer. Your task is to rate rationality score for the path and output your explanations for the score.
The score of rationality range from [Fully Reasonable], [Partially Reasonable] to [Unreasonable].
\end{lstlisting}
\end{tcolorbox}
\caption{Instructions for \cre.}
\label{fig:pro-reason}
\end{figure*}

\begin{figure*}[htbp]
\begin{tcolorbox}[left=1mm,right=1mm,top=0.mm, bottom=0mm,colback=white]
\begin{lstlisting}
You will be given a query and the answers, where the answers may consist of one or more individual answers, separated by commas(,). 
Your task is to generate a **rating** to evaluate whether the answer is a useful response to the query. 
Use the following entailment scale to give the utility score:
[Utility:5]: Generally, the output provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.
[Utility:4]: Generally, the output mostly fulfills the need in the query and provides helpful answers, while there can be some minor improvements, such as discussing more detailed information or providing additional correct answers beyond the current output.
[Utility:3]: Generally, the output is correct and acceptable, but there are obvious problems, such as being too vague or not specific enough, limiting its helpfulness in addressing the query. 
[Utility:2]: Generally, the output still discusses the topic of the query, but it is incorrect or does not actually meet the requirements of the query.
[Utility:1]: Generally, the output is completely irrelevant to the query or does not give an answer in the end.
\end{lstlisting}
\end{tcolorbox}
\caption{Instructions for \cuse.}
\label{fig:pro-uti}
\end{figure*}

% case 1
\begin{table*}[t!]
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{2pt}
\footnotesize
    \centering
    \begin{tabular}{p{14cm}}
\toprule
% \textbf{Input} & \textbf{Output} \\\midrule
{\bf Input:} Who is Niall Ferguson's wife? (from \texttt{WebQSP})\\
{\bf ToG:} \\
\textit{Depth-1:} Niall Ferguson $\to$ people.person.children $\to$ Thomas Ferguson,
Niall Ferguson $\to$ people.person.gender $\to$ Erkek,
Niall Ferguson $\to$ people.person.nationality $\to$ Scotland,
Niall Ferguson $\to$ people.person.nationality $\to$ United Kingdom, \\
\textit{Depth-2:} Scotland $\to$ base.aareas.schema.administrative\_area.administrative\_parent $\to$ United Kingdom,
Thomas Ferguson $\to$ people.person.gender $\to$ Erkek,
Thomas Ferguson $\to$ people.person.parents $\to$ Ayaan Hirsi Ali,
Thomas Ferguson $\to$ people.person.parents $\to$ Niall Ferguson,
Erkek $\to$ base.gender.gender\_identity.people $\to$ UnName Entity, \\
 \textit{Depth-3:}
UnName Entity $\to$ base.gender.personal\_gender\_identity.person $\to$ Renée Richards,
Ayaan Hirsi Ali $\to$ people.person.children $\to$ Thomas Ferguson,
Niall Ferguson $\to$ people.person.children $\to$ Thomas Ferguson,
UnName Entity $\to$ base.gender.personal\_gender\_identity.gender\_identity $\to$ Erkek,
Niall Ferguson $\to$ people.person.gender $\to$ Erkek. \\
\textit{Answer:} Niall Ferguson's wife is Ayaan Hirsi Ali.\\
\midrule
{\bf \model:} \\
Niall Ferguson $\to$ people.person.spouse\_s $\to$ m.0j4jq57\textcolor{myred}{[Partially Reasonable]},
m.0j4jq57 $\to$ Ayaan Hirsi Ali.\textcolor{myred}{[Fully Reasonable]} \\
Marriage $\to$ people.marriage.type\_of\_union $\to$ m.0j4jt43. \textcolor{myred}{[Unreasonable]}\\
\textit{Answer:} Ayaan Hirsi Ali \textcolor{myred}{[Utility:5]}\\
\bottomrule
 \end{tabular}
    \caption{Case Study 1 between ToG and \model.}\label{case:1}
\end{table*}

% case 2
\begin{table*}[t!]
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{2pt}
\footnotesize
    \centering
    \begin{tabular}{p{14cm}}
\toprule
{\bf Input:} What type of art does Marc Chagall do? (from \texttt{WebQSP})\\
{\bf ToG:} \\
\textit{Depth-1:} Marc Chagall $\to$ visual\_art.visual\_artist.art\_forms $\to$ Glezniec\={i}ba,  
Marc Chagall $\to$ visual\_art.visual\_artist.artworks $\to$ America Windows,  
Marc Chagall $\to$ visual\_art.visual\_artist.artworks $\to$ Autoritratto con sette dita,  
Marc Chagall $\to$ visual\_art.visual\_artist.artworks $\to$ Birth,  
Marc Chagall $\to$ visual\_art.visual\_artist.artworks $\to$ Calvary.\\
\textit{Answer:} Painting\\
\midrule
{\bf \model:} \\
Marc Chagall $\to$ visual\_art.visual\_artist.associated\_periods\_or\_movements $\to$ Modern art\textcolor{myred}{[Fully Relevant]}, 
Marc Chagall $\to$ visual\_art.visual\_artist.associated\_periods\_or\_movements $\to$ Cubism\textcolor{myred}{[Partially Relevant]}, 
Marc Chagall $\to$ visual\_art.visual\_artist.associated\_periods\_or\_movements $\to$ Surrealism\textcolor{myred}{[Partially Relevant]}, 
Marc Chagall $\to$ visual\_art.visual\_artist.associated\_periods\_or\_movements $\to$ Expressionism\textcolor{myred}{[Partially Relevant]}, 
Marc Chagall $\to$ visual\_art.visual\_artist.associated\_periods\_or\_movements $\to$ Fauvism\textcolor{myred}{[Partially Relevant]}.\\
\textit{Answer:} Expressionism; Modern art; Cubism; Surrealism; Fauvism \textcolor{myred}{[Utility:3]}\\
\bottomrule
 \end{tabular}
    \caption{Case Study 2 between ToG and \model.}\label{case:2}
\end{table*}

% case 3
\begin{table*}[t!]
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{2pt}
\footnotesize
    \centering
    \begin{tabular}{p{14cm}}
\toprule
{\bf Input:} Where do Florida Panthers play? (from \texttt{WebQSP})\\
{\bf ToG:} \\
\textit{Depth-1:} Florida Panthers $\to$ sports.sports\_team.location $\to$ Sunrise,
Florida Panthers $\to$ sports.sports\_team.venue $\to$ UnName\_Entity,
Florida Panthers $\to$ sports.sports\_team.venue $\to$ UnName\_Entity,
Florida Panthers $\to$ ice\_hockey.hockey\_team.conference $\to$ Eastern Conference,
Florida Panthers $\to$ ice\_hockey.hockey\_team.division $\to$ Atlantic Division,\\
\textit{Depth-2:} Sunrise $\to$ sports.sports\_team\_location.teams $\to$ Florida Panthers,
UnName\_Entity $\to$ sports.team\_venue\_relationship.venue $\to$ Miami Arena,
UnName\_Entity $\to$ sports.team\_venue\_relationship.team $\to$ Florida Panthers,
Eastern Conference $\to$ ice\_hockey.hockey\_conference.league $\to$ National Hockey League,
Atlantic Division $\to$ ice\_hockey.hockey\_division.conference $\to$ Eastern Conference.\\
\textit{Answer:} Sunrise\\
\midrule
{\bf \model:} \\
Florida Panthers $\to$ sports.sports\_team.arena\_stadium $\to$ Miami Arena\textcolor{myred}{[Partially Relevant]},
Florida Panthers $\to$ sports.sports\_team.arena\_stadium $\to$ BB\&T Center\textcolor{myred}{[Fully Relevant]}.\\
\textit{Answer:} Miami Arena; BB\&T Center \textcolor{myred}{[Utility:4]}\\
\bottomrule
 \end{tabular}
    \caption{Case Study 3 between ToG and \model.}\label{case:3}
\end{table*}

\begin{algorithm*}
\caption{$\mathcal{M}_{gen}$ Data Creation}\label{algo:data_gen}
% \Require Retrieval model \mret, Critic model \mcrt, passage collections $\{d_1, \ldots, d_N\}$
 {\bfseries Input: }{ Query $q$, Valid reasoning path $w=w_{1:D}$, Retriever $\mathcal{R}$, Critic Model $\mathcal{C}$}\\
  {\bfseries Output: }{Augmented reasoning path and self-reflection tokens}\\
\SetKwComment{Comment}{\textcolor{blue}{/* }}{\textcolor{blue}{*/} }
% \State Initialize $\mathcal{M}_{reward} \gets \mathcal{M}$
% \State Sample $P_k$ from $P$
\While{$d \leq D$} 
 {$(r_d, e_d) = w_d$\;
 Add \rret~== True \;
 $\mathbf{C_r}$ = Retrieve$(q, r^d, e^d)$ using $\mathcal{R}$ 
\tcc*{Get candidate relationships}
\mcrt predicts \crel for each $c_r \in \mathbf{C_r}$   \; 
% \textcolor{blue}{Predict relevance of relations}
Add \eret~== True\;
 $\mathbf{C_e}$ = SearchTailNode$(e^d, r^d)$
 \tcc*{Get candidate brother nodes}
\mcrt predicts \crel for each $c_e \in \mathbf{C_e}$\;
\mcrt predicts \cre based on current reasoning path $w_{<=d}$ \\
\tcc{Evaluate reasoness based on current path}
\uIf{\cre == \textcolor{myred}{\small{\texttt{[UnReasonable]}}}}
{break
\tcc*{Early stop for unreasonable path}}}

    % \State \mcrt predicts \crel for each $c_e \in \mathbf{C_e}$ 
Add \rret~== False\;
\mcrt predicts \cuse for each $a$ in $\mathcal{A}$ \;
% \textcolor{blue}{Predict overall utility} 
% Add augmented $(q, w, r)$ to $\mathcal{D}$    

\end{algorithm*}