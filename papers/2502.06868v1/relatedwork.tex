\section{Related Works}
\label{sec:appendix}

\subsection{Knowledge Editing}
Model editing has gained significant attention for its ability to efficiently update LLMs. Existing approaches can be categorized into four types: \textbf{Fine-tuning} mainly applies layer-wise adjustments to incorporate new knowledge into LLMs \cite{zhu2021modifying}. \textbf{Meta Learning} trains hypernetworks to act as editors, predicting parameter updates to inject new knowledge \cite{de2021editing, mitchell2022fast}. \textbf{Memory-based} enhances LLMs with external memory or additional parameters, allowing new knowledge to be added without altering LLMs \cite{mitchell2022memory, huang2023transformerpatcher}.

Among all types, \textbf{Locate-then-Edit} has gained significant traction for its ability to modify specific knowledge within LLMs. Methods like KN\cite{dai-etal-2022-knowledge} and ROME\cite{meng2022locating} locate and update factual knowledge by targeting neurons or multi-layer perceptrons (MLPs) that store such information. MEMIT\cite{meng2022memit} extends ROME by distributing updates across multiple intermediate MLP sublayers, enabling large-scale knowledge editing. Additionally, PMET\cite{li2024pmet} combines information from both multi-head Self-attention (MHSA) and MLP modules during optimization, producing more accurate MLP outputs for final edits.

While model editing has shown great promise, some researches have identified issues such as model collapse\cite{yang2024butterfly, gu2024model} and knowledge conflicts\cite{li2024unveiling}. This paper focuses on how the correlation between knowledge impacts the performance of model editing, particularly in the context of multiple knowledge edits.

% \textbf{Fine-tuning.}\quad These methods apply layer-wise fine-tuning to incorporate new knowledge into LLMs. For example, FT\cite{zhu2021modifying} propose fine-tuning LLMs with a norm constraint between the edited and original model parameters to reduce the risk of catastrophic forgetting.

% \textbf{Meta Learning.}\quad This category trains a hypernetwork to act as an editor, predicting parameter updates to inject new knowledge. KE\cite{de2021editing} uses a trained hypernetwork to predict parameter modifications for each edit request. Similarly, MEND\cite{mitchell2022fast} employs hypernetworks to learn a low-rank decomposition of fine-tuning gradients, enabling efficient modifications of LLMs to accommodate new facts.

% \textbf{Memory-based.}\quad These methods enhance LLMs with external memory or additional parameters, allowing new knowledge to be added without altering the core model. SERAC\cite{mitchell2022memory} uses external memory to store edits and retrieves relevant information. T-Patcher\cite{huang2023transformerpatcher} introduces key-value pairs into MLP modules to incorporate specific knowledge without affecting unrelated information.

% \textbf{Locate-then-Edit.}\quad  This paradigm have gained significant traction for its ability to modify specific knowledge within LLMs. For instance, methods like KN\cite{dai-etal-2022-knowledge} and ROME\cite{meng2022locating} developed techniques to locate and update factual knowledge by targeting neurons or multi-layer perceptrons (MLPs) that store such information. MEMIT\cite{meng2022memit} further enhances ROME by distributing updates across multiple intermediate MLP sublayers, accommodating large-scale knowledge editing. Additionally, PMET\cite{li2024pmet} combines information from both the multi-head Self-attention (MHSA) and MLP modules during the optimization process, allowing it to produce more accurate MLP output for editing at the final body position.

% The goal of model editing is to update specific knowledge LLMs accurately and efficiently, without affecting unrelated information. Formally, given factual knowledge $t=(s,r,o)$, consisting of subject $s$, relation $r$, and object $o$, and an LLM $f_\theta$, along with target factual knowledge $t'=(s,r,o')$, where $o' \neq o$, the goal of a model editing algorithm $\xi$ is to optimize the parameter $\theta$ into $\theta'$ so that the edited model $f_{\theta'}$: $\xi(f_{\theta},t)=f_{\theta'}$ correctly produces $o'$ when provided with the prompt $p(s,r)$, as $f_{\theta'}(p(s,r))=o'$. 

% Among various model editing methods, \textit{Rome-like editing methods}, such as ROME and MEMIT, have garnered significant attention due to their outstanding editing performance. These methods first employ causal tracing to identify that factual knowledge is primarily stored in the early MLP layers of LLMs. Based on the hypothesis that "the MLP modules in Transformer layers can be viewed as linear key-value associative memory", we can get $Wk=v$, where $W$ represents the downsample component of MLP, and the key-value pair $(k, v)$ corresponds to a factual triplet $t=(s, r, o)$. Here, $k$ represents the subject $s$, while $v$ encodes the attributes of $s$, including $r$ and $o$. Therefore, when $t=(s, r, o)$ needs to be replaced by $t^*=(s, r, o_{\ast})$, we only need to find the corresponding key vector $k_{\ast}$ and the new value vector $v_{\ast}$. By calculating the update $\Delta W$, we can effectively update the stored knowledge in the large model.

% Take ROME for example assigns $k_{\ast}$ as an average vector derived from subject $s$ with a small set of $N$ randomly sampled prefixes:

% \begin{equation}
%   \label{eq:example}
%   k_{\ast} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{K}(x_i \oplus s)
% \end{equation}

% \noindent where $\mathcal{K}$ is the output of the first MLP layer in transformer block, $x_i$ is the prefixes, and $\oplus$ is string concatenation operator. 

% Then $v_{\ast}$ is calculated from $(s,r,o_{\ast})$ and the specially loss function. Given $(k_{\ast}, v_{\ast})$, ROME finds optimal $\widehat{W}$ to solve the following problem:

% \begin{equation}
%   \label{eq:example}
%   \text{minimize} \ \|\widehat{W}K - V\| \quad \text{s.t.} \quad \widehat{W}k_{\ast} = v_{\ast}
% \end{equation}

% It has the following closed-form solution:

% \begin{equation}
%   \label{eq:example}
%   \widehat{W} = W + \frac{(v_{*} - W k_{*}) (C^{-1} k_{*})^{\top}}{(C^{-1} k_{*})^{\top} k_{*}}
% \end{equation}

% \noindent where $W$ denotes the weight matrix of the second layer of the MLP before editing, $\widehat{W}$ denotes the weight matrix after editing, and $C=KK^{\top}$ is a precached constant.

% For other Rome-like Editing Methods, MEMIT further improves ROME by evenly distributing updates across multiple intermediate MLP sublylayers to accommodate the needs of large-scale knowledge editing. The PMET combines information from both the multi-head Self-attention (MHSA) and MLP modules during the optimization process, allowing it to generate a more accurate MLP output for editing at the final body position.

\subsection{Sequantial-editing vs. Batch-editing} \label{sec:comparison}

\textit{Sequential-editing} and \textit{batch-editing} are two strategies commonly used to update large amounts of knowledge in LLMs\cite{yao-etal-2023-editing}. Specifically, \textit{sequential-editing} refers to making multiple edits one after another, where the model should ideally retain previous changes as new edits are introduced. In contrast, \textit{batch-editing} involves editing multiple pieces of knowledge in a model at once. Notably, these two strategies can be combined to create a more flexible knowledge editing approach.

For the purposes of this study, we evaluate these strategies independently: In \textit{sequential-editing}, the batch size is set to 1, and in \textit{batch-editing}, the number of consecutive edits is set to 1, ensuring clear comparisons and facilitate experimental evaluation.

% \begin{table}[h]\small
%   \centering
%   \renewcommand\arraystretch{1.5}
%   \begin{tabular}{cccccc}
%     \hline
%      Relation Number & > 0     & > 1  & > 2 & > 3 & > 4 \\
%     \hline
%      Subject Number  & 20391  & 785 & 47 & 9  & 0  \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%   }
% \end{table}

% 数据集的构建