% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{enumitem}  % 导入enumitem宏包
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage[marginal]{footmisc}
\renewcommand{\thefootnote}{}

% If the title and author information does not fit in the area allocated, uncomment the following
% \setlength\titlebox{<dim>}
% and set <dim> to something 5cm or larger.

\title{Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject}

%  \textbf{Zenghao Duan}\textsuperscript{1,2}\footnotemark[*],
%  \textbf{Wenbin Duan}\textsuperscript{3}\footnotemark[*],
%  \textbf{Zhiyi Yin}\textsuperscript{1}\footnotemark[†],
%  \textbf{Yinghan Shen}\textsuperscript{1}\footnotemark[†],

\author{
 \textbf{Zenghao Duan}\textsuperscript{1,2,*},
 \textbf{Wenbin Duan}\textsuperscript{3,*},
 \textbf{Zhiyi Yin}\textsuperscript{1,\(\dagger\)},
 \textbf{Yinghan Shen}\textsuperscript{1,\(\dagger\)},
\\
 \textbf{Shaoling Jing}\textsuperscript{1},
 \textbf{Jie Zhang}\textsuperscript{1},
 \textbf{Huawei Shen}\textsuperscript{1},
 \textbf{Xueqi Cheng}\textsuperscript{1},
\\
 \textsuperscript{1}Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
 \\
 \textsuperscript{2}University of Chinese Academy of Sciences, Beijing, China
 \\
 \textsuperscript{3}People's Public Security University of China, Beijing, China
\\
 \small{
     \href{mailto:email@domain}{\{duanzenghao24s, yinzhiyi, Shenyinghan, jingshaoling, zhangjie, shenhuawei, cxq\}@ict.ac.cn}
 }
%  \\
%  \small{
%      \href{mailto:email@domain}{202021250032@stu.ppsuc.edu.cn}
%  }
}

\begin{document}

\maketitle

\setcounter{footnote}{0} 

% 处理特殊符号脚注
\stepcounter{footnote}
\footnotetext[\value{footnote}]{* Equal Contributions}
\stepcounter{footnote}
\footnotetext[\value{footnote}]{\(\dagger\) Corresponding authors}

% 处理数字编号脚注
\setcounter{footnote}{1} % 设置脚注计数器为 3
\footnotetext[1]{Our benchmark and source code are available at: \url{https://github.com/Zhow01/S2RKE}}


\begin{abstract}

% Knowledge editing has become a promising approach for efficiently and precisely updating knowledge in large language models (LLMs). However, in the context of multiple knowledge edits, the complex correlations between knowledge have been largely overlooked in existing research. In this work, we investigate these correlations and, through preliminary experiments, find that current state-of-the-art editing methods struggle to edit multiple related knowledge pieces for the same subject. To address the lack of relevant editing data for identical subjects in traditional benchmarks, we construct the \textbf{$\text{S}^2\text{RKE}$} (\textbf{S}ame-subject \textbf{R}elated \textbf{K}nowledge \textbf{E}diting) benchmark. Our a series of experiments show that only mainstream locate-then-edit methods, such as ROME and MEMIT, exhibit \textit{"related knowledge perturbation,"} where subsequent edits interfere with earlier ones. Further analysis reveals that these methods over-rely on subject information, neglecting other critical factors, resulting in reduced editing effectiveness. 

% 文章的标题是否需要改成ROME范式遇到的多条知识编辑遇到的失败？

Knowledge editing has become a promising approach for efficiently and precisely updating knowledge embedded in large language models (LLMs). In this work, we focus on \textbf{Same-Subject Editing}, which involves modifying multiple attributes of a single entity to ensure comprehensive and consistent updates to entity-centric knowledge. Through preliminary observation, we identify a significant challenge: \textit{Current state-of-the-art editing methods struggle when tasked with editing multiple related knowledge pieces for the same subject.} To address the lack of relevant editing data for identical subjects in traditional benchmarks, we introduce the \textbf{$\text{S}^2\text{RKE}$} (\textbf{S}ame-subject \textbf{R}elated \textbf{K}nowledge \textbf{E}diting) benchmark. Our extensive experiments reveal that only mainstream locate-then-edit methods, such as ROME and MEMIT, exhibit \textit{"related knowledge perturbation,"} where subsequent edits interfere with earlier ones. Further analysis reveals that these methods over-rely on subject information, neglecting other critical factors, resulting in reduced editing effectiveness.

\end{abstract}


\section{Introduction}

% 写作逻辑：
    % 1. 大背景：“知识编辑”出现的意义是...+ROME/MEMIT是两种重要的知识编辑方法；
    % 2. 小背景：在不同的知识编辑场景中，same-subject编辑是指...，具有...的重要意义。
    % 3. Pilot Observation发现了...

% 知识编辑大背景
% Recently, large language models (LLMs) \cite{achiam2023gpt, touvron2023llama} have achieved remarkable advancements, significantly improving performance across various natural language processing tasks. However, as the external world continuously evolves, the knowledge stored within these models requires regular updates \cite{zhang2024comprehensive}. Retraining entire models is resource-intensive, leading to increased interest in  \textit{knowledge editing} (a.k.a., \textit{model editing}) \cite{yao-etal-2023-editing}, which allows for efficient knowledge updates without retraining the entire model.

% 大背景
The dynamic nature of real-world knowledge necessitates efficient methods for updating specific facts in large language models (LLMs) \cite{achiam2023gpt, touvron2023llama} without compromising their overall performance.
\textit{Knowledge editing}(a.k.a., \textit{model editing}) \cite{yao-etal-2023-editing} has emerged as a promising solution to address this challenge, enabling targeted updates to model parameters without requiring full retraining. 
Among existing methods, \textit{locate-then-edit} methods, such as ROME \cite{meng2022locating} and MEMIT \cite{meng2022memit}, have shown effectiveness in making precise modifications to Transformer layer parameters \cite{vaswani2017attention}. However, their broader applicability across diverse editing scenarios remains insufficiently explored.

\begin{figure}[t]
  \center
  \includegraphics[width=\columnwidth]{fig/fig1.pdf}
  \caption{Comparison of performance on Different and Same-Subject Editing. (a) Editing individual knowledge pieces for distinct subjects, "James" and "Messi," results in excellent performance. (b) Editing two related knowledge pieces for the same subject, "James," leads to poor performance.}
  \label{fig:introduction}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=2\columnwidth]{fig/fig2.pdf}
  \caption{The results of \textit{sequential-editing} by three different schemes on GPT-J using MEMIT, comparing five evaluation metrics. The values of Score(\textbf{S}), Efficacy Success(\textbf{ES}) and Paraphrase Success(\textbf{PS}) always decreased with the subject density, but Neighborhood Success(\textbf{NS}) and Perplexity(\textbf{PPL}) remained unchanged.}
  \label{fig:pilot sequential}
\end{figure*}


% Among various editing methods, \textit{Locate-then-edit} methods, such as ROME \cite{meng2022locating} and MEMIT \cite{meng2022memit}, stand out for their ability to modify knowledge in LLMs by precisely updating the parameters in Transformer layers \cite{vaswani2017attention}. However, in the context of multiple knowledge edits \cite{li2024can}, the complex correlations between knowledge pieces can significantly impact editing performance. Existing research mainly focuses on scenarios where multiple knowledge pieces are edited around a single subject.

% 小背景
In particular, \textbf{Same-Subject Editing}, modifying multiple attributes of a single entity, plays a critical role in ensuring comprehensive and consistent updates to entity-centric knowledge. 
As shown in Figure~\ref{fig:introduction}, an entity like "James" may require simultaneous edits to attributes such as "isCitizenOf," "playsFor," and others. 
This process refines the entity's representation by resolving attribute conflicts and synchronizing interdependent facts. Despite its significance, same-subject editing has largely been overlooked in existing research.

% However, a broader perspective on multiple knowledge editing ought to incorporate two scenarios: (1) Different-subject editing, where the knowledge consists of a subject and a specific relational attribute,editing across different subjects; (2) Same-subject editing, where the knowledge comprises a subject and multiple relational attributes,editing within the same subject. Of course, the objects of editing may differ or remain the same. Existing research works are mainly confined to Different-subject editing , yet lack established metrics for evaluating the effects of Same-subject editing. However, in the context of multiple knowledge editing, we argue that Same-subject editing holds greater practical significance, as it better reflects real-world complexities where knowledge systems often require coordinated modifications across multiple attributes within a unified subject framework. For example, as shown in Figure~\ref{fig:introduction}, we typically want to update both "James plays for Lakers" and "James is a citizen of USA", rather than just updating one of them.

% We conducted preliminary observation by collecting suitable data from the COUTERFACT dataset, and identify an unusual failure unexpectedly: \textit{some top-performing editing methods struggle to edit multiple related knowledge pieces for the same subject.} As shown in Figure~\ref{fig:introduction}, model editors perform well when editing one piece of knowledge each for "James" and "Messi" (Figure~\ref{fig:introduction}a). However, when editing two pieces of knowledge for the same subject, "James," the methods become significantly less effective (Figure~\ref{fig:introduction}b). This observation raises two key questions:

% \begin{itemize}[itemsep=0pt, parsep=0pt]
%     \item \textit{Is this failure a common issue across different LLMs and editing methods?}
%     \item \textit{What causes the failure when editing multiple related knowledge pieces about same subject?}
% \end{itemize}

% 我们观察和提出的问题
Through preliminary observations, we identify an unusual failure: \textit{Some top-performing editing methods struggle to edit multiple related knowledge pieces for the same subject.} As illustrated in Figure~\ref{fig:introduction}, model editors perform well when editing individual knowledge pieces for different subjects, such as "James" and "Messi" (Figure~\ref{fig:introduction}a). However, when tasked with editing two related pieces of knowledge for the same subject, "James," these editors become significantly less effective (Figure~\ref{fig:introduction}b). This observation raises two key questions:

\begin{itemize}[itemsep=0pt, parsep=0pt]
    \item \textit{Is this failure a common issue across different LLMs and editing methods?}
    \item \textit{What causes the failure when editing multiple related knowledge pieces about same subject?}
\end{itemize}

% 数据集构建&实验验证
Existing benchmarks, such as COUNTERFACT \cite{meng2022locating}, lack sufficient examples of same-subject editing, making it difficult to explore the underlying mechanisms of this failure. To address this gap, we introduce the \textbf{$\text{S}^2\text{RKE}$} (\textbf{S}ame-subject \textbf{R}elated \textbf{K}nowledge \textbf{E}diting) benchmark, which associates each subject with multiple related edits. We systematically evaluate various editing methods on LLMs of different sizes using $\text{S}^2\text{RKE}$, applying both \textit{sequential-editing} and \textit{batch-editing}. Surprisingly, the results show that only mainstream locate-then-edit methods, such as MEMIT \cite{meng2022memit}, fail to effectively update multiple related information for  the same subject. Moreover, our in-depth analysis reveals that this failure occurs because subsequent edits interfere with previous ones, a phenomenon we term \textit{"related knowledge perturbation."}

Furthermore, we find that locate-then-edit methods exhibiting \textit{"related knowledge perturbation"} update the weight matrix of the MLP module by calculating key-value pairs. Specifically, the key is derived from the input of the subject’s last token in the MLP module’s down-sampling layer. % 编辑方法的描述
Our experiments conclude that the perturbation arises from an over-reliance on subject information during editing. When multiple related pieces of knowledge share the same subject, the calculated keys remain highly similar. As a result, subsequent edits interfere with earlier ones, diminishing the overall effectiveness of the editing process.

In essence, our main contributions are as follows: (1) We propose the $\text{S}^2\text{RKE}$ benchmark for Same-Subject Editing and highlight the issue of "\textit{related knowledge perturbation}." (2) We demonstrate that locate-then-edit methods fail to update multiple related facts for the same subject due to an over-reliance on subject-specific information.


\begin{figure*}[t]
  \centering
  \includegraphics[width=2\columnwidth]{fig/fig4.pdf}
  \caption{The results of differences in  \textit{sequential-editing} results in two scenarios on three LLMs by six editing methods. \textbf{Score Difference (SD)} represents the difference in editing performance between the two experimental schemes when editing the same amount of knowledge under the same method.}
  \label{fig:Failue sequential}
\end{figure*}

\section{Preliminary}
\subsection{Knowledge in Language Model}
% 大模型的基本公式，简单提及有知识；
% Autoregressive, decoder-only LLMs map a sequence of tokens \(x = [x_1, \dots, x_T] \in X \), where \( x_i \in V \) is a token from the vocabulary \( V \), to a probability distribution \( y \in Y \subset \mathbb{R}^{|V|} \), predicting the continuation of the next token in the sequence. In the Transformer architecture, the \( i \)-th token is embedded as a series of hidden state vectors \( h^{(l)}_i \), starting from \( h^{(0)}_i = \text{emb}(x_i) + \text{pos}(i) \in \mathbb{R}^H \). The final output \( y = \text{decode}(h^{(L)}_T) \) is read from the last hidden state. At each layer, global attention \( a^{(l)}(i) \) and contributions from the local MLP \( m^{(l)} \), computed from previous layers, are added, where each token can only attend to the states of prior tokens.

Autoregressive, decoder-only large language models (LLMs) process a sequence of tokens \(x = [x_1, \dots, x_T] \in X\), where each token \(x_i \in V\) is drawn from a vocabulary \(V\), and predict the probability distribution \(y \in Y \subset \mathbb{R}^{|V|}\) for the next token in the sequence. In the Transformer architecture, each token \(x_i\) is embedded into a sequence of hidden state vectors \(h^{(l)}_i\), starting from the initial embedding \(h^{(0)}_i = \text{emb}(x_i) + \text{pos}(i) \in \mathbb{R}^H\). The final output \(y = \text{decode}(h^{(L)}_T)\) is derived from the last hidden state of the sequence. At each layer \(l\), the hidden state \(h_i^{(l)}\) is updated through a combination of global attention \(a_i^{(l)}\) and contributions from the local MLP \(m_i^{(l)}\), where each token attends only to preceding tokens:

\begin{equation}
h_i^{(l)} = h_i^{(l-1)} + a_i^{(l)} + m_i^{(l)},
\end{equation}
\begin{equation}
a_i^{(l)} = \text{attn}^{(l)} \left( h_1^{(l-1)}
h_2^{(l-1)}, \dots, h_i^{(l-1)} \right),
\end{equation}
\begin{equation}
m_i^{(l)} = W_{\text{proj}}^{(l)} \sigma \left( W_{\text{fc}}^{(l)} \gamma \left( a_i^{(l)} + h_i^{(l-1)} \right) \right),
\end{equation}

In many previous studies, knowledge has been represented as triples $(s, r, o)$, where $s$, $r$, and $o$ denote subject, relation, and object respectively (e.g., \text{James} ($s$), \text{playsFor} ($r$), and \text{Lakers} ($o$))\cite{meng2022locating,li2024pmet}. Researchers typically design natural language templates for each relation type and integrate these templates with subject terms to create question or cloze-style prompts.

\subsection{Same-Subject Editing}

% In a broader context, knowledge editing should enable the querying and modification of knowledge within language models by utilizing diverse combinations of subjects (\(s\)) and relations (\(r\)) as prompts. Previous work has focused on modifying knowledge in the model \((s, r, o) \rightarrow (s, r, o^*)\), where, for example, in the popular COUTERFACT dataset, a subject (\(s\)) usually corresponds to a relation (\(r\)) as a cloze-style prompt.For example, after editing the fact (\(\text{``James''}, \text{``playsFor''}, \text{``Lakers''}, \text{``Object\_new''}\)), traditional editing often discontinues further knowledge editing related to the same subject \(s = \text{``James''}\) and instead shifts focus to editing knowledge for a new subjectentity, such as \(\text{(``Messi``, ``isCitizenOf``, ``Argentina``,``Object\_new``)}\). However, this risks overlooking knowledge perturbations within the same subject.

In a broad sense, knowledge editing aims to allow for the querying and modification of a wide range of facts within language models by combining different subjects \((s)\) and relations \((r)\) as prompts. Existing work typically focuses on modifying individual facts expressed as \((s, r, o) \rightarrow (s, r, o_{\ast})\), where each subject \((s)\) is associated with a specific relation \((r)\). However, traditional editing often isolates the editing process to a single relation. This leads to the discontinuation of further knowledge edits for the same subject and a shift towards editing knowledge for a new subject. It risks overlooking potential perturbations in knowledge when editing multiple related facts for the same subject.

% In a broader sense, knowledge editing should allow for querying and modifying a wide range of facts within language models by combining different subjects \((s)\) and relations \((r)\) as prompts. Existing work typically focuses on modifying individual facts in the form \((s, r, o) \rightarrow (s, r, o^*)\), where each subject \((s)\) is associated with a specific relation \((r)\). For instance, once a fact such as \((\text{``James''}, \text{``playsFor''}, \text{``Lakers''})\) is edited, most approaches switch to editing facts for another subject (e.g., \((\text{``Messi''}, \text{``isCitizenOf''}, \text{``Argentina''})\)), overlooking additional facts tied to the same subject \(\text{``James''}\). This practice risks missing or introducing unintended changes---collectively referred to as \emph{knowledge perturbations}---within the same subject.

We introduce the concept of  \textbf{Same-Subject Editing}, where multiple relations are edited simultaneously for a single subject. Instead of focusing solely on the traditional \((s, r, o)\) format, we extend the editing process to structured prompts such as \((s, R, O)\), where $R = \{r_i\}_{i=1}^N$ represents a set of relations and $O = \{o_i\}_{i=1}^N$ represents their corresponding objects. For example, \{(\text{"James"}, \text{"playsFor"}, \text{"Lakers"}), (\text{"James"}, \text{"isCitizenOf"}, \text{"USA"})\}.
We formally define the edited fact set as $e = {(s, r_i, o_i)}_{i=1}^N$ and define the edited model as $M^* = F(M, e)$, where $F$ is the editing function that updates the original model $M$. It ensures that knowledge updates remain consistent across all related attributes of the same subject.

% Therefore, we explores a scenario where multiple relations serve as prompts under a given subject. Specifically, we extend the conventional $(s, r,o)$ pairing to structured prompts $(s, R, O)$ where $R = \{r_i\}_{i=1}^N$ and $O = \{o_i\}_{i=1}^N$, exemplified by 
% \{(\text{``James''}, \text{``playsFor''}, \text{``Lakers''}), (\text{``James''}, \text{``isCitizenOf''}, \text{``USA''})\}.
% Formally defining the edited fact set $e = \{(s, r_i, o_i)\}_{i=1}^N$, the edited model $M^*$ is derived through \(M^* = F(M, e)\),
% where $F$ denotes the editing function applied to the original model $M$.



\section{Pilot Observation}
In this section, we conduct a pilot observation to reveal potential issues with same-subject editing.
% —i.e., updating multiple pieces of knowledge related to a single subject. 

% \textbf{Editing Dataset}\quad  After comparing the existing commonly used knowledge editing datasets zsRE and COUNTERFACT, we find that only COUNTERFACT contains a small amount of data meeting the experimental requirements and is suitable for the preliminary observation experiment. Specifically, zsRE lacks clearly differentiated relationship representation, making it difficult to find multiple related knowledge of the same entity; However, the entity distribution in COUNTERFACT is very sparse, and only a small number of entities have editable relevant knowledge, as shown in Table 1. Therefore, this section selects only eligible data from COUNTERFACT for analysis.

% \textbf{Evaluation Setup}\quad In this section, we focus on using MEMIT to edit GPT-J \cite{wang2021gpt}. Due to limitations in widely used model editing datasets, only a small number of entities in the COUNTERFACT dataset have multiple editable pieces of related knowledge (see Table 1). Therefore, we select only the qualified data from COUNTERFACT for our experiment. To provide clearer comparisons of the results, we categorize the editing datas into three levels based on the number of associated knowledge edited for each entity:

\textbf{Evaluation Setup.} We focus on using MEMIT \cite{meng2022memit} to edit GPT-J \cite{wang2021gpt}, since their excellent performance in editing multiple pieces of knowledge. To analyze the impact of editing density—defined here as the average number of related edits per subject in the editing sequence—we divide our experimental schemes into three categories:

\begin{enumerate}[label=\alph*), itemsep=0pt, parsep=0pt]
    \item \textit{\textbf{High-Density}: Edit \textbf{n} pieces of knowledge in total, with each subject edited for \textbf{3} related pieces of knowledge.}
    \item \textit{\textbf{Medium-Density}: Edit \textbf{n} pieces of knowledge in total, with each subject edited for \textbf{2} related pieces of knowledge.}
    \item \textit{\textbf{Low-Density}:  Edit \textbf{n} pieces of knowledge in total, with each subject edited for \textbf{1} related pieces of knowledge.}
\end{enumerate}

% We conducted both \textit{sequential-editing} and \textit{batch-editing} to evaluate the \textit{efficacy}, \textit{generalization}, \textit{specificity}, and \textit{overall performance} \cite{yang2024butterfly} of the edited model. For detailed experimental settings, please refer to the attachment.

Based on the above schemes, we select qualified data from COUNTERFACT \cite{meng2022locating} and conduct experiments using both \textit{sequential-editing} and \textit{batch-editing} (See Appendix~\ref{sec:comparison} for comparison of sequential- and batch-editing). The editing performance is comprehensively evaluated across four dimensions: \textbf{efficacy}, \textbf{generalization}, \textbf{specificity}, and \textbf{overall performance} (See Appendix \ref{Evolution metrics} for detailed metric descriptions).

% 实验结论+问题提出
\textbf{Result \& Analysis.} Figure~\ref{fig:pilot sequential} and Figure~\ref{fig: pilot batch} show the experimental results of employing MEMIT to edit GPT-J through \textit{sequential-editing} and \textit{batch-editing}, respectively.   % 现象
It is evident that when editing the same number of knowledge, the denser the subject distribution, the worse the editing performance, while the impact on the model's downstream performance remains similar.
However, the scarcity of sufficiently dense same-subject instances in existing editing datasets limits the scope of experimental verification. We will further investigate this phenomenon in subsequent sections.

% To explore this further, we raise two key questions:
% \begin{itemize}[itemsep=0pt, parsep=0pt]
%   \item \textit{Do all editing methods exhibit poor performance when editing related knowledge?}
%   \item \textit{What is the specific cause of this performance degradation during editing?}
% \end{itemize}

% It can be obviously observed that during Sequential editing, MEMIT's ES and PS decrease significantly with the increase of the number of related knowledge contained in each edited entity in the editing sequence. However, NS and PPL did not change significantly, indicating that the language generation capability of the model remained stable despite the decline in editing performance. The result is shown in Figure 2. In addition, the experimental results of Batch-editing are consistent with the phenomenon of Sequential editing, and the results are attached. Based on these preliminary findings, we raise two questions worth exploring:

% Due to the limitations of the existing dataset, which limited the experiment size, we reconstructed a new dataset (see section 4) to explore the above issues more fully.

% 主实验部分
\section{Related Knowledge Perturbation}

% In this section, we systematically studied the performance of different model editing methods when editing multiple associated knowledge for the same subject. Our experiments revealed that ROME-Like editing methods exhibit \textit{"associated knowledge perturbation"} phenomenon, where subsequent edits interfere with previous ones.

Furthermore, we construct a benchmark and evaluate the performance of editing methods when editing related knowledge for the same subject.

\subsection{$\text{S}^2\text{RKE}$ Benchmark}
% 参考MQUAKE、History Matters、ConceptEdit、CF论文进行写作；

% In order to overcome the limitations of current model editing datasets, particularly in relation to the editing of multiple assoociated pieces of knowledge for a single subject, we have developed the \textbf{MAPKE} (\textbf{M}ultiple \textbf{A}ssociated \textbf{P}ieces of \textbf{K}nowledge \textbf{E}diting) benchmark. 

 We introduce the \textbf{$\text{S}^2\text{RKE}$} (\textbf{S}ame-subject \textbf{R}elated \textbf{K}nowledge \textbf{E}diting) benchmark, specifically designed to facilitate the editing of multiple related pieces of knowledge for each subject. It covers six categories of subjects, comprising of 4,503 subjects and 43 relationships, with each subject having an average of 4.9 related knowledge items. See Appendix \ref{technical details} for additional technical details about its construction and Table \ref{table: Comparison of different benchmarks} for comparison of statistics between $\text{S}^2\text{RKE}$ and COUNTERFACT.

\begin{table}[t]
\centering
\scalebox{0.73}{
\begin{tabular}{lcc}
\hline
\textbf{Item}  & \textbf{$\text{S}^2\text{RKE}$}& \textbf{COUNTERFACT} \\
\hline
Records       & 22064   & 21919   \\
Subjects      & 4503    & 20391  \\
Relations     & 43      & 32   \\
Maximum records per subject    & 13     & 4   \\
Minimum records per subject    & 3      & 1   \\
Average records per subject    & 4.9    & 1.1  \\
\hline
\end{tabular}}
\caption{Comparison of different benchmarks.}
\label{table: Comparison of different benchmarks}
\end{table}

% The statistics of MAPKE are shown in Table~\ref{table:MAPKE}, and the detailed construction process is shown in the appendix

% The MRCF dataset has been meticulously crafted to enhance the evaluation and advancement of sophisticated model editing techniques capable of managing multi-dimensional knowledge pertaining to a single entity. For an in-depth discussion on the dataset construction methodology and its formatting, please consult the appendix

\subsection{Failure of Editing Methods}

% \subsubsection{Evaluation Setup}    \label{Evaluation Setup}

\textbf{Editing Methods.} We evaluate six widely-used editing methods: ROME \cite{meng2022locating}, MEMIT \cite{meng2022memit}, PMET \cite{li2024pmet}, FT \cite{zhu2021modifying}, MEND \cite{mitchell2022fast}, and KN \cite{dai-etal-2022-knowledge}. 

\noindent\textbf{Selected LLMs.} Experiments are conducted on three LLMs with different parameter sizes: GPT-2 XL (1.5B) \cite{radford2019language}, GPT-J (6B) \cite{wang2021gpt}, and LLaMA-2 (7B) \cite{touvron2023llama}.

% For comprehensive comparative experiments, we selected three ROME-Like editing methods (ROME, MEMIT, PMET) and three other model editing methods (FT, MEND, KN). Additionally, three large language models (LLMs) with different parameter sizes—GPT-2 XL (1.5B), GPT-J (6B), and LLaMA-2 (7B)—were chosen to evaluate the performance of each editing method across different model scales. For detailed experimental settings, please refer to the appendix.

% To verify whether different editing methods can effectively edit multiple associated pieces of knowledge for the same subject, we designed the following comparison scheme:

We design two experimental schemes to assess how editing related knowledge impacts performance: \textbf{\textit{Same-Subject}}, where all edited knowledge shares the same subject, \textbf{\textit{Different-Subject}}, where each edit involves a different subject. Experimental data are selected from the $\text{S}^2\text{RKE}$ benchmark. 

Our pilot observation indicates that while knowledge correlation impacts editing effectiveness, it has little effect on overall model performance. So we focus on the \textbf{Score(S)} metric and introduce the \textbf{Score Difference (SD)} metric, defined as SD = Score(same-subject) – Score(different-subject), to quantify performance degradation when editing related knowledge for the same subject. To ensure reliability, each test was repeated 30 times with different editing instances. See Appendix \ref{detail experiment} for more details.


% \begin{enumerate}[label=\alph*), itemsep=0pt, parsep=0pt]
%     \item \textit{\textbf{High-Density}: Edit \textbf{n} pieces of knowledge, focusing on a single subject.}
%     \item \textit{\textbf{Low-Density}: Edit \textbf{n} pieces of knowledge, with each corresponding to different subject.}
% \end{enumerate}

% All the editing operations in the sequence are concentrated on the same entity.
% Only one piece of related knowledge is edited for each entity in the sequence.

% \subsubsection{Result \& Analysis}

\textbf{Result \& Analysis.} Figure~\ref{fig:Failue sequential} and Figure~\ref{fig:Failue batch} show the results of \textit{sequential-editing} and \textit{batch-editing} on three LLMs using six methods, respectively. The line in each figure represents the Score Difference (SD). The results show that locate-then-edit methods (e.g., ROME, MEMIT, PMET) suffer significant performance degradation under Same-Subject editing, as reflected by a substantial negative Score Difference (SD). In contrast, methods with generally lower editing effectiveness show minimal sensitivity to the relatedness of the edited knowledge. These findings confirm that knowledge correlation markedly impairs the editing performance of certain methods.

% It is evident that when multiple associated pieces of knowledge are edited for the same subject, the performance of ROME-like editing methods is significantly lower compared to editing knowledge for different subjects. Moreover, as the number of edits increases, the performance gap between the two scheme widens. In contrast, the other editing methods do not show similar behavior, indicating that ROME-like methods ace specific limitations when editing multiple associated pieces of knowledge for a single subject.

% Through the results, we hypothesize that when multiple pieces of knowledge are edited for the same subject using ROME-like methods, subsequent edits interfere with previous ones, leading to a decline in overall editing effectiveness. We will test this hypothesis in the following chapter.

\subsection{Analysis of Failures}

We further examine how the sequence of knowledge edits affects locate-then-edit methods by isolating the interference of sequential updates. For this purpose, we devised two experimental settings: \textbf{\textit{Homogeneous-Editing}}, where the first and last edits target the same subject, and \textbf{\textit{Heterogeneous-Editing}}, in which they target different subject. Experiments were performed using ROME, MEMIT, and PMET across three LLMs, with each configuration repeated 30 times on different instances from the $\text{S}^2\text{RKE}$ benchmark to ensure robust results. 

% To further investigate whether subsequent edits interfere with previous edits when using ROME-like editing methods to edit associated knowledge, we designed the following experimental scheme:
% \begin{enumerate}[label=\alph*), itemsep=0pt, parsep=0pt]
%     \item \textit{\textbf{High-grade}: Edit \textbf{n} pieces of knowledge, ensuring that only the subject of the last edit matches that of the first edit.}
%     \item \textit{\textbf{Low-grade}: Edit \textbf{n} pieces of knowledge and all the subject is distinct for each edit.}
% \end{enumerate}
% According to~\ref{Evaluation Setup}, we select three ROME-like editing methods for \textit{sequential-editing} or \textit{batch-editing} on three LLMs. To clearly highlight the impact of the editing phenomenon, we utilize the EA (Edit Accuracy) metrics and focus on the differences observed the scheme.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fig/fig13.pdf}
  \caption{The results of \textit{sequential-editing} on GPT-2 XL and GPT-J using mainstream locate-then-edit methods. The bars represent the \textbf{Score (S)} of two strategies, and the line represents the \textbf{Score Difference (SD)} between the two strategies.}
  \label{fig:defference edit 1}
\end{figure}

\textbf{Result \& Analysis.} Figure~\ref{fig:defference edit 1} shows the sequential-editing results on GPT-2 XL and GPT-J, while Figures~\ref{fig:defference edit 2} and~\ref{fig: Failue of defference batch} provide additional results. Under the Homogeneous-Editing setting, the initial edit's score is much lower than in the Heterogeneous-Editing condition. This clearly indicates that later edits interfere with earlier ones. We call this effect "\textit{related knowledge perturbation}," which exposes a key limitation of current locate-then-edit approaches when processing multiple sequential updates. These findings highlight the need for better strategies in managing sequential knowledge updates. The next section will analysis the causes of \textit{related knowledge perturbation}.

% The line in the figure represents the \textbf{Score Difference (SD)}.


\section{Perturbation Analysis}

% This section analyzes the causes of "related knowledge perturbation" and verifying them through experiments. 

% In this section, we focus on analyzing the causes of the "associated knowledge perturbation" observed in ROME-like methods.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fig/fig14.pdf}
  \caption{Illustration of related knowledge perturbation in same-subject editing.}
  \label{fig:illustration}
\end{figure}


\subsection{Causes of Perturbation}
% From the above findings, only mainstream locate-then-edit methods, represented by ROME and MEMIT, lead to \textit{"related knowledge perturbation".} These methods all employ causal tracing to identify that factual knowledge is primarily stored in the early MLP layers of LLMs. Based on the hypothesis that "the MLP modules in Transformer layers can be viewed as linear key-value associative memory," they solve for $Wk=v$, where $W$ represents the downsampling component $W_{\text{proj}}^{(l)}$ of MLP, and the key-value pair $(k, v)$ corresponds to a factual triplet $t=(s, r, o)$. Here, $k$ represents the subject $s$, while $v$ encodes the attributes of $s$, including $r$ and $o$. Therefore, when $t=(s, r, o)$ needs to be replaced by $t_{\ast}=(s, r, o_{\ast})$, it is only necessary to find the corresponding key vector $k_{\ast}$ and the new value vector $v_{\ast}$. By calculating the update $\Delta W$, they can effectively update the stored knowledge in the large model.

Our experiments show that only mainstream locate-then-edit methods (e.g., ROME and MEMIT) exhibit \textit{related knowledge perturbation}. These methods all employ causal tracing to identify that factual knowledge is primarily stored in the early MLP layers of LLMs. Based on the hypothesis that "the MLP modules in Transformer layers can be viewed as linear key-value associative memory," \cite{geva2020transformer} they solve for $Wk=v$, where $W$ represents the downsampling component $W_{\text{proj}}^{(l)}$ of MLP, and the key-value pair $(k, v)$ corresponds to a factual triplet $t=(s, r, o)$, as shown in Figure~\ref{fig:illustration}. Here, $k$ represents the subject $s$, while $v$ encodes the attributes of $s$, including $r$ and $o$. To update \(t\) to \(t_{\ast}=(s, r, o_{\ast})\), they compute a new key \(k_{\ast}\) and value \(v_{\ast}\) via an update \(\Delta W\).


However, $k_{\ast}$ is only derived from the input of the subject's last token in the MLP module's downsampling layer:

\begin{equation}
  \label{eq:example}
  k_{\ast} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{K}(x_i \oplus p),
\end{equation}

\noindent where $\mathcal{K}$ is the output of the first MLP layer in transformer block, $x_i$ represents the randomly sampled prefixes, and $\oplus$ denotes the string concatenation operator. 

% Therefore we speculate that \textit{"related knowledge perturbation"} in these methods is due to an over-reliance on subject information. When multiple related pieces of knowledge are edited around the same subject $s$, the key value $k_{\ast}$ remains the same, since all the knowledge share the same subject. This causes subsequent edits to interfere with earlier ones, reducing overall editing performance.

Therefore, we speculate that \textit{"related knowledge perturbation"} stems from an over-reliance on subject information. When editing multiple pieces of knowledge for the same subject $s$, the key value $k_{\ast}$ remains constant, causing later edits to interfere with earlier ones and reducing performance.


\subsection{Experiment Validation}

To verify the above speculation, we used MEMIT to edit two pieces of knowledge on GPT-J through \textit{sequential-editing} and \textit{batch-editing}, designing two experimental schemes: \textbf{\textit{Same-Subject}} and \textbf{\textit{Different-Subject}}. We then examine the relationship between the \textbf{cosine similarity} of the two keys and the \textit{Efficacy Success} of editing the first piece of knowledge. Cosine similarity was chosen because it measures how similar the two keys are in vector space, helping us understand how closely related the two knowledge pieces are.

% Take ROME for example assigns $k_{\ast}$ as an average vector derived from subject $s$ with a small set of $N$ randomly sampled prefixes:
% According to the section~\ref{background}, ROME-like editing methods determine the parameter update value $\Delta W$ of the MLP subsampler by editing the $k_{\ast}$ and $v_{\ast}$ calculated from the knowledge triplet $t^{\ast}=(s, r, o_{\ast})$. Therefore, the calculation results of $k_{\ast}$ and $v_{\ast}$ are crucial to parameter updating and directly affect the effect of editing. 
% Therefore, we used ROME to edit two pieces of knowledge on GPT-2 XL in succession and designed a set of comparison schemes:
% \begin{enumerate}[label=\alph*), itemsep=0pt, parsep=0pt]
%     \item \textit{\textbf{Same-Subject}: The subject of the two knowledge edits is the same.}
%     \item \textit{\textbf{Different-Subject}: The subject of the two edited articles is different.}
% \end{enumerate}
% We randomly selected editing examples from the MAPKE and counted the ES of the first piece of knowledge as well as the cosine similarity of the key and value matrices of the two pieces of knowledge under different editing conditions.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fig/fig6.pdf}
  \caption{The relationship between the \textbf{cosine similarity} of keys and the \textbf{Efficacy Success (ES)} of the first knowledge editing using MEMIT to edit GPT-J, under \textit{sequential-editing} and \textit{batch-editing}. }
  \label{fig:similar}
\end{figure}

\textbf{Result \& Analysis}\quad Figure \ref{fig:similar} shows the relationship between key similarity and the first knowledge editing Efficacy Success. The results indicate that when two pieces of knowledge related to the same subject are edited, the CS of the key approaches 1. Meanwhile, the ES of editing the first piece of knowledge is significantly lower compared to the case where the two edited pieces of edited knowledge are related to different subjects. This supports our hypothesis that since the key calculation only focuses on subject information, subsequent edits for the same subject interfere with earlier ones, leading to \textit{"related knowledge perturbation"}.

% The findings indicate that when two pieces of knowledge being edited pertain to the same subject, the Edit Success (ES) value is generally lower in instances of subject inconsistency, aligning with the experimental results from the previous chapter. Further analysis demonstrates that when subjects are consistent, the calculated key vectors exhibit high similarity, whereas the similarity among value vectors is significantly reduced. Consequently, it becomes evident that while editing multiple related pieces of knowledge regarding a single subject—despite differences in content—their key values remain highly similar. This phenomenon results in subsequent edits overwriting prior editing information during parameter updates. This occurs because key calculations rely exclusively on the feature vector associated with the subject; thus, when editing related knowledge within a singular subject context, computed key values tend to be very close together. As a result, subsequent editing exerts a stronger interference effect, leading to considerable perturbation of earlier edits.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.9\columnwidth]{fig/fig3.pdf}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

\section{Conclusion}
In this paper, we identify a key limitation of mainstream locate-then-edit methods, called \textit{"related knowledge perturbation"}, which occurs when editing multiple related pieces of knowledge for the same subject. Using the $\text{S}^2\text{RKE}$ benchmark, we show through experiments that over-reliance on subject information leads to interference between subsequent edits, highlighting the challenges in same-subject editing. 

\section{Limitation}

We acknowledge several limitations in our work. First, while this paper provides an initial exploration into the complex correlations between knowledge and identifies the phenomenon of related knowledge perturbation, it does not propose a comprehensive solution to address this issue. This omission leaves room for future research to develop effective mitigation strategies. 

Additionally, due to computational resource constraints, our experiments did not extend to larger language models, such as Llama2-13b. Future investigations could benefit from testing our findings on such models to further validate the effectiveness and generalizability of the observed phenomena.


\section{Acknowledgement}
We would like to express our sincere gratitude to all the reviewers for their valuable feedback, which greatly contributed to the improvement of this research. This work was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (No. XDB0680202).




\bibliography{custom}
\appendix

% 背景知识
\section{Related Works}  \label{sec:appendix}

\subsection{Knowledge Editing}
Model editing has gained significant attention for its ability to efficiently update LLMs. Existing approaches can be categorized into four types: \textbf{Fine-tuning} mainly applies layer-wise adjustments to incorporate new knowledge into LLMs \cite{zhu2021modifying}. \textbf{Meta Learning} trains hypernetworks to act as editors, predicting parameter updates to inject new knowledge \cite{de2021editing, mitchell2022fast}. \textbf{Memory-based} enhances LLMs with external memory or additional parameters, allowing new knowledge to be added without altering LLMs \cite{mitchell2022memory, huang2023transformerpatcher}.

Among all types, \textbf{Locate-then-Edit} has gained significant traction for its ability to modify specific knowledge within LLMs. Methods like KN\cite{dai-etal-2022-knowledge} and ROME\cite{meng2022locating} locate and update factual knowledge by targeting neurons or multi-layer perceptrons (MLPs) that store such information. MEMIT\cite{meng2022memit} extends ROME by distributing updates across multiple intermediate MLP sublayers, enabling large-scale knowledge editing. Additionally, PMET\cite{li2024pmet} combines information from both multi-head Self-attention (MHSA) and MLP modules during optimization, producing more accurate MLP outputs for final edits.

While model editing has shown great promise, some researches have identified issues such as model collapse\cite{yang2024butterfly, gu2024model} and knowledge conflicts\cite{li2024unveiling}. This paper focuses on how the correlation between knowledge impacts the performance of model editing, particularly in the context of multiple knowledge edits.

% \textbf{Fine-tuning.}\quad These methods apply layer-wise fine-tuning to incorporate new knowledge into LLMs. For example, FT\cite{zhu2021modifying} propose fine-tuning LLMs with a norm constraint between the edited and original model parameters to reduce the risk of catastrophic forgetting.

% \textbf{Meta Learning.}\quad This category trains a hypernetwork to act as an editor, predicting parameter updates to inject new knowledge. KE\cite{de2021editing} uses a trained hypernetwork to predict parameter modifications for each edit request. Similarly, MEND\cite{mitchell2022fast} employs hypernetworks to learn a low-rank decomposition of fine-tuning gradients, enabling efficient modifications of LLMs to accommodate new facts.

% \textbf{Memory-based.}\quad These methods enhance LLMs with external memory or additional parameters, allowing new knowledge to be added without altering the core model. SERAC\cite{mitchell2022memory} uses external memory to store edits and retrieves relevant information. T-Patcher\cite{huang2023transformerpatcher} introduces key-value pairs into MLP modules to incorporate specific knowledge without affecting unrelated information.

% \textbf{Locate-then-Edit.}\quad  This paradigm have gained significant traction for its ability to modify specific knowledge within LLMs. For instance, methods like KN\cite{dai-etal-2022-knowledge} and ROME\cite{meng2022locating} developed techniques to locate and update factual knowledge by targeting neurons or multi-layer perceptrons (MLPs) that store such information. MEMIT\cite{meng2022memit} further enhances ROME by distributing updates across multiple intermediate MLP sublayers, accommodating large-scale knowledge editing. Additionally, PMET\cite{li2024pmet} combines information from both the multi-head Self-attention (MHSA) and MLP modules during the optimization process, allowing it to produce more accurate MLP output for editing at the final body position.

% The goal of model editing is to update specific knowledge LLMs accurately and efficiently, without affecting unrelated information. Formally, given factual knowledge $t=(s,r,o)$, consisting of subject $s$, relation $r$, and object $o$, and an LLM $f_\theta$, along with target factual knowledge $t'=(s,r,o')$, where $o' \neq o$, the goal of a model editing algorithm $\xi$ is to optimize the parameter $\theta$ into $\theta'$ so that the edited model $f_{\theta'}$: $\xi(f_{\theta},t)=f_{\theta'}$ correctly produces $o'$ when provided with the prompt $p(s,r)$, as $f_{\theta'}(p(s,r))=o'$. 

% Among various model editing methods, \textit{Rome-like editing methods}, such as ROME and MEMIT, have garnered significant attention due to their outstanding editing performance. These methods first employ causal tracing to identify that factual knowledge is primarily stored in the early MLP layers of LLMs. Based on the hypothesis that "the MLP modules in Transformer layers can be viewed as linear key-value associative memory", we can get $Wk=v$, where $W$ represents the downsample component of MLP, and the key-value pair $(k, v)$ corresponds to a factual triplet $t=(s, r, o)$. Here, $k$ represents the subject $s$, while $v$ encodes the attributes of $s$, including $r$ and $o$. Therefore, when $t=(s, r, o)$ needs to be replaced by $t^*=(s, r, o_{\ast})$, we only need to find the corresponding key vector $k_{\ast}$ and the new value vector $v_{\ast}$. By calculating the update $\Delta W$, we can effectively update the stored knowledge in the large model.

% Take ROME for example assigns $k_{\ast}$ as an average vector derived from subject $s$ with a small set of $N$ randomly sampled prefixes:

% \begin{equation}
%   \label{eq:example}
%   k_{\ast} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{K}(x_i \oplus s)
% \end{equation}

% \noindent where $\mathcal{K}$ is the output of the first MLP layer in transformer block, $x_i$ is the prefixes, and $\oplus$ is string concatenation operator. 

% Then $v_{\ast}$ is calculated from $(s,r,o_{\ast})$ and the specially loss function. Given $(k_{\ast}, v_{\ast})$, ROME finds optimal $\widehat{W}$ to solve the following problem:

% \begin{equation}
%   \label{eq:example}
%   \text{minimize} \ \|\widehat{W}K - V\| \quad \text{s.t.} \quad \widehat{W}k_{\ast} = v_{\ast}
% \end{equation}

% It has the following closed-form solution:

% \begin{equation}
%   \label{eq:example}
%   \widehat{W} = W + \frac{(v_{*} - W k_{*}) (C^{-1} k_{*})^{\top}}{(C^{-1} k_{*})^{\top} k_{*}}
% \end{equation}

% \noindent where $W$ denotes the weight matrix of the second layer of the MLP before editing, $\widehat{W}$ denotes the weight matrix after editing, and $C=KK^{\top}$ is a precached constant.

% For other Rome-like Editing Methods, MEMIT further improves ROME by evenly distributing updates across multiple intermediate MLP sublylayers to accommodate the needs of large-scale knowledge editing. The PMET combines information from both the multi-head Self-attention (MHSA) and MLP modules during the optimization process, allowing it to generate a more accurate MLP output for editing at the final body position.

\subsection{Sequantial-editing vs. Batch-editing} \label{sec:comparison}

\textit{Sequential-editing} and \textit{batch-editing} are two strategies commonly used to update large amounts of knowledge in LLMs\cite{yao-etal-2023-editing}. Specifically, \textit{sequential-editing} refers to making multiple edits one after another, where the model should ideally retain previous changes as new edits are introduced. In contrast, \textit{batch-editing} involves editing multiple pieces of knowledge in a model at once. Notably, these two strategies can be combined to create a more flexible knowledge editing approach.

For the purposes of this study, we evaluate these strategies independently: In \textit{sequential-editing}, the batch size is set to 1, and in \textit{batch-editing}, the number of consecutive edits is set to 1, ensuring clear comparisons and facilitate experimental evaluation.

% \begin{table}[h]\small
%   \centering
%   \renewcommand\arraystretch{1.5}
%   \begin{tabular}{cccccc}
%     \hline
%      Relation Number & > 0     & > 1  & > 2 & > 3 & > 4 \\
%     \hline
%      Subject Number  & 20391  & 785 & 47 & 9  & 0  \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%   }
% \end{table}

% 数据集的构建
\section{Details of $\text{S}^2\text{RKE}$ Benchmark}  \label{technical details}

\subsection{Data Construction}

In this paper, \textbf{$\text{S}^2\text{RKE}$} (\textbf{S}ame-subject \textbf{R}elated \textbf{K}nowledge \textbf{E}diting) benchmark is built on the YAGO3.0.3, which combines Wikipedia, WordNet, GeoNames and other data sources, and was released in 2022. The construction process is detailed below, covering four key aspects:

\textbf{Triple filtering.}\quad Based on YAGO's top-level classification, we categorize the entities to be edited into six groups: Person, Building, Organization, Abstraction, Artifact and GeoEntity. From these categories, we screen out 43 relationships. Unlike COUNTERFACT, $\text{S}^2\text{RKE}$ innovatively includes both literal- and data-type relationships, enabling broader coverage of relationship types. Finally, We then select entities with the most relationship instances from each category and generated correct triplets $(s, r, o)$.

\textbf{Requested rewrite.}\quad To evaluate model efficacy, we select the relation $r$ from the triplet $(s, r, o)$ and generate a counterfactual triplet $(s, r, o_{\ast})$. We create natural language templates $P(r)$ for each relation $r$, using ChatGPT-4o to generate templates based on examples from the PARAREL \cite{elazar2021measuring} dataset. After generating multiple templates, we manually select the three most suitable ones to ensure test diversity and template consistency.

\textbf{Paraphrase prompts.}\quad To evaluate the generalization of model editing methods, we use the moonshot-v1 for generating longer text, combined with the description of the edited entity and a simplified prompt template for each relation. This process produce semantically equivalent but more complex sentences $P^P$, designed to test the model’s ability to handle diverse expressions.

\textbf{Neighborhood prompts.}\quad In order to evaluate the specificity of the model editing methods, we identify related triples $(s_{\ast}, r_{\ast}, o)$ for the object $o$ of the original triplet $(s, r, o)$, using the YAGO database. These neighborhood triplets are converted into natural language $P^N$ using simple templates$P(r_{\ast})$, specifically constructed for each relation $r_{\ast}$.

\begin{table}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{lcccc}
\hline
\textbf{Categories}  & \textbf{Subjects} & \textbf{Relations} & \textbf{Edits(all)} & \textbf{Edits(Avg)} \\
\hline
Person      & 592   & 29 & 5706  & 9.6 \\
Organization& 874   & 7  & 2897   & 3.3 \\
Building    & 679   & 6  & 3419  & 4.6 \\
Artifact    & 857   & 6  & 3632  & 4.2 \\
Abstraction & 734   & 8  & 2203  & 3.0 \\
GeoEntity   & 912   & 12 & 4207  & 5.0 \\
\hline
All         & 4503  & 43 & 22064 & 4.9 \\
\hline
\end{tabular}}
\caption{Data statistics of the $\text{S}^2\text{RKE}$ benchmark.}
\label{table:MAPKE}
\end{table}


\begin{table*}[t]
\centering
\small
\scalebox{0.8}{ 
\begin{tabular}{llll}
\hline
\textbf{ID} & \textbf{Relation} & \textbf{Domain} & \textbf{Range} \\
\hline
1  & \texttt{<hasPages>}           & rdfs:domain owl:Thing                               & rdfs:range xsd:nonNegativeInteger \\
2  & \texttt{<isCitizenOf>}        & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_country\_108544813>} \\
3  & \texttt{<diedOnDate>}         & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range xsd:date \\
4  & \texttt{<hasGender>}          & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_sex\_105006698>} \\
5  & \texttt{<wasBornOnDate>}      & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range xsd:date \\
6  & \texttt{<hasDuration>}        & rdfs:domain owl:Thing                               & rdfs:range \texttt{<s>} \\
7  & \texttt{<hasWeight>}          & rdfs:domain \texttt{<wordnet\_physical\_entity\_100001930>} & rdfs:range \texttt{<kg>} \\
8  & \texttt{<hasHeight>}          & rdfs:domain \texttt{<wordnet\_physical\_entity\_100001930>} & rdfs:range \texttt{<m>} \\
9  & \texttt{<hasLength>}          & rdfs:domain \texttt{<yagoGeoEntity>}                & rdfs:range \texttt{<km>} \\
10 & \texttt{<hasWonPrize>}        & rdfs:domain \texttt{<yagoLegalActorGeo>}            & rdfs:range \texttt{<wordnet\_award\_106696483>} \\
11 & \texttt{<owns>}               & rdfs:domain \texttt{<yagoLegalActorGeo>}            & rdfs:range owl:Thing \\
12 & \texttt{<created>}            & rdfs:domain \texttt{<yagoLegalActor>}               & rdfs:range owl:Thing \\
13 & \texttt{<participatedIn>}     & rdfs:domain \texttt{<yagoLegalActorGeo>}            & rdfs:range owl:Thing \\
14 & \texttt{<isAffiliatedTo>}     & rdfs:domain \texttt{<yagoLegalActor>}               & rdfs:range \texttt{<wordnet\_organization\_108008335>} \\
15 & \texttt{<hasAcademicAdvisor>}  & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_person\_100007846>} \\
16 & \texttt{<graduatedFrom>}      & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_university\_108286569>} \\
17 & \texttt{<hasChild>}           & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_person\_100007846>} \\
18 & \texttt{<edited>}             & rdfs:domain \texttt{<wordnet\_editor\_110044879>}   & rdfs:range owl:Thing \\
19 & \texttt{<directed>}           & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_movie\_106613686>} \\
20 & \texttt{<wroteMusicFor>}      & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_movie\_106613686>} \\
21 & \texttt{<playsFor>}           & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_organization\_108008335>} \\
22 & \texttt{<isPoliticianOf>}     & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_organization\_108008335>} \\
23 & \texttt{<isLeaderOf>}         & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_organization\_108008335>} \\
24 & \texttt{<influences>}         & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_person\_100007846>} \\
25 & \texttt{<isMarriedTo>}        & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_person\_100007846>} \\
26 & \texttt{<worksAt>}            & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_organization\_108008335>} \\
27 & \texttt{<isInterestedIn>}     & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range owl:Thing \\
28 & \texttt{<livesIn>}            & rdfs:domain \texttt{<yagoLegalActorGeo>}            & rdfs:range \texttt{<wordnet\_location\_100021767>} \\
29 & \texttt{<isKnownFor>}         & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range owl:Thing \\
30 & \texttt{<actedIn>}            & rdfs:domain \texttt{<wordnet\_location\_100021767>}  & rdfs:range \texttt{<wordnet\_movie\_106613686>} \\
31 & \texttt{<hasArea>}            & rdfs:domain \texttt{<wordnet\_location\_100021767>}  & rdfs:range xsd:km2 \\
32 & \texttt{<hasCurrency>}        & rdfs:domain \texttt{<wordnet\_location\_100021767>}  & rdfs:range \texttt{<wordnet\_currency\_108524613>} \\
33 & \texttt{<dealsWith>}          & rdfs:domain \texttt{<wordnet\_person\_100007846>}   & rdfs:range \texttt{<wordnet\_country\_108544813>} \\
34 & \texttt{<hasOfficialLanguage>}      & rdfs:domain \texttt{<wordnet\_location\_100021767>}   & rdfs:range \texttt{<wordnet\_language\_106282651>} \\
35 & \texttt{<hasCapital>}         & rdfs:domain \texttt{<wordnet\_location\_100027167>}     & rdfs:range \texttt{<wordnet\_city\_108524735>} \\
36 & \texttt{<wasCreatedOnDate>}    & rdfs:domain owl:Thing     & rdfs:range xsd:date \\
37 & \texttt{<isLocatedIn>}         & rdfs:domain \texttt{<yagoPermanentlyLocatedEntity>}   & rdfs:range \texttt{<yagoGeoEntity>} \\
38 & \texttt{<hasLongitude>}        & rdfs:domain \texttt{<yagoGeoEntity>}   & rdfs:range \texttt{<degrees>} \\
39 & \texttt{<happenedOnDate>}      & rdfs:domain \texttt{<wordnet\_event\_100029378>}   & rdfs:range xsd:date \\
40 & \texttt{<happenedIn>}         & rdfs:domain \texttt{<wordnet\_event\_100029378>}  & rdfs:range \texttt{<yagoGeoEntity>} \\
41 & \texttt{<hasLatitude>}         & rdfs:domain \texttt{<yagoGeoEntity>}  & rdfs:range \texttt{<degrees>} \\
42 & \texttt{<wasBornIn>}         & rdfs:domain \texttt{<wordnet\_person\_100007846>}  & rdfs:range \texttt{<yagoGeoEntity>} \\
43 & \texttt{<diedIn>}         & rdfs:domain \texttt{<wordnet\_person\_100007846>}  & rdfs:range \texttt{<yagoGeoEntity>} \\
\hline
\end{tabular}
}
\caption{Summary of domain and range properties for selected relations in $\text{S}^2\text{RKE}$.}
\end{table*}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\columnwidth]{fig/fig9.pdf}
  \caption{The results of \textit{sequential-editing} on LLaMA-2 7B using mainstream locate-then-edit methods. The bars represent the \textbf{Score (S)} of two strategies, and the line represents the \textbf{Score Difference (SD)} between the two strategies.}
  \label{fig:defference edit 2}
\end{figure}


% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=2\columnwidth]{fig/fig12.pdf}
%   \caption{The results of \textit{batch-editing} on GPT-J using MEMIT, comparing five evaluation metrics of three different schemes. }
%   \label{fig: pilot batch}
% \end{figure*}
% \vspace{-\baselineskip} % 添加负间距
% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=2\columnwidth]{fig/fig7.pdf}
%   \caption{The results of \textit{batch-editing} on three LLMs by six editing methods. \textbf{Score Difference (SD)} represents the difference in editing performance between the two experimental schemes when editing the same amount of knowledge under the same method.}
%   \label{fig:Failue batch}
% \end{figure*}
% \vspace{-\baselineskip} % 添加负间距
% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=2\columnwidth]{fig/fig10.pdf}
%   \caption{The results of \textit{batch-editing} on three LLMs using mainstream locate-then-edit methods. The bars represent the \textbf{Score (S)} of two strategies, and the line represents the \textbf{Score Difference (SD)} between the two strategies.}
%   \label{fig: Failue of defference batch}
% \end{figure*}

\begin{figure*}[t]
  \centering
  \begin{subfigure}{2\columnwidth}
    \includegraphics[width=\textwidth]{fig/fig12.pdf}
    \caption{The results of \textit{batch-editing} on GPT-J using MEMIT, comparing five evaluation metrics of three different schemes. }
    \label{fig: pilot batch}
  \end{subfigure}
  
  \vspace{2em} % 可以调整子图之间的间距

  \begin{subfigure}{2\columnwidth}
    \includegraphics[width=\textwidth]{fig/fig7.pdf}
    \caption{The results of \textit{batch-editing} on three LLMs by six editing methods. \textbf{Score Difference (SD)} represents the difference in editing performance between the two experimental schemes when editing the same amount of knowledge under the same method.}
    \label{fig:Failue batch}
  \end{subfigure}
  
  \vspace{2em} % 可以调整子图之间的间距

  \begin{subfigure}{2\columnwidth}
    \includegraphics[width=\textwidth]{fig/fig10.pdf}
    \caption{The results of \textit{batch-editing} on three LLMs using mainstream locate-then-edit methods. The bars represent the \textbf{Score (S)} of two strategies, and the line represents the \textbf{Score Difference (SD)} between the two strategies.}
    \label{fig: Failue of defference batch}
  \end{subfigure}
\end{figure*}

\subsection{Data Summary}
% % MQUAKE写作较规范，可以参考
% This section describes data specifications, statistics, and data formats in detail.

\textbf{Data standardization.}\quad Firstly, we standardize the description of each edited to ensure clear distinctions between them. Additionally, we handle relations involving literal- and date-type appropriately, with literal-type storing integers and date-type limited to years. Special characters in object values are also replaced or removed to ensure consistency and operability of the data format.

\textbf{Data statistics.}\quad The $\text{S}^2\text{RKE}$ benchmark contains 6 categories of edited entity, with a total of 3704 subjects and 43 specific relationships, spread across 3 categories of relationship. On average, each entity contains 4.9 edited knowledge entries, with Person entities having the highest number of edits. See Table \ref{table:MAPKE} for statistics of $\text{S}^2\text{RKE}$.

\textbf{Data format.}\quad In summary, each record in the $\text{S}^2\text{RKE}$ benchmark $D$ consists of a subject $s$ and its multiple related requested rewrite. ${r,o, o_{\ast}, P(r)}$. For each rewrite, the benchmark also includes one paraphrase prompt$P^P$ and two neighborhood prompts $P^N$. See Figure for a sample record in SMRKE, complete with three related edits for the same subject.

\begin{figure*}[t]
  \centering
  \includegraphics[width=2.2\columnwidth]{fig/fig_case.png}
  \caption{Case example in $\text{S}^2\text{RKE}$.}
  \label{fig: Failue of ROME-like batch}
\end{figure*}



\section{Detailed Experimental Setup} \label{detail experiment}

\subsection{Editing Methods}

In this paper, we use six editing methods: 

\textbf{FT} \cite{zhu2021modifying} applies an $\ell_{\infty}$ norm constraint on the fine-tuning loss, limiting the difference between the original and edited model’s parameters to reduce side effects.

\textbf{MEND} \cite{mitchell2022fast} uses a collection of small hypernetworks to learn a rank-one decomposition of the gradient obtained by standard fine-tuning, enabling tractable edits in LLMs.

\textbf{KN} \cite{dai-etal-2022-knowledge} select neurons associated with knowledge expression via gradient-based attributions, then modify MLP layer at the rows corresponding to those neurons by adding scaled embedding vectors.

\textbf{ROME} \cite{meng2022locating} uses causal tracing to localize the knowledge storage at a specific MLP layer in a transformer, and then updates knowledge by altering the weight matrix with rank-one update.

\textbf{MEMIT} \cite{meng2022memit} extends ROME by distributing updates across multiple MLP layers, enabling large-scale edits.

\textbf{PMET} \cite{li2024pmet} enhances MEMIT by integrating information from both the multi-head self-attention (MHSA) and MLP modules during the optimization process.

It is worth noting that ROME and KN can only \textit{sequential-editing}. All experiments are conducted using the EasyEdit \cite{wang2023easyedit}, ensuring standardized and reproducible evaluations. 


\subsection{Selected Models}

In this paper, we select three large language models (LLMs): 

\textbf{GPT-2 XL} \cite{radford2019language}, a 1.5 billion parameter version of GPT-2,is a transformer-based language model developed by OpenAI.

\textbf{GPT-J} \cite{wang2021gpt}, developed by EleutherAI, is a GPT-3-like open-source LLM with 6 billion parameters, trained on \textit{The Pile}.

\textbf{LLaMA2-7B} \cite{touvron2023llama}, a 7 billion parameter version of LLaMA 2 from Meta AI, is a leading open-source LLM, known for its advanced training techniques and optimizations.



\subsection{Evaluation Metrics} \label{Evolution metrics}

To comprehensively evaluate the experimental results, we evaluate editing methods across four dimensions:

\textbf{Efficacy.} We measure efficacy using the Efficacy Success (\textbf{ES}) metric. Specifically, when triple $(s, r, o)$ is updated to $(s, r, o_{\ast})$, ES calculates the success rate of the target edit by determining the probability that the condition $P[o{\ast}] > P[o]$ is satisfied.

\textbf{Generalization.} To evaluate generalization, we use Paraphrase Success (\textbf{PS}) metric, which measures the probability that $P[o_{\ast}] > P[o]$ when the model is prompted with a paraphrase of the original $(s, r)$. 

\textbf{Specificity.}  For specificity, we adopt the Neighborhood Success (\textbf{NS}) metric, which tests the probability that $P[o_c] > P[o_{\ast}]$ for triplet $(s, r, o_c)$, where $o_c$ lies outside the range of the factual edits.

\textbf{Overall Performance.} We assess overall model performance using Perplexity (\textbf{PPL}), based on prior studies by \citet{yang2024butterfly, yang2024fall}. An increase in perplexity generally indicates a decrease in the model's performance in generation tasks.

Finally, to evaluate the balance between efficacy, generalization, and specificity, we report the harmonic mean of \textbf{ES}, \textbf{PS}, and \textbf{NS} indicators as a comprehensive score (\textbf{S}), providing a holistic view of the model’s behavior across these dimensions.

\end{document}




% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.


