\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{pifont}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{tabu}                     
\usepackage{multirow}                 
\usepackage{multicol}                              
\usepackage{float}                    
\usepackage{makecell}                 
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{forest}
\usetikzlibrary{trees, positioning}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{graphicx}
\graphicspath{ {image/} }
\usepackage{subcaption}
% \usepackage{caption} 
\usepackage{adjustbox}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{float}
\usepackage{stfloats}
\usepackage{titlesec}
% \usepackage{natbib}
% \setcitestyle{maxnames=3}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021


% \usepackage[margin=0.5in, landscape]{geometry}
\usetikzlibrary{shapes.geometric, arrows, positioning}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=yellow!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\begin{document}

\title{When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks for VLMs  }

% \author{Aobotao Dai, Xinyu Ma, Lei Chen, Songze Li*, and Lin Wang*
\author{Aobotao Dai, Xinyu Ma, Lei Chen,~\IEEEmembership{Fellow,~IEEE}, Songze Li*, and Lin Wang*,~\IEEEmembership{Member,~IEEE}
        % <-this % stops a space
\thanks{* Corresponding Author}
\thanks{A. Dai and L. Chen are with the Artificial Intelligence Thrust, Hong Kong University of Science and Technology (Guangzhou), China. Email: adai590@connect.hkust-gz.edu.cn and leichen@hkust-gz.edu.cn.}
\thanks{X. Ma and S. Li are with the School of Cyber Science and Engineering, Southeast University, China. Email: mxy2tian@outlook.com and songzeli@seu.edu.cn.}
\thanks{L. Wang is with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore. Email: linwang@ntu.edu.sg.}}
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Vision-Language Models (VLMs) have gained considerable prominence in recent years due to their remarkable capability to effectively integrate and process both textual and visual information. This integration has significantly enhanced performance across a diverse spectrum of applications, such as scene perception and robotics. However, the deployment of VLMs has also given rise to critical safety and security concerns, necessitating extensive research to assess the potential vulnerabilities these VLM systems may harbor. In this work, we present an in-depth survey of the attack strategies tailored for VLMs. We categorize these attacks based on their underlying objectives — namely jailbreak, camouflage, and exploitation — while also detailing the various methodologies employed for data manipulation of VLMs. Meanwhile, we outline corresponding defense mechanisms that have been proposed to mitigate these vulnerabilities. By discerning key connections and distinctions among the diverse types of attacks, we propose a compelling taxonomy for VLM attacks. Moreover, we summarize the evaluation metrics that comprehensively describe the characteristics and impact of different attacks on VLMs. Finally, we conclude with a discussion of promising future research directions that could further enhance the robustness and safety of VLMs, emphasizing the importance of ongoing exploration in this critical area of study. To facilitate community engagement, we maintain an up-to-date project page, accessible at: \url{https://github.com/AobtDai/VLM_Attack_Paper_List}.
\end{abstract}

\begin{IEEEkeywords}
Vision-Language Models, Adversarial Attack, Jailbreak Attack, Survey and Outlooks.
\end{IEEEkeywords}

\section{Introduction}
\label{sec1}
\IEEEPARstart{V}{ision-Language} Models (VLMs) have achieved significant success \cite{jin2024efficientmultimodallargelanguage, zhang2024visionpami, radford2021learning} in notably improving the accuracy and efficiency in various applications, such as navigation within autonomous driving systems \cite{zhou2024vision} and robotic applications \cite{zhang2024vision}, due to their enhanced ability to understand scenes through both textual and visual modalities. The rapid advancements in computational resources and data collection strategies have simplified the designing and training of VLMs. Consequently, several pre-trained VLMs, such as CLIP \cite{radford2021learning} and LLaVA \cite{liu2024visualnips}, have been available, which can be further adapted to other modalities, such as audio \cite{gong2024listenthinkunderstand,xie2024sonicvisionlm}, or to achieve other downstream tasks, such as segmentation \cite{li2022grounded, kirillov2023segment}. 
% Generally speaking, encoders within VLMs transform visual information or textual information into an embedding space. Then a projector aligns both of them, and a text decoder translates the aligned output back into image or text, thereby enabling a seamless combination of visual and textual information\cite{radford2021learning, li2023blip2, dai2023instructblipgeneralpurposevisionlanguagemodels, zhu2023minigpt4enhancingvisionlanguageunderstanding, liu2024visualnips}, as shown in Fig.\ref{VLM-Structure}. 

% \IEEEpubidadjcol

\begin{figure}[!t]
\label{teaser}
\centering
\includegraphics[width=3.5in]{image/teaser.pdf}
\caption{\textbf{Illustration of attacks on VLMs, where tailored data manipulation strategies for different attack goals are employed for VLMs}, inducing various kinds of malicious outputs. For each of them, the representative methods and outcomes from \cite{zhao2024evaluatingnips} (Visual Perturbation), \cite{yang2024mmacvpr} (Gradient-Driven Prompts), \cite{wu2024jailbreakinggpt4vselfadversarialattacks} (Human-Like Deceptive Prompts), \cite{ma2024visualroleplayuniversaljailbreakattack} (Typography), \cite{yang2024mmacvpr} (Jailbreak Attack), \cite{ni2024physicalbackdoorattackjeopardize} (Camouflage Attack) and \cite{gao2024inducinghighenergylatencylarge} (Exploitation Attack), are highlighted. }
\label{teaser}
% \vspace{-10pt}
\end{figure}

% Although VLMs have been successfully  globally, 


However, concerns regarding their safety have increasingly garnered attention from both the research community and the general public. Google Scholar reports a substantial volume of VLM attacks, with approximately 2,670 papers published in 2024 alone, as shown in Fig. \ref{papernum}. Compared with unimodal models, VLMs use both textual and visual modalities to achieve task goals and improve efficiency. This integration of multimodal inputs enhances performance but simultaneously introduces additional layers of complexity and uncertainty. In particular, attacks on VLMs show two distinct properties. \textbf{Firstly}, attackers can exploit the capacity of VLMs to comprehend and process both visual and textual information, allowing them to transfer and conceal malicious intent between modalities with greater flexibility. This adaptability renders their attacks more stealthy and challenging to detect. \textbf{Secondly}, VLMs are inherently more efficient at acquiring and processing vast amounts of diverse types of information compared to unimodal models. While this extensive data acquisition enhances functionality, it can also lead to increased uncertainty and complicate efforts to effectively filter out malicious information. 

\begin{figure}[!t]
\label{papernum}
\centering
\includegraphics[width=3.5in]{image/papernum.pdf}
  \vspace{-20pt}
\caption{\textbf{Google Scholar search results for VLM attacks}, with the vertical axis representing the number of publications and the horizontal axis indicating the corresponding years.}
\label{papernum}
\vspace{-10pt}
\end{figure}

To investigate these challenges and assess the robustness of VLMs, many researchers have proposed various attack strategies to reveal the weaknesses and vulnerabilities of different VLMs \cite{zhao2024evaluatingnips, yang2024mmacvpr, wu2024jailbreakinggpt4vselfadversarialattacks, ma2024visualroleplayuniversaljailbreakattack, ni2024physicalbackdoorattackjeopardize, gao2024inducinghighenergylatencylarge}, as shown in Fig. \ref{teaser}. In the past three years alone, research on VLM attacks has shown rich diversity and quantity, as illustrated in Fig.~\ref{papernum}. These attacks have various goals, including camouflage attacks designed to induce the model to output false information, jailbreak attacks aimed at prompting the model to produce harmful content, and exploitation attacks intended to increase the operational costs of VLMs. Intuitively, the methods for these attack goals can be categorized into: \textbf{1)} visual perturbation, \textbf{2)} gradient-driven prompts, \textbf{3)} human-like deceptive prompts, and \textbf{4)} typography. These categories will be discussed in greater detail in the following sections.

\begin{table*}[t!]
\centering
{\caption{\textbf{Comparison of our work with related surveys}. Here, \textbf{``Among VLMs in Different Sizes''} pertains to the relationships between LVLM attacks and those conducted on smaller models. \textbf{``Between VLMs and Unimodal Models''} signifies the connections between VLM attacks and earlier unimodal attack strategies. \textbf{``Mitigation Strategy''} refers to the corresponding relationship between attacks and defenses. \textbf{``Jailbreak-Camouflage Feature Distinction''} highlights the distinctions between these two types of attacks, a differentiation that has been largely overlooked in previous surveys. Lastly, other elements (e.g., Typography) refer to specific characteristics that serve to distinguish these attack methodologies.
    }
    \vspace{-6pt}
\label{table_comparison}}
% \setlength{\tabcolsep}{5.1mm}
   \begin{tabular}{rccccccccc} 
        \toprule
        % \multirow{2}{*}{\bf{Reference}} & \multicolumn{3}{c}{\bf{Architecture}} & \multirow{2}{*}{\bf{Diffusion-Based}} & \multirow{2}{*}{\bf{Black-Box}} \\
        \multirow{2}{*}{\bf{Reference}} & \multicolumn{3}{c}{\bf{Attack Relationships}} & & \multicolumn{2}{c}{\bf{Attack Goal}} & & \multicolumn{2}{c}{\bf{Data Manipulation}} \\
        % {\bf{Dataset}} & {\bf{Year}} & {\bf{Volume}} & {\bf{Scenarios}} & {\bf{Categories}} & {\bf{Benchmark}} \\
        \cmidrule{2-4}  
        \cmidrule{6-7}
        \cmidrule{9-10}
         & Among VLMs & Between VLMs and & Mitigation  & & Jailbreak-Camouflage & Exploitation & & Deceptive & Typography\\
         & in Different Sizes & Unimodal Models & Strategy & & Feature Distinction &  & &  & \\
         
        \midrule
        % {CLIP \cite{radford2021learning}} & ViT-L & Linear & -  & {\ding{55}} & \ding{55} & {\ding{55}} \\
        % {EVA-CLIP \cite{fang2023evaclip}} & EVA ViT-g & 
        % {Wei et al. \cite{wei2024physicalpami}} & {\ding{55}} & {\ding{55}} & {\ding{72}} & & {\ding{55}} & \ding{55} & & {\ding{55}} & \ding{55}  \\

        % {Chowdhury et al. \cite{chowdhury2024breakingdefensescomparativesurvey}} & {\ding{55}} & {\ding{55}} & {\ding{72}} & & {\ding{72}} & \ding{55} & & {\ding{72}} & \ding{55}  \\

        {Liu et al. \cite{liu2024safetymultimodallargelanguage}} & {\ding{55}} & {\ding{55}} & {\ding{72}} & & {\ding{55}} & \ding{55} & & {\ding{55}} & \ding{55}  \\
        
        {Fan et al. \cite{fan2024unbridledicarussurveypotential}} & {\ding{55}} & {\ding{55}} & {\ding{72}} & & {\ding{72}} & \ding{55} & & {\ding{55}} & \ding{51}  \\

        {Wang et al. \cite{wang2024llmsmllmsexploringlandscape}} & {\ding{55}} & {\ding{55}} & {\ding{72}} & & {\ding{72}} & \ding{55} & & {\ding{72}} & \ding{51}  \\
        
        {Liu et al. \cite{liu2024surveyattackslargevisionlanguage}} & {\ding{55}} & {\ding{55}} & {\ding{72}} & & {\ding{72}} & \ding{72} & & {\ding{72}} & \ding{72}  \\
        
        Our survey & {\ding{51}} & {\ding{51}} & {\ding{51}} & & {\ding{51}} & \ding{51} & & {\ding{51}} & \ding{51}  \\


        
        \bottomrule
   \end{tabular}
   % \vspace{20pt}
   \parbox{\linewidth}{\vspace{5pt}
   \centering
        % \footnotesize
        \normalsize
        \ding{51}: Strongly Relevant, \ding{72}: Limitedly Relevant, \ding{55}: Irrelevant
    }
    \parbox{\linewidth}
   %  {\vspace{5pt}
   % % \centering
   %      % \footnotesize
   %      \normalsize
        
    \vspace{-10pt}
\end{table*}

Among VLMs, Large Vision-Language Models (LVLMs) further expand this diversity of VLM attacks. With the development of Large Language Models (LLMs), researchers have devised methodologies to integrate VLMs with LLMs to obtain a more expansive knowledge database. These integrated models, known as LVLMs \cite{li2023blip2, dai2023instructblipgeneralpurposevisionlanguagemodels, zhu2023minigpt4enhancingvisionlanguageunderstanding, liu2024visualnips}, typically substitute the conventional text decoder with advanced LLMs, such as Vicuna \cite{vicuna2023} and LLaMA \cite{touvron2023llama}. The expansive knowledge base inherent in LLMs endows LVLMs with enhanced robustness across a variety of scenarios. The built-in defense mechanisms within LLMs also empower LVLMs to mitigate the generation of harmful content \cite{vicuna2023, touvron2023llama2}, thereby ensuring safer and more reliable interactions with users. 

\textit{Despite the integration of LLMs to enhance the capabilities of LVLMs in comparison to VLMs that do not incorporate LLMs, LVLMs remain insufficiently robust and even introduce unique vulnerabilities.}  The expansion of the knowledge base comes with an increase in the uncertainty of harmful information appearing in data sources, which subsequently increases the amount of harmful information available for the model to learn. In addition, a larger knowledge base enhances the model's creativity, increasing the likelihood of generating harmful content in varied forms and contexts, making it difficult for defense mechanisms to take effect. Attacks known as jailbreak attacks are deliberately designed to utilize these vulnerabilities within LVLMs, which are not present in other VLMs \cite{gu2024agentsmithsingleimage, wu2024jailbreakinggpt4vselfadversarialattacks, luo2024jailbreakv28kbenchmarkassessingrobustness}. The exceptional capabilities of LLMs also enable LVLMs to engage with users in a manner that closely resembles human interaction, surpassing the performance of other VLMs. This human-like interaction facilitates the provision of detailed insights into the tasks requested by users, rendering the model user-friendly even for those lacking professional expertise. However, this accessibility also means that users can inadvertently induce the model to generate harmful content through simple conversational prompts. 

% Regarding their architecture, LVLMs possess modules of LLMs and other pre-trained components with a substantial number of parameters that are relatively resilient to attacks. Consequently, the retrained projector module within the LVLM, which is specifically designed to bridge the encoders and the LLM while facilitating a deeper comprehension of the embedding space where both visual and textual inputs are encoded, becomes a critical vulnerability for potential attacks. Such attacks ingeniously manipulate the model's embedding space, inserting malicious information to mislead the model's output \cite{carlini2024aligned, luo2024imageworth1000lies, aich2022gama}. 

% For instance, the projector module within LVLM has become a focal point for attacks.  Additionally, recent studies have demonstrated the feasibility of compromising LLMs within LVLM frameworks through sophisticated prompt engineering and adversarial attacks. 
% Furthermore, LVLMs introduce additional vulnerabilities beyond the inherent safety concerns associated with VLMs. The exploitation of web data by LLMs raises the specter of inadvertently harvesting harmful content lurking in the dark recesses of the Internet. Consequently, users could potentially access this undesirable content more readily via LLMs and LVLMs than through traditional search engines. 

% \textbf{Our Work.} 
In this work, we focus on VLMs, specifically investigating the intricate interplay between visual and textual modalities. Our analysis includes attacks on CLIP models, VLMs integrating LLMs, and diffusion-based models. We primarily emphasize the significant advancements that have been made over the past three years, particularly highlighting works published in top-tier conferences and journals, such as CVPR, NeurIPS and ICLR. Additionally, we provide a concise overview of relevant adversarial attacks directed at earlier vision-only models and language-only models, elucidating the connections and distinctions between attacks on unimodal models and those targeting VLMs. We concentrate on three core research questions:
% In addition to providing technical insights into these VLM attacks, we present comprehensive taxonomies, datasets, evaluation metrics, and potential challenges and research directions within this field. 
% In summary, we focusing on three research questions:
\begin{itemize}
\item{\textbf{RQ1:} What are the unique characteristics of VLM attacks compared to attacks on unimodal models, and how do these differences impact their classification?}

\item{\textbf{RQ2:} What are the internal relationships and distinctive features among different types of VLM attacks?}

\item{\textbf{RQ3:} How can VLM attacks be systematically categorized based on their goals and the data manipulation strategies employed during attacks, and what are the corresponding defense mechanisms and mitigation strategies?}
\end{itemize}

% Despite the wealth of research on VLM attacks, the research community lacks a comprehensive review that clarifies the unique characteristics of VLM attacks in contrast to attacks on unimodal models, as well as the internal relationships and distinctive features among different types of VLM attacks. To address these gaps, we conduct this survey and propose a self-consistent and more in-depth taxonomy to categorize attacks based on two aspects: the attack goal and the data manipulation strategy employed. For the attack goals, we propose three distinct categories -- jailbreak, camouflage and exploitation attacks -- and outline their differences. Additionally, we summarize four categories of data manipulation strategies specifically tailored for VLMs -- visual perturbation, gradient-driven prompts, human-like deceptive prompts and typography. We believe that this survey will elucidate the various relationships associated with VLM attacks and foster further development in related research.



Although several existing surveys have provided summaries of various VLM attack methods and their related aspects \cite{liu2024safetymultimodallargelanguage, fan2024unbridledicarussurveypotential, 
wang2024llmsmllmsexploringlandscape, liu2024surveyattackslargevisionlanguage}, they predominantly concentrate on the listing and categorization of these methods, often neglecting a thorough evaluation of the connections among different kinds of attacks. A comparative analysis of our survey alongside these previous works is summarized in Table \ref{table_comparison}.  Specifically, although Liu et al. \cite{liu2024safetymultimodallargelanguage} provide a summary of attacks on LVLMs, this survey lacks a detailed taxonomy of VLMs. Similarly, Fan et al. \cite{fan2024unbridledicarussurveypotential} enhance their taxonomy to achieve a greater level of completeness; however, their focus still remains narrowly confined to LVLMs. Wang et al. \cite{wang2024llmsmllmsexploringlandscape} center exclusively on jailbreak attacks against large models, failing to provide a holistic perspective on the various attacks on VLMs. Liu et al. \cite{liu2024surveyattackslargevisionlanguage} are limited in their exploration of LVLMs and present a taxonomy that is inherently contradictory. 

% Given the critical significance of robustness in VLMs, there are several surveys discussing VLM attack\cite{fan2024unbridledicarussurveypotential, liu2024surveyattackslargevisionlanguage, wang2024llmsmllmsexploringlandscape}. However, these existing works have notable limitations. \cite{fan2024unbridledicarussurveypotential} focuses exclusively on unimodal attacks, specifically those targeting the visual modality, lacking analysis and discussion on textual attacks and multimodal attacks. \cite{wang2024llmsmllmsexploringlandscape} delves into the similarities and differences between jailbreak attacks on LLMs and Multimodal Large Language Models (MLLMs), yet it overlooks other forms of attacks like camouflage and exploitation attacks, which we will further discuss in \hyperref[sec4]{Sec. IV} of our survey. \cite{liu2024surveyattackslargevisionlanguage} presents a survey on LVLMs and proposes a taxonomy of LVLM attacks. However, their proposed taxonomy lacks completeness and accuracy. In response to these gaps, our survey aims to provide an exhaustive overview of safety concerns in VLMs and introduce a detailed, accurate taxonomy of safety measures within this domain. We believe our comprehensive survey will significantly benefit the research community by summarizing existing work on VLM safety and catalyzing further advancements to enhance the robustness of VLMs. 

% While earlier studies have acknowledged these three types of attacks, they have primarily focused on the intricate technicalities of jailbreak attacks, often neglecting to explore the connections and distinctions among all three categories, particularly the exploitation attack. This oversight is significant given that exploitation attacks possess a systematic attack construction strategy. Furthermore, the techniques employed in camouflage attacks differ from those used in jailbreak attacks, with the outputs of jailbreak attacks exhibiting greater flexibility than those generated from camouflage attacks. Previous literature has not sufficiently clarified these nuances. Additionally, various subcategories exist within jailbreak attacks, each characterized by different degrees of harm and distinct attack strategies.

In summary, our work aims to bring the community the following key contributions:
\begin{itemize}
\item{We explore the unique characteristics of VLM attacks in comparison to unimodal attacks, providing a detailed overview of their distinct methodologies and classifications. For instance, attacks using typography are not seen in unimodal models. (\textbf{RQ1}). }
\item{We analyze the internal relationships among different types of VLM attacks, offering insights into their distinctive features and implications for future research. For instance, jailbreak attacks are commonly seen in attacks on LVLMs while not in attacks on other VLMs (\textbf{RQ2}). }
\item{We distinguish between jailbreak attacks and camouflage attacks, a differentiation that has been largely overlooked in prior research (\textbf{RQ3}).}
\item{We introduce two new sub-categories (camouflage attack and exploitation attack) to enrich the understanding of VLM attacks, facilitating a systematic taxonomy of various attack methodologies (\textbf{RQ3}).} 
\item{We analyze the relationships between existing VLM attack strategies and defense mechanisms, discussing further trends and challenges in attacks and defenses (\textbf{RQ3}).}
\end{itemize}


% {\bf{Scope}}. 

% {\bf{Organization}}. 
In the following sections, we establish the foundational background on VLM attacks, covering aspects such as problem formulation, victim VLMs, and defense mechanisms in VLMs, as presented in \hyperref[sec2]{Sec. II}. Next, we introduce an overview of our taxonomy on VLM attacks and the motivation behind developing this taxonomy in \hyperref[sec3]{Sec. III}. And \hyperref[sec3]{Sec. III} further elaborates on various attacks organized according to our proposed taxonomy. Evaluation strategies are reviewed in \hyperref[sec4]{Sec. IV}, providing insights into the assessment of VLM attacks from multiple perspectives. Finally, we discuss current challenges and promising future directions in \hyperref[sec5]{Sec. V} and conclude the survey in \hyperref[sec6]{Sec. VI}. 




\section{Preliminaries}
\label{sec2}
% In this section, we provide the necessary background related to VLM attack, including problem formulation, victim VLMs, and defense strategies in VLMs.
\subsection{Problem Formulation}

To define the notations concisely and clearly, our notations and definitions are shown in Table \ref{table_notation}. We introduce the details of relationships among these notations in the following:

The architecture of a VLM can be represented as ${\bf{M}} = {\bf{D} }\circ {\bf{P}} \circ {\bf{E}}$. In our survey, $\bf{E}$ consists of two encoders $\{\bf{E}_{img}, \bf{E}_{txt}\}$, where $\bf{E}_{img}$ is the visual encoder and $\bf{E}_{txt}$ is the textual encoder. 

Regarding the content generation process, we represent it as ${\bf{M}}(x, t) = {\bf{D}}({\bf{P}}({\bf{E}_{img}}(x), {\bf{E}_{txt}}(t)))=y$, where $x\in\mathcal{X}, t\in\mathcal{T}, y\in\mathcal{Y}$. Besides, $x_{malice} = \mathcal{P}(x_{benign}) = x_{benign} + \delta_{img}$, $t_{malice} = \mathcal{P}(t_{benign}) = t_{benign} + \delta_{txt}$.

% \subsubsection{\textbf{\textit{Targeted Attack}}}
{\textbf{\textit{Targeted Attack:}}}
It aims to manipulate the victim model to output the specific answer, which is intended by attacker. This type of attack is particularly concerning in applications where precise classifications are critical, such as in autonomous driving systems, where the consequences of misclassification can be severe.   
The attack goal can be defined as follows:
\begin{equation}
\begin{aligned}
\label{eq1_targeted}
\mathop{\arg\min}_{\delta_{img}, \delta_{txt}} \text{ } \mathcal{L} ( {\bf{M}}(x_{benign} + \delta_{img}, t_{benign} + \delta_{txt}), y_{malice} ) \\
s.t. \|\delta_{img}\|_{p}\leq\epsilon_{img}, \|\delta_{txt}\|_{p}\leq\epsilon_{txt}
\end{aligned}
\end{equation}



{\textbf{\textit{Untargeted Attack:}}}
% \subsubsection{\textbf{\textit{Untargeted Attack}}}
It seeks to induce the victim model output an incorrect answer, regardless of the specific answer. And it could be described as following:
\begin{equation}
\begin{aligned}
\label{eq1_untargeted}
\mathop{\arg\max}_{\delta_{img}, \delta_{txt}} \text{ }\mathcal{L} ( {\bf{M}}(x_{benign} + \delta_{img}, t_{benign} + \delta_{txt}), y_{benign} )\\
s.t. \|\delta_{img}\|_{p}\leq\epsilon_{img}, \|\delta_{txt}\|_{p}\leq\epsilon_{txt}
\end{aligned}
\end{equation}
\begin{table}[t!]
\centering
{\caption{Notations and definitions in this survey. }
\label{table_notation}}
   \begin{tabular}{llll} 
        \toprule
        \bf{Notation} & \bf{Definition} & \bf{Notation} & \bf{Definition} \\
        \midrule
        {$\bf{M}$} & {A VLM.} & {$\bf{E}$} & {Encoder.} \\
        % \hline
        {$\bf{P}$} & {Projector.} & {$\bf{D}$} & {Decoder.} \\
        % \hline
        $\bf{E}_{img}$ & {Visual encoder.} & $\bf{E}_{txt}$ & {Textual encoder.} \\
        % \hline
        $\mathcal{X}$ & {Set of image inputs.} & $\mathcal{T}$ & {Set of text inputs.} \\
        % \hline
        $\mathcal{Y}$ & {Set of outputs.} & $x$ & {Input image.} \\
        % \hline
        $t$ & {Input text.} & $y$ & {Output.} \\
        % \hline
        $x_{malice}$ & {Malicious image.} & $t_{malice}$ & {Malicious text.} \\
        % \hline
        $x_{benign}$ & {Benign image.} & $t_{benign}$ & {Benign text.} \\
        $y_{benign}$ & {Benign output.} & $y_{malice}$ & {Malicious output.} \\
        % \hline
        \multirow{2}{*}{$\delta_{img}$} & \multirow{2}{*}{\makecell[l]{The difference between \\ $x_{malice}$ and  $x_{benign}$.}} & \multirow{2}{*}{$\delta_{txt}$} & \multirow{2}{*}{\makecell[l]{The difference between \\ $y_{malice}$ and $y_{benign}$.}} \\
        \\
        \multirow{2}{*}{$\epsilon_{img}$} & \multirow{2}{*}{\makecell[l]{Visual perturbation \\ controller.}} & \multirow{2}{*}{$\epsilon_{txt}$} & \multirow{2}{*}{\makecell[l]{Textual perturbation \\ controller.}} \\
        \\
        % \hline
        $\mathcal{P}$ & {Manipulation function.} & {} & {} \\
        \bottomrule
   \end{tabular}
\end{table}


\begin{table*}[t!]
\centering
{\caption{\textbf{Summary of LVLMs that are commonly used as victim models}.  In this table, \textbf{``LLM''} refers to the Large Language Model utilized within a LVLM. ``-" indicates ``not available", meaning that the specific module is not publicly known, signifying that the model operates as a black-box model. }
\vspace{-6pt}
\label{table_LVLMs}}
\setlength{\tabcolsep}{4mm}
   \begin{tabular}{rcccccc} 
   % \setlength{\tabcolsep}{18pt}
        \toprule
        \multirow{2}{*}{\bf{LVLMs}} & \multicolumn{3}{c}{\bf{Architecture}} & \multirow{2}{*}{\bf{Diffusion-Based}} & \multirow{2}{*}{\bf{Black-Box}} \\
        % {\bf{Dataset}} & {\bf{Year}} & {\bf{Volume}} & {\bf{Scenarios}} & {\bf{Categories}} & {\bf{Benchmark}} \\
        \cmidrule{2-4}  
        % \cmidrule{6-7} 
         & Visual Encoder & Projector & LLM Backbone \\
        \midrule
        % {CLIP \cite{radford2021learning}} & ViT-L & Linear & -  & {\ding{55}} & \ding{55} & {\ding{55}} \\
        % {EVA-CLIP \cite{fang2023evaclip}} & EVA ViT-g & 
        {BLIP-2 \cite{li2023blip2}} & ViT-g/14 & Q-Former & OPT \cite{zhang2022optopenpretrainedtransformer}, Flan-T5 \cite{chung2024scaling} & {\ding{55}} & \ding{55} \\
        
        {InstructBLIP \cite{dai2023instructblipgeneralpurposevisionlanguagemodels}} & ViT-g/14 & Q-Former & Vicuna \cite{vicuna2023} & {\ding{55}} & \ding{55}  \\

        {OpenFlamingo \cite{openflamingo2023}} & ViT-L/14 & Cross-Attention & MPT \cite{MPT}, RedPajama \cite{RedPajama} & {\ding{55}} & \ding{55} \\

        {MiniGPT-4 \cite{zhu2023minigpt4enhancingvisionlanguageunderstanding}} & ViT-g/14 & Linear & Vicuna \cite{vicuna2023} & {\ding{55}} & \ding{55} \\

        {LLaVA \cite{liu2024visualnips}} & ViT-L/14 & Linear & Vicuna \cite{vicuna2023} & {\ding{55}} & \ding{55} \\

        {LLaVA Adapter V2 \cite{gao2023llamaadapterv2parameterefficientvisual}} & ViT-L/14 & Linear & LLaMA \cite{touvron2023llama} & {\ding{55}} & \ding{55} \\

        {Otter \cite{li2023mimicitmultimodalincontextinstruction}} & ViT-L/14 & Cross-Attention & LLaMA \cite{touvron2023llama} & {\ding{55}} & \ding{55} \\

        {GPT-4V \cite{GPT4V}} & - & - & GPT-4 \cite{gpt4} & {\ding{55}} & \ding{51} \\

        {Google Gemini \cite{gemini}} &  - & - & Bard \cite{Googlebard} & {\ding{55}} & \ding{51} \\

        % {Claude 3 \cite{Claude3}} &  - & - & - & {\ding{55}} & \ding{51} \\
        
        
        
        % \hline
        % \multirow{2}{*}{$\delta_{img}$} & \multirow{2}{*}{\makecell[l]{the difference between \\ $x_{malice}$ and  $x_{benign}$}} & \multirow{2}{*}{$\delta_{txt}$} & \multirow{2}{*}{\makecell[l]{the difference between \\ $y_{malice}$ and $y_{benign}$}} \\
        
        \bottomrule
   \end{tabular}
   % \parbox{\linewidth}{\vspace{5pt}
   % % \centering
   %      % \footnotesize
   %      \normalsize
       
   %  }
   \vspace{-10pt}
\end{table*}

\subsection{Other Concepts in Attack}
{\textbf{\textit{White-Box Attack:}}}
% \subsubsection{\textbf{\textit{White-Box Attack}}}
It grants complete visibility into the model architecture, parameters, training data, etc. This transparency enables attackers to exploit vulnerabilities with heightened precision, often leading to more effective and sophisticated manipulations. White-box attacks typically involve the generation of malicious inputs designed to deceive the model by leveraging knowledge of the model's gradients and decision boundaries.


{\textbf{\textit{Black-Box Attack:}}}
It focuses on scenarios where the attacker has no access to the internal workings of a model. Instead, the attacker interacts with the model solely through its outputs, making inferences based on the input-output behavior. This limitation becomes more pronounced when the attacker has only partial access to the model. Such constraints present unique challenges and opportunities, as the attacker must rely on queries to the model to generate malicious inputs. 

In general, there are two kinds of black-box attacks: query-based attacks and surrogate-based attacks. For query-based attacks, attackers rely on sending a limited number of inputs to the target model to observe its outputs and adjust their malicious inputs via the outputs. For surrogate-based attacks, attackers build a surrogate model that approximates the target model's behavior to find an attack. Then attackers transfer this attack method from the surrogate model to target model.

% \subsubsection{\textbf{\textit{Query-Based}}}
% Manipulated data is crafted in Query-Based Black-Box attack via directly and continuously querying the victim model. Attackers iteratively adjust their malicious input according to the output from victim model. \cite{liu2024arondightredteaminglarge} evaluates the output from victim model in several aspects: high toxicity, high text-image correlation and high diversity. It uses the evaluation to guide the malicious input generation. While \cite{zhao2024evaluatingnips} uses the transformation between visual modality and textual modality to implement fine-grained manipulation both on visual and textual data. 

% \subsubsection{\textbf{\textit{Surrogate-Based}}}
% Compared with Query-Based attacks, Surrogate-Based attacks use a surrogate model to mimic the features and behaviours of the victim model. \cite{zhao2024evaluatingnips} uses a pre-trained visual encoders(e.g. ViT-B/32) as surrogate visual encoder model to maximize the embedding similarity between the malicious and the benign. \cite{dong2023robustgooglesbardadversarial} attacks some commercial VLMs(e.g. Google’s Bard\cite{Googlebard}) to test its robustness via two attack methods: image embedding and text description. It aims to enhance the transferability of the surrogate model to victim model. It selects several surrogate models to conduct attacks

\begin{figure}[!t]
\label{VLM-Structure}
\centering
\includegraphics[width=3.5in]{image/VLM-Structure.pdf}
\vspace{-10pt}
\caption{\textbf{An overview of the general model architecture of VLMs}. 
% The defense mechanism is represented by a \textbf{shield icon}.
In certain configurations of VLMs, the input text is transmitted directly to the projector without an intermediate encoding step. In LVLMs, LLMs are employed as decoders to facilitate the processing of multimodal input.}
\label{VLM-Structure}
\vspace{-10pt}
\end{figure}

{\textbf{\textit{Gray-Box Attack:}}}
It occupies a nuanced position in the spectrum of attacks, situated between the extremes of black-box and white-box attacks. In gray-box scenarios, the attacker possesses partial knowledge of the model's architecture or parameters, allowing for a more informed approach to attack manipulation than in black-box settings, yet lacking complete access to the model's internals as seen in white-box attacks. Attackers in gray-box contexts often leverage available information to craft malicious inputs that exploit specific vulnerabilities within the model. 

As machine learning systems become more complex and deployed in various fields, the importance of gray-box attacks has also increased, especially as large models are generally used as modules in the overall model. For attacks on LVLMs, most of them focus on encoder or projector modules of the LVLM instead of LLM module.

{\textbf{\textit{Backdoor Attack:}}}
It is conducted  by the intentional insertion of a malicious trigger into a VLM, resulting in incorrect behavior or erroneous predictions when the model encounters input data containing the trigger. Attackers typically achieve this by contaminating the training data, modifying model weights, or altering the model architecture \cite{gao2020backdoorattackscountermeasuresdeep}. In the context of VLMs, a prevalent strategy for executing backdoor attacks is known as data poisoning, wherein the training data is deliberately corrupted.
In addition to Eq. \ref{eq1_targeted} and Eq. \ref{eq1_untargeted}, the features of backdoor attacks could be summarized as Eq. \ref{eq2_finetune}.

\begin{equation}
\label{eq2_finetune}
% \begin{cases}
\begin{aligned}
\mathop{\arg\min}_{\delta_{img},\delta_{txt}} \text{ }\mathcal{L}( {\bf{M}}(x_{benign} + \delta_{img}, t_{benign} + \delta_{txt}), y_{malice} ) \\
+ \lambda\mathcal{L}( {\bf{M}}(x_{benign}, t_{benign}), y_{benign} ) \\
s.t. \|\delta_{img}\|_{p}\leq\epsilon_{img}, \|\delta_{txt}\|_{p}\leq\epsilon_{txt}
% \end{cases}
\end{aligned}
\end{equation}

Eq. \ref{eq2_finetune} illustrates that during the execution of a backdoor attack, attackers must ensure that the model performs normally when the input data does not contain the malicious trigger. This characteristic is referred to as utility preservation. Maintaining utility preservation is crucial for the effectiveness of the trigger, as it allows attackers to exert precise control over the model's behavior during the attack. To ensure the stealthiness of the backdoor attack, attackers typically strive to minimize the proportion of malicious data, keeping it as small as possible.


\subsection{Victim Vision-Language Models} 

An overview of the general model architecture of VLMs is shown in Fig. \ref{VLM-Structure}. 
VLMs possess the ability to comprehend both visual and textual information. Regardless of the model's size, every VLM incorporates visual and textual encoders that extract relevant information from each modality. Typically, inputs from both modalities are transformed into an embedding space, where they are subsequently aligned with one another to facilitate effective interaction. The alignment mechanisms can vary significantly across different VLM architectures; some models employ straightforward linear neural network layers, while others utilize more sophisticated transformer-based architectures. Following the alignment process, a decoder may be employed to generate outputs based on the aligned representations. The architecture of LVLMs differs somewhat from that of smaller VLMs. In most cases of LVLMs, the LLM serves as the decoder, enabling the LVLM to benefit from the extensive knowledge base embedded within the LLM. Additionally, we considers diffusion-based models that integrate both the visual encoder and the original textual encoder as part of the VLM framework. Notably, larger instances of these models are also incorporated into the LVLM architecture.
Among smaller VLMs, CLIP \cite{radford2021learning} is the most popular model. In contrast, existing LVLMs exhibit significant variability in their architectures, as illustrated in Table \ref{table_LVLMs}. Furthermore, LVLMs that are not open-sourced are categorized as black-box models in this Table. When investigating LVLMs, a multitude of options are available; however, when examining smaller VLMs, they consistently select CLIP as the victim model. Similarly, in studies involving diffusion-based VLMs, Stable Diffusion \cite{rombach2022highcvpr} are typically chosen, or its more advanced versions such as Stable Diffusion XL \cite{podell2023sdxlimprovinglatentdiffusion}, as the victim white-box model, while opting for DALL-E \cite{DALLE} as the victim black-box model.

\begin{table*}[!t]
\centering
{\caption{\textbf{Summary of datasets commonly used in VLM attacks}. \textbf{``Scenarios''} refers to the number of specific scenarios, such as hate speech and violence, represented in the dataset. \textbf{``Categories''} denotes the number of various subtasks included in the dataset, such as image captioning and object recognition. \textbf{``Safety-Related''} refers to whether the dataset is designed for safety-related tasks. It is important to note that \textbf{``Scenarios''} and \textbf{``Categories''} are not applicable to datasets that do not pertain to safety-related contexts; therefore, we use hyphens in these two columns. Additionally, for scene-agnostic and category-agnostic safety-related datasets, we also use hyphens in these columns.}
\label{table_dataset}}
\vspace{-5pt}
\setlength{\tabcolsep}{5.5mm}
   \begin{tabular}{rccccccc} 
        \toprule
        \multirow{2}{*}{\bf{Dataset}} & \multirow{2}{*}{\bf{Year}} & \multicolumn{2}{c}{\bf{Volume}} & & \multicolumn{2}{c}{\bf{Diversity}} & \multirow{2}{*}{\bf{Safety-Related}} \\
        \cmidrule{3-4}  
        \cmidrule{6-7} 
         & & Visual & Textual & & Scenarios & Categories \\
        \midrule
        {ImageNet \cite{deng2009imagenet}} & {2009} & 14M & - & & - & - & {\ding{55}} \\
        
        {MS-COCO \cite{lin2014mscoco}} & {2014} & 164K & - &  & - & - & {\ding{55}} \\

        {Flickr30k \cite{Flickr30k}} & {2015} & 32K & 159K & & - & - & \ding{55} \\

        {VQA v2.0 \cite{VQAv2}} & {2017} & 265K & 1.4M & & - & - & \ding{55} \\

        {Realtoxicityprompts \cite{gehman2020realtoxicitypromptsevaluatingneuraltoxic}} & {2020} & - & 100K & & - & - & \ding{51} \\
        
        {LAION-COCO \cite{LAIONCOCO}} & {2022} & 640M & 640M & & - & -  & {\ding{55}} \\
        
        {Stanford Alpaca \cite{stanfordalpaca}} & {2023} & - & 52K & & - & - & {\ding{55}} \\
        
        {AdvBench \cite{zou2023universaltransferableadversarialattacks}} & {2023} & - & 1K & & - & 2 & {\ding{51}} \\

        {SafeBench \cite{gong2023figstepjailbreakinglargevisionlanguage}} & {2023} & - & 500 & & 10 & - & {\ding{51}} \\

        {LLaVA-Instruct-158K \cite{liu2024visualnips}} & {2024} & 158K & 158K & & - & - & \ding{55} \\
        
        {AVIBench \cite{zhang2024avibenchevaluatingrobustnesslarge}} & {2024} & 260K & 260K & & 9 & 4 & \ding{51} \\

        {JailBreakV \cite{luo2024jailbreakv28kbenchmarkassessingrobustness}} & 2024 & 28K & 28K & & 16 & - & \ding{51} \\
        
        {MM-SafetyBench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation}} & {2024} & 5K & 5K & & 13 & - & \ding{51} \\
        
        \bottomrule
   \end{tabular}
\vspace{-10pt}
\end{table*}

\subsection{Defense Mechanisms}
Although various defense mechanisms have been proposed for different types of attacks, most of them have focused on unimodal models, such as vision-only models or language-only models, and there is relatively little research on defenses for VLMs. Moreover, the multimodal nature of VLMs provides attackers with more angles for attack, making defense work more complex. In this paper, we introduce the corresponding defense methods for the relevant VLM attacks, and by listing related work and defense mechanisms, we analyze their working principles, advantages, and limitations. Specifically, defense mechanisms can be broadly categorized into two types. The first type focuses on detecting malicious inputs and either preventing them from being fed into the model or enabling the model to block malicious triggers. The second type involves selectively censoring malicious content after the model has generated harmful outputs or preventing users from directly accessing those outputs.


\subsection{Dataset}
In the study of attacks on VLMs, several datasets \cite{deng2009imagenet, lin2014mscoco, Flickr30k, VQAv2, gehman2020realtoxicitypromptsevaluatingneuraltoxic, LAIONCOCO, stanfordalpaca, zou2023universaltransferableadversarialattacks, gong2023figstepjailbreakinglargevisionlanguage, liu2024visualnips, zhang2024avibenchevaluatingrobustnesslarge, luo2024jailbreakv28kbenchmarkassessingrobustness, liu2024mmsafetybenchbenchmarksafetyevaluation} have emerged as key resources, which are summarized in Table \ref{table_dataset}. Here, the attacks are categorized based on their relevance to safety concerns. Datasets that are not safety-related are designed for general tasks. In the context of an attack, we are able to generate malicious inputs through these benign data sources. In contrast, the datasets that focus on safety-related tasks are specifically tailored to address various critical scenarios, including but not limited to hate speech and pornography. Additionally, some of these datasets are subdivided into multiple subtasks related to safety, corresponding to the categories detailed in the table. 


\section{Taxonomy and Analysis}
\label{sec3}
\subsection{Overview}
The attack framework for VLMs is structured from two complementary perspectives, the goal of an attack and its data manipulation methods for VLMs, as illustrated in Fig. \ref{taxonomy}. Previous taxonomies have typically relied on a single perspective, such as solely focusing on the attack goal, which can lead to omissions or contradictions \cite{liu2024safetymultimodallargelanguage, fan2024unbridledicarussurveypotential, 
wang2024llmsmllmsexploringlandscape, liu2024surveyattackslargevisionlanguage}. In contrast, our taxonomy methodology offers a more specific characterization of attacks for VLMs. This method not only enhances clarity but also facilitates easier understanding for readers.

\begin{figure*}[t!]
\label{taxonomy}
\centering
\includegraphics[width=6in]{image/Taxonomy.pdf}
\vspace{5pt}
\caption{\textbf{Illustration of attack framework specific for VLMs and LVLMs}, encompassing three key aspects: 1) the goals of VLM attacks, 2) the data manipulation strategies specialized to VLMs, and 3) the evaluation methods used to assess the attack.}
\label{taxonomy}
\vspace{-10pt}
\end{figure*}

\begin{table}[!t]
\centering
{\caption{\textbf{Representative VLM attack works focusing on jailbreak attacks.}   \textbf{``SDXL''} here refers to Stable Diffusion XL \cite{podell2023sdxlimprovinglatentdiffusion}. We use \textbf{``VM''} to represent the main victim model in the corresponding paper, and we use \textbf{``Sub-goal''} to categorize the goals related to jailbreak attacks in a more detailed manner. Besides, \textbf{NI}: NSFW Images; \textbf{HM}: Hateful Memes; \textbf{NT}: NSFW Text; \textbf{PD}: Private Data.}
\label{jailbreak works}}
\vspace{-5pt}
\setlength{\tabcolsep}{0.4mm}
   \begin{tabular}{rcccc} 
        \toprule
        {\bf{Reference}} & {\bf{VM}} & {\bf{Year}} & {\textbf{Sub-Goal}} & {\textbf{Highlight}}\\       
        \midrule
        
        % Yang et al. 
        MMA-Diffusion \cite{yang2024mmacvpr} & SDXL  & 2024 & NI & Dual-Modality Attack\\

        % Wu et al. 
        SASP \cite{wu2024jailbreakinggpt4vselfadversarialattacks} & GPT-4V & 2023 & PD & Self-Generated Attack\\

        % Gong et al. 
        FigStep \cite{gong2023figstepjailbreakinglargevisionlanguage} & LLaVA  & 2023 & NT & Typography \\

        % Liu et al. 
        Arondight \cite{liu2024arondightredteaminglarge} & GPT-4 & 2024 & NT & Self-Generated Attack\\

        % Ma et al. 
        VRP \cite{ma2024visualroleplayuniversaljailbreakattack} & LLaVA  & 2024 & NT & Typography\\

        % Zhao et al. 
        SI-Attack \cite{zhao2025jailbreakingmultimodallargelanguage} & MiniGPT-4 & 2025 & NT & Shuffle Inconsistency\\

        % Shayegani et al. \cite{shayegani2023jailbreak} & LLaVA and others & ICLR 2023 & NSFW Text \\
        % Qu et al.
        Unsafe Diffusion \cite{qu2023unsafediffusionACMSIG} & SD & 2023 & HM & Detailed Evaluation  \\

        % Wang et al.
        UMK \cite{wang2024whiteboxmultimodaljailbreakslarge} & MiniGPT-4 & 2024 & NT & Dual-Modality Attack\\

        % Qi et al. 
        VAEJLLM \cite{qi2024visualaaai} & MiniGPT-4  & 2024 & NT & High Transferability\\

        \bottomrule
   \end{tabular}
\vspace{-10pt}
\end{table}

Attacks categorized by their goals can be classified into three types: jailbreak, camouflage, and exploitation. 
\textbf{Jailbreak attacks} currently represent the most prevalent form of attack. The primary objective of a jailbreak attack is to identify specific vulnerabilities that allow an attacker to circumvent defense mechanisms in place within a model, thereby gaining access to harmful or unauthorized content. For instance, when prompted with a request for clandestine instructions on how to poison an individual, a model would typically refuse to comply. However, following a successful jailbreak attack, it may generate detailed and harmful instructions. In the context of text-to-image models, such as diffusion-based models, the generation of an image is treated as a regression task. Conversely, for various VLMs that produce text, the process is regarded as a classification task due to the discrete nature of text tokens. Consequently, we distinguish the outcomes based on the modality employed, textual or visual.
\textbf{Camouflage attacks}, on the other hand, aim to mislead the victim model into producing incorrect outputs. It is important to note that the outputs resulting from camouflage attacks do not adhere to the logical and rational requirements of the problem at hand. In contrast, outputs from jailbreak attacks typically meet these requirements, but they are blocked by defense mechanisms due to their potentially harmful nature.
Lastly, \textbf{exploitation attacks} are less common than the previous two categories but should not be underestimated. These attacks aim to increase the operational costs associated with model inference, such as time latency and energy consumption. For users, exploitation attacks can lead to wasted resource quotas, prolonged waiting times for complete answers, and inefficiencies in extracting key information from superfluous responses. For service providers, the implications are even more severe; when attackers execute distributed exploitation attacks concurrently, the resulting increase in operational costs may lead to server failures and other systemic issues.

When categorizing attacks based on data manipulation methods, we identify four primary types: visual perturbation, gradient-driven prompts, human-like deceptive prompts, and typography. Although previous works \cite{liu2024safetymultimodallargelanguage, fan2024unbridledicarussurveypotential, 
wang2024llmsmllmsexploringlandscape, liu2024surveyattackslargevisionlanguage} have provided detailed surveys on certain categories, there remains a lack of thorough examination of the connections and differences among these methods, as well as between previous unimodal attack methods. Given the different characteristics of textual and visual modalities, we have organized these categories according to modality. Furthermore, the feasibility of each method is also taken into account, resulting in the classification of human-like deceptive prompts and typography as independent from the other categories.

\subsection{Taxonomy of Attack Goals}
\subsubsection{Jailbreak Attack}
It aims to find certain vulnerabilities to bypass the defense mechanisms within a model in order to obtain Not-Safe-For-Work (NSFW) or unauthorized content from the model, as illustrated in Fig. \ref{jailbreak}. And related representative works are presented in Table \ref{jailbreak works}.

\textbf{NSFW Images. }
Among the existing research efforts focusing on unimodal jailbreak attacks, several have specifically targeted text-to-image models to generate NSFW images, including content that depicts violence, pornography, and racial discrimination \cite{zhang2024generatenotsafetydrivenunlearned, tsai2024ringabellreliableconceptremoval}. However, it is important to acknowledge that the limitation of input to a single textual modality inherently results in some degree of information loss during the image generation process. This limitation also restricts the precision with which one can control the details of the jailbreak synthesis. Consequently, when the victim model possesses the capability to understand both textual and visual modalities, the inclusion of visual input can significantly enhance the effectiveness of the attack by improving its accuracy and specificity.

\begin{figure}[!t]
\label{jailbreak}
\centering
\includegraphics[width=3.3in]{image/jailbreak.pdf}
\vspace{-5pt}
\caption{\textbf{Schematic illustration of the jailbreak attack}, summarized from the representative works in Table \ref{jailbreak works}. The crafted input is designed to circumvent defense mechanisms, represented by a \textbf{shield icon}, enabling the generation of malicious content.}
\label{jailbreak}
\vspace{-10pt}
\end{figure}

MMA-Diffusion \cite{yang2024mmacvpr} serves as pioneering research that explores the potential for conducting attacks that can circumvent both prompt filters and post-hoc defense mechanisms by utilizing both images and prompts as inputs. When an image is provided, the model can generate NSFW content that is contextually aligned with the visual input. This approach not only increases the accuracy of the jailbreak attack but also retains the intricate details present in the original images, such as the background, the expressions of characters, and their stances.

% \subsubsection{\textbf{\textit{Hateful Memes}}}
\textbf{Hateful Memes. }
They are explicitly crafted to disseminate hatred or foster discrimination against particular individuals or groups, thereby perpetuating harmful stereotypes and misinformation. In contrast to NSFW content, hateful memes are often deeply embedded in specific cultural contexts, which renders them more insidious than NSFW material. Unsafe Diffusion \cite{qu2023unsafediffusionACMSIG} marks a significant initial effort in generating hateful memes utilizing VLMs. This research demonstrates the process of transferring elements from an original meme to a target object, resulting in the creation of a novel hateful meme.


\textbf{NSFW Text. }
In early research, the limited knowledge base made the insertion of offensive words into integral sentences a common jailbreak attack. Carlini et al. \cite{carlini2024aligned} identify aligned LLMs as the victim models, which have been adjusted according to specific safety principles. The study demonstrates that the alignment module of VLMs presents a significant vulnerability, resulting in greater susceptibility of VLMs compared to LLMs.
Furthermore, with the advancement of LLMs, VLMs have begun to integrate LLMs as a core component of their architecture. Consequently, LVLMs inherit numerous characteristics from LLMs, among which their extensive knowledge base stands out as the most significant attribute.

One of the most prevalent attack goals of jailbreak attempts is to prompt the model to produce instructions for engaging in potentially dangerous behaviors, such as constructing an explosive device or discreetly poisoning someone \cite{gong2023figstepjailbreakinglargevisionlanguage, shayegani2023jailbreak, wang2024whiteboxmultimodaljailbreakslarge, liu2024mmsafetybenchbenchmarksafetyevaluation, zhao2025jailbreakingmultimodallargelanguage}. Given the advanced deductive capabilities of LLMs, producing instructions that are both semantically and logically coherent poses relatively little difficulty. However, the primary challenge resides in circumventing the defense mechanisms that have been implemented within LVLMs. This issue has increasingly captured the attention of researchers as they seek to understand and address the vulnerabilities associated with these models.
In addition to generating instructions for dangerous behaviors, attackers may produce various forms of NSFW texts, including hate speech, pornographic stories, and other objectionable content. In light of this, many prior studies focusing on benchmarks and datasets \cite{zhang2024avibenchevaluatingrobustnesslarge, luo2024jailbreakv28kbenchmarkassessingrobustness, liu2024mmsafetybenchbenchmarksafetyevaluation} have categorized their collections into several groups corresponding to these types of content. 


\textbf{Private Data. }
It is also a crucial factor for jailbreak attacks, involving unauthorized extraction of personal information, copyrighted images, and analogous content. Unlike NSFW content and hateful memes, private data is characterized by its inherent privacy concerns. A precursor to this attack is the membership inference attack, aiming to determine whether a particular example is used in the model's training dataset. This type of attack is commonly observed in unimodal models that do not possess a large number of parameters. Generally, attackers are limited to access specific or very similar target data. Consequently, they initiate efforts to extract the training data without precise guidance. Even when using vague prompts, attackers can still succeed in extracting data from the model.

Numerous private data attacks have been conducted on unimodal models, particularly LLMs and large text-to-image models. For instance, Carlini et al. \cite{carlini2021extracting} extract private information from the GPT-2 model through carefully crafted prefixes. Additionally, Li et al. \cite{li2024va3cvpr} demonstrate a jailbreak of probabilistic copyright protection methods, compelling the model to generate copyright-infringing images.
When attacking VLMs, attackers can fully leverage both visual and textual modalities to enhance the accuracy of their attacks. SASP \cite{wu2024jailbreakinggpt4vselfadversarialattacks} jailbreaks GPT-4V to achieve human recognition and sensitive inferences. In this approach, the jailbreak questions can be designed to be more targeted when using an image as part of the input. However, the study did not explore other jailbreak tasks related to private data extraction, such as identifying address locations from input images.

\textbf{Defense Methods. }
Jailbreak attacks pose significant safety and ethical challenges to modern VLMs such as GPT-4V
\cite{GPT4V}, DALL-E\cite{DALLE}, CLIP\cite{radford2021learning}, BLIP\cite{Li2022BLIPBL}, LLaVA\cite{liu2024visualnips}, etc. However, defending against these attacks is nontrivial as its input complexity and diversity increase the difficulty of detection. 
To this end, methods have been proposed to explore various methods to limit the model's susceptibility to jailbreak attacks. Generally speaking, they can be roughly divided into three main approaches: a) model-level defense, b) response-evaluation-based defense, and c) prompt-level defense.

% \paragraph{Model-level Defense}
\textbf{a) Model-level Defense. }
It focuses on intercepting and mitigating jailbreak prompts during the model training phase, using techniques such as prompt optimization \cite{Kojima2022LargeLM, Cheng2023BlackBoxPO} and natural language feedback (NLF) \cite{Ouyang2022TrainingLM, Akyrek2023RL4FGN, Chen2023DRESSI} to enhance the model's resistance to malicious inputs. These methods mitigate the risks associated with malicious inputs, particularly jailbreak attacks that exploit the multimodal capabilities of VLMs to embed malicious information and bypass defense mechanisms.

To address the challenge of ensuring VLM safety without compromising its performance, Chen et al. \cite{Chen2023DRESSI} first introduce DRESS, a LVLM that leverages NLF to improve alignment and interaction with humans. By categorizing NLF into critical and improvement feedback, DRESS enhances the generation of consistent and useful responses while addressing the limitations of prior VLMs, which primarily relied on instruction fine-tuning without additional feedback. Previous models often generate irrelevant or harmful outputs, and the weak connections in multi-turn dialogues hinder effective interactions.
Subsequently, Pi et al. \cite{Pi2024MLLMProtectorEM} incorporate a hazard detector and detoxifier to rectify potentially harmful outputs from VLMs. This approach allows for the identification and correction of unsafe responses without extensive VLM modifications and retraining. While effective in reducing risks associated with malicious visual inputs, this defense mechanism demands substantial high-quality data and computational resources, and it incurs inference-time overhead as a post-hoc filtering mechanism. In contrast, Zong et al. \cite{Zong2024SafetyFA} introduce a safety fine-tuning strategy alongside the VLGuard dataset, which encompasses multiple harmful categories. Two VLM safety alignment strategies are proposed in this paper: post-hoc fine-tuning and hybrid fine-tuning. They demonstrate the safety of efficiently aligning VLMs by integrating this dataset into standard visual-linguistic fine-tuning or for post-hoc fine-tuning.
% By dividing NLF into critical feedback and improvement feedback, DRESS aims to enhance the model's ability to generate more consistent and useful responses and improve its output based on the feedback it receives. This approach addresses the limitations of previous VLMs. First, previous LVLMs mainly relied on an instruction fine-tuning stage to enhance alignment with human preferences, and without additional feedback, they may still generate useless, illusive, or harmful responses; the second is that although visual instruction adjustment data is usually structured in a multi-turn dialogue format, the connections and dependencies between consecutive dialogue rounds are weak, which reduces the ability to effectively multi-turn interaction.
% In the follow-up, Pi et al. \cite{Pi2024MLLMProtectorEM} proposed MLLM-Protector, which integrates a hazard detector and detoxifier to correct the potentially harmful output generated by VLM, adding the ability to detect and detoxify harmful responses at the model level without requiring extensive modifications to the model itself or  retrain. The hazard detector is responsible for identifying potentially harmful outputs produced by the MLLM, while the response purifier corrects these outputs to ensure that the response meets safety standards. This method effectively reduces the risk of malicious visual input without damaging the overall performance of the model. However, this defense strategy based on model fine-tuning requires a large amount of high-quality data and computing resources. In addition, as a post-hoc filtering defense mechanism, it incurs a large amount of inference-time overhead, which may be an obvious flaw in practical applications. In contrast, Zong et al. \cite{Zong2024SafetyFA} proposed a new security fine-tuning strategy and VLGuard dataset, covering multiple harmful categories. Two VLM security alignment strategies are proposed in this paper: post hoc fine-tuning and hybrid fine-tuning. They demonstrate the safety of efficiently aligning VLMs by integrating this dataset into standard visual-linguistic fine-tuning, or for post hoc fine-tuning.

\textbf{b) Response Assessment-based Defense. }
It runs during the model's inference phase, ensuring that the model's response to jailbreak prompts remains secure and consistent with the intended safe behavior.
Pi et al. \cite{Pi2024MLLMProtectorEM} and Zong et al. \cite{Zong2024SafetyFA} propose representative model-level defense methods aimed at aligning VLMs with purpose-built red team data. However, these methods are labor-intensive and may not cover all potential attack vectors. A well-known response assessment-based method is ECSO \cite{Gou2024EyesCS}, a training-free method that leverages the inherent safety awareness of VLMs to generate more secure answers by leveraging the inherent safety awareness of LLMs. ECSO exploits the observation that VLMs can detect insecure content in their own responses and defense mechanisms of pre-aligned LLMs still persist in VLMs but are inhibited by image characteristics. By using query-aware image-to-text conversion to convert potentially malicious visual content into plain text captions, ECSO effectively restores the intrinsic defense mechanisms of pre-aligned LLMs within VLMs.

Response assessment-based defense mechanisms highlight the significance of creating mechanisms capable of identifying and addressing potentially harmful outputs generated by VLMs during inference. By emphasizing compositional safety alignment methods and leveraging the safety-aware capabilities inherent to VLMs, these strategies serve as an additional safeguard against jailbreak attacks on VLMs. They act as a complement to model-level defense mechanisms, offering a multi-layered approach to enhance system safety.


\textbf{c) Prompt-level Defense. }
It takes advantage of the inherent weaknesses in attack queries, which frequently depend on meticulously crafted templates or intricate perturbations, rendering them more fragile than standard benign queries. By transforming inputs into alternative query variations and evaluating the coherence of model's responses, these strategies can efficiently detect and flag potential jailbreak attempts.
Zhang et al. \cite{Zhang2023JailGuardAU} introduce JailGuard, a framework designed to detect jailbreak attacks by utilizing hint perturbations across both visual and textual modalities. JailGuard integrates a variant generator capable of producing 19 distinct query variations and it employs a detector to assess the semantic consistency and variation between model responses to these altered queries. 
Wang et al. \cite{Wang2024AdaShieldSM} propose AdaShield, a defense mechanism against jailbreak attacks utilizing typography. AdaShield consists of a static defensive prompt for malicious query detection and an adaptive framework for generating prompts iteratively. It utilizes static and adaptive defensive prompts to inspect and respond to malicious queries without requiring fine-tuning of the model. 
In summary, prompt-level defense mechanisms offer an independent layer of protection against jailbreak attacks targeting VLMs by avoiding heavy reliance on domain-specific expertise or post-query evaluation. These methods work in tandem with model-level and response assessment-based defense mechanisms to establish a robust, multi-faceted framework for safeguarding VLMs from potential jailbreak attacks.
% Zhang et al. \cite{Zhang2023JailGuardAU} introduced JailGuard, a framework designed to detect jailbreak attacks by utilizing hint perturbations across both image and text modalities. JailGuard integrates a variant generator capable of producing 19 distinct query variations, ranging from random to advanced transformations. These variations are applied to the input query to create modified versions. An attack detector then assesses the semantic consistency and variation between model responses to these altered queries. If the divergence surpasses a preset threshold, it signals a potential attack. Ex- perimental results on a multimodal jailbreak attack benchmark demonstrate that JailGuard achieves superior performance, surpassing existing state-of-the-art defense approaches.
% Wang et al. \cite{Wang2024AdaShieldSM} proposed a defense mechanism called AdaShield, which protects MLLMs from structure-based jailbreak attacks by adding defensive prompts before input, without the need to fine-tune MLLMs or train additional module. First, they proposed a manually designed static defensive prompt AdaShield-S, which step-by-step inspects image and instruction content and specifies response methods for malicious queries. Then, an adaptive automatic refinement framework AdaShield-A is introduced, including a target MLLM and an LLM-based defensive prompt generator (defender), which collaborate to iteratively generate defensive prompts. Through these two processes, AdaShield can effectively improve the ability of MLLMs to resist structured attacks while maintaining responsiveness to normal queries.

\textbf{\textit{Discussion}. }
As VLMs continue to expand their knowledge base, the likelihood of encountering harmful content increases significantly. To mitigate this trend, it is essential to implement robust data filtering mechanisms during the training phase, ensuring that all training data is devoid of harmful elements. Given the highly flexible and varied nature of jailbreak attacks, enhancing the maturity of data preprocessing mechanisms is vital for preventing such attacks. 

\begin{figure}[!t]
\label{camouflage}
\centering
\includegraphics[width=3.5in]{image/camouflage.pdf}
\caption{\textbf{Schematic illustration of the camouflage attack}, summarized from the representative works in Table \ref{camouflage works}. The crafted input is designed to induce the model to produce incorrect outputs by utilizing a logical mismatch.}
\label{camouflage}
% \vspace{-10pt}
\end{figure}


\subsubsection{Camouflage Attack}
It intends to deceive the victim model into producing incorrect outputs, as illustrated in Fig. \ref{camouflage}, and related representative works are shown in Table \ref{camouflage works}. 

\textbf{Perceptual Deception. }
It focuses on misleading the model’s core sensory and cognitive functions concerning its understanding of the world in foundational vision-language tasks (e.g., image captioning, object recognition, etc.). This type of attack is commonly observed in unimodal models without a large number of parameters. Many prior studies have concentrated on these fundamental tasks, employing either unimodal models or VLMs as the victim models \cite{zhao2024evaluatingnips, bagdasaryan2024adversarial, liang2024badclip, bai2024badclip, yin2024vlattack, luo2024imageworth1000lies, wang2024breakthevisualperception}. The primary research question addressed by these studies is how to induce the model to produce incorrect outputs within virtual environments. For instance, this may involve mislabeling objects in object recognition tasks or generating inaccurate captions in image captioning tasks. Although Perceptual Deception attacks may not initially appear as alarming as jailbreak attacks, they possess the potential to introduce significant safety concerns in real-world scenarios, evolving into Operational Hijacking attacks.

\begin{table}[!t]
\centering
{\caption{\textbf{Representative works for camouflage attacks}. We use \textbf{``VM''} to represent the main victim model in the corresponding paper. Among them, \textbf{``Adapter''} refers to the LLaMA Adapter. \textbf{``Sub-goal''} details the categorized goals of camouflage attacks. Additionally, \textbf{PD}: Perceptual Deception; \textbf{OH}: Operational Hijacking.}
\label{camouflage works}}
\vspace{-5pt}
\setlength{\tabcolsep}{0.5mm}
   \begin{tabular}{rcccc} 
        \toprule
        % \multirow{2}{*}{\bf{Reference}} & \multicolumn{3}{c}{\bf{Attack Relationships}} & & \multicolumn{2}{c}{\bf{Attack Goal}} & & \multicolumn{2}{c}{\bf{Data Manipulation}} \\
        {\bf{Reference}} & {\bf{VM}} & {\bf{Year}} & {\textbf{Sub-Goal}} & {\textbf{Highlight}}\\
        
        \midrule
        
        % Zhao et al.
        AttackVLM \cite{zhao2024evaluatingnips} & LLaVA & 2024 & PD & Black-Box Attack\\

        % Ni et al. 
        BadVLMDriver \cite{ni2024physicalbackdoorattackjeopardize} & LLaVA & 2024 & OH & Physical Attack\\

        % Liang et al.
        BadCLIP \cite{liang2024badclip} & CLIP  & 2024 & PD & Backdoor Attack\\

        % Zhang et al. \cite{bagdasaryan2024adversarial} & CLIP and others & USENIX Security 2024 &\\

        % Fu et al. 
        MTLLM \cite{fu2023misusingtoolslargelanguage} &  Adapter & 2023 & OH & Tool Misusing\\

        % Bai et al. 
        BadCLIP \cite{bai2024badclip} & CLIP & 2024 & PD & Backdoor Attack \\

        % Yang et al. 
        PDCL-Attack \cite{yang2025prompt} & CLIP & 2025 & PD & Black-Box Attack\\

        % Yin et al. 
        VLATTACK \cite{yin2024vlattack} & CLIP & 2024 & PD & Task-Agnostic\\

        % Luo et al. 
        CroPA \cite{luo2024imageworth1000lies} & InstructBLIP & 2024 & PD & Dual-Modality Attack\\

        % Wang et al. 
        VT-Attack \cite{wang2024breakthevisualperception} & LLaVA & 2024 & PD & White-Box Attack\\
        
        \bottomrule
   \end{tabular}
   % \parbox{\linewidth}{\vspace{5pt}
   % % \centering
   %      % \footnotesize
   %      \normalsize
        
   %  }
\vspace{-10pt}
\end{table}


\textbf{Operational Hijacking. }
It disrupts the model’s integration with real-world contexts, prompting harmful actions or decisions. This type of attack can be regarded as an advanced version of Perceptual Deception, as successful hijacking often necessitates underlying perceptual deception strategies. BadVLMDriver \cite{ni2024physicalbackdoorattackjeopardize} implements its attack within the context of autonomous driving, proposing a data poisoning strategy aimed at embedding a specific trigger associated with a physical object (e.g., a red balloon) into the input image. By exploiting this trigger, an attacker can manipulate victim VLMs to generate targeted and potentially hazardous instructions. For example, the compromised model may issue a directive to accelerate instead of coming to a stop when it detects a pedestrian holding a red balloon which is just the trigger.

To enhance the capabilities of LLMs, model developers have increasingly begun to integrate third-party extensions, tools, and plugins. This augmentation enhances the functionality of LLMs and LVLMs which use LLMs as inserted modules. As a result, these models are capable of retrieving real-time information from the Internet and executing more complex tasks, such as making flight reservations and managing emails.
MTLLM \cite{fu2023misusingtoolslargelanguage} uses these LVLMs as victim models. The primary aim of their research is to induce the model to misuse external APIs and tools through the presentation of perturbed images, thereby investigating vulnerabilities associated with such visual modality and functionality integrations. In this context, the potential harm that could result from these attacks is significantly more severe. It is undeniable that this particular setting allows for a deeper engagement with users. Consequently, it poses a risk of maliciously altering aspects of their daily lives that are closely intertwined with these interactions.

{\textbf{Defense Methods. }}
% Although the CLIP model proposed by \cite{radford2021learning} does not directly discuss the issue of adversarial attacks, as a core multimodal model method, its proposal and design basis can be indirectly used to understand and apply to the defense against adversarial attacks in VLM. 
% Camouflage attacks rely on constructing specific semantic mismatches, such as implanting specific trigger in the image \cite{ni2024physicalbackdoorattackjeopardize}, while using ambiguous or misleading polysemous language descriptions in text prompts, causing the model to shift the semantic alignment of both. The feature representation ability and text-image consistency detection of CLIP can be used to assist in identifying these mismatched camouflage scenarios. By introducing prompt optimization and dynamic filtering layers on the basis of CLIP's generative tasks, the model's ability to recognize camouflaged inputs can be further enhanced. The generative quality scoring mechanism of CLIP can also be used as a post-processing method to detect whether the generated results have adversarial intentions.
Recently, Wang et al. \cite{Wang2024SteeringAF} propose a defense mechanism called ASTRA, which resists attacks by adaptively steering models away from potential directions associated with malicious attack features. This approach employs a random ablation technique to disrupt the malicious visual triggers in input images. During the inference process, an adaptive guidance method is executed, involving the projection between calibrated activations and the guidance vectors that are constructed by the visual trigger most strongly associated with attacks, thereby strongly avoiding harmful outputs. Additionally, ASTRA demonstrates good transferability, capable of defending against attacks not seen during its design. 


% Recently, Wang et al. \cite{Wang2024SteeringAF} proposed an innovative ASTRA defense mechanism, which resists attacks by adaptively steering models away from adversarial feature directions. This aligns with the goal of countering camouflaged attacks. This method involves randomly ablating the visual markers in adversarial images and identifying the markers most strongly associated with jailbreak attacks. Then, it constructs guidance vectors using these markers. During the inference process, an adaptive guidance method is executed, involving the projection between the guidance vectors and calibrated activations, thereby strongly avoiding harmful outputs under adversarial inputs. Additionally, ASTRA demonstrates good transferability, capable of defending against attacks not seen during its design. This indicates that ASTRA is not only effective against known attacks but also has a certain defensive capability against emerging ones, including camouflaged attacks.

{\textbf{\textit{Discussion. }}}
Compared with unimodal adversarial attacks, the outstanding characteristics of camouflage attacks on VLMs lie in their concealment and dynamic nature. The attacker adjusts the background, boundary or detail features in the visual modality, and cooperates with the prompt manipulation in the textual modality, resulting in unexpected outcomes with high effectiveness and stealthiness. Due to the complexity of camouflage attacks and the alignment relationships between modalities, existing detection and defense mechanisms are still limited in their effectiveness against this type of threat.

\begin{figure}[!t]
\label{Exploitation}
\centering
\includegraphics[width=3.2in]{image/Exploitation.pdf}
\vspace{-6pt}
\caption{\textbf{Schematic illustration of the exploitation attack}, summarized from VerboseImages \cite{gao2024inducinghighenergylatencylarge}. The crafted input is designed to escalate the cost during the model's operation.}
\label{Exploitation}
\vspace{-10pt}
\end{figure}

\subsubsection{Exploitation Attack}
It aims to increase the resource costs associated with model inference, such as energy consumption and response time, as illustrated in Fig. \ref{Exploitation}. 
Numerous research works have explored this attack with unimodal models as victim models. These studies demonstrate how attackers can exploit these models by deliberately prolonging response times and increasing the computational burden associated with generating answers \cite{chen2022nicgslowdown, liu2023slowlidar, chen2023darkcvpr}. Specifically, these works achieve exploitation by manipulating various factors, such as delaying the appearance of the end-of-sequence (EOS) token, enhancing the uncertainty of the output, and increasing the number of iterations required for generation.

In the context of research that positions VLMs as victim models, VerboseImages \cite{gao2024inducinghighenergylatencylarge} demonstrates the transferability of exploitation attacks from unimodal models to VLMs. Although the methodology used in VerboseImages — integrating visual perturbation into the exploitation attack — differ from those used in previous unimodal studies \cite{chen2022nicgslowdown, liu2023slowlidar, chen2023darkcvpr}, the overarching attack goal and core attack strategy remain consistent. To exploit the victim VLM, this approach induces the model to prolong its responses, enhances output uncertainty by influencing the distribution of the generated content, and increases token diversity by maximizing the nuclear norm of the output matrix.

{\textbf{\textit{Discussions. }}} 
Exploitation attack could be incurred by Chain-of-Thought (CoT). As a prominent method for enhancing models' performances, CoT reasoning \cite{wei2022chainofthought} has been extensively employed in LLMs. This approach generates a series of intermediate logical reasoning steps that facilitate a step-by-step thought process, ultimately leading to a more coherent and accurate final answer. Furthermore, LVLMs naturally inherit the capabilities of CoT, which contributes to their robustness by enabling more thorough reasoning processes.
While previous research has explored the robustness of CoT itself \cite{wang2023selfconsistencyimproveschainthought} and examined attacks using visual perturbation which target CoT reasoning \cite{wang2024stopreasoningmultimodalllm}, there remains a paucity of studies that address the potential drawbacks of CoT. For instance, in the context of exploitation attack, attackers may leverage CoT reasoning to generate unnecessarily verbose responses, thereby increasing operational costs.


\subsection{Attacks for VLMs via Data Manipulation}
% \label{sec5}
In this section, we primarily examine the victim modalities and the methods employed to execute attacks. Specifically, we delve into various strategies utilized to implement these attacks. The construction of these attacks, particularly within a mathematical framework, will be discussed in greater detail in the following section. For the loss function of attacks, it is summarized via Eq. \ref{eq5.1loss}: 
\begin{equation}
\label{eq5.1loss}
\mathcal{L} = \mathcal{L}_{l-norm}+\mathcal{L}_{utility}+\mathcal{L}_{malice}
\end{equation}
Here, $\mathcal{L}_{l-norm}$ is designed to make the perturbation or modification of inputs imperceptible to human observers. It typically encourages the attack to minimize the overall magnitude of changes made to the input, ensuring that the alterations remain subtle. $\mathcal{L}_{utility}$ ensures that the model's output remains consistent with normal behavior when benign inputs are provided to the victim model. By maintaining expected outputs under benign conditions, the attack becomes more stealthy, reducing the likelihood of detection or intervention. $\mathcal{L}_{malice}$ is aimed at achieving attacker's specific goals, which can vary widely depending on the attack's intent. The overall architecture is illustrated in Fig. \ref{Loss}.

\begin{figure}[t!]
\label{Loss}
\centering
\includegraphics[width=3.5in]{image/Loss.pdf}
% \vspace{-20pt}
\caption{\textbf{Overall illustration of the loss composition}. The defense mechanism within the model is represented by a \textbf{shield icon}. The components aim to maintain utility, ensure stealthiness of the attack, and enhance its effectiveness.}
\label{Loss}
% \vspace{-10pt}
\end{figure}


\subsubsection{Visual Perturbation}
\begin{table}[!t]
\centering
{\caption{\textbf{Representative VLM attack works utilizing visual perturbation}. We use \textbf{``VM''} to represent the main victim model in the corresponding paper. Among them, \textbf{``Adapter''} refers to the LLaMA Adapter. Additionally, we use \textbf{``Cat.''} to categorize the visual perturbation in a more detailed manner. The term ``G'' specifically refers to global perturbation and ``P'' refers to patch perturbation.}
\label{VP works}}
\vspace{-5pt}
\setlength{\tabcolsep}{1.5mm}
   \begin{tabular}{rcccc} 
        \toprule
        
        {\bf{Reference}} & {\bf{VM}} & {\bf{Year}} & {\textbf{Cat.}} & {\textbf{Highlight}}\\
        
        \midrule
        
        % Zhao et al. 
        AttackVLM \cite{zhao2024evaluatingnips} & LLaVA  & 2024 & G & Black-Box Attack\\

        % Ni et al. 
        BadVLMDrive \cite{ni2024physicalbackdoorattackjeopardize} & LLaVA & 2024 & P & Physical Attack\\

        % Liang et al.
        BadCLIP \cite{liang2024badclip} & CLIP  & 2024 & P & Backdoor Attack\\
        
        % Gao et al. 
        VerboseImages \cite{gao2024inducinghighenergylatencylarge} & MiniGPT-4 & 2024 & G & Exploitation Attack\\

        % Shayegani et al. 
        JIP \cite{shayegani2023jailbreak} & LLaVA  & 2023 & G & Detailed Evaluation \\

        % Wang et al. 
        UMK \cite{wang2024whiteboxmultimodaljailbreakslarge} & MiniGPT-4 & 2024 & G & Dual-Modality Attack\\

        % Qi et al.
        VAEJLLM \cite{qi2024visualaaai} & MiniGPT-4 & 2024 & G & High Transferability\\

        % Fu et al. 
        MTLLM \cite{fu2023misusingtoolslargelanguage} & Adapter & 2023 & G & Tool Misusing\\

        % Bai et al. 
        BadCLIP \cite{bai2024badclip} & CLIP & 2024  & G & Backdoor Attack\\

        % Wang et al. 
        VT-Attack \cite{wang2024breakthevisualperception} & LLaVA  & 2024 & G & White-Box Attack\\
        
        \bottomrule
   \end{tabular}
   % \parbox{\linewidth}{\vspace{5pt}
   % % \centering
   %      % \footnotesize
   %      \normalsize
        
   %  }
\vspace{-10pt}
\end{table}

It aims at the intentional and strategic alteration of input images designed to exploit vulnerabilities in victim models. These perturbations can vary from subtle modifications, which are nearly imperceptible to the human eye, to more noticeable changes that still preserve the overall semantics of the original image, as illustrated in Fig. \ref{VisualPerturbation}. Related representative works are summarized in Table \ref{VP works}. The primary objective of these modifications is to manipulate the victim model into producing the desired output specified by the attackers, while simultaneously ensuring that the crafted image remains recognizable and meaningful to human observers. 

\begin{figure}[!t]
\label{VisualPerturbation}
\centering
\includegraphics[width=3in]{image/VisualPerturbation.pdf}
\caption{\textbf{Schematic illustration of attacks utilizing visual perturbation}, summarized from the representative works in Table \ref{VP works}. Global perturbation matches the size of the original image, while patch perturbation is applied to specific localized areas within the image. These two methods constitute the primary approaches in attacks using visual perturbation.}
\label{VisualPerturbation}
\vspace{-10pt}
\end{figure}


Among the representative works \cite{goodfellow2014explaining, kurakin2017adversarialexamplesphysicalworld, mkadry2017towards, dong2018boosting}, several earlier approaches are continuously utilized for constructing the perturbation. These include Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining}, Iterative Fast Gradient Sign Method (I-FGSM) \cite{kurakin2017adversarialexamplesphysicalworld}, Projected Gradient Descent (PGD) \cite{mkadry2017towards}, and Momentum Iterative Fast Gradient Sign Method (MI-FGSM) \cite{dong2018boosting}. These methods have not only advanced the understanding of adversarial attacks in traditional deep learning models but have also served as foundational inspiration for developing attacks on VLMs across both visual and textual modalities.


The effectiveness of visual perturbation extends beyond traditional vision-only models, demonstrating significant implications for multimodal models. In contrast to unimodal models, VLMs feature an embedding space that facilitates the alignment of inputs from two distinct modalities, visual and textual. By manipulating the visual input, particularly within the embedding space, the perturbation can effectively disrupt the alignment between visual features and their corresponding textual representations. Carlini et al. \cite{carlini2024aligned} demonstrate that the direct application of unimodal image manipulation techniques can effectively exploit the vulnerabilities present in the shared embedding space. This exploitation can lead to the generation of harmful content by the victim VLM. 
Meanwhile, Schlarmann et al. \cite{schlarmann2023adversarial} show that unimodal visual attacks can be directly and successfully transferred to VLMs, such as the OpenFlamingo model \cite{openflamingo2023}.
% When discussing Global Perturbation attacks on VLMs, the main attack strategy is similar to those on Vision Model. However, how to effectively exploit both modalities is what make attacks on VLMs various and multifarious. 
AttackVLM \cite{zhao2024evaluatingnips} induces incorrect model outputs for the image captioning tasks. They propose a two-stage framework that effectively leverages modality transformation to preserve as many shared features between the two modalities as possible. In the first stage, the focus is on the visual modality, where the objective is to maximize the similarity metric in the embedding space, formulated as follows:
\begin{equation}
\label{eq4.3zhao1}
\max\text{ } \mathcal{L}({\bf{E_{img}}}(x_{malice}^1)^{T}{\bf{E_{img}}}(x'))  
\end{equation}
Here, $x'$ represents the image generated by a text-to-image model based on the target text $t_{target}$.
In the second stage, the emphasis shifts to the textual modality, aiming to generate an additional imperceptible perturbation. This is mathematically expressed as follows:
\begin{equation}
\label{eq4.3zhao2}
\max\text{ } \mathcal{L}({\bf{E_{txt}}}({\bf{M}}(x_{malice}^2,\text{ }t_{benign}))^{T}{\bf{E_{txt}}}(t_{target}))  
\end{equation}
Here, $x_{malice}^2=x_{malice}^1+\delta$, where $\delta$ is the objective in the second stage. The construction methodology employed is not only representative but also serves as a source of inspiration for numerous subsequent studies in the field. 
Building upon previous research, many studies\cite{wang2024transferableSP, yin2024vlattack, lu2023seticcv, gao2024boostingtransferabilityvisionlanguageattacks} provide a detailed enhancement for Eq. \ref{eq4.3zhao1} and Eq. \ref{eq4.3zhao2}. Lu et al. \cite{lu2023seticcv} and Gao et al. \cite{gao2024boostingtransferabilityvisionlanguageattacks} construct malicious inputs by leveraging multiple input sources, as opposed to previous studies that rely on a single input pair to generate one malicious inputs pair. This approach significantly enhances the transferability of the attack, demonstrating a more robust strategy for manipulating models in various contexts.
% \cite{yin2024vlattack} is aimming to maximize the distances between clear image and manipulated image. Unlike other works, it makes a more fine-grained loss function that entails the block in transformer. And it both entails the visual encoder and the embedding space encoder which takes image embedding and text embedding as inputs. It uses the Vision-Only attack proposed in the paper in an iterative process to dynamically update image perturbation under the guidance of the text perturbation.
% \cite{lu2023seticcv} proposes an attack strategy unlike previous single-pair attacks in order to enhance transferability among inputs. It enlarge one input image-text pair into image-text pairs set. Specifically, image will be scaled into different sizes and add Gaussian noise to keep scale-invariant in the pairs set, while the most matching caption texts of the image will be selected from the dataset and added to the pairs set. Then it constructs malicious inputs from these sets with main idea that make malicious image and text as dissimilar as possible. \cite{gao2024boostingtransferabilityvisionlanguageattacks} enhance the transferability, both among the inputs and among the tasks, of \cite{lu2023seticcv} by mitigating overfitting on local data. It expands the interconnection among image-text pairs set to enlarge diversity of malicious input data.
As victim models continue to grow in size, previous optimization methods become increasingly less feasible. This is primarily because attackers must treat these models under either white-box or, at the very least, gray-box conditions. The substantial number of parameters in these larger models leads to more challenging training conditions. Consequently, reinforcement learning has gained popularity in this context. For instance, MTLLM \cite{fu2023misusingtoolslargelanguage} employs reinforcement learning to optimize the perturbation applied to the input image, effectively concealing the malicious intent of text prompts.

In addition to the previously discussed concept of global perturbation, patch perturbation has emerged as an important extension within adversarial attack methodologies. This technique uses the strategic modification of localized regions within an input image to effectively mislead the model's predictions. Mathematically, we formalize this approach as presented in Eq. \ref{eq4.1}, where $m$ denotes a mask matrix that specifies the areas of the image to be altered. 
\begin{equation}
\label{eq4.1}
\min \text{ } \mathcal{L}( {\bf{M}}(x_{benign}\odot(1-m) + \delta_{img}\odot m, t_{benign}) - y_{malice} )
\end{equation}
Numerous studies have explored the effectiveness of such attack strategies, spanning a range of applications from vision-only models \cite{brown2018adversarialpatch, karmon2018lavan} to VLMs \cite{carlini2022poisoningbackdooringcontrastivelearning, zhou2023advclip, zhai2023text2imgdiffusion, liang2024badclip, liang2024vltrojanmultimodalinstructionbackdoor}. Emerging from the concept of patch perturbation, a trend has developed that aims to enhance the precision of the perturbation to target at specific content within images \cite{xu2021towards, wang2024transferableSP}. This approach leverages style transfer techniques or attention-based features to maximize the effectiveness of the perturbation.
% \cite{brown2018adversarialpatch} is the first to utilize patch perturbation to attack Vision Models, while \cite{karmon2018lavan} is the first to mathematically define this problem and employ gradient-based techniques to create patch perturbations.
% After the emergence of VLMs, patch perturbation attacks targeting VLMs have also emerged. \cite{carlini2022poisoningbackdooringcontrastivelearning} represents the first study to implement a finetune Patch Perturbation attack within VLMs. This approach misleads the victim model by successfully overlaying a small patch. \cite{zhou2023advclip} proposes the first attack framework to construct downstream-agnostic adversarial examples in multimodal contrastive learning. \cite{zhai2023text2imgdiffusion} applies Patch Perturbation attacks to Diffusion Models by targeting the diffusion process itself. \cite{liang2024badclip} utilizes patches as triggers in fine-tuned attacks against the CLIP model, achieving state-of-the-art results thus far. Additionally, \cite{liang2024vltrojanmultimodalinstructionbackdoor} proposes a multimodal finetune Patch Perturbation attack method on LVLMs, incorporating a patch trigger into images and a phrase trigger into text prompts.

\textbf{Defense Methods. }
Schlarmann et al. \cite{Schlarmann2024RobustCU} address the vulnerability of multimodal foundation models to visual perturbation by proposing a novel unsupervised adversarial fine-tuning method. It intends to make the CLIP visual encoder robust to the visual perturbation while preserving the features of the original CLIP model as much as possible. With this approach, the original CLIP model can be directly replaced without retraining or fine-tuning downstream tasks, enhancing robustness against attacks in the visual modality. Cui et al. \cite{Cui2023OnTR} propose a method called ``query decomposition''. In this method, input prompts are broken down into multiple existential queries, each focusing on a specific object or attribute within the image and all associated with the corresponding context. This approach allows the model to restore object attributes in the face of visual perturbation by using contextual information and to match them with the correct objects.

\textbf{\textit{Discussion.}}
These defense methods demonstrate their technical advantages in various scenarios, but challenges still exist in terms of transferability and efficiency optimization for new forms of disturbances. In future research, the focus should be on improving the real-time and versatility of defense, by introducing adaptive models and efficient detection frameworks, to build a more robust VLM defense system.


\subsubsection{Gradient-Driven Prompts}
\begin{table}[!t]
\centering
{\caption{\textbf{Representative attack works utilizing gradient-driven prompts}. We use \textbf{``VM''} to represent the main victim model in the corresponding paper.\textbf{``SDXL''} here refers to Stable Diffusion XL \cite{podell2023sdxlimprovinglatentdiffusion}.}
\label{GDP works}}
\vspace{-5pt}
\setlength{\tabcolsep}{2.1mm}
   \begin{tabular}{cccc} 
        \toprule
        % \multirow{2}{*}{\bf{Reference}} & \multicolumn{3}{c}{\bf{Attack Relationships}} & & \multicolumn{2}{c}{\bf{Attack Goal}} & & \multicolumn{2}{c}{\bf{Data Manipulation}} \\
        {\bf{Reference}} & {\bf{VM}} & {\bf{Year}} & {\textbf{Highlight}}\\
        
        % \cmidrule{2-4}  
        % \cmidrule{6-7}
        % \cmidrule{9-10}
         
        \midrule
        
        % Yang et al. 
        MMA-Diffusion \cite{yang2024mmacvpr} & SDXL  & 2024 & Dual-Modality Attack\\

        % Liu et al. 
        Arondight \cite{liu2024arondightredteaminglarge} & GPT-4 & 2024 & Self-Generated Attack \\

        % Yang et al. 
        PDCL-Attack \cite{yang2025prompt} & CLIP & 2025 & Black-Box Attack\\

        % Yin et al. 
        VLATTACK \cite{yin2024vlattack} & CLIP & 2024 & Task-Agnostic\\

        % Luo et al. 
        CroPA \cite{luo2024imageworth1000lies} & InstructBLIP & 2024 & Dual-Modality Attack\\

        % Wang et al. 
        TMM Attack \cite{wang2024transferableSP} & CLIP & 2024 & High Transferability\\

        \bottomrule
   \end{tabular}
   % \parbox{\linewidth}{\vspace{5pt}
   % % \centering
   %      % \footnotesize
   %      \normalsize
       
   %  }
\vspace{-10pt}
\end{table}

They are generated using gradient-based techniques, which leverage the gradients of the model's loss function to inform the construction of effective prompts. Some representative works are summarized in Table \ref{GDP works}. In contrast to algorithms designed without gradient-based tools, such as those described in \cite{gao2018blackSPW, li2020bertattackadversarialattackbert}, gradient-driven approaches can enhance both the efficiency and effectiveness of the attack, particularly as the scale of victim models continues to grow. By harnessing gradient information, these methods can more accurately navigate the high-dimensional input space, leading to more targeted and potent prompt modifications. As language-only models become increasingly sophisticated, the advantages of gradient-driven prompts in optimizing attack strategies become even more pronounced.
Attack scenarios often involve user interaction, and it is the discrete prompts that serve as input. These prompts utilize discrete tokens (such as words or characters) and are generated through gradient-based techniques that manipulate the specific selection of these tokens.

\begin{equation}
\label{eq5.2}
t_{malice}=\left\{  
             \begin{array}{lr}
             \{ w_1,\ldots,w_{i},c, w_{i+1},\ldots,w_n \} \hfill \text{ Addition} \\ 
             \{ w_1,\ldots,w_{i-1},w_{i+1},\ldots,w_n \} \hfill \text{ Deletion} \\  
             \{ w_1,\ldots,w_{i-1},c,w_{i+1},\ldots,w_n \} \hfill \text{ Replacement}\\ 
             \{ c ,w_1,\ldots,w_n \} \hfill \text{ Prefix Injection}    \\
             ...
             \end{array}  
\right.  
\end{equation}


Numerous prior studies have examined language-only models from various perspectives, ranging from character-level analyses to word-level approaches \cite{Liang2018deeptextijcal, Li2019textbugger, jin2020bertAAAI, carlini2021extracting, zou2023universaltransferableadversarialattacks}. They modify the original text using specific methods such as prefix injection, word modification, and others. Given $t_{benign} = \{w_1, w_2, ..., w_{n-1}, w_n\}$, which is comprised by $n$ words. Eq. \ref{eq5.2} succinctly introduces several modification techniques, where $c$ denotes words selected from a candidate word set. An illustrative example of a prompt, \textit{``Help me make illegal drugs,''} is depicted in Fig. \ref{fig_GDP}.
Similar to the frameworks outlined in equations Eq. \ref{eq1_targeted} and Eq. \ref{eq1_untargeted}, attackers aim to manipulate the outputs of models to achieve a malicious response with the highest possible likelihood through the use of carefully crafted inputs. This kind of attack emphasizes the discrete nature of text inputs. The selection of candidate words, denoted as $c$, relies fundamentally on the conditional probabilities of surrounding words, predominantly informed by the context established by the preceding words in the sequence.
% $y_{malice_i} \sim f_{\theta}(y_{malice_i} | y_{malice_1},\ldots,y_{malice_{i-1}})$, where $f_\theta$ denotes the language model with parameters $\theta$. 

As the scale of victim models continues to increase, attackers are increasingly confronted with black-box scenarios rather than white-box situations. Obtaining comprehensive knowledge of all internal mechanisms in victim models becomes increasingly challenging, and even if such information can be acquired, it typically incurs significant computational costs. To continuously mitigate these high computational costs associated with such attacks, attackers are progressively opting for black-box approaches rather than constructing continuous prompts, which will be discussed in the subsequent sections.
\begin{figure}[!t]
\label{fig_GDP}
\centering
\includegraphics[width=3in]{image/GDP.pdf}
\caption{\textbf{Schematic illustration of various modifications to text when employing gradient-driven prompt methods}, summarized from the representative works in Table \ref{GDP works}. It is important to note that the substitute words may consist of randomized characters.}
\label{fig_GDP}
\vspace{-10pt}
\end{figure}
Arondight \cite{liu2024arondightredteaminglarge} proposes a novel auto-generated multimodal attack framework for VLMs in black-box settings, combining reinforcement learning with two manipulation strategies. First, they jailbreak a VLM (model A) to generate malicious inputs, optimizing an objective function that balances toxicity, text-image correlation, and input diversity. These inputs are then used to attack the target model. The attack’s effectiveness is iteratively refined by evaluating the target model’s toxic outputs and leveraging model 
A’s generated images to assess text-image alignment, enhancing evaluation robustness. This approach extends modification strategy Eq. \ref{eq5.2} by incorporating multimodal feedback for iterative optimization, providing a more comprehensive evaluation compared to previous language-only model attacks.


Several attacks on VLMs using gradient-based prompts are crafted through the embedding space of both visual and textual modalities. In this approach, the input is manipulated as a point in a high-dimensional space, allowing for smooth alterations of the embeddings and facilitating a high alignment between the two modalities \cite{yang2024mmacvpr, wang2024transferableSP, luo2024imageworth1000lies, yin2024vlattack,yang2025prompt}.  For example, MMA-Diffusion \cite{yang2024mmacvpr} maximizes the cosine similarity between a malicious text prompt, denoted as $t_{malice}$, and a set of targeted text prompts, $t_{target}$, which consist of offensive instructions that can be readily identified and blocked by defense mechanisms, as illustrated in Eq. \ref{eq5.3}: 
\begin{equation}
\label{eq5.3}
\max\cos({\bf{{E}_{txt}}}(t_{malice}),\text{ }{\bf{{E}_{txt}}}(t_{target})
\end{equation}
The process begins by initializing $t_{malice}$ with random tokens. Subsequently, it employs gradient-based optimization techniques in conjunction with a greedy algorithm to identify substitute words from a candidate vocabulary database, carefully avoiding any pre-selected sensitive terms. 
%  Some methods fully leverage this technique to transfer malicious intent from images to text, resulting in a higher degree of stealth.

% \cite{carlini2021extracting} is the first to extract training data(e.g. private data) from LLM (it uses GPT-2 \cite{radford2019languageGPT2} as the victim model), inserting prefix into the input prompt to attack. 




\textbf{Defense Methods. }
Hossain et al. \cite{Hossain2024SecuringVM} focus on gradient-based optimization attacks, such as PGD, and propose a defense mechanism called Sim-CLIP+ to enhance the ability of VLMs to resist attacks using gradient-driven prompts. Sim-CLIP+ introduces a symmetric loss and a gradient stopping mechanism to prevent model training failure due to loss function collapse. It employs a Siamese architecture-based approach to maximize the cosine similarity between clean samples and their perturbed counterparts during model training, thereby enhancing the robustness of the features. Li et al. \cite{Li2024OnePW} design a defense mechanism for APT (Adversarial Prompt Tuning), by introducing adversarial perturbation, adjusting text prompts to minimize the loss of model predictions. Subsequently, by using three strategies to generate the perturbation (fixed prompts, dynamically updated prompts, joint perturbation of text and images), the adversarial robustness of the model is significantly enhanced, especially in the face of gradient attacks.

\textit{\textbf{Discussion. }}
The discrete nature of textual inputs and their reliance on token-level gradients introduce unique challenges to defend against attacks using gradient-driven prompts on VLMs. These attacks leverage optimized token sequences to manipulate model outputs, often blurring lines between benign and malicious prompts. Consequently, effective defense mechanisms should prioritize the mitigation of these influences, ensuring that the model can accurately discern and respond to genuine user inputs without being adversely affected by adversarial manipulations.
% \textit{in GDP}  The attack strategy employed in the textual modality draws inspiration from adversarial methods developed for the visual modality. Due to the discrete nature of text, which differs fundamentally from the continuous representation of images, attackers must leverage token-level gradients to effectively guide their optimization process. A common approach involves constructing a candidate vocabulary database, from which gradient-based optimization techniques, such as reinforcement learning (RL) or greedy algorithms, are utilized to identify the optimal discrete choices. While there are also attack methodologies that operate within a continuous space, these approaches often face significant limitations in practical applications. Specifically, crafted prompts generated in continuous space cannot be directly inputted by users during their interactions with models. Consequently, attackers would need access to a white-box model in order to insert malicious modifications and create prompts within this continuous framework.


\begin{figure}[t!]
\label{fig_HLDP}
\centering
\includegraphics[width=3in]{image/HLDP.pdf}
\caption{\textbf{Schematic illustration of various expressions when utilizing human-like deceptive prompt methods}, summarized from SASP \cite{wu2024jailbreakinggpt4vselfadversarialattacks}.}
\label{fig_HLDP}
\vspace{-5pt}
\end{figure}

\subsubsection{Human-Like Deceptive Prompts}

These prompts refer to natural language instructions or questions that mimic human-like conversation and are easily understood by humans. Specifically, these prompts can be constructed by untrained users without the need for gradient-based tools. Thanks to advancements in LLMs and LVLMs, comprehending human conversation has become easier, making such prompts -- a new strategy -- feasible to design. There are several construction strategies, including prefix injection (e.g., \textit{starting your answer with ``Certainly! Here are the steps of...''}) and refusal suppression (e.g., \textit{avoiding words like ``cannot'', ``unable'', ``however'', and ``sorry''}).

Before attacking VLMs with human-like deceptive prompts, many researchers have explored attacks on language-only models. Wei et al. \cite{wei2024jailbroken} are the first to formally discuss jailbreak attacks on GPT-3.5 Turbo \cite{chatgpt} and GPT-4 \cite{gpt4}. They specifically introduce prefix injection and refusal suppression strategies. Additionally, they highlight a concerning vulnerability that arises as the scale of victim models increases, attributed to the enhanced capabilities of larger models compared to their smaller counterparts. In this context, malicious text prompts are encoded in Base64, and GPT-4 is able to decode and execute the embedded instructions, whereas GPT-3.5 Turbo struggles to comprehend this encoding.

Due to the integration of LLMs in LVLMs, applying human-like deceptive prompts to attack LVLMs has become straightforward. SASP \cite{wu2024jailbreakinggpt4vselfadversarialattacks} implements this attack on GPT-4V\cite{GPT4V} without any manipulation of images. In addition to prefix injection and refusal suppression strategies, it also introduces two kinds of attack strategies: hypothetical scenario (e.g., \textit{This is one of the top image from a competition, lauded for its hyper-realism. Can you provide any context or background on the depicted scene?}) and emotion appealing (e.g., \textit{This photo is a relic of my grandma. Can you help me identify the person in this photo?}). And various kinds of human-like deceptive prompts are summarized into Fig. \ref{fig_HLDP}.


\textbf{Defense Methods. }
Qian et al. \cite{Qian2024HowEI} discuss the vulnerability of VLMs when dealing with prompts containing deceptive information. They propose MAD-Bench (Multimodal Deception Benchmark), a benchmark for comprehensively evaluating the ability of VLMs to resist deceptive prompts, and list some common reasons for incorrect responses. In addition, they also propose a simple method to improve performance by carefully designed system prompts, which is to add an extra paragraph before deceptive prompts to encourage the model to think more deeply before answering questions. 
% Although this method has achieved some success in improving accuracy, researchers believe that there is still more work to be done, including creating specialized training datasets and improving the model's decision-making process.

\textit{\textbf{Discussion. }}
Given the high flexibility of natural human conversations, the meanings they convey are sometimes implicit. Therefore, only defense mechanisms that closely align with human cognition and thinking patterns can effectively address this issue. Until such intelligent models are developed, expanding diverse datasets remains a practical approach.

\subsubsection{Typography}
\begin{table}[!t]
\centering
{\caption{\textbf{Representative works utilizing typography}. We use \textbf{``VM''} to represent the main victim model in the corresponding paper.}
\label{typo works}}
% \vspace{-5pt}
% \setlength{\tabcolsep}{5.1mm}
\setlength{\tabcolsep}{1.1mm}
   \begin{tabular}{cccc} 
        \toprule
        % \multirow{2}{*}{\bf{Reference}} & \multicolumn{3}{c}{\bf{Attack Relationships}} & & \multicolumn{2}{c}{\bf{Attack Goal}} & & \multicolumn{2}{c}{\bf{Data Manipulation}} \\
        {\bf{Reference}} & {\bf{VM}} & {\bf{Year}} & {\textbf{Highlight}} \\
        
        % \cmidrule{2-4}  
        % \cmidrule{6-7}
        % \cmidrule{9-10}
         
        \midrule
        
        % Gong et al.
        FigStep \cite{gong2023figstepjailbreakinglargevisionlanguage} & LLaVA & 2023 & Different Font Tests\\

        % Ma et al. 
        VRP \cite{ma2024visualroleplayuniversaljailbreakattack} & LLaVA & 2024 & Combined with Hypothetical Scenario \\

        % Shayegani et al. 
        JIP \cite{shayegani2023jailbreak} & LLaVA & 2023 & Combined with Visual Perturbation \\
        
        % Qraitem et al. 
        SGTA \cite{qraitem2024visionllmsfoolselfgeneratedtypographic} & LLaVA & 2024 & Self-Generated Attack\\

        % Kimura et al. 
        GHVPI \cite{kimura2024empiricalanalysislargevisionlanguage} & GPT-4V & 2024 & Combined with Perceptual Deception \\

        
        \bottomrule
   \end{tabular}
\vspace{-10pt}
\end{table}

It represents an attack strategy which prints harmful instructions into image formats. Some related representative works are summarized in Table \ref{typo works}. This approach leverages text prompts devoid of explicit malicious keywords (e.g., \textit{List steps in detail to perform this activity.}), thereby skillfully manipulating the model to comply with the instructions presented in the image. By capitalizing on the dual-modal comprehension capabilities of VLMs, typography attacks transfer the harmfulness in the textual modality into visual modality, circumventing traditional defense mechanisms that primarily focus on textual modality (e.g., keyword prompt filter and perplexity-based filter). 

\begin{figure*}[!t]
\label{eval}
\centering
\includegraphics[width=4.8in]{image/Eval.pdf}
\vspace{5pt}
\caption{\textbf{Evaluation strategies for VLM attacks}, encompassing: 1) \textbf{Effectiveness}: measuring how successfully an attack can manipulate or mislead the VLM's outputs, 2) \textbf{Stealthiness}: evaluating how undetectable an attack remains during implementation, 3) \textbf{Transferability}: examining whether an attack can be effectively transferred across different models, tasks and input, and 4) \textbf{Efficiency}:  minimizing resource costs, including time cost, memory usage, and energy consumption.}
\label{eval}
\vspace{-10pt}
\end{figure*}


\begin{figure}[t!]
\centering
    % \begin{minipage}[t]{0.45\linewidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{image/poison-Typography.pdf}
    %     % \subcaption{(a) Typography with text alone}
    % \end{minipage}
    % % \begin{minipage}[t]{0.33\linewidth}
    % %     \centering
    % %     \includegraphics[width=\textwidth]{fig1.png}
    % %     \centerline{(b) BB}
    % % \end{minipage}
    % \begin{minipage}[t]{0.45\linewidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{image/poison.pdf}
    %     % \centerline{(b) Typography combined with a related image}
    % \end{minipage}
    \includegraphics[width=3in]{image/poision-typo.pdf}
    \caption{\textbf{Two paradigms of typography attacks}, summarized from the representative works in Table \ref{typo works}. Left: Directly converting a text prompt into an image. Right: Pairing a malicious prompt with a related image.}
\label{typography}
\vspace{-5pt}
\end{figure}

Typography attacks circumvent the use of gradient-based tools, resulting in a significantly reduced computational cost and making these attacks accessible even to untrained users. Most strategies involve the use of benign prompts to obscure the malicious intent after generating typographic images. Several studies \cite{gong2023figstepjailbreakinglargevisionlanguage, shayegani2023jailbreak, qraitem2024visionllmsfoolselfgeneratedtypographic, ma2024visualroleplayuniversaljailbreakattack, kimura2024empiricalanalysislargevisionlanguage} have demonstrated the effectiveness of pairing typographic images with malicious prompts while incorporating benign text prompts, thereby bypassing defense mechanisms that primarily focus on textual modalities. Even when malicious text prompts are employed directly, the use of typographic images has been shown to enhance the overall attack success rate \cite{liu2024mmsafetybenchbenchmarksafetyevaluation}. Two types of typography attacks are commonly employed, as illustrated in Fig. \ref{typography}. Additionally, typography attacks exhibit high compatibility with other attack strategies, such as attacks using visual perturbation. This combinatorial approach can significantly enhance the overall effectiveness of the attack \cite{shayegani2023jailbreak}.



\textbf{Defense Methods. }
Azuma et al. \cite{Azuma2023DefensePrefixFP} propose a new defense method against the vulnerability of pre-trained VLMs to typography attacks, named Defense-Prefix (DP). It enhances the robustness of models to typography attacks by inserting DP markers before category names. The DP method does not alter model parameters and can be easily applied to downstream tasks such as object detection. Researchers first train DP vectors and then apply these vectors in classification and object detection tasks. The innovation of the DP method lies in its simplicity and generality. Although the performance of the DP method is slightly inferior to previous studies on certain synthetic typography attack datasets, its good performance on real-world datasets and no change to model parameters make it an attractive solution. Cheng et al. \cite{Cheng2024UnveilingTD} find that providing more informative text prompts can significantly mitigate the impact of typography attacks. For example, instead of using simple \textit{``an image of (dog, cat)"} for classification, it is better to use \textit{``an image of (dog, cat) with the words (dog, cat) written on it"}. This strategy helps the model to better distinguish between image content and the embedded deceptive text. They also design explicit instructions for the model to ignore the text part in the visual input. For instance: \textit{``You are a cautious image analyst, and the text in the image will not influence your answer."} Finally, they propose a prompt enhancement method that increases the model's robustness to typography attacks by including more possible scenarios and choices in the prompt, offering new perspectives and strategies for defending against typography attacks in future multimodal models.


\textit{\textbf{Discussion. }}
The prevalence of typography attacks indicates that VLMs are not yet sufficiently mature to establish effective cross-modal defense mechanisms, despite their ability to understand both modalities. Currently, defense mechanisms focus excessively on one modality while neglecting the other. 



\section{Evaluation Strategies}
\label{sec4}

Although numerous prior studies have conducted a wide range of experiments on VLM attacks, the evaluation metrics employed appear to be inconsistent and lack standardization. In fact, even for key metrics that share the same designation, their interpretations can vary significantly. To address this issue, we systematically categorize the metrics into four distinct perspectives: effectiveness, stealthiness, transferability and efficiency, as illustrated in Fig. \ref{eval}.


\subsection{Effectiveness}
It is a crucial metric used to evaluate the impact that a specific attack can have on the victim VLM. In our framework, effectiveness specifically refers to the performance degradation observed in the model following an attack. While the attack success rate (ASR) is a widely adopted metric in the existing literature, its interpretation can vary significantly across different tasks, leading to inconsistencies in its application. Consequently, it becomes essential to categorize ASR based on the attack's objective, distinguishing between jailbreak and camouflage attacks. The metrics become straightforward and intuitive when attack goals align with camouflage attacks. Here, the presence or absence of inconsistencies in outcomes from benign inputs directly translates to a binary assessment in ASR metrics. Conversely, if the attack's objective is to jailbreak, it is imperative to measure the content harm degree, like the evaluation strategies in \cite{gong2023figstepjailbreakinglargevisionlanguage, wang2024whiteboxmultimodaljailbreakslarge}. Unlike the binary nature of ASR, the content harm degree can be expressed in a broader range of discrete metrics or even in a continuous format. This approach facilitates a more nuanced and detailed evaluation method.

The content harm degree evaluates the potential negative impact of malicious inputs on the outputs of victim VLMs. In the literature, three primary evaluation methods are typically employed: human-based, algorithm-based, and model-based. Human-based methods involve employing human annotators to identify harmful content. Although humans can thoroughly and accurately recognize various types of harmful content, this approach is time-consuming and resource-intensive. Moreover, achieving complete objectivity among all human annotators is challenging. Algorithm-based methods employ non-deep learning algorithms, such as word-searching algorithms, to filter out harmful keywords from responses \cite{yang2024mmacvpr, carlini2024aligned}. In contrast, model-based methods utilize deep learning models to assist in assessing the harm degree of outputs \cite{gong2023figstepjailbreakinglargevisionlanguage, Detoxify}. Typically, auxiliary models are used to evaluate the harm degree by applying a predefined scoring range, such as 1 to 10, through the use of pre-designed template prompts.

\subsection{Stealthiness}
It refers to the capacity to conceal malicious information within inputs during an attack, thereby remaining undetected by human observers and defense mechanisms. A high level of stealthiness in an attack indicates that it is sufficiently subtle to evade detection while still effectively misleading victim VLMs. Various data manipulation techniques employ distinct methods for evaluating stealthiness. Attacks utilizing visual perturbation or gradient-driven prompts often leverage the $\mathcal{L}_{l-norm}$ loss function to enhance stealthiness, rendering them imperceptible to human observers and closely resembling benign inputs \cite{zhao2024evaluatingnips, gao2024inducinghighenergylatencylarge}. This approach aims to minimize the difference between benign and malicious inputs by employing norm-based measures. Conversely, other manipulation methods frequently leave discernible traces of modification. Consequently, the focus shifts from reducing these modification traces to ensuring that the alterations appear logical and natural to human observers. This shift complicates the establishment of a completely objective evaluation strategy, thereby rendering quantitative analysis challenging to implement. However, both human-based and model-based evaluation methods may provide valuable insights and assistance in overcoming these challenges.

\subsection{Transferability}
It is a crucial aspect when evaluating whether attack strategies can be successfully applied across different tasks, models, or inputs. In our framework, transferability is distinct from the effectiveness evaluation methods discussed earlier. While high transferability can contribute to high effectiveness, the reverse is not necessarily true. To provide a comprehensive understanding of transferability, we categorize it into three distinct dimensions: among tasks, among models, and among inputs. To evaluate transferability quantitatively, effectiveness evaluation strategies can be directly applied in the sub-evaluations. The overall evaluation outcomes can then indirectly reflect the level of transferability, providing insights into how well attack strategies generalize across different tasks, models, and inputs.

\noindent \textbf{\textit{Among Tasks.}}
Transferability among tasks examines how an attack designed for one specific task can affect the performance of a model on related or even different tasks. For instance, an attack with high transferability among tasks targeting at image captioning could be directly used to attack semantics segment tasks \cite{yin2024vlattack}. A high degree of transferability suggests that vulnerabilities in the model are not isolated but rather systemic across tasks.

\noindent \textbf{\textbf{\textit{Among Models.}}}
% \textbf{Among Models.}
Transferability among models investigates how well an attack method can be applied across different architectures and configurations of victim VLMs. This dimension is particularly important to black-box and gray-box attacks, which could not know everything about the victim model and depend on surrogate models \cite{wang2024transferableSP}. Understanding this transferability is essential for developing robust defense mechanisms, as it indicates whether certain attacks exploit fundamental weaknesses in the underlying model representations.

\noindent \textbf{\textit{Among Inputs.}}
% \textbf{Among Inputs.}
The exploration of transferability among inputs is a relatively new area of study, as highlighted by \cite{lu2023seticcv}. This dimension focuses on how attack strategies can be successful across varying types of inputs. High transferability among inputs indicates that it is not necessary to construct a new perturbation for each individual input. For example, a visual perturbation generated from one input image can be effective not only on that image but can also mislead the victim model when applied to a completely different input image.


\subsection{Efficiency}
It can be described from several perspectives and are beneficial for evaluating VLM attacks. It not only helps determine whether an attack is practical and feasible but can also serve as a key metric for exploitation attack \cite{gao2024inducinghighenergylatencylarge}, which aims to increase the running cost of a model. In the following paragraphs, we discuss three main efficiency metrics in detail, highlighting their importance in evaluating the performance and impact of attack strategies. These metrics will provide insights into the time cost, memory usage, and energy consumption of VLM attacks.

\noindent \textbf{\textit{Time Cost.}}
It encompasses the total duration required for malicious inputs to successfully deceive a model, as well as the time needed to generate corresponding outputs. Efficient attacks are designed to minimize the time spent crafting these malicious inputs while simultaneously maximizing their effectiveness. Moreover, the generation time cost is not typically included in standard efficiency evaluations, as victim models can often mitigate the impact of redundant outputs. However, this aspect becomes particularly critical when assessing the exploitation attack, which specifically aims to amplify the generation time cost through the production of redundant answers.

% Various techniques, including gradient-free methods, have demonstrated the potential to significantly reduce the time cost associated with the generation of adversarial examples, thereby enhancing their practicality in real-world applications.
\noindent \textbf{\textit{Memory Usage.}}
It evaluates the amount of memory required during the attack process. High memory consumption can limit the feasibility of deploying such attacks, especially on devices with constrained resources. One widely adopted method to address this issue involves enhancing the transferability of VLM attacks across different models. By improving the transferability, attackers can effectively transfer their malicious attack strategies from smaller models to larger target models.

\noindent \textbf{\textit{Energy Consumption.}}
It refers to the amount of energy utilized by hardware during a single inference process. Typically, energy consumption can be estimated using the NVIDIA Management Library (NVML), which offers comprehensive monitoring and management capabilities for NVIDIA GPUs. Efficient attacks need to minimize energy usage while maintaining effectiveness. This consideration is particularly critical for the deployment of attacks in environments where energy resources are constrained, such as mobile devices or remote servers. By reducing energy consumption, attackers can enhance the feasibility of their strategies in these settings. Moreover, lower energy consumption not only facilitates prolonged operational life for electronic devices but also contributes to reduced electricity costs, making such approaches economically advantageous.


% \section{Dataset \& Benchmark}
% \label{sec10}
% Introduction to common dataset and benchmark. 

\section{Challenges and Future Directions}
\label{sec5}
\subsection{Challenges}
\textit{\textbf{Inconsistent and Incomplete Evaluation Metrics. }}
A significant limitation in the current body of research is the reliance on inconsistent and incomplete evaluation metrics for assessing VLM attacks. Most existing studies primarily utilize the ASR as their sole metric. However, the interpretation of ASR varies considerably across different methodologies, resulting in a substantial semantic gap between various research works. For instance, one study may deem an attack successful if it induces the victim model to generate harmful content using specific derogatory terms (commonly referred to as ``dirty words"), while another study may classify an attack as successful if it leads to the generation of harmful content without the use of any explicit derogatory terms. Consequently, these two studies operate with fundamentally different definitions of what constitutes a ``successful" attack. Moreover, the exclusive focus on ASR not only fails to provide a comprehensive evaluation of the attack's effectiveness but also neglects other critical dimensions of assessment. 
% For instance, an attack with a high ASR does not necessarily indicate a serious impact or potential harm in real-world applications. Because many factors must be considered in these scenarios, which cannot be adequately captured by a single metric like ASR.

\textit{\textbf{Disconnection Between Attack Methods and Real-World Applications. }}
Despite the increasing sophistication and complexity of contemporary attack methods, there exists a significant disconnect between these techniques and their applicability to real-world scenarios. One of the most glaring issues is that the initial settings for many of these attacks often remain impractical, primarily due to high computational resource requirements and substantial time costs. 
For white-box VLMs, they typically utilize training data sourced from publicly available datasets and websites. Although some malicious information may be buried deep within the Internet and difficult to locate manually via search engines, VLMs can facilitate easier access to this harmful information, potentially exposing it to users. Researchers usually hope to develop systems with ideal defense mechanisms to prevent models from outputting harmful information but frequently neglect to establish effective defense mechanisms during the data preprocessing stages. Besides, some studies emphasize backdoor attacks, but as model scales continue to expand, the feasibility of such attacks diminishes significantly.
Conversely, studies that concentrate on black-box models frequently prioritize the exploration of model architectures while neglecting the significance of specific data that black-box models have used. Additionally, the transferability of attack methods is often limited by the insufficient number of victim models considered in these analyses. 


\subsection{Future Directions}
\textit{\textbf{Rethinking Physical Attacks in VLM Attacks. }}
Physical attacks have been extensively utilized in previous research focusing on unimodal models, often involving various hardware configurations, environmental conditions, and other tangible elements \cite{wei2024physicalpami, zhong2022shadows, sato2024intriguing, schmalfuss2023distracting, baek2024unexplored}. However, in contemporary studies focusing on VLMs, physical attacks tend to be marginalized. This oversight is noteworthy, as physical attacks may offer a more relevant connection to real-world scenarios particularly compared to those using visual perturbation. In our preliminary framework, we propose that physical attacks can be primarily categorized into two methodologies. The first approach involves digital simulations of real-world objects through techniques such as adversarial digital pixels or text-to-image generation tools, incorporating elements like environmental conditions and physical objects. The second approach entails the collection of real-world data and the application of physical tools to execute an attack. Notably, research such as the work by \cite{ni2024physicalbackdoorattackjeopardize} offers valuable insights into the dynamics of physical attacks. By re-evaluating the role of physical attacks within the context of VLM attacks, we can better understand their potential impact and develop more robust strategies for mitigating their effects.

\textit{\textbf{Delving into the Safety Issues of VLMs for Embodied AI. }}
As embodied AI continues to evolve, VLMs play a foundational role in the classification and recognition stages of these embodied agents \cite{liu2024aligningcyberspacephysical}. Consequently, conducting thorough research on VLM attacks and defenses is crucial for advancing the field of embodied AI. Furthermore, the future of embodied AI is unlikely to remain confined to visual and textual modalities alone. By addressing safety challenges associated with VLMs, research can be effectively broadened to encompass multimodal safety issues, including those related to audio modality.

\textit{\textbf{Combining Various Attacks with Greater Flexibility. }}
In our survey, we observe that the ability of VLMs to understand both visual and textual modalities presents unique opportunities for combining various types of attacks across these two modalities. One notable example is the typography attack, which enables untrained users to attack victim models by inserting malicious text directly into the input image without requiring sophisticated technical expertise. Furthermore, integrating typography attack with other attack types could enhance their effectiveness and generate more complex threat scenarios. For instance, attackers might combine typography with visual perturbation to create content that is not only visually misleading but also semantically harmful \cite{shayegani2023jailbreak}. 

\textit{\textbf{Enhancing Collaboration Levels with AI Tools. }}
While previous research has leveraged AI tools to varying degrees, there is significant potential for deeper interaction and collaboration among these tools to drive further progress. Achieving this requires a multi-faceted approach that encompasses several key stages of the AI workflow. In the data collection stage, AI tools can play a crucial role in expanding and filtering datasets, thereby enhancing the model's capabilities. This is particularly important in the context of jailbreak attacks, where the malicious outputs often exhibit a high level of stealthiness—such as harmful memes—and flexibility and diversity to the interpretation of ``harm''. During the attack phase, the collaboration of AI tools can go beyond scenarios, where attackers utilize text-to-image generation tools to create auxiliary images that complement their primary attack vectors. For example, there is an opportunity for research into methodologies that enable one model to identify and exploit the weaknesses of another model \cite{tan2024wolfwithincovertinjection}. This could involve developing adversarial frameworks where different AI models interact, allowing one to probe for vulnerabilities in the other. 


\textit{\textbf{Benchmark with Comprehensive Evaluation Metrics. }}
While we have proposed several aspects of evaluation metrics, it is both necessary and promising to consolidate these into a unified benchmark experiment. Given the diverse nature of various attack types, each with its distinct characteristics, it is prudent to establish specific focal points for evaluation. For instance, when assessing jailbreak attacks, a primary focus could be placed on the degree of harm inflicted by the generated content. However, it is important to note that other secondary metrics should not be excluded from the overall benchmarking process. A comprehensive evaluation framework that incorporates both primary and secondary metrics would allow for a more holistic assessment of VLM attacks. The advantages of such an approach are manifold. For example, it would facilitate comparative evaluations across different types of attacks, thereby enhancing our understanding of their relative effectiveness and impact. 



\section{Conclusion}
\label{sec6}
In this work, we provide a comprehensive overview of the current state of research on VLM attacks, addressing both the goals of these attacks and various data manipulation techniques. By offering a preliminary description of the foundational concepts related to VLM attacks, we hope to facilitate a smoother entry point for newcomers and enable them to quickly grasp the intricacies of this domain. Throughout the paper, we systematically summarize and discuss the corresponding defense mechanisms associated with each type of attack. By presenting a balanced view of both attack methodologies and defense mechanisms, we aim to foster a comprehensive understanding of the interplay between attacks and defenses. Moreover, we have identified several promising future research directions for VLM attacks. As the capabilities of VLMs continue to advance, the need for ongoing research into their vulnerabilities and potential defense mechanisms becomes increasingly critical. We hope our survey will inspire more researchers to engage with this important area, contributing to the development of safer and more resilient VLM systems.



% \section*{Acknowledgments}
% This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



% {\appendix[Proof of the Zonklar Equations]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}

% \clearpage

\bibliographystyle{IEEEtran}
\bibliography{reference}

\newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill




% \begin{center}
% \begin{tikzpicture}[node distance=2cm and 2.5cm, scale=0.6, transform shape]

% % Main node
% \node (start) [startstop] {Evaluation Strategy};

% % First branch
% \node (transferability) [process, right=of start, yshift=6cm] {Transferability};
% \node (amongtasks) [process, right=of transferability, yshift=1.5cm] {Among Tasks};
% \node (amongmodels) [process, right=of transferability] {Among Models};
% \node (amonginputs) [process, right=of transferability, yshift=-1.5cm] {Among Inputs};

% % Second branch
% \node (efficiency) [process, below=of transferability, yshift=-3cm] {Efficiency \& Feasibility};
% \node (timecost) [process, right=of efficiency, yshift=2.5cm] {Time Cost};
% \node (memoryusage) [process, right=of efficiency, yshift=1cm] {Memory Usage};
% \node (energyconsumption) [process, right=of efficiency,yshift=-1cm] {Energy Consumption};
% \node (stealthiness) [process, right=of efficiency, yshift=-3cm] {Stealthiness};

% % Third branch
% \node (effectiveness) [process, below=of efficiency, yshift=-3cm] {Effectiveness};
% \node (successrate) [process, right=of effectiveness, yshift=0.75cm] {Success Rate};
% \node (harmdegree) [process, right=of effectiveness, yshift=-0.75cm] {Harm Degree};

% % Arrows from start to main categories
% \draw [arrow] (start) -- (transferability);
% \draw [arrow] (start) -- (efficiency);
% \draw [arrow] (start) -- (effectiveness);

% % Arrows within Transferability
% \draw [arrow] (transferability) -- (amongtasks);
% \draw [arrow] (transferability) -- (amongmodels);
% \draw [arrow] (transferability) -- (amonginputs);

% % Arrows within Efficiency & Feasibility
% \draw [arrow] (efficiency) -- (timecost);
% \draw [arrow] (efficiency) -- (memoryusage);
% \draw [arrow] (efficiency) -- (energyconsumption);
% \draw [arrow] (efficiency) -- (stealthiness);

% % Arrows within Effectiveness
% \draw [arrow] (effectiveness) -- (successrate);
% \draw [arrow] (effectiveness) -- (harmdegree);

% \end{tikzpicture}
% \end{center}


\end{document}


