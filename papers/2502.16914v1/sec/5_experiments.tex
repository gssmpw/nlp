\section{Experiments} \label{experiments}
In this section, we detail the experimental setup used to evaluate the performance of the \textit{ENACT-Heart}. The overall workflow of this process is summarized in Fig. \ref{fig:flowchart}, which outlines the key steps from data preparation to the final ensemble prediction.

\subsection{Dataset Used}
The heart sound dataset provided in the PASCAL Classifying Heart Sounds Challenge was used for training and testing the models \cite{bentley_pascal_nodate}. The audio files in the dataset vary in length, ranging from 1 second to 30 seconds.

\input{figures/flowchart}

\subsection{Data Preprocessing}
\textbf{Creating Dataframe.}
The PASCAL dataset consists of two main folders, each containing labeled audio files. These folders were combined into a single dataframe, which includes the file paths and corresponding labels. An exploratory data analysis (EDA) was performed to understand the distribution of classes and identify any potential issues such as missing or mislabeled data.

\textbf{Normalizing.}
To address the inconsistent length of audio recordings, a preprocessing step was implemented. Each audio file was segmented into 5-second clips. If an audio file was shorter than 5 seconds, it was zero-padded to meet the required input length.

\textbf{Data Augmentation.}
To increase the dataset size and introduce variability, Gaussian noise was added to the audio files, generating 9 additional noisy versions for each original file. Gaussian noise with a mean of 0 and a standard deviation of 0.1 was added to the audio files to generate augmented data. Each audio file was segmented into 5-second clips to standardize the input length for model training. This resulted in a total of 10 versions per audio file (1 original + 9 augmented). The augmented audio data was included in the dataframe, and corresponding labels were updated to reflect the augmentation process.

\subsection{Audiovisual Image Data Generation}
Two types of visual representations were generated for each audio file: spectrograms and centroid graphs.

\textbf{Spectrogram.}
Spectrograms were generated to visualize the frequency content of the audio files over time. Given the presence of various background noises in real-world conditions, a low-pass filter set at 195 Hz was applied. This filter helps to emphasize the cardiac sounds, which predominantly occur in the lower frequency range, while reducing noise from higher frequencies. The spectrogram images were saved with dimensions that matched the input requirements of the models.

\textbf{Centroid of Amplitude Visualization.}
The spectral centroid, representing the "center of mass" of the spectrum, was calculated for each audio file. This metric provides a concise representation of where the majority of the spectral energy is concentrated. A centroid graph was generated, overlaying the normalized waveform and centroid values. This combined visualization provided a robust input for the CNN model. The purpose of this approach was twofold:
\begin{enumerate}
    \item The waveform contains all the detailed information of the audio signal, capturing every nuance and variation.
    \item The spectral centroid highlights the important features of the signal by indicating where the audio information is concentrated (i.e., beats). This simplification helps the CNN model to more easily pick out relevant patterns, enhancing its ability to classify the recordings accurately.
\end{enumerate}

\subsection{Model Training}
\textbf{ViT.}
The ViT model was trained using the generated spectrogram images. The training process involved splitting the data into training and validation sets, followed by model training with appropriate hyperparameters such as batch size of 32 and 50 epochs, using an Adam optimizer with a learning rate of 0.001. Data augmentation techniques, such as random noise addition, were applied to improve model robustness.

\textbf{CNN.}
The CNN model was trained using the centroid graphs. The CNN model architecture included three convolutional layers followed by max-pooling and dropout layers. The data was split into training and validation sets, and the model was trained with optimized hyperparameters, including a batch size of 32 and 50 epochs, using an Adam optimizer with a learning rate of 0.001. The CNN model also benefited from the data augmentation techniques applied during preprocessing.


\subsection{Ensemble Method}
To leverage the strengths of both the ViT and CNN models, the MoE ensemble method was employed. The ensemble model combined the predictions from both models by assigning different weights, $w_{\text{ViT}}$ and $w_{\text{CNN}}$, to each model's predictions. Specifically, weight combinations were systematically tested, with $w_{\text{ViT}}$ ranging from 0 to 1 in increments of 0.05, and $w_{\text{CNN}} = 1 - w_{\text{ViT}}$.

The ensemble prediction $P_{e}$ was calculated using the following equation:
\begin{equation}
\begin{split}    
P_{\text{ensemble}} & = w_{\text{ViT}} \times P_{\text{ViT}} + w_{\text{CNN}} \times P_{\text{CNN}}, \\
\text{where } w_{\text{ViT}} & = 0.05k \\
w_{\text{CNN}} & = 1 - w_{\text{ViT}} \\
k & \in [0, 20] \cap \mathbb{Z}
\end{split}
\end{equation}
\\