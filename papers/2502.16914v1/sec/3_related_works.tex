\section{Related Works} \label{related_works}
\subsection{Thematically Related Works}
Thematically, the proposed method lies in the field of computer-assisted diagnosis (CAD) systems for heart disease. CAD systems for heart diseases utilize computation techniques such as machine learning, pattern recognition, and AI to analyze cardiac data and provide decision-support tools for healthcare providers.

There have been numerous attempts to apply CAD systems for heart diseases in diverse modalities, including but not limited to ECG, cardiac CT/MRI, etc. These systems analyze cardiac data to identify abnormalities and patterns - especially indicators of certain heart diseases. However, the direction of the majority of these researches are pointed mainly toward computer vision over audio AI, mainly due to the advanced deep learning models available.

For some researches that emphasized sound classification, its methodologies have varied slightly from the approach proposed in this study. For instance, Jumphoo et al. utilized a CNN for feature extraction and Data-efficient Image Transformer (DieT), a variant of the ViT model, for classification tasks through stacking \cite{jumphoo_exploiting_2024}. Another heart sound classification model, proposed by Liu et al., also uses ViT for classification but employs a different image modality called bispectral patterns and relies solely on ViT without integrating other models \cite{liu_heart_2023}. While these studies highlight the effectiveness of ViT individually, they do not explore the potential benefits of using an ensemble approach.

Overall, the integration of multiple distinct AI models and modalities from the same sound inputs, as proposed in the \ENACT using an MoE approach, has not been attempted yet. This novel methodology leverages the strengths of both ViT and CNN models, potentially offering a more robust and accurate solution for heart sound classification.

\subsection{Methodologically Related Works}
The use of computer vision as a tool for machine hearing is an emerging approach. There have been attempts to use computer vision techniques as a method of machine hearing - analyzing audio signals by treating them as visual data. 

Hsu \textit{et al.} \cite{hsu_deep_2021} presented a deep learning-based music classification through mel-spectrogram and Fourier tempogram features. Although the concept of using multiple different audiovisual modalities and models from singular sound data is there, the paper employed the short-chunk CNN + ResNet as the backbone architecture of their models.
