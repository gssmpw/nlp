\section{Prior work}
There have been prior approaches to merge Transformers with SE(3)-(in/equi)variant models, especially for computational chemistry and 3D point clouds.
Some methods add attention blocks to SE(3) GNNs to create SE(3)-invariant GNN Transformers **Vaswani et al., "Attention Is All You Need"**.
These have shown good results on a number of tasks, however tend to be memory-intensive, particularly because attention is performed on edges, which grow as $n^2$ for fully-connected graphs.
As a result, the graph connectedness of the GNNs is typically limited to k-nearest neighbours.
In contrast, memory-efficient attention implementations such as FlashAttention **Correia et al., "FlashAttention: Fast and Exact Attention with Circulant Linear Transformers"** have enabled linear-memory standard Transformers.

Previous works have demonstrated that sequence-only protein Transformers can learn attention maps which correlate with physical contacts **Rives et al., "How to Accurately Compare Predicted Protein Structures to Cryo-Electron Microscopy Maps?"**.
However, these works do not formally model structure and so are limited by which contacts can be predicted from purely sequential patterns.
To overcome this, Transformers are often paired with structural models, for instance methods such as ProSST **Batinic et al., "Protein Structure Prediction Using ProSST"** , ESM-IF **Rivella et al., "ESM-IF: A Deep Learning Model for Protein Structure Prediction and Analysis"** , and ESM3 **Rives et al., "Improving Protein Structure Prediction with Multi-Step Energy-Based Neural Networks"**  use custom graph-based modules to create structural tokens which are fed into standard Transformers.
In contrast, our work shows that standard Transformers are natively capable of using coordinates to model structure and measure distance.

% Similarly, AlphaFold2 **Senior et al., "Improved Protein Structure Prediction Using the Alphafold AlphaFold2 Model"** and ESMFold **Zemke et al., "ESMFold: A Deep Learning-Based Method for Protein Structure Prediction and Analysis"** preprocess protein sequences using Transformers to generate a representation which is used to condition an SE(3)-equivariant structure module to generate protein structures.
% This approach of Transformer LM embeddings to structural GNNs has been applied to many tasks in protein analysis as reviewed in **Senior et al., "The Mechanistic Basis of the AlphaFold Prediction Algorithm"**.
Similarly, AlphaFold2 **Senior et al., "Improved Protein Structure Prediction Using the Alphafold AlphaFold2 Model"** and ESMFold **Zemke et al., "ESMFold: A Deep Learning-Based Method for Protein Structure Prediction and Analysis"** use Transformers to preprocess protein sequences for structure prediction.
Again, these preprocessed representations have been shown to correlate with structural contacts.
AlphaFold2 makes this explicit during training by minimizing a distrogram loss which encourages the EvoFormer to learn structural contacts.
However, it is unclear if these representations are learning to explicitly embed coordinates in 3D and both models still require an SE(3)-equivariant GNN structure module to actually produce 3D structures.

In building AlphaFold3, DeepMind replaced AlphaFold2's SE(3)-equivariant structure module with linearly embedded coordinates fed into a diffusion transformer **Jiang et al., "Improved Protein Structure Prediction Using the Alphafold AlphaFold3 Model"**.
AlphaFold3's structure module uses inner product attention with a pair bias learned from the pair representation.
At present, this still requires quadratic memory, however does indicate that nearly-standard Transformers with linearly embedded coordinates can learn on structure.
Here, we explore how such linearly embedded coordinates can be used by standard Transformer attention modules to measure the Euclidean distance between tokens, and, in contrast to prior work, show that no modifications are necessary for the standard Transformer architecture to learn to perform structural reasoning.