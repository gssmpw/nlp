\section{Prior work}
There have been prior approaches to merge Transformers with SE(3)-(in/equi)variant models, especially for computational chemistry and 3D point clouds.
Some methods add attention blocks to SE(3) GNNs to create SE(3)-invariant GNN Transformers \citep{fuchs_se3-transformers_2020, liao_equiformer_2023}.
These have shown good results on a number of tasks, however tend to be memory-intensive, particularly because attention is performed on edges, which grow as $n^2$ for fully-connected graphs.
As a result, the graph connectedness of the GNNs is typically limited to k-nearest neighbours.
In contrast, memory-efficient attention implementations such as FlashAttention \citep{dao_flashattention_2022, dao_flashattention-2_2023} have enabled linear-memory standard Transformers.

Previous works have demonstrated that sequence-only protein Transformers can learn attention maps which correlate with physical contacts \citep{lin_evolutionary-scale_2023, vig_bertology_2020}.
However, these works do not formally model structure and so are limited by which contacts can be predicted from purely sequential patterns.
To overcome this, Transformers are often paired with structural models, for instance methods such as ProSST \citep{li_prosst_2024}, ESM-IF \citep{hsu_learning_2022}, and ESM3 \citep{hayes_simulating_2024} use custom graph-based modules to create structural tokens which are fed into standard Transformers.
In contrast, our work shows that standard Transformers are natively capable of using coordinates to model structure and measure distance.

% Similarly, AlphaFold2 \citep{jumper_highly_2021} and ESMFold \citep{lin_evolutionary-scale_2023} preprocess protein sequences using Transformers to generate a representation which is used to condition an SE(3)-equivariant structure module to generate protein structures.
% This approach of Transformer LM embeddings to structural GNNs has been applied to many tasks in protein analysis as reviewed in \citet{wu_integration_2023}.
Similarly, AlphaFold2 \citep{jumper_highly_2021} and ESMFold \citep{lin_evolutionary-scale_2023} use Transformers to preprocess protein sequences for structure prediction.
Again, these preprocessed representations have been shown to correlate with structural contacts.
AlphaFold2 makes this explicit during training by minimizing a distrogram loss which encourages the EvoFormer to learn structural contacts.
However, it is unclear if these representations are learning to explicitly embed coordinates in 3D and both models still require an SE(3)-equivariant GNN structure module to actually produce 3D structures.

In building AlphaFold3, DeepMind replaced AlphaFold2's SE(3)-equivariant structure module with linearly embedded coordinates fed into a diffusion transformer \citep{abramson_accurate_2024}.
AlphaFold3's structure module uses inner product attention with a pair bias learned from the pair representation.
At present, this still requires quadratic memory, however does indicate that nearly-standard Transformers with linearly embedded coordinates can learn on structure.
Here, we explore how such linearly embedded coordinates can be used by standard Transformer attention modules to measure the Euclidean distance between tokens, and, in contrast to prior work, show that no modifications are necessary for the standard Transformer architecture to learn to perform structural reasoning.