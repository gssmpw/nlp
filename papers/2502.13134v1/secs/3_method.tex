% \begin{abstract}
%     Humanoid robots have shown success in locomotion and manipulation. Although these are great basic abilities, humanoids are still required to quickly understand human instructions, and react based on human interaction signals, if we would like them to assist humans in daily life.
%     Unfortunately, most existing works focus on solely human intention recognition or specific ability learning, often treating these tasks separately, which prevents real-time interaction.
%     In this work, we plan to bring the real-time reaction ability to humanoid robots, allowing humans to interrupt robots at any time, and making robots respond to humans immediately.
%     In this work, we aim to equip humanoid robots with the ability to react instantly to human signals, enabling continuous human-robot interaction without delays.
%     % Methodology
%     To support such abilities, we propose a general humanoid-human-object interaction framework, named \our, i.e., Real-time Humanoid-human Interaction and Object manipulation.
%     \our provides a unified view of reactive motion, instruction-based manipulation, and safety concerns, over multiple human signal modalities, such as languages, images, and motions.
%     \our is a hierarchical learning framework, enabling humanoids to learn reaction skills from human-human-object demonstrations and teleoperation data.
%     % 1) object manipulation skills from teleoperation datasets, 2) reactive motion skills and 3) human intention from human-object-human interaction datasets.
%     In particular, it decouples the interaction process into two levels: 1) a high-level planner inferring human intentions from real-time human behaviors; and 2) a low-level controller achieving reactive motion behaviors and object manipulation skills based on the predicted intentions.
%     We evaluate the proposed framework on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in various scenarios.
%     We believe our framework will bring robots, especially humanoids, closer to our daily lives.
% \end{abstract}

\section{\our Framework}
% \our is designed to learn real-time humanoid-human-object interaction with human demonstrations. 
In \our, the humanoid robot acts as the follower and learns to predict the human intention $I$ with the reactive planner, then utilizes the corresponding skill $K$ to finish the interaction.
Those skills are classified into interactive motion, manipulation, and idle. 
Interactive motion skills, simply called motion skills, enable the robot to react to the leader's intentions with real-time motion. 
Manipulation skills enable the robot to handle objects based on the predicted intentions. 
Idle refers to the robot maintaining its joints in a default state.
\Cref{fig:formulation} illustrates the framework of \our, and \Cref{fig:method} provides the detailed network architecture of each sub-module in our implementation.
% the learning implementation of sub-modules.
% We deploy \our on a real-robot platform in two different scenarios, a restaurant waiter and an office assistant.

% Scenario 1: robot as a restaurant waiter, and Scenario 2: robot as a office assistant.
\input{figs/3_2_method.tex}

\subsection{Data Collection}
\noindent\textbf{Human-object-human interaction data.}
To learn the interaction between humans and robots, we first collect a dataset of human-object-human interaction~\cite{zhang2024core4d}, where two people perform a series of daily interaction tasks with various objects.
% advantage to human-robot interaction data: 
% 1. can be collected without a real robot, is 
%  to collect and can potentially be scaled to different tasks and scenarios.
% 2. more general and have potential to be retarget to different robots.
In comparison to human-robot interaction data, human-object-human interaction data can be collected without a real robot, which is cheaper to collect and easier to scale to more skills in various of scenarios.
% TODO: move the leader-follower description to the formulation section.
The dataset contains interaction between two people in two different scenarios, dining and office. 
The dataset is recorded with a simple motion capture system, and a stereo RGB-D camera in the first-person view of the follower. 
The motion capture system that collects the follower's behavior is described in \Cref{app:mocap}.
Motion data is retargeted to the humanoid robot and used by imitation learning algorithms to construct the reactive motion skills.
% \jingxiao{Add details of retarget.}
The stereo RGB-D camera records the leader's behavior $\mathcal{H}$ and the environment state $\mathcal{E}$, which is used to predict the leader's intention $I$.

We label each frame $t$ in the interaction data with the leader's intention $I_t$ and the follower's skill $K^{(t)}$, which are represented as ID integers of intentions and skills. We add additional labels for the occupancy $p \in \mathcal{P}$ of the robot, 
% identifying if the end-effector, i.e. hands, is interacting with an object, or is empty.
indicating whether the end-effector (i.e., the hands) is empty or interacting with an object, with distinct labels assigned to different objects.
% The leader's intention is used to predict the next action of the follower. The follower's behavior is used to learn the reactive motion skills. 
% The environment information includes the hand occupancy of both people, described as the object ID if the hand is interacting with an object, or $0$ if the hand is empty. 
% The environment information is used to predict the leader's intention based on the hand occupancy of the leader.

\noindent\textbf{Teleoperation data.}
Different from the interactive motion skills, 
% the manipulation skills require more precise control of the robot's end-effector.
certain skills, such as picking up a cup, require more precise control of the robot's end-effector and the manipulation of some objects.
To ensure the success of those skills, 
we collect demonstrations with a teleoperation system~\cite{cheng2024tv}, where the human's motion is captured with a VR device, and the robot's joint positions are set by retargeting the human's motion.
This system records the control commands, the robot's proprioception, and stereo videos from a camera on the robot's head to perceive the environment.
We also label the frames where the skill showcase is completed successfully, referred to as the success signal, which is used to learn the finish condition of the manipulation skills.

\input{figs/3_4_data.tex}

\Cref{fig:data} shows information collected in the interaction and teleoperation dataset and the requirement of real robot deployment. More details are described in \Cref{app:implementation}.

\subsection{Reactive Planner}
\label{sec:reactive-planner}
% Model Structure
% - Model input: Zed Input(body, hand, obj) & Hand Occupancy
% - Model output: Human Intention
The reactive planner is designed to infer the leader's intention $I_t$ from the real-time observation $O_t$ and decide the next skill $K^{(t+1)}$ of the robot.
The planner is a Transformer model, which takes the human's motion and the environment information as input, and predicts the leader's intention at a 30Hz frequency.
% We define the leader's intention as an instruction for the robot to perform a specific skill, such as \texttt{pick up a can}, \texttt{stamp a file}, \texttt{shake hands}, or \texttt{idle}, which is represented as an integer $I_t$ at time step $t$.
To enhance the generalization of the model, we do not input observed images directly, but extract the human's body and hand postures, the human's hand and head position, and the nearest object to hands from the RGB-D images, as well as the robot's hand occupancy $p_t$.
% At each time step $t$, we extract the human's body and hand posture, $M_t$ and $H_t$, the bounding box of objects and hand $B^i_t \in D^3$, and the human's head pose $P_t$.
We retarget human hand postures to a robot hand with 6 degrees of freedom (DoF) and represent the hand posture as the position of each joint.

% Switch of different skills.
There are two types of skills that correspond to the leader's intentions: motion skills and manipulation skills. Each skill $K$ has a start condition $s_K \in \mathcal{P}$ and an end transition $e_K \in \mathcal{P}$. 
The start condition shows the required hand occupancy of each hand to start the skill. For example, the skill to cheer with the leader requires the humanoid to hold a can of drink in the right hand.
The end transition determines the change of hand occupancy after finishing this skill successfully. For example, for the skill of picking up a can, the start condition and end transition are \texttt{[empty, empty]} and \texttt{[empty, can]} respectively.
A comprehensive description of all skills is shown in \Cref{app:skills}.

The humanoid robot starts from an idle state. If the reactive planner predicts a human intention $I$ consistently for $n_{r}$ time steps, the humanoid robot switches to the corresponding skill $T=f(I)$.
The human intentions are meant to ``start'' the execution of a skill, rather than ``keep'' the current execution. For example, the leader only needs to point at the can for a while at the beginning to get the robot to pick the can on the table, instead of pointing all the time. 

After a skill is initiated, the motion skill persists until a change in human intention occurs, while the manipulation skill persists until the execution is judged successful or exceeds a time limit.
% or fails to succeed for too long. 
When a skill is complete, the robot returns to the idle state.

% Skill Cancel
To enable low-latency interaction, the application of most of the skills can be interrupted by another skill when a different human intention lasts $k_2$ steps. 
The motion skills can be easily undone by immediately moving to an idle pose, while the interruption of the manipulation skills is complicated as it requires reversing the object state.
We use a corresponding reverse skill to interrupt each interruptible manipulation skill. For example, the skill of placing the can is a reversal of the skill of picking the can.
% We show the detailed rules of transition between skills in ~\Cref{algo:planner}.
We show the detailed transitions between skills in \Cref{app:implementation}.

% Long-time Planning
When the current occupancy is not satisfied with the start condition of a skill $p_t \neq s_K$, the skill is not able to start.
To satisfy the requirement, we 
We build a directed graph of occupancy transition. The node $n\in \mathcal{P}$ is hand occupancy and the edge $e\in \mathcal{T}$ is skills. 
% Before start a skill with unsatisfied condition, we find the shortest path from current occupancy to the start condition, and execute the skill of path in order. (Polish this)
Before starting to demonstrate a skill $K$ with an unsatisfied condition, we find the shortest path from current occupancy $p_t$ to the start condition $s_K$, and execute the skill series $\{K_1, K_2, \ldots  \}$ in order.
After the operations mentioned above are done, the target skill $K$ can be utilized. The occupancy graphs of 2 scenarios are shown in~\Cref{app:implementation}.

\subsection{Interactive Motion Skills}
\label{sec:inter-motion-skill}
In humanoid-human interactions that do not involve complex object manipulation, the primary objective of the humanoid robot is to produce smooth, consistent motions while providing robust real-time feedback on human behavior. 
To accomplish this, we employ a multi-body motion diffusion model~\citep{liang2024intergen} to generate low-level interactive motion skills. 
% While our model structure and training pipeline are mainly borrowed from InterGen~\citep{liang2024intergen}, we made some critical changes on the input and output sides to make it suitable for our human-humanoid interaction schema. 
% The original InterGen is proposed for the symmetric prediction of two-person future motion trajectories, while in our setting, the humanoid and human are heterogeneous and asymmetric. Specifically, we modify the model output to only include the motion trajectories of the humanoid robot.

Different from multi-person motion generation, the humanoid and human are heterogeneous and asymmetric in the humanoid-human interaction.
We represent the human motion $m^1_t$ as a 6D rotation vector for each joint, and the humanoid motion $m^2_t$ as the target of humanoid robot joint positions. Both motions are simplified to arm and hand joints. We also add hand occupancy $p_t$ and human intention $I_t$ as input to the model, to ensure the robot's motion is consistent with the human's intention. Details can be found in \Cref{app:implementation}.

Our model predicts the future motion of the humanoid robot $m^2_{t+1:t+5}$ based on the history of human motion $m^1_{t-30:t}$ and humanoid motion $m^2_{t-30:t}$.
The model predicts 5 future frames of humanoid motion with a 3 Hz frequency, which generates 30 frames of motion in one second.

The network structure is a Transformer-based model, which takes the loss of reconstructing the humanoid motion as the main loss, and the velocity loss of motion as an auxiliary loss.
% We represent human motion in the SMPL format~\citep{SMPL:2015}, where each body joint is encoded as a 6D rotation vector. %Skinned Multi-Person Linear Model
% For humanoid motion representation, we simply use joint positions. The hand motions of both the human and the humanoid are simplified to 6 degrees of freedom (DoF) per hand, treating joints on the same finger as coupled joints.
% In addition, certain motions may involve objects held in the hand. To address this, we incorporate an extra input dimension to indicate whether the hand is holding an object and, if so, specify the object ID.

\subsection{Manipulation Skills}
\noindent\textbf{Imitation learning.}
To enhance the smoothness and robustness of interactive motion reactions, the motion generation model described in Section~\ref{sec:inter-motion-skill} omits RGB images as input. However, this design makes the model insufficient for dexterous object manipulation, resulting in a lower success rate in practice. 
Meanwhile, as mentioned earlier, the retargeting inevitably introduces deviations between the humanoid’s end-effector poses and original human motions, further leading to manipulation failures.
% Furthermore, the input of human motions is less critical in humanoid-object manipulations. 

As a result, we train independent Action Chunking Transformer (ACT)~\citep{zhao2023learning} models for each low-level manipulation skill. 
The ACT model enables 30Hz real-time inference of the robot's joint positions.
Demonstrations collected by teleoperation are manually segmented and labeled as distinct skills for model training.
To satisfy low-latency requirements, we trained a paired reverse skill model for each manipulation skill, enabling the robot to handle human interruptions properly.

\noindent\textbf{Learning terminal conditions.}
In our multi-skill interactive manipulation framework, the model must recognize when a current skill is completed in order to transition to the next skill. 
% Second, certain skills can be interrupted by human intervention, requiring the robot to ``reverse" the interrupted skill and return to a rest pose. 
In addition to desired humanoid joint positions, each manipulation policy predicts an additional success signal. The signal is an indicator of whether the skill is completed, and we add an extra cross-entropy loss to train the 0/1 classification.
% and whether the skill can be interrupted at the current step. 
% For interruptible skills, we train a paired reverse skill model by collecting corresponding demonstrations, enabling the robot to handle human interruptions properly.

\noindent\textbf{Robust and safe manipulation.}
We crop the image to the region of robot-object interaction as the manipulation model input. The cropped image input removes the leader human's body and only keeps the hand information, which helps the model focus on the manipulation skill and be robust to human appearance and behavior changes.
For skills with a single arm, the input/output of the model only includes the corresponding arm information.
We also collect in-skill interruption data, where the robot pauses or withdraws its current movement if it collides with the human or the target object is unreachable.
Such data helps the robot to exhibit safe behavior and in-skill reflection to the change of human behavior or environment, even if the human intention is not changed.

\subsection{Safety Supervisor}

A safety supervisor serves as a global guarantee of safe robot actions, which forces the robot to pause immediately when potential harm is detected.
In this module, the collision box of the robot is calculated based on several selected key points on the arms.
They are updated by forward kinematics as the movement of the arm.
Meanwhile, global coordinates of human hands are obtained by the depth camera.

The safety module judges whether the robot collision box is to collide with human hands. 
In case the distance between them is too close, the safe supervisor sends an unsafe signal to halt the robot at the current pose until the distance recovers into a safe range.
We use the Euclidean distance from human hand key points to robot arm key points as a simple but effective approach to calculating the collision box. 

The safe supervisor takes strong aids to ensure the robot would not hurt humans by avoiding collision, especially in skills where they should contact at a rather close distance such as handshake and handover the plate.

\subsection{Real Robot Platform}
\input{figs/3_3_robot.tex}

\label{method:real-robot}

% \input{tabs/4_0_planner.tex}
We implement \our on a Unitree H1 humanoid robot with an active head, equipped with a RGB-D stereo camera ZED mini, illustrated in~\Cref{fig:humanoid}.
The human body posture is detected from a body detection model in ZED API with 38 joints and represented as 6D rotation vectors of joints. 
We capture the leader human's intention and the state of objects from the RGB-D images.
The hand posture is detected by a hand reconstruction model, HaMeR~\citep{pavlakos2024reconstructing}.
We record the information of nearest objects to hands, including the object ID, distance to the hand, and the IOU of the object bounding box and the hand bounding box.
The 3D bounding box of objects and hands is detected from a stereo RGB-D camera, ZED-mini, with a fine-tuned YoloV11~\citep{khanam2024yolov11} model and a body detection model in ZED API. 
% The RGB-D stereo camera enable estimate the 3D position of the objects and hands, which is represented in a WORLD coordinate system. 
The RGB-D stereo camera enables retrieval of the 3D position of any 2D pixel in the image, represented in world coordinates. The orientation of the humanoid's body determines the X and Y axes, while gravity defines the Z axis.