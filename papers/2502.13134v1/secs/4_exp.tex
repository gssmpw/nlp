\input{tabs/4_0_planner.tex}
\section{Experiment}
In experiments, we evaluate the performance of \our on two different scenarios: a dining waiter scenario and an office assistant scenario, also called Scene 1 and Scene 2. The details of the skills in each scenario are shown in ~\Cref{app:skills}. The robot should react with its arms, hands, and active head. 
We first evaluate the performance of each module in \our, including human intention prediction, motion generation, and manipulation skills. 
To show the effectiveness of our framework, we compare it with end-to-end models on different numbers of skills. 
The results show that all modules in \our perform well in the skills. The framework outperforms the end-to-end models and is more robust to out-of-distribution data.
We also analyze the failure of the system and show the crucial challenges in the real-world deployment of humanoid robots.

\subsection{Framework Performance}
\noindent\textbf{1) Human Intention Prediction}

We evaluate the performance of our human intention prediction module by calculating the mAP score on datasets. 
We split the training data into 80\% for training and 20\% for validation, and also collected a test set under the robot-human-interaction deployment setting.
The test set contains 3 different people, each of whom performs all intentions of 2 scenarios, which ensures the diversity of the test set.
\Cref{tab:planner} shows that although deploying the model to the real world 
leads to a decrease in performance, but our model still outperforms all baselines.

Our method takes both \textit{human motion} and \textit{hand details} including hand 3D position and object nearest to hand as input. Compared to the baseline that only inputs human motion, our method performs better in our test set, demonstrating that \textit{hand details} is necessary information to differentiate between different human intentions. We also test the performance of VLMs on human intention prediction. Qwen2-VL-2B-Instruct can infer at a frequency of 30Hz, but its performance is poor even after being finetuned on our training set, probably due to a relatively small amount of training data. While GPT-4o-mini can perform quite well on our test set, it takes too long for an inference which leads to a slow reaction of the robot. 
When calculating mAP for the VLMs, we assume that the probability of the class output by VLM is 1, while the probabilities of all other classes are 0.

\input{tabs/4_1_motion.tex}
\noindent\textbf{2) Motion Generation}
% \textbf{Exp2: Motion Generation.}

We compare our motion generation module with three baselines on all the motions involved in our skills (\textit{handshake}, \textit{wave}, \textit{cheers}, \textit{thumbup}, \textit{spread hand}, \textit{take photo}). The baselines are:
\begin{itemize}[leftmargin=*]
    \item \textbf{Zero Velocity}: the repetition of the last pose observed, as a simplest baseline.
    \item \textbf{Ours (w/o diffusion)}: Generate the motions directly with a Transformer model with the same structure as the denoiser used in our Diffusion-based method, without the full diffusion framework.
    \item \textbf{Ours (w/o human motion)}: Generate humanoid motions only conditioning on the human intention label and history of humanoid motions, without the guidance of detailed human motion.
\end{itemize}

The metrics to evaluate the performance of the motion generation modules are:
\begin{itemize}[leftmargin=*]
    \item \textbf{FID}: 
    The FID score~\cite{heusel2017gans} is leveraged to assess the similarity between synthesized and real motions quantitatively.
    % \item \textbf{Joint Error}: 
    % The Joint Error is calculated directly by averaging the distance between generated joint position and the ground truth over all joints involved.
    \item \textbf{JPE}: 
    We calculate the Joint Position Error (JPE) based on the forward kinematic results of the generated robot joint 3D position to measure the poses of all the individuals. JPE is averaged over all hand joints and finger joints.
    \item \textbf{Diversity}: 
    We calculate the average Euclidean distances of $300$ randomly sampled pairs of motions in latent space to measure motion diversity in the generated motion dataset. The Diversity of motions generated by the model is expected to be closer to the Diversity of Real Data.
    \item \textbf{MModality}: 
    MModality captures the ability of the model to generate diverse motions for the same human intention label and human motion sample.
    We sample 20 motions within one fixed human intention label and one fixed sample of human motion to form 10 pairs, and measure the average latent Euclidean distances of the pairs. The average overall human intention and human motion pairs are reported. 
    % Note that the baseline \textbf{Ours (w/o diffusion)} will always yield the same output for fixed inputs and thus do not possess MModality.
\end{itemize}

~\Cref{tab:motion} shows the performance of the motion generation module, where \our significantly outperforms all baselines on FID and JPE, demonstrating better quality of generated motions. The baseline without the diffusion process gets the lowest score, indicating that the sampling process of the diffusion model helps to synthesize better motions.
Diversity also matters since different people may react differently to the same motion, and we expect our robot to obtain diverse behavior as well. As is shown in \Cref{tab:motion}, our approach outperforms the baseline without diffusion in terms of Diversity, thanks to the stochasticity introduced by the diffusion process and the ability of diffusion models to fit high-dimensional distributions. The baseline model without human motion inputs gets a higher MModality score, because it may generate quite different motions given only a human intention label. Yet it performs poorer on FID
and JPE, since human intention alone can not guide the model to generate expressive motions with accurate and desired reactive meaning. For instance, the policy may give various angles and poses of a stretched hand given only the intention \texttt{Shake Hands}; in contrast, given the exact human pose and hand positions, our model can stretch the hand to the exact position and shake hand with the human, highlighting its real-time reactive capability.
% \jingxiao{Revise this part, add more explanation.}

% \jingxiao{Metrics ref to ~\cite{valls2024echo}, ~\cite{liang2024intergen} and ~\cite{zhang2024core4d}.}
\input{tabs/4_2_manip.tex}
\noindent\textbf{3) Objects Manipulation}
\label{sec:exp-manip}

% \textbf{Exp3: Manipulation Success Rate.}
We further test the manipulation performance, and collect the results in \Cref{tab:manipulation}, comparing with statistics recorded from human teleoperation. We compute the success rate and the averaged time based on $20$ independent tests, on the main object in two different scenes.
% In this and the following experiments, we use success rate, the simple but representative metric to evaluate the manipulation skills.
% Typically, $20$ independent tests are performed to calculate the ratio of success times among them. 
% To determine the effectiveness of our framework in this experiment, the average time is calculated by the arithmetic mean value of the span in seconds of $20$ tests.
% In this tabular, skills are sorted into groups by the main object in manipulation to avoid verbose representation.
% The mean success rate and summed time in each of them are shown.
The detailed statistics of each certain skill can be found in \Cref{app:result}.%, \Cref{tab:framework_detailed}.


Our manipulation module shows good performance aligned with human teleoperators. 
As only success data of human teleoperation is used in training, the module even outperforms humans by success rate on groups with rather simple motions such as \textit{can}, \textit{tissue}, \textit{book} and \textit{lamp}. 
In skills requiring a more delicate operation, the trained model slightly falls behind. 
We mainly consequence the fault for the heterogeneity between humans and the robot. 
The inadequate DoFs of the robot arms and lack of haptic sensing on the dexterous hand add great difficulty to some of the skills.
To be specific, the former would lead to the failure of a smooth trajectory, ending with the cap sticking on the hatstand (in group \textit{cap}), and the latter makes it challenging for the robot to determine whether the stamp is pressed to a fair location (in group \textit{stamp}).

It is also noteworthy that the average operation time of our manipulation module is slightly longer on most of the skills than that of human data. 
This is because (1) some skills have a periodical motion, making the progress predictor output ending progress value later than the ground truth. (2) the robot would slowly move back to a fair initialized joint position range at the start to avoid violent actions, this adds to lags as a trade-off.

To determine the effectiveness of the in-skill interruption data, we analyze the impact of training data with human disturbance on three typical manipulation motions. 
With a fixed amount of total training data, 1$\%$, 10$\%$, or 20$\%$ of them are replaced by data where the robot motion is disturbed.
Take the skill \textit{Pick Can} as an example: the robot finds a human is looting the can when it intends to pick, then the robot should withdraw its motion and try to pick again when the human returns the can.

The success rate of a proper motion is shown in \Cref{tab:safety_manipulation}.
The result shows that with an increasing ratio of disturbance data, the success rate of dealing with human violations is getting higher. 
Models with few disturbance data are not capable of a withdraw action, and only mixing as a ratio of 20$\%$ could make a success rate of 85$\%$ in simple skills.

\noindent\textbf{4) Framework Structure}
\input{figs/4_2_e2e_scene}
\input{tabs/4_3_framework.tex}
% \label{sec:exp-framework}
% \textbf{HH-D v.s. HR-D data collection speed.}
% \textbf{Exp5: Our Framework v.s. End2End ACT Model, teleoperation on 1,3,5 skills.}
To highlight the accessibility to multiple skills of our \textbf{two-level} framework, we also compare end-to-end (E2E) baselines, similar to the setting of \citet{cardenas2024xbg}. 
We implement an ACT policy~\cite{zhao2023learning,cheng2024tv}, leaving the input image not cropped to capture the human intention and robot motion information correctly.
As a concise case, the baselines are trained on data of only $1$ motions (\textit{cheers}), $3$ skills (along with \textit{pick}, \textit{place}, meaning pick or place the can) and $5$ skills (along with \textit{handshake}, \textit{wave} also) in a simple scene, and are named E2E/1, E2E/3 and E2E/5 separately. 
In dataset collection, $100$ slices for each skill are collected separately, the same as an average number of that in the manipulation skills.
% Images with full human intent and robot motion information are put into the ACT model for training.
In deployment, we test the E2E model on the in-distribution (I.D.) scene in which human clothing and object arrangement are the same as the recorded datasets, and in the out-of-distribution (O.O.D.) scene these conditions vary.
% \jingxiao{Add explain or image to the OOD scene.}
% \jiahang{I fail to find images to describe this, only add text, need help.}

In detail, the I.D. scene features a leader human wearing a light purple shirt, with a Coke can placed to the right of the robot, and a dark green plate on the table.
The O.O.D. scenes include, but are not limited to, the following changes: replacing the Coke can (red) with a Sprite can (green), swapping the dark green plate for a light red or light blue one, adding more plates in front of the leader human, changing the leader human's outfit, for example, to a down jacket, or replacing the leader human with another character, such as one in a striped sweater or a white coat.
\Cref{fig:e2e_scene} shows an example highlighting the scene difference with respect to the leader human.

The success rates on average of different numbers of skills are shown in \Cref{tab:framework}, where we can observe \our outperforms the E2E baselines on the skills of better prediction of human intention and robustness to O.O.D. data. 
When trained with data of only one skill, the E2E framework is able to predict human intention and perform a successful motion. 
However, when more different skills are used, the E2E framework struggles to predict the correct intention.
Meanwhile, without the information on hand occupancy, the model is likely to fail to tell the difference among skills with similar camera view, i.e. \textit{cheers}, \textit{pick}, and \textit{place}.
Moreover, the coupling of prediction, generation, and manipulation leads to uncropped images as input.
The high dimension of images and noise cripple the robustness of the prediction module.
Interacting with O.O.D. human body and clothing, or under some O.O.D. table arrangement, the model fails to generate proper motion to finish the skill showcase.
The detailed success rate is shown in \Cref{app:result}.
.
\subsection{Analysis of System Failure}
As a framework of multiple modules, the failure of the system could be caused by various reasons. 

% 
\noindent\textbf{Error and limitation of sensors.} 
Most of the perception of our implementation of \our is based on one RGB-D camera. However, the estimation of 3D position often shifts with time and missing when the estimated object is occluded by other objects or the robot arms. The cumulative error of the sensors leads to a misunderstanding of human intention and incorrect judgment of the safety supervisor.

\noindent\textbf{Stability of hardware.}
The zero position of the robot arm may have shifted in a small range, which leads to the incorrect proprioception.
Also, the robot's electronics age over time, which causes errors that require precise control of the robot's end-effector.

\noindent\textbf{Failure of Model Generalization.}
Due to the limited data collection, the model may fail to generalize to extreme out-of-distribution scenarios, although we mitigate this issue by cropping the image to the region of interest for manipulation skills and using extracted information rather than raw images for human intention prediction.
Some unseen human clothing or unexpected object arrangement may lead to the failure of the manipulation.
Also, non-standard sitting posture or body shape can also have an impact on the prediction of human posture and intention, which leads to misunderstanding of human intention.
Fortunately, human leaders can intervene to correct the robot's behavior in \our, which helps prevent a complete breakdown of the system.