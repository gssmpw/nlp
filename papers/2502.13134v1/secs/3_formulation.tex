\section{Problem Formulation}
\label{sec:formulation}

% Formulation: 
% 1. Environment + Human Instruction -> Robot Action
% 2. Leader-Follower setting: Human as leader, robot as follower
% 3. Robot's Ability: Interactive Motion, Manipulation, Idle
% 4. Real-Time Design: Low-latency instruction: Interuption


In this work, we consider the interaction as a leader-follower formulation~\cite{vianelloHumanHumanoidInteractionCooperation2021}, where the human is the leader and the humanoid robot is the follower.
Define $\mathcal{I}$ as the set of human intentions and $\mathcal{K}$ as the set of robot skills.
At time step $t$, the leader shows an intention $I_t \in \mathcal{I}$ for the follower to perform a skill $K_t \in \mathcal{K}$, such as picking up a can, brushing a plate, or stamping a file.
We assume one intention corresponds to at most one skill, and the robot should be able to switch between skills in real time. The map function is defined as $f: \mathcal{I} \rightarrow \mathcal{K}$.

% Type and property of task:
    % - interactive Motion, manipulation, idle
    % - real-time design: low-latency instruction, in task reflection to human.
The skills of the robot can be categorized into three types: interactive motion, manipulation, and idle.
The interactive motion skills require the robot to perform expressive and diverse behavior, and the manipulation skills require the robot to interact with objects in the environment precisely. 
When the human leader does not show any intention, the robot will be in an idle state and do nothing.
The real-time interaction design requires the robot to respond to the leader's intentions with low latency and in-skill reflection to the human leader.
Low latency requires the robot to predict the leader's intention in real time and interrupt the current skill when the leader shows a new intention.
In-skill reflection requires the robot to react to human motion and environment even if the human intention is not changed, such as pausing current movement if the robot collides with the human or the target object is not reachable.

% The design of the idle state ensures the robustness of the interaction process. 
% The robot should avoid being affected by the uninstructed actions of a human, for example, a human might pick up a Coke can on a table. This behavior is similar to pointing at a Coke can for the robot to pick up, but the robot should be able to recognize it as idle.

% To ensure the safety of the interaction process, the interaction framework also includes a safety supervisor module, which monitors the robot's behavior. 
% When potential harm to the human leader is detected, the safety supervisor will stop the robot's action immediately and only allow the robot to continue the interaction when safety is guaranteed.

We formulate the observation space $\mathcal{O}$ of a humanoid robot as the combination of the
environment state $\mathcal{E}$ and the human behavior $\mathcal{H}$, i.e., $\mathcal{O} = \mathcal{E}  \bigodot  \mathcal{H} $.
The environment state $\mathcal{E}$ includes the robot's proprioception and the object state. The human behavior $\mathcal{H}$ consists of the leader's intention and the leader's behavior. 
To reduce the complexity of observation, our framework decomposes interaction policy into several sub-modules and separately designs the observation space for each module.
Compared to end-to-end models, this decomposed design allows the robot to learn humanoid-human-object interaction from human-object-human demonstration data, which is more sample-efficient and scalable.
% Main challenge: the diversity of human apperance, including clothing and body shape.


% Real-time Design
% To enable real-time interaction, \our is designed to respond to the leader's instruction with a low-latency reactive planner and interruptable low-level skills.
% The reactive planner predicts the leader's intention with 30Hz, and once the planner finds the current intention conflicts with the executing task, the planner will try to interrupt the current task and switch to the new task immediately.



