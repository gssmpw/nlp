
\section{Introduction}
% Introduction
Humanoid robots are increasingly being explored to perform tasks in diverse environments~\citep{agravante2019human,johnson2015team,Kheddar2019}. Their human-like morphology provides a potential for acting with human-like dexterity, making them ideal for general-purpose daily-life human assistants.
However, most recent progresses only focus on learning basic abilities such as locomotion~\cite{HumanoidTransformer2023}, object manipulation~\cite{cheng2024tv}, and expressive motion~\cite{cheng2024express}.

% making them potentially useful in a wide range of applications, including daily life assistance~\citep{agravante2019human}, disaster response~\citep{johnson2015team}, and industrial automation~\citep{Kheddar2019}.
% Prior Work and Limitations
Considering how we as humans react to our friends, a practically helpful humanoid assistant should possess three fundamental capabilities:
1) skill proficiency, equipped with diverse and essential skills to achieve various tasks;
2) intention recognition, capable of discerning human intentions, from either motion or language; and
3) instant feedback, able to respond in real-time with feasible actions. 
Nonetheless, most studies on human-robot interaction only focus on only one or two of these aspects. %, rather than addressing all three in tandem.
For instance, a significant body of work on human-robot interaction focuses on
object handover~\citep{strabalaSeamlessHumanRobotHandovers2013, tulbureFastPerceptionHumanRobot2024}, or interactive motion generation~\citep{prasadMoVEIntMixtureVariational2024, butepageImitatingGeneratingDeep2019, liangInterGenDiffusionbasedMultihuman2024, liuPhysReactionPhysicallyPlausible2024, mascaroRobotInteractionBehavior2024}, lacking the ability to switch between different tasks in real-time.
Some others focus on recognizing human intentions~\citep{duarteActionAnticipationReading2018, enanRoboticDetectionHumanComprehensible2022, fangEgoPAT3Dv2Predicting3D2024, mascaro2023hoi4abot, scherfAreYouSure2024}, which simplify the diversity of reaction and treat the interaction as an alternated two-stage process. The robot cannot be interrupted once a task is in progress, and further human commands can only be executed after the completion of the robot's current task.
Many recent works have attempted to combine the ability of general foundation models to enable robots to understand the complexity of human interactions~\citep{tanneberg2024help, wuHumanObjectInteractionHumanLevel2024}, but they often suffer from high latency and are not suitable for real-time interaction tasks.
% Challenge: Lack of unified framework
% A unified framework that jointly addresses real-time human intent recognition and continuous control for object manipulation remains unexplored.
% Thereafter, a unified framework that jointly addresses human-robot interaction with real-time intent recognition and various skills remains unexplored.
These limitations hinder robots from rapid interventions and robust, multi-step interactions in human-centered tasks. Therefore, a unified framework that masters human-robot interaction with real-time intention recognition and various skills is urgently needed to tackle the above challenges.

% 3. Combine basic abilities with Visual Language Models, which are in high-latency and not suitable for real-time environments.
% Recent works have attempted to combine various basic abilities with a general foundation model, such as the Large Language Model or Vison Language Model, to enable robots to understand the complexity of human interactions~\cite{tanneberg2024help, wuHumanObjectInteractionHumanLevel2024}. However, these models are often high-latency and not suitable for real-time environments, which limits the potential for natural and effective human-robot collaboration, particularly in scenarios that require immediate response or adaptation to changing human needs. 
% \citet{cardenas2024xbg} tries another approach, learning an end-to-end model by imitation to achieve real-time interaction, with 5 different tasks.
% Limited by the sample efficiency, this end-to-end paradigm is difficult to scale to more tasks, and lacks the ability to manipulate objects precisely.

% Our Approach
To achieve this goal, we propose \our, a hierarchical learning framework for \underline{R}eactive \underline{H}umanoid-human \underline{IN}teraction and \underline{O}bject Manipulation.
\our decouples the interaction process into two levels: a high-level planner that infers human intentions from real-time human behaviors, and a low-level controller that achieves reactive motion behaviors and object manipulation skills based on predicted intentions. The high-level planner updates at high frequency, and the low-level controller is designed to be interruptable, enabling it to react to high-level commands at any time.
To ensure the scalability of \our across a wide range of skills, we design a pipeline for learning the interactions from human-object-human demonstration and teleoperation data, which can be easily extended to different tasks and scenarios.
We implement \our on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in various scenarios (see \Cref{fig:teaser}). 
Although this work only focuses on the upper body of a humanoid including the head, arms, and hands, it has the potential to be extended to whole-body humanoid interaction with a unified humanoid controller, and finally brings robots, especially humanoids, closer to our daily lives.
% \Cref{fig:teaser} shows the ability of \our to interact with humans and objects in real-time.

% Contributions
Our main contributions are in the following aspects:
\begin{itemize}[leftmargin=*]
    % \item We propose a unified learning framework that combines human-robot interaction and object manipulation, enabling humanoid robots to recognize human intentions and react with various basic skills in real-time.
    \item We propose the first humanoid learning architecture that seamlessly integrates intention recognition with real-time human-object-humanoid interaction skills, enabling the robot to respond to human instruction and switch between different tasks immediately. 
    \item We design a pipeline for learning the interactions from human demonstrations, which can easily scale to different tasks and scenarios.
    \item We implement \our on the Unitree H1 humanoid robot and demonstrate its effectiveness, flexibility, and safety in 2 scenarios with over 20 tasks, and open-source the code and datasets to facilitate future research.
\end{itemize}
