% \newpage
\clearpage
% Appendix
\appendix
\label{sec:appendix}

\subsection{Real World Setup}

\noindent\textbf{Deployment hardware}
% DOF of the robot, workspace, etc.
The humanoid robot on which we deploy \our is Unitree H1~\cite{H1-page}.
Following \citet{cheng2024tv}, we assembled two DYNAMIXEL XL330-M288-T motors~\citep{dynamixel-page} with 3D printed gimble parts and a ZED Mini stereo camera ~\cite{zed-page} for two-DoF (yaw and pitch) active sensing.
Each arm of H1 has 5 DoFs and a 6-DoF end-effector from~\cite{dexterous-page}, and other DoFs on the robot are not used.


\noindent\textbf{Motion capture system}
\label{app:mocap}
We use ArUco markers and two cameras to build a simple motion capture system.
We put four ArUco markers on the four corners of the workspace table to locate the cameras.
Each human in the workspace wears two 3D-printed wristbands with four ArUco markers on each of them, illustrated in \Cref{fig:mocap_sys}. 
The cameras are calibrated and located using the OpenCV library and capture the human wristband's position in real time.
Each wristband has an additional IMU sensor to capture the orientation of the human's wrist.
To reduce the noise in the collected data, we use a Kalman filter to smooth the data.
\input{figs/X_1_setups}

\noindent\textbf{Motion Detection and Object Detection}
As is illustrated in \Cref{method:real-robot}, we use the Body Tracking feature in ZED API to detect the body motion of the human and a fine-tuned YoloV11~\cite{khanam2024yolov11} model to detect objects on the table. For hand detection, we use HaMeR\cite{pavlakos2024reconstructing} to obtain the human hand motion and then retarget\cite{qin2024anyteleopgeneralvisionbaseddexterous} it to the 6-DoF robot hand. The visualization results are shown in \Cref{fig:zed_detect}.


\subsection{Skill Descriptions}
\label{app:skills}
%  task description, succ. condition, reverse task(if exist), and instruction
In this section, we describe the skills that we deploy on the humanoid robot. The details include the description, success condition, reverse skill (if exists), and the human intention related to the skill.
Note that the intention is inferred mainly from human behavior, hand positions, and the relative location of objects.
The latter two can be concluded trivially to the start condition and end transition, and are shown by skill in \Cref{tab:skills}.
We concern the intention mainly on the human body motion in the narration as follows:


\noindent\textbf{1) Scenario 1: Humanoid as a Dining Waiter}

% Description of this scenario.
In this scenario, the leader human and the robot sit face-to-face at the side of a dining table. 
There are plates with food, a Coke can, a tissue box, and a sponge on the table.
In the following skill descriptions, the humanoid robot takes the role of a helpful waiter and serves the leader human a meal with these objects.
$10$ skills related to $4$ objects are listed below:

\begin{itemize}[leftmargin=*]
    \item \textit{Pick Can}: The robot picks up a can with its right arm from the table. 
    The success condition is that the can is lifted off the table.
    The interruption data includes the human taking the can away or the human putting his hand on the can. 
    \textit{Place Can} is the corresponding reverse skill.
    The leader human shows the intention by pointing to the can when the right robot arm is empty.
    
    \item \textit{Place Can}: The robot places the can back on the table with its right arm. 
    The success condition is that the can is placed on the table and the robot's hand is lifted off the can.
    The intention of this skill is shown by the leader human pointing to the place on the table where the can was placed before.
    
    \item \textit{Get Plate from Human}: The robot fetches a plate from the hand of the human with its left arm.
    The success condition is the plate in the dexterous hand when the human loosens the grip of the plate.
    The leader human shows the intention by handing a plate forward.
    
    \item \textit{Place Plate to Stack}: The robot stacks the plate in its right hand onto a pile of plates on the table.
    The success condition is the plate settled on the top of the plate pile without slipping.
    The intention is given by the leader human pointing to the stack.
    Interruption data, in which the human touches the plate to stick the motion, is added to the collected dataset.
    
    \item \textit{Pick Place from Table}: The robot lifts a plate on the table by both arms and holds it in the left hand.
    Application of this skill succeeds if there is no slippage in the motion until the plate is held.
    The leader human points to the plate to show the intention.
    
    \item \textit{Handover Plate}: The robot protracts its left arm to give the leader human the plate on it.
    The showcase of this skill ends when the plate is put into the human hand.
    The leader human simply stretches the right hand out to show the intention.
    It is the reverse skill of \textit{Get Plate from Human}.
    
    \item \textit{Pick Sponge}: The robot picks up the sponge with its right arm. 
    The sponge is placed beside the can.
    When it is lifted off the table the skill showcase succeeds.
    % \textit{Place Sponge} is the reverse skill.
    The leader human shows intention by mimicking washing.
    Data where the human snatches the sponge before the robot reaches it adds to the dataset. 
    In case this happens during deployment, the robot withdraws its hand to the idle state.

    \item \textit{Brush with Sponge}: It is a complex skill using both arms.
    The start condition is a plate in the left hand and a sponge in the right one when the leader human makes the washing gesture (same as that in \textit{Pick Sponge}) again.
    To apply this skill, the robot moves the sponge close to the plate and rubs the sponge on the plate to brush it.
    The success condition is the robot keeps the periodic brushing motion for over $10$ seconds.

    \item \textit{Place Sponge}: The reverse skill of \textit{Pick Sponge}.
    The robot puts the sponge in the right hand back onto the table to complete the skill demonstration.
    The intention is shown by the leader human pointing to the place on the table where the sponge was placed before (similar to skill \textit{Place Can}).

    \item \textit{Pick a Piece of Tissue}: The leader human points to the tissue box to express the intention. 
    Then the robot uses its left hand to pull a tissue from the tissue box placed on the table corner and gives it to the leader human.
    The skill showcase succeeds when the leader human receives the tissue.
\end{itemize}

\input{tabs/X_0_tasks.tex}

\noindent\textbf{2) Scenario 2: Humanoid as an Office Assistant}

In this scenario, the leader human sits across the humanoid robot at an office table.
This time the robot transforms into an office assistant and deals with complicated cases such as stamping paper for approval, settling a baseball cap on the rack, picking and handing a book over, and reacting properly if the human takes a snap in working.
There are $7$ skills related to $4$ objects in this scenario.

\begin{itemize}[leftmargin=*]

\item \textit{Settle Cap}: The robot gets a cap from the leader human's hand and settles it on a hat rack with its right arm.
The skill showcase begins with the human holding the cap with both hands and ends with the robot pulling its hand back from the hat rack.

\item \textit{Handover Cap}: The robot takes the cap off the hat rack and sends it to the leader human.
It is the reverse skill of \textit{Settle Cap}.
The related intention is inferred when seeing the human pointing to the rack.
The success condition is that the human has received the cap.

\item \textit{Pick Book}: The robot picks a book from the shelf and hands it over.
The skill begins with the human gesturing toward the book.
When the human takes the book, this skill is completed successfully.

\item \textit{Pick Stamp}: The robot picks up the stamp on the table with its right hand. 
The skill succeeds when the stamp is lifted near the hand in an idle posture.
The leader human instructs the execution of this skill by passing along the paper.

\item \textit{Stamp the Paper}: It is a delicate operation to make an issue for approval.
The robot presses the stamp down onto the paper to mark a sign.
This skill is considered successful only if one mark is imprinted.
It is noteworthy that printing more than one mark in a single execution means that the model fails to predict the ending, thus being treated as a failure case.
The sign of the related intention is the leader human pointing at the paper.
To make an in-skill interruption, the human covers the paper with a hand to make the robot withdraw its hand if the pressing is not done.

\item \textit{Place Stamp}: The robot places the stamp back with its right hand.
It is the reverse skill of \textit{Pick Stamp} and is triggered by withdrawing the paper.

\item \textit{Turn off/on the Lamp}: Turning on and off the lamp share the same motion, and thus are trained as one skill.
When the human slumps over the office desk to take a nap, the robot taps the switch of the lamp to turn off it.
And when the human wakes up and lifts the head, the robot operates the same motion to turn on the lamp.
    
\end{itemize}

\noindent\textbf{3) Interactive Motion Skills}

Some skills are not involved with object operation and, thus, are not trained as manipulation skills. 
They are noted as motion skills.
These skills are considered successful when the robot performs the motion properly as the human shows the intention and recovers to the idle posture when the intention no longer sustains.

\begin{itemize}[leftmargin=*]
\item \textit{Cheers}: The robot reaches out the right hand to touch the bottle held by the right hand of the human.
Though holding a Coke can in the right hand during deployment, the robot does not manipulate the object.
For this reason, this skill is not trained in a manipulation demonstration model.

\item \textit{Wave}: The robot lifts up the right hand and waves the right hand when the leader human is waving also.

\item \textit{Shake Hands}: The robot stretches its right hand out to touch the hand of the leader human with a handshaking posture.

\item \textit{Take Photo}: The robot lifts up the right hand and makes a V-sign when the human raises the phone to take a photo, and puts the hand done as the human puts away the phone.

\item \textit{Thumb Up}: The robot reaches both hands out with the thumbs up as the human gives it a thumb-up.
Human intention with the left hand, right hand, or both is approved.

\item \textit{Spread out Hands}: The robot stretches its arms out to the sides with palms up when the leader human spreads its hands out.
    
\end{itemize}

\subsection{Prompt for VLMs}
Here are the prompts we give to Qwen and GPT-4o-mini in evaluation of the intention prediction module.

% For the \textbf{Dining} scenario, our prompt is:
\noindent\begin{tcolorbox}[
    colframe=darkgray, % Dark grey frame color
    boxrule=0.5pt, % Frame thickness
    colback=lightgray!20, %
    arc=3pt, % Rounded corners
    fontupper=\small,
    % width=.475\textwidth,
    breakable, title={Prompt for the \textbf{Dining} scenario},
    % height=9cm,
    ]
    
\textit{You are a humanoid robot sitting in front of a human and equipped with a camera slightly tilted downward on your head, providing a first-person perspective. I am assigning you a new task to recognize to human gestures in front of you. Remember, the person is sitting facing you, so be mindful of their gestures. If the person is holding a cup to you and trying to cheer with you, answer `Cheers'. If the person is giving you a thumbs-up, answer `Thumbup'. If the person extends their right hand to shake hands with you, answer `ShakingHand'.If the person is waving to you with the right hand, answer `Waving`. If the person is taking a photo of you with a cellphone, answer `Taking Photo`. If the person is spreading out both hands in a gesture of resignation, answer `Spreading Hands`. If the person is pointing to a Coke can in the middle of the table (on your right side), answer `Pointing Can'. If the person is pointing to an empty spot on the table with no objects (on your right side), answer `Pointing Table`. If the person is pointing to a tissue box at the far left of the table, answer `Pointing TissueBox'. If the person is pointing to a plate in the middle of the table (just in front of you), answer `Pointing Plate'. If the person is holding out the right hand with the palm open toward you, answer `Palmup'. If the person is handing you a plate, answer `Handing Plate'. If the person is clenching their right fist, holding their left hand open and upward, and placing their right hand above the left as if pretending to wash a plate, answer `Washing'. If the person is pointing at a stack of plates on the left side of the table, answer `Pointing Plates'. If the person is pointing at a sponge on the right side of the table, answer `Pointing Sponge'. If the person is crossing his arms to form an X shape, answer `Cancel'. If no significant gestures are made, answer `Idle'. 
Respond directly with the corresponding options [Cheers, Thumbup, ShakingHand, Pointing Can, Pointing TissueBox, Pointing Plate, Palmup, Handing Plate, Washing, Pointing Plates, Pointing Sponge, Cancel, Idle] based on the current image and observed gestures. Directly reply with the chosen answer only, without any additional characters.}
\end{tcolorbox}

% For the \textbf{Office} scenario, our prompt is:
\noindent\begin{tcolorbox}[
    colframe=darkgray, % Dark grey frame color
    boxrule=0.5pt, % Frame thickness
    colback=lightgray!20, %
    arc=3pt, % Rounded corners
    fontupper=\small,
    % width=.475\textwidth,
    breakable, title={Prompt for the \textbf{Office} scenario},
    % height=9cm,
    ]
\textit{You are a humanoid robot sitting in front of a human and equipped with a camera slightly tilted downward on your head, providing a first-person perspective. I am assigning you a new task to recognize human gestures in front of you. Remember, the person is sitting facing you, so be mindful of their gestures. If the person is giving you a thumbs-up, answer `Thumbup`. If the person extends their right hand to shake hands with you, answer `ShakingHand`. If the person is waving to you with the right hand, answer `Waving`. If the person is taking a photo of you with a cellphone, answer `Taking Photo`. If the person is spreading out both hands in a gesture of resignation, answer `Spreading Hands`. If the person is handing you a cap, answer `Handing Cap`. If the person is pointing at a cap place on the right of the table, answer `Pointing Cap`. If the person is handing a document to you with both hands and you are NOT holding a stamp, answer `Handing File`. If a document is placed in the center of the table in front of you, and the person is pointing to it with the right hand, answer `Pointing File`. If the person retrieves the document from your side of the table to the other side, directly across from you, and you are still holding the stamp, answer `Retrieve File`. If the person is lying down on the table and the lamp is ON, answer `Lie Down`. If the person is sitting up from the table and the lamp is OFF, answer `Sit up`. If the person is pointing at the books standing in the top left corner of the table, answer `Pointing Book`. If the person is crossing the arms to form an X shape, answer `Cancel`. If no significant gestures are made, answer `Idle`. You are NOT holding a stamp right now and the lamp is now ON, observe the image and gestures carefully. Respond directly with the corresponding options [Pointing Book, Handing Cap, Pointing Cap, Handing File, Pointing File, Retrieve File, Lie Down, Sit up, ShakingHand, Thumbup, Cancel, Idle]. Directly reply with the chosen answer ONLY, without any additional characters.}
\end{tcolorbox}

The sentence `\textit{You are NOT holding a stamp right
now and the lamp is now ON}' is modified at each query according to the current situation (whether the robot is holding a stamp and whether the lamp is on).

\subsection{Implementation of RHINO Modules}

\label{app:implementation}

\noindent\textbf{1) Reactive Planner}
\input{tabs/3_0_planner_algo.tex}

\input{figs/X_2_occupancy}

% \input{tabs/X_5_planner.tex}
The switching logic of the skill planner is listed in ~\Cref{alg:planner}.
The directed graphs of occupancy are shown in~\Cref{fig:occupancy}.
Here we further explain the \texttt{Recognize\_Human\_Intention()} function in detail, which is implemented as a transformer-based classifier. The model input includes:
\begin{itemize}[leftmargin=*]
    \item \textbf{Upper Body Human Posture}: a 36-dim human upper body skeleton, namely the 6D rotation of the wrist, elbow and shoulder joints for each arm.
    \item \textbf{Human Hand Pose}: a 12-dim human hand pose vector. For each hand, we retarget the detected human hand pose to our robot hand with IK, and take the 6 joint pos as human hand pose vector.
    \item \textbf{Robot Hand Occupancy}: a 10-dim robot hand occupancy label. Since we have at most 5 objects in total (Can, Cup, Plate, Sponge, Tissue), we use a 5-dim one-hot label for each hand to represent the object held in the robot's hand. If the robot is not holding anything, the label will be all-zeros.
    \item \textbf{Human Details}: a 19-dim vector, including the x and y-axis of each human hand position, the z-axis (height) of the human head position, and a 7-dim label for the nearest object to each hand. The nearest object label is concatenated by a 5-dim one-hot label of the object type, the distance from the object to the human hand, and the average of IOU and IOFs of the object bounding box and the human hand bounding box.
\end{itemize}
We use an MLP encoder to encode the concatenated vector of  \textbf{Upper Body Human Posture}, \textbf{Human Hand Pose} and \textbf{Robot Hand Occupancy}, and another MLP to encode \textbf{Human Details} to latent dimension. The concatenated latent vector is processed by a Transformer backbone, followed by a final MLP layer to predict the human intention class. The 
hyper-parameters of the Transformer backbone are listed in ~\Cref{tab:planner-hyper}.

\input{tabs/X_3_model_hyper.tex}

\noindent\textbf{2) Interactive Motion Generation}

% \input{tabs/X_6_motion}
For interactive motion generation, we use a transformer-based diffusion model, which denoises the past 30 frames of human and robot motions and future 5 frames of robot motions. Both human motion and robot motion consist of upper-body motion (36-dim for humans and 10-dim for humanoid), hand motion (6-dim for each hand,) and hand occupancy label (5-dim one-hot label for each hand). Besides, the predicted human intention label is also conditioned during the diffusion process. The hyper-parameters of our model are listed in \Cref{tab:motion-hyper}.


\noindent\textbf{3) Manipulation Skills}

Thanks to the stability of model training, most of the hyper-parameters are basically consistent across all skills.
The volume of data for training each skill is shown as a column in \Cref{tab:skills}.
The hyper-parameters in training ACT models~\cite{zhao2023learning} are shown as \Cref{tab:act-hyper}.
% We mainly adopted those from ~\cite{cheng2024tv} and adjusted some important hyper-parameters.

\input{tabs/X_2_manipulation.tex}

For the prediction of the success signal, we marked the last $n_{s}$ frames of the recorded data as $1$ (completed) and other frames as $0$ to generate a 0/1 label.
$n_{s}$ is set to $25$ in most of the skills and shifted to $10$ in three of them of which the ending frames changed sharply in motion.
The special skills are \textit{Pick Stamp}, \textit{Stamp the Paper}, and \textit{Place Stamp}.

% Jingxiao: Ignore some details
% \input{tabs/X_4_skill_hyper.tex}

% Some simple but effective tricks are utilized to make skills perform better when deployed on the robot. 
% Firstly, we set a \textbf{progress threshold} to filter the noise of terminal condition prediction. 
% During deployment, the skill execution is terminated when the model predicts a success signal larger than the threshold.
% The threshold is typically $0.85$ and varies in certain skills.
% Moreover, to avoid verbose discussion that the robot should start from any state to finish the skill, we set the control target to an \textbf{initial joint position} before performing a skill.
% The initial position is typically selected as the first action of one slice in the collected dataset (noted as \texttt{fixed}) and is replaced by the model prediction of the first state (noted as \texttt{predicted}) in certain skills in consideration of robustness.
% However, there are some cases where the start and end state of the same skill are similar, leading to termination soon or dragging on in skill execution.
% We assign a pair of \textbf{warm-up time} and \textbf{time-out period} to avoid this phenomenon.
% A skill can not be predicted to be completed before the former and is regarded as terminated after the latter.
% They are typically $2$ and $15$ seconds respectively, and adjusted in only a few skills.
% In some skills with periodic features in the motion, the \textbf{time-out period} takes the place of \textbf{progress threshold}.
% In those skills, the robot executes a series of periodic actions until time out.

% The detailed selection of hyper-parameters across the manipulation skills is shown as \Cref{tab:skill_hyper}. 
% It shows that the manipulation module needs only a few simple adjustments to work on various skills.




\noindent\textbf{4) Safety Supervisor}

\input{figs/X_5_safe}

The collision box is calculated using $14$ key points across each arm. The key points at specific joints and their midpoints are identified as follows:

\begin{itemize}[leftmargin=*]
    \item The origins of the shoulder pitch, shoulder yaw, elbow, and wrist joints are defined as key points.
    \item Additional key points include the midpoints between the shoulder yaw and elbow joints, and between the elbow and wrist joints.
    \item A further key point is defined at one-third the distance beyond the elbow towards the wrist, extending from the segment between these two joints.
\end{itemize}
This structured delineation allows for precise calculations pertinent to robotic arm movements within a predefined spatial configuration.

The human hands are shaped by the detected key points from body detection model of ZED API, from which each hand is reconstructed as $5$ points.
Once one of the points is close to any robot key point in $0.1$ meters, an unsafe signal is broadcast to pause the robot control.

We also provide the visualization of the safety supervisor, of which the interface shown in \Cref{fig:safe}. When the human hand key-points collide with any collision box, the supervisor will send an unsafe signal to halt the robot.
Our safety supervisor runs at $30$Hz.


\subsection{Detailed Experiment Results}

\label{app:result}

\noindent\textbf{1) Planner}
\input{figs/X_4_confusion_matrix}
We use confusion matrices to show the classification performance of our planner on the test dataset. The confusion matrices for our model, our model without human details and GPT-4o-mini on the test datasets of the dining and office scenarios are shown in \Cref{fig:Confusion Matrix}. 
% To show the results more clearly, We did not color the cell in the top left corner (the cell enclosed by a dashed line) since "idle" accounts for a significant proportion in the data. 

As is shown in the confusion matrices, although the model mainly relies on human body motion and human hand motion input for classification, human details can help the model better deal with certain situations, such as avoiding mis-classification into Idle.

\noindent\textbf{2) Objects Manipulation}

The detailed success rates and average execution times across skills are presented in \Cref{tab:manipulation_detailed}, from which the statistics in \Cref{tab:manipulation} are derived.

In most skills, the manipulation module of \our autonomously executes motions following the patterns of teleoperation data within a comparable time frame.
Trained exclusively on successful human teleoperation cases, the module demonstrates both effectiveness and robustness to slight scene variations during deployment. 
As a result, it achieves higher success rates in skills involving simple motions with abundant training data, such as \textit{Pick Can}, \textit{Handover Plate}, and \textit{Place Stamp}.

However, certain skills pose challenges for the manipulation module. In \textit{Place Plate to Stack} and \textit{Stamp the Paper}, the robot hesitates to drop the plate or press the stamp due to prediction noise. 
In \textit{Pick Plate from Table}, it must overcome increased friction against the table when joint positions deviate from those in the collected data.
Another challenge arises in \textit{Brush with Sponge}, where the success signal predictor struggles to assess the progress of the periodic motion accurately. 
As a result, termination is constrained by a $10$-second timeout. 
These various factors contribute to a longer average execution time for these four skills compared to human performance.

Referring to the experiment on in-skill interruption data presented in \Cref{tab:safety_manipulation}, we select \textit{Pick Can}, \textit{Stamp the Paper}, and \textit{Place Plate to Stack} as representative skills for interruptions occurring during the stages of fetching an object, operating with the object, and returning the object, respectively.

To ensure a controlled data volume across different interruption ratios, we assign a fixed data amount of $M$ to each skill. 
In a full data collection for any given skill, the total data amount is $N$, with $N_{in}$ representing the portion containing in-skill interruptions. 
The ratio of interrupted data in a selected subset is denoted as $\alpha$, meaning that $\lceil\alpha M\rceil$ slices contain interruptions. 
To maintain this proportion, we set $M=N-N_{in}+1$. 
Specifically, $M$ is set to $69$, $66$, and $76$ for the three skills, respectively.

It is worth noting that the assigned data amount is smaller than that used in full data collection models (see \Cref{tab:skills}), which results in a decrease in skill success rate and interrupt success rate compared to the final model.

Each ACT model in this experiment uses the same hyper-parameters as those employed for the corresponding skill in both training and deployment with the full data collection.


\noindent\textbf{3) End2End Policy}

\input{tabs/X_1_end2end.tex}

The detailed result to derive \Cref{tab:framework} is shown in \Cref{tab:framework_detailed}.
For each single skill, we collected $100$ slices of motion in the dataset, which is close to the average volume of all the manipulation skills.
As the small model is not capable of the skills which are totally unseen, we only examine each E2E model with the success rate of skills in the training data.


