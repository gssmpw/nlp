\input{figs/3_1_formulation.tex}
\section{Related Works}
Recent progress in building a human assistant robot can be divided into three categories: 1) recognition of human intention,n, 2) basic skills, and 3) unified interaction framework, as shown in \Cref{fig:formulation}(b).
We summarize related works in each category and highlight the differences between our work.

\subsection{Human Intention Recognition.}
Humanoid robots need to estimate the human physical and mental states to provide appropriate assistance~\cite{vianelloHumanHumanoidInteractionCooperation2021}.
More specifically, many signals can be used to infer human intentions, such as whole-body motion~\cite{tulbureFastPerceptionHumanRobot2024, yangReactiveHumantoRobotHandovers2021}, forces~\cite{agravante2019human}, gaze~\cite{duarteActionAnticipationReading2018,scherfAreYouSure2024}, and language~\cite{tanneberg2024help}.
Object information in the environment also plays an important role in predicting human intention by combining it with human motion.
Human-object interaction, such as pointing gestures~\cite{fangEgoPAT3Dv2Predicting3D2024} and grabbing objects~\cite{mascaro2023hoi4abot}, provides a broader semantic space for human actions.
Most works on human intention recognition treat the interaction as a two-stage process, where the robot first predicts the human intention and then executes the task. This design simplifies the diversity of reactions and neglects the real-time reaction ability of the robot. Our work aims to react to human signals in real time, enabling the downstream tasks to be interrupted at any time.

\subsection{Basic Skills}
\noindent\textbf{Interactive motion synthesis.}
% robot-human motion
In human-robot interaction (HRI), learning to generate interactive and expressive motions, such as shaking hands and waving, are fundamental skills. 
% Early works generate limited motion by learning manually designed motion primitives~\cite{}. 
% Recently, some works have taken deep learning methods to generate diverse motion for animation~\citep{}.
The human-like morphology of humanoid robots provides a unique opportunity to learn natural motion from retargeted human motion data~\citep{fuHumanPlusHumanoidShadowing2024}. Human motion data can be collected from motion capture systems or network videos. Compared to collecting robot motion data, it has a lower cost and higher scalability.
% 2-human interaction data
Recent works~\cite{liangInterGenDiffusionbasedMultihuman2024, xuInterXVersatileHumanHuman2023} collect multi-human motion data, capturing real-time interaction and reaction between humans. 
Building on this, studies encode social scenes \cite{mascaroRobotInteractionBehavior2024}, simulate reactions \cite{liuPhysReactionPhysicallyPlausible2024}, or deploy interaction models on robots \cite{prasadMoVEIntMixtureVariational2024}. Our work focuses on learning interactive motion from human-human-object interaction data.
% \citet{mascaroRobotInteractionBehavior2024} encode the social scene and generate human-robot interaction from multi-human motion data. \citet{liuPhysReactionPhysicallyPlausible2024} learn to generate physically plausible multi-humanoid reactions in a simulator, which shows the potential possibility of transferring to real humanoid robots.
% \citet{prasadMoVEIntMixtureVariational2024} uses a Variational Autoencoder (VAE) with Mixture of Experts fashion to learn human-robot interaction and deploy the learned model on a dual-arm robot. 
% Our work learns the interactive motion from human-human-object interaction data.

\noindent\textbf{Object manipulation.}
The ability to manipulate objects is another fundamental skill for a humanoid assistant robot, which requires more precise control of the robot's end-effector.
% Also learning manipulation skills from human data is possible~\cite{}. 
Limited by the dexterity of the robot, especially the degree of freedom of our humanoid robot's arm and hand, imitating learning from real-world teleoperation data~\cite{chi2023diffusion,pari2021surprising,mandlekar2021matters} is a more practical way to ensure success, compared to learning from human data~\cite{wang2023mimicplay, qin2024anyteleopgeneralvisionbaseddexterous, zhu2024vision}.
Open-Television~\cite{cheng2024tv} developed a teleoperation system with a VR device to control the arms and neck of humanoid robots and show the effectiveness and efficiency of learning manipulation skills with ACT~\cite{zhao2023learning} policy.
Our work learns the manipulation skills based on the teleoperation data.

\subsection{Unified Interaction Framework}
% \textbf{Language-based Interaction.}
% Large Language Models
Recent works have attempted to leverage the capacity of general foundation models, such as large language models (LLMs) or vison-language models (VLMs), to enable robots to understand human intention in the format of text-based instructions~\cite{tanneberg2024help}. 
However, such interaction is often high-latency and not suitable for real-time environments, limiting the potential for natural and effective human-robot collaboration, particularly in scenarios that require immediate response or adaptation to changing human needs. 
\citet{asfourARMAR6HighPerformanceHumanoid2019} designed rules of the real-time human-robot interaction, which is hard to scale up.
\citet{cardenas2024xbg} tries to learn an end-to-end model by imitation to achieve real-time interaction with 5 different tasks. Limited by the sample efficiency, this end-to-end paradigm makes it difficult to scale to more tasks.

Our framework decouples the interaction process and enables each module to model the interaction with different observation spaces, which is more sample-efficient and scalable. We also deploy the framework on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in two different scenarios and more than 20 tasks.
