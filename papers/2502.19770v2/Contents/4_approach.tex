

\section{TAPE Methodology} \label{muv_method}

%Tailored Posterior Difference for Auditing of Machine Unlearning

\subsection{Overview of the TAPE}

%Based on the previously introduction for machine unlearning auditing, w

%We propose a solution that trans a Reconstructor using posterior differences of unlearning to extract the unlearned information. 
We illustrate the overview methodology process in \Cref{fig_reconstructionattack}, which includes two main steps. 

%we employ a reconstructor model to extract the unlearned information from the posterior difference.

\noindent
\textbf{Unlearned Shadow Model Building.} In this step, we propose a method to quickly build the unlearned shadow models with only the user's samples. Our method utilizes the first-order influence estimation function to effectively estimate the unlearning influence and remove it from the original model, thus quickly mimicking the unlearned model to generate the posterior differences. 


\noindent
\textbf{Reconstructor Training.} We then train a reconstructor model to evaluate how much information about the erased samples is unlearned. An unlearned data perturbation strategy and an unlearned influence-based division strategy are proposed to augment the posterior differences for reconstruction for multiple samples. Both strategies are implemented utilizing the advantage that the unlearning user knows and prepares the unlearned samples.


%Existing privacy reconstructing methods~\cite{Hu2024sp,salem2020updates,balle2022reconstructing} are only suitable for recovering a single sample but fail to recover multiple samples for a model or posterior difference. 

%We address this limitation by proposing 

%, because the user specifies these unlearned samples as unlearning requests.


 
\subsection{Constructing Unlearned Shadow Model to Mimic Posterior Difference} \label{fast_g}


The unlearning user possesses a local dataset $D_{local}$, including the unlearned data $D_u$. %, which was once used to train the ML service model. Now, the user wants to unlearn $D_u$ from the ML service model and verify the unlearning effectiveness. 
As the unlearning verification is executed on the unlearning user side, the user can utilize the local dataset $D_{local}$ to construct unlearned shadow models to mimic posterior differences.  
%Only VBU \cite{nguyen2020variational} can implement unlearning with solely the unlearned samples; however, it is only suitable for Bayesian models.

\noindent
\textbf{Constructing Unlearned Shadow Model.} Many existing machine unlearning algorithms rely on the assistance of the remaining dataset $D \backslash D_u$. We propose a method based on the influence function theory \cite{koh2017understanding,basu2020second,bae2022if} in ML to quickly approximate an unlearned shadow model with only the unlearned data $D_u$. Specifically, when we remove $D_u$ from a trained model $\theta_t$ for unlearning, the empirical risk minimization (ERM) can be written as:
\begin{equation}
	\small
	\mathcal{L}_{D \backslash D_u}(\theta) = \frac{1}{n - m} \sum_{x \in D \backslash D_u} \ell (x; \theta),
\end{equation}
where $n$ is the size of $D$, $m$ is the size of $D_u$, and $\ell(x;\theta)$ is the loss. 

Similar to \cite{basu2020second}, we evaluate the effect of up-weighting a group of training samples on model parameters. Note that in this case, the updated weights must still form a valid distribution. Specifically, if a group of training samples is up-weighted, the weights of the remaining samples should be down-weighted to preserve the sum to one constraint of weights in the ERM formulation. 
We assume that the weights of samples in $D_u$ have been up-weighted all by $\epsilon$ and use $\frac{m}{n}$ to denote the fraction of up-weighted training samples. This results in a down-weighting of the rest of the training data by $\tilde{\epsilon} = \frac{m}{n-m} \epsilon$, to preserve the empirical weight distribution of the training dataset. Then, the ERM can be translated as:
\begin{equation} \label{loss_of_unlearning}
	\mathcal{L}^{\epsilon}_{D \backslash D_u} (\theta) = \frac{1}{n} (\sum_{x \in D \backslash D_u} (1 - \tilde{\epsilon})\ell(x;\theta) + \sum_{x \in D_u}(1+\epsilon)\ell(x;\theta)).
\end{equation}
In the above equation, if $\epsilon=0$, we get the original loss function $\mathcal{L}_{\emptyset}(\theta)$ (none of the training data points are unlearned) and if $\epsilon=-1$, we get the loss function $\mathcal{L}_{D \backslash D_u}(\theta)$ (specified samples are removed).

%\begin{equation}
%\theta^{\epsilon}_{D \backslash D_u} = \arg \min_{\theta} \mathcal{L}^{\epsilon}_{D \backslash D_u} (\theta),
%\end{equation}
%where

Let $\theta^{\epsilon}_{D \backslash D_u}$ denote the optimal parameters for $\mathcal{L}^{\epsilon}_{D \backslash D_u}$ minimization, and $\theta^*$ denote the optimal parameters trained on $D$. The unlearned shadow models can be approximately achieved by removing the estimated data influence from the trained model as follows,
\begin{equation} \label{shadow_model}
	\theta^{\epsilon}_{D \backslash D_u}  =  \theta_t  -  \frac{\epsilon}{n-m} \sum_{x_u \in D_u} \nabla \ell (x_u; \theta_t),
\end{equation}
where $\epsilon \in [-1,0]$ is used for unlearning, $m$ is the size of the erased dataset and $n$ is the size of the training dataset. $\Delta \theta \simeq  - \frac{\epsilon}{n-m} \sum_{x_u \in D_u} \nabla \ell (x_u; \theta_t)$ is the estimaed data influence at current trained model $\theta_t$. We omit the proof of the shadow model estimation in \Cref{shadow_model} as it is similar to the proofs in \cite{koh2017understanding,basu2020second}. Constructing the unlearned shadow model based on \Cref{shadow_model} only relies on the unlearned samples and is convenient for the user to implement. 

%%We present the proof of the data influence estimation in \Cref{first_order} at Appendix~\ref{proof_of_theorem_1}. 
%Calculating the model difference using the above approximation method has been demonstrated to be much more efficient than the naive retraining method.  During the practical training process, we employ the stochastic estimation method to further speed up the Hessian matrix calculation following \cite{koh2017understanding}. 

\iffalse
\begin{theorem}[Unlearned Shadow Model based on First-order Influence Estimation] 
	\label{first_order}
	When computing the influence of a few samples, such as $m$ samples, the scaling of the first-order $\theta^{(1)}$ is $\frac{m}{n}$, while the scaling of the second-order coefficient is $\frac{m^2}{n^2}$, which is very small when $n$ is large. Thus, the second-order term can be ignored, and the data influence at current model $\theta^*$ can be estimated as $\Delta \theta \simeq  - \frac{\epsilon}{n-m} \sum_{x_u \in D_u} \nabla \ell (x_u; \theta^*)$. Then unlearned shadow model can be achieved based on the estimation as 
	
	
\end{theorem}
\fi 
 


\noindent
\textbf{Mimicking Posterior Difference.} 
With the above method to construct unlearned shadow models, then, we can easily achieve the mimicked posterior differences. For instance, assuming the local dataset $D_{local}$ contains $m$ samples $X_1, X_2, ..., X_m$, we can construct $m$ unlearned shadow models $\theta_{D \backslash X_1}, \theta_{D \backslash X_2}, ..., \theta_{D \backslash X_m}$ for each sample, where $\backslash X$ means unlearning the sample $X$. Based on these unlearned shadow models, we can mimic the corresponding unlearned posterior, $\hat{Y}_{\backslash X_1, local}, \hat{Y}_{\backslash X_2, local}, ..., \hat{Y}_{\backslash X_m, local}$, using the local dataset. The posterior difference can be calculated through \Cref{posterior_diff}, denoted as $\delta_1, \delta_2, ..., \delta_m$, as shown in \Cref{fig_reconstructionattack}. Together with the corresponding shadow unlearning set's ground truth information, the training data for the reconstructor model to evaluate the unlearned information is derived.



%We can achieve the best auditing effect if we use the flattened parameters of the service model difference. However, using all the parameters as input for the Verifier and Reconstructor would be computationally expensive for large models. We design the multi-task information bottleneck (MT-IB) structure and only choose the difference of the informative representation layer to replace the whole model difference to improve the scalability. 


 
 

\subsection{Reconstructor Model Training with Two Strategies for Multiple Samples Auditing} \label{two_s}
 
%\subsubsection{} \label{recons}

\noindent
\textbf{Reconstructor Training for Unlearning Effectiveness Assessment.} Like \citep{salem2020updates}, we employ the autoencoder ($\texttt{AE}$) architecture to construct the Reconstructor, which includes an encoder and a decoder, as shown in \Cref{fig_reconstructionattack}(b). Its goal is to learn an efficient encoding for the posterior differences $\delta$. The encoder encodes the posterior difference into a latent vector $\mu$, and the decoder decodes the latent vector to reconstruct the unlearned samples. We employ mean squared error (MSE) as the loss function, $\mathcal{L}_{\texttt{AE}} = ||\hat{X_u} - X_u||^2_2,$ where $\hat{X_u} = \texttt{AE} (\delta_u)$ is the reconstructed sample for $X_u$. 

Existing studies \cite{salem2020updates, Hu2024sp, balle2022reconstructing} showed effective reconstruction for a single sample for the updated model difference. However, they are infeasible for reconstructing multiple samples. For unlearning effectiveness auditing, the unlearning user has the knowledge of the unlearned samples. With this advantage, we design two strategies: one augments posterior differences by perturbing unlearned data before unlearning and one augments posterior differences by individually dividing the posterior difference after unlearning, enabling evaluate how much information is unlearned for multiple samples. 


 

%\subsubsection{Unlearned Data Perturbation to Augment Posterior Difference} \label{data_Perturbation}

%Many unlearning methods directly compare the posterior difference between $\hat{Y}_{t, D_u}$ and $\hat{Y}_{u, D_u}$ to evaluate the unlearning effectiveness. Usually, they treat a degradation of the accurate prediction probability as a sign of successful unlearning for the unlearned model \cite{bourtoule2021machine,warnecke2024machine}. However, not all unlearning operations will cause a significant degradation of posterior probability on the unlearned samples. The model performance will remain, especially when there are some samples in the remaining dataset that are similar to the erased samples. It will also hinder the audit of unlearning effectiveness.  

\noindent
\textbf{Unlearned Data Perturbation before Unlearning.} 
We propose an unlearned data perturbation method to augment posterior difference, assisting the unlearning effectiveness verification. Specifically, we hope to introduce a perturbation $\Delta^p$ to the unlearned sample $X_u$ to augment the unlearned posterior for the reconstructor, so that it can effectively evaluate how much information is unlearned. At the same time, unlearning the perturbed specified data should maintain the unlearned model's utility on the remaining dataset. Since our final purpose is to improve the reconstructed information, we can formalize the unlearned data perturbation as follows to find the suitable perturbation. 
\begin{equation} \label{perturb_loss}
	\small
	\begin{aligned}
			&\min_{\Delta^p} \mathcal{L}_{\texttt{AE}} (\hat{X_u}', X_u + \Delta^p) \\
			&\text{s.t.} \hspace{3mm}  \Delta^p \in \arg \min_{\theta_{ \backslash (X_u + \Delta^p)}} \sum_{x \in D_r} \ell (x;\theta_{\backslash (X_u + \Delta^p)})
	\end{aligned}
\vspace{-2mm}
\end{equation}
where $\hat{X_u}' = \texttt{AE}(\delta_{\backslash (X_u + \Delta^p)})$, meaning that the samples are reconstructed based on the posterior difference that unlearns the perturbed data $X_u + \Delta^p$. We define the constraint that $ \Delta^p : \| \Delta^p \|_{\infty} \leq \alpha$ to ensure that the perturbed data will not be too different from the original data. We can combine these two losses together and treat them as two objectives, thus can be optimized with two-objective optimization methods \cite{sener2018multi,poirion2017descent,guo2020learning}. During the perturbation optimization process, we fix the trained model $\theta_t$ and the reconstruction model $\texttt{AE}$. We only update the perturbation $\Delta^p$ of $X_u$ to induce an augmented unlearned posterior difference $\delta_{\backslash (X_u + \Delta^p)}$, which improves the reconstruction effect. To find an effective perturbation, we can employ the restars technique, which is inspired from \cite{GeipingFHCT0G21,QinMGKDFDSK19}, and we provide the corresponding algorithm in \Cref{UDP_algorithm}. 

\iffalse
, and the corresponding algorithm is presented in \Cref{Unlearned_d_p}.


\begin{algorithm}[t]
	%\small
	\caption{Unlearning Data Perturbation (UDP)} \label{Unlearned_d_p}
	\begin{small} % small, normalsize
		\BlankLine
		\KwIn{Trained model $\theta^*$, reconstruction model $\texttt{AE}$, unlearned data $X_u$, perturbation limit $\alpha$, local dataset $D_{local}$ }
		\KwOut{The perturbed unlearning data, $X_u' = X_u + \Delta^{p}$} %, and three metrics for verification
		\SetNlSty{}{}{} % This line removes the vertical line before the for-loop
		\SetKwFunction{UDP}{\textbf{UDP}}
		\SetKwProg{Fn}{procedure}{:}{end procedure}
		\SetNlSty{}{}{} % This line removes the vertical line before the for-loop
		\Fn{\UDP{$\theta^*$, $\texttt{AE}$, $X_u$, $\alpha$, $D_{local}$ }}{
			\For{$r \gets 1$ \KwTo $R$ restarts}{
				$\Delta^{p}_{r} \gets \mathcal{N}(0,1)$  \hspace{4mm}    $\rhd$ Initialize random perturbation. \\
				\For{$i \gets 1$ \KwTo $m$ optimization steps}{
					$X_{u,i}^p \gets X_u + \Delta^{p}_r$  \hspace{0mm} $\rhd$ Add the perturbation to data. \\
					$\theta_{\backslash (X_{u,i}^p)} \gets \theta^* - \frac{\epsilon}{n-1} \nabla \ell (X_{u,i}^p;\theta^*)$ $\rhd$  According to \Cref{shadow_model}.  \\
					$\delta_{u,i}^p \gets \theta^*(D_{local}) - \theta_{\backslash (X_{u,i}^p)}(D_{local})$ $\rhd$ Calculate posterior difference according to \Cref{posterior_diff}.  \\
					$\nabla \mathcal{L}_{\texttt{AE}} \gets \nabla \mathcal{L}_{\texttt{AE}} (\texttt{AE}(\delta_{u,i}^p) , X_{u,i}^p)$ $\rhd$  According to \Cref{perturb_loss}. \\			
					$\Delta^{p}_{r} \gets \Delta^{p}_{r} - \eta \nabla \mathcal{L}_{\texttt{AE}} (\texttt{AE}(\delta_{u,i}^p) , X_{u,i}^p) $   \hspace{2mm} $\rhd$ Update perturbation with limitation $\|\Delta^{p}_{r}\|_{\infty} \leq \alpha $. \\
				}
			}
			Choose the optimal $\Delta^p_r$ with minimal value in $\mathcal{L}_{\texttt{AE}}$ as $\Delta^{p*}$.\\
			\Return $X_u' = X_u + \Delta^{p*}$
		}
	\end{small}
\end{algorithm}
\fi 


%\subsubsection{Unlearned Influence-based Division}


\noindent
\textbf{Unlearning Influence-based Division after Unlearning.} 
The unlearning influence-based division strategy utilizes the convenient properties of the first-order data influence estimation. After achieving the overall posterior differences for multiple samples $\delta_{\backslash D_u}$, the user can quickly estimate the basic data influence for each integrated sample $x_u \in D_u$, and we divide the overall posterior difference according to the weight of each sample's influence. We assume the divided posterior difference of the integrated sample obeys a Gaussian distribution:
%\vspace{-2mm}
\begin{equation} 
	\vspace{-2mm}
	\small
	\begin{aligned}
		\delta_{\backslash x_u}  \sim \mathcal{N}(\frac{\delta_{\backslash D_u} }{ \sum_{x_u \in D_u} \nabla  \ell (x_u;\theta_t)} \cdot \nabla  \ell (x_u;\theta_t), \sigma^2), \\
		\text{s.t.} \   \delta_{\backslash D_u} = \sum_{x_u \in D_u}  \delta_{\backslash x_u} ,
	\end{aligned}
%\vspace{-1mm}
\end{equation}
where we keep the divided posterior difference values as the mean and add a random deviation to it; meanwhile, we keep the sum of all the split slice posterior differences $\sum_{x_u \in D_u}  \delta_{\backslash x_u}$ equal to the overall posterior difference $\delta_{\backslash D_u}$. Thus, we ensure every reconstruction has a unique divided posterior difference without additional change or noise in the original $\delta_{\backslash D_u}$. This operation changes the reconstruction task for multiple samples based on the same $\delta_{\backslash D_u}$ as reconstructing every single sample of $D_u$ based on multiple divided posterior differences.  




\iffalse 
\vspace{2mm}
\noindent
\textbf{Masking.} Masking is mainly used to improve the reconstruction quality for multiple samples. This is because training a qualified Reconstructor for unlearning effectiveness assessment is much more difficult than training a Verifier for data removal verification based on the model difference. Although a regular autoencoder performs well on some simple datasets, it is hard to reconstruct a clear image on complex datasets. Compared with \cite{salem2020updates}, our advantage is that the server can access the full training data samples, enabling us to implement the masking strategy. 
Inspired by MAE \cite{he2022masked}, we randomly mask 70\% pixels of the specified-unlearned samples and assemble the masked samples with the model difference as the input for Reconstructor training. 
\begin{equation}
	\hat{X}_u = \text{Reconstructor}(\texttt{mask}(X_u) + \Delta_z \theta_{x_u})
\end{equation}
This strategy combines the core idea of unlearning difference reconstruction training and MAE, improving the reconstruction quality significantly.




\subsubsection{Verifier Training for Data Removal Verification} \label{verifier_describe}



Different from existing works that infer if samples are unlearned in a black-box setting \cite{chen2021machine,kurmanji2024towards,lu2022label}, we utilize the white-box access of the server to construct a more powerful Verifier to validate the unlearning execution. With access to the full training dataset, we directly construct the verification training data, $(\Delta \theta_{a, unl.}, Y_{unl.})$ as the positive samples and $(\Delta \theta_{a, rem.}, Y_{rem.})$ as the negative samples, to train the Verifier model. $\Delta \theta_{a, unl.}$ are the simulated model differences where the unlearned model is trained using the remaining dataset without the unlearned samples. 
And $\Delta \theta_{a, rem.}$ are the simulated model difference where the unlearned model is trained using the remaining dataset still containing the erased samples. We treat them as comparative samples and train a classifying model, Verifier, so that if the model difference of new unlearning request $\Delta \theta_{x_u}$ comes, we can infer if it has been unlearned.  
\fi 
