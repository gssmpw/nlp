 



% in \Cref{UEV_algorithm}.


\iffalse
\section{Datasets} \label{datasets_appendix}

\begin{table}[h]
	\scriptsize
	\caption{Dataset statistics.}
	\label{dataset_table}
	\vspace{-3mm}
	\resizebox{\linewidth}{!}{
		\setlength\tabcolsep{7.5pt}
		\begin{tabular}{cccc}
			\toprule[0.8pt]
			Dataset & Feature Dimension  & \#. Classes & \#. Samples \\
			\midrule
			MNIST & 28×28×1 & 10 & 70,000  \\  
			\rowcolor{verylightgray}
			CIFAR10 & 32×32×3 & 10 & 60,000  \\  
			STL-10 & 96x96x3 &10 & 5000 \\
			\rowcolor{verylightgray}
			CelebA & 178×218×3 & 2 (Gender) & 202,599 \\
			\bottomrule[0.8pt]
	\end{tabular}}
\end{table}

The statistics of all datasets used in our experiments are listed and introduced in \Cref{dataset_table}. MNIST, CIFAR10, and STL-10 are benchmark datasets utilized for 10-class image classification tasks, offering a range of objective categories with varying levels of learning complexity. Our experiment on CelebA is to identify the gender attributes of the face images. The task is a binary classification problem, different from the ones on MNIST, CIFAR10 and STL-10. We also introduce them below

\begin{itemize}
	\item \textbf{MNIST.} MNIST contains 60,000 handwritten digit images for the training and 10,000 handwritten digit images for the testing. All these black and white digits are size normalized, and centered in a fixed-size image with 28 × 28 pixels.
	\item \textbf{CIFAR10.} CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.
	\item \textbf{STL-10.} STL-10 dataset consists of 13,000 color images with 5,000 training images and 8,000 test images. STL-10 has 10 classes of airplanes, birds, cars, cats, dear, dogs, horses, monkeys, ships, and trucks with each image having a higher resolution of 96x96 pixels. Compared to the above two datasets, STL-10 can be considered as a more challenging dataset with higher learning complexity.
	\item  \textbf{CelebA.} CelebA is a large-scale face attributes dataset with more than 200,000 celebrity images, each with 40 attribute annotations, and the size of each image is 178×218.
\end{itemize}

\fi 




\section{The Verifier Training Process}  \label{verifi_train}



\begin{algorithm}[h]
	%\small
	\caption{Verifier Model Training (VMT)} \label{verifier_trianing}
	\begin{small} % small, normalsize
		\BlankLine
		\KwIn{Reconstruction model $\texttt{AE}$, posterior differences $\delta$, local dataset $D_{local}$, unlearned dataset $D_u$ }
		\KwOut{The Verifier Model $\mathcal{V}$}  
		\SetNlSty{}{}{} % This line removes the vertical line before the for-loop
		\SetKwFunction{VMT}{\textbf{VMT}}
		\SetKwProg{Fn}{procedure}{:}{end procedure}
		\SetNlSty{}{}{} % This line removes the vertical line before the for-loop
		\Fn{\VMT{$\texttt{AE}$, $\delta$, $D_{local}$, $D_u$}}{
			Initialize a verification dataset $D_{veri.}$ \\
			\For{$x_u$ in $D_u$, $x_i$ in $D_{local} \backslash D_u$ }{
				$D_{veri.}$ adds the positive sample ($\texttt{AE}(\delta_{\backslash x_u}), x_u; 1$) \\
				$D_{veri.}$ adds the negative sample ($\texttt{AE}(\delta_{\backslash x_u}), x_i; 0$) \\
			}
			Initialize a Verifier model $\mathcal{V}$ \\
			Train $\mathcal{V}$ on the constructed $D_{veri.}$ using a cross entropy loss  \\
			\Return the trained $\mathcal{V}$
		}
	\end{small}
\end{algorithm}


%The Verifier Model Training (VMT) algorithm is designed to construct a model that can distinguish between the unlearned and still remaining data instances. We first construct a verification dataset. For each instance in the unlearned dataset, the reconstructor model reconstructs information from the posterior difference that unlearns the instance, and we set the corresponding label equal to 1. For each instance in the local dataset that is not part of the unlearned dataset, we set a negative label for the instance and the reconstructed sample pair. These samples are added to the verification dataset. A verifier model is then initialized and trained on this constructed dataset using a cross-entropy loss. We return the final trained Verifier model.



This Verifier aims to identify if the recovered samples are unlearned samples. Specifically, we first construct a verification dataset $D_{veri.}$. For each instance in the unlearned dataset, and the reconstructor model reconstructs based on the posterior difference of the instance, and we set the corresponding label equal to 1. We add it as the postive sample ($\texttt{AE}(\delta_{\backslash x_u}), x_u; 1$) into $D_{veri.}$ For each instance in the local dataset that is not part of the unlearned dataset, we set a negative label for the instance and the reconstructed sample pair, i.e. ($\texttt{AE}(\delta_{\backslash x_u}), x_i; 0$). These samples are added to the verification dataset too. A verifier model is then initialized and trained on this constructed dataset using a cross-entropy loss. The Verifier model training algorithm is presented in \Cref{verifier_trianing}. %We return the final trained Verifier model.



 
\section{Metrics and Requirements for Auditing} \label{detailed_metrics}
%We now define the requirements of an effective solution to the unlearning effectiveness auditing problem. In terms of verification capacity and model utility, the scheme must be able to assess unlearning effectiveness, verify the data removal, and preserve functionality.




\begin{table*}[t]
	% \tiny
	\scriptsize
	\caption{Overall Evaluation Results on MNIST, CIFAR10, STL-10, and CelebA. 	\vspace{-2mm}}
	\label{tab_total}
	\resizebox{\linewidth}{!}{
		\setlength\tabcolsep{4.pt}
		\begin{tabular}{c|cccccccccccc}
			\toprule[1pt]
			\multirow{2}{*} { \makecell[c]{\textbf{Single-Sample} \\ \textbf{Unlearning Auditing}} } & \multicolumn{3}{c} {MNIST, $\text{\it ESS}=1$}& \multicolumn{3}{c} {CIFAR10, $\text{\it ESS}=1$} & \multicolumn{3}{c} {STL-10, $\text{\it ESS}=1$}  & \multicolumn{3}{c} {CelebA, $\text{\it ESS}=1$} \\
			\cmidrule(r){2-4}   \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13}
			& Original & MIB \cite{hu2022membership}  	 & TAPE	  & Original	& MIB    & TAPE	 & Original	 &   MIB    & TAPE  & Original	 &   MIB    & TAPE \\
			\midrule %\thinmidrule
			Running time (s)  	  & 620 &  	637								& \textbf{143}  		         &651  &  672	& \textbf{135}	 	&781 		& 815 & \textbf{79.81}  &1546 		& 1622 & \textbf{32.76} \\
			Model Utility (Acc.)	 &\textbf{99.14\%}       & 98.31\%   &\textbf{99.14\%}         & \textbf{81.62\%} 	  & 79.45\%   & \textbf{81.62\%}   &\textbf{68.99\%} & 67.54\% & \textbf{68.99\%}  & \textbf{96.93\% } & 96.05\%  & \textbf{96.93\% }  \\
			Rec. Sim.  		 	& -  		 & -  					&\textbf{0.965}   						 			& - & - & \textbf{0.974} & -  	   &-& \textbf{0.175}  & -  	   &-& \textbf{0.977} \\
			Unl. Verifiability     &  $0.00\%$   & $0.00\%$  &\textbf{99.43\%}    &   $0.00\%$ &  $0.00\%$ & \textbf{98.76\%}   &  $0.00\%$  &  $0.00\%$   & \textbf{84.00\%}  &  $0.00\%$ &  $0.00\%$ & \textbf{97.64\%} \\
			\midrule[0.12em]
			\multirow{2}{*} {\makecell[c]{\textbf{Multi-Sample} \\ \textbf{Unlearning Auditing}}} & \multicolumn{3}{c} {MNIST, $\text{\it ESS}=20$}& \multicolumn{3}{c} {CIFAR10, $\text{\it ESS}=20$} & \multicolumn{3}{c} {STL-10, $\text{\it ESS}=2$} & \multicolumn{3}{c} {CelebA, $\text{\it ESS}=20$} \\
			\cmidrule(r){2-4}   \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13}
			& Original & MIB \cite{hu2022membership}   	 & TAPE & Original & MIB   & TAPE	& Original	 &   MIB   & TAPE & Original	 &   MIB   & TAPE  \\
			\midrule 
			Running time (s)   & 613  & 638    &  \textbf{113}    & 644   & 673 			& \textbf{113} 	 & 781	& 809  & \textbf{74.90}   & 1570	& 1663  & \textbf{21.43}  \\
			Model Acc.	     & \textbf{99.05\%}   & 98.73\%    & \textbf{99.05\%}    & \textbf{81.62\%}    & 79.13\%    & \textbf{81.62\%} & \textbf{68.99\%} &  67.26\% &  \textbf{68.99\%}  &  \textbf{97.01\%} &  96.88\%  &  \textbf{97.01\%} \\
			Rec. Sim.  			    &-   			& -  			& \textbf{0.933}  			&-& -									& \textbf{0.973}					 	&- & - & \textbf{0.174}  &- & - & \textbf{0.970} \\
			Unl. Verifiability   	  & $0.00\%$   & $0.00\%$ & \textbf{98.67\%}     & $0.00\%$ & $0.00\%$ & \textbf{97.44\%}  	& $0.00\%$ & $0.00\%$ & \textbf{84.40\%} &  $0.00\%$ &  $0.00\%$  &  \textbf{94.57\%} \\
			\bottomrule[1pt]
	\end{tabular}}
	%	\vspace{-4mm}
\end{table*}




%\subsubsection{The Ability to Assess Unlearning Effectiveness and Verify Data Removal} \label{similarity_ability}

 
\noindent
\textbf{Data Removal Verifiability.} 
Existing backdoor-based unlearning verification methods can only provide the data removal verifiability based on the backdoor attack success rate \cite{guo2023verifying,hu2022membership}. We also train a Verifier (a classifying model) to identify the reconstructed data of the unlearned samples and the reconstructed data of the samples that still remain. We propose Verifiability to evaluate the accuracy of the Verifier, which calculates the correct classifying rate as 
\begin{equation} \label{verifiability_def}
	\textbf{Verifiability: } \hspace{4mm}  V =  \frac{ 1}{m}\sum_{x_u \in D_u} \mathbb{I}( \text{Verifier}(\delta_u, x_{u})=1),  
\end{equation}
where $m$ is the size of the unlearned dataset $D_{u}$ and $\mathbb{I}$ is the indicator function that equals 1 when its argument is true ($\text{Verifier}(\delta_u, x_{u})=1$) and 0 otherwise. 






\iffalse
%\subsubsection{Functionality Preservation}
\noindent
\textbf{Functionality Preservation.} 
The unlearning auditing scheme should not come at the cost of significant model utility degradation. In other words, the performance of the model after processing auditing should be only slightly worse than, if not equivalent to, the originally trained model. The formal definition is 
\begin{equation}
	\Pr_{(x,y) \in D} [\theta(x) =y]   \approx \Pr_{(x,y) \in D}[\theta_{\texttt{A}}(x)=y].
\end{equation}
It checks that the performance of the model with an auditing module $\theta_{\texttt{A}}$ does not deviate too much from those of original trained $\theta$.  

\fi




\section{Additional Experiments} \label{additional_exp}


\subsection{Overview Evaluation of TAPE} \label{overall_eval_app}

We demonstrate the overview evaluation results of different unlearning auditing methods on MNIST, CIFAR10, STL-10 and CelebA, presented in \Cref{tab_total}. The upper half of \Cref{tab_total} demonstrates the evaluations of the single-sample unlearning auditing, and the lower half of \Cref{tab_total} presents the evaluations of the multi-sample unlearning auditing. The bolded values indicate the best performance among the compared methods. We fill a dash when the method does not contain the evaluation metrics. 


 
\noindent
\textbf{Setup.}
We measure auditing methods based on the four above-introduced evaluation metrics in single-sample and multi-sample unlearning scenarios. In single-sample verification, the Erased Sample Size ({\it ESS}) is equal to 1 and $\text{\it ESS}=20$ for the multi-sample scenario. On STL-10, we set $\text{\it ESS}=2$ for the multi-sample scenario, as STL-10 only contains 5000 training samples, which is much smaller than other datasets. The evaluation here is tested based on the retraining-based unlearning method SISA \cite{bourtoule2021machine}. To better illustrate the functionality preservation and efficiency, we record the performance of solely training the original model, shown as ``Original'' in \Cref{tab_total}.

 

\noindent
\textbf{Evaluation of Efficiency.}
Since TAPE does not involve the original model training process, it consumes much less running time than MIB and ``Original''. The ``Original'' is training the original model before unlearning, and the MIB method needs to backdoor the model during the initial model training process before unlearning. Specifically, TAPE achieves more than $4.5\times$ speedup in efficiency on MNIST, $5\times$ speedup on CIFAR10, $10\times$ speedup on STL-10, and $50\times$ speedup on CelebA. On CelebA, the best speedup is up to $75\times$.




\noindent
\textbf{Evaluation of Functionality Preservation.}
The effect of functionality preservation is measured by model accuracy. In both single-sample and multi-sample unlearning auditing, our TAPE always achieves better functionality preservation than MIB. The highest accuracy preservation is around $2\%$, achieved on CIFAR10. The reason is that the MIB method needs to mix backdoored samples into the training dataset, and the backdoored samples with modified labels will negatively influence model utility. On the contrary, the TAPE scheme is independent of the original model training process; hence, our method will not influence the model utility of the original ML service model, keeping the same model accuracy as ``Original'', demonstrating better functionality preservation.


\noindent
\textbf{Evaluation of Unlearning Auditing Effect.}
We use reconstruction similarity to measure how much information about the specified samples is unlearned. The MIB method is unable to provide such an assessment of unlearned information for evaluation of unlearning effectiveness. Hence, we fill a dash of MIB in this metric. Reconstruction for a single sample always achieves better results than for multiple samples, which confirms our previous analysis and existing works \cite{salem2020updates,balle2022reconstructing}. The unlearned posterior difference of a single sample contains more information about such a sample than a posterior difference of multiple samples, as information from multiple samples is interwoven together in one posterior difference.  


To align with existing unlearning verification methods, we propose the verifiability metric to evaluate the data removal status, which is defined in \Cref{verifiability_def}. Since the erased sample size is small and only genuine unlearned samples are evaluated in this experiment, the MIB cannot successfully verify the unlearning of any genuine samples in \Cref{tab_total}. In both single-sample and multi-sample unlearning scenarios, TAPE provides effective data removal verification (accuracy larger than $95\%$ on MNIST, CIFAR10, and CelebA). Moreover, data removal status verification for a single sample always achieves better results than for multiple samples.





\iffalse
\begin{tcolorbox}[colback=white, boxrule=0.3mm]
	\noindent \textbf{Takeaway 1.} TAPE achieves the best efficiency and model functionality preservation as our scheme is independent of the original model training. Moreover, TAPE is effective in providing unlearning auditing for genuine unlearned samples, while MIB does not achieve this.
\end{tcolorbox}
\fi 

%(both how much information is unlearned and data removal status verification)


\subsection{Impact on Efficiency of Erased Samples Size ($\text{\it ESS}$)}  \label{imp_of_efficiency_ESS}


\noindent
\textbf{Impact on Efficiency.} 
The main components of the running time of TAPE are building unlearned shadow models and training the reconstructor. The running time of these two processes is highly related to the size of the user's local dataset. In our experiments, we randomly select $0.5\%$ samples on MNIST and CIFAR10 and choose $0.06\%$ samples on CelebA as the local dataset.

\Cref{evaluation_of_running_time} shows the running time of TAPE and MIB on the three datasets. The running time of TAPE has no significant relationship with the $\text{\it ESS}$ because the running time of TAPE (shadow model building and reconstructor training) highly depends on the size of the user's local dataset. For MIB, the running time has no obvious variations when $\text{\it ESS}$ increases. This is because the MIB verification preparation is accompanied by the original model training, which is heavily related to the size of training datasets. 
TAPE has a much more efficient running time compared with MIB, as TAPE is independent of the original model training. 



\begin{figure}[t]
	\centering
	%\hspace{-3mm}
	%\vspace{-2mm}
	\subfloat{    
		\includegraphics[scale=0.4]{Contents/Figures/Experiments_r/On_MNIST/Running_time/mnist_rt_sample_size_bar}
	}
	\vspace{-1mm}
	\caption{Running time about different $ESS$.}
	\vspace{-2mm}
	\label{evaluation_of_running_time} 
\end{figure}









\iffalse


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.97\linewidth]{../../../../PycharmProjects/MUV_by_reconstruction/Experiments/On_MNIST/Ablation_exp/second_order_compare}
	\vspace{-2mm}
	\caption{Comparison between the first-order and second-order model differences. The legends stand for the first-order and second-order model difference when the erased data is ``In'' or ``Not In'' the remaining dataset.}
	\vspace{-2mm}
	\label{fig:secondordercompare}
\end{figure*}




\section{Additional Ablation Study Experiments} \label{ad_exp}








\subsection{Impact of first-order and second-order influence estimation for unlearned shadow model building}

Usually, the first-order model difference approximation is effective when the size $m$ of $D_u$ is small. However, when computing the group of many erased samples model difference, the second-order coefficient is in the order of $\frac{m^2}{n^2}$, which can be large when the size of $D_u$ is large. In this situation, we need to take both first-order perturbation $\theta^{(1)}$  and second-order perturbation $\theta^{(2)}$ into account. %Below is the corresponding calculation for second order model difference approximation. 
We present the experiments of the first-order and second-order model differences in \Cref{fig:secondordercompare}.
Theoretically, the MUA-MD of the second-order model difference will perform better than the MUA-MD of the first-order model difference because the second-order model difference approximation includes additional second-order information. In \cite{basu2020second}, the authors conducted experiments to remove 50\% samples to demonstrate the improvement of the second-order influence approximation. 
However, since we only unlearn from 1 to 100 samples, the second-order coefficient in this situation is from $\frac{1}{60000^2}$ to $\frac{100^2}{60000^2}$, which is still very small.  Hence, in this scenario, the impact of the second order is minimal, resulting in similar model performances. % when evaluated from the perspectives of Average UE, reconstruction similarity, and verifiability. From the running time perspective, calculating the second-order model difference takes more time than only calculating the first-order model difference, which is apparent when $\it{ESS}=1$.


\begin{theorem}[Second order model difference approximation] 
	\label{second_order}
	If the third-derivative of the loss function at $\theta^*$ is sufficiently small, the second-order influence function for a group of erased samples $D_u$ is:
	\begin{equation}
		\Delta \theta \simeq \mathcal{I}^{(2)}(D_u) = \mathcal{I}^{(1)}(D_u) + \mathcal{I}'(D_u)
	\end{equation}
	where 
	\begin{equation}
		\small
		\mathcal{I}'(D_u) = \frac{1}{n-m} (I - (\nabla^2 L_{\emptyset}(\theta^*))^{-1} \cdot \frac{1}{m} \sum_{x_u \in D_u} \nabla^2 L(x_u;\theta^*) ) \mathcal{I}^{(1)} (D_u).
	\end{equation}
\end{theorem}
\fi



%in \cite{}, they conducted experiments to remove 50\% samples to demonstrate the improvement of the second-order influence approximation.

% that's all folks


\iffalse
\section{Preliminaries}\label{pr_section}
\subsection{Machine Unlearning}
%Recent legislation such as GDPR and CCPA enact the ``right to be forgotten'', which allows individuals to request the removal of their specified data from trained models to preserve their privacy. 

%In machine unlearning, the model server should remove the influence of the specified samples $D_u$ from models that were trained based on the training set $D$ \cite{cao2015towards,neel2021descent}. The server needs to guarantee that the unlearned model should perform like the model that is retrained without seeing the erased samples $D_u$. 

%There are already many machine unlearning methods. Here, we briefly introduce the \textit{gold-standard} retraining method and two representative \textit{exact} and \textit{approximate} unlearning methods.



\noindent
\textbf{Retraining from Scratch.} The gold-standard machine unlearning method is to retrain the whole model from scratch. Formally, there is already a trained model with optimal parameters denoting as $\theta^*$, and its corresponding training dataset is $D$. This approach retrains a new model $\theta_u$ on the remaining dataset $D_r = D \backslash D_u$, where $D_u$ is the requested unlearning dataset. We call this $\theta_u$ the parameters of the unlearned model. Although retraining one hundred percent satisfies the requirements of unlearning, the computational and storage overhead of retraining is too large when the original dataset $D$ is large, and the unlearning requests are frequent. 

%To reduce the computational cost, several efficient exact and approximate approaches have been proposed.

\vspace{2mm}
\noindent
\textbf{SISA.} 
SISA is a successful unlearning extension based on naive retraining to improve the unlearning efficiency \cite{bourtoule2021machine,koch2023no}. Most other exact unlearning methods adopt a similar assemble strategy like SISA \cite{chen2022graph,yan2022arcane,chen2022recommendation}. The main process of SISA divides the full data $D$ into several shards $D^1, D^2, ..., D^k$ and trains sub-models with parameters $\theta^1, \theta^2, ..., \theta^k$ for each shard. When the server receives a request for unlearning sample $x_u$, it just needs to retrain the sub-model $\theta^i$ of shard $D^i$ that contains $x_u$. Since the size of the shard, $D^i$, is much smaller than $D$, the computational overhead of SISA is much smaller than the naive retraining method.



\vspace{2mm}
\noindent
\textbf{HBU.} Hessian matrix based unlearning (HBU) is a representative approximate unlearning method that aims to unlearn a posterior directly based on the original model quickly \cite{sekhari2021remember,guo2019certified,mehta2022deep,warnecke2021machine}. \citeauthor{guo2019certified} \cite{guo2019certified} proposed a certified-removal mechanism, which is based on the Hessian matrix and achieves unlearning by Newton Update. An example of unlearning one sample $x_u$ can be defined as 
\[
\theta_u = \theta^* + H^{-1}_{\theta^*} \nabla L(x_u, \theta^*),
\]
where $H^{-1}$ is the inverse Hessian matrix of loss function $L(D \backslash x_u,\cdot)$ at $\theta^*$, denoted as $H_{\theta^*} = \nabla^2 L(D \backslash x_u, \theta^*)$. Other approximate unlearning methods that directly unlearn based on the original trained models also employ the update equation like this but with different unlearning loss functions \cite{nguyen2020variational,fu2022knowledge,wang2023machine}.



\subsection{Verification of Machine Unlearning}
Most existing unlearning verification methods utilize backdoors to assist in verifying whether the data is unlearned \cite{hu2022membership,sommer2022athena,guo2023verifying}. These verification methods mixed backdoored samples $D_b$ with users' normal data $D_u$ before uploading for original model training, which can be described as $D_{u, b} = D_u \cup D_b$. By mixing a certain number of backdoored samples, the model will be backdoored during the model training process.  
Then, the user can verify if his/her data is learned or unlearned in the model by testing whether backdoors can attack the model. However, we find that the $D_u$ and $D_b$ mixed in the user's dataset $D_{u,b}$ are actually two distinct sub-sets. The appearance or disappearance of the backdoor can only verify if the backdoored data $D_b$ is used or not used in the model, while it cannot represent the users' normal data $D_u$ is used or not.



To better illustrate the different model performance of $D_u$ and $D_b$ during training, we conduct an approximate unlearning (VBU~\cite{nguyen2020variational}) on MNIST, presented in \Cref{fig:mnistepochaccdrop}. The model accuracy diminished rapidly on $D_b$ while slowly on unlearned data $D_u$ and test data during the unlearning training process, which was also studied in \cite{gao2023backdoor}. The evidence clearly shows that backdoor accuracy only indicates whether backdoored samples are unlearned in the model, while it does not serve as proof of unlearning for normal data.

%an experiment to show their performance when executing, The model accuracy performs variously on backdoored data $D_b$ and normal data $D_u$ during the unlearning training process. 

\fi
