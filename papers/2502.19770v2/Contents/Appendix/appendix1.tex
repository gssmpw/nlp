



\begin{table*}[h]
	% \tiny
	\scriptsize
	\caption{An overview of machine unlearning auditing methods. %\vspace{-2mm}
	}
	\label{overview_of_auditing_method}
	\resizebox{\linewidth}{!}{
		\setlength\tabcolsep{3.pt}
		\begin{tabular}{c|cccccccc}
			\toprule[1pt]
			\multirow{2}{*} { \makecell[c]{\textbf{Unlearning} \\ \textbf{Auditing} \\ \textbf{Methods}} } & \multicolumn{2}{c} {\textbf{Involving Processes}  } & \multicolumn{2}{c} { \textbf{Auditing Data Type}} & \multicolumn{2}{c} {\textbf{Unlearning Methods}} & \multicolumn{2}{c} { \textbf{Unlearning Scenarios}}  \\
			\cmidrule(r){2-3}   \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
			& \makecell[c]{{Original training} \\ {and unlearning	}  }  & \makecell[c]{{Only unlearning } \\ {process }  } & \makecell[c]{{Backdoored (marked)} \\ {samples	}  }    & \makecell[c]{{Genuine} \\ { samples	}  }   & \makecell[c]{{Exact} \\ {unlearning}  }    &\makecell[c]{{Approximate} \\ {unlearning}  } &   \makecell[c]{{Single} \\ {sample	}  }  &   \makecell[c]{{Multi} \\ {samples	}  }   \\ 
			\midrule
			MIB~\cite{hu2022membership} &\filledcircle & \emptycircle & \filledcircle&\emptycircle	&\filledcircle & \emptycircle &\emptycircle   	  & \filledcircle  \\
			Athena~\cite{sommer2022athena} &\filledcircle & \emptycircle & \filledcircle  &\emptycircle 	&\filledcircle &\emptycircle  &\emptycircle      & \filledcircle  \\
			Verify in the dark~\cite{guo2023verifying} &\filledcircle & \emptycircle & \filledcircle  &\emptycircle  &\filledcircle & \emptycircle &\emptycircle       & \filledcircle	  \\
			Verifi~\cite{gao2024verifi} &\filledcircle &\emptycircle & \filledcircle  &\emptycircle  	&\filledcircle & \emptycircle &\emptycircle       	  & \filledcircle  \\
			TAPE (Ours)	     &\emptycircle & \filledcircle & \emptycircle  & \filledcircle  	&\filledcircle  &\filledcircle & \filledcircle       	  & \filledcircle  \\
			\bottomrule[1pt]
	\end{tabular}}
	\begin{tabbing}
		\filledcircle: the auditing method is applicable; 
		\emptycircle: the auditing method is not applicable.
	\end{tabbing}
	\vspace{-2mm}
\end{table*}

\section{Additional Related Work Discussion} \label{different_with_existing}

\subsection{Machine Unlearning in the Web-Related Studies} 
Machine unlearning--the process of efficiently removing specific data influences from trained models--has been explored in diverse applications across Web-based systems, such as graph-based systems and personalized applications \citep{lin2024incentive,pan2023unlearning,zhu2023heterogeneous,wu2023gif}. In graph-based systems, \citeauthor{pan2023unlearning} \citep{pan2023unlearning} proposed an unlearning method to unlearn the graph classifiers with limited access to the original data, and \citeauthor{wu2023gif} \citep{wu2023gif} introduced a general strategy leveraging influence functions to efficiently remove specific graph data while preserving model integrity. In personalized applications, \citeauthor{lin2024incentive} \citep{lin2024incentive} introduced dynamic client selection with incentive mechanisms to enhance the federated unlearning efficiency, while \citep{zhu2023heterogeneous} extended federated unlearning to the heterogeneous knowledge graph, aiming to balance both privacy and model utility preservation. To achieve a better unlearning service, \cite{liu2024breaking} further explored the challenge of balancing privacy, utility, and efficiency and proposed a controllable unlearning framework to overcome this challenge.

\subsection{Difference from Existing Studies}  
Our TAPE approach is significantly different from existing unlearning verification methods \cite{hu2022membership,sommer2022athena,guo2023verifying,gao2024verifi} in terms of the involving processes, auditing data type, unlearning scenarios, and unlearning methods, as depicted in \Cref{overview_of_auditing_method}. First, the significant difference is that the auditing of our method only involves the unlearning process, while the backdoor-based methods must involve both the original training and unlearning processes to ensure the service model first learns the backdoor. Second, most existing auditing methods are based on backdooring techniques and need to backdoor or mark samples for verification \cite{hu2022membership,sommer2022athena,guo2023verifying,gao2024verifi}. As we analyzed in the above subsection, they can only validate the backdoored samples and are only applicable to the exact unlearning methods as exact unlearning methods guarantee the deletion from the dataset level. Our method does not mix any other data to the training dataset, and the auditing is based on the posterior difference, which is suitable for genuine samples in both exact and approximate unlearning methods. Third, backdoor-based auditing methods are only feasible for multi-sample unlearning scenarios because just using a single sample makes it hard to backdoor the model \cite{wang2019neural,lin2020composite,zeng2023narcissus,nguyen2020input}, hence failing to provide unlearning verification for a single sample. 



%We also note that TAPE shares similarities with some studies investigating privacy leakage caused by the model updated difference \cite{salem2020updates,chen2021machine,balle2022reconstructing,Hu2024sp}. They aimed to extract as much private information as possible from model differences. However, it is well-known that while these methods are effective for single-sample reconstruction, they are less effective for multi-sample reconstruction. We have one advantageous difference to ensure we can provide a more effective information reconstruction that is suitable for unlearning auditing for multi-samples. Specifically, the unlearning verification user knows the unlearned samples, as users specify these samples, while the settings in \cite{salem2020updates,chen2021machine,balle2022reconstructing,Hu2024sp} have no information about the inferred samples. With the knowledge of the unlearned samples, we can design the posterior augment methods to facilitate the unlearning auditing for multiple samples.



 

\section{MLaaS Scenario and Threat Model} \label{threat_model}

Our problem is introduced in a simple machine unlearning as a service (MLaaS) scenario for ease of understanding. Under the MLaaS scenario, there are two main entities involved: an ML server that collects data from users, trains models, and provides the ML service, and users that contribute their data for ML model training. 

\noindent
\textbf{The ML Server's Ability.}
To uphold the ``right to be forgotten'' legislation and establish a privacy-protecting environment, the ML server is responsible for conducting machine unlearning operations. However, it is challenging to audit the unlearning effect for users to confirm that the unlearning is processed and prevent the spoof of unlearning from the ML server. In alignment with common unlearning verification settings \cite{hu2022membership,guo2023verifying}, we assume the ML server is honest for learning training but may spoof users for unlearning, i.e., it reliably hosts the learning process but may deceive users during unlearning operations by pretending unlearning has been executed when it has not. It is reasonable for the ML server to pretend to execute unlearning operations to avoid the degradation of model utility. Moreover, this assumption is more plausible than assuming the server will forge an unlearning update~\cite{thudi2022necessity}. Forging an unlearning update would require the server to simulate the disappearance of specified data and the corresponding resulting in model utility degradation, which demands significant effort without any benefit, making it an unlikely motivation. 

\noindent
\textbf{The Unlearning Users' Ability.}
We consider the scenario where the unlearning user has only black-box access to the ML service model, which is one of the most challenging scenarios \cite{salem2020updates,Hu2024sp}. In unlearning scenarios, the unlearning user possesses a local dataset, including the erased samples, which constitutes the entire training dataset for the ML service model \cite{warnecke2024machine,hu2024eraser}; however, the user has no access to the entire dataset. This just allows the user to query the model with their own data in a black-box access to obtain the corresponding posteriors and design the unlearning requests with specific data for unlearning verification purposes. 
Furthermore, we assume the unlearning user knows the unlearning algorithms, which is confirmed by both server and users, commonly used in other works \cite{hu2023duty}. However, even if the unlearning user knows the algorithms, without the remaining dataset, the user still cannot achieve the corresponding unlearning results of most unlearning algorithms. To relax the difficulty, we consider the unlearning user to be able to establish the same ML model as the current target ML service model with respect to model architecture. This can be achieved through model hyperparameter stealing attacks \cite{wang2018stealing,Seong2018towards,salem2020updates}. The unlearning user leverages this knowledge to simulate the unlearned shadow models and mimic the behavior of the ML service model based on the designed unlearning requests, thereby deriving the posterior differences necessary for training the reconstruction model to evaluate the unlearning effectiveness. % \Cref{fast_g}.   

 
 
 \section{Unlearning Data Perturbation (UDP) Algorithm} \label{UDP_algorithm}
 
 
 \Cref{Unlearned_d_p} demonstrates how to use the R restarts to find the satisfied perturbation for the unlearning data to augment the posterior difference for auditing.
 
 \begin{algorithm}[t]
 	%\small
 	\caption{Unlearning Data Perturbation (UDP)} \label{Unlearned_d_p}
 	\begin{small} % small, normalsize
 		\BlankLine
 		\KwIn{Trained model $\theta^*$, reconstruction model $\texttt{AE}$, unlearned data $X_u$, perturbation limit $\alpha$, local dataset $D_{local}$ }
 		\KwOut{The perturbed unlearning data, $X_u' = X_u + \Delta^{p}$} %, and three metrics for verification
 		\SetNlSty{}{}{} % This line removes the vertical line before the for-loop
 		\SetKwFunction{UDP}{\textbf{UDP}}
 		\SetKwProg{Fn}{procedure}{:}{end procedure}
 		\SetNlSty{}{}{} % This line removes the vertical line before the for-loop
 		\Fn{\UDP{$\theta^*$, $\texttt{AE}$, $X_u$, $\alpha$, $D_{local}$ }}{
 			\For{$r \gets 1$ \KwTo $R$ restarts}{
 				$\Delta^{p}_{r} \gets \mathcal{N}(0,1)$  \hspace{4mm}    $\rhd$ Initialize random perturbation. \\
 				\For{$i \gets 1$ \KwTo $m$ optimization steps}{
 					$X_{u,i}^p \gets X_u + \Delta^{p}_r$  \hspace{0mm} $\rhd$ Add the perturbation to data. \\
 					$\theta_{\backslash (X_{u,i}^p)} \gets \theta^* - \frac{\epsilon}{n-1} \nabla \ell (X_{u,i}^p;\theta^*)$ $\rhd$  According to \Cref{shadow_model}.  \\
 					$\delta_{u,i}^p \gets \theta^*(D_{local}) - \theta_{\backslash (X_{u,i}^p)}(D_{local})$ $\rhd$ Calculate posterior difference according to \Cref{posterior_diff}.  \\
 					$\nabla \mathcal{L}_{\texttt{AE}} \gets \nabla \mathcal{L}_{\texttt{AE}} (\texttt{AE}(\delta_{u,i}^p) , X_{u,i}^p)$ $\rhd$  According to \Cref{perturb_loss}. \\			
 					$\Delta^{p}_{r} \gets \Delta^{p}_{r} - \eta \nabla \mathcal{L}_{\texttt{AE}} (\texttt{AE}(\delta_{u,i}^p) , X_{u,i}^p) $   \hspace{2mm} $\rhd$ Update perturbation with limitation $\|\Delta^{p}_{r}\|_{\infty} \leq \alpha $. \\
 				}
 			}
 			Choose the optimal $\Delta^p_r$ with minimal value in $\mathcal{L}_{\texttt{AE}}$ as $\Delta^{p*}$.\\
 			\Return $X_u' = X_u + \Delta^{p*}$
 		}
 	\end{small}
 \end{algorithm}
 


 \iffalse

\section{Proof of \Cref{first_order}} \label{proof_of_theorem_1}

\iffalse
The changes in the model parameters can be expand using the perturbation theory \cite{avrachenkov2013analytic} as:
\begin{equation} \label{expanding_loss}
	\Delta \theta = \theta^{\epsilon}_{D \backslash D_u} - \theta^* = \mathcal{O}(\epsilon)\theta^{(1)} + \mathcal{O}(\epsilon^2)\theta^{(2)} + \mathcal{O}(\epsilon^3)\theta^{(3)} + \cdot \cdot \cdot,
\end{equation}
where each unlearning sample in $D_u$ is up-weighted by a factor of $\epsilon$.  $\theta^{(1)}$ denotes the first-order (in $\epsilon$) perturbation and $\theta^{(2)}$ is the second-order model perturbation.
\fi

%\subsection{Proof of \Cref{first_order}} 
\begin{proof}
	We provide a derivation of the first-order model difference approximation $\Delta \theta \simeq \frac{\epsilon}{n-m} \sum_{x_u \in D_u} \nabla \ell (x_u; \theta^*)$ in \Cref{first_order}. %in the context of loss minimization (M-estimation).
	We define that $\theta^*$ minimizes the empirical risk: 
	\begin{equation}
		R(\theta) \overset{\text{def}}{=} \frac{1}{n} \sum_{x_i \in D} \ell (x_i;\theta),
	\end{equation}
	where $n$ is the size of the training dataset $D$.
	We assume that $R$ is strictly twice-differentiable and convex in $\theta$, thus, we can positively define 
	\begin{equation}
		H_{\theta^*} \overset{\text{def}}{=} \nabla^2 R(\theta^*) = \frac{1}{n} \sum_{x_i \in D} \nabla^2_{\theta} \ell (x_i;\theta).
	\end{equation}
	When removing an unlearning dataset $D_u$ with size $m$, $\theta^{\epsilon}_{D \backslash D_u}$ will be the optimal parameter set for the interpolated loss function $\mathcal{L}^{\epsilon}_{D \backslash D_u}(\theta)$, as shown in \Cref{loss_of_unlearning}. Due to the first-order stationary condition, we have
	\begin{equation}\label{nabla_loss_of_unlearning}
		\begin{aligned}
			0 = \nabla \mathcal{L}_{D \backslash D_u}^{\epsilon} (\theta^{\epsilon}_{D \backslash D_u})& =  \nabla \mathcal{L}_{\emptyset}(\theta^{\epsilon}_{D \backslash D_u}) \\
			&+\frac{1}{n} ( - \tilde{\epsilon} \sum_{x \in D \backslash D_u} +\epsilon \sum_{x \in D_u}) \nabla \ell (x;\theta^{\epsilon}_{D \backslash D_u}).
		\end{aligned}
	\end{equation}
	Let $\theta^{\epsilon}_{D \backslash D_u}$ denote the optimal parameters for $\mathcal{L}^{\epsilon}_{D \backslash D_u}$ minimization, and $\theta^*$ denote the optimal parameters trained on $D$. The changes in the model parameters can be expand using the perturbation theory \cite{avrachenkov2013analytic} as:
	\begin{equation} \label{expanding_loss}
		\Delta \theta = \theta^{\epsilon}_{D \backslash D_u} - \theta^* = \mathcal{O}(\epsilon)\theta^{(1)} + \mathcal{O}(\epsilon^2)\theta^{(2)} + \mathcal{O}(\epsilon^3)\theta^{(3)} + \cdot \cdot \cdot,
	\end{equation}
	where each unlearning sample in $D_u$ is up-weighted by a factor of $\epsilon$.  $\theta^{(1)}$ denotes the first-order (in $\epsilon$) perturbation and $\theta^{(2)}$ is the second-order model perturbation. 
	
	The main idea is to use Taylor series for expanding $\nabla \mathcal{L}_{\emptyset}(\theta^{\epsilon}_{D \backslash D_u})$ around $\theta^*$ base on the perturbation series defined in \Cref{expanding_loss} and compare the terms of the same order in $\epsilon$:
	\begin{equation} \label{expanding_loss_delta}
		\nabla \mathcal{L}_{\emptyset}(\theta^{\epsilon}_{D \backslash D_u}) = \nabla \mathcal{L}_{\emptyset}(\theta^*) + \nabla^2 \mathcal{L}_{\emptyset}(\theta^*)(\theta^{\epsilon}_{D \backslash D_u} - \theta^*) + \cdot \cdot \cdot.
	\end{equation}
	Similarly, we can also expand $\nabla \ell (x;\theta^{\epsilon}_{D \backslash D_u})$ around $\theta^*$ using Taylor series expansion. To derive $\theta^{(1)}$, we expand \Cref{nabla_loss_of_unlearning} and compare the terms with coefficient $\mathcal{O}(\epsilon)$: 
	\begin{equation}
		\begin{aligned}
			&\epsilon \nabla^2 \mathcal{L}_{\emptyset}(\theta^*) \theta^{(1)} \\
			&=\frac{1}{n} (  \tilde{\epsilon} \sum_{x \in D \backslash D_u} - \epsilon \sum_{x \in D_u})  \nabla \ell (x;\theta^*) \\
			&= \tilde{\epsilon}  \nabla \mathcal{L}_{\emptyset}(\theta^*)  - \frac{1}{n} (  \tilde{\epsilon}  + \epsilon)  \sum_{x\in D_u} \nabla \ell (x;\theta^*) \\
			& = -\frac{1}{n} (  \tilde{\epsilon}  + \epsilon  )  \sum_{x\in D_u}  \nabla \ell (x;\theta^*) \\
			& = -\frac{1}{n-m} \epsilon   \sum_{x\in D_u}  \nabla \ell (x;\theta^*).
		\end{aligned}
	\end{equation}
	$\theta^{(1)}$ is the first-order approximation of the group influence function. $\epsilon \in [-1,0]$ is used for unlearning.
\end{proof}
%(1 - )L(x;\theta)  (1)L(x;\theta)

\fi 