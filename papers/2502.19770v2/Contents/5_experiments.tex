
\section{Performance Evaluation}
\subsection{Settings} \label{exp_setting}

%In this section, we first introduce the datasets, models, and evaluation metrics used in the experiments. Then, we introduce the compared state-of-the-art unlearning verification methods and the audited unlearning benchmarks. 







%\subsection{Experimental Setup}

%\subsubsection{Datasets and Compared Methods}

%\subsection{Datasets and Models}
\noindent
\textbf{Datasets.}
We conducted experiments on four widely adopted public datasets: MNIST \cite{deng2012mnist}, CIFAR10 \cite{krizhevsky2009learning}, STL-10 \cite{coates2011analysis}, and CelebA \cite{liu2018large}. These datasets offer a range of objective categories with varying levels of learning complexity. %, and the corresponding statistics are listed and introduced in \Cref{datasets_appendix}.


\noindent
\textbf{Models.}
In our experiments, we select a 5-layer multi-layer perceptron~(MLP) connected by ReLU, a 7-layer convolutional neural network (CNN), and ResNet-18. Specifically, we use two 5-layer MLP models, one as the encoder and one as the decoder, to consist of the reconstructor for MNIST. We use two 7-layer CNNs to consist of the reconstructor for CIFAR10, STL-10, and CelebA. The main structure of the ML service model is implemented with a ResNet-18. Moreover, to align with existing backdoor-based verification methods, we also train a Verifier model using a 5-layer MLP model, which is trained after reconstruction. The Verifier model training algorithm is presented in \Cref{verifi_train}.


 %and the training process of a Verifier model is introduced in \Cref{verifi_train}. 

%The Verifier Model Training (VMT) algorithm is designed to construct a model that can distinguish between the unlearned and still remaining data instances. 

%On the MNIST dataset, we set the learning rate $\eta = 0.001$. On CIFAR10, STL-10, and CelebA, we set the learning rate $\eta=0.0005$. During training, we set the minibatch size to $16$ on MNIST, CIFAR10 and STL-10, and the minibatch size to $160$ on CelebA. All algorithms are implemented using Pytorch 3.8, and the experiments are conducted on NVIDIA Quadro RTX 6000 GPUs.




%\subsection{Metric}
\noindent
\textbf{Metric.}
We use three metrics, model accuracy, reconstruction similarity, and verifiability, to measure the ability previously defined for the unlearning auditing scheme. Moreover, we use the running time to assess the methods' efficiency. We briefly summarize the metric as follows. %A detailed introduction is presented in \Cref{detailed_metrics}.
\begin{itemize}[itemsep=0pt, parsep=0pt, leftmargin=*]
	\item \textbf{Accuracy.} Model accuracy evaluates functionality preservation and shows whether the auditing methods influence the utility of the service model. 
	\item  \textbf{Reconstruction Similarity.} It evaluates how much information about the specified samples is unlearned by reconstructed cosine similarity, as introduced in \Cref{similarity_eq}.% in \Cref{detailed_metrics}.  
	\item \textbf{Verifiability.} Verifiability is used to measure the data removal verification by calculating the correct classifying rate of the $\text{Verifier}$, which is defined in \Cref{verifiability_def} in \Cref{detailed_metrics}. 
	\item  \textbf{Running Time.} It is used to assess the efficiency, which records the running time of the entire process of each method.
\end{itemize}


%\subsection{Compared Unlearning Verification and Machine Unlearning Benchmarks}

\noindent
\textbf{Compared Unlearning Verification Benchmarks.}
There are mainly three data removal verification solutions \cite{hu2022membership,sommer2022athena,guo2023verifying}, all based on backdooring methods. 
We only compare our method with MIB \cite{hu2022membership} because MIB is the most popular and has the best verification effect among these three methods. Note that since these methods can only support verifying the backdoored samples, most of the evaluation for MIB is verifying for unlearning only backdoored samples; our method is verifying for unlearning genuine samples. 

 
 %\citeauthor{guo2023verifying}~\cite{guo2023verifying} put much attention into designing invisible backdoor triggers, hence, to some extent, diminishing the verification ability. MIB \cite{hu2022membership} has the best verification effect among these three methods. Therefore, we directly compare our TAPE with the MIB method.

 
\noindent
\textbf{Unlearning Benchmarks.} 
The evaluation for unlearning verification methods is conducted on four mainstream unlearning algorithms: SISA~\cite{bourtoule2021machine}, HBU~\cite{guo2019certified}, VBU~\cite{nguyen2020variational} and RFU~\cite{wang2023machine}. 

\iffalse
We briefly summarize these unlearning benchmarks below,
\begin{itemize}[itemsep=0pt, parsep=0pt, leftmargin=*]
	\item \textbf{SISA \cite{bourtoule2021machine}.} SISA is an exact unlearning method. SISA divides the dataset $D$ into several shards $D^1, D^2, ..., D^k$ and trains sub-models $\theta^1, \theta^2, ..., \theta^k$ for each shard. When the server receives a request for unlearning sample $x_u$, it just needs to retrain the sub-model $\theta^i$ of shard $D^i$ that contains $x_u$. We set $k=5$ disjoint shards and corresponding sub-models. We put the unlearned samples only on one shard, which is the ideal scenario of SISA.  
	\item \textbf{VBU \cite{nguyen2020variational}.} VBU is an approximate unlearning method based on variational Bayesian inference. For the convenience of experiments, we set a middle layer of original neural networks as the Bayesian layer and calculate the unlearning loss according to \cite{nguyen2020variational} based on the Bayesian layer and erased samples for unlearning. 
	\item  \textbf{HBU \cite{sekhari2021remember}.} HUB is an approximate unlearning method, which needs to calculate the inverse Hessian matrix of the remaining dataset as the weight for an unlearning update. We implement HBU  follow the unlearning process as introduced in \cite{sekhari2021remember}. 
	\item \textbf{RFU \cite{wang2023machine}.} RFU is an approximate unlearning method. It tries to unlearn a bottleneck representation by minimizing the mutual information between the representation and the erased samples. We set a middle layer of the original model as the representation layer and implement unlearning according to \cite{wang2023machine}.
\end{itemize}
\fi 


 
 

%\section{Evaluation} \label{exp_eval}






 
\begin{figure}[t]
	\centering
	\includegraphics[width=0.99\linewidth]{Contents/Figures/test_06_all}
	\vspace{-2mm}
	\caption{Auditing for different unlearning methods. TAPE consistently achieves significant efficiency improvement and a better unlearning auditing effect for a single sample (SS) than for multiple samples (MS). 
		\vspace{-5mm}
	} 
	%	\vspace{-2mm}
	\label{evaluation_of_unl}
\end{figure}







\subsection{Evaluations of Unlearning Auditing based on Various Unlearning Benchmarks} \label{exp_on_unl_benchmarks}
 
%In single-sample verification, the Erased Sample Size ({\it ESS}) is equal to 1 and $\text{\it ESS}=20$ for the multi-sample scenario. On STL-10, we set $\text{\it ESS}=2$ for the multi-sample scenario, as STL-10 only contains 5000 training samples, which is much smaller than other datasets.
 
%\vspace{2mm}
\noindent
\textbf{Setup.} 
We demonstrate the evaluation of unlearning auditing methods on four mainstream unlearning benchmarks in \Cref{evaluation_of_unl}.
We evaluate unlearning scenarios of both single-sample (SS) where the Erased Sample Size ({\it ESS}) is 1 and multi-sample (MS) where {\it ESS}=20. Since the MIB method is unable to verify only for unlearning genuine samples, we here evaluate the verification of MIB for backdoored multi-samples (B-MS), $D_{b} \gets (X_b + \text{trigger}, Y_{target})$, which add a white block patch as the trigger at the right bottom of chosen images and change the corresponding labels for the backdooring target. When evaluating TAPE, to keep the setting similar to MIB, we add the perturbation with the same limit distance as the trigger patch to the genuine unlearned samples but do not change the labels for backdooring, $D_{u,p} \gets (X_u+\Delta^p, Y_{u})$, which is achieved through the unlearned data perturbation (UDP) method. 


%To straightforwardly display the compared verification results of having or not having unlearned specified samples, we correspondingly set two situations, i.e., the erased samples $D_{u,b}$ and $D_{u,nois}$ not in or still in the remaining dataset. 

\noindent
\textbf{Evaluations of Auditing Efficiency.} 
The first row of \Cref{evaluation_of_unl} shows the running time of TAPE and MIB across different unlearning algorithms on MNIST, CIFAR-10, and CelebA. TAPE significantly outperforms MIB in terms of running time, as TAPE only involves the unlearning training process. The greatest speedup is observed on CelebA. Additionally, TAPE takes more time for single-sample unlearning auditing compared to multi-sample unlearning auditing, as training the reconstructor model on a single-sample level for a local dataset requires more time.


 
\noindent
\textbf{Evaluations of Unlearning Auditing Effect.} 
The second row, which depicts reconstruction similarity, illustrates the evaluation results of how much information about the specified samples has been unlearned. As MIB is unable to measure the extent of information unlearned from the model, it is omitted in this row. Among all unlearning methods (SISA, VBU, RFU, and HBU), TAPE achieves better reconstruction similarity for single-sample unlearning than for multi-sample unlearning. This suggests that unlearning a single sample tends to reveal more information about the erased sample in the unlearning posterior difference.
 
 
 
%\noindent
%\textbf{Evaluations of Data Removal Verification.} 
The third row shows the comparison with MIB by the verifiability of data removal verification. All methods have a high verifiability result. It indicates that all unlearning methods are effective in these evaluations to answer if the samples are unlearned from the model. However, we should note that in the experiments, MIB only verifies the backdoored samples $D_{u,b}$, while TAPE can verify the genuine samples $D_{u,p}$, which has kept the original labels. 


We also demonstrate an overall evaluation for MIB and TAPE on SISA \cite{bourtoule2021machine} in \Cref{eval_on_C_and_S}. Here, we evaluate auditing genuine samples for both MIB and TAPE instead of setting backdoored samples for MIB. TAPE achieves effective auditing results as analyzed in \Cref{evaluation_of_unl}. However, the MIB cannot successfully verify the unlearning of any genuine samples in the Unl. Verifiability of \Cref{eval_on_C_and_S}. Moreover, since the TAPE scheme is independent of the original model training process, it will not influence the model utility of the original ML service model, keeping the same model accuracy as the ``Original''.  We present additional experimental results in \Cref{overall_eval_app}.


% to solve Problem \ref{execution_problem}

\iffalse
\begin{tcolorbox}[colback=white, boxrule=0.3mm]
	\noindent \textbf{Takeaway 2.} TAPE is more efficient for multi-sample unlearning requests and delivers more effective unlearning auditing for single-sample requests, in both exact and approximate unlearning benchmarks.
\end{tcolorbox}
 \fi 
 


%\section{Ablation Study} \label{exp_abla}
%In this section, we conduct a comprehensive ablation study to investigate how different factors affect the performance of unlearning auditing.


%Some results of the ablation study are presented in Appendix~\ref{ad_exp}.


\begin{figure}[t]
	\centering
	\vspace{-2mm}
	\hspace{-6mm}
	\subfloat{ 	\label{fig:mnistbackaccbetacurve} 
		\includegraphics[scale=0.205]{Contents/Figures/Experiments_r/On_MNIST/Rec_MSE/mnist_rec_sim_sample_size_sim_analysis}
	}		
	\hspace{-3mm}
	\subfloat{  	\label{fig:cifar10recsimsamplesizesimanalysis}
		\includegraphics[scale=0.205]{Contents/Figures/Experiments_r/On_CIFAR10/Rec_MSE/cifar10_rec_sim_sample_size_sim_analysis}
	}	
	\hspace{-3mm}
	\subfloat{ \label{fig:celebarecsimsamplesizesimanalysis}
		\includegraphics[scale=0.205]{Contents/Figures/Experiments_r/On_CelebA/Rec_MSE/celeba_rec_sim_sample_size_sim_analysis}
	} \vspace{-2mm}
	\\
	\hspace{-6mm}
	\subfloat{ \label{fig:mnistaccbetacurve}
		\includegraphics[scale=0.205]{Contents/Figures/Experiments_r/On_MNIST/Verifibility/mnist_verifiability_sample_size_sim_analysis}
	}
	\hspace{-3mm}
	\subfloat{ \label{fig:cifar10verifiabilitysamplesizesimanalysis}
		\includegraphics[scale=0.205]{Contents/Figures/Experiments_r/On_CIFAR10/Verifiability/cifar10_verifiability_sample_size_sim_analysis}
	}
	\hspace{-3mm}
	\subfloat{ 	\label{fig:celebaverifiabilitysamplesizesimanalysis}
		\includegraphics[scale=0.205]{Contents/Figures/Experiments_r/On_CelebA/Verifiability/celeba_verifiability_sample_size_sim_analysis}
	}  \vspace{-2mm}
	\caption{Evaluations of impact about different $\text{\it ESS}$. Here, we evaluate the unlearning verification of genuine samples (GS) rather than backdoored samples for MIB.  \vspace{-2mm}} 
	\label{evaluation_of_in_or_not_in} 
\end{figure}


 \begin{table}[t]
	% \tiny
	\scriptsize
	\caption{Evaluation Results on CIFAR10 and STL-10\vspace{-2mm}}
	\label{eval_on_C_and_S}
	\resizebox{\linewidth}{!}{
		\setlength\tabcolsep{2.pt}
		\begin{tabular}{c|cccccc}
			\toprule[1pt]
			\multirow{2}{*}{} & \multicolumn{3}{c} {CIFAR10, $\text{\it ESS}=20$} & \multicolumn{3}{c} {STL-10, $\text{\it ESS}=2$} \\
			\cmidrule(r){2-4}   \cmidrule(r){5-7}
			& Original & MIB \cite{hu2022membership}   	 & TAPE & Original & MIB   & TAPE\\
			\midrule 
			Running time (s)   & 644   & 673 			& \textbf{113} 	 & 781	& 809  & \textbf{74.90}    \\
			Model Acc.	         & \textbf{81.62\%}    & 79.13\%    & \textbf{81.62\%} & \textbf{68.99\%} &  67.26\% &  \textbf{68.99\%}  \\
			Rec. Sim.  			 	&-& -									& \textbf{0.973}					 	&- & - & \textbf{0.174}  \\
			Unl. Verifiability     & $0.00\%$ & $0.00\%$ & \textbf{97.44\%}  	& $0.00\%$ & $0.00\%$ & \textbf{84.40\%}\\
			\bottomrule[1pt]
	\end{tabular}}
		\vspace{-4mm}
\end{table}


 \begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{Contents/Figures/Experiments_r/On_MNIST/Ablation_exp/ablation_exp}
	\vspace{-4mm}
	\caption{Ablation study about the unlearned data perturbation (UDP) and unlearning influence-based division (UID) strategies of TAPE on MNIST. The legends stand for the entire TAPE, TAPE without (w/o) the UDP strategy, TAPE w/o the UID strategy, and TAPE w/o both strategies.}
	\vspace{-4mm}
	\label{fig:ablationexp}
\end{figure*}



\subsection{Ablation Study of Erased Samples Size ($\text{\it ESS}$)} \label{impact_of_ess}
\noindent
\textbf{Setup.}
\Cref{evaluation_of_in_or_not_in} illustrates the impact of $\text{\it ESS}$ when providing unlearning auditing on MNIST, CIFAR10 and CelebA. The largest $\text{\it ESS}$ is 100 in this experiment, which is around $0.2\%$ of training datasets in MNIST and CIFAR10. In practice, $0.2\%$ data might be very large for unlearning, which has been analyzed in \cite{bertram2019five,chen2021machine}. To illustrate a better comparison, we verify the unlearning of genuine perturbed samples $D_{u,p}$ for both MIB and TAPE. The ablation study is conducted based on another representative unlearning method, VBU \cite{nguyen2020variational}, to demonstrate the scalability of our method for different unlearning methods. Due to the page limitation, we present the impact on the efficiency of $\text{\it ESS}$ in \Cref{imp_of_efficiency_ESS}.% and additional experimental results in \Cref{additional_exp}.
%\vspace{2mm}
 
%The results in \cite{bertram2019five} present that 3.2 million requests for deleting URLs have been issued to Google from 2014 to 2019, constituting less than $0.2\%$ of the total URLs Google indexes in the 5 years. 




\noindent
\textbf{Impact on Unlearning Auditing.}
\Cref{evaluation_of_in_or_not_in} shows the evaluation of both auditing how much information is unlearned (the first row) and data removal status verification (the second row) on MNIST, CIFAR10 and CelebA. When $\text{\it ESS}=1$, TAPE can effectively provide the auditing of unlearning information and data removal status on all datasets. By contrast, MIB cannot support the data removal verification of genuine samples (black dotted line in the second row in \Cref{evaluation_of_in_or_not_in}). Moreover, both two evaluation metrics have a decreasing trend when $\text{\it ESS}$ increases. 





\iffalse
\begin{tcolorbox}[colback=white, boxrule=0.3mm]
	\noindent \textbf{Takeaway 3.} 
	TAPE achieves significant efficiency improvement because it is independent of the original ML service model training. The performance of TAPE for auditing how much information is unlearned and data removal status decreases as $\text{\it ESS}$ increases. 
\end{tcolorbox}
\fi 
 
 
 
 

 

 
 
 \subsection{Ablation Study of Two Strategies}
 
 %We conduct an ablation study to evaluate our two designed strategies, unlearned data perturbation (UDP) and unlearning influence-based division (UID). %, in improving the auditing effect for multi-sample unlearning scenarios. 
 
 
 \noindent
 \textbf{Setup.}
 We conduct the experiments on MNIST in four situations. ``TAPE'' means the entire scheme with the two strategies. ``TAPE w/o UDP'' means TAPE without the UDP strategy while keeping the UID strategy, and ``TAPE w/o UID'' means that we remove the UID strategy of TAPE while keeping the UDP strategy. The ``TAPE w/o both'' means we remove both strategies of TAPE for auditing.  
 
 
 \noindent
 \textbf{Impact on Efficiency.} We present the efficiency evaluation in the first sub-figure of \Cref{fig:ablationexp}. Since the UDP strategy introduces $R=10$ restarts training to find perturbations for erased samples, it consumes around 10 seconds in TAPE. Compared with UDP, UID almost does not consume computational time, as the main time consumption is the unlearned shadow model building and reconstructor training.
 
 
 \noindent
 \textbf{Impact on Auditing Unlearning Effectiveness.} The second sub-figure in \Cref{fig:ablationexp} shows the reconstruction similarity of different methods. All methods achieve a high reconstruction similarity when $\it{ESS} = 1$, which shows the effectiveness of TAPE in single-sample unlearning auditing even without the two strategies. However, when the posterior difference contains information of multiple samples, $\it{ESS} > 1$, if we don't have the UID strategy, the reconstruction similarity drops dramatically, showing as ``TAPE w/o UID'' and ``TAPE w/o both''. The UID plays a vital role in the reconstruction of multiple samples. While the UDP strategy also enhances the reconstruction quality, as shown in ``TAPE w/o UDP'',  its impact is not as substantial as the UID strategy. By contrast, in the third sub-figure in \Cref{fig:ablationexp}, the UDP strategy impacts the verifiability of data removal than the UID strategy, showing as ``TAPE w/o UDP'' and ``TAPE w/o UID''. These results show the significant improvement of the two strategies in benefiting unlearning auditing of how much information is unlearned and data removal status.
 
 
 
 
 
 \begin{figure*}[t]
 	\centering
 	\hspace{-3mm}
 	\subfloat{  	\label{fig:mnistrunningtimenoiseanalysisbar}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_MNIST/Running_time/mnist_runningtime_noise_analysis_bar}
 	} 
 	\hspace{-2mm}
 	\subfloat{  	 	\label{fig:cifar10runningtimenoiseanalysisbar}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_CIFAR10/Running_time/cifar10_runningtime_noise_analysis_bar}
 	}
 	\hspace{-2mm}
 	\subfloat{ 		\label{fig:stl10rtnoiseanalysisbar}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_STL10/Running_time/stl10_rt_noise_analysis_bar}
 	}			
 	\hspace{-2mm}
 	\subfloat{ 	 	\label{fig:celebarunningtimenoiseanalysisbar}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_CelebA/Running_time/celebA_runningtime_noise_analysis_bar}
 	} \vspace{-3mm} \\
 	\hspace{-3mm}
 	\subfloat{ 	  	\label{fig:mnistrecmsenoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_MNIST/Rec_MSE/mnist_rec_mse_noise_analysis}
 	}			
 	\hspace{-2mm}
 	\subfloat{   	\label{fig:cifar10recmsenoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_CIFAR10/Rec_MSE/cifar10_rec_mse_noise_analysis}
 	} 
 	\hspace{-2mm}
 	\subfloat{     	\label{fig:stl10recmsenoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_STL10/Rec_similarity/stl10_rec_mse_noise_analysis}
 	}
 	\hspace{-2mm}
 	\subfloat{  	\label{fig:celebarecmsenoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_CelebA/Rec_MSE/celeba_rec_mse_noise_analysis}
 	} \vspace{-3mm}  \\
 	\hspace{-3mm}
 	\subfloat{ 	\label{fig:mnistverifiabilitynoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_MNIST/Verifibility/mnist_verifiability_noise_analysis}
 	}			
 	\hspace{-2mm}
 	\subfloat{   	\label{fig:cifar10verifiabilitynoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_CIFAR10/Verifiability/cifar10_verifiability_noise_analysis}
 	} 
 	\hspace{-2mm}
 	\subfloat{  	\label{fig:stl10verifiabilitynoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_STL10/verificability/stl10_verifiability_noise_analysis}
 	}
 	\hspace{-2mm}
 	\subfloat{   	\label{fig:celebaverifiabilitynoiseanalysis}
 		\includegraphics[scale=0.25]{Contents/Figures/Experiments_r/On_CelebA/Verifiability/celeba_verifiability_noise_analysis}
 	}%\vspace{-2mm}
 	\caption{Evaluations of the impact of the unlearned data perturbation limit. ``SS'' stands for single-sample unlearning scenario, and ``MS'' means multi-sample unlearning scenario. ``B-SS'' means backdoored single-sample scenario, and ``B-MS'' means backdoored multi-sample unlearning scenario.} 
 	\vspace{-2mm}
 	\label{evaluation_of_noise} 
 \end{figure*}

\subsection{Detailed Ablation Study of UDP} \label{adding_noise}

We additionally evaluate the impact of unlearned data perturbation. We find that only perturbation without changing the labels as backdoor-based methods already significantly improves the information reconstruction and assists the verifiability of data removal.


\noindent
\textbf{Setup.} 
In this experiment, we keep all other parameters fixed while only changing the perturbation limitation value $\alpha$. As introduced in \Cref{two_s}, the UDP outputs $D_{u,p} \gets (X_u + \Delta^p, Y_u)$, and the perturbation is limited as $\| \Delta^p \|_{\infty} \leq \alpha$. We set the perturbation limit distance value from 0 to 25 on MNIST and CIFAR10, 0 to 75 on STL-10, and 0 to 150 on CelebA, which is determined by the data size. We only perturb the unlearned data but do not change the corresponding labels to ensure the utility of the genuine samples. To better illustrate the impact of $\alpha$, we keep a copy of original data $D_u$ in the remaining dataset, which is the hardest scenario for unlearning effectiveness auditing. 
The unlearning method employed here is the approximate unlearning method VBU. 
We evaluate our method in both single-sample (SS) and multi-sample (MS) unlearning requests. For the MIB method, we evaluate in the backdoored single-sample (B-SS) scenario, and we control the backdooring trigger patches with the sample distance limitation value as our methods to ensure they can be compared. The experimental results on MNIST, CIFAR10, STL-10, and CelebA are presented in \Cref{evaluation_of_noise}.  


 \noindent
 \textbf{Impact on Efficiency.}
The first column illustrates the running time of TAPE and MIB, which clearly demonstrates the improvement of TAPE in terms of efficiency. The reason is that the verification models of TAPE are trained independently of the original ML service model training process, which significantly shortens the running time for verification. During the experiment, we fix $R=10$ restarts. The perturbation limitation has no significant impact on running time, as the running time remains consistent across different perturbation limits.


%\noindent
%\textbf{Relationship between Data Similarity and Perturbation Limit Value.} The first column in \Cref{evaluation_of_noise} shows the relationship between the perturbation limit $\alpha$ and data similarity, where the similarity is between the original image data $D_{u}$ and the perturbed data $D_{u, p}$. On all the datasets, adding more perturbation will decrease the similarity. 

 
 



\noindent
\textbf{Impact on Unlearning Auditing Effect.} 
With larger perturbations, we augment the unlearning posterior difference, increasing the reconstruction similarity and enabling more information about the erased samples to be extracted. The trend is indicated in unlearning single samples (the green line) and multi-samples (the blue line) on all three datasets. Moreover, the results in the second row in \Cref{evaluation_of_noise} also clearly confirm our previous analysis: auditing for a single sample achieves a much better result than auditing for multiple samples, which is reflected in the significant gap between the two scenarios.

In TAPE, a larger perturbation of unlearned data makes data removal verification easier. In the third row in \Cref{evaluation_of_noise}, it is obvious that the verifiability increases as the perturbation limitation increases. When the perturbation limitation is less than $5$, it is hard to distinguish if the samples $D_{u,p}$ are unlearned because the remaining dataset contains a similar original sample $D_{u}$. However, when the perturbation limitation increases larger than $5$, the TAPE verifiability will greatly improve for genuine single-sample and multi-sample unlearning. MIB performs similarly when verifying the unlearning of backdoored multiple samples (B-MS). However, MIB fails to verify the unlearning of a single sample, as only one sample cannot backdoor the original ML service model.



%If the unlearned samples are more dissimilar to the remaining samples, the model difference will be more obvious, with a higher UE value as shown in the second column in \Cref{evaluation_of_noise}, which confirms the analysis in Appendix~\ref{theorey_analysis_of_similarity}. The trend is indicated in unlearning single samples (the green line) and multi-samples (the blue line) on all three datasets. The larger UE means the model difference contains more information about the unlearned samples. Therefore, the reconstructed samples based on such difference have a higher reconstruction similarity to the unlearned samples, shown in the third column in \Cref{evaluation_of_noise}. Higher reconstruction similarity demonstrates the unlearning model difference removes more information about the erased samples from the model, showing better unlearning effectiveness.


\iffalse

\begin{tcolorbox}[colback=white, boxrule=0.3mm]
	\noindent \textbf{Takeaway 4.} 
	The UDP strategy effectively improves unlearned data information extraction and verifiability, and larger perturbation limitation achieves a better improvement.
\end{tcolorbox}

\fi 




 


