

\begin{abstract}
%Increasing studies focus on machine unlearning as it upholds users' right to be forgotten, under which individuals can request the removal of their specified samples from trained models. 
With the increasing prevalence of Web-based platforms handling vast amounts of user data, machine unlearning has emerged as a crucial mechanism to uphold users' right to be forgotten, enabling individuals to request the removal of their specified data from trained models. 
However, the auditing of machine unlearning processes remains significantly underexplored. Although some existing methods offer unlearning auditing by leveraging backdoors, these backdoor-based approaches are inefficient and impractical, as they necessitate involvement in the initial model training process to embed the backdoors. In this paper, we propose a TAilored Posterior diffErence (TAPE) method to provide unlearning auditing independently of original model training. We observe that the process of machine unlearning inherently introduces changes in the model, which contains information related to the erased data. TAPE leverages unlearning model differences to assess how much information has been removed through the unlearning operation. Firstly, TAPE mimics the unlearned posterior differences by quickly building unlearned shadow models based on first-order influence estimation. Secondly, we train a Reconstructor model to extract and evaluate the private information of the unlearned posterior differences to audit unlearning. Existing privacy reconstructing methods based on posterior differences are only feasible for model updates of a single sample. To enable the reconstruction effective for multi-sample unlearning requests, we propose two strategies, unlearned data perturbation and unlearned influence-based division, to augment the posterior difference. Extensive experimental results indicate the significant superiority of TAPE over the state-of-the-art unlearning verification methods, at least 4.5$\times$ efficiency speedup and supporting the auditing for broader unlearning scenarios. 

%Recent studies increasingly emphasize the importance of machine unlearning as a means to uphold users' right to be forgotten, which allows individuals to request the removal of specific data samples from trained models. However, the auditing of machine unlearning processes remains significantly underexplored. While some existing methodologies propose the use of backdoors to facilitate unlearning audits, these approaches are hindered by inefficiencies and practical limitations. Specifically, backdoor-based techniques require intervention during the initial model training phase to embed the necessary backdoors, which undermines their scalability and practical applicability​​.

%This study uncovers two factors that are seldom explored but have significant impacts on unlearning and auditing: (1) the similarity between the erased and remaining samples and (2) the task type of the original learning model, which benefits studies regarding unlearning implementation, auditing, and forging attacks.

\end{abstract}