
\section{Preliminary and Problem Statement} \label{problem_df}

%In this section, we introduce the ML scenario and the threat model, then the unlearning auditing problem statement, and finally, the metrics and requirements for unlearning auditing.

%including the data removal verification and the unlearning effectiveness assessment, and finally, the definition of the solution scheme.

%



%\subsection{Machine Unlearning Auditing Problem}

To facilitate the understanding of the unlearning auditing problem, we first introduce the main process of unlearning. A detailed introduction about the 
 and threat model is presented in \Cref{threat_model}. 

\noindent
\textbf{Machine Unlearning.} The unlearning process usually includes the following phases. (1) The server trained a model with parameters $\theta_t$ derived from dataset $D$. (2) The unlearning user uploads the unlearning requested dataset $D_u$ to the server for unlearning. (3) The server conducts an unlearning algorithm $\mathcal{U}$ to remove $D_u$'s contribution from $\theta_t$ and results in an unlearned model with parameters $\theta_{u, D \backslash D_u}$, also denoted as $\theta_u$. 

Most existing backdoor-based unlearning verification methods tried to solve data removal verification but can only answer if the backdoored samples are unlearned. Answering whether the backdoored data is or not deleted is insufficient for trustworthy unlearning auditing. We should assess the unlearning effectiveness of the model, i.e., how much private information about the requested unlearning samples is removed from the model.

 
\begin{prob_state}[Unlearning Effectiveness Audit] 
	\label{effectiveness_problem}	
Given the described unlearning scenario, the potential for unlearning execution spoofing by the server, and the capabilities of the unlearning user, auditing unlearning effectiveness necessitates a method for unlearning users to evaluate the extent to which information about $D_u$ has been unlearned from $\theta_t$ to $\theta_{u}$.
\end{prob_state}

% 
% that the unlearning user develop a method to evaluate the extent to which information about $D_u$ has been unlearned from $\theta_T$ to $\theta_{U}$.

It is important to note that the problem statement inherently includes the issue of data removal verification. If one can effectively measure how much information related to the erased samples has been unlearned, this measurement can serve as the basis for determining whether the data has been properly unlearned. %, thereby addressing the data removal verification through an evaluation of unlearning effectiveness.
We try to conduct unlearning auditing based on the unlearning updated posterior difference as it contains essential information about the erased samples. To achieve the auditing goals, we need to mimic the unlearning posterior difference and extract and quantify the unlearned information from it. We utilize the model's output layer results of the original and unlearned models on the user's local dataset to generate the posterior difference. 
 %Then, we conduct the unlearning effectiveness audit based on this posterior difference. 
We define the unlearning posterior difference as follows.


\noindent
\textbf{Posterior Difference.}
The unlearning user first queries the trained ML model $\theta_t$ before unlearning with all samples of $D_{local}$ and concatenates the received outputs to form a vector $\hat{Y}_{t, local}$. Then, the user queries the unlearned model $\theta_u$ with samples in the $D_{local}$ and creates a vector $\hat{Y}_{u, local}$. In the end, the user sets the posterior difference, denoted by $\delta$, to the difference of both outputs: 
\begin{equation} \label{posterior_diff}
	%\small
	\delta = \hat{Y}_{t,local} - \hat{Y}_{u,local}.
\end{equation}
Note that the dimension of $\delta$ is the product of $D_{local}$'s cardinality and the number of classes of the target dataset. For example, in this paper, CIFAR-10 and MNIST are 10-class datasets, while we just identify the gender attributes of CelebA, which is a binary classification. As we set the local dataset $0.5\%$ of CIFAR-10 and MNIST, and $0.06\%$ of CelebA, this indicates the dimension of $\delta$ is 2500 for CIFAR-10, 3000 for MNIST, and 1210 for CelebA.



\begin{figure*}[t]
	\centering
	\includegraphics[width=0.97\linewidth]{Contents/Figures/reconstruction_attack}
	\vspace{-2mm}
	\caption{The main process of the TAPE method. (a) The first part quickly builds the unlearned shadow models through first-order influence estimation based on the user's local dataset $D_{local}$ to mimic the unlearning posterior difference $\delta$. (b) Two posterior difference augment strategies are proposed to make the reconstruction suitable for multi-sample unlearning. \vspace{-2mm}}
	\label{fig_reconstructionattack}
\end{figure*}

\noindent
\textbf{Unlearned Information Reconstruction to Assess How Much Information is Unlearned.}
To assess the unlearning effectiveness, we employ a reconstructor model to extract the unlearned information from the posterior difference. We employ the cosine similarity between the reconstructed and original unlearned samples to assess how much information of the unlearned information can be recovered from the unlearning update:  
\begin{equation} \label{similarity_eq}
	%\small
	\textbf{Rec. Similarity:} \hspace{12mm} \text{sim}(\hat{X}_{u}, X_u) = \frac{\hat{X}_{u} \cdot X_u}{ \| \hat{X}_{u} \| \cdot \| X_u \|}.
\end{equation}
Here, $\hat{X}_{u} \cdot X_u$ is the dot product of the reconstructed vectors $\hat{X}_{u}$ and original unlearned samples vectors $X_u$. $\| \hat{X}_{u}\|$ and $\|X_u\|$ are the Euclidean norms of the two vectors. A higher reconstruction similarity means more information about the erased samples is unlearned from the model.



