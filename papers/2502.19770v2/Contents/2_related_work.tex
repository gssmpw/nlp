
 

\section{Related Work} \label{related_work}

\iffalse
\subsection{Machine Unlearning}
Machine unlearning is proposed as a solution to the right to be forgotten in the machine learning context \cite{cao2015towards,chen2021machine}. According to the legislation, the most legitimate approach is retraining from scratch based on the remaining dataset~\cite{cao2015towards,thudi2022necessity}. However, the naive retraining method incurs high computational and storage overhead when the dataset is large, and requests are frequently revoked \cite{sekhari2021remember,gupta2021adaptive}. Thus, many studies tried to solve the machine unlearning problem in an effective and efficient way \cite{bourtoule2021machine,yanarcane2022unlearning,guo2019certified,nguyen2020variational}. 

Existing studies can be roughly categorized into \textit{exact unlearning}~\cite{cao2015towards,bourtoule2021machine,yanarcane2022unlearning,hu2024eraser} and \textit{approximate unlearning} methods~\cite{guo2019certified,nguyen2020variational,wang2023machine,graves2021amnesiac}. 
The exact retraining methods are extended from naive retraining and aim to reduce the computational cost of retraining a new model by redesigning the learning algorithms~\cite{cao2015towards,bourtoule2021machine} and storing the intermediate parameters during the learning process~\cite{yanarcane2022unlearning,hu2024eraser}. Approximate unlearning seeks to modify the model using only the original model and the samples to be erased, approximating a model as if it were retrained on the remaining dataset~\cite{guo2019certified,nguyen2020variational,wang2023machine,warnecke2024machine}. 

\fi 


 



%\subsection{Verification of Machine Unlearning}



%Few studies paid attention to the problem of providing unlearning audits to prove whether users' data are removed and how much information is unlearned \cite{thudi2022necessity}. 

Few backdoor-based solutions \cite{hu2022membership,sommer2022athena,guo2023verifying,gao2024verifi} provided data removal verification for machine unlearning. These studies mixed backdoored samples to users' data for backdooring servers' service ML models during the model training process. Then, they inferred whether the users' data was unlearned by testing if the backdoor disappeared from the service models \cite{hu2022membership,guo2023verifying}.
However, these backdoor-based methods have two oblivious limitations. 

First, these methods rely on the original ML model training process, which is impractical in real-world scenarios due to users being unaware of which samples will need to be unlearned in the future, as well as the high computational costs involved. Second, these methods only build the connection between the backdoored samples and models, while the backdoored dataset and genuine dataset are still separate from the models' perspective~ \cite{wang2019neural,zeng2023narcissus}. The backdoored and the erased genuine datasets perform differently during model training, as the corresponding experimental results are shown in \Cref{fig_mnistepochaccdrop}. The removal of backdoor triggers can actually only verify whether the backdoored samples are unlearned as the backdoored samples and unlearned genuine samples perform differently during the approximate unlearning training process \cite{nguyen2020variational}. When the accuracy of backdoored samples drops to $0\%$, the model accuracy on genuine unlearned data and test data is still around $80\%$. These results are consistent with current backdoor studies \cite{gao2023backdoor,pan2023asset,wang2023mm}. We present more detailed discussion in \Cref{different_with_existing}.

%More detailed discussion about related work is presented in \Cref{different_with_existing}.

%Moreover, we summary the difference with existing works in \Cref{different_with_existing}.

 


 
\iffalse


\subsection{Difference from Existing Studies} \label{different_with_existing1}





Our TAPE approach is significantly different from existing unlearning verification methods \cite{hu2022membership,sommer2022athena,guo2023verifying,gao2024verifi} in terms of the involving processes, auditing data type, unlearning scenarios, and unlearning methods, as depicted in \Cref{overview_of_auditing_method}. First, the significant difference is that the auditing of our method only involves the unlearning process, while the backdoor-based methods must involve both the original training and unlearning processes to ensure the service model first learns the backdoor. Second, most existing auditing methods are based on backdooring techniques and need to backdoor or mark samples for verification \cite{hu2022membership,sommer2022athena,guo2023verifying,gao2024verifi}. As we analyzed in the above subsection, they can only validate the backdoored samples and are only applicable to the exact unlearning methods as exact unlearning methods guarantee the deletion from the dataset level. Our method does not mix any other data to the training dataset, and the auditing is based on the posterior difference, which is suitable for genuine samples in both exact and approximate unlearning methods. Third, backdoor-based auditing methods are only feasible for multi-sample unlearning scenarios because just using a single sample makes it hard to backdoor the model \cite{wang2019neural,zeng2023narcissus}, hence failing to provide unlearning verification for a single sample. 


\fi 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.87\linewidth]{Contents/Figures//mnist_epoch_acc_drop}
	\vspace{-2mm}
	\caption{Approximate unlearning process on genuine unlearned data $D_u$ and backdoored data $D_b$ on MNIST. During unlearning, the backdoor accuracy drops to 0\% at the blue Vertical line. Meanwhile, the model accuracy on genuine unlearned data $D_u$ and test data is still around $80\%$.
	}
	\vspace{-2mm} 
	\label{fig_mnistepochaccdrop}
\end{figure}


%We note that TAPE shares similarities with some studies investigating privacy leakage caused by the model updated difference \cite{salem2020updates,chen2021machine,balle2022reconstructing,Hu2024sp}. They aimed to extract as much private information as possible from model differences. However, it is well-known that while these methods are effective for single-sample reconstruction, they are less effective for multi-sample reconstruction. We have one advantageous difference to ensure we can provide a more effective information reconstruction that is suitable for unlearning auditing for multi-samples. Specifically, the unlearning verification user knows the unlearned samples, as users specify these samples, while the settings in \cite{salem2020updates,chen2021machine,balle2022reconstructing,Hu2024sp} have no information about the inferred samples. With the knowledge of the unlearned samples, we can design the posterior augment methods to facilitate the unlearning auditing for multiple samples.




%However, in these attacking studies, the key target is to extract as much as possible private information from model differences in a black-box setting to indicate the attacking effectiveness, while our auditing method aims to assess how different unlearned samples influence the model and infer whether the specified samples are unlearned.

%some studies that 

%there are some studies 



% \subsection{Erased Data Reconstruction}
%There are many studies investigating the information leakage caused by the model updated difference \cite{salem2020updates,chen2021machine,balle2022reconstructing,zanella2020analyzing}, including ML service updates and unlearning updates. 
 
 
%The key idea of these methods is that the model update will contain the private information of samples that are used for model updating or unlearning. The main process was introduced in \cite{salem2020updates}, where \citeauthor{salem2020updates} treated the posterior difference of two versions of the model (before and after the updating) as inputs for Autoencoders and GANs to reconstruct the data used in the update. \citeauthor{chen2021machine} \cite{chen2021machine} employed a similar model structure but studied the membership inference attacks caused by unlearning updates. Our method is inspired by these attacking methods. 
%Although our method is inspired by these attacking methods, in our scenario, since the server has the ability to access all dataset and model parameters, we can design a more efficient and effective method to assist in unlearning verification. 

\iffalse
\textbf{Privacy Reconstruction Attacks.}
There are some privacy-leakage researches highly related to our work \cite{salem2020updates,chen2021machine,balle2022reconstructing,zanella2020analyzing}. In \cite{salem2020updates}, the authors designed many reconstruction-attacking methods to reconstruct the samples used in the ML model online update. They exploit the leaked information from two versions of the target ML model before and after updating. \citeauthor{chen2021machine} \cite{chen2021machine} studied that the membership inference attacks based on the unlearning update. They pointed out that the unlearning update (difference between the model before and after unlearning) can also leak private information about the unlearned samples. \cite{zanella2020analyzing} studied the amount of information about the changes in the training data from language model snapshots before and after an update.
\fi

