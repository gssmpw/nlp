\section{Introduction} \label{intro}




Rising concerns over personal data privacy have led to the enactment of stringent privacy regulations and laws, such as the General Data Protection Regulation (GDPR)~\cite{mantelero2013eu}. These legal frameworks guarantee individuals the ``right to be forgotten'', granting the right to request the removal of their data when participating in machine learning (ML) services. This right has sparked significant interest in the research community, giving rise to the concept of ``machine unlearning'' --- a field that explores methods for erasing the influence of user-specified samples from trained ML models~\cite{bourtoule2021machine,neel2021descent,warnecke2024machine}. For example, in Web-based recommendation systems that collect huge amounts of sensitive user data, effective unlearning methods are essential for protecting user privacy \citep{liu2024breaking,chen2022recommendation}. Although many unlearning techniques are proposed, most of them focus on developing unlearning optimization algorithms while ignoring the provision of unlearning auditing. 



%An effective unlearning auditing method could better support the ``right to be forgotten'' legislation with a thorough privacy protection guarantee for individuals. Existing machine unlearning studies put a huge amount of effort into developing methods to erase the influence of specified samples from trained models \cite{bourtoule2021machine, sekhari2021remember, warnecke2021machine,ullah2021machine}. The ML servers usually only release the unlearned version of models to the public after executing the unlearning algorithm. Users often face challenges in verifying unlearning effectiveness, especially when they have only access to the interface of the unlearned model in a black-box setting \cite{thudi2022necessity,jagielski2022measuring}. Although an effective unlearning audit method can provide a comprehensive privacy guarantee for individuals to support the ``right to be forgotten'' legislation, developing such a tool is essential yet challenging.



\noindent
\textbf{Research Gap.}
There are a few works provided unlearning execution verification based on backdoor techniques~\cite{hu2022membership,guo2023verifying,sommer2022athena}. However, the backdoor-based methods have two oblivious disadvantages: (1) they are inefficient in practice as they are required to backdoor the model in the original model training period; (2) they cannot provide exact verification for genuine samples.


First, the efficacy of backdoor-based unlearning verification schemes hinges on model backdooring during the initial model training process \cite{hu2022membership,guo2023verifying}, as shown in~\Cref{fig_unlearningauditfigure1}(a), which is impractical and inefficient. Users are unlikely to foresee the need to unlearn specific samples at the outset, making it unreasonable to incorporate tailored backdoors for specified samples during the initial training phase. Furthermore, involving the model training process in this way introduces inefficiencies, as it would be more effective to design an audit method that focuses solely on the machine unlearning operation. % and remains independent of the initial training process. 


Second, the backdooring method can only build the connection between backdoored samples and models, but the backdoored samples and erased genuine samples are distinct datasets~\cite{gao2023backdoor,pan2023asset}. These two datasets behave differently during model training, especially in approximate unlearning \cite{nguyen2020variational,fu2022knowledge}, where the model accuracy on backdoored samples diminishes much faster than that on genuine samples~\cite{wang2019neural,pan2023asset}. 
It indicates that the removal of backdoors can only verify whether the backdoored samples are unlearned from the model rather than genuine samples. 

 


\noindent
\textbf{Research Question.}
Based on the research gap, we pose the research question: \textit{``When an unlearning request is uploaded and processed, can we provide a practical audit service that verifies data removal and assesses the effectiveness of unlearning?''} Specifically, for practicality, the audit should only involve the unlearning process, and for effectiveness, it should rigorously determine whether the specified data has been unlearned and evaluate how much information has been erased from the model. 

\noindent
\textbf{Motivation.}
%ML models learn patterns and relationships from the training dataset, which are embedded in the model's parameters and behavior \cite{goodfellow2016deep,janfaza2023mercury}. 
The process of machine unlearning inevitably results in two versions of the model: one before and one after unlearning \cite{chen2021machine}. The difference between these models encapsulates the privacy information of the erased samples \cite{Hu2024sp,chen2021machine,zhang2023conditional}. Our approach to audit unlearning effectiveness is based solely on analyzing these model differences, as illustrated in~\Cref{fig_unlearningauditfigure1}(b).
Auditing unlearning effectiveness based on model differences offers two key advantages. First, this method is practical and efficient, as it focuses exclusively on the unlearning operations without requiring involvement in the initial model training process. Second, auditing based on model differences supports broader unlearning scenarios and requests because unlearning for either genuine or backdoored samples results in model differences. By contrast, backdoor-based methods are only effective for backdoored samples as they can only build connections between backdoored samples and models. 



\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{Contents/Figures/unlearning_audit_figure1} 
	\caption{(a) The backdoor-based verification and (b) The motivation of auditing unlearning effectiveness based on the posterior difference. The scheme (b) only involves the unlearning process rather than the initial model training. \vspace{-4mm}
	}
	\label{fig_unlearningauditfigure1}
\end{figure}


 

  
\noindent
\textbf{Our Work.}
In this paper, we address the research question by formalizing the machine unlearning auditing problem and introducing an approach called TAPE, designed to audit unlearning effectiveness solely based on the unlearning process. TAPE contributes one method and two strategies to effectively train a Reconstructor model to evaluate how much private information is unlearned to audit this unlearning update. As a preparatory step, TAPE mimics unlearning posterior differences as input data for the Reconstructor by proposing an unlearned shadow model establishment method based on first-order influence estimation. While existing privacy reconstruction methods are effective for single samples, they are impractical for multiple samples--a common scenario in unlearning requests. To address this, we leverage the fact that the unlearning user knows and uploads the erased data, allowing us to design two strategies to ensure our method is suitable for multi-sample scenarios. Specifically, we design the unlearned data perturbation strategy to augment the posterior difference for a better reconstruction effect of unlearned samples. Additionally, we develop an unlearned influence-based division strategy, which transforms the reconstruction task from dealing with multiple samples as a single posterior difference to reconstructing each sample individually based on multiple divided posterior differences, significantly enhancing the overall reconstruction effectiveness. 




We conduct extensive experiments on four representative datasets and four mainstream unlearning benchmarks to evaluate the proposed method, in which the results indicate the superiority of TAPE over the start-of-the-art auditing methods \cite{hu2022membership,guo2023verifying} in terms of both efficiency and efficacy. From the efficiency perspective, our TAPE method achieves at least $4.5\times$ speedup on all datasets and at most $75\times$ speedup on the CelebA dataset than backdoor-based methods, as TAPE only involves the unlearned process. In contrast, backdoor-based methods must backdoor the service model during the initial training process, which is computationally expensive. From the efficacy perspective, our TAPE provides effective auditing of genuine samples for both exact and approximate unlearning algorithms, while the verification of backdoor-based methods only targets backdoored samples. 

 



Our contributions are summarized as follows:
\begin{itemize}[itemsep=0pt, parsep=0pt, leftmargin=*]
	\item This paper is the \textit{first} to investigate the auditing for machine unlearning involves only the unlearning process, which is much different from existing backdoor-based methods that rely on backdooring the model during the initial training process. Moreover, our auditing study is feasible for genuine unlearned samples rather than only backdoor-marked samples. 
	\item We propose a TAPE method based on the posterior difference to auditing unlearning. TAPE introduces a novel method to quickly establish unlearned shadow models that mimic the posterior differences and incorporates two posterior augmentation strategies to facilitate auditing the unlearning of multiple samples. 
	\item We conduct extensive experiments on both exact and approximate unlearning methods across representative datasets and various model architectures. The findings validate significant improvements in efficiency and broader applicability to adaptive unlearning scenarios compared with the state-of-the-art unlearning verification methods. 
	\item The source code and the artifact of TAPE is released at \url{https://anonymous.4open.science/r/TAPE-30D0}, which creates a new tool for measuring the effectiveness of machine unlearning methods, shedding light on the design of future unlearning auditing methods. 
\end{itemize}

 

% \noindent \textbf{Roadmap.} We introduce the related work in Section \ref{related_work}. We introduce the threat model and formalize the machine unlearning auditing problem in Section \ref{problem_df}. Section \ref{muv_method} presents the details of our proposed TAPE for unlearning effectiveness verification. In \Cref{exp_setting,exp_eval,exp_abla}, we introduce the experimental setting and illustrate the experimental results to evaluate the proposed TAPE. We summarize the paper in Section \ref{s_a_fw}. 

 
