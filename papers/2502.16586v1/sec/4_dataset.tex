\section{Datasets and Benchmarks}
\label{sec:datasets}

% \textcolor{red}{The evolution of text-rich image understanding tasks is profoundly intertwined with the proliferation of publicly available datasets and benchmarks, where dataset construction manifests a dual trajectory of scenario specialization and task sophistication. Contemporary datasets predominantly center on six pivotal domains: document, chart, natural scene, table, mathematical reasoning, and graphical user interface (GUI) interaction, addressing multi-tiered demands spanning from single-page document parsing to cross-page contextual reasoning, and from elementary information retrieval to advanced logical computations.}

The rapid advancements in TIU tasks have been fundamentally driven by the proliferation of specialized datasets and standardized benchmarks. As illustrated in Table \ref{tab:datawithbenchmark}, we systematically categorize TIU-related datasets into two types: \textit{domain-specific} (Document, Chart, Scene, Table, and GUI) and \textit{comprehensive} scenarios. 
%This taxonomy not only reflects the evolving landscape of TIU research but also highlights the community's focused efforts on addressing domain-specific challenges through targeted dataset creation.

% \textcolor{red}{Key considerations in dataset design include image scale, diversity of question-answer pairs, annotation quality, and applicability to real-world scenarios. These factors are critical for creating datasets that are comprehensive and accurately represent the challenges faced in Text-Image Understanding (TIU) tasks.} 

Specifically, some datasets are derived by converting training data from traditional tasks~\cite{guan2022industrial,guan2023self} into Visual Question Answering (VQA) formats, such as text detection, text spotting, table recognition, and \emph{etc.}. These datasets are typically utilized for modality alignment in the first stage of training, enabling models to bridge the gap between textual and visual information effectively. Other datasets are specifically designed in VQA formats for certain scenarios, such as DocVQA~\cite{mathew2021docvqa}, InfoVQA~\cite{mathew2022infographicvqa}, ChartQA~\cite{masry2022chartqa}, and TextVQA~\cite{singh2019towards}. These datasets have played a pivotal role in advancing the field of TIU by providing structured and domain-specific challenges. Their introduction has significantly accelerated progress in tasks like document understanding, chart interpretation, and natural scene text comprehension. Consequently, published papers frequently report these metrics, as they not only contribute to instruction alignment in the second stage of training but also serve as essential benchmarks for evaluating model performance.

In addition to training datasets, there is a distinct category of datasets that are exclusively designed for evaluating specific capabilities of MLLMs. Examples include TableVQA-Bench~\cite{Kim2024ARXIV_TableVQA_Bench_A}, ChartBench~\cite{Xu2024ARXIV_ChartBench_A_Benchmark}, and MMLongBench-Doc~\cite{Ma2024NEURIPS_MMLongBench_Doc_Benchmarking}. These datasets are tailored to assess advanced functionalities such as long-context understanding, cross-modal reasoning, and domain-specific comprehension. By providing targeted evaluation frameworks, they enable researchers to identify strengths and weaknesses in MLLMs, driving further innovation and refinement in the field.


% \begin{table*}[t]
%     \centering
%     \scalebox{0.95}{
%     \input{Table/dataset}
%     }
%     \caption{Representative datasets designed for rich text image understanding in different scenes}
%     \label{tab:my_label}
% \end{table*}

% % benchmark
% \begin{table*}[t]
%     \centering
%     \scalebox{0.95}{
%     \input{MLLM_OCR/Table/benchmark}
%     }
%     \caption{Publicly available evaluation benchmarks designed for rich text image understanding in different scenes}
%     \label{tab:benchmark}
% \end{table*}