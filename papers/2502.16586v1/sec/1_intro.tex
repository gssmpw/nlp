\section{Introduction}

% 介绍Ttext-rich image 及TIU 任务，引出基于LLM 的方法近期发展迅猛，急需一篇综述来介绍相关的进展


%Historically, perception and understanding tasks were handled separately through specialized models or multi-stage pipelines. Recent advances in vision-language models have unified these capabilities under Visual Question Answering (VQA) frameworks, driving research toward end-to-end universal models. As shown in Figure 1, the emergence of large language models (LLMs) marks a paradigm shift. Pre-LLM approaches like LayoutLM \cite{Xu2019ARXIV_LayoutLM_Pre_training} and Donut relied on self-supervised pre-training (masked language modeling, layout prediction) or OCR-aware tasks (text recognition, order recovery), followed by task-specific fine-tuning. While effective in controlled settings, these methods exhibit limited zero-shot generalization.

Text-rich images play a pivotal role in real-world scenarios by efficiently conveying complex information and improving accessibility \cite{biten2019scene}. Accurately interpreting these images is essential for automating information extraction, advancing AI systems, and optimizing user interactions. To formalize this research domain, we term it \textbf{T}ext-rich \textbf{I}mage \textbf{U}nderstanding (\textbf{TIU}), which encompasses two core capabilities: perception and understanding. The perception dimension focuses on visual recognition tasks, such as text detection \cite{liao2022real}, text recognition~\cite{guan2025ccdplus}, formula recognition \cite{TRUONG2024110531, guan2024posformer}, and document layout analysis \cite{Yupan2022ARXIV_LayoutLMv3_Pre_training}. The understanding dimension, conversely, requires semantic reasoning for applications like key information extraction and document-based visual question answering (\emph{e.g.}, DocVQA \cite{mathew2021docvqa}, ChartQA \cite{masry2022chartqa}, and TextVQA~\cite{singh2019towards}).

Historically, perception and understanding tasks were handled separately through specialized models or multi-stage pipelines. Recent advances in vision-language models have unified these tasks within Visual Question Answering (VQA) paradigms, driving research towards the development of end-to-end universal models.


Figure \ref{fig:Development} presents an evolutionary timeline delineating critical milestones in unified text-rich image understanding models. The trajectory reveals two distinct eras: (a) The pre-LLM period (2019-2022) characterized by specialized architectures like LayoutLM \cite{Xu2019ARXIV_LayoutLM_Pre_training} and Donut \cite{Kim2021SEMANTIC_Donut_Document_Understanding}, which employed modality-specific pre-training objectives (masked language modeling, masked image modeling, \textit{etc.}) coupled with OCR-derived supervision (text recognition, spatial order recovery, \textit{etc.}). While effective in controlled settings, these models exhibited limited adaptability to open-domain scenarios due to their task-specific fine-tuning requirements and constrained cross-modal interaction mechanisms. (b) The post-LLM era (2023–present) is marked by the growing popularity of LLMs. Some studies propose Multimodal Large Language Models (MLLMs), which integrate LLM with visual encoders to jointly process visual tokens and linguistic elements through unified attention mechanisms, achieving end-to-end sequence modeling.
% In the following sections of this paper, such large models will be uniformly referred to as MLLMs.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figure/2.pdf} 
    \vspace{-2em}
    \caption{The general model architecture of MLLMs and the implementation choices for each component.} 
    \label{fig:Architecture} 
    \vspace{-1em}
\end{figure*}

This paradigm evolution addresses two critical limitations of earlier methods. First, the emergent MLLM framework eliminates modality-specific inductive biases through homogeneous token representation, enabling seamless multi-task integration. Second, the linguistic priors encoded in LLMs empower unprecedented zero-shot generalization and allow direct application to diverse tasks without task-specific tuning. 

% \textcolor{red}{Whether general-purpose MLLMs or TIU-specific models}, 
Although these MLLMs present impressive and inspiring results, their rapid evolution and broad adoption have made tracking cutting-edge advancements increasingly challenging. Therefore, a systematic review that is tailored for documents to summarize and analyse these methods is in demand. However, 
existing surveys on text-rich image understanding often exhibit narrow focus: they either analyze domain-specific scenarios (e.g., tables and figures \cite{huang2024detection}, charts \cite{Huang2024ARXIV_From_Pixels_to, alshetairy2024transformersutilizationchartunderstanding}, forms \cite{abdallah2024transformers}) or emphasize unified deep learning frameworks \cite{subramani2011survey,ding2024deep}.

Our systematic survey addresses the gap by providing the first comprehensive analysis of nearly all TIU MLLMs in four dimensions: Model Architectures (Section \ref{sec:model_architecture}), Training Pipeline  (Section \ref{sec:training_pipeline}), Datasets and Benchmarks (Section \ref{sec:datasets}), Challenges and Trends (Section \ref{sec:summary}). This holds both academic and practical significance for advancing the field.



%To systematically establish a research framework for TIU MLLMs, we outline its design framework through the following two aspects: Model Architecture \ref{sec:model_architecture}), Training Pipeline (Section \ref{sec:training_pipeline}). Section \ref{sec:datasets} organizes mainstream datasets and benchmarks for training and evaluation. Finally, Section \ref{sec:summary} synthesizes key insights, identifies unresolved challenges, and proposes research directions to inspire the design of more powerful TIU MLLMs.

% We decompose the general model architecture into three core components: Visual Encoder (Section \ref{section_visual_encoder}), Modality Connector (Section \ref{section_connector}), and LLM Decoder (Section 2.3).
% The training pipeline elaborates on the current mainstream alignment methods, primarily consisting of three stages: Modality Alignment (i.e. Pre-training) (Section 3.1), Instruction Alignment (i.e. Fine-tuning) (Section 3.2), and Preference Alignment (i.e. Post-training) (Section 3.3). Among these, Pre-training and Fine-tuning are essential, while Post-training is optional. 
%Subsequently, in Section 4, we summarize the mainstream datasets used for pre-training and fine-tuning. In Section 5, we comprehensively review the mainstream benchmarks and the performance of major LLM-based TIU models on these benchmarks. In Section 6, we conclude the paper by summarizing the key points, highlighting existing challenges, and proposing potential research directions. We hope that this survey will help researchers gain a deeper understanding of the field and inspire the design of more efficient LLM-based TIU models.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{Figure/3.pdf}
    \vspace{-1em}
    \captionof{figure}{The evolutionary tree of modern LLMs traces the development of language models in recent years and highlights some of the most well-known models. According to the classification of Encoders, the \textcolor[HTML]{2E54A1}{blue} branch is ocr-free, the \textcolor[HTML]{C81D31}{pink} branch is ocr-based, and the \textcolor[HTML]{588F32}{green} branch is Mixture of Encoders.}
    \label{fig:2}
    \vspace{-1em}
\end{figure*}