\section{Training Pipeline}
\label{sec:training_pipeline}
The training pipeline of MLLM for TIU can be delineated into three main stages: 1) Modality Alignment (\textbf{MA}); 2) Instruction Alignment (\textbf{IA}); and 3) Preference Alignment ({\textbf{PA}}).

\subsection{Modality Alignment}
In this stage, previous works typically use OCR data from traditional OCR tasks~\cite{guan2024bridging,guan2023ccd} to pre-train the MLLM, which aims to bridge the modality gap. The general alignment methods can be categorized into three types: recognition, localization, and parsing.

\noindent \textbf{Read Full Text.} UReader~\cite{ye2023ureader} is the first to explore unified document-level understanding, which introduces the Read Full Text task in VQA for pre-training. Specifically, they include 1) reading all texts from top to bottom and left to right, and 2) reading the remaining texts based on given texts.
% UReader leverages text and positional information from images to organize the text in a common reading order: from top to bottom and left to right. There are two approaches to reading the entire text: one is to start from the beginning and read the complete text, and the other is to set a starting point, provide the text to the left of this starting point, and require the model to continue reading from there. For example, an instruction to start reading from the beginning could be ''Recognize the text in the image.'' An instruction to continue reading might be ''The words on this image are {left text}. Continue reading the text.''
Compared to reading the full text, some works~\cite{Lv2024ARXIV_KOSMOS_2_5, Hu2024ARXIV_mPLUG_DocOwl_1} proposes a more structured reading approach by predicting the image markdown, not text transcriptions.
% transcribing the text transcriptions into Markdown format. This approach provides richer structural information for document understanding. For example, $<s><image>Image Embedding</image><md>[Markdown Text]</s>$. Additionally, KOSMOS-2.5 generates the coordinates for each line of text, with each text block being assigned its spatial coordinates in the image, represented as $<s><image>Image Embedding</image><ocr>{\cup}_{n=1}^{N}(B_n \oplus T_n) </s>$, with $\oplus$ indicating the alignment of text lines$T_n$ with their corresponding coordinate bounding boxes$B_n$.Notably, the two tasks are distinguished using <ocr> and <md> in the prompt.

\noindent \textbf{Reading Partial Text within Localization.} Due to the length of document texts, instructions for reading the full text may risk truncation because of the limited token length in LLMs. To address these limitations, Park \emph{et al.}\cite{park2024hierarchical} introduced two novel tasks: Reading Partial Text (RPT) and Predicting Text Position (PTP). The former randomly selects and reads continuous portions of text in the reading order from top to bottom and left to right. For example, ``Q: What is the text in the image between the first 30\%, from 20\% to 40\%, or the last 16\%?'' For the PTP task, given a text segment, the MLLM aims to infer its relative position (percentage format) within the full text. 
% Notably, the relative position here refers to the percentage of the entire text that the segment occupies. 
For example, ``Q: Where is the text {query texts} located within the image? A: 40\% to 80\%''.
% \noindent \textbf{Location.} The aforementioned method by Park et al. uses percentage-based positioning to help LLMs enhance their understanding of the layout of visual text. 
However, this approach can be somewhat obscure and challenging to express accurately. 

Alternatively, some methods~\cite{hu2024mplug_docowl1.5_arxiv,yu2024texthawk,liu2024hrvda} extract texts based on specific spatial positions, which are summarized into two types.
% In addition, extracting text using specific positional information is also a popular method for instruction fine-tuning. The specific methods can be divided into the following two approaches: 
1) Text Recognition aims to extract the textual content from a given position in the image, ensuring that the model can accurately recognize and extract text within specific regions.
2) Text Grounding involves identifying the corresponding bounding box for specific text in the image, which assists the model in understanding the document layout.

% matching specific text in the image with image regions to identify the corresponding bounding box, helping the model understand the layout of text within the visual scene.

% Due to the varying dimensions of each image, the same region may present different coordinate values after image processing, which is disadvantageous for model learning. Researchers typically normalize the representation of bounding boxes, transforming each continuous value within the bounding box into a discrete position label, such as a range from 0 to 999. For example, DocOWL has designed two complementary localization tasks: Multi-grained Text Grounding and Multi-grained Text Recognition. These tasks define four granularities of text: word, phrase, line, and block. A ''word'' indicates a bounding box containing only a single word, a ''phrase'' consists of multiple adjacent words, a ''line'' is composed of a horizontal line of text, and a ''block'' is a combination of multiple consecutive lines.

\noindent \textbf{Parsing.} In document images, many elements (charts, formulas, and tables) may not be represented using plain text. An increasing number of researchers are now focusing on these element parsing. 
% These elements are often rendered using different formats, such as Markdown, LaTeX, or CSV. 
1) Chart Parsing. Chart types include vertical bars, horizontal bars, lines, scatter plots, and pie charts. Charts serve as visual representations of tables, and organizing text in reading order fails to capture their structure. To preserve their mathematical properties, researchers often convert charts into tables. This process involves breaking down the chart into x/y axes and their corresponding values, which can be represented in Markdown, CSV formats, or even converted into Python code. This approach enables models to better understand the chart's specific meaning.
%%%previous versions
% Chart types include vertical bars, horizontal bars, lines, scatter plots, and pie charts. Charts can be considered as visual representations of tables, and organizing text in reading order does not capture the structure of charts. Therefore, researchers often parse charts into tables to best preserve their mathematical properties. The entire chart can be broken down into x/y axes and their corresponding values. Such data can often be represented in Markdown and CSV formats, and some researchers even convert them into Python code. This approach allows models to gain a deeper understanding of the specific meaning of the entire chart.

2) Table Parsing. Compared to charts, tables have a more standardized structure, where rows and columns form key-value pairs. Common formats for representing tables include LaTeX, Markdown, and HTML. Markdown is often used for simple tables due to its concise text format, while HTML can handle cells that span multiple rows and columns, despite its use of many paired tags like <tr></tr> and <td></td>. Some tables, with complex spanning, custom lines, spacing, or multi-page length, require LaTeX for representation. However, the diversity in LaTeX representations can make these tables challenging for models to fully understand.
%%%previous versions
% Compared to charts, tables have a more standardized structure, where the correspondence between rows and columns represents key-value pairs. Common formats for representing tables include LaTeX, Markdown, and HTML. HTML and Markdown are often used to represent simple tables. Markdown provides a more concise text sequence, whereas, unlike Markdown, HTML can accommodate cells that span multiple rows and columns. However, HTML includes many paired tags, such as '$<tr></tr>$' and '$<td></td>$'. Some tables exceed the representational capabilities of HTML and Markdown, requiring LaTeX for representation. This includes complex row and column spanning, custom table lines and spacing, and multi-page long tables. However, due to the diversity in LaTeX representations, such tables are often challenging for models to truly understand.

3) Formula Parsing. Besides tables and charts, formulas are also commonly used. In the pre-training phase, models learn the LaTeX representation of formula images, enhancing their understanding of formulas. This provides a solid foundation for tasks involving formula computation and reasoning during the instruction alignment.
%%%previous versions
% In addition to tables and charts, formulas are also commonly used data for instruction fine-tuning. During the pre-training phase, models are trained to learn the LaTeX representation of formula images, enhancing the model's understanding of formulas. This lays a solid foundation for tasks involving formula computation and reasoning during the fine-tuning phase.

\subsection{Instruction Alignment}

Upon completing the modality alignment pre-training stage, the MLLM acquires basic visual recognition and dialogue capabilities. However, to achieve human-aligned intelligence, three critical capability gaps must be addressed:(1) Advanced multimodal perception and cross-modal reasoning abilities; (2) Prompt robustness across diverse formulations; (3) Zero-shot generalization for unseen task scenarios. To bridge these gaps, instruction alignment through supervised fine-tuning (SFT) has emerged as an effective paradigm. This phase typically unfreezes all model parameters and employs instructional data with structured templates. 
% Taking visual question answering (VQA) as a representative task, the canonical instruction format is: \texttt{<image><question><answer>}.

To systematically address these challenges, we have categorized the current methods emerging in instruction alignment into three distinct levels:

\noindent \textbf{1) Level 1: Visual-Semantic Anchoring.}  We categorize these instructions into two types: i) Answer within the image; and ii) Answer without the image.
This type of instruction data where answers are located directly within the image, assists MLLMs refine their accuracy in generating responses that are directly linked to specific visual content, reducing reliance on generic or contextually weak answers~\cite{mathew2021docvqa, mathew2022infographicvqa}.
% An example may be: \texttt{<image><question>What is the name of the book?<answer>La La Land.}
Certain tasks require reasoning based on world knowledge and involve complex inference procedures, such as scientific question answering~\cite{masry2022chartqa,chen2021geoqa}. Consequently, these instructions are designed with the common characteristic that the answer is not directly visible in the image. This encourages the model to utilize its linguistic comprehension and external knowledge, enhancing its advanced reasoning and inference capabilities. An example might be: ``Q: How much higher is the red bar compared to the yellow bar in the chart, in terms of percentage? A: 12.1\%.''

\noindent \textbf{2) Level 2: Prompt Diversity Augmentation.} To bolster robustness in handling a broader spectrum of prompts, rather than being limited to specific prompts tailored for particular tasks, researchers often employ data augmentation on the question component of the instruction stream. A popular strategy involves leveraging existing large language models to rephrase the same question in multiple ways. For example, consider the original question: ``What is written on the sign in the image?'' It can be rephrased as: ``Can you read the text displayed on the sign shown in the image?''``Identify the sign in the image.''``Please examine the image and list the words that appear within the sign.''
% \texttt{''What does the text on the sign say?''}
% \texttt{''Look at the sign in the image. What information is provided on its label?''}
% \texttt{''Read the text on the sign.''}
% \texttt{''Observe the sign in the image and describe the text content it contains.''}
By utilizing such varied templates, researchers can train MLLMs to better interpret and respond to a wide range of prompts, thereby enhancing their flexibility and accuracy in real-world applications.

\noindent \textbf{3) Level 3: Zero-shot Generalization.} To enhance the generalization ability to handle unseen tasks, several strategies typically are employed:

Chain of Thought (CoT)~\cite{wei2022chain} reasoning involves breaking down complex problems into a series of intermediate steps or sub-tasks, allowing a model to tackle each part systematically. Some studies have demonstrated improvements by incorporating text-level CoT reasoning~\cite{zhang2024cfret} or box-level visual CoT supervision~\cite{shao2025visual}. To better illustrate the process, consider the prompt: ``What is the average of the last four countries' data?'', the CoT reasoning unfolds as follows: i) Identify the data for the last four countries;
ii) Calculate the sum of these values;
iii) Calculate the average by dividing the sum by the number of countries.
% i) Identify the data for the last four countries: Germany (63), Italy (63), Greece (62), Poland (62);
% ii) Calculate the sum of these values: 63 + 63 + 62 + 62 = 250;
% iii) Calculate the average by dividing the sum by the number of countries: 250 / 4 = 62.5.

Another strategy is Retrieval-Augmented Generation (RAG). RAG~\cite{arslan2024survey} combines the strengths of retrieval-based and generation-based approaches by integrating an information retrieval component with a generative model. This method allows the model to access a vast external knowledge base, retrieving pertinent information to inform and enhance the generation process. 
% For example, consider a scenario where a user asks the question: \texttt{''What are the key achievements of Marie Curie?''}
% i) Retrieval Step: The model first queries a large corpus of scientific articles, biographies, and historical documents to retrieve relevant information about Marie Curie. 
% ii) Generation Step: Using the retrieved information as context, the generative model constructs a detailed response. 

\begin{table*}[!t]
    \centering
    \scalebox{0.9}{
    \input{MLLM_OCR/Table/datawithbench}
    }
    \vspace{-0.5em}
    \caption{Representative datasets and benchmarks for Text-rich Image Understanding. Each dataset is marked for training and testing typically according to its content, functions, and user requirements. 
    % ``Train'' indicates that the dataset is typically used for training. ``Test'' indicates that the dataset is used as an evaluation benchmark. 
    % "Train + Test" indicates that the dataset is divided into both training and benchmark portions.
    }
    \label{tab:datawithbenchmark}
    \vspace{-1.5em}
\end{table*}

\subsection{Preference Alignment}
In the modality and instruction alignment stages, the model predicts the next token based on previous ground-truth tokens during training, and on its own prior outputs during inference. If errors occur in the outputs, this can lead to a distribution shift in inference. The more output the model has, the more serious this phenomenon becomes. In previous natural language processing (NLP) works~\cite{lai2024step,pang2025iterative}, a series of preference alignment techniques~\cite{rafailov2024direct, ouyang2022training,shao2024deepseekmath,wang2024mdpo} have been proposed to optimize the output of the model to make it more consistent with human values and expectations. Benefiting from the success of preference alignment applied to NLP, InternVL2-MPO~\cite{wang2024enhancing} introduces preference alignment to the multimodal field and proposes a Mixed Preference Optimization (MPO) to improve multimodal reasoning. Specifically, they propose a continuation-based Dropout Next Token Prediction (DropoutNTP) pipeline for samples lacking clear ground truth and a correctness-based pipeline for samples with clear ground truth. This strategy improves the performance of the model on OCRBench~\cite{fu2024ocrbench}. Nevertheless, its potential to enhance document multimodal reasoning remains under-explored. 

% To solve this problem, a series of preference alignment techniques have been proposed to optimize the output of the model to make it more consistent with human values and expectations. They include Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022training}, Mixed Preference Optimization (MPO)~\cite{wang2024enhancing}, Differential Privacy Optimization (DPO)~\cite{rafailov2024direct}, Group Relative Policy Optimization (GRPO)~\cite{shao2024deepseekmath}, multimodal Direct Preference Optimization (mDPO)~\cite{wang2024mdpo}, \emph{etc.}. 


% Through modality alignment and instruction alignment, MLLM has excellent multimodal understanding, generation, and certain generalization capabilities driven by a large amount of data. However, MLLM still generates inappropriate or unexpected results, often leading to inaccurate, incontextual, or unethical answers.
% \noindent \textbf{RLHF} draws on the idea of reinforcement learning and relies on human feedback to train the model.

% (Reinforcement Learning from Human Feedback) \cite{ouyang2022training} is generally considered a classic work on LLM preference alignment. It draws on the idea of reinforcement learning and relies on human feedback to train the model. The RLHF process consists of two stages: First, the reward model (RM) is trained through human preference data, and then the reward model is used to guide the reinforcement learning optimization of the policy model, in this case, the llMs. RLHF can effectively utilize human preference feedback to better conform the model to human values and expectations. However, it has several notable issues, such as high memory usage, unstable training and complex procedures.

% \noindent \textbf{DPO} (Differential Privacy Optimization) \cite{rafailov2024direct} simplifies the Reinforcement Learning from Human Feedback (RLHF) process, reparameterizes the reward function in the reinforcement learning process and directly learns the policy model from the preference data without an explicit reward model. By incorporating the reward formula into the Bradley-Terry ranking objective, the model is more likely to generate outputs that are acceptable to humans. It simplifies the training process, improves the stability and efficiency of training, and avoids the inconsistency problem between the reward model and the policy model in RLHF. However, DPO cannot fully utilize the reward model and is only applicable to paired preference data, and cannot handle a wider range of feedback types.

% \noindent \textbf{GRPO} (Group Relative Policy Optimization) \cite{shao2024deepseekmath} abandons the traditional Critic model and estimates the advantage function directly through the relative rewards of samples within the group, thereby simplifying the training process and significantly reducing the demand for computing and storage resources. Through relative rewards within the group, GRPO can optimize the strategy more quickly and reduce the variance of strategy updates. Especially when facing complex long-term decision-making problems, it can effectively improve efficiency and ensure a more stable learning process. However, since GRPO needs to sample a set of actions for each state, this may increase the sampling cost in some cases, and GRPO may not be as stable as other methods in some tasks, especially when the reward signal is sparse.

% \noindent \textbf{mDPO} (multimodal Direct Preference Optimization) \cite{wang2024mdpo} is a preference optimization method for multimodal large language models, which aims to address the shortcomings of DPO for image modalities in multimodal scenarios. mDPO introduces new image preference pairs and an image-based reward anchoring mechanism, which effectively solves the unconditional preference problem in multimodal preference optimization, that is, the problem in which the model ignores the image conditions during the optimization process. By introducing image preference pairs, mDPO can better utilize visual information, improve the model's ability to understand images, significantly reduce the hallucination output of MLLM, and improve model performance. However, due to the introduction of image preference comparison, mDPO will bring additional memory and computational costs during the optimization process, which is usually more than twice that of DPO under the same circumstances.
