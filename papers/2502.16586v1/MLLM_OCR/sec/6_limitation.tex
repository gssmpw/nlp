\section{Limitations}

This paper provides a systematic review of multimodal large language models (MLLMs) in the field of Text-rich Image Understanding (TIU). While the research team has conducted comprehensive retrieval and integration of core literature prior to the submission deadline, certain minor studies may still remain uncovered. It should be particularly noted that due to publisher formatting requirements, the exposition of existing technical approaches and benchmark datasets in this work maintains essential conciseness. For complete algorithmic implementation details and experimental parameter configurations, researchers are strongly recommended to consult the original publications.

%按场景介绍TIU 领域常用的 benchmark， eval metrics，
%分析当前 benchmark 存在的问题后，提出新的 benchmark，并提供当前已知可测试的 MLLM 模型在这个 benchmark 的结果





% \subsection{Evaluation metrics}
% Evaluation metrics play a critical role in assessing the performance of MLLM across various tasks and benchmarks. They include Precision, Recall, F1 Score, Accuracy, MAP, and ANLS. More details will be introduced in Supplemental Martial.
% Specifically, we introduce the important ANLS metric in detail:

% The key evaluation metrics are as follows:

% \textcolor{red}{\subsubsection{Precision, Recall, F1 Score, and Accuracy}}

% Precision, Recall, F1 Score, and Accuracy are pivotal in assessing model performance across diverse tasks.

% \noindent \textbf{Precision} measures the model's ability to correctly identify positive instances within multi-modal data. This metric is calculated as follows:
% \begin{gather}
%     \mathrm{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}}
% \end{gather}

% \noindent \textbf{Recall} evaluates the model's capability to identify all relevant positive instances in multi-modal tasks. This metric is calculated as follows:
% \begin{gather}
%     \mathrm{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}
% \end{gather}

% \noindent \textbf{F1 Score}, as the harmonic mean of precision and recall, is particularly valuable in scenarios where a balance between these two metrics is desired. This metric is calculated as follows:
% \begin{gather}
%     \mathrm{F1~Score}=\frac{2\times\mathrm{Precision}\times\mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}
% \end{gather}

% \noindent \textbf{Accuracy} represents the proportion of correctly predicted observations and is the most straightforward performance metric. This metric is calculated as follows:
% \begin{gather}
%     \mathrm{Accuracy}=\frac{\text{TP}+\text{TN}}{\text{Total Samples}}
% \end{gather}
% Where, TP (TP) are the correctly predicted positive samples. FP (FP) are the incorrectly predicted positive samples. FN (FN) are the incorrectly predicted negative samples. TN (TN) are the correctly predicted negative samples. Total Samples is the total number of samples
% in the dataset.

% \subsubsection{Mean Average Precision (MAP)}
% MAP is employed to evaluate the precision of retrieval tasks involving multi-modal data, such as retrieving the most relevant images from a database based on a given text description. MAP is employed to evaluate the precision of retrieval tasks involving multi-modal data, such as retrieving the most relevant images from a database based on a given text description.  MAP effectively measures the model's average performance across multiple queries, ensuring the model excels not only in isolated instances but consistently. MAP effectively measures the model's average performance across multiple queries, ensuring the model excels not only in isolated instances but consistently. The formula for calculating MAP is as follows:
% \begin{gather}
% \mathrm{MAP}=\frac{1}{Q}\sum_{q=1}^Q\mathrm{AP}(q)
% \end{gather}
% Where, $Q$ is the total number of queries. $\mathrm{AP}(q)$ is the Average Precision for query $q$, calculated as the average precision of relevant documents at each position in the ranked list:
% \begin{gather}
% \mathrm{AP}(q)=\frac{1}{N_q}\sum_{k=1}^{N_q}\mathrm{Precision}(k)\times\mathrm{Relevance}(k)
% \end{gather}
% Where, $N_q$ is the total number of retrieved documents for query $q$. $\mathrm{Precision}(k)$ is the precision at position $k$ in the ranked list. $\mathrm{Relevance}(k)$ is an indicator function that
% takes the value 1 if the document at position
% $k$ is relevant to query $q$, and 0 otherwise.

% \textcolor{red}{\subsubsection{Average Normalized Levenshtein Similarity (ANLS)}}
% ANLS~\cite{biten2019scene} captures the OCR mistakes applying a slight penalization in case of correct intended responses, but badly recognized. It also makes use of a threshold of value 0.5 that dictates whether the output of the metric will be the ANLS if its value is equal to or bigger than 0.5 or 0 otherwise. The key point of this threshold is to determine if the answer has been correctly selected but not properly recognized, or on the contrary, the output is a wrong text selected from the options and given as an answer.

% More formally, the ANLS between the net output and the ground truth answers is given by the following equation. Where $N$ is the total number of questions, $M$ total number of GT answers per question, $a_{ij}$ the ground truth answers where $i = {0, ..., N}$, and $j = {0, ..., M}$, and $o_{qi}$ be the network’s answer for the ith question $q_i$:
% \begin{gather}
% \mathrm{ANLS}=\frac{1}{N} \sum_{i=0}^N\left(\max _j s\left(a_{i j}, o_{q i}\right)\right)
% \end{gather}
% Where, $s\left(a_{i j}, o_{q i}\right)$ is defined as follows:
% \begin{gather}
% s\left(a_{i j}, o_{q i}\right)= \begin{cases}1-\mathrm{NL}\left(a_{i j}, o_{q i}\right), & \text { if } \mathrm{NL}\left(a_{i j}, o_{q i}\right)< \tau\\ 0, & \text { if } \mathrm{NL}\left(a_{i j}, o_{q i}\right) \geq\tau \end{cases}
% \end{gather}

