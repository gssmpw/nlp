\begin{abstract}
The recent emergence of Multi-modal Large Language Models (MLLMs) has introduced a new dimension to the Text-rich Image Understanding (TIU) field, with models demonstrating impressive and inspiring performance. However, their rapid evolution and widespread adoption have made it increasingly challenging to keep up with the latest advancements. To address this, we present a systematic and comprehensive survey to facilitate further research on TIU MLLMs. Initially, we outline the timeline, architecture, and pipeline of nearly all TIU MLLMs. Then, we review the performance of selected models on mainstream benchmarks. Finally, we explore promising directions, challenges, and limitations within the field.
% LLM-based text-rich image understanding models, through joint vision-language modeling, overcome modality bias and enhance zero-shot capabilities, significantly advancing the comprehension of text-rich images and establishing a new research paradigm. This paper systematically reviews the progress of multimodal large language models (MLLMs) in the TIU domain, analyzing design paradigms of model architectures, training methodologies, datasets, and performance benchmarks. It identifies future challenges and provides theoretical foundations and practical pathways for developing universal TIU models.

\end{abstract}
% \begin{IEEEkeywords}
% Document Understanding, Multimodal Large Language Model, Survey.
% \end{IEEEkeywords}