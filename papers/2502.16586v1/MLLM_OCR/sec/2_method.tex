\section{Model Architecture}
\label{sec:model_architecture}
%VDU任务详细介绍：1.答案在图上；2.答案不在图上；及公共数据集

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{Figure/Architecture.png} 
%     \caption{The general model architecture of MLLMs and the implementation choices for each component} 
%     \label{fig:Architecture} 
% \end{figure*}


TIU MLLM methods typically leverage pre-trained general visual foundation models to extract robust visual features or employ OCR engines to capture text and layout information from images. A modality connector is then used to align these visual features with the semantic space of the language features from the LLM. Finally, the combined visual-language features are fed into the LLM, which utilizes its powerful comprehension capabilities for semantic reasoning to generate the final answer.
As illustrated in Figure~\ref{fig:Architecture}, the framework of TIU MLLMs can be abstracted into three core components: Visual Encoder, Modality Connector, and LLM Decoder. 
% In the subsequent sections, we systematically analyze the implementation choices and technical considerations for each component.


%Multi-modal Document Understanding aims to extract meaningful information from text images of various types, such as charts, tables, documents, and other scene texts, through a question-driven image-to-sequence task.
%Some early studies~\cite{yang2021tap} have explored end-to-end solutions within a specialist model, which may not provide broad robustness and generality for various scenarios.
%The recent emergence of Multi-modal Large Language Models (MLLMs) has introduced a novel dimension to the field by linking visual image tokens and language tokens in a sequence-to-sequence format, thereby facilitating task unification. This structure seamlessly integrates computer vision with natural language processing, allowing MLLMs to significantly enhance text reading capabilities, supported by large-scale data and GPU resources. These methods can be roughly categorized into two types: OCR-dependent MLLMs~\cite{lu2024bounding,lee2025moai,luo2022bi,Instructdoc,Doclayllm} and OCR-free MLLMs~\cite{internvl,minigpt,MiNi-Monkey,Monkey,Mplug-Docowl2,UReader}.
%A Bounding Box is Worth One Token:Interleaving Layout and Text in a Large Language Model for Document Understanding
\subsection{Visual Encoder}
\label{section_visual_encoder}
The Visual Encoder $\mathcal{F}(\cdot)$ is responsible for transforming input image $\mathbf{I}$ into feature representations $\mathbf{V}$, expressed as $\mathbf{V} = \mathcal{F}(\cdot)(\mathbf{I})$. As illustrated in Figure \ref{fig:2}, these encoders can be categorized into OCR-free, OCR-based, or a hybrid approach.
% \begin{equation}
%     \begin{aligned}
%         \mathbf{F} = \mathcal{F}(\cdot)(\mathbf{I})
%     \end{aligned}
% \end{equation}
% Depending on the task requirements, the Visual Encoder can be implemented in various forms, including: OCR-free encoder, OCR-based encoder, or a mixture of encoders. 
% \textcolor{red}{Below is a summary of common types of Visual Encoders and their characteristics:}



\noindent \textbf{OCR-free Encoder} is widely used to extract high-level visual features, effectively capturing essential information about objects, scenes, and textures. The commonly used OCR-free encoders include (1) \textbf{CLIP} \cite{radford2021CLIP}; (2) \textbf{ConvNeXt} \cite{woo2023convnext}; (3) \textbf{SAM} \cite{kirillov2023SAM}; (4) \textbf{DINOv2} \cite{oquab2023dinov2}; (5) \textbf{Swin-T} \cite{liu2021swin}; (6) \textbf{InternViT} \cite{chen2024internvlscalingvisionfoundation}.

% 1) \textbf{CLIP-ViT} \cite{radford2021CLIP}, excels in vision-language alignment, making it well-suited for processing global semantic information; 

% 2) \textbf{ConvNeXt} \cite{woo2023convnext}, performs exceptionally well in visual tasks, making it ideal for extracting local detailed features; 

% 3) \textbf{SAM} \cite{kirillov2023SAM}, focuses on image segmentation, providing precise object boundary details;

% 4) \textbf{DINOv2} \cite{oquab2023dinov2}, with its self-supervised learning capabilities, is highly effective in capturing rich and robust visual representations across diverse tasks; 

% 5) \textbf{SwinTransformer}, known for its hierarchical architecture and shifted window mechanism, excels in handling high-resolution images and capturing multi-scale features, making it a powerful choice for tasks requiring detailed spatial understanding; 

% 6) \textbf{InternViT}, achieves direct alignment with LLMs during training, enabling it to exhibit superior adaptability and performance in multimodal tasks compared to other ViT models.

\noindent \textbf{OCR-based Encoder} processes textual content and layout information from OCR outputs through three primary paradigms:
(1) \textbf{Direct Input} injects raw OCR texts into LLMs, though long sequences degrade inference efficiency \cite{he2023icl};
(2) \textbf{Cross-Attention} dynamically selects salient content via attention mechanisms within LLMs \cite{Wang2023ARXIV_DocLLM_A_layout};
(3) \textbf{External Encoder} employs specialized models like BLIP-2~\cite{li2023blip2bootstrappinglanguageimagepretraining}, DocFormerv2~\cite{nacson2024docvlmmakevlmefficient} or LayoutLMv3~\cite{Yupan2022ARXIV_LayoutLMv3_Pre_training} to structure OCR features before LLM integration \cite{ Tanaka2024AAAI_InstructDoc_A_Dataset, Luo2024CVPR_LayoutLLM_Layout_Instruction, Fujitake2024LREC_LayoutLLM_Large_Language}.

\noindent \textbf{Mixture of Encoders} strategies address TIU task complexity through two dominant configurations:
(1) \textbf{Dual OCR-Free} architectures (\emph{e.g.}, CLIP+SAM) combine complementary visual encoders to jointly capture global semantics and local details~\cite{wei2024vary};
(2) \textbf{Hybrid OCR-Free/OCR-Based} systems (\emph{e.g.}, CLIP+LayoutLMv3) synergize visual feature extraction with text-layout understanding, proving particularly effective for document-level tasks requiring multimodal reasoning \cite{Liao2024ARXIV_DocLayLLM_An_Efficient}.

\begin{comment}
\noindent \textbf{OCR-Based Encoder} is designed to efficiently encode textual content and layout information extracted by the OCR engine, thereby providing robust support for subsequent semantic understanding tasks. The forms of OCR encoders can be summarized as follows:

1) \textbf{Direct Input}: This approach inputs OCR texts or layouts into the LLM directly, which results in excessively long input lengths, adversely affecting inference speed and model performance.
%%%previous version
% Directly inputs the OCR texts into the LLM, or models document layouts by inserting spaces and newline characters between OCR-recognized text lines, providing the LLM with input that includes basic layout information. However, a significant drawback is that the input length may become excessively long, which can negatively impact inference speed and model performance.

2) \textbf{Cross-Attention}: This approach selects important contents from OCR engine outputs by a cross-attention mechanism within the LLM.
%%%previous version
% Through a cross-attention mechanism, the interaction between OCR positional information and text features is achieved within the LLM. This approach alters the internal structure of the LLM, potentially affecting its generality and generalizability.

3) \textbf{External Encoder}: \textcolor{red}{Utilizes BLIP-2 or LayoutLMv3 to encode document OCR information, which is then fed into the LLM.} 
% It avoids the issue of excessively long inputs and enhances information density. However it requires additional encoders, and understanding capability of these encoders may not match that of the LLM, necessitating significant training resources for alignment.


\noindent \textbf{Mixture of Encoders}
The inspiration for using a mixture of encoders stems from the complexity and diversity of TIU tasks. While single visual encoders excel in specific tasks, they often struggle to balance the capture of global and local information when faced with high-resolution images, complex scenes, or multitask learning. 
% Given the theoretically vast number of possible combinations, only the mainstream mixture strategies in LLM-based TIU tasks are listed here:
Here, we outline the mainstream mixture strategies used in LLM-based TIU tasks:

1) \textbf{Dual OCR-Free Encoders}. This approach combines two OCR-free encoders, where one excels at processing low-resolution inputs and the other specializes in high-resolution inputs, such as CLIP paired with SAM. By leveraging their complementary strengths, this integration enables the model to capture both global semantic information and detailed local features, significantly enhancing its ability to handle complex visual tasks.

%%%previous version
% This approach combines two OCR-free encoders, designed with the rationale that one excels at processing low-resolution inputs while the other specializes in high-resolution inputs, such as CLIP and ConvNeXt, or CLIP and SAM. By leveraging their complementary strengths in extracting high-level visual features, the integration of these encoders enables the model to simultaneously capture global semantic information and fine-grained local details, thereby significantly enhancing its capability to handle complex visual tasks.

2) \textbf{Hybrid of OCR-Free and OCR-Based Encoders}. This strategy combines an OCR-free encoder (e.g., CLIP) with an OCR-based encoder to simultaneously process visual and textual information. OCR-free encoders focus on extracting high-level visual features, such as objects and scenes, while OCR-based encoders specialize in capturing textual content and layout information from images. This hybrid approach is particularly effective for tasks involving text-rich images or documents.
\end{comment}

\subsection{Modality Connector}
\label{section_connector}

Visual embeddings $\mathbf{V} = [\mathbf{v}_{1}, \mathbf{v}_{2},..., \mathbf{v}_{n}]$ and language embeddings $\mathbf{T} = [\mathbf{t}_{1}, \mathbf{t}_{2},..., \mathbf{t}_{l}]$ belong to different modalities. Consequently, to bridge the gap between them and create unified sequence representations that can be processed by large language models (LLMs), a modality connector $\xi: \mathbf{V} \xrightarrow{} \mathbf{T}$ is typically employed, which is responsible for converting $n$ visual features into $m$ visual tokens.
We review the strategies previously utilized in the literature for this purpose. 

Specifically, the modality connector can be easily implemented using a simple linear projector or multi-layer perception (MLP), \emph{i.e.,} $m=n$, but faces challenges in scalability and efficiency. Recent works also proposed more effective and innovative modality connectors from various perspectives, such as token compression and token reduction. The former focuses on reducing the number of inputs to the MLLM token with lossless compression, and the latter addresses the issue of costly tokens by removing redundant and unimportant token representations, such as background tokens.

\noindent \textbf{Token Compression}

1) Pixel shuffle \cite{chen2024far} rearranges the elements of a high-resolution feature map $(h, w)$ to form a lower-resolution feature map $(\frac{h}{s}, \frac{w}{s})$ by redistributing the spatial dimensions into the depth (channels) of the feature map. Here, $s$ denotes the compression rate. 
% This operation effectively reduces the features' width and height, while increasing the number of channels. Specifically, pixel shuffle typically has two variations: image-level shuffle and window-level shuffle. The former divides the entire feature map into $s\times s$ parts and concatenates each part $(\frac{h}{s}, \frac{w}{s})$ along the channel dimension. In contrast, similar to the Swin Transformer, the window-level shuffle performs the shuffle process within each window of size  $s\times s$ and concatenates the shuffled features along the spatial dimension.
We summarized the process as $\xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{\frac{h}{s} \times \frac{w}{s} \times (s\times C)}$.
% \begin{equation}
%     \xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{\frac{h}{s} \times \frac{w}{s} \times (s\times C)}
% \end{equation}

% 2) Cross-attention used in \cite{bai2023qwenvlversatilevisionlanguagemodel} operates on the queries which are a group of trainable vectors and the keys which are the image features produced by the vision encoder. 
% \begin{equation}
%     \xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{q \times D}
% \end{equation}

\noindent \textbf{Token Reducer} 

1) Cross Attention \cite{alayrac2022flamingovisuallanguagemodel,li2023blip2bootstrappinglanguageimagepretraining,chen2024internvlscalingvisionfoundation,dai2023instructblipgeneralpurposevisionlanguagemodels} operates on the queries (a group of trainable vectors or the key features of the model itself) and the keys which are the image features produced by the vision encoder. We summarized the process as $\xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{q \times D}$. 
% \begin{equation}
%     \xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{q \times D}
% \end{equation}


% introduces a set of custom-defined learnable embeddings that serve as adaptive filters to selectively extract valuable information from image embeddings relevant to the task. Specifically, several cross-attention layers are applied to facilitate this process, where these learnable embeddings act as the query, while the image embeddings serve as the key and value. In MLLMs, the number $q$ of learnable embeddings is typically set to a value significantly lower than the total number of image embeddings. We summarized the process as follows:
% \begin{gather}
%     \xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{q \times D}
% \end{gather}

2) H-Reducer~\cite{Hu2024ARXIV_mPLUG_DocOwl_1} introduces the $1 \times 4$ convolution layer to reduce visual features, as the horizontal texts are widely found in natural scenes and semantically coherent.
% consists of a convolution layer and a fully connected linear layer. The convolution kernel is set to $1 \times 4$, as the horizontal texts are widely found in natural scenes and semantically coherent. Then, a fully connected layer is used to align reduced visual features to the language embedding space $\mathbb{R}^{D}$. 
We summarized the process as $\xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{h \times \frac{w}{4} \times D}$.
% \begin{gather}
%     \xi: \mathbb{R}^{h \times w \times C} \xrightarrow{} \mathbb{R}^{h \times \frac{w}{4} \times D}
% \end{gather}

3) C/D-abstract~\cite{cha2024honeybee} employs Convolution and Deformable Attention respectively to achieve both flexibility and locality preservation.

4) Attention Pooling~\cite{Liu2024ARXIV_TextMonkey_An_OCR,Huang2024ARXIV_Mini_Monkey_Alleviating} identifies important tokens and removes redundant ones. To evaluate the redundancy of image features, the similarity between image tokens is often utilized~\cite{Liu2024ARXIV_TextMonkey_An_OCR}. This method selects tokens that are highly unique and lack closely similar counterparts. Average pooling is the most special one.
\begin{table*}[!t]
    \centering
    \scalebox{0.78}{
    \input{Table/MLLM}
    }
    \vspace{-0.5em}
    \caption{The summary of representative mainstream MLLMs, including the model architectures, training pipelines, and scores on the four most popular benchmarks of TIU. ``Private'' indicates that the MLLM utilizes a proprietary large model. ``$\dagger$'' indicates the results are obtained by downloading official open-source model and testing it locally.}
    \label{tab:mllm_summary}
    \vspace{-1.5em}
\end{table*}

\subsection{LLM Decoder}
% The LLM decoder receives the aligned features from the Modality Connector, integrates them into the language context for reasoning, and generates the final output based on specific task requirements, such as text answers, image descriptions, structured data, and more.
The aligned features are fed into the LLM decoder together with the language embeddings for reasoning. We list the commonly used LLMs in MLLM:
% Below, we outline the commonly used LLMs in MLLMs: %, along with their key characteristics:

\noindent \textbf{LLaMA Series}. LLaMA \cite{touvron2023llama, touvron2023llama2, dubey2024llama} is a series of open-source large language models developed by Meta, aimed at promoting openness and innovation in artificial intelligence technology, LLaMA series include models of varying parameter scales (e.g., 7B, 13B, 34B). 
% Since the release of the first-generation model LLaMA 1 in 2023, the series has undergone continuous iterative upgrades in terms of parameter scale, training data, context window, and multimodal capabilities. The latest LLaMA 3.2 supports a context length of 128K tokens.

\noindent \textbf{Qwen Series}. Qwen \cite{bai2023qwen,yang2024qwen2}, developed by Alibaba, is a multilingual LLM that supports both Chinese and English.
%Qwen-VL, its multimodal variant, enables joint understanding and generation of images and text.
% Since its initial release in 2023, the Qwen series of models has become a significant force in the field of large models through continuous technological innovation and open-source strategies. The latest version of this series is Qwen2.5, which supports a context window of up to 1M tokens and demonstrates exceptional performance in reasoning tasks. Multimodal models based on Qwen, such as Qwen-VL \cite{bai2023qwenvl} and Qwen2-VL \cite{wang2024qwen2}, have also shown immense potential in the field of TIU.


\noindent \textbf{Vicuna Series}. Vicuna \cite{zheng2023judging} is an open-source large language model built on LLaMA, developed by research teams from institutions including UC Berkeley, CMU, and Stanford. 
% It is offered in two parameter sizes: 7B and 13B. The Vicuna series has multiple versions, with the widely used Vicuna-v1.5 version supporting context lengths of 4K and 16K tokens. Since the acclaimed open-source multimodal large language model LLaVA \cite{liu2024visual, liu2024improved} adopted Vicuna as its LLM backbone, numerous TIU models have been enhanced based on LLaVA. This has significantly advanced the development of LLM-based TIU methods.


\noindent \textbf{InternLM series}. InternLM \cite{cai2024internlm2} is an open-source large language model series developed by the Shanghai Artificial Intelligence Laboratory, with the latest version, InternLM 2.5, offering parameter sizes of 1.8B, 7B, and 20B. 
% As a bilingual model, it supports both Chinese and English, making it versatile for a wide range of applications. A key highlight of InternLM is its ability to handle a 1M (1 million) token context window, which makes it particularly effective for tasks involving long-text processing, such as document comprehension and multi-turn dialogue.






% \subsection{OCR-dependent MLLMs} 
% They enhance document understanding by integrating text, layout, and other data extracted from external OCR tools~\cite{PP-OCRv3} into large language models. LayTextLLM~\cite{LayTextLLM} and DocLayLLM~\cite{Doclayllm} both utilize an external OCR engine to extract layout and text, integrating them into a LLM for document understanding. However, this integration complicates the workflow and leads to an excess of auxiliary tokens, particularly in images with dense texts.

% \noindent \textbf{Document encoder}

% \noindent \textbf{CoT}

% \noindent \textbf{Ocr output as context}

% % 这段应该结合 intro 里对当前方法的批评，再次强调当前的方法缺少spatially-aware alignment
% \subsection{OCR-free MLLMs} 
% These methods perform the multi-modal document understanding task by directly producing question-driven outputs in an end-to-end manner. These methods typically focus on high-resolution image processing~\cite{Monkey,UReader,mplug-docowl1.5,docpedia}, efficient token compression~\cite{zhang2024token,Mplug-Docowl2,yu2024texthawk2}, and refined attention mechanisms~\cite{MiNi-Monkey,VisualCoT}. 

% \noindent \textbf{High resolution}

% \noindent \textbf{Visual encoder alignment}

% \noindent \textbf{Token compresssion}


% \subsection{Vision Language Pre-training}
% Inspired by recent advancements~\cite{CLIP,DINO,SigLIP,SIGA,SAM} in pre-training techniques, the integration of image and text multi-modal information into OCR-related tasks has gained increasing attention. Using cross-modal visual-language priors, early works focused on endowing visual foundation models with semantic knowledge~\cite{FastTCM,TCM,SKTM,VLPT,oCLIP,duan2024odm,guan2025bridging,CCD} for applications such as text spotting, detection, recognition, removal, and super-resolution. As MLLMs rapidly develop, researchers are further capitalizing on these visual-language priors to bridge visual and language modalities through diverse pre-training tasks~\cite{UReader,Cream,Doclayllm,DocLLM,docpedia,LayTextLLM,internvl,liu2024textmonkey,minigpt,mplug-docowl1.5}. For instance, UReader~\cite{UReader} introduces the Read Full Text (RFT) task in VQA form for enhancing document-level understanding. Park \emph{et al.}~\cite{parkhierarchical} propose two new pretext tasks: Reading Partial Text (RPT) and Predicting Text Position (PTP). Similarly, KOSMOS-2.5~\cite{lv2023kosmos} designs a Visual Text Grounding (VTG) task, which inputs the texts within images and produces the corresponding bounding boxes. mPLUG-DocOWL~\cite{mplug-docowl1.5} integrates multiple tasks to conduct the struct-aware parsing in documents, tables, charts, and natural scenes. However, these question-driven image-to-sequence tasks predominantly emphasize \emph{semantic alignment}, and may rely on the powerful semantic context capabilities of LLMs when responding. Following these VQA forms, we further introduce an additional mask generation pre-text task (VQAMask) to explicitly facilitate spatially-aware visual-language alignment.