\section{Related Work}
\textbf{Layer Normalization in Language Models.} 
LN ____ was initially applied after the residual connection in the original Transformer ____, which is known as Post-LN. Later on, Pre-LN  ____ dominated LLMs, due to its compelling performance and stability ____. Prior works have studied the effect of Pre-LN and Post-LN. 
____ proves that Post-LN tends to have larger gradients near the output layer, which necessitates smaller learning rates to stabilize training, whereas Pre-LN scales down gradients with the depth of the model, working better for deep Transformers.  ____ empirically confirmed that Pre-LN facilitates stacking more layers and Post-LN suffers from gradient vanishing. The idea of connecting multiple layers was proposed in previous works ____. Adaptive Model Initialization (Admin) was introduced to use additional parameters to control residual dependencies, stabilizing Post-LN. DeepNorm ____ enables stacking 1000-layer Transformer by upscaling the residual connection before applying LN. Additionally, ____ proposed Sandwich LayerNorm, normalizing both the input and output of each transformer sub-layer. ____ introduced B2T to bypass all LN except the final one in each layer. ____ recently combines Post-LN and Pre-LN to enhance the middle layers.