[
  {
    "index": 0,
    "papers": [
      {
        "key": "ba2016layer",
        "author": "Ba, Jimmy Lei",
        "title": "Layer normalization"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "baevski2018adaptive",
        "author": "Baevski, Alexei and Auli, Michael",
        "title": "Adaptive input representations for neural language modeling"
      },
      {
        "key": "dai2019transformer",
        "author": "Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan",
        "title": "Transformer-xl: Attentive language models beyond a fixed-length context"
      },
      {
        "key": "nguyen2019transformers",
        "author": "Nguyen, Toan Q and Salazar, Julian",
        "title": "Transformers without tears: Improving the normalization of self-attention"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral 7B"
      },
      {
        "key": "bi2024deepseek",
        "author": "Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others",
        "title": "Deepseek llm: Scaling open-source language models with longtermism"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xiong2020layer",
        "author": "Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan",
        "title": "On layer normalization in the transformer architecture"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2019learning",
        "author": "Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S",
        "title": "Learning deep transformer models for machine translation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "bapna2018training",
        "author": "Bapna, Ankur and Chen, Mia Xu and Firat, Orhan and Cao, Yuan and Wu, Yonghui",
        "title": "Training deeper neural machine translation models with transparent attention"
      },
      {
        "key": "dou2018exploiting",
        "author": "Dou, Zi-Yi and Tu, Zhaopeng and Wang, Xing and Shi, Shuming and Zhang, Tong",
        "title": "Exploiting deep representations for neural machine translation"
      },
      {
        "key": "wang2019learning",
        "author": "Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S",
        "title": "Learning deep transformer models for machine translation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024deepnet",
        "author": "Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu",
        "title": "Deepnet: Scaling transformers to 1,000 layers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ding2021cogview",
        "author": "Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others",
        "title": "Cogview: Mastering text-to-image generation via transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "takase2022b2t",
        "author": "Takase, Sho and Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun",
        "title": "B2t connection: Serving stability and performance in deep transformers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2024mix",
        "author": "Li, Pengxiang and Yin, Lu and Liu, Shiwei",
        "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN"
      }
    ]
  }
]