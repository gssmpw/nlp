\section{Related Work}
\textbf{Layer Normalization in Language Models.} 
LN \citep{ba2016layer} was initially applied after the residual connection in the original Transformer \citep{vaswani2017attention}, which is known as Post-LN. Later on, Pre-LN  \citep{baevski2018adaptive,dai2019transformer,nguyen2019transformers} dominated LLMs, due to its compelling performance and stability \citep{brown2020language,touvron2023llama,jiang2023mistral,bi2024deepseek}. Prior works have studied the effect of Pre-LN and Post-LN. 
\citet{xiong2020layer} proves that Post-LN tends to have larger gradients near the output layer, which necessitates smaller learning rates to stabilize training, whereas Pre-LN scales down gradients with the depth of the model, working better for deep Transformers.  \citet{wang2019learning} empirically confirmed that Pre-LN facilitates stacking more layers and Post-LN suffers from gradient vanishing. The idea of connecting multiple layers was proposed in previous works \citep{bapna2018training,dou2018exploiting,wang2019learning}. Adaptive Model Initialization (Admin) was introduced to use additional parameters to control residual dependencies, stabilizing Post-LN. DeepNorm \citep{wang2024deepnet} enables stacking 1000-layer Transformer by upscaling the residual connection before applying LN. Additionally, \citet{ding2021cogview} proposed Sandwich LayerNorm, normalizing both the input and output of each transformer sub-layer. \citet{takase2022b2t} introduced B2T to bypass all LN except the final one in each layer. \citet{li2024mix} recently combines Post-LN and Pre-LN to enhance the middle layers.