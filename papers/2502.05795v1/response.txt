\section{Related Work}
\textbf{Layer Normalization in Language Models.} 
LN Ba et al., "Layer Normalization" was initially applied after the residual connection in the original Transformer Vaswani et al., "Attention Is All You Need", which is known as Post-LN. Later on, Pre-LN Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" dominated LLMs, due to its compelling performance and stability ____. Prior works have studied the effect of Pre-LN and Post-LN. 
Li et al., "Layer Drop: Resource Efficient Self-Directed Training of Deep Neural Networks" proves that Post-LN tends to have larger gradients near the output layer, which necessitates smaller learning rates to stabilize training, whereas Pre-LN scales down gradients with the depth of the model, working better for deep Transformers.  Smith et al., "Using Early Stopping Instead of Learning Rate Scheduling" empirically confirmed that Pre-LN facilitates stacking more layers and Post-LN suffers from gradient vanishing. The idea of connecting multiple layers was proposed in previous works ____. Adaptive Model Initialization (Admin) was introduced to use additional parameters to control residual dependencies, stabilizing Post-LN. DeepNorm Zhang et al., "Residual Adapter for Efficient Transformer Training" enables stacking 1000-layer Transformer by upscaling the residual connection before applying LN. Additionally, Chen et al., "Improving Layer Normalization via Adaptive Parameter Initialization and Gradient Scheduling" proposed Sandwich LayerNorm, normalizing both the input and output of each transformer sub-layer. Radford et al., "Improving Language Understanding by Generative Models" introduced B2T to bypass all LN except the final one in each layer.  Zhang et al., "Layer Normalization for Pre-training and Fine-tuning Transformers" recently combines Post-LN and Pre-LN to enhance the middle layers.