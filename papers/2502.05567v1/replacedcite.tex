\section{Related Work}
\smallsec{Autoformalization}
The task of autoformalization can be seen as a machine translation problem ____, which fundamentally adheres to the principle of mapping content from the source language into expressions that are consistent with the established syntax and lexical system of the target language. Early approaches ____ have utilized neural machine translation techniques to address the task of autoformalizing theorem statements. With the rapid advancements in LLMs, recent research on LLM-based autoformalization can be broadly categorized into three main paradigms. Specifically, researchers ____ have developed the application of few-shot prompting to enable LLMs to perform autoformalization effectively, while works such as ProofNet ____ and MMA ____ have further refined autoformalization performance by fine-tuning LLMs using parallel statements. Furthermore, ____ integrate retrieval-augmented generation techniques with LLMs to achieve additional improvements. 

\begin{figure*}[!htbp]
\centering
\includegraphics[width=\columnwidth]{figures/atlas_framework.pdf}
\caption{Overview of ATLAS. (a) Data Lifting: Collect mathematical topics from Mathlib to construct the concept repository, thereby preparing for subsequent data synthesis. (b) Data Synthesis: Randomly sample a pair of concepts from the concept repository, then generate an NL statement using NL-Gen teacher model. Subsequently, utilize the student model to translate this NL statement into a FL statement, which is then parsed and passed to the Lean compiler. Any FL statement that fails compilation is forwarded to FL-Rev teacher model for revision, while the modified and successfully compiled FL statement, along with the previously validated FL statement, are sent to FL-Align teacher model for semantic accuracy test. FL statement that passes the semantic accuracy check together with its corresponding NL statement constitutes a high-quality parallel statement, whereas those that fail is discarded, retaining only the NL statement for use in the next iteration. (c) Data Augmentation: Augment the FL statement obtained in the previous step using both proof and contraposition methods, and then use LLMs to construct NL statements. These parallel statements, along with the previously generated parallel statements, collectively form the ATLAS dataset.}
\label{fig:framework}
\end{figure*}

Meanwhile, another research direction focuses on the autoformalization of proofs ____, which is more challenging and resembles a simplified automated theorem-proving task. For example, DSP ____ uses LLMs to generate informal proofs, which are then mapped to formal proof sketches. These sketches are used to guide automated  theorem provers in filling these proof gaps.

\smallsec{Dataset Generation}
Obtaining large-scale, high-quality datasets of parallel statements remains a significant challenge. Previous efforts, such as MMA ____ and HERALD ____, have approached this problem by leveraging Mathlib to extract FL statements and employing LLMs to generate their NL counterparts. While effective, the limited scope and size of Mathlib constrains the scalability of these datasets. On the other hand, approaches like Lean Workbook ____ and DeepSeek-Prover ____ take the opposite direction, collecting NL statements from large-scale web sources and translating them into FL representations. While this strategy enables the generation of large-scale datasets, these web-based pipelines are susceptible to noise from low-quality sources and depend heavily on extensive post-processing for filtering and validation, which diminishes overall efficiency and compromises dataset quality. In contrast, our proposed method synthesizes NL statements within the controlled concept repository, leveraging a teacher-student paradigm and the Lean compiler to iteratively refine the quality of FL statements, which construct a significantly larger and higher-quality dataset of parallel statements, addressing the scalability and reliability issues of prior work.