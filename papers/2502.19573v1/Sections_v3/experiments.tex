\section{Experiments} \label{sec:experiments}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/V8_unseg_vs_seg.pdf}
    \caption{Accuracy of various models on our benchmark is depicted with \textit{solid} lines, where each line represents a different model suite (e.g., OPT) ranging from the smallest to the largest variant, fine-tuned on datasets of varying sizes as indicated by the line colors. For comparison, models trained under a simpler setup are shown with \textit{dashed} lines, where all information necessary to answer a question is contained within a single training document, eliminating the need to recall information from multiple documents.}
    \label{fig:unseg_vs_seg}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/V8_unseg_vs_seg_gap.pdf}
    \caption{
        Gap in accuracy between the standard and simplified setup in \autoref{fig:unseg_vs_seg}, for a same sized model trained on a same sized dataset.The effect of scaling the dataset and model size varies greatly depending on the architecture.
    }
    \label{fig:unseg_vs_seg_gap}
\end{figure*}

\subsection{Setup} \label{sec:setup}
\paragraph{Dataset.} To evaluate the impact of the number of training examples on the model performance, we generate six datasets containing 1K to 64K diarists, with each successive dataset doubling in size compared to its predecessor. By incrementally enlarging the dataset size as described, models see a broader array of examples from which they can learn to derive their generative capabilities, while simultaneously being challenged to memorize a larger volume of documents.

For each individual, we generate 1 to 8 diary entries, with each entry consisting of 1 to 8 attributes. The training, validation and test sets each contain an equal distribution of individuals who have written one, two, three, etc. diary entries. Similarly, we maintain a uniform distribution for document lengths. Dataset details, such as the number of authors, diary entries, and Q/A pairs, are provided in \autoref{app:dataset_details}.

\paragraph{Models.} We benchmark the following suit of publicly available models: decoder-only OPT (7M to 2.7B)~\citep{opt} and Pythia (70M to 2.8B)~\citep{pythia}, and encoder-decoder Flan-T5 (80M to 3B)~\citep{flan-t5}. A comparison of these architectures is provided in \autoref{app:model_dif}. Training hyper-parameters are provided in \autoref{app:training_details}. Unless specified otherwise, reported metrics are based on the test set.

\subsection{Results}

\paragraph{Effect of Architecture \& Scale.} We first evaluate the impact of architecture, model size, and dataset size on performance. We fine-tune each model on our datasets and report their performance as solid lines, labeled as `standard setup' in \autoref{fig:unseg_vs_seg}. The horizontal axis represents model size, the vertical axis indicates the percentage of correctly answered questions, and the line color signifies the dataset size. Each line on the plot corresponds to a specific architecture (e.g., OPT), ranging from the smallest to the largest model, trained on a particular dataset size. Due to the significant computational cost associated with these experiments, we do not exhaustively explore all combinations of dataset and model size. Instead, we focus on a representative subset of combinations, sufficient to analyze and discern the key trends effectively.

For the OPT suite, we observe a general trend where performance improves as both model size and dataset size increase. Beginning with the smallest variant, which consists of 7M parameters, performance initially improves as the dataset expands, peaking at 4K diarists. However, beyond this threshold, further scaling of the dataset leads to a decline in performance. This pattern suggests that while larger datasets enhance generalization, there comes a point where the model's capacity becomes saturated, causing diminishing returns or even a drop in effectiveness.

In contrast, the 125M-parameter OPT model demonstrates a markedly different behavior. This model is sufficiently large that increasing the dataset size up to the maximum tested---64K diarists---results in consistent performance improvements. The difference in performance between this model trained on the smallest versus the largest dataset is particularly striking, highlighting the significant impact of dataset scaling when paired with a model of sufficient capacity.

Furthermore, increasing the model size while keeping the dataset size constant generally leads to performance gains.

The Pythia models exhibit a similar trend to the OPT suite, where performance improves as both model size and dataset size increase. However, an interesting distinction emerges when comparing the two architectures: performance gains appear sooner in OPT models than in Pythia. Specifically, the 125M-parameter OPT model significantly outperforms the 160M-parameter Pythia model when trained on our largest datasets. This discrepancy underscores differences in how quickly the studied capability emerges depending on the underlying model architecture.

Finally, the performance of Flan-T5 models exhibits a distinct pattern compared to the other architectures. On the smallest datasets, increasing model size alone does not lead to any noticeable improvements. Performance gains only begin to emerge at 783M parameters, and even then, only when trained on the two largest datasets.

Due to computational limitations, we were unable to test the largest Flan-T5 model, with 2.8B parameters, on our largest datasets. However, the overall results suggest that this capability does indeed emerge with sufficient scale---though the rate at which it develops varies depending on the model architecture.
%- The performance of the smallest OPT model not saturating as the dataset size scales is especially intriguing.
%- Indeed, while more data can help with generalization, it also makes the task more difficult, as more data needs to be memorized by the model.
%- It is possible that the smallest OPT model in Figure \ref{fig:unseg_vs_seg} might have enough capacity to memorize even our largest dataset.
%- To verify this, we would need to increase the size of the dataset further to see if eventually the performance saturates or decreases.
%- Given that we lack the compute to perform such an experiment, we instead train a much smaller OPT model (7 million parameters) on our datasets

\paragraph{Effect of Distributed Information.} We compare the model performance against a second set of models trained in a simpler setup. Particularly, this second group of models is trained on identical datasets, but with all diary entries authored by the same individual merged into a single training document rather than each entry being its own document. This approach is equivalent to training the models on the answers directly, requiring them to simply memorize and recall single documents. The performance gap between these two setups highlights the added difficulty of dealing with information spread across multiple training documents. This distribution could affect how information is stored in the model's parameters, potentially making it harder for the model to consolidate it when it is dispersed.

In \autoref{fig:unseg_vs_seg}, the results of training within this more straightforward setup are shown as dashed lines, labeled `simplified setup'. In all cases, these models exhibit significantly improved performance compared to the same base model trained within the distributed setup. Interestingly, all Flan-T5 models achieve near-perfect accuracy in this simplified setup whereas OPT and Pythia suites do not, despite performing well and improving with scale.

To better illustrate the performance gap between both setups, we provide a clear visualization in \autoref{fig:unseg_vs_seg_gap}. The vertical axis shows the accuracy gap between the `simplified' and `standard' setup, for models of the same size, trained on datasets containing the same number of individuals. Results are grouped by model size, with colors denoting dataset size.

For the OPT models, the gap narrows as the dataset size increases, with the exception of the smallest model. In the case of Pythia, the gap only seems to narrow for larger models trained on sufficiently large datasets. Lastly, for Flan-T5, the performance gap barely shrinks as both dataset and model size scale, with the exception of the 780M parameter model trained on the largest datasets.

It remains unclear why Flan-T5 models perform so well in the simpler setup but so poorly in the standard setup. Given that the model has near perfect accuracy in the prior, its poor performance in the latter cannot be attributed to an issue in the methodology, as the process is the same in both cases. The only difference is that, in the latter case, the model must recall information from multiple documents rather than a single one. Therefore, the model specifically has an issue with this aspect.

For all models, it is uncertain whether their performance in both setups will continue to improve with scale and if the gap will eventually disappear.

%Notably, all models achieve near 100\% accuracy on the Q/A pairs in the training set, as well as in memorizing and recalling individual diary entries (not shown in any figure). Therefore, the observed performance gap is not due to difficulty in memorizing the training data.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/V8_docs_to_recalls_x_acc_dataset_size_8K.pdf}
    \caption{Impact of the number of documents needing to be recalled on the likelihood of a model's answer containing an error. Results are from models trained on our 8K dataset. Surprisingly, we observe no significant difference in performance as the number of documents to recall increases.}
    \label{fig:num_docs_x_accuracy}
\end{figure*}

%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\textwidth]{figures/V8_docs_to_recall_hist_summary_dataset_size_16K.pdf}
%    \caption{Number of documents recalled by the model, in comparison with the target number of documents.}
%    \label{fig:docs_to_recall_hist_summary}
%\end{figure*}

%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=\columnwidth]{figures/V8_docs_to_recall_hist_summary_4docs_dataset_size_16K.pdf}
%    \caption{Number of documents recalled by each model when the target is four. Results are averaged over model sizes for simplicity, from models trained on our largest dataset. Full results are provided in \autoref{fig:hist_num_docs}.}
%    \label{fig:docs_to_recall_hist_summary}
%\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/V8_docs_to_recall_hist_OPT_dataset_size_8K.pdf}\\[10pt]
    \includegraphics[width=\textwidth]{figures/V8_docs_to_recall_hist_Pythia_dataset_size_8K.pdf}
    \caption{Number of documents recalled by each model in comparison with the target. Color indicates model size. Results are on our 8K dataset. We observe that as the model scales, its ability to recall the correct number of documents emerges.}
    \label{fig:hist_num_docs}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/V8_docs_to_recall_hist_Flan-T5_dataset_size_8K.pdf}\\[10pt]
    \includegraphics[width=\textwidth]{figures/V8_docs_to_recall_hist_Flan-T5_dataset_size_32K.pdf}
    \caption{Number of documents recalled by Flan-T5 models compared to the target. Color indicates model size. Results are shown for both the 8K and 32K datasets. On the 8K dataset, recall appears inconsistent regardless of model size, whereas on the 32K dataset, recall improves with scaling. This highlights the importance of increasing both model and dataset size.}
    \label{fig:hist_num_docs_flan}
\end{figure*}

\paragraph{Effect of Number of Documents.} Next, we explore how the number of documents to be consolidated and recalled impacts model performance. In \autoref{fig:num_docs_x_accuracy}, we report accuracy grouped by the number of documents in the target answer (horizontal axis). Line color indicates model size. To maintain clarity, we display only the performance of models trained on the 8K diarist dataset, as the observed trends are consistent across other datasets. Notably, there are no results on the simpler setup in this and further analyses.

Surprisingly, models do not demonstrate a decline in performance when more diary entries need to be recalled. Given the increased content to be generated, one might expect a higher propensity for errors in the model answers. However, this observation could be attributed to the model's capacity being sufficient, and performance deterioration might only appear when recalling a much greater number of documents.

To gain deeper insights into model behavior, we analyze the number of documents recalled by the models in comparison with the target number of documents (\autoref{fig:hist_num_docs} \& \ref{fig:hist_num_docs_flan}). %\autoref{fig:docs_to_recall_hist_summary} shows this distribution when the target number of documents is four. Performance is averaged over model size for simplicity. Full results are available in \autoref{fig:hist_num_docs}. 
%We find that, regardless of the target number of documents to recall, Flan-T5 models tend to recall a random number of documents. This contrasts with OPT models, which recall the expected number of entries across all scales.

For both OPT and Pythia models trained on a dataset of 8K diarists, smaller models appear to recall a random number of documents. However, as model size increases, the ability to accurately determine the appropriate number of documents to recall emerges.

In contrast, Flan-T5 models trained on the same 8K-diarist dataset consistently retrieve a seemingly random number of documents, regardless of model scale. Interestingly, when scaling up to a dataset of 32K diarists, Flan-T5 exhibits a pattern similar to that of OPT and Pythia---where the capability to recognize how many documents should be recalled emerges as model size increases.

%These findings help explain the differences observed in \autoref{fig:unseg_vs_seg}. Although OPT consistently recalls the correct number of diary entries, its less-than-perfect accuracy suggests errors in the factual information being recalled. Notably, models that recall too many entries hallucinate those documents.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/V8_sen_to_recalls_x_doc_acc_dataset_size_8K.pdf}
    \caption{Taking the subset of documents in a model's answer, which are also in the target, we measure the number of such documents that are free of errors, defined as the `document accuracy'. We then categorize this rate by the length of the corresponding target document, in order to measure its effect on the recall capabilities of the model. Results are from models trained on the 8K dataset. Peculiarly, we find that documents recalled by models aren't more likely to contain errors as their length increases.}
    \label{fig:doc_len_x_accuracy}
\end{figure*}

%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{figures/V8_sen_to_recall_hist_summary_dataset_size_16K.pdf}
%    \caption{Number of sentences to be recalled in a document compared to the target number of sentences. All models generally are able to properly recall the correct number of sentences regardless of the document length.}
%    \label{fig:sen_to_recall_hist_summary}
%\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/V8_sen_to_recall_hist_OPT_dataset_size_8K.pdf}\\[10pt]
    \includegraphics[width=\textwidth]{figures/V8_sen_to_recall_hist_Pythia_dataset_size_8K.pdf}\\[10pt]
    \includegraphics[width=\textwidth]{figures/V8_sen_to_recall_hist_Flan-T5_dataset_size_8K.pdf}
    \caption{Number of sentences to be recalled in a document compared to the target number of sentences. Color indicates model size. Results are from models trained on our dataset with 8K diarists. Models are able to properly recall the correct number of sentences given sufficient scale.}
    \label{fig:hist_num_sen}
\end{figure*}

\paragraph{Effect of Document Length.} Previously, our method for measuring accuracy involved counting the number of model answers that matched the target answer exactly. We now shift our focus to evaluating the accuracy of individual documents within a model's answer, which we refer to as \textit{document accuracy}.

In this analysis, we only consider the documents recalled by the model that are also present in the target answer, regardless of whether these documents are correct. Our objective is to examine how the length of the target documents influences the model's ability to recall them accurately. Hence, we restrict our analysis to this specific subset of documents, as we need a target for their length.

For these selected documents, we count those that are free of errors and represent this rate on the vertical axis of \autoref{fig:doc_len_x_accuracy}. The performance is categorized by the target length on the vertical axis, and the line color indicates the size of the model. Once again, to maintain clarity, we display only the performance of models trained on the 8K diarist dataset.

Across all models, performance appears unaffected by document length, despite the expectation that longer documents might introduce more errors. One possibility is that the models are sufficiently large to handle even the longest tested documents without performance degradation. However, we hypothesize that much longer documents may eventually impair performance, which would require further testing.

To further understand model behavior, we analyzed the number of recalled sentences, in comparison with the target document length. The histograms in \autoref{fig:hist_num_sen} illustrate these distributions for each model, with color indicating the model size. Similar to our analysis of the number of recalled documents, we observe that model performance improves with scale: smaller models appear to recall a random number of sentences, whereas larger models consistently recall the exact number. Interestingly, however, the smallest Flan-T5 models recall the correct number of sentences, suggesting that they are better at memorizing documents than OPT and Pythia models of the same size.

%Interestingly, Flan-T5 accurately recalls the correct document length but struggles with the number of documents. Therefore, for all models, the challenge lies more in document consolidation than memorization.

%\autoref{fig:num_docs_x_accuracy} presents the relationship between the quantity of documents to be recalled (represented on the horizontal axis), and the model's accuracy (depicted on the vertical axis). The color coding of each line corresponds to the model size. Surprisingly, the results reveal a general resilience across all models to the number of documents required for recall, with only Pythia models appearing to benefit from an increase in model scale. Expanding on this, we analyze whether models recall the exact number of expected documents. In \autoref{fig:docs_to_recall_hist_summary}, we have three histograms, each representing a specific target document count, illustrating the distribution of documents actually recalled by the models. For each model suit, the performance is averaged across model size. Full performance is shown in \autoref{fig:hist_num_docs}. Interestingly, OPT models seem to recall the correct number of documents while Flan-T5 models recall a random amount. Pythia models occupy a middle ground, with smaller models struggling to recall the correct number of documents, while larger ones learn to do so appropriately. These trends corroborate the performance patterns identified in \autoref{fig:unseg_vs_seg} and \ref{fig:num_docs_x_accuracy}, offering further insight into each model's document recall capabilities.

%Subsequently, our attention shifts to the role of document length in influencing model performance (\autoref{fig:doc_len_x_accuracy}). For this purpose, document length is quantified by the number of sentences (or attributes) within a diary entry. Mirroring observations related to the number of documents, the impact of sentence quantity on model performance seems negligible across the board, with only Pythia models observing benefits from enhanced model size. In our continued analysis, we examine the ability to accurately recall the correct number of sentences, as opposed to recalling too few or inaccurately generating additional ones. \autoref{fig:sen_to_recall_hist_summary} depicts the number of sentences recalled by each model versus the number of sentences present in the target document. Again, performance is averaged across model size, and full results are shown in \autoref{fig:hist_num_sen}. Here, all models appear capable of recalling the appropriate number of sentences. Considering the subpar performance of Pythia and Flan-T5 observed in \autoref{fig:num_docs_x_accuracy}, it becomes evident that the issue lies not in the length but in the accuracy of the recalled sentences. Specifically, we find that models always recall the appropriate type of attribute (e.g., ``Weather''), but struggle to recall the correct value (e.g., ``Sunny'').

%Altogether, we find the lack of degradation when scaling the amount of documents to be recalled or their length intriguing. Given the increased amount of content to be recalled, we expected the probability of an error appearing in the models' answers to increase accordingly. This observation, however, may be influenced by the `toy' nature of our experimental setup, where the model knows it must recall between one and eight documents/sentences. Real-world scenarios might yield different results. Additionally, it remains plausible that a noticeable decline in performance could emerge with a more substantial increase in both the number of documents to recall and their length. Further research on this matter is warranted.

%\subsubsection{Main Takeaways}
%All things considered, the ability of LLMs to consolidate and recall training documents during inference seems to largely depend on the model's architecture, size, pre-training conditions, and the volume of the dataset used for fine-tuning. For certain architectures, merely scaling the dataset proves enough for this capability to emerge, while others seem more reliant on the joint scaling of model and dataset size. Some models, also appear far from capable of learning this task, regardless of efforts to scale up both the dataset and the model.

%Regarding the number of documents that a model must recall or the length of these documents, these factors do not seem to play a critical role in affecting model performance. Indeed, the key factor appears to be the model's proficiency in accurately recalling the correct number of documents, rather than the quantity involved.

%In the subsequent section, we delve into the implications of these findings, exploring their practical significance in the broader context of LLMs, and chart a course for future inquiries.

\paragraph{Investigating Performance Discrepancies.} Our results indicate that the ability to consolidate and accurately recall the correct number of documents varies depending on the model suite, but the underlying reasons for this discrepancy remain unclear. At a high level, these differences in performance could be due to several factors: architectural variations, the effectiveness of pre-trained weights for fine-tuning on this task, the fine-tuning hyperparameters, or a combination of these elements.

To investigate this further, we fine-tuned an OPT-125M, a Pythia-70M, and a Flan-T5 Small model, all with randomly initialized weights, using our dataset with 32K diarists. We then compare their performance against the pre-trained models that were fine-tuned on the dataset of the same size.

Our findings reveal that the Pythia model initialized with random weights significantly outperform the pre-trained weights (\autoref{tab:scratch_vs_pretrained}). This suggests that architectural differences are not responsible for the poor performance of this model. Instead, the issue lies in the capability of the pre-trained weights to be effectively fine-tuned for this specific task.

\begin{table}[t!]
  \centering
  \begin{tabular}{lcc}
    \toprule
    Model & Pre-trained & Scratch \\
    \midrule
    OPT-125M & \textbf{91.94\%}  & 82.19\% \\
    Pythia-70M & 21.56\% & \textbf{45.50\%} \\
    %Pythia-160M &  & 73.25\%  \\
    Flan-T5 Small & \textbf{13.12\%} & 12.06\% \\
    \bottomrule
  \end{tabular}
  \caption{Comparison between fine-tuning pre-trained models versus training models initialized with random weights on the 32K dataset.}
  \label{tab:scratch_vs_pretrained}
\end{table}

Regarding Flan-T5, fine-tuning with randomly initialized weights does not appear to enhance performance when compared to fine-tuning the pre-trained model. This observation suggests that the model's architecture is responsible for the observed differences in performance.

Although fine-tuning hyperparameters could also be a factor, we conducted a thorough search. Additionally, models in the simpler setup performed well and were trained with identical hyperparameters. Conversely, in the standard setup, while models were able to memorize the training samples and Q/A examples, the solutions learned by the pre-trained Pythia-70M and Flan-T5 Small does not generalize well to the validation and test Q/A, unlike the OPT model.

%Reflecting on these insights and the results shown in \autoref{fig:unseg_vs_seg}, it appears that larger Pythia models can learn solutions that generalize well. This contrasts with smaller Pythia models, whose pretrained weights seem to memorize the training data rather than generalize from it. For OPT, all models, including the smallest, were able to learn solutions that generalize well. Lastly, the Flan-T5 models seem to face difficulties in consolidating information from multiple documents. This struggle contrasts with their near perfect ability to recall information from a single document, which could be attributed either to architectural limitations or improper hyperparameter settings.

\subsection{Behavior Analysis}
Our analyses thus far have focused on prompting the model with a question and allowing it to generate an answer. Now, we examine how the models respond when prompted with a question followed by part of the answer. We experiment with the following combinations:
\begin{enumerate}[label=\Alph*.]
    \item The second document alone, skipping the first.
    \item The first document followed by the third, intentionally omitting the second.
    \item The first half of the first document only.
    \item The first half of the first document followed by the second document.
    \item The last document followed by the first.
\end{enumerate}
These scenarios were tested using OPT and Pythia models trained on our largest dataset. We find that in all cases except the last, the models continued the answer seamlessly. Specifically:
\begin{enumerate}[label=\Alph*.]
    \item They follow the second document with the third, then the fourth, etc.
    \item They follow the third document with the fourth, then the fifth, and so on.
    \item They follow the first half of the first document with the second half, then proceed to the second document, the third, and so forth.
    \item They follow the second document with the third, then the fourth, and so on.
    \item They follow the first document with the second, third, etc., but eventually skip some documents, including the last one.
\end{enumerate}
These results indicate that the position of tokens in the sequence is not a significant factor. Instead, the models demonstrate a robust ability to continue the sequence as long as the tokens are in a logical order.

\subsection{Comprehensive Analysis}
Reflecting on our experimental observations, we can gain insights into the capabilities and failures of these models. We've observed that, given sufficient scale, the documents recalled by the models are typically of the correct length (\autoref{fig:hist_num_sen}) and error-free (\autoref{fig:doc_len_x_accuracy}). Additionally, models trained under the simplified setup successfully recall information from a single training document (\autoref{fig:unseg_vs_seg}). Therefore, the issue appears not to lie in the content of the recalled documents but rather in the quantity of documents being recalled. Indeed, with improper scale, models seem incapable of recalling the correct number of documents, instead recalling a random number of documents (\autoref{fig:hist_num_docs}).

Interestingly, the smallest Pythia model performs better if fine-tuned starting from random weights rather than the pre-trained weights (\autoref{tab:scratch_vs_pretrained}), suggesting that the poor performance of the pre-trained weights cannot be completely attributed to an architectural reason. Instead, the issue partly appears to be with the pre-training weights failing to learn a solution that generalizes to the problem of recalling the correct number of documents, rather than merely memorizing the training samples. Why this discrepancy occurs, particularly in contrast to the larger pre-trained Pythia models remains unclear and warrants further research. Different hyperparameters could potentially enable the smaller models to generalize well to our problem, but it is uncertain if this can be achieved without severely degrading the language modeling capabilities of the pre-trained model.

Regarding Flan-T5, given that the smallest model fine-tuned from scratch performs as poorly as the one fine-tuned from pre-trained weights, the root cause of the poor performance could be either architectural or due to improper hyperparameters. Additionally, the size of the model appears to influence its performance. Since Flan-T5 follows an encoder-decoder architecture, unlike the decoder-only structures of models such as OPT and Pythia, its parameters are divided roughly equally between the encoder and decoder. Consequently, the second largest Flan-T5 model's decoder is comparable in size to that of the third smallest Pythia model, which coincides with the point where performance begins to improve for Pythia (as seen in \autoref{fig:unseg_vs_seg}). Models within the Pythia suite smaller than this threshold do not show significant performance gains. However, the smallest Pythia model, when trained from scratch, outperforms Flan-T5 under similar conditions. This highlights that architectural factors can hinder the emergence of capabilities for same sized models. As for scale, our hypothesis is that the smaller models lack the capacity to develop the necessary circuitry to perform this task, but further research will be necessary to pinpoint the exact cause and clarify the challenges faced by these smaller models.