\section{Discussion} \label{sec:discussion}
%Our primary inquiry centered on determining whether LLMs possess self-awareness regarding the extent of their own knowledge. This question holds significant importance since trust in an LLM's output and its confidence is essential for these models to be truly effective. For an LLM to be reliable, it must have an understanding of its knowledge limits. Furthermore, the ability to recall all pertinent information about a specific topic is valuable for addressing many complex problems.

%Our findings indicate that not all models exhibit this capability. The underlying reason why some models are able to learn solutions that generalize well to this problem, while others merely memorize the training data, remains unclear. This uncertainty implies that, at present, there is no definitive method to ascertain an LLM's ability to perform such tasks without explicit testing.

%Consequently, more research is required to deepen our understanding of this issue. 

In addition to the questions raised in the previous section, the following additional questions should be considered.

\subsection{Language Modeling}
One aspect not addressed in this study is whether models can perform the given task while retaining their language modeling capabilities. Due to the size of the models examined, repetitive fine-tuning on the training documents is necessary for them to memorize the data, which leads to overfitting on the task. Ideally, experiments would need to be conducted on much larger models, incorporating the training documents into the pre-training corpus, followed by standard instruction tuning. One of the tasks in this tuning would involve recalling all documents related to a given topic. This approach would help determine if a model can accomplish this in a manner that is useful for solving problems. Unfortunately, we currently lack the computational resources to conduct such experiments, and hence we leave this for future work.

\subsection{Information Distribution}
We observed a notable performance gap between the standard and simplified setups, supporting the findings by  ~\citet{prato-etal-2023-epik}. Their research indicates that LLMs more easily recall multiple pieces of information when this information is contained in a single training sample rather than dispersed across multiple samples. This raises questions about how the distribution of topic-related information across multiple training documents affects an LLM's ability to gauge its knowledge. Particularly, the impact of this distribution on the internal mechanisms of the LLM is not well understood.

Numerous studies have shown that language models can memorize entire passages and documents within their weights, enabling them to recall this information during inference~\cite{2020arXiv201207805C,2022arXiv220207646C,NEURIPS2022_fa0509f4,NEURIPS2023_59404fb8,DEWYNTER2023100024,2024arXiv240511577C}. Consequently, the strong performance of models in the simpler setup, where they only need to recall information from a single document per topic, is not surprising.

However, it remains unclear why recalling information from multiple documents presents a greater challenge. Specifically, how is this information encoded within the model parameters~\cite{singh-etal-2020-bertnesia,dai-etal-2022-knowledge,2022arXiv220205262M} and how does dispersed information affect the recalling process? Understanding these mechanisms is crucial for improving the performance of language models, as many real-world problems necessitate recalling information from multiple training documents.

\subsection{Knowledge Awareness \& Understanding}
While we have demonstrated that some LLMs possess an awareness of the extent of their knowledge concerning the topics in our benchmark, this does not necessarily mean that these models can gauge their knowledge across any topic.

Determining whether LLMs can accurately assess the scope of their understanding of topics from their pre-training corpus requires further investigation. Topics in practice could cover a wide array of subjects, including individuals, locations, events, and concepts. However, we believe that the specific type of topic is likely not an influential factor.

The critical element, in our view, is the breadth of these topics, which relates to the amount of information relevant for each. Our findings did not show a decline in model performance when recalling up to eight documents. Nevertheless, this observation might change if the number of documents were significantly increased. Further research is necessary to explore the limits and capabilities of models in handling broader topics.

A more profound question is the extent to which LLMs understand the scope of their entire knowledge base, or at least subsets of it. Given the vast amount of information LLMs learn during training, comprehending the scope of this knowledge or its subsets seems incredibly challenging. Yet, it would be beneficial for a model to understand the extent of its own expertise.

Finally, it is important to note that understanding the scope of oneâ€™s knowledge concerning a topic does not imply an understanding of that topic itself. Whether LLMs truly comprehend the knowledge they have memorized is a different research question from ours and is an active area of investigation~\cite{Bender2021OnTD,li2022emergent,2023arXiv231002207G}.