\section{Methodology} \label{sec:methodology}
The foundation of our analysis hinges on the ability of models to memorize and recall information. To avoid the influence of existing data, which might be part of the pre-training corpus of the language models we are benchmarking, we generate our own. This ensures that the models have never encountered the data during pre-training, thereby preventing any contamination of our results.

In essence, our approach involves: (i) {\bf generating} the training documents, (ii) {\bf fine-tuning} a language model using its pre-training objective to memorize these documents, and (iii) {\bf testing} the language model's ability to recall all related documents. We delineate each stage of our framework in the following sections.

\subsection{Data Generation}
Given $N$ diarists, where $N$ is a hyperparameter, we generate a random number of diary entries for each diarist, following the template:
\begin{quote}
    \texttt{\hspace{-2pt}\{name\}'s Diary Entry \{i\}}\\
    \texttt{\{attribute\}}\\
    \texttt{\vdots} \\
    \texttt{\{attribute\}}
\end{quote}
where \texttt{\{name\}} is the diarist (e.g., ``Tom'') and \texttt{\{i\}} is the entry number (e.g., ``1''). The document contains a random number of \texttt{\{attribute\}}, each selected randomly without replacement from a set (\autoref{tab:attributes}), along with a randomly chosen value (e.g., ``Time: Morning''). Additionally, for each individual, we have one question following the format:
\begin{quote}
\texttt{Recall all of \{name\}'s diary entries, in order.}
\end{quote}
The answer to the question is the concatenation of the individual's diary entries in ascending order by entry numbers. \autoref{fig:setup} illustrates examples of both generated documents and questions.

To effectively train the model, we incorporate 90\% of the question-answer (Q/A) pairs, along with \textit{all} diary entries, into the training set. The remaining 10\% of the questions are evenly divided into a validation set and a test set. By adding Q/A examples to the training set, the model can learn the evaluation task, similar to the process of instruction-tuning.

Initially, we trained the model first on the documents and then on the evaluation task. However, this approach led to catastrophic forgetting of the documents and overfitting on the Q/A examples. Therefore, we decided to fine-tune the model on both simultaneously to prevent these issues.

\subsection{Fine-Tuning \& Evaluation}
To benchmark an LLM, we begin by fine-tuning it using its pre-training objective, such as causal language modeling, on our training set. This fine-tuning process mirrors the standard training of an LLM on a text corpus. Depending on the architecture of the LLM, we format the input as follows:
\begin{itemize}
    \item \textbf{Decoder-Only Models (e.g., OPT):} For both diary entries and Q/A pairs, the training objective is causal language modeling. In the case of Q/A pairs, we concatenate the question with the answer into a single text sequence, separated by an end-of-line token (`\textbackslash n').
    \item \textbf{Encoder-Decoder Models (e.g., Flan-T5):} When processing a diary entry, the first line (e.g., ``Tom's Diary Entry 1'') is input to the encoder, and the decoder generates the \textit{entire} document. For Q/A pairs, the question is fed to the encoder, and the decoder predicts the answer.
\end{itemize}
Throughout the fine-tuning process, we periodically evaluate the model on the validation set. For decoder-only architectures, the model is prompted with a question with the goal of generating the corresponding answer. For encoder-decoder architectures, the question is given to the encoder and the decoder must produce the answer.

We fine-tune up until the validation performance plateaus. We then select the best checkpoint based on peak validation performance, and evaluate the model on our test set using the same procedure as with the validation set. Performance is measured in terms of accuracy, defined as the number of correctly answered questions. An answer is deemed correct if it matches the ground truth exactly, with no errors in the number of documents recalled and the content of each recalled document.

\subsection{Design Motivation}
Requiring the model to consolidate information from multiple training documents allows us to assess whether it understands the extent of its knowledge related to the individual in question. Specifically, during training, the model memorizes the diary entries. Then, in the evaluation phase, it needs to know how many documents to recall, meaning the model must know how many diary entries it knows about the individual. If a model consistently recalls the exact number of documents, it demonstrates an understanding of the scope of its knowledge regarding that individual. Conversely, a model which does not know how many documents it knows, would recall a random number.

As for our choice of using synthetic data, it allows us to precisely control its distribution and properties. This extends to the length and content of the documents, as well as the number of diary entries authored by an individual. By using attributes as the body of the documents, we can manage the entropy, ensuring that each sentence contains a fixed amount of information. Consequently, adding an additional sentence consistently increases the document's information by that fixed amount.

This approach enables us to examine how document length affects the model in a more controlled manner compared to using real data. While we have arbitrarily chosen individuals as the topic linking multiple documents, this could have been any other concept. We believe this choice does not impact the observed trends in the results.

Overall, our benchmark is designed to facilitate the study of this problem and its key variables in a controlled environment, emulating the challenge faced by language models of memorizing information during training and understanding the extent of their knowledge concerning specific topics.