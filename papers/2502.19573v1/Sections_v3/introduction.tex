\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/fig1.drawio.pdf}
    \caption{We train LLMs using diary entries from various individuals, with each diarist contributing a random number of entries. We then task the models with recalling all entries written by a specific individual, evaluating their ability to accurately recall the exact number of documents authored by that person.}
    \label{fig:setup}
\end{figure*}

\section{Introduction} \label{sec:intro}
Large Language Models are renowned for their ability to memorize vast amounts of information encountered during training~\cite{GPT4,llama2,gemini}. This information, stored in their parameters, can be recalled during inference, serving both for information retrieval and problem-solving~\cite{2015arXiv150605869V,Radford2019LanguageMA,2022arXiv221011416C,2023arXiv230414767G}. While it is well-established that LLMs can act as knowledge bases~\cite{2019arXiv190901066P,heinzerling-inui-2021-language,2022arXiv220406031A}, the extent to which they understand their own knowledge is less clear~\cite{2024arXiv240115449L}. For instance, do these models know if or when they know the answer to a question~\cite{2022arXiv220705221K,2023arXiv230518153Y}? Can they quantify their own expertise on a topic? Are they aware if some of their knowledge contradicts other information they possess? Can they differentiate between explicitly learned information and implicit knowledge?

These questions are crucial, as awareness of one’s own knowledge and limitations is a vital aspect of any intelligent system. Without it, an AI could be prone to hallucinate~\cite{2023arXiv230906794Y,2024arXiv240111817X}, lie about its expertise~\cite{2023arXiv230413734A,Pacchiardi2023How}, overestimate its responses~\cite{2020arXiv200307892D,GPT4}, or contradict itself~\cite{2023arXiv230708678C}, all of which are undesirable traits for AI systems intended to be useful.

This study focuses on understanding whether LLMs know the extent of their knowledge on specific topics, such as individuals, locations, events or concepts. To explore this, we task LLMs with enumerating everything they know about a given topic—no more, no less. Should a model consistently recall just the right amount of information, it suggests an understanding of the scope of its own knowledge on that topic. Conversely, if a model does not know how much it knows, it may recall too little or hallucinate additional information.

Our approach involves fine-tuning LLMs on the diary entries of various fictitious individuals. Each entry is treated as an individual document in our fine-tuning dataset, with each diarist authoring a random number of entries. During inference, we ask the models to recall all diary entries of a specified individual in chronological order. We then evaluate whether the recalled entries match the original entries both in terms of content and quantity. \autoref{fig:setup} provides an illustrative example.

We benchmark the performance of the OPT~\cite{opt}, Pythia~\cite{pythia}, and Flan-T5~\cite{flan-t5} suites of models. Our key findings are as follows:
\begin{itemize}
    \item All tested LLMs, if scaled sufficiently, demonstrate an understanding of how much they know. This capability appears to emerge at different rates depending on the architecture. For example, an OPT model of a particular size can perform this task effectively if the fine-tuning dataset is sufficiently large, whereas Pythia and Flan-T5 models of the same size require further scaling.
    \item When these conditions are not met (i.e., insufficient scaling), models often recall a random number of diary entries, either recalling too few or hallucinating additional ones.
    \item Interestingly, the number and length of documents do not impact model performance, demonstrating that models are equally effective at memorizing short and long documents, as well as recalling topics associated with a single document or multiple documents.
\end{itemize}
Finally, we discuss potential factors responsible for the observed differences in the emergence of this capability. Overall, our work contributes to a deeper understanding of the inner workings of LLMs, shedding light on a not-so-well-understood aspect of these models.

The insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics.