\section{Dataset Details} \label{app:dataset_details}

\subsection{Size}
    \begin{table*}[ht]
    \centering
    \begin{tabular}{rrrrr}
        \toprule
        Diarists & Total Documents & Train Q/A & Val Q/A & Test Q/A \\
        \midrule
        1K & 4,482 & 894 & 50 & 50 \\
        2K & 9,012 & 1,804 & 100 & 100 \\
        4K & 17,895 & 3,577 & 199 & 199 \\
        8K & 36,000 & 7,200 & 400 & 400 \\
        16K & 72,000 & 14,400 & 800 & 800 \\
        32K & 144,000 & 28,800 & 1,600 & 1,600 \\
        64K & 288,000 & 57,600 & 3,200 & 3,200 \\
        \bottomrule
    \end{tabular}
    \caption{Specifications for each dataset used in our experiments.} \label{tab:dataset_details}
\end{table*}

The details of each dataset used in our experiments are provided in \autoref{tab:dataset_details}. The training set of each dataset consists of all of the documents (diary entries), as well as the train Q/A pairs. The validation and test sets solely consist of Q/A pairs. There are no overlaps between the Q/A pairs in the training, validation and test sets. As previously mentioned, for each author, we generate 1 to 8 diary entries, where each entry contains 1 to 8 sentences (excluding the title).

\subsection{Attributes}
The list of attributes sampled for each diary entry along with possible values are presented in \autoref{tab:attributes}. Sampling of both the attribute and its value is always performed randomly. A document cannot contain the same attribute more than once, irrespective of its value.
\begin{table*}[ht!]
    \centering
    \begin{tabular}{ll}
        \toprule
        Attribute & Possible Values \\
        \midrule
        \texttt{Location} & \texttt{[City, Countryside]} \\
        \texttt{Time} & \texttt{[Morning, Evening]} \\
        \texttt{Weather} & \texttt{[Sunny, Rain]} \\
        \texttt{Mood} & \texttt{[Happy, Sad]} \\
        \texttt{Restfulness} & \texttt{[Tired, Rested]} \\
        \texttt{Stress Level} & \texttt{[Stressed, Relaxed]} \\
        \texttt{Physical Activity} & \texttt{[Running, Weight Training]} \\
        \texttt{Meditated} & \texttt{[Yes, No]}\\
        \bottomrule
    \end{tabular}
    \caption{List of attributes used in diary entries, along with their possible values.}
    \label{tab:attributes}
\end{table*}

% \begin{enumerate}[noitemsep]
%     \item Location: [City, Countryside]
%     \item Time: [Morning, Evening]
%     \item Weather: [Sunny, Rain]
%     \item Mood: [Happy, Sad]
%     \item Restfulness: [Tired, Rested]
%     \item Stress Level: [Stressed, Relaxed]
%     \item Physical Activity: [Running, Weight Training]
%     \item Meditated: [Yes, No]
% \end{enumerate}

\section{Model Suite Differences} \label{app:model_dif}
The following outlines the architectural differences between the OPT, Pythia, and Flan-T5 suite of models, emphasizing their unique characteristics and training details.

Both OPT and Pythia are based on the GPT-3 architecture, with only slight variations. OPT employs learned positional embeddings and utilizes the ReLU activation function~\cite{relu}. In contrast, Pythia incorporates rotary positional embeddings and the GELU activation function~\cite{gelu}. A notable distinction in Pythia's architecture is its use of parallel residual connections, where the self-attention and feed-forward blocks run concurrently, and their outputs are summed along with the residual. This differs from OPT's sequential arrangement, where the self-attention block is followed by the feed-forward block. Additionally, Pythia forgoes the application of dropout after the attention and feed-forward blocks, unlike OPT, which applies a dropout rate of 0.1.

Turning to Flan-T5, this model remains largely faithful to the original Transformer architecture~\cite{2017arXiv170603762V}, with a few key exceptions. Layer normalization~\cite{2016arXiv160706450L} in Flan-T5 is applied before the residual, self-attention, and feed-forward blocks. In contrast, OPT and Pythia place the residual connections before the layer normalization. Flan-T5 also does not include a bias term in the layer normalization and adopts relative positional embeddings.

Regarding pre-training, OPT is trained on The Pile~\cite{ThePile} along with other datasets, whereas Pythia is exclusively trained on The Pile. Flan-T5 is a fine-tuned version of T5~\cite{T5}, with both models being trained on a mix of datasets. For more detailed information on the pre-training specifics and hyperparameters, readers are encouraged to refer to the respective papers for each model~\cite{opt,pythia,flan-t5}.

\section{Training Details} \label{app:training_details}
\begin{table*}[ht]
    \centering
    \begin{tabular}{lrr}
        \toprule
        Model & Parameters & LR \\
        \midrule
        OPT 7M & 7,490,560 & 4e-4 \\
        OPT 125M & 125,239,296 & 6e-5 \\
        OPT 350M & 331,196,416 & 3e-5 \\
        OPT 1.3B & 1,315,758,080 & 2e-5 \\
        OPT 2.7B & 2,651,596,800 & 1.6e-5 \\
        Flan-T5 Small & 76,961,152 & 1e-4 \\
        Flan-T5 Base & 247,577,856 & 1e-4 \\
        Flan-T5 Large & 783,150,080 & 1e-4 \\
        Flan-T5 XL & 2,849,757,184 & 1e-4 \\
        Pythia 70M & 70,426,624 & 1e-4 \\
        Pythia 160M & 162,322,944 & 6e-5 \\
        Pythia 410M & 405,334,016 & 3e-5 \\
        Pythia 1B & 1,011,781,632 & 3e-5 \\
        Pythia 1.4B & 1,414,647,808 & 2e-5 \\
        Pythia 2.8B & 2,775,208,960 & 1.6e-5 \\
        \bottomrule
    \end{tabular}
    \caption{Model size and learning rate used to fine-tune each model in our experiments.} \label{tab:model_hyperparameters}
\end{table*}

We train our models until they converge by employing the Adam optimizer~\cite{Adam}, which is configured with beta values of 0.9 and 0.999, and an epsilon of 1e-8. No weight decay is applied in this process. The learning rate is initially set to zero and then linearly increased to reach the model-specific rate detailed in \autoref{tab:model_hyperparameters} over the course of 3,600 steps. After this warm-up period, the learning rate is maintained constant. We set the batch size to 32.

%\section{Effect of Number of Documents (Full Results)} \label{app:effect_num_docs}
%\autoref{fig:hist_num_docs} shows the number of documents recalled by each model compared to the target number of documents to be recalled. Results are from models trained on our largest dataset. Color indicates model size.

%\section{Effect of Document Length (Full Results)} \label{app:effect_doc_len}
%\autoref{fig:hist_num_sen} shows the number of sentences recalled by each model compared to the target document length. Results are from models trained on our largest dataset. Color indicates model size.