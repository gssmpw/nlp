%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{float}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis from Japanese Haiku}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Chunan Yu $^1$
\and
Yidong Han $^1$\and
Chaotao Ding $^{5}$\and
Ying Zang $^{1,*}$\and
Lanyun Zhu $^{6}$\\
Xinhao Chen $^{7}$\and
Zejian Li $^{3}$\and
Renjun Xu $^{4,*}$\And
Tianrun Chen $^{2,5,*}$\\
\affiliations
$^1$ School of Information Engineering, Huzhou University \\
$^2$ College of Computer Science and Technology, Zhejiang University \\
$^3$ College of Software Engineering, Zhejiang University \\
$^4$ Center for Data Science, Zhejiang University \\
$^5$ KOKONI 3D, Moxin (Huzhou) Technology Co., LTD. \\
$^6$ Singapore University of Technology and Design \\
$^7$ School of Humanities, Wenzhou University \\
\emails
02750@zjhu.edu.cn\and
rux@zju.edu.cn\and
tianrun.chen@kokoni3d.com
}
% \fi

\begin{document}
\maketitle

\section{More Details about the Evaluation Metric}
    To simulate immersive trajectories in 3D scenes, we render images by rotating and translating the camera. For image quality evaluation, we employ multiple no-reference standards, such as NIQE \cite{mittal2012making} and BRISQUE \cite{mittal2012no}, which are commonly used to assess the quality of field images. Among the state-of-the-art methods, QAlign \cite{wu2023q} stands out, utilizing a large multimodal model fine-tuned on extensive image quality datasets. We adopt the "quality" mode of QAlign, with a primary focus on the perceptual aspects of the image content. VQAScore \cite{lin2025evaluating}, a method based on a visual question answering (VQA) model, is employed to evaluate text-to-image generation quality and assess the alignment between images and their corresponding text.
\subsection{NIQE}
    NIQE (Natural Image Quality Evaluator) is a no-reference image quality assessment metric based on natural scene statistical features. It employs a Gaussian Mixture Model (GMM) to model the probability distribution of natural image features and uses this distribution to evaluate the quality score of the input image. A lower score indicates higher image quality. The calculation process primarily involves image preprocessing, feature extraction, Gaussian Mixture Modeling (GMM), and the computation of the quality score for the input image. The mathematical expression for NIQE is:
\begin{align}
    \begin{split}
    sqrt{(V_1 - V_2)^T (\frac{\sum_1 + \sum_2}{2})^{-1} (V_1 - V_2)}
    \end{split}
\end{align}
where \( V_1 \), \( V_2 \), and \( \Sigma_1 \), \( \Sigma_2 \) represent the mean vectors and covariance matrices of the multivariate Gaussian models for natural and distorted images, respectively. NIQE is primarily used to assess the naturalness of an image, with smaller values indicating more natural images. Additionally, since NIQE does not require a training process, it offers advantages in terms of high computational efficiency and simplicity in implementation.

\subsection{BRISQUE}

    BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator) is a no-reference image quality assessment metric based on natural scene statistical features. It employs Support Vector Machines (SVM) to learn the mapping relationship between image quality and image features, and then predicts the quality score of the image. A lower score indicates higher image quality. The calculation process primarily involves image preprocessing, feature extraction, feature normalization, and image quality prediction using SVM. BRISQUE offers advantages in computational efficiency and strong generalization ability. The mathematical expression for BRISQUE is:
    
    To compute the MSCN coefficients, the image enhancement \( I(i, j) \) at pixel location \( (i, j) \) is transformed into its luminance \( I^{(i, j)} \) as follows:
    
    \[
    I^{(i, j)} = \frac{I(i, j) - u(i, j)}{\sigma(i, j)} + C
    \]
    where \( i \in \{1, 2, \dots, M\} \) and \( j \in \{1, 2, \dots, N\} \), with \( M \) and \( N \) representing the height and width of the image, respectively. \( u(i, j) \) and \( \sigma(i, j) \) are the local mean and local variance, respectively. The local mean \( u \) is obtained by applying Gaussian blurring to the original image, while the local variance \( \sigma \) is the square root of the Gaussian blur of the squared difference between the original image and the local mean. The formulas for \( u \) and \( \sigma \) are as follows:
    
    \[
    u = W * I
    \]
    
    \[
    \sigma = \sqrt{W * (I - u)^2}
    \]
    where \( W \) is the window function for Gaussian blurring.
    
\subsection{QAlign}
    Through training large multimodal models (LMMs) to align with human subjective judgments in visual scoring, it was observed that human evaluators in subjective studies primarily learn and judge discrete levels defined by text. Q-ALIGN mimics this subjective process by training LMMs using text-defined scoring categories rather than raw numerical scores. Q-ALIGN achieves state-of-the-art performance in tasks such as Image Quality Assessment (IQA), Image Aesthetic Assessment (IAA), and Video Quality Assessment (VQA) under the original LMM framework.


\subsection{VQAScore}

    The novelty of VQAScore lies in its approach of converting text prompts into specific questions and using a Visual Question Answering (VQA) model to assess the alignment between the image and the text. This method not only simplifies the evaluation process but also improves the accuracy and reliability of the assessment. VQAScore employs a bidirectional image-question encoder, which allows the image content and the textual question to influence each other, better simulating the way humans understand images and text. VQAScore has outperformed traditional methods in multiple benchmark tests, demonstrating exceptional performance in evaluating complex image-text alignment tasks.
    
    VQAScore transforms a text prompt into a direct question and then uses the VQA model to evaluate the probability of a positive answer to that question. For example, given an image and a piece of text, VQAScore constructs a question by converting the text "The moon is above the cow" into "Does this image show 'The moon is above the cow'? Please answer 'yes' or 'no'." The VQA model then receives this formatted question and computes the probability of generating the answer "yes."

\section{More Experiments}
\subsection{More Ablation Experiment}
    To gain a deeper understanding of the effectiveness of the components in our proposed Hierarchical Literary-Criticism Theory Grounded Parsing (H-LCTGP) method, we performed a comprehensive ablation study. The experimental results are presented in Tab. \ref{Ablation}, where we systematically evaluate the contributions of key elements, including GPT enhancement, LLM enhancement, and the integration of Key Elements. Specifically, we assess how each component impacts the overall performance of the model, helping us to isolate the importance of each feature. The findings highlight the significance of the proposed enhancements and demonstrate the robustness of the HLC-TGP method across different configurations.

\begin{table}[h]
\caption{Ablation Study Results for Hierarchical Literary-Criticism Theory Grounded Parsing (H-LCTGP) Method}
\vspace{-0.5cm}
\label{Ablation}
\resizebox{0.5\textwidth}{!}{% 自动缩放表格以适应页面宽度
\begin{tabular}{ccccc}                                                                                                         \\ \hline
           & \multicolumn{1}{c}{NIQE$\downarrow$} & \multicolumn{1}{c}{BRISQUE$\downarrow$} & \multicolumn{1}{c}{Align$\uparrow$}  & \multicolumn{1}{c}{VQAScore$\uparrow$} \\ \hline
\textbf{Ours}        & \textbf{14.337}                            & \textbf{47.343}                                & \textbf{2.403}                  & \textbf{0.727}           \\ 
w/o Key elements   & 14.998         & 48.633        & 2.359          & 0.672       \\
w/o LLMs + Key elements  & 14.729      & 61.762           & 2.088         & 0.605         \\
\hline
\end{tabular}
}
\end{table}

\subsection{More Visualization results}
    To thoroughly demonstrate the overall effectiveness of our approach, we took a systematic approach by re-collecting a set of classic Haiku poems, which have long been recognized as a challenging and nuanced form of textual input.
    
    We carefully selected a diverse set of classic Haiku poems, ensuring that the collection included a wide range of themes, such as nature, seasons, emotions, and human experiences. These poems, while short in length, are highly contextual and require a nuanced understanding of both the literal and figurative elements to generate accurate visual representations.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{image/more.pdf}
    \caption{\textbf{Additional Experimental Visualizations.} Our provides further visual examples from our experiments, showcasing the effectiveness and versatility of our approach in various scenarios.}
    \vspace{-2mm}
    \label{more}
\end{figure*}

    As shown in Fig. \ref{more}, our enhanced approach were visually far more detailed, coherent. Similarly, for poems that conveyed emotional depth or complex natural imagery, our method was able to capture the essence of these emotions in the scene, enriching the visual experience beyond a mere literal interpretation of the words.
    
\subsection{Human Ratings to the Correctness of LLM}

    We recruited 10 volunteers with a background in traditional poem critism and analysis. The volunteers were asked to analyze and evaluate two generated texts: one using GPT-4’s HaiKu Reasoning and the other using LLM-enhanced prompts. Participants were instructed to score each description on a scale of 1-5 based on how well it represented "the correct description of a scene" (Suitable). In the post-study questionnaire, when asked, ”Do you think LLM yield correct understanding?” all participants agreed that our system generate correct result. One participant mentioned the LLM tend to generate redundant sentences (multiple sentences describe the same meaning). Another participant mentioned that the result generated by LLM have no emotions (only emotional words), which is different to some classic literacy criticism literatures). The average scores were compiled in Tab. \ref{user}. The high score indicate that the LLM can be a successful literacy parser.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{image/LLM_text.pdf}
    \caption{Haiku Enhancement \textbf{integrating traditional literary analysis principles}}
    \vspace{-2mm}
    \label{gpt1}
\end{figure*}

\begin{table}[h]
\centering
\caption{Mean Opinion Scores (1-5) from User Study}
\label{user}
\setlength{\tabcolsep}{2.0mm}
\resizebox{0.2\textwidth}{!}{% 等比例缩放表格，使其宽度适应页面
\begin{tabular}{cc}
\hline
           & \multicolumn{1}{c}{Suitable (Score 1-5)}  \\ \hline
GPT  & 3.910 $\pm$ 0.122                          \\
{LLM}        & \textbf{4.340 $\pm$ 0.111}         \\ \hline
\end{tabular}
}
\end{table} 

Additionally, we provided more examples of the text enhancement process for Haiku using LLMs, as shown in Fig. \ref{gpt1} and Fig. \ref{gpt2}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{supp/gpt1.pdf}
    \caption{Haiku Enhancement \textbf{integrating traditional literary analysis principles}}
    \vspace{-2mm}
    \label{gpt1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{supp/gpt2.pdf}
    \caption{Haiku Enhancement \textbf{integrating traditional literary analysis principles}}
    \vspace{-2mm}
    \label{gpt2}
\end{figure*}

\bibliographystyle{named}
\bibliography{sup}

\end{document}

