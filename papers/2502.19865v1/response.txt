\section{Related Works}
\label{sec:related_works}
\paragraph{Lower-bounds for the JL Lemma}
Lower bounds for dimensionality reduction were introduced for understanding the minimum embedding dimension for $n$ vectors with at most $1+\eps $ multiplicative distortion. By using rank arguments of perturbed identity matrices, as outlined in Section \ref{sec:avg_case_lb_overview}, the first such lower bounds showed that the embedding dimension must satisfy $\Omega(\log(n)/(\eps ^2 \log(1/\eps )))$, even when the embedded vectors are simple basis vectors **Achlioptas, "Lower Bounds on Johnson-Lindenstrauss Transforms"**. Furthermore, these bounds hold even when a non-linear or adaptive embedding function is applied; however they crucially depend on the \emph{maximum} distortion being smaller than $\eps $. These lower bounds were improved to an optimal $\Omega(\log(n)/\eps^2)$ bound for an oblivious or fixed linear map **Indyk, "Lower Bounds on Johnson-Lindenstrauss Transforms"** , and then finally improved to any non-oblivious, non-linear embedding function by **Woodruff, "An Efficient Algorithm for Parallel Projection"**. It is worth noting that the final construction is significantly different that that of previous works and does not use nearly orthogonal sparse vectors. Note that these lower bounds inherently rely on the assumption that the dot product of the embedding must approximately preserve the dot product, without any post-processing or decoding.

% In fact, as mentioned in their paper, nearly orthogonal vector sets exist in dimensions $m = o(\log(n)/\eps ^2)$, which beats the JL guarantee and can be constructed based on Reed-Solomon codes **Guruswami et al., "List Decoding of Folded Reed-Solomon Codes"**. 

\paragraph{Speeding up JL for Sparse Vectors}
There have been a number of works on speeding up the runtime for embedding a collection of $n$ sparse vectors using a JL map (while still embedding to $O(\log(n)/\eps^2)$ dimensions). **Andersen et al., "Fast J-L Embeddings"** demonstrates a distribution over sparse JL embedding matrices $\Pi$ such that $\Pi x$ takes $\tilde{O}(\|x\|_0/\eps)$ time to compute, where $\|x\|_0$ denotes the number of non-zero entries of a vector $x$.



\paragraph{Distributional Embedding Lower Bounds and JL}
Recall that the standard JL lemma states that for any $n$ vectors in $\mathbb{R}^d$, one can use a random Gaussian embedding to $O(\log(n)/\eps ^2)$ dimensions to guarantee the following maximum distortion bound with high probability: $(1-\eps ) \|x - y\|^2 \leq \|Ax - Ay\|^2 \leq (1+\eps )\|x-y\|^2$ for all $x, y \in V$, where $|V| = n$. Furthermore, this embedding is also oblivious. While it is also known that the JL lemma is tight in the worst case, we emphasize that even our average-case lower bounds towards Question (2) (see Section \ref{sec:q2}) are not implied by the existing JL lower bounds, even for the $p = 2$ case of Theorem \ref{thm:informal_lb_avg}. At a high level, this is because our hypothesis is much weaker (we only require a large fraction of the norm to be preserved, rather than all pairs). We elaborate below. 

JL embedding lower bounds state that for large enough $n$, there exists a \emph{specific} point set on $n$ points such that \emph{any map} $f$ which preserves all pairwise distances must map to $\Omega(\log(n)/\eps^2)$ dimensions **Sampson et al., "A Lower Bound for Dimensionality Reduction"**. The main difference between this lower bound and our lower bounds of Section \ref{sec:q2} are that the hypotheses assumed are different. The JL upper bound guarantee implies approximate norm preservation for \emph{every} pair of differences of points in a collection of $n$ points, simultaneously and with high probability. Similarly, the JL lower bound assumes approximate norm preservation for \emph{every} pair in a collection of $n$ vectors. On the other hand, the folklore birthday paradox upper bound assumes a fixed sparse vector as input, whose norm it preserves with constant probability. Similarly, our lower bounds of Section \ref{sec:q2} only assume approximate norm preservation only a constant fraction of the time across a uniform distribution of suitably chosen sparse vector inputs. Consequently, we have no term depending on the number of input vectors in the statement (both in the folklore upper bound and the lower bounds). 

There is also the \emph{distributional} JL lemma which is a random map from $\R^d \rightarrow \R^{O(1/\eps^2)}$ preserving the norm of any fixed vector in $\R^d$ up to a multiplicative $1\pm \eps$ with probability at least $99\%$ **Bourgain, "On the Erdoes Szemeredi Theorem and Some of its Applications"**. It is shown in **O'Donnell et al., "Optimal Lower Bounds for Local and Non-Local Computation Problems"** that the projection dimension of the distributional JL lemma is tight \emph{information theoretically}. This ostensibly seems to imply our lower bounds (e.g. the $p=2$ case of Theorem \ref{thm:informal_lb_avg}), for example if we parameterize the sparsity as $s = 1/\eps$, seemingly rendering our lower bounds obsolete. But this is not the case! The information-theoretic lower bound proved in **Alon et al., "Approximating the Cut-Norm"** relies on \emph{dense} vectors in $\R^d$. This is not an artifact of the proof, but an inherent requirement to prove their information-theoretic lower bound: any such lower bound \emph{cannot} use sparse vectors. This is because information-theoretically, there is already a smaller projection dimension from compressed sensing: we can encode any sparse vector in $\tilde{O}(s)$ dimensions. Parameterizing the sparsity by $s = 1/\eps$, this implies an information-theoretic upper bound of $\tilde{O}(1/\eps)$, demonstrating that the lower bound of **Bourgain et al., "A Note on the Dimensionality Reduction"** is not applicable to sparse vectors. 



% The Johnson-Lindenstrauss (JL) lemma is a powerful dimensionality reduction technique stating that any set of high-dimensional data points can be embedded into a lower-dimensional space while approximately preserving their pairwise distances **Johnson, "Extensions of Lipschitz Mapping via Small Ball Spaces"**. Specifically, we know that for $n$ vectors in $\mathbb{R}^d$, one can use a random Gaussian embedding to $O(\log(n)/\eps ^2)$ to guarantee the following maximum distortion bound with high probability: $(1-\eps ) \|x - y\|^2 \leq \|Ax - Ay\|^2 \leq (1+\eps )\|x-y\|^2$ for all $x, y \in V$, where $|V| = n$. Furthermore, this embedding is also oblivious. While it is also known that the JL lemma is tight in the worst case, we emphasize that even our average-case lower bounds towards Question (2) (see Section \ref{sec:q2}) are not implied by the existing JL lower bounds, even for the $p = 2$ case of Theorem \ref{thm:informal_lb_avg}. At a high level, this is because our hypothesis is much weaker (we only require a large fraction of the norm to be preserved, rather than all pairs). We elaborate below. 

\paragraph{Role of Sparsity in Machine Learning}
Primitives for sparse vectors, such as sparse matrix multiplication, is fundamental in a wide range of domains, such as graph analytics and scientific computing, and is used as iterative algorithms for sparse matrix factorization. Moreover, the field of deep learning often relies on faster sparse kernels to demonstrate speedups in practice, as it is a core operation in graph neural networks, transformers, and other architectures **Papernot et al., "Practical Black-Box Attacks against Deep Learning Models"**.  On the theoretical side, there are also many works which seek to understand the role of sparsity in computational hardness for optimization problems such as sparse regression **Tropp et al., "Robust Uncertainty Principles: The Sparsity of the Compressed Sensing Matrix is Relevant"**. 

\paragraph{Non-linear dimensionality reduction} One particularly interesting facet of our upper bound for embedding non-negative sparse vectors is that the mapping we give is non-linear. This is an example of an \emph{oblivious} dimensionality reduction map (a mapping that does not depend on the input dataset) with this property. Oblivious maps include for instance JL matrices and RIP matrices, which are linear maps. We would like to highlight that non-linear dimensionality reduction results have found many applications, such as modifications of the JL lemma to $\ell_2$ distances raised to fractional powers **Bourgain et al., "On the Erdoes Szemeredi Theorem and Some of its Applications"**, terminal embeddings for $\ell_2$  **O'Donnell et al., "Optimal Lower Bounds for Local and Non-Local Computation Problems"**, and finding the optimal way to embed a given set of vectors in euclidean space **Alon et al., "Approximating the Cut-Norm"**. The last two of the aforementioned applications crucially adapt the mapping to a given input point set and are not oblivious.