\documentclass[11pt]{article}
\usepackage{paper}
\usepackage{subcaption}


\newcommand{\TODO}[1]{\textcolor{red}{[TODO\@ifnotempty{#1}{: #1}]}}
\newcommand{\sandeep}[1]{\textcolor{purple}{[sandeep\@ifnotempty{#1}{: #1}]}}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}



\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{wrapfig}


\title{Beyond Worst-Case Dimensionality Reduction for Sparse Vectors}
\author{
Sandeep Silwal\thanks{UW-Madison. \texttt{silwal@cs.wisc.edu}.} \and 
David P. Woodruff\thanks{CMU and Google Research.  \texttt{dwoodruf@andrew.cmu.edu}}
\and 
Qiuyi (Richard) Zhang\thanks{Google Deepmind. \texttt{qiuyiz@google.com}}
}
\date{}

\begin{document}

\maketitle
\begin{abstract}
We study beyond worst-case dimensionality reduction for $s$-sparse vectors (vectors with at most $s$ non-zero coordinates). Our work is divided into two parts, each focusing on a different facet of beyond worst-case analysis:
\\

\noindent (a)  We first consider average-case guarantees for embedding $s$-sparse vectors. Here, a well-known folklore upper bound based on the birthday-paradox states: For any collection $X$ of $s$-sparse vectors in $\R^d$, there exists a linear map $A: \R^d \rightarrow \R^{O(s^2)}$ which \emph{exactly} preserves the norm of $99\%$ of the vectors in $X$ in any $\ell_p$ norm (as opposed to the usual setting where guarantees hold for all vectors). We provide novel lower bounds showing that this is indeed optimal in many settings. Specifically, any oblivious linear map satisfying similar average-case guarantees must map to $\Omega(s^2)$ dimensions. The same lower bound also holds for a wider class of sufficiently smooth maps, including `encoder-decoder schemes', where we compare the norm of the original vector to that of a smooth function of the embedding. These lower bounds reveal a surprising separation result for smooth embeddings of sparse vectors, as an upper bound of $O(s \log(d))$ is possible if we instead use arbitrary functions, e.g., via compressed sensing algorithms.
\\

\noindent (b) Given these lower bounds, we specialize to sparse \emph{non-negative} vectors to hopes of improved upper bounds. For a dataset $X$ of non-negative $s$-sparse vectors and any $p \ge 1$, we can non-linearly embed $X$ to $O(s\log(|X|s)/\eps^2)$ dimensions while preserving all pairwise distances in $\ell_p$ norm up to $1\pm \eps$, with no dependence on $p$. Surprisingly, the non-negativity assumption enables much smaller embeddings than arbitrary sparse vectors, where the best known bound suffers an exponential $(\log |X|)^{O(p)}$ dependence. Our map also guarantees \emph{exact} dimensionality reduction for the $\ell_{\infty}$ norm by embedding $X$ into $O(s\log |X|)$ dimensions, which is tight. We further give separation results showing that both the non-linearity of $f$ and the non-negativity of $X$ are necessary, and provide downstream algorithmic improvements using our embedding.
\end{abstract}

\setcounter{page}{0}
\thispagestyle{empty}

\newpage


\tableofcontents
\setcounter{page}{0}
\thispagestyle{empty}

\newpage
\section{Introduction}

Many popular algorithms for data processing in machine learning suffer from large running times on high-dimensional datasets. To alleviate this \emph{curse of dimensionality}, a common paradigm is to first embed the data into a lower dimension and then run any desired algorithm in the embedded space. Arguably the most fundamental result in this area is the the Johnson-Lindenstrauss (JL) lemma. It states that any set of high-dimensional data  $X$ with $|X| = n$ can be embedded into an $O(\log n/\eps ^2)$-dimensional space while approximately preserving all pairwise $\ell_2$ distances up to $\eps $ relative error \cite{johnson1984extensions}. While very versatile, the JL lemma is still a pessimistic worst case bound. This has led to a proliferation of works studying better trade-offs in more constrained settings, with the goal of exploiting intrinsic structure within the problem or in the dataset to obtain smaller embedding dimensions beyond JL. This `fine-grained' approach has found success in many domains including nearest neighbor search \cite{indyk2007nearest, andoni2018approximate}, clustering \cite{boutsidis2010random, MakarychevMR19, becchetti2019oblivious, narayanan2021randomized, izzo2021dimensionality, jiang2023moderate}, and numerical linear algebra \cite{woodruff2014sketching, cohen2015dimensionality}. 

In this paper, we consider \emph{data sparsity} as the structure to exploit to obtain better dimensionality reduction. Sparsity is an ubiquitous property of datasets and plays a crucial role in many tasks across machine learning, statistics, and signal-processing. 
Theoretically, sparsity assumptions allow for dimensionality reduction \emph{upper bounds} for sparse vectors in general $\ell_p$ norms.  In contrast, virtually all the aforementioned progress to improve beyond JL has been limited to the $\ell_2$ norm\footnote{This is not for the lack of trying: there exist fundamental limits disallowing for dimensionality reduction for general $\ell_p$ norms \cite{BrinkmanC05} for dense vectors.}. However, all known embedding\footnote{in this paper, we use the terms dimensionality reduction and embedding interchangeably. } results for sparse vectors rapidly degrade as $p$ increases. For $\ell_p$ norms, it is known that any linear dimensionality reduction for sparse vectors must suffer an exponential dependence on $p$ on the embedding dimension \cite{ZhuGR15}. In particular for the important $\ell_{\infty}$ case, all known bounds become vacuous.

However, if we relax to \emph{average-case} guarantees and only require $99\%$ of the pairs of distances in a dataset of sparse vectors to be preserved, then a well known folklore dimensionality reduction map is known to give excellent dimensionality reduction. It is based on the birthday-paradox map:

\begin{definition}[Birthday Paradox Map]\label{def:birthdayparadox}
Consider the linear map $f: \R^d \rightarrow \R^{m}$ where every coordinate in $\{1, \ldots, d\} = [d]$ is mapped uniformly at random to one of $m$ coordinates. The coordinates in $[d]$ that are mapped to the same `bucket' in $[m]$ are summed.
\end{definition} 


For any fixed $s$-sparse vector $x \in \R^d$, the birthday paradox implies that if $m \ge Cs^2$ for a large constant $C$, then with probability at least $99\%$, all the support elements of $x$ do not collide. Thus with probability at least $99\%$, $\|f(x)\|_p = \|x\|_p$ holds for any fixed sparse $x$. Since $f$ is linear, this also implies the same statement for pairwise distances. Interestingly, this map guarantees \emph{exact} dimensionality reduction (for most pairs), the embedding dimension does not depend on $p$, and the guarantees hold for arbitrary sparse vectors. The natural question following from this discussion is if we can improve upon the $O(s^2)$ upper bound of the birthday paradox map, under the same average-case guarantees. This is the first question we address.
\begin{quote}
    \textit{(Q1) Is the birthday paradox map optimal for `average-case' dimensionality reduction of general sparse vectors?} 
\end{quote}

In our contributions detailed in Section \ref{sec:our_contributions}, we prove novel lower bounds demonstrating that the birthday paradox map is optimal in many natural settings.  This prompts us to ask if conditions \emph{in addition to} sparsity enable improved dimensionality reduction upper bounds. Towards this end, we analyze the novel setting of sparse \emph{non-negative} vectors, i.e., sparse vectors whose non-zero coordinates are positive, to go beyond existing lower bounds. This additional constraint is motivated in two ways. Practically in many applications, many sparse vector datasets are naturally non-negative, such as one-hot vectors in recommendation datasets or ReLU activations in deep learning \cite{he2024relu}. Theoretically, there exists other fundamental problems demonstrating separations between non-negative sparse vectors and arbitrary sparse vectors \cite{BringmannFN21, BringmannFN22, jin2024shaving}. Note that one cannot assume non-negativity by simply shifting the dataset, as shifting can destroy sparsity. Thus the second question we ask is:

\begin{quote}
    \textit{(Q2) Can we obtain improved dimensionality reduction for non-negative sparse vectors in general $\ell_p$ norms, such as $\ell_{\infty}$?} 
\end{quote}


% Many popular algorithms for data processing in machine learning suffer from large running times on high-dimensional datasets. To alleviate this \emph{curse of dimensionality}, a common paradigm is to first embed the data into a lower dimension and then run any desired algorithm in the embedded space. Arguably the most fundamental result in this area is the the Johnson-Lindenstrauss (JL) lemma. It states that any set of high-dimensional data  $X$ with $|X| = n$ can be embedded into an $O(\log n/\eps ^2)$-dimensional space while approximately preserving all pairwise $\ell_2$ distances up to $\eps $ relative error \cite{johnson1984extensions}. While very versatile, the JL lemma is still a pessimistic worst case bound. This has led to a proliferation of works studying better trade-offs in more constrained settings, with the goal of exploiting intrinsic structure within the problem or in the dataset to obtain smaller embedding dimensions beyond JL. This `fine-grained' approach has found success in many domains including nearest neighbor search \cite{indyk2007nearest, andoni2018approximate}, clustering \cite{boutsidis2010random, MakarychevMR19, becchetti2019oblivious, narayanan2021randomized, izzo2021dimensionality, jiang2023moderate}, and numerical linear algebra \cite{woodruff2014sketching, cohen2015dimensionality}. 

% In this paper, we consider \emph{data sparsity} as the structure to exploit to obtain better dimensionality reduction. %Sparsity is an ubiquitous property of datasets and plays a crucial role in many tasks across machine learning, statistics, and signal-processing. 
% Theoretically, sparsity assumptions allow for smaller dimensionality reduction \emph{upper bounds} for sparse vectors in general $\ell_p$ norms.  In contrast, virtually all the aforementioned progress to improve beyond JL has been limited to the $\ell_2$ norm\footnote{This is not for the lack of trying: there exist fundamental limits disallowing for dimensionality reduction for general $\ell_p$ norms \cite{BrinkmanC05} for dense vectors.}. However, all known embedding\footnote{in this paper, we use the terms dimensionality reduction and embedding interchangeably. } results for sparse vectors rapidly degrade as $p$ increases. For $\ell_p$ norms, it is known that any linear dimensionality reduction for sparse vectors must suffer an exponential dependence on $p$ on the embedding dimension \cite{ZhuGR15}. In particular for the important $\ell_{\infty}$ case, all known bounds become vacuous.


% % The aim of this paper is to shed further light on the landscape of dimensionality reduction for sparse vectors. We are interested in maps $f$ which embed a set $X \subset \R^d$ of $s$-sparse vectors (vectors with at most $s$ non-zero entries) and guarantee that all pairwise distances, measured in $\ell_p$ norm, between vectors in $X$ are approximately preserved under $f$. Our work explores four natural questions. 

% % Building upon prior works which studied such maps for general $\ell_p$ norms, the first question asks:

% % \begin{quote}
% %     \textit{(Q1) Can we obtain improved dimensional reduction for preserving pairwise distances between sparse vectors for general $\ell_p$ norms?} 
% % \end{quote}


% Thus, we first explore conditions \emph{in addition to} sparsity to enable improved dimensionality reduction. Towards this end, we analyze the novel setting of sparse \emph{non-negative} vectors, i.e., sparse vectors whose non-zero coordinates are positive, to go beyond worst-case behavior. This additional constraint is motivated in two ways. Practically in many applications, many sparse vector datasets are naturally non-negative, such as one-hot vectors in recommendation datasets or ReLU activations in deep learning \cite{he2024relu}. Theoretically, non-negativity has been exploited in other algorithms for sparse vectors, e.g., there exist separations between computing the convolution of non-negative sparse vectors and computing the convolution of arbitrary sparse vectors \cite{BringmannFN21, BringmannFN22, jin2024shaving}. Note that one cannot assume non-negativity by simply shifting the dataset, as shifting can destroy sparsity. The first question we ask is:

% \begin{quote}
%     \textit{(Q1) Can we obtain improved dimensionality reduction for non-negative sparse vectors in general $\ell_p$ norms, such as $\ell_{\infty}$?} 
% \end{quote}



% We now switch to another angle of attack to go beyond worst-case guarantees for sparse vectors. The second part is motivated by an existing folklore dimensionality reduction map, based on the birthday paradox, which is known to given an excellent \emph{average-case} guarantee for embedding sparse vectors.

% \begin{definition}[Birthday Paradox Map]\label{def:birthdayparadox}
% Consider the linear map $f: \R^d \rightarrow \R^{m}$ where every coordinate in $\{1, \ldots, d\} = [d]$ is mapped uniformly at random to one of $m$ coordinates. The coordinates in $[d]$ that are mapped to the same `bucket' in $[m]$ are summed.
% \end{definition} 


% % Towards this, we ask what if the embedding is only required to preserve the distances between constantly many pairs (for example, $99\%$ of the pairwise distance)? Can this lead to better upper bounds? In this `average-case' setting where we are interested in preserving the distance between a large constant fraction of pairs, there exists a positive answer based on a

% % As discussed further in Section \ref{sec:our_contributions}, our positive answer to Question (2) involves embedding with a \emph{non-linear map}, and the embedding dimension has no dependence on $p$. This is most interesting in light of the fact that all prior dimensionality reduction upper bounds we are aware of (in virtually any context), use linear maps. Thus the following question explores the role of linearity in embedding sparse vectors.

% % \begin{quote}
% %     \textit{(Q3) What are the limitations of linear maps in dimensionality reduction for sparse vectors?} 
% % \end{quote}

% % The lower bounds used to answer the above question are shown by demonstrating specific sparse datasets where preserving \emph{all pairs} simultaneously is provably impossible. Therefore, it is natural to ask if relaxed guarantees can circumvent the strong lower bounds we show for Question (3). 


% For any fixed $s$-sparse vector $x \in \R^d$, the birthday paradox implies that if $m \ge Cs^2$ for a large constant $C$, then with probability at least $99\%$, all the support elements of $x$ do not collide. Thus with probability at least $99\%$, $\|f(x)\|_p = \|x\|_p$ holds for any fixed sparse $x$. Interestingly, this map guarantees \emph{exact} dimensionality reduction (for most vectors), the embedding dimension does not depend on $p$, and the guarantees hold for arbitrary sparse vectors. The natural question following from this discussion is if we can improve upon the $O(s^2)$ upper bound of the birthday paradox map, under the same average-case guarantees. This is the second question we address.
% \begin{quote}
%     \textit{(Q2) Is the birthday paradox map optimal for `average-case' dimensionality reduction of general sparse vectors?} 
% \end{quote}

\section{Our Contributions}\label{sec:our_contributions}

We detail our contributions to each question (Question 1 in Section \ref{sec:q2} and Question 2 in Section \ref{sec:q1}).  Table \ref{tab:results_lb} summarizes our lower bound results, Table \ref{tab:results_ub} summarizes our upper bound results, and Table \ref{tab:results_app} lists applications of our upper bounds. Additional related works are discussed in Section \ref{sec:related_works}.




\subsection{Question 1: Average case lower bounds for general sparse vectors}\label{sec:q2}
Our main contribution towards (Q1) is to demonstrate that the average-case guarantees of the folklore birthday paradox upper bound of Definition \ref{def:birthdayparadox} is \emph{optimal} in many natural settings. First recall that the folklore upper bound guarantees that for any $p$ and for any point set of $s$-sparse vectors, there exists an embedding into $O(s^2)$ dimensions which preserves the $\ell_p$ norm \emph{exactly} for $99\%$ of the vectors (this is a weaker hypothesis than requiring pairwise distances to preserved as long as the all zeros vector is also mapped to the all zeros vector).

\paragraph{$\bullet$ Arbitrary linear maps.} Our first result states that any linear map with arbitrary real entries which satisfies the properties of the folklore upper bound must embed to at least $m = \Omega(s^2)$ dimensions. Our lower bound holds for any even integer $p$. More precisely, we construct a set of $s$-sparse vectors in $d$-dimensions, where we show that any linear map $A$ which guarantees exact norm-preservation for more than $99\%$ of this set (i.e., a randomly chosen vector in the set has a $> 99\%$ chance of their $\ell_p$ norm being preserved under the map) must map to $\Omega(s^2)$ dimensions. Note that the lower bound below for $d = s^2$ automatically implies the same lower bound for all higher $d$ by padding zeros in the `hard' instance.

% We note that proving lower bounds for linear maps is motivated by the fact that in settings such as streaming algorithms, \emph{any} algorithm may as well be a linear mapping (under some restrictions) \cite{li2014turnstile}.

\begin{theorem}[Informal, see Theorem \ref{thm:lb_avg_lp}]\label{thm:informal_lb_avg}
  Let $p \ge 2$ be an even integer. There exists a point set $S \subset \R^{s^2}$ of $s$-sparse vectors such that any linear map $A: \R^{s^2} \rightarrow \R^m$ satisfying $\|Ax\|_p = \|x\|_p$ for $99\%$ of vectors $x \in S$ must map to $m = \Omega(s^2)$ dimensions.
\end{theorem}

We note that our lower bound, even for the $p=2$ case, is not implied by the known lower bounds for the JL lemma \cite{alon2003problems, larsen2014johnson, larsen2017optimality}, even if some of the hard point sets in prior works are sparse. This is because the hypotheses of the lower bounds are different. We prove a lower bound against only preserving $99\%$ of distances, rather than all of them. Our lower bound also has no term that depends on the size of the point set (matching the folklore upper bound). 

There is also a known lower bound for the JL lemma (again $p=2$) where only one distance must be preserved with high probability (similar to the setting of the above theorem) \cite{jayram2013optimal}. However, this lower bound is only against \emph{dense} vectors, and so it cannot imply Theorem \ref{thm:informal_lb_avg}. Prior lower bounds for the JL lemma are discussed in detail in Section \ref{sec:related_works}.

\paragraph{$\bullet$  Beyond Exact Preservation.} Our lower bound for linear maps extends to a stronger statement for the $\ell_2$ case. We show that $m = \Omega(s^2)$ even if we only require the \emph{weaker} guarantee of $|\|Ax\|_2^2 - \|x\|_2^2| \le O(\frac{\|x\|_2^2}s)$. This is optimal as any larger relative error of $\eps   \gg 1/s$ can be accomplished with the standard linear JL  map with $\tilde{O}(1/\eps ^2) = o(s^2)$ dimensions. This reveals an interesting phase transition: any larger error than $1/s$ enables $o(s^2)$ dimensional embeddings but any smaller error requires $\Omega(s^2)$ dimensional embeddings ($1/s$ error is no easier than $0$ error!).

\begin{theorem}[Informal, see Theorem \ref{thm:lb_linear_l2}] \label{thm:l2_approx_simple}
 There exists a point set $S \subset \R^{s^2}$ of $s$-sparse vectors such that any linear map $A: \R^{s^2} \rightarrow \R^m$ satisfying $|\|Ax\|_2^2 - \|x\|_2^2| \le O(\|x\|_2^2/s)$ for $99\%$ of vectors $x \in S$ must map to $m = \Omega(s^2)$ dimensions.
\end{theorem}

We also extend this lower bound for linear maps to the case where we require inner-products to be preserved, rather than $\ell_2$ norms (Theorem~\ref{thm:lb_linear_innerproduct}). 


\paragraph{$\bullet$ Arbitrary smooth embeddings.} For the $\ell_2$ case, we consider a more general class of embeddings $f: \R^{s^2} \rightarrow \R^m$ where $f(x) = (f_1(x), \ldots, f_m(x))$
and each $f_i: \R^{s^2} \rightarrow \R$ is twice differentiable with continuous second partial derivatives. We prove that this large richer class of mappings still requires $m = \Omega(s^2)$ to satisfy the guarantees of the folklore upper bound for the $\ell_2$ case.
\begin{theorem}[Informal, see Theorem \ref{thm:general}]
The lower bound statement of Theorem \ref{thm:l2_approx_simple} extends to the general class of mappings defined above.
\end{theorem}

\paragraph{$\bullet$ Beyond Embeddings.} Our $\ell_2$ lower bounds hold even when we are not restricted to computing the norm on the embeddings produced by a mapping $f$ and can use another function $g$ (on top of the output of $f$) to compute the norm. Formally, we define an encoding-decoding scheme using an \emph{encoding} function $f$ which maps an $s$-sparse vector in $\R^{s^2}$ to a dimension $m$. A \emph{decoding} function $g$ then takes the output of $f$ and maps it to a potentially much larger dimension $k$. Our goal is to show that even if $k$ is large, as long as $m$ is substantially less than the initial sparsity squared, we cannot have $\|g(f(x))\|_2 \approx \|x\|_2$. 


\begin{restatable}{defs}{encoderdecoder}
\label{def:encoder_decoder}
We suppose the encoder and decoder functions satisfy the following.
 \begin{itemize}
    \item (Encoder function)  $f: \R^{s^2} \rightarrow \R^m$ where 
\[f(x) = (f_1(x), \ldots, f_m(x))\]
and  each $f_i: \R^{s^2} \rightarrow \R$ is twice differentiable with continuous second partial derivatives.
\item (Decoder function) $g: \R^{m} \rightarrow \R^{k}$ where 
\[g(x) = (g_1(x), \ldots, g_{k}(x))\]
and  each $g_i: \R^{m} \rightarrow \R$ is twice differentiable with continuous second partial derivatives.
\end{itemize}   
\end{restatable}


\begin{theorem}[Informal, see Theorem \ref{thm:encoder_decoder}]
The lower bound statement of Theorem \ref{thm:l2_approx_simple} extends to the case of encoder/decoder schemes of Definition \ref{def:encoder_decoder} and shows that $f$ (the encoder) must map to $\Omega(s^2)$ dimensions.
\end{theorem}

Perhaps surprisingly, the theorem above states that if $f$ and $g$ are both sufficiently smooth, then $f$ still must map to $\Omega(s^2)$ dimensions (matching the folklore upper bound). That is, the whole procedure is still `bottle-necked' by the dimension of $f$.

One reason for why this lower bound is surprising is that the hypothesis that $f$ and $g$ are smooth is quite crucial. In fact, the lower bound cannot hold if $g$ is arbitrary, even if $f$ is constrained to be a linear map. Indeed, compressed sensing algorithms tells us that there exists a suitable linear map $A: \R^d \rightarrow \R^{O(s\log(d))}$  (the so called RIP matrices \cite{candes2005decoding}) and a decoding function $g$ such that $g(Ax) = x$ for any $s$-sparse $x$. However, $g$ is usually a complicated optimization step. For example in the popular Lasso algorithm for exact recovery, $g$ amounts to solving a linear program \cite{foucart}, and the optimal solutions to such optimization programs can be highly discontinuous. If we also for arbitrary encoding functions $f$, then $f$ can also just be a list of the non-zero coordinates of $x$ along with their values, thereby also mapping into $\tilde{O}(s)$ dimensions. Clearly this $f$ is also highly discontinuous. 

Thus, our results reveal an interesting separation result in the context of compressed sensing and sketching in general: if we let the encoding and decoding functions be arbitrary, then we can map to $\tilde{O}(s)$ dimensions. On the other hand, imposing even mild smoothness assumptions on $f$ and $g$ implies a much larger $\Omega(s^2)$ lower bound. See Section \ref{sec:related_works} for more details.

\begin{table}[!ht]
\centering
{\renewcommand{\arraystretch}{1.5}
\begin{tabular}{c|c|c|c}
\textbf{Point Set}                                                            & \textbf{Guarantee}                                                                                                                                                                                                                                  & \textbf{\begin{tabular}[c]{@{}c@{}}Our Lower \\ Bound\end{tabular}} & \textbf{Thm.} \\ \hline
\begin{tabular}[c]{@{}c@{}}$O(1)$-sparse \\ non-negative vectors\end{tabular} & \begin{tabular}[c]{@{}c@{}}$ \frac{\|x\|_{\infty}}2 \le \|f(x)\|_{\infty} \le \frac{3\|x\|_{\infty}}2$\\ Linear $f$\end{tabular}                                                                                                                   & $\Omega(d)$                                                         & \ref{thm:linear_lb}        \\ \hline
\begin{tabular}[c]{@{}c@{}}$O(1)$-sparse \\ non-negative vectors\end{tabular} & \begin{tabular}[c]{@{}c@{}}$ \|x+y\|_{\infty} \le \|f(x)+f(y)\|_{\infty} \le (2-\eps) \cdot \|x+y\|_{\infty}$\\ Arbitrary $f$, any $\eps > 0$\end{tabular}                                                                                              & $\Omega(d)$                                                         & \ref{thm:2_hardness_addt}          \\ \hline
\begin{tabular}[c]{@{}c@{}}$O(1)$-sparse \\ non-negative vectors\end{tabular} & \begin{tabular}[c]{@{}c@{}}$\forall r> 0, 0.99 \|rx\|_{\infty} \le \|f(rx)\|_{\infty} \le 1.01 \|rx\|_{\infty}$\\ Arbitrary $f$ with continuous second-order derivatives\end{tabular}                                                             & $\Omega(d)$                                                         & \ref{thm:lb_smooth}        \\ \hline
\begin{tabular}[c]{@{}c@{}}$O(1)$-sparse \\ vectors\end{tabular}              & \begin{tabular}[c]{@{}c@{}}$ 0.9 \cdot \|x-y\|_{\infty} \le \|f(x) - f(y)\|_{\infty} \le 1.1 \cdot \|x - y\|_{\infty}$\\ $\|x+y\|_{\infty} \le \|f(x)+ f(y) \|_{\infty} \le C \cdot \|x + y\|_{\infty}$\\ Arbitrary $f$, any $C > 0$\end{tabular} & $\Omega(d)$                                                         & \ref{thm:lb_nonneg}   \\ \hline
\begin{tabular}[c]{@{}c@{}}$s$-sparse \\ vectors\end{tabular}              & \begin{tabular}[c]{@{}c@{}}$\|f(x) - f(y)\|_{\infty} \le \|x-y\|_{\infty}$\\ $C \|x-y\|_{1} \le \|f(x) - f(y)\|_{1}$\\ Arbitrary $f$, any $C > 0$\end{tabular} & $\Omega(Cs)$                                                         & \ref{thm:omega_s_lb}   \\ \hline
\begin{tabular}[c]{@{}c@{}}$s$-sparse \\ vectors\end{tabular}              & \begin{tabular}[c]{@{}c@{}}$\|f(x)\|_p = \|x\|_p$ for $99\%$ of the points\\ Linear $f$, even $p$\end{tabular} & $\Omega(s^2)$                                                         & \ref{thm:lb_avg_lp}   \\ \hline
\begin{tabular}[c]{@{}c@{}}$s$-sparse \\ vectors\end{tabular}              & \begin{tabular}[c]{@{}c@{}}$| \|f(x)\|_2^2 - \|x\|_2^2 | \le O(\|x\|_2^2/s)$ for $99\%$ of points\\ Arbitrary $f$ with continuous second-order derivatives\end{tabular} & $\Omega(s^2)$  & \ref{thm:general}      \\ \hline
\begin{tabular}[c]{@{}c@{}}$s$-sparse \\ vectors\end{tabular}              & \begin{tabular}[c]{@{}c@{}}$| \|g(f(x))\|_2^2 - \|x\|_2^2 | \le O(\|x\|_2^2/s)$ for $99\%$ of the points \\ Arbitrary $f$, $g$ with continuous second-order derivatives\end{tabular} & $\Omega(s^2)$                                                         & \ref{thm:encoder_decoder}    
\end{tabular}
}
\caption{Our lower bound results. $d$ is the ambient dimension.  The first five guarantees hold for every pair of vectors $x,y$ in the point set specified in the respective theorems. The second to last column is the dimension that $f$ must map to. In the last row, $g$ is allowed to map to an arbitrary dimension.}
\label{tab:results_lb}
\end{table}

\subsection{Question 2: Upper bounds for embedding non-negative sparse vectors}\label{sec:q1}
% Our first result is an embedding for general sparse vectors.
% \begin{theorem}[Informal, see Theorem \ref{thm:final_embedding_general}]
%     For every $p \ge 1$, there exists a linear map $f: \R^d \rightarrow \R^{m}$ for $m = s^{p+2}\log(s)\log(d/\eps)2^{O(p)}/\eps^2$ which satisfies $\|f(x)\|_p = (1 \pm \eps)\|x\|_p$ for all $s$-sparse vectors $x \in \R^d$.
% \end{theorem}
% In comparison, the best prior bound achieves embedding dimension $p^{O(p)} s^{p} \log^{p-1}(d)/\eps^2$, also via a linear map. While the exponent of $s$ is smaller by $2$ in the prior bound, it incurs an exponential dependence on $p$ in both the terms $p^{O(p)}$ and $\log(d)^{p-1}$. Thus, qualitatively, we obtain improvements when the sparsity is small (for example $s \lesssim \log^{p-3}(d)$). Quantitatively, our improvement can be seen when we embed $\ell_{\infty}$ into a large $\ell_p$ norm for sparse vectors. The choice of $p = O(\log(s)/\eps)$ is the natural choice as the $\ell_{O(\log(s)/\eps)}$ norm approximates the $\ell_{\infty}$ norm up to a multiplicative $1\pm \eps$ factor for $s$-sparse vectors. We have the following corollary.

% \begin{corollary}[Informal, see Corollary \ref{cor:linf}]\label{cor:linf_simple}
%      Let $p = O(\log(s)/\eps)$. There is a linear map $f: \R^d \rightarrow \R^m$ such that
%      $\|f(x)\|_p = (1\pm \eps)\|x\|_{\infty}$ for all $s$-sparse $x \in \R^d$ for the following $m$:
%     \begin{itemize}
%         \item If we use the embedding of \cite{ZhuGR15}, then  $m = s^{O(\eps^{-1}(\log \log d + \log s)) }/\eps^2$.
%         \item If we use the embedding of Theorem \ref{thm:final_embedding_general}, then  $m = \log(s)\log(d/\eps)s^{O(\eps^{-1}\log s) }/\eps^2$.
%     \end{itemize}
% \end{corollary}

% Thus, our embedding avoids a multiplicative overhead of $s^{O(\eps^{-1} \log \log d)}$.

% \subsection{Question 2}
% Perhaps the most conceptually interesting contribution of the paper is the following theorem for embedding non-negative sparse vectors.

We begin with our upper bound theorem for embedding non-negative sparse-vectors.

\begin{theorem}[Informal, see Theorem \ref{thm:final_non_negative}]\label{thm:nonneg_simple}
    Let $X$ be a set of $n$ non-negative $s$-sparse vectors. For every $ p \ge 1$, there exists a \textbf{non-linear} map $f: \R^d \rightarrow \R^{m}$ for $m = O(\log(ns) \cdot s/\eps^2)$ which satisfies $\|f(x) - f(y)\|_p = (1 \pm \eps)\|x-y\|_p$ for all $x,y \in X$. For the $\ell_{\infty}$ case, we can instead guarantee $\|f(x) - f(y)\|_{\infty} = \|x-y\|_{\infty}$ for all $x,y \in X$ with $m = O(s \log n)$.
\end{theorem}
We note that for every $p$, it is the same map $f$ (up to scaling by a known factor) that gives the guarantees of the above theorem. There are three interesting aspects of Theorem \ref{thm:nonneg_simple} that we want to highlight:
\begin{enumerate}
    \item The map $f$ is non-linear and non-smooth.
    \item $X$ needs to be a point sets of non-negative sparse vectors.
    \item Our embedding gives a non-trivial embedding for $\ell_{\infty}$.
\end{enumerate}

% In fact, our non-linear map is powerful enough to give dimensionality reduction for a wide class of `monotone' functions which generalize $\ell_p$ norms. Our guarantees extend to the setting where for vectors $x,y$, we measure their `length' as $\sum_i g(|x_i-y_i|)$, for $g: [0, \infty) \rightarrow [0, \infty)$ any monotonically increasing function; see Corollary \ref{cor:monotone}.

Given these points, we additionally show that our upper bound for embedding non-negative sparse vectors presented in Theorem \ref{thm:nonneg_simple} is optimal in many ways.

\paragraph{$\bullet$ Non-linearity is necessary.} We give an example of an  $O(1)$-sparse and a non-negative point set in $\R^d$ where any \emph{linear map} requires $\Omega(d)$ dimensions to preserve the $\ell_{\infty}$ norm up to relative error $0.5$. This is particularly interesting as many prior \emph{oblivious} dimensionality reduction maps, i.e. maps which do not depend on the input dataset such as JL or RIP matrices, are linear. However in the natural case of non-negative sparse vectors, linear maps are provably insufficient. 

\begin{restatable}{thm}{linearlb}
\label{thm:linear_lb}
    Let $S$ be the set of all $10$-sparse vectors in $\R^d$ with all non-zero coordinates equal to $1$. Let $A: \R^d \rightarrow \R^m$ be a matrix such that $\frac{1}2 \|x\|_{\infty} \le \|Ax\|_{\infty} \le \frac{3}2 \|x\|_{\infty} \, \forall x \in S$. Then $m = \Omega(d)$.
\end{restatable}


\paragraph{$\bullet$ `Non-smoothness' is necessary.} Our upper bound of Theorem \ref{thm:nonneg_simple} is continuous but not differentiable. Motivated by this phenomenon, we show that any upper bound satisfying Theorem \ref{thm:nonneg_simple} cannot be `very smooth': any twice differentiable map (satisfying the same hypothesis) must embed into $\Omega(d)$ dimensions.


\begin{restatable}{thm}{lbsmooth}
\label{thm:lb_smooth}
    Suppose $f: \R^d \rightarrow \R^m$ is such that $f(x) = (f_1(x), \ldots, f_m(x))$ where each $f_i$ is twice differentiable with continuous second partial derivatives. Let $S$ be the set of all $10$-sparse vectors in $\R^d$ with all non-zero coordinates being equal to $1$. Suppose $f$ satisfies 
    \[  0.99 \|rx\|_{\infty} \le \|f(rx)\|_{\infty} \le 1.01 \|rx\|_{\infty} \quad \forall r> 0, \forall x \in S. \]
    Then $m = \Omega(d)$.
\end{restatable}


\paragraph{$\bullet$ Sums of non-negative sparse vectors cannot be preserved.} It is natural to ask if the map $f$ of Theorem \ref{thm:nonneg_simple} can also preserve \emph{sums} of non-negative sparse vectors: $\|f(x) + f(y) \|_p \approx \|x + y\|_p$? Indeed, our upper bound of Theorem \ref{thm:nonneg_simple} has the additional property that it preserves the norms of sums in $\ell_{\infty}$ norm up to a multiplicative factor of $2$, even if we embed into \emph{one} dimension (see the formal Theorem \ref{thm:final_non_negative}). We show that the approximation factor $2$ is tight in a very strong sense.

\begin{restatable}{thm}{twohardness}
\label{thm:2_hardness_addt}
    Let $e_i$ be the $i$th basis vector in $\R^d$. Consider the set $S = \{e_i \} \cup \{0\}$ of $d+1$ vectors. Suppose $f: S \rightarrow \R^m$ be an arbitrary mapping which satisfies
    \[  \|x+y\|_{\infty} \le \|f(x) + f(y)\|_{\infty} \le (2-\eps) \|x+y\|_{\infty} \quad \forall x,y \in S \]
    for any $\eps > 0$. Then $m \ge d$.
\end{restatable}

\paragraph{$\bullet$ General sparse vectors are hard to embed.} We also consider dropping the non-negative hypothesis. We again show that for the $\ell_{\infty}$ case, upper bounds as in the non-negative case cannot exist for general sparse vectors. Note that prior lower bounds studied in \cite{ZhuGR15} are restricted to linear maps. In contrast, we show in the following theorem that any map which satisfies the same hypothesis as Theorem \ref{thm:nonneg_simple}, but which holds for general sparse vectors with possibly \emph{negative} entries, must embed into $\Omega(d)$ dimensions.


\begin{restatable}{thm}{lbnonnegvec}
\label{thm:lb_nonneg}
    Let $e_i$ be the $i$th basis vector in $\R^d$. Consider the set $S = \{ \pm e_i \} \cup \{0\}$ of $2d+1$ vectors. Suppose $f: S \rightarrow \R^m$ be an arbitrary mapping which satisfies
    \[  0.9 \|x-y\|_{\infty} \le \|f(x) - f(y)\|_{\infty} \le 1.1 \|x-y\|_{\infty} \quad \forall x,y \in S, \]
    \[  \|x+y\|_{\infty} \le \|f(x) + f(y)\|_{\infty} \le C \|x+y\|_{\infty} \quad \forall x,y \in S \]
    for any $C \ge 1$. Then $m \ge d$.
\end{restatable}



The final lower bound implies that $\tilde{\Omega}(s)$ embedding dimension is necessary in Theorem \ref{thm:final_non_negative}.

\begin{theorem}[Informal, see Theorem \ref{thm:omega_s_lb}]
    Any map $f$ satisfying similar guarantees to that of Theorem \ref{thm:final_non_negative} with $\eps = O(1)$ must map to $\Omega(s/\log(n))$ dimensions.
\end{theorem}

% Lastly, we remark on an interesting consequence of Theorem \ref{thm:nonneg_simple}. It can be extended to embeddings for \emph{arbitrary} sparse vectors with bounded entries. Contrasting it with the known $\Omega(s^p)$ lower bounds for embedding arbitrary $s$-sparse vectors (using linear maps) from \cite{ZhuGR15} hints that the `hardness' for embedding arbitrary sparse vectors maybe due to entries with a large range.


% \begin{theorem}[Informal, see Theorem \ref{thm:general_discrete}]
%     Let $X$ be a set of $n$ $s$-sparse vectors with entries in $\{-\Delta, \ldots, 0, \ldots, \Delta\}$. For every $p \ge 1$, there exists a non-linear map $f: \R^d \rightarrow \R^{m}$ for $m = O(s^2\Delta^{O(p)} \log(n)/\eps^2)$ which satisfies $\|f(x) - f(y)\|_p = (1 \pm \eps)\|x-y\|_p$ for all $x,y \in X$.
% \end{theorem}


\begin{table}[!htbp]
\centering
{\renewcommand{\arraystretch}{1.7}
\begin{tabular}{c|c|c|c|c}
\textbf{Approx.}                    & \textbf{$\ell_p$ norm} & \textbf{\begin{tabular}[c]{@{}c@{}}Our embedding\\ dimension\end{tabular}}                                                                                           & \textbf{\begin{tabular}[c]{@{}c@{}}Prior embedding\\ dimension\end{tabular}}                                                                  & \textbf{\begin{tabular}[c]{@{}c@{}}Prior\\ Reference\end{tabular}} \\ \hline
\multirow{3}{*}{$1\pm \varepsilon$} & $2 < p <  \infty$        & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} \\ $O\left( \frac{s \log(n)}{\varepsilon^2} \right)$\\Non-linear Map\end{tabular}} & \begin{tabular}[c]{@{}c@{}}$ \frac{s^2}{\eps^2} \cdot (p \log(n))^{O(p)}$\\ Linear Map\end{tabular} & \cite{ZhuGR15}                                                  \\ \cline{2-2} \cline{4-5} 
                                    & $ 1 < p < 2$       &                                                                                                                                                                      & \begin{tabular}[c]{@{}c@{}}$O\left(\frac{s^p \log(d)}{\eps^2} + \frac{s^{4 - 2/p-p}\log(d)}{\eps^{2/(p-1)}}\right) $\\ Linear Map\end{tabular}                                             &    \cite{ZhuGR15}            \\ \cline{2-2} \cline{4-5} 
                                    & $ p \in \{ 1, 2 \}$  &                                                                                                                                                                      & \begin{tabular}[c]{@{}c@{}}$ O\left(\frac{s \log(d/s)}{\eps^2} \right) $\\ Linear Map\end{tabular}                                           & \cite{candes2005decoding, berinde2008combining}                                                 \\ \hline
1                                   & $p = \infty$           & \begin{tabular}[c]{@{}c@{}}$O(s \log(n))$\\ Non-linear Map\end{tabular}                                                     & -                                                                                                                                             & -                                                                 
\end{tabular}

}
\caption{ Our upper bound results. The ambient dimension is $d$. Our upper bounds are in Theorem \ref{thm:final_non_negative}, and hold for a set of $n$ non-negative $s$-sparse vectors. The first row is not explicitly given in \cite{ZhuGR15}, but can be derived from their argument. However, their result also holds for $n$ arbitrary $s$-sparse vectors. The $1 \le p \le 2$ cases of prior work can also handle all $s$-sparse vectors simultaneously.}
\label{tab:results_ub}
\end{table}



\subsection{Applications of Our Non-negative Embedding}
Our dimensionality reduction upper bound for non-negative sparse vectors, just as the JL lemma, has a slew of downstream algorithmic applications. We present a small subset of some of the new applications here, with a focus on more fundamental geometric optimization problems that have been well studied. 
At a high level, dimensionality reduction allows us to make black-box use of any existing algorithm for a geometric task in low-dimensions. If the dimensionality reduction step is sufficiently powerful, we can hope to get faster runtimes at the cost of a small approximation factor loss; see Table \ref{tab:results_app} for a summary of our downstream algorithmic applications (diameter, max-cut, $k$-clustering, and more).


\begin{table}[!htpb]
\centering
{\renewcommand{\arraystretch}{1.5}
\begin{tabular}{c|c|c|c|c}
\textbf{Problem}      & \textbf{Definition} & \textbf{\begin{tabular}[c]{@{}c@{}}Embedding \\ Dimension\end{tabular}} & \textbf{Distortion} & \textbf{Reference} \\ \hline
Diameter              &   \ref{def:diameter}                & $O(s^2)$                                                                & $1$                 & Lemma \ref{lem:diam} and Theorem \ref{thm:diameter}            \\
Max-Cut               & \ref{def:maxcut}                  & $O(s/\eps^2)$                                                           & $1\pm \eps$         & Theorem \ref{thm:maxcut}            \\
$k$-median/$k$-center & \ref{def:clustering}                  & $O(s \log(n)/\eps^2)$                                                 & $4 \pm \eps$        & Theorem \ref{thm:clustering}           \\
$k$-means             & \ref{def:clustering}                  & $O(s \log(n)/\eps^2)$                                                 & $16 \pm \eps$       & Theorem \ref{thm:clustering}          \\
Distance Estimation   & \ref{def:dist_estimation}                  & $O(s/\eps^2)$                                                           & $1 \pm \eps$        & Theorem \ref{thm:dist_est1}          
\end{tabular}
}
\caption{Applications of our dimensionality reduction upper bound. The input is always a dataset of $n$ $s$-sparse non-negative vectors and the underlying norm is $\ell_p$ for an arbitrary $p \ge 1$. The distortion bounds hold with probability at least $99\%$. See the theorem statements for full details.}
\label{tab:results_app}
\end{table}

% \paragraph{A Note on Sparse Matrix Multiplication.}
%  By leveraging the existing birthday paradox map of Definition \ref{def:birthdayparadox} we can embed sparse vectors to $O(s^2\log(n))$ dimensions before applying matrix multiplication. This lets us multiply two $n \times n$ $A$ and $B$ with arbitrary real entries, where every row of $A$ is promised to be $s$-sparse and every column of $B$ is promised to be $s$-sparse, in time $T(n, s^2\log(n))$, where $T(n, a)$ is the time to multiply a $n\times a$ and a $a \times n$ matrix. Our multiplication result recovers some of the guarantees as the classical and current state of the art result of \cite{yuster2005fast}, however it requires an arguable simpler algorithm, avoiding a divide and conquer approach used in \cite{yuster2005fast}.

% \begin{corollary}[Informal, see Corollary \ref{cor:linf}]
% Consider the setting of Corollary \ref{cor:linf_simple} with a dataset $X$ of $s$-sparse vectors.  If the entries of $X$ are in $\{-O(1), \ldots, O(1)\}$, then there is a map with $m = \log(|X|) s^{O(1/\eps)}/\eps^2$.
% \end{corollary}



\section{Technical Overview}\label{sec:technical_overview}

We give on overview of our  average-case lower bounds for embedding arbitrary sparse vectors and upper bound for embedding non-negative sparse vectors.




\subsection{Overview: Average-Case Lower Bounds}\label{sec:avg_case_lb_overview}

In this section, we provide insights to our lower bound result of Theorem \ref{thm:lb_linear_l2}. Similar ideas are used in all of our average-case lower bounds, so we focus on this case which is the most instructive. 

First we briefly outline how a known lower bounds for the JL lemma is proven. We state the proof of \cite{alon2003problems} where a slightly suboptimal lower bound is given (the result is tight up to $\log(1/\eps)$ factors), but is perhaps more pedagogically useful in relation to our approach.

\cite{alon2003problems} construct a specific set of $n$ unit vectors and consider an embedding of these points into $m$ dimensions which preserves \emph{all} pairwise distances up to $1\pm \eps$ multiplicative factor, or the inner product up to additive $\pm \eps$. Letting $X \in \R^{n \times m}$ denote the matrix of embeddings, they consider the gram matrix $XX^T$. The key point is that they have precise control on \emph{all} the entries of $XX^T$ due to the JL assumption. This allows them to argue that the rank of $X$ must be sufficiently large, implying a lower bound for $m$, the embedding dimension. 

For us, we cannot directly employ this approach. Our lower bound hypothesis is weaker since we only guarantee that the norm is preserved say $99\%$ of the time. This means that we have no control over a constant fraction of the entries in $XX^T$, which can wildly influence the rank. However, this approach suggests that \emph{rank} is a useful parameter and this observation is the starting point of our lower bound.

We now outline our lower bound approach for the case of a linear map $A$ given in Theorem \ref{thm:lb_linear_l2}. We first construct a suitable `hard' input. We remark that showing a lower bound under this specific hard example extends to a lower bound for general point sets, as the former is a special case of the latter. The hard input we use can be represented as a distribution over sparse vectors. Our distribution first randomly samples the support elements and then puts random Gaussian values in the sampled support. The distribution we use, $\textup{Unif}_{t,r}$, is defined below.

\begin{restatable}{defs}{unif}
\label{defunif}
To generate $x \sim \textup{Unif}_{t,r}$,
\begin{enumerate}
    \item First pick $t$ coordinates uniformly at random to be the support. 
    \item The non-zero coordinates of $x$ are mean zero i.i.d. Gaussians with variance $r$.
\end{enumerate}
\end{restatable}

If $r = 1$, we also denote the distribution as $\textup{Unif}_{t}$. Theorem \ref{thm:lb_linear_l2} states the following.

\begin{restatable}{thm}{linearltwo}
\label{thm:lb_linear_l2}
    Let $A: \R^{s^2} \rightarrow \R^m$ be a linear map and  $\gamma \le C/s$ for a sufficiently small constant $C > 0$. If $A$ is  such that for any $1 \le t \le s$,
    \[ \Pr_{u\sim \textup{Unif}_t}\left( |\|Au\|_2^2 - \|u\|_2^2| \le \gamma \|u\|_2^2 \right) \ge 0.99,\]
    then $m \ge s^2/1000$.
\end{restatable}
Note that as stated above, we are assuming the ambient dimension is $s^2$. This easily generalizes to any larger dimension by padding zeros.

To begin the proof, we first assume for contradiction that $m \ll s^2$. Now note that 
\[ \|Ax\|_2^2 - \|x\|_2^2 = \sum_{i,j} x_i x_j \langle A_i, A_j \rangle  - \sum_i x_i^2 = \text{Tr}(xx^TA^TA) - \sum_i x_i^2  \]
where $A_i$ are the columns of $A$. For simplicity, let's ignore the $\sum_i x_i^2$ portion for the rest of the discussion. We also assume the sum $\sum_{i,j} x_i x_j \langle A_i, A_j \rangle$ is over $i \ne j$. (In the formal proof, we show that the $\sum_i x_i^2$ portion and the $\sum_i x_i^2 \|A_i\|_2^2$ `roughly cancel'.) We view this expression as a polynomial $P(x)$ of degree $2$ in the variables $x_i$. Then the condition $| \|Ax\|_2^2 - \|x\|_2^2 | \le \gamma \|x\|_2^2$ implies that with large probability, $P(x)$ lies in a fixed interval $I$ of size $O(1)$ (as typically, $\gamma \|x\|_2^2 = O(1)$). Our goal now is to show that $P(x)$ has variance $\Omega(1)$ (under the randomness of $x$), implying it does not lie in any fixed $I$ with large constant probability, violating the hypothesis of the theorem.

Now it would be very bad for us if $P$ takes on `very small' values over a typical choice of $x$. This means that we cannot rule out $P \in I$. For example, $P$ could even be equal to $0$ if the \emph{non-zero} entries of $x^Tx$ only collide with the \emph{zero} entries of $A^TA$. To avoid this possibility, we first demonstrate that a large fraction of the columns of $A$ have norms very close to $1$. However, there are $s^2$ columns, which are all in $\R^m$. Since we assumed $m$ to be very small, this means that we have $\gg m$ almost-unit vectors in $m$ dimensions. We show that this implies there are at least $\Omega(s^2)$ pairs of $A_i$ and $A_j$ that have non-zero inner products (note that there are $\Theta(s^4)$ total pairs). This bound crucially relies on the fact that $m \ll s^2$ and can be thought of as a `generalized' version of the rank argument used in \cite{alon2003problems}. Since any pair $(i,j)$ is non-zero in $x^Tx$ with probability approximately $1/s^2$, this means $x^Tx$ and $A^TA$ `collide' on a non-zero entry with a large constant probability.

However, this is not quite strong enough for our purposes since the coefficient of $P$, which are exactly $\langle A_i, A_j \rangle$, can be very close to $0$ and unluckily, $x^Tx$ could only collide on such small entries. Thus, our refined aim is to show that the coefficients of $P$ that collide with the non-zero entries of $x^Tx$ are `large' in the sense that the sum of these coefficient squared is $\Omega(1)$. It can be checked that this is sufficient to show the variance of $P(x) = \Omega(1)$.

Then in the formal proof, we show that the sum of non-zero coefficient squared is $\Omega(1)$, via a `fine-grained' exploitation of the fact that $m \ll s^2$ (i.e. another appearance of rank). Now to argue anti-concentration of $P(x)$, we invoke the following classical inequality which states that the random variable $P(x)$ cannot be concentrated in an interval that is much smaller than the variance (which is directly related to the sum of non-zero coefficients squared).


\begin{lemma}[Theorem 8 in \cite{carbery2001distributional}]\label{lem:poly_anti_concentration}
    Let $P : \R^n \rightarrow \R$ be a degree $d$ polynomial and $Z$ denote the random variable where $P$ is evaluated on a standard Gaussian vector in $\R^n$. Then
    \[ \Pr(|Z| \le \eps \sqrt{\textup{Var}(Z)}) \le  O(d \eps^{1/d}). \]
\end{lemma}

An application of Lemma \ref{lem:poly_anti_concentration} then implies that with sufficiently large constant probability, $P(x)$ \emph{does not} lie in the interval $I$ defined above, contradicting the theorem hypothesis that $\|Ax\|_2^2$ approximately preserves the norm of $x$ with large probability. Hence, our assumption that $m \ll s^2$ is not valid, finishing the proof. The formal details are given in the proof of Theorem \ref{thm:lb_linear_l2}.


\subsection{Overview: Embedding Non-negative Sparse Vectors}

The goal of this section is to motivate our upper bound result of Theorem \ref{thm:final_non_negative}. For simplicity, we only focus on the $\ell_{\infty}$ norm case of the theorem in the overview, but the ideas generalize to any $\ell_p$.

The main idea is to first construct a map with slightly weaker guarantees. We initially give a construction of a (randomized) map $f: \R^d \rightarrow \R^{O(s)}$ such that
\begin{enumerate}
    \item $f$ preserves the $\ell_{\infty}$ norm of any pair in $X$ with probability $99\%$.
    \item $f$ is never expanding deterministically: $\|f(x) - f(y)\|_{\infty} \le \|x-y\|_{\infty}$ always.
\end{enumerate}
Before discussing the construction of $f$, let's quickly see why the two points are beneficial towards the final construction. The final construction simply concatenates $O( \log n)$ independent copies of $f$. Due to independence, with high probability, every pair $x,y$ will have at least one copy of $f$ which `certifies' the $\ell_{\infty}$ norm between them is at least $\|x-y\|_{\infty}$ (due to property (1)). Furthermore, since we know every copy of $f$ is non-expanding, we will never overestimate the distance  (due to property (2)). Putting together these two statements  implies the guarantees of our main theorem (for the $\ell_{\infty}$ case). 

Now we describe the construction for $f$ which satisfies the two properties listed above. Property (1) actually holds for the birthday paradox map. However, this will not ultimately work since it cannot guarantee property (2). (And more directly, our lower bound in Theorem \ref{thm:linear_lb} rules out all linear maps). Instead, we use a \emph{highly non-linear} map $f$. Similar to the birthday paradox map, we start by hashing all coordinates of the ambient dimension, $d$, to a set of $O(s)$ buckets. These buckets are the coordinates of the embedding. The crucial difference is that instead of summing the coordinates that land in a bucket, we take the \emph{maximum}, a very unintuitive operation. More precisely, for a sparse vector $x$, we look at the buckets where the support elements of $x$ land. Then for all buckets which are non-empty, we take the maximum of the support elements of $x$ that land in the bucket, while all empty buckets get $0$. 

The hashing step already guarantees property (1), similar to the birthday paradox upper bound. So it remains to check property (2). This reduces to checking the following: suppose coordinates $1,\ldots, k$ map to the same bucket under $f$. Let $x$ and $y$ be two non-negative sparse vectors. Then we want $|\max( x_1, \ldots, x_k) - \max(y_1, \ldots, y_k)| \le \max_i |x_i - y_i|$, where $x_i$ and $y_i$ are the $i$th entries of $x$ and $y$ (they may be $0$). We can check that this is sufficient to imply property (2). To show this claim, imagine all the coordinates $x_i$ and $y_i$ together on the real line. They must all be to the right of the origin due to the non-negativity constraint. The right most coordinate, say $x_1$ without loss of generality, is always closer to the rightmost coordinate among the $y$'s, than it is to $y_1$. So the claim follows.

This construction crucially relies on the non-negativity of the vectors. An explicit example where our proposed map fails for arbitrary sparse vectors is as follows: consider two sparse vectors $x = [-1, 0, \ldots, 0]$ and $y = [0, 1, 0, \ldots, 0]$. Clearly, $\|x-y\|_{\infty} = 1$. Now suppose the first two coordinates hash to the same bucket under $f$. Then the first coordinate of $f(x)$ will have coordinate $-1$, since we take the maximum of all the support elements of $x$ that land in the first bucket. In this trivial case, the bucket is a singleton. Similarly, the first coordinate of the embedding of $y$ will be $1$, so $\|f(x) - f(y)\|_{\infty} = 2$. Thus, property (2) does not hold as the distance expands. One could try to massage the map $f$ to fix this particular issue, but our Theorem \ref{thm:lb_nonneg} rules out the existence of \emph{any map} which preserves the distances between arbitrary sparse vectors in $\ell_{\infty}$ norm.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{images/linfcomparision.pdf}
\caption{A simple plot demonstrating the performance of our non-negative embedding (Theorem \ref{thm:final_non_negative}) versus the map of \cite{ZhuGR15}. The dots represent a $10$-sparse vectors in $\R^{1000}$ with non-zero entries chosen uniformly in $[0,1]$. The $x$-axis is the true $\ell_{\infty}$ norm and the $y$ axis is the approximated norm using the two different maps. Every vector has two dots (one for each map). We adapt both maps to embed to $\R^{50}$, but the performance is qualitatively similar for other $m$. We see that the performance of our map is demonstrably superior (it hugs the $y = x$ line).}
\label{figure:distortion}
\end{figure}


\subsection{Open Problems}

We note some interesting questions that follow naturally from our work.
\begin{enumerate}
    \item What is the right $\eps$ and $s$ dependency for sparse non-negative dimension reduction (Theorem \ref{thm:final_non_negative})? Can we improve upon the $\tilde{\Omega}(s)$ lower bound of Theorem \ref{thm:omega_s_lb} to also include $\eps$ factors? For example, for every $p \ge 1$, can we construct a set of $n$ non-negative $s$-sparse vectors such that any embedding $f$ to $m$ dimensions which preserves all pairwise $\ell_p$ distances up to distortion $1\pm \eps$ requires $m = \Omega(s\log(n)/\eps^2)$? Interestingly, such a lower bound cannot be true for the $p=2$ case since JL already gives us an upper bound of $O(\log n/\eps^2)$ (regardless if the vectors are sparse or dense), so we must look at other $p$'s. On the other hand, for the $p = 2$ case, the JL lower bound of \cite{alon2003problems} implies a $\Omega(\log(n)/(\eps^2 \log(1/\eps)))$ lower bound for arbitrary mappings (for a $1\pm \eps$ approximation) since their hard point set consists of basis vectors (i.e. $1$-sparse). Thus, we know something about the $\eps$ dependency for the $p=2$ case. Can we extend this lower bound to other $\ell_p$ norms? Note that our `average-case' lower bounds discussed in Section \ref{sec:avg_case_lb_overview} are not directly applicable since they rule out exact (not approximate) dimensionality reduction for a constant fraction of (not all) pairs of vectors.
    % \item Can we obtain $o(s^p)$ embedding dimension for preserving the norm general $s$-sparse vectors? Both our upper bound of Theorem \ref{thm:final_embedding_general} and the result of \cite{ZhuGR15} suffer a $s^{\Theta(p)}$ dependence in the embedding dimension (see Table \ref{tab:results_ub}). Our lower bound in Theorem \ref{thm:lb_nonneg} suggests that an exponential dependence on $p$ is likely necessary, since we rule out the existence of any non-trivial mapping for the $\ell_{\infty}$ case. Note that \cite{ZhuGR15} provide a $\Omega(s^p)$ lower bound, but only for lienar maps.
    \item Our `average-case' lower bound of Theorem \ref{thm:lb_avg_lp} states that the folklore birthday-paradox mapping of Definition \ref{def:birthdayparadox} is optimal for linear maps as well as sufficiently-smooth maps. It is an interesting question to prove a similar bound for arbitrary maps for the question of preserving $99\%$ of distances between sparse vectors. Note that for linear maps, preserving distances is equivalent to preserving norms (on the set of differences of vectors), but this is not the case for arbitrary maps. The question of only preserving norms is easy for arbitrary maps: there exists an upper bound of $1$ dimensions by just writing down the norm exactly, so the question of preserving distances is the natural one.
    
    \item What is the power of \emph{non}-linear embeddings in dimensionality reduction? Many \emph{oblivious} dimensionality reduction upper bounds in literature use linear maps. Our results demonstrate a separation between linear and non-linear maps in the natural case of non-negative sparse vectors, and it is intriguing to ask if such separations exist in other settings.
    \item In general, are there other natural dataset assumptions or relaxations that enable beyond worst case dimensionality reduction?
\end{enumerate}

% \section{Outline of the Appendix}
% Unfortunately due to space constraints, we are only able to describe our the results and provide an intuitive overview of our main ideas. Full details can be found in the appendix, which is organized as follows. Section \ref{sec:related_works} contains a discussion on related works. Section \ref{sec:avgcase} gives full proofs for lower bounds for Question (1). Section \ref{sec:sparse_nonneg} gives the full proofs for our upper bounds for Question (2). Section \ref{sec:nonneglb} proves that all of the hypothesis in our upper bound for embedding for non-negative sparse vectors are necessary. Lastly, Section \ref{sec:applications} gives novel algorithmic applications based on our embedding for non-negative sparse vectors.

\section{Related Works}\label{sec:related_works}
\paragraph{Lower-bounds for the JL Lemma}
Lower bounds for dimensionality reduction were introduced for understanding the minimum embedding dimension for $n$ vectors with at most $1+\eps $ multiplicative distortion. By using rank arguments of perturbed identity matrices, as outlined in Section \ref{sec:avg_case_lb_overview}, the first such lower bounds showed that the embedding dimension must satisfy $\Omega(\log(n)/(\eps ^2 \log(1/\eps )))$, even when the embedded vectors are simple basis vectors \cite{alon2003problems}. Furthermore, these bounds hold even when a non-linear or adaptive embedding function is applied; however they crucially depend on the \emph{maximum} distortion being smaller than $\eps $. These lower bounds were improved to an optimal $\Omega(\log(n)/\eps^2)$ bound for an oblivious or fixed linear map \cite{larsen2014johnson}, and then finally improved to any non-oblivious, non-linear embedding function by \cite{larsen2017optimality}. It is worth noting that the final construction is significantly different that that of previous works and does not use nearly orthogonal sparse vectors. Note that these lower bounds inherently rely on the assumption that the dot product of the embedding must approximately preserve the dot product, without any post-processing or decoding.

% In fact, as mentioned in their paper, nearly orthogonal vector sets exist in dimensions $m = o(\log(n)/\eps ^2)$, which beats the JL guarantee and can be constructed based on Reed-Solomon codes \cite{nelson2014deterministic}. 

\paragraph{Speeding up JL for Sparse Vectors}
There have been a number of works on speeding up the runtime for embedding a collection of $n$ sparse vectors using a JL map (while still embedding to $O(\log(n)/\eps^2)$ dimensions). \cite{kane2014sparser} demonstrates a distribution over sparse JL embedding matrices $\Pi$ such that $\Pi x$ takes $\tilde{O}(\|x\|_0/\eps)$ time to compute, where $\|x\|_0$ denotes the number of non-zero entries of a vector $x$.



\paragraph{Distributional Embedding Lower Bounds and JL}
Recall that the standard JL lemma states that for any $n$ vectors in $\mathbb{R}^d$, one can use a random Gaussian embedding to $O(\log(n)/\eps ^2)$ dimensions to guarantee the following maximum distortion bound with high probability: $(1-\eps ) \|x - y\|^2 \leq \|Ax - Ay\|^2 \leq (1+\eps )\|x-y\|^2$ for all $x, y \in V$, where $|V| = n$. Furthermore, this embedding is also oblivious. While it is also known that the JL lemma is tight in the worst case, we emphasize that even our average-case lower bounds towards Question (2) (see Section \ref{sec:q2}) are not implied by the existing JL lower bounds, even for the $p = 2$ case of Theorem \ref{thm:informal_lb_avg}. At a high level, this is because our hypothesis is much weaker (we only require a large fraction of the norm to be preserved, rather than all pairs). We elaborate below. 

JL embedding lower bounds state that for large enough $n$, there exists a \emph{specific} point set on $n$ points such that \emph{any map} $f$ which preserves all pairwise distances must map to $\Omega(\log(n)/\eps^2)$ dimensions \cite{larsen2017optimality}. The main difference between this lower bound and our lower bounds of Section \ref{sec:q2} are that the hypotheses assumed are different. The JL upper bound guarantee implies approximate norm preservation for \emph{every} pair of differences of points in a collection of $n$ points, simultaneously and with high probability. Similarly, the JL lower bound assumes approximate norm preservation for \emph{every} pair in a collection of $n$ vectors. On the other hand, the folklore birthday paradox upper bound assumes a fixed sparse vector as input, whose norm it preserves with constant probability. Similarly, our lower bounds of Section \ref{sec:q2} only assume approximate norm preservation only a constant fraction of the time across a uniform distribution of suitably chosen sparse vector inputs. Consequently, we have no term depending on the number of input vectors in the statement (both in the folklore upper bound and the lower bounds). 

There is also the \emph{distributional} JL lemma which is a random map from $\R^d \rightarrow \R^{O(1/\eps^2)}$ preserving the norm of any fixed vector in $\R^d$ up to a multiplicative $1\pm \eps$ with probability at least $99\%$ \cite{johnson1984extensions}. It is shown in \cite{jayram2013optimal} that the projection dimension of the distributional JL lemma is tight \emph{information theoretically}. This ostensibly seems to imply our lower bounds (e.g. the $p=2$ case of Theorem \ref{thm:informal_lb_avg}), for example if we parameterize the sparsity as $s = 1/\eps$, seemingly rendering our lower bounds obsolete. But this is not the case! The information-theoretic lower bound proved in \cite{jayram2013optimal} relies on \emph{dense} vectors in $\R^d$. This is not an artifact of the proof, but an inherent requirement to prove their information-theoretic lower bound: any such lower bound \emph{cannot} use sparse vectors. This is because information-theoretically, there is already a smaller projection dimension from compressed sensing: we can encode any sparse vector in $\tilde{O}(s)$ dimensions. Parameterizing the sparsity by $s = 1/\eps$, this implies an information-theoretic upper bound of $\tilde{O}(1/\eps)$, demonstrating that the lower bound of \cite{jayram2013optimal} is not applicable to sparse vectors. 



% The Johnson-Lindenstrauss (JL) lemma is a powerful dimensionality reduction technique stating that any set of high-dimensional data points can be embedded into a lower-dimensional space while approximately preserving their pairwise distances \cite{johnson1984extensions}. Specifically, we know that for $n$ vectors in $\mathbb{R}^d$, one can use a random Gaussian embedding to $O(\log(n)/\eps ^2)$ to guarantee the following maximum distortion bound with high probability: $(1-\eps ) \|x - y\|^2 \leq \|Ax - Ay\|^2 \leq (1+\eps )\|x-y\|^2$ for all $x, y \in V$, where $|V| = n$. Furthermore, this embedding is oblivious as the matrix distribution does not depend on the input vector set. This simple linear oblivious embedding allows for efficient processing and analysis of complex datasets without sacrificing accuracy and has played a pivotal role in reducing the computational complexity of tasks such as nearest neighbor search, clustering, and manifold learning \cite{vempala2005random}.



\paragraph{Compressed Sensing}
The field of compressed sensing studies decoding to recover encoded or compressed sparse vectors \cite{donoho2006compressed}. The restricted isometry property (RIP), introduced in \cite{candes2005decoding}, is a way to recover sparse vectors and focuses on bounding the RIP constant: the maximum distortion $D$ of a linear map $A \in \R^{m \times d }$ on all $s$-sparse vectors: $( \|x\|_p \leq \|Ax\|_p \leq D\|x\|_p$ for all $x$ with $x \in \R^d$ and $s$-sparse. The mappings under matrices with good RIP constants can serve as the compressed representation. Furthermore, matrices with such properties automatically give dimensionality reduction for $s$-sparse vectors. The prior known bounds for $m$ for RIP matrices with distortion $1 \pm \eps$ are $O(s \log (d/s)/\eps^2)$ for the $p = 1$ and $2$ case \cite{candes2005decoding, berinde2008combining}. For other values of $p$, \cite{ZhuGR15} gave bounds of the form $\tilde{O}(s^p)$. If we only require to embed $n$ many $s$-sparse vectors, then one can calculate that \cite{ZhuGR15}'s bound can be converted to $s^2 (\log(n) p)^{O(p)}$, ignoring $\eps$ factors (see Table \ref{tab:results_ub}). Thus, both bounds suffer an exponential dependence on $p$. \cite{ZhuGR15} further showed that $m = \Omega(s^p)$ if $A$ has a bounded RIP constant.

We note that if the RIP constant is small enough, then applying a linear programming decoding via $\ell_1$ minimization recovers the sparse vectors when compressed to the informational-theoretical optimal $O(s\log(d))$ dimensions, with a matching lower bound \cite{li2020lower, mai2023optimal}. There are other alternatives, e.g. based on CountSketch \cite{charikar2002finding, price20111+}, but all these decoders solve a complex optimization problem. Moreover, these mappings do not serve as embeddings/dimensionality reduction (e.g. the $\ell_p$ of the original vector is not approximated by the $\ell_p$ norm of the mapping).



% For $\eps $-approximate sparse recovery with non-sparse outputs, usage of CountSketch embeddings \cite{charikar2002finding} has proven useful to provide $O(s\log(d)/\eps  + s\log(s/\eps )/\eps ^2)$ algorithms for $\ell_2$-sparse recovery \cite{}, with nearly matching lower bounds \cite{mai2023optimal}.
% \textcolor{red}{TODO: talk about RIP matrices here ... and sparse recovery}

\paragraph{Role of Sparsity in Machine Learning}
Primitives for sparse vectors, such as sparse matrix multiplication, is fundamental in a wide range of domains, such as graph analytics and scientific computing, and is used as iterative algorithms for sparse matrix factorization. Moreover, the field of deep learning often relies on faster sparse kernels to demonstrate speedups in practice, as it is a core operation in graph neural networks, transformers, and other architectures \cite{child2019generating, hoefler2021sparsity}.  On the theoretical side, there are also many works which seek to understand the role of sparsity in computational hardness for optimization problems such as sparse regression \cite{Natarajan95,davis1997adaptive,Mahabadi15,FosterKT15,Har-PeledIM18,ChenYW19,gupte2021finegrained, price2022hardness}. 

\paragraph{Non-linear dimensionality reduction} One particularly interesting facet of our upper bound for embedding non-negative sparse vectors is that the mapping we give is non-linear. This is an example of an \emph{oblivious} dimensionality reduction map (a mapping that does not depend on the input dataset) with this property. Oblivious maps include for instance JL matrices and RIP matrices, which are linear maps. We would like to highlight that non-linear dimensionality reduction results have found many applications, such as modifications of the JL lemma to $\ell_2$ distances raised to fractional powers \cite{gottlieb2015nonlinear, bartal2011dimensionality}, terminal embeddings for $\ell_2$ \cite{cherapanamjeri2024terminal, narayanan2019optimal, mahabadi2018nonlinear, elkin2016terminal}, and finding the optimal way to embed a given set of vectors in euclidean space \cite{grotschel2012geometric}. The last two of the aforementioned applications crucially adapt the mapping to a given input point set and are not oblivious.




% \section{Embedding for sparse vectors}\label{sec:sparse_general}

% We prove Theorem \ref{thm:final_embedding_general} in this section and give an embedding for sparse vectors. Later in Section \ref{sec:sparse_nonneg}, we give an embedding tailored to non-negative sparse vectors.

% Our embedding is constructed in two parts. In the first part, we consider the birthday paradox linear mapping $A: \R^d \rightarrow \R^m$ given in Definition \ref{def:birthdayparadox}. It is a random mapping which preserves \emph{all} $\ell_p$ norms of a \emph{fixed} pair of sparse vectors with constant probability. Furthermore, it will have bounded expansion. Our final embedding is formed by stacking many independent copies of our base mappings. Our final embedding is the following.
% \begin{definition}[Final Embedding]\label{def:final_embedding_general}
% A $(d, m, T)$ embedding $F: \R^d \rightarrow \R^{mT}$ is defined as follows. For every $1 \le i \le T$, let $A_i: \R^d \rightarrow \R^{m}$ be an independent copy of the random mapping of Definition \ref{def:birthdayparadox}. $F$ is defined by stacking the matrices $A_i$ on top of each other (note that $F$ is linear), and dividing by $T^{1/p}$. In other words,
% \[ Fx = \frac{1}{T^{1/p}} \cdot \bigoplus_{i = 1}^T A_ix.\]
% \end{definition}

% First we show the birthday paradox map has bounded distortion.

% \begin{lemma}\label{lem:base_general_properties}
%     Let $A: \R^d \rightarrow \R^{100s^2/\delta}$ be the mapping of Definition \ref{def:birthdayparadox}. For any $s$-sparse vector $x \in \R^d$ independent of $A$, 
%     \[ \Pr(\forall p, \, \|Ax\|_p = \|x\|_p) \ge 1-\delta. \]
%     For every $s$-sparse vector $x, \|Ax\|_p \le s^{1-1/p}\|x\|_p$ deterministically for all $p \ge 1$.
% \end{lemma}
% \begin{proof}
%    Consider the columns of $A$ corresponding to the support of $x$. There are only at most $s$ such columns. The probability that the non-zero rows of any two of these columns are the same is at most 
%    \[ \binom{2s}{2} \cdot \frac{1}{(100s^2/\delta)} < \delta. \]
%    Thus with probability at least $1-\delta$, the non-zero entries of $Ax$ are exactly the non-zero entries of $x$. Under this event, the value of any $\ell_p$ norm (or any coordinate symmetric function) is preserved.
   
%    The second statement follows from the following fact: for any real numbers $x_1, \ldots, x_k$ and $p \ge 1$, we have
%    \begin{equation}\label{eq:convexity}
%        (x_1 + \ldots + x_k)^p \le k^{p-1}(|x_1|^p + \ldots + |x_k|^p) 
%    \end{equation}
%    due to convexity.
% \end{proof}

% We now prove the main theorem of the section. At a high level, we can compensate for the (bounded) distortion of the birthday paradox map by concatenating multiple independent copies. 

% \begin{theorem}\label{thm:final_embedding_general}
%      Let $p \ge 1$ and $F: \R^d \rightarrow \R^{s^{p+2}\log(s)\log(d/\eps)2^{O(p)}/\eps^2}$ be a $(d, m, T)$ embedding as stated in Definition \ref{def:final_embedding_general} for $m = O( 2^{O(p)} s^2/\eps)$ and $T = O( \log(s)\log(d/\eps) 2^{O(p)}s^{p}/\eps)$. Then 
%     \[\Pr\left( \forall \textup{ $s$-sparse } x \in \R^d, \,  \|Fx\|_p = (1 \pm \eps) \|x\|_p  \right) \ge 0.99. \]
% \end{theorem}
% \begin{proof}
%     Let $A_1, \ldots, A_T$ be the linear maps $A_i: \R^d \rightarrow \R^m$ for $F$ where $T = \log(s)\log(d/\eps) 2^{O(p)}s^{p}/\eps$ and $m = 2^{O(p)} s^2/\eps$. Consider a fixed $x$ for now.  First, note that each $i$ satisfies $\|A_ix\|_p = \|x\|_p$ with failure probability at most $s^2/m \le \eps$. Then with failure probability at most $\exp(-\Omega(T \eps))$, at least $1-10 \eps$ fraction of the $A_i$'s satisfy $\|A_i x\|_p = \|x\|_p$. Condition on this event. Then,
% \[ \|Fx\|_p^p \ge (1-10 \eps)\|x\|_p^p.\]
% Now for $t = 2^{k}, k \ge 0$, let $\mathcal{E}_t^i$ be the event that at least $t$ \emph{and} at most $2t$ non-zero coordinates of $x$ collide together under $A_i$. We have  
%    \[\Pr(\mathcal{E}_t^i) \le \binom{s}t \cdot \frac{1}{m^{t-1}} := p_t. \]  
% Since $t \ge 2$, we have
% \begin{align*}
%     p_t (2t)^{p-1} &= \binom{s}t \cdot \frac{(2t)^{p-1}}{m^{t-1}} \le m \left( \frac{es}{tm} \right)^t (2t)^{p-1} \\
%     &\le \frac{\eps^{t-1}}{s^{t-2}} \cdot \frac{e^t (2t)^{p-1}}{2^{100p(t-1)}} \\
%     &\le \eps^{t-1} e^t \cdot \frac{(2t)^{p-1}}{2^{100p(t-1)}} \\
%     &\le \eps^{t-1} e^t \left( \frac{(2t)^{\frac{1}{t-1}}}{2^{100}} \right)^{p(t-1)} \\
%  & \le \eps/2^{t}.
%     \end{align*}
% Now due to the independence of the different $A_i$'s, the probability that at least $ O\left( \frac{T \eps}{(2t)^{p-1}} \cdot \frac{1}{\min \left( 2^t, \log s \right)}\right)$ of the $A_i$'s satisfy $\mathcal{E}_i^t$ is at most 
% \[\exp\left(-\Omega\left(\frac{T \eps}{(2t)^{p-1 }\log s}\right)\right) \le \left(\frac{\eps}d \right)^{1000s}\]
% by the Chernoff bound and adjusting constants. Thus with failure probability at most $(\eps/d)^{100s}$, for every $t$ simultaneously, the number of $A_i$'s that satisfy $\mathcal{E}_t^i$ is at most $O\left( \frac{T \eps}{(2t)^{p-1}} \cdot \frac{1}{\min \left( 2^t, \log s \right)}\right)$. Conditioning on this, we have
% \[
%      \|Fx\|_p^p \le (1 - 10 \eps)\|x\|_p^p + \sum_{t \ge 2} (2t)^{p-1} O\left( \frac{\eps}{(2t)^{p-1}}  \cdot \frac{1}{\min \left( 2^t, \log s \right)}\right) \|x\|_p^p,
% \]
%    where the outer $(2t)^{p-1}$ factor is due to the same argument as in Equation \ref{eq:convexity}. Now since there are at most $O(\log s)$ terms in the above sum,
% \[  \sum_{t \ge 2} (2t)^{p-1} O\left( \frac{\eps}{(2t)^{p-1}}  \cdot \frac{1}{\min \left( 2^t, \log s \right)}\right) \le \sum_{t \ge 2} (2t)^{p-1} O\left( \frac{\eps}{(2t)^{p-1}}  \cdot \left( \frac{1}{2^t} + \frac{1}{\log s} \right)\right) = O(\eps), \]
% and thus with failure probability at most $(\eps/d)^{100s}$,
% \[  \|Fx\|_p^p \le (1 - 10 \eps)\|x\|_p^p + O(\eps) \|x\|_p^p \le (1 + O(\eps))\|x\|_p^p. \]
% So far we have considered a single fixed $x$. We now apply this reasoning to a large collection via the union bound. Let $\mathcal{N}$ be an $\gamma$-net in $\ell_p$ norm of all $s$-sparse vectors $z$ in $\R^d$ satisfying $\|z\|_p = 1$ with $\gamma = \eps^2/d^2$. We have that $|\mathcal{N}| \le (d/\eps)^{50s}$. By our choice of $T$ and the above calculations, we know that 
% \[(1-10\eps) \|z\|_p^p \le \|Fz\|_p^p \le (1+10\eps)\|z\|_p^p\]
% for all $z \in \mathcal{N}$ simultaneously with probability at least $0.99$. By taking the $1/p$th power, we also have 
% \begin{equation}\label{eq:sandwich}
%    \forall z \in \mathcal{N}, \, (1-10\eps) \|z\|_p   \le \|Fz\|_p \le (1+10\eps)\|z\|_p
% \end{equation}
% where we have used the fact that $1+10\eps \ge  (1+10\eps)^{1/p}$ and similarly, $(1-10\eps)^{1/p} \ge 1-10\eps$ since $p \ge 1$ and $\eps \in (0,1)$. We now extend the guarantees of Equation \eqref{eq:sandwich} for \emph{all} $s$-sparse $y \in \R^d$.

% Since $F$ is linear, we can without loss of generality assume $\|y\|_p = 1$ by scaling. Pick $z \in \mathcal{N}$ that satisfies $\|z - y\|_{p} \le \gamma$. We know $\|z\|_p = 1$ and $|\|Fz\|_p - 1| \le 10\eps$. Then 
% \[ |1 - \|Fy\|_p| \le | \|Fz\|_p - \|Fy\|_p | + 10\eps  \le \|F(z-y)\|_p + 10\eps, \]
% and 
% \[ \|F(z-y)\|_p^p \le \gamma^p \max_i \|A_i\|_p^p \le  \gamma^p \max_i( \|A_i\|_{\infty}^{p-1} \|A_i\|_1) \le  d \gamma^p\]
% (where we are using the induced matrix norm). Hence, 
% \[|\|Fy\|_p - 1| =  |\|Fy\|_p - \|y\|_p| \le 10\eps + d^{1/p} \gamma \le 11\eps = 11 \eps \|y\|_p, \]
% completing the proof. The theorem statement follows by adjusting the value of $\eps$.
% \end{proof}

% \begin{theorem}\label{thm:final_embedding_general}
%     Let $p \ge 1$ and $F: \R^d \rightarrow \R^{s^{2p+1}\log(d/\eps)/\eps^2}$ be a $(d, O(s^{p+1}/\eps), O(s^p \log(d/\eps)/\eps))$ embedding as stated in Definition \ref{def:final_embedding_general}. Then 
%     \[\Pr\left( \forall \textup{ $s$-sparse } x \in \R^d, \,  \|Fx\|_p = (1 \pm \eps) \|x\|_p  \right) \ge 0.99. \]
% \end{theorem}
% \begin{proof}
% Let $A_1, \ldots, A_T$ be the linear maps $A_i: \R^d \rightarrow \R^m$ that comprise $B$ where $T = O(s^p \log(d/\eps)/\eps)$ and $m = O(s^{p+1}/\eps)$. Consider a fixed $x$ for now. We have 
% \[ \|Fx\|_p^p = \frac{1}T \cdot \sum_{i=1}^T \|A_ix\|_p^p.\]
% Each fixed $A_i$ satisfies $\|Ax_i\|_p = \|x\|_p$ with failure probability at most $\delta := O(s^2/m)$. Due to independence of the $A_i$'s, we have that with failure probability at most $\exp(-\Omega(T \delta))$, at least $1-10 \cdot \delta$ fraction of the $A_i$'s satisfy $\|A_i x\|_p = \|x\|_p$. Condition on this event. Then,
% \[ \|Fx\|_p^p \ge (1-10 \delta)\|x\|_p^p ,\]
% and
% \[ \|Fx\|_p^p \le (1-10\delta) \|x\|_p^p + 10\delta  s^{p-1}\|x\|_p^p \le (1+10\delta s^{p-1}) \|x\|_p^p,\]
% from Lemma \ref{lem:base_general_properties}. By our choice of $\delta$, it follows that 
% \[ (1-10\eps) \|x\|_p^p \le \|Fx\|_p^p \le (1+10\eps) \|x\|_p^p. \]
% So far we have considered a single fixed $x$. We now apply this reasoning to a large collection via the union bound. Let $\mathcal{N}$ be an $\gamma$-net in $\ell_p$ norm of all $s$-sparse vectors $z$ in $\R^d$ satisfying $\|z\|_p = 1$ with $\gamma = \eps^2/d^2$. We have that $|\mathcal{N}| \le (d/\eps)^{100s}$. By our choice of $T$ and the above calculations, we know that 
% \[(1-10\eps) \|z\|_p^p \le \|Fz\|_p^p \le (1+10\eps)\|z\|_p^p\]
% for all $z \in \mathcal{N}$ simultaneously with probability at least $0.99$. By taking the $1/p$th power, we also have 
% \begin{equation}\label{eq:sandwich}
%    \forall z \in \mathcal{N}, \, (1-10\eps) \|z\|_p   \le \|Fz\|_p \le (1+10\eps)\|z\|_p
% \end{equation}
% where we have used the fact that $1+10\eps \ge  (1+10\eps)^{1/p}$ and similarly, $(1-10\eps)^{1/p} \ge 1-10\eps$ since $p \ge 1$ and $\eps \in (0,1)$. We now extend the guarantees of Equation \eqref{eq:sandwich} for \emph{all} $s$-sparse $y \in \R^d$.

% Since $F$ is linear, we can without loss of generality assume $\|y\|_p = 1$ by scaling. Pick $z \in \mathcal{N}$ that satisfies $\|z - y\|_{p} \le \gamma$. We know $\|z\|_p = 1$ and $|\|Fz\|_p - 1| \le 10\eps$. Then 
% \[ |1 - \|Fy\|_p| \le | \|Fz\|_p - \|Fy\|_p | + 10\eps  \le \|F(z-y)\|_p + 10\eps, \]
% and 
% \[ \|F(z-y)\|_p^p \le \gamma^p \max_i \|A_i\|_p^p \le  \gamma^p \max_i( \|A_i\|_{\infty}^{p-1} \|A_i\|_1) \le  d \gamma^p\]
% (where we are using the induced matrix norm). Hence, 
% \[|\|Fy\|_p - 1| =  |\|Fy\|_p - \|y\|_p| \le 10\eps + d^{1/p} \gamma \le 11\eps = 11 \eps \|y\|_p, \]
% completing the proof. The theorem statement follows by adjusting the value of $\eps$.
% \end{proof}




\section{Average-Case Lower Bounds for Embedding General Sparse Vectors}\label{sec:avgcase}

The goal of this section is to prove lower bounds showing that the birthday paradox map of Definition \ref{def:birthdayparadox} is optimal in many settings. The point set we use are generated randomly from the following distribution as discussed in Section \ref{sec:technical_overview}.

\unif*

Our first result is to show that any linear map with the same average-case guarantees as the birthday paradox map must map to $\Omega(s^2)$ dimensions. Note that the birthday paradox map satisfies the hypothesis of the theorem statement below up to constant factors (by mapping to $Cs^2$ dimensions for a sufficiently large constant $C$). This is because any fixed set of coordinates of size at most $s$ hashes to unique buckets under the map with probability say at least $99\%$ (by picking large enough $C$). If this is the case, then it does not matter what entries we put in this set of coordinates.

\begin{theorem}\label{thm:lb_avg_lp}
     Let $A: \R^{s^2} \rightarrow \R^m$ be a linear map and $p \ge 2$ be an even integer. If $A$ is  such that for any $1 \le t \le s$,
    \[ \Pr_{u\sim \textup{Unif}_t}\left( \|Au\|_p =\|u\|_p \right) \ge 0.99,\]
    then $m \ge s^2/1000$.
\end{theorem}

\begin{proof}
    Suppose for the sake of contradiction that $m < s^2/1000$. The proof is roughly divided into three parts. The first part shows that $A^TA$ has many `non-zero' coordinates. The second part shows that a random sparse vector $u$ (as chosen in the hypothesis of the theorem statement) has `many' pairs of coordinates $u_i$ and $u_j$ such that the corresponding $(i,j)$ entry in $A^TA$ is also non-zero. The last part then shows that the prior result implies that $A$ \emph{does not} (approximately) preserve the norm of $u$ with a large constant probability, contradicting the assumption of the theorem. The last part relies on the fact that the zero set of a non-zero polynomial has measure $0$.
\\

    \noindent \textbf{Part $\#1$.} Let $A_1, \ldots, A_{s^2}$ denote the columns of $A$. Let $v$ be a random vector chosen from $\textup{Unif}_1$, and let $i$ be its support with $v_i$ the corresponding non-zero entry. Then $\|Av\|_p^p = \|A_i\|_p^p \cdot v_i^p$ and $\|v\|_p^p = v_i^p$. Then the hypothesis implies that 
    \begin{equation}\label{eq:1}
        \Pr(|\|A_i\|_2^2 -1| \le 0.00001) \ge 0.99.
    \end{equation}
    (Note that we deliberately work with a weaker hypothesis since it will be more useful to us later on). 
    Thus, a $0.99$ fraction of the columns of $A$ have Euclidean norm in the range $[0.9999, 1.0001]$. Let $A_X$ be the restriction to such columns. Note that all diagonal entries of $A_X^TA_X$ are at in the range $[0.999, 1.001]$ and all entries are bounded by $1.001$ in absolute value (via Cauchy-Schwarz on the columns of $A_X$). For $i \ge 1$, we let $\ell_i$ denote the number of \emph{non-diagonal} entries of $A_X^TA_X$ whose absolute values are in $(2^{-i}, 2^{-i+1}]$ and $\ell_0$ denote the rest of the non-diagonal entries (with absolute values in $(1, 1.001]$). We show the following claims hold.
    \begin{enumerate}[\alph*)]
        \item $\|A_X\|_F^2 \ge (0.99)^2s^2$,
        \item $\|A_X^T A_X\|_F^2 \ge \frac{(.99)^4s^4}m$,
        \item $\sum_{i \ge 0} \ell_i 2^{-2i+2} \ge \frac{(.99)^4s^4}m - 1.001 s^2$.
    \end{enumerate}
 Claim $(a)$ readily follows from inequality \eqref{eq:1}. To show $(b)$, note that singular values of $A_X^TA_X$ are the squared singular values of $A_X$ so $\|A_X^T A_X\|_F^2 = \sum_{i=1}^m \sigma_i(A_X)^4$ (since the rank of $A$ is at most $m$).
    By Cauchy–Schwarz, 
\[
    m  \cdot \|A_X^TA_X\|_F^2 = m \sum_i \sigma_i(A_X)^4 \ge \left( \sum_i \sigma_i(A_X)^2 \right)^2 = \|A_X\|_F^4 \ge (.99)^4s^4.
\]

    To show (c), note that all diagonal entries of $A_X^TA_X$ are at in the range $[0.999, 1.001]$. Claim (c) then follows from using the lower bound of Claim (b) since there are at most $s^2$ diagonal entries.
\\

    \noindent \textbf{Part $\#2$.} Now let $u$ be a vector drawn from $\textup{Unif}_s$ and let $T$ be its support.
    The hypothesis of the theorem states that 
    \begin{equation}\label{eq:2}
        \Pr(\|Au\|_p = \|u\|_p) \ge 0.99.
    \end{equation}
    Our goal is to demonstrate a contradiction by showing $ \Pr(\|Au\|_p \ne \|u\|_p) \ge 0.02$, which contradicts \eqref{eq:2}.
    
    Let $u_X$ be the restriction of $u$ to the coordinates in $X$ (i.e., only keep the coordinates in $X$). Consider $u_X u_X^T$. By our choice of $u$, the non-zero entries of $u_Xu_X^T$ lie on a random principal submatrix of size $|T \cap X| \times |T \cap X|$. We show that with a sufficiently large constant probability, both $u_X u_X^T \in \R^{|X| \times |X|}$ and $A_X^TA_X\in \R^{|X| \times |X|}$ have a non-zero value in `many' shared entries.

    Towards this end, let $Y_{ij}$ be the indicator variable for the entry $(i,j)$ in $u_X u_X^T$ being non-zero and let $s_{ij}$ denote the \emph{squared} $(i,j)$ entry of $A_X^T A_X$. 
    We know 
    \[\E[Y_{ij}] = \frac{s(s-1)}{s^2(s^2-1)} = \frac{1}{s(s+1)}.\]
    Finally, let $Z = \sum_{i,j} s_{ij}Y_{ij}$. Recalling our partitions $\ell_k$ that we defined earlier and Claim (c), we have
   \[\E[Z] = \sum_{i,j} s_{ij}\E[Y_{ij}] \ge \frac{1}{s(s+1)} \sum_k \ell_k 2^{-2k} \ge \frac{1}{4s(s+1)} \left( \frac{(.99)^4s^4}m - 1.001 s^2 \right) > 200.
\]
 We want to show $Z$ concentrates well around its mean. Towards this end, we bound $\E[Z^2]$. We have 
\[ Z^2 = \left( \sum_{i,j}s_{ij}Y_{ij} \right)^2 = \sum_{i,j} s^2_{ij} Y_{ij} + \sum_{i,j,j'} s_{ij}s_{ij'}Y_{ij}Y_{ij'} + \sum_{i,j,i',j'} s_{ij}s_{i'j'}Y_{ij}Y_{i'j'}.
\]
Since $s_{ij}^2 \le 1.001$, the first term can be bounded as
    \[\E\left[  \sum_{i,j} s^2_{ij} Y_{ij} \right] \le \frac{1.001}{s(s+1)} \sum_{i,j} s_{ij} = 1.001\E[Z]. \]
    Similarly, the third term can be bounded as 
    \begin{align*}
    &\E\left[ \sum_{i,j,i',j'} s_{ij}s_{i'j'}Y_{ij}Y_{i'j'} \right] =  \sum_{i,j,i',j'} s_{ij}s_{i'j'}\left( \frac{1}{s(s+1)} \right)^2\\
    &\le \left( \sum_{i,j} \frac{s_{ij}}{s(s+1)} \right)^2 = \left( \E[Z] \right)^2.
    \end{align*}

    It remains to bound the second term. We know $\E[Y_{ij}Y_{ij'}] \le 1/s^3$. For a row $i$ of $A_X^TA_X$, let $Z_k(i)$ denote the number of entries in that row which are in level $\ell_k$. Then the second sum is
    \begin{align*}
    \E\left[ \sum_{i,j,j'} s_{ij}s_{ij'}Y_{ij}Y_{ij'} \right] &\le s \sum_i \sum_{j, j'} \frac{s_{ij}s_{ij'}}{s^4} \\
    &\le  4s \sum_i \sum_{k,k'} \frac{Z_k(i)Z_{k'}(i)}{s^4 2^{2k} 2^{2k'}}\\
    &= 4s \sum_i \left( \sum_k \frac{Z_k(i)}{s^2 2^{2k}} \right)^2.
    \end{align*}

    Let $t_i = \sum_k Z_k(i)2^{-2k}$.
    We now consider the following two cases.
\\

\noindent \textbf{Case 1}: At least half of the $t_i$'s are at least $s$. In this case, we note that 
\[\E[Z] \ge \sum_i \sum_k \frac{Z_k(i)}{2^{2k}s^2} = \sum_i \frac{t_i}{s^2} \ge \frac{|X|}{2s} \ge 0.495s.  \]
We have
\[ 4s \sum_i \left( \sum_k \frac{Z_k(i)}{s^2 2^{2k}} \right)^2 \le 4s \sum_i \sum_k \frac{Z_k(i)}{s^2 2^{2k}} \le 4s \E[Z] \]
where we have used the fact that $\sum_k Z_k(i) \le s^2$. So altogether,
\[\E[Z^2] \le 5s\E[Z] + \E[Z]^2 \le 12\E[Z]^2.
\]
Thus the Paley–Zygmund inequality implies that 
\[ \Pr(Z \ge 1) \ge (1 - 0.01)^2 \cdot \frac{\E[Z]^2}{\E[Z^2]} \ge \frac{1}{20}.  \]

\noindent \textbf{Case 2}: At least half of the $t_i$'s are at most $s$. In this case, let $A_{X'}^TA_{X'}$ be $A_X^TA_X$ restricted to the rows where $t_i \le s$. We know $|X'| \ge 0.5|X|$. Our goal is to show that $A_{X'}^TA_{X'}$ still has at least `many' non-zero off-diagonal entries. 

The proof is identical to the proof of the three claims $(a),(b),(c)$ above. All sums below only pertain to the matrix $A_{X'}^TA_{X'}$. Indeed, we know $\|A_{X'}\|_F^2 \ge 0.495s^2$ so
\[m  \cdot \|A_{X'}^TA_{X'}\|_F^2 = m \sum_i \sigma_i(A_{X'})^4 \ge \left( \sum_i \sigma_i(A_{X'})^2 \right)^2 = \|A_{X'}\|_F^4 \ge (.495)^2s^4.    
\]

Combining with the fact that all non-diagonal entries are bounded by $1.001$ in absolute value, the above inequality implies that $\sum_{i \ge 0} \ell_i 2^{-2i+2} \ge (.495)^2s^4/(1.001m) - s^2 \ge 50s^2$. Furthermore, for a row $i$ of $A_{X'}^TA_{X'}$, 
\[ \sum_k \frac{Z_k(i)}{s^2 2^{2k}} = \frac{t_i}{s^2} \le \frac{1}s, \]
so we can bound
\begin{align*}
4s \sum_i \left( \sum_k \frac{Z_k(i)}{s^2 2^{2k}} \right)^2 &\le 4\sum_i \sum_k \frac{Z_k(i)}{s^2 2^{2k}} \\
&= 4 \sum_i \frac{t_i}{s^2} \le 4\E[Z].    
\end{align*}
Thus,
\[\E[Z^2] \le 6\E[Z] + \E[Z]^2.
\]
So by Payely-Zygmund,
\[ \Pr(Z \ge 1) \ge (1 - 0.01)^2 \cdot \frac{\E[Z]^2}{\E[Z^2]} \ge \frac{1}{20}.  \]
Thus we see that $Z \ge 1$ with probability at least $.05$ in both cases.
\\


\noindent \textbf{Part $\#3$.} Recalling that $T$ is the support set of $u$, we have $Au\sum_{t \in T} u_tA_t$ and thus,

\begin{equation}
\left \|\sum_{t \in T} u_tA_t \right \|_p^p - \|u\|_p^p = \sum_{ i_1, \ldots, i_p} u_{i_1} \cdots u_{i_p} \left( \sum_{j \in [m]} A_{i_1}(j) \cdots A_{i_p}(j) \right) - \sum_{t \in T} u_t^2 := P(u),
\end{equation}
where the outer sum is over all $p$-sized tuples of indices (with repeats allowed), the notation $A_{i_1}(j)$ denotes the $j$th entry of the column $A_{i_1}$, and the inner sum is over all the entries of the column. 

We claim that if $p$ is an even integer, then $P(u)$ is a non-zero polynomial. To do so, we demonstrate a monomial $u_{i_1} \cdots u_{i_p}$ with a non-zero coefficient. Now if we assume $Z \ge 1$, then this implies there exists two columns say $A_1$ and $A_2$ of $A$ such that $\langle A_1, A_2 \rangle$ is non-zero and both $1$ and $2$ are in the support set $T$ of $u$. We consider the following different cases for $p$.

If $p = 2$, consider the monomial $u_1u_2$. It's coefficient in $P$ is $\sum_j A_1(j)A_2(j) = \langle A_1, A_2 \rangle \ne 0$, so we are done. Now if $p > 2$ and even, instead consider the monomial $u_1^{p-2}u_2^2$. We claim that this is non-zero. Indeed, since $p-2$ is also even, $\sum_j A_1(j)^{p-2} A_2(j)^{2}$ must be non-zero since it is a sum of non-negative terms, one of which is non-zero (since at least one $j$ satisfies $A_1(j)A_2(j) \ne 0$). So we are also done.

Now finally, we note that since the entries of $u$ in its support are picked from a continuous distribution and $P$ is non-zero, the probability of the event $P(u) = 0$ is also $0$\footnote{e.g. see \cite{1920527}}. However, $\|Au\|_p = \|u\|_p$ implies $\|Au\|_p^p - \|u\|_p^p = P(u) = 0$. Overall, with probability at least $0.05$ (the event that $Z \ge 1$), we have $P(u) \ne 0$, contradicting the hypothesis \eqref{eq:2}. Thus, $m \ge s^2/1000$, as desired.
\end{proof}



We extend the previous theorem to the case of \emph{approximate} norm preservation for the $\ell_2$ case.

\linearltwo*

\begin{proof}
    Suppose for the sake of contradiction that $m < s^2/1000$. The proof is roughly divided into three parts as in Theorem \ref{thm:lb_avg_lp}. The first part shows that $A^TA$ has many `non-zero' coordinates. The second part shows that a random sparse vector $u$ (as chosen in the hypothesis of the theorem statement) has `many' pairs of coordinates $u_i$ and $u_j$ such that the corresponding $(i,j)$ entry in $A^TA$ is also non-zero. The last part then shows that the prior result implies that $A$ \emph{does not} (approximately) preserve the norm of $u$ with a large constant probability, contradicting the assumption of the theorem. The last part relies on bounding the probability that a random polynomial lies in an unexpectedly small interval. Since the first two parts are identical, we just present the last part. 

    Recall the random variable $Z$ defined in the proof of Theorem \ref{thm:lb_avg_lp}. There we showed that $Z \ge 1$ with probability at least $0.05$.

\noindent \textbf{Part $\#3$.} Recalling that $T$ is the support set of $u$, we have $\|Au\|_2^2 - \|u\|_2^2 = \left \|\sum_{t \in T} u_tA_t \right \|_2^2 - \|u\|_2^2$.

\begin{equation}
\left \|\sum_{t \in T} u_tA_t \right \|_2^2 - \|u\|_2^2 = \sum_{t \ne t' \in T} u_t u_{t'} \langle A_t, A_{t'} \rangle + \sum_{t \in T} \epsilon_t u_t^2 := P(u)
\end{equation}


where $|\epsilon_t| \le 0.001$.
If we pick the entries of $u$ to be standard Gaussians, we have
\[ \E[P(u)] = \sum_t \epsilon_t.\]
Recalling that $Y_{ij}$ is the indicator for $u_X^Tu_X$ having a non-zero entry at $(i,j)$, we have
\[ \E[P(u)^2] \ge \sum_{i,j} s_{ij}Y_{ij} + \sum_{t \ne t' \in T} \epsilon_t \epsilon_{t'} + \sum_{t \in T} 3 \epsilon_t^2 \]
so 
\[\textup{Var}(P(u)) = \E[P(u)^2] - (\E[P(u)])^2 \ge \sum_{i,j} s_{ij}Y_{ij} = Z.  \]
Thus, if we assume that $Z \ge 1$, then Lemma \ref{lem:poly_anti_concentration} implies that if the variables of $u$ in the support $T$ are picked from the standard Gaussian distribution, then the probability that $P(u) \in [-\eps, \eps] = O(\sqrt{\eps})$ for any sufficiently small $\eps > 0$. Now consider the following three events.
\begin{itemize}
    \item $\mathcal{E}_1$ = event that $\|u\|_2^2 \le 100s$. 
    \item $\mathcal{E}_2$ = event that $Z \ge 1$.
    \item $\mathcal{E}_3$ = event that $P(u) \not \in [-c, c]$ for a sufficiently small constant $c$.
\end{itemize}
By picking $c$ to be a small enough constant, we know that \emph{all} the events hold with probability at least $0.02$. Let's condition on all of these events holding. If so, we show that the condition $|\|Au\|_2^2 - \|u\|_2^2| \le \gamma \|u\|_2^2$ cannot hold. Indeed, the condition directly implies that $\|Au\|_2^2 - \|u\|_2^2 = P(u)$ lies in an interval strictly contained in $[-c, c]$. Thus altogether, with probability at least $0.02$, we have $|\|Au\|_2^2 - \|u\|_2^2| > \gamma \|u\|_2^2$, contradicting inequality \eqref{eq:2}. This finishes the proof.
\end{proof}

We now extend the result of the prior theorem to a more general class of mappings. We recall the general class that we consider, as described in Section \ref{sec:our_contributions}. We let $f: \R^{s^2} \rightarrow \R^m$ where $f(x) = (f_1(x), \ldots, f_m(x))$ and assume each $f_i: \R^{s^2} \rightarrow \R$ is twice differentiable with continuous second partial derivatives. 

At a high level, the level of smoothness assumed allows us to consider a taylor expansion, where we approximate each $f_i$ using a linear function up to some quadratic error. By taking the expansion sufficiently close to the origin, the quadratic error becomes negligible, thereby reducing the problem to the linear case. Crucially, we use the fact that our sparse vectors are drawn from a `scale invariant' distribution, in the sense that scaling a vector sampled from $\textup{Unif}_{t,r}$ is equivalent to sampling from $\textup{Unif}_{t,r'}$ for an appropriate $r'$. The full details are given in the proof below.

\begin{restatable}{thm}{general}\label{thm:general}
Suppose $\gamma \le C/s$ for a sufficiently small constant $C > 0$. If $f: \R^{s^2} \rightarrow \R^m$ as defined above is such that  for any $1 \le t \le s$ and any $r > 0$,
    \[ \Pr_{x\sim \textup{Unif}_{t,r}}\left(| \|f(x)\|_2^2 -\|x\|_2^2| \le \gamma \|x\|_2^2 \right) \ge 0.999,\]
    then $m \ge s^2/1000$.   
\end{restatable}

\begin{proof}
    Note that we may assume that $f_i(0) = 0$; otherwise our guarantees would trivially fail when $r \to 0$.
    Since the second partial derivatives of all $f_i$ which comprise $f$ are continuous, they are bounded in magnitude in $[-1,1]^{s^2}$, a compact set. Let $L$ be such an upper bound which holds for all $i$ (note that $L$ may depend on $s$). Now we let $c \ll 1$ be a sufficiently small value which will be determined shortly. For any fixed $f_i$, Taylor's theorem for multivariate functions\footnote{\url{https://en.wikipedia.org/wiki/Taylor's\_theorem\#Taylor's\_theorem\_for\_multivariate_functions}}
 implies that for any $x \in [-c,c]^{s^2}$, 

    \[ |f_i(x) - \langle \nabla f_i(0), x \rangle| \le L\left(x_1 + \ldots + x_{s^2}\right)^2 \le Ls^4 c^2  \]
    for every $i$. Let $A: \R^{s^2} \rightarrow \R^m$ be the matrix with $a_i$ as it's rows. The above inequality implies that 
    \[ \|f(x) - Ax\|_{\infty} \le c^2 s^4L. \]
Thus Lemma \ref{lem:deviation} implies that 
\begin{equation}\label{eq:norm_bound}
    | \|f(x)\|_2^2 - \|Ax\|_2^2 | \le c^3 \cdot \text{poly}(s, L) 
\end{equation}
for all $x \in [-c,c]^{s^2}$ satisfying $ \|f(x)\|_2^2 \le 2\|x\|_2^2$. By picking $c$ sufficiently small, we can say the following:


% Let $\delta$ be the absolute value of the minimum non-zero entry of $A^TA$, which exists, and pick $c \ll \min(1/s^{1000}, \delta^{1000}, 1/L^{1000})$. Letting $\gamma:= \min(\delta^{100}, s^{-100})$, we can say the following:
    \begin{itemize}
        \item For any $x \in [-c,c]^{s^2}$ such that $|\|f(x)\|_2^2- \|x\|_2^2| \le \gamma \|x\|_2^2$, we also have $|\|Ax\|_2^2 - \|x\|_2^2| \le \gamma \|x\|_2^2 + c^3 \cdot \text{poly}(s,L)$,
        \item For any $x$ where $|\|Ax\|_2^2 - \|x\|_2^2| \le\gamma'\|x\|_2^2$ holds for some scalar $\gamma' > 0$, then for any scalar $r > 0$, we also have $|\|Ay\|_2^2 - \|y\|_2^2| \le \gamma' \|y\|_2^2$ where $y =r \cdot x$.
    \end{itemize}
    The first claim follows from inequality \eqref{eq:norm_bound}, and the second claim follows from the fact that $A$ is a linear map. Now note that by picking a large enough constant $\beta \gg 1$, we know that sampling from any $t$, $x \sim \textup{Unif}_{t,c/s^{\beta}}$ satisfies $\|x\|_2^2 \ge c^{2.5} /s^{\beta'}$ (for some other constant $\beta' > 0$) with probability $0.999$. Thus a sufficiently small choice of $c$ and large enough $\beta$ implies the following:

    \begin{itemize}
        \item For any $x \in [-c,c]^{s^2}$ such that $|\|f(x)\|_2^2- \|x\|_2^2| \le \gamma \|x\|_2^2$, we also have $|\|Ax\|_2^2 - \|x\|_2^2| \le 2\gamma \|x\|_2^2$,
        \item For any $t$, $x$ sampled from $\textup{Unif}_{t,c/s^{\beta}}$ is in $[-c, c]^{s^2}$ with probability at least $0.999$.
    \end{itemize}
        
    Now note that a uniformly chosen vector in $\textup{Unif}_{t,r}$ for any $r > 0$ is just a scaled uniformly chosen vector in $\textup{Unif}_{t,1}$. Due to our hypothesis on $f$, it then follows that $A$ is such that for any $1 \le t \le s$,
  \[ \Pr_{x\sim\textup{Unif}_{t,1}}\left( \left | \|Ax\|_2^2  - \|x\|_2^2 \right| \le 2\gamma \|x\|_2^2 \right) \ge 0.99.\]

   The theorem then follows from Theorem \ref{thm:lb_linear_l2} (note that $2\gamma$ is \emph{smaller} than the tolerance required in that proof).
\end{proof}

As discussed in Section \ref{sec:our_contributions}, we further extend our prior result to encoder decoder schemes (recalled below), where another `decoder' function can be applied on top of the embeddings to compute the $\ell_2$ norm. Our lower bound shows that as long as both the encoder and decoder functions are sufficiently smooth, the encoder function is required to map to $\Omega(s^2)$ dimensions. We do not restrict the embedding dimension of the decoder function. In other words, the whole process is `bottle necked' by the inner dimension. We recall the definition of encoder decoder schemes for convenience. 

\encoderdecoder*

\begin{restatable}[Encoder/Decoder Schemes]{thm}{encoderdecoderthm}\label{thm:encoder_decoder}
   Let $\gamma \le C/s$ for a sufficiently small constant $C > 0$. If $h(x) = g(f(x)): \R^{s^2} \rightarrow \R^{s^2}$ for $k = s^2$ (as defined above) is such that for any $1 \le t \le s$ and any $r > 0$,
    \[ \Pr_{x\sim \textup{Unif}_{t,r}}\left( |\|h(x)\|_2^2 - \|x\|_2^2| \le \gamma \|x\|_2^2 \right) \ge 0.999,\]
    then $m \ge s^2/1000$. 
\end{restatable}

\begin{proof}
Similar to the proof of Theorem \ref{thm:general}, let $A$ be the matrix where the $i$th row is equal to $\nabla h_i(0)$. Note that $A: \R^{s^2} \rightarrow \R^{s^2}$. We first claim that $A$ has rank at most $m$. To see this, note that 
\[ h(x) = (g_1(f(x)), \ldots, g_{s^2}(f(x)). \]
For $1 \le j \le s^2$, let $h_j(x) = g_j(f(x))$. Denoting the input variables of $g_j$ as  $g_j(y_1, \ldots, y_m)$, the chain rule tells us that for any $j$ and $i$,
\[ \frac{\partial h_j}{\partial x_i} = \sum_{\ell = 1}^m \frac{\partial g_j}{\partial y_{\ell}} \cdot \frac{\partial f_{\ell}}{\partial x_i} = \langle q_i, p_j \rangle\]
where 
\[q_i = \left(  \frac{\partial f_1}{\partial x_i}, \ldots,  \frac{\partial f_m}{\partial x_i}\right) \in \R^m \]
and
\[p_j = \left(  \frac{\partial g_j}{\partial y_1}, \ldots,  \frac{\partial g_j}{\partial y_m}\right) \in \R^m.\]
(For simplicity, we are omitting the fact that all the parital derivatives of $f_{\ell}$ are being evaluated at $0$ and the parital derivatives of $g_j$ are being evaluated $f(0)$). Letting 
\[
B =
\left[
  \begin{array}{ccc}
    \horzbar & q_1^T & \horzbar \\
    \horzbar & q_2^T & \horzbar \\
             & \vdots    &          \\
    \horzbar & q_{s^2}^T & \horzbar
  \end{array}
\right] \in \R^{s^2 \times m}, \quad 
C =
\left[
  \begin{array}{ccc}
    \horzbar & p_1^T & \horzbar \\
    \horzbar & p_2^T & \horzbar \\
             & \vdots    &          \\
    \horzbar & p_{s^2}^T & \horzbar
  \end{array}
\right] \in \R^{s^2 \times m},
\]
we see that 
\[ A = \left[
  \begin{array}{ccc}
    p_1^TB^T  \\
    p_2^TB^T  \\
    \vdots            \\
    p_{s^2}^TB^T 
  \end{array}
\right] = CB^T \in \R^{s^2 \times s^{2}}.
 \]

Thus, $A$ has rank at most $m$. Now the rest of the proof proceeds by combining elements of Theorem \ref{thm:lb_linear_l2} and \ref{thm:general}, which we only briefly sketch for simplicity.

First, identical to the proof of Theorem \ref{thm:general}, a second-order multi-variable talyor expansion around $0$ implies that $A$ is such that for any $1 \le t \le s$
 \[ \Pr_{x\sim\textup{Unif}_{t,1}}\left( \left | \|Ax\|_2^2  - \|x\|_2^2 \right| \le \gamma \|x\|_2^2 \right) \ge 0.99.\]

Now we cannot directly invoke Theorem \ref{thm:lb_linear_l2}, since the matrix $A$ in the statement of Theorem \ref{thm:lb_linear_l2} maps $\R^{s^2}$ to $\R^m$, but $A$ is $\R^{s^2} \rightarrow \R^{s^2}$. However, note that the proof of Theorem \ref{thm:lb_linear_l2} only relies on the \emph{rank} of the matrix $A^TA$, which is at most $m$ (also true here). Thus, the rest of the proof is identical to the proof of Theorem \ref{thm:lb_linear_l2}.
\end{proof}

Finally, we extend the lower bound of Theorem \ref{thm:lb_linear_l2} to the case of approximately preserving inner products.

\begin{restatable}{thm}{innerprod}\label{thm:lb_linear_innerproduct}
      Let $A: \R^{s^2} \rightarrow \R^m$ be a non-zero linear map and $\gamma \le C/s$ for a sufficiently small constant $C > 0$. If $A$ is such that for any $1 \le t,t' \le s$,
    \[ \Pr_{x\sim \textup{Unif}_t, y \sim\textup{Unif}_{t'} }\left( |\langle Ax, Ay \rangle - \langle x, y \rangle| \le \gamma \|x\|_2 \|y\|_2 \right) \ge 0.99\]
    and 
    \[ \Pr_{ y \sim\textup{Unif}_{1} }\left( |\|Ay\|_2^2- \|y\|_2^2| \le \gamma \|y\|_2^2 \right) \ge 0.99,\]
    then $m \ge s^2/1000$.  
\end{restatable}


\begin{proof}

The proof is almost identical to that of Theorem \ref{thm:lb_linear_l2} but we present the full details for completeness, since we are working with two vectors instead of one. Suppose for the sake of contradiction that $m < s^2/10^3$. The proof is again roughly divided into three parts. The first part shows that $A^TA$ has many `non-zero' coordinates. The second part shows that a pair of random sparse vector $u$ and $v$ (as chosen in the hypothesis of the theorem statement) have `many' pairs of coordinates $u_i$ and $v_j$ such that the corresponding $(i,j)$ entry in $A^TA$ is also non-zero. The last part then shows that the prior result implies that $A$ \emph{does not} (approximately) preserve the inner product with a large constant probability, contradicting the assumption of the theorem. The last part relies on bounding the probability that a random polynomial lies in an unusually small interval. 
\\

 \noindent \textbf{Part $\#1$.}    
    \\
    
    \noindent Let $A_1, \ldots, A_{s^2}$ denote the columns of $A$. Let $v$ be a random vector chosen from $\textup{Unif}_1$, and let $i$ be its support with $v_i$ the corresponding non-zero entry. Then $\|Av\|_2^2 = \|A_i\|_2^2 \cdot v_i^2$ and $\|v\|_2^2 = v_i^2$. Then the hypothesis implies that 
    \begin{equation}\label{eq:1_prod}
        \Pr(|\|A_i\|_2^2 -1| \le 0.0001) \ge 0.99.
    \end{equation}
    Thus, a $0.99$ fraction of the columns of $A$ have Euclidean norm in the range $[0.9999, 1.0001]$. Let $A_X$ be the restriction to such columns. Note that all diagonal entries of $A_X^TA_X$ are at in the range $[0.999, 1.001]$ and all entries are bounded by $1.001$ in absolute value (via Cauchy-Schwarz on the columns of $A_X$). For $i \ge 1$, we let $\ell_i$ denote the number of \emph{non-diagonal} entries of $A_X^TA_X$ whose absolute values are in $(2^{-i}, 2^{-i+1}]$ and $\ell_0$ denote the rest of the non-diagonal entries (with absolute values in $(1, 1.001]$). We show the following claims hold.
    \begin{enumerate}[\alph*)]
        \item $\|A_X\|_F^2 \ge (0.99)^2s^2$,
        \item $\|A_X^T A_X\|_F^2 \ge \frac{(.99)^4s^4}m$,
        \item $\sum_{i \ge 0} \ell_i 2^{-2i+2} \ge \frac{(.99)^4s^4}m - 1.001 s^2$.
    \end{enumerate}
 Claim $(a)$ readily follows from inequality \eqref{eq:1_prod}. To show $(b)$, note that singular values of $A_X^TA_X$ are the squared singular values of $A_X$ so $\|A_X^T A_X\|_F^2 = \sum_{i=1}^m \sigma_i(A_X)^4$ (since the rank of $A$ is at most $m$).
    By Cauchy–Schwarz, 
    \[ m  \cdot \|A_X^TA_X\|_F^2 = m \sum_i \sigma_i(A_X)^4 \ge \left( \sum_i \sigma_i(A_X)^2 \right)^2 = \|A_X\|_F^4 \ge (.99)^4s^4. \]
    To show (c), note that all diagonal entries of $A_X^TA_X$ are at in the range $[0.999, 1.001]$. Claim (c) then follows from using the lower bound of Claim (b) since there are at most $s^2$ diagonal entries.
\\

  \noindent \textbf{Part $\#2$.}    
    \\
    
    \noindent Now let $u$ and $v$ be a vector drawn from $\textup{Unif}_s$ and $\textup{Unif}_r$ respectively and let $T_u$ and $T_v$ be their corresponding support sets.
    The hypothesis of the theorem states that 
    \begin{equation}\label{eq:2_inner}
        \Pr(| \langle Au, Av \rangle  - \langle u, v \rangle| \le \gamma \|u\|_2\|v\|_2) \ge 0.99.
    \end{equation}
    Our goal is to demonstrate a contradiction by showing $ \Pr(\langle Au, Av \rangle | > \gamma \|u\|_2\|v\|_2) \ge 0.02$, which contradicts \eqref{eq:2_inner}.
    
    Let $u_X$ be the restriction of $u$ to the coordinates in $X$ (i.e., only keep the coordinates in $X$) and similarly define $v_X$. Consider $u_X v_X^T$. The non-zero entries of $u_Xv_X^T$ lie on a random principal submatrix. We show that with a sufficiently large constant probability, both $u_X v_X^T \in \R^{|X| \times |X|}$ and $A_X^TA_X\in \R^{|X| \times |X|}$ have a non-zero value in `many' shared entries.

    Towards this end, let $Y_{ij}$ be the indicator variable for the entry $(i,j)$ in $u_X u_X^T$ being non-zero and let $s_{ij}$ denote the \emph{squared} $(i,j)$ entry of $A_X^T A_X$. 
    We know 
    \[\E[Y_{ij}] =  \frac{1}{s^2}.\]
    Finally, let $Z = \sum_{i,j} s_{ij}Y_{ij}$. Recalling our partitions $\ell_k$ that we defined earlier and Claim (c), we have
    \[ \E[Z] = \sum_{i,j} s_{ij}\E[Y_{ij}] \ge \frac{1}{s^2} \sum_k \ell_k 2^{-2k} \ge \frac{1}{4s^2} \left( \frac{(.99)^4s^4}m - 1.001 s^2 \right) > 200.\]
    The same proof as in Theorem \ref{thm:lb_linear_l2} shows that  $Z \ge 1$ with probability at least $.05$.
\\

\noindent \textbf{Part $\#3$.}    
\\

\noindent Recalling that $T_u$  and $T_v$ are the support sets of $u$ and $v$, we have 
\begin{equation}\label{eq:expansion_prod}
\langle Au, Av \rangle  - \langle u, v \rangle = \sum_{i,j} u_i v_j \langle A_i, A_j \rangle - \sum_i u_i v_i := P(u, v).
\end{equation}
If we pick the entries of $u$ and $v$ to be standard Gaussians, we have $\E[P] = 0$.
Recalling that $Y_{ij}$ is the indicator for $u_X^Tv_X$ having a non-zero entry at $(i,j)$, we have
\[ \E[P^2] \ge \sum_{i,j} s_{ij}Y_{ij} = Z.\]
Thus, if we assume that $Z \ge 1$, then Lemma \ref{lem:poly_anti_concentration} implies that if the variables of $u$ and $v$ in their support are picked from the standard Gaussian distribution, then the probability that $P \in [-\eps, \eps] = O(\sqrt{\eps})$ for any sufficiently small $\eps > 0$. Now consider the following three events.
\begin{itemize}
    \item $\mathcal{E}_1$ = event that $\|u\|_2 \|v\|_2 \le 100s$. 
    \item $\mathcal{E}_2$ = event that $Z \ge 1$.
    \item $\mathcal{E}_3$ = event that $P \not \in [-c, c]$ for a sufficiently small constant $c$.
\end{itemize}
By picking $c$ to be a small enough constant, we know that \emph{all} the events hold with probability at least $0.02$. Let's condition on all of these events holding. If so, we show that the condition $|\langle Au, Av \rangle  - \langle u, v \rangle| \le \gamma \|u\|_2\|v\|_2$ cannot hold. Indeed, the condition directly implies that $\langle Au, Av \rangle  - \langle u, v \rangle = P$ lies in an interval strictly contained in $[-c, c]$. Thus altogether, with probability at least $0.02$, we have $|\langle Au, Av \rangle  - \langle u, v \rangle| > \gamma \|u\|_2\|v\|_2$, contradicting inequality \eqref{eq:2_inner}. This finishes the proof.
\end{proof}



\section{Upper Bounds for Embedding for Non-negative Sparse Vectors}\label{sec:sparse_nonneg}
In this section we present our embeddings for non-negative sparse vectors and prove Theorem \ref{thm:final_non_negative}. See section \ref{sec:technical_overview} for an overview.

Our embedding is constructed in two parts. In the first part, we introduce a `base' mapping $f: \R^d \rightarrow \R^m$. It will be a random mapping which preserves \emph{all} $\ell_p$ norms of a \emph{fixed} pair of non-negative sparse vectors with constant probability. Crucially, it will be non-expanding. Our final embedding is formed by stacking many independent copies of our base mapping.

The base mapping maps every coordinate into one of $m$ buckets and takes the max of all non-zero coordinate elements that map to any bucket. The \emph{max} pooling operation makes this map non-linear. 

\begin{definition}[Base mapping]\label{def:base_mapping}
We define a mapping $f: \R^d \rightarrow \R^m$. Pick a uniformly random function from $h: [d] \rightarrow [m]$. For every $i \in [m]$, let $S_i = \{ j \in [d] \mid h(j) = i\}$.  We define $f(x) \in \R^m$ as follows. For every $i \in [m]$, 
\[ f(x)_i = \begin{cases} \max \left(\left \{ x_j \mid j \in S_i \right\}\right) \, \text{ if } S_i \ne \emptyset, \\
0, \quad \text{otherwise.} 
\end{cases}
\]
\end{definition}

Our final embedding is the following. It stacks many copies of the base mapping. Note $\bigoplus$ denotes vector concatenation. 

\begin{definition}[Final Embedding]\label{def:final_embedding}
A non-negative $(d, m, T)$ embedding $F: \R^d \rightarrow \R^{mT}$ is defined as follows. For every $1 \le i \le T$, let $f_i: \R^d \rightarrow \R^{m}$ be an independent copy of the random mapping of Definition \ref{def:base_mapping}. Then for any $x \in \R^d$,
\[ F(x) = \bigoplus_{i = 1}^T f_i(x).\]
\end{definition}
The key properties of the base mapping are proved below. As discussed in Section \ref{sec:technical_overview}, the non-expansion property is crucial and is only guaranteed for non-negative vectors due to the max operation.
\begin{theorem}\label{thm:non_negative_embedding_base}
Let $f : \R^d \rightarrow \R^{m}$ be a random mapping of Definition \ref{def:base_mapping}. It satisfies the following:
\begin{enumerate}
    \item $f(0) = 0$ deterministically for all values $m \ge 1$.
    \item For any pair of non-negative $s$-sparse vectors $x,y \in \R^d$ (both independent of $f$), if we take $m = \Omega(s^2/\delta)$,
        \[ \Pr(\forall p, \, \|f(x) - f(y) \|_p = \|x-y\|_p) \ge 1-\delta. \]
    \item For every pair of non-negative vectors $x$ and $y$ (not necessarily sparse) and any embedding dimension $m$, $\|f(x) - f(y)\|_p \le \|x-y\|_p$ deterministically for all $p \ge 1$.
\end{enumerate}
\end{theorem}

\begin{proof}
   The first property follow from the definition of $f$. Let $m = 100s^2/\delta$ and $h$ be the uniformly random function from $[d] \rightarrow [m]$ that constitutes $f$. 
   
   Let $x$ and $y$ be two fixed $s$-sparse vectors in $\R^d$. Proving the second condition relies solely on the fact that $h$ is likely to separate all the non-zero coordinates of $x$ and $y$. Indeed, the union of their supports is of size at most $2s$. Under $h$, the probability that some two domain elements in their union collide is at most 
   \[ \binom{2s}2 \cdot \frac{1}{(100s^2/\delta)} < \delta. \]
   This event means that very coordinate in the union of the supports of $x$ and $y$ is mapped to a unique coordinate in $[m]$, and thus for every $p$,
   \[ \|x-y\|_p^p =  \sum_{j = 1}^d |x_i - y_i|^p = \sum_{i = 1}^{m} |f(x)_i - f(y)_i|^p = \|f(x) - f(y)\|_p^p,   \]
   proving the second condition.

   The third condition crucially relies on the fact that we are using the `max' operation to define $f$. For any pair of non-negative vectors $x$ and $y$, we have 
\[\|f(x) - f(y)\|^p = \sum_{i = 1}^m |f(x)_j - f(y)_j|^p = \sum_{i=1}^m | \max(\{x_j \mid h(j) = i\}) - \max(\{y_j \mid h(j) = i\}) |^p. \]
It suffices to prove the following: if $a_1,\ldots, a_k$ and $b_1,\ldots, b_k$ are non-negative real numbers then 
\begin{equation}\label{ineq:main}
     |\max(a_1, \ldots, a_k) - \max(b_1, \ldots, b_k)| \le \max_t |a_t - b_t|. 
\end{equation}
This is because assuming the claim, we have 
\[\sum_{i=1}^m | \max(\{x_j \mid h(j) = i\}) - \max(\{y_j \mid h(j) = i\}) |^p \le \sum_{i = 1}^m \max_{j \mid h(j) = i} |x_j - y_j|^p \le \sum_{j=1}^d |x_i - y_i|^p, \]
since $h$ maps every coordinate in $[d]$ to only one coordinate in $[m]$. 

Now to prove the claim, assume without loss of generality that $a_{t'} = \max(a_t) \ge \max_t(b_t)$. Then $|a_{t'} - \max_t(b_t)| \le |a_{t'} - b_{t'}|$ since in the real line, we have the ordering $b_{t'} \le \max_t(b_t) \le a_{t'}$.
\end{proof}

Building upon Theorem \ref{thm:non_negative_embedding_base}, we give an embedding which approximately preserves all pairwise distances between points in a dataset of non-negative sparse vectors.



\begin{theorem}[Embedding for non-negative sparse vectors]\label{thm:final_non_negative}
Let $F: \R^d \rightarrow \R^{mT}$ be a non-negative $(d, m, T)$ embedding for $m = O(s/\eps)$ and $T = O(\log(ns)/\eps)$ as stated in Definition \ref{def:final_embedding}. Let $X \subset \R^d$ be a dataset of $n$ non-negative $s$-sparse vectors (which is independent of $F$). We have
\begin{enumerate}
    \item \[\Pr\left(\forall p \ge 1, \forall x, y \in X, \frac{\|F(x) - F(y)\|_p^p/T}{\|x-y\|_p^p} \in 1\pm\eps \right) \ge 1 - 1/\textup{poly}(n).\]
    \item For the $\ell_{\infty}$ norm, it suffices to take $m = O(s)$ and $T = O(\log n)$ and guarantee that
    \[\Pr\left(  \forall x, y \in X, \|F(x) - F(y)\|_{\infty} = \|x-y\|_{\infty} \right) \ge 1 - 1/\textup{poly}(n).\]
    \item Additionally for the $\ell_{\infty}$ norm, it suffices to take $m = 1$ and $T = 1$ and guarantee that \[\Pr\left(  \forall x, y \in X, \frac{\|F(x) + F(y)\|_{\infty}}{\|x+y\|_{\infty}} \in [1, 2] \right) =1.\]
    \end{enumerate}
\end{theorem}
\begin{proof}
We first prove part (1). Note that \[F = \bigoplus_{k=1}^T f_k\] where each $f_k: \R^d \rightarrow \R^{O(s/\eps)}$ is the mapping of Definition \ref{def:base_mapping} and $T = O(\log (ns)/\eps)$. Now consider an arbitrary fixed pair $x,y \in X$. First by the non-expanding property of each $f_k$, we always have 
\[ \sum_{k} \|f_k(x) - f_k(y)\|_p^p \le T \cdot \|x-y\|_p^p. \]
Now we show that the sum is also sufficiently large. Consider any fixed coordinate $i$ in the union of the support of $x$ and $y$. For any $f_k$, the probability that $i$ does not collide with any other support element is at least $1-\eps/100$ since there are only $O(s)$ total coordinates in the union of the supports of $x$ and $y$, and $f_k$ maps to $O(s/\eps)$ dimensions. Since the $f_k$'s are independent, this means that $i$ will not collide with any other support elements for at least $1-\eps$ fraction of the $f_k$'s with failure probability at most $\exp(-\Omega(T \eps))$ (Lemma \ref{lem:binomial_large_p}). By union bounding, we can extend this statement to \emph{all} the coordinates in the support of $x$ and $y$, except with failure probability at most $s \exp(-\Omega(T \eps)) \ll 1/\text{poly}(n)$, by our choice of $T$. Now if coordinate $i$ does not collide with any others in some $f_k$, then we know that $\|f_k(x) - f_k(y)\|_p^p \ge (x_i - y_i)^p$. We can sum this relation over all coordinates $i$ and all $f_k$. Thus, except with failure probability at most $1/\text{poly}(n)$, we have 
\[ \sum_k \|f_k(x) - f_k(y) \|_p^p \ge (1-\eps)T  \cdot \|x-y\|_p^p, \]
as desired.
    



% For this pair, we know every $f_k$ is non-expanding and satisfies $\|f_k(x) - f_k(y)\|_p^p = \|x-y\|_p^p$ for all $p \ge 1$ simultaneously with probability at least $1-\eps/10$. Since $f_k$'s are independent, the number of indices $k$ for which exact equality holds is a binomial random variable. A standard concentration bound (Lemma \ref{lem:binomial_large_p}) shows that $\|f_k(x) - f_k(y)\|_p^p = \|x-y\|_p^p$ for at least a $(1-\eps/2)T$ of the indices with failure probability at most $\exp(-\Omega(T\eps)) = 1/\textup{poly}(n)$. If this is the case, we have 
%     \[ \sum_{k} \|f_k(x) - f_k(y)\|_p^p \ge (1-\eps/2)T \cdot \|x-y\|_p^p \] and by the non-expanding property of $f_k$, we also have 
%     \[ \sum_{k} \|f_k(x) - f_k(y)\|_p^p \le T \cdot \|x-y\|_p^p. \]
% The first part of the theorem for the $m = O(s^2/\eps)$ case then follows by a union bound over all $\Theta(n^2)$ pairs. 

% We now show that $m = O(s/\eps^2)$ also suffices. Consider the random variable $\|f_1(x) - f_1(y)\|_p^p$. We know deterministically $\|f_1(x) - f_1(y)\|_p^p \le \|x-y\|_p^p$ no matter what. 
% For a non-zero coordinate $z_i$ of $z := x-y$, let $t_i$ denote the indicator random variable for the event that $i$ does not collide with any of the other non-zero coordinates of $z$. By our choice of $m$, we know that $\E[t_i] \ge 1-\eps^2$ since the sparsity of $z$ is $O(s)$, as it is the difference of two $s$-sparse vectors. Thus,
% \[ \E[\|f_1(x) - f_1(y)\|_p^p] \ge (1-\eps^2)\|x-y\|_p^p, \]
% so by Markov's inequality, 
% \[ 0 \le \E\left[\|x-y\|_p^p - \|f_1(x) - f_1(y)\|_p^p\right] \le \eps \|x-y\|_p^p\]
% with probability at least $1-\eps$. Now by applying the Chernoff bound as before, with failure probability at most  $\exp(-\Omega(T\eps)) = 1/\textup{poly}(n)$, we have $\|f_k(x) - f_k(y)\|_p^p \ge (1-\eps)\|x-y\|_p^p$ for at least a $(1-\eps/2)T$ of the indices. The proof finishes identically as before.



% \textcolor{red}{....}
%   First note that \[F = \bigoplus_{k=1}^T f_k\] where each $f_k: \R^d \rightarrow \R^{m}$ for $m =  O\left(\log (\log(s)/\eps) \cdot \left( \frac{s}\eps + \frac{1}{\eps^2} \right) \right)$ is the mapping of Definition \ref{def:base_mapping} and $T = O(\log (n)/\eps)$. Now consider an arbitrary fixed pair $x,y \in X$. Our ultimate goal is to show that $\|F(x) - F(y)\|_p^p/T = (1\pm \eps)\|x-y\|_p^p$. Recall that every $f_k$ is non-expanding, so intuitively, we have to argue that each $f_k$ certifies a `sufficiently large' distance between the pair. We formalize this intuition.
  
%   Noting that our map satisfies $F(rx) = rF(x)$ for any scalar $r \ge 0$, we can assume $\|x-y\|_{p}^p = 1$ by scaling. Now let $L_0 = 0, L_1 = \eps/(100s)$, and $L_{i+1} = 2L_{i}$ for all $O(\log(s/\eps)) \ge i \ge 1$ (we stop when $L_i \ge 1$). Group the \emph{non-zero} coordinates of $z := x-y$ as follows. Put the $j$th coordinate $z_j$ of $z$ in $i$th group iff $|z_j|^p \in [L_{i-1}, L_i)$ and let $\ell_i$ be the size of the $i$th group. 

% For now let's focus on an arbitrary group $i \ge 2$. Our goal is to show that `most' of the coordinates in this group are mapped to unique buckets under $f_1$. Towards this end, let $z_{1} \ldots, z_{\ell_i}$ be the coordinates in the group (after renaming the indices). For each coordinate, define $t_1, \ldots, t_{\ell_i}$ to be the following Bernoulli random variables: $t_j$ is equal to $1$ iff $z_j$ collides with some other non-zero coordinate of $z$ (in any group) under $f_1$. We have $\E[t_j] \le \eps/(100\log\log(s))$ since there are at most $O(s)$ non-zero coordinates ($z$ is the difference of two $s$-sparse vectors), and $f_1$ maps to $m$ dimensions. We can check that the $t_j$ variables are \emph{negatively associated}\footnote{while they are correlated, their negative dependence still implies a Chernoff like concentration}, and thus by the Chernoff bound for negatively associated random variables \cite{joag1983negative, wajc2017negative}, we have $p_i := \Pr(\sum_j t_j \ge \eps \ell_i) \le \exp(-\Omega(\eps \ell_i))$. 

% Now we consider two types of groups (excluding group $i = 1$). A group is called \emph{big} if $\ell_i \ge \log (\log(s)/\eps)/\eps$ and \emph{small} otherwise. 
% First, $\sum_{i \mid \ell_i \text{ is big }} p_i \le \eps/100$. Thus with probability at least $1-\eps/100$, all big groups have at most $\eps/100$-fraction of their coordinates collide with any other non-zero coordinate of $z$. Condition on this event.

% Now in the small case, we have $\sum_{i \mid \ell_i \text{ is small}} \ell_i \le O\left(\log(s/\eps) \log(\log(s)/\eps)/\eps\right)$. Thus, the probability that all of the coordinates in the union of all the small groups are all isolated under $f_1$ (i.e., they don't collide with any other non-zero coordinate of $z$), is at least 


% Furthermore, if $\ell_i \le \log(s)$... we have that with probability at least $1-\eps/(100\log(s)$, that all the coordinates in the group $i$ are separated. Since there are at most $O(\log(s))$ groups, all groups with ... are epearted by the union bound.

% Condition on these two events. In this case, we have 
% \[0 \le \|x-y\|_p^p -  \|f(x) - f(y)\|_p^p \le \eps \|x-y\|_p^p, \]
% and thus ...


% \textcolor{red}{bucketing ...}

The $\ell_{\infty}$ case can be handled as follows. For a fixed pair $x,y$, suppose that the first coordinate witnesses their $\ell_{\infty}$ norm. If we take $m = O(s)$, then the first coordinate does not collide with any other coordinate with probability at least $99\%$ (there maybe collisions among other coordinates). Thus with large constant probability, a fixed base mapping $f$ of Definition \ref{def:base_mapping} certifies that $\|f(x) - f(y)\|_{\infty} \ge \|x-y\|_{\infty}$. But we always have $\|f(x) - f(y)\|_{\infty} \le \|x-y\|_{\infty}$ deterministically. Thus with $O(\log n)$ repetitions, every pair $x,y$ will have at least one copy of the base mapping which ensures that the $\ell_{\infty}$ distance is \emph{exactly} preserved.

For the third part of the theorem, note that since the coordinates are non-negative and $F(x) \in \R$ is just the maximum coordinate, we trivially have $\|F(x) + F(y)\|_{\infty} \ge \|x+y\|_{\infty}$. 
In the other direction, we claim that $\|f(x) + f(y)\|_{\infty} \le 2\|x+y\|_{\infty}$ always holds deterministically where $f$ is our base mapping of Definition \ref{def:base_mapping}. Indeed, it suffices to show that for any non-negative real numbers $a_1, \ldots, a_r$ and $b_1, \ldots, b_r$, we always have 
\begin{equation}\label{eq:additive_bound}
    \max(a_1, \ldots, a_r) + \max(b_1, \ldots, b_r) \le 2 \max_{t \in [r]} (a_t + b_t).
\end{equation}

This is seen to be true by just taking $t = \argmax a_t$ (w.l.o.g. $\max(a_t) \ge \max(b_t)$). This completes the proof.
\end{proof}

The \emph{additive} guarantees of our mapping $F$ also extends to general $\ell_p$ norms, with an overhead of $2^{O(p)}$. 

\begin{corollary}
Let $p \ge 1$ and $F: \R^d \rightarrow \R^{s^22^{O(p)} \log(n)/\eps^2}$ be a non-negative $(d, s^22^{O(p)}/\eps, T)$ embedding for $T = 2^{O(p)}\log(n)/\eps$ as stated in Definition \ref{def:final_embedding}. Let $X \subset \R^d$ be a dataset of $n$ non-negative $s$-sparse vectors (which is independent of $F$). We have
\[\Pr\left( \forall x, y \in X, \frac{\|F(x) + F(y)\|_p^p/T}{\|x+y\|_p^p} \in 1\pm\eps \right) \ge 1 - 1/\textup{poly}(n).\]
\end{corollary}
\begin{proof}
The proof is very similar to that of Theorem \ref{thm:final_non_negative} so we only highlight the differences. For the parameters, we take independent base embeddings $f_k: \R^d \rightarrow \R^{m}$ for $m = s^2 2^{O(p)}/\eps$ (of Theorem \ref{thm:non_negative_embedding_base}) and $T = \log(n)2^{O(p)}/\eps$. Let $\gamma = s^2/m$. First, for any fixed pair $x,y \in X$  we have $\|f_k(x) + f_k(y)\|_p^p = \|x+y\|_p^p$ holds for at least $1-\gamma$ fraction of $k$'s (due to property (2) in Theorem \ref{thm:non_negative_embedding_base}) with failure probability at most $1/\text{poly}(n)$. Thus we have
\[\sum_k \|f_k(x) + f_k(y)\|_p^p \ge (1-\gamma)T \cdot \|x+y\|_p^p.\]
To bound the other direction, we also always have $\|f_k(x) + f_k(y)\|_p^p \le 2^p \|x+y\|_p^p$ for every $k$ from Eq. \ref{eq:additive_bound}. This means that
\[\sum_k \|f_k(x) + f_k(y)\|_p^p \le (1-\gamma)T \cdot \|x+y\|_p^p + \gamma T 2^p \|x+y\|_p^p.\]
Recalling the value of $\gamma$ finishes the proof as in Theorem \ref{thm:final_non_negative}.
\end{proof}

If we limit the entries of the sparse vectors to be from a discrete set, then we can extend our non linear map to sparse vectors with arbitrary entries as well. Contrasting it with the known $\Omega(s^p)$ lower bounds for embedding arbitrary $s$-sparse vectors (using linear maps) from \cite{ZhuGR15} hints that the `hardness' for embedding arbitrary sparse vectors maybe due to entries with a large range.

\begin{theorem}\label{thm:general_discrete}
    Let $p \ge 1$ and $F: \R^d \rightarrow \R^{s^2\Delta^{O(p)} \log(n)/\eps^2}$ be a $(d, s^2\Delta^{O(p)}/\eps), T)$ embedding for $T = \log(n)\Delta^{O(p)}/\eps$ as stated in Definition \ref{def:final_embedding}. Let $X \subset \R^d$ be a dataset of $n$ $s$-sparse vectors with entries in the discrete set $\{-\Delta, \ldots, \Delta\}$. We have
\[ \Pr\left(\forall x, y \in X, \frac{\|F(x) - F(y)\|_p^p}{\|x-y\|_p^p} \in [ (1-\eps)T,  T ] \right) \ge 1 - 1/\textup{poly}(n). \]
\end{theorem}

\begin{proof}
    We again consider  \[F = \bigoplus_{k=1}^T f_k\] where each $f_k: \R^d \rightarrow \R^{m}$ for $m = s^2(2\Delta)^p/\eps$ is the mapping of Definition \ref{def:base_mapping} and $T = O(\log (n)(2\Delta)^p/\eps)$. We can check that the first two properties of $f$ in Theorem \ref{thm:non_negative_embedding_base} hold for arbitrary $s$-sparse vectors. However, the third property crucially relies on non-negative entries. Nevertheless, we can obtain the following variant, assuming the entries are in a discrete set: for every pair of vectors $x,y \in \{-\Delta, \ldots, \Delta\}^d$, we have $\|f_k(x) - f_k(y)\|_p \le \Delta\|x-y\|_p$. Indeed, as in the proof of Theorem \ref{thm:non_negative_embedding_base}, it suffices to prove the following: if $a_1,\ldots, a_k$ and $b_1,\ldots, b_k$ are all in $\{-\Delta, \ldots, \Delta\}$, then \[ |\max(a_1, \ldots, a_k) - \max(b_1, \ldots, b_k)| \le 2\Delta \cdot \max_t |a_t - b_t|. \]
    And this holds because if the RHS is $0$, then so is the LHS. Otherwise, the RHS is at least $2\Delta \cdot 1$ and the LHS is always bounded by $2\Delta$.

    Equipped with this, we similarly have that with failure probability at most $\exp(-\Omega(T \gamma)) = 1/\textup{poly}(n)$, $\|f_k(x) - f_k(y)\|_p^p = \|x-y\|_p^p$ for at least $(1-\gamma/2)T$ fraction of the indices $k$ where $\gamma = \eps/(2\Delta)^p$. If this is the case, then again 
     \[ \sum_{k} \|f_k(x) - f_k(y)\|_p^p \ge (1-\gamma/2)T \cdot \|x-y\|_p^p \] and by the bounded-expanding property of $f_k$, we also have 
    \[ \sum_{k} \|f_k(x) - f_k(y)\|_p^p \le (1-\gamma/2)T \cdot \|x-y\|_p^p  + \gamma T (2\Delta)^p\|x-y\|_p^p. \]
    Putting everything together, we have $ \sum_{k} \|f_k(x) - f_k(y)\|_p^p \in (1\pm \eps/2)T\cdot \|x-y\|_p^p$. And the final result follows from union bounding over all $\Theta(n^2)$ pairs, as desired.    
\end{proof}


% Lastly, we note a corollary which states that our dimensionality reduction results extend to setting where we use a `monotone' function to measure distances. This wide class includes all $\ell_p$ norms.

% \begin{corollary}\label{cor:monotone}
%  Let $X \subset \R^d$ be a set of $n$ non-negative $s$-sparse vectors. Let $g: [0, \infty) \rightarrow [0, \infty)$ be a monotonically increasing function. There exists a \textbf{non-linear} map $f: \R^d \rightarrow \R^{m}$ for $m = O(\log(ns) \cdot s/\eps^2)$ which satisfies $\sum_{i=1}^m g(|f(x)_i - f(y)_i|) = (1 \pm \eps) \sum_{i=1}^d g(|x_i - y_i|)$ for all $x,y \in X$.
% \end{corollary}

% \begin{proof}[Proof Sketch]
% Recall inequality \eqref{ineq:main}: if $a_1,\ldots, a_k$ and $b_1,\ldots, b_k$ are non-negative real numbers then 
% \[ |\max(a_1, \ldots, a_k) - \max(b_1, \ldots, b_k)| \le \max_t |a_t - b_t|. \]

% If we apply any monotonic function $g$, we also have 
% \[ g(|\max(a_1, \ldots, a_k) - \max(b_1, \ldots, b_k)|) \le g(\max_t |a_t - b_t|). \]


% Thus, the `base map of Theorem \ref{thm:non_negative_embedding_base} satisfies the following strengthening of the third point: For every pair of non-negative vectors $x$ and $y$ (not necessarily sparse) and any embedding dimension $m$, $\sum_i g(|f(x)_i - f(y)_i|) \le \sum_i g(|x_i - y_i|)$ deterministically (note the first sum is over the coordinates in the projected dimension and the second sum is over the coordinates in the original space).

% Now the rest of the proof is identical to that of Theorem \ref{thm:final_non_negative}. We use the exact same map. The main points of the analysis are, to summarize:

% \begin{itemize}
%     \item We always have $\sum_k \left(  \sum_i g(|f_k(x)_i - f_k(y)_i|) \right) \le T \cdot  \sum_i g(|x_i - y_i|)$.
%     \item With probability $1-\eps/100$, every coordinate in the union of the supports of $x$ and $y$ is isolated in every $f_k$, independently. Thus, every such coordinate is isolated in at least $1-\eps$ fraction of the $f_k$'s with high probability by our choice of $T$.
%     \item The above implies that $\sum_k \bigg( \sum_i g(|f_k(x)_i - f_k(y)_i|) \bigg) \ge (1-\eps) T \cdot  \sum_i g(|x_i - y_i|)$ for all pair of vectors $x,y \in X$ with high probability, again by our choice of $T$.
% \end{itemize}
% \end{proof}

% As a corollary, we can embed $\ell_{\infty}$ into a smaller dimensional space in $\ell_{O(\log(s)/\eps)}$.

% \begin{corollary}\label{cor:linf}Let $p = O(\log(s)/\eps)$. Let $X$ be a set of $s$-sparse vectors in $\R^d$ (arbitrary entries). There is an embedding of $X$ into $\ell_p^m$ which preserves all pairwise $\ell_{\infty}$ distances up to $1\pm \eps$ factor for the following values of $m$:
%     \begin{itemize}
%         \item If we use the embedding of \cite{ZhuGR15}, then $m = s^{O(\eps^{-1}(\log \log d + \log s)) }/\eps^2$.
%         \item If we use the embedding of Theorem \ref{thm:final_embedding_general}, then $m = \log(s) \log(d/\eps)s^{O(\eps^{-1}\log s) }/\eps^2$.
%         \item If $|X| \subset \{-O(1), \ldots, O(1)\}$, then the embedding of Theorem \ref{thm:general_discrete} gives $m = \log(|X|)s^{O(1/\eps)}/\eps^2$.
%     \end{itemize}
% \end{corollary}
% \begin{proof}
%     The proof follows by noting that in $s$ dimensions, the $\ell_{\infty}$ norm is $1\pm \eps$ approximated by the $\ell_p$ norm for $p = O(\log(s)/\eps)$ (Lemma \ref{lem:linfapprox}), and plugging in this value of $p$ in the respective theorems.
% \end{proof}




\section{Lower Bounds for Embedding Non-Negative Sparse Vectors}\label{sec:nonneglb}

In this section, we provide lower bounds for dimensionality reduction for sparse vectors under various hypotheses. As stated in Section \ref{sec:our_contributions}, together they demonstrate that our non-negative sparse embedding of Theorem \ref{thm:final_non_negative} is optimal in many natural ways. See Section \ref{sec:our_contributions} for an overview.

We begin by showing that a linear map cannot have the same guarantees as Theorem \ref{thm:final_non_negative}.

\linearlb*

\begin{proof}
Suppose for the sake of contradiction that $m < d/100$. By considering the basis vectors, every column of $A$ must have an entry with absolute value at least $1/2$. Then there must exist a row $r$ of $A$ which has at least $d/m \ge 100$ such entries. At least $50$ of such entries in row $r$ must be of the same sign. Let $x$ be an indicator vector for the column of $10$ these entries in row $r$. Note that $x \in S$ and $|(Ax)_r| \ge 5$ which implies $\|Ax\|_{\infty} \ge 5$, contradicting our assumption on $A$. Thus, $m \ge d/100 = \Omega(d)$, as desired.
\end{proof}


The following theorem states that preserving the norms of the sum of vectors is impossible a factor of $2-\eps$ for any $\eps > 0$ in $\ell_{\infty}$, even if we only restrict to non-negative sparse vectors. Note that Theorem \ref{thm:final_non_negative} preserves the norms of the sums up to a factor of $2$. 


\twohardness*
\begin{proof}
Suppose for the sake of contradiction that $m < d$. First note that $f(0)$ must be the all $0$'s vector by taking $x = y = 0$. Then taking $x = e_i$ and $y = 0$ implies that $\|f(e_i)\|_{\infty} \in [1, 2-\eps]$ for all $i$. Again label such coordinates of $f(e_i)$ that lie in this range as `large.' If $m < d$, then there exists $i$ and $j$ such that $f(e_i)$ and $f(e_j)$ have the same large index by Pigeonhole. We know that $\|f(e_i) + f(e_j)\|_{\infty} \in [1, 2-\eps]$ by our hypothesis. Now if the large entries of $f(e_i)$ and $f(e_j)$ have the same sign, then their sum in absolute value is at least $2$, which cannot happen from the above observation. On the other hand, if they have different signs, then the largest sum (in absolute value) that these entries can add to is at most $1-\eps$ (either from $2-\eps + (-1)$ or $-(2 - \eps) + 1$), which also cannot happen. These cases are exhaustive which means our assumption $m < d$ cannot hold, and we must have $m \ge d$.
\end{proof}

The following theorem shows that any mapping $f$ also cannot be `too smooth'. Note that in our mapping, we use the maximum function, which is not differentiable.

\lbsmooth*

\begin{proof}
Note that $f_i(0) = 0$ by taking $r \to 0$.
    Since the second partial derivatives of all $f_i$ are continuous, they are bounded in magnitude in the compact set $[0,1]^{d}$. Let $T$ be such an upper bound which holds for all $i$. Now we let $r \ll 1$ be a sufficiently small value which will be determined shortly. For any fixed $f_i$, Taylor's theorem for multivariate functions\footnote{\url{https://en.wikipedia.org/wiki/Taylor's\_theorem\#Taylor's\_theorem\_for\_multivariate_functions}}
 implies that for any $y \in [0,r]^{d}$ which is $O(1)$-sparse, 
    \[ |f_i(y) - \langle \nabla f_i(0), y \rangle| \le T\left(y_1 + \ldots + y_{d}\right)^2 \le O(Tr^2)  \]
    for every $i$. Let $A: \R^{d} \rightarrow \R^m$ be the matrix with $a_i$ as it's rows. The above inequality implies that 
    \[ \|f(y) - Ay\|_{\infty} \le O(Tr^2). \]
Thus if $r > 0$ sufficiently small, $z$ is any vector such that $\|z\|_{\infty} = 1$, and
    \[  0.99 \|rz\|_{\infty} \le \|f(rz)\|_{\infty} \le 1.01 \|rz\|_{\infty},\]
    then we must also have 
\[  \frac{1}2 \|rz\|_{\infty} \le \|A(rz)\|_{\infty} \le \frac{3}2 \|rz\|_{\infty}.\]
But since $A$ is linear, this actually implies that 
\[  \frac{1}2 \|r'z\|_{\infty} \le \|A(r'z)\|_{\infty} \le \frac{3}2 \|r'z\|_{\infty}\]
    for all $r' > 0$. Putting everything together, (letting $z$ be the vectors in the hypothesis of the theorem statement), implies that
\[  \frac{1}2 \|z\|_{\infty} \le \|Ax\|_{\infty} \le \frac{3}2 \|z\|_{\infty} \quad \forall z \in S.\] 
Then Theorem \ref{thm:linear_lb} implies that $m = \Omega(d)$, as desired.
\end{proof}


The next lower bound of this section demonstrates that the non-negative hypothesis is crucial. If we drop the non-negativity constraint, then any map cannot satisfy the guarantees promised by Theorem \ref{thm:final_non_negative}.

\lbnonnegvec*


\begin{proof}
    Suppose for the sake of contradiction that $m \le d-1$. We first show that $f(e_i) = - f(-e_i)$ for all $i$. Indeed, $e_i + (-e_i) = 0$ so we must have $\|f(e_i) + f(-e_i)\|_{\infty} = 0 \implies f(e_i) = - f(-e_i)$. We also have $f(0) = 0 \in \R^m$, also using the second relation. This means that $\|f(\pm e_i)\|_{\infty} \in [0.9, 1.1]$ for all $i$ by using the first relation with $x = \pm e_i$ and $y = 0$.
    
    For a vector $v \in S$, call the indices where $\|f(v)\|_{\infty} \in [0.9, 1.1]$ \emph{large entries}. Our goal is to show that most vectors in $S$ need to have distinct large entries. This will imply that $m$ must be large since $|S|$ is large. 
    
    Indeed, if $m \le d-1$, then there must exists some $i \ne j$ such that $f(e_i)$ and $f(e_j)$ share the same large entry index by Pigeonhole. Then either $f(e_j)$ or $f(-e_j) = -f(e_j)$ also has the \emph{opposite sign} as the large entry of $f(e_i)$. So at least in one case, we can find two vectors $x,y \in S$ such that $\|f(x) - f(y)\|_{\infty} \ge 2 \cdot 0.9 = 1.8$. However, since $i \ne j$, we know that $\|x-y\|_{\infty} = 1$, contradicting the first relation. Hence, $m \ge d$, as desired.
\end{proof}

Finally, the last lower bound demonstrates that any map with weaker guarantees than that of Theorem \ref{thm:final_non_negative} must map to $\tilde{\Omega}(s)$ dimensions. Note that Theorem \ref{thm:final_non_negative} satisfies the hypothesis of the theorem bellow with $C = O(1/\log n)$ for a dataset of $n$ non-negative $s$-sparse vectors.

\begin{theorem}\label{thm:omega_s_lb}
Suppose $d > 2s$ and let $S = \left\{\sum_{i = 1}^{s} e_i, \sum_{i = s+1}^{2s} e_i, 0 \right\}$. Suppose $f: S \rightarrow \R^m$ be an arbitrary mapping which satisfies
\[ \|f(x) - f(y)\|_{\infty} \le \|x-y\|_{\infty} \quad \forall x,y \in S,\]
\[ C \|x-y\|_{1} \le \|f(x) - f(y)\|_{1} \quad \forall x,y \in S,\]
for any $C > 0$. Then $m  \ge Cs$.
\end{theorem}
\begin{proof}
By shifting, we can assume $f(0) = 0$. Let $v_1 =\sum_{i = 1}^{s} e_i$ and $v_2 = \sum_{i = s+1}^{2s} e_i$. Then the first relation implies that all coordinates of $v_1$ and $v_2$ are bounded by $1$ in absolute value. The second relation them implies that 
\[  2Cs \le \|f(v_1) - f(v_2)\|_1 \le \|f(v_1)\|_1 + \|f(v_2)\|_1 \le 2m, \] 
yielding $m \ge Cs$.
\end{proof}


\section{Applications of Our Non-negative Embedding}\label{sec:applications}
We now present applications of our embeddings for sparse vectors for many problems in geometric optimization. We always assume our input is a dataset $X$ of $n$ non-negative $s$-sparse vectors in $\R^d$. 



\subsection{Computing Diameter}
Our goal is to compute the diameter of the dataset $X$ in $\ell_p$ norm (see Definition \ref{def:diameter}). The first lemma shows that the diameter is preserved when projecting onto very low dimensions.

\begin{definition}\label{def:diameter} $\textup{diameter}_p = \max_{x, y \in X} \|x-y\|_p$.
\end{definition}

\begin{lemma}\label{lem:diam}
Let $f: \R^d \rightarrow \R^{O(s^2)}$ mapping given in Definition \ref{def:base_mapping}. Let $\tilde{X} = \{f(x) \mid x \in X\} \subset \R^{O(s^2)}$. We have
\[ \Pr\left( \forall p, \textup{diameter}_p(X) = \textup{diameter}_p(\tilde{X}) \right) \ge 0.99. \]
For the $\ell_{\infty}$ case, we can instead embed to $O(s)$ dimensions.
\end{lemma}
\begin{proof}
    Let $(x,y)$ be a pair in $X$ that witnesses the diameter. Theorem \ref{thm:non_negative_embedding_base} implies that for any $p$, the distance between $x$ and $y$ is preserved under $f$ with probability at least $99\%$. The proof of the $\ell_{\infty}$ case is the same as in Theorem \ref{thm:final_non_negative}: the coordinate which witnesses the distance between $x$ and $y$ does not collide with any of the other support elements with constant probability. Furthermore, Theorem \ref{thm:non_negative_embedding_base} also implies that all other distances do not expand under $f$. This completes the proof.
\end{proof}

This implies the following corollary stating that any `low dimensional' algorithm for computing the diameter can be used to compute the diameter of $X$, after composing with our dimensionality reduction.

\begin{corollary}\label{cor:diam}
    Given $p \ge 1$, consider an algorithm which computes a $C$-factor approximation of the diameter of any $n$ point dataset in $d$-dimensions in $Q(n, d, C)$ time. Then there exists an algorithm which computes a $C$ approximation of the diameter correctly with $99\%$ probability of our dataset $X$ in time $Q(n, O(s^2), C) + O(ns)$. For the $\ell_{\infty}$ case, the corresponding bound is $Q(n, O(s), C) + O(ns)$.
\end{corollary}
\begin{proof}
    We simply project our dataset $X$ using $f$ as stated in Lemma \ref{lem:diam} and apply the algorithm $Q$ in the projected space.
\end{proof}

Appealing to existing algorithms on diameter computation implies the following results. 


\begin{restatable}{thm}{diameter}
\label{thm:diameter}
Let $X$ be dataset of non-negative $s$-sparse vectors. We have the following algorithms:
    \begin{enumerate}
        \item Using \cite{chan2018applications}, for any constant integer $p$, we can compute a $1 + \eps$ approximation to $\textup{diameter}_p$ of $X$ in time $\tilde{O}(n/\sqrt{\eps} + ns + 2^{O(s^2 \log(1/\eps)}))$ which is correct with probability $99\%$.
        \item We can exactly compute the diameter of $X$ in $\ell_{\infty}$ norm in time $O(ns)$. This algorithm can be implemented in a stream using $O(s)$ words of memory.
        \item We can exactly compute the diameter of $X$ in $\ell_1$ norm in time $n2^{O(s)}$. This algorithm can be implemented in a stream using $2^{O(s)}$ words of memory.
    \end{enumerate}
\end{restatable}

\begin{proof}
    The first result just follows from appealing to Corollary 4.1 in \cite{chan2018applications} which gives an algorithm for computing the diameter in $\ell_p$ in low-dimensional spaces, and appropriately plugging in the bounds as specified in Corollary \ref{cor:diam}.

    For the second result, we recall a folklore streaming algorithm for computing diameter in $\ell_{\infty}$ norm. We have 
    \[\text{diameter}_p(X) = \max_{\text{dimension } i} \max_{x,y \in X} |x_i - y_i| = \max_{\text{dimension } i} \left( \max_{x\in X} x_i -  \min_{y \in X} y_i\right). \]
    Thus, it just suffices to only keep track the maximum and the minimum coordinate along every dimension. Via Lemma \ref{lem:diam}, it suffices to assume the dimension is only $O(s)$. The non-streaming algorithm simply scans the points along every dimension as well.
    
    Finally for the third result, we recall a well known isometric embedding of $\ell_1^k$ into $\ell_{\infty}^{2^k}$ for any $k \ge 1$: simply map any $x \in \R^d$ to $Ax \in \R^{2^k}$ where $A$ has all possible $\pm 1$ vectors as rows. Applying this embedding reduces the $\ell_1$ case to the $\ell_{\infty}$ case addressed above.
\end{proof}


\subsection{Dimensionality Reduction for Max Cut}
We consider the max cut problem defined as follows.
\begin{definition}\label{def:maxcut}
$\textup{MaxCut}_p(X) = \max_{S \subseteq X} \sum_{x \in S, y \in X \setminus S} \|x-y\|_p^p.$
\end{definition}

To the best of our knowledge, dimensionality reduction for max cut has been only studied in the $\ell_2$ case where its known that $O(1/\eps^2)$ dimensions suffice to estimate the max cut of an arbitrary sized dataset up to $1\pm \eps$ (only for $\ell_2$) \cite{LammersenSS09, ChenJK23}. 

We show the following dimensionality reduction for max cut in the general $\ell_p$ case, where the point set consists of non-negative sparse vectors.

\begin{theorem}\label{thm:maxcut}
Let $f: \R^d \rightarrow \R^{O(s/\eps^2)}$ be the mapping given in Theorem \ref{thm:non_negative_embedding_base}. Let $\tilde{X} = \{f(x) \mid x \in X\}$. For every $p \ge 1$, we have
\[ \mathbb{E}\left[\left | \textup{MaxCut}_p(\tilde{X}) - \textup{MaxCut}_p(X) \right|\right]  \le O(\eps)  \cdot \textup{MaxCut}_p(X) . \]
\end{theorem}
\begin{proof}
We drop the dependence on $p$ for the sake of clarity. Let $E$ be the set of edges that participate in a fixed optimal max cut for $X$. Let $\overline{\textup{MaxCut}}(\tilde{X})$ be the value of the cut given by $E$ in the projected dimension (note that $E$ is deterministic but the value is random since the projection is random).

Fix any pair $x,y$ and consider the random variable $\|f(x) - f(y)\|_p^p$. We know deterministically $\|f(x) - f_1(y)\|_p^p \le \|x-y\|_p^p$. For a non-zero coordinate $z_i$ of $z := x-y$, let $t_i$ denote the indicator random variable for the event that $i$ does not collide with any of the other non-zero coordinates of $z$. By our choice of $m$, we know that $\E[t_i] \ge 1-\eps^2$ since the sparsity of $z$ is $O(s)$, as it is the difference of two $s$-sparse vectors. Thus,
\[ \E[\|f(x) - f(y)\|_p^p] \ge (1-\eps^2)\|x-y\|_p^p, \]
so by Markov's inequality, 
\[ 0 \le \|x-y\|_p^p - \|f(x) - f(y)\|_p^p \le \eps \|x-y\|_p^p\] with probability at least $1-\eps$. We have 
\begin{align*}
    \mathbb{E}\left[ \left| \text{MaxCut}(X) - \overline{\text{MaxCut}}(\tilde{X}) \right| \right] &\le \sum_{(x,y) \in E} \mathbb{E}\left[| \|f(x) - f(y)\|_p^p - \|x-y\|_p^p| \right] \\
    &\le \sum_{(x,y) \in E} O(\eps) \cdot \|x-y\|_p^p \\
    &= O(\eps) \cdot \text{MaxCut}(X),
\end{align*}
Now we make two simple observations. First, we always have 
\[ \text{MaxCut}(\tilde{X}) \ge \overline{\textup{MaxCut}}(\tilde{X}), \]
since $\text{MaxCut}(\tilde{X})$ is the best cut in the projected space. Secondly, since the value of every cut never expands in the projected dimension due to property (3) in Theorem \ref{thm:non_negative_embedding_base}, we also have that the value of \emph{every} cut in the projected space is at most $\textup{MaxCut}(X)$ deterministically. In particular, it must also be true that 
\[\textup{MaxCut}(X) \ge  \text{MaxCut}(\tilde{X}).\] 
In particular, this means that we have 
\[ 0 \le \text{MaxCut}(X) - \text{MaxCut}(\tilde{X}) \le \text{MaxCut}(X) - \overline{\textup{MaxCut}}(\tilde{X}), \]
so the random variable $\left|\textup{MaxCut}(\tilde{X}) - \textup{MaxCut}(X)\right|$ is always bounded by $\left| \text{MaxCut}(X) - \overline{\text{MaxCut}}(\tilde{X}) \right|$, which finishes the proof.
\end{proof}

% As a corollary, we obtain the following streaming algorithm by combining our dimensionality reduction result above with a known streaming algorithms for max-cut given in \cite{ChenJK23}.

% \begin{corollary}
%    There is a randomized streaming algorithm that, given $0 < \eps < 1/2$, $p \ge 1,$ and a non-negative $s$-sparse $X \subseteq [\textup{poly}(n)]^d$ presented as a stream, uses space $\tilde{O}(\textup{poly}(\eps^{-1}s))$ and reports an estimate that with probability at least $2/3$ is a $(1 + \eps)$-approximation to $\textup{MaxCut}_p(X)$.
% \end{corollary}



\subsection{Clustering Applications}

We consider the (arguably) most well-studied formulations of clustering: $k$-median, $k$-means, and $k$-center. Dimensionality reduction for these problems have been well studied in the $\ell_2$ case \cite{MakarychevMR19}. Here we consider dimensionality reduction for general $\ell_p$ norms, but with a restricted set of vectors (non-negative sparse).

We recall their definitions below.

\begin{definition}\label{def:clustering}
We are interested in general $\ell_p$ norm formulations of the $k$-median/center/means clustering objectives.
    \begin{itemize}
    \item \textbf{$k$-median}: Given a dataset $X = \{x_1, \ldots, x_n\}$ of $n$ points $\in \R^d$, the goal is to find a partition $\mathcal{C} = \{C_1, C_2, \ldots, C_k\}$ of $[n]$ into $k$ non-empty parts (clusters) to minimize the following:
\[ \textup{cost}(\mathcal{C}(X)) = \sum_{i=1}^k \min_{u_i \in \R^d} \sum_{j \in C_i} \|x_j - u_i\|_p.\]
\item \textbf{$k$-center}: Same as $k$-median, but we define the cost as 
\[ \textup{cost}(\mathcal{C}(X)) = \max_k \min_{u_i \in \R^d} \max_{j \in C_i} \|x_j - u_i\|_p.\]
\item \textbf{$k$-means}: Same as $k$-median, but we instead use the \emph{squared} distances $\|x - u_i\|_p^2$.
\end{itemize}
\end{definition}

 Note that for these clustering problems, the centers $u_i$ do not have to be in $X$ and can be \emph{arbitrary} points in space. That is, once the points are partitioned, we optimize for the choice of centers. Note that even though our initial dataset may satisfy structural properties as sparsity, is it likely that the optimal centers chosen will not. Thus, while our embedding of Theorem \ref{thm:final_non_negative} guarantees that $\ell_p$ distances between our dataset is preserved, there is no meaningful notion of what the center is under the embedding. Nevertheless, our embedding implies that the cost of \emph{every} clustering is preserved up to a small multiplicative factor.

\begin{theorem}\label{thm:clustering}
    Consider the $k$-median or $k$-center problem. Let $X$ be a set of $n$ non-negative $s$-sparse vectors in $\R^d$. Let $F: \R^d \rightarrow \R^{O(s \log(n)/\eps^2)}$ be the mapping given in Theorem \ref{thm:final_non_negative} and $\tilde{X} = \{F(x) \mid x \in X\}$. We have
    \[ \Pr\left( \forall \mathcal{C},\textup{cost}(\mathcal{C}(\tilde{X})) = (4 \pm \eps) \textup{cost}(\mathcal{C}(X)) \right) \ge 1 - 1/\textup{poly}(n). \]
    For $k$-means, an identical statement as above holds, except we replace $4$ with $16$.
\end{theorem}
\begin{proof}
    Fix a clustering $\mathcal{C}$ (partition of the datapoints). The complication arises due the fact that $\textup{cost}(\mathcal{C}(X))$ optimizes for (non-necessarily sparse) centers in $\R^d$ whereas $\mathcal{C}(\tilde{X}))$ optimizes for centers in the projected space. There may not be an analogue of a center chosen in $\R^d$ in the projected space. To get around this, we show we can simply \emph{move} the centers to their closest data point in $X$ with a small multiplicative loss. The proof proceeds formally as follows.

    We first only consider the case of $k$-median and $k$-center. Assume that $F$ preserves all pairwise distances between points in $X$, which holds with probability at least $1 - 1/\textup{poly}(n)$ and condition on this event. Take any one of the $k$ partitions $C$ of $\mathcal{C}$ and consider the best center $u$ for this partition in $\R^d$. Now let $u'$ be the closest center to $u$ in its partition $C$. Moving $u$ to $u'$ will never decrease the cost (by the optimality of $u$). We claim that it will also never increase the cost by more than a factor of $2$. Indeed for $k$-median, the cost increases by a factor of $|C| \cdot \|u - u'\|_p$ by triangle inequality, which is less than $\sum_{j \in C} \|x_j - u\|_p$ by the minimiality of $u'$. Now the sum is just the original cost. For $k$-center a similar reasoning holds. 

    Call such clusterings where the centers are restricted to the dataset points in $X$ as \emph{basic}. The above reasoning implies that given a partition $\mathcal{C}$, the cost of the optimum clusterings and the optimum basic clusterings only differ by a multiplicative factor of $2$. Of course the same reasoning is also true in the projected dimension. But note that the costs of \emph{all} basic clusterings are preserved under $F$, since all pairwise distances are preserved. 
  Thus we have the following chain of inequalities: 
  \[ \textup{cost}(\mathcal{C}(\tilde{X}), \text{ basic}) \le 2\textup{cost}(\mathcal{C}(\tilde{X})) \le 2\textup{cost}(\mathcal{C}(\tilde{X}), \text{ basic})  = 2(1 \pm \eps)\textup{cost}(\mathcal{C}(X), \text{ basic}), \]
  and similarly 
  \[ 2(1 \pm \eps)\textup{cost}(\mathcal{C}(X), \text{ basic}) \le 4(1 \pm \eps)\textup{cost}(\mathcal{C}(X)) \le  4(1 \pm \eps)\textup{cost}(\mathcal{C}(X), \text{ basic}).\]
  Adjusting $\eps$, altogether, this shows that $\textup{cost}(\mathcal{C}(\tilde{X}))$ and $ \textup{cost}(\mathcal{C}(X))$ are within a factor of $4\pm \eps$ of each other. The argument does not depend on the choice of $\mathcal{C}$ (once we condition on the event that all pairwise distances are preserved). 
The proof also easily extends to $k$-means, where the basic clusterings now increase the cost by at most a factor of $4$ due to the squared triangle inequality.

\end{proof}


\subsection{Distance Estimation}
We first define the distance estimation problem.
\begin{definition}\label{def:dist_estimation}
    In the distance estimation problem, we want to preprocess a dataset $X$ and output a data structure $\mathcal{D}$. Then on any query $y$, $\mathcal{D}$ outputs an approximation to $\sum_{x \in X} \|x-y\|_p^p$.
\end{definition}
 We have the following result. 

\begin{theorem}\label{thm:dist_est1}
   Given a dataset $X\subset \R^d$ of $n$ non-negative $s$-sparse vectors and even integer $p$, we can compute a data structure $\mathcal{D}$ using $O(n\log(n)ps/\eps^2)$ preprocessing time. For any fixed non-negative $s$-sparse query $y$, $\mathcal{D}(y)$ computes a value $t$ satisfying 
   \[  \left | t - \sum_{x \in X} \|x - y\|_p^p  \right| \le  \eps \cdot \sum_{x \in X} \|x - y\|_p^p \]
   with probability $1-1/\textup{poly}(n)$ with query time $O(\log(n)ps/\eps^2)$.
\end{theorem}
\begin{proof}
We will have $O(\log n)$ independent $d$-variate polynomials $P_1, \ldots P_{O(\log n)}$. Each $P_i$ will output an estimate $t_i$ with the guarantee that
    \[  \mathbb{E}\left[ \left | t_i - \sum_{x \in X} \|x - y\|_p^p  \right| \right] \le  \eps \cdot \sum_{x \in X} \|x - y\|_p^p, \]
    which extends to the high-probability guarantee by just taking medians of all the independent estimates. 

    $P_1$ will be constructed by using the `base' mapping $f_1: \R^d \rightarrow \R^{m}$ for $m = O(s/\eps^2)$ of Theorem \ref{thm:non_negative_embedding_base}. We set $\tilde{X}_1 = \{f_1(x) \mid x \in X\}$, and let $P_1(z) = \sum_{x \in \tilde{X}_1} \|x - z\|_p^p$. Then $t_1$ is simply $P_1(f_1(y))$. 
    
    Note that each $x \in X$ satisfies $\E[|\|f_1(x) - f_1(y)\|_p^p - \|x-y\|_p^p] \le \eps\|x - y\|_p^p$ (see the proof of Theorem \ref{thm:maxcut}). Finally, we repeat this independently for all $P_i$. The construction and query times are only dependent on the polynomial evaluation.
    
  Note that each $P_i$ is a polynomial with the number of monomials bounded by $O(mp)$. It takes $O(nmp)$ time to construct $P$ and given $z$, we can compute $P(z)$ in time $O(mp)$ as well. Theorem \ref{thm:final_non_negative} guarantees the quality of the approximation.
    

\end{proof}

\noindent {\bf Acknowledgments:} 
D. Woodruff worked on this in part at Google and while visiting the Simons Institute for the Theory of Computing. He also thanks support from a Simons Investigator Award.  

\bibliography{main}
\bibliographystyle{alpha}

\appendix




\section{Useful inequalities}
\begin{lemma}\label{lem:binomial_large_p}
    If $X_1, \ldots, X_t$ are i.i.d. Bernoulli$(p)$ for $p \ge 1-\eps/100$ then 
    \[\Pr\left( \left| \sum_{i=1}^t X_i - tp \right|  \ge \eps t \right) \le  \exp \left(- \Omega(\eps t) \right). \]
\end{lemma}
\begin{proof}[Proof Sketch]
    Consider the complement random variables $Y_i = 1-X_i$. These are Bernoulli$(1-p)$ and so by a standard Chernoff bound, 
    \[ \Pr\left( \left | \sum_{i=1}^t Y_i - t(1-p) \right| \ge \eps t \right) \le \exp \left( - \Omega(\eps t) \right),\]
    as desired.
\end{proof}

\begin{lemma}\label{lem:linfapprox}
    Let $z \in \R^d$, $p \ge 10\log(d)/\eps,$ and $\eps \in (0, 1/10)$. Then $\|z\|_p \in (1\pm \eps) \|z\|_{\infty}$.
\end{lemma}
\begin{proof}
    Without loss of generality, suppose $\|z\|_{\infty} = |z_1| = \max_{i \in [d]} |z_i|$. We have 
    \[ d |z_1|^p \ge \|z\|_p^p \ge |z_1|^p. \]
    Taking $1/p$-th powers gives us 
    \[ d^{1/p}|z_1|\ge \|z\|_p \ge |z_1|,\]
    and the lemma follows by noting that $d^{1/p} = \exp(\log(d)/p) = \exp(\eps/10) = 1 + \Theta(\eps)$.
\end{proof}

\begin{lemma}\label{lem:deviation}
    If $a,b \in \R^d$ satisfy $\|a - b\|_{\infty} \le \delta$, then $| \|a\|_2^2 - \|b\|_2^2| \le 2\delta \sqrt{d}\|a\|_2 + \delta^2 d$.
\end{lemma}
\begin{proof}
Let $c_i = b_i - a_i$. We know that $|c_i| \le \delta$. Then $b_i = a_i+c_i$ and $a_i^2 - b_i^2 = a_i^2 - (a_i + c_i)^2 = c_i^2 - 2a_ic_i$ so $|\sum_i (a_i^2 - b_i^2)| \le d\delta^2 + 2\delta \sum_i |a_i| \le 2\delta \sqrt{d}\|a\|_2 + \delta^2 d$. 
\end{proof}


\end{document}



