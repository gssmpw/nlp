% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{liesenfeld2023timing,
    title = "The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems",
    author = "Liesenfeld, Andreas  and
      Lopez, Alianda  and
      Dingemanse, Mark",
    editor = "Stoyanchev, Svetlana  and
      Joty, Shafiq  and
      Schlangen, David  and
      Dusek, Ondrej  and
      Kennington, Casey  and
      Alikhani, Malihe",
    booktitle = "Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigdial-1.45/",
    doi = "10.18653/v1/2023.sigdial-1.45",
    pages = "482--495",
    abstract = "Speech recognition systems are a key intermediary in voice-driven human-computer interaction. Although speech recognition works well for pristine monologic audio, real-life use cases in open-ended interactive settings still present many challenges. We argue that timing is mission-critical for dialogue systems, and evaluate 5 major commercial ASR systems for their conversational and multilingual support. We find that word error rates for natural conversational data in 6 languages remain abysmal, and that overlap remains a key challenge (study 1). This impacts especially the recognition of conversational words (study 2), and in turn has dire consequences for downstream intent recognition (study 3). Our findings help to evaluate the current state of conversational ASR, contribute towards multidimensional error analysis and evaluation, and identify phenomena that need most attention on the way to build robust interactive speech technologies."
}




@inproceedings{qasemi2021paco,
    title = "{P}a{C}o: Preconditions Attributed to Commonsense Knowledge",
    author = "Qasemi, Ehsan  and
      Ilievski, Filip  and
      Chen, Muhao  and
      Szekely, Pedro",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.505/",
    doi = "10.18653/v1/2022.findings-emnlp.505",
    pages = "6781--6796",
    abstract = "Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that a glass is used for drinking water, unless the glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language models' (LMs) impressive performance on inferring commonsense knowledge, it is unclear whether they understand the circumstantial preconditions. To address this gap, we propose a novel challenge of reasoning with circumstantial preconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand preconditions of commonsense statements expressed in natural language. Based on this dataset, we create three canonical evaluation tasks and use them to examine the capability of existing LMs to understand situational preconditions. Our results reveal a 10-30{\%} gap between machine and human performance on our tasks, which shows that reasoning with preconditions is an open challenge."
}


@inproceedings{qamar2023speaking,
    title = "Who is Speaking? Speaker-Aware Multiparty Dialogue Act Classification",
    author = "Qamar, Ayesha  and
      Pyarelal, Adarsh  and
      Huang, Ruihong",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.678/",
    doi = "10.18653/v1/2023.findings-emnlp.678",
    pages = "10122--10135",
    abstract = "Utterances do not occur in isolation in dialogues; it is essential to have the information of who the speaker of an utterance is to be able to recover the speaker`s intention with respect to the surrounding context. Beyond simply capturing speaker switches, identifying how speakers interact with each other in a dialogue is crucial to understanding conversational flow. This becomes increasingly important and simultaneously difficult to model when more than two interlocutors take part in a conversation. To overcome this challenge, we propose to explicitly add speaker awareness to each utterance representation. To that end, we use a graph neural network to model how each speaker is behaving within the local context of a conversation. The speaker representations learned this way are then used to update their respective utterance representations. We experiment with both multiparticipant and dyadic conversations on the MRDA and SwDA datasets and show the effectiveness of our approach."
}


@inproceedings{goswami2023weakly,
    title = "Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages",
    author = "Goswami, Koustava  and
      Rani, Priya  and
      Fransen, Theodorus  and
      McCrae, John",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.38/",
    doi = "10.18653/v1/2023.findings-emnlp.38",
    pages = "531--541",
    abstract = "Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection tasks based on orthographic, phonetic or state-of-the-art contextual language models, which under-perform for most under-resourced languages. This paper proposes a novel language-agnostic weakly-supervised deep cognate detection framework for under-resourced languages using morphological knowledge from closely related languages. We train an encoder to gain morphological knowledge of a language and transfer the knowledge to perform unsupervised and weakly-supervised cognate detection tasks with and without the pivot language for the closely-related languages. While unsupervised, it overcomes the need for hand-crafted annotation of cognates. We performed experiments on different published cognate detection datasets across language families and observed not only significant improvement over the state-of-the-art but also our method outperformed the state-of-the-art supervised and unsupervised methods. Our model can be extended to a wide range of languages from any language family as it overcomes the requirement of the annotation of the cognate pairs for training."
}


@inproceedings{huang2023mclf,
    title = "{MCLF}: A Multi-grained Contrastive Learning Framework for {ASR}-robust Spoken Language Understanding",
    author = "Huang, Zhiqi  and
      Chen, Dongsheng  and
      Zhu, Zhihong  and
      Cheng, Xuxin",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.533/",
    doi = "10.18653/v1/2023.findings-emnlp.533",
    pages = "7936--7949",
    abstract = "Enhancing the robustness towards Automatic Speech Recognition (ASR) errors is of great importance for Spoken Language Understanding (SLU). Trending ASR-robust SLU systems have witnessed impressive improvements through global contrastive learning. However, although most ASR errors occur only at local positions of utterances, they can easily lead to severe semantic changes, and utterance-level classification or comparison is difficult to distinguish such differences. To address the problem, we propose a two-stage multi-grained contrastive learning framework dubbed MCLF. Technically, we first adapt the pre-trained language models to downstream SLU datasets via the proposed multi-grained contrastive learning objective and then fine-tune it on the corresponding dataset. Besides, to facilitate contrastive learning in the pre-training stage, we explore several data augmentation methods to expand the training data. Experimental results and detailed analyses on four datasets and four BERT-like backbone models demonstrate the effectiveness of our approach."
}



@article{gunthner1998polyphony,
title = {Polyphony and the ‘layering of voices’ in reported dialogues: An analysis of the use of prosodic devices in everyday reported speech},
journal = {Journal of Pragmatics},
volume = {31},
number = {5},
pages = {685-708},
year = {1999},
issn = {0378-2166},
doi = {10.1016/S0378-2166(98)00093-9},
url = {https://www.sciencedirect.com/science/article/pii/S0378216698000939},
author = {Susanne Günthner},
keywords = {Polyphony, Layering of voices, Reported speech, Prosoy, Prosodic features, Voice quality, Contextualization},
abstract = {In this paper I shall analyze the prosodic and voice quality techniques which speakers use in reported dialogues to contextualize their point of view towards the reconstructed utterances. The analysis is based on data of informal German conversations (dinner table conversations, coffee-break chats and telephone interactions) among friends and family members. Methods of interpretative sociolinguistics (Gumperz, 1982), conversation analysis and interactional analysis of prosody (Couper-Kuhlen and Selting, 1996) are used to investigate the devices which speakers use in order to evaluate the reconstructed utterances. It will be argued that speakers not only (re)construct past dialogues but that often in this process of ‘de- and recontextualization’ simultaneously communicate their evaluative perspective on the quoted utterances. The analysis will focus on the following questions: How do speakers communicate their point of view to the reported dialogues? Which prosodic means and devices of speech quality are used to communicate the speaker's perspective towards the quoted utterance as concordant or discordant? I shall argue that participants in everyday interactions also use poetic devices and polyphonic strategies described by Bakhtin (1981) as ‘layering of voices’. In contrast to literary texts, ‘polyphonic layering of voices’ in everyday reported dialogues is mainly achieved by means of prosody (Günthner, 1993).}
}


@Article{Shannon1948,
  Title                    = {A mathematical theory of communication},
  Author                   = {Shannon, Claude Elwood},
  Journal                  = {The Bell System Technical Journal},
  Year                     = {1948},

  Month                    = {July},
  Number                   = {3},
  Pages                    = {379-423},
  Volume                   = {27},

  Doi                      = {10.1002/j.1538-7305.1948.tb01338.x},
  ISSN                     = {0005-8580}
}


@article{yan2022survey,
author = {Yan, Chen and Ji, Xiaoyu and Wang, Kai and Jiang, Qinhong and Jin, Zizhi and Xu, Wenyuan},
title = {A Survey on Voice Assistant Security: Attacks and Countermeasures},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527153},
doi = {10.1145/3527153},
abstract = {Voice assistants (VA) have become prevalent on a wide range of personal devices such as smartphones and smart speakers. As companies build voice assistants with extra functionalities, attacks that trick a voice assistant into performing malicious behaviors can pose a significant threat to a user’s security, privacy, and even safety. However, the diverse attacks and stand-alone defenses in the literature often lack a systematic perspective, making it challenging for designers to properly identify, understand, and mitigate the security threats against voice assistants. To overcome this problem, this article provides a thorough survey of the attacks and countermeasures for voice assistants. We systematize a broad category of relevant but seemingly unrelated attacks by the vulnerable system components and attack methods, and categorize existing countermeasures based on the defensive strategies from a system designer’s perspective. To assist designers in planning defense based on their demands, we provide a qualitative comparison of existing countermeasures by the implementation cost, usability, and security and propose practical suggestions. We envision this work can help build more reliability into voice assistants and promote research in this fast-evolving area.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {84},
numpages = {36},
keywords = {Voice assistant, security, attack, defense, speech, voice interaction}
}



@misc{zeinali2019but,
      title={{BUT} System Description to {V}ox{C}eleb Speaker Recognition Challenge 2019}, 
  author={Zeinali, Hossein and Wang, Shuai and Silnova, Anna and Mat{\v{e}}jka, Pavel and Plchot, Old{\v{r}}ich},
      year={2019},
      eprint={1910.12592},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/1910.12592}, 
}


@inproceedings{lau2005testing,
	address = {Berlin, Heidelberg},
	title = {Testing voice mimicry with the {YOHO} speaker verification corpus},
	isbn = {978-3-540-31997-9},
	abstract = {The aim of this paper is to determine how vulnerable a speaker verification system is to conscious effort by impostors to mimic a client of the system. The paper explores systematically how much closer an impostor can get to another speaker’s voice by repeated attempts. Experiments on 138 speakers in the YOHO database and six people who played a role as imitators showed a fact that professional linguists could successfully attack the system. Non-professional people could have a good chance if they know their closest speaker in the database.},
	booktitle = {Knowledge-{Based} {Intelligent} {Information} and {Engineering} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lau, Yee W. and Tran, Dat and Wagner, Michael},
	editor = {Khosla, Rajiv and Howlett, Robert J. and Jain, Lakhmi C.},
	year = {2005},
	pages = {15--21},
    doi={10.1007/11554028_3}
}



@INPROCEEDINGS{lau2004vulnerability,

  author={Yee Wah Lau and Wagner, M. and Tran, D.},

  booktitle={Proceedings of 2004 International Symposium on Intelligent Multimedia, Video and Speech Processing, 2004.}, 

  title={Vulnerability of speaker verification to voice mimicking}, 

  year={2004},

  volume={},

  number={},

  pages={145-148},

  keywords={Databases;Data security;Information security;Speaker recognition;Speech synthesis;Biometrics;Error analysis;Computer security;Speech processing;Parameter estimation},

  doi={10.1109/ISIMP.2004.1434021}}



@INPROCEEDINGS{Chen2021RealBob,

  author={Chen, Guangke and Chenb, Sen and Fan, Lingling and Du, Xiaoning and Zhao, Zhe and Song, Fu and Liu, Yang},

  booktitle={2021 IEEE Symposium on Security and Privacy (SP)}, 

  title={Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems}, 

  year={2021},

  volume={},

  number={},

  pages={694-711},

  keywords={Privacy;Systematics;Biometrics (access control);Speech recognition;Distortion;Robustness;Speaker recognition;adversarial-attack;speaker-recognition;speaker-identification;speaker-verification},

  doi={10.1109/SP40001.2021.00004}}


@article{Davis80-COP,
	Author = {Steven B. Davis and Paul Mermelstein},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {4},
	Pages = {357--366},
	Title = {Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences},
	Volume = {28},
  Month = aug,
	Year = {1980}}

@article{Rabiner89-ATO,
	Author = {Lawrence R. Rabiner},
	Journal = {Proceedings of the IEEE},
	Number = {2},
	Pages = {257--286},
	Title = {A Tutorial on Hidden {Markov} Models and Selected Applications in Speech Recognition},
	Volume = {77},
  Month = feb,
	Year = {1989}}

@book{Hastie09-TEO,
	Address = {New York},
	Author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	Publisher = {Springer},
	Title = {The Elements of Statistical Learning -- Data Mining, Inference, and Prediction},
	Year = {2009}}

@inproceedings{Smith22-XXX,
	Author = {Jane Smith and Firstname2 Lastname2 and Firstname3 Lastname3},
	Pages = {100--104},
	Title = {A really good paper about {D}ynamic {T}ime {W}arping},
 	Booktitle = {Proc. {INTERSPEECH} 2022 -- 23\textsuperscript{rd} Annual Conference of the International Speech Communication Association},
    Address = {{Incheon, Korea}},
    Month = {{Sep.}},
	Year = {2022}}

 % use the Crossref field to copy any unspecified fields (such as booktitle) from another entry, 
@inproceedings{Jones22-XXX,
	Author = {Robert Jones and Firstname2 Lastname2 and Firstname3 Lastname3},
	Crossref = {Smith22-XXX},
	Pages = {105--109},
	Title = {An excellent paper introducing the {ABC} toolkit}}

@inproceedings{moore19_interspeech,
  author={Roger K. Moore and Lucy Skidmore},
  title={On the Use/Misuse of the Term {`Phoneme'}},
    Address = {{Graz, Austria}},
    Month = {{Sep.}},
  year=2019,
 	Booktitle = {Proc. {INTERSPEECH} 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association},
  pages={2340--2344},
  doi={10.21437/Interspeech.2019-2711}
}


@inproceedings{baevski2020wav2vec,
 author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {12449--12460},
 publisher = {Curran Associates, Inc.},
 title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf},
 volume = {33},
 year = {2020}
}



@ARTICLE{schuller2010cross,
  author={Schuller, Bj{\"o}rn and Vlasenko, Bogdan and Eyben, Florian and W{\"o}llmer, Martin and Stuhlsatz, Andr{\'e} and Wendemuth, Andreas and Rigoll, Gerhard},
  journal={IEEE Transactions on Affective Computing}, 
  title={Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies}, 
  year={2010},
  volume={1},
  number={2},
  pages={119-131},
  keywords={Databases;Emotion recognition;Emotion recognition;Speech recognition;Acoustics;Affective computing;speech emotion recognition;cross-corpus evaluation;normalization},
  doi={10.1109/T-AFFC.2010.8}}



@inproceedings{callison2022dungeons,
    title = "{D}ungeons and {D}ragons as a Dialog Challenge for Artificial Intelligence",
    author = "Callison-Burch, Chris  and
      Tomar, Gaurav Singh  and
      Martin, Lara J.  and
      Ippolito, Daphne  and
      Bailis, Suma  and
      Reitter, David",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.637/",
    doi = "10.18653/v1/2022.emnlp-main.637",
    pages = "9379--9393",
    abstract = "AI researchers have posited Dungeons and Dragons (D{\&}D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D{\&}D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history. We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words. We automatically annotate the data with partial state information about the game play. We train a large language model (LM) to generate the next game turn, conditioning it on different information. The LM can respond as a particular character or as the player who runs the game{---}i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy). We perform a human evaluation to determine what factors make the generated output plausible and interesting. We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output."
}




@misc{galvez2021people,
      title={The People's Speech: A Large-Scale Diverse {E}nglish Speech Recognition Dataset for Commercial Usage}, 
      author={Daniel Galvez and Greg Diamos and Juan Ciro and Juan Felipe Cerón and Keith Achorn and Anjali Gopi and David Kanter and Maximilian Lam and Mark Mazumder and Vijay Janapa Reddi},
      year={2021},
      eprint={2111.09344},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.09344}, 
}



@inproceedings{park2020speaker,
  title     = {Speaker Diarization with Lexical Information},
  author    = {Tae Jin Park and Kyu J. Han and Jing Huang and Xiaodong He and Bowen Zhou and Panayiotis Georgiou and Shrikanth Narayanan},
  year      = {2019},
  booktitle = {Proc. Interspeech 2019},
  pages     = {391--395},
  doi       = {10.21437/Interspeech.2019-1947},
  issn      = {2958-1796},
}

@article{Revathi_forensic_2021,
	title = {Forensic investigation for twin identification from speech: perceptual and gamma-tone features and models},
	volume = {80},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-10639-z},
	doi = {10.1007/s11042-021-10639-z},
	abstract = {To assist an investigation process, forensic experts compare and analyze audio recordings. Speech utterances are compared by humans and/or machines for use in court for investigation. Scientific research community insists for specific automatic or human-based approach to identify uniquxy2e audio features from identical twins group. Filters can be employed to enhance an audio recording for improving clarity. This may entail removal of unnecessary noise to enrich the intelligibility of speech. Forensic audio experts can examine a variety of characteristics of the audio recording to decide the possibility of alterations in the collected evidences. This includes confirming the integrity and authenticating that the content is what it purports to be. Thiswork named as FIST(Forensic Investigation for Twin Identification from Speech: Perceptual and Gamma-tone Features and Models) proposes an automated system to identify a twin from identical twin pairs by the use of gamma-tone features and perceptual features.The proposed features are excerpted from the set of training speeches and templates are created for each twin based on vector quantisation (VQ), Fuzzy C means clustering (FCM) and multivariate hidden Markov modelling (MHMM) techniques. For testing, features are extracted from the set of test utterances and worked out to the templates for classification. Based on the type of classifier used, classification of twin is carried out with minimum distance and maximum loglikelihood value. The proposed features are examinedfor sub-optimal and true success rates as key performance metrics to assess the system and also a comparative analysis is made across the proposed features. Among the inspected features, Gammatone energy features expose better performance in comparison to perceptual features by attaining the overall sub-optimal success rate and true success rate as97.8375\% and 92.75\% for Gammatone energy features with VQ based modelling technique. This work FIST has also been analysed by inducing disturbance in the form of speech interference from their own twin pairs and Gamma-tone energy feature with VQ based modelling technique performs better for twin identification. A high claim of 99.625\% and 95.0625\% accuracy has been achieved by employing decision level fusion classifier.},
	number = {12},
	journal = {Multimedia Tools and Applications},
	author = {Revathi, A. and Sasikaladevi, N. and Geetha, K.},
	month = may,
	year = {2021},
	pages = {18301--18315},
}



@article{Wangetal2021system,
  author       = {Yuxuan Wang and
                  Mao{-}Kui He and
                  Shutong Niu and
                  Lei Sun and
                  Tian Gao and
                  Xin Fang and
                  Jia Pan and
                  Jun Du and
                  Chin{-}Hui Lee},
  title        = {{USTC-NELSLIP} System Description for {DIHARD-III} Challenge},
  journal      = {CoRR},
  volume       = {abs/2103.10661},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.10661},
  eprinttype    = {arXiv},
  eprint       = {2103.10661},
  timestamp    = {Sun, 31 Dec 2023 00:37:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-10661.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{ryant21_interspeech,
  author={Neville Ryant and Prachi Singh and Venkat Krishnamohan and Rajat Varma and Kenneth Church and Christopher Cieri and Jun Du and Sriram Ganapathy and Mark Liberman},
  title={{The Third DIHARD Diarization Challenge}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={3570--3574},
  doi={10.21437/Interspeech.2021-1208}
}

@INPROCEEDINGS{Flemotomos2020Lexical,
  author={Flemotomos, Nikolaos and Dimitriadis, Dimitrios},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Memory Augmented Architecture for Continuous Speaker Identification in Meetings}, 
  year={2020},
  volume={},
  number={},
  pages={6524-6528},
  keywords={Measurement;Error analysis;Memory architecture;Neural networks;Signal processing;Task analysis;Speech processing;speaker identification;diarization;memory networks;meeting analysis},
  doi={10.1109/ICASSP40776.2020.9053152}}



@ARTICLE{ellis2017computers,

  author={Ellis, Simon and Hendler, James},

  journal={IEEE Intelligent Systems}, 

  title={Computers Play Chess, Computers Play Go…Humans Play {D}ungeons \& {D}ragons}, 

  year={2017},

  volume={32},

  number={4},

  pages={31-34},

  keywords={Games;Artificial intelligence;Creativity;Complexity theory;Pattern recognition;Cognitive systems;artificial intelligence;game-playing;Dungeons and Dragons;artificial intelligence;intelligent systems},

  doi={10.1109/MIS.2017.3121545}}



@article{mann1947test,
author={Mann, Henry B and Whitney, Donald R},
title = {{On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other}},
volume = {18},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {50 -- 60},
year = {1947},
doi = {10.1214/aoms/1177730491},
URL = {https://doi.org/10.1214/aoms/1177730491}
}



@inproceedings{si2021telling,
    title = "Telling Stories through Multi-User Dialogue by Modeling Character Relations",
    author = "Si, Wai Man  and
      Ammanabrolu, Prithviraj  and
      Riedl, Mark",
    editor = "Li, Haizhou  and
      Levow, Gina-Anne  and
      Yu, Zhou  and
      Gupta, Chitralekha  and
      Sisman, Berrak  and
      Cai, Siqi  and
      Vandyke, David  and
      Dethlefs, Nina  and
      Wu, Yan  and
      Li, Junyi Jessy",
    booktitle = "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = jul,
    year = "2021",
    address = "Singapore and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sigdial-1.30/",
    doi = "10.18653/v1/2021.sigdial-1.30",
    pages = "269--275",
    abstract = "This paper explores character-driven story continuation, in which the story emerges through characters' first- and second-person narration as well as dialogue{---}requiring models to select language that is consistent with a character`s persona and their relationships with other characters while following and advancing the story. We hypothesize that a multi-task model that trains on character dialogue plus character relationship information improves transformer-based story continuation. To this end, we extend the Critical Role Dungeons and Dragons Dataset (Rameshkumar and Bailey, 2020){---}consisting of dialogue transcripts of people collaboratively telling a story while playing the role-playing game Dungeons and Dragons{---}with automatically extracted relationships between each pair of interacting characters as well as their personas. A series of ablations lend evidence to our hypothesis, showing that our multi-task model using character relationships improves story continuation accuracy over strong baselines."
}


@article{park2022review,
  title={A review of speaker diarization: Recent advances with deep learning},
  author={Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J and Watanabe, Shinji and Narayanan, Shrikanth},
  journal={Computer Speech \& Language},
volume = {72},
pages = {101317},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101317},
  publisher={Elsevier}
}

@inproceedings{watanabe2020chime,
  title     = {{CHiME}-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings},
  author    = {Shinji Watanabe and Michael Mandel and Jon Barker and Emmanuel Vincent and Ashish Arora and Xuankai Chang and Sanjeev Khudanpur and Vimal Manohar and Daniel Povey and Desh Raj and David Snyder and Aswin Shanmugam Subramanian and Jan Trmal and Bar Ben Yair and Christoph Boeddeker and Zhaoheng Ni and Yusuke Fujita and Shota Horiguchi and Naoyuki Kanda and Takuya Yoshioka and Neville Ryant},
  year      = {2020},
  booktitle = {6th International Workshop on Speech Processing in Everyday Environments (CHiME 2020)},
  pages     = {1--7},
  doi       = {10.21437/CHiME.2020-1},
}

@inproceedings{Bredin23,
  title     = {pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe},
  author    = {Hervé Bredin},
  year      = {2023},
  booktitle = {Proc. Interspeech 2023},
  pages     = {1983--1987},
  doi       = {10.21437/Interspeech.2023-105},
  issn      = {2958-1796},
}


@inproceedings{Chung20,
  title     = {Spot the Conversation: Speaker Diarisation in the Wild},
  author    = {Joon Son Chung and Jaesung Huh and Arsha Nagrani and Triantafyllos Afouras and Andrew Zisserman},
  year      = {2020},
  booktitle = {Proc. Interspeech 2020},
  pages     = {299--303},
  doi       = {10.21437/Interspeech.2020-2337},
  issn      = {2958-1796},
}


@INPROCEEDINGS{janin2003icsi,

  author={Janin, A. and Baron, D. and Edwards, J. and Ellis, D. and Gelbart, D. and Morgan, N. and Peskin, B. and Pfau, T. and Shriberg, E. and Stolcke, A. and Wooters, C.},

  booktitle={2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).}, 

  title={The {ICSI} Meeting Corpus}, 

  year={2003},

  volume={1},

  number={},

  pages={I-I},

  keywords={Speech recognition;Speech processing;Audio recording;Microphones},

  doi={10.1109/ICASSP.2003.1198793}}


@inproceedings{janin2004icsi,
  title={The {ICSI} meeting project: Resources and research},
  author={Janin, Adam and Ang, Jeremy and Bhagat, Sonali and Dhillon, Rajdip and Edwards, Jane and Macias-Guarasa, Javier and Morgan, Nelson and Peskin, Barbara and Shriberg, Elizabeth and Stolcke, Andreas and others},
  booktitle={Proceedings of the 2004 ICASSP NIST Meeting Recognition Workshop},
  year={2004}
}


@inproceedings{kraaij2005ami,
title = "The {AMI} meeting corpus",
keywords = "METIS-230270",
author={Kraaij, Wessel and Hain, Thomas and Lincoln, Mike and Post, Wilfried},
year = "2005",
month = aug,
day = "30",
language = "Undefined",
isbn = "90-74821-71-5",
pages = "137--140",
editor = "L.P.J.J. Noldus and F Grieco and L.W.S. Loijens and P.H. Zimmerman",
booktitle = "Proceedings of Measuring Behavior 2005, 5th International Conference on Methods and Techniques in Behavioral Research",
publisher = "Noldus Information Technology",

}




@inproceedings{louis2018deep,
    title = "Deep {D}ungeons and {D}ragons: Learning Character-Action Interactions from Role-Playing Game Transcripts",
    author = "Louis, Annie  and
      Sutton, Charles",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2111/",
    doi = "10.18653/v1/N18-2111",
    pages = "708--713",
    abstract = "An essential aspect to understanding narratives is to grasp the interaction between characters in a story and the actions they take. We examine whether computational models can capture this interaction, when both character attributes and actions are expressed as complex natural language descriptions. We propose role-playing games as a testbed for this problem, and introduce a large corpus of game transcripts collected from online discussion forums. Using neural language models which combine character and action descriptions from these stories, we show that we can learn the latent ties. Action sequences are better predicted when the character performing the action is also taken into account, and vice versa for character attributes."
}



@inproceedings{martin2018dungeons,
title={Dungeons and {DQN}s: Toward Reinforcement Learning Agents that Play Tabletop Roleplaying Games},
author={Martin, Lara J and Sood, Srijan and Riedl, Mark O},
booktitle={Proceedings of the Joint Workshop on Intelligent Narrative Technologies and Workshop on Intelligent Cinematography and Editing},
editor={H. Wu, M. Si, A. Jhala},
location={Edmonton, Canada},
month={November},
year={2018},
publisher={CEUR-WS},
url={https://ceur-ws.org/Vol-2321/paper4.pdf}
}

@inproceedings{newman2022generating,
    title = "Generating Descriptive and Rules-Adhering Spells for {D}ungeons {\&} {D}ragons {F}ifth {E}dition",
    author = "Newman, Pax  and
      Liu, Yudong",
    editor = "Madge, Chris",
    booktitle = "Proceedings of the 9th Workshop on Games and Natural Language Processing within the 13th Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.games-1.7/",
    pages = "54--60",
    abstract = "We examine the task of generating unique content for the spell system of the tabletop roleplaying game Dungeons and Dragons Fifth Edition using several generative language models. Due to the descriptive nature of the game Dungeons and Dragons Fifth Edition, it presents a number of interesting avenues for generation and analysis of text. In particular, the {\textquotedblleft}spell{\textquotedblright} system of the game has interesting and unique characteristics as it is primarily made up of high level and descriptive text but has many of the game`s main rules embedded with that text. Thus, we examine the capabilities of several models on the task of generating new content for this game, evaluating the performance through the use of both score-based methods and a survey on the best performing model to determine how the generated content conforms to the rules of the game and how well they might be used in the game."
}



@inproceedings{Plaquet23,
  title     = {Powerset multi-class cross entropy loss for neural speaker diarization},
  author    = {Alexis Plaquet and Hervé Bredin},
  year      = {2023},
  booktitle = {Proc. Interspeech 2023},
  pages     = {3222--3226},
  doi       = {10.21437/Interspeech.2023-205},
  issn      = {2958-1796},
}


@inproceedings{pyannote.metrics,
  author = {Herv\'e Bredin},
  title = {{pyannote.metrics: a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems}},
  booktitle = {{Interspeech 2017, 18th Annual Conference of the International Speech Communication Association}},
  year = {2017},
  month = {August},
  address = {Stockholm, Sweden},
  url = {http://pyannote.github.io/pyannote-metrics},
}



@inproceedings{rameshkumar2020storytelling,
    title = "Storytelling with Dialogue: a Critical Role {D}ungeons and {D}ragons Dataset",
    author = "Rameshkumar, Revanth  and
      Bailey, Peter",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.459/",
    doi = "10.18653/v1/2020.acl-main.459",
    pages = "5121--5134",
    abstract = "This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game. The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues. In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches, and we provide an abstractive summarization benchmark and evaluation."
}



@INPROCEEDINGS{wang2023wespeaker,
  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Wespeaker: A Research and Production Oriented Speaker Embedding Learning Toolkit}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Codes;Production;Signal processing;Data models;Acoustics;Speaker recognition;Task analysis;Wespeaker;Speaker embedding;Speaker verification;Speaker diarization},
  doi={10.1109/ICASSP49357.2023.10096626}}



@article{CALLHOME,
	author = {Canavan, Alexandra and Graff, David and Zipperlen, George},
	title = {{CALLHOME} {A}merican {E}nglish {S}peech {LDC97S42}},
    year = {1997},
	journal={{P}hiladelphia: Linguistic Data Consortium},
    doi={https://doi.org/10.35111/exq3-x930}
}

@misc{pyannoteTechnical,
	author = {pyannote},
	title = {{Release Version 2.1.1 · pyannote/pyannote-audio}},
    year = {2023},
    url={https://github.com/pyannote/pyannote-audio/releases/tag/2.1.1},
}

@misc{spacy2,
   AUTHOR  = {Honnibal, Matthew AND Montani, Ines},
   TITLE   = {spaCy · {Industrial-strength Natural Language Processing in Python}},
   YEAR={2023},
   URL = {https://spacy.io/}
}

@misc{sileroVAD,
  author = {Silero Team},
  title = {Silero {VAD}: pre-trained enterprise-grade Voice Activity Detector {(VAD)}, Number Detector and Language Classifier},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/snakers4/silero-vad}},
  commit = {9060f664f20eabb66328e4002a41479ff288f14c},
  email = {hello@silero.ai}
}



@inbook{nagavi2024comprehensive,
author = {Nagavi, Trisiladevi C. and Samanvitha, S. and Sudhanva, Shreya and Shivakumar, Sukirth and Hullur, Vibha},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9781394214624},
title = {Comprehensive Analysis of State-of-the-Art Approaches for Speaker Diarization},
booktitle = {Automatic Speech Recognition and Translation for Low Resource Languages},
chapter = {19},
pages = {427-444},
doi = {10.1002/9781394214624.ch19},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394214624.ch19},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781394214624.ch19},
year = {2024},
keywords = {Speaker diarization, speech processing system, embedding, spectral clustering, mel-frequency cepstral coefficients, diarization error rate, audiovisual modeling},
abstract = {Summary Speaker diarization is the ability to compare, recognize, comprehend, and segregate different sound waves on the basis of the identity of the speaker. As an illustration of this theory, different ways to achieve these objectives are analyzed in this book chapter. Speaker diarization can prove to be crucial in the future with regards to the field of education, healthcare, forensics, smart traffic management, media, etc. There are numerous steps associated in the process of speaker diarization and each step can be accomplished using different models. The steps involved in the speaker diarization include voice activity detection, feature extraction, segmentation, embedding extraction, and clustering. Voice detection can be achieved using Simulink in Matlab, software such as Audacity, Webrtcvad, or other deep learning methods. Further, mel-frequency cepstral coefficients (MFCC) and linear predictive cepstral coefficients (LPCC) are well-known methods available for speech feature extraction. Additionally, segmentation can be achieved using metric-based approaches or by using deep neural networks. There are several frameworks available by Python for the purpose of embedding extraction based on the type of vectors to be extracted. As a last step, clustering can be realized through methods such as K-means, mean-shift, spectral clustering combined with distance metrics such as Euclidean distance, Minkowski's distance, etc. A comprehensive analysis approach for speaker diarization with a low diarization error rate (DER), datasets, challenges, and applications is discussed in this chapter.}
}


@inproceedings{grunert2023speaker,
  title={Speaker diarization systems in the context of forensic audio analysis},
  author={Gr{\"u}nert, David and de Spindler, Alexandre and Dellwo, Volker},
  booktitle={31st Annual Conference of the International Association for Forensic Phonetics and Acoustics (IAFPA), Zurich, Switzerland, 9-12 July 2023},
  pages={74--75},
  year={2023},
  organization={Centre for Forensic Phonetics and Acoustics (CFPA)},
  url={https://www.iafpa2023.uzh.ch/dam/jcr:519171ea-58b0-4ff4-8333-91664dc44a54/BoA-IAFPA23.pdf}
}


@mastersthesis{medaramitta2021evaluating,
  author       = {Raveendra Medaramitta},
  title        = {Evaluating the Performance of Using Speaker Diarization for Speech Separation of In-Person Role-Play Dialogues},
  school       = {Wright State University},
  year         = {2021},
  type         = {Master's Thesis},
  month        = {},
  address      = {},
  abstract     = {Development of professional communication skills, such as motivational interviewing, often requires experiential learning through expert instructor-guided role-plays between the trainee and a standard patient/actor. Due to the growing demand for such skills in practices, e.g., for health care providers in the management of mental health challenges, chronic conditions, substance misuse disorders, etc., there is an urgent need to improve the efficacy and scalability of such role-play based experiential learning, which are often bottlenecked by the time-consuming performance assessment process. WSU is developing ReadMI (Real-time Assessment of Dialogue in Motivational Interviewing) to address this challenge, a mobile AI solution aiming to provide automated performance assessment based on ASR and NLP. The main goal of this thesis research is to investigate current commercially available speaker diarization capabilities and evaluate their performance in separating the speeches between the trainee and the standard patient/actor in an in-person role-play training environment where the crosstalk could interfere with the operation and performance of ReadMI. Specifically, this thesis research has: 1.) identified the major commercially-available speaker diarization systems, such as those from Google, Amazon, IBM, and Rev.ai; 2.) designed and implemented corresponding evaluation systems that integrate these commercially available cloud services for operating in the in-person role-play training environments; and, 3.) completed an experimental study that evaluated and compared the performance of the speaker diarization services from Google and Amazon. The main finding of this thesis is that the current speaker diarization capabilities alone are not able to provide sufficient performance for our particular use case when integrating them into ReadMI for operating in in-person role-play training environments. But this thesis research potentially provides a clear baseline reference to future developers for integrating future speaker diarization capabilities into similar applications.},
  pages        = {45},
  department   = {Department of Computer Science and Engineering},
  url={https://corescholar.libraries.wright.edu/etd_all/2517/}
}
