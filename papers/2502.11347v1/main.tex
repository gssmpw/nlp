%%
%% This is file `main.tex' based on `sample-sigconf.tex' (q.v. for spurce of that,
%%
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the original source file `sample-sigconf.tex'
%% in the `Sample' folder.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.

%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%%\documentclass[manuscript,review,anonymous]{acmart}
%% This version is used for drafting and final submission
\documentclass[sigconf, noacm]{acmart}


%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
%% \providecommand\BibTeX{{%
%%    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%%\setcopyright{acmlicensed}
%%\copyrightyear{2024}
%%\acmYear{2024}
%%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Conference acronym 'XX]{Make sure to enter the correct
%%  conference title from your rights confirmation email}{June 03--05,
%%  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if th title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY} 
%%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.

%% % Location of your graphics files for figures, here a sub-folder to the main project folder
\graphicspath{{./images/}} 
\raggedbottom

\usepackage{soul}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{multirow,multicol}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Evaluating the Performance of the DeepSeek Model in Confidential Computing Environment}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Dong}
% %\authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% %\orcid{1234-5678-9012}
% \author{Qian Wang}
% %\authornotemark[1]
% \email{{cdong12}@ucmerced.edu}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

\author{Ben Dong}
\affiliation{%
  \institution{University of California, Merced}
 % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Merced}
  \country{USA}}
\email{cdong12@ucmerced.edu}

\author{Qian Wang}
\affiliation{%
  \institution{University of California, Merced}
  \city{Merced}
  \country{USA}
}
\email{qianwang@ucmerced.edu}

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}
\settopmatter{printacmref=false} % Remove ACM reference format
\renewcommand\footnotetextcopyrightpermission[1]{} % Remove copyright footnote
\acmConference[]{}{ }{ }

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The increasing adoption of Large Language Models (LLMs) in cloud environments raises critical security concerns, particularly regarding model confidentiality and data privacy. Confidential computing, enabled by Trusted Execution Environments (TEEs), offers a promising solution to mitigate these risks. However, existing TEE implementations, primarily CPU-based, struggle to efficiently support the resource-intensive nature of LLM inference and training. In this work, we present the first evaluation of the DeepSeek model within a TEE-enabled confidential computing environment, specifically utilizing Intel Trust Domain Extensions (TDX). Our study benchmarks DeepSeekâ€™s performance across CPU-only, CPU-GPU hybrid, and TEE-based implementations. For smaller parameter sets, such as DeepSeek-R1-1.5B, the TDX implementation outperforms the CPU version in executing computations within a secure environment. It highlights the potential for efficiently deploying LLM models on resource-constrained systems while ensuring security. The overall GPU-to-CPU performance ratio averages 12 across different model sizes, with smaller models exhibiting a lower ratio. Additionally, we provide foundational insights and guidance on optimizing CPU-GPU confidential computing solutions for scalable and secure AI deployments. Our findings contribute to the advancement of privacy-preserving AI, paving the way for efficient and secure LLM inference in confidential computing environments.
\end{abstract}

%%
%% The code below is generated by the tool at: http://dl.acm.org/ccs.cfm
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002978.10003006.10003007.10003009</concept_id>
       <concept_desc>Security and privacy~Trusted computing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Trusted computing}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Confidential Computing, Trusted Execution Environment, Large Language Models, Performance Optimization}

%% The following are not a requirement, delete if not using
%\received{20 February 2024}  %% inital submission date
%\received[revised]{12 March 2024} %% interim new draft
%\received[accepted]{5 June 2024}  %% publication version

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Cloud computing provides scalability and flexibility for computational tasks, particularly in machine learning applications. However, deploying ML models and processing sensitive data in cloud infrastructure introduces significant security risks. For example, unauthorized access to confidential user data can compromise data security within the cloud environment \cite{wallace2020concealed,tramer2016stealing}. This is especially critical in sectors like healthcare, where patient records could be exposed, or finance, where unauthorized access to banking transactions could lead to fraud and identity theft. Additionally, state-of-the-art machine learning models, such as large language models, require substantial financial investment for training and development. When deployed in cloud environments, they become vulnerable to theft in untrusted cloud infrastructure. 

In this work, we consider a threat model where both the user data and the machine learning model are at risk of leakage when processed in the shared cloud environment. Sensitive information, including personal photos, health records, and proprietary datasets, is at risk of exposure through reverse data inference attacks. Also, the integrity of ML models transmitted and executed in the cloud is susceptible to adversarial manipulations, including model inversion and poisoning attacks \cite{carlini2020extracting}. Protecting both data and model confidentiality is therefore critical.

Confidential computing, particularly through Trusted Execution Environments, provides a promising solution to secure both data and models. TEE provides a secure enclave for computation, protecting both data and models while mitigating risks in cloud computing environments. Prior research has leveraged TEEs, such as Intelâ€™s Software Guard Extensions (SGX) \cite{intel_sgx_whitepaper}, to secure ML workloads by isolating sensitive computations within secure enclaves \cite{narra2019privacy}. While SGX offers strong protection, its limited memory capacity (approximately 1 GB) and complex interface pose challenges for large-scale ML applications \cite{shen2022soter,sun2023shadownet}. To overcome these limitations, Intelâ€™s Trust Domain Extensions (TDX) \cite{intel_tdx_whitepaper} introduces secure Virtual Machines (VMs) that can process larger ML models while maintaining robust security guarantees.

Even though TDX significantly expands memory capacity compared to its predecessors, deploying the advanced large language models, such as GPT \cite{brown2020language} and Gemini \cite{deepmind2023gemini}, in a confidential computing environment presents additional challenges. First, these models are relatively large, typically starting from at least 1 billion parameters and reaching over 100 billion or more. Even hosting these models locally poses a significant challenge due to their high memory requirements.
Additionally, these models demand extensive computational resources, including high-performance GPUs for both training and inference. Ensuring security while maintaining efficiency in TEE-based environments requires careful optimization of both hardware and software components. 

Among these LLMs, DeepSeek is an advanced AI-driven model optimized for efficient resource utilization and high-performance computing \cite{bi2024deepseek}. Its main difference from other models lies in its ability to intelligently allocate resources using cutting-edge algorithms to ensure superior computational efficiency. It also enhances scalability, particularly in secure environments like TDX. For example, the distilled version of the DeepSeek model offers significant advantages by reducing model size and memory footprint, enabling intelligent resource allocation for optimal performance, even in constrained environments \cite{guo2024deepseek}. This is especially valuable in secure environments like TDX, where both efficiency and security are critical.
Exploring DeepSeek in TDX enables us to leverage its capabilities for secure and high-performance computing, facilitating faster data processing. This integration not only protects models from various security threats but also drives innovation in secure computing environments. Moreover, it provides insights into effectively addressing the security vs. performance trade-off, ensuring an optimal balance between protection and efficiency.

\subsection*{Contribution}

The main contributions of this paper are summarized as follows,
\begin{itemize}
    \item This paper presents the first evaluation of the DeepSeek model's performance within a TEE. By deploying DeepSeek in a secure enclave, we analyze how TEEs impact computational efficiency, resource utilization, and the inference performance. Our study provides a baseline understanding of DeepSeekâ€™s behavior in confidential computing environments, offering critical insights into its feasibility for secure AI inference.
    \item We conduct a comparative analysis of DeepSeekâ€™s performance across TEE-based, CPU-only, and CPU-GPU implementations. This benchmarking allows us to identify key performance bottlenecks and highlight trade-offs between security and computational efficiency. Our findings also help confidential computing technology vendors in optimizing CPU-GPU integration and addressing scalability challenges in secure AI workloads.
    \item Beyond performance evaluation, this study provides foundational insights for the broader research community on adopting CPU-GPU confidential computing solutions. By establishing a framework for evaluating large-scale AI models in TEEs, we contribute to the advancement of privacy-preserving AI technologies.
\end{itemize}

%In this paper, we present the first evaluation of the DeepSeek model's performance within a TEE. Our goal is to analyze the resource optimization of the DeepSeek model and compare its performance against CPU-only and CPU-GPU-based implementations as baselines. We highlight key observations from our experiments to assist confidential computing technology vendors in identifying and addressing potential performance bottlenecks in their implementations. 

%This evaluation provides valuable insights into DeepSeek's computational efficiency and its suitability for confidential computing environments. Additionally, our findings will lay the groundwork for future efforts to integrate and guide the broader research community in making informed decisions on adopting CPU-GPU confidential computing solutions effectively.




\section{Implementing LLM Models on TEE}

\subsection{Confidential Computing} Advanced LLMs are expensive to train and fine-tune due to their high computational and resource requirements. For instance, state-of-the-art models like ChatGPT-4 require thousands of GPUs (e.g., over 25,000 NVIDIA A100 GPUs) and months of training, with estimated costs exceeding \$100 million.  As a result, trained and fine-tuned models, particularly their weights, are highly sensitive intellectual property (IP) that must be safeguarded against unauthorized access and misuse. Additionally, during inference, user inputs may contain confidential or personally identifiable information (PII), which must be protected from potential security threats and malicious actors to ensure data privacy and integrity.


\begin{figure}[htb]
    \centering
    \captionsetup{font=small} 
    \includegraphics[width=1.1\linewidth]{images/tdx.drawio.pdf}
    \caption{Trust Environment Settings with Data I/O to Shared memory and GPU.}
    \label{fig:tdx}
\end{figure}

Confidential computing offers a robust solution by ensuring that all data and workloads are executed within a secure enclave, where input-output communications are encrypted, shown as in Figure \ref{fig:tdx}. The Virtual Machine container, secured within a trusted enclave, ensures a confidential execution environment with exclusive access to allocated private memory. All plaintext computations occur securely within this isolated domain. Data exchanges between the enclave and shared memory undergo encryption and decryption. Additionally, the system interfaces with a PCI-connected device to facilitate collaborative processing between the CPU and GPU to optimize workload distribution for LLM computations.
This prevents unauthorized access and mitigates attacks to extract private data. However, current confidential computing solutions primarily rely on CPU-based TEEs with limited resources, restricting their ability to support large models like LLMs. This constraint creates a trade-off between security and performance, as secure execution environments struggle to handle the computational demands of large-scale AI models efficiently. 

Here, we evaluate the performance trade-offs between CPU-based confidential computing (CPU-TDX), standard CPU execution (CPU only), and GPU-accelerated platforms (CPU-GPU). Our study provides valuable insights for selecting the optimal balance between security and computational efficiency in deploying confidential inference workloads. 

%The high-level objective of CPU-based confidential computing is to enable secure enclaves, also known as TEEs, to isolate sensitive computations and protect against a wide range of attacks, including side-channel attacks, memory snooping, and threats from untrusted cloud operators. 

\subsection{LLM Models} 
For this evaluation, we employed DeepSeek R1, a state-of-the-art reasoning-oriented large language model, across three different parameter configurations: 1.5 billion (1.5B), 7 billion (7B), and 14 billion (14B). DeepSeek R1 is designed for efficient logical reasoning and problem-solving, making it well-suited for workloads where reasoning and logic are critical. Running localized LLM inference within a confidential computing environment provides enhanced security guarantees by ensuring that both model weights and user input remain protected from unauthorized access or leakage. This is particularly important for enterprises and organizations handling sensitive data, as it mitigates risks associated with model inversion attacks, data breaches, and side-channel attacks. By leveraging Intel TDX, we analyze the trade-offs between security and computational performance when deploying DeepSeek R1 within a trusted execution environment, offering insights into the feasibility of secure and efficient AI inference.

\section{Experimental Results}
\subsection{Evaluation Platform}
The evaluation was conducted using a high-performance host machine equipped with two Intel Xeon Gold 6530 CPUs, each featuring 32 cores and 512 GB of DDR5 memory operating at 4800 MHz. The testing environment was deployed within an Intel Trust Domain Extensions virtual machine, configured according to Canonical Ubuntuâ€™s official installation guide. The host system's BIOS settings were adjusted to enable TDX support, ensuring the correct hardware configuration for secure execution. To benchmark the performance of LLM inference, Ollama was executed directly within the TDX VM. A comparative evaluation was performed using Docker containers to simulate different execution environments. Specifically, a container was launched from Ollamaâ€™s official image with CPU and memory constraints simulating the TDX VM configuration. Additionally, a separate container was created with GPU access using NVIDIAâ€™s container toolkit to assess the performance differential between CPU-based execution and GPU acceleration. All performance data was gathered using Ollamaâ€™s built-in logging mechanisms, ensuring consistency across test conditions. This setup enables a comprehensive analysis of inference efficiency in confidential computing environments while highlighting the trade-offs between security and computational performance.
\hspace{-0.5cm}
\begin{figure}[htb]
\hspace{-0.5cm}
    \centering
    \hspace{-0.5cm}
    \captionsetup{font=small} 
    \hspace{-0.5cm}
    \includegraphics[width=1.1\linewidth]{images/download.png}
    \hspace{-0.5cm}
    \caption{Model Performance on CPU with and w/o TDX.}
    \hspace{-0.5cm}
    \label{fig:performance}
    \hspace{-0.5cm}
\end{figure}

\subsection{Inference Workload} LLM inference in a TEE involves executing a trained machine-learning model to generate outputs based on user input prompts while ensuring data confidentiality and model integrity. In this benchmark, we evaluate different configurations where either a CPU or a CPU-GPU combination handles the workload. Typically, in standard LLM inference, the CPU is responsible for data loading, preprocessing, and postprocessing, while the GPU accelerates core computations. However, in confidential computing mode, the enclave-CPU takes on additional responsibilities for secure data processing to maintain model integrity and data privacy. The inference workflow begins with loading the trained machine learning model into the memory allocated to the Trusted Domain (TD). Once inside the TD, computations occur in an isolated environment without direct communication with the untrusted external environment, ensuring that sensitive data and model parameters remain protected from potential threats. This evaluation provides insights into the performance impact of running LLM inference within a TEE and highlights the trade-offs between security and computational efficiency across different hardware configurations.



\begin{table}[h]
\captionsetup{font=small} 
\centering
\resizebox{\columnwidth}{!}{ % Scales the table to fit one column
\begin{tabular}{c||c|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Model Size}} & \multicolumn{3}{c|}{\textbf{Performance (tokens/s)}} & \multicolumn{2}{c}{\textbf{Comp. Ratio}} \\ \cline{2-6}
 & \textbf{GPU-CPU} & \textbf{CPU only} & \textbf{TDX} & \textbf{$\frac{GPU}{TDX}$} & \textbf{$\frac{CPU}{TDX}$} \\ \hline
1.5B & 202.88  & 10.25  & 25.67 & 7.9 & 0.4 \\ \hline
7B    & 117.02 & 8.53  & 6.42  & 18.2 & 1.3 \\ \hline
14B & 69.14 & 7.13 & 3.44 & 9.7 & 2 \\ \hline
\end{tabular}
}
\caption{Comparison of Performance by Model With GPU (Tokens/s): GPU-CPU represents inference running directly on the host machine with GPU accelerations enabled. CPU-only refers to running on the host machine without GPU acceleration enabled. TDX runs within a TD container and restricted to 32 CPU cores and 100GB of DRAM.}
\label{tab:model_performance_gpu}
\end{table}


\subsection{Result Analysis} The results from both the figure and the table highlight the performance differences between Docker-based CPU execution and Intel TDX, suggesting that TDX's optimizations for secure computing may also enhance CPU performance in certain scenarios. In CPU-only configurations (as shown in Figure \ref{fig:performance}), the TDX environment with 62 cores and 510GB of DRAM achieves the highest evaluation rate for the smallest model (1.5B), reaching approximately 25.71 tokens/sâ€”more than twice the performance observed in Docker-based CPU tests. This significant improvement could be attributed to TDX's optimized CPU execution, minimizing software-induced inefficiencies. However, as the size of the model increases, the performance gap narrows, with both TDX and Docker CPU-based environments showing significant slowdowns for the 14B model, where evaluation rates are limited to around 3.33 tokens/s. This suggests that while TDX can leverage high-core configurations effectively for smaller models, larger models demand higher memory bandwidth and computational resources, limiting the advantage of TDX's CPU optimizations.

In contrast, Table \ref{tab:model_performance_gpu} shows that GPU acceleration drastically improves inference speed across all configurations, far surpassing CPU-only performance. Even with resource constraints, GPU-based inference reaches over 202 tokens/s for the 1.5B model, significantly outperforming any CPU configuration. The 7B and 14B models also maintain high performance, achieving approximately 117 and 69 tokens/s, respectively. These results emphasize the critical role of GPU acceleration in the efficient deployment of large LLMs. However, current TDX implementations do not fully support GPU utilization within the secure enclave, presenting a key limitation in balancing security and performance. Future research should focus on integrating GPU acceleration within TDX while preserving its security guarantees, enabling confidential AI inference to achieve both high security and computational efficiency.

\section{Discussion}
Our evaluation of the DeepSeek model in a TEE underscores the complexities and trade-offs involved in the deployment of LLMs securely. Although TEEs, such as Intel-TDX, offer robust protections against unauthorized access, the reliance on CPU-based enclaves introduces performance limitations, particularly for compute-intensive workloads with larger size of models. GPU acceleration is essential for efficient model execution, yet current confidential computing frameworks do not fully support GPU-based processing within secure enclaves. This limitation results in a significant trade-off between maintaining security and achieving high-performance inference.

Furthermore, our results highlight key considerations for optimizing confidential AI deployments. The computational overhead of maintaining security, such as memory encryption and secure data handling within the enclave, leads to increased inference latency. However, as confidential computing technologies evolve, emerging solutions like secure GPU virtualization and hybrid execution models may help mitigate these overheads. Future research should focus on refining these technologies to enable efficient, secure AI processing without sacrificing performance.

\section{Conclusion}

This paper presents the first performance evaluation of the DeepSeek model in a confidential computing environment, comparing CPU-based TEEs, standard CPU execution, and GPU-accelerated platforms. Our findings provide critical insights into the feasibility of running LLM inference securely, highlighting the need for balancing security and computational efficiency. While CPU-based TEE ensures strong model integrity and data confidentiality, its performance constraints necessitate further advancements in confidential computing frameworks, particularly in enabling GPU acceleration. To sum up, this work serves as a foundation for future studies, guiding the development of scalable and efficient confidential computing solutions for AI applications.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% Acknowledgements go here. Delete enclosing begin/end markers if there are no acknowledgements.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

%%
\end{document}
\endinput
%%
%% End of file `main.tex'.
