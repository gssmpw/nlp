\section{Related Work}
A straightforward approach to incorporating contextual information from frame-level embeddings is statistical aggregation across the temporal dimension, such as mean and variance. 
However, not all the frame-level embeddings contribute equally to the utterance-level representation for Language Identification. Therefore, relying solely on naive statistics might not adequately capture the global interactions across temporal dimensions. 
To address this limitation, \textbf{A}ttentive \textbf{S}tatistical \textbf{P}ooling (ASP) has emerged as a valuable technique.
In this approach, the frame-level embeddings are combined through attention weights followed by the statistical pooling layer resulting in more emphasis on the most discriminative features while reducing the impacts of less relevant features.  
Okabe \emph{et al.}____ proposed the idea of ASP and applied it to speaker verification.
The authors showed that ASP can capture long-term variations in speaker characteristics more effectively. 
On the other hand, Monteiro \emph{et al.}____ used the idea of ASP for end-to-end spoken language identification.
Wang~\emph{et al.}____ introduced an attentive temporal pooling method tailored for streaming language identification, enabling real-time processing of sequential data. 
Recently, Liu~\emph{et al.}____ integrated ASP into a self-supervised learning framework for spoken language identification. 
The standard attention formulation employed in ASP exhibits quadratic computational complexity, posing scalability and inference challenges.\par  
In the next section, we describe the adoption of recently developed attention formulations.
In particular, we choose to study performer-attention____ and agent-attention____.
% When considering their computational complexity, the vanilla self-attention is quadratic in terms of sequence length, the agent attention, and performer-attention both have linear time complexity. 
In contrast to vanilla self-attention used in ASP, agent-attention and performer-attention provide a linear complexity in terms of input sequence length.
We demonstrate their efficacy against vanilla self-attention when combined with statistical pooling for the language identification task.
\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/lid_block_diag.pdf}
    \caption{Block diagram for the LID classifier. The BEST-RQ block is pre-trained using self-supervised learning framework followed by fine-tuning of LID classifier.}
    \label{fig:lid_block}
\end{figure}