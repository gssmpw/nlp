@article{ASP_Okabe2018,
  title={Attentive Statistics Pooling for Deep Speaker Embedding},
  author={Koji Okabe and Takafumi Koshinaka and Koichi Shinoda},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.10963}
}
@inproceedings{ASP_Monteiro2020,
  title={On The Performance of Time-Pooling Strategies for End-to-End Spoken Language Identification},
  author={Jo{\~a}o Monteiro and Md. Jahangir Alam and Tiago H. Falk},
  booktitle={International Conference on Language Resources and Evaluation},
  year={2020}
}
@inproceedings{ASP_Wang2022,
  title={Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech},
  author={Quan Wang and Yang Yu and Jason W. Pelecanos and Yiling Huang and Ignacio L{\'o}pez-Moreno},
  booktitle={The Speaker and Language Recognition Workshop},
  year={2022}
}
@article{ASP_Liu2022,
  title={Efficient Self-Supervised Learning Representations for Spoken Language Identification},
  author={Hexin Liu and Leibny Paola Garc{\'i}a-Perera and Andy W. H. Khong and Eng Siong Chng and Suzy J. Styles and Sanjeev Khudanpur},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2022},
  volume={16},
  pages={1296-1307}
}
 % use the Crossref field to copy any unspecified fields (such as booktitle) from another entry, 
@article{agent_attn_Han2023,
  title={Agent Attention: On the Integration of Softmax and Linear Attention},
  author={Dongchen Han and Tianzhu Ye and Yizeng Han and Zhuofan Xia and Shiji Song and Gao Huang},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.08874}
}
@article{performer_Choromanski2020,
  title={Rethinking Attention with Performers},
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tam{\'a}s Sarl{\'o}s and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy J. Colwell and Adrian Weller},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.14794}
}
@article{swish_Ramachandran2017,
  title={Swish: a Self-Gated Activation Function},
  author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  journal={arXiv: Neural and Evolutionary Computing},
  year={2017}
}
@article{asr_Zhang2022,
  title={Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification},
  author={C. Zhang and Bo Li and Tara N. Sainath and Trevor Strohman and Sepand Mavandadi and Shuo-yiin Chang and Parisa Haghani},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.06058}
}
@inproceedings{lid_asr_Weiner2012,
  title={Integration of language identification into a recognition system for spoken conversations containing code-Switches},
  author={Jochen Weiner and Ngoc Thang Vu and Dominic Telaar and Florian Metze and Tanja Schultz and Dau-Cheng Lyu and Chng Eng Siong and Haizhou Li},
  booktitle={Workshop on Spoken Language Technologies for Under-resourced Languages},
  year={2012}
}
@article{lid_asr_GonzalezDominguez2015,
  title={A Real-Time End-to-End Multilingual Speech Recognition Architecture},
  author={Javier Gonzalez-Dominguez and David Eustis and Ignacio L{\'o}pez-Moreno and Andrew W. Senior and Fran√ßoise Beaufays and Pedro J. Moreno},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2015},
  volume={9},
  pages={749-759}
}
@article{lid_translation_Sefara2021,
  title={Transformer-based Machine Translation for Low-resourced Languages embedded with Language Identification},
  author={T. Sefara and Skhumbuzo Zwane and Nelisiwe Gama and Hlawulani Sibisi and Phillemon N. Senoamadi and Vukosi Marivate},
  journal={ \textup{in} Proc. of Information Communications Technology and Society (ICTAS)},
  year={2021},
  pages={127-132}
}
@article{lid_contentFilteringCaswell2020,
  title={Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus},
  author={Isaac Caswell and Theresa Breiner and Daan van Esch and Ankur Bapna},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.14571}
}
@article{lid_accent_Etman2015,
  title={Language and Dialect Identification: A survey},
  author={A. Etman and Louis Beex},
  journal={\textup{in} Proc. of SAI Intelligent Systems Conference (IntelliSys)},
  year={2015},
  pages={220-231}
}
@inproceedings{lid_shortseg_balleda2000,
  title={Language identification from short segments of speech},
  author={Balleda, Jyotsana and Murthy, Hema A and Nagarajan, T},
  booktitle={Sixth International Conference on Spoken Language Processing},
  year={2000}
}
@article{ssl_survey_liu2021,
  title={Self-supervised learning: Generative or contrastive},
  author={Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
  journal={IEEE transactions on knowledge and data engineering},
  volume={35},
  number={1},
  pages={857--876},
  year={2021},
  publisher={IEEE}
}
@article{ssl_cpc_oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}
@article{w2v2_baevski2020,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}
@inproceedings{w2v2bert_chung2021,
  title={W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training},
  author={Chung, Yu-An and Zhang, Yu and Han, Wei and Chiu, Chung-Cheng and Qin, James and Pang, Ruoming and Wu, Yonghui},
  booktitle={IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={244--250},
  year={2021}
}
@article{hubert_hsu2021,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}
@article{attn_cv_guo2022,
  title={Attention mechanisms in computer vision: A survey},
  author={Guo, Meng-Hao and Xu, Tian-Xing and Liu, Jiang-Jiang and Liu, Zheng-Ning and Jiang, Peng-Tao and Mu, Tai-Jiang and Zhang, Song-Hai and Martin, Ralph R and Cheng, Ming-Ming and Hu, Shi-Min},
  journal={Computational visual media},
  volume={8},
  number={3},
  pages={331--368},
  year={2022},
  publisher={Springer}
}
@inproceedings{bestrq,
  title={Self-supervised learning with random-projection quantizer for speech recognition},
  author={Chiu, Chung-Cheng and Qin, James and Zhang, Yu and Yu, Jiahui and Wu, Yonghui},
  booktitle={International Conference on Machine Learning},
  pages={3915--3924},
  year={2022},
  organization={PMLR}
}
@article{voxpopuli,
  title={VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation},
  author={Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2101.00390},
  year={2021}
}
@inproceedings{fleurs,
  title={Fleurs: Few-shot learning evaluation of universal representations of speech},
  author={Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur},
  booktitle={IEEE Spoken Language Technology Workshop (SLT)},
  pages={798--805},
  year={2023},
}
@inproceedings{voxlingua,
  title={VoxLingua107: a dataset for spoken language recognition},
  author={Valk, J{\"o}rgen and Alum{\"a}e, Tanel},
  booktitle={IEEE Spoken Language Technology Workshop (SLT)},
  pages={652--658},
  year={2021}
}
@inproceedings{gaussian_kernel,
  author    = {Ali Rahimi and
               Benjamin Recht},
  title     = {Random Features for Large-Scale Kernel Machines},
  booktitle = {Advances in Neural Information Processing Systems 20, Proceedings
               of the Twenty-First Annual Conference on Neural Information Processing
               Systems, Vancouver, British Columbia, Canada, December 3-6, 2007},
  pages     = {1177--1184},
  publisher = {Curran Associates, Inc.},
  year      = {2007},
  url       = {http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines},
}
@inproceedings{dwc_featdiverse,
  title={Flatten transformer: Vision transformer using focused linear attention},
  author={Han, Dongchen and Pan, Xuran and Han, Yizeng and Song, Shiji and Huang, Gao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5961--5971},
  year={2023}
}
@article{attention_Vaswani2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{nlp_survey2020,
  title={An introductory survey on attention mechanisms in {NLP} problems},
  author={Hu, Dichao},
  booktitle={Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2},
  pages={432--448},
  year={2020},
  organization={Springer}
}
@inproceedings{ssl_asr_FB_kim2023,
  title={ASBERT: Asr-specific self-supervised learning with self-training},
  author={Kim, Hyung Yong and Kim, Byeong-Yeol and Yoo, Seung Woo and Lim, Youshin and Lim, Yunkyu and Lee, Hanbin},
  booktitle={IEEE Spoken Language Technology Workshop (SLT)},
  pages={9--14},
  year={2023},
}
@article{transformer_survey_zhuang2023,
  title={A survey on efficient training of transformers},
  author={Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
  journal={arXiv preprint arXiv:2302.01107},
  year={2023}
}
@article{adam_kingma2014,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{lr_bert_devlin2019,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}