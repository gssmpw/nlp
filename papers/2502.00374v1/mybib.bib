@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}

@misc{popuri2022enhanced,
      title={Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation}, 
      author={Sravya Popuri and Peng-Jen Chen and Changhan Wang and Juan Pino and Yossi Adi and Jiatao Gu and Wei-Ning Hsu and Ann Lee},
      year={2022},
      eprint={2204.02967},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ren2022fastspeech,
      title={FastSpeech 2: Fast and High-Quality End-to-End Text to Speech}, 
      author={Yi Ren and Chenxu Hu and Xu Tan and Tao Qin and Sheng Zhao and Zhou Zhao and Tie-Yan Liu},
      year={2022},
      eprint={2006.04558},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@inproceedings{hu2021neural,
title={Neural Dubber: Dubbing for Videos According to Scripts},
author={Hu, Chenxu and Tian, Qiao and Li, Tingle and Yuping, Wang and Wang, Yuxuan and Zhao, Hang},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021}
}

@misc{wang2018style,
      title={Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis}, 
      author={Yuxuan Wang and Daisy Stanton and Yu Zhang and RJ Skerry-Ryan and Eric Battenberg and Joel Shor and Ying Xiao and Fei Ren and Ye Jia and Rif A. Saurous},
      year={2018},
      eprint={1803.09017},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lee2022direct,
      title={Direct speech-to-speech translation with discrete units}, 
      author={Ann Lee and Peng-Jen Chen and Changhan Wang and Jiatao Gu and Sravya Popuri and Xutai Ma and Adam Polyak and Yossi Adi and Qing He and Yun Tang and Juan Pino and Wei-Ning Hsu},
      year={2022},
      eprint={2107.05604},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{huang2023transpeech,
      title={TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation}, 
      author={Rongjie Huang and Jinglin Liu and Huadai Liu and Yi Ren and Lichao Zhang and Jinzheng He and Zhou Zhao},
      year={2023},
      eprint={2205.12523},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sun2020fullyhierarchical,
      title={Fully-hierarchical fine-grained prosody modeling for interpretable speech synthesis}, 
      author={Guangzhi Sun and Yu Zhang and Ron J. Weiss and Yuan Cao and Heiga Zen and Yonghui Wu},
      year={2020},
      eprint={2002.03785},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@misc{wang2017tacotron,
      title={Tacotron: Towards End-to-End Speech Synthesis}, 
      author={Yuxuan Wang and RJ Skerry-Ryan and Daisy Stanton and Yonghui Wu and Ron J. Weiss and Navdeep Jaitly and Zongheng Yang and Ying Xiao and Zhifeng Chen and Samy Bengio and Quoc Le and Yannis Agiomyrgiannakis and Rob Clark and Rif A. Saurous},
      year={2017},
      eprint={1703.10135},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shen2018natural,
      title={Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions}, 
      author={Jonathan Shen and Ruoming Pang and Ron J. Weiss and Mike Schuster and Navdeep Jaitly and Zongheng Yang and Zhifeng Chen and Yu Zhang and Yuxuan Wang and RJ Skerry-Ryan and Rif A. Saurous and Yannis Agiomyrgiannakis and Yonghui Wu},
      year={2018},
      eprint={1712.05884},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ren2019fastspeech,
      title={FastSpeech: Fast, Robust and Controllable Text to Speech}, 
      author={Yi Ren and Yangjun Ruan and Xu Tan and Tao Qin and Sheng Zhao and Zhou Zhao and Tie-Yan Liu},
      year={2019},
      eprint={1905.09263},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hsu2021hubert,
      title={HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units}, 
      author={Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},
      year={2021},
      eprint={2106.07447},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2021voxpopuli,
      title={VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation}, 
      author={Changhan Wang and Morgane Rivière and Ann Lee and Anne Wu and Chaitanya Talnikar and Daniel Haziza and Mary Williamson and Juan Pino and Emmanuel Dupoux},
      year={2021},
      eprint={2101.00390},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kong2020hifigan,
      title={HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis}, 
      author={Jungil Kong and Jaehyeon Kim and Jaekyoung Bae},
      year={2020},
      eprint={2010.05646},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{ljspeech17,
  author       = {Keith Ito and Linda Johnson},
  title        = {The LJ Speech Dataset},
  howpublished = {\url{https://keithito.com/LJ-Speech-Dataset/}},
  year         = 2017
}

@misc{skerryryan2018endtoend,
      title={Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron}, 
      author={RJ Skerry-Ryan and Eric Battenberg and Ying Xiao and Yuxuan Wang and Daisy Stanton and Joel Shor and Ron J. Weiss and Rob Clark and Rif A. Saurous},
      year={2018},
      eprint={1803.09047},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}






@article{Davis80-COP,
	Author = {Steven B. Davis and Paul Mermelstein},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {4},
	Pages = {357--366},
	Title = {Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences},
	Volume = {28},
  Month = aug,
	Year = {1980}}

@article{Rabiner89-ATO,
	Author = {Lawrence R. Rabiner},
	Journal = {Proceedings of the IEEE},
	Number = {2},
	Pages = {257--286},
	Title = {A Tutorial on Hidden {Markov} Models and Selected Applications in Speech Recognition},
	Volume = {77},
  Month = feb,
	Year = {1989}}

@book{Hastie09-TEO,
	Address = {New York},
	Author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	Publisher = {Springer},
	Title = {The Elements of Statistical Learning -- Data Mining, Inference, and Prediction},
	Year = {2009}}

@inproceedings{Smith22-XXX,
	Author = {Jane Smith and Firstname2 Lastname2 and Firstname3 Lastname3},
	Pages = {100--104},
	Title = {A really good paper about {D}ynamic {T}ime {W}arping},
 	Booktitle = {Proc. {INTERSPEECH} 2022 -- 23\textsuperscript{rd} Annual Conference of the International Speech Communication Association},
    Address = {{Incheon, Korea}},
    Month = {{Sep.}},
	Year = {2022}}

 % use the Crossref field to copy any unspecified fields (such as booktitle) from another entry, 
@inproceedings{Jones22-XXX,
	Author = {Robert Jones and Firstname2 Lastname2 and Firstname3 Lastname3},
	Crossref = {Smith22-XXX},
	Pages = {105--109},
	Title = {An excellent paper introducing the {ABC} toolkit}}

@inproceedings{moore19_interspeech,
  author={Roger K. Moore and Lucy Skidmore},
  title={On the Use/Misuse of the Term {`Phoneme'}},
    Address = {{Graz, Austria}},
    Month = {{Sep.}},
  year=2019,
 	Booktitle = {Proc. {INTERSPEECH} 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association},
  pages={2340--2344},
  doi={10.21437/Interspeech.2019-2711}
}





%%% ====================================================================
%%%  BibTeX-file{
%%%     author          = "David Rhead",
%%%     version         = "1.00",
%%%     date            = "17 Feb 1990",
%%%     time            = "17:00 GMT",
%%%     filename        = "test.bib",
%%%     address         = "Cripps Computing Centre,
%%%                        University of Nottingham,
%%%                        University Park,
%%%                        Nottingham,
%%%                        NG7 2RD,
%%%                        United Kingdom",
%%%     telephone       = "+44 602 484848 Ext 2670",
%%%     FAX             = "+44 602 588138",
%%%     checksum        = "05151 839 2908 25082",
%%%     email           = "David_Rhead at uk.ac.nott.vme (JANET)",
%%%     codetable       = "ISO/ASCII",
%%%     keywords        = "bibliography, citation, references",
%%%     supported       = "no",
%%%     docstring       = "This BibTeX database file contains entries
%%%                        designed for testing whether a BibTeX style
%%%                        file lays references out as recommended by
%%%                        certain authorities.  (Note, however, that
%%%                        the BS 1629 examples are from the 1976
%%%                        edition.  The file needs updating to use
%%%                        examples from the 1989 edition instead.)
%%%
%%%                        The checksum field above contains a CRC-16
%%%                        checksum as the first value, followed by the
%%%                        equivalent of the standard UNIX wc (word
%%%                        count) utility output of lines, words, and
%%%                        characters.  This is produced by Robert
%%%                        Solovay's checksum utility.",
%%%  }
%%% ====================================================================
%% @COMMENT{Some other standard works describing conventions for citations
%%      and bibliographies}

@Book{Eco:1990,
  author = 	 "Umberto Eco",
  title = 	 "The Limits of Interpretation",
  publisher = 	 "Indian University Press",
  year = 	 1990}

@Inproceedings{Martin-90,
  author = 	 "Jannik Strötgen and Michael Gertz",
  title = 	 "Temporal Tagging on Different Domains: Challenges, Strategies, and Gold Standards",
  booktitle = "Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)",
  year = 	 "2012",
  month = 	 "may",
  date =     "23-25",
  pages = 	 "3746--3753",
  address =  "Istanbul, Turkey",
  editor = 	 "Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet U\v{g}ur Do\v{g}an and Bente Maegaard and Joseph Mariani and Asuncion Moreno and Jan Odijk and Stelios Piperidis",
  publisher = "European Language Resource Association  (ELRA)",
  isbn = "978-2-9517408-7-7",
}

@Book{Chercheur-94,
  title =        "Case-Based Reasoning",
  author =       "J.L. Chercheur",
  publisher =    "Morgan Kaufman Publishers",
  year =         "1994",
  edition =      "2nd",
  address =      "San Mateo, CA",
}

@Article{CastorPollux-92,
  author =       "A. Castor and L. E. Pollux",
  title =        "The use of user modelling to guide inference and learning",
  journal =      "Applied Intelligence",
  volume =       "2(1)",
  pages =        "37--53",
  year =         "1992",
}

%fictitious reference
@Book{Superman-Batman-Catwoman-Spiderman-00,
  author =       "S. Superman and B. Batman and C. Catwoman and S. Spiderman",
  title =        "Superheroes experiences with books",
  journal =      "Journal journal journal",
  year =         2000,
  publisher =    "The Phantom Editors Associates",
  edition =      "20th",
  address =      "Gotham City",
}


%% @COMMENT{Some examples taken from document describing BS 1629}

@Book{hoel-71-whole,
  author =       "Paul Gerhard Hoel",
  title =        "Elementary Statistics",
  publisher =    "Wiley",
  year =         "1971",
  series =       "Wiley series in probability and mathematical
                 statistics",
  address =      "New York, Chichester",
  edition =      "3rd",
  note =         "ISBN 0~471~40300",
}

@Inbook{hoel-71-portion,
  author =       "Paul Gerhard Hoel",
  title =        "Elementary Statistics",
  publisher =    "Wiley",
  year =         "1971",
  series =       "Wiley series in probability and mathematical
                 statistics",
  address =      "New York, Chichester",
  edition =      "3rd",
  note =         "ISBN 0~471~40300",
  pages =        "19--33",
}

@Book{singer-whole,
  year =         "1954--58",
  editor =       "Charles Joseph Singer and E. J. Holmyard and A. R.
                 Hall",
  title =        "A history of technology",
  address =      "London",
  publisher =    "Oxford University Press",
  note =         "5 vol.",
}


@Inproceedings{chomsky-73,
  author =       "N. Chomsky",
  year =         "1973",
  title =        "Conditions on Transformations",
  booktitle =    "A festschrift for {Morris Halle}",
  editor =       "S. R. Anderson and P. Kiparsky",
  publisher =    "Holt, Rinehart \& Winston",
  address =      "New York",
}

@Manual{bs-2570-manual,
  author =       "BSI",
  title =        "Natural Fibre Twines",
  organization = "British Standards Institution",
  address =      "London",
  edition =      "3rd",
  year =         "1973",
  note =         "BS 2570",
}

@Techreport{bs-2570-techreport,
  author =       "BSI",
  title =        "Natural Fibre Twines",
  institution =  "British Standards Institution",
  address =      "London",
  year =         "1973",
  type =         "BS",
  number =       "2570",
  note =         "3rd. edn.",
}



@Book{Jespersen:1922,
  author = 	 "Otto Jespersen",
  title = 	 "Language: Its Nature, Development, and Origin",
  publisher = 	 "Allen and Unwin",
  year = 	 1922}



% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@article{dalmia2021searchable,
  title={Searchable hidden intermediates for end-to-end models of decomposable sequence tasks},
  author={Dalmia, Siddharth and Yan, Brian and Raunak, Vikas and Metze, Florian and Watanabe, Shinji},
  journal={arXiv preprint arXiv:2105.00573},
  year={2021}
}

@inproceedings{inaguma2021fast,
  title={Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates},
  author={Inaguma, Hirofumi and Dalmia, Siddharth and Yan, Brian and Watanabe, Shinji},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={922--929},
  year={2021},
  organization={IEEE}
}

@article{zhang2019lattice,
  title={Lattice transformer for speech translation},
  author={Zhang, Pei and Chen, Boxing and Ge, Niyu and Fan, Kai},
  journal={arXiv preprint arXiv:1906.05551},
  year={2019}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}




@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}



@article{ye2022gigast,
  title={GigaST: A 10,000-hour Pseudo Speech Translation Corpus},
  author={Ye, Rong and Zhao, Chengqi and Ko, Tom and Meng, Chutong and Wang, Tao and Wang, Mingxuan and Cao, Jun},
  journal={arXiv preprint arXiv:2204.03939},
  year={2022}
}

@article{chen2021gigaspeech,
  title={Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio},
  author={Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and others},
  journal={arXiv preprint arXiv:2106.06909},
  year={2021}
}

@article{lee2021direct,
  title={Direct speech-to-speech translation with discrete units},
  author={Lee, Ann and Chen, Peng-Jen and Wang, Changhan and Gu, Jiatao and Popuri, Sravya and Ma, Xutai and Polyak, Adam and Adi, Yossi and He, Qing and Tang, Yun and others},
  journal={arXiv preprint arXiv:2107.05604},
  year={2021}
}


@article{popuri2022enhanced,
  title={Enhanced direct speech-to-speech translation using self-supervised pre-training and data augmentation},
  author={Popuri, Sravya and Chen, Peng-Jen and Wang, Changhan and Pino, Juan and Adi, Yossi and Gu, Jiatao and Hsu, Wei-Ning and Lee, Ann},
  journal={arXiv preprint arXiv:2204.02967},
  year={2022}
}


@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}


@article{tang2020multilingual,
  title={Multilingual translation with extensible multilingual pretraining and finetuning},
  author={Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela},
  journal={arXiv preprint arXiv:2008.00401},
  year={2020}
}


@article{li2020multilingual,
  title={Multilingual speech translation with efficient finetuning of pretrained models},
  author={Li, Xian and Wang, Changhan and Tang, Yun and Tran, Chau and Tang, Yuqing and Pino, Juan and Baevski, Alexei and Conneau, Alexis and Auli, Michael},
  journal={arXiv preprint arXiv:2010.12829},
  year={2020}
}


@article{berard2016listen,
  title={Listen and translate: A proof of concept for end-to-end speech-to-text translation},
  author={B{\'e}rard, Alexandre and Pietquin, Olivier and Servan, Christophe and Besacier, Laurent},
  journal={arXiv preprint arXiv:1612.01744},
  year={2016}
}


@inproceedings{dong2018speech,
  title={Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition},
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5884--5888},
  year={2018},
  organization={IEEE}
}


@article{liu2019end,
  title={End-to-end speech translation with knowledge distillation},
  author={Liu, Yuchen and Xiong, Hao and He, Zhongjun and Zhang, Jiajun and Wu, Hua and Wang, Haifeng and Zong, Chengqing},
  journal={arXiv preprint arXiv:1904.08075},
  year={2019}
}

@article{yao2021wenet,
  title={Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit},
  author={Yao, Zhuoyuan and Wu, Di and Wang, Xiong and Zhang, Binbin and Yu, Fan and Yang, Chao and Peng, Zhendong and Chen, Xiaoyu and Xie, Lei and Lei, Xin},
  journal={arXiv preprint arXiv:2102.01547},
  year={2021}
}


@article{zhang2022wenet,
  title={Wenet 2.0: More productive end-to-end speech recognition toolkit},
  author={Zhang, Binbin and Wu, Di and Peng, Zhendong and Song, Xingchen and Yao, Zhuoyuan and Lv, Hang and Xie, Lei and Yang, Chao and Pan, Fuping and Niu, Jianwei},
  journal={arXiv preprint arXiv:2203.15455},
  year={2022}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{han2021pre,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}


@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{li2020multilingual,
  title={Multilingual speech translation with efficient finetuning of pretrained models},
  author={Li, Xian and Wang, Changhan and Tang, Yun and Tran, Chau and Tang, Yuqing and Pino, Juan and Baevski, Alexei and Conneau, Alexis and Auli, Michael},
  journal={arXiv preprint arXiv:2010.12829},
  year={2020}
}

@article{le2021lightweight,
  title={Lightweight adapter tuning for multilingual speech translation},
  author={Le, Hang and Pino, Juan and Wang, Changhan and Gu, Jiatao and Schwab, Didier and Besacier, Laurent},
  journal={arXiv preprint arXiv:2106.01463},
  year={2021}
}

@article{weiss2017sequence,
  title={Sequence-to-sequence models can directly translate foreign speech},
  author={Weiss, Ron J and Chorowski, Jan and Jaitly, Navdeep and Wu, Yonghui and Chen, Zhifeng},
  journal={arXiv preprint arXiv:1703.08581},
  year={2017}
}

@article{anastasopoulos2018tied,
  title={Tied multitask learning for neural speech translation},
  author={Anastasopoulos, Antonios and Chiang, David},
  journal={arXiv preprint arXiv:1802.06655},
  year={2018}
}

@inproceedings{berard2018end,
  title={End-to-end automatic speech translation of audiobooks},
  author={B{\'e}rard, Alexandre and Besacier, Laurent and Kocabiyikoglu, Ali Can and Pietquin, Olivier},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6224--6228},
  year={2018},
  organization={IEEE}
}

@article{liu2019end,
  title={End-to-end speech translation with knowledge distillation},
  author={Liu, Yuchen and Xiong, Hao and He, Zhongjun and Zhang, Jiajun and Wu, Hua and Wang, Haifeng and Zong, Chengqing},
  journal={arXiv preprint arXiv:1904.08075},
  year={2019}
}

@inproceedings{jia2019leveraging,
  title={Leveraging weakly supervised data to improve end-to-end speech-to-text translation},
  author={Jia, Ye and Johnson, Melvin and Macherey, Wolfgang and Weiss, Ron J and Cao, Yuan and Chiu, Chung-Cheng and Ari, Naveen and Laurenzo, Stella and Wu, Yonghui},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7180--7184},
  year={2019},
  organization={IEEE}
}

@article{pino2020self,
  title={Self-training for end-to-end speech translation},
  author={Pino, Juan and Xu, Qiantong and Ma, Xutai and Dousti, Mohammad Javad and Tang, Yun},
  journal={arXiv preprint arXiv:2006.02490},
  year={2020}
}

@article{lam2022sample,
  title={Sample, translate, recombine: Leveraging audio alignments for data augmentation in end-to-end speech translation},
  author={Lam, Tsz Kin and Schamoni, Shigehiko and Riezler, Stefan},
  journal={arXiv preprint arXiv:2203.08757},
  year={2022}
}

@article{li2022efficient,
  title={Efficient Speech Translation with Pre-trained Models},
  author={Li, Zhaolin and Niehues, Jan},
  journal={arXiv preprint arXiv:2211.04939},
  year={2022}
}



@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@article{yang2021superb,
  title={Superb: Speech processing universal performance benchmark},
  author={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y and Liu, Andy T and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and others},
  journal={arXiv preprint arXiv:2105.01051},
  year={2021}
}


@article{lakhotia2021generative,
  title={On generative spoken language modeling from raw audio},
  author={Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1336--1354},
  year={2021},
  publisher={MIT Press}
}

@article{polyak2021speech,
  title={Speech resynthesis from discrete disentangled self-supervised representations},
  author={Polyak, Adam and Adi, Yossi and Copet, Jade and Kharitonov, Eugene and Lakhotia, Kushal and Hsu, Wei-Ning and Mohamed, Abdelrahman and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2104.00355},
  year={2021}
}



@inproceedings{tjandra2019speech,
  title={Speech-to-speech translation between untranscribed unknown languages},
  author={Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={593--600},
  year={2019},
  organization={IEEE}
}


@inproceedings{zhang2021uwspeech,
  title={Uwspeech: Speech to speech translation for unwritten languages},
  author={Zhang, Chen and Tan, Xu and Ren, Yi and Qin, Tao and Zhang, Kejun and Liu, Tie-Yan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14319--14327},
  year={2021}
}


article{wang2021voxpopuli,
  title={Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation},
  author={Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2101.00390},
  year={2021}
}

@article{berard2016listen,
  title={Listen and translate: A proof of concept for end-to-end speech-to-text translation},
  author={B{\'e}rard, Alexandre and Pietquin, Olivier and Servan, Christophe and Besacier, Laurent},
  journal={arXiv preprint arXiv:1612.01744},
  year={2016}
}

@inproceedings{bahar2019comparative,
  title={A comparative study on end-to-end speech to text translation},
  author={Bahar, Parnia and Bieschke, Tobias and Ney, Hermann},
  booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={792--799},
  year={2019},
  organization={IEEE}
}


@article{li2020multilingual,
  title={Multilingual speech translation with efficient finetuning of pretrained models},
  author={Li, Xian and Wang, Changhan and Tang, Yun and Tran, Chau and Tang, Yuqing and Pino, Juan and Baevski, Alexei and Conneau, Alexis and Auli, Michael},
  journal={arXiv preprint arXiv:2010.12829},
  year={2020}
}


@inproceedings{matusov2005integration,
  title={On the integration of speech recognition and statistical machine translation},
  author={Matusov, Evgeny and Kanthak, Stephan and Ney, Hermann},
  booktitle={Ninth European Conference on Speech Communication and Technology},
  year={2005}
}

@inproceedings{stoian2020analyzing,
  title={Analyzing ASR pretraining for low-resource speech-to-text translation},
  author={Stoian, Mihaela C and Bansal, Sameer and Goldwater, Sharon},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7909--7913},
  year={2020},
  organization={IEEE}
}

@article{pino2020self,
  title={Self-training for end-to-end speech translation},
  author={Pino, Juan and Xu, Qiantong and Ma, Xutai and Dousti, Mohammad Javad and Tang, Yun},
  journal={arXiv preprint arXiv:2006.02490},
  year={2020}
}

@article{wang2021large,
  title={Large-scale self-and semi-supervised learning for speech translation},
  author={Wang, Changhan and Wu, Anne and Pino, Juan and Baevski, Alexei and Auli, Michael and Conneau, Alexis},
  journal={arXiv preprint arXiv:2104.06678},
  year={2021}
}

@inproceedings{bertoldi2005new,
  title={A new decoder for spoken language translation based on confusion networks},
  author={Bertoldi, Nicola and Federico, Marcello},
  booktitle={IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.},
  pages={86--91},
  year={2005},
  organization={IEEE}
}

@inproceedings{beck2019neural,
  title={Neural speech translation using lattice transformations and graph networks},
  author={Beck, Daniel and Cohn, Trevor and Haffari, Gholamreza},
  booktitle={Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)},
  pages={26--31},
  year={2019}
}

@article{sperber2019self,
  title={Self-attentional models for lattice inputs},
  author={Sperber, Matthias and Neubig, Graham and Pham, Ngoc-Quan and Waibel, Alex},
  journal={arXiv preprint arXiv:1906.01617},
  year={2019}
}

@inproceedings{peitz2012spoken,
  title={Spoken language translation using automatically transcribed text in training},
  author={Peitz, Stephan and Wiesler, Simon and Nu{\ss}baum-Thom, Markus and Ney, Hermann},
  booktitle={Proceedings of the 9th International Workshop on Spoken Language Translation: Papers},
  year={2012}
}

@article{cheng2019breaking,
  title={Breaking the data barrier: Towards robust speech translation via adversarial stability training},
  author={Cheng, Qiao and Fang, Meiyuan and Han, Yaqian and Huang, Jin and Duan, Yitao},
  journal={arXiv preprint arXiv:1909.11430},
  year={2019}
}

@article{di2019robust,
  title={Robust neural machine translation for clean and noisy speech transcripts},
  author={Di Gangi, Mattia Antonino and Enyedi, Robert and Brusadin, Alessandra and Federico, Marcello},
  journal={arXiv preprint arXiv:1910.10238},
  year={2019}
}



@article{inaguma2021source,
  title={Source and target bidirectional knowledge distillation for end-to-end speech translation},
  author={Inaguma, Hirofumi and Kawahara, Tatsuya and Watanabe, Shinji},
  journal={arXiv preprint arXiv:2104.06457},
  year={2021}
}


@article{fang2022stemm,
  title={STEMM: Self-learning with speech-text manifold mixup for speech translation},
  author={Fang, Qingkai and Ye, Rong and Li, Lei and Feng, Yang and Wang, Mingxuan},
  journal={arXiv preprint arXiv:2203.10426},
  year={2022}
}


@inproceedings{tang2021general,
  title={A general multi-task learning framework to leverage text data for speech to text tasks},
  author={Tang, Yun and Pino, Juan and Wang, Changhan and Ma, Xutai and Genzel, Dmitriy},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6209--6213},
  year={2021},
  organization={IEEE}
}


@inproceedings{zheng2021fused,
  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},
  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},
  booktitle={International Conference on Machine Learning},
  pages={12736--12746},
  year={2021},
  organization={PMLR}
}


@article{tang2021improving,
  title={Improving speech translation by understanding and learning from the auxiliary text translation task},
  author={Tang, Yun and Pino, Juan and Li, Xian and Wang, Changhan and Genzel, Dmitriy},
  journal={arXiv preprint arXiv:2107.05782},
  year={2021}
}


@article{han2021learning,
  title={Learning shared semantic space for speech-to-text translation},
  author={Han, Chi and Wang, Mingxuan and Ji, Heng and Li, Lei},
  journal={arXiv preprint arXiv:2105.03095},
  year={2021}
}



@article{ye2021end,
  title={End-to-end speech translation via cross-modal progressive training},
  author={Ye, Rong and Wang, Mingxuan and Li, Lei},
  journal={arXiv preprint arXiv:2104.10380},
  year={2021}
}


@article{xu2021stacked,
  title={Stacked acoustic-and-textual encoding: Integrating the pre-trained models into speech translation encoders},
  author={Xu, Chen and Hu, Bojie and Li, Yanyang and Zhang, Yuhao and Ju, Qi and Xiao, Tong and Zhu, Jingbo and others},
  journal={arXiv preprint arXiv:2105.05752},
  year={2021}
}


@article{fang2022stemm,
  title={STEMM: Self-learning with speech-text manifold mixup for speech translation},
  author={Fang, Qingkai and Ye, Rong and Li, Lei and Feng, Yang and Wang, Mingxuan},
  journal={arXiv preprint arXiv:2203.10426},
  year={2022}
}



@inproceedings{indurthi2021task,
  title={Task aware multi-task learning for speech to text tasks},
  author={Indurthi, Sathish and Zaidi, Mohd Abbas and Lakumarapu, Nikhil Kumar and Lee, Beomseok and Han, Hyojung and Ahn, Seokchan and Kim, Sangha and Kim, Chanwoo and Hwang, Inchul},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7723--7727},
  year={2021},
  organization={IEEE}
}


@article{tang2022unified,
  title={Unified speech-text pre-training for speech translation and recognition},
  author={Tang, Yun and Gong, Hongyu and Dong, Ning and Wang, Changhan and Hsu, Wei-Ning and Gu, Jiatao and Baevski, Alexei and Li, Xian and Mohamed, Abdelrahman and Auli, Michael and others},
  journal={arXiv preprint arXiv:2204.05409},
  year={2022}
}


@article{ye2022cross,
  title={Cross-modal contrastive learning for speech translation},
  author={Ye, Rong and Wang, Mingxuan and Li, Lei},
  journal={arXiv preprint arXiv:2205.02444},
  year={2022}
}



@inproceedings{di2019must,
  title={Must-c: a multilingual speech translation corpus},
  author={Di Gangi, Mattia A and Cattoni, Roldano and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2012--2017},
  year={2019},
  organization={Association for Computational Linguistics}
}

@article{wang2021voxpopuli,
  title={Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation},
  author={Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2101.00390},
  year={2021}
}



@article{10.1162/tacl_a_00343,
    author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
    title = "{Multilingual Denoising Pre-training for Neural Machine Translation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {726-742},
    year = {2020},
    month = {11},
    abstract = "{This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00343},
    url = {https://doi.org/10.1162/tacl\_a\_00343},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00343/1923401/tacl\_a\_00343.pdf},
}


@inproceedings{koehn-2005-europarl,
    title = "{E}uroparl: A Parallel Corpus for Statistical Machine Translation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.11",
    pages = "79--86",
    abstract = "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
}


@article{lison2016opensubtitles2016,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}











@article{yin2023improving,
  title={Improving speech translation by fusing speech and text},
  author={Yin, Wenbiao and Liu, Zhicheng and Zhao, Chengqi and Wang, Tao and Tong, Jian and Ye, Rong},
  journal={arXiv preprint arXiv:2305.14042},
  year={2023}
}


@InProceedings{charalampopoulos_et_al:LIPIcs.ESA.2021.30,
  author =	{Charalampopoulos, Panagiotis and Kociumaka, Tomasz and Pissis, Solon P. and Radoszewski, Jakub},
  title =	{{Faster Algorithms for Longest Common Substring}},
  booktitle =	{29th Annual European Symposium on Algorithms (ESA 2021)},
  pages =	{30:1--30:17},
  series =	{Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN =	{978-3-95977-204-4},
  ISSN =	{1868-8969},
  year =	{2021},
  volume =	{204},
  editor =	{Mutzel, Petra and Pagh, Rasmus and Herman, Grzegorz},
  publisher =	{Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{https://drops-dev.dagstuhl.de/entities/document/10.4230/LIPIcs.ESA.2021.30},
  URN =		{urn:nbn:de:0030-drops-146114},
  doi =		{10.4230/LIPIcs.ESA.2021.30},
  annote =	{Keywords: longest common substring, k mismatches, wavelet tree}
}




## related works


@inproceedings{quan2005integrated,
  title={Integrated n-best re-ranking for spoken language translation.},
  author={Quan, Vu Hai and Federico, Marcello and Cettolo, Mauro},
  booktitle={Interspeech},
  pages={3181--3184},
  year={2005}
}



@inproceedings{ma2020neural,
  title={Neural lattice search for speech recognition},
  author={Ma, Rao and Li, Hao and Liu, Qi and Chen, Lu and Yu, Kai},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7794--7798},
  year={2020},
  organization={IEEE}
}


# get label
@inproceedings{huang2023holistic,
  title={A holistic cascade system, benchmark, and human evaluation protocol for expressive speech-to-speech translation},
  author={Huang, Wen-Chin and Peloquin, Benjamin and Kao, Justine and Wang, Changhan and Gong, Hongyu and Salesky, Elizabeth and Adi, Yossi and Lee, Ann and Chen, Peng-Jen},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}


@article{barrault2023seamless,
  title={Seamless: Multilingual Expressive and Streaming Speech Translation},
  author={Barrault, Lo{\"\i}c and Chung, Yu-An and Meglioli, Mariano Coria and Dale, David and Dong, Ning and Duppenthaler, Mark and Duquenne, Paul-Ambroise and Ellis, Brian and Elsahar, Hady and Haaheim, Justin and others},
  journal={arXiv preprint arXiv:2312.05187},
  year={2023}
}






@inproceedings{lee-etal-2022-direct,
    title = "Direct Speech-to-Speech Translation With Discrete Units",
    author = "Lee, Ann  and
      Chen, Peng-Jen  and
      Wang, Changhan  and
      Gu, Jiatao  and
      Popuri, Sravya  and
      Ma, Xutai  and
      Polyak, Adam  and
      Adi, Yossi  and
      He, Qing  and
      Tang, Yun  and
      Pino, Juan  and
      Hsu, Wei-Ning",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.235",
    doi = "10.18653/v1/2022.acl-long.235",
    pages = "3327--3339",
    abstract = "We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech. When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features. When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages.",
}




@article{popuri2022enhanced,
  title={Enhanced direct speech-to-speech translation using self-supervised pre-training and data augmentation},
  author={Popuri, Sravya and Chen, Peng-Jen and Wang, Changhan and Pino, Juan and Adi, Yossi and Gu, Jiatao and Hsu, Wei-Ning and Lee, Ann},
  journal={arXiv preprint arXiv:2204.02967},
  year={2022}
}



@inproceedings{jia2022translatotron,
  title={Translatotron 2: High-quality direct speech-to-speech translation with voice preservation},
  author={Jia, Ye and Ramanovich, Michelle Tadmor and Remez, Tal and Pomerantz, Roi},
  booktitle={International Conference on Machine Learning},
  pages={10120--10134},
  year={2022},
  organization={PMLR}
}




@article{jia2019direct,
  title={Direct speech-to-speech translation with a sequence-to-sequence model},
  author={Jia, Ye and Weiss, Ron J and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
  journal={arXiv preprint arXiv:1904.06037},
  year={2019}
}




@article{lee2021textless,
  title={Textless speech-to-speech translation on real data},
  author={Lee, Ann and Gong, Hongyu and Duquenne, Paul-Ambroise and Schwenk, Holger and Chen, Peng-Jen and Wang, Changhan and Popuri, Sravya and Adi, Yossi and Pino, Juan and Gu, Jiatao and others},
  journal={arXiv preprint arXiv:2112.08352},
  year={2021}
}




# toolkit

@article{wang2020fairseq,
  title={Fairseq S2T: Fast speech-to-text modeling with fairseq},
  author={Wang, Changhan and Tang, Yun and Ma, Xutai and Wu, Anne and Popuri, Sravya and Okhonko, Dmytro and Pino, Juan},
  journal={arXiv preprint arXiv:2010.05171},
  year={2020}
}

@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}


@article{wang2021fairseq,
  title={fairseq s\^{} 2: A scalable and integrable speech synthesis toolkit},
  author={Wang, Changhan and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Lee, Ann and Chen, Peng-Jen and Gu, Jiatao and Pino, Juan},
  journal={arXiv preprint arXiv:2109.06912},
  year={2021}
}






@inproceedings{waibel2023face,
  title={Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos},
  author={Waibel, Alexander and Behr, Moritz and Yaman, Dogucan and Eyiokur, Fevziye Irem and Nguyen, Tuan-Nam and Mullov, Carlos and Demirtas, Mehmet Arif and Kantarci, Alperen and Constantin, Stefan and Ekenel, Hazim Kemal},
  booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}




@article{do2018sequence,
  title={Sequence-to-sequence models for emphasis speech translation},
  author={Do, Quoc Truong and Sakti, Sakriani and Nakamura, Satoshi},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={26},
  number={10},
  pages={1873--1883},
  year={2018},
  publisher={IEEE}
}



@article{do2016preserving,
  title={Preserving word-level emphasis in speech-to-speech translation},
  author={Do, Quoc Truong and Toda, Tomoki and Neubig, Graham and Sakti, Sakriani and Nakamura, Satoshi},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={25},
  number={3},
  pages={544--556},
  year={2016},
  publisher={IEEE}
}



@article{wang2021voxpopuli,
  title={VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation},
  author={Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2101.00390},
  year={2021}
}



# dataset
@misc{ljspeech17,
  author       = {Keith Ito and Linda Johnson},
  title        = {The LJ Speech Dataset},
  howpublished = {\url{https://keithito.com/LJ-Speech-Dataset/}},
  year         = 2017
}

@article{veaux2017english,
  title={English multi-speaker corpus for CSTR voice cloning toolkit},
  author={Veaux, Christophe and Yamagishi, Junichi and MacDonald, Kirsten and others},
  journal={The Centre for Speech Technology Research (CSTR), University of Edinburgh},
  year={2017}
}


@inproceedings{Kahn_2020,
   title={Libri-Light: A Benchmark for ASR with Limited or No Supervision},
   url={http://dx.doi.org/10.1109/ICASSP40776.2020.9052942},
   DOI={10.1109/icassp40776.2020.9052942},
   booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   publisher={IEEE},
   author={Kahn, J. and Riviere, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazare, P.E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and Likhomanenko, T. and Synnaeve, G. and Joulin, A. and Mohamed, A. and Dupoux, E.},
   year={2020},
   month=may }



@misc{polyak2021speech,
      title={Speech Resynthesis from Discrete Disentangled Self-Supervised Representations}, 
      author={Adam Polyak and Yossi Adi and Jade Copet and Eugene Kharitonov and Kushal Lakhotia and Wei-Ning Hsu and Abdelrahman Mohamed and Emmanuel Dupoux},
      year={2021},
      eprint={2104.00355},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}






@inproceedings{lavie1997janus,
  title={JANUS-III: Speech-to-speech translation in multiple languages},
  author={Lavie, Alon and Waibel, Alex and Levin, Lori and Finke, Michael and Gates, Donna and Gavalda, Marsal and Zeppenfeld, Torsten and Zhan, Puming},
  booktitle={1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={1},
  pages={99--102},
  year={1997},
  organization={IEEE}
}




@article{nakamura2006atr,
  title={The ATR multilingual speech-to-speech translation system},
  author={Nakamura, Satoshi and Markov, Konstantin and Nakaiwa, Hiromi and Kikui, Gen-ichiro and Kawai, Hisashi and Jitsuhiro, Takatoshi and Zhang, J-S and Yamamoto, Hirofumi and Sumita, Eiichiro and Yamamoto, Seiichi},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={14},
  number={2},
  pages={365--376},
  year={2006},
  publisher={IEEE}
}




@article{diwan2023unit,
  title={Unit-based Speech-to-Speech Translation Without Parallel Data},
  author={Diwan, Anuj and Srinivasan, Anirudh and Harwath, David and Choi, Eunsol},
  journal={arXiv preprint arXiv:2305.15405},
  year={2023}
}

@article{davis1980comparison,
  title={Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},
  author={Davis, Steven and Mermelstein, Paul},
  journal={IEEE transactions on acoustics, speech, and signal processing},
  volume={28},
  number={4},
  pages={357--366},
  year={1980},
  publisher={IEEE}
}

@article{nachmani2023translatotron,
  title={Translatotron 3: Speech to speech translation with monolingual data},
  author={Nachmani, Eliya and Levkovitch, Alon and Ding, Yifan and Asawaroengchai, Chulayutsh and Zen, Heiga and Ramanovich, Michelle Tadmor},
  journal={arXiv preprint arXiv:2305.17547},
  year={2023}
}


@article{desplanques2020ecapa,
  title={Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification},
  author={Desplanques, Brecht and Thienpondt, Jenthe and Demuynck, Kris},
  journal={arXiv preprint arXiv:2005.07143},
  year={2020}
}