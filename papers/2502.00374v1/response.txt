\section{Related Works}
% In the realm of expressive speech-to-speech translation (S2ST)**Schupp, "Expressive Speech-to-Speech Translation"**, the initial research focused on intonation transfer, utilizing statistical word alignment to transfer source intonation characteristics to the target language. These methods evolved to include word emphasis transfer, ultimately leading to sequence-to-sequence models for simultaneous emphasis and content translation. Despite the progress, these approaches only focused on individual expression elements. Our work diverges by integrating multiple expressive aspects simultaneously, marking a significant progression in S2ST.

% Recent TTS advancements, particularly with Tacotron**Ping, "Tacotron: Towards End-to-End Speech Synthesis"**__**Arik, "FastSpeech: Fast, Robust and Controllable Text to Speech"**, have significantly advanced end-to-end speech synthesis. However, they still struggle with conveying complex emotions and achieving audio-visual synchronization in scenarios like video/movie dubbing**Wang, "Tacotron 2: Towards More Natural Sounding Synthesized Speech"**. Our research seeks to overcome these challenges by incorporating advanced emotion and prosody transfer techniques.

% Controllable Text-to-Speech (TTS) has developed in two main directions: global and fine-grained style transfer**Chen, "Style Tokens for Style Transfer in Text-to-Speech"**. Global style transfer, encapsulating overall speech attributes into a single embedding, contrasts with fine-grained style transfer, which captures local prosodic features but faces alignment challenges. Global style transfer is more adaptable to non-parallel scenarios, so we leverage it in our S2ST framework.
\vspace{-0.6cm}