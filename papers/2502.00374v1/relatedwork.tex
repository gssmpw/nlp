\section{Related Works}
% In the realm of expressive speech-to-speech translation (S2ST)~\cite{lee2022direct,popuri2022enhanced,huang2023transpeech,huang2023holistic,barrault2023seamless}, the initial research focused on intonation transfer, utilizing statistical word alignment to transfer source intonation characteristics to the target language. These methods evolved to include word emphasis transfer, ultimately leading to sequence-to-sequence models for simultaneous emphasis and content translation. Despite the progress, these approaches only focused on individual expression elements. Our work diverges by integrating multiple expressive aspects simultaneously, marking a significant progression in S2ST.

% Recent TTS advancements, particularly with Tacotron~\cite{wang2017tacotron, shen2018natural} and FastSpeech~\cite{ren2019fastspeech, ren2022fastspeech} models, have significantly advanced end-to-end speech synthesis. However, they still struggle with conveying complex emotions and achieving audio-visual synchronization in scenarios like video/movie dubbing~\cite{hu2021neural}. Our research seeks to overcome these challenges by incorporating advanced emotion and prosody transfer techniques.

% Controllable Text-to-Speech (TTS) has developed in two main directions: global and fine-grained style transfer~\cite{wang2018style,skerryryan2018endtoend,ren2022fastspeech,sun2020fullyhierarchical}. Global style transfer, encapsulating overall speech attributes into a single embedding, contrasts with fine-grained style transfer, which captures local prosodic features but faces alignment challenges. Global style transfer is more adaptable to non-parallel scenarios, so we leverage it in our S2ST framework.
\vspace{-0.6cm}