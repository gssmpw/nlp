% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[subrefformat=parens]{subcaption}
\usepackage{amsmath}
\usepackage{amsfonts}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Analyzing the Inner Workings of Transformers \\ in Compositional Generalization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Ryoma Kumon \and Hitomi Yanaka \\
  The University of Tokyo \\
  \texttt{\{kumoryo9, hyanaka\}@is.s.u-tokyo.ac.jp}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\newcommand{\todo}[1]{\textcolor{red}{(TODO:#1)}}
\newcommand{\subnetwork}[0]{\texttt{GENSUB}}
\newcommand{\dobjppiobjpp}[0]{\textsc{pp-iobj}}
\newcommand{\dobjppsubjpp}[0]{\textsc{pp-subj}}
\newcommand{\novelrciobj}[0]{\textit{iobj-RC}}
\begin{document}
\maketitle
\begin{abstract}
The compositional generalization abilities of neural models have been sought after for human-like linguistic competence.
The popular method to evaluate their compositional generalization abilities is to assess their input-output behavior.
However, it does not allow for an understanding of the internal mechanisms, and the models' underlying competence in compositional generalization remains unclear.
To address this problem, we explore the inner workings of a Transformer model by
finding an existing subnetwork that contributes to the generalization performance and by performing causal analyses on how the model utilizes syntactic features.
We find that the model depends on syntactic features to output the correct answer, but that the subnetwork with much better generalization performance than the whole model relies on a non-compositional algorithm in addition to the syntactic features.
We also show that the subnetwork improves its generalization performance relatively slowly during the training compared to the in-distribution one, and the non-compositional solution is acquired in the early stages of the training.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Compositional generalization, the ability to understand the meaning of a novel language expression based on the composition of known words and syntactic structures~\citep{partee1984compositionality, fodor1988connectionism}, is one of the crucial aspects in terms of the robustness for unseen language data.
To assess the compositional generalization abilities of neural models, most existing studies have primarily focused on evaluating model outputs in compositional generalization benchmarks~\citep{kim-linzen-2020-cogs, li-etal-2021-compositional, dankers-etal-2022-paradox, li-etal-2023-slog}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/sources/fig1_15.png}
    \caption{In this study, we investigate what neural models employ in compositional generalization tasks.}
    \label{fig:1}
\end{figure}
However, the model outputs do not necessarily reflect the underlying competence because good performance in the benchmarks does not guarantee that the model implements a solution that generalizes based on compositional rules and vice versa.
In addition, while a growing body of work on model interepretability has investigated the inner workings of Transformer~\citep{vaswani2017attention}-based models~\citep{ferrando2024primerinnerworkingstransformerbased, rai2024practicalreviewmechanisticinterpretability}, compositional generalization has rarely been the focus of such studies.
\citet{yao-koller-2022-structural} and \citet{murty2023characterizing} analyzed the internal mechanisms in compositional generalization but did not focus on the usage of linguistic features, which is the central part of compositional generalization.
We consider that compositional generalization requires determining the meaning of a language expression based on how its parts are combined syntactically.
Thus, the internal mechanisms of the model in compositional generalization are still unclear, and unveiling it would enhance the understanding of the model's competence.

In this work, we analyze the inner workings of neural models to understand what type of syntactic features the models depend on in tasks requiring compositional generalization.
Our analysis method\footnote{Our code and data will be publicly available.} consists of (i) identifying subnetworks within the model that perform well in the generalization and (ii) investigating how syntactic features causally affect the original model and its subnetwork, as shown in Figure~\ref{fig:1}.
The causal analysis involves removing the linguistic concept of interest from the model and comparing the generalization performance before and after the removal.
To rigorously evaluate and analyze the compositional generalization abilities, we focus not on pretrained models but a Transformer model trained from scratch because pretraining data contains syntactic structures that should be unseen in this experiment and pretrained models may not need to generalize compositionally~\citep{kim2022uncontrolledlexicalexposureleads}.

We also aim to conduct a detailed analysis by experimenting with various settings.
This study employs two commonly used tasks for evaluating compositional generalization: machine translation and semantic parsing.
We test the models with two patterns of compositional generalization, \dobjppiobjpp{} and \dobjppsubjpp{}, which are similar yet different (See Section~\ref{subsec:cg-pattern} for details).

The findings from our experiments suggest that the model and its subnetwork that contributes to the generalization performance indeed leverage syntactic structures.
However, intriguingly, we also discover that the subnetwork implements other solutions that do not depend on syntactic structures.
From these results, we argue that the solutions that the models employ are partly non-compositional.
Moreover, analysis of the model at different epochs during the training revealed that the model gradually develops a subnetwork with better generalization performance.
The causal analysis of the subnetwork showed that the non-compositional solution was learned during the early phrase of the training.
We argue that Transformer models need a better inductive bias to generalize compositionally utilizing syntactic features.

\section{Background}
\label{sec:background}

\subsection{Compositional Generalization}
\label{subsec:compositional-generalization}
Several studies have explored the compositional generalization abilities of modern neural models by focusing on the model performance in tasks such as semantic parsing~\cite{kim-linzen-2020-cogs, csordas-etal-2021-devil, yao-koller-2022-structural,kim2022uncontrolledlexicalexposureleads, li-etal-2023-slog} and machine translation~\cite{li-etal-2021-compositional, dankers-etal-2022-paradox, kumon-etal-2024-evaluating}.
Such studies indicate that the models lack compositional generalization abilities in general, while others have worked on improving them by modifying the model architectures~\cite{bergan2021edge,ontanon-etal-2022-making}.
\citet{yao-koller-2022-structural} briefly analyzed which part of seq2seq models causes poor performance in compositional generalization by probing the encoder.
They attributed the poor performance to the decoder, stating that the encoder has the linguistic knowledge needed to solve the generalization tasks but the decoder does not use it.
These studies focused mainly on the model outputs and the encoded properties.
In contrast, our research analyzes the inner workings of models with subnetwork search and causal analysis, which should show insights into what features causally impact the model behavior.
We consider that a compositional solution generalizes by leveraging syntactic structures consistently in a bottom-up manner.
In our experiments, we focus on syntactic features that a compositional solution should utilize.
Thus, we regard a solution that does not employ these features as non-compositional.


%\subsection{Interpretability from linguistic perspective\todo{}}
\subsection{Linguistic Approach to Interpretability}
\label{subsec:interpretability}
% Neural models have been analyzed in various ways.
% Probing is used to discover what models encode in their inner representations.
% Numerous studies have utilized probing to test how pretrained language models encode linguistic information such as sentence structure~\citep{tenny2019learn, hewitt-manning-2019-structural}.
% Probing is often criticized because probing classifiers may learn the task itself, which leads us to overestimate the models' linguistic abilities~\citep{hewitt-liang-2019-designing, pimentel-etal-2020-information}.
% To address this, \citet{cao-etal-2021-low} proposed subnetwork probing, which is a pruning-based probing method that achieves better accuracy in the target linguistic task and worse at learning the task itself than a multi-layer perceptron.

One approach to interpret neural models is studying the causal relationship between the target feature and the model behavior based on interventions.
Interventions alter a model's inputs or the inner representations of a model so that the target feature is the only change and test the effect on the model outputs.
Recent work has analyzed the linguistic mechanisms~\citep{tucker-etal-2021-modified, elazar-etal-2021-amnesic, ravfogel-etal-2021-counterfactual, feder-etal-2021-causalm, amini-etal-2023-naturalistic, belrose2023leace, arora-etal-2024-causalgym}.
\citet{belrose2023leace} utilized concept erasure, which removes only the concept of interest from the model and tests causal links by comparing the predictions by the model before and after the removal.
We employ \citet{belrose2023leace}'s method for causally analyzing the causal role of syntactic features in compositional generalization, because of its compatibility with our compositional generalization tasks.

Another line of work have explored finding subnetworks with specific properties of interest as a method for model analysis.
\citet{cao-etal-2021-low} proposed subnetwork probing, a pruning-based method that searches for a subnetwork that perform a target linguistic task.
As for linguistic generalization, previous studies have found subnetworks that perform syntactic generalization~\citep{bhaskar2024heuristiccoreunderstandingsubnetwork},  hierarchical generalization~\citep{ahuja2024learningsyntaxplantingtrees}, and compositional generalization~\citep{hu2024compositional}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/sources/fig2_12.png}
    \caption{Overview of analysis process.
    It consists of three phrases: training base models, subnetwork probing, and causal analysis.}
    \label{fig:overview}
\end{figure*}


\section{Analysis Method}
\label{sec:method}

Figure~\ref{fig:overview} presents our method of analyzing the inner workings of a Transformer model in compositional generalization.
It consists of the following three phases;
training a base model, subnetwork probing, and causal analysis.

\subsection{Training Base Model}
\label{subsec:training}
To test the compositional generalization abilities of a model, we first construct a dataset that contains the training, in-distribution test, and out-of-distribution generalization sets.
From this point forward, we refer to the in-distribution test set as the test set and the out-of-distribution generalization set as the generalization set.
The generalization set contains unseen syntactic structures that are combinations of those in the training set and requires models to fill the gaps.
The dataset is constructed based on a rule-based pipeline used in SGET~\citep{kumon-etal-2024-evaluating}, which evaluates the compositional generalization abilities of neural models on English-Japanese translation tasks.
The strict control of sentence generation in SGET utilizing PCFGs (Probabilistic Context-Free Grammars) allows for controlled gaps between the training set and generalization set, which enables the precise evaluation of generalization abilities.

Then, we train a Transformer model from scratch with the training set.
As for training tasks, we adopt machine translation and semantic parsing, which are commonly used in existing studies on the evaluation of compositional generalization.
The reason for using two tasks instead of just one is to investigate how the output format of a task impacts the models' inner processes.
The logical forms in the semantic parsing dataset are created mostly based on the rules proposed by \citet{reddy-etal-2017-universal} and postprocessings, following \citet{kim-linzen-2020-cogs}.
We remove redundant tokens while maintaining semantic interpretation, following \citet{wu-etal-2023-recogs}.

\subsection{Subnetwork Probing}
\label{subsec:subnetwork-probing}
Neural models have been shown to develop subnetworks for several kinds of linguistic generalization~\citep{bhaskar-etal-2024-heuristic, ahuja2024learningsyntaxplantingtrees} and modular solutions for compositionality~\citep{lepori2023break}.
Based on these findings, we hypothesize that the vanilla Transformer develops a subnetwork that generalizes compositionally.
To test the hypothesis, we apply subnetwork probing~\citep{cao-etal-2021-low}, a method for discovering an existing subnetwork that achieves high accuracy on a task, to the trained base model.
Subnetwork probing performs pruning-based probing by training a learnable mask.
This method is shown to have low complexity, which means that the mask itself does not learn the task much, and the abilities of the original models are preserved as desired.

In this work, we acquire a subnetwork that performs well in compositional generalization, if any, through subnetwork probing.
We use the generalization set to train masks and prune the models.
The details of subnetwork probing are in Appendix~\ref{sec:subnetwork-probing}.

% We called the discovered subnetwork \subnetwork.
% We also apply subnetwork probing to a randomly initialized vanilla Transformer to make sure that the learned mask does not learn the task.

% \subsection{Probing Linguistic Knowledge}
% \label{subsec:probing}
% It is essential to test whether a model encodes the linguistic knowledge required for compositional generalization before testing whether the model uses the knowledge for the output because a model naturally cannot use knowledge without encoding it.(\todo{?})
% For this purpose, we probe its encoder with the tasks used by \citet{elazar-etal-2021-amnesic}.
% Specifically, we adopt the following two tasks: syntactic dependency labels and syntactic constituency boundaries (marking the beginning and the end of phrases).
% Both of them are multi-label binary classification tasks, and requires models to assign binary labels to each dependency or constituency boundary for each token.
% We probe both the base model and the discovered subnetwork in \ref{subsec:subnetwork-probing} and compare the results to see the difference in knowledge encoded in each model.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{latex/sources/fig3_14.png}
    \caption{The multi-label classification task used in concept scrubbing for removing syntactic constituency.}
    \label{fig:classification}
\end{figure}


\subsection{Causal Analysis}
\label{subsec:causal}
We next analyze the trained models and discovered subnetworks in terms of the extent to which they depend on syntactic structures to generate answers in machine translation and semantic parsing.
One of the methods for analyzing the inner workings is to remove target features from a model and observe the causal effect of the removal.
LEACE~\citep{belrose2023leace} is a method for the concept erasure.
It removes only the target concept with as little impact on the original model as possible.
The method updates inner representations so that no linear classifiers can predict concept labels more accurately than a constant function and other concepts are preserved in the model.
Removing a concept from deep neural networks is achieved by a procedure called concept scrubbing~\citep{belrose2023leace}.
Concept scrubbing sequentially applies LEACE to every layer of a model from the first layer to the last one.
In particular, after LEACE is applied to a layer and scrubs a concept in it, the scrubbed representations are passed to the next layer, where LEACE is applied again.

We apply concept scrubbing to both the base models and the discovered subnetworks, removing the target syntactic knowledge.
After the concept removal, we evaluate the model predictions in the test set and generalization set of machine translation and semantic parsing.
The comparison of the model performances between before and after the concept removal reveals the causal effect of the syntactic feature of interest on the predictions.
Concept scrubbing is suitable for our analysis because it does not require creating alternative inputs or interchange interventions, which are difficult to define for controlling syntactic features in the generalization setting.

Concept scrubbing uses a classification task that represents the concept of interest to erase it.
We choose syntactic constituency and syntactic dependency as the concepts for model analysis.
We define multi-label classification tasks to represent each concept based on sequence tagging tasks by \citet{elazar-etal-2021-amnesic}, as shown in Figure~\ref{fig:classification}.
The task for syntactic constituency is tagging the beginning and the end of a phrase, and the task for syntactic dependency is labeling dependency relations.
We also test the impact of removing narrower concepts, which differs according to each generalization pattern, such as syntactic constituency regarding only the prepositional phrase (PP) modification of an indirect object noun phrase (NP).
In this case, labels are assigned only to the tokens involved in the concepts.
For example, when considering syntactic constituency regarding the PP modification of an indirect object NP, the beginning and the end of the PP and the NP containing the PP and the modified NP (not the modified NP) are labeled as ones.

As for the dataset for this classification task, English sentences are generated using the same rule-based method as the one for the main task datasets.
The labels are tagged based on syntax trees generated as by-products in the sentence generation process.
Constituency boundaries are based on the Penn Treebank~\citep{marcus-etal-1993-building} definitions, and dependency relations are based on the Universal Dependencies~\citep{mcdonald-etal-2013-universal} definitions.
Using these tasks and datasets, we test whether the models depend on syntactic constituency and syntactic dependency in compositional generalization.

\section{Experimental settings}
\label{sec:experiment}

\begin{table*}[t]
    \centering
    \begin{tabular}{p{0.20\linewidth}p{0.37\linewidth}p{0.35\linewidth}}
    \toprule
    Pattern & Training & Generalization \\
    \midrule
    PP in indirect object NP&
     The child gave \textbf{the pen on the table} to Liam.&
     The friend gave \textbf{the girl in the room} a hat.\\
    PP in subject NP &
     The child broke \textbf{a cup on the table}. &
    \textbf{The friend in the room} broke a cup.\\
     \bottomrule
    \end{tabular}
    \caption{Two compositional generalization patterns that we test in the experiments.}
    \label{tab:generalization_pattern}
\end{table*}

\subsection{Compositional Generalization Pattern}
\label{subsec:cg-pattern}

In this work, we focus on two compositional generalization patterns: PP in indirect object NP (\dobjppiobjpp{}) and PP in subject NP (\dobjppsubjpp{}{}), as shown in Table~\ref{tab:generalization_pattern}.
These two patterns are relatively simple from the syntactic perspective, which allows for easier causal analyses using multi-label classification tasks.

\paragraph{PP in indirect object NP (\dobjppiobjpp{})}
In this pattern, all the NPs modified by PPs in the training set appear in the direct object position.
Then, models trained on the training set are expected to generalize to PPs modifying indirect object NPs in the generalization set.
As \citet{li-etal-2023-slog} pointed out, some sentences have an indirect object NP modified by a PP before a direct object NP, and the dependency between a verb and the direct object NP goes across the PP, which makes the generalization more complex.

For causal analysis in this pattern, we test the impact of three narrower syntactic constituency and dependency concepts, the PP modification of indirect object NPs, direct object NPs, and all NPs, along with overall syntactic constituency and dependency.
\paragraph{PP in subject NP (\dobjppsubjpp{})}
Similarly to \dobjppiobjpp{}, all the NPs modified by PPs in the training set appear in the direct object position.
Models trained on the training set are expected to generalize to PPs modifying subject NPs in the generalization set.
One thing that makes \dobjppsubjpp{} difficult is that PP modifiers do not appear at the beginning of sentences in the training set.
The models may have to generalize to the novel placement of PP modifiers in addition to the novel grammatical role of modified NPs.
\citet{wu-etal-2023-recogs} add sentences with preposed PP modifiers in the training set to mitigate this issue, but we do not employ this approach for simpler comparisons between \dobjppiobjpp{} and \dobjppsubjpp{}.

For causal analysis in this pattern, we test the impact of three narrower syntactic constituency and dependency concepts, the PP modification of subject NPs, direct object NPs, and all NPs, along with overall syntactic constituency and dependency.
% \paragraph{Hint patterns}
% We add ``hint'' syntactic structures to the training set and train another base model.
% Those ``hints'' do not give models direct solutions but make the generalization easier.
% For example, in \dobjppiobjpp{} pattern, we add sentences with relative clauses modifying indirect object NPs.
% It makes the model understand that indirect object NPs can be modified like direct object NPs, which would make the generalization to \dobjppiobjpp{} easier than without ``hints''.

% \subsection{Target Linguistic Properties}
% We adopt the same two tasks as in probing~\ref{subsec:probing}, but we use several variants with different ground truths of labels and boundaries to analyze the causal effect of different dependencies and constituency boundaries.
% In syntactic dependency labels task, for example, we assign labels to all dependencies when analyzing the causal effect of syntactic dependencies in general.
% On the other hand, when analyzing the causal effect of the modification of a prepositional phrase (PP) to a noun phrase in the indirect object position, we assign labels only to the corresponding dependency because labels to other dependencies would lead to the removal of non-target properties.
% We utilize these variants to explore in detail what knowledge or concepts models use for the outputs, especially in the context of compositional generalization.

\subsection{Dataset}
As explained in Section~\ref{sec:method}, we newly construct datasets for each of machine translation and semantic parsing and for classification tasks used in concept scrubbing.
Each of the machine translation and semantic parsing datasets consists of a training set of 80,000 samples, a test set of 10,000 samples, and a generalization set of 30,000 samples.
We split the generalization set into two parts.
One part is used in training masks in subnetwork probing (Section~\ref{subsec:subnetwork-probing}), and the other is used in evaluating the trained models and subnetworks (Section~\ref{subsec:causal}).
Note that the generalization set is constructed for each generalization pattern, and subnetwork probing is performed for each pattern as well.
The dataset for classification tasks contains 9,000 samples, and all of them are used for concept scrubbing.

\subsection{Training Details}
We train an encoder-decoder Transformer model from scratch on our dataset.
The model has 3 encoder and 3 decoder layers, 4 attention heads.
We set batch size to 256, the number of epochs to 500, learning rate to 0.0001, and weight decay to 0.1.
We do not use early stopping, as \citet{csordas-etal-2021-devil} showed continued training without it improves model performance in compositional generalization.

As for subnetwork probing, we train a pruning mask for the trained model.
We set batch size to 256, the number of training epochs to 300, learning rate to 0.0005.
We do not use early stopping in subnetwork probing.

We run the experiments three times with random seeds and report the average scores as the results.
The final checkpoints of each training phrase are used for the main results (Section~\ref{sec:results}).

\subsection{Evaluation Metric}
\label{subsec:metric}
Following previous studies on evaluating compositional generalization~\citep{kim-linzen-2020-cogs, kumon-etal-2024-evaluating}, we adopt exact match accuracy for evaluation metrics of both machine translation and semantic parsing.
The rule-based pipeline for our dataset generation is designed so that a correct output can be determined uniquely if a model follows compositional rules; thus, using exact match accuracy in this experiment is appropriate.


\section{Results}
\label{sec:results}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{p{0.05\linewidth}p{0.22\linewidth}p{0.14\linewidth}p{0.14\linewidth}p{0.15\linewidth}}\toprule
    Task & Model & Test & \dobjppiobjpp{} & \dobjppsubjpp{}\\\midrule
    \multirow{3}{*}{MT} & Base & $99.9_{\pm 0.0}$ & $47.0_{\pm 2.2}$ & $0.0_{\pm 0.0}$  \\
    & \dobjppiobjpp{} Sub. & $99.8_{\pm 0.0}$ & $91.4_{\pm 3.0}$ & ---\\
    & \dobjppsubjpp{} Sub. & $96.5_{\pm 3.5}$ & --- & $57.0_{\pm 18.8}$ \\
    \multirow{3}{*}{SP} & Base & $99.8_{\pm 0.0}$ & $55.3_{\pm 4.1}$ & $0.1_{\pm 0.0}$\\
    & \dobjppiobjpp{} Sub. & $94.3_{\pm 0.8}$ & $91.2_{\pm 5.5}$ & ---\\
    & \dobjppsubjpp{} Sub. & $98.3_{\pm 0.0}$ & --- & $12.4_{\pm 7.4}$\\\bottomrule
    \end{tabular}
    \caption{Average exact match accuracy (\%) of the base models and subnetworks in machine translation and semantic parsing.
    MT stands for machine translation and SP stands for semantic parsing.
    \dobjppiobjpp{} Sub. (resp. \dobjppsubjpp{} Sub.) stands for the subnetwork for \dobjppiobjpp{} (resp. \dobjppsubjpp{}).
    The column labeled \dobjppiobjpp{} (resp. \dobjppsubjpp{}) represents the generalization performance in \dobjppiobjpp{} (resp. \dobjppsubjpp{}).}
    \label{tab:results_overall}
\end{table}
\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_500_scrub_plot_gen_base_con.png}
    \subcaption{Constituency (Base)}
    \label{fig:results_causal_a}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_500_scrub_plot_gen_base_dep.png}
    \subcaption{Dependency (Base)}
    \label{fig:results_causal_b}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_500_scrub_plot_gen_sub_con.png}
    \subcaption{Constituency (Sub.)}
    \label{fig:results_causal_c}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_500_scrub_plot_gen_sub_dep.png}
    \subcaption{Dependency (Sub.)}
    \label{fig:results_causal_d}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_500_scrub_plot_gen_base_con.png}
    \subcaption{Constituency (Base)}
    \label{fig:results_causal_e}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_500_scrub_plot_gen_base_dep.png}
    \subcaption{Dependency (Base)}
    \label{fig:results_causal_f}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_500_scrub_plot_gen_sub_con.png}
    \subcaption{Constituency (Sub.)}
    \label{fig:results_causal_g}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_500_scrub_plot_gen_sub_dep.png}
    \subcaption{Dependency (Sub.)}
    \label{fig:results_causal_h}
    \end{minipage}
    \caption{Results of causal analysis in \dobjppiobjpp{} (\ref{fig:results_causal_a}-\ref{fig:results_causal_d}) and in \dobjppsubjpp{} (\ref{fig:results_causal_e}-\ref{fig:results_causal_h}).
    Each bar shows the performance in the generalization set after the corresponding concept removal.
    "All" refers to entirely removing the corresponding syntactic feature.
    "Iobj-mod" (resp. "dobj-mod", "subj-mod") refers to removing the corresponding syntactic feature regarding the PP modifications of indirect object (resp. direct object, subject) NPs.
    "Mod" refers to removing the corresponding syntactic feature regarding the PP modifications.}
    \label{fig:results_causal}
\end{figure*}
\subsection{Output Evaluation}
\label{subsec:overall}
Before analyzing the inner workings of the models, we first present the model performance in main tasks without concept scrubbing.
Table~\ref{tab:results_overall} shows the results of the base models and subnetworks in machine translation and semantic parsing.

The base models and subnetworks both performed nearly perfectly in the test set of both machine translation and semantic parsing.
The performance of the base model was much worse both in \dobjppiobjpp{} and \dobjppsubjpp{} than that in the test set, which is consistent with the results of previous studies testing the same generalization patterns~\citep{li-etal-2023-slog, kumon-etal-2024-evaluating}.
On the other hand, the subnetwork scored more than 90\% accuracy in \dobjppiobjpp{} in both main tasks while keeping the in-distribution performance.
This suggests that some part of the trained model implements a certain algorithm that solves these compositional generalization tasks.
The subnetwork in \dobjppsubjpp{} also performed much better in the generalization set than the base model.
It is surprising to see these positive results, considering that existing studies have shown Transformer's poor performance in compositional generalization tasks.

\subsection{Causal Analysis}
\label{subsec:results_causal}
\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.35\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_ja_orig_scrub_step.png}
    \subcaption{Machine translation}
    \end{minipage}
    \begin{minipage}[b]{0.35\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_cogs_orig_scrub_step.png}
    \subcaption{Semantic parsing}
    \end{minipage}
    \caption{The shift of average accuracy of the models in \dobjppiobjpp{} over training epochs.
    Test and Gen. stand for the accuracy on the test and generalization set, respectively.
    Base and Sub. stand for the accuracy of the base model and subnetwork, respectively.}
    \label{fig:results_overall_epoch}
\end{figure*}
\subsubsection{Generalization in \dobjppiobjpp{}}
Figure~\ref{fig:results_causal_a}-\ref{fig:results_causal_d} presents the results of causal analysis of the base model and subnetwork in \dobjppiobjpp{}.
First, the base model performs much worse when syntactic constituency or dependency is removed, which shows the model's reliance on these syntactic features to correctly solve the main tasks.
However, the base model cannot be considered to have a compositionally generalizing solution because the generalization performance overall was far from perfect, and a compositionally generalizing model should perform nearly perfectly in the generalization set.

Next, we focus on the subnetwork, which achieved better accuracy in the generalization set than the base model.
Entirely removing syntactic constituency or dependency decreased the accuracy of the subnetwork to almost zero except when removing syntactic dependency in machine translation.
This shows that the subnetwork also depends on the syntactic features in general.

We then discuss the impact of the removal of constituency information on the modification of indirect object NPs.
If the subnetwork implements a compositional solution, removing the constituency information on the modification of indirect object NPs would decrease the accuracy to zero.
However, the difference in the performance before and after this concept removal is not as large as when the concept of the constituency is entirely removed, although the generalization performance of the subnetwork decreases to some extent.
A similar trend was seen in the removal of syntactic dependency.
These results suggest that the subnetwork somewhat depends on the constituency and dependency regarding the modification of indirect object NPs.
At the same time, the subnetwork implements a solution that somehow solves \dobjppiobjpp{} in machine translation and semantic parsing yet cannot be considered as a compositionally generalizing one.


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_ja_scrub_step_con_.png}
    \subcaption{Constituency (\dobjppiobjpp{})}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_ja_scrub_step_dep_.png}
    \subcaption{Dependency (\dobjppiobjpp{})}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_ja_scrub_step_con_.png}
    \subcaption{Constituency (\dobjppsubjpp{})}
    \end{minipage}
    \begin{minipage}[b]{0.245\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_ja_scrub_step_dep_.png}
    \subcaption{Dependency (\dobjppsubjpp{})}
    \end{minipage}
    \caption{The shift of average accuracy on the generalization set of the subnetworks with each concept removed in machine translation over training epochs.}
    \label{fig:results_causal_epoch}
\end{figure*}
\subsubsection{Generalization in \dobjppsubjpp{}}
In contrast to \dobjppiobjpp{}, the generalization performance of the subnetwork is far from 100\% accuracy, so the subnetwork is not expected to have a perfect solution that generalizes compositionally in \dobjppsubjpp{}.
We still examine what the subnetwork depends on in the main tasks.

Figure~\ref{fig:results_causal_e}-\ref{fig:results_causal_h} shows the results of causal analysis of the subnetwork in \dobjppsubjpp{}.
Similar to \dobjppiobjpp{}, when the information of syntactic constituency or dependency was entirely removed, the performance of the subnetwork dropped to almost 0\% except when removing syntactic dependency in machine translation.
The subnetwork depends on this information in \dobjppsubjpp{} as well.

As for the impact of the removal of the constituency regarding the modification of subject NPs, the performance dropped almost as much as the removal of the entire constituency.
It suggests the subnetworks' heavy reliance on the modification of subject NPs and their ability to properly use compositional rules at least when the output is correct.
On the other hand, when the constituency regarding direct object NPs, the performance improved significantly especially in semantic parsing.
This implies that the subnetwork was overfitted to the modification of direct object NPs, and it prevented the subnetwork from achieving better accuracy in \dobjppsubjpp{}.
Also, the subnetwork seems capable of using the information of modification of NPs regardless of whether NPs are subject or direct object, as this removal would not improve the accuracy if the subnetwork learns only that the modification can only come with direct object NPs.


% \section{Discussion}
% \label{sec:discussion}

\subsection{Transition during training}
We next present how the model performance evolved throughout training in \dobjppiobjpp{}, shown in Figure~\ref{fig:results_overall_epoch}.
It shows that the accuracy in the test set grew rapidly, whereas the accuracy in the generalization set improved slowly.
In addition, comparing the base model and subnetwork in machine translation, the generalization performance of the subnetwork continued to improve through 500 epochs, and that of the base model improved only slightly.
Therefore, the model may gradually learn an algorithm that can solve the generalization task through the training process with the machine translation task without changing the behavior of the whole model much.
The difference in the performance transition between the base model and subnetwork is much less noticeable in semantic parsing.
These results suggest that differences in the task settings, such as output formats where structures are more explicitly represented in semantic parsing, influence generalization performance.
A similar tendency was discovered in \dobjppsubjpp{} (See Appendix~\ref{sec:causal_other} for details), although the generalization performance was generally lower.

We next look into how the inner workings of the subnetwork changed as the training went on.
Figure~\ref{fig:results_causal_epoch} shows the shift of the generalization performance in machine translation of the subnetwork with each linguistic feature removed.
In machine translation, the generalization performance of the subnetwork with certain syntactic feature removed was mostly consistent after 200 epochs.
This strongly suggests that the subnetwork learned a non-compositional solution in the early stage of the training and retained it throughout the training.
Combined with the observation that the original subnetwork continued to improve its accuracy beyond 200 epochs, this result also indicates that a compositional solution relying on syntactic features was acquired gradually.

Similarly, in semantic parsing, the subnetwork with a certain linguistic feature removed mostly retains its performance after 200 epochs, regardless of the generalization accuracy of the original subnetwork.
The detailed results of semantic parsing are in Appendix~\ref{sec:causal_other}.



\section{Discussion}
\label{sec:discussion}
\subsection{Reliability of LEACE}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{p{0.23\linewidth}p{0.17\linewidth}p{0.19\linewidth}p{0.19\linewidth}}\toprule
    Model & Original & Constituency Removed & Dependency Removed \\\midrule
    Base & $90.1_{\pm 2.8}$ & $88.8_{\pm 4.6}$ & $90.0_{\pm 3.6}$\\
    \dobjppiobjpp{} Sub. & $87.9_{\pm 4.8}$ & $83.8_{\pm 6.3}$ & $89.3_{\pm 3.0}$\\ 
    \dobjppsubjpp{} Sub. & $88.0_{\pm 2.1}$ &  & \\
    \bottomrule
    \end{tabular}
    \caption{Average accuracy in word-to-word translation of content words.}
    \label{tab:results_word}
\end{table}
Since LEACE is a linear concept erasure method, it may fail to remove concepts encoded non-linearly.
However, the results when all syntactic information was removed (Figure~\ref{fig:results_causal}) indicate that most syntactic information used in machine translation and semantic parsing is encoded in linear subspaces.
Thus, the impact of non-linearly encoded features should be negligible.

We also validate that LEACE does not erase concepts orthogonal to syntactic ones.
We test the model with a word-to-word translation (English to Japanese) of content words. Word-to-word translation of content words can be solved without relying on syntax at all. We probe the representation of the final layer of the encoder by a one-layer linear classifier for word-to-word translation. We focus on evaluating the models trained with machine translation datasets. Accuracy is calculated as the proportion of the sentences where all content words are translated correctly.
The results, shown in Table~\ref{tab:results_word}, suggest that the removal of syntactic features lead to almost no loss of performance in word-to-word translation, confirming that concepts orthogonal to syntactic ones are preserved in LEACE.

\subsection{Impact of Adding Hint in Training}
\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_500_scrub_plot_hint_gen_base_con.png}
    \subcaption{Constituency (Base)}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_500_scrub_plot_hint_gen_base_dep.png}
    \subcaption{Dependency (Base)}
    \end{minipage}
    \caption{Results of causal analysis in \dobjppiobjpp{} when trained with a hint.}
    \label{fig:results_hint}
\end{figure}
Finally, we investigate how a Transformer model performs under a setting where compositional generalization is easier.
We augment the training set with sentences containing syntactic structures that provide clues for generalization.
In particular, we focus on \dobjppiobjpp{} and augment the training set with sentences that have a relative clause (RC) modifying an indirect object NP and with ones that have an RC modifying a direct object NP.
It should be easier for the model trained with the data involving PPs modifying direct object NPs to generalize to a PP modifying an indirect object NP based on the newly provided hints.

Figure~\ref{fig:results_hint} presents the results of causal analysis.
Compared with the results without any hint (Figure~\ref{fig:results_causal_a},\ref{fig:results_causal_b}), the generalization performance without any concept removal improved by about 40\% in both main tasks.
Moreover, the performance after the removal of syntactic features regarding the PP modifications of indirect object NPs improved only slightly.
This suggests that the algorithm that was implemented in the base model and contributed to the gain in the generalization performance relied on those specific syntactic features.
Thus, the model might implement a more robust compositional solution by utilizing the provided hints.

\section{Conclusion}
\label{sec:conclusion}
In this work, we investigated the inner mechanisms of a Transformer model in compositional generalization tasks.
The experimental results showed that the model utilizes syntactic features to some extent in the generalization but that its subnetwork with better generalization accuracy depends on non-syntactic features as well.
It indicates that the model develops a non-compositional solution internally and fails to generalize compositionally even when the generalization performance is decent.
We believe that this paper serves as a foundation for analyzing the underlying mechanisms in compositional generalization from the linguistic perspective.
Future work might consider other generalization patterns or other linguistic features to obtain a more profound insight into the linguistic competence of neural models.

\section*{Limitation}
The compositional generalization tasks used in this work are based on synthetically generated datasets.
They might not represent the variety in natural language expressions enough.
However, the precise evaluation requires these controlled settings because all the lexical items and syntactic structures have to be properly split into the training and generalization set.
Therefore, using a natural corpus for this experiment would demand much effort.
We leave it for future work.

Another limitation is that the results in this experiment do not necessarily transfer to larger-scaled models because we tested relatively small models trained on a small synthetic dataset following previous studies.
It would be worth exploring how the trends discovered here change with the model size gets larger.


\section*{Ethical Considerations}
All of our datasets are constructed for the sole purpose of the model analysis from the linguistic perspective.
They do not contain any contents that can be potentially harmful or offensive.

\section*{Acknowledgments}
This work was supported by JSPS KAKENHI grant number JP24H00809, JST PRESTO grant number JPMJPR21C8.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{anthology, custom}

\appendix

\section{Subnetwork Probing}
\label{sec:subnetwork-probing}
Subnetwork probing~\citep{cao-etal-2021-low} trains a mask to find a subnetwork of interest.
Let $\phi \in \mathbb{R}^d$ be the weights of a model and $Z_{i}\in [0,1]$ be the mask for the weight $\phi_i$.
$Z_i$ follows the hard concrete function parameterized with temperature $\beta_i$ and a random variable $\theta_i$,
\begin{align*}
U_i &\sim \mathrm{Unif}[0, 1]\\
S_i &= \sigma\left(\frac{1}{\beta}\left(\log\frac{U_i}{1-U_i}+\theta_i\right)\right)\\
Z_i &= \min (1, \max(0, S_i(\zeta-\gamma)+\gamma)),
\end{align*}
and $\zeta=1.1, \gamma=-0.1$ are fixed here.

Subnetwork probing optimizes the mask parameter $\theta$ by minimizing the following loss function:
\begin{align*}
    &\frac{1}{|D|}\sum_{(x, y)\in D}\mathbb{E}_{U_i\sim \mathrm{Unif}[0,1]}L(f(x; \phi * z(U, \theta)), y) \\
    &+ \lambda\mathbb{E}|\theta|_0.
\end{align*}
The first term is the loss function for the model $f$ masked by $Z_i=z(U_i,\theta_i)$, and the second term corresponds to the penalty for non-zero masks to induce sparsity.
During inference, the mask $Z_i$ is binarized to $\{0, 1\}$ based on a threshold.
% \input{latex/sections/a2_results_base}

\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.7\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_ja_orig_scrub_step.png}
    \subcaption{Machine translation}
    \end{minipage}
    \begin{minipage}[b]{0.7\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_cogs_orig_scrub_step.png}
    \subcaption{Semantic parsing}
    \end{minipage}
    \caption{The shift of average accuracy of the models in \dobjppsubjpp{} over training epochs.}
    \label{fig:results_overall_epoch_other}
\end{figure}
\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_cogs_scrub_step_con_.png}
    \subcaption{Constituency (\dobjppiobjpp{})}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/dobjpp2iobjpp_cogs_scrub_step_dep_.png}
    \subcaption{Dependency (\dobjppiobjpp{})}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_cogs_scrub_step_con_.png}
    \subcaption{Constituency (\dobjppsubjpp{})}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{latex/sources/objpp2subjpp_cogs_scrub_step_dep_.png}
    \subcaption{Dependency (\dobjppsubjpp{})}
    \end{minipage}
    \caption{The shift of average accuracy of the models with each concept removed in semantic parsing over training epochs.}
    \label{fig:results_causal_epoch_other}
\end{figure}
\section{Other results of the transition during training}
\label{sec:causal_other}
Figure~\ref{fig:results_overall_epoch_other} presents how the model performance changes during training in \dobjppsubjpp{}.
Figure~\ref{fig:results_causal_epoch_other} shows the shift of the generalization performance in semantic parsing when a certain syntactic feature is removed.



\section{Details of Discovered Subnetworks}
\label{sec:results_subnetwork}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{p{0.12\linewidth}p{0.15\linewidth}p{0.15\linewidth}p{0.15\linewidth}p{0.15\linewidth}}\toprule
    Task & \multicolumn{2}{c}{MT} & \multicolumn{2}{c}{SP}\\
    Layer & \dobjppiobjpp{} Sub. & \dobjppsubjpp{} Sub. & \dobjppiobjpp{} Sub. & \dobjppsubjpp{} Sub. \\\midrule
    Enc. 0 & $55.7_{\pm 0.2}$ & $58.0_{\pm 0.3}$ & $49.8_{\pm 0.4}$ & $53.9_{\pm 0.4}$\\
    Enc. 1 & $57.9_{\pm 0.7}$ & $61.4_{\pm 0.2}$ & $51.5_{\pm 0.3}$ & $56.6_{\pm 0.4}$\\ 
    Enc. 2 & $59.6_{\pm 1.1}$ & $54.0_{\pm 0.5}$ & $63.9_{\pm 0.9}$ & $59.2_{\pm 0.2}$\\
    Dec. 0 & $59.3_{\pm 0.9}$ & $63.1_{\pm 0.6}$ & $53.5_{\pm 0.6}$ & $59.0_{\pm 0.5}$\\
    Dec. 1 & $63.8_{\pm 0.7}$ & $67.5_{\pm 0.3}$ & $58.8_{\pm 0.2}$ & $64.9_{\pm 0.2}$\\
    Dec. 2 & $68.5_{\pm 0.1}$ & $71.3_{\pm 0.2}$ & $61.6_{\pm 0.2}$ & $67.4_{\pm 0.1}$\\
    \bottomrule
    \end{tabular}
    \caption{Average proportion of unmasked weights in the two patterns and two tasks.}
    \label{tab:results_subnetwork}
\end{table}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{llll}\toprule
    Layer & Overall & Attention & MLP \\\midrule
    Enc. 0 & $55.7_{\pm 0.2}$ & $62.0_{\pm 0.3}$ & $52.5_{\pm 0.2}$\\
    Enc. 1 & $57.9_{\pm 0.7}$ & $56.8_{\pm 0.8}$ & $58.4_{\pm 1.0}$\\ 
    Enc. 2 & $59.6_{\pm 1.1}$ & $58.3_{\pm 1.3}$ & $61.0_{\pm 0.8}$\\
    Dec. 0 & $59.3_{\pm 0.9}$ & $53.9_{\pm 1.4}$ & $64.6_{\pm 0.5}$\\
    Dec. 1 & $63.8_{\pm 0.7}$ & $59.9_{\pm 1.5}$ & $67.6_{\pm 0.4}$\\
    Dec. 2 & $68.5_{\pm 0.1}$ & $64.7_{\pm 0.1}$ & $72.3_{\pm 0.2}$\\
    \bottomrule
    \end{tabular}
    \caption{Average proportion of unmasked weights in \dobjppiobjpp{} and machine translation.}
    \label{tab:results_subnetwork_ex}
\end{table}

Following \citet{cao-etal-2021-low}, we calculated the proportion of unmasked weights.
The trend of the proportion was mostly the same for all tasks and patterns.
The proportion of unmasked weights in each encoder and decoder layer was around 50-70\%, but the proportion is larger for deeper layers both in encoder and decoder.
Also, there were more unmasked weights in the decoder than in the encoder generally.
As for the proportion of unmasked weights in MLPs and attention blocks, most layers have more unmasked weights in MLPs than in attention blocks.
We show the proportion of unmasked weights in the extracted subnetworks in Table~\ref{tab:results_subnetwork} and \ref{tab:results_subnetwork_ex}.



\section{Computational Resources}
We used NVIDIA V100 GPUs for all the experiments.
The total runtime for the training and evaluation was around 300 hours.
\end{document}
