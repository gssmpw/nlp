\input{sources/fig2}
\section{Analysis Method}
\label{sec:method}

Figure~\ref{fig:overview} presents our method of analyzing the inner workings of a Transformer model in compositional generalization.
It consists of the following three phases:
training a base model, subnetwork probing, and causal analysis.

\subsection{Training Base Model}
\label{subsec:training}
To test the compositional generalization abilities of a model, we first construct a dataset that contains the training set, in-distribution test set, and out-of-distribution generalization set.
Hereinafter, we refer to the in-distribution test set as the test set and the out-of-distribution generalization set as the generalization set.
The generalization set contains unseen syntactic structures that are combinations of those in the training set and requires models to fill the gaps.
The dataset is constructed based on a rule-based pipeline used in SGET~\citep{kumon-etal-2024-evaluating}, which evaluates the compositional generalization abilities of neural models on English--Japanese translation tasks.
The strict control of sentence generation in SGET utilizing PCFGs (Probabilistic Context-Free Grammars) allows for controlled gaps between the training set and generalization set, which enables the precise evaluation of generalization abilities.

Next, we train a Transformer model from scratch with the training set.
As for training tasks, we adopt machine translation and semantic parsing, which have been commonly used in existing studies of evaluating compositional generalization.
The reason for using two tasks instead of just one is to investigate how the output format of a task impacts the models' inner processes.
The logical forms in the semantic parsing dataset are created mostly based on the rules proposed by \citet{reddy-etal-2017-universal} and postprocessings, following \citet{kim-linzen-2020-cogs}.
We remove redundant tokens while maintaining semantic interpretation, following \citet{wu-etal-2023-recogs}.

\subsection{Subnetwork Probing}
\label{subsec:subnetwork-probing}
Neural models have been shown to develop subnetworks for several types of linguistic generalization~\citep{bhaskar-etal-2024-heuristic, ahuja2024learningsyntaxplantingtrees} and modular solutions for compositionality~\citep{lepori2023break}.
Based on these findings, we hypothesize that the vanilla Transformer develops a subnetwork that generalizes compositionally.
To test this hypothesis, to the trained base model we apply subnetwork probing~\citep{cao-etal-2021-low}, a method for discovering an existing subnetwork that achieves high accuracy on a task.
Subnetwork probing performs pruning-based probing by training a learnable mask.
This method is shown to have low complexity, which means that the mask itself does not learn the task much, and the abilities of the original models are preserved as desired.

In this work, we acquire a subnetwork that performs well in compositional generalization, if any, through subnetwork probing.
We use the generalization set to train masks and prune the models.
The details of subnetwork probing are in Appendix~\ref{sec:subnetwork-probing}.

% We called the discovered subnetwork \subnetwork.
% We also apply subnetwork probing to a randomly initialized vanilla Transformer to make sure that the learned mask does not learn the task.

% \subsection{Probing Linguistic Knowledge}
% \label{subsec:probing}
% It is essential to test whether a model encodes the linguistic knowledge required for compositional generalization before testing whether the model uses the knowledge for the output because a model naturally cannot use knowledge without encoding it.(\todo{?})
% For this purpose, we probe its encoder with the tasks used by \citet{elazar-etal-2021-amnesic}.
% Specifically, we adopt the following two tasks: syntactic dependency labels and syntactic constituency boundaries (marking the beginning and the end of phrases).
% Both of them are multi-label binary classification tasks, and requires models to assign binary labels to each dependency or constituency boundary for each token.
% We probe both the base model and the discovered subnetwork in \ref{subsec:subnetwork-probing} and compare the results to see the difference in knowledge encoded in each model.

\input{sources/fig3}
\subsection{Causal Analysis}
\label{subsec:causal}
Next, we analyze the trained models and discovered subnetworks in terms of the extent to which they depend on syntactic structures to generate answers in machine translation and semantic parsing.
One of the methods for analyzing the inner workings is to remove target features from a model and observe the causal effect of the removal.
LEACE~\citep{belrose2023leace} is a method for concept erasure in which only the target concept is removed, with as little impact on the original model as possible.
The method updates inner representations so that no linear classifiers can predict concept labels more accurately than a constant function and other concepts are preserved in the model.
Removing a concept from deep neural networks is achieved by a procedure called concept scrubbing~\citep{belrose2023leace}, which sequentially applies LEACE to every layer of a model from the first to the last.
In particular, after LEACE is applied to a layer and scrubs a concept therein, the scrubbed representations are passed to the next layer, where LEACE is applied again.

We apply concept scrubbing to both the base models and the discovered subnetworks, removing the target syntactic knowledge.
After the concept removal, we evaluate the model predictions in the test set and generalization set of machine translation and semantic parsing.
Comparing the model performances before and after the concept removal reveals the causal effect of the syntactic feature of interest on the predictions.
Concept scrubbing is suitable for our analysis because it does not require creating alternative inputs or interchange interventions, which are difficult to define for controlling syntactic features in the generalization setting.

Concept scrubbing uses a classification task that represents the concept of interest to erase it.
We choose syntactic constituency and syntactic dependency as the concepts for model analysis.
We define multi-label classification tasks to represent each concept based on sequence tagging tasks by \citet{elazar-etal-2021-amnesic}, as shown in Figure~\ref{fig:classification}.
The task for syntactic constituency is tagging the beginning and end of a phrase, and the task for syntactic dependency is labeling dependency relations.
We also test the impact of removing narrower concepts, which differs according to each generalization pattern, such as syntactic constituency regarding only the prepositional phrase (PP) modification of an indirect object noun phrase (NP).
In this case, labels are assigned only to the tokens involved in the concepts.
For example, when considering syntactic constituency regarding the PP modification of an indirect object NP, the beginning and end of the PP and the NP containing the PP and the modified NP (not the modified NP) are labeled as ones.

As for the dataset for this classification task, English sentences are generated using the same rule-based method as the one for the main task datasets.
The labels are tagged based on syntax trees generated as by-products in the sentence generation process.
Constituency boundaries are based on the Penn Treebank~\citep{marcus-etal-1993-building} definitions, and dependency relations are based on the Universal Dependencies~\citep{mcdonald-etal-2013-universal} definitions.
Using these tasks and datasets, we test whether the models depend on syntactic constituency and syntactic dependency in compositional generalization.