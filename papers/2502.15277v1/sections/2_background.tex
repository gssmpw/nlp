\section{Background}
\label{sec:background}

\subsection{Compositional Generalization}
\label{subsec:compositional-generalization}
Several studies have explored the compositional generalization abilities of modern neural models by focusing on the model performance in tasks such as semantic parsing~\cite{kim-linzen-2020-cogs, csordas-etal-2021-devil, yao-koller-2022-structural,kim2022uncontrolledlexicalexposureleads, li-etal-2023-slog} and machine translation~\cite{li-etal-2021-compositional, dankers-etal-2022-paradox, kumon-etal-2024-evaluating}.
Such studies indicate that the models lack compositional generalization abilities in general, while others have worked on improving them by modifying the model architectures~\cite{bergan2021edge,ontanon-etal-2022-making}.
\citet{yao-koller-2022-structural} briefly analyzed which part of seq2seq models causes poor performance in compositional generalization by probing the encoder.
They attributed the poor performance to the decoder, stating that the encoder has the linguistic knowledge needed to solve the generalization tasks but the decoder does not use it.
These studies focused mainly on the model outputs and the encoded properties, and there has been a debate on whether behavioral evaluation is sufficient to assess compositionality in neural models~\citep{mccurdy-etal-2024-toward}.
Our research analyzes the inner workings of models with subnetwork search and causal analysis, which should give insights into what features causally impact the model behavior.
We consider that a compositional solution generalizes by leveraging syntactic structures consistently in a bottom-up manner.
In our experiments, we focus on syntactic features that a compositional solution should utilize.
Thus, we regard a solution that does not employ these features as non-compositional.


%\subsection{Interpretability from linguistic perspective\todo{}}
\subsection{Linguistic Approach to Interpretability}
\label{subsec:interpretability}
% Neural models have been analyzed in various ways.
% Probing is used to discover what models encode in their inner representations.
% Numerous studies have utilized probing to test how pretrained language models encode linguistic information such as sentence structure~\citep{tenny2019learn, hewitt-manning-2019-structural}.
% Probing is often criticized because probing classifiers may learn the task itself, which leads us to overestimate the models' linguistic abilities~\citep{hewitt-liang-2019-designing, pimentel-etal-2020-information}.
% To address this, \citet{cao-etal-2021-low} proposed subnetwork probing, which is a pruning-based probing method that achieves better accuracy in the target linguistic task and worse at learning the task itself than a multi-layer perceptron.

One approach to interpreting neural models is to study the causal relationship between the target feature and the model behavior based on interventions.
Interventions alter either a model's inputs or its inner representations so that the target feature is the only change, and they test how that change affects the model's outputs.
Recent work has analyzed the linguistic mechanisms~\citep{tucker-etal-2021-modified, elazar-etal-2021-amnesic, ravfogel-etal-2021-counterfactual, feder-etal-2021-causalm, amini-etal-2023-naturalistic, belrose2023leace, arora-etal-2024-causalgym}.
\citet{belrose2023leace} utilized concept erasure, which removes only the concept of interest from the model and tests causal links by comparing the predictions by the model before and after the removal.
We employ their method for causally analyzing the causal role of syntactic features in compositional generalization because of its compatibility with our compositional generalization tasks.

Another line of work has explored finding subnetworks with specific properties of interest as a method for model analysis.
\citet{cao-etal-2021-low} proposed subnetwork probing, a pruning-based method that searches for a subnetwork that performs a target linguistic task.
As for linguistic generalization, previous studies have found subnetworks that perform syntactic generalization~\citep{bhaskar-etal-2024-heuristic},  hierarchical generalization~\citep{ahuja2024learningsyntaxplantingtrees}, and compositional generalization~\citep{hu2024compositional}.