\section{Subnetwork Probing}
\label{sec:subnetwork-probing}
Subnetwork probing~\citep{cao-etal-2021-low} trains a mask to find a subnetwork of interest.
Let $\phi \in \mathbb{R}^d$ be the weights of a model and $Z_{i}\in [0,1]$ be the mask for the weight $\phi_i$.
$Z_i$ follows the hard concrete function parameterized with temperature $\beta_i$ and a random variable $\theta_i$, that is,
\begin{align*}
U_i &\sim \mathrm{Unif}[0, 1],\\
S_i &= \sigma\left(\frac{1}{\beta}\left(\log\frac{U_i}{1-U_i}+\theta_i\right)\right),\\
Z_i &= \min (1, \max(0, S_i(\zeta-\gamma)+\gamma)),
\end{align*}
and $\zeta=1.1$ and $\gamma=-0.1$ are fixed here.

Subnetwork probing optimizes the mask parameter $\theta$ by minimizing the following loss function:
\begin{align*}
    &\frac{1}{|D|}\sum_{(x, y)\in D}\mathbb{E}_{U_i\sim \mathrm{Unif}[0,1]}L(f(x; \phi * z(U, \theta)), y) \\
    &+ \lambda\mathbb{E}|\theta|_0.
\end{align*}
The first term is the loss function for the model $f$ masked by $Z_i=z(U_i,\theta_i)$, and the second term corresponds to the penalty for non-zero masks to induce sparsity.
During inference, the mask $Z_i$ is binarized to $\{0, 1\}$ based on a threshold.