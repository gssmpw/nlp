\section{Introduction}
\label{sec:introduction}
Compositional generalization, the ability to understand the meaning of a novel language expression based on the composition of known words and syntactic structures~\citep{partee1984compositionality, fodor1988connectionism}, is a crucial aspect for robustness against unseen language data.
To assess the compositional generalization abilities of neural models, most existing studies have primarily focused on evaluating model outputs in compositional generalization benchmarks~\citep{kim-linzen-2020-cogs, li-etal-2021-compositional, dankers-etal-2022-paradox, li-etal-2023-slog}.
\input{sources/fig1}
However, the model outputs do not necessarily reflect the underlying competence because good performance in the benchmarks does not guarantee that the model implements a solution that generalizes based on compositional rules (i.e., compositional syntax) and vice versa.
In addition, while a growing body of work on model interpretability has investigated the inner workings of Transformer~\citep{vaswani2017attention}-based models~\citep{ferrando2024primerinnerworkingstransformerbased, rai2024practicalreviewmechanisticinterpretability}, compositional generalization has rarely been the focus of such studies.
\citet{yao-koller-2022-structural} and \citet{murty2023characterizing} analyzed the internal mechanisms in compositional generalization but did not focus on the usage of syntactic features, which is the central part of compositional generalization.
Thus, the internal mechanisms of the model in compositional generalization are still unclear, and unveiling them would enhance the understanding of the model's competence.

In this work, we analyze the inner workings of neural models to understand what type of syntactic features the models depend on in tasks requiring compositional generalization.
Our analysis method\footnote{Our code and data are available at \url{https://github.com/ynklab/CG_interp}.} consists of (i) identifying subnetworks within the model that perform well in the generalization and (ii) investigating how syntactic features causally affect the original model and its subnetwork, as shown in Figure~\ref{fig:1}.
The causal analysis involves removing the linguistic concept of interest from the model and comparing the generalization performance before and after the removal.
To rigorously evaluate and analyze the compositional generalization abilities, we focus not on pretrained models but on a Transformer model trained from scratch; this is because pretraining data contain syntactic structures that should be unseen in this experiment, and pretrained models may not need to generalize compositionally~\citep{kim2022uncontrolledlexicalexposureleads}.

We also aim to conduct a detailed analysis by experimenting with various settings.
This study employs two commonly used tasks for evaluating compositional generalization: machine translation and semantic parsing.
We test the models with two patterns of compositional generalization, \dobjppiobjpp{} and \dobjppsubjpp{}, which are similar yet different (see Section~\ref{subsec:cg-pattern} for details).

The findings from our experiments suggest that the model and its subnetwork that contributes to the generalization performance indeed leverage syntactic structures.
However, intriguingly, we also discover that the subnetwork implements other solutions that do not depend on syntactic structures.
From these results, we argue that the solutions that the models employ are partly non-compositional.
Moreover, analysis of the model at different epochs during the training revealed that the model gradually develops a subnetwork with better generalization performance.
The causal analysis of the subnetwork showed that the non-compositional solution was learned during the early phase of the training.
We argue that Transformer models need a better inductive bias to generalize compositionally utilizing syntactic features.