\section{Results}
\label{sec:results}
\input{sources/tab_results_overall}
\input{sources/fig_results_causal_dobjpp2iobjpp}
\subsection{Output Evaluation}
\label{subsec:overall}
Before analyzing the inner workings of the models, we present the model performance in main tasks without concept scrubbing.
Table~\ref{tab:results_overall} shows the results of the base models and subnetworks in machine translation and semantic parsing.

The base models and subnetworks both performed nearly perfectly in the test set of both machine translation and semantic parsing.
The performance of the base model was much worse in both \dobjppiobjpp{} and \dobjppsubjpp{} than that in the test set, which is consistent with the results of previous studies testing the same generalization patterns~\citep{li-etal-2023-slog, kumon-etal-2024-evaluating}.
On the other hand, the subnetwork scored more than 90\% accuracy in \dobjppiobjpp{} in both main tasks while keeping the in-distribution performance.
This suggests that some part of the trained model implements a certain algorithm that solves these compositional generalization tasks.
The subnetwork in \dobjppsubjpp{} also performed much better in the generalization set than did the base model.
It is surprising to see these positive results, considering that previous studies have shown Transformers' poor performance in compositional generalization tasks.

\subsection{Causal Analysis}
\label{subsec:results_causal}
\input{sources/fig_results_overall_epoch}
\subsubsection{Generalization in \dobjppiobjpp{}}
Figures~\ref{fig:results_causal_a}--\ref{fig:results_causal_d} present the results of causal analysis of the base model and subnetwork in \dobjppiobjpp{}.
First, the base model performed much worse when syntactic constituency or dependency was removed, which shows the model's reliance on these syntactic features to correctly solve the main tasks.
However, the base model cannot be considered to have a compositionally generalizing solution because the generalization performance overall was far from perfect, and a compositionally generalizing model should perform nearly perfectly in the generalization set.

Next, we focus on the subnetwork, which achieved better accuracy in the generalization set than did the base model.
Entirely removing syntactic constituency or dependency decreased the accuracy of the subnetwork to almost zero except when removing syntactic dependency in machine translation.
This shows that the subnetwork also depends on the syntactic features in general.

We then discuss the impact of the removal of constituency information on the modification of indirect object NPs.
If the subnetwork implements a compositional solution, then removing the constituency information on the modification of indirect object NPs would decrease the accuracy to zero.
However, the difference in the performance before and after this concept removal is not as large as when the concept of the constituency is removed entirely, although the generalization performance of the subnetwork decreases to some extent.
A similar trend was seen in the removal of syntactic dependency, although the decline in the generalization performance was smaller.
These results suggest that the subnetwork depends somewhat on the constituency and dependency regarding the modification of indirect object NPs.
At the same time, the subnetwork implements a solution that somehow handles \dobjppiobjpp{} in machine translation and semantic parsing yet cannot be considered as a compositionally generalizing one.

Furthermore, regarding the differences in the results between the two tasks, the drop in the generalization performance after the removal of syntactic information was smaller in semantic parsing than in machine translation.
It indicates that the models relied on syntactic features instead of a non-compositional solution more in semantic parsing than in machine translation.

\input{sources/fig_results_causal_epoch}
\subsubsection{Generalization in \dobjppsubjpp{}}
In contrast to \dobjppiobjpp{}, the generalization performance of the subnetwork is far from 100\% accuracy, so the subnetwork is not expected to have a perfect solution that generalizes compositionally in \dobjppsubjpp{}.
However, we still examine on what the subnetwork depends in the main tasks.

Figures~\ref{fig:results_causal_e}--\ref{fig:results_causal_h} show the results of causal analysis of the subnetwork in \dobjppsubjpp{}.
Similar to \dobjppiobjpp{}, when the information of syntactic constituency or dependency was entirely removed, the performance of the subnetwork dropped to almost 0\% except when removing syntactic dependency in machine translation.
The subnetwork depends on this information in \dobjppsubjpp{} as well.

As for the impact of the removal of the constituency regarding the modification of subject NPs, the performance dropped almost as much as with the removal of the entire constituency.
This suggests the subnetworks' heavy reliance on the modification of subject NPs and their ability to properly use compositional rules at least when the output is correct.
On the other hand, when the constituency regarding direct object NPs was removed, the performance improved considerably especially in semantic parsing.
This implies that the subnetwork was overfitted to the modification of direct object NPs, and this prevented the subnetwork from achieving better accuracy in \dobjppsubjpp{}.
Also, the subnetwork seems capable of using the information of modification of NPs regardless of whether NPs are subjects or direct objects, as this removal would not improve the accuracy if the subnetwork learns only that the modification can only come with direct object NPs.
