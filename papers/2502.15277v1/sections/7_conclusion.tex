\section{Conclusion}
\label{sec:conclusion}
In this work, we investigated the inner mechanisms of a Transformer model in compositional generalization tasks.
The experimental results showed that the model utilizes syntactic features to some extent in the generalization but that its subnetwork with better generalization accuracy depends on non-syntactic features as well.
This indicates that the model develops a non-compositional solution internally and fails to generalize compositionally even when the generalization performance is decent.
This paper serves as a foundation for analyzing the underlying mechanisms in compositional generalization from the linguistic perspective.
Future work might consider other generalization patterns or other linguistic features to obtain a more profound insight into the linguistic competence of neural models.