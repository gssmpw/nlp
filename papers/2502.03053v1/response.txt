\section{Related work}
\label{sec:related_work}

\textbf{Document-level test sets}

Most resources in MT research are sentence-level, but there is a number of test sets that include not only individual sentences but also longer texts. These are usually made without trying to target any context-sensitive phenomenon and typically the evaluation is performed at sentence-level, i.e., by feeding the sentences separately to the evaluation metrics **Papineni et al., "The ACL Anthology Corpus: A Dataset of 300+ Papers"**.
These works usually take the form of a source file, target file and a file with document ids. The order of lines creates a mapping between the source and target sentences as well as the document ids, therefore all three files must contain the same number of lines.

Our test set diverges by taking a novel approach and abandoning the idea of a perfect sentence-to-sentence alignment. We consider that it constrains the translation process and strips the texts of high-level phenomena such as information reorganisation, sentence splitting and merging. 
Section \ref{sec:imperf-alignment} details the reasons behind this choice.

There are also a number of document-level training datasets **Kocmi et al., "A New Benchmark for Document-Level Machine Translation"** and recently there have been efforts to recover the contexts of large sentence-level datasets **Liao et al., "Recovering Context in Sentence-Level Datasets"**. Since these are for training purposes, they are out of the scope of this work that focuses on test data.

As for datasets specialised on financial language, these are very scarce; we can mention the European Central Bank corpus that is included in OPUS **Tiedemann et al., "News from OPUS - A Large Collection of Parallel Corpora"** and a diachronic corpus based on a banking magazine **Saggion et al., "A Diachronic Corpus for Financial Language Understanding"**. However, these are resources are the sentence level.

\textbf{Targeted context-sensitive evaluation}

An important line of work in the document-level MT evaluation are test-suites. These are manually crafted test sets that target specific context-sensitive phenomena and the evaluation focuses on errors when translating them. There are two approaches: 1) evaluation based on references and 2) evaluation based on contrastive pairs.

The first type is inspired by the traditional approach of comparing the generated translation to a reference **Koehn et al., "Montreal Corpus of Machine Translation"**.
The use of references has the disadvantage that it only measures the agreement between the reference and hypothesis and not the internal agreement across generated sentences, i.e., whether the translation respects agreement with its previous and following sentences.

To overcome this, the contrastive pairs were proposed. For every source sentence, multiple context sentences are provided, along with a correct translation, as well as an incorrect one (or multiple incorrect ones) **Chollak et al., "Contrastive Pairs for Contextualized Machine Translation"**. The evaluation measures if the model gives a higher probability to the correct translation without actually requiring the model to generate it. The disadvantage of these test-suites is the need to access models' probabilities, which is impossible for most commercial systems, and another flaw is that even if the model gives a higher probability to the correct answer, it doesn't guarantee that the generated translation would be correct. 
To build new targeted test-suites more easily, automatic means to identify sentences that contain context-sensitive phenomena were proposed  **Bertoldi et al., "Automatic Detection of Context-Sensitive Phenomena"**.

The development of new test-suites usually requires a high level of linguistic expertise, knowledge about the studied context-sensitive phenomena and/or tools designed for the task. Therefore, test-suites are only available for some languages and well studied phenomena, which makes the evaluation limited. This makes the test-suites difficult to use as a universal method for document-level MT evaluation. This issue is usually bridged by combining a test-suite evaluation with a general quality test set evaluation. However, a more ideal approach would be being able to capture the models' capabilities to deal with context, as well as its overall translation quality. 
In the following, we present the DOLFIN test set that aims to evaluate both of these aspects.