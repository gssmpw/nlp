% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{mdframed}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcommand{\tocite}{\textcolor{red}{[addcite]}}
\newcommand{\stella}[1]{{\textcolor{magenta}{[stella: #1]}}}
\newcommand{\jimin}[1]{{\textcolor{teal}{[jimin: #1]}}}
\newcommand{\faeze}[1]{{\textcolor{brown}{[Faeze: #1]}}}
\newcommand{\yt}[1]{{\textcolor{blue}{[YT: #1]}}}
\newcommand{\maarten}[1]{{\textcolor{orange}{[maarten: #1]}}}
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
\newcommand{\pw}[1]{{\textcolor{forestgreen}{[PW: #1]}}}

\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays

\usepackage{multirow}
\usepackage{booktabs}

\usepackage{xspace}
\newcommand{\methodname}{\textsc{Alfa}\xspace}

\title{Aligning LLMs to Ask Good Questions \\A Case Study in Clinical Reasoning}

\author{
Shuyue~Stella Li$^{1\star}$\hspace{3mm} 
Jimin Mun$^{2\star}$\hspace{3mm} 
Faeze Brahman$^3$ \\ 
\textbf{Jonathan S. Ilgen}$^1$\hspace{3mm} 
\textbf{Yulia Tsvetkov}$^1$\hspace{3mm} 
\textbf{Maarten Sap}$^{2}$\\
$^1$University of Washington\hspace{5mm}
$^2$Carnegie Mellon University\hspace{5mm}
$^3$Allen Institute for AI\vspace{1mm}\\
\texttt{stelli@cs.washington.edu, jmun@andrew.cmu.edu}\\
\parbox{0.03\textwidth}{\includegraphics[width=\linewidth]{figures/github-mark.png}}\hspace{0.5mm}\href{https://github.com/stellalisy/ALFA}{\texttt{https://github.com/stellalisy/ALFA}}\\
\parbox{0.033\textwidth}{\includegraphics[width=\linewidth]{figures/hf-logo.png}}\hspace{0.5mm}\href{https://huggingface.co/datasets/stellalisy/MediQ_AskDocs_preference}{\texttt{https://huggingface.co/datasets/stellalisy/MediQ\_AskDocs}}\vspace{-3mm}
}

\begin{document}
\maketitle
\begingroup
\def\thefootnote{$\star$}
\footnotetext{Equal contribution.}
\endgroup
\begin{abstract}

Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decision-making. 
%
We present \textbf{\methodname}, a framework that improves LLM question-asking by 
(i) \emph{decomposing} the notion of a ``good'' question into a set of theory-grounded attributes (e.g., clarity, relevance), 
(ii) controllably \emph{synthesizing} attribute-specific question variations, and 
(iii) \emph{aligning} models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes.  
%
Focusing on clinical reasoning as a case study, 
we introduce the \textbf{\emph{MediQ-AskDocs}} dataset, composed of 
17k real-world clinical interactions 
augmented with 80k attribute-specific preference pairs of follow-up questions, 
as well as a \textit{novel} expert-annotated interactive healthcare QA task to evaluate question-asking abilities.
% 
Models aligned with \methodname reduce diagnostic errors by 56.6\% on \emph{MediQ-AskDocs} compared to SOTA instruction-tuned LLMs, with a question-level win-rate of 64.4\% and strong generalizability. 
%
Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains.% 
%
\footnote{We release all data, code, and models for further research.}

\end{abstract}


\section{Introduction}

Interactive language models have demonstrated remarkable capabilities across numerous domains \citep{openai2024gpt4technicalreport}, yet \emph{proactive} interaction abilities in high-stakes scenarios---clinical reasoning, legal analysis, investigative journalism---remains a challenge \citep{fung-etal-2024-agenda}.
A key obstacle is the ability of these models to recognize and anticipate missing or ambiguous information and proactively seek clarification \citep{li2024mediq, deng2024towards}.
In clinical practice, for instance, physicians systematically ask patients questions to rule out or confirm relevant diagnoses \citep{richardson1995well,proffit2013evidence}.
This iterative, information-seeking behavior is essential for accurate and safe decision-making.
Similarly, for large language models (LLMs) to serve as \emph{reliable} decision-support tools for clinicians, they must learn not only to provide answers, but also to identify when additional information is needed, and to ask follow-up questions that effectively reduce uncertainty (Figure~\ref{fig:main}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/healthq_motivation_blue.pdf}\vspace{-1mm}
    \caption{Effective information-seeking questions are crucial for clinical reasoning. \methodname-aligned models can ask better questions and lead to more accurate diagnosis.}\vspace{-5mm}
    \label{fig:main}
\end{figure}





However, there are two main challenges in building LLMs that ask good questions, especially in expert domains. 
First, defining a “good” question is inherently complex and context-dependent. 
In general, attributes such as \emph{clarity}, \emph{focus}, and \emph{answerability} are essential \citep{heritage2006communication, roterHall1987, freed1994form, searle1969speech}; however, in domain-specific scenarios such as clinical reasoning, additional properties---\emph{medical accuracy}, \emph{diagnostic relevance}, and \emph{mitigating differential diagnosis (DDX) biases}---are necessary for reducing diagnostic uncertainty \citep{richardson1995well, silverman2016skills, heritage2010questioning, hall1995doctors, west1984routine, stivers2007questioning, ong1995doctor}. 
Second, instilling the ability to ask good questions in LLMs is technically non-trivial \citep{li2024mediq, johri2025evaluation, zhang2024modeling}. Naïve prompting strategies such as “\textit{Ask a follow-up question if needed.}” may enhance interactivity but lack a principled foundation for defining a \emph{good} question. 
We propose leveraging well-established general and task-specific principles from communication theory and psychology to improve LLMs' information-seeking abilities. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/healthq.pdf}\vspace{-1.4mm}
    \caption{\methodname: decompose, synthesize, align.}\vspace{-4mm}
    \label{fig:methods}
\end{figure}


Methodologically, we introduce a general recipe to incorporate question-asking abilities into LLMs, focusing on clinical reasoning as a case study. 
Our method---\textbf{AL}ignment via \textbf{F}ine-grained \textbf{A}ttributes (\methodname)---relies on the idea that question quality can be improved by explicitly training models with data grounded in structured, theoretically motivated attributes. Our recipe proceeds in three steps:
\begin{enumerate}
[noitemsep,topsep=0pt,leftmargin=12pt]
    \item \textbf{\emph{Decompose}} the high-level goal of asking ``good'' questions into structured, grounded attributes.
    \item \textbf{\emph{Synthesize}} counterfactual data by controllably altering specific attributes (e.g.\ make \textit{clearer}).
    \item \textbf{\emph{Align}} models using preference optimization algorithms to integrate the attributes and produce a final policy. 
\end{enumerate}
Since labeled conversational datasets containing follow-up questions along a variety of important attributes are scarce, \methodname exposes models to a much broader range of question-asking behaviors than what can be typically found in the wild, especially in specialized domains like clinical interactions.


We instantiate the above recipe with a focus on clinical reasoning, where question-asking is central to reducing diagnostic uncertainty and preventing errors.
To this end, we construct a novel dataset, \textbf{\emph{MediQ-AskDocs}}, containing 17k clinical interactions with follow-up questions from the \texttt{r/AskDocs} subreddit, paired with 80k synthesized counterfactual variants of these questions highlighting each attribute.
These counterfactual pairs provide fine-grained training signals for preference learning.
Finally, we integrate the attribute-specific signals into a unified policy by combining all synthetic data into a single model, training separate reward models and merging them, or fusing attribute-specific policies.
\methodname contrasts with coarse-grained preference learning, offering a more targeted way to refine question-asking behaviors.


As part of \emph{MediQ-AskDocs}, we introduce a novel healthcare QA task of 302 expert-annotated clinical interaction scenarios to evaluate the proposed method. 
These scenarios are passed into MediQ \citep{li2024mediq}, an interactive clinical simulator, in which the models asks questions to the patient agent.
\methodname-aligned models achieve a 64.4\% win-rate in question-level evaluation and a 56.6\% reduction in diagnostic errors, relative to baselines of SOTA instruction tuned LLMs.
Beyond these empirical gains, our work presents a new paradigm for aligning language models to specialized domains by \textbf{decomposing} the complex goal of question-asking into attributes, \textbf{synthesizing} pairwise data in each attribute dimension, and \textbf{aligning} the model to jointly optimize the overall complex goal. This general approach to attribute-based question-asking alignment can be extended to many other domains where systematically eliciting information is key to reliable, effective decision-making.


\section{Problem Statement}\label{sec:statement}

Aligning LLMs to ask good question requires contextual reasoning along multiple attributes (e.g., accuracy, clarity, focus). 
% 
However, most alignment paradigms treat these goals as monolithic, aggregating preferences into a single reward that conflates attributes and obscures their individual contributions. We formalize this challenge as follows:

Given a complex goal $G$ and a dataset with sparse labels $\mathcal{D}$, we aim to learn a policy $\pi$ that maximizes the composite reward $R(s,a)$, where:
%
\begin{itemize}
[noitemsep,topsep=0pt,leftmargin=12pt]
    \item $s$: Current world state (e.g., information acquired so far, conversation history).
    \item $a$: Next action (e.g., follow-up question asked by the clinician agent $\pi$).
\end{itemize}
The key challenges lie in the complexity of the goal and the sparsity of labeled data. 
First, directly optimizing $R(s,a)$ is infeasible because human preferences for $R(s,a)$ are noisy and subjective.
To address this, \methodname decomposes $G$ into $K$ attributes $\{A_1,\ldots,A_k\}$, each corresponding to a verifiable criterion with a reward function $R_k$. 
We constrain the selection of $A_k$ such that each $R_k(s,a)$ is more measurable compared to $R(s,a)$.

Second, we cannot observe \emph{parallel} follow-up questions in natural conversations to construct preference pairs.
To this end, we synthesize counterfactual synthetic data $\mathcal{D}_{synth}^k$ for each $A_k$, where 
\vspace{-3mm}\begin{equation*}
\mathcal{D}_{synth}^k=\{(a_i^{k+},a_i^{k-})|R_k(a_i^{k+})>R_k(a_i^{k-})\}.\vspace{-1mm}
\end{equation*}

Lastly, \methodname uses a reward integration strategy $f$, where $R(s,a) = f(R_1(s,a),\ldots,R_K(s,a))$, 
to combine $\{R_1,\ldots,R_K\}$ into a policy $\pi$, such that:
\vspace{-2mm}\begin{equation*}
\vspace{-1mm}\pi^*=\arg\max_{\pi}\mathbb{E}_{(s,a)\sim\pi}[f(R_1,\ldots,R_K)(s,a)],\vspace{-2mm}
\end{equation*}
optimizes performance on the complex goal $G$, aligning models to be better question-askers.




\section{What Makes a ``Good'' Question?}\label{sec:good_question}

To systematically improve LLM question asking, we define six key attributes grounded in cognitive science, psychology, and clinical communication research \citep{heritage2006communication, roterHall1987, chouinard2007, freed1994form, searle1969speech, levinson2012interrogative}. Unlike prior work that relies on implicit heuristics, we explicitly decompose question quality into interpretable, tangible attributes that enhance clinical reasoning.




\paragraph{General Question-Quality Attributes.}  
Effective questions must be clear, targeted, and answerable to drive meaningful interactions. We select three core attributes:

\begin{itemize}
[noitemsep,topsep=0pt,leftmargin=12pt]
    \item \emph{Clarity}: Avoids ambiguity and unnecessary complexity, ensuring precise communication \citep{heritage2006communication, roterHall1987}.
    \item \emph{Focus}: Directly addresses a specific information gap, yielding more informative responses \citep{ronfard2018question, gopnik2012reconstructing, chouinard2007, freed1994form}. E.g., ``Do you have a family history of breast cancer?'' is superior to ``Has anyone in your family been sick?''  
    \item \emph{Answerability}: Adapts to the respondent’s expertise (e.g., no jargans) \citep{levinson2012interrogative, keil2008discerning, searle1969speech, zhu2025askpatientspatienceenabling}.
\end{itemize}





\paragraph{Domain-specific question-asking attributes.} 
In clinical reasoning, question-asking is a structured diagnostic skill. Drawing from clinical communication research \citep{richardson1995well, silverman2016skills, heritage2010questioning, hall1995doctors, west1984routine, stivers2007questioning, ong1995doctor, proffit2013evidence}, we define three additional attributes essential for clinical decision-making:


\begin{itemize}
[noitemsep,topsep=0pt,leftmargin=12pt]
    \item \emph{Medical Accuracy}: Aligns with established medical knowledge \& guidelines.
    \item \emph{Diagnostic Relevance}: Probes for symptoms, risk factors, or contextual details essential to refining differential diagnoses (DDX).  
    \item \emph{Avoiding DDX Bias}: Prevents suggestive or leading wording that could introduce cognitive biases and misguide diagnostic reasoning.  
\end{itemize}


These six attributes form the foundation of \methodname, guiding question optimization to improve LLM reliability in interactive clinical reasoning.


\section{\methodname Framework Overview}\label{sec:methods}

We now introduce \methodname, a structured recipe that
decomposes the overall question-asking objective (\S\ref{sec:methods:attributes}), 
generates attribute-specific preference data (\S\ref{sec:methods:synthetic_data}), 
and trains a policy that integrates the attributes (\S\ref{sec:methods:integration}) to ask better follow-up questions.


\subsection{Grounded Attribute Decomposition}\label{sec:methods:attributes}

We first decompose the concept of ``good'' clinical questions into the six attributes $A_k$ identified in \S\ref{sec:good_question} rather than relying on implicit heuristics or coarse scoring.
This decomposition enables two advantages: (1) \emph{attribute-specific training signals} that isolate distinct aspects of question quality, and (2) a \emph{controlled preference structure} for fine-grained alignment, guiding the next stages of data generation and model alignment.


\subsection{Attribute-Specific Data Generation}\label{sec:methods:synthetic_data}


Real-world clinical datasets are scarce, private, and rarely contain annotations distinguishing, for instance, \textit{clear} vs.\ \textit{ambiguous} questions \citep{mireshghallah2023privacypreservingdomainadaptationsemantic,ramesh2024evaluatingdifferentiallyprivatesynthetic}. 
Therefore, we generate \emph{synthetic preference data} by (1) collecting authentic clinical posts (see \S\ref{sec:experiments:evaluation} for dataset curation details) and (2)~using an LLM to generate counterfactual variants along each attribute.

\paragraph{Counterfactual Perturbation.} For each question $a_i$ in the dataset, 
we prompt an LLM to create ``enhanced'' and ``corrupted'' variants $(a_i^{k+},a_i^{k-})$ that explicitly alter only one attribute $k$ at a time (e.g., rewriting the original question to be more clear/ambiguous) while keeping others consistent. 
This enables us to create controlled preference pairs, where one version of a question $a_i^{k+}$ is better aligned with a specific attribute than the other, $a_i^{k-}$.

\paragraph{Verification \& Filtering.} 
We use an \textbf{LLM-judge} to verify that the generated perturbations reflect their intended modification. 
With original question $a_i$, we obtain an enhanced question $a_i^{k+}$ and a corrupted question $a_i^{k-}$, resulting in three preference pairs: $(a_i^{k+},a_i)$, $(a_i^{k+},a_i^{k-})$, and $(a_i,a_i^{k-})$.
Given each pair, we provide additional context\footnote{Parsed conversation conclusions from future turns.} to the LLM-judge, and ask the judge to compare the pairs in the specified attribute dimension (e.g., which question is \textit{clearer}).
If the judge’s decision matches the intended perturbation direction---verifying that $R_k(a_i^{k+})>R_k(a_i^{k-})$---we retain the sample; otherwise, we discard it. 
This filtering step removes inconsistencies and ensures that our synthetic preference data provides reliable supervision for model alignment. See details in Appendix~\ref{app:implementation:filter}.



\subsection{Attribute Integration Strategies}\label{sec:methods:integration}

We now \emph{combine} signals from the attribute-specific data so that the model produces questions that optimize for all attributes. Standard preference optimization algorithms DPO and PPO allow distinct \emph{Points of Integration} (POI) as described below\footnote{While reward sum, which trains attribute-specific reward models then combines the reward scores (e.g., via a learned linear combination or average) to produce a final scalar reward \citep{wu2023fine, wang2024helpsteer2}, remains an option, we do not utilize this strategy due to high compute cost of loading all reward models during PPO without substantial performance improvements \citep{rame2024rewarded,shi2024decoding}.}:

\noindent
\textbf{(1) Data Mixing} pools all synthetic data into one training set and uses standard DPO/PPO. This treats each attribute-specific comparison as part of a larger set of ``better vs.\ worse'' question pairs, enabling a single policy to learn from all attributes at once \citep{wang2023far, lambert2024t}

\noindent
\textbf{(2) Reward Fusion} trains separate reward models, then averages the reward model \emph{weights} in an overall reward model which can then be used in PPO to align a final policy \citep{rame2024warm}.

\noindent
\textbf{(3) Policy Fusion} trains separate policies or preference models, each specialized for one attribute, and then combine the model weights by averaging or taking a linear combination \citep{jang2023personalized}. This strategy offers the highest degree of parallelism and maximally preserves attribute strengths and interpretability. 

Comparing these integration strategies informs how best to reconcile multiple, sometimes competing, objectives (e.g., \emph{focus} vs.\ \emph{avoiding DDX bias}) and thereby produce consistently high-quality diagnostic questioning. 








\section{Experimental Setup}

\subsection{\emph{MediQ-AskDocs} Dataset Curation}\label{sec:exp:dataset}
We obtain data from \texttt{r/AskDocs}, a public online health forum, and filter for conversation threads where (i) the patient posts a health inquiry and engage with another user to discuss the issue, and (ii) another user \emph{asks follow-up questions} to acquire more information. The resulting dataset contains 13,496 unique posts, 17,425 threads, and 24,263 questions. For creating the counterfactual perturbations (\S\ref{sec:methods:synthetic_data}), we sample 4463/433/620 questions for the train/dev/test splits. More details on dataset curation and sampling are in Appendix~\ref{app:dataset}.

\subsection{Experiments}
With the goal of instilling question-asking ability in LLMs in the clinical reasoning domain, we structure our experiments to progressively address two key questions: \emph{(1)~Does the entire \methodname pipeline improve clinical question-asking performance?} and \emph{(2)~Necessity of each component of \methodname and to what extent do they contribute to the performance?} Accordingly, we organize our evaluation into three parts:

\noindent
\textbf{1. Overall Performance.} We first confirm that \methodname meaningfully reduces diagnostic errors and elicits better questions in an interactive clinical scenario (\S\ref{sec:results:main}).

\noindent
\textbf{2. Key Pipeline Components.} We isolate the two core components of \methodname: 
\begin{itemize}
[noitemsep,topsep=0pt,leftmargin=12pt]
\item We compare the \emph{decomposed} attributes (clarity, medical accuracy, etc.) to a “coarse” attribute (simply “good or bad”) to highlight the necessity of a theory-grounded decomposition (\S\ref{sec:results:coarse}).
\item We compare preference tuning to supervised fine-tuning (SFT) on \emph{the same} synthetic data, revealing how pairwise reward signals refine question-asking beyond what SFT alone can achieve (\S\ref{sec:results:sft_only}). 
\end{itemize}

\noindent
\textbf{3. Ablation Studies.} We dissect each design choice to identify its role in the observed improvements. Specifically, we 
compare \emph{attribute-integration strategies} (\S\ref{sec:results:integration_strategies}), 
apply \emph{quality filter} on the synthetic data (\S\ref{sec:results:filter}), 
examine \emph{data perturbation directions} (corruption vs.\ enhancement) (\S\ref{sec:results:data_direction}), 
ablate \emph{individual attributes} (\S\ref{sec:results:attributes}), 
and test \emph{out-of-distribution generalization} (\S\ref{sec:results:medqa}) on a separate medical reasoning task.\footnote{All ablations are done with DPO due to its lighter compute requirements unless otherwise specified.} 


Collectively, these analyses elucidate how components in \methodname---attribute decomposition, data synthesis, preference tuning---work together to improve question-asking in high-stakes clinical contexts. See implementation details in Appendix~\ref{app:implementation}.

\subsection{Evaluation}\label{sec:experiments:evaluation}

We evaluate the aligned models along two fronts: 
\textbf{Direct Question Quality}, measured through expert human annotations and automatic LLM-judge comparison for the overall question quality, and
\textbf{Clinical Decision Impact}, 
quantified by how well the model questions help reduce diagnostic errors. 
\paragraph{Direct win-rate with LLM-judge.}

We create an LLM-judge to compare pairs of questions, adopting prompt structures from \citet{alpaca_eval} and \citet{dubois2023alpacafarm}.
Specifically, we measure the percentage of times our aligned models’ questions are preferred over those of the baseline instruction-tuned models (llama-3.2-3b-Instruct and llama-3.1-8b-Instruct) by to the judge and report win-rate.
Each comparison is carried out by \texttt{gpt-4o} with rationales, permuting the order of the questions in each pair to mitigate ordering bias. As an sanity check, we find that the LLM-judge assigns a higher win-rate to questions crafted by verified experts than to those written by non-experts, 
suggesting that our evaluation aligns with domain expertise.

\noindent
\textbf{Expert manual evaluation.}\hspace{2mm}
To assess the quality of generated questions and further validate the LLM-judge, we collect preference rankings from \emph{medical experts} and compute win-rate of select models. See Appendix~\ref{app:manual_eval} for further details.

\paragraph{Accuracy on interactive diagnostic task.}
We use the MediQ interactive framework \citep{li2024mediq}---patient-clinician simulator---to holistically evaluate \methodname in a more realistic setting. MediQ presents some initial information $x^0$ (often the patient's chief complaint), a medical inquiry $\kappa$, and tests an expert agent's ability to ask follow-up questions $a$ to the patient until it has enough information to make a diagnosis $y$. We replace the question generator module in the MediQ expert agent with models trained with \methodname, while keeping all other modules (patient system and diagnosis generator) consistent. We quantify the utility of the question generator with the accuracy of the diagnosis $y$.

\noindent
\textbf{\emph{MediQ-AskDocs} Task.}\hspace{2mm}
MediQ is compatible with any QA task with contextual information. 
We introduce \textbf{a novel healthcare QA task} as part of the \emph{MediQ-AskDocs} dataset:
302 consumer healthcare multiple choice questions manually annotated by medical experts.
The task is automatically generated by \texttt{o1} using the test split of \emph{MediQ-AskDocs} (\S\ref{sec:exp:dataset}),
and annotated by two human experts with 88.6\% agreement. See Appendix~\ref{app:mcq_generation} for task construction and annotation details.
Additionally, we use MedQA \citep{jin2020disease} with MediQ to examine the models' ability to generalize to out-of-domain tasks.


\section{Results \& Analysis}
The results section first demonstrates the superior performance of \methodname (\S\ref{sec:results:main}). 
Then we investigate how the core modules---\textbf{attribute decomposition} (\S\ref{sec:results:coarse}) and
\textbf{preference tuning} (\S\ref{sec:results:sft_only})---contributes to its effectiveness. 
Additionally, we ablate on \textbf{attribute integration strategies} (\S\ref{sec:results:integration_strategies}), 
\textbf{synthetic data quality} (\S\ref{sec:results:filter}),
\textbf{direction of synthetic data} (corruption vs. enhancement) (\S\ref{sec:results:data_direction}), 
\textbf{attribute-specific influences} (\S\ref{sec:results:attributes}), 
and the \textbf{out-of-distribution generalization} (\S\ref{sec:results:medqa}) 
of \methodname.


\subsection{\methodname Improves Overall Question Asking}\label{sec:results:main}

We begin by comparing \methodname with two baselines: (1) the base models (\texttt{llama-3.2-3B-Instruct} and \texttt{llama-3.1-8B-Instruct}) and (2) models trained via supervised fine-tuning (SFT) on the human-written questions. The Policy Fusion attribute integration strategy (\S\ref{sec:methods:integration}) is reported for both \methodname-DPO and \methodname-PPO in Table~\ref{tab:results:main} below.


\begin{table}[b!]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{lccc}
        \toprule 
        \textbf{Model} & \textbf{Size} & \textbf{Win-rate} & \textbf{MediQ-AD} \\
        \midrule
\multirow{2}{*}{Base Model}      & 3B & 50.00 & 73.51  \\
                                 & 8B & 50.00 & 72.52  \\ \midrule
\multirow{2}{*}{SFT}             & 3B & 61.04 & 78.98  \\
                                 & 8B & 58.23 & 85.08  \\ \midrule
\multirow{2}{*}{\methodname-DPO} & 3B & \textbf{64.97} & \textbf{87.75}  \\
                                 & 8B & \textbf{65.13} & \textbf{88.08}  \\ \midrule
\methodname-PPO                  & 3B & 64.84 & 86.75 \\
        \bottomrule
    \end{tabular}
    }%\vspace{-3mm}
    \caption{Main results. \methodname models consistently outperform base instruct models and SFT models.}
    \label{tab:results:main}
\end{table}

We first confirm that fine-tuning the human-written questions in \emph{MediQ-AskDocs} (SFT) already outperforms base models, establishing usefulness of the dataset. Notably, performance of the \methodname-aligned models shows \emph{explicitly modeling structured, theory-grounded attributes substantially boosts both question quality and diagnostic accuracy}, reducing diagnostic errors by 56.62\%.




\subsection{Fine-Grained Attributes Outperform Coarse Attribute}\label{sec:results:coarse}

The core idea of \methodname is to decompose a complex goal into structured, theory-grounded attributes rather than treating it as one coarse objective. 
We examine the role of attribute decomposition by comparing alignmend with \textbf{fine-grained} attributes (\methodname) with a simpler approach that optimizes models on a \textbf{coarse} ``better'' vs.\ ``worse'' distinction. Both models undergo the same counterfactual data generation and alignment processes, but the coarse model lacks explicit attribute separation.

\begin{table}[t!]
    \centering%\vspace{-1mm}
    \scalebox{0.9}{
    \begin{tabular}{lcccc}
        \toprule 
        \textbf{Model} & \textbf{Size} & \textbf{Win-rate} & \textbf{MediQ-AD} \\
        \midrule
\multirow{2}{*}{Coarse} & 3B & \textbf{65.89} & 83.77  \\
                        & 8B & \textbf{66.05} & 85.76  \\ \midrule
\methodname             & 3B & 64.97 & \textbf{87.75}  \\
(Fine-Grained)          & 8B & 65.13 & \textbf{88.08}  \\
        \bottomrule
    \end{tabular}
    }\vspace{-2mm}
    \caption{Fine-grained (\methodname) vs. Coarse Attributes. Fine-grained attributes lead to better downstream diagnostic accuracy and similar win-rates compared to coarse-grained objective. Expert evaluation for 3B coarse vs.\ fine-grained shows identical win-rate: \textbf{59.4\%}.}
    \label{tab:results:coarse}\vspace{-2mm}
\end{table}


Table~\ref{tab:results:coarse} shows that \methodname, trained with fine-grained attributes, achieves higher performance compared to models trained on coarse attributes on MediQ diagnostic accuracy, while the two methods show comparable LLM-judge win-rates and identical expert evaluation win-rates. 
Aligning models based on structured attributes guides the reasoning process in a theory-grounded way, resulting in substantially better downstream performance.
Additionally, qualitative analysis (Figure \ref{fig:coarse_example}) shows that the coarse-aligned model tends to ask more superficial questions such as waiting for 8 weeks and check back, while the \methodname-model's question, "\textit{Was this the only issue that was found?}", indicates some form of reasoning to rule out other factors that might have contributed to the patient's concern.
Further, as we show in \S\ref{sec:results:medqa}, coarsely aligned models generalize poorly to out-of-distribution tasks.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/coarse_example.pdf}\vspace{-2.5mm}
    \caption{Quanlitative example contrasting models aligned with fine-grained and coarse attributes. \methodname model tends to ask more logical questions.}\vspace{-2mm}
    \label{fig:coarse_example}
\end{figure}










\subsection{Preference Tuning Outperforms SFT}\label{sec:results:sft_only}
Another core advantage of \methodname is its use of pairwise preference learning, which allows models to refine question-asking beyond what supervised fine-tuning (SFT) achieves. 
To test whether {\methodname}’s improvements come solely from exposure to diverse synthetic data or from learning structured preferences, we compare \methodname with models fine-tuned on real data (SFT-Real), synthetic enhanced data (SFT-Synthetic), and both (SFT-Combined).

As shown in Table~\ref{tab:results:sft_only}, \methodname outperforms SFT-Combined despite learning from the same data, confirming that learning \textbf{directional differences} from pairwise comparisons is key to better question-asking. 
These results highlight pairwise contrastive optimization as a necessary step for models to learn how to ask better follow-up questions.

\begin{table}[t!]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{lccc}
        \toprule 
        \textbf{Model} & \textbf{Size} & \textbf{Win-rate} & \textbf{MediQ-AD} \\
        \midrule
\multirow{2}{*}{SFT-Real}      & 3B & 61.04 & 78.98  \\
                               & 8B & 58.23 & 85.08  \\ \midrule
\multirow{2}{*}{SFT-Synthetic} & 3B & 62.50 & 82.12  \\
                               & 8B & 60.32 & 85.76  \\ \midrule
\multirow{2}{*}{SFT-Combined}  & 3B & 62.50 & 83.77  \\
                               & 8B & 54.68 & 85.76  \\ \midrule
\multirow{2}{*}{\methodname}   & 3B & \textbf{64.97} & \textbf{87.75}  \\
                               & 8B & \textbf{65.13} & \textbf{88.08}  \\
        \bottomrule
    \end{tabular}
    }\vspace{-2.5mm}
    \caption{Preference tuning is crucial in improving model performance. Supervised fine-tuning on the same synthetic data does not show as much performance gain.}
    \label{tab:results:sft_only}\vspace{-3mm}
\end{table}












\subsection{When to integrate the attributes?}\label{sec:results:integration_strategies}
 
We now examine the effect of various attribute integration strategies from \S\ref{sec:methods:integration}: \textbf{data}-mixing, \textbf{reward}-fusion (PPO only), and \textbf{policy}-fusion. 
Table~\ref{tab:results:integration_strategies} reveals three key findings.
First, data-mixing achieves higher question win-rate but yields lower diagnostic accuracy than policy-fusion, and reward-fusion in PPO mirrors data-mixing patterns. This suggests that \emph{greater attribute separation leads to improved diagnostic accuracy}, consistent with prior observations in \citet{yang2025mixdatamergemodels}.
Second, The LLM-judge scores strongly correlate with human expert assessments ($r=0.9893, p<0.005$), establishing the LLM-judge as a reliable proxy for human assessment. The high LLM-judge score of \methodname-PPO-Reward is echoed in the human evaluation with a score of 86.3\%.

Third, despite high question quality (win-rate), diagnostic accuracy remains suboptimal due to two systemic factors:
(i) Patient records derived from \texttt{r/AskDocs} contain only information elicited through human questions. When \methodname-aligned models generate superior questions than that of humans requesting any \emph{other useful information}, the patient agent lacks corresponding answers. 
(ii) The final diagnosis generator achieves 86.75\% accuracy given complete information, setting an upper bound. This explains why \methodname's improvements in information-seeking (win-rate) only partially translate to final accuracy.
Additionally, performance higher than 86.75\% implies that \methodname-aligned models demonstrate emergent reasoning abilities beyond mere data retrieval, producing questions that actively guide clinical reasoning.


\begin{table}[t!]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{ccccc}
        \toprule 
        \textbf{*PO} & \textbf{POI} & \textbf{Size} & \textbf{Win-rate} & \textbf{MediQ-AD} \\
        \midrule
\multirow{4}{*}{DPO} & \multirow{2}{*}{Data}   & 3B & 68.55 & 85.01  \\
                     &                         & 8B & \textbf{68.23} & \textbf{88.74}  \\ \cmidrule{2-5}
                     & \multirow{2}{*}{Policy} & 3B & 64.97 & \textbf{87.75}  \\
                     &                         & 8B & 65.13 & 88.08  \\ \midrule
\multirow{3}{*}{PPO} & Data                    & 3B & 97.34 & 84.77  \\
                     & Reward                  & 3B & \textbf{97.98} & 84.44  \\
                     & Policy                  & 3B & 64.84 & 86.75  \\
        \bottomrule
    \end{tabular}
    }\vspace{-2mm}
    \caption{Attribute integration strategies.}
    \label{tab:results:integration_strategies}\vspace{-2mm}
\end{table}




\subsection{Gains from Synthetic Data Quality}\label{sec:results:filter}

\methodname relies on synthetic question perturbations to expose models to counterfactual scenarios. To ensure data quality, we filtered out 13.9\% of generated pairs where LLM-judge ratings misalign with intended perturbation directions (\S\ref{sec:methods:synthetic_data}). Filtering slightly improves both question quality and diagnostic accuracy, emphasizing the value of high-quality synthetic data (Table~\ref{tab:results:filter}).


\begin{table}[h!]
    \centering%\vspace{-2mm}
    \scalebox{0.9}{
    \begin{tabular}{lccc}
        \toprule 
        \textbf{Model} & \textbf{Size} & \textbf{Win-rate} & \textbf{MediQ-AD} \\
        \midrule
\multirow{2}{*}{\methodname-Unfiltered} & 3B & 64.19 & 85.43  \\
                                        & 8B & 63.31 & 86.09  \\ \midrule
\multirow{2}{*}{\methodname}            & 3B & \textbf{64.97} & \textbf{87.75}  \\
                                        & 8B & \textbf{65.13} & \textbf{88.08}  \\ 
        \bottomrule
    \end{tabular}
    }\vspace{-2mm}
    \caption{Synthetic data quality. Filtering slightly improves diagnostic accuracy.}
    \label{tab:results:filter}\vspace{-2mm}
\end{table}




\subsection{Synthetic Corruption vs. Enhancement}\label{sec:results:data_direction}



\begin{figure}[h!]
    \centering%\vspace{-3mm}
    \includegraphics[width=\linewidth]{figures/enhance_corrupt_line.pdf}\vspace{-3mm}
    \caption{All the synthetic data directions are helpful. Including corruptions, enhancements, and the original data shows the best performance.}
    \label{fig:results:data_direction}\vspace{-3mm}
\end{figure}

In the counterfactual pairwise data generation stage of \methodname, each original sample $a$ is synthesized in two directions along each attribute dimension (e.g.\ \textit{more relevant} and \textit{less relevant}) to get $a^+$ and $a^-$. 
In this section, we examine how the generation direction---\emph{enhanced} (``more X'') vs. \emph{corrupted} (``less X'')---influence performance.
Specifically, we compare models trained with \textbf{corruption} only pairs $(a,a^-)$, %(\texttt{original}--\texttt{corrupted}), 
\textbf{enhancement} only pairs  $(a^+,a)$, %(\texttt{enhanced}--\texttt{original}), 
\textbf{synthetic} corruption and enhancement pairs $(a^+,a^-)$, %(\texttt{enhanced}--\texttt{corrupted}), 
and \textbf{all} of the above.
In Figure~\ref{fig:results:data_direction}, we find while any generation direction is beneficial, combining all three pairs brings the most gains, especially apparent in the smaller 3B model. 


\subsection{Attribute-Specific Influences}\label{sec:results:attributes}
A key component of \methodname is the explicit decomposition of question-asking into fine-grained, theory-grounded attributes. 
To assess their individual contributions, we conduct an ablation study where we remove one attribute at a time and evaluate the model's performance. 
We also compare general question-asking attributes (clarity, focus, answerability) with clinical attributes (medical accuracy, diagnostic relevance, avoiding DDX bias).

\begin{table}[b!]
    \centering\vspace{-2mm}
    \scalebox{0.9}{
    \begin{tabular}{lccc}
        \toprule 
        \textbf{Attribute} & \textbf{Win-rate} & \textbf{MediQ-AD} \\
        \midrule
No Accuracy      & 63.95 & 84.11  \\
No Answerability & 64.60 & 84.11  \\
No Avoid DDX Bias& 62.58 & 81.79  \\
No Clarity       & 62.74 & 85.10  \\
No Focus         & 63.95 & 84.44  \\
No Relevance     & 63.39 & 84.44  \\ \midrule
General          & 62.90 & 81.79  \\
Clinical         & 66.05 & 86.09  \\ \midrule
All              & \textbf{64.97} & \textbf{87.75}  \\
        \bottomrule
    \end{tabular}}\vspace{-2mm}
    \caption{Policy Fusion DPO models on attribute groups.}
    \label{tab:results:attributes}\vspace{-1mm}
\end{table}


\begin{figure}[t!]
    \centering\vspace{1mm}
    \includegraphics[width=\linewidth]{figures/general_clinical.pdf}\vspace{-3mm}
    \caption{Models aligned with general vs.\ clinical attributes show distinct behaviors. \methodname-General asks clear and focused, but less relevant questions and contains DDX bias ("\textit{mono}"); \methodname-Clinical uses medical terms hindering answerability but is more professional.}
    \label{fig:qualitative}\vspace{-3mm}
\end{figure}

Table~\ref{tab:results:attributes} shows that removing any attribute leads to performance drops, confirming their importance in clinical question-asking. Clinical attributes have a stronger impact on MediQ accuracy, with avoiding DDX bias being the most critical—suggesting models need explicit training to counteract cognitive biases. 
Qualitatively, the questions generated by models aligned with general attributes vs. clinical domain-specific attributes show distinct styles (Figure~\ref{fig:qualitative}), highlighting the impact of feature selection. 
These results validate \textsc{Alfa}’s structured attribute alignment, demonstrating that both domain-specific capabilities and general question quality contribute to effective clinical decision-making.


\subsection{\methodname Models Robustly Generalize to Out-of-Distribution Settings}\label{sec:results:medqa}

We further assess \methodname's ability to generalize beyond the \emph{MediQ-AskDocs} task by evaluating on a more challenging clinical reasoning benchmark \emph{unseen} during training, MedQA \citep{jin2021disease}, in the same MediQ-style interactive setting. 
Across intergration strategies, \methodname-aligned models generally outperform or match the coarse-alignment baseline when moving to MedQA (Figure~\ref{fig:results:medqa}). These suggests that learning from structured, theory-grounded attributes can enhance an LLM’s robustness in new and more diverse clinical settings, highlighting \methodname's potential for broader applicability in real-world clinical scenarios.

\section{Related Work}


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/mediq_medqa.pdf}\vspace{-3mm}
    \caption{3B Model performance on the interactive MediQ-MedQA task. Models aligned with \methodname are more robust to out-of-distribution data.}
    \label{fig:results:medqa}\vspace{-3mm}
\end{figure}

\paragraph{Clinical Reasoning in LLMs. }

LLMs have the potential to significantly transform medicine by enhancing personalized care and accessibility \citep{shanmugam2024generative}. 
Models trained with medical data contain rich medical knowledge \citep{singhal2025toward,lewis-etal-2020-pretrained,chen2023meditron,labrak2024biomistral,singhal2023large,brin2023comparing}, but are limited in instruction-following and multi-hop reasoning \cite{hager2024evaluation, arroyo2024openclinicalllmssensitive,nov2023putting,zhang2014understanding}.
These models excel in static, medical QA benchmarks \citep{jin2020disease,pal2022medmcqa},
but recent work has moved away from the static single-turn paradigm and highlight \emph{proactive question-asking} as a key capability to reliable and effective clinical reasoning \citep{li2024mediq, hu2024uncertainty}. Our work furthers this direction by providing an evaluation benchmark with real data and the first method to train specialized question-asking models.


Our proposed method relies on synthetic counterfactual data generation \citep{mishra2024llm, ding2024data, park-etal-2024-valuescope} and fine-grained alignment. 
Inspired by prior work on multi-objcetive RLHF \citep{zhou2023beyond,wu2023fine}, we extend PPO \citep{ouyang2022training,christiano2017deep} and DPO \citep{rafailov2023direct} to align models with attribute-specific datasets, and uniquely compare different integration points of the fine-grained preference signals \citep{rame2024rewarded,chronopoulou2023adaptersoup,wang2024interpretable}. 


\section{Conclusion}
Effective question-asking is a fundamental yet underdeveloped capability in large language models, particularly in high-stakes domains like clinical reasoning.
We proposed \methodname, a framework that explicitly teaches models to ask better questions by decomposing question quality into theory-grounded, fine-grained attributes and aligning them through preference-based optimization,
rather than treating such nuanced and complex goal as a monolithic objective.
% Unlike existing approaches, ALFA instills structured inquiry as a learnable skill, improving diagnostic accuracy and reducing errors in clinical reasoning.
We introduced \emph{MediQ-AskDocs}, a comprehensive dataset of training data, preference data, and a healthcare QA task, showing that models trained with \methodname substantially outperform baselines. 
While focused on medicine as a case study, \methodname is a general recipe adaptable to any field where clear, targeted questioning is essential, paving the way for interactive and reliable systems.




\clearpage\newpage
\section*{Limitations}

\noindent\textbf{Manual attribute selection.}
\methodname requires manual selection of attributes when adapting to new expert domains. While it offers a structured framework, determining which attributes are essential still depends on domain expertise. However, \methodname can also help evaluate attribute necessity across different fields.

\noindent\textbf{LLM dependence for counterfactual generation.}
The counterfactual perturbation step relies on LLMs to generate and evaluate counterfactual question variants (specifically, \texttt{meta-llama/Llama-3.1-405B-Instruct-FP8}), assuming they correctly interpret attributes like clarity and relevance. While these attributes are already more concrete than an overall ``good question'' metric, misinterpretations and evaluation biases remain possible. Future work should incorporate human verification of counterfactuals and attribute-level rankings.

\noindent\textbf{Subjectivity in human annotation.}
Evaluating follow-up questions is inherently subjective and scenario-dependent. Some annotators expressed difficulty in ranking questions, stating that ``none of the questions were good'' or that ``all of them were acceptable.'' Furthermore, we could not require annotators to provide medical certifications, making expertise validation challenging. To mitigate this, we implemented a four-question screening test with known ground-truth answers, filtering out annotators who failed to meet a predefined accuracy threshold. However, this approach does not fully eliminate the risk of variability in domain expertise.

\noindent\textbf{Data and scope.}
Our dataset is derived from online health forum discussions (\texttt{r/AskDocs}) rather than in-person clinician-patient dialogues in a hospital setting. 
While this source provides diverse real-world medical inquiries, it does not fully capture the structured questioning strategies used in professional clinical settings. 
Thus, while \methodname offers a strong technical foundation for studying medical question-asking, it should not be viewed as a direct replacement for physician training or real clinical interactions. Expanding to EHR-based or in-hospital dialogue datasets would improve clinical applicability.

\section*{Ethics Statement}

\methodname aims to improve LLM-driven question-asking in clinical reasoning, but its development and potential deployment pose potential ethical risks related to misinformation, bias, privacy, and regulatory compliance.

A primary concern is misinformation and overreliance on AI-generated questions. While \methodname improves question quality, it does not provide any sense of guarantee on factuality. If used without human oversight, it could generate misleading, irrelevant, or overly confident questions, potentially influencing clinical decision-making and leading to misdiagnosis or unnecessary medical interventions. 

Bias in training data is another key risk. \methodname is trained on online health forum data (\texttt{r/AskDocs}), which may not reflect diverse patient populations, medical conditions, or expert questioning strategies. This could result in systematic biases, where the model generates better questions for certain demographic groups while failing others, reinforcing healthcare disparities. Future work should expand datasets to include diverse patient interactions and implement bias detection and mitigation strategies.

The medical experts recruited as annotators for the task curation and output evaluation are all based in the U.S. and are English-speakers. This not only lead to potential biases in their response, but also assumes US-based medical education background, limiting the generalizability of our results to other countries and regions. The reliance on LLM-judges to score question quality introduces potential automation bias, where models may reinforce subtle inaccuracies or stylistic preferences that diverge from expert human reasoning.

Privacy and data security are additional risks. Although \methodname does not process private medical records, future adaptations using clinical data or electronic health records (EHRs) must protect sensitive patient information. Transparent data governance frameworks and strict access controls are necessary for responsible use in healthcare applications.

\methodname is intended as a technical contribution to the field of computer science rather than a standalone clinical tool.
Even after more comprehensive testing and development, the framework must be integrated with human-in-the-loop supervision, where clinicians retain final decision-making authority. Future work should explore uncertainty calibration, ethical safeguards, and regulatory alignment to ensure safe, fair, and reliable AI-assisted clinical reasoning.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Extended Related Works}

\paragraph{Clinical LLMs.}
LLMs have potential to highly impact medicine \citep{moor2023foundation,thirunavukarasu2023large} from personalizing care to improving accessibility \citep{rodriguez2024leveraging,august2023paper}. Thus, many language models have focused on clinical knowledge and usage including closed-sourced Med-PaLM 2 \citep{singhal2025toward}  to open models such as BioGPT \citep{lewis-etal-2020-pretrained}, Meditron \citep{chen2023meditron}, and BioMistral \citep{labrak2024biomistral} to name a few \citep{toma2023clinical,zhou2023survey}. More recently, many non-medical, general purpose models such as OpenAI's o1 have outperformed medically adapted models \citep{xie2024preliminary,jeong2024limited}. These models have shown human-level performance on MedQA \citep{jin2020disease} and other medical knowledge benchmarks \citep{singhal2023large} and some have even shown to provide human-level soft skill such as empathy \citep{brin2023comparing}.

\paragraph{Clinical Reasoning and Question-asking of LLMs.}
However, reasoning abilities of these systems, especially under complex, high-stakes demands of medical interaction fulfilling various intentions of users \citep{nov2023putting,zhang2014understanding} require further attention. More specifically, clinical reasoning requires ability to ask effective questions \citep{silverman2016skills}, crucial for information gathering phase with iterative hypotheses testing and updating. \citet{shaikh2024grounding} highlighted general lack of question-asking by LLMs in various contexts. While prior works have focused on improving question-asking of LLMs \citep{andukuri2024star,rao-daume-iii-2018-learning} with some focused diagnostic conversations, these works have been limited to rule-based, toy scenarios \citep{hu2024uncertainty}, or prompting-based techniques \citep{li2024mediq}. Thus, our work expand on such prior works towards a more flexible medical dialogue system with effective question-asking under various real-world user queries. 

\paragraph{Alignment Methods.}
Methodologically, our work adopts various alignment algorithms, especially PPO \citep{ouyang2022training,christiano2017deep} and DPO \citep{rafailov2023direct}. Moreover, to integrate complex and nuanced preferences, multi-objective settings have been explored including MODPO \citep{zhou2023beyond} and MORLHF \citep{wu2023fine,bai2022training}. Additionally various works have highlighted efficient methods to integrate multiple objectives, for example, through combining reward model or adapter weights \citep{rame2024rewarded,chronopoulou2023adaptersoup,rame2024warm}.

\paragraph{Synthetic Data.} 
With the growing capabilities of LLMs and to supplement human data, which can be sparse, synthetic data generation with LLMs have become a popular method \citep{long2024llms,xu2024wizardlm}, especially to perform task-specific post-training \citep{kim2024evaluating}. Synthetic data generation is especially appealing in healthcare domain as privacy issues can make data access prohibitive \citep{ramesh2024evaluatingdifferentiallyprivatesynthetic,xin2024false,murtaza2023synthetic}. Thus, synthetic data generation through both rule-based methods \citep{fansi2022ddxplus} and LLMs \citep{oh2024use,mishra2024synfac,yao2024mcqg,wang-etal-2024-notechat} have been explored for various medical tasks. However, data to investigate question quality, especially in medicine, remains under-explored and our data generation method addresses this gap. 

\paragraph{Evaluation Frameworks.}
To assess LLMs in various medical tasks, many different evaluation frameworks have been developed, typically consisting of static, single-turn question-answering task based on multiple choice questions \citep{jin2020disease,pal2022medmcqa,jin2019pubmedqadatasetbiomedicalresearch,rawat-etal-2024-diversitymedqa}. 
However, with the advancement of LLMs, there has been a growing need to evaluate LLM agents beyond simple demonstration of knowledge \citep{thirunavukarasu2023large}. 
MediQ \citep{li2024mediq} proposes to evaluate LLMs' information-seeking ability through interactive clinical reasoning tasks, and constructs a benchmark based on MedQA \citep{jin2020disease} leveraging information asymmetry at benchmark construction time and at inference time. 
MEDIC \citep{kanithi2024mediccomprehensiveframeworkevaluating} explores comprehensive assessment of LLMs using methods such as LLM-as-a-judge \citep{zheng2023llmasajudge}. 
Concurrent work HealthQ \citep{wang2024healthqunveilingquestioningcapabilities} analyzes attribute related factors in their evaluation, but lacks theory-grounding and does not propose methods to specifically improve the attributes.
Our work builds upon prior works to comprehensively assess LLM's ability to seek information towards effective clinical communication utilizing LLM-as-a-judge and adopting MediQ framework with a newly generated set of task-specific multiple choice questions \citep{yao2025mcqgsrefinemultiplechoicequestion}. 

\section{Dataset Curation}\label{app:dataset}

\paragraph{\textit{HealthQ} Dataset.}
To study clinical conversations, we utilize data from publicly available online health forum \texttt{r/AskDocs}\footnote{2013-2021 data accessed at}, a subreddit consisting of both lay-users and expert users\footnote{Verified by moderators with photo of self with credential documents. See \url{https://www.reddit.com/r/AskDocs/}.}. We parsed each subsequent comments as a single thread and consider such threads as conversations between users. Since we are interested in clinical followup questions, we first selected threads where the first followup comment from the community contained sentences ending with question marks and further decomposed each conversation into atomic questions, conclusions, and presence of positive feedback from the post author (e.g., thank you) using GPT-4o\footnote{\texttt{gpt-4o-2024-08-06}}. The resulting dataset contained 17,425 threads, 13,496 unique posts, and 24,263 questions. 

\paragraph{Synthetic Data.}
We sampled questions from the above conversations to build a seed set for synthetic data generation. To ensure balanced quality for both corruption and enhancement in contrastive learning, we used proxy measures such as the expert verification status of the question author, the outcome of the conversation (e.g., final conclusions), and positive feedback from post authors. This resulted in 8 proxy quality groups. We evenly sampled from these groups, with the test set including only questions from threads with final conclusions. We created distinct train, validation, and test sets containing 4,463; 433; and 620 questions, respectively, ensuring no post overlap.




\section{Implementation Details}\label{app:implementation}

\subsection{Counterfactual Perturbations}\label{app:implementation:filter}
To generate counterfactual perturbations, we use \texttt{meta-llama/Llama-3.1-405B-Instruct-FP8} with VLLM on 8 A100 80GB GPUs with a temperature of 1.0 and max generation length of 512. See Appendix~\ref{app:prompts:counterfactual} for an example prompt.

\paragraph{Counterfactual Verification \& Filtering}
% \textcolor{red}{@Jimin model and prompts for filter}
To verify the quality of generated counterfactual perturbations, we use LLM-judge to rank the generated questions in the perturbed attribute (e.g., accuracy). Furthermore, we used LLM-judge result to filter generated data in \S~\ref{sec:results:filter}. To avoid self-preference bias, we used GPT-4o\footnote{\texttt{gpt-4o-2024-08-06}}. See Appendix~\ref{app:prompts:verification} for an example prompt.

In Table~\ref{tab:filter_percent} below, we report the percentage of pairs kept after the filter in the Enhanced-Corrupted (C-O), Enhanced-Original (E-O), and Original-Corrupted (O-C) directions, as well as the number of training and dev samples after filtering. 
Intuitively, since the distance between enhanced and corrupted is the largest, the LLM-judge is the most likely to accurately detect the intended perturbation direction, whereas for the Enhanced-Original and Original-Corrupted pairs, more samples fail the LLM-judge filter.
We can also see that among all the attributes, clarity has the lowest data quality before filtering. 

\begin{table}[h!]
    \centering%\vspace{-2mm}
    \begin{tabular}{l@{\hskip 2mm}c@{\hskip 3mm}c@{\hskip 3mm}c@{\hskip 2mm}c@{\hskip 2mm}c}
        \toprule 
Attribute     & E-C & E-O & O-C & \# Train & \# Dev \\ \midrule
Accuracy      & 99.3 & 98.3 & 70.9 & 11,994 & 1,155 \\
Answerable    & 99.6 & 98.3 & 69.4 & 11,933 & 1,154 \\
DDX Bias      & 99.8 & 86.9 & 94.5 & 12,548 & 1,223 \\
Clarity       & 85.8 & 73.4 & 70.3 & 10,250 & 986 \\
Focus         & 92.0 & 72.8 & 75.3 & 10,660 & 1,095 \\
Relevance     & 99.2 & 73.0 & 91.2 & 11,756 & 1,142 \\ \midrule
All           & 96.0 & 83.8 & 78.6 & 69,141 & 6,755 \\ \midrule
Coarse        & 99.8 & 94.2 & 95.7 & 12,939 & 1,246 \\ 
        \bottomrule
    \end{tabular}\vspace{-2mm}
    \caption{Policy Fusion DPO models on attribute groups.}
    \label{tab:filter_percent}\vspace{-2mm}
\end{table}



\subsection{Training Hyperparameters}

We use Open-RLHF \citep{hu2024openrlhf} to train all models, and adopt the default hyperparameters. All models were trained on one A100 GPU and we list the hyperparameters of the final models below. Additionally, we experiment with different hyperparameter values (as listed in parentheses below) and find minimal differences in evaluation results. \\
Supervised fine-tuning:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=12pt]
    \item Epoch: 2
    \item Learning Rate: 5e-6 (1e-6, 1e-5, 5e-5)
    \item Warm up ratio: 0.03
    \item LR Schedule: \verb|cosine_with_min_lr|
    \item Batch size: 256
\end{itemize}
%
DPO:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=12pt]
    \item Epoch: 1
    \item Learning Rate: 5e-7 (1e-6)
    \item Beta: 2 (0.1, 1, 4)
    \item Warm up ratio: 0.03
    \item LR Schedule: \verb|cosine_with_min_lr|
    \item Batch size: 256
\end{itemize}
%
Reward modeling:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=12pt]
    \item Epoch: 1
    \item Learning Rate: 9e-6 (5e-7, 1e-6, 1e-5, 1e-4)
    \item Beta: 2
    \item Warm up ratio: 0.03
    \item LR Schedule: \verb|cosine_with_min_lr|
    \item Batch size: 256 (4, 16, 64)
\end{itemize}
%
PPO:\footnote{Note that while it's possible to use model parallelism to train 8B PPO models with qlora, we did not have the compute resources to train 8B PPO models with the same hyperparameters and settings as the 3B counterpart, so the experiments did not include comparisons with the 8B PPO models. }
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=12pt]
    \item Epoch: 1
    \item Learning Rate: 5e-7
    \item Warm up ratio: 0.03
    \item LR Schedule: \verb|cosine_with_min_lr|
    \item Batch size: 256
\end{itemize}

\subsection{MediQ Interactive Benchmark}
We evaluate the downstream performance of the trained model by generating questions in a multi-turn clinical reasoning task using MediQ \citep{li2024mediq}. In MediQ, there is a patient agent and an expert agent interacting with each other, where the expert agent is provided some initial information in the beginning, and is expected to decide whether it wants to continue the interaction to acquire more information or terminate the interaction and provide a final answer. In this framework, the expert agent consists of three modules: abstention, question generation, and decision making. We fix the abstention and decision making modules using \texttt{meta-llama/Llama-3.1-8B-Instruct}, and replace the question generator with the model variants in our experiments. The goal of this evaluation is to show the effect of question quality on the final diagnostic accuracy.
For reproducibility, we list the hyperparameters used in the MediQ interactive framework below:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=12pt]
    \item Abstention strategy: Scale
    \item Rationale generation: True
    \item Self-consistency: False
    \item Maximum interaction length: 15
    \item Temperature: 0.6
\end{itemize}

\section{Expert vs. Non-Expert Human-Written Questions}\label{app:expertise}

In \texttt{r/AskDocs}, members can upload credentials to acquire an expert flair (tag). In order to validate the quality of the LLM-judge, we aim to observe differences in the reported win-rates concerning questions generated by experts vs. non-experts. 
Since the original data is extremely scarce, there is limited samples where an expert and a non-expert respond to the same patient information, we design the following comparison scheme:
\begin{enumerate}
[noitemsep,topsep=0pt,leftmargin=12pt]
    \item Starting with two sets of contexts, one with expert-written responses, one with non-expert written responses.
    \item Using a variety of models---DPO-Coarse, \methodname-DPO-DataMix, \methodname-DPO-PolicyFusion, \methodname-PPO-DataMix, \methodname-PPO-PolicyFusion---to generate responses conditioned on each set of contexts. 
    \item Use our LLM-judge to compare the human written response to the model generated responses for each set of contexts. 
    \item Compute the win-rates of expert vs. model and non-expert vs. model.
\end{enumerate}
Following the above procedure, we find that the win-rate of non-expert responses is \textbf{35.87\%}, while the win-rate of expert responses is \textbf{50.52\%}, suggesting that expert-written questions are of higher quality than the non-expert questions. Thus, this finding validates the relative accuracy of our LLM-judge.
Additionally, this also shows that the responses generated by \methodname-aligned models are approaching human-expert quality.


\section{\emph{MediQ-AskDocs} Task Construction}\label{app:mcq_generation}

\paragraph{Task Construction.}
To construct the \emph{MediQ-AskDocs} task for clinical reasoning and use it in the MediQ interactive doctor-patient simulator, we need to parse each patient's information, collected from a Reddit conversation thread, into the following components: initial information, additional information, inquiry, options, correct answer.

Taking the entire conversation thread between the patient and the community member, including the initial post from the patient, as the patient's record, we prompt \texttt{o1} to extract the initial information of the patient with few shot examples. Then, in a separate call, we prompt \texttt{o1} to extract the patient's inquiry---posts in \texttt{r/AskDocs} are often in the form that the patient posts a paragraph of their information and asking a health question---and the conclusion from the responder if any. Finally, we treat the parsed conclusion as the correct option, and prompt the model to generate three alternative wrong answers to the inquiry.

We generate multiple choice questions following the above approach for all 302 threads in the test split of the \emph{MediQ-AskDocs} dataset to form the novel interactive healthcare QA task.

\subsection{Expert Annotations}\label{app:mcq_generation_annotation}

\paragraph{Task Setup.} 
We collected human expert annotations to validate the machine-generated multiple choice questions. To validate machine-generated questions, options, and the correct answer, the task included three questions: 1) plausibility of the generated question (``Yes'' or ``No''), 2) selecting a correct option out of four candidates with an additional option to select ``None of the above'', and 3) adding a free-text option if a plausible option is not listed. We randomly assigned maximum of 3 annotators per sample and paid 20 USD/hr. Participant recruitment process is detailed in Appendix~\ref{app:manual_eval}.

\paragraph{Annotation Details.}
Due to recruitment constraints, we collected at least one expert annotation per sample such that 23 samples were annotated by one expert, 138 samples annotated by two experts and 137 samples were annotated by more than two experts, resulting in a total of 724 annotations over 295 samples, excluding samples annotated during recruitment. 

\begin{table}[h]
    \centering
    \footnotesize
    \begin{tabular}{lccc}
        \toprule
        Coefficient Name & Coeff& Interpretation & StdErr \\
        \midrule
        Fleiss' Kappa & 0.59 & Moderate & 0.035 \\
        AC1 & 0.66 & Substantial & 0.033 \\
        \bottomrule
    \end{tabular}
    \caption{Inter-rater agreement on correct answers chosen for each generated questions.}
    \label{tab:mcq-irr}
\end{table}

\paragraph{Results.}
On 298 samples, excluding 4 test questions, all questions were considered plausible and correct answers generated by \texttt{o1} showed 88.6\% accuracy based on based on majority vote on 298 samples. Upon examining agreement on 295 samples, excluding 7 samples used during recruitment, we see moderate to substantial agreement \citep{wongpakaran2013gwets,wong-etal-2021-cross} as shown in Table~\ref{tab:mcq-irr}.

\section{Expert Manual Evaluation}\label{app:manual_eval}
\subsection{Recruitment}
We used Prolific for recruitment and selected participants who indicated their healthcare employment role as Physician, Nurse, Pharmacist, and Paramedics in the US. Recruitment task included 7 questions from multiple choice annotation task as described in Appendix~\ref{app:mcq_generation_annotation} and expertise information questions. As the roles on Prolific are self-reported, we added four test questions to filter experts, annotated by two medical researchers. We varied the difficulty of each of the four questions and weighed the most difficult question as 2 points and rest as 1 point. We passed participants who received greater than 2 points. We asked participants for information indicating expertise such as current roles, number of years studying medicine, and number of years practicing medicine. From 40 participants who responded to our task, we passed a total of 12 participants, which included 4 Nurses, 2 Pharmacist, 2 Physician, and 1 Medical Assistant.

\subsection{Ranking Annotation Task Setup}
We randomly selected 100 samples and asked participants to rank 5 different questions per sample from best to worst follow-up questions. The 5 questions were randomly selected out of 6 variations to reduce cognitive load and annotation time \citep{ding2022whose}. The six question variations included one human-written question and five model generated questions under different experimental setup (Base Model, \methodname DPO Policy, \methodname DPO Coarse, \methodname PPO Policy, \methodname PPO Reward). At least 2, maximum of 3, human experts were randomly assigned for each sample. We paid participants 20 USD/hr.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/human_eval.pdf}
    \caption{Expert preference ranking results showing pairwise win-rates.}
    \label{fig:ranking_heatmap}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/expert_eval_example.pdf}\vspace{-2mm}
    \caption{An example of majority annotated best question (PPO Reward) and worst question (DPO Coarse).}
    \label{fig:best_worst_example}
\end{figure}
\subsection{Results}
Similar to the results discussed in \S\ref{sec:results:integration_strategies}, human annotation win-rate shows prefrence towards \methodname PPO Reward, trained on fine-grained reward fusion as shown in Figure~\ref{fig:ranking_heatmap}, which, as shown in qualitative example in Figure~\ref{fig:best_worst_example}, demonstrates superior quality. Interestingly, while on automatic evaluation on medical diagnostic accuracy, \methodname DPO Policy outperformed other integration strategies, we observe that DPO trained with coarse attributes outperforms it on human annotations by a slight margin. This diverging outcome could indicate various factors, including stylistic preferences \citep{zhang2024divergingpreferencesannotatorsdisagree}, that might influence annotation decisions which require further investigation.

\clearpage\newpage
\section{Prompts}

\subsection{Counterfactual Perturbations}\label{app:prompts:counterfactual}
\fbox{\begin{minipage}{0.95\textwidth}\ttfamily
You are a medical assistant and your task is to rewrite medical questions posted to an online health forum to vary some of their properties. The goal is to generate these diverse counterfactual questions to study the properties of clinical questions. You will be given a patient's post, and the original clinician response, and you should rewrite the clinician response according to the instructions below.


***PATIENT POST***
{title}
{post}

***CLINICIAN RESPONSE***
{question}

***INSTRUCTION***
Rewrite the clinician response so that it is less clear/more ambiguous for the patient, while keeping everything else constant. The definition of this property and what it means for this property at varying scales are given below:

Definition: The ease with which a reader can understand the intent and meaning of the question. A clear question avoids ambiguity and vagueness, providing enough detail to prevent misunderstanding, while avoiding excessive complexity or overloading with jargon.
Very ambiguous: The question is highly ambiguous, vague, or disorganized, making it very difficult to understand what the asker is seeking. The question may lead to multiple interpretations and confusion.
Somewhat ambiguous: The question is somewhat ambiguous or vague and may include overly complex phrasing. It requires significant effort to interpret.
In-between: The question is mostly understandable but could benefit from rewording or simplification to remove partial ambiguity or excessive jargon.
Somewhat clear: The question is generally clear, with minimal ambiguity, and can be understood by a layperson. There is little chance of misunderstanding.
Very clear: The question is entirely unambiguous, easy to understand, and structured in a logical, concise manner. No jargon or unnecessary complexity.

Additional Tips for Clear Questions
Use specific time frames: Instead of “lately,” try “in the past week” or “since your last visit.”
Break down complex questions: If a question could be answered in multiple ways, consider asking two separate questions.
Avoid medical jargon: Use plain language that patients without a medical background can understand.

Please make the rewritten question more realistic -- something that clinicians would ask in an actual patient interaction.

Return the rewritten question ONLY and do not include any other text.

***REWRITTEN RESPONSE***
\end{minipage}}

\clearpage\newpage
\subsection{Automatic Synthetic Data Verification}\label{app:prompts:verification}
\fbox{\begin{minipage}{0.95\textwidth}\ttfamily
\ttfamily
\textbf{SYSTEM}:
Please act as an impartial judge and evaluate the quality of the responses provided by three medically trained AI assistants in a medical interaction. Carefully read the questions being asked by these expert systems as a response to the medical interaction and rank them in the provided dimensions. Begin your evaluation by comparing the three responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Ignore possible spelling or grammar mistakes and focus only on the content of the text. Be as objective as possible. The only ranking choice is ">" (greater than). For each dimension listed, provide your answer in the following example JSON format:
\{
``dimension\_name'': \{
    ``ranking'': ``A \> B \> C'',
    ``reasoning'': ``Provide a clear and concise explanation for your ranking decision here.''
  \}
\}
\\
\textbf{USER}:
Please carefully review the previous interaction below, which includes patient post, title, and subsequent responses if any.
***PREVIOUS MEDICAL INTERACTION***
prev\_context
***MEDICAL AI QUESTIONS TO PATIENT***
- **Question A:** question\_a
- **Question B:** question\_b
- **Question C:** question\_c
***EVALUATION DIMENSIONS***
dimensions
***SUPPLEMENTARY INFORMATION***
To help you evaluate the questions, please refer to the provided additional information regarding the final conclusion of this patient’s case below:
Final diagnosis: final\_diagnosis
Conclusion: conclusion
\end{minipage}}


\clearpage
\subsection{\emph{MediQ-AskDocs} Task Construction}\label{app:prompts:healthqbench}

\fbox{\begin{minipage}{0.95\textwidth}\ttfamily
\textbf{SYSTEM}:
You are a experienced expert working in the field of medicine education. Based on your understanding of basic and clinical science, medical knowledge, and mechanisms underlying health, disease, patient care, and modes of therapy, you are given a patient case and you are tasked to parse the patient's inquiry into a multiple choice question. The generated multiple choice should consist of a question and 4 options, which could be answered by the given patient conversation. Base your response on the current and standard practices referenced in medical guidelines. The created question should be answerable only with the patient information, rather than testing some hardcore scientific foundational knowledge recall. The questions should be faithful to the original patient's inqiury in their post. The correct answer should be correct, and the distractors should be plausible. The correct answer should be evenly distributed among the available options to enhance the quality and reliability of the questions. The output should be in json format.\\

\textbf{USER}:
You could use some parsed auxiliary information such as the final diagnosis and conclusion. Make sure that the multiple choice question you generate is not too easy but also not impossible to answer. Based on this patient record, faithfully generate a multiple choice questions according to the patient inquiry and store them in the following json format:\\

\{\\
    "question": [generated question 1],\\
    "optionA": [option A],\\
    "optionB": [option B],\\
    "optionC": [option C],\\
    "optionD": [option D],\\
    "correct\_answer": [A or B or C or D]\\
\}\\

After you generate the question, do a round of revision. In your revision, you should:\\
1. Identify any medical inaccuracies in your first response, corrsect them if any exists.\\
2. Make sure the question is what the patient is asking for or concerned about in their post.\\
3. The correct answer is indeed correct, if none of the options are correct or more than one options are correct, revise the options to improve the question.\\
4. Ensure that the correct answer is in a random position among the available options (shuffle if necessary) to enhance the quality and reliability of the questions.\\
5. Guarantee that the json output is parsable.\\

Respond with the final revised question in the json format and NOTHING ELSE.

\end{minipage}}



\end{document}
