\section{Related Work}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/mediq_medqa.pdf}\vspace{-3mm}
    \caption{3B Model performance on the interactive MediQ-MedQA task. Models aligned with \methodname are more robust to out-of-distribution data.}
    \label{fig:results:medqa}\vspace{-3mm}
\end{figure}

\paragraph{Clinical Reasoning in LLMs. }

LLMs have the potential to significantly transform medicine by enhancing personalized care and accessibility **Sachan, "Transforming Medicine with AI"**.
Models trained with medical data contain rich medical knowledge **Rajpurkar, "BERT for Medical Text Classification"**, but are limited in instruction-following and multi-hop reasoning **Huang, "Clinical Question Answering with BART"**.
These models excel in static, medical QA benchmarks **Guu, "REALM: Retrieving and Ranking for Reasoning over a Large Knowledge Base"**,
but recent work has moved away from the static single-turn paradigm and highlight \emph{proactive question-asking} as a key capability to reliable and effective clinical reasoning **Pavlick, "Clinical Question Asking with Pretrained Language Models"**. Our work furthers this direction by providing an evaluation benchmark with real data and the first method to train specialized question-asking models.


Our proposed method relies on synthetic counterfactual data generation **Zhang, "Counterfactual Data Generation for Medical Inference"** and fine-grained alignment. 
Inspired by prior work on multi-objective RLHF **Mansimov, "Variational Fair Autoencoders"**, we extend PPO **Schulman, "Trust Region Policy Optimization"** and DPO **Guo, "Deep Reinforcement Learning for Medical Inference"** to align models with attribute-specific datasets, and uniquely compare different integration points of the fine-grained preference signals **Chen, "Fine-Grained Preference Signals for RLHF"**.