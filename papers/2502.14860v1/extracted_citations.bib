@misc{arroyo2024openclinicalllmssensitive,
      title={Open (Clinical) LLMs are Sensitive to Instruction Phrasings}, 
      author={Alberto Mario Ceballos Arroyo and Monica Munnangi and Jiuding Sun and Karen Y. C. Zhang and Denis Jered McInerney and Byron C. Wallace and Silvio Amir},
      year={2024},
      eprint={2407.09429},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09429}, 
}

@article{brin2023comparing,
  title={Comparing ChatGPT and GPT-4 performance in USMLE soft skill assessments},
  author={Brin, Dana and Sorin, Vera and Vaid, Akhil and Soroush, Ali and Glicksberg, Benjamin S and Charney, Alexander W and Nadkarni, Girish and Klang, Eyal},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={16492},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{chen2023meditron,
  title={Meditron-70b: Scaling medical pretraining for large language models},
  author={Chen, Zeming and Cano, Alejandro Hern{\'a}ndez and Romanou, Angelika and Bonnet, Antoine and Matoba, Kyle and Salvi, Francesco and Pagliardini, Matteo and Fan, Simin and K{\"o}pf, Andreas and Mohtashami, Amirkeivan and others},
  journal={arXiv preprint arXiv:2311.16079},
  year={2023}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chronopoulou2023adaptersoup,
  title={Adaptersoup: Weight averaging to improve generalization of pretrained language models},
  author={Chronopoulou, Alexandra and Peters, Matthew E and Fraser, Alexander and Dodge, Jesse},
  journal={arXiv preprint arXiv:2302.07027},
  year={2023}
}

@inproceedings{ding2024data,
  title={Data augmentation using llms: Data perspectives, learning paradigms and challenges},
  author={Ding, Bosheng and Qin, Chengwei and Zhao, Ruochen and Luo, Tianze and Li, Xinze and Chen, Guizhen and Xia, Wenhan and Hu, Junjie and Tuan, Luu Anh and Joty, Shafiq},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={1679--1705},
  year={2024}
}

@article{hager2024evaluation,
  title={Evaluation and mitigation of the limitations of large language models in clinical decision-making},
  author={Hager, Paul and Jungmann, Friederike and Holland, Robbie and Bhagat, Kunal and Hubrecht, Inga and Knauer, Manuel and Vielhauer, Jakob and Makowski, Marcus and Braren, Rickmer and Kaissis, Georgios and others},
  journal={Nature medicine},
  volume={30},
  number={9},
  pages={2613--2622},
  year={2024},
  publisher={Nature Publishing Group US New York}
}

@article{hu2024uncertainty,
  title={Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models},
  author={Hu, Zhiyuan and Liu, Chumin and Feng, Xidong and Zhao, Yilun and Ng, See-Kiong and Luu, Anh Tuan and He, Junxian and Koh, Pang Wei and Hooi, Bryan},
  journal={arXiv preprint arXiv:2402.03271},
  year={2024}
}

@article{jin2020disease,
  title={What disease does this patient have},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={A Large-scale Open Domain Question Answering Dataset from Medical Exams. arXiv [cs. CL]},
  year={2020}
}

@article{labrak2024biomistral,
  title={Biomistral: A collection of open-source pretrained large language models for medical domains},
  author={Labrak, Yanis and Bazoge, Adrien and Morin, Emmanuel and Gourraud, Pierre-Antoine and Rouvier, Mickael and Dufour, Richard},
  journal={arXiv preprint arXiv:2402.10373},
  year={2024}
}

@inproceedings{li2024mediq,
  title={MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning},
  author={Li, Shuyue Stella and Balachandran, Vidhisha and Feng, Shangbin and Ilgen, Jonathan S and Pierson, Emma and Koh, Pang Wei and Tsvetkov, Yulia},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{mishra2024llm,
  title={LLM-Guided Counterfactual Data Generation for Fairer AI},
  author={Mishra, Ashish and Nayak, Gyanaranjan and Bhattacharya, Suparna and Kumar, Tarun and Shah, Arpit and Foltin, Martin},
  booktitle={Companion Proceedings of the ACM on Web Conference 2024},
  pages={1538--1545},
  year={2024}
}

@article{nov2023putting,
  title={Putting ChatGPTâ€™s medical advice to the (Turing) test: survey study},
  author={Nov, Oded and Singh, Nina and Mann, Devin},
  journal={JMIR Medical Education},
  volume={9},
  pages={e46939},
  year={2023},
  publisher={JMIR Publications Toronto, Canada}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@InProceedings{pal2022medmcqa,
  title = 	 {MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author =       {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle = 	 {Proceedings of the Conference on Health, Inference, and Learning},
  pages = 	 {248--260},
  year = 	 {2022},
  editor = 	 {Flores, Gerardo and Chen, George H and Pollard, Tom and Ho, Joyce C and Naumann, Tristan},
  volume = 	 {174},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--08 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v174/pal22a/pal22a.pdf},
  url = 	 {https://proceedings.mlr.press/v174/pal22a.html},
  abstract = 	 {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS &amp; NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects &amp; topics. A detailed explanation of the solution, along with the above information, is provided in this study.}
}

@inproceedings{park-etal-2024-valuescope,
    title = "{V}alue{S}cope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions",
    author = "Park, Chan Young  and
      Li, Shuyue Stella  and
      Jung, Hayoung  and
      Volkova, Svitlana  and
      Mitra, Tanu  and
      Jurgens, David  and
      Tsvetkov, Yulia",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.972/",
    doi = "10.18653/v1/2024.findings-emnlp.972",
    pages = "16659--16695",
    abstract = "This study introduces ValueScope, a framework leveraging language models to quantify social norms and values within online communities, grounded in social science perspectives on normative structures. We employ ValueScope to dissect and analyze linguistic and stylistic expressions across 13 Reddit communities categorized under gender, politics, science, and finance. Our analysis provides a quantitative foundation confirming that even closely related communities exhibit remarkably diverse norms. This diversity supports existing theories and adds a new dimension to understanding community interactions. ValueScope not only delineates differences in social norms but also effectively tracks their evolution and the influence of significant external events like the U.S. presidential elections and the emergence of new sub-communities. The framework thus highlights the pivotal role of social norms in shaping online interactions, presenting a substantial advance in both the theory and application of social norm studies in digital spaces."
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53728--53741},
  year={2023}
}

@article{rame2024rewarded,
  title={Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards},
  author={Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shanmugam2024generative,
  title={Generative ai in medicine},
  author={Shanmugam, Divya and Agrawal, Monica and Movva, Rajiv and Chen, Irene Y and Ghassemi, Marzyeh and Pierson, Emma},
  journal={arXiv preprint arXiv:2412.10337},
  year={2024}
}

@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group}
}

@article{singhal2025toward,
  title={Toward expert-level medical question answering with large language models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Amin, Mohamed and Hou, Le and Clark, Kevin and Pfohl, Stephen R and Cole-Lewis, Heather and others},
  journal={Nature Medicine},
  pages={1--8},
  year={2025},
  publisher={Nature Publishing Group US New York}
}

@article{wang2024interpretable,
  title={Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts},
  author={Wang, Haoxiang and Xiong, Wei and Xie, Tengyang and Zhao, Han and Zhang, Tong},
  journal={arXiv preprint arXiv:2406.12845},
  year={2024}
}

@article{wu2023fine,
  title={Fine-grained human feedback gives better rewards for language model training},
  author={Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={59008--59033},
  year={2023}
}

@inproceedings{zhang2014understanding,
  title={Understanding user intents in online health forums},
  author={Zhang, Thomas and Cho, Jason HD and Zhai, Chengxiang},
  booktitle={Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
  pages={220--229},
  year={2014}
}

@article{zhou2023beyond,
  title={Beyond one-preference-for-all: Multi-objective direct preference optimization},
  author={Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint arXiv:2310.03708},
  year={2023}
}

