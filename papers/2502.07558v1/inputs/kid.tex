
\section{Sparsification measure via Kernel Ignoring Decomposition of 
Local Densities of States}
\label{sec:KID}


In the previous sections, we have outlined the importance of the spectral information of \( \Lu k \) and the way it is exploited to obtain the sparsifier \( \mc L \) with the computationally demanding GER vector \( \b r \). Here we demonstrate that \( \b r \) can be efficiently approximated via functional descriptors of the up-Laplacian spectrum known as spectral densities or densities of states \( \mu_k(\lambda \mid L_k)\), \cite{benson2016higher}; later we propose a novel method for computing \( \mu_k(\lambda \mid L_k)\) that avoids pathological structures in the up- and down-Laplacians' spectra.

\subsection{Density of States}

\begin{definition}[Density of States]
      For a given symmetric matrix \( A = Q \Lambda Q^\top \) with \( Q^\top Q = I \) and diagonal \( \Lambda = \diag \left( \lambda_1, \dots \lambda_{n} \right) \), the \emph{spectral density} or \emph{density of states} (DoS) is defined as 
      \begin{equation*}
            \mu( \lambda \mid A ) = \frac{1}{n} \sum_{i=1}^{n} \bm\delta \left( \lambda - \lambda_i \right)
      \end{equation*}
      where \( \bm\delta(\lambda )\) is a Dirac delta function. Additionally, let \( \b q_i \) be a corresponding unit eigenvector of \( A \) (such that \( A \b q_i = \lambda_i \b q_i \) and \( Q = \left( \b q_1 \mid \b q_2 \mid \dots \mid \b q_n \right)\)); then one can define a set of local densities of states (\emph{LDoS}):
      \begin{equation*}
            \mu_j ( \lambda \mid A ) = \sum_{i=1}^{n} \left| \b e_j^\top \b q_i \right|^2 \bm\delta \left( \lambda - \lambda_i \right)
      \end{equation*}
      with \( \b e_j \) being the $j$-th canonical basis vector. 
\end{definition}
Here, the DoS function \( \mu( \lambda \mid A ) \) contains the overall spectrum of the operator \( A \) while the family of LDoS \( \mu_j ( \lambda \mid A )\) describes the contribution of the simplex \( \sigma_j \in \V k \) to the spectral information.

Finally, one should note that by definition, DoS and LDoS are generalized functions; hence, the quality of their computation is difficult to assess directly. To this end, one can instead consider their histogram representations:
\begin{equation*}
    h_i = \int_{x_i}^{x_i+\Delta_x} \mu( \lambda \mid A ) d\lambda, \qquad h^{(j)}_i = \int_{x_i}^{x_i+\Delta_x} \mu_j( \lambda \mid A ) d\lambda 
\end{equation*}
which correspond to the discretized output of the convolution of spectral densities with a smooth approximation of the identity function \( K_{\Delta h} \), \( \left[ \mu(\lambda \mid A ) * K_{\Delta h }  \right]\).


With the notion of spectral density, we now obtain the following reformulation of the GER vector \( \b r \)  and its computation:
\begin{theorem}[GER through LDoS]\label{thm:GER_DOS}
      For a given simplicial complex \( \mc K \), with \(k\)-th order up-Laplacian \( \Lu k = B_{k+1} W_{k+1}^2 B_{k+1}^\top\), the generalized effective resistance \( \b r \) can be computed through a family of local densities of states \( \{ \mu_i(\lambda \mid \Ld {k+1 }) \} \) as follows:
      \begin{equation*}
            \b r_i = \int_{\ds R } (1 - \ds 1_0(\lambda)) \mu_i ( \lambda \mid \Ld {k+1} ) d\lambda 
      \end{equation*}
\end{theorem}

Theorem~\ref{thm:GER_DOS} implies that it is sufficient to obtain the LDoS family \( \{ \mu_i(\lambda \mid \Ld {k+1 }) \} \) for the next down-Laplacian \( \Ld {k+1 } \) \textit{efficiently} in order to compute the sparsifying probability measure at the level of \(k\)-simplices. At the same time, by its definition, the spectral density \( \left\{ \mu_i(\lambda \mid A ) \right\}\) requires the complete spectral information of the original operator \( A \) and, hence, is not immediately computationally beneficial. To avoid this demanding computational overhead, we leverage the functional nature of the LDoS and obtain an efficient approximation of \( \{ \mu_i(\lambda \mid \Ld {k+1 }) \} \) via truncated polynomial expansion.



\subsection{Efficient approximation of LDoS}
Fast approximations of spectral densities are typically based on Kernel Polynomial Methods (KPM) that schematically operate as follows:
\begin{enumerate}[leftmargin=*]
      \item shift the operator \( A \mapsto H \) so that \( \sigma( H ) \subseteq [a; b] \);
      \item consider a polynomial basis \( T_m(x)\) on \( [a; b]\), orthogonal with respect to the weight function \( w(x)\); then it holds
      \[ \mu_j(\lambda \mid H ) = \sum_{m=0}^\infty d_{mj} w( \lambda )T_m(\lambda) \]
      where the coefficients \( d_{mj} \) are known as \emph{moments}. In practice, one is interested in a truncated decomposition of the form \( \widehat \mu_j(\lambda \mid H ) = \sum_{m=0}^M d_{mj} w(x)T_m(x) \) for some \(M\); %{\color{red}FT: is it $d_m$ or $d_{mj}$?}
      \item the values of \( d_{mj} \) are functions of \( T_m(H)\) and can be efficiently sampled via Monte-Carlo methods, exploiting the three-term recurrence of orthogonal polynomial bases, \cite{benson2016higher}; we review this step in more detail below.
\end{enumerate}

We point out that typical choices for the shift interval and the polynomial basis are \( [-1, 1]\) and the Chebyshev polynomials of the first kind, respectively.

The fundamental limitation of KPM is the polynomial nature of the decomposition, which may require a large number of moments \( M \) for \( \widehat \mu_j(\lambda \mid H )\), in order to produce an accurate approximation for ``pathologic'' functions that are far from being polynomials. In the case of the DoS and LDoS, a particularly pathological setting is associated with eigenvalues of high multiplicity, which result in dominating ``spikes'' in the histogram representations of the spectral densities. In this case, one would have to approximate an outlier with a polynomial function. In the case of the classical graph Laplacian \( L_0 \) and the adjacency matrix, these spikes may be caused by over-represented motifs in the graph \cite{benson2016higher}. However, in this setting, one knows the closed form of the corresponding eigenspace, and thus, the over-represented eigenvalues can be explicitly filtered out. For the general case of up- and down-Laplacians \( \Lu k \) and \( \Ld k \) of order \( k>0 \), the eigenspaces with high multiplicity are unavoidable due to the Hodge decomposition, \eqref{eq:hodge_decomposition}: indeed, since \( \im B_k^\top \subseteq \ker B_{k+1}^\top = \ker \Lu k \) and \( \im B_{k+1} \subseteq \ker B_k = \ker \Ld k \), both operators have large kernels which are detrimental to KPM approximation; moreover, kernel's bases depend on the topology of the simplicial complex and cannot be easily estimated.

Below, we propose a novel modified method for approximating LDoS that intentionally avoids the spike in the kernel of \( \Ld {k+1}\), with all the necessary definitions.


\subsection{Kernel-ignoring Decomposition }

As discussed above, the quality of the KPM approximation of LDoS \( \{ \mu_j( \lambda \mid \Ld k ) \} \) suffers from the large null space of the operator; at the same time, Theorem~\ref{thm:GER_DOS} suggests that the target GER vector ignores the values of \( \mu_j( \lambda \mid \Ld {k+1} )\) associated with the kernel due to the \( ( 1 - \ds 1_0(\lambda))\) term. Note that, since \( \mu_j( \lambda \mid \Ld {k+1} ) \approx \sum_{m =0}^M d_{mj} w(\lambda) T_m(\lambda)  \) is a functional decomposition, it does not allow local errors; instead, the approximation error associated with the spike at \( \lambda = 0 \) spreads across the whole domain. For that reason, we suggest a modified shifting technique that leads to a decomposition that ignores the singular value of \( \lambda \) associated with the operator's null space.

In pre-existing methods, one sets \( H = \frac{2}{\lambda_{\max{}}} \Ld {k+1} - I \) where \( \lambda_{\max{}}\) denotes the largest eigenvalue of \( \Ld {k+1}\), so that \( \sigma(H) \subseteq [-1; 1]\) and the null eigenvectors of \( \Ld {k+1}\) are shifted to \( -1 \in \sigma(H)\). Instead, we set \( H = \frac{1}{ \lambda_{\max{}} } \Ld {k+1}\) and define a symmetrized version of the LDoS as:
\begin{equation}
      \tilde \mu_j( \lambda \mid H ) = \begin{cases}
            \mu_j( \lambda \mid H ), & \text{ if } \lambda \in (0, 1] \\
            0, & \text{ if } \lambda = 0 \\ 
            -\mu_j(-\lambda \mid H ), & \text{ if } \lambda \in [-1, 0) 
      \end{cases}
\end{equation}
Note that the support of the symmetrized \(\tilde \mu_j( \lambda \mid H )\) still falls within \( [-1; 1 ]\), and the spike associated with the value \( \lambda = 0 \) is by design tied to \( 0 \).

The remainder of the approximation approach is essentially inherited from the KPM method: let us assume that \( T_m(x)\) are Chebyhev polynomials of the first kind. Specifically,
\begin{equation}
      T_0(x) = 1, \; T_1(x) = x, \qquad T_{m+1}(x) = 2x T_m(x) - T_{m-1} (x)
\end{equation}
which as an orthonormal basis on \( [-1, 1 ]\) (with respect to the weight function \(w(x) = \frac{2}{(1+\delta_{0m}) \pi \sqrt{1-x^2}}\)). 

Since \( \tilde \mu_j( \lambda \mid H ) \) is an odd function by design, we decompose 
\begin{equation}
      \tilde \mu_j( \lambda \mid H ) = \sum_{m=0}^\infty d_{mj} w(\lambda)T_m(\lambda)
\end{equation}
where \( d_{mj} = 0 \) for even \( m \), thus requiring the decomposition by odd Chebyshev polynomials. 

Due to the orthonormality of \( T_m(x)\) (see the derivation in Appendix~\ref{app:eq11}), we have
\begin{equation}\label{eq:diagonal}
      \begin{aligned}
            d_{mj} = \left\langle \tilde \mu_j(\lambda \mid H ), T_m(x) \right\rangle & = 2\; \sum_{\mathclap{\substack{\lambda_i \in \sigma(H)\backslash \{ 0 \}}}} \;\; \left| \b e_j^\top \b q_i \right|^2  T_m(\lambda_i) = 
            2 \left[ T_m(H) \right]_{jj}  
      \end{aligned}
\end{equation}
thus, $d_{m\bullet} = 2 \diag( T_m(H) )$. 
Note that instead of computing \( T_m(H) \) one can use Monte-Carlo estimations for the trace and diagonal, specifically:
\begin{equation}
     \mathrm{diag}\, X = \ds E \left[ \b z \odot X \b z \right], \qquad \mathrm{diag}\, X \approx \frac{1}{N_z } ( Z \odot X Z ) \b 1
\end{equation}
where \( \b z \) is a vector of i.i.d. random variables with zero mean and unit variance, \( \b 1 \) is a vector of ones, and \( Z \) is a matrix collecting \(N_z\) copies of \( \b z \) column-wise  \cite{hutchinson1989stochastic, meyer2021hutch++}.

The MC-sampling reduces \( \diag ( T_m(H))\) calculations to simple \texttt{matvec} operations, which can be efficiently updated for the next order of moments \(d_{m+1, \bullet}\) due to the recurrent definition of \( T_m(x)\): indeed, assume we store the values of \( T_i(H) Z \) for \( i = 0\dots m\); then in order to get \(T_{m+1}(H) Z\) one needs to compute \( T_{m+1}(H) Z = 2 H \cdot ( T_{m}(H) Z ) - ( T_{m-1}(H) Z ) \) requiring only one additional \texttt{matvec} operation. As a result, the computational cost of the approximation is fixed to \( \mc O \left(  N_z M \,\texttt{nnz} (H) \right)\), where \( \mc O \left( \texttt{nnz} (H) \right)\) is the cost of one \texttt{matvec} operation for the operator \( H \).


\subsection{Error Propagation and the Choice of Constants }

The computational complexity of the KID method described above does not allow for a straightforward comparison with the direct GER vector computation via the pseudo-inverse of \( \Lu{k} \) because it is formulated in terms of the approximation parameters \( N_z \) and \( M \) rather than the number of simplices \( m_k \). Nonetheless, this can be addressed through error estimates for the LDoS approximation, as we demonstrate below.

Let us consider the histogram representations of the exact symmetrized LDoS \( h^{(j)} = \left[ \tilde \mu_j(\lambda \mid A ) * K_{\Delta h }  \right] \) and KPM approximation \( \wh h^{(j)}_M = \left[ \left( \sum_{m=0}^M d_{mj} w(\lambda)T_m(\lambda) \right) * K_{\Delta h }  \right] \) where the moments \( d_{mj}\) are MC-sampled. 
Using the estimation bound from \cite[Thm 4.2]{benson2016higher}%{\color{red}FT: can we cite the exact equation/theorem in that paper?}
, we obtain:
\begin{equation}
      \label{eq:err1}
      \ds E \left\| h^{(j)} - \wh h^{(j)}_M \right\|_\infty \le \frac{1}{\Delta h} \left( \frac{6L}{M} + \frac{2 \| K_{\Delta h} \|_\infty}{\pi \sqrt{N_z}} \right)
\end{equation}
where \( L \) is the Lipschitz constant of \(h^{(j)}\). This estimate is the key to obtaining the following (proof moved to the appendix):
\begin{theorem}\label{thm:error}
      For any fixed \( \delta > 0 \), let \( \bf p \) and \( \wh{ \bf p }\) be the exact and the KID-approximate sparsifying probability measures for a given sufficiently dense simplicial complex \( \mc K \). If the approximation \( \wh{ \bf p } \) is obtained with \( M \ge 24 L \frac{m_{k+1}}{\delta m_k}\) and \( N_z \ge \frac{8 \| K_{\Delta h}\|^2}{\pi^2} \frac{m^2_{k+1}}{\delta^2 m_k^2}\), then
      \begin{equation}
           \| \bf p - \wh{ \bf p } \|_\infty \le \delta 
      \end{equation}
      and the computational complexity of the KID approximation is 
      \[ 
      \mc O \left( \delta^{-3} {m^4_{k+1}}{m_k^{-3}} \right).
      \]
\end{theorem}
To get a more stream-lined understanding of the overall complexity, we recall that \( m_{k+1} = \mc O \left(  m_k^{ 1 + \frac{1}{k+1} } \right) \) for a simplicial complex lifted from a completely connected graph, resulting into the worst-case complexity of \( \mc O \left( \delta^{-3} m_k^{ 1 + \frac{4}{k+1}} \right)\). In other words, our approximation is not worse than direct computation for \( k = 1 \) (even in the densest case) and is asymptotically linear in \( k \).

