\section{Benchmarking}\label{sec:benchmark}

In this section, we show the performance of the KID approximation for the GER vector \( \b r \) and its computational complexity. In particular, through our experimental evaluation, we aim to do the following:
\begin{enumerate}[label=\bfseries(\roman*),leftmargin=*]
      \item support the asymptotic estimate for the approximation error \eqref{eq:err1} in terms of the number of moments \( M \) and the number of MC-vectors \( N_z \);
      \item support the computational complexity of the KID approximation using the (scaled) oracle choice for the parameters, Theorem~\ref{thm:error};
      \item compare the actual execution time of the approximation to the direct computation for complexes of different sizes and densities.
\end{enumerate}

\paragraph{Vietoris--Rips filtration.} Theorem~\ref{thm:error} and Equation~\eqref{eq:err1} describe the performance of the developed method in terms of the number of simplices \( m_k \). In order to appropriately numerically illustrate these behaviours, one should consider a family of arbitrarily large and dense simplicial complexes. For this reason, we opt to use simplicial complexes induced by the filtration procedure on point clouds. Formally, we proceed as follows:
\begin{enumerate}[leftmargin=*]
      \item  we consider \( m_0 \) points embedded in $\mathbb R^2$, sampled randomly in two clusters, i.e., \( \frac{m_0}{2}\) points are sampled from \( \mc N( \b 0, I )\) and \( \frac{m_0}{2}\) points are sampled from \( \mc N( c \b 1, I )\), for some \( c > 0 \); 
      \item then, for a fixed filtration threshold \( \epsilon > 0 \), a simplex \( \sigma = [v_{i_1}, ... v_{i_p} ] \) on these nodes enters the generated complex \( \mc K \) if and only if $d_{\mc M }(v_{i_j}, v_{i_k}) \le \epsilon$ for all paris $j$ and $k$. 
\end{enumerate}
This straightforward filtration is known as  Vietoris--Rips filtration, and the corresponding complex \( \mc K \) as a VR-complex. An illustrative example is provided in Figure~\ref{fig:example}. In the chosen setup, the value of the filtration parameter \( \epsilon \) naturally governs the density of the generated simplicial complexes of every order, as shown by the right panel in Figure~\ref{fig:example}: larger values of \( \epsilon \) define complexes with a higher number of edges, triangles, tetrahedrons, etc., until every possible simplex is included in \( \mc K \).

\begin{figure}[t]
      \centering
      \includegraphics[width = 1.0\columnwidth]{figures/example.pdf}
      \caption{ Example of VR-filtration. Left pane: point cloud with \( m_0 = 40 \) and filtration \( \epsilon = 1.5 \), inter-cluster distance \( c = 3 \). Right pane: dynamics of the number of simplices of different orders for varying filtration parameter \( \epsilon\). \label{fig:example}}
\end{figure}

\paragraph{Parameter choice and computational complexity.} 
The error estimate from Equation~\ref{eq:err1} suggests that the approximation error for the sparsifying norm \( \b p \) scales as \( M^{-1}\) in terms of the number of moments and as \( N_z^{-1/2}\) in terms of the number of Monte-Carlo vectors (MC-vectors). To illustrate this behaviour, we fix one of the parameters (\( M \) or \( N_z \)) to their theoretical estimates provided by Theorem~\ref{thm:error} and demonstrate the dynamic of the error \( \| \b p - \wh{\b p }\|_\infty \) as the function of the other parameter. As shown by Figure~\ref{fig:M_Nz}, the overall scaling law coincides with the estimates of Equation~\ref{eq:err1} in the case of \(1 \)-sparsification for \( \Lu 1\) operator. Note that all experiments are conducted in the at least minimally-dense setting, i.e. \( m_2 \ge m_1 \ln m_1 \).
\begin{figure}[t]
      \centering
      \includegraphics[width = 1.0\columnwidth]{figures/err_filt.pdf}
      \caption{
            Dependence of the approximation error \( \| \b p - \wh{ \b p } \|_\infty \) on the number of moments \( M \) and number of MC vectors \( N_z \). Values are tested up to (scaled) theoretical bounds from Thm~\ref{thm:error} (in red); line colors correspond to varying \( m_0 \) in the point cloud. Left pane: errors vs the number of moments \( M \) with fixed theoretical \( N_z \); right pane: errors vs the number of MC vectors \( N_z \) with fixed theoretical \( M \).  Errors are averaged over several generated VR-complexes; colored areas correspond to the spread of values. \label{fig:M_Nz}
      }
\end{figure}

Here, we explicitly highlight two observations from Figure~\ref{fig:M_Nz}: (1) larger and denser simplicial complexes tend to exhibit faster convergence in both parameters (especially in the number of moments \( M \)), and (2) Theorem~\ref{thm:error} provides theoretical (greedy) estimates for \( M \) and \( N_z \) that are sufficient for achieving the target approximation quality \(\delta\) and can be interpreted asymptotically. Consequently, one may choose scaled (and empirically sufficient) values for these parameters:
\[
M = \left\lceil \frac{m_{k+1}}{\delta\,m_k} \right\rceil
\quad\text{and}\quad
N_z = \left\lceil \frac{1}{10}\,\frac{8}{\pi^2}\,\frac{m_{k+1}^2}{\delta^2\,m_k^2} \right\rceil.
\]

\begin{figure}[t]
      \centering
      \includegraphics[width = 1.0\columnwidth]{figures/timings.pdf}
      \caption{
            Execution time of KID approximation for effective resistance of triangles, \( \V 2\) (left), and tethrahedrons, \( \V 3 \) (right). Line colors correspond to varying \( m_0 \)  in the point cloud; theoretical estimation of the computational complexity is given in dash.
            Execution times are averaged over several generated VR-complexes; colored areas correspond to the spread of values.  \label{fig:times}
       }
\end{figure}

Given this choice of parameters, in Figure~\ref{fig:times} we demonstrate that the complexity estimate
\(\mathcal{O}\!\bigl(\tfrac{m_{k+1}^4}{m_k^3}\bigr)\)
from Theorem~\ref{thm:error} aligns with the actual execution time of the KID approximation for varying filtration parameters \(\epsilon\) in the cases of \(1\)- and \(2\)-sparsification of VR-complexes.

\paragraph{Comparison with the direct computation.}

Finally, we compare the execution time of the KID approximation with that of the direct computation of the sparsifying measure \( \b{p} \) for \( 1 \)-sparsification, using the approximation parameters mentioned above (see Figure~\ref{fig:comparison}). Note that although the densest case complexity estimate 
\(\mathcal{O}\!\bigl(\delta^{-3}\,m_k^{\,1+\frac{4}{k+1}}\bigr)\)
suggests that the KID method's execution time might be comparable to direct computation, in practice the developed algorithm is significantly faster while still maintaining the target approximation error 
\(\|\b{p} - \widehat{\b{p}}\|_\infty \le \frac{\delta}{m_2}\).


\begin{figure}[t]
      \centering
      \includegraphics[width = 1.0\columnwidth]{figures/filtration100.pdf}
      \caption{
            Computation time comparison between KID-approximation \( \wh{ \b p} \), solid line, and directly computed sparsifying measure \( \b p \), dashed line (left), and corresponding approximation error \( \| \wh{ \b p} - \b p \|_\infty \) (right). Target approximation error is given in dotted line (right pane);  line colors correspond to varying \( m_0 \)  in the point cloud.
            Execution times are averaged over several generated VR-complexes. \label{fig:comparison}      
      }
\end{figure}

Additionally, we note that the performance of the direct computation of \(\b{p}\) for the largest considered point cloud with \(m_0 = 125\) highlights another important advantage of the KID approximation: reduced memory consumption. Indeed, whether one uses the definition of the GER vector 
\[
\b{r} 
= \diag \Bigl( B_{k+1}^\top (\Lu{k})^\dagger B_{k+1} \Bigr) 
\]
or the reformulation in terms of the right singular vector from Theorem~\ref{thm:GER_DOS}, a full SVD of \(\Lu{k}\) is required. In the case of point clouds with \(m_0 = 125\), denser VR-complexes lead to real-valued matrices of size \(10^4 \times 10^4\), resulting in substantial memory demands for the SVD. By contrast, the KID approximation avoids this decomposition and restricts the additional memory usage to storing Monte-Carlo matrices \(Z\) and their \texttt{matvecs} of dimension \(m_{k+1} \times N_z\), which is comparatively smaller.