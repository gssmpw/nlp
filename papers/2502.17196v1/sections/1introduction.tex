\section{Introduction}
Deep learning architectures have achieved remarkable breakthroughs in domains such text~\cite{Dubey2024TheL3}, vision~\cite{oquab2024dinov,esser2024scaling}, or multimodal tasks~\cite{NEURIPS2023_6dcf277e}, prompting widespread interest in their application to real-world problems~\cite{li2023llavamed}. However, as these models are increasingly used in high-stakes scenarios, understanding their decision-making process becomes crucial to ensure their decisions are grounded in meaningful variables rather than spurious correlations~\cite{Jeanneret_2022_ACCV,augustin2022diffusion}. This necessity has driven the development of trustworthy and interpretable architectures.

Interpretable-by-Design (ID) architectures~\cite{NEURIPS2019_adf7ee2d} aim to inherently explain their decision-making process, eliminating the need for external interpretability tools. Ideally, these architectures maintain comparable performance to traditional black-box methods while providing insights into their internal workings. Despite their promise, existing ID architectures largely rely on convolutional neural networks (CNNs) as feature extractors~\cite{wang2023learning,Boehle2022CVPR,pmlr-v119-koh20a,oikarinen2023labelfree}. Given the recent success of transformer-based models~\cite{dosovitskiy2021an}, transitioning to interpretable transformers is a logical next step.

Although Vision Transformers (ViTs)~\cite{dosovitskiy2021an} have demonstrated superior performance in computer vision tasks, the literature on their explainability and interpretability remains sparse. While attention maps are sometimes considered interpretable, many studies~\cite{serrano-smith-2019-attention,Jain2019AttentionIN} argue that they provide little to no insight into the actual decision-making process. 
We concur that attention maps offer only partial and insufficient cues, and instead route towards ID architectures.

In this paper, we address this gap by introducing a novel interpretable transformer-like architecture. Our proposed model, the Hindered Transformer (HiT), advances the understanding of ViTs by analyzing the flow of individual image patches and decomposing the classification token (\CLS) into contributions from each individual token. By constructing predictions as the sum of these token contributions, HiT achieves interpretability without relying on external methods~\cite{alain2016understanding} or gradient-based approaches~\cite{simonyan2013deep}, classifying it as an ID method.



We summarize our contributions as follows: i) We propose the Hindered Transformer (HiT) backbone, a variant of vision transformers that is inherently interpretable. ii) We empirically validate HiT on six datasets -- ImageNet~\cite{deng2009imagenet}, CUB 2011~\cite{WahCUB_200_2011}, Stanford Dogs~\cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}, Stanford Cars~\cite{krause20133d}, FGVC-Aircraft~\cite{maji13fine-grained}, and Oxford-IIIT Pets~\cite{parkhi12a} -- demonstrating its interpretability both quantitatively and qualitatively, outperforming recent ID transformer-based models and post-hoc methods.

To encourage future research, we will release our code and pretrained weights upon publication.


