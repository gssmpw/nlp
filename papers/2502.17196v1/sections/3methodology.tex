\section{Methodology}



In this section, we present our proposed approach and the rationale behind it. First, in \S\ref{sec:prelim-hit}, we present the preliminaries for the multi-head attention mechanism and ViTs. Next, in \S\ref{sec:mha} we will show that attention and multi-head attention output can be decomposed into the individual contributions of the inputs. Finally, in \S\ref{sec:presenting-hit} we present our novel architecture, the Hindered Transformer (HiT). The core of our method is to minimise the mixing of patch-level information, which allows us to express the classification token (\CLS) in ViTs as the sum of individual tokens, a direct result of \S\ref{sec:mha}. In other words, this simplification allows us to check the contribution of each token.

\subsection{Preliminaries: Transformers, ViTs and Notations}\label{sec:prelim-hit}

The transformer architecture is built upon the Scaled Dot-Product Attention operation~\cite{vaswani2017attention}, commonly referred to as the \emph{attention}. Given a query token sequence ${x}^q \in \mathbb{R}^{L_q \times d_{model}}$ and a target sequence (or key-value sequence) ${x}^t \in \mathbb{R}^{L_t \times d_{model}}$, where $L_q$ and $L_t$ are their respective sequence lengths and $d_{model}$ is the token dimension, the attention mechanism is computed as follows:
\begin{equation}\label{eq:attn}
\begin{split}
    Q &= x^q\,W_Q + b_Q \\
    K &= x^t\,W_K + b_K \\
    V &= x^t\,W_V + b_V \\
    A(x^q, x^t) &= softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{split}
\end{equation}
where the output is a sequence of the same length as $x^q$, $d_k$ is the dimension of the linear transformations, and $W_i\in\mathbb{R}^{d_{model}\times d_k}$ and $b_i\in\mathbb{R}^{d_k}$ are the weights of the linear projection $i\in\{Q, K, V\}$. In addition, Vaswani \textit{et al.}~\cite{vaswani2017attention} proposed to compute the attention mechanism $h$ times in parallel, setting $d_k = d_{model} / h$ for each individual attention operation. The resulting vectors of each individual attention, formally called heads, are concatenated and linearly post-processed to obtain the final result. This operation is called multi-head attention, and it is described as follows:
\begin{equation}
    MHA(x^q, x^t) = \underbrace{[A^1(x^q, x^t); ...; A^h(x^q, x^t)]}_{\texttt{Concatenate $h$ times}}W_o + b_o,
\end{equation}
with $A^i$ being the $i^{th}$ attention mechanism in the MHA, and $W_o\in\mathbb{R}^{d_{model}\times d_{model}}$ and $b_o\in\mathbb{R}^{d_{model}}$ the linear transformation parameters. 

In computer vision, to incorporate image data into this sequence-based formulation, the ViT first partitions the input image into $N^2$ equal-sized patches and linearly projects them to create the patch token sequence\footnote{For the rest of the paper, we will use the terms token and patch interchangeably, referring to the image patch tokens.}.
Additionally, following standard practice, a learnable classification token \CLS is prepended to the patch sequence. Furthermore, each patch token is summed with a positional embedding to encode its spatial location within the image. For the remainder of the paper, the sequence $x \in \mathbb{R}^{(N^2 + 1) \times d_{model}}$ denotes the concatenation of the patch tokens and the \CLS token, where $x[0]$ corresponds to the \CLS token.

The main ViT block builds on the MHA operation, followed by a token-wise MLP block, as in text-based transformers. 
Formally, given a set of patches $x_l$ at layer $l$, the ViT block first computes a globalized set of tokens using the MHA block. 
The resulting output is summed with a skip connection. 
Then, the output is fed into a token-wise MLP to post-process each token, followed, again, by a skip connection.
This block is summarized as follows
\begin{equation}\label{eq:vit-block}
\begin{split}
    x_{l}' &= x_l + MHA(x_l, x_l) \\
    x_{l+1} &= x_l' + MLP(x_l')
\end{split}
\end{equation}
Note that before the MHA and MLP blocks, a LayerNorm~\cite{ba2016layer} operation is applied to the data sequence, but for simplicity, we omit this operation.
Finally, the \CLS token is fed into a LayerNorm followed by a linear classifier to produce the logits of the classification task.

\subsection{Multi-Head Attention and Patch Mixing in Transformers}\label{sec:mha}


In this section, we aim to decompose the MHA operation to demonstrate that it is possible to retrieve the individual contributions of each token. In this way, we aim to lay the foundation for our architecture, which is described in the next section. 

Let's start by focusing on the attention operation (Eq.~\ref{eq:attn}). Since we will focus on the \CLS token later, and to simplify the analysis, let's assume that the query sequence has length $L_q=1$. 
Consequently, the attention mechanism can be rewritten as
\begin{equation}\label{eq:attn-decomp}
    A(x^q, x^t) = \sum_{v\in x^t} a(v, x^q, x^t)(v\,W_V + b_v),
\end{equation}
where $a(v, x^q, x^t)$ is the attention of a single token $v\in x_t$. Here, Eq.~\ref{eq:attn-decomp} shows that we can decompose the attention mechanism into separately processed patches - each patch $v$ in $x^t$ adds $a(v, x^q, x^t)(v\,W_V + b_v)$. Accordingly, if $x^t$ contains purely local information, the output of the attention is \emph{a sum of local data}.


To continue, we incorporate the previous observation into multi-head attention and verify that we can still unroll this operation into a \emph{sum of separate vectors}. One might be concerned that the concatenation-linear operation will mix each token. However, we argue that the result is still valid, since concatenating and linearly transforming the resulting vector is equivalent to linearly transforming each head and adding them together. 
Formally, by denoting $W_v^i$ and $b_v^i$ as the weights of the linear transformation generating the value sequence of $i^{th}$ head, and breaking apart $W_o$ into $h$ separate matrices, $W_o = [W_o^1; W_o^2; ...; W_o^h]$, with $W_o^i\in\mathbb{R}^{d_k\times d_{model}}$, then, the MHA becomes
\begin{equation}
\begin{split}
    MHA(x^q, x^t) &= b_o + \sum_{v\in x^t} v'(v) \\
    \text{where} \quad v'(v) &= \sum_{i=1}^h a(v, x^q, x^t) (v\,W_v^i+b_v^i) W_o^i.
\end{split}
\end{equation}
The previous result implies that we can still decompose the MHA result as the sum of vector patches, regardless of the number of heads in the MHA. So the same conclusion holds as in Eq.~\ref{eq:attn-decomp}: if the content in $x^t$ is local, then we can unravel the MHA mechanisms into \emph{local contributions}. %

\subsection{Untangling Visual Transformers}\label{sec:presenting-hit}

Unlike single MHA layers, ViTs operate on global features.
To integrate local information, these architectures use two mechanisms: the MHA layers, which spread the information within tokens, and the nonlinear MLPs, which introduce complex correlations even when applied to a linear combination of local contributions. For better explainability, it would be ideal if the classifier's decision could be expressed as a combination of information from individual patches, allowing a more interpretable understanding of how local information contributes to global predictions.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/hit-block.pdf}
    \caption{\textbf{ViT and HiT blocks.} While the ViT block mixes the patch data, HiT uniquely updates the \CLS via the MHA, but avoids post-processing the classification token in the MLP, allowing the \CLS to be unrolled at the last layer as individual contributions.}
    \label{fig:hit}
\end{figure}

In this section, we describe our proposed architecture: Hindered Transformer (HiT). By constraining the image tokens to contain only local information along all inference blocks, and by avoiding mixing the \CLS token, our novel method is able to partition the \CLS token into each individual patch, a direct outcome of the previous section. Fig.~\ref{fig:hit} shows the difference between the ViT block, and our block.


The first challenge is then constraining the data flow between patches.
To do so, we create an intermediate architecture that uses \CLS token $x_l[0]$ as the query in the MHA operation, and the rest of the sequence $x_l$ as the key-value input.
So, the output from the MHA is a single token that is summed to $x_l[0]$.
Then, as in ViTs, we will post-process each token in the sequence with the MLP.
Thus, the ViT update function in Eq.~\ref{eq:vit-block} is transformed to
\begin{equation}\label{eq:pat-block}
\begin{split}
    x'_l[0] &= x_l[0] + MHA(x_l[0], x_l)\\
    x'_l[1:] &= x_l[1:] \\
    x_{l+1} &= x'_l + MLP(x'_l).
\end{split}
\end{equation}

The previous model solves one problem by limiting the merging of data in local patches. 
However, processing the \CLS token through the MLP mixes the local information provided by the MHA block, as well as the value and output operations. 
Since our goal is to disentangle the data flow into individual contributions, we need to further constrain this processing.
To do this, we simply avoid updating the \CLS token through the MLP and passing it to the target sequence.
So, our block inference is
\begin{equation}\label{eq:hit-block}
\begin{split}
    x_{l+1}[0] &= x_l[0] + MHA(x_l[0], x_l[1:])\\
    x_{l+1}[1:] &= x_l[1:] + MLP(x_l[1:])
\end{split}
\end{equation}
We call the final architecture the Hindered Transformer (HiT), as we hinder the connections of the ViT. 
In a nutshell, HiT only updates the \CLS token via the MHA, while the MLP blocks update the image patches. These restrictions help to preserve purely local information in each token, while allowing the \CLS token to be unrolled. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/hit-saliency.pdf}
    \caption{\textbf{Saliency Maps computation using HiT.} From the results from \S\ref{sec:mha} and the definition of our architecture, HiT enables to extract the individual contribution per token and per layer. By adding together all tokens per layer, we can rearrange the tokens in a spatial layout and use the linear layer \textit{\`a la} CAM~\cite{zhou2016learning} to extract the contribution of each token.}
    \label{fig:hit-saliency}
\end{figure}

Since the classification token is not post-processed with MLP or MHA, the final image classification is the sum of the individual tokens in all layers, as shown in \S\ref{sec:mha}. 
Therefore, the \CLS in the last layer is
\begin{equation}\label{eq:hit-cls}
\begin{split}
    x_L[0] &= x_0[0] + \sum_{l=0}^{L-1} MHA(x_l[0], x_l[1:]) \\
    &= x_0[0] + \sum_{l=0}^{L-1} \left[ b_o^l + \sum_{v\in x_l[1:]} v'_l(v) \right] \\
    &=\sum_{l=0}^{L-1} \sum_{v\in x_l[1:]} \left[v'_l(v) + \frac{b_o^l}{N^2} + \frac{x_0[0]}{LN^2} \right].
\end{split}
\end{equation}
Please note that we distribute the biases $b_o^l$ of the projection operation in the MHA head evenly to each patch $v'_l(v)$. 
In a similar fashion, we spread $x_0[0]$ into all tokens for all layers.

One advantage of this architecture is that we can easily compute saliency maps, as shown in Fig.~\ref{fig:hit-saliency}.
The double sum in Eq.~\ref{eq:hit-cls} can be decomposed as a tensor $\mathrm{R}^{L\times N^2 \times d_{model}}$, where the final image representation is the sum over the first and second dimensions, \ie the layer and token dimension, respectively.
Thus, and similarly to CAM~\cite{zhou2016learning}, to compute the regions of interest used by the model for an input image, we simply run the linear classifier on each patch to get the map.
This rationale is similar to the LRP~\cite{bach2015pixel} method in the sense that the sum of value in the saliency is equal to the output logit for that specific class.

\subsection{Token Pooling}\label{sec:pooling}


Token pooling \cite{tokenpooling}, which involves downsampling the number of tokens as one progresses through the layers, is commonly used to improve the computational efficiency of standard transformers. This pooling technique effectively addresses the issue of representation power (as empirically demonstrated in \S~\ref{sec:perf-loss}) by expanding the receptive field of the image tokens in the deeper layers. We choose to adopt this approach due to its significant advantages.

To achieve this, we first reorganize the tokens into their spatial layout and then perform the pooling operation. However, we need to adapt the explanation generation approach to accommodate the pooled tokens. Typically, this involves using the backward operation of the pooling operator. In our case, since we rely on average pooling, which is linear, the backward operation is simply the transposed operator, which replicates each output token across the associated \(2 \times 2\) block and divides by \(4\). In other words, we distribute the importance of the pooling step equally among the contributing tokens.



