
\section{Related Work}

The performance of neural networks on computer vision tasks is well-established, but the complexity of these models can make them difficult to understand. This lack of transparency is problematic and has motivated the scientific community to develop methods for making neural networks more interpretable. There are two main approaches to this problem: post-hoc methods, which seek to analyze an already-trained model, and interpretable by design architectures, which aim to create models whose decision-making processes are inherently transparent.

Many post-hoc methods have been proposed in the literature, including counterfactual explanations~\cite{augustin2022diffusion,Jeanneret_2023_CVPR,zemni2023octet}, saliency maps~\cite{Wang_2023_ICCV,Jalwana_2021_CVPR,8237336,Petsiuk2018rise}, and model distillation~\cite{Ge_2021_CVPR,tan2018learning}. 
Closer to our work, a few attempts have been made to explain a ViT architecture via post-hoc algorithms. 
For instance, Abnar and Zuidema~\cite{abnar2020quantifying} proposed to compute a score per token by recursively propagating the attention maps in a top-bottom approach. 
In addition, some methods extended the LRP~\cite{bach2015pixel} paradigm to include the attention heads~\cite{chefer2021generic,Chefer_2021_CVPR}. 
The previous methods leverage the mechanisms of transformers to estimate the individual contribution of each token. 
While these approaches provide insights into trained models, they often rely on external tools or post-processing, which can limit their generalizability and integration into the model itself.
 


 
The field of interpretable by design architectures is diverse, as there is no single approach to explaining the complex behaviors neural networks. While many methods have been proposed, there has been a recent focus on prototypical part networks, such as ProtoPNets introduced by Chen~\etal~\cite{NEURIPS2019_adf7ee2d}. ProtoPNets computes class predictions based on the distances between patches of the final feature map and some prototypes, which can be visualized. 
Additionally, there have been many variations of ProtoPNets proposed in the literature~\cite{wang2023learning,Nauta_2023_CVPR,nauta2021looks,carmichael2024pixel,bontempelli2023conceptlevel,ukai2023this, rymarczyk2021interpretable,Wang_2021_ICCV,nauta2021neural,donnelly2022deformable,Dai_2017_ICCV,hase2019interpretable}.These variations aim to refine ProtoPNets to enhance performance and interpretability but primarily focus on convolutional neural networks (CNNs)~\cite{7780459,Simonyan15}, limiting their application to modern transformer architectures.

In addition to ProtoPNets, there are other methods that use alternative forms of prototypes. 
For example, PDiscoNet~\cite{van2023pdisconet} automatically detects parts of objects and uses them for the final classification. Similarly, BagNet~\cite{brendel2018approximating} mimics the Bag-of-Features approach to understand the decision-making process. 
Concept Bottleneck Models~\cite{pmlr-v119-koh20a,oikarinen2023labelfree} use concepts to explain their decisions.
Finally, a family of networks propose interpretable layers, such as B-cos networks~\cite{Boehle2022CVPR}, that learns an easily interpretable input-dependent linear transformation. 
The work of Zhang~\etal~\cite{zhang2018interpretable} proposed convolutional interpretable layers. 
Finally, some works use some variation of decoupled networks~\cite{liang2020training,Li2019InterpretableNN,shen2021interpretable} to highlight what a filter is looking at.



Concerning visual transformers, several recent works have attempted to make transformers more interpretable by modifying their architecture. Some papers try to push the boundaries to include transformer-based heads~\cite{hong2024concept,Paul2023ASI,rigotti2022attentionbased,pmlr-v162-kim22g} for prototype interpretability. However, they only include a single self-attention mechanism on top of a CNN backbone.  ProtoPFormer~\cite{Xue2022ProtoPFormerCO} suggests including a ViT backbone in the prototype setup. To do this, ProtoPFormer includes a prototype layer on top of both the classification and image tokens. However, this architecture does not guarantee that the tokens contain purely local information, especially since computing a classification on top of image tokens increases the receptive fields of the tokens~\cite{raghu2021do}. While innovative, this architecture does not ensure that the tokens contain purely local information due to the increased receptive fields of tokens when combining classification and image tokens~\cite{raghu2021do}. B-cos networks V2~\cite{Boehle2024TPAMI} use the same rational as the original B-cos~\cite{Boehle2022CVPR} approach, but they extend it to transformers. In a few words, B-cos networks summarize their inner workings as a single linear function, creating the attribution map by simply multiplying the input and the summarized network. However, this extension still relies on convolutional layers as the initial feature extraction stage, limiting its interpretability scope. %
Finally, A-ViT~\cite{yin2022avit} was originally designed for faster inference by removing tokens at certain layers. Despite this, this mechanism serves as an interpretable system, as the most important tokens are retained until the last layer.  This approach, however, focuses on efficiency rather than interpretability as its primary goal. %


In contrast with this literature, we propose an architecture interpretable by design, by adapting the main building blocks of vision transformers to disentangle the contributions of each image patch. This enables us to compute the salient regions of the image without the need for any non-traditional training or invasive methods. 
Unlike other approaches, we avoid relying on the attention mechanism of the multi-head attention (MHA) block to produce saliency maps. Instead, our network inherently generates these maps as part of its decision-making process. 
 

