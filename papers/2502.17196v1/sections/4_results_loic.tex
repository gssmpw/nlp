\section{Experiments}




\subsection{Evaluation Protocols}

The quantitative evaluation of the saliency map quality is performed using the insertion-deletion metrics~\cite{Petsiuk2018rise}. The insertion process is performed iteratively by creating a perturbed copy of the input image by progressively adding regions from the original image. This copy is initially filled either with zeros or with a blurred version of the image. Starting with the most influential regions according to the saliency map and moving to the least influential regions, each step adds an original region in place of the perturbed one. During this process, the class probability of the perturbed image is tracked for the originally predicted class. This results in a curve for each input image, where the x-axis represents the percentage of inserted regions and the y-axis represents the probability. 
Then, the insertion metric is computed with the area under the curve (AUC) of the mean probability curve for all tested instances. 
The deletion metric works in a similar fashion, removing the original patches starting with the most influential and continuing until all patches are removed. 
If a saliency map generator accurately identifies the most relevant components influencing the model's decision, it will produce a steep increase (or decrease) in the insertion (or deletion) curve. A sharp transition in these curves confirms that the highlighted regions are indeed those utilized by the model for classification. Consequently, a reliable saliency method will achieve a higher (lower) AUC for the insertion (deletion) metric.

Insertion-deletion curves generated by different models cannot be directly compared if the models do not have the same calibration of their outputs, which is the case in these experiments. Therefore, we propose normalizing the insertion-deletion metrics by calculating the normalized AUC (nAUC). 
This normalization adjusts the mean probability curve based on the maximum and minimum values of the curve before computing the AUC.


Lastly, when assessing interpretable by-design approaches, it is necessary to evaluate the trade-off between raw performance on the task vs the gain in interpretability. Specifically, we compare the top-1 accuracy in contrast to the insertion-deletion metrics. %


As for the dataset, we evaluated HiT on six diverse image classification datasets, traditionally used for evaluating interpretability architectures: ImageNet~\cite{deng2009imagenet}, CUB-2011~\cite{WahCUB_200_2011}, Stanford Dogs~\cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}, Stanford Cars~\cite{krause20133d}, FGVC-Aircraft~\cite{maji13fine-grained}, and Oxford-IIIT Pets~\cite{parkhi12a}. In the supplementary material, we described in-depth each dataset. 





\subsection{Implementation Details}


To train HiT on ImageNet~\cite{deng2009imagenet}, we used the official DeiT3 codebase~\cite{Touvron2022DeiTIR} and followed a similar setup to their method. 
Model-wise, HiT uses the same configuration as a ViT except that we removed the last MLP block as it is not used. 
Regarding the pooling layers, we added two.
For HiT-B and HiT-S, we trained our models for 600 epochs using the AdamW optimizer~\cite{loshchilov2018decoupled} with a learning rate of $8\times10^{-4}$, a weight decay of 0.05, a batch size of 4096, 20 warm-up epochs, a cosine annealing scheduler~\cite{loshchilov2017sgdr}, and ThreeAugment~\cite{Touvron2022DeiTIR} data augmentation. 
Unlike the DeiT3 training regime, we did not use the binary cross entropy loss or any LayerDrop~\cite{Fan2020Reducing} regularization, but the traditional cross entropy with a smoothing of 0.1 and an attention dropout of 0.2.
To fine-tune HiT in the other datasets, we trained our models similarly to the ImageNet's configuration, but instead we set the batch size to 512, the number of epochs to 300 and the learning rate to $5\times10^{-5}$ for Standford Dogs and Oxford Pets, $4\times10^{-4}$ for Standford Cars and FGVC-Aircraft, and $1\times10^{-4}$ for CUB 2011.






\begin{table*}[t]
    \centering
    \scriptsize
    \begin{tabular}{C{1.7cm}|C{0.8cm}C{0.8cm}|C{0.8cm}C{0.8cm}|C{0.8cm}C{0.8cm}|C{0.8cm}C{0.8cm}|C{0.8cm}C{0.8cm}|C{0.8cm}C{0.8cm}}\toprule
        \multirow{2}{*}{Method}       & \multicolumn{2}{c|}{ImageNet} & \multicolumn{2}{c|}{CUB 2011} & \multicolumn{2}{c|}{Stanford Cars} & \multicolumn{2}{c|}{Stanford Dogs} & \multicolumn{2}{c|}{FGVC-Aircrafts} & \multicolumn{2}{c}{Oxford-IIIT Pets} \\ \cmidrule{2-13}
                      & I-Z (\ua) & D-Z (\da) & I-Z (\ua) & D-Z (\da) & I-Z (\ua) & D-Z (\da) & I-Z (\ua) & D-Z (\da) & I-Z (\ua) & D-Z (\da) & I-Z (\ua) & D-Z (\da) \\ \midrule
        HiT           & \textbf{0.57} & \textbf{0.12} & \textbf{0.36} & \textbf{0.06} & \textbf{0.66} & \textbf{0.09} & \textbf{0.64} & \textbf{0.13} & \textbf{0.60} & \textbf{0.08} & \textbf{0.67} & \textbf{0.18}\\
        HiT + Rollout & 0.40  & 0.21  & 0.47  & 0.19  & 0.49  & 0.16  & 0.50  & 0.21 & 0.52 & 0.11 & 0.58 & 0.29 \\
        HiT + GradCAM & 0.49  & 0.15  & 0.33  & 0.10  & 0.57  & 0.13  & 0.60  & 0.15 & 0.53 & 0.09 & 0.64 & 0.25 \\ \midrule
                      & I-B (\ua) & D-B (\da) & I-B (\ua) & D-B (\da) & I-B (\ua) & D-B (\da) & I-B (\ua) & D-B (\da) & I-B (\ua) & D-B (\da) & I-B (\ua) & D-B (\da) \\ \midrule
        HiT           & \textbf{0.58} & \textbf{0.23} & \textbf{0.52} & \textbf{0.14} & \textbf{0.61} & \textbf{0.20} & \textbf{0.62} & \textbf{0.27} & \textbf{0.59} & \textbf{0.17} & \textbf{0.60} & \textbf{0.26} \\
        HiT + Rollout & 0.45  & 0.31  & 0.47  & 0.19  & 0.52  & 0.28  & 0.53  & 0.35 & 0.55 & 0.2 & 0.53 & 0.35 \\
        HiT + GradCAM & 0.53  & 0.28  & 0.49  & 0.21  & 0.56  & 0.26  & 0.61  & 0.31 & 0.57 & 0.19 & 0.60 & 0.32 \\ \bottomrule
    \end{tabular}
    \caption{\textbf{HiT and Explainability methods:} We quantitatively compare HiT maps and those created by GradCAM and the modified rollout matrix (mean attention) using AUC (no normalization required here). The assessment shows that HiT maps are more faithful than those generated by GradCAM or the rollout matrix. Higher insertion is better, while lower deletion is better. I and D refers to the Insertion and Deletion metrics, respectively. Z is the zero-corrupted image, while B is the blurred corruption strategy.}
    \label{tab:interpretability-assessment}
    \vspace{-3mm}
\end{table*}


\begin{figure}[t]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{images/pat-id-imnet.pdf}
        \caption{ImageNet}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=.85\textwidth]{images/pat-id-vs-ppf-blur.pdf}
        \caption{CUB-2011}
    \end{subfigure}
    \caption{\textbf{Interpretability comparison.} We tested whether HiT's saliency maps provide better information than ProtoPFormer's, A-ViT's maps, B-cos, the rollout attention, and GradCAM. The results indicate that our methods are indeed more interpretable. %
    }
    \label{fig:ppf-vs-hit}
\end{figure}


\subsection{Quantitative Evaluation of HiT}
\label{sec:exps:quantitative}



In Fig.~\ref{tab:interpretability-assessment}, we compare the normalized insertion-deletion curves obtained from HiT and other baseline models using the blur strategy, as established in the literature. The baselines assessed include A-ViT~\cite{yin2022avit}, B-cos~\cite{Boehle2024TPAMI}, and DeiT-B~\cite{pmlr-v139-touvron21a}. For B-cos, the saliency maps are intrinsically generated, while for DeiT-B, we rely on post-hoc extraction methods. Specifically, we use the Rollout Matrix~\cite{abnar2020quantifying} and GradCAM~\cite{8237336}, referred to as DeiT-R and DeiT-GC, respectively. To create A-ViT saliency map, we utilized the layer where tokens were discarded, as described in their paper. Additionally, to mitigate the bias of the sorting algorithm when selecting the most important tokens (which tends to favor top-left corner tokens first), we added a small amount of random noise to the saliency map.


Additionally, we evaluate ProtoPFormer~\cite{Xue2022ProtoPFormerCO} on the CUB-2011 dataset. ProtoPFormer employs a Rollout Matrix to filter out irrelevant tokens for its final computation, and we used this Rollout Matrix to define the salient regions for the insertion-deletion analysis.


The results of this experiment are presented in Fig.~\ref{fig:ppf-vs-hit}a for ImageNet and Fig.~\ref{fig:ppf-vs-hit}b for CUB-2011. The nAUC scores and the profiles of the curves indicate that HiT outperforms other ID methods in terms of interpretability.

\begin{table}[t]
    \centering
    \tiny
    \begin{tabular}{C{0.07\textwidth}|C{0.035\textwidth}|C{0.025\textwidth}C{0.025\textwidth}C{0.025\textwidth}C{0.025\textwidth}C{0.025\textwidth}C{0.025\textwidth}}\toprule
       \textbf{Model}   & \textbf{Interp.} & \textbf{IMNet} & \textbf{CUB}     & \textbf{Dogs} & \textbf{Cars}  & \textbf{Aircraft} & \textbf{Pets}\\ \midrule
       DeiT3-B & $\chi$   & 83.6     & 84.9    & 94.0 & 92.8 & 85.3 & 95.0 \\
       DeiT-B  & $\chi$   & 81.1     & 84.9    & 93.4 & 93.0 & 84.9 & 95.1 \\
       B-cos-B & \checkmark & 74.4     & -       & -    & -    & - & - \\
       HiT-B   & \checkmark & 75.0     & 79.0    & 86.8 & 86.2 & 79.8 & 88.6 \\ \midrule
       DeiT3-S & $\chi$   & 81.4     & 83.1    & 90.6 & 93.0 & 83.9 & 94.5  \\
       DeiT-S  & $\chi$   & 79.8     & 83.0    & 89.6 & 92.4 & 83.4 & 94.3\\
       A-ViT-S & \checkmark & 78.6     & -       & -    & - & -& -   \\
       B-cos-S & \checkmark & 69.2     & -       & -    & - & -& -   \\
       ProtoPFormer-S & \checkmark & -        & 84.9    & 90.0 & 90.9 & - & -\\
       HiT-S   & \checkmark & 71.4     & 76.1    & 80.3 & 85.2  & 77.4 & 88.5\\ \bottomrule
    \end{tabular}
    \caption{\textbf{Top1 Accuracy.} Our proposed models have a clear performance loss compared to other ViTs. However, the ViT gains come at the expense of interpretability, whereas the HiT has acceptable performance while being explicable. ProtoPFormer performance were extracted from their paper.}  %
    \label{tab:main-results-hit}
    \vspace{-3mm}
\end{table}

To further enhance our understanding of HiT's interpretability, we analyze the unnormalized insertion-deletion metrics of traditional post-hoc methods compared to HiT in Tab.~\ref{tab:interpretability-assessment}. Specifically, we compare the saliency maps extracted from HiT with those generated by GradCAM~\cite{8237336} and an adapted Rollout Matrix~\cite{abnar2020quantifying}, both computed on HiT. Overall, the inherent saliency maps of HiT demonstrate superior performance compared to the post-hoc methods for both insertion and deletion metrics. This finding suggests that these post-hoc algorithms do not consistently identify the regions used for classification.

Finally, in Tab.~\ref{tab:main-results-hit}, we present the accuracy performance of our proposed architecture compared to both non-interpretable and interpretable alternatives. As expected, all interpretable architectures, with the exception of ProtoPFormer, show a decrease in performance relative to the non-interpretable baseline. However, our Hindered Transformer maintains clear advantages in interpretability without a significant drop in performance.



\subsection{Qualitative Evaluation}




In the next part of our study, we show in Fig.~\ref{fig:qualitative-hit} a qualitative comparison between the saliency maps generated by HiT and those computed with GradCAM and the Rollout Matrix when applied to HiT. We make three main conclusions. First, we found that our method focuses on certain parts of objects, regardless of whether the prediction is accurate or not. To quantify our claim, we computed the center of mass for each explanation and checked whether it fell within the object's bounding box in the CUB test set. We achieved 93.4\% accuracy, which strongly supports our claim. Second, our qualitative analysis suggests that misclassification generally occurs due to similar features between image classes. However, since our method generates saliency maps, HiT inherently adopts their weaknesses: it shows where the decision was made, but not which features were used. Third, Rollout and GradCAM produce noisy maps. For example, the former shows edges and highlights the general shape of the object, while the latter produces misaligned coarse maps with our base model.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{reb-images/qualitative-2.pdf}
    \caption{\textbf{Qualitative Comparison.} We show the image and its saliency maps produced by HiT and their homologuous using Rollout and GradCAM. We noticed that HiT tends to use the object's features in the image for its prediction, independently if its prediction is erroneous or not.}
    \label{fig:qualitative-hit}
    \vspace{-3mm}
\end{figure}



\subsection{Layer-wise Contribution}

Another advantage of HiT is that we can compute the contribution of each layer. 
Similar to computing the saliency maps spatially, we can create the layer-wise output tokens and look for their individual contributions. 
To this end, we show the results in Fig.~\ref{fig:layer-saliency}a for four tested dataset.
Without any surprise, we can see that most of the discriminative features are in the final layers. 

To ensure that our results are valid, we tested several ablations of our trained model on the ImageNet dataset, shown in Fig.\ref{fig:layer-saliency}b. 
For instance, we tested the accuracy drop by removing or adding a layer of choice (\emph{Excluding/Exclusive Layer} in the figure). 
Similarly, we check the performance loss by removing/adding layers in a cascaded manner, dubbed \emph{cumulative removed/inserted layer}. 
The results corroborate our previous conclusions: our novel architecture is capable of showing the contribution of each individual layer without relying on external methods, such as Linear Probing~\cite{alain2016understanding}, to understand the basic functioning of their inner layers.

\begin{figure}[t]
\begin{subfigure}{0.495\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{images/pat-saliency-layer.pdf}
    \caption{Layer saliencies per dataset.}
\end{subfigure}
\begin{subfigure}{0.495\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{images/pat-saliency-layer-exp.pdf}
    \caption{Empirical validation of layer-wise saliency.}
\end{subfigure}
    \caption{\textbf{Layer Saliency.} HiT has more advantages than just image saliency. (a) The first experiment shows that HiT computes the contribution per layer. Without any surprise, the final layers have a greater contribution. (b) We empirically validate our findings in ImageNet with a variety of experiments. Indeed, the results show that by removing certain layers, we obtain larger expected results congruent with the layer saliency.}
    \label{fig:layer-saliency}
    \vspace{-3mm}
\end{figure}

\subsection{Sanity Check}

\begin{figure}[t]
\centering
\begin{subfigure}{0.95\linewidth}
    \centering
    \includegraphics[width=0.9\textwidth]{images/pat-corr.pdf}
    \caption{Saliency Rank Correlation}
\end{subfigure}
\hfill
\begin{subfigure}{0.95\linewidth}
    \centering
    \includegraphics[width=0.9\textwidth]{images/pat-rank-corr.pdf}
    \caption{Saliency Correlation}
\end{subfigure}
\begin{subfigure}{0.95\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{images/random.pdf}
    \caption{Example Images}
\end{subfigure}
\caption{\textbf{Sanity checks on HiT.} We measure the (a) rank correlation and (b) Pearson correlation of HiT saliency maps before and after randomising the parameters of a layer.   (c) Some visual examples of the saliency maps of randomised models.}
\label{fig:sanity}
\vspace{-3mm}
\end{figure}

Adebayo~\etal~\cite{NEURIPS2018_294a8ed2} highlight that certain maps, such as edge detectors, may appear visually coherent, although they are actually unrelated to the model's decision. 
In order to address this issue, they proposed sanity check, which involves the iterative randomisation of the parameters of the network layers. This process begins with the deepest layers and proceeds step-by-step to the shallower ones. The resulting saliency map produced at each randomization step is then compared with the original map.
In our study, we adopt the same methodology as Adebayo~\etal~\cite{NEURIPS2018_294a8ed2} and compute the absolute rank correlation between the original saliency map and the one generated after randomization, as shown in Fig.~\ref{fig:sanity}a. Additionally, we include the absolute Pearson correlation coefficient in Fig.~\ref{fig:sanity}b, and some examples in Fig.~\ref{fig:sanity}c.

Foremost, we noticed that the rank correlation has a steeper slope than the Pearson correlation for the linear classification layer. 
This shows that there is a greater similarity with the Pearson correlation. 
However, the values are relatively low (less than 0.5), indicating large variations.
Secondly, both metrics reach a plateau for the subsequent randomized models. 
This low similarity suggests that the salient regions highlighted by our model are indeed what the model sees.
Finally, Fig.~\ref{fig:sanity}c shows some qualitative examples produced by the randomization of all blocks, showing that, effectively, the weights' randomization reflect a large variation in the produced saliency.




\subsection{Ablating HiT}\label{sec:perf-loss}


In \S\ref{sec:pooling}, we suggested that token pooling layers will increase the representation power of HiT. 
We hypothesize that the lack of inter-token connections would downgrade greatly the performance. 
Thus, in this section, we empirically validate that the inclusion of token pooling layers increases the performance of the model. 
In addition, we implemented a more powerful pooling strategy used by IdentityFormer~\cite{yu2024metaformer}: a 3Ã—3 convolution with a stride of 2.
We focus on this architecture because it shares similar characteristics with HiT, where each token contains its own information.
Lastly, we theorize that optimizing HiT architectures is challenging. To address this, we adopt an approach similar to DeiT3~\cite{Touvron2022DeiTIR}, training our model for 300 and 600 epochs. Finally, we observed that using binary cross-entropy adversely affects the model's performance, contrary to its effect on DeiT3.

\begin{table}[t]
    \centering
    \tiny
    \begin{tabular}{c|ccc|c} \toprule
       \textbf{Architecture} & \textbf{Pooling}     & \textbf{Loss} & \textbf{Epochs} & \textbf{Val ImageNet}\\ \midrule
       HiT-S        & None        & XE   & 300    & 65.6 \\
       HiT-S        & None        & XE   & 600    & 67.8 \\
       HiT-S        & 2x2 AvgPool & XE   & 300    & 69.3 \\
       HiT-S        & 2x2 AvgPool & XE   & 600    & \textbf{71.4} \\
       HiT-S        & None        & BCE  & 400    & 59.9 \\
       HiT-S        & None        & BCE  & 800    & 62.6 \\ \midrule
       HiT-B        & None        & XE   & 600    & 71.5 \\
       HiT-B        & 2x2 AvgPool & XE   & 600    & \textbf{75.0} \\ \midrule
       HiT-s18      & 2x2 AvgPool & XE   & 300    & 65.6 \\
       HiT-s18      & 2x2 AvgPool & XE   & 600    & 69.3 \\
       HiT-s18      & 3x3 Conv    & XE   & 300    & \textbf{75.9} \\ 
       \bottomrule
    \end{tabular}
    \caption{\textbf{Performance loss ablation.} HiT's performance loss stems from the limited information shared between tokens. Concurrently, the results suggest that HiT's optimization problem is more challenging, as extended training periods lead to more significant performance improvements.}
    \label{tab:perf_loss}
    \vspace{-3mm}
\end{table}

We present the results in Table~\ref{tab:perf_loss}. The findings align with our suspicions: the lack of transferred information between patches significantly reduces the model's accuracy. For instance, by merely adding the convolutional pooling layer of IdentityFormer, we increase the accuracy of a HiT-s18~\cite{yu2024metaformer} from $65\%$ to $75\%$. However, these convolutional layers compromise our model's interpretability by entangling information between tokens. 
Despite this gain in performance, when HiT does not use pooling layers, it's interpretability increases significantly, generating more precise explanations -- please refer to the supplemental material for an empirical comparison. 
In addition, the binary cross entropy reduces its performance. 
Finally, unlike DeiT3 training schemes, our network benefits significantly from increasing the number of epochs. We believe that this result indicates HiT has not yet converged.
