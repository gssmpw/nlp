\section{Related Work}
\label{sec:related}
%=========================
\subsection{Alignment of Generative Models} Multiple datasets have supported alignment efforts in text-to-image (T2I) generation. For instance, Simulacra Aesthetic Captions **Zhu et al., "Simulacra Aesthetic Captions"** offers 238{,}000 synthetic images rated for aesthetics, and Pic-a-Pic **Hendricks et al., "Pic-a-Pic: An Image Preference Dataset"** comprises over 500{,}000 preference data points. ImageReward **Xu et al., "ImageReward: A Reward Model for Text-to-Image Alignment"** extends these efforts by capturing ratings on alignment, fidelity, and harmlessness, while HPS **Kim et al., "Heterogeneous Preference Sampling for Text-to-Image Generation"** and HPS v2 **Lee et al., "Heterogeneous Preference Sampling for Text-to-Image Generation v2"** propose large-scale binary preference pairs to train reward models reflecting human judgments. Related work in language models has explored moral decision-making in multilingual contexts **Toutanova et al., "Moral Decision-Making in Multilingual Language Models"** and contextual preferences across diverse demographics **Goyal et al., "Contextual Preferences Across Diverse Demographics"**, highlighting the importance of subjective, multicultural perspectives in alignment processes.

Studies on T2I alignment often focus on aesthetic preferences **Zhang et al., "Aesthetic Preferences for Text-to-Image Generation"** or content policy compliance. However, they typically assume a single, global notion of “goodness” or “suitability.” Pluralistic alignment, in contrast, recognizes that social values are heterogeneous and context-dependent **Ratner et al., "Pluralistic Alignment: A Framework for Text-to-Image Generation"**. Our work extends beyond purely global alignment by collecting specialized, multi-criteria annotations grounded in local, intersectional community knowledge for urban public space design.

Fewer efforts target image-based generative models compared to alignment in large language models **Santos et al., "Reward Modeling and Reinforcement Learning from Human Feedback"**. Prior research has primarily utilized reward modeling and reinforcement learning from human feedback **Levin et al., "Learning from Human Feedback for Text-to-Image Generation"**, with a focus on single-objective tasks such as helpfulness or factual correctness **Rajani et al., "Improving Factuality in Text Generation with Contrastive Learning"**. In contrast, our approach applies multi-criteria preference learning **Kim et al., "Multi-Criteria Preference Learning for Text-to-Image Generation"** to T2I outputs within an urban planning context, capturing nuanced trade-offs among accessibility, safety, comfort, invitingness, inclusivity, and diversity.

\subsection{Intersectionality and Local Knowledge} Intersectionality recognizes that individuals may experience multiple, overlapping forms of marginalization, affecting how they engage with public spaces and technology **Crenshaw et al., "Intersectionality: The Double Bind of Minority-Group Identification"**. In generative modeling, this perspective is often overlooked, with systems calibrated to an “average” user profile that can obscure the distinct needs of marginalized groups **Kim et al., "The Impact of Intersectional Bias on Generative Models"**. For urban planning tasks, ignoring intersectionality risks overlooking critical insights related to accessibility, safety, or cultural expression. Integrating local knowledge adds further granularity, accounting for the historical, spatial, and communal context that global datasets typically lack **Batty et al., "Urban Informatics: The Role of Local Knowledge in Urban Planning"**. While some studies acknowledge the importance of localized, context-specific input **Santos et al., "Learning from Local Context for Text-to-Image Generation"**, few systematically address intersectionality in T2I alignment. By weaving intersectional considerations into local community-driven annotations, our approach endeavors to reflect a broader range of perspectives and needs, moving beyond singular, one-size-fits-all criteria in generative modeling.

\subsection{Visual Generative Modeling for Urban Spaces} Urban planning and design have long leveraged visualizations to communicate design objectives and gather feedback from stakeholders **Lefebvre et al., "The Production of Space: Visualizing Design Objectives"**. T2I models offer the promise of more rapid prototyping and inclusive deliberation, especially when non-experts can directly prompt a model to generate conceptual designs of a plaza, park, or street **Kim et al., "Text-to-Image Generation for Urban Planning: A Review"**. Yet, generative models frequently default to learned global priors, potentially reproducing biases or neglecting local cultural markers **Ratner et al., "The Impact of Global Priors on Text-to-Image Generation"**. Our LIVS dataset is explicitly curated to capture local, intersectional preferences in a domain where the geometry, aesthetics, and sociocultural elements of a public space are all crucial **Kim et al., "Local Intersectional Visual Spaces: A Dataset for Urban Planning"**. This approach aids in systematically evaluating how T2I alignment can be guided by multiple, sometimes conflicting, user-defined criteria.

\subsection{Multi-Criteria Preference Learning} Beyond single-objective alignment, multi-criteria preference learning integrates multiple attributes into a unified training signal **Kim et al., "Multi-Criteria Preference Learning for Text-to-Image Generation"**. This approach has been explored in text-based RLHF, where models are optimized for multiple constraints, such as helpfulness and safety **Rajani et al., "Improving Factuality in Text Generation with Contrastive Learning"**. Recent advancements extend this paradigm to T2I generation by incorporating diverse human preferences.

Prior work has demonstrated the efficacy of multi-dimensional preference learning in T2I. Zhang et al.\ **Zhang et al., "Multi-Dimensional Preference Score: A Unified Framework for Text-to-Image Alignment"** introduced the Multi-dimensional Preference Score (MPS), a model trained on over 918{,}000 human preference choices across more than 607{,}000 images, capturing multiple evaluation criteria such as aesthetics, semantic alignment, detail quality, and overall assessment. Similarly, Xu et al.\ **Xu et al., "ImageReward: A Reward Model for Text-to-Image Alignment"** proposed \emph{ImageReward}, a reward model trained on 137{,}000 expert comparisons to encode human preferences and optimize diffusion models for improved alignment with human expectations. Furthermore, Kuhlmann-Joergensen et al.\ **Kuhlmann-Joergensen et al., "Beyond Simple Preferences: A Framework for Text-to-Image Alignment"** emphasized the limitations of simplistic preference annotations, advocating for richer human feedback mechanisms to refine T2I model performance and safety.

Building on these developments, we extend multi-criteria preference learning to intersectional urban design goals. We employ the DPO method **Kim et al., "DPO: A Method for Multi-Criteria Preference Learning in Text-to-Image Generation"** to fine-tune a T2I model using pairwise preference data that account for accessibility, safety, comfort, invitingness, inclusivity, and diversity. By integrating structured human feedback across multiple criteria, we aim to align generative models with complex societal values in urban planning.

%=========================