\section{Discussions and Conclusion}

Due to time constraints, we were limited in the amount of data gathered as well as methodology. 
Future work should include

1. Possibly larger text corpuses. Due to the small size of this corpus, human input was feasible. If we include, for example, all accepted solutions to all problems, then it should use a retrieval algorithm. The previous paper used bag-of-words, a somewhat outdated technique. Using encoding models could yield better results.

2. Changes in the prompt. There might be more optimal prompts and possibly different prompts for different models.

3. Investigation of OpenAI's O1 and eventually O3. These, however, approach higher performing levels. 



\paragraph{Short-term Implications:}
The capabilities of these models bring forth a discussion about academic integrity in olympiads. The biggest impact is in Olympiad programming, as it is usually done remotely compared to olympiad math or physics. Current models do not have strong enough capabilities on their own to design algorithms at the highest level. For example, in the USA Computing Olympiad, with four divisions of vastly ranging difficulties, models cannot reliably solve problems at any level and zero at the highest level. Most of the tests were also done with problems before these models' training cutoff dates, and in this paper models failed to solve *any* problems past the training cutoff. But as these preliminary results suggest, models offer significant advantage during algorithm implementation. Whether it's allowed as an implementation tool or banned is an ongoing discussion topic.

In recognition of these issues, the current workflow for our AI-assisted approach remains closed-source and is shared only among a small group of individuals. Restricting access to the implementation details helps prevent misuse, at least in the near term, while we explore the broader implications and benefits of AI-driven code generation.

\paragraph{Long-Term Evolution.}
Looking beyond immediate considerations, it is increasingly clear that future generations of programmers will code hand-in-hand with AI assistants. The policies and format of these competitions must adapt to the advances of AI  in order to stay relevant, engaging, and educational. One possibility is the development of new types of problems or scoring systems that focus more on high-level conceptual reasoning and less on rote implementation. Competitions might also emphasize aspects of creativity, human insight, or real-world problem modeling that current AI systems still struggle to automate.
 

 