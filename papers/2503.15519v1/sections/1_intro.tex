
\section{Introduction}
Over the last decade, the application of artificial intelligence (AI) to software engineering and programming education has grown exponentially. Advances in deep learning have led to AI models capable of generating code automatically, significantly reducing the manual effort required for software development. These systems, initially focused on simpler coding tasks such as writing boilerplate code or performing routine refactoring, have rapidly progressed to solving more challenging problems. Recent AI-powered code-generation platforms, such as OpenAI Codex~\cite{chen2021evaluatinglargelanguagemodels} and DeepMind’s AlphaCode~\cite{Li2022}, have demonstrated that large language models (LLMs) trained on massive datasets of source code can not only write correct solutions to many standard programming tasks, but also approach near-human or even superhuman performance on certain competitive programming problems.

Despite these achievements, it remains an open question which levels of coding competitions AI systems can reliably handle. Competitive programming tasks, particularly from high-level contests like the USA Computing Olympiad (USACO), often require not just algorithmic knowledge and puzzle solving but also creativity and efficient implementation. Recently, Wang \emph{et al.}~\cite{shi2024languagemodelssolveolympiad} introduced the USACO benchmark with 307 problems, accompanied by detailed tests, reference solutions, and official analyses. Their findings show that even powerful models like GPT-4 achieve only an 8.7\% pass@1 accuracy using zero-shot chain-of-thought prompting, and the best inference method tested reaches 20.2\% accuracy by integrating techniques such as self-reflection and retrieval. In addition, a small amount of human-driven guidance (in the form of hints) can significantly improve GPT-4's performance on problems previously unsolved by any model or method. These results underscore that state-of-the-art LLMs, while impressive, still face substantial difficulties on complex algorithmic tasks.

Beyond direct competition, AI has also been studied as a tool to enhance coding education and practice. Multiple research initiatives have investigated how AI-driven tutoring systems can improve learning outcomes by providing real-time feedback, hints, and personalized learning pathways. For aspiring programmers, such systems reduce the cognitive load associated with debugging and encourage active problem-solving. Moreover, widely available AI assistants like GitHub Copilot have been reported to boost developer productivity~\cite{chen2021evaluatinglargelanguagemodels}. While most existing studies focus on novice programmers, the benefits of AI assistance for more experienced coders---particularly those competing at a high level---are not yet thoroughly explored.

Therefore, in this work, we seek to address a key question: 

\textbf{Q: Can AI help experienced coding competition participants accelerate their problem solving?} 

Specifically, rather than having AI  solve entire challenges, our approach leverages a c\textbf{ollaborative paradigm} where a human competitor devises the algorithmic strategy but then consults an AI assistant to implement and refine the solution. By combining human insights on problem decomposition with AI’s speed and breadth of coding expertise, we hypothesize that this hybrid approach may improve problem-solving efficiency while still preserving the creative and analytical aspects of algorithm design.

To test this hypothesis, we have designed an experiment that compares the performance of experienced competitors on a set of tasks both \emph{with} and \emph{without} an AI assistant. By quantifying the metric on time to solution, we aim to offer insight into how AI assistance might affect high-level competitive programming outcomes. In addition, this study lays groundwork for future research on integrating AI more seamlessly into competitive coding workflows and helping participants---from novices to experts---boost their performance through advanced yet collaborative tools.

This work represents a first step toward understanding how experienced coding competition participants can benefit from AI-assisted solutions. There is much work to be done for a more comprehensive evaluation, from novices to experts, using questions of different difficulty levels, having a larger sample size, etc. 

Last, the work brings to the forefront several critical questions and potential directions for future research. In the short term, AI’s ability to generate complex code challenges the integrity of competitions like USACO, as participants may rely on AI tools to gain unfair advantages. To prevent misuse, we currently keep our AI-assisted workflow closed-source. Over the long run, coding competitions may need to  evolve in order to stay relevant, engaging, and educational,   as AI becomes an increasingly powerful  tool.


 









