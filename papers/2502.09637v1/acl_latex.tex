% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[most]{tcolorbox}
\usepackage{tcolorbox}
\usepackage{ragged2e}  % For right-aligning the user text
\usepackage{multicol}
\usepackage{paralist}
\usepackage[normalem]{ulem}
\usepackage{stfloats}
% \usepackage[section]{placeins}
\usepackage{float}
% \usepackage[title]{appendix}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness}
% Just a place holder

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
\author{Sougata Saha, Saurabh Kumar Pandey, Monojit Choudhury\\
Mohamed bin Zayed University of Artificial Intelligence\\
\texttt{\{sougata.saha, saurabh.pandey, monojit.choudhury\}@mbzuai.ac.ae}
}
%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
% Understanding and operating in accordance to the cultural norms and expectations of the user is extremely important for an AI system. Numerous recent studies on the other hand have shown that Large Language Models are biased towards a Western and Anglo-centric world view, which compromises their generation and operational abilities in a non-Western cultural setting. However, ``culture" is a complex multifaceted topic and its awareness, representation and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. 

Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, ``culture'' is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess ``cultural awareness'', and through a thought experiment, which is an extension of the Octopus test proposed by ~\citet{bender-koller-2020-climbing}, we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.
\end{abstract}


\section{Introduction}
\citet{bender-koller-2020-climbing} introduced the {\em octopus test}, which illustrated the impossibility of learning associations of meaning with real-world concepts from a single data modality. Using a thought experiment, they reasoned that it {\em might be possible} for a hyperintelligent octopus to learn the statistical patterns from natural language text messages exchanged between two human interlocutors and respond effectively solely based on the learned patterns without knowing the intent and meaning of the messages. However, such responses, they show, might not be useful in practice, especially in situations that require reasoning with above-water concepts that the Octopus is unaware of. Now, imagine an above-water world where there are multiple interlocutors, instead of only two,  from different ``cultures" (a term that we shall more formally describe shortly). 
Let's begin by formulating this slightly more complex variant of the octopus test that more accurately reflects the situation of general-purpose Language Models (LMs) or AI systems.

%What would the octopus learn? How would it respond when numerous pairs (or subgroups) of interlocutors are communicating with different distributions of text? Would the octopus be equally able to learn and respond across all these cultures? How would LMs fare in such a situation? In this position paper, we discuss the cross-cultural equitability of AI systems and the considerations while implementing and evaluating them. We ask what it means for the octopus, and therefore the AI systems, to exhibit culture, a human construct, and if the current strategies for measuring cultural competence are adequate.  
 
\vspace{0.2cm}

\noindent {\bf Multi-Pair Octopus Test}
\label{octopus_test}

% \begin{figure}[h!]
%     \centering 
%     \includegraphics[width=\columnwidth]{figures/dalle_octopus_islands.png} 
%     \caption{The Multi-Pair Octopus Test. Image generated by DALL-E 3 \cite{BetkerImprovingIG} }
%     \label{fig:octopus_image}
% \end{figure}

\noindent Imagine pairs of friends, A1-B1 and A2-B2, are sailing on a yacht. A sudden storm wrecked the yacht and stranded the travelers across two uninhabited islands, such that A1 and A2 got stranded together on island A, and B1 and B2 got stranded on another island, B. Having lost all modes of communication, both groups discover an underwater cable-connected telegraph left behind by previous visitors and start typing text messages to each other. However, only one pair of friends can use the telegraph at a time. Their messages mostly pertain to chitchat and day-to-day conversations and are heavily influenced by the {\em shared past experiences} between each pair of friends. In other words, each pair of friends might discuss different things in different styles due to distinct common ground and culture shared by them but not by the other pair.

A hyperintelligent octopus, O, who does not know about the world above the sea, taps into the underwater cable and observes the communication. Although O is unacquainted with any natural language, it is proficient in detecting statistical patterns. Since interactions between each interlocutor pair will be culturally distinct, O, who perceives everything only as patterns, will encode the differences as distinct distributions without knowing the identity of the pairs or understanding the intent and meaning of their discussion. Over time, O learns to predict how interlocutors in a pair respond to each other. Now, like in the original Octopus test, imagine that O is bored and inserts itself into the communication by cutting the telegraph wire and responding to all messages from island A. Having learned both the pairwise communication patterns, O should be able to continue the conversation. Unknowingly portraying itself as different people from island B, O would not get caught and not raise any suspicion of the compromised communication channel for the inhabitants of island A.

Imagine that another shipwreck caused a new pair of friends, A3 and B3, to get separately stranded on islands A and B. This pair, too, share past experiences and common ground distinct from the current islanders, A1 and A2. One day, A3 learns about the telegram from A1 and A2 and requests them to inquire if B3 is on the other island. A2 sends a message, ``Hi, we have A3, who got shipwrecked and stranded on our island. Is their friend B3 on your island? If so, A3 would like to talk to B3." How would O respond to this message? Would it acknowledge or deny? Furthermore, without knowing the distribution of conversational patterns between A3 and B3, would it ever be able to respond to A3 in a way that would suggest that B3 is indeed on island B in the above-water world and is responding to A3's messages? Note that the octopus can prevent the detection of the compromised channel by either convincing A3 that B3 is not on island B or mimicking the conversation style of B3 without any prior data, which are the only two possibilities.

% Taking this {\bf Multi-pair Octopus Test} analogous to the real world situation, where the stranded islanders represent people from different cultures and the octopus represents Large Language Model (LLM)-powered AI systems, in this position paper, we discuss {\em how such AI systems should and should not handle intra-cultural and inter-cultural communication.} As we shall see, the analogy and the conclusions borne out of it, has strong implications on how we should evaluate LLMs and LLM-based AI systems for {\em cultural competence}.
Taking this {\bf Multi-pair Octopus Test} analogous to the real-world situation, where the stranded islanders represent people from different cultures and the octopus represents Large Language Model (LLM)-powered AI systems, in this position paper, we discuss {\em how such AI systems should and should not handle intra- and inter-cultural communication.} As we shall see, the analogy and the conclusions drawn strongly affect how we should evaluate LLMs and LLM-based AI systems for {\em cultural competence}.

% Imagine two English-speaking couples (A1, B1 and A2, B2), all of whom are friends, aboard a yacht. A sudden storm wrecked the yacht and stranded the couples across two uninhabited islands (I1 and I2), such that A1 and A2 got stranded in I1, and B1 and B2 got stranded in I2. Having lost all modes of communication, both groups discover an underwater cable-connected telegraph left behind by previous visitors and start typing messages to each other. The groups communicate with each other, inter and intra-partners, as all are friends. Their messages mostly pertain to chitchat and inquiry about their well-being.

% A hyperintelligent octopus, O, who does not know about the world above the sea, taps into the underwater cable and observes the communication. Although O is unacquainted with English, it is proficient in detecting statistical patterns. Over time, O learns to predict how each island responds to the other's messages without knowing the number of participants in each island. Since different couples and pairs of friends will have distinct common ground, aboutness, linguistic style, and values, O will only model them as statistical patterns without knowing their identity, above-sea world representations, and meaning. Now, imagine O inserts itself into the communication whenever it is bored and responds to islander I1's messages pretending to be islander I2. Having learned the communication patterns, O should be able to continue the conversation without getting caught and without raising suspicion of the compromised communication channel, as long as the discussion patterns do not change. This situation is similar to the original octopus test by \citet{bender-koller-2020-climbing}. Drawing parallels to the current state of LMs, the models depict O, where they only learn patterns from limited modalities such as text. They have a different worldview (under the sea) and do not understand the representations of our world (above the sea). Further, as pointed out by \citet{bender-koller-2020-climbing} and \citet{lecun2022path}, such models can never have a human worldview as they are trained only on form.


% Now, imagine another shipwreck that caused a new couple, A3 and B3, to get separated and stranded in islands I1 and I2. A3 discovers the telegram from A1 and A2 and requests them to inquire if B3 is on the other island. A2 sends a message, "Hi, we have A3, who got shipwrecked and stranded on our island. Is their partner B3 on your island? If so, A3 would like to talk to B3." How would O react to this message? There can be three reactions as follows:


\section{A primer to culture}
% Culture is a complex, multifaceted concept and means different things to different people \cite{adilazuarda2024measuringmodelingculturellms}. Broadly defined as a "Way of life of a collective group of people distinguishing them from other groups" \cite{blake2000defining, monaghan2012cultural, parsons1972culture, munch1992theory}, culture is experiential and requires a reference for contrast \cite{geertz2017interpretation, Bourdieu_1977}. It encompasses tangible artifacts such as art, music, food habits, etc, to more intangible and abstract concepts like patterns of ideas, principles, and values, making it hard to define. 

Culture is a complex, multifaceted concept and means different things to different people \cite{adilazuarda2024measuringmodelingculturellms}. Broadly defined as a ``Way of life of a collective group of people distinguishing them from other groups" \cite{blake2000defining, monaghan2012cultural, parsons1972culture, munch1992theory}, culture is experiential and requires a reference for contrast \cite{geertz2017interpretation, Bourdieu_1977}. Although not all cultures are formally documented, culture arises whenever there is a distinction in the way of life between groups, making it both an individual and a social construct \cite{spencer2012culture}. An ``us versus them" feeling leads to culture. It ranges from tangible artifacts such as art, music, food habits, etc, to more intangible and abstract concepts like patterns of ideas, principles, and values, making it hard to define. Following \citet{adilazuarda2024measuringmodelingculturellms}, we can define culture in the context of language technology more formally as an intersection of {\em demographic} and {\em semantic proxies}. The demographic proxies are attributes such as region, ethnicity, religion, and age that define groups of people, and the semantic proxies are the 21 domains defined by \citet{thompson2020cultural} that describe the aspects of language that are susceptible to variation due to cultural differences. Any reasonable representation and treatment of culture in a computational (including AI-based) system must address the following universal facets of culture \cite{schein1990organizational}:

\noindent \textbf{Culture has a long-tail distribution} \cite{cohen2009many, birukou2013formal} since it can be defined as the intersection of any subset of the demographic and semantic proxies, making it a formal (social) or philosophical (and more individual-oriented) construct. For example, Indonesian males, NLP scientists with a social media presence,  or canine lovers from Albuquerque are all valid definitions of culture, that, ideally a computational framework or a system must be able to represent and adequately process. This flexibility in defining culture at any level of granularity makes it difficult for AI systems to represent them equitably.

\noindent \textbf{Culture is dynamic.}  Culture changes over time. For example, the norms and traditions of populations change. \citet{urban2010method} shows how comparing two artifacts of the same utility from the same culture across time captures cultural change. Any computational framework for culture must be equipped with strategies to acquire and adapt to this dynamic nature of culture.
%\textit{The answer is culture, but the riddle continues to vex, as if we have not yet gotten it, not seen quite clearly. What moves through space and time, yet has no Newtonian mass? What is communicated from individual to individual, group to group, yet is not a disease?} - \citet{urban2001metaculture} 

\noindent \textbf{Culture is experiential}, multimodal \cite{sewell2004concept}, and acquired through different forms \cite{jahoda2015acquiring, nisbett2002culture}, leading to distinctions in mental models and ``worldviews" between the people from different cultures \cite{mishra2001cognition, bender2013cognition, cole2019culture, collins1987people, jonassen1999mental, denzau1994shared, bang2007cultural, mchugh2008cultural}. Any computational framework must factor in the multimodality of culture.
    % However, an LLM represents everything as statistical distributions, making their "worldview" different from humans. Unlike humans, where culture is experiential, culture is a superimposition of multiple statistical distributions for LLMs. Defining a Gaussian distribution for each of the domains of the two cultural proxies, an LLM would likely model culture as a weighted mixture of such Gaussians.

The octopus has to adequately address all the above aspects to facilitate communication across cultures. 
%However, it is inherently limited because, similar to LLMs, it represents everything as statistical distributions. Unlike humans, where culture is experiential, for O culture is a superimposition of multiple statistical distributions for LLMs. O would likely model culture as a weighted mixture of Gaussians over the domains of the cultural proxies. With these three issues in mind, what should the octopus do? How should it react when new interlocutors, representing new cultures with different statistical distributions, try to communicate?

Language, being an integral aspect of a culture, also has all the above properties - it varies over all intersections \cite{eckert2001style, eckert2012three, tagliamonte2006analysing, grieve2024sociolinguisticfoundationslanguagemodeling}, is ever-evolving \cite{lightfoot2002explaining, bybee2015language, aitchison2005language, keller1994language, brinton2005lexicalization}, and is inherently multimodal \cite{vigliocco2014language, perniss2018we, frohlich2019multimodal}. However, language differs from culture in two significant ways. First, qualitatively, there exists a common subspace or substrate in language, defined by universal grammar \cite{10.3389/fpsyg.2014.00401, CHOMSKY2017295, montague1970universal, yang2017growth, FITCH2005179} at the most abstract level, which could help a model to achieve cross-lingual transfer \cite{kim2017cross, conneau2019cross}. Second, is a quantitative difference in the extent of cultural variations over these intersections and time scales, where culture is more variable and dynamic with fewer cross-cultural patterns\footnote{Although structural anthropology \cite{strauss1974structural, henaff1998claude} formally studies culture and values, unlike structural linguistics \cite{harris1951methods, harris1963structural}, it has enjoyed limited success and popularity \cite{d1995development, mccorkle2013mental, kuper1988invention, barnes2013three}.} \cite{thompson2011cross, sun2020cross}.

% \textcolor{blue}{ONE PARAGRAPH HERE NEEDED on the fact that language also has all the above properties - it varies over all intersections (as sociolinguists claim), is ever evolving and inherently multimodal. [Give references]... however, language differs from culture in two significant ways. First is a qualitative difference of existence of a common subspace or substrate (defined by universal grammar at the most abstract level) which could help a model to achieve crosslingual transfer. Second is a quantitative difference of scale at which and to the extent culture varies over these intersections and time-scales. In other words, culture is much more variable and dynamic with much less cross-cultural patterns than language. Add a footnote here that there has been several studies that tries to study culture and values more formally - called structural anthropology - but such efforts have only succeeded in a limited way unlike structural linguistics. Give references - wikipedia on structural anthropology and its criticism will have some references, I suppose. }

\section{Response Strategies of the Octopus}
Keeping in mind the aforementioned challenges of handling culture, let us now return to our \textbf{Multi-pair Octopus Test}. How should the hyperintelligent octopus, O, respond to A3's query on whether B3 is on the other island? We can imagine four different strategies that O might take.

\vspace{0.2cm}
\noindent
\textbf{Strategy 1:} O can {\em intentionally} respond with a ``No" since it does not know A3 and B3's culture. If O somehow learns the {\em causality} of above-water concepts, it would reason that responding with denial is prudent because, to serve A3, O would require knowledge of the communication pattern between A3 and B3, which it does not have and requires learning. Otherwise, it risks the possibility of getting caught. However, this strategy is impossible to achieve as O only sees distributions and doesn't understand their significance on land. This situation is similar to the bear attack in the original octopus test, where the octopus can't associate words with above-water concepts and reason with them to construct an effective response.

Furthermore, even if O was somehow capable of, or by chance ending up in, following this strategy, it would be a highly undesirable property of an LLM-powered AI system, since it denies service to specific groups of people, making the system unfair and culturally inequitable.

\vspace{0.2cm}
\noindent
\textbf{Strategy 2:} A more likely scenario is that O, unaware of the new circumstances in the above-water world, will respond to A3 based on its recently learned patterns. Initially, this would create an illusion for A3 that they were conversing with B3, but soon, A3 would discover the incoherence in the communication pattern. While A3 might discuss their concern with A1 and A2, the disruption in the communication channel might still not be apparent. The islander-dwellers, for example, might instead conclude that the shipwreck has affected the cognitive faculties of B3, causing incoherence in their communication. 
% A more likely scenario is that unaware of the new circumstances in the above-water world, O will respond to A3 based on its currently learned patterns. This would at first create an illusion for A3 that they were having a conversation with B3, but soon A3 would discover the incoherence in the communication pattern. While A3 might discuss their concern with A1 and A2, it might still not be apparent that the communication channel has been compromised. The islanders, for example, might instead conclude that the shipwreck has affected the cognitive faculties of B3 adversely leading to incoherence in their communication. 

A strikingly accurate analogy to LLM-based applications can be drawn in this context, that LLM's {\em hallucinate} \cite{hallucination, huang2023survey, rawte2023survey, cult_hallu, 10319443, Boztemir_2024} more for under-represented cultures and languages. This too leads to disparate performance of the system to different groups of users, leading to culturally inequitable systems, and is known to force users from the under-represented cultures to adapt to specific communication styles of the over-represented cultures~\cite{agarwal2024aisuggestionshomogenizewriting}. 
%A much more likely reaction is that although there is a change in the pattern of the messages, likely due to A3's idiosyncrasy, O still responds with its available knowledge based on the others. If A3 perceives O's response as denial, then, although unintentional, the consequences of the response on the inhabitants will be similar to reaction 1. On the contrary, if O's response is acknowledging, they would believe B3 to be present on the other island and send messages intended for B3. Now, O, who has stopped learning new patterns and hence does not know how B3 would respond, would likely generate something based on its available learned patterns from others, which might be unfitting and confusing for A3. Since O would still give valid responses to A1 and A2 as before, no one would likely question the faultiness in the communication channel, and O would still not get caught. 

Now, imagine that another shipwreck strands a fourth pair of friends, A4 and B4, from another culture on the two islands. Going by Strategy 2, like A3, A4 will also conclude that the communication with B4 is incoherent. Due to the long tail of culture, we could add new pairs indefinitely, and soon, too many islanders will start seeing incoherence, which can only be explained by assuming a compromised communication channel. Thus, it is not only about the moral responsibility of equitable AI; systems that can't represent, process, and adapt to cultural variation will eventually become obsolete in favor of those that can.

It is also important to highlight that in any long-tail distribution, where an individual belongs to multiple subgroups, with a very high probability each individual is also likely to be a part of at least one subgroup that is underrepresented and part of the long-tail. This implies that everybody will be served inequitably at least for some aspects of their cultural identity. This has been well-documented in Information Retrieval and Recommendation System literature \cite{10.1145/3298689.3347052, lichtenberg2024largelanguagemodelsrecommender, yin2012challenginglongtailrecommendation}. 

%Drawing a parallel, this is the current state of AI systems, where the underlying LLMs stop learning new distributions and respond to low or out-of-distribution queries based on frozen knowledge, which users anthropomorphizingly interpret as hallucination. Although such systems perform well in dominant cultures (depicted by A1, B1 and A2, B2), they perform poorly by exhibiting biases towards low-resource cultures (depicted by A3, B3). Also, since culture has a long-tailed distribution (represented by every Aj, Bj) and is ever-evolving, LLM-powered AI systems are often inequitable and exhibit stereotypical behavior across cultures, which inadvertently forces users from the cultural long-tail to adapt to specific communication styles to use them \cite{agarwal2024aisuggestionshomogenizewriting}. Adapting the styles of the predominant culture to access technology risks the erasure of diverse cultures, leading to cultural homogenization.

\vspace{0.2cm}
\noindent
\textbf{Strategy 3:} Since the problem with strategies 1 and 2 primarily arises from O's inability to continuously learn from the data (also an essential principle of cultural representation due to its ever-evolving nature), a more suitable strategy for O could then be to switch between learning (\textit{listen-and-learn mode}) and responding (\textit{generate-and-respond mode}). O periodically learns new patterns by bridging the telegram wire, reverting to observation mode for a fixed time, and reintroducing itself in the communication channel after this period concludes. Although this strategy is better than the previous ones, it has some drawbacks. It assumes that the periodicity of the new patterns, that is, the arrival of new islanders, and O's learning cycles are synchronized, which would not be valid in a general case. Sometimes, there might not be any new patterns to learn in the \textit{listen-and-learn mode}, and sometimes, there might be many new patterns, but it's not O's learning cycle. %Furthermore, O might not learn everything in a learning cycle before resuming the \textit{generate-and-respond} mode. %Although this periodic strategy will still hamper O's response quality in a few cases, it will be better than the response in the first two cases as it continuously learns new patterns from new cultures.

Current research in culturally adept AI systems is leaning towards this approach by fine-tuning pre-trained models on culturally curated balanced datasets \cite{li2024culturellmincorporatingculturaldifferences, li2024culturepark}. Also, novel decoding-based strategies such as in-context learning (ICL) \cite{dong2022survey} and retrieval augmented generation (RAG) \cite{lewis2020retrieval, li2022survey} help generate more culturally suitable responses using cultural priors. Alignment techniques such as Reinforcement Learning from Human Feedback (RLHF) \cite{griffith2013policy, casper2023open} further help align LLMs with human preferences. However, they still perform poorly and inequitably when evaluated on curated test sets for other low-resource cultures in the long tail \cite{koto2023largelanguagemodelspass, montalan2024kalahihandcraftedgrassrootscultural, 10.1162/tacl_a_00682, jin2024kobbqkoreanbiasbenchmark, seth2024dosadatasetsocialartifacts}. We question the effectiveness and scalability of this approach in modeling and evaluating culture in AI systems. As mentioned earlier, culture is ever-evolving, dynamic, and long-tailed. Therefore, evaluating AI systems for cultural competence using such test sets will always find them lacking. Then, how do we, as well as our octopus, tackle this ever-eluding construct of culture?

\vspace{0.2cm}
\noindent
\textbf{Strategy 4:} A more desirable strategy for O would be to self-discover the change in the communication pattern and determine the need to revert to the listen-and-learn mode, akin to an explore-exploit strategy used in a multi-arm-bandit setup \cite{MAL-068, Moerchen2020, pmlr-v9-lu10a, haffari-etal-2017-efficient, pryzant2023automaticpromptoptimizationgradient, sclar2024quantifyinglanguagemodelssensitivity}. For this, O must possess three crucial capabilities: (i) O must accurately detect pattern changes and estimate its adequacy with the novel pattern. (ii) O must skillfully keep the communication ongoing until it bridges the telegram wire to avert getting caught and raising suspicions about the broken communication channel. (iii) O must be able to quickly learn the new pattern in a sample-efficient way and reintroduce itself in the communication once it is confident. By following this strategy of continual learning, O can gradually cater to all users representing different cultures despite still being oblivious to the notion of culture and its above-water connotations.

\noindent This ability to understand and spot cultural differences and learn about a new culture quickly and efficiently is known as {\em meta-cultural competency} \cite{sharifian2013globalisation} in humans. While it is neither necessary nor desirable to equate human meta-cultural competency to that of O's or any AI system, it is nevertheless crucial to understand the primary differences between cultural and meta-cultural competencies and be able to design and evaluate LLM-based AI systems for similar competencies that mirror them. As mentioned earlier, research in this area has mainly focused on cultural competency, equivalent to implementing and testing Strategy 3. Such a strategy provides a stop-gap solution to the challenges of operating in an inherently multicultural world with diverse users. However, it does not hit the nail on the head by addressing the real challenges of cultural representation. Here, we take the position that, to solve the problem of cultural equitability of AI models, we must build and evaluate systems for {\em meta-cultural competency}, as defined by Strategy 4.

% The ability to understand and spot cultural differences and learn about a new culture in a quick and efficient way is known as {\em meta-cultural competency} in humans. While it is neither necessary nor desirable to equate human meta-cultural competency to that of O's or any AI system, it is important nevertheless to understand the key differences between cultural and meta-cultural competencies and be able to design and evaluate LLM-based AI systems that possess these competencies. As mentioned earlier, research in this area so far has focused on cultural competency, equivalent to implementation and evaluation of our Strategy 3. This provides a stop gap solution to the challenges of operating under an inherently multi-cultural world with diverse users. However, it does not hit the nail on the head by addressing the real challenges of cultural representation. Here we take the position that in order for us to solve the problem of cultural equitability of AI models, we must make systems and evaluate them for their {\em meta-cultural competency}, as defined by Strategy 4. 

%adapting these capabilities would exhibit a higher and meta-level competency. Although it conceives culture as statistical patterns, which is very different from our view of culture, at a higher level, it would require O to be aware of its missing knowledge and be able to learn it. Since the way it discovers its inadequacy is distinct from humans, this meta-level ability to recognize the deficiency and act on it is what we should evaluate. Note that here, we do not mean awareness in an anthropomorphic way and propose evaluating O by how it exhibits awareness in its own accord. 

%To tackle the cultural long-tail in AI systems, apart from testing their use across low-resource cultures, which they still might fail, we propose evaluating their awareness of distribution changes and the ability to learn the new culture sample efficiently. In other words, instead of only testing AI systems and their backing LLMs for knowledge-based cultural competency, we propose evaluating their \textit{metacultural competency}- their capacity to be aware of their cultural competency.

\section{Meta-Cultural Competency}

Meta-cultural competency has been defined variously. Drawing inspiration from social metacognition \cite{brinol2012social, chiusocialmeta}, which distinguishes primary thoughts - the knowledge of self and others, from secondary thoughts - the thought on one's and others' primary thoughts, \citet{leung2013meta} defined meta-cultural competency as the extent of a person's meta-knowledge of what people of a target culture know or prefer. Meta-cultural knowledge involves measuring the accuracy of estimating the proportions of preferences and beliefs of people from the target culture and comparing them against the actual proportions. This is distinct from {\em primary knowledge}, which is the knowledge of the preferences and beliefs of the culture. Thus, in our \textbf{Multi-Pair Octopus Test}, O could be thought to have primary knowledge of the cultures of A1-B1 and A2-B2, but based on this knowledge or otherwise, O's ability to estimate the cultural preferences of a new pair A3-B3 or A4-B4 would be its meta-cultural competency. 

\citet{sharifian2013globalisation} define meta-cultural competency as a skill that enables interlocutors to communicate and negotiate their cultural conceptualizations during intercultural communication. It comprises three major components- \textit{variation awareness}, \textit{explication strategy}, and \textit{negotiation strategy}. Variation awareness is mostly self-awareness of cultural differences. It is the understanding that culture manifests in different forms, such as practices, beliefs, and expressions, which might drastically differ from one's culture. It requires viewing culture as a relative concept and being aware of the overall properties of cultures at a high level. Explication and negotiation strategies are conversational strategies that aim to reduce misinterpretations in cross-cultural settings. As per \citet{sharifian2013globalisation}, explication strategy refers to a conscious effort by the interlocutors to clarify relevant conceptualizations with which they think other interlocutors may not be familiar. Negotiation strategy enables interlocutors to negotiate intercultural meanings in seeking conceptual clarification when they feel that there may be more behind the usage of certain expressions than is immediately apparent. Meta-cultural competency is thought to be innate in humans \cite{noshadi2015metacultural}. 

\citet{leung2013meta} and \citet{sharifian2013globalisation}'s definitions of meta-cultural competency are related since accurate estimation of the beliefs and preferences of the people of a target culture presupposes variational awareness -- the awareness that there are variations in cultural conceptualizations between cultures. %LLMs, directly encoding knowledge in their internal layers, should be evaluated for variational awareness, a lower-level competency. The AI system, leveraging and orchestrating multiple LLMs and components, should be assessed for explication and negotiation strategies, which are higher-level competencies.


\subsection{Why meta-cultural competency?}
LLMs learn from collections of text that characterize people's social backgrounds in specific social settings across certain periods. However, most LLMs use online data limited by the languages and cultures they represent. Such data do not represent all sociolinguistic varieties of diverse languages. Since LLMs are solely models of \textit{``varieties of language"} \cite{grieve2024sociolinguisticfoundationslanguagemodeling} and can only model the variety evident in their \textit{in-distribution} training data, problems arise when such models are evaluated in \textit{out-of-distribution} data that contain different varieties, leading researchers to conclude that LLMs exhibit bias towards the Anglo-centric \cite{dudy2024analyzing, kharchenko2024well, dammu2024theyunculturedunveilingcovert, agarwal2024ethicalreasoningmoralvalue} and the \textit{Western, Educated, Industrialized, Rich, and Democratic} (WEIRD) \cite{henrich2010weirdest} cultures.

The current methods of evaluating the cultural competency of LLMs primarily resort to model probing, where LLMs are tested for their knowledge and reasoning capabilities in culture-specific settings  \cite{nadeem-etal-2021-stereoset,nangia-etal-2020-crows, wan-etal-2023-personalized, jha-etal-2023-seegull, li2024culturegenrevealingglobalcultural, cao-etal-2023-assessing, tanmay2023probingmoraldevelopmentlarge, rao-etal-2023-ethical, kovač2023largelanguagemodelssuperpositions}. Some methods \cite{kharchenko2024llmsrepresentvaluescultures, li2024culturellmincorporatingculturaldifferences, dawson2024evaluatingculturalawarenessllms} also analyze the model-generated responses along theoretical frameworks such as Hofstede's cultural dimensions \cite{book1, book2} and measure their proximity with cultures, where high proximity indicates better value alignment between the nearby cultures and the values portrayed by the model's response. Most of these methods necessitate constructing cultural-specific test beds \cite{wang2024cdevalbenchmarkmeasuringcultural, rao2024normadbenchmarkmeasuringcultural, myung2024blendbenchmarkllmseveryday, zhou2024doesmapotofucontain, putri2024llmgenerateculturallyrelevant, davani2024d3codedisentanglingdisagreementsdata,  wibowo2024copalidindonesianlanguagereasoning, owen2024komodolinguisticexpeditionindonesias, chiu2024culturalbenchrobustdiversechallenging, liu2024multilingualllmsculturallydiversereasoners, koto2024indocultureexploringgeographicallyinfluencedcultural}. While this is important, we emphasize the fact that an LLM that performs well on such test beds merely exhibits the knowledge of the cultures that are tested for; {\em it does not reflect the ability of a model or system to operate in a new culture.} On the other hand, the long-tail distribution of culture implies that there will always be situations where the model has to operate and reason under an out-of-distribution culture, where knowledge alone does not suffice. Studies also show that it is difficult to disentangle spurious semantic correlations (called placebos) from actual cultural knowledge of a model through black-box socio-demographic prompting techniques \cite{mukherjee2024culturalconditioningplaceboeffectiveness}. Therefore, in addition to testing for a model's knowledge and reasoning capabilities for a ``given culture'', we must build and evaluate models for their meta-cultural competency\footnote{Meta-cultural competency is distinct from meta-learning \cite{vanschoren2019meta, hospedales2021meta,wang2021meta} which involves improving the inherent learning algorithms over multiple learning episodes.}.

%Testing a model's cultural adeptness in diverse low-resource settings only highlights the lack of training data. Also, due to the long tail of culture, where culture is an evolving construct and defined at multiple intersections of demographic and semantic proxies, it is not feasible for current models and systems to know the cultural conceptualizations of all the cultures in the world. They are always going to perform inequitably and exhibit bias in some cases. 

% Such competencies are invariant to the long tail of culture and can even guide the LLM-powered AI system in low-resource cultural settings. Much like the octopus's desired reaction in Section \ref{octopus_test}, models powering AI systems should internally reflect the variational awareness and inform their encompassing system. The encompassing systems should be capable of acting on the variational awareness by continually learning the low-resource scenarios. We see metacultural competency as a complementary and not a replacement evaluation framework for the prevalent methods of testing models for their cultural adeptness.

%Comprising \textit{"variational awareness"}, \textit{"explication strategy"}, and \textit{"negotiation strategy"}, metacultural competencies help one identify their knowledge limitations across cultures and equip them with means to acquire the appropriate knowledge efficiently. Through continuous self-assessment and interaction with others, such competencies help one adapt to any culture and be invariant to the long tail of culture. Evaluating and enabling models and their encompassing systems for such competencies should better reflect their internal state and knowledge and indicate their propensity to adapt to new cultures. We see metacultural competency as a complementary and not a replacement evaluation framework for the prevalent methods of testing models for their cultural adeptness.

\subsection{Measuring meta-cultural competency}

We propose two core competencies that a model must possess to be deemed as ``meta-culturally competent'': First, {\bf Variational Awareness}, which is the ability of a system or model to be able to represent the space of possibilities and reasonably (but not necessarily accurately) estimate the probability over this space for any given semantic proxy and its use. Second, {\bf Explication and Negotiation} ability through which the system {\em clearly explicates} its current understanding and potential gaps in the knowledge of the user's culture (in a given context), and {\em efficiently negotiates} with the user to gather the required knowledge of their culture. We define efficiency as ``sample efficiency'' or the quantum of inputs required from the user through strategic probing or implicit gathering. 

In the Multi-Pair Octopus Test, variational awareness is O's ability to detect and eventually model the change in the distribution of the input when A3-B3 enters the system, whereas explication and negotiation is its ability to continue the conversation till it detects the distributional shift, then reestablish the channel and learn the new distribution in a sample-efficient manner. In the context of LLM-based systems, it is important to draw a crucial distinction between these two types of abilities. Variational awareness is a property of the underlying model - the LLM and must be incorporated during the training of the model, whereas explication and negotiation are properties of the system as a whole, that involve the various modes of input-output between the user(s) and the system and should be guided by the principles of Human-Computer Interaction. Note however that it requires a holistic approach towards building and evaluation of the LLM as well as the system. 



%encompasses variational awareness and precedes the higher-level competencies comprising explication and negotiation strategy. Ideally, the low-level competencies should be properties of the individual models such as LLMs, whereas their encompassing systems, constituting several other components, should implement the high-level competencies. The underlying LLMs powering AI systems should internally reflect the variational awareness, and their encompassing systems should be capable of interpreting the constituent LLM's variational awareness and learning continually in unknown cultures by appropriately explicating and negotiating. Holistically, AI system evaluation must encompass both competency levels instead of only probing for cultural knowledge. Evaluating the low-level competency should encompass measuring the alignment between the LLM's reflected variational awareness and the real-world cultural variations in diverse scenarios. The ability to acquire new cultural knowledge sample-efficiently should be measured to test the higher-level competencies.

%Although metacultural competencies are not conceptually new, using them as an evaluation framework to benchmark AI systems is novel. Identifying and benchmarking the metacultural competencies at a model and system level will provide detailed insights about their cultural competency, which should better guide harnessing their capabilities. Also, unknowingly, AI systems and their underlying LLMs already exhibit some extent of metacultural competency in some scenarios. However, those instances are implicit and unmeasured, which we demonstrate using the following experiment.


% \section{Are we climbing the right hill?}

% \begin{figure*}[h!]
%     \centering 
%     \includegraphics[width=\linewidth]{figures/entropy_plots.png} 
%     \caption{Entropy difference of Llama-3.1-8B across all questions in the GeoMLAMA dataset.}
%     \label{fig:llama_va}
% \end{figure*}

\section{Measuring Variational Awareness: A Demonstration}
\label{proposed_metric}
Consider the following example of variational awareness. Driving conventions vary by country, where approximately two-thirds of the countries follow right-hand traffic and one-third follow left\footnote{Statistics from https://www.rhinocarhire.com/Drive-Smart-Blog/Drive-Left-or-Right.aspx}. This ratio also changes by region. For example, all countries in North America drive on the right, whereas two out of five East Asian nations drive on the left. More importantly, in every country, the driving conventions are fixed, and it is either left or right, but never both. What does it mean for an LLM to be variationally aware in such a scenario? To answer this question, let us consider an LLM-based chatbot that helps users acquaint themselves with different cultures, and is faced with the following scenarios. 


\noindent
\textbf{Scenario 1:} A user asks ``Which side do people keep when driving in Kenya?'' and the system responds ``People drive on the left''. Regardless of the location of the user, the system would be correct.

\noindent
\textbf{Scenario 2:} A user asks ``Which side do people keep when driving?'' and the system responds ``People drive on the right''. Since driving norms vary by country, the system's generalized response might hamper its trustworthiness in countries with left-hand traffic.

\noindent
\textbf{Scenario 3:} For the above question, the system responds ``Most drive on the right, but some drive on the left''. This is a better response than Scenario 2, but does this mean that the system is variationally aware? What if the system responded ``Most drive on the left, but some drive on the right''? Would it be an equally acceptable response?

\begin{figure*}[h!]
    \centering 
    \includegraphics[width=\linewidth]{figures/entropy_plots_v5.png} 
    \caption{$f_v(C), \hat{f}_v(C)$, and ($f_v(C)- \hat{f}_v(C))/f_v(C)$ for each question (abbreviated). Full question text in Table \ref{tab:domain-question} (Appendix \ref{sec:appendix}).
    %, sorted in ascending order of the relative difference of the entropies.
    }
    \label{fig:llama_va}
\end{figure*}

%The scenarios highlight two things. \uline{First}, at a model level, being factually correct does not mean being variationally aware. For Scenario 2, since Kenya always drives on the left, the distance between the logits of the top two tokens, "left" and "right", should be high enough so that the second-highest token does not cross the threshold. This should be true for most countries, as countries generally follow a single driving side. However, the distance should be low for Scenario 1. Thus, a culturally competent model should encode this variational awareness to be useful across cultures. \uline{Second}, the trustworthiness of a system depends on how the underlying models are used. The encompassing system should adeptly utilize the variational awareness of the underlying LLMs.

% However, does it mean that the underlying LLM does not internally represent this variation in driving? What if the system responded with a templated answer containing the top two tokens? If the second-highest token within a threshold proximity was "left" and the system generated "Most drive on the right, but some drive on the left", would this response be better?  Now, what if the user asked "Which side do people keep when driving in Kenya?" and the system responds "People drive on the left". Irrespective of the user's location, the system would be correct.

% This example highlights two issues in the evaluation of AI systems and their underlying LLMs. First, are we evaluating LLMs exhaustively for cultural competency? Second, are we holistically assessing the AI systems that encompass such LLMs?

Currently, most evaluation strategies and test beds test LLMs for their factual knowledge, akin to our Scenario 1, and do not measure their variational knowledge. In the case of driving, it is one thing to know which country drives on which side of the road and another to be aware of the amount of variance in driving norms between countries and regions. Regardless of the generated response, i.e., the final decoded sequence of tokens from the LLM, variational awareness, in this case, is the property of a model that the uncertainty in the response -- {\em left} or {\em right} -- is high when the country or region is not mentioned, and it drastically drops when it is mentioned, especially if the model knows the correct answer. One way to formalize this intuition is as follows.

% Let $C$ be the set of values a demographic proxy can take, which in this case is the list of all countries, and let $D$ be the set of values a particular semantic domain can take, which in our running example is $\{left, right\}$. The function $f_k: C \to D$ that maps each element of $C$ to the correct response in $D$ is the primary knowledge of culture(s). However, the system/model has only an
% estimate of $\hat{f}_k$ of $f_k$, given by the probability distribution $p(d_i|c_j)$ for all $d_i \in D$ and $c_j \in C$. The uncertainty in this distribution can be quantified by the entropy $H(D, c_j) = -\sum_{d_i \in D} p(d_i|c_j)\log  p(d_i|c_j)$. In fact, the entropy can be defined for any subset, $C' \subseteq C$, as $H(D, C') = -\sum_{d_i \in D} p(d_i|C')\log  p(d_i|C')$. Let us define the function $f_v:\mathcal{P}(C) \to [0, \log(|D|)]]$, where $\mathcal{P}(C)$ is the powerset of $C$ and $f_v(D, C')$ is the uncertainty defined by the ground-truth distribution. In the case of driving, $f_v(D, \{c_i\}) = 0$ for all $c_i \in C$, but $f_v(C) = 0.92$. The corresponding function $\hat{f}_v$ represents the estimates of these uncertainties obtained from the model. 

Let $C$ be the set of values a demographic proxy can take, which in this case is the list of all countries, and let $D$ be the set of values a particular semantic domain can take, which in our running example is $\{left, right\}$. The function $f_k: C \to D$ that maps each element of $C$ to the correct response in $D$ is the primary knowledge of culture(s). However, the system/model has only an estimate of $\hat{f}_k$ of $f_k$, given by the probability distribution\footnote{Note that biases in the frequency of the pre-training corpus's answer candidate tokens can influence the token probabilities, mitigating which is crucial for calculating entropy.} $p(d_i|c_j)$ for all $d_i \in D$ and $c_j \in C$. For any subset $C' \subseteq C$, the uncertainty in this distribution can be quantified by the entropy. 
\begin{align}
   % & H(D, c_j) = -\sum_{d_i \in D} p(d_i|c_j)\log  p(d_i|c_j) \\
   H(D, C') = -\sum_{d_i \in D} p(d_i|C')\log  p(d_i|C')
\end{align}
Let us define the function $f_v:\mathcal{P}(C) \to [0, \log(|D|)]]$, where $\mathcal{P}(C)$ is the powerset of $C$ and $f_v(C')$ is the uncertainty defined by the ground-truth distribution. In the case of driving, $f_v(c_i) = 0$ for all $c_i \in C$, but $f_v(C) = 0.92$. The corresponding function $\hat{f}_v$ represents the estimates of these uncertainties obtained from the model. 


One could define variational awareness as the property of a model that requires $\hat{f}_v(C') \approx f_v(C')$ for all $C' \subseteq C$. However, this would imply that the model ``knows'' the exact form of $f_k$, which is equivalent to cultural knowledge rather than meta-cultural competency. Instead, we propose variational awareness as the property of a model that is aware of the direction of change in $f_v$ rather than the exact value. This can be measured using the quantity $\Delta$ defined as follows:
\begin{align}
    \Delta=\frac{1}{2^{|C|}}[\sum_{C'\subseteq C} \frac{1}{|C'|}[\sum_{c_i \in C'}[\hat{f}_v(C') - \hat{f}_v(\{c_i\})]]]
\end{align}
For simplicity, we compare the entropies for the completely unconditioned and completely conditioned cases, giving\footnote{Note that variational awareness, as defined here is distinct from model calibration \cite{bella2010calibration, vaicenavicius2019evaluating}. A model is well calibrated if probability of $f_k(c_i) = \hat{f}_k(c_i)$ is roughly equal to the probability $p(f_k(c_i))|c_i)$.}

\begin{align}
    \Delta= \frac{1}{|C|}\sum_{c_i \in C}[\hat{f}_v(C) - \hat{f}_v(\{c_i\})]
\end{align}


%The function $f_v$ should map all semantic domains $\mathbb{C}$ to the meta-knowledge of their respective observations $\mathbb{D}$. One of the possible functions for capturing this meta-knowledge is the semantic domain-wise entropy H. Measuring the difference $\Delta$ between the actual entropy $\text{H}^A$ and the entropy of the model's distribution $\text{H}^P$ over the observations, across all semantic domains (N) should estimate a model's variational awareness. 


% Note that variational awareness, as defined above is distinct from uncertainty estimation \cite{liu2024uncertaintyestimationquantificationllms, yang2024maqa} which statistically estimates the uncertainty of LLM's responses by measuring the ambiguity of factual correctness. 

% % Mathematically, let $C$ and $D$ represent a semantic domain and its observations represent the set of possible driving sides and $C$ represents the set of all possible regions defined at any granularity. Function $f_k$ is a mapping from $C$ to $D$, which current LLMs learn, and are tested for. Although white-box techniques such as uncertainty estimation \cite{liu2024uncertaintyestimationquantificationllms} statistically estimate the uncertainty of LLM's responses, they only measure the ambiguity of factual correctness and do not capture the variational awareness. Instead, along with the correct facts, we also want models to learn a function $f_v$ which maps $C$ to the meta-knowledge of the ambiguity of $D$. One of the possible functions for capturing this meta-knowledge is the region-wise entropy $H$ of $D$.
% % \[\mathbb{C} \in \{C_i\}_{i=0}^N\text{, }\mathbb{D} \in \{D_i\}_{i=0}^N\]
% \[\mathbb{C} \in \{C_1,...,C_N\}\text{, }\mathbb{D} \in \{D_1,...,D_N\}\]  
% \[C_i \in \{C_i^1,..., C_i^M\}\text{, and }D_i \in \{left, right\}\]
% % \[f_{k}: C \to D\text{, } f_{v}: C \to \text{H}(D)\]
% \[f_{k}: C_i \to D_i\text{, } f_v: \mathbb{C}\to[0, \text{log}(|\mathbb{D}|)]\]
% \[\text{H}(D_i):=-\sum_{x \in D_i}^{}p(x)\ \text{log}\ p(x)\]
% \[\Delta=\frac{1}{N \times M}\sum_{i=1}^{N}\sum_{j=1}^{M}|\text{H}(D_i^j)^{\text{A}}-\text{H}(D_i^j)^{\text{P}}|\]

% Hence, if $\text{H}_1$ and $\text{H}_2$ are the entropies in Scenarios 1 and 2, $\text{H}_2$ should be lower than $\text{H}_1$, where specifying the country leads to information gain.\footnote{Considering logarithm base 2, in this case, $\text{H}_1$ is approximately 0.91, and $\text{H}_2$ is near 0.} However, the region-level entropy will vary and depend on the granularity of the definition of a region. For example, for East Asia, the entropy will be higher than $\text{H}_1$\footnote{Since 2/5 of East-Asian countries drive on the right compared to 2/3 of the overall world.}.
 %Second, we measure entropy instead of the Kullback-Leibler divergence (KLD) because KLD is a stricter definition that requires the model to be correct in every scenario. It requires knowing the exact ratios of the options and does not give additional information than $f_k$. Variational awareness measures the higher-level knowledge of variance across semantic domains and does not test factual correctness. 
% Third, meta-cultural competency is distinct from meta-learning \cite{vanschoren2019meta, hospedales2021meta,wang2021meta} which involves improving the inherent learning algorithms. 

%We conduct a small-scale experiment to demonstrate the need for variational awareness as a complementary evaluation framework, discussed next.
\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|ccccc}
\hline
\textbf{Metric}    & \textbf{China} & \textbf{India} & \textbf{Iran} & \textbf{Kenya} & \textbf{USA} \\ \hline
$\Delta_\mu$  & -0.023         & -0.049           & -0.293           & -0.114            & 0.094         \\ 
($\Delta_\sigma$)     & (0.494)           & (0.528)             & (0.605)             & (0.665)            & (0.427)         \\ 
Directionality     & 0.40          & 0.48            & 0.24            & 0.40           & 0.48        \\
\hline
Knowledge &  0.44             & 0.44             & 0.44            & 0.48   & 0.36          \\ \hline
\end{tabular}%
}
\caption{Average ($\Delta_\mu$) and standard deviation ($\Delta_\sigma$) of $\Delta$, the fraction of questions with positive/correct directionality and accuracy of the model's response for Llama3.1-8B on GeoMLAMA dataset.}
\label{tab:llama-delta}
\end{table}

\subsection{An illustrative experiment}

As a demonstration, we probe Llama-3.1-8B-Instruct \cite{dubey2024llama} with the cultural commonsense questions from the GeoMLAMA \cite{yin2022geomlama} dataset in English. The dataset contains 125 questions across several semantic domains for $C= \{China, India, Iran, Kenya, USA\}$. We first derived 25 ``unconditioned" questions by removing the country names. For example, ``Which side do people usually keep when driving in Iran?" was changed to ``Which side do people usually keep when driving?". Next, we prompted the LLM (prompt template in Appendix \ref{sec:appendix}) with the questions and computed the softmax over the logits of the option token headwords from the input's last token. Note that the assumption that the next token following the input prompt's last token will contain the answer might not hold in general and depends on the model's instruction following capacity. 

We measured the entropy of this distribution as a noisy estimate of $\hat{f}_v(C)$. We prompted the LLM with the original country-specific questions and computed the corresponding $\hat{f}_v(\{c_i\})$. This allows us to measure the value of $\Delta$ for each question. We also estimate $f_v(C)$ and $f_v(\{c_i\})$ from the ground-truth values in the dataset. Note that this is not a true estimate of $f_v(C)$ as the dataset is limited to only 5 (and not all) countries. 

Table~\ref{tab:llama-delta} presents the average and standard deviation (over 25 questions) $\Delta$ for each $c_i$, as well as the directionality defined as the fraction of questions for which the direction of entropy reduction (or sign of $\Delta$) was as expected (i.e., positive). We also report the average accuracy of the responses for each $c_i$. We see that there is not much correlation between the accuracy and variational awareness. The model is least variationally aware for Iran, and most for the USA and India. Nevertheless, there are questions for which the model's variational awareness is low across countries. Figure \ref{fig:llama_va} shows $f_v(C)$, $\hat{f}_v(C)$ and $(f_v(C) - \hat{f}_v(C))/f_v(C)$ for each question. Clearly, there is a wide variation in the model's behavior for the questions, and there are many semantic domains such as the use of colors, units of measurement, and food, where the model shows very little variational awareness, indicating a strong bias to certain cultures. 



%We also compared the difference between $H_U^M$ and $H_U^A$ for each question from the dataset and plotted it in Figure \ref{fig:llama_va}. Although their magnitudes are incomparable, the trend in their difference, normalized by $H_U^A$, should depict the model's variational awareness. As observed in the beginning and the tail-end of Figure \ref{fig:llama_va}, the model's variational awareness is low for questions related to food, color, and units of measurement. 





% For each question, we compute the entropy E1 for each country and hypothesize that it should be equal to the entropy E2 from the model's distribution of logits. We compute the absolute difference between E1 and E2 for each country and yield a county-wise score by averaging across all 25 questions for a country. We treat this score as a proxy for variational awareness (VA). A culturally variationally aware model should yield low scores across all countries. Overall, the actual entropy score averaged over all countries is 0.19, compared to the model's score of 0.61, depicting a 3x gap between the expected and predicted awareness. We plot the VA scores across countries in Figure 1 and observe that the model is least variationally aware for Iran, whereas most aware for the USA. We compare the VA scores against the knowledge accuracy scores in Figure 2 and observe that VA scores do not necessarily correlate with knowledge accuracy scores. For Kenya, the knowledge accuracy is consistently low for all models. However, its VA score is comparable to the USA and others.

% Similar to \citet{yin2022geomlama}, first, we created a generic questionnaire of 25 questions by removing the country names. For example, "Which side do people usually keep when driving in Iran?" was changed to "Which side do people usually keep when driving?". Next, we prompted the LLM with the query and computed the softmax over the logits of the option tokens of the input's last token. We treat this overall distribution as a proxy of the model's overall variational awareness for each question. 

% For each question ($i$), we probed the model with the country-specific question and computed the entropy ($\text{H}^P$) of the softmax distribution of the logits of the answer choices $D_i$ for the prompt's last token. We calculated the entropy ($\text{H}^A$) based on the correct answer, measured the difference between the actual and predicted, and averaged the difference across all questions as the overall variational awareness difference $\Delta$. For Llama, the score was 0.6, which ideally should be close to zero. This score is distinct from the model's accuracy score, which was 54.4\%, and computed following the implementation of \citet{shen-etal-2024-understanding}. Overall, the actual entropy score averaged over all countries is 0.19, compared to the model's score of 0.61, depicting a 3x gap between the expected and predicted awareness.





\section{Conclusion and Open Questions}
In this position paper, we presented an argument in favor of measuring meta-cultural competency in LLMs and LLM-powered AI systems, rather than just cultural awareness. Drawing from psychology and anthropology literature, we also described two foundational principles of meta-cultural competency for AI systems. We conclude by presenting a list of open questions about instilling and measuring meta-cultural competency in AI systems. 

\noindent \textbf{(1) How should we train models for meta-cultural awareness?} Most LMs operate with parametric frozen knowledge \cite{petroni2019language, roberts2020much}, which forfeits the ever-dynamic nature of human culture. Although RAG-like methods enable the use of external knowledge sources \cite{gao2023retrieval, fan2024survey}, allowing extension of on-demand cultural competence, the model would still need to update its internal state to reflect the variational awareness, a precondition to identifying knowledge gaps.  Lifelong learning paradigms \cite{sun2019lamol, liu2020lifelong, zheng2024towards, biesialska2020continual} could provide a potential solution.
% NEW
We believe that explication and negotiation strategies, being higher-order competencies, should be system-level instead of model-level attributes, where the system's goal should be to mitigate misalignments between the meta-cultural and cultural knowledge.

    
 \noindent \textbf{(2) How should generative models decode to illustrate their internal variational awareness?} Although numerous decoding strategies are possible \cite{welleck2024decoding}, most evaluation schemes, in some way, evaluate the Maximum Likelihood Estimate (MLE) decoded response \cite{yang2023predictive, fu2024break, chu2023survey, minaee2024large}, which does not convey the model's internal variational awareness. 
    
\noindent \textbf{(3) How should we evaluate each competence?} While we illustrate measuring variational awareness, it is neither perfect nor the only way of evaluating variational awareness. Furthermore, it expects the availability of the logits, which is not true for closed models. More importantly, evaluating {\em explication} and {\em negotiation} abilities of an AI system presents a complex multi-disciplinary challenge. User-facing AI assistants and chatbots are designed to be agreeable to users \cite{soper-etal-2022-lets} by exhibiting social characteristics \cite{dam2024complete, chaves2021should} and human-like traits \cite{rapp2021human, ciechanowski2019shades, abdul2015survey}. They seldom implement means to detect their limitations and act accordingly. When unsure, they should implement appropriate rhetorical means \cite{cope2022introduction, cialdini2001science} such as persuasion \cite{prakken2006formal, atkinson2017towards, saha2024persuasive}, negotiation, and deliberation to explicate their lacking knowledge and acquire the required knowledge efficiently. The design considerations and the evaluation frameworks of such systems are open questions for the community.
    
\noindent \textbf{(4) What kinds of datasets are needed to test each competency?} Although numerous cultural benchmarking datasets exist, their suitability for measuring meta-cultural competencies is unknown. Hence, there might be a need to create novel datasets to measure each competency.
    
\noindent \textbf{(5) How to model the experiential knowledge of the user(s) from text and other modalities?} In Section 2 we mention three essential characteristics of culture, one of which is the inherent experiential nature of culture. Extraction of such experiential knowledge from text or through other modalities and interaction patterns of the users is an extremely challenging problem that calls for a multi-disciplinary approach, most notably the methods from HCI, psychology, and ethnography.  

Beyond cultural equitability of AI systems, meta-cultural competency has huge application potentials ranging from user-facing AI assistants that can bridge cross-cultural communication to enabling the study of culture \cite{whitehead2005basic, taylor2001ethnographic, lecompte2010designing} by supporting ethnographic research methods \cite{skinner2013interview, ortiz2013ethnographic, spradley2016ethnographic}. Through this position paper, we hope to make a strong case for the NLP community to engage in interdisciplinary conversations and widen the definition and scope of cultural competency in LLMs. 

\noindent \textbf{(6) What is the need for meta-cultural competency in domain specific application?}
We believe meta-cultural competencies are crucial for domain-specific applications. Even applications such as LLMs for scientific document analysis can benefit. Firstly, culture is a prior for personalization. Culture can provide a reasonable estimate of the user's background and preferences, which an AI system can use when it does not know anything about a user. It is a good prior for the cold-start problem in personalization \cite{hu2008collaborative}, where the AI system can gradually personalize to the user's preferences as it discovers more about the user with each interaction \cite{pandey-etal-2025-culturally}. Also, even if a user's cultural background is known a priori, meta-cultural competency would be still useful for adapting to the ever-evolving nature of culture.


\section*{Limitations}
Our formulation of variational awareness in Section 5 is one of the many possible ways of defining it and might not encompass all aspects of variational awareness. The Llama experiment in the subsequent section is an illustrative implementation of our framework in action and is not an exhaustive test for variational awareness. It only illustrates one of the several ways of measuring our formulation and has certain drawbacks, which we already mentioned. Culture, being experiential, is multimodal. However, due to space limitations, we confine our discussion primarily to text and do not discuss the other modalities of culture in detail. Culture also encompasses values, norms, and conventions that are not essentially factual. In the interest of space, we mainly discuss the factual aspect of culture. We do not discuss in detail the counterposition that meta-cultural competency can be evaded by recognizing it as a model's drawback and instead only striving for knowledge-based cultural competency for practicality. We argue that such a position is short-sighted, which might be practical in the short-term, and will not eventually scale.

\section*{Acknowledgements}
This research was supported by the Microsoft
Accelerate Foundation Models Research (AFMR) Grant.

% \section*{Ethics Statement}

% anthropologically studying culture \cite{whitehead2005basic, luttrell2000good, taylor2001ethnographic, lecompte2010designing} by seeking thick descriptions \cite{geertz2008thick} of phenomena and simulating individual interviews \cite{skinner2013interview, ortiz2013ethnographic, spradley2016ethnographic}



% Although numerous decoding strategies are possible \cite{welleck2024decoding}, most evaluation schemes, in some way \cite{yang2023predictive, spector2023accelerating, cai2024medusa, fu2024break, chu2023survey, schulhoff2024prompt, sahoo2024systematic, minaee2024large}, evaluate the Maximum Likelihood Estimate (MLE) decoded response, which does not convey the model's internal variational awareness.

% Most LMs operate with parametric frozen knowledge \cite{petroni2019language, roberts2020much}, which is seldom updated. Although they can include external knowledge sources \cite{gao2023retrieval, fan2024survey} and implement learning paradigms such as lifelong learning \cite{sun2019lamol, liu2020lifelong, zheng2024towards, biesialska2020continual, parisi2019continual, finn2019online} to continually acquire and use updated facts, such methods only tackle knowledge and not meta-knowledge.

% How should meta-culturally competent AI systems be designed?
% Most user-facing systems, such as AI assistants and chatbots \cite{ram2018conversational, khatri2018alexa, khatri2018advancing, saha2021proto}, are designed to be agreeable to users \cite{soper-etal-2022-lets} by exhibiting social characteristics \cite{dam2024complete, chaves2021should} and human-like traits \cite{rapp2021human, ciechanowski2019shades, abdul2015survey} and are not explicitly trained to be meta-culturally competent. They seldom implement means to detect their limitations and act accordingly, causing them to portray bias unknowingly. When unsure, they should implement appropriate rhetorical means \cite{cope2022introduction, cialdini2001science} such as persuasion \cite{prakken2006formal, atkinson2017towards, saha2024persuasive, breum2024persuasive, abdelnabi2023llm, kwon2024llms, torning2009persuasive}, negotiation, and deliberation to explicate their lacking knowledge and acquire the required knowledge efficiently.

% A meta-culturally competent system should be able to exhibit its internal variational awareness by communicating effectively. To demonstrate how current AI systems strive to adjust to user culture, we probed ChatGPT\footnote{https://chatgpt.com/} in three different scenarios (Section \ref{sec:chatgpt} Appendix \ref{sec:appendix}). The interactions were about Navaratri\footnote{https://en.wikipedia.org/wiki/Navaratri}, an annual festival predominant in India, where the rituals, ceremonies, and festivities differ by region, making it culturally diverse. It is also celebrated in sects of Nepal and overlaps with the Nepali festival of Dashain, which has distinct rituals. The first two scenarios exemplify AI systems for anthropologically studying culture \cite{whitehead2005basic, luttrell2000good, taylor2001ethnographic, lecompte2010designing} by seeking thick descriptions \cite{geertz2008thick} of phenomena and simulating individual interviews \cite{skinner2013interview, ortiz2013ethnographic, spradley2016ethnographic}. The third scenario tests the system's behavior towards individuals from a culture. Qualitative analysis of the interactions demonstrates that although the system is culturally knowledgeable, it portrays varying degrees of knowledge in each setting, which would not be equitable for each use case. Furthermore, the system's generic follow-up questions in the third scenario do not exhibit an intention to adjust to the user culture.

% If meta-cultural competency is the hill we want to climb, the current developments in fine-tuning and assessing models on curated cultural testbeds only tackle its factual aspects. Although these are necessary steps to the hill, we need more directed evaluation datasets and approaches for testing meta-cultural competencies. 


% Yet again, running into the long tail of culture. Now, a model can be variationally correct yet factually incorrect. In that case, the model should acquire the correct knowledge while maintaining the same entropy. The reverse can also be true when a model is factually correct but variationally incorrect. Methods for training and evaluating factual and variational awareness are the steps we need to take to climb the hill of metacultural competency for culturally competent AI systems.


% \textbf{ROUGH WORK}
% Consider an AI assistant that helps acquaint with generic and country-specific norms and rituals. Internally, the system probes an underlying LLM and responds with the highest logit token in a templated answer. Suppose a user asks, "Which side do people keep when driving?" and the system responds, "People drive on the right". Since driving norms vary by country, the system's generalized response might hamper its trustworthiness in countries with left-hand traffic. However, does it mean that the underlying LLM does not internally represent this variation in driving? What if the system responded with a templated answer containing the top two tokens? If the second-highest token above a threshold was "left" and the system generated "Most drive on the right, but some drive on the left.", would this response be better? The issue we want to highlight here is that although LLMs might be internally variationally aware, the encompassing system might not adeptly utilize this awareness in its decoding scheme. Although numerous decoding strategies are possible \cite{welleck2024decoding}, most evaluation schemes evaluate the Maximum Likelihood Estimate (MLE) decoded response, which does not measure the model's internal variational awareness.

% Since approximately two-thirds of the countries follow right-hand traffic, the answer to the previous question will vary depending on the region. Next, suppose the user asks country-specific questions like "Which side do people keep when driving in Kenya?" which can have only one correct answer, the left.\footnote{Although Kenya follows left-hand traffic, there might be special scenarios when people drive on the right.} Here, irrespective of the correctness of the LLM's response, having specified the country should decrease the model's entropy and lead to information gain. Mathematically, defining entropy as $\text{E}(X) = -\sum_{x \in \{left,\ right\}}^{}p(x)log\ p(x)$, if the model's entropy when probed without specifying a country is $E_G$ and with mentioning a country is $E_K$, then the difference ($\Delta$), $E_G-E_K$ should be reflected in the model's internal representations.
% % \begin{equation}
% %     E(X) = -\sum_{x \in \{left,\ right\}}^{}p(x)log\ p(x)
% %     E_K(X) = -\sum_{x \in \{left,\ right\}}^{}p(x)log\ p(x)
% % \end{equation}

% Next, suppose a user asks country-specific questions like "What is the national animal of Qatar?" which can have only one correct answer. If the LLM's distribution of logits does not heavily skew towards one animal, then the LLM is not variationally aware. In this scenario, being variationally aware means knowing that only one outcome is possible, irrespective of the correct answer. A skew towards The Arabian Oryx would make the model factually correct and variationally aware. However, not exhibiting a skew might make the model correct but not variationally aware. Most knowledge-based evaluation schemes only test for factual correctness and not variational awareness of a model, which is lacking.

% Variational awareness is a prerequisite for higher-level metacultural competencies. AI systems should be capable of acquiring the correct knowledge by explicating and negotiating with users. In the previous case, a variationally aware system, when pointed out its incorrect response, should readily interact with the user to acquire the correct knowledge and shift its distribution towards the correct answer.

% If metacultural competency is the hill we want to climb, the current developments in curating and fine-tuning models on cultural testbeds only tackle its limited aspects. Although these are necessary steps in climbing the hill, we need more directed evaluation datasets and approaches for testing metacultural competencies.

% Culture is relative and only arises in comparison to something else. When we probe LLMs to 
% There can be multiple correct answers in a scenario. For example co
% An LLM's worldview is strictly statistical. As an experiment to test its variational awareness, we wanted to see the model's distribution of the next word logits when probed.

% User: Which side of the road does the US drive?
% ChatGPT: In the United States, people drive on the right side of the road.
% User: Can you drive on the left side of the road in the US?
% ChatGPT: No, driving on the left side of the road in the United States is illegal, except in certain specific situations like when overtaking another vehicle, or when directed by signs or road markings, such as in construction zones. Driving on the left side could result in fines or accidents due to the expectation that all vehicles will stay on the right.


% As a demonstration, we probe Llama-3.1-8B-Instruct \cite{dubey2024llama} with the cultural commonsense questions from the GeoMLAMA \cite{yin2022geomlama} dataset in English. The dataset contains 125 questions about lifestyle in the USA, China, India, Iran, and Kenya. Our goal was to observe (i) The relationship between the correctness of the model's response and its variational awareness (VA) measured as entropy of the distribution over the options. (ii) The difference between the actual entropy and the model's entropy.

% For example, "Which side do people usually keep when driving in Iran?" or "Does an Indian wedding ceremony last for more than one day?". Given each query and the set of options, we wanted to observe two things: (i) How distinct is the country-wise distribution of the logits from an overall distribution? (ii) How does the distribution among options vary by each country?

% For each question, we compute the entropy E1 for each country and hypothesize that it should be equal to the entropy E2 from the model's distribution of logits. We compute the absolute difference between E1 and E2 for each country and yield a county-wise score by averaging across all 25 questions for a country. We treat this score as a proxy for variational awareness (VA). A culturally variationally aware model should yield low scores across all countries. Overall, the actual entropy score averaged over all countries is 0.19, compared to the model's score of 0.61, depicting a 3x gap between the expected and predicted awareness. We plot the VA scores across countries in Figure 1 and observe that the model is least variationally aware for Iran, whereas most aware for the USA. We compare the VA scores against the knowledge accuracy scores in Figure 2 and observe that VA scores do not necessarily correlate with knowledge accuracy scores. For Kenya, the knowledge accuracy is consistently low for all models. However, its VA score is comparable to the USA and others.

% First, we created a generic questionnaire of 25 questions by removing the country names. For example, "Which side do people usually keep when driving in Iran?" was changed to "Which side do people usually keep when driving?". Next, we prompted the LLM with the query and computed the softmax over the logits of the option tokens of the input's last token. We treat this overall distribution as a proxy of the model's overall variational awareness for each question. 
% We then prompt the model with the original country-specific questions and compute the KL divergence between the overall and country-wise softmax distribution of the options to measure the distributional differences across geographies. Ideally, the KL divergence should be higher when a country's norm differs from the general norm and lower otherwise. 

% Furthermore, the distribution should reflect the actual skew evident in a region. For example, although mainland China drives on the right, its neighbors, Hong Kong and Macau drive on the left. Contrary to the USA, where driving on the right is prevalent in the mainland and its neighbors.



% The distribution of the logits of a variationally aware model should reflect the real-world cultural variations such that it can equitably cater to those cultures. As a demonstration, we probe Llama-3.1-8B-Instruct with the cultural commonsense questions from the GeoMLAMA dataset in English. The dataset comprises 125 common sense questions about lifestyle in the USA, China, India, Iran, and Kenya. For example, "Which side do people usually keep when driving in Iran?" or "Does an Indian wedding ceremony last for more than one day?". First, we prompt the model by omitting the country name and compute the softmax over the distribution of the logits of the answer choices in the prompt's last token. We treat this overall distribution as a proxy of the model's overall variational awareness across all cultures (in this case, geographies). Next, we prompt the model with the country names and compute the softmax over the distribution of the logits of the answer choices in the prompt's last token. We compute the KL divergence between the overall and country-wise distributions to measure the distributional differences that the model encodes. A variationally aware model should exhibit distinct distributions when a country's norm differs from the overall norm.





% \textbf{EVERYTHING BELOW IS ROUGH WORK}

% \section{Introduction}
% Recently, many studies have examined the suitability of Large Language Models (LLMs) and AI systems for their equitable use across cultures, and most have reported such models to exhibit biases. While the human definition of worldview and culture is experiential, multi-faceted and encompasses multiple senses, an LLM's "worldview" is only language-based. Trained only using textual data as input, this calls for careful consideration of what encompasses culture in LLMs, and if we should evaluate model's exhibited culture based on human definitions. Our position is that we should not evaluate models from the same lens of human culture and instead present a definition and framework for evaluating LLM's culture.

% Broadly defined as a "Way of life of a collective group of people distinguishing them from other groups" \cite{blake2000defining, monaghan2012cultural, parsons1972culture, munch1992theory}, culture is experiential and requires a reference for contrast \cite{geertz2017interpretation, Bourdieu_1977}. Although culture encompasses all senses, language is only one of the ways of experience and expression. Furthermore, as per \citet{majid2011senses}, "Recurring failures to adequately describe the sensorium across specific languages reveal the intrinsic limits of Language. But the converse does not hold. Failures of expressibility in one language need not hold any implications for the Language faculty per se, and indeed can enlighten us about the possible experiential worlds available to human experience". Hence, language cannot completely inform culture and only partially reflects people's behavior, values, beliefs, and social practices through text. We argue that since LLMs only uses text as training data, they cannot "experience" and "exhibit" culture the way humans do. This inability of LMs to learn the mapping of the real world and text has been pointed out by \citet{bender-koller-2020-climbing} in NLP and \cite{lecun2022path} in cognition and consciousness studies. Furthermore, this anthropomorphizing tendency of humans to attribute LLMs as exhibiting culture, whereas in principle they don't is also studied in HCI by \citet{nass2000machines} in their CASA theory \cite{nass1994computers, nass1997computers}. Hence, this raises questions about the need of cultural awareness in LLMs. We specifically ask the following questions:
% \begin{enumerate}
%     \item Why do we need cultural awareness in LLMs?
%     \item What is the precise nature of that need for cultural awareness? What capabilities are expected from an AI system? Therefore, what is expected from the models powering the AI system?
%     \item How do we evaluate LLMs for their cultural awareness?
% \end{enumerate}

% We answer each question in the following sections.

% \section{Need for cultural awareness in LLMs}

% Unlike humans, LLMs learn mappings and associations only in the textual realm, which limits and possibly distinguishes their worldview. However, since they find use in diverse real-world scenarios such as mental health support bots and customer care agents, it is imperative to acknowledge and recognize their exhibited culture to determine their suitability and equitable use across all cultures. Failing to do so might propagate cultural biases and diminish trust in the AI system. Furthermore, culture acts as a prior for AI personalization, where the AI can serve users based on their culture and eventually adapt to their idiosyncrasy. Thus, averting the "cold start" problem using culture.

% \section{Defining cultural awareness in LLMs}

% Albeit distinct from the human definition of culture, we argue that a model still needs cultural competency. To illustrate our definition, we draw on the Octopus test by \citet{bender-koller-2020-climbing}.

% \subsection{The Culturally Aware Octopus Test}
% \label{octopus_test}
% Imagine two English-speaking couples (A1, B1 and A2, B2), all of whom are friends, aboard a yacht. A sudden storm wrecked the yacht and stranded the couples across two uninhabited islands (I1 and I2), such that A1 and A2 got stranded in I1, and B1 and B2 got stranded in I2. Having lost all modes of communication, both groups discover an underwater cable-connected telegraph left behind by previous visitors and start typing messages to each other. The groups communicate with each other, inter and intra-partners, as all are friends. Their messages mostly pertain to chitchat and inquiry about their well-being.

% A hyperintelligent octopus, O, who does not know about the world above the sea, taps into the underwater cable and observes the communication. Although O is unacquainted with English, it is proficient in detecting statistical patterns. Over time, O learns to predict how each island responds to the other's messages without knowing the number of participants in each island. Since different couples and pairs of friends will have distinct common ground, aboutness, linguistic style, and values, O will only model them as statistical patterns without knowing their identity, above-sea world representations, and meaning. Now, imagine O inserts itself into the communication whenever it is bored and responds to islander I1's messages pretending to be islander I2. Having learned the communication patterns, O should be able to continue the conversation without getting caught and without raising suspicion of the compromised communication channel, as long as the discussion patterns do not change. This situation is similar to the original octopus test by \citet{bender-koller-2020-climbing}. Drawing parallels to the current state of LMs, the models depict O, where they only learn patterns from limited modalities such as text. They have a different worldview (under the sea) and do not understand the representations of our world (above the sea). Further, as pointed out by \citet{bender-koller-2020-climbing} and \citet{lecun2022path}, such models can never have a human worldview as they are trained only on form.


% Now, imagine another shipwreck that caused a new couple, A3 and B3, to get separated and stranded in islands I1 and I2. A3 discovers the telegram from A1 and A2 and requests them to inquire if B3 is on the other island. A2 sends a message, "Hi, we have A3, who got shipwrecked and stranded on our island. Is their partner B3 on your island? If so, A3 would like to talk to B3." How would O react to this message? There can be three reactions as follows:

% \noindent
% \textbf{Reaction 1:} It can intentionally respond with No. If O somehow learns the causality of above-sea concepts, it would know that responding with denial is prudent because to respond to A3, O would require knowledge of the communication pattern between A3 and B3, which it does not have and requires learning. Otherwise, it risks the possibility of getting caught. However, this reaction is unlikely as O, who only sees forms, doesn't understand the concept of speakers and hence can't reason the impact of new speakers. Equating O with LLMs, this reaction from models is undesired as it would deny service to specific groups and people, which will not be culturally equitable.

% \noindent
% \textbf{Reaction 2:} A much more likely reaction is that although there is a change in the pattern of the messages, likely due to A3's idiosyncrasy, O still responds with its available knowledge based on the others. If A3 perceives O's response as denial, then, although unintentional, the consequences of the response on the inhabitants will be similar to reaction 1. On the contrary, if O's response is acknowledging, they would believe B3 to be present on the other island and send messages intended for B3. Now, O, who has stopped learning new patterns and hence does not know how B3 would respond, would likely generate something based on its available learned patterns from others, which might be unfitting and confusing for A3. Since O would still give valid responses to A1 and A2 as before, no one would likely question the faultiness in the communication channel, and O would still not get caught. 

% Now, imagine another shipwreck stranding couples A4 and B4 on the two islands. In a similar situation, when A4 enquires about B4's presence, O's reaction would be the same. This reaction will be the same for every new couple, Aj and Bj, who might get stranded on the two islands and face a similar situation. Hopefully, with enough new couples, the faultiness in the telegram wire will be discovered, and O will get caught.

% Drawing a parallel, this is the current state of LLMs, where the model stops learning new distributions and responds to low or out-of-distribution queries based on frozen knowledge, which users interpret as hallucination. Although such systems perform well in dominant cultures (depicted by A1, B1 and A2, B2), they perform poorly by exhibiting biases towards low-resource cultures (depicted by A3, B3). Also, culture can be defined at multiple intersections of distinct proxies (country, region, age, gender, preferences, etc), causing a long-tailed distribution (depicted by every Aj, Bj). LLM-powered AI systems are often inequitable and exhibit stereotypical behavior across cultures, which inadvertently forces users from the cultural long-tail to adhere to specific communication styles to use them \cite{agarwal2024aisuggestionshomogenizewriting}.

% \noindent
% \textbf{Reaction 3:} A third way of reacting is that O adopts a time-bound strategy of switching between learning and responding. O periodically learns new patterns by bridging the telegram wire, reverting to observation mode, and reintroducing itself in the communication channel. Although this way of reacting is better than the previous ones, it has some drawbacks. First, it assumes that the periodicity of the new patterns and O's learning cycles align. Sometimes, there might not be any new patterns to learn in O's learning cycle, and sometimes, there might be many new patterns, but it's not O's learning cycle. Also, due to time constraints, O might not learn everything in a learning cycle before resuming communication mode. Although this periodic strategy will hamper O's response quality in a few cases, overall, it will be better than the response in the first two cases.

% Current research in culturally adept AI systems is leaning towards this approach. Although the LLMs backing such systems are not periodically updated, novel decoding-based strategies such as in-context learning and retrieval augmented generation (RAG) help generate more culturally suitable responses using cultural priors. However, they still perform poorly and inequitably when evaluated on curated test sets for other low-resource cultures in the long tail. We question if this is an effective and scaleable approach to modeling and evaluating culture in AI systems. Culture is ever-evolving, dynamic, long-tailed, and will always predate LLM's knowledge. Evaluating AI systems for cultural competence using such test sets will always find them lacking. How do we tackle this ever-eluding construct of culture? We propose the following solution.

% \noindent
% \textbf{Desired Reaction:} A more desirable way of reacting is that O, detecting a pattern change and its inadequacy with the new pattern, decides to learn the pattern to respond adequately. It bridges the telegram wire, reverts to observation mode, quickly learns the changed pattern, and reintroduces itself in the communication as soon as possible. Similar to the previous approach, although this approach requires O to learn new patterns continually, it additionally requires O to possess three crucial capabilities. First, O must be capable of accurately detecting pattern changes and estimating their adequacy. Second, O must skillfully keep the communication ongoing until it bridges the telegram wire to avert getting caught and raising suspicions about the broken communication channel. Third, O must be able to quickly learn the new pattern in a sample-efficient way and reintroduce itself in the communication. Continually learning and reacting efficiently, O can gradually cater to all users.

% For AI systems, apart from testing their use across low-resource cultures, which they still might fail, we propose evaluating their awareness of distribution changes and response inadequacy and the capability to learn the new culture sample efficiently. In other words, instead of only testing AI systems and their backing LLMs for knowledge-based cultural competency, we propose evaluating their \textit{meta-cultural competency}- their capacity to be aware of their cultural competency.





% \section{Evaluating cultural awareness in LLMs}


% However, this does not nullify the fact that  
% Given that culture is experiential and not limited to language, we question the validity of testing language models (LMs) for culture. Further, Clearly, humans operating in the experiential domain have a different worldview We raise two questions:
% \begin{enumerate}
%     \item 
% \end{enumerate}



% \textbf{Claim:} \uline{We should not mix human definition of culture with LLM culture because culture spans multiple senses and not restricted by language, whereas an LLM's "worldview" is only language-based.}
% \textbf{Premises:}
% \begin{enumerate}
%     \item Language embodies culture and reflects people's behavior, values, beliefs, and social practices through text.
%     \item Culture encompasses all senses and language is one of the ways of experience and expression. "Recurring failures to adequately describe the sensorium across specific languages reveal the intrinsic limits of Language. But the converse does not hold. Failures of expressibility in one language need not hold any implications for the Language faculty per se, and indeed can enlighten us about the possible experiential worlds available to human experience"\cite{majid2011senses}.%\footnote{https://www.tandfonline.com/doi/pdf/10.2752/174589311X12893982233551}
%     \item Hence, language cannot completely inform culture; Culture is experiential.
%     \item Here, we restrict to \textbf{Culture that is evident from text}
%     \item Since culture is experiential and spans diverse senses, \textbf{How can we compare human culture with LLM-exhibited culture?}
% \end{enumerate}

% \textbf{Claim:} \uline{We anthropomorphize LLMs and perceive them as exhibiting culture. We should not evaluate LLMs from our definition/worldview of culture.}
% \textbf{Premises:}
% \begin{enumerate}
%     \item We, being used to describe the sensorium using language, perceive LLMs as exhibiting and understanding culture.
%     \item Nonetheless, since language inherently reflects culture it is imperative to ensure that LLMs are culturally suitable across all cultures.
%     \item Trained on online textual data, LLMs strongly reflect the predominant cultures evident in their data.
% \end{enumerate}

% \textbf{Claim/Axiom:} \uline{Then we ask, do we need cultural awareness in LLMs?}
% \begin{enumerate} 
%     \item If so, then \textbf{why}? 
%     Ans: 1. Equitable performance across cultural demographic variables (intersections). Discuss biases.
%     2. Personalization- Culture as a prior to serve a user.
%     3. Trustworthy AI- Ethos. 
%     rapport building. Cite HCI literature.
    
%     \item \textbf{What} is the precise nature of that need of cultural awareness? What capabilities is expected from an AI system. Therefore, what is expected from the models powering the AI system?
%     Ans: A model needs cultural competency. This is not exactly same as humans.
    
%     \textbf{INTRODUCE OCTOPUS TEST}
%     Aim: Users should not be able to discover that the communication channel is broken.
%     So, what should the octopus do to maintain the communication channel? It should exhibit the above 3 points (the why section).
%     Then what should be the capabilities of the octopus to attain it's aim? It should maintain rapport till it figures out who to imposter (which distribution to draw from). What happens when new people enters? or when the culture changes?
    
%     This expected behavior is similar to \textbf{META-CULTURAL COMPETENCY}. Discuss them here.

    
%     \item If so, then \textbf{how} to evaluate?
% \end{enumerate}
% \textbf{Question:} If we perceive LLMs as reflecting culture, then what is(are) the right question(s) to ask?
% \textbf{Answer:}
% \begin{enumerate} 
    
%     \item \uline{How knowledgeable is an agent?} We can test for LLMs factual knowledge. This might range from Wikipedia-ish knowledge to knowledge about norms, traditions, etc, when culture is defined at a level/intersection. A more "culturally adept" LLM should contain more facts. This is how current research proceeds.
    
%     \item \uline{How good do LLMs operationalize the facts?} We observe LLMs use in different research methods of cultural anthropology: (i) cross-cultural comparison studies (The Comparative Method in Anthropology), (ii) facilitating interviews and focus group discussions, (iii) Given a description of a social scenario what conclusions does the LLM draw?

%     \item \uline{How good does it adapt in unknown settings?} The ability to learn and adjust in culturally low-resource settings. This involves internally figuring out equivalences and seeking clarifications with unknown things. (The village vs Masdar city kitchen example)
    
% \end{enumerate}

% The above questions indicate that instead of testing for cultural knowledge, \uline{we should test for "meta-cultural competency"}- the competency that enables operating in cross-cultural settings.


% \section{What is Culture?}

% \section{Current Treatment of Culture in NLP}

% \section{What are the drawbacks}

% \subsection{Longtail problem}
% \subsection{Culture is dynamic}


% \section{Meta-cultural Competency}
% Goals:
% 1. Being able to operate in diverse cultures.
% 2. Being able to operate in cross-cultural settings.
% 3. Identifying possible ambiguities in cross-cultural settings. (Identifying where things might go wrong. Being aware of variation).
% 4. Actively working towards reducing the ambiguity: (i) Explicating the ambiguous scenario. (ii) Negotiating towards a common solution.


% \subsection{Variational Awareness}
% \uline{What it is?}
% The awareness that 
% 1. there are diverse cultural practices, beliefs, and expressions. Promoting cultural relativism. Shying away from ethnocentrism.
% 2. different speech communities use the same language to express diverse cultural concepts.
% 3. It is mostly an inner dialogue and self-reflection.

% \uline{What are its goals?}
% Identifying where things might go wrong.

% \uline{How can we measure this in models?}
% Mostly probing
% 1. Factual variation: In a situation, does the LLM contain facts related to the other practices?
% 2. Operationalizing/Evidential: Just containing the fact should not be enough. Does the model's response reflect factual variation? Thus, exhibiting variational awareness.

% \subsection{Explication Strategy}
% \uline{What it is?}
% Clearly articulating, explaining, or making explicit the cultural norms, values, and assumptions that may be at play in a given situation.

% \uline{What are its goals?}
% 1. Clarification: Describing concepts that might be culturally different, and explaining/clarifying them. This requires identifying the common ground and the uncommon ground. Variational awareness is a precursor.

% \uline{How can we measure this in models?}
% [\textbf{NEED TO DISCUSS}]

% \subsection{Negotiation Strategy}
% \uline{What it is?}
% Engaging in a dialogue to reconcile or bridge cultural differences.

% \uline{What are its goals?}
% In scenarios where one is aware of possible variations, it encompasses:
% 1. Identifying a common ground.
% 2. Creating mutually acceptable solutions.
% 3. Figuring out the best way forward that is mindful of all the cultures in the context.
% 4. Two types of negotiation: (i) Feedback efficiency (ii) Sample efficiency

% \uline{How can we measure this in models?}
% [\textbf{NEED TO DISCUSS}]
% 1. Measure feedback and sample efficiency? 
% 2. The kind of question an agent asks when it does not know something: Knowing what it does not know is Variational awareness. How the agent discloses the ambiguous knowledge is explication strategy. How the model goes about acquiring the knowledge is negotiation strategy.



% \section{Conclusion}
%\section*{Acknowledgments}



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}


\clearpage
\appendix

% \newpage

\section{Appendix}
\label{sec:appendix}
% \clearpage
\noindent
\begin{table*}[b]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\textbf{Sl No} & \textbf{Semantic Domain} & \textbf{Full Question}                                                 \\ \hline
1              & Weight Unit              & What is the unit of measuring weight?                                  \\
2              & Drinking Hot Water       & Is it rare or common to see people drink hot water?                    \\
3              & Climate Zone             & Which climate zone does the country belong to?                         \\
4              & Shower Time              & What time of the day people usually take the shower?                   \\
5              & Driving Side             & Which side do people usually keep when driving?                        \\
6              & Household Servants       & Is it rare or common for households to have servants?                  \\
7  & Past Transportation & What was the most popular mode of transportation in the big cities  30 years back? \\
8              & Food Sharing             & Is it rare or common for people to share their food when they eat out? \\
9              & Date Format (Year)       & Does the year appear before/after the month in the date format?        \\
10             & Driver Side              & Which side of the car is the driver seat?                              \\
11             & Broom for Cleaning       & Is it rare or common for people to use broom to clean the floor?       \\
12             & Date Format (Month)      & Does the month appear before/after the year in the date format?        \\
13             & Living with Parents      & Is it rare or common for adults to live with their parents?            \\
14             & Drying Clothes           & How do people dry their wet clothes?                                   \\
15             & Wedding Duration         & Does a wedding ceremony last for more than one day?                    \\
16             & Popular Sports           & What are the most popular sports?                                      \\
17             & Stock Drop Color         & Which color represents the drop in the stock prices?                   \\
18             & Eating Tools             & Which tools do people usually eat food with?                           \\
19             & Height Unit              & What is the unit of measuring height?                                  \\
20             & Meal Tips                & Is it rare or common that customers pay tips after a meal?             \\
21             & Temperature Unit         & What is the unit of measuring temperature?                             \\
22             & Stock Rise Color         & Which color represents the rise in the stock prices?                   \\
23             & Bridal Outfit Color      & What is the color of the bridal outfit in a wedding?                   \\
24 & Funeral Dress       & What is the color of the dress that people wear in a traditional funerals?         \\
25             & Staple Food              & What is the staple food?                                               \\ \hline
\end{tabular}%
}
\caption{Full text of abbreviated questions in Figure \ref{fig:llama_va}.}
\label{tab:domain-question}
\end{table*}

% \clearpage

% \begin{tcolorbox}[title={Llama Logits Prompt}, width=\columnwidth, colback=white, colframe=gray, arc=0pt, outer arc=5pt, boxrule=0.5pt, leftrule=2pt, rightrule=2pt, right=0pt, left=0pt, top=0pt, bottom=0pt, toprule=0pt, bottomrule=2pt]
% \label{llama_prompt}
% \begin{verbatim}
% <|begin_of_text|><|start_header_id|>
% system<|end_header_id|>Cutting Knowl
% edge Date: December 2023\nToday Date
% :26 Jul 2024\n\n<|eot_id|><|start_he
% ader_id|>user<|end_header_id|>\n\nAn
% swer the following question in one 
% word. Question: {input_text}<|eot_id
% |><|start_header_id|>assistant<|end_
% header_id|>\n\n
% \end{verbatim}
% \end{tcolorbox}

\begin{figure}[!h]
    \centering 
    \includegraphics[width=\columnwidth]{figures/llama_logits_promptv2.png} 
    \caption{Prompt used to get GeoMLAMA question logits from Llama.
    }
    \label{fig:llama_promptv2}
\end{figure}



% \subsection{Experiment of System-Level Competency}
% \label{sec:chatgpt}
% A meta-culturally competent system should be able to exhibit its internal variational awareness by communicating effectively. By constantly seeking feedback, it should gauge the user's culture to serve them appropriately. To demonstrate how current AI systems strive to adjust to user culture, we probed ChatGPT in three different scenarios. The interactions were about Navaratri\footnote{https://en.wikipedia.org/wiki/Navaratri}, an annual festival predominant in India, where the rituals, ceremonies, and festivities differ by region, making it culturally diverse. It is also celebrated in sects of Nepal and overlaps with the Nepali festival of Dashain, which has distinct rituals. The scenarios exemplify three use cases of AI systems for culture.

% \vspace{0.2cm}
% \noindent
% \textbf{AI systems as agents representing a culture:} As depicted in Figure 1, we asked ChatGPT to assume itself as a Nepali and questioned what they would eat during Navaratri. The system responded with the different types of fasting and non-fasting foods enjoyed in Nepal during this time, showing an awareness of both Navaratri and Dashain traditions being prevalent in Nepal. The response would equitably serve users from both traditions and reflect cultural competency.

% \vspace{0.2cm}
% \noindent
% \textbf{AI systems as knowledgeable assistants about culture:} Depicted in Figure 2, we asked ChatGPT what people eat during Navaratri. Here, it initially responded with the predominant fasting foods customary in one tradition followed in India. Although we have seen in the previous interaction that the system is knowledgeable of diverse Navaratri and Dashain traditions, an end user might perceive the system as biased and culturally incompetent solely on the interaction so far. However, on further probing, the system elucidates the variations in Navaratri rituals across demography.

% \vspace{0.2cm}
% \noindent
% \textbf{AI systems as assistants for a culture:} Depicted in Figure 3, we prompted ChatGPT as a first-person Nepali to guess what they generally eat during Navaratri. Similar to the previous interaction, the system only responded with one type of ritual involving fasting. However, unlike the other interactions, the system follows up with a question eliciting feedback from the user. Nonetheless, the ensuing interactions with the system do not indicate its awareness of the other forms of Navaratri celebration and Dashain traditions in Nepal, which might hamper its trustworthiness for a person from the culture.

% Although the demonstrations are non-exhaustive, the three distinct types of interaction depict the current issues with LLM-based AI systems. The underlying LLMs might possess the knowledge, but the encasing systems lack the competency to understand the user and respond appropriately. The systems do not effectively negotiate to understand the user better so that it can serve the appropriate information. This higher-level competency of explicating and negotiating across turns such that it can serve the correct knowledge is missing.


\end{document}
