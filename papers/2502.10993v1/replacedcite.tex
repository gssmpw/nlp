\section{Related Work}
Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by integrating non-parametric knowledge to improve generation quality. 
Early approaches~\cite {izacard2022atlas, shi2023replug} treat LLMs as passive consumers of retrieved information, assuming that retrieval inherently improves generation. However, recent studies____ highlight that retrieved knowledge is often noisy, incomplete, or even misleading, which can hurt rather than enhance the performance LLMs. 


To better align the retrieved information with LLMs, ____ propose initial assessments to determine whether retrieval is necessary, using either BERT-family models or the frozen LLM as a classifier. However, these approaches depend on classifier accuracy and does not improve the LLM’s inherent ability to handle noisy information. BlendFilter____, ASTUTE____, and RobustRAG____ leverage the LLM itself to analyze or filter irrelevant information. However, their effectiveness is contingent on the model’s capability; small-scale LLMs often fail to achieve reliable results due to their limited intelligence. 
Another line of research____ investigates training LLMs to handle noisy contexts, e.g., those retrieved from external corpora. These approaches typically leverage powerful models such as GPT-4 or require extensive labeled data from auxiliary tasks to generate high-quality responses. However, such reliance on large-scale models and costly annotations limits scalability and practicality, particularly for resource-constrained applications. 
Very recently, 
InstructRAG____ instructs LLMs to provide rationales linking answers to retrieved passages, but pure supervised fine-tuning cannot fully unleash the model's reasoning capability towards RAG applications.  
KnowPO____ and DPA-RAG____ optimize model preferences to improve noisy information analysis. Yet, they still require access to powerful LLMs (e.g. GPT-4) to create preference data. 
Unlike existing approaches, {\name} is specifically designed to enhance the robustness of SLMs against noisy retrieved information through margin-aware preference optimization, eliminating the need for additional classifiers or high-resource LLMs.