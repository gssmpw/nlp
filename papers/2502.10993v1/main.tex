\documentclass[11pt]{article}
\usepackage[top=1.0in, bottom=1.0in, left=1.12in, right=1.12in]{geometry}
\usepackage{authblk}
% \usepackage[T1,T2A]{fontenc}
% \usepackage[english,russian]{babel}
% \PassOptionsToPackage{table,xcdraw}
\usepackage{colortbl}
\usepackage{caption}
% \usepackage{cite}
\usepackage{microtype}
\usepackage{graphicx} % Required for inserting images
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{arydshln}
\usepackage{url}
\usepackage{amsmath} 
\usepackage{array}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbding}
\usepackage{bm}
\usepackage{amsthm}  
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{float}
\usepackage{physics}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage[most]{tcolorbox}
\usepackage{lineno}
\usepackage{siunitx}
% \newcommand{\name}{\textsc{RoseRAG}}
\newcommand{\name}{\textsc{RoseRAG}}
\usepackage{tablefootnote}
\usepackage{wrapfig}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}

% Fix 
\usepackage[utf8]{inputenc}
\usepackage[russian,main=english]{babel}  % English as main language
\usepackage[T1,T2A]{fontenc}
\usepackage{alphabeta}

% Directly set the proof name
\renewcommand{\proofname}{Proof}

% Additional caption fixes
\addto\captionsenglish{%
  \renewcommand{\refname}{References}%
  \renewcommand{\bibname}{References}%
}

% Ensure English is the main language
\selectlanguage{english}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]

\newcommand{\df}{\mathrm{d}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\odds}{\text{odds}_{\theta}}
\newcommand{\E}{\text{Exp}}

\input{commands}



\title{{\name}: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization}

\author[$\bullet \S$]{Tianci Liu}
\author[$\bullet \star$]{Haoxiang Jiang}
\author[$\dagger$]{Tianze Wang}
\author[$^*$]{Ran Xu}
\author[$\diamond$]{Yue Yu}
\author[$\dagger$]{Linjun Zhang}
\author[$\diamond$]{Tuo Zhao}
\author[$\heartsuit$]{Haoyu Wang}
\affil[$\star$]{Independent Researcher}
\affil[$\S$]{Purdue University}
\affil[$\dagger$]{Rutgers University}
\affil[$^*$]{Emory University}
\affil[$\diamond$]{Georgia Institute of Technology}
\affil[$\heartsuit$]{SUNY Albany}
\affil[$\S$]{\texttt{liu3351@purdue.edu}}
\affil[$\star$]{\texttt{HaoxiangJ03@gmail.com}}
\affil[$\heartsuit$]{\texttt{hwang28@albany.edu}}
\date{}
\begin{document}

\maketitle
\def\thefootnote{$\bullet$}\footnotetext{These authors contributed equally to this work}\def\thefootnote{\arabic{footnote}}
\begin{abstract}
Large language models (LLMs) have achieved impressive performance but face high computational costs and latency, limiting their deployment in resource-constrained settings. In contrast, small-scale LLMs (SLMs) are more efficient yet struggle to capture evolving real-world knowledge. Retrieval-augmented generation (RAG) helps by integrating external knowledge, but imperfect retrieval can introduce distracting noise that misleads SLMs. 
We propose {\name}, a robust RAG framework for SLMs via Margin-aware Preference Optimization. 
{\name} employs multi-turn prompting for detailed reasoning, rejection sampling for high-quality explanations, and contrastive preference selection to refine responses by maximizing the likelihood gap between preferred and non-preferred outputs.
By integrating these components into a margin-aware optimization process, {\name} robustly enhances the accuracy and reliability of SLMs for RAG applications. Extensive experiments on three open-domain question answering benchmarks indicate that our innovative {\name} surpasses state-of-the-art baselines significantly.
\end{abstract}

\section{Introduction}
Large language models~(LLMs) have demonstrated remarkable capabilities in a wide array of natural language processing tasks~\citep{achiam2023gpt,team2024gemma,dubey2024llama,guo2025deepseek}. However, these powerful models are typically large-scale, requiring substantial computational resources for training, and often incurring high latency during 
\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.48\columnwidth}
    \includegraphics[width=\linewidth]{intro1.pdf}
    \caption{}
    % \caption{Ground Truth Documents with Varying Numbers of Noisy Documents}
    \label{fig:ground_truth}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\columnwidth}
    \includegraphics[width=\linewidth]{intro2.pdf}
    \caption{}
    % \caption{Performance w.r.t. Varying Numbers of Retrieved Documents}
    \label{fig:retrieved_docs}
  \end{subfigure}
  \caption{Pilot studies. Fig.~\ref{fig:ground_truth}: Ground Truth Documents with varying amounts of noisy documents. Fig.~\ref{fig:retrieved_docs}: Performance w.r.t. varying numbers of retrieved documents. Both the two sub-figures are results with Qwen2.5-1.5B-Instruct on HotpotQA.\vspace{-1ex}}
  \label{fig:performance}
\end{figure}
inference~\citep{zhou2024survey}. Such limitations serve as a key hurdle that prevents these models from being deployed in real practice.
In contrast, small-scale LLMs (SLMs) offer a viable alternative by providing high utility while remaining computationally efficient and easier to deploy in resource-constrained environments~\citep{lu2024small,vernikos2024small}.


Despite their efficiency, SLMs are fundamentally constrained by their limited capacity. During pre-training, they cannot fully capture the vast and continuously evolving body of real-world knowledge~\citep{ovadia-etal-2024-fine}. As a result, SLMs often struggle in real-world scenarios where accurate responses depend on newly emerging or dynamically updated information. Retrieval-augmented generation (RAG)~\citep{lewis2020retrieval} mitigates this limitation by retrieving a top-$K$ set of semantically relevant documents at inference time, which are then conditioned upon during response generation. By decoupling knowledge retrieval from parametric memory, RAG enhances the adaptability of SLMs, improving response fidelity while obviating the need for expensive and frequent model retraining~\citep{wang2023self,asai2024selfrag,xu2024simrag}.
However, one inherent challenge for RAG pipelines is the \emph{imperfect retrieval information}. The top-$K$ documents returned by a retriever may include irrelevant or noisy information, which can mislead small-scale LLMs that are not robust enough to effectively filter out such distractions. As shown in Fig.~\ref{fig:performance}, the susceptibility of SLMs to retrieval noise highlights a critical bottleneck in RAG pipelines. 
This limitation highlights the necessity of developing more robust mechanisms to enhance SLMs' resilience against spurious retrieval artifacts and improve their reliability in downstream tasks.



Several existing works have sought to enhance the robustness of RAG systems. 
\emph{Prompting-based approaches} \citep{wang2024astute, wang2024blendfilter} attempt to mitigate noise by instructing the model to disregard irrelevant information during inference. However, these techniques heavily depend on the model's intrinsic ability to distinguish useful from spurious content—a capability that SLMs often lack due to their limited reasoning and generalization capacity. 
On the other hand, \emph{Fine-tuning-based strategies} \citep{wei2024instructrag, yoranmaking} aim to improve retrieval robustness by training models on curated, denoised datasets using supervised fine-tuning (SFT).
Unfortunately, SFT tends to mimic the behavior present in the training data, making it highly sensitive to noise and prone to overfitting~\citep{chu2025sft}, which ultimately limits the generalization ability of the model.

In light of these challenges, we propose \textbf{Ro}bu\textbf{s}t R\textbf{e}t\textbf{r}ieval-\textbf{a}ugmented \textbf{G}eneration with Small LLMs via Margin-aware Preference Optimization~({\name}). {\name} introduces a novel framework that robustly aligns SLMs with high-quality responses \emph{without distillation from teacher LLMs} through a three-stage process: \emph{preference data generation}, \emph{contrastive preference data selection}, and \emph{margin-aware preference optimization}. 
Specifically, we employ a multi-turn prompting strategy to generate detailed rationales, coupled with rejection sampling~\citep{stiennon2020learning,guo2025deepseek} to filter out spurious reasoning, thereby mitigating the influence of noisy retrieved documents. 
Besides, our contrastive selection mechanism identifies the most \textit{challenging} response pairs, enabling the model to explicitly maximize the margin between preferred and non-preferred outputs. 
By integrating these components into a unified optimization framework, {\name} significantly improves the reliability and accuracy of small-scale LLMs in retrieval-augmented settings, especially under imperfect retrieval conditions that mirror real-world scenarios. 


Our contributions are summarized as follows:
\begin{itemize}[leftmargin=1em]
\item We propose {\name}, a novel RAG framework that enhances SLM robustness against noisy retrieval using margin-aware preference optimization, reducing dependence on distillation from stronger models. 
\item We propose a multi-turn prompting strategy coupled with rejection sampling to generate and filter robust reasoning rationales, for boosting the quality of preference data. 
\item We design a contrastive preference data selection scheme to maximize the margin between chosen and rejected responses, leading to more discriminative and generalizable model behavior. 
\item We conduct extensive experiments to demonstrate that RoseRAG significantly improves response quality in retrieval-augmented settings, paving the way for more effective deployment of small-scale LLMs in real-world applications. \end{itemize}



\section{Related Work}

Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by integrating non-parametric knowledge to improve generation quality. 
Early approaches~\cite {izacard2022atlas, shi2023replug} treat LLMs as passive consumers of retrieved information, assuming that retrieval inherently improves generation. However, recent studies~\citep{wang2023self, li2023llatrieval, yu2024rankrag} highlight that retrieved knowledge is often noisy, incomplete, or even misleading, which can hurt rather than enhance the performance LLMs. 


To better align the retrieved information with LLMs, \citet{wang2023self,jeong2024adaptive} propose initial assessments to determine whether retrieval is necessary, using either BERT-family models or the frozen LLM as a classifier. However, these approaches depend on classifier accuracy and does not improve the LLM’s inherent ability to handle noisy information. BlendFilter~\citep{wang2024blendfilter}, ASTUTE~\citep{wang2024astute}, and RobustRAG~\citep{xiang2024certifiably} leverage the LLM itself to analyze or filter irrelevant information. However, their effectiveness is contingent on the model’s capability; small-scale LLMs often fail to achieve reliable results due to their limited intelligence. 
Another line of research~\citep{fang2024enhancing, yoranmaking, yu2023chain, yu2024rankrag} investigates training LLMs to handle noisy contexts, e.g., those retrieved from external corpora. These approaches typically leverage powerful models such as GPT-4 or require extensive labeled data from auxiliary tasks to generate high-quality responses. However, such reliance on large-scale models and costly annotations limits scalability and practicality, particularly for resource-constrained applications. 
Very recently, 
InstructRAG~\citep{wei2024instructrag} instructs LLMs to provide rationales linking answers to retrieved passages, but pure supervised fine-tuning cannot fully unleash the model's reasoning capability towards RAG applications.  
KnowPO~\citep{zhang2024knowpo} and DPA-RAG~\citep{dong2024understand} optimize model preferences to improve noisy information analysis. Yet, they still require access to powerful LLMs (e.g. GPT-4) to create preference data. 
Unlike existing approaches, {\name} is specifically designed to enhance the robustness of SLMs against noisy retrieved information through margin-aware preference optimization, eliminating the need for additional classifiers or high-resource LLMs.




\section{Preliminary}
In this section, we introduce the concept of standard retrieval-augmented generation~(RAG) and monolithic preference optimization~(ORPO).
\subsection{Retrieval-augmented Generation}
Given a pre-trained Large Language Model (LLM) $\mathcal{M}_{\theta}$, a knowledge base $\mathcal{K}=\{\mathcal{K}_{i}\}_{i=1}^{k}$ (where $k$ represents the number of documents), a retriever $\mathcal{R}(\cdot)$, and a query $q$, the vanilla RAG, i.e. retrieve-then-generate, is to retrieve top-$K$ related documents from the knowledge base first and then generate answer based on retrieved information, which can be formulated as
\begin{align}
\label{eq:vanilla_rag}
    \notag\mathcal{K}_{q}&=\mathcal{R}(q,\mathcal{K};K),\\
     y\sim P_{\theta}(y&|\texttt{Prompt}_{\texttt{CoT}}(q,\mathcal{K}_{q})),
\end{align}
where $y$ and $\texttt{Prompt}_{\texttt{CoT}}(\cdot)$ represent the generated response and the chain-of-thought~(CoT) prompt, respectively.
\subsection{Monolithic Preference Optimization}
Preference alignment for large language models has traditionally relied on multi-stage procedures—such as reinforcement learning from human feedback (RLHF)~\cite{ouyang2022training,bai2022training}—that require an additional reference model to guide and stabilize training. In contrast, ORPO~\cite{hong2024orpo} is a monolithic approach that integrates preference alignment directly into the supervised fine-tuning (SFT) phase, thereby obviating the need for a separate reference model. ORPO augments the standard negative log-likelihood loss with an odds ratio-based penalty that contrasts the probabilities of generating a \emph{chosen} (preferred) response and a \emph{rejected} (non-preferred) response. Specifically, given an input \(x\) and corresponding responses \(y_w\) (chosen) and \(y_l\) (rejected), the odds of generating a response are defined as $\text{odds}_\theta(y \mid x) = \frac{P_\theta(y \mid x)}{1 - P_\theta(y \mid x)}$,
% \[
% \text{odds}_\theta(y \mid x) = \frac{P_\theta(y \mid x)}{1 - P_\theta(y \mid x)},
% \]
and the odds ratio is given by $\text{OR}_\theta(x, y_w, y_l) = \frac{\text{odds}_\theta(y_w \mid x)}{\text{odds}_\theta(y_l \mid x)}$.
% \[
% \text{OR}_\theta(x, y_w, y_l) = \frac{\text{odds}_\theta(y_w \mid x)}{\text{odds}_\theta(y_l \mid x)}.
% \]
The overall loss function is formulated as
\begin{align}
\notag&\ell_{\mathrm{ORPO}}(x, y_w, y_l) = \ell_{\mathrm{SFT}} \\&\quad + \beta \left( -\log \sigma\!\left(\log \text{OR}_\theta(x, y_w, y_l)\right) \right),
\end{align}
where \(\ell_{\mathrm{SFT}}\) is the conventional supervised fine-tuning loss, \(\sigma(\cdot)\) denotes the sigmoid function, and \(\beta\) is a hyperparameter that regulates the strength of the preference alignment signal. By explicitly encouraging a larger margin between the chosen and rejected responses, ORPO enables more stable gradient updates and improved alignment performance, as evidenced by its strong empirical results on benchmarks such as AlpacaEval~\cite{alpaca_eval,dubois2024length,dubois2023alpacafarm} and MT-Bench~\cite{zheng2023judging}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\columnwidth]{framework2.pdf}
    \caption{Framework of proposed {\name}.}
    \label{fig:framework}
\end{figure}

\section{Method}
Our {\name} enhances retrieval-augmented generation for SLMs through margin-aware preference optimization. It consists of three key stages, as shown in Fig.~\ref{fig:framework}: (1) \emph{Preference Data Generation}, where the model is prompted with retrieved knowledge and ground-truth answers to generate rationales, filtered via rejection sampling; (2) \emph{Preference Data Selection}, which employs a contrastive strategy to maximize the margin between the least likely chosen response and the most likely rejected response; and (3) \emph{Preference Optimization}, where the model is trained using an ORPO loss. This unified framework effectively aligns SLM outputs with high-quality responses, thereby improving model robustness in retrieval-augmented settings.








% RoseRAG leverages margin-aware preference optimization to improve retrieval-augmented generation for SLMs. 
% It comprises three steps. First, in the \emph{Preference Data Generation} stage, we prompt the model with retrieved knowledge and the ground-truth answer to elicit a detailed rationale, and then apply rejection sampling to retain only those rationales that yield the correct answer. Second, in the \emph{Preference Data Selection} stage, we employ a contrastive strategy to select, among multiple candidates, the chosen response with minimal likelihood and the rejected response with maximal likelihood, thereby maximizing the margin between them. Finally, in the \emph{Preference Optimization} stage, we optimize the model using an ORPO loss. Together, these components form a unified framework that aligns generations from SLMs with desired responses.
\subsection{Preference Data Generation}
In order to enable our model to accurately evaluate the relevance of retrieved documents and generate reliable rationales, we design a multi-turn prompting strategy that encourages the model to articulate its reasoning process. The core idea is to guide the model into providing a concise, step-by-step explanation for arriving at an answer by explicitly informing the model that the given knowledge may consist of irrelevant information, while also increasing the likelihood of correctness by including the ground-truth label in the prompt. By explicitly supplying the ground-truth answer, the model is pushed toward generating a rationale that is consistent with the expected output, thus facilitating the subsequent preference optimization.

We instantiate this process by constructing a prompt composed of a system message, a user message, and an initial assistant message. The system message defines the task and constraints, the user message provides a set of retrieved knowledge documents $\mathcal{K}_{q}$, a question $q$, and the ground-truth answer $a^{*}$, and the assistant message seeds the reasoning process. The prompt is presented in Fig.~\ref{fig:rationale}. Given these messages, the model generates a context \(r\) that encapsulates its analytical reasoning. To mitigate the risk of incorrect analysis, we employ rejection sampling~\cite{liu2023statistical,guo2025deepseek}: we sample an answer \(a \sim P_{\theta}(y \mid q, r)\) and compare it with the ground-truth label. If \(a\) matches the ground truth, the generated context \(r\) will be retained as the chosen response \(y_{w}\); otherwise, it will be filtered. For the rejected response \(y_{l}\), we adopt the vanilla RAG response (as defined in Eqn.~\eqref{eq:vanilla_rag}). This strategy not only leverages the ground-truth label to promote the generation of correct rationales, but also filters out spurious reasoning. Therefore, it does not depend on expensive close-sourced powerful LLMs like ChatGPT~\cite{ouyang2022training}, Gemini~\cite{geminiteam2024geminifamilyhighlycapable}, and Claude~\cite{bai2022training}, and can enable SLMs to produce high-quality preference data as well.
\begin{figure}[htbp]
\centering
\begin{tcolorbox}[
    colback=gray!15,
    colframe=gray!75,
    title=Rationale Generation,
    fonttitle=\large\bfseries\sffamily\color{white},
    coltitle=white,
    bottomrule=0pt,
    toprule=0pt,
    leftrule=0pt,
    rightrule=0pt,
    rounded corners,
    % width=0.9\linewidth
]
\textbf{System Prompt:} You are a useful assistant. I will provide one question, 
several pieces of knowledge (which may be related or unrelated to the question), 
and the answer to the question. Please explain your reasoning process in a single 
paragraph consisting of no more than four sentences. If the provided knowledge is 
insufficient, you may make an informed guess, but do not respond with "Unknown".

\medskip
\textbf{User Prompt:} Knowledge: $\mathcal{K}_{q}$\\
Question: $q$\\
Answer: $a^{*}$

\medskip
\textbf{Assistant Prompt:} Let's think step by step.

\medskip
\textbf{Output:} \{rationale $r$\}
\end{tcolorbox}
\caption{Overview of the rationale generation process.}
\label{fig:rationale}
\end{figure}

\subsection{Preference Data Selection}
We propose a preference data selection method based on contrastive learning~\cite{tian2020makes,wang2022contrastive,cui2021parametric,chen2020simple,li2020prototypical}, designed to improve the model’s ability to distinguish between chosen and rejected responses. The underlying intuition is that explicitly maximizing the margin between the preferred (chosen) and non-preferred (rejected) outputs forces the model to learn more discriminative representations, thereby enhancing its generalization capability.

In practice, after the Preference Data Generation step, for one query \(q\) we will have \(n\) candidate chosen responses \(\{y_w^1, y_w^2, \dots, y_w^n\}\) and \(m\) candidate rejected responses \(\{y_l^1, y_l^2, \dots, y_l^m\}\). Each candidate is evaluated by its likelihood under the initial model \(\theta\), denoted as \(P_\theta(y \mid \texttt{Prompt}_{\texttt{CoT}}(q,\mathcal{K}_{q}))\). To maximize the contrast between the two sets, we select the candidate chosen response that the model is least confident about, and the candidate rejected response that it is most confident with~\cite{robinsoncontrastive}. Formally, the final selected responses are defined as:

\begin{equation}\label{eq:pref selection}
    \begin{aligned}
       & y_w^* = \arg\min_{1 \leq i \leq n} P_\theta(y_w^i \mid \texttt{Prompt}_{\texttt{CoT}}(q,\mathcal{K}_{q})), \\
       & y_l^* = \arg\max_{1 \leq i \leq m} P_\theta(y_l^i \mid \texttt{Prompt}_{\texttt{CoT}}(q,\mathcal{K}_{q})).
    \end{aligned}
\end{equation}
% \begin{align}
% \label{eq:pref selection}
%         y_w^* = \arg\min_{1 \leq i \leq n} P_\theta(y_w^i \mid q), \\
%    \notag y_l^* = \arg\max_{1 \leq i \leq m} P_\theta(y_l^i \mid q).
% \end{align}

The motivation behind this selection strategy is two-fold. First, by choosing the chosen response with the minimal likelihood, we identify cases where the model struggles to assign high confidence to the correct answer; such instances provide a strong corrective signal that refines the model’s understanding of the desired output. Second, by selecting the rejected response with the maximal likelihood, we target cases where the model erroneously favors an undesirable output. This contrastive selection process accentuates the differences between correct and incorrect responses, thereby forcing the model to maximize the margin between them and promoting a more robust and effective preference optimization. Moreover, by ensuring that the selected rationales yield the correct answer, we minimize false positives, further strengthening the overall training signal.



\subsection{Preference Optimization}
Given a preference tuple \((x, y_w^{*}, y_l^{*})\), where \(x\) is the input prompt $\texttt{Prompt}_{\texttt{CoT}}(q,\mathcal{K}_{q})$, \(y_w^*\) is the chosen (preferred) response, and \(y_l^*\) is the rejected (non-preferred) response, our objective is to minimize the ORPO loss:
\begin{equation}\label{eq:orpo}
    \min_{\theta}\ \bE_{(x,y_w^*, y_l^*)} \ell_{\mathrm{ORPO}}(x, y_w^{*}, y_l^{*}).
\end{equation}
The \(y_w^{*}\), and \(y_l^{*}\) are obtained from our preference data generation and selection steps as stated before.



\section{Theoretical Analysis}
In this section, we provide a theoretical analysis to support the effectiveness of {\name}. We derive a closed-form solution for the model learned by {\name} and examine how the proposed preference selection strategy influences the model's behavior.


We begin with a regularity condition ensuring that the optimization space is sufficiently expressive to achieve the global optimum.
\begin{assumption} \label{assump:large space}
    Assume that $P(y|x)$ belongs to the optimization space $\{P_\theta: \theta\in \Theta\}$ such that 
    \begin{equation*}
        P(y|x) = \exp(Z(x))/\left(\frac{q_l^*(y|x)}{q_w^*(y|x)} + \exp(Z(x))\right),
    \end{equation*}
    where $q_w^*(y|x)$ and $q_l^*(y|x)$ denote the distribution of $y_w^*$ and $y_l^*$ given the prompt $x$ respectively.
 \end{assumption}
\noindent This condition ensures that  $P_\theta$ can attain an optimal solution, allowing us to express the optimizer in a closed form.
\begin{lemma}
    Under Assumption \ref{assump:large space}, the solution to optimizing Eqn.~\eqref{eq:orpo} is
    \begin{equation}
        P_\theta(y|x) = \frac{\exp(Z(x))}{\frac{q_l^*(y|x)}{q_w^*(y|x)} + \exp(Z(x))} 
    \end{equation}
    where $Z(x)$ is partition function such that $\sum_y P_\theta(y|x) = 1$.
\end{lemma}
To illustrate the benefit of the proposed preference selection strategy, we consider a scenario where $y_w$ and $y_l$ are random variables following
% \begin{small}
\begin{align}\label{eq: dgp}
    \notag y_w &\sim f_w(x) + \text{Exp}(\lambda),\\ y_l& \sim f_l(x)+\text{Exp}(\lambda),
\end{align}
% \end{small}
where $\text{Exp}(\lambda)$ denotes an exponential random variable with rate $\lambda$, and $f_w(x)$ and $f_l(x)$  represent the central locations of $y_w$ and $y_l$, respectively. To ensure a meaningful selection process, we assume $f_w(x) > f_l(x)$, indicating that $y_w$ is generally preferred over $y_l$.

We compare our method to a baseline that applies ORPO without preference selection (i.e., $n=1$ in Eqn.~\eqref{eq:pref selection}). Let $\tilde{P}_\theta(y|x)$ denote the solution obtained without selection. We measure the response quality using the absolute distance from the expected preferred response, $L(y)=|y-\bE[y_w]|$. The following theorem formalizes the advantage of our method. The proof is deferred to App~\ref{app:proof}.

\begin{theorem} \label{thm:closer dist}
    Under \eqref{eq: dgp}, assume 
 $\tilde{P}_\theta(y|x)$ generates $y$ with density function:
\begin{equation*}
    p \cdot \lambda e^{-\lambda[y- f_w(x)]} + (1-p) \cdot \lambda e^{-\lambda[y- f_l(x)]}.
\end{equation*}
    Let $y\sim P_\theta(y|x)$ and $\tilde{y} \sim \tilde{P}_\theta(y|x)$.
    Then, the expected absolute distance under $P_\theta$ is smaller than that under $\tilde{P}_\theta$:
    \begin{equation}
        \mathbb{E}_{y\sim P_\theta(y|x)}[L(y)]<\mathbb{E}_{\tilde{y} \sim \tilde{P}_\theta(y|x)}[L(\tilde y)].
    \end{equation}
\end{theorem}
Theorem \ref{thm:closer dist} suggests that the proposed preference selection strategy improves response alignment with the preferred choice $y_w$. Our analysis reveals that this selection process amplifies the gap between preferred and non-preferred responses, compelling the model to enhance their separation and ultimately leading to more accurate and reliable outputs.

%Due to the monotonicity of the probability density function, $q_l^*$ corresponds to the minimum value among $y_l^i$, while $q_w^*$ corresponds to the maximum value among $y_w^i$. 
% \begin{lemma}
%     Denote $\Tilde{P}_\theta(y|x)$ as the solution to minimizing the ORPO loss without the selection step:
%     \begin{equation}
%         \tilde{P}_\theta(y|x) = \frac{\exp(Z(x))}{\frac{q_l(y|x)}{q_w(y|x)} + \exp(Z(x))}
%     \end{equation}
%     % Then $P_{\theta}(y|x)$ generates $y\sim f_w(x) + \exp(\lambda)$ with probability greater than $\tilde{P}_\theta(y|x)$, and generates $y\sim f_l(x) + \exp(\lambda)$ with probability lower than $\tilde{P}_\theta(y|x)$:   
%     % \begin{equation}
%     %     \begin{aligned}
%     %        \notag & P_{\theta}(y|x) > \tilde{P}_\theta(y|x) & \text{if}~y \sim f_w(x) + \text{Exp}(\lambda) \\
%     %        \notag & P_{\theta}(y|x) < \tilde{P}_\theta(y|x) & \text{if}~y \sim f_l(x) + \text{Exp}(\lambda)
%     %     \end{aligned}
%     % \end{equation}
% \end{lemma}
\section{Experiment}
In this section, we extensively evaluate the proposed {\name} and answer the following questions: RQ1)~How does {\name} perform compared to state-of-the-art baselines? RQ2)~What are the roles of rejection sampling and preference data selection in model performance improvements respectively? RQ3)~Can the proposed {\name} benefit from more retrieved documents? RQ4)~Is it possible to apply different preference optimization method to {\name}? RQ5)~How does the performance change with varying $\beta$?
% \begin{itemize}
%     \item[RQ1] How does {\name} perform compared to state-of-the-art RAG baselines?
%     \item[RQ2] What are the roles of rejection sampling and proposed preference data selection in model
% performance improvements respectively?
%     \item[RQ3] Can the proposed {\name} benefit from more retrieved documents?
%     \item[RQ4] Is it possible to apply different preference optimization method to {\name} ?
%     \item[RQ5] How does the performance change with varying $\beta$?
% \end{itemize}
\subsection{Datasets and Experiment Settings}
\paragraph{Datasets and Evaluation Metrics.} We conduct experiments on three public benchmarks, including HotPotQA~\citep{yang2018hotpotqa}, 2WikiMultiHopQA~\citep{ho2020constructing}, and StrategyQA~\citep{geva2021did}. Following \citet{shao-etal-2023-enhancing,wang2024blendfilter}, we evaluate the first 500 questions from the development dataset for HotPotQA and 2WikiMultiHopQA, and evaluate questions from the development dataset for StrategyQA. For multi-hop question answering datasets, we employ exact match~(EM) and F1 as evaluation metrics, and for the commonsense reasoning dataset, which is a binary classification task, we use accuracy and F1 score as the metrics. To evaluate the retrieval performance, we leverage widely used Recall as the evaluation metric.
\paragraph{Baselines.} We adopt following state-of-the-art baselines to evaluate against {\name}: 1)~CoT Prompting~\citep{wei2022chain}, 2)~ReAct~\citep{yao2022react}, 3)~SelfAsk~\citep{press2022measuring}, 4)~BlendFilter~\citep{wang2024blendfilter}, 5)~InstructRAG~\citep{wei2024instructrag}, 6)~RetRobust~\citep{yoranmaking}, 7)~ASTUTE~\citep{wang2024astute}, and 8)~ICL+RAG~\cite{park2024enhancing}. We show more detailed information about baselines in the Appendix~\ref{app:baseline}.
\paragraph{Implementation Details.} We evaluate models with three small-sclae LLMs: Qwen2.5-1.5B-Instruct~\citep{qwen2.5}, Llama-3.2-1B-Instruct$\footnote{\url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md}}$, and gemma-2-2b-it~\citep{team2024gemma}. We utilize the state-of-the-art efficient retrieval method ColBERT v2~\citep{santhanam2022colbertv2} as the retriever implemented by \citet{khattab2022demonstrate,khattab2023dspy}. The knowledge base we employ is the collection of Wikipedia abstracts dumped in 2017~\cite{khattab2023dspy}. We show the detailed information about our implementations in the Appendix~\ref{app:details}.



% \begin{table}[h!]
% \centering
% \caption{Performance of {\name} with Qwen2.5-1.5B-Instruct  as the backbone. }
% \label{tab:qwen}
% \resizebox{0.5\textwidth}{!}{%
% \begin{tabular}{@{}l
% >{\columncolor[HTML]{FEE8E7}}c 
% >{\columncolor[HTML]{FEE8E7}}c 
% >{\columncolor[HTML]{F0F0FD}}c 
% >{\columncolor[HTML]{F0F0FD}}c 
% >{\columncolor[HTML]{FFF3E4}}c 
% >{\columncolor[HTML]{FFF3E4}}c @{}}
% \toprule
% \multicolumn{1}{c}{}                         & \multicolumn{2}{c}{\cellcolor[HTML]{FEE8E7}HotPotQA} & \multicolumn{2}{c}{\cellcolor[HTML]{F0F0FD}2WikiMultiHopQA} & \multicolumn{2}{c}{\cellcolor[HTML]{FFF3E4}StrategyQA} \\ \cmidrule(l){2-7} 
% \multicolumn{1}{c}{\multirow{-2}{*}{Method}} & EM                  & F1                    & EM                      & F1                       & Acc                        & F1                        \\ \midrule
% CoT                                          & 13.0                         & 20.9                  & 18.8                             & 23.5                     & 55.9                       & 17.9                      \\
% vanilla RAG                                  & 27.2                         & 38.4                  & 6.2                              & 11.4                     & 55.5                       & 23.9                      \\
% ReAct                                        & 14.9                         & 25.9                  & 9.6                              & 22.2                     & 55.0                       & 48.8                      \\
% SelfAsk                                      & 20.4                         & 32.8                  & 19.2                             & 25.3                     & 51.1                       & 42.9                      \\
% BlendFilter                                  & 26.4                         & 37.5                  & 19.4                             & 24.2                     & 59.4                       & 45.0                      \\
% InstructRAG                                  & 31.2                         & 39.6                  & 23.6                             & 26.9                     & 53.7                       & 39.1                      \\
% RetRobust                                    &                              &                       &                                  &                          &                            &                           \\
% % ASTUTE                                       & 0.4                          & 0.9                   & 0.2                              & 0.2                      & 55.0                       & 31.8                      \\
% ASTUTE                                       & 21.4                         & 27.8                  & 19.0                             & 23.4                     & 55.0                       & 12.0                      \\
% \color{red} ICL+RAG                          & 28.6                         & 39.3                  & 21.8                             & 26.4                     & 56.3                       & 27.5                      \\
% {\name}                                      & 34.8                         & 44.8                  & 31.6                             & 35.0                     & 59.8                       & 52.1                      \\ \bottomrule
% \end{tabular}%
% }
% \end{table}


% \begin{table}[h!]
% \centering
% \caption{Performance of {\name} with Llama-3.2-1B-Instruct as the backbone.}
% \label{tab:llama}
% \resizebox{0.5\textwidth}{!}{%
% \begin{tabular}{@{}l
% >{\columncolor[HTML]{FEE8E7}}c 
% >{\columncolor[HTML]{FEE8E7}}c 
% >{\columncolor[HTML]{F0F0FD}}c 
% >{\columncolor[HTML]{F0F0FD}}c 
% >{\columncolor[HTML]{FFF3E4}}c 
% >{\columncolor[HTML]{FFF3E4}}c @{}}
% \toprule
% \multicolumn{1}{c}{}                         & \multicolumn{2}{c}{\cellcolor[HTML]{FEE8E7}HotPotQA} & \multicolumn{2}{c}{\cellcolor[HTML]{F0F0FD}2WikiMultiHopQA} & \multicolumn{2}{c}{\cellcolor[HTML]{FFF3E4}StrategyQA} \\ \cmidrule(l){2-7} 
% \multicolumn{1}{c}{\multirow{-2}{*}{Method}} & EM                  & F1                    & EM                      & F1                       & Acc                        & F1                        \\ \midrule
% CoT                                          & 13.6                         & 19.5                  & 13.6                             & 19.0                     & 52.8                       & 23.9                      \\
% vanilla RAG                                  & 27.8                         & 37.0                  & 16.0                             & 21.9                     & 54.6                       & 33.3                      \\
% ReAct                                        & 15.4                         & 24.7                  & 10.2                             & 15.9                     & 51.1                       & 61.9                      \\
% SelfAsk                                      & 12.0                         & 18.0                  & 12.4                             & 18.5                     & 51.1                       & 42.9                      \\
% BlendFilter                                  & 20.6                         & 29.7                  & 17.8                             & 23.1                     & 55.9                       & 34.8                      \\
% InstructRAG                                  & 28.8                         & 39.2                  & 23.8                             & 27.4                     & 56.8                       & 29.8                      \\
% RetRobust                                    &                              &                       &                                  &                          &                            &                           \\
% ASTUTE                                       & 0.00                         & 0.00                  & 0.00                             & 0.00                     & 58.1                       & 35.1                      \\
% \color{red} ICL+RAG                          & 25.4                         & 36.1                  & 19.2                             & 24.2                     & 54.6                       & 29.7                      \\
% RoseRAG                                      & 33.4                         & 44.1                  & 30.2                             & 35.8                     & 61.1                       & 43.3                      \\ \bottomrule
% \end{tabular}%
% }
% \end{table}

% \begin{table}[h!]

\begin{table}[h!]
\centering
% \begin{minipage}{0.48\textwidth}
  \centering
  % \renewcommand\arraystretch{0.91}
  \caption{Performance of {\name} with Qwen2.5-1.5B-Instruct as the backbone.}
  \label{tab:qwen}
  % \resizebox{1\linewidth}{!}{%
    \begin{tabular}{l
      >{\centering\arraybackslash\columncolor[HTML]{FEE8E7}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FEE8E7}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{F0F0FD}}p{1.3cm} 
      >{\centering\arraybackslash\columncolor[HTML]{F0F0FD}}p{1.3cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FFF3E4}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FFF3E4}}p{0.8cm} }
    \toprule
    \multicolumn{1}{c}{}                         & \multicolumn{2}{c}{\cellcolor[HTML]{FEE8E7}HotPotQA} & \multicolumn{2}{c}{\cellcolor[HTML]{F0F0FD}2WikiMultiHopQA} & \multicolumn{2}{c}{\cellcolor[HTML]{FFF3E4}StrategyQA} \\ \cmidrule{2-7} 
    \multicolumn{1}{c}{\multirow{-2}{*}{Method}} & EM                  & F1                    & EM                      & F1                       & Acc                        & F1                        \\ \midrule
    CoT                                          & 13.0                         & 20.9                  & 18.8                             & 23.5                     & 55.9                       & 17.9                      \\\hdashline
    vanilla RAG                                  & 27.2                         & 38.4                  & 6.2                              & 11.4                     & 55.5                       & 23.9                      \\
    ReAct                                        & 14.9                         & 25.9                  & 9.6                              & 22.2                     & 55.0                       & 48.8                      \\
    SelfAsk                                      & 20.4                         & 32.8                  & 19.2                             & 25.3                     & 51.1                       & 42.9                      \\
    BlendFilter                                  & 26.4                         & 37.5                  & 19.4                             & 24.2                     & \underline{59.4}                       & \underline{45.0}                      \\
    InstructRAG                                  & \underline{31.2}                         & \underline{39.6}                  & \underline{23.6}                             & \underline{26.9}                     & 53.7                       & 39.1                      \\
  
    RetRobust                                    & 16.8                         & 24.6                  & 13.0                             & 19.9                     & 51.5                       & 37.3                      \\
    ASTUTE                                       & 21.4                         & 27.8                  & 19.0                             & 23.4                     & 55.0                       & 12.0                      \\
    ICL+RAG                       & 28.6                         & 39.3                  & 21.8                             & 26.4                     & 56.3                       & 27.5                      \\\hdashline
    {\name}                                      & \textbf{34.8}                         & \textbf{44.8}                  & \textbf{31.6 }                            & \textbf{35.0}                     & \textbf{59.8}                       & \textbf{52.1}                      \\ \bottomrule
    \end{tabular}%
  % }
\end{table}
% \end{table}

\begin{table}[h!]
% \begin{minipage}{0.48\textwidth}
  \centering
  % \renewcommand\arraystretch{0.91}
  \caption{Performance of {\name} with Llama-3.2-1B-Instruct as the backbone.}
  \label{tab:llama}
  % \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l
      >{\centering\arraybackslash\columncolor[HTML]{FEE8E7}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FEE8E7}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{F0F0FD}}p{1.3cm} 
      >{\centering\arraybackslash\columncolor[HTML]{F0F0FD}}p{1.3cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FFF3E4}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FFF3E4}}p{0.8cm} }
    \toprule
    \multicolumn{1}{c}{}                         & \multicolumn{2}{c}{\cellcolor[HTML]{FEE8E7}HotPotQA} & \multicolumn{2}{c}{\cellcolor[HTML]{F0F0FD}2WikiMultiHopQA} & \multicolumn{2}{c}{\cellcolor[HTML]{FFF3E4}StrategyQA} \\ \cmidrule(l){2-7} 
    \multicolumn{1}{c}{\multirow{-2}{*}{Method}} & EM                  & F1                    & EM                      & F1                       & Acc                        & F1                        \\ \midrule
    CoT                                          & 13.6                         & 19.5                  & 13.6                             & 19.0                     & 52.8                       & 23.9                      \\ \hdashline
    vanilla RAG                                  & 27.8                         & 37.0                  & 16.0                             & 21.9                     & 54.6                       & 33.3                      \\
    ReAct                                        & 15.4                         & 24.7                  & 10.2                             & 15.9                     & 51.1                       & 61.9                      \\
    SelfAsk                                      & 12.0                         & 18.0                  & 12.4                             & 18.5                     & 51.1                       & 42.9                      \\
    BlendFilter                                  & 20.6                         & 29.7                  & 17.8                             & 23.1                     & 55.9                       & 34.8                      \\
    InstructRAG                                  & \underline{28.8}                         & \underline{39.2}                  & \underline{23.8}                             & \underline{27.4}                     & 56.8                       & 29.8                      \\
    RetRobust    & 17.6                         & 26.4                  & 20.0                             & 26.5                     & 56.8                       & 39.3                            \\
    ASTUTE\tablefootnote{ASTUTE relies on powerful LLM. We cannot extract answers when using Llama-3.2-1B-Instruct.}                                       & -                         & -                  & -                             & -                     & \underline{58.1}                       & 35.1                      \\
    ICL+RAG                       & 25.4                         & 36.1                  & 19.2                             & 24.2                     & 54.6                       & 29.7                      \\ \hdashline
    RoseRAG                                      & \textbf{33.4}                         & \textbf{44.1}                  & 30.2                             & \textbf{35.8}                     & \textbf{61.1}                       & \underline{43.3}                      \\ \bottomrule
    \end{tabular}%
  % }
\end{table}
% \end{minipage}
% \hfill
\begin{table}[h!]
% \begin{minipage}{0.48\textwidth}
\centering
% \renewcommand\arraystretch{0.91}
\caption{Performance of {\name} with gemma-2-2b-it as the backbone.}
\label{tab:gemma}
% \resizebox{1\columnwidth}{!}{
\begin{tabular}{l
      >{\centering\arraybackslash\columncolor[HTML]{FEE8E7}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FEE8E7}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{F0F0FD}}p{1.3cm} 
      >{\centering\arraybackslash\columncolor[HTML]{F0F0FD}}p{1.3cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FFF3E4}}p{0.8cm} 
      >{\centering\arraybackslash\columncolor[HTML]{FFF3E4}}p{0.8cm} }
\toprule
\multicolumn{1}{c}{}                         & \multicolumn{2}{c}{\cellcolor[HTML]{FEE8E7}HotPotQA} & \multicolumn{2}{c}{\cellcolor[HTML]{F0F0FD}2WikiMultiHopQA} & \multicolumn{2}{c}{\cellcolor[HTML]{FFF3E4}StrategyQA} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c}{\multirow{-2}{*}{Method}} & EM                  & F1                    & EM                      & F1                       & Acc                        & F1                        \\ \midrule
CoT                                          & 20.6                         & 27.9                  & 20.4                             & 24.5                     & 60.7                       & 44.4                      \\\hdashline
vanilla RAG                                  & 36.4                         & 46.6                  & 15.2                             & 20.2                     & 56.8                       & 39.3                      \\
ReAct                                        & 26.6                         & 38.1                  & 21.0                             & 26.9                     & 55.5                       & 45.2                      \\
SelfAsk                                      & 32.8                         & 43.5                  & 23.4                             & 30.5                     & 60.7                       & 45.8                      \\
BlendFilter                                  & 34.6                         & 45.4                  & 23.2                             & 29.6                     & \underline{62.5}                       & 49.4                      \\
InstructRAG                                  & \underline{38.0}                         & \underline{49.4}                  & \underline{29.0}                             & \underline{35.0}                     & 60.3                       & \underline{59.2}                      \\
RetRobust    & 30.4                         & 40.4                  & 22.0                             & 26.5                     & 64.2                       & 54.9                          \\
% ASTUTE                                       & 20.2                         & 27.8                  & 17.4                             & 22.2                     & 62.4                       & 41.9                      \\
ASTUTE                                       & 22.4                         & 32.5                  & 14.8                             & 19.5                     & 62.0                       & 42.0                      \\
ICL+RAG                          & 32.4                         & 42.9                  & 16.4                             & 22.5                     & 62.4                       & 54.3                      \\\hdashline
RoseRAG                                      & \textbf{42.4}                         & \textbf{54.0}                  & \textbf{37.2}                             & \textbf{42.7}                     & \textbf{67.7}                       & \textbf{60.2}                      \\ \bottomrule
\end{tabular}
% }
% \end{minipage}
\end{table}
% \end{table}
\subsection{Performance Comparison (RQ1)}

To evaluate the effectiveness of the proposed {\name} framework, we conduct experiments on three benchmark datasets: HotPotQA, 2WikiMultiHopQA, and StrategyQA. We compare RoseRAG against multiple retrieval-augmented generation (RAG) baselines using three different small-scale LLM backbones: Qwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct, and gemma-2-2B-it. From the results presented in Tables~\ref{tab:qwen}, \ref{tab:llama}, and \ref{tab:gemma}, we observe two key findings. 

First, the proposed {\name} consistently outperforms all baseline methods across different datasets and model backbones, demonstrating its effectiveness in retrieval-augmented generation for small-scale LLMs. The substantial improvements in EM and F1 scores indicate that the margin-aware preference optimization and rejection sampling in RoseRAG significantly enhance reasoning accuracy. Notably, compared to InstructRAG, RoseRAG achieves superior performance, highlighting the necessity of incorporating preference optimization in addition to supervised fine-tuning.

Second, the results reveal that SLMs exhibit distinct properties compared to powerful large-scale LLMs, which impacts the effectiveness of various RAG methods. Many retrieval-based techniques that have shown strong performance with large-scale models fail to maintain similar improvements on small models. Methods such as ReAct and SelfAsk, which decompose original queries into sub-questions, struggle because SLMs lack the reasoning ability to perform accurate decomposition. Similarly, approaches like BlendFilter and ASTUTE, which rely on the LLM itself to filter irrelevant information, perform poorly since small models are less capable of distinguishing irrelevant from noisy content. The performance of InstructRAG and {\name} highlights the necessity of fine-tuning small-scale LLMs to enhance their ability to process noisy retrieval results effectively. These findings underscore the importance of adapting RAG strategies specifically for small-scale SLMs rather than directly transferring techniques optimized for large-scale LLMs.

% \end{minipage}%



\begin{table}[h!]
\centering
\caption{Performance~(Exaxt Match/F1) of {\name} using different types of generated preference data on HotPotQA with Qwen2.5-1.5B-Instruct.}
\label{tab:min_max}
% \resizebox{1\columnwidth}{!}{
\begin{tabular}{@{}cl|ccc@{}}
\toprule
\multicolumn{2}{l|}{\multirow{2}{*}{}}                         & \multicolumn{3}{c}{Positive}                   \\ \cmidrule(l){3-5} 
\multicolumn{2}{l|}{}                                          & w/o Selection & Minimal            & Maximal   \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Negative}} & w/o Selection & 30.6/39.9     & 33.2/43.1          & 27.6/37.1 \\
\multicolumn{1}{c|}{}                          & Minimal       & 29.0/39.2     & 33.2/42.2          & 26.6/35.3 \\
\multicolumn{1}{c|}{}                          & Maximal       & 33.0/43.6     & \textbf{34.8/44.8} & 28.8/38.4 \\ \bottomrule
\end{tabular}
% }
\end{table}

\subsection{Effectiveness of Data Selection (RQ2)}



To assess the impact of our preference data selection strategy on performance, we conduct experiments on HotPotQA using Qwen2.5-1.5B-Instruct as the backbone. In this study, we vary the selection method for both the positive (chosen) and negative (rejected) responses, comparing three scenarios: no selection, selection based on the minimal likelihood, and selection based on the maximal likelihood. The performance—measured in Exact Match (EM) and F1 scores—is reported in Table~\ref{tab:min_max}. Based on the table, we have following findings:

\noindent \textbf{Overall Effectiveness.} Introducing a selection strategy for the preference data markedly improves performance compared to the baseline without any selection. For example, when only the positive responses are refined using the minimal likelihood criterion (with negative responses remaining unselected), the EM/F1 improves from 30.6/39.9 to 33.2/43.1. This result confirms that our preference data selection module effectively enhances the quality of the training signal.

\noindent \textbf{Optimal Selection Strategy.} The optimal performance (34.8/44.8) occurs when positive responses have the lowest likelihood and negative responses have the highest. This aligns with our intuition: the minimal likelihood positive response likely represents a scenario where the model is less confident and, therefore, benefits more from corrective feedback.  Simultaneously, selecting the maximal likelihood negative response targets cases where the LLM is confidently incorrect, offering a strong contrastive signal. Using the opposite criteria (maximal likelihood for positive/minimal likelihood for negative) leads to a notable performance drop.

\noindent \textbf{Importance of Positive Selection.} The results suggest that selecting the minimal likelihood candidate for the positive response is particularly critical, as it plays a direct role in the negative log-likelihood (NLL) loss. By emphasizing these uncertain yet correct outputs, the model receives a more effective corrective signal, facilitating better alignment with the desired responses. 



\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\columnwidth]{rej_sampling.pdf}
    \caption{Accuracy of {\name} with and without rejection sampling with Qwen2.5-1.5B-Instruct. \vspace{-1ex}}
    \label{fig:rej_sampling}
\end{figure}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{recall_em.pdf}
        \caption{Accuracy with respect to different number of retrieved documents.}
        \label{fig:recall_em}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{preference_opt.pdf}
        \caption{Performance of RoseRAG with different preference optimization method.}
        \label{fig:preference_opt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{beta.pdf}
        \caption{Accuracy with  different values of $\beta$.}
        \label{fig:beta}
    \end{subfigure}
    \vspace{-.5ex}
    \caption{Comparison of different experimental settings. Experiments are conducted on HotPotQA with Qwen2.5-1.5B-Instruct as the backbone. \vspace{-2ex}}
    % \label{fig:three_figures}
\end{figure*}

\subsection{Effectiveness of Rejection Sampling (RQ2)}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1\columnwidth]{rej_sampling.eps}
%     \caption{Accuracy of {\name} with and without rejection sampling with Qwen2.5-1.5B-Instruct on HotPotQA.}
%     \label{fig:rej_sampling}
% \end{figure}

In this section, we evaluate the effectiveness of rejection sampling in {\name}. Specifically, we compare the performance of {\name} with and without rejection sampling on two benchmark datasets: HotPotQA and 2WikiMultiHopQA. From results in Fig.~\ref{fig:rej_sampling}, we can find that rejection sampling is important for performance improvement. On HotPotQA, rejection sampling improves both the Exact Match and F1 scores significantly. Specifically, the EM score increases from 0.33 to 0.41, and the F1 score rises from 0.37 to 0.46. This demonstrates that rejection sampling enables the model to generate more accurate and contextually relevant responses by filtering out spurious rationales.



% \begin{figure}[h!]
%     \centering
%     \subfigure[Accuracy with respect to different number of retrieved documents on HotPotQA with Qwen2.5-1.5B-Instruct.]{%
%         \includegraphics[width=0.32\textwidth]{recall_em.eps}
%         \label{fig:recall_em}
%     }
%     \subfigure[Performance comparison of RoseRAG with different preference optimization method with Qwen2.5-1.5B-Instruct on HotPotQA.]{%
%         \includegraphics[width=0.32\textwidth]{preference_opt.eps}
%         \label{fig:preference_opt}
%     }
%     \subfigure[Performance comparison of RoseRAG with different preference optimization method with Qwen2.5-1.5B-Instruct on HotPotQA.]{%
%         \includegraphics[width=0.32\textwidth]{preference_opt.eps}
%         \label{fig:preference_opt}
%     }
%     % \caption{Comparison of different experimental settings.}
%     % \label{fig:three_figures}
% \end{figure}



\subsection{Different Numbers of Retrieved Documents (RQ3)}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1\columnwidth]{recall_em.eps}
%     \caption{Accuracy with respect to different number of retrieved documents on HotPotQA with Qwen2.5-1.5B-Instruct.}
%     \label{fig:recall_em}
% \end{figure}

Fig.~\ref{fig:recall_em} compares {\name} and vanilla RAG on HotPotQA as the number of retrieved documents increases (\(K = \{1, 2, 5, 8, 10\}\)). 
The retrieval recall improves significantly when the number of retrieved documents increases from 1 to 5 but plateaus beyond that. 
% but the improvement slows as \(K\) increases to 8 and 10, reflecting a saturation in the likelihood of retrieving additional relevant documents. T
{\name} closely follows this trend, with its EM score increasing rapidly when retrieving 1, 2, and 5 documents. However, beyond \(K = 5\), the EM score plateaus, and the performance at \(K = 10\) remains nearly identical to that at \(K = 5\), indicating that {\name} effectively utilizes the most relevant retrieved information without being hindered by less relevant documents. 
% In contrast, the performance of vanilla RAG exhibits a different trend. 
% The EM score of vanilla RAG drops significantly after retrieving more than 2 documents, indicating its inability to filter noise and integrate relevant content effectively due to its limited capacity. 
In contrast, vanilla RAG’s EM score drops after retrieving more than 2 documents, 
indicating its inability to filter noise and integrate relevant content effectively due to its limited capacity.
In comparison, RoseRAG demonstrates its robustness and effectiveness by consistently improving or maintaining high performance as the number of retrieved documents increases. 
These results highlight {\name}’s robustness in handling larger retrieval sizes for RAG applications. 
% These findings emphasize the capability of {\name} to adapt to larger retrieval sizes and maximize retrieval-augmented generation performance.


\subsection{Different Preference Optimization Methods (RQ4)}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1\columnwidth]{preference_opt.eps}
%     \caption{Performance comparison of RoseRAG with different preference optimization method with Qwen2.5-1.5B-Instruct on HotPotQA.}
%     \label{fig:preference_opt}
% \end{figure}

Fig.~\ref{fig:preference_opt} presents the performance comparison of {\name} using different preference optimization methods, including ORPO, RPO~\cite{pang2025iterative}, CPO~\cite{xu2024contrastive}, and SimPO+SFT~\cite{meng2025simpo}, on HotPotQA dataset. 
The results indicate that all methods achieve comparable performance in both Exact Match and F1 scores, suggesting that the proposed {\name} framework is robust and does not rely on a specific preference optimization technique. 
While minor variations exist, the consistency across methods highlights the generalizability of {\name} in effectively leveraging preference optimization for retrieval-augmented generation. 
This observation underscores the flexibility of our approach, allowing it to integrate seamlessly with various preference optimization approaches. 
Consequently, {\name} can be adapted to different optimization settings, making it a versatile solution for enhancing the reasoning capabilities of SLMs in retrieval-augmented generation tasks.







% \begin{table}[h!]
% \centering
% \caption{Performance~(Exaxt Match/F1) of {\name} using different types of generated preference data on HotPotQA with Qwen2.5-1.5B-Instruct.}
% \label{tab:min_max}
% \begin{tabular}{@{}cl|ccc@{}}
% \toprule
% \multicolumn{2}{l|}{\multirow{2}{*}{}}                         & \multicolumn{3}{c}{Positive}                   \\ \cmidrule(l){3-5} 
% \multicolumn{2}{l|}{}                                          & w/o Selection & Minimal            & Maximal   \\ \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{Negative}} & w/o Selection & 30.6/39.9     & 33.2/43.1          & 27.6/37.1 \\
% \multicolumn{1}{c|}{}                          & Minimal       & 29.0/39.2     & 33.2/42.2          & 26.6/35.3 \\
% \multicolumn{1}{c|}{}                          & Maximal       & 33.0/43.6     & \textbf{34.8/44.8} & 28.8/38.4 \\ \bottomrule
% \end{tabular}
% \end{table}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{rej_sampling.eps}
%     \caption{Accuracy of {\name} w/ and w/o reject sampling with Qwen2.5-1.5B-Instruct on HotPotQA.}
%     \label{fig:rej_sampling}
% \end{figure}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{preference_opt.eps}
%     \caption{Performance comparison of RoseRAG with different preference optimization method with Qwen2.5-1.5B-Instruct on HotPotQA.}
%     \label{fig:preference_opt}
% \end{figure}
\subsection{Different Values of $\beta$ (RQ5)}

% \begin{figure}[h!]
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1\columnwidth]{beta.eps}
%     \caption{Accuracy with respect to different values of $\beta$ with Qwen2.5-1.5B-Instruct on HotPotQA.}
%     \label{fig:beta}
% % \end{figure}
% \end{figure}
Fig.~\ref{fig:beta} presents the performance of {\name} on HotPotQA with varying values of \(\beta\), which controls the strength of preference alignment in the ORPO loss. 
We evaluate \(\beta\) over the range \(\{0, 0.05, 0.1\), \(0.2, 0.3, 0.5, 1, 1.5, 2\}\) and observe that setting \(\beta\) within the interval \((0, 0.5)\) leads to better performance compared to both \(\beta = 0\) and excessively large values of \(\beta\). 
This result underscores the necessity of preference alignment, as models trained solely with supervised fine-tuning (\(\beta = 0\)) exhibit suboptimal performance. 
However, as \(\beta\) increases beyond 0.5, both the Exact Match and F1 scores decline, indicating that an overly strong preference alignment term causes the model to focus excessively on optimizing preference differences while neglecting the learning of positive responses. 
% This trade-off suggests that an appropriately chosen \(\beta\) is crucial for balancing preference alignment and effective learning from positive responses, ultimately leading to improved retrieval-augmented generation performance.



\section{Conclusion}
We introduce {\name}, a novel framework that enhances the robustness of small-scale LLMs (SLMs) in retrieval-augmented generation through margin-aware preference optimization. By leveraging multi-turn prompting with rejection sampling and contrastive preference data selection, our approach effectively mitigates the impact of noisy retrieved content. We conducted extensive experiments on three benchmarks, and the results demonstrate that {\name} outperforms state-of-the-art baselines. Moreover, {\name} can be generalized well for different kinds of SLMs, justifying its generalizability and broad applicability for RAG.




\clearpage

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliographystyle{unsrtnat}
\bibliography{custom}





\clearpage
\appendix
\onecolumn
\section{Baselines}\label{app:baseline}
We adopt following state-of-the-art baselines to evaluate our proposed {\name}: 
\begin{itemize}[leftmargin=1em]
    \item CoT~\cite{wei2022chain} instructs the LLM to generate answers accompanied by explicit reasoning steps.
    \item ReAct~\cite{yao2022react} integrates reasoning, action, and observation steps, concluding the generation process upon reaching a terminal state. The action step involves either generating a query to retrieve additional knowledge or finalizing the generation, while the observation step incorporates the retrieved knowledge documents.
    \item SelfAsk~\cite{press2022measuring} involves generating follow-up questions, retrieving relevant information, and answering these follow-up questions. Each retrieval operation is based on the generated follow-up questions. When no additional follow-up questions are produced, the LLM provides the answer to the original question. We prepend the newly retrieved knowledge to the original question following the approach of \citet{yoran-etal-2023-answering}. 
    \item BlendFilter~\cite{wang2024blendfilter} combines internal and external knowledge to enhance retrieval quality. Additionally, it employs the LLM to filter out irrelevant information, thereby preventing the model from being misled.
    \item InstructRAG~\cite{wei2024instructrag} instructs the LLM to generate rationales that analyze the relevance between the query and the retrieved knowledge. Subsequently, it conducts supervised fine-tuning (SFT) on the LLM using the generated rationales.
    \item RetRobust~\cite{yoranmaking} introduces an additional NLI model to assess the relationship between the query and the retrieved knowledge. If the relationship is deemed irrelevant, the model disregards the corresponding knowledge during generation.
    \item ASTUTE~\cite{wang2024astute} mitigates the pitfalls of imperfect retrieval by adaptively eliciting and integrating essential internal knowledge with externally retrieved data. Its iterative, source-aware consolidation process effectively resolves knowledge conflicts, yielding more accurate and trustworthy responses even in worst-case scenarios.
    \item ICL+RAG~\cite{park2024enhancing} leverages machine reading comprehension examples to guide the model in identifying unanswerable queries and resolving conflicting information from retrieved texts. By providing tailored in-context demonstrations during inference, the approach improves the reasoning capabilities and overall accuracy of retrieval-augmented language models on open-domain QA tasks.
\end{itemize}
\section{Implementation Details.}\label{app:details}
We evaluate models with three small-sclae LLMs: Qwen2.5-1.5B-Instruct~\citep{qwen2.5}, Llama-3.2-1B-Instruct~$\footnote{\url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md}}$, and gemma-2-2b-it~\citep{team2024gemma}. We employ the state-of-the-art efficient retrieval method ColBERT v2~\citep{santhanam2022colbertv2} as implemented by \citet{khattab2022demonstrate,khattab2023dspy}, which applies quantization to accelerate approximate nearest neighbor search. Our knowledge base comprises Wikipedia abstracts from the 2017 dump~\citep{khattab2023dspy}. Experiments are conducted using Transformers~\citep{wolf-etal-2020-transformers}, TRL~\citep{vonwerra2022trl}, and vLLM~\citep{kwon2023efficient}. In all experiments, we adopt a 3-shot in-context learning setting following the approach of \citet{shao-etal-2023-enhancing,wang2024blendfilter}, with the value of $K$ set to 5 for all methods. The prompts for generating preference data are shown in Fig.\ref{fig:hot_2wiki_rationale} and Fig.\ref{fig:StrategyQA_rationale}, while those for question answering are presented in Fig.\ref{fig:hotpotqa_prompt}, Fig.\ref{fig:2wiki_prompt}, and Fig.~\ref{fig:StrategyQA_prompt}. We generate preference data using the first 10k training samples from HotPotQA and 2WikiMultiHopQA, as well as the entire training set of StrategyQA. For all backbones, we apply LoRA with a rank of 16 and $\texttt{lora\_alpha}=32$, targeting ''$\texttt{all-linear}$'' modules. Experiments are conducted on eight 80G A100 NVIDIA GPUs. The learning rate and number of training epochs are selected from $\{2e-5, 3e-5, 5e-5, 2e-4, 5e-4\}$ and $\{2, 3, 4\}$, respectively.


\section{Mathematical Derivations}\label{app:proof}
\begin{lemma} \label{app lem: loss function property}
    (Lemma C.1 in \cite{chen2024selfplayfinetuningconvertsweak}) Denote $ U(t):= \log(1 +\exp(-t))$. For $a,b>0$, the following inequality holds
    $$a \cdot U(t) + b \cdot U(-t) \geq a \log(1 + b/a) + b\log(1+a/b)$$
    and equality holds if and only if $t = \log(a/b)$
\end{lemma}
\begin{lemma}
    Under Assumption \ref{assump:large space}, the solution to minimizing the ORPO loss  $\ell_{ORPO}(x, y_w^{*}, y_l^{*})$ is
    \begin{equation} \label{equ:close form}
        P_\theta(y|x) = \frac{\exp(Z(x))}{\frac{q_l^*(y|x)}{q_w^*(y|x)} + \exp(Z(x))} 
    \end{equation}
    where $Z(x)$ is partition function such that $\sum_y P_\theta(y|x) = 1$.
\end{lemma}
\begin{proof} Consider the last term in general ORPO loss
    \begin{equation}
    \begin{aligned}
        & 2\bE_{(x,y_w,y_l)} \left[- \log \sigma\!\left(\log \text{OR}_\theta(x, y_w, y_l)\right)\right] \\
        = & 2\bE_{(x,y_w,y_l)} U\left(\log \odds(y_w|x)  - \log \odds(y_l|x)\right) \\
        = & \int  q(x)q_w(y_w|x)q_l(y_l|x)  U\left(\log \odds(y_w|x)  - \log \odds(y_l|x)\right) \df x\df y \\
        & + \int  q(x)q_w(y_l|x)q_l(y_w|x)  U\left(\log \odds(y_l|x)  - \log \odds(y_w|x)\right) \df x\df y  \\
        \geq & \int  q(x)q_w(y_w|x)q_l(y_l|x)  \log \left(1 + \frac{q_w(y_l|x) q_l(y_w|x)}{q_w(y_w|x) q_l(y_l|x)}\right) \df x\df y\\
        & + \int  q(x)q_w(y_l|x)q_l(y_w|x)  \log \left(1 + \frac{q_w(y_w|x) q_l(y_l|x)}{q_w(y_l|x) q_l(y_w|x)}\right) \df x\df y
    \end{aligned}
\end{equation}
where the first inequality follows from Lemma \ref{app lem: loss function property}. For equivalence,
\begin{equation}
    \begin{aligned}
        \log \odds(y_w|x)  - \log \odds(y_l|x) = \log \frac{q_w(y_w|x) q_l(y_l|x)}{q_w(y_l|x) q_l(y_w|x)}
    \end{aligned}
\end{equation}
Thus, for any $x,y_w,y_l$, 
\begin{equation} \label{equ:odd form}
    \log \odds(y_w|x) - \log \frac{q_w(y_w|x)}{q_l(y_w|x)} = \log \odds(y_l|x) - \log \frac{q_w(y_l|x)}{q_l(y_l|x)}
\end{equation}
Therefore, Eqn.~\eqref{equ:odd form} holds if and only if there exists some $Z(x)$ such that
\begin{equation}
    \log \frac{P_\theta(y|x)}{1-P_\theta(y|x)} = Z(x) + \log \frac{q_w(y|x)}{q_l(y|x)}
        \Longleftrightarrow  P_\theta(y|x) = \frac{\exp(Z(x))}{\frac{q_l(y|x)}{q_w(y|x)} + \exp(Z(x))} 
\end{equation}
Finally, substituting $q_l$ and $q_w$ with $q_l^*$ and $q_w^*$ respectively, yields Eqn.~\eqref{equ:close form}.
\end{proof}

\begin{lemma} \label{app lem: larger prob}
    Denote $\Tilde{P}_\theta(y|x)$ as the solution to minimizing ORPO loss without the selection step. Then $P_{\theta}(y|x)$ generates $y\sim f_w(x) + \exp(\lambda)$ with probability greater than $\tilde{P}_\theta(y|x)$, and generates $y\sim f_l(x) + \exp(\lambda)$ with probability lower than $\tilde{P}_\theta(y|x)$:   
    \begin{equation}
        \begin{aligned}
            & P_{\theta}(y|x) > \tilde{P}_\theta(y|x) & \text{if}~y \sim f_w(x) + \E(\lambda) \\
            & P_{\theta}(y|x) < \tilde{P}_\theta(y|x) & \text{if}~y \sim f_l(x) + \E(\lambda)
        \end{aligned}
    \end{equation}
\end{lemma}
\begin{proof}
Order statistics $Y_{(k)}$, representing the $k$-th smallest value in a sample drawn from an exponential distribution $\E(\lambda)$, has the probability density function:
\begin{equation}
    \frac{n!}{(k-1)!(n-k)!}\left[1-e^{-\lambda y}\right]^{k-1}[e^{-\lambda y}]^{n-k}\lambda e^{-\lambda y}
 \end{equation}
Let $q_l^*$ denote the distribution of minimum value among $y_l^i$ and $q_w^*$ denote the the distribution of maximum value among $y_w^i$. The ratio of their corresponding densities is given by:
\begin{equation}
\frac{q_l^*(y|x)}{q_w^*(y|x)} = \left(\frac{\exp(-\lambda (y - f_l(x)))}{1 - \exp(-\lambda (y - f_w(x)))}\right)^{n-1} \frac{\exp(-\lambda (y - f_l(x)))}{\exp(-\lambda (y - f_w(x)))}. 
\end{equation}
Rewriting this expression yields,
\begin{equation} 
\frac{q_l^*(y|x)}{q_w^*(y|x)} = \left(\frac{\exp(-\lambda (y - f_l(x)))}{1 - \exp(-\lambda (y - f_w(x)))}\right)^{n-1} \frac{q_l(y|x)}{q_w(y|x)}.
\end{equation}
For $y\sim f_w(x) + \E(\lambda)$, define $d_w = y - f_w(x)$ and $d_l = y - f_l(x)$. Then,
$$d_l = d_w + [f_w(x) - f_l(x)] = d_w - c,\quad \text{where}~c = f_l(x) - f_w(x) < 0.$$
By Lemma \ref{app lem: loss function property}, the closed form solutions differ only in the ratio $R$: 
\begin{equation}
    \begin{aligned}
        R & = \frac{\exp(-\lambda (y-f_l(x)))}{1 - \exp(-\lambda (y-f_w(x)))} = \frac{\exp(-\lambda d_l)}{1 - \exp(-\lambda d_w)} = \frac{\exp(\lambda c)\exp(-\lambda d_w)}{1 - \exp(-\lambda d_w)}
    \end{aligned}
\end{equation}
Since $d_w \sim \E(\lambda)$, we have $U = \exp(-\lambda d_w)\sim \text{Uniform}(0,1)$, allowing us to express:
\begin{equation}
    \begin{aligned}
        R < 1 \Longleftrightarrow U <\frac{1}{1 + \exp(\lambda c)} \Longrightarrow \bP(R<1) = \frac{1}{1 + \exp(\lambda c)} > \frac{1}{2}
    \end{aligned}
\end{equation}
Similarly, for $y\sim f_l(x) + \E(\lambda)$, we obatin $\bP(R>1) > 1/2$. This implies that the distribution $P_{\theta}(y|x)$ generates $y\sim f_w(x) + \exp(\lambda)$ with a probability greater than $\tilde{P}_\theta(y|x)$ and generates $y\sim f_l(x) + \exp(\lambda)$ with probability lower than $\tilde{P}_\theta(y|x)$.
\end{proof}
\begin{theorem} \label{app thm:closer dist}
    Under \eqref{eq: dgp}, assume 
 $\tilde{P}_\theta(y|x)$ generates $y$ with density function:
\begin{equation*}
    p \cdot \lambda e^{-\lambda[y- f_w(x)]} + (1-p) \cdot \lambda e^{-\lambda[y- f_l(x)]}.
\end{equation*}
    Let $y\sim P_\theta(y|x)$ and $\tilde{y} \sim \tilde{P}_\theta(y|x)$.
    Then, the expected absolute distance under $P_\theta$ is smaller than that under $\tilde{P}_\theta$:
    \begin{equation}
        \mathbb{E}_{y\sim P_\theta(y|x)}[L(y)]<\mathbb{E}_{\tilde{y} \sim \tilde{P}_\theta(y|x)}[L(\tilde y)].
    \end{equation}
\end{theorem}
\begin{proof}
        Denote $\tilde{L}_1 = \left\vert \xi  \right\vert$ and $\tilde{L}_2 = \left\vert \xi + c \right\vert$, where $\xi \sim \E(\lambda) - 1/\lambda$. Then
    \begin{equation}
    \mathbb{E}_{\tilde{y} \sim \tilde{P}_\theta(y|x)}[L(\tilde y)] = p\bE_\xi[\tilde{L}_1] + (1-p)\bE_\xi[\tilde{L}_2]
\end{equation}
A well‐known fact is that function $\phi(t) = \bE\vert \xi - t\vert$ is convex in $t$ and attains its unique global minimum at the median $m$ of $\xi$. Since median of $\E(\lambda)$ is $(\log2)/\lambda$, it follows that: 
\begin{equation}
    m = \frac{\log2 - 1}{\lambda}<0
\end{equation}
Given that $m<0<-c$, we conclude:
\begin{equation}
    \phi(-c) > \phi(0) \Longleftrightarrow \bE_\xi[\tilde{L}_2] > \bE_\xi[\tilde{L}_1]
\end{equation}
By Lemma \ref{app lem: larger prob}, $P_\theta(y|x)$ has larger $p$ than $\tilde{P}_\theta(y|x)$, denoted as $q$. 
\begin{equation}
    \begin{aligned}
        \mathbb{E}_{y\sim P_\theta(y|x)}[L(y)] - \mathbb{E}_{\tilde{y} \sim \tilde{P}_\theta(y|x)}[L(\tilde y)] = (q-p) \cdot \left[\bE_\xi[\tilde{L}_1] - \bE_\xi[\tilde{L}_2]\right] < 0
    \end{aligned}
\end{equation}
\end{proof}




\section{Prompt}

\begin{figure}[htbp]
\centering
\begin{tcolorbox}[
    colback=gray!15,
    colframe=gray!75,
    % title=Rationale Generation on HotPotQA and 2WikiMultiHopQA,
    fonttitle=\large\bfseries\sffamily\color{white},
    coltitle=white,
    bottomrule=0pt,
    toprule=0pt,
    leftrule=0pt,
    rightrule=0pt,
    rounded corners,
    % width=0.9\linewidth
]
\textbf{System Prompt:} You are a useful assistant. I will provide one question, 
several pieces of knowledge (which may be related or unrelated to the question), 
and the answer to the question. Please explain your reasoning process in a single 
paragraph consisting of no more than four sentences. If the provided knowledge is 
insufficient, you may make an informed guess, but do not respond with "Unknown".

\medskip
\textbf{User Prompt:} Knowledge: $\mathcal{K}_{q}$\\
Question: $q$\\
Answer: $a^{*}$

\medskip
\textbf{Assistant Prompt:} Let's think step by step.

\medskip
\textbf{Output:} \{rationale $r$\}
\end{tcolorbox}
\caption{Prompt for rationale generation on HotPotQA and 2WikiMultiHopQA}
\label{fig:hot_2wiki_rationale}
\end{figure}


\begin{figure}[htbp]
\centering
\begin{tcolorbox}[
    colback=gray!15,
    colframe=gray!75,
    % title=Rationale Generation on HotPotQA and 2WikiMultiHopQA,
    fonttitle=\large\bfseries\sffamily\color{white},
    coltitle=white,
    bottomrule=0pt,
    toprule=0pt,
    leftrule=0pt,
    rightrule=0pt,
    rounded corners,
    % width=0.9\linewidth
]
% \begin{verbatim}
\textbf{System Prompt:} You are a useful assistant. I will provide one question, 
several knowledge (may related or unrelated to the question), and the answer to the question. 
Please show the think process about how to get the answer. If the given knowledge is insufficient,  
you can guess. Do not tell me Unknown. 
Your output should be in one paragraph within four sentences.

\medskip
\textbf{User Prompt:} Knowledge: $\mathcal{K}_{q}$\\
Question: $q$\\
Answer: $a^{*}$

\medskip
\textbf{Assistant Prompt:} Let's think step by step.

\medskip
\textbf{Output:} \{rationale $r$\}

\medskip
//\texttt{If rationale $r$ shows it can not conclude the answer}

\textbf{System Prompt:} You are a useful assistant. I will provide one question and the answer to the question. 
Please show the reasoning process about how to get the answer. Please use your own memorized knowledge to do reasoning. Do not mention the given answer explicitly in your reasoning. 
Your output should be in one paragraph with several sentences.
\medskip

\textbf{User Prompt:} Question: $q$\\
Answer: $a^{*}$
\medskip

\textbf{Model Output:} \{rationale $r_{wk}$\}

\medskip
\textbf{Output:} \{rationale $r$+However, we can answer the question based on known knowledge.+$r_{wk}$\}
% \end{verbatim}
\end{tcolorbox}
\caption{Prompt for rationale generation on StrategyQA.}
\label{fig:StrategyQA_rationale}
\end{figure}


\begin{figure*}[htbp]
\centering
\begin{tcolorbox}[
    colback=gray!15,
    colframe=gray!75,
    % title=Rationale Generation on HotPotQA and 2WikiMultiHopQA,
    fonttitle=\large\bfseries\sffamily\color{white},
    coltitle=white,
    bottomrule=0pt,
    toprule=0pt,
    leftrule=0pt,
    rightrule=0pt,
    rounded corners,
    % width=0.9\linewidth
]
\textbf{System Prompt:} You are a useful assistant. You should answer the following question quickly and accurately based on given knowledge. Provide your reasoning in few sentences and answer in one or few words. Please begin your final answer with 'So the answer is'.

\medskip
\textbf{User Prompt:} Knowledge:Mr. Big (film) | Mr. Big is a 2007 documentary directed and produced by Tiffany Burns and edited by Alec MacNeill Richardson. The documentary examines the "Mr. Big" undercover methods used by the Royal Canadian Mounted Police (RCMP). In these operations, RCMP officers pose as gang criminals and develop a relationship with the target in the hope of eventually determining what, if any, knowledge the target has of the crime being investigated. "Mr. Big" operations have been credited with securing difficult convictions in a large number of cases, but concerns have been raised that they involve a risk of false confessions and wrongful convictions.

It Might Get Loud | It Might Get Loud is a 2008 American documentary film by filmmaker Davis Guggenheim. It explores the careers and styles of prominent rock musicians Jimmy Page, The Edge, and Jack White. The film received a wide release on August 14, 2009 in the U.S. by Sony Pictures Classics.

Question:Are It Might Get Loud and Mr. Big both Canadian documentaries?

\medskip
\textbf{Assistant Prompt:} Let's think step by step. 
Mr. Big is a 2007 documentary which examines the "Mr. Big" undercover methods used by the Royal Canadian Mounted Police. However, Are It Might Get Loud is a 2008 American documentary film.
So the answer is no.

\medskip
\textbf{User Prompt:} Knowledge:Leslie H. Martinson | Leslie Herbert "Les" Martinson (January 16, 1915 – September 3, 2016) was an American television and film director.

László Benedek | László Benedek (] ; March 5, 1905 – March 11, 1992; sometimes "Laslo Benedek") was a Hungarian-born film director and cinematographer, most notable for directing "The Wild One" (1953).

Question:Were László Benedek and Leslie H. Martinson both film directors?

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
László Benedek was a Hungarian-born film director and Leslie H. Martinson was an American film director.
So the answer is yes.

\medskip
\textbf{User Prompt:} Knowledge:Lucium | Lucium was the proposed name for an alleged new element found by chemist Prosper Barrière in 1896 in the mineral monazite. Later, William Crookes confirmed that the new element was actually an impure sample of yttrium.

William Crookes | Sir William Crookes ( ; 17 June 1832 – 4 April 1919) was an English chemist and physicist who attended the Royal College of Chemistry in London, and worked on spectroscopy. He was a pioneer of vacuum tubes, inventing the Crookes tube which was made in 1875. Crookes was the inventor of the Crookes radiometer, which today is made and sold as a novelty item. Late in life, he became interested in spiritualism, and became the president of the Society for Psychical Research.

Question:Lucium was confimed to be an impure sample of yttrium by an English chemist who became the president of what? 

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
Lucium was confimed to be an impure sample of yttrium by William Crookes. William Crookes is Sir William Crookes. Sir William Crookes became the president of the Society for Psychical Research.
So the answer is Society for Psychical Research.

\textbf{User Prompt:} Knowledge: $\mathcal{K}_{q}$\\
Question: $q$

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
\end{tcolorbox}
\caption{Prompt for HotPotQA}
\label{fig:hotpotqa_prompt}
\end{figure*}


\begin{figure*}[htbp]
\centering
\begin{tcolorbox}[
    colback=gray!15,
    colframe=gray!75,
    % title=Rationale Generation on HotPotQA and 2WikiMultiHopQA,
    fonttitle=\large\bfseries\sffamily\color{white},
    coltitle=white,
    bottomrule=0pt,
    toprule=0pt,
    leftrule=0pt,
    rightrule=0pt,
    rounded corners,
    % width=0.9\linewidth
]
\textbf{System Prompt:} You are a useful assistant. You should answer the following question quickly and accurately based on given knowledge. Provide your reasoning in few sentences and answer in one or few words. Please begin your final answer with 'So the answer is'.

\medskip
\textbf{User Prompt:} Knowledge:Valentin the Good | Valentin the Good (Czech: "Valentin Dobrotivý" ) is a 1942 Czech comedy film directed by Martin Frič.

The Falcon (film) | Banović Strahinja (Serbian Cyrillic: Бановић Страхиња, internationally released as The Falcon) is a 1981 Yugoslavian adventure film written and directed by Vatroslav Mimica based on Strahinja Banović, a hero of Serbian epic poetry. It entered the section "Officina Veneziana" at the 38th Venice International Film Festival.

Martin Frič | Martin Frič (29 March 1902 – 26 August 1968) was a Czech film director, screenwriter and actor. He had more than 100 directing credits between 1929 and 1968, including feature films, shorts and documentary films.
Vatroslav Mimica | Vatroslav Mimica (born 25 June 1923) is a Croatian film director and screenwriter.

Question:Do both films The Falcon (Film) and Valentin The Good have the directors from the same country?


\medskip
\textbf{Assistant Prompt:} Let's think step by step.
The Falcon (Film) is directed by Martin Frič. Martin Frič was a Czech film director. Valentin The Good is directed by Vatroslav Mimica. Vatroslav Mimica is a Croatian film director. Czech is different from Croatia. 
So the answer is no.

\medskip
\textbf{User Prompt:} Knowledge:Wedding Night In Paradise (1950 film) | Wedding Night in Paradise (German: Hochzeitsnacht im Paradies) is a 1950 West German musical comedy film directed by Géza von Bolváry and starring Johannes Heesters, Claude Farell and Gretl Schörg .

Géza von Bolváry | Géza von Bolváry (full name Géza Maria von Bolváry-Zahn; 26 December 1897 – 10 August 1961) was a Hungarian actor, screenwriter and film director, who worked principally in Germany and Austria.

Question:What nationality is the director of film Wedding Night In Paradise (1950 Film)?

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
Wedding Night In Paradise (1950 film) is directed by Géza von Bolváry. Géza von Bolváry was a Hungarian actor, screenwriter and film director.
So the answer is Hungarian.

\medskip
\textbf{User Prompt:} Knowledge:Rhescuporis I (Odrysian) | Rhescuporis I (Ancient Greek: Ραισκούπορις) was a king of the Odrysian kingdom of Thrace in 240 BC - 215 BC, succeeding his father, Cotys III.

Cotys III (Odrysian) | Cotys III (Ancient Greek: Κότυς) was a king of the Odrysian kingdom of Thrace in ca. 270 BC, succeeding his father, Raizdos.

Question:Who is Rhescuporis I (Odrysian)'s paternal grandfather? 

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
The father of Rhescuporis I (Odrysian) is Cotys III. The father of Cotys III is Raizdos.
So the answer is Raizdos.

\textbf{User Prompt:} Knowledge: $\mathcal{K}_{q}$\\
Question: $q$

\medskip
\textbf{Assistant Prompt:} Let's think step by step.


\end{tcolorbox}
\caption{Prompt for 2WikiMultiHopQA}
\label{fig:2wiki_prompt}
\end{figure*}


\begin{figure*}[htbp]
\centering
\begin{tcolorbox}[
    colback=gray!15,
    colframe=gray!75,
    % title=Rationale Generation on HotPotQA and 2WikiMultiHopQA,
    fonttitle=\large\bfseries\sffamily\color{white},
    coltitle=white,
    bottomrule=0pt,
    toprule=0pt,
    leftrule=0pt,
    rightrule=0pt,
    rounded corners,
    % width=0.9\linewidth
]
\textbf{System Prompt:} You are a useful assistant. You should answer the following question quickly and accurately based on given knowledge. If the information provided is insufficient to answer, please respond based on your own knowledge. Provide your reasoning in few sentences first and then show the final answer which should always be either Yes or No, and NOTHING ELSE. Please begin your final answer with 'So the answer is'.

\medskip
\textbf{User Prompt:} Knowledge:Laxative | Laxatives, purgatives, or aperients are substances that loosen stools and increase bowel movements. They are used to treat and/or prevent constipation.

Question:Do people take laxatives because they enjoy diarrhea?


\medskip
\textbf{Assistant Prompt:} Let's think step by step.
Laxatives are substances that loosen stools and increase bowel movements. People take laxatives to treat and/or prevent constipation.
So the answer is No.

\medskip
\textbf{User Prompt:} Knowledge:Durian (disambiguation) | Durian is a large odorous spike-covered fruit of several southeast Asian tree species belonging to the genus "Durio".

Durian | The durian ( ) or is the fruit of several tree species belonging to the genus Durio. The name "durian" is derived from the Malay languages word for duri or "spike", a reference to the numerous spike protuberances of the fruit, together with the noun-building suffix -an. There are 30 recognised "Durio" species, at least nine of which produce edible fruit, and over 300 named varieties in Thailand and 100 in Malaysia. " Durio zibethinus" is the only species available in the international market: all other species are sold only in their local regions. There are hundreds of durian cultivars; many consumers express preferences for specific cultivars, which fetch higher prices in the market.

Question:Could Durian cause someone's stomach to feel unwell?

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant smells can make people feel nauseous.
So the answer is Yes.

\medskip
\textbf{User Prompt:} Knowledge:Monty Python and the Holy Grail | Monty Python and the Holy Grail is a 1975 British absurdist comedy film concerning the Arthurian legend, written and performed by the Monty Python comedy group (Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin), and directed by Gilliam and Jones. It was conceived during the hiatus between the third and fourth series of their BBC television series "Monty Python\'s Flying Circus".

Question:Did the swallow play a role in a famous film about King Arthur? 

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
Monty Python and the Holy Grail was a famous film about King Arthur. In Monty Python and the Holy Grail, swallows are mentioned several times.
So the answer is Yes.

\textbf{User Prompt:} Knowledge: $\mathcal{K}_{q}$\\
Question: $q$

\medskip
\textbf{Assistant Prompt:} Let's think step by step.
\end{tcolorbox}
\caption{Prompt for StrategyQA}
\label{fig:StrategyQA_prompt}
\end{figure*}

\end{document}
