\section{Related Work}
\paragraph{Synthetic data generation.} Prior works have explored generating synthetic data for various stages of an LLM's development pipeline: pre-training ____, instruction-finetuning ____, alignment ____ and task-specific fine-tuning ____. The main focus of our work, however, is to generate high-quality challenging problems for evaluation and benchmarking. There is very limited existing literature in this area. ____ created a narrative-based question answering (QA) benchmark using a neuro-symbolic pipeline that first samples facts, and then uses an LLM to generate a narrative. ____ created a synthetic QA benchmark by extracting entities and their reference chains from existing stories and then prompting an LLM to generate questions over them. ____ create a code understanding benchmark by prompting CodeLlama ____ to generate python functions and their inputs and designing tasks based on predicting either the input or the output for a given function. In contrast to these works, we focus on presenting a general framework to design scalable pipelines to create challenging benchmarks across multiple domains. Moreover, we focus on realistic tasks such as information-seeking QA and repository-level code generation, both with long contexts that we generate completely from scratch.

\paragraph{Task-specific synthetic data.} Recent works have explored generating synthetic datasets for content-grounded QA tasks. ____ 
 use an LLM to develop information-seeking dialogue datasets based on text extracted from Wikipedia and the Web. ____ prompt an LLM with wikipedia text to generate a question-answer pair. In contrast, we design a benchmark for document-based information-seeking questions that model realistic situations. Moreover, our pipeline generates entire the documents using LLMs, allowing a higher degree of control. There has also been significant interest in generating synthetic data for code. ____ employ an LLM-based framework to create examples for tasks based on existing raw code data. ____ generate code instruction data by prompting an LLM with seed code snippets from existing repositories. In this work, we focus on repository-level code completion, where we generate the repository contexts completely from scratch. Moreover, we also synthetically generate the corresponding test code to evaluate each example. Generating synthetic data to improve math reasoning has recently been a very active area of research. Previous work has explored generating new math problems by prompting LLMs with examples from existing datasets ____. Similar to our work, some prior works have focused on creating challenging math problems. ____ and ____ use computational graphs to craft problems of high complexity. While the problem contexts in these benchmarks state the entity-value relationships directly, we focus on generating more traditional word problems that require reasoning about the complexities and ambiguities arising from unconstrained use of natural language. ____ employ a human-in-the-loop approach to generate novel and difficult math problems by prompting LLMs with multiple core skills that must be used in the problem. ____ employ iterative question composition where they iteratively prompt an LLM with a seed question to generate more complex variations of it. In this work, we design a completely automated pipeline to craft grade-school level math problems that are challenging to solve even for the LLM that generated them.