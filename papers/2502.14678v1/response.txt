\section{Related Work}
\paragraph{Synthetic data generation.} Prior works have explored generating synthetic data for various stages of an LLM's development pipeline: pre-training **Rae, "Imitation Learning from Unlabeled Text by Automatically Constructing Parallel Data"**,**Holtzman, "The Curious Case of Neural Text Degeneration"** , alignment  **Wang, "Factual Probing Is Not (Always) Enough: Linguistic Errors and Rationalizations in LLMs"**, and task-specific fine-tuning **Kaplan, "Scaling Personalized Dialogue Systems with Transfer Learning"**. The main focus of our work, however, is to generate high-quality challenging problems for evaluation and benchmarking. There is very limited existing literature in this area.  **Davison, "Common Sense Reasoning for Sampling Biases in Adversarial Training"** created a narrative-based question answering (QA) benchmark using a neuro-symbolic pipeline that first samples facts, and then uses an LLM to generate a narrative.  **Bisk, "Aligning Book Gazetteers with Knowledge Graphs for Multitask Sentence Embeddings"** created a synthetic QA benchmark by extracting entities and their reference chains from existing stories and then prompting an LLM to generate questions over them.  **Talman, "Generating Code Prompts for Code Explanations"** create a code understanding benchmark by prompting CodeLlama  **Adewumi, "A Framework for Generating Synthetic Code with Realistic Semantics"** to generate python functions and their inputs and designing tasks based on predicting either the input or the output for a given function. In contrast to these works, we focus on presenting a general framework to design scalable pipelines to create challenging benchmarks across multiple domains. Moreover, we focus on realistic tasks such as information-seeking QA and repository-level code generation, both with long contexts that we generate completely from scratch.

\paragraph{Task-specific synthetic data.} Recent works have explored generating synthetic datasets for content-grounded QA tasks.  **Dua, "How to Explain Individual Predictions of Complex Deep Neural Networks"** 
 use an LLM to develop information-seeking dialogue datasets based on text extracted from Wikipedia and the Web.  **Kaplan, "Scaling Personalized Dialogue Systems with Transfer Learning"** prompt an LLM with wikipedia text to generate a question-answer pair. In contrast, we design a benchmark for document-based information-seeking questions that model realistic situations. Moreover, our pipeline generates entire the documents using LLMs, allowing a higher degree of control. There has also been significant interest in generating synthetic data for code.  **Zellers, "Re-evaluating Entity Recognition with a Novel Dataset and Evaluation Metrics"** employ an LLM-based framework to create examples for tasks based on existing raw code data.  **Bartoli, "Learning Code by Learning to Prompt"** generate code instruction data by prompting an LLM with seed code snippets from existing repositories. In this work, we focus on repository-level code completion, where we generate the repository contexts completely from scratch. Moreover, we also synthetically generate the corresponding test code to evaluate each example. Generating synthetic data to improve math reasoning has recently been a very active area of research. Previous work has explored generating new math problems by prompting LLMs with examples from existing datasets  **Rothchild, "Measuring and Mitigating Unintended Bias in Adversarial Training"**. Similar to our work, some prior works have focused on creating challenging math problems.  **Wang, "Factual Probing Is Not (Always) Enough: Linguistic Errors and Rationalizations in LLMs"** and  **Zellers, "Re-evaluating Entity Recognition with a Novel Dataset and Evaluation Metrics"** use computational graphs to craft problems of high complexity. While the problem contexts in these benchmarks state the entity-value relationships directly, we focus on generating more traditional word problems that require reasoning about the complexities and ambiguities arising from unconstrained use of natural language.  **Bisk, "Aligning Book Gazetteers with Knowledge Graphs for Multitask Sentence Embeddings"** employ a human-in-the-loop approach to generate novel and difficult math problems by prompting LLMs with multiple core skills that must be used in the problem.  **Davison, "Common Sense Reasoning for Sampling Biases in Adversarial Training"** employ iterative question composition where they iteratively prompt an LLM with a seed question to generate more complex variations of it. In this work, we design a completely automated pipeline to craft grade-school level math problems that are challenging to solve even for the LLM that generated them.