% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% neurips
%\PassOptionsToPackage{numbers, square}{natbib}
%\usepackage{neurips_2022}
%\renewcommand{\paragraph}{\textbf}

\usepackage[
  margin=1in,
  footskip=30pt
 ]{geometry}

 % Standard package includes
% \usepackage{times}
\usepackage{latexsym}

% arxiv
\usepackage[round,sort,comma]{natbib}
\usepackage{fullpage}
\usepackage{mathpazo}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage[colorlinks=true,citecolor=blue,breaklinks,backref=page]{hyperref} 
\usepackage{url}

\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{%
  \ifcase #1 (Not cited)%
  \or (Cited on page~#2)%
  \else (Cited on pages~#2)%
  \fi%
}

\hypersetup{
    colorlinks,
    citecolor=[rgb]{0.00, 0.45, 0.70}, %
    linkcolor=[rgb]{0.7, 0, 0.4},
    urlcolor=[rgb]{0.7, 0, 0.4} %
}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{xr}
\usepackage{wrapfig}
\usepackage{lipsum}

\usepackage{listings}
\usepackage[most]{tcolorbox}
\tcbuselibrary{listingsutf8}

\usepackage{booktabs}       % professional-quality tables
\usepackage{siunitx}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{balance}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{soul}
\usepackage{blindtext}
\usepackage{multirow}
\usepackage{pifont}% http://ctan.org/pkg/pifont

\usepackage{arydshln}

\usepackage{xskak}

\newcommand{\chessqueen}{\textsuperscript{\resizebox{0.3cm}{!}{$\BlackQueenOnWhite$}}}
\newcommand{\chessking}{\textsuperscript{\resizebox{0.3cm}{!}{$\BlackKingOnWhite$}}}
\newcommand{\chessrook}{\textsuperscript{\resizebox{0.3cm}{!}{$\BlackRookOnWhite$}}}
\newcommand{\chessknight}{\textsuperscript{\resizebox{0.3cm}{!}{$\BlackKnightOnWhite$}}}
\newcommand{\chessbishop}{\textsuperscript{\resizebox{0.3cm}{!}{$\BlackBishopOnWhite$}}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
%\newcommand{\rowgroup}[1]{\hspace{-1.5em}#1}

%\usepackage{todonotes}
\renewcommand{\UrlFont}{\ttfamily\small}

\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}

\newcommand{\todo}[1]{\textcolor{red}{\textsc{To Do:} #1}}

\newcommand{\dataname}{\textsc{CHASE}}
\newcommand{\qaname}{\textsc{CHASE-QA}}
\newcommand{\codename}{\textsc{CHASE-Code}}
\newcommand{\mathname}{\textsc{CHASE-Math}}


\title{How to Get Your LLM to Generate Challenging \\ Problems for Evaluation}

\author{Arkil Patel\chessking{} \qquad Siva Reddy\chessking{}\chessqueen{}\chessknight{}  \qquad Dzmitry Bahdanau\chessking{}\chessqueen{}\chessknight{} \\
\vspace{-4mm} \\
  \normalsize{\raise4ex\hbox{}\chessking{}Mila and McGill University} \\ \vspace{-2mm}\normalsize{\raise4ex\hbox{}\chessqueen{}ServiceNow Research  \quad \raise4ex\hbox{}\chessknight{}Canada CIFAR AI Chair} \\
   \vspace{0mm} \\
\normalsize{ \texttt{\{arkil.patel, siva.reddy, bahdanau\}@mila.quebec}}}

\date{}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The pace of evolution of Large Language Models (LLMs) necessitates new approaches for rigorous and comprehensive evaluation. Traditional human annotation is increasingly impracticable due to the complexities and costs involved in generating high-quality, challenging problems. In this work, we introduce \textbf{\dataname{}}, a unified framework to synthetically generate challenging problems using LLMs without human involvement.  For a given task, our approach builds a hard problem in a bottom-up manner from simpler components. Moreover, our framework decomposes the generation process into independently verifiable sub-tasks, thereby ensuring a high level of quality and correctness. We implement \dataname{} to create evaluation benchmarks across three diverse domains: (1) document-based question answering, (2) repository-level code completion, and (3) math reasoning. The performance of state-of-the-art LLMs on these synthetic benchmarks lies in the range of 40-60\% accuracy, thereby demonstrating the effectiveness of our framework at generating challenging problems. We publicly release our benchmarks and code.\footnote{Data available on \href{https://huggingface.co/collections/McGill-NLP/chase-67b732a9462862d18d7f85bd}{huggingface}, Code available at: \href{https://github.com/McGill-NLP/CHASE}{https://github.com/McGill-NLP/CHASE}}

\end{abstract}

\section{Introduction}

In the past few years, we have witnessed the emergence of powerful Large Language Models (LLMs) \citep{openai2024gpt4technicalreport, geminiteam2024gemini15unlockingmultimodal,dubey2024llama3herdmodels} that exhibit remarkable performance over a wide range of tasks. However, the resources for evaluating these models have not kept pace with their rapid evolution and increased capabilities. Contemporary LLMs have saturated many existing reasoning benchmarks \citep{chen2021evaluatinglargelanguagemodels, gsm8k}. Developing challenging problems for such reasoning tasks can be both expensive and time-consuming, especially for non-expert human annotators. Moreover, there are some settings, such as tasks requiring long-context reasoning over thousands of tokens, where the generation of high quality data by humans is impracticable. Additionally, we are facing a significant depletion of interesting data that is publicly available (such as SAT exams, Olympiad problems, etc.) for creating benchmarks. Hence, we believe that the conventional approach of developing evaluation benchmarks through human annotation and existing Web content is limited, and it is worthwhile to explore using LLMs for generating evaluation data.

Synthetic data generation has emerged as a powerful paradigm in recent years driven by the wide-spread availability of cheaper and faster LLMs that can effectively follow instructions. The focus of most prior works, however, has been on improving models by generating synthetic data for some form of training \citep{phi, wang-etal-2023-self-instruct, xu2024wizardlm}. In contrast, using synthetic data for evaluation has been relatively underexplored.



\begin{figure}[t]
    \centering    \includegraphics[scale=0.095, trim=10 0 0 20, clip]{images/chase_fig.pdf}
    \caption{\textit{Top:} Illustrating the high-level ideas behind our proposed \dataname{} framework. \textit{Bottom left:} Pipeline for creating an example in \qaname{}. \textit{Bottom right:} Pipeline for creating a math word problem in \mathname{}. The pipeline for \codename{} is illustrated in Figure \ref{fig:chase_code} in the Appendix.} \label{fig:chase_main}
\end{figure}

There are considerable advantages in using synthetic data for evaluation: it is comparatively inexpensive, highly scalable, and can be renewed periodically to mitigate contamination concerns. However, there are two main challenges: first, \emph{how can we create \textbf{hard} and \textbf{realistic} problems?} and second, \emph{how can we automatically \textbf{verify the correctness} of the generated data?} Typical synthetic data generation pipelines are either focused on self-improvement or on distillation. In the former, a model generates large-scale synthetic data that provides useful guidance for training. However, a large portion of such model-generated data is incorrect \citep{wang-etal-2023-self-instruct}, which makes it difficult to adapt such pipelines for evaluation. Distillation-based approaches use stronger models to generate complex problems for training weaker models \citep{xu2024wizardlm}. However, we would like to design an approach that allows us to craft problems that are \emph{challenging} to solve even for the LLM that generated them.

In this work, we present the \textbf{\dataname{}} framework: \textbf{CH}allenging \textbf{A}I with \textbf{S}ynthetic \textbf{E}valuations. Our methodology is based on two main ideas (see Figure \ref{fig:chase_main}) geared towards addressing the above-mentioned challenges. First, we create problems in a \emph{bottom-up} manner where we iteratively hide parts of the solution within the problem's context. This makes problems challenging because finding the solution will require multiple steps of drawing inferences or reasoning over a longer context. Second, we decompose the generation process into simpler, \emph{individually verifiable sub-tasks}. 
This facilitates fine-grained verification for correctness at each stage of the generation process.

We implemented our framework to create challenging benchmarks across three diverse domains. (1) \textbf{\qaname{}} is a long-context document-based question answering benchmark. This benchmark simulates the real-world application of answering user queries based on information that is spread across multiple documents, most of which may be irrelevant. (2) \textbf{\codename{}} is a repository-level code completion benchmark. It consists of problems from two different domains: \emph{data pre-processing} functions, and \emph{algorithms}. This dataset attempts to simulate the real-world application of generating code in existing repositories based on user specifications. (3) \textbf{\mathname{}} is a grade-school level math word problems benchmark consisting of problems involving arithmetic reasoning. We built this benchmark to show the utility of our framework in building challenging problems for seemingly easy tasks on which models have already saturated existing benchmarks \citep{gsm8k}.

Experiments with $\mathbf{15}$ contemporary LLMs show that the datasets generated using \dataname{} are challenging for all models. The best performing LLMs only achieve accuracies in the range of $\mathbf{\sim40-60}$\% across the three different domains. We further highlight the utility of \dataname{} by comparing it with other prompting-based data generation baselines such as \emph{Evol-Instruct} \citep{xu2024wizardlm}, which yield data with significant errors, apart from being relatively easier for models to solve. Our results also reveal large gaps in performance between different LLMs, all of which perform similarly on existing benchmarks like MMLU \citep{hendrycks2021measuring} or HumanEval \citep{chen2021evaluatinglargelanguagemodels}. Lastly, using our long-context benchmarks, we show that the performance of LLMs decreases drastically (sometimes by upto 70\%) when we increase the context size beyond 50k tokens. 

\begin{figure}[t]
    \centering    \includegraphics[scale=0.16]{images/chase_examples.pdf}
    \caption{Examples of problems from all three benchmarks created using \dataname{}.} \label{fig:chase_examples}
\end{figure}

\section{Related Work}

\paragraph{Synthetic data generation.} Prior works have explored generating synthetic data for various stages of an LLM's development pipeline: pre-training \citep{phi, cosmopedia}, instruction-finetuning \citep{wang-etal-2023-self-instruct, xu2024wizardlm}, alignment \citep{bai2022constitutionalaiharmlessnessai,leerlaif} and task-specific fine-tuning \citep{pmlr-v235-wei24h, yu2024metamath}. The main focus of our work, however, is to generate high-quality challenging problems for evaluation and benchmarking. There is very limited existing literature in this area. \citet{sprague2024musr} created a narrative-based question answering (QA) benchmark using a neuro-symbolic pipeline that first samples facts, and then uses an LLM to generate a narrative. \citet{bohnet2024longspanquestionansweringautomaticquestion} created a synthetic QA benchmark by extracting entities and their reference chains from existing stories and then prompting an LLM to generate questions over them. \citet{gu2024cruxevalbenchmarkcodereasoning} create a code understanding benchmark by prompting CodeLlama \citep{rozière2024codellamaopenfoundation} to generate python functions and their inputs and designing tasks based on predicting either the input or the output for a given function. In contrast to these works, we focus on presenting a general framework to design scalable pipelines to create challenging benchmarks across multiple domains. Moreover, we focus on realistic tasks such as information-seeking QA and repository-level code generation, both with long contexts that we generate completely from scratch.

\paragraph{Task-specific synthetic data.} Recent works have explored generating synthetic datasets for content-grounded QA tasks. \citet{pmlr-v162-dai22a} 
 use an LLM to develop information-seeking dialogue datasets based on text extracted from Wikipedia and the Web. \citet{genie} prompt an LLM with wikipedia text to generate a question-answer pair. In contrast, we design a benchmark for document-based information-seeking questions that model realistic situations. Moreover, our pipeline generates entire the documents using LLMs, allowing a higher degree of control. There has also been significant interest in generating synthetic data for code. \citet{yu-etal-2024-wavecoder} employ an LLM-based framework to create examples for tasks based on existing raw code data. \citet{pmlr-v235-wei24h} generate code instruction data by prompting an LLM with seed code snippets from existing repositories. In this work, we focus on repository-level code completion, where we generate the repository contexts completely from scratch. Moreover, we also synthetically generate the corresponding test code to evaluate each example. Generating synthetic data to improve math reasoning has recently been a very active area of research. Previous work has explored generating new math problems by prompting LLMs with examples from existing datasets \citep{liu2023tinygsmachieving80gsm8k, yu2024metamath, lu-etal-2024-mathgenie}. Similar to our work, some prior works have focused on creating challenging math problems. \citet{ye2024physicslanguagemodels21} and \citet{zhou2025gsminfinitellmsbehaveinfinitely} use computational graphs to craft problems of high complexity. While the problem contexts in these benchmarks state the entity-value relationships directly, we focus on generating more traditional word problems that require reasoning about the complexities and ambiguities arising from unconstrained use of natural language. \citet{shah2024aiassistedgenerationdifficultmath} employ a human-in-the-loop approach to generate novel and difficult math problems by prompting LLMs with multiple core skills that must be used in the problem. \citet{liu2024augmentingmathwordproblems} employ iterative question composition where they iteratively prompt an LLM with a seed question to generate more complex variations of it. In this work, we design a completely automated pipeline to craft grade-school level math problems that are challenging to solve even for the LLM that generated them.


\section{The \dataname{} Framework and Benchmarks}\label{sec:framework}

Our framework for generating synthetic data is based on two key ideas as illustrated in Figure \ref{fig:chase_main}.\\

\noindent \textbf{1. Bottom-up problem creation.} 

\noindent We abandon the forward-thinking approach of first creating a difficult problem and then obtaining the corresponding solution, as followed by most works that create synthetic data for training \citep{xu2024wizardlm, liu2024augmentingmathwordproblems}. If we first synthesize a complex problem and then obtain its corresponding solution from the generating LLM itself, then that problem is inherently solvable by that LLM.  However, we wish to craft problems that are challenging even for the model which generates them. Hence, we instead take a different approach where we either generate or start with a simpler problem-solution pair, and then \emph{bottom-up} build a challenging context. We make the problem's context challenging by systematically \emph{hiding} components of the solution or reasoning such that they need to be either extracted from a long context or inferred based on given information.\\

\noindent\textbf{2. Decomposition into simpler, verifiable sub-tasks.}

\noindent We design pipelines that break down the generation process into simpler sub-tasks. Each individual LLM in the pipeline (i.e., each inference call) performs a simpler, specific function in the generation process. This provides us with multiple benefits. First, it grants us more control over each step of the generation process. We can treat each step as a task by itself and optimize the corresponding inference parameters individually. This also allows us to better manage the complexity and diversity of the generated data depending on our requirements. Second, and perhaps more importantly, it facilitates fine-grained verification. We deploy LLMs that are not part of the generation process to check the correctness and quality of the generated data at each step. We believe that LLMs can be relied upon for verification because our framework makes each verification task smaller and simpler compared to the main task of generating or solving the problem we are crafting.\\

We show the effectiveness and ease of adaptation of our framework by implementing it to create challenging problems across three diverse domains. \textbf{\qaname{}} is a document-grounded question answering task consisting of $671$ problems. Each example in \qaname{} consists of a set of documents and a question-answer pair, as illustrated in Figure \ref{fig:chase_examples} left. Models need to reason over a long context (more than 6k tokens) because the relevant information is spread across multiple documents. \textbf{\codename{}} is a repository-level code completion benchmark consisting of $500$ problems. Given a repository of Python functions, the task is to implement a new function based on a set of objectives, as shown in Figure \ref{fig:chase_examples} centre. We create data for two domains: (1) \emph{data pre-processing} operations such as dataframe manipulation, string processing, etc., and (2) \emph{algorithms} such as graph operations, array manipulations, etc. \textbf{\mathname{}} consists of $500$ grade-school level math word problems involving only basic arithmetic operations. An example of the task is provided in Figure \ref{fig:chase_examples} right.

\section{Construction Pipelines}

In this section, we discuss our implementation of the \dataname{} framework for all three domains. Our pipelines use two different LLMs: the generator $\mathbf{G}$ and verifier $\mathbf{V}$.

\subsection{Constructing \qaname{}}

We generate \qaname{} completely from scratch without relying on existing contexts or any seed examples from previous datasets. Following the \dataname{} framework, we create each example in a bottom-up manner by first generating the question-answer pair, and then generating the corresponding documents. Our pipeline for creating \qaname{} is illustrated in Figure \ref{fig:chase_main} bottom left. We describe it in detail below. The prompts are provided in Appendix \ref{app:prompts_qa}.

\paragraph{Generating diverse scenarios.} We generate a set of diverse realistic scenarios in which a \emph{user persona} seeks to find some information from a \emph{collection of documents}. For example, a `grad student in NYC' searching the `laws on renting and subletting'. We prompt $\mathbf{G}$ to generate scenarios in the form of a tuple $(\texttt{persona},\ \texttt{collection\_name})$ by bootstrapping it with $5$ annotated scenarios, and later prompting it with its own generated scenarios.
% The exact prompt is provided in Figure \ref{prompt:qa_scenarios} in Appendix \ref{app:prompts}.

\paragraph{Generating question-answer (QA) pairs.} We design programmatic prompts with a given scenario as the variable to prompt $\mathbf{G}$ to generate a realistic information-seeking question that the \texttt{persona} might want to know about from \texttt{collection\_name} set of documents. For example, a `grad student' might pose the question, `what is the procedure for refusing increase in rent?', whose answer can be found spread across multiple documents on the government's laws on renting. Additionally, $\mathbf{G}$ must generate the corresponding answer. We prompt $\mathbf{G}$ to generate questions and answers where the answers are a composition of multiple points or ideas. Lastly, $\mathbf{G}$ must generate the outline of the documents (only title and abstract) which will contain the answer. The idea is that it must separate out the answer points and assign them to these different documents.
% The exact prompt for generating the question-answer (QA) pair is provided in Figure \ref{prompt:qa_qa} in Appendix \ref{app:prompts}.

\paragraph{Generating irrelevant information.} To make the task more challenging, for each QA pair, we prompt $\mathbf{G}$ to generate other QA pairs where the answer is of a similar type as the ground-truth answer. An example of a similar question for our running example with the grad student is, `how do I increase the rent for an appartment I am subletting?'. The intuition is that the corresponding answers to such similar questions will be of a similar flavour to the ground-truth answer, but ultimately \emph{irrelevant} for answering the question. This will make the generated data challenging since it will confuse the model when all of this similar type of information is spread across a long context. It is, however, important to verify that none of this generated irrelevant information is actually relevant for the question (otherwise it will make our ground-truth answer incomplete). We individually prompt $\mathbf{V}$ with the original question and each of the supposed irrelevant information points to check if any part of them is relevant for answering the question (see Figure \ref{fig:chase_main} bottom left for an example of an irrelevant point discarded by $\mathbf{V}$ because it was relevant for the original question).

\paragraph{Generating documents.} For each example, we have generated a QA pair, along with some similar but irrelevant QA pairs. For each of these QA pairs, we separately prompt $\mathbf{G}$ to generate long documents where the documents must discuss the corresponding answer points assigned to it, along with many other irrelevant points. Together, all these documents form up the context for that example. We verify two things to ensure the correctness of the task: (1) none of the documents should contain any extra information related to the question, apart from the ground-truth answer points, and (2) all of the ground-truth answer points must be discussed in the documents. We do this by rigorously prompting $\mathbf{V}$ with individual documents and ground-truth answer points.


\subsection{Constructing \codename{}}

We generate \codename{} completely from scratch without relying on existing contexts or any seed examples from previous datasets. Our pipeline for creating \codename{} is shown in Figure \ref{fig:chase_code} in the Appendix. We describe it in detail below. The prompts are provided in Appendix \ref{app:prompts_code}.

\paragraph{Generating Python functions.} We begin by first generating a set of diverse and realistic Python functions. We prompt $\mathbf{G}$ to generate Python functions for a particular domain by bootstrapping it with $3$ annotated functions in that domain, and later prompting it with its own generated functions. These generated functions will act as the \emph{helper} functions in the repository context which may or may not be called in the answer code function. Given each generated helper function, we prompt $\mathbf{V}$ to generate a Python code which initializes sample inputs for the function and then calls it using them. We then execute this code to verify whether the generated helper function executes correctly.

\paragraph{Generating problem statement and answer code.} To create a single example, we randomly sample $n$ of the previously generated helper functions, and prompt $\mathbf{G}$ to create a complex function that calls at least $k$ of these provided helper functions (hereafter called \emph{relevant} helper functions) apart from implementing additional logic. This complex function is our \emph{answer code}. Additionally $\mathbf{G}$ must elaborate in natural language what objectives the complex function achieves, which forms our \emph{problem statement}. Similar to the case of helper functions, we prompt $\mathbf{V}$ to generate test code to check if the generated answer code executes correctly. To verify whether the problem statement sufficiently specifies the answer code, we prompt $\mathbf{V}$ with the problem statement and corresponding \emph{relevant} helper functions and check whether the output is semantically equivalent to the answer code (using the test code obtained in the next step).

\paragraph{Generating test code.} To enable automatic execution-based testing, we prompt $\mathbf{G}$ with the generated answer function to implement a test code for it in Python. The test code must independently implement the logic of the answer code. It must then initialize the parameters of the answer function with sample values, and compare the output with its own implementation. We execute the generated test code to check if the answer code passes. We discard all examples for which (1) the test code does not execute properly, or (2) the test code executes but the answer code fails the test.

\paragraph{Building code repository.} For each example, we build a unique repository of Python files. The repository consists of the \emph{relevant} helper functions spread across different files, along with $m$ randomly sampled irrelevant Python functions from our previously generated set. The core difficulty of this task arises from understanding the entire long context of code functions, and identifying which ones are relevant for the provided problem statement.


\subsection{Constructing \mathname{}}

We sample math word problems (MWP) from existing datasets as seed examples to build our benchmark. Following \dataname{}, we bottom-up build a complex problem by iteratively increasing the reasoning depth of the problem. Our pipeline used for creating \mathname{} can be seen in Figure \ref{fig:chase_main} bottom right. We describe it in more detail below. The prompts are provided in Appendix \ref{app:prompts_math}.

\paragraph{Breaking down seed MWP.} A seed MWP $s$ is characterised by the tuple $s = (p, a)$ where $p$ is the problem, and $a$ is the answer. We prompt $\mathbf{G}$ to break down $p$ into two parts: the context $c$, which provides all the information, and the question $q$, which asks about some unknown quantity.

\paragraph{Create continuation of MWP.} We prompt $\mathbf{G}$ with an initial seed MWP $s_0 = (p_0, a_0)$ to build a new problem which is a continuation of the previous problem. More precisely, $\mathbf{G}$ should output a new problem $s_1 = (p_1, a_1)$, where the context of $p_1$, i.e., $c_1$ assumes $a_0$ as given information (without explicitly stating it). For example, in Figure \ref{fig:chase_main} bottom right, the model assumes \emph{Jack has 4 pens} as given information, and  creates a new continuation context, \emph{Jill has thrice as many pens as Jack has now}. The model also generates a new question $q_1$, \emph{how many pens does Jill have?} whose answer $a_1 = 12$ is obtained by performing an arithmetic operation (here, \emph{multiplication by 3}) over $a_0=4$.


\paragraph{Combining seed MWP with its continuation.} By combining the seed problem with its continuation, we get a new MWP $s = (p, a)$ with a higher reasoning depth, where the context $c$ of the combined problem $p$ is a concatenation of the contexts of the seed problem and the continuation $c = c_0 \cdot c_1$. The question for the combined problem will be the one generated by the model, i.e., $q_1$, and the answer $a = a_1$. We refer to Figure \ref{fig:chase_main} bottom right for illustration.

\paragraph{Iteratively increase reasoning depth.} We increase the reasoning depth of a given seed MWP by creating new continuations in an iterative manner. Each new continuation $s_i$ formed after the $i^{\text{th}}$ iteration becomes the seed problem for the $(i+1)^{\text{th}}$ iteration. The final problem after $j$ successful iterations, i.e., with a reasoning depth of $j+1$, is given by context $c = c_0 \cdot c_1 \dots c_j$, question $q_j$, and answer $a = a_j$.

Since each new problem created by $\mathbf{G}$ has a low reasoning depth of the same difficulty as the problems in the seed datasets, we verify their correctness using a non-identical ensemble of verifier models $\{\mathbf{V_1}, \mathbf{V_2}, \dots, \mathbf{V_n}\}$, each of which performs well on the seed dataset. We prompt each $\mathbf{V_k}$ with the generated context $c_i$ and question $q_i$ and check whether the prediction is the same as the generated answer $a_i$. If this fails for any verifier, we discard $s_i$ and begin again with $s_{i-1}$ as the seed MWP (see Figure \ref{fig:chase_main} bottom right).


\section{Experiments}

\subsection{Implementation Details}

\paragraph{Generating \qaname{}.} We use GPT-4o \citep{openai2024gpt4technicalreport} as the generator $\mathbf{G}$, and GPT-4o-mini as the verifier $\mathbf{V}$. We first sampled $500$ unique scenarios. For each scenario, we generate $2$ QA pairs. For each of the resulting $1000$ unique QA pairs, we obtain \emph{irrelevant} information by generating $4$ similar QA pairs. We then generate the corresponding documents containing the ground-truth answer as well as irrelevant information for each of the $1000$ examples. To increase the complexity of the resulting benchmark, we carry out a form of rejection sampling. We evaluate GPT-4o-mini twice on the task, and randomly discard half of the problems on which it was correct both times. This yielded the final benchmark of $671$ examples.

\paragraph{Generating \codename{}.} We use GPT-4o-mini \citep{openai2024gpt4technicalreport} as the generator $\mathbf{G}$, and Gemini-1.5-Flash as the verifier $\mathbf{V}$. We made this choice because generating even a small amount of challenging code problems required a large number of iterations, since a lot of the model-generated code at various stages would fail to execute or be semantically incorrect. For each domain, we first sampled $500$ different helper functions that execute without errors. Then we prompt the model with $n=10$ random helper functions to generate a problem statement and corresponding answer code that calls at least $k=4$ helper functions. We do this to create $1000$ different examples for each domain. Next, we generate up to $10$ test codes for each example and keep only those examples for which a generated test code successfully passed for the corresponding answer code. We also carry out the verification of correctness of problem statement as describe before. This way, we end up with $290$ examples for the \emph{algorithms} domain and $300$ examples for the \emph{data pre-processing} domain. We again use GPT-4o-mini for rejection samping and randomly discard around half of the problems on which it was correct. This way, we end up with a total of $500$ examples in the benchmark, with $250$ examples for each domain. For each example, we randomly sample $m=100$ \emph{irrelevant} helper functions and distribute them into $10$ Python files to constitute the repository context.

\paragraph{Generating \mathname{}.} We use GPT-4o-mini \citep{openai2024gpt4technicalreport} as the generator $\mathbf{G}$, and an ensemble of Gemini-1.5-Flash and Llama-3.1-70B as the verifier $\mathbf{V}$. In practice, we observed that many of the model generated problems would fail at various stages of verification, so it is faster and cheaper to query the smaller models. We start with $2.3$k seed problems taken from the test sets of GSM8k \citep{gsm8k} and SVAMP \citep{svamp}. We set the maximum and minimum reasoning depth at $8$ and $2$ respectively. For each problem, we iterate $15$ times to generate a problem continuation. Note that many of these iterations fail to produce a correct continuation of the problem, in which case we discard that generation and retry from that point in the subsequent iteration. We carry out this process $3$ times. In this manner, we generated around $1500$ problems. We then carry out rejection sampling and roughly discarded $75$\% of the problems that GPT-4o-mini could solve. In the end, we end up with a total of $500$ challenging MWPs.

\paragraph{Task parameters.} For \qaname{} and \codename{}, we prompt models with the instruction for the task, along with the corresponding long-context and question. The prompt formats are provided in Figure \ref{prompt:qa_solve} and \ref{prompt:code_solve} respectively in Appendix \ref{app:prompts}. For \mathname{}, we prompt models with 8-shot chain-of-thought \citep{cot} as shown in Figure \ref{prompt:math_solve} in Appendix \ref{app:prompts_math}. We decode for a maximum of $1024$ tokens with a temperature of $0.5$.

\begin{figure*}[t]
    \centering
    \begin{minipage}[c]{0.48\textwidth}
        \renewcommand{\arraystretch}{1.5}
        \captionof{table}{The performance of various LLMs on all $3$ domains of the \dataname{} benchmark. We measure the accuracy of the predictions for \qaname{} and \mathname{}, and pass@1 for \codename{}. \textsc{Data} and \textsc{Algo} refer to the \emph{data pre-processing} and \emph{algorithms} sub-domains of \codename{}. Numbers in \textbf{bold} indicate best performance on domain while \underline{underline} indicates best-in-class performance.} 
        \label{tab:main_results}
        \footnotesize{
        \resizebox{\textwidth}{!}{%
            \begin{tabular}{m{8.3em} >{\centering\arraybackslash}m{2.5em} | >{\centering\arraybackslash}m{2.5em}>{\centering\arraybackslash}m{2.5em} | >{\centering\arraybackslash}m{2.5em}}
			\hline
			\multirow{2}{*}{\textsc{\textbf{Models}}} & \multirow{2}{*}{\textsc{\textbf{QA}}} & \multicolumn{2}{c|}{\textsc{\textbf{Code}}} & \multirow{2}{*}{\textsc{\textbf{Math}}} \\
            & & \textsc{\textbf{Data}} & \textsc{\textbf{Algo}} & \\
			\hline
			Gemini-1.5-Pro & \textbf{\underline{63.2}} & \textbf{\underline{35.6}} & \textbf{\underline{40.8}} & \textbf{\underline{65.4}} \\
			\hdashline[0.5pt/2pt]
			GPT-4o & 55.3 & 26.8 & 22.4 & 59.8  \\
            \hdashline[0.5pt/2pt]
			Claude-3.5-Sonnet & 36.1 & 19.6 & 25.2 & 64.2  \\
            \hline
            Gemini-1.5-Flash & \underline{55.1} & \underline{25.6} & \underline{31.6} & 56.6  \\
            \hdashline[0.5pt/2pt]
            GPT-4o-mini & 50.2 & 19.6 & 18 & 48.4  \\
            \hdashline[0.5pt/2pt]
            Claude-3-Haiku & 32.6 & 18 & 25.6 & 44.2  \\
            \hdashline[0.5pt/2pt]
            Llama-3.1-70B & 41.3 & 12.4 & 18.8 & 53.4  \\
            \hdashline[0.5pt/2pt]
            Mistral Large 2 & 34.1 & 4.8 & 5.2 & \underline{59.6}  \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-72B & 38.3 & 14.4 & 8.4 & 58.4  \\
            \hdashline[0.5pt/2pt]
            Command R+ & 41.7 & 0 & 0 & 43.2  \\
            \hdashline[0.5pt/2pt]
            DBRX & 15.7 & 1.2 & 3.2 & 21.6  \\
            \hdashline[0.5pt/2pt]
            Phi-3.5-MoE & 10.6 & 0.4 & 1.2 & 39.4  \\
            \hdashline[0.5pt/2pt]
            % Gemma-2-27B\footnotemark & 27.6 & - & - & 47.33  \\
            % \hdashline[0.5pt/2pt]
            Mistral Small & 35.5 & 1.2 & 1.6 & 50.6  \\
            \hline
            Llama-3.1-8B & \underline{25.2} & \underline{0.8} & 3.2 & 32.2  \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-7B & 22.2 & 0 & \underline{4.4} & \underline{42.8}  \\
			\hline
		\end{tabular}
        }
        }
    \end{minipage}%
    \hfill
    \begin{minipage}[c]{0.48\textwidth}
        \centering
        \subfloat{{\includegraphics[scale = 0.38, trim=0 0 0 0, clip]{images/qa_context.pdf}}} \\[0.5\baselineskip]

        \subfloat{{\includegraphics[scale = 0.38, trim=0 0 0 0, clip]{images/code_context.pdf}}}
        \nextfloat
        \caption{Performance of LLMs decreases uniformly with increasing context sizes for the 100 example subset of \qaname{} (\textit{top}) and the 55 example subset of \codename{} (\textit{bottom}).}
        \label{fig:context_vary}
    \end{minipage}%
\end{figure*}


\paragraph{Evaluation.} The ground-truth answers for \qaname{} are verbose text, organized in bullet points. While this simulates real-world complexity, it also makes evaluation difficult. Since it is intractable to employ expert humans for evaluation, we deploy an LLM-as-a-judge to automatically assess the correctness of predictions. A prediction is considered to be correct if and only if it is (1) \emph{complete}, i.e., it includes all the points mentioned in the ground-truth answer, and (2) \emph{relevant}, i.e., it provides information only pertaining to the current question. We use GPT-4o as the judge and measure the accuracy as the percentage of predictions judged to be correct. The prompt format used for evaluation is provided in Figure \ref{prompt:qa_evaluation} in Appendix \ref{app:prompts_qa}. For \codename{}, we measure the pass@1 execution accuracy, i.e., whether the model generated code correctly passes when we execute the corresponding test code in the first attempt. For \mathname{}, we measure the exact match accuracy against the ground-truth numerical answer.

\paragraph{Models.} We evaluated a total of $15$ different LLMs: Gemini-1.5-Pro and Flash \citep{geminiteam2024gemini15unlockingmultimodal}, GPT-4o and GPT-4o-mini \citep{openai2024gpt4technicalreport}, Claude-3.5-Sonnet \cite{claude3.5}, Claude-3-Haiku \citep{claude3}, Llama-3.1 8B and 70B \citep{dubey2024llama3herdmodels}, Mistral Small and Large 2 \citep{mistral}, Qwen2.5 7B and 72B \citep{qwen2, qwen2.5}, Cohere Command R+ \citep{cohere}, DBRX-Instruct \citep{dbrx}, and Phi-3.5-MoE \citep{abdin2024phi3technicalreporthighly}. Implementation details are provided in Appendix \ref{app:imple}.

\subsection{Results and Discussion}\label{sec:results}

% \subsubsection{Performance of Models}

\paragraph{Performance of models.} Table \ref{tab:main_results} shows the performance of all $15$ LLMs on all three bechmarks. For \qaname{}, all models, including the generator (and judge) GPT-4o, find the task challenging. The best performing model is Gemini-1.5-Pro which achieves only about $63$\% accuracy, suggesting massive room for improvement. Models struggle even more on \codename{}, with the best performing model only achieving $38.2$\% average accuracy. For \mathname{}, we see that even the most powerful LLMs only achieve $65.4$\% accuracy on a benchmark composed of grade-school level math word problems. Overall, these results clearly indicate the utility of the \dataname{} framework in crafting challenging problems that even state-of-the-art LLMs struggle to solve. We provide examples of errors made by Gemini-1.5-Pro on all three benchmarks and analyze them in Appendix \ref{app:errors}.

On all benchmarks, we see huge variations in performance between the models. These results highlight our framework's potential for differentiating between state-of-the-art LLMs that all perform similarly on standard benchmarks like MMLU \citep{hendrycks2021measuring} or HumanEval \citep{chen2021evaluatinglargelanguagemodels}. We further note some interesting observations. On both long-context benchmarks, there is a substantial gap between the Gemini models and other LLMs, clearly exhibiting the strong long-context reasoning capabilities of Gemini. Another interesting observation is that most LLMs are stronger on the \emph{algorithms} domain, while GPT-4o is stronger at \emph{data pre-processing}. This demonstrates the utility of our benchmark at identifying such targeted differences in performance which could be very helpful for real tasks. For math reasoning, we have seen weak LLMs like Llama-3.1-8B and Phi-3.5 get around $85-90$\% accuracies on the popular GSM8k and SVAMP benchmarks. However, we see a large difference ($\sim25-30$\%) between their performance and that of the state-of-the-art models, which may indicate contamination of existing benchmarks \citep{zhang2024careful}.

% \subsubsection{Impact of Context Size}

\begin{figure*}[t]
    \centering
    \begin{minipage}[c]{0.48\textwidth}
    \centering
    \renewcommand{\arraystretch}{1.5}
    \captionof{table}{Performance of LLMs on data generated by direct prompting approaches without using \dataname{}.} 
    \label{tab:baseline}
    \footnotesize{
    \begin{tabular}{m{8.5em} | >{\centering\arraybackslash}m{4em} | >{\centering\arraybackslash}m{4em}}
    \toprule
        \textbf{\textsc{Model}} & \textbf{\textsc{QA}} & \textbf{\textsc{Math}} \\
        \hline
        Gemini-1.5-Pro & 81 & 85.7 \\
        \hdashline[0.5pt/2pt]
        GPT-4o & 78 & 88.9 \\
        \hdashline[0.5pt/2pt]
        Claude-3.5-Sonnet & 73 & 82.5 \\
        \hline
    \end{tabular}
    }
    \end{minipage}%
    \hfill
    \begin{minipage}[c]{0.48\textwidth}
    \renewcommand{\arraystretch}{1.5}
    \centering
    \captionof{table}{Accuracy of LLMs increases marginally on \mathname{} when fine-tuned on data generated by Llama-3.1-8B.} 
    \label{tab:finetune}
    \footnotesize{
    \begin{tabular}{m{6em} | >{\centering\arraybackslash}m{4em} | >{\centering\arraybackslash}m{6em}}
    \toprule
        \textbf{\textsc{Model}} & \textbf{\textsc{Base}} & \textbf{\textsc{Fine-tuned}} \\
        \hline
        Llama-3.1-8B & 30 & 34.7 \\
        \hdashline[0.5pt/2pt]
        Mistral-7B & 3.3 & 4.7 \\
        \hdashline[0.5pt/2pt]
        Qwen2-7B & 12.7 & 15.3 \\
        \hline
    \end{tabular}
    }
    \end{minipage}%
\end{figure*}

% \subsection{Human Verification}

% \begin{wraptable}{r}{0.43\textwidth}
%     \centering
%     \renewcommand{\arraystretch}{1.25}
%     \captionof{table}{Accuracy of LLMs increases on \mathname{} when fine-tuned on data generated by Llama-3.1-8B.} 
%     \label{tab:finetune}
%     \footnotesize{
%     \begin{tabular}{m{6em} | >{\centering\arraybackslash}m{2.5em} | >{\centering\arraybackslash}m{6em}}
%         \textbf{\textsc{Model}} & \textbf{\textsc{Base}} & \textbf{\textsc{Fine-tuned}} \\
%         \hline
%         Llama-3.1-8B & 30 & 34.7 \\
%         \hdashline[0.5pt/2pt]
%         Mistral-7B & 3.3 & 4.7 \\
%         \hdashline[0.5pt/2pt]
%         Qwen2-7B & 12.7 & 15.3 \\
%         \hline
%     \end{tabular}
%     }
% \end{wraptable}

\paragraph{Direct generation baseline.} We experimented with directly prompting models to generate challenging data for the QA and math tasks, without using the \dataname{} framework. For QA, we prompt GPT-4o with unique examples from \qaname{} as the seed task and instruct it to generate new examples in a manner similar to \citet{honovich-etal-2023-unnatural} and \citet{wang-etal-2023-self-instruct}. For math, we adapt the \emph{Evol-Instruct} method \citep{xu2024wizardlm} to generate more complex problems given seed examples from GSM8k. We carry out the same proportion of rejection sampling as we did for \qaname{} and \mathname{} for fair comparison. We generated a total of $100$ examples for both tasks. For the math task, we manually examined the generated problems and found that $34$ of them had some kind of error such as the problem text being ambiguous or vague or the reasoning and answer being incorrect. Carrying out a detailed manual verification for the QA problems is impracticable, however, we believe it is highly likely that a significant portion of it is incorrect. We evaluated GPT-4o, Gemini-1.5-Pro, and Claude-3.5-Sonnet on these datasets and provide the results in Table \ref{tab:baseline}. For both tasks, we observe that we are unable to generate challenging data by direct prompting baselines.

% \paragraph{Quality and impact of \dataname{} benchmarks.} For checking \emph{correctness}, we manually reviewed $100$ examples from each of the $3$ benchmarks. We found only $6$ errors in \qaname{}, $3$ errors in \codename{} and $7$ errors in \mathname{}. We believe these error rates are sufficiently low, thereby encouraging reliance on \dataname{}-generated benchmarks for evaluation. The \emph{difficulty} of our benchmarks is apparent from the results in Table \ref{tab:main_results}. We also provide task-specific intrinsic complexity metrics for each benchmark in Appendix \ref{app:complexity_metrics}. Apart from providing a high level of quality, we believe the \dataname{} benchmarks help advance the evaluation paradigms in their respective domains significantly. \qaname{} provides the first QA benchmark that reasons over long documents of publicly unavailable text, where only some of the documents contain relevant information. Unlike contemporary repo-level benchmarks \citep{jimenez2024swebench}, \codename{} is not bottle-necked by the availability of high-quality repositories and existing tests, and allows targeted evaluation of code generation abilities. \mathname{} raises the bar for reasoning about simple math operations in natural language word problems.

\paragraph{Impact of context size.} We studied the impact of varying the context size for long-context reasoning. For each example in a randomly-sampled $100$-example subset of \qaname{}, we increase the context size by concatenating the documents in that example with irrelevant documents randomly sampled from other examples. For \codename{}, we create a subset of $55$ randomly-sampled examples for each of the domains and increase the context size by concatenating irrelevant code functions in the corresponding repository context. Figure \ref{fig:context_vary} plots the performances of $4$ LLMs across different context sizes. For both benchmarks, we see a consistent and significant decrease in model performance as we scale up the context size. Hence, even though most modern LLMs have large context sizes (upwards of 128k), they still struggle to reason even at the scale of 30-40k tokens.

\paragraph{Human verification of LLM judgements.} We measure the correlation of the GPT-4o evaluator's judgement and $3$ human annotators over $100$ randomly sampled predictions made by Gemini-1.5-Pro on \qaname{}. The accuracy of GPT-4o's judgement as measured against the majority vote of the annotators was $91$\%. Moreover, Cohen's kappa \citep{cohen1960coefficient} between the majority vote of the annotators and the LLM judge came out to be $0.82$, which indicates almost-perfect agreement. Additional details regarding the setup of these experiments can be found in Appendix \ref{app:imple}.

\paragraph{Fine-tuning smaller models.} We study whether we can use smaller models (around 7B scale) to generate useful fine-tuning data for themselves following \dataname{} pipelines to perform better on evaluation benchmarks created by stronger models. We generate $\sim10$k math problems using Llama-3.1-8B as both the generator and the verifier and fine-tune $3$ small models. Table \ref{tab:finetune} shows the accuracies on \mathname{} before and after fine-tuning. We see marginal performance improvements across all models. These results indicate that the evaluation data generated using significantly stronger models cannot be easily solved by such weak models even when fine-tuned on data generated by themselves using the exact same pipeline.

% the  seem to indicate that  simply fine-tuning on data created by small models is not sufficient to counter using the same pipeline as the one used to create the evaluation benchmark is not necessarily sufficient to significantly improve performance.


% \paragraph{Synthetic data for code generation.}

% \paragraph{Synthetic data for math reasoning.} 

% Moreover, our extensive verification aspects of the pipeline ensure a very high level of correctness for the generated data.

\section{Conclusion}

In this work, we presented \dataname{}, a framework to synthetically generate challenging problems for evaluation. Our framework offers multiple benefits. First, it is scalable and can efficiently generate hundreds of examples. Second, it is renewable and can be used to regenerate fresh datasets for a given task at periodic intervals, thereby mitigating contamination concerns. Third, it can effectively help evaluate tasks (such as long-context reasoning) which are inherently difficult for humans to evaluate. Fourth, it provides a high level of quality (compared to existing synthetic data generation approaches) because of extensive verification. And finally, it can be used to generate \emph{difficult} examples for a given task. We implemented \dataname{} on three different domains: document-based question answering, repository-level code completion, and math reasoning. Empirically, we showed that \dataname{} successfully generates examples that are difficult even for the state-of-the-art LLMs to solve. We believe the \dataname{} benchmarks also help advance the evaluation paradigms in their respective domains significantly. \qaname{} provides the first QA benchmark that reasons over long documents of publicly unavailable text, where only some of the documents contain relevant information. Unlike contemporary repo-level benchmarks, \codename{} is not bottle-necked by the availability of high-quality repositories and existing tests, and allows targeted evaluation of code generation abilities. \mathname{} raises the bar for reasoning about simple math operations in natural language word problems. Our results raise several interesting questions to explore in future work: (a) How can we modify this framework to easily adapt to different tasks? (b) How can we verify LLM generations more effectively? We hope our work will catalyze many such studies on using synthetic data for evaluation.


\section{Limitations}

\paragraph{Quality and Correctness.} Since we do not manually verify every example in the benchmarks and only rely on LLM-based \emph{soft} verification, it is likely that for some proportion of examples, the question is objectively ambiguous or the ground-truth annotation is incorrect. We briefly discuss manual verification in Appendix \ref{app:verify}. Note that this work is a preliminary exploration into using synthetic data for evaluation. We believe that errors at a small scale are acceptable considering the other advantages of the framework. We look forward to future studies developing more effective automatic verification strategies. Further note that some of the examples we generated using \dataname{}, while being semantically correct, use unnatural or difficult-to-parse language which may lead to ambiguity. This is a general trait of text generated from LLMs, and our framework is unfortunately susceptible to it.



\paragraph{Size of benchmarks.} The datasets we release are comparatively smaller in size. Our framework necessitates querying the generator and especially the verifier many times for crafting each example. While this increases the quality and correctness of the data, it significantly increases the cost of generation. Moreover, a large portion of the intermediate generations in our pipeline are discarded because of extensive verification, which significantly reduces the yield. Further note that we wanted to keep our benchmarks accessible. It would have been prohibitively expensive to run experiments on long-context benchmarks with a large number of examples. Our focus in this work is to present the \dataname{} framework and we believe our experiments, albeit on smaller-sized datasets, convincingly show its utility in generating challenging problems for evaluation. 
% Small benchmarks, if generated with a high bar for quality and correctness, can still be very impactful as evidenced by past works like HumanEval \citep{chen2021evaluatinglargelanguagemodels}. 

\paragraph{Adaptability.} While we have shown how we implemented \dataname{} on three different domains, it is not trivial to adapt the framework to other tasks. Although the high level ideas behind \dataname{} are easy enough to follow, it takes multiple trials and errors to design a working pipeline for any given task. However, we are optimistic that advances in LLMs' abilities to more precisely follow instructions will make such pipelines easier to construct in the future.

\section*{Acknowledgments}

Arkil was partly supported by the Canada Graduate Scholarship – Master’s (CGS-M) funded by the Natural Sciences and Engineering Research Council of Canada (NSERC). We thank our colleagues at Mila and McGill University for helpful
discussions and for providing valuable feedback.

% \newpage

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix

\section{Roadmap}\label{app:roadmap}

The appendix is organized as follows. 


\begin{itemize}
    \item In Section \ref{app:imple}, we provide the implementation details for our experiments.
    % \item In Section \ref{app:ethics}, we discuss ethical considerations surrounding our work.
    \item In Section \ref{app:extra_exp}, we discuss some additional experimental results.
    \item In Section \ref{app:rel_work}, we discuss some additional related work.
    \item In Section \ref{app:errors}, we analyze errors made by LLMs while generating and solving \dataname{} benchmarks.
    \item In Section \ref{app:prompts}, we provide the exact prompts used in this work.
    
\end{itemize}

\section{Implementation Details}\label{app:imple}

Our code is implemented in PyTorch \citep{pytorch} and makes use of the HuggingFace Transformers library \citep{huggingface} and the vLLM library \citep{kwon2023efficient} for running efficient inference locally on LLMs. All experiments with open models were done on our cluster with $8$ NVIDIA A6000 GPUs with $48$ GB memory. Experiments using GPT-4o and GPT-4o-mini were carried out using the OpenAI API.\footnote{\href{https://platform.openai.com}{https://platform.openai.com}}. Experiments using Gemini-1.5-Pro and Gemini-1.5-Flash were carried out using the Google AI Studio.\footnote{\href{https://aistudio.google.com}{https://aistudio.google.com}} Experiments with Claude-3.5-Sonnet and Claude-3-Haiku were carried out using Anthropic's API.\footnote{\href{https://console.anthropic.com}{https://console.anthropic.com}} We provide the exact identifier and version for each LLM we experimented with in Table \ref{tab:models}.

\begin{table}[ht]
    \footnotesize
    \centering
    \captionof{table}{Model identifiers for the $17$ models we studied in our work. Models that are openly available are provided with links to their corresponding pages on Huggingface Hub.} 
    \label{tab:models}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{ll}
    \toprule
    \textbf{\textsc{Model}} & \textbf{\textsc{Exact Identifier}} \\
    \midrule
    Llama-3.1-8B & \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{meta-llama/Llama-3.1-8B-Instruct} \\
    Llama-3.1-70B & \href{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{meta-llama/Llama-3.1-70B-Instruct} \\
    Mistral-7B & \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1}{mistralai/Mistral-7B-Instruct-v0.1} \\
    Mistral Small & \href{https://huggingface.co/mistralai/Mistral-Small-Instruct-2409}{mistralai/Mistral-Small-Instruct-2409} \\
    Mistral Large 2 & \href{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}{mistralai/Mistral-Large-Instruct-2407} \\
    Qwen2-7B & \href{https://huggingface.co/Qwen/Qwen2-7B-Instruct}{Qwen/Qwen2-7B-Instruct} \\
    Qwen2.5-7B & \href{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}{Qwen/Qwen2.5-7B-Instruct} \\
    Qwen2.5-72B & \href{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct}{Qwen/Qwen2.5-72B-Instruct} \\
    Command R+ & \href{https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024}{CohereForAI/c4ai-command-r-plus-08-2024} \\
    DBRX & \href{https://huggingface.co/databricks/dbrx-instruct}{databricks/dbrx-instruct} \\
    Phi-3.5-MoE & \href{https://huggingface.co/microsoft/Phi-3.5-MoE-instruct}{microsoft/Phi-3.5-MoE-instruct} \\
    \midrule
    GPT-4o-mini & gpt-4o-mini-2024-07-18 \\
    GPT-4o & gpt-4o-2024-05-13 \\
    Gemini-1.5-Flash & gemini-1.5-flash-001 \\
    Gemini-1.5-Pro & gemini-1.5-pro-001 \\
    Claude-3-Haiku & claude-3-haiku-20240307 \\
    Claude-3.5-Sonnet & claude-3-5-sonnet-20240620 \\
    \bottomrule
    \end{tabular}
    \renewcommand{\arraystretch}{1}
\end{table}

\paragraph{Fine-tuning.} We use the torchtune library and fine-tune using LoRA \citep{hu2022lora}. We did not extensively tune the hyperparameters as that is not the focus of this work. We used the following hyperparameters:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Learning rate: 5e-4
    \item LoRA rank: 16
    \item LoRA alpha: 32
    \item Batch size: 2
    \item Scheduler: Cosine
    \item Precision: Brain \texttt{float16} (i.e., \texttt{bf16})
    \item Optimizer: AdamW \citep{loshchilov_decoupled_2019}
\end{itemize}

\begin{figure}[t]
    \centering    \includegraphics[scale=0.13, trim=0 0 0 20, clip]{images/chase_code.pdf}
    \caption{Pipeline for creating an example in \codename{}.} \label{fig:chase_code}
\end{figure}

\paragraph{Human verification of LLM judge.} We carry out human verification on Amazon Mechanical Turk. We first randomly sampled $10$ of the predictions made by Gemini-1.5-Pro on \qaname{} and manually evaluated them. We then publish them as a batch of 10 Human Intelligence Tasks (HITs) to serve as a qualification task to identify workers who will do the task properly. Note that each model prediction that needs to be judged is a HIT. Once we identified $3$ workers that did perfectly on our qualification task, we published a batch of $100$ randomly sampled predictions accessible only to those workers. Note that we sampled a balanced set based on the LLM judge's evaluation: $50$ that were marked by GPT-4o as correct and $50$ that were marked as incorrect. The instructions provided to the workers and the setup of the task is kept exactly the same as the one provided to the LLM judge as shown by the prompt in Figure \ref{prompt:qa_evaluation}. We paid \$$0.5$ USD to the workers for every example.

\paragraph{Cost of creation.} In Table \ref{tab:cost}, we report the estimated cost of creating the three benchmarks, both in terms of inference time and API expenses. Note that the inference time assumes sequential execution of each part of the pipeline with only one process running at a time. Hence, the generation can be made considerably faster with increased parallelism. This table does not include the cost of other experiments in the paper nor does it include the cost of background experiments that went into designing the pipelines. We estimate the total of these costs to be over \$1000 USD.

\begin{table}[t]
    \footnotesize
    \centering
    \captionof{table}{Estimated cost of creating the benchmarks in terms of inference time and money.} 
    \label{tab:cost}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{m{8em}  >{\centering\arraybackslash}m{12em}  >{\centering\arraybackslash}m{8em} }
    \toprule
    \textbf{\textsc{Benchmark}} & \textbf{\textsc{Inference Time (Hours)}} & \textbf{\textsc{Cost (USD)}} \\
    \midrule
            \qaname{} & 40 & 100  \\
            \hdashline[0.5pt/2pt]
            \codename{} & 55 & 150  \\
            \hdashline[0.5pt/2pt]
		\mathname{} & 200 & 40  \\
    \bottomrule
    \end{tabular}
    \renewcommand{\arraystretch}{1}
\end{table}



\begin{table}[t]
    \centering
    \caption{Dataset statistics of \qaname{} and intrinsic complexity metrics to measure the difficulty of examples.} 
    \label{tab:compexity_qa}
    \renewcommand{\arraystretch}{1.5}
    \footnotesize{
    \begin{tabular}{m{18em}  >{\centering\arraybackslash}m{5em} }
    \toprule
            \textsc{\textbf{Criteria}} &\\
    \midrule
            Number of examples & 671 \\
    \hdashline[0.5pt/2pt]
            Average context size & 6000 \\
    \hdashline[0.5pt/2pt]
            Average number of answer points & 3.6 \\
    \hdashline[0.5pt/2pt]
            Average number of relevant documents & 3.3 \\
    \hdashline[0.5pt/2pt]
            Average number of irrelevant documents & 7.7 \\
    \bottomrule
    \end{tabular}
    }
    \renewcommand{\arraystretch}{1}
\end{table}


\begin{table}[t]
    \centering
    \caption{Dataset statistics of \codename{} and intrinsic complexity metrics to measure the difficulty of examples.} 
    \label{tab:compexity_code}
    \renewcommand{\arraystretch}{1.5}
    \footnotesize{
    \begin{tabular}{m{15em}  >{\centering\arraybackslash}m{5em} > {\centering\arraybackslash}m{5em} }
    \toprule
            \textsc{\textbf{Criteria}} & \textsc{\textbf{Algo}} & \textsc{\textbf{DP}}\\
    \midrule
            Number of examples & 250 & 250 \\
    \hdashline[0.5pt/2pt]
            Average context size & 17000 & 17000 \\
    \hdashline[0.5pt/2pt]
            Average number of statements & 22.0 & 11.4 \\
    \hdashline[0.5pt/2pt]
            Average Cyclomatic Complexity & 15.4 & 8.2 \\
    \hdashline[0.5pt/2pt]
            Average Halstead Difficulty & 19.6 & 12.2 \\
    \bottomrule
    \end{tabular}
    }
    \renewcommand{\arraystretch}{1}
\end{table}


\begin{table}[t]
    \centering
    \caption{Dataset statistics of \mathname{} and intrinsic complexity metrics to measure the difficulty of examples.} 
    \label{tab:compexity_math}
    \renewcommand{\arraystretch}{1.5}
    \footnotesize{
    \begin{tabular}{m{20em}  >{\centering\arraybackslash}m{5em} }
    \toprule
            \textsc{\textbf{Criteria}} &\\
    \midrule
            Number of examples & 500 \\
    \hdashline[0.5pt/2pt]
            Average reasoning depth & 3.5 \\
    \hdashline[0.5pt/2pt]
            Average number of words in question & 254.9 \\
    \bottomrule
    \end{tabular}
    }
    \renewcommand{\arraystretch}{1}
\end{table}



% \section{Ethical Considerations}\label{app:ethics}

% \todo{Discuss.}

\begin{table}[t]
    \footnotesize
    \centering
    \captionof{table}{Comparison of model performances (pass@1) on \codename{} and HumanEval, a widely-used benchmark for code generation.} 
    \label{tab:code_compare}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{m{8.5em}  >{\centering\arraybackslash}m{8em}  >{\centering\arraybackslash}m{5em} }
    \toprule
    \textbf{\textsc{Model}} & \textbf{\textsc{CHASE-Code}} & \textbf{\textsc{HumanEval}} \\
    \midrule
            Gemini-1.5-Pro & 38.2 & 84.1  \\
            \hdashline[0.5pt/2pt]
            GPT-4o & 24.6 & 90.2  \\
            \hdashline[0.5pt/2pt]
		Claude-3.5-Sonnet & 22.4 & 92.0  \\
            \hline
            Gemini-1.5-Flash & 28.6 & 74.3 \\
            \hdashline[0.5pt/2pt]
            GPT-4o-mini & 18.8 & 86.6  \\
            \hdashline[0.5pt/2pt]
            Claude-3-Haiku & 21.8 & 75.9  \\
            \hdashline[0.5pt/2pt]
            Llama-3.1-70B & 15.6 & 80.5  \\
            \hdashline[0.5pt/2pt]
            Mistral Large 2 & 5.0 & 92.1  \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-72B & 11.4 & 86.6  \\
            \hdashline[0.5pt/2pt]
            Command R+ & 0 & 70.1 \\
            \hdashline[0.5pt/2pt]
            DBRX & 2.2 & 70.1 \\
            \hdashline[0.5pt/2pt]
            Phi-3.5-MoE & 0.8 & 70.7 \\
            \hdashline[0.5pt/2pt]
            % Gemma-2-27B\footnotemark & 27.6 & - & - & 47.33  \\
            % \hdashline[0.5pt/2pt]
            Mistral Small & 1.4 & 73.8 \\
            \hline
            Llama-3.1-8B & 2.0 & 72.6 \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-7B & 2.2 & 57.9 \\
    \bottomrule
    \end{tabular}
    \renewcommand{\arraystretch}{1}
\end{table}

\begin{table}[t]
    \footnotesize
    \centering
    \captionof{table}{Comparison of model performances on \mathname{} and GSM8k, a widely-used benchmark for grade-school level math word problem solving.} 
    \label{tab:math_compare}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{m{8.5em}  >{\centering\arraybackslash}m{8em}  >{\centering\arraybackslash}m{5em} }
    \toprule
    \textbf{\textsc{Model}} & \textbf{\textsc{CHASE-Math}} & \textbf{\textsc{GSM8k}} \\
    \midrule
            Gemini-1.5-Pro & \textbf{65.4} & 90.8  \\
            \hdashline[0.5pt/2pt]
            GPT-4o & 59.8 & 96.1  \\
            \hdashline[0.5pt/2pt]
		Claude-3.5-Sonnet & 64.2 & \textbf{96.4}  \\
            \hline
            Gemini-1.5-Flash & 56.6 & 86.2 \\
            \hdashline[0.5pt/2pt]
            GPT-4o-mini & 48.4 & 94.2  \\
            \hdashline[0.5pt/2pt]
            Claude-3-Haiku & 44.2 & 79.2  \\
            \hdashline[0.5pt/2pt]
            Llama-3.1-70B & 53.4 & 95.1  \\
            \hdashline[0.5pt/2pt]
            Mistral Large 2 & 59.6 & 92.7  \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-72B & 58.4 & 95.8  \\
            \hdashline[0.5pt/2pt]
            Command R+ & 43.2 & 70.7 \\
            \hdashline[0.5pt/2pt]
            DBRX & 21.6 & 72.7 \\
            \hdashline[0.5pt/2pt]
            Phi-3.5-MoE & 39.4 & 88.7 \\
            \hdashline[0.5pt/2pt]
            % Gemma-2-27B\footnotemark & 27.6 & - & - & 47.33  \\
            % \hdashline[0.5pt/2pt]
            Mistral Small & 50.6 & 87.4 \\
            \hline
            Llama-3.1-8B & 32.2 & 84.5 \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-7B & 42.8 & 85.4 \\
    \bottomrule
    \end{tabular}
    \renewcommand{\arraystretch}{1}
\end{table}



\begin{table}[t]
    \footnotesize
    \centering
    \captionof{table}{Measuring performance of all models on \qaname{} with alternative soft metrics, K-Precision and Recall.} 
    \label{tab:qa_metrics}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{m{8.5em}  >{\centering\arraybackslash}m{7em} >{\centering\arraybackslash}m{7em} >{\centering\arraybackslash}m{7em} }
    \toprule
    \textbf{\textsc{Model}} & \textbf{\textsc{Accuracy}} & \textbf{\textsc{K-Precision}} & \textbf{\textsc{Recall}} \\
    \midrule
            Gemini-1.5-Pro & \textbf{\underline{63.2}} & 85.1 & \textbf{\underline{68.6}} \\
			\hdashline[0.5pt/2pt]
		GPT-4o & 55.3 & \textbf{\underline{86.7}} & 58.3  \\
            \hdashline[0.5pt/2pt]
		Claude-3.5-Sonnet & 36.1 & 77.6 & 49.0 \\
            \hline
            Gemini-1.5-Flash & \underline{55.1} & \underline{82.3} & \underline{61.7} \\
            \hdashline[0.5pt/2pt]
            GPT-4o-mini & 50.2 & 74.1 & 50.7  \\
            \hdashline[0.5pt/2pt]
            Claude-3-Haiku & 32.6 & 70.9 & 40.9  \\
            \hdashline[0.5pt/2pt]
            Llama-3.1-70B & 41.3 & 76.3 & 46.1  \\
            \hdashline[0.5pt/2pt]
            Mistral Large 2 & 34.1 & 72.4 & 42.9 \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-72B & 38.3 & 78.2 & 47.9 \\
            \hdashline[0.5pt/2pt]
            Command R+ & 41.7 & 71.7 & 47.4  \\
            \hdashline[0.5pt/2pt]
            DBRX & 15.7 & 53.2 & 35.0  \\
            \hdashline[0.5pt/2pt]
            Phi-3.5-MoE & 10.6 & 45.0 & 25.6  \\
            \hdashline[0.5pt/2pt]
            % Gemma-2-27B\footnotemark & 27.6 & - & - & 47.33  \\
            % \hdashline[0.5pt/2pt]
            Mistral Small & 35.5 & 77.2 & 41.1 \\
            \hline
            Llama-3.1-8B & \underline{25.2} & \underline{61.3} & \underline{32.0}  \\
            \hdashline[0.5pt/2pt]
            Qwen2.5-7B & 22.2 & 56.9 & 30.3  \\
    \bottomrule
    \end{tabular}
    \renewcommand{\arraystretch}{1}
\end{table}

\section{Additional Results and Discussion}\label{app:extra_exp}

\subsection{Manual Verification}\label{app:verify}

We manually reviewed $100$ examples from each of the $3$ benchmarks. For \qaname{}, we verify whether the question and the corresponding ground-truth answer are correct based on the context. For \codename{}, we verify whether the answer code seeks to implement the objectives stated in the problem statement. For \mathname{}, we verify whether the ground-truth answer and reasoning are correct for the problem. We found $6$ errors in \qaname{}, $3$ errors in \codename{} and $7$ errors in \mathname{}. Examples of such errors in generation are discussed in Appendix \ref{app:errors}. We believe these error rates are sufficiently low, thereby encouraging reliance on \dataname{}-generated benchmarks for evaluation.

\subsection{Post-hoc Automatic Verification for \mathname{}}

In our experiments, we found that some post-hoc filtration techniques work well for weeding out the incorrect examples for \mathname{}. We outline a general recipe here. Given a set of \emph{held-out} verifier LLMs (not being evaluated) $\{V_1, V_2, ..., V_n\}$, we can obtain predictions for all the problems generated by applying the \dataname{} procedure. Problems for which the predictions of at least $k$ verifiers ($k>1$) matches the ground-truth annotation can be included in the dataset provided that the predictions of the remaining $n-k$ verifiers do not agree on an answer different from the ground-truth. The intuition here is that if multiple LLMs that did not participate in generating the problem-answer pair also reach the same answer when provided with the problem, then there is a high chance that the problem-answer pair is correct. Problems for which a majority of the verifiers agree on an answer different from the ground-truth should be discarded. Finally, the small proportion of problems for which different verifiers make different predictions, none of which match the ground-truth answer can be manually examined and rectified.

\subsection{Intrinsic Complexities of \dataname{} Benchmarks}\label{app:complexity_metrics}

For all three \dataname{} benchmarks, evaluate the intrinsic complexity using various metrics. For \qaname{}, we measure the average number of answer points per question along with the average number of relevant and irrelevant documents. For \codename{} we use AST tree-based metrics such as Cyclomatic Complexity \citep{ebert2016cyclomatic} and Halstead Difficulty \citep{halstead} of the answer code combined with the helper functions. For \mathname{}, we measure the average reasoning depth of the problems. The statistics and complexity values for \qaname{}, \codename{}, and \mathname{} are provided in Tables \ref{tab:compexity_qa}, \ref{tab:compexity_code}, \ref{tab:compexity_math} respectively.




\subsection{Comparison of Model Performances On Similar Datasets}

\textbf{\qaname{}} consists of long-context realistic-situation-based information-seeking QA problems. The most similar benchmarks are Loong \citep{wang2024leavedocumentbehindbenchmarking}, which consists of long-context QA problems requiring reasoning over documents (more than 100k tokens long) from domains such as academic papers and financial reports, and LooGLE \citep{li-etal-2024-loogle}, which consists of long-dependency QA problems over wikipedia and movie scripts (around 32k tokens context). The best performing models on these datasets achieve scores of around 53\% and 54\% respectively. The best performing model on \qaname{} achieves a score of around 63\%, which reduces to around 55\% when we scale the context size to comparable levels of 30k tokens.

\textbf{\codename{}} consists of repository-level code generation problems. HumanEval \citep{chen2021evaluatinglargelanguagemodels} is the most widely-used challenging code generation benchmark. We compare the performances of all models on both datasets in Table \ref{tab:code_compare}. We can clearly see that \codename{} is a much more challenging benchmark. Recently, some repository-level code benchmarks have also been proposed. SWE-Bench \citep{jimenez2024swebench} is a benchmark of around 2300 software engineering problems compiled from GitHub issues in popular repositories. EvoCodeBench \citep{li2024evocodebench} consists of 275 repository-level code generation problems based on popular GitHub repositories. The best performing models on these benchmarks achieve around 42\% and 20\% scores respectively.

\textbf{\mathname{}} consists of grade-school level math word problems. The most widely-used challenging benchmark for this task is GSM8k \citep{gsm8k}, comprising of 1319 examples. We compare the performances of all models on both datasets in Table \ref{tab:math_compare}. It is clear that GSM8k has mostly become saturated, with many state-of-the-art models achieving more than 90\% accuracies. In comparison, \mathname{} is still very difficult for all models to solve. Moreover, the differences in performance between different models is much larger, which enables more confident comparison.

\subsection{Alternative Metrics of Evaluation for \qaname{}}

The metric of accuracy for \qaname{} punishes models for not being concise and generating too many answer points that are not a part of the ground-truth answer. In this section, we present our experimental results with other softer evaluation metrics. We adapt two metrics that have been used by previous works for open-domain question answering \citep{adlakha-etal-2022-topiocqa}: (1) \textbf{K-Precision}, which for a particular example, evaluates whether all of the answer points in the model's prediction are discussed in the documents, and (2) \textbf{Recall}, which evaluates whether all the ground truth answer points are a part of the model's prediction. K-Precision is used to measure the \emph{faithfulness} of the model's prediction to the provided documents. Recall is used to measure the \emph{correctness} of the model's prediction compared to the ground-truth. We define both the metrics as binary per example. Similar to how we calculated accuracy, we use GPT-4o as a judge with the prompts provided in Figure \ref{prompt:qa_precision} and Figure \ref{prompt:qa_recall} respectively. The results are provided in Table \ref{tab:qa_metrics}.

Note that the errors in \qaname{} pertain to the cases where the ground-truth answer may not completely encompass all the relevant information about the question that is mentioned in the documents. We believe that comparisons of models on the basis of recall is relatively less affected by the presence of such errors. This is because if a model has comparatively lesser recall, that means that it generated more responses where it did not include the ground-truth information (irrespective of whether it generated any extra relevant information for the question that is not in the ground truth).

\subsection{Effect of Prompt for Solving \mathname{}}

Considering the fact that \mathname{} is built by increasingly concatenating problems, we experiment with solving it using a different prompt format that explicitly instructs the model to process one sentence at a time, from the first to the last until it arrives at the final answer. We also illustrate this methodology in the prompt using $8$ problems different from the original chain-of-thought prompt examples. Each of these new problems have a much higher reasoning depth. The prompt is provided in Figure \ref{prompt:math_solve_sentence}. The results for $3$ different models are shown in Table \ref{tab:math_sentence}. While there is a clear increase in performance for all models, the task still remains difficult to solve, in general. Examples of errors made by models even with this better prompting technique are provided in Figure \ref{error:math_solve_gemini_sentence} and Figure \ref{error:math_solve_gemini_sentence_2}.



\begin{table}[t]
    \footnotesize
    \centering
    \captionof{table}{Effect of prompt (see Figure \ref{prompt:math_solve_sentence}) that explicitly instructs the model to solve \mathname{} problems by processing one sentence at a time.} 
    \label{tab:math_sentence}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{m{8em}  >{\centering\arraybackslash}m{12em}  >{\centering\arraybackslash}m{15em}}
    \toprule
    \textbf{\textsc{Model}} & \textbf{\textsc{8-shot CoT (default)}} & \textbf{\textsc{8-shot sentence-by-sentence}} \\
    \midrule
    Gemini-1.5-Pro & 65.4 & 69.2 \\
    \hdashline[0.5pt/2pt]
    GPT-4o & 59.8 & 61.4 \\
    \hdashline[0.5pt/2pt]
    Llama-3.1-70B & 53.4 & 56.8 \\
    \bottomrule
    \end{tabular}
    \renewcommand{\arraystretch}{1}
\end{table}


\section{Additional Related Work}\label{app:rel_work}

\subsection{Synthetic Data Generation}

Early works explored the potential of pretrained generative language models such as BERT \citep{devlin-etal-2019-bert}, GPT2 \citep{radford2019language}, and GPT-3 \citep{gpt3} to generate datasets for fine-tuning \citep{schick-schutze-2021-generating, ye-etal-2022-zerogen} or for data augmentation \citep{kumar-etal-2020-data, yoo-etal-2021-gpt3mix-leveraging}. With increasingly powerful LLMs \citep{instructgpt, openai2024gpt4technicalreport} being released in recent years combined with the benefits obtained from instruction fine-tuning \citep{wei2022finetuned, scalingIFT}, the focus has shifted more towards generating synthetic instructions data. \citet{honovich-etal-2023-unnatural} and \citet{wang-etal-2023-self-instruct} prompt GPT-3 with seed examples to automatically generate a large set of diverse tasks. \citet{xu2024wizardlm} introduced the \emph{Evol-Instruct} pipeline to generate more complex examples from a given seed example. 
% \citet{nayak-etal-2024-learning} designed a workflow for generating instruction-tuning data conditioned on unannotated text as input. 
\citet{mukherjee2023orcaprogressivelearningcomplex} leverage explanation traces from GPT-4 to create a large synthetic instruction-tuning dataset for fine-tuning smaller LLMs. \citet{mitra2024agentinstructgenerativeteachingagentic} design an agentic framework that uses raw text and code data as seeds to generate large scale synthetic data of prompts and responses. 

Apart from synthetic instructions datasets, there has also been interest in using synthetic data for pre-training. \citet{phi} and \citet{cosmopedia} generate high-quality textbooks for pre-training small LLMs to unlock better reasoning performance. Recent works \citep{bai2022constitutionalaiharmlessnessai,leerlaif} have also explored using synthetic preference data for aligning language models. Synthetic data has started to become a major component in the post-training development phase of contemporary LLMs \citep{dubey2024llama3herdmodels} to improve their reasoning capabilites. For a comprehensive discussion of major ideas and issues in synthetic data generation, we refer the reader to \citet{synthetic_survey}'s survey. 

\subsection{Synthetic Data for Code Generation and Math Reasoning}

% There is a severe lack of high-quality data for content-grounded QA tasks. In this section, we discuss some recent works that have explored generating synthetic datasets for such tasks. An extensive discussion of human-annotated QA datasets can be found in Appendix \ref{app:rel_work}. \citet{pmlr-v162-dai22a} develop information-seeking dialogue datasets by using a T5-based trained model to predict partial dialogues in question form based on responses from pieces of text extracted from Wikipedia and the web. \citet{genie} propose a pipeline to extract content from wikipedia, and then prompting an LLM with it to generate a question-answer pair. We take a more application-oriented view and design a benchmark for document-based information-seeking questions that model realistic situations. Moreover, our pipeline generates the entire content for the examples from LLMs, which allows a higher degree of control.

There has been significant recent interest in generating synthetic data for code. \citet{wen2024groundingdatasciencecode} utilize I/O specification apart from synthetic intents to generate data science code instructions. \citet{patel-etal-2024-evaluating} generate code library specifications using GPT-4 to evaluate whether LLMs can  learn new code libraries in-context. \citet{chen2023codet} and \citet{prasad2025learninggenerateunittests} focus on generating unit tests for code generation. \citet{patel-etal-2023-magnifico} implement an LLM-based procedure to paraphrase problems for SQL code generation. In this work, we automatically generate all parts of a code generation problem: the repository context, problem specification, ground truth answer code, and tests.

Generating synthetic data to improve math reasoning has also been a very active area of research. \citet{tang2024mathscalescalinginstructiontuning} explored extracting topics and knowledge from math problems to prompt an LLM to generate new data. \cite{hong2024evaluatingllmsmathematicalcoding} create math problems by using GPT-4 to perturb existing problems in GSM8k. \citet{toshniwal2024openmathinstruct118millionmath} create an instruction-tuning dataset by synthesizing code-interpreter style solutions for existing math problems using open-source LLMs. \citet{yang2024benchmarkingllmsoptimizationmodeling} create a reverse socratic approach to synthesize math problems from optimization scenarios. \citet{luo2023wizardmathempoweringmathematicalreasoning} generate diverse math instructions data using \emph{Evol-Instruct} and then train LLMs using reinforcement learning. \citet{yue2024mammoth} build an instruction-tuning dataset by using GPT-4 to generate hybrid CoT and PoT rationales for examples from diverse math datasets. \citet{yu2024metamath} bootstrap problems from existing math datasets by using an LLM to rephrase the question text.

% \subsection{Synthetic Data for Evaluation and Benchmarking}

% All the works listed in the previous sections focus on generating synthetic data for various stages of an LLM's development pipeline: pre-training, post-training, instruction fine-tuning and alignment. However, the main focus of our work is to generate high-quality challenging problems for evaluation and benchmarking. There is very limited existing literature in this area. \citet{sprague2024musr} created a narrative-based QA benchmark using a neuro-symbolic pipeline that first samples facts, and then uses an LLM to build a reasoning tree over them, along with generating a narrative. \citet{bohnet2024longspanquestionansweringautomaticquestion} created another synthetic QA benchmark by extracting entities and their reference chains from existing stories and then prompting an LLM to generate questions over them. \citet{gu2024cruxevalbenchmarkcodereasoning} create a code understanding benchmark by prompting CodeLlama \citep{rozière2024codellamaopenfoundation} to generate python functions and their inputs and designing tasks based on predicting either the input or the output for a given function. In contrast to these works, we focus on presenting a general framework which can be used to design scalable pipelines to create challenging benchmarks across multiple domains. Moreover, we focus on realistic tasks such as information-seeking QA and repository-level code generation.

\subsection{Human-curated Evaluation Benchmarks}

\paragraph{Question Answering.} There have been numerous context-grounded question answering benchmarks proposed in the past. These include SQuAD \citep{rajpurkar-etal-2016-squad, rajpurkar2018knowdontknowunanswerable}, HotpotQA \citep{yang-etal-2018-hotpotqa}, DROP \citep{dua-etal-2019-drop}, HAS-QA, Qasper \citep{dasigi-etal-2021-dataset}, \citep{has-qa} TopioCQA \citep{adlakha-etal-2022-topiocqa}, NovelQA \citet{wang2024novelqabenchmarkingquestionanswering}, and RepliQA \citep{monteiro2024repliqaquestionansweringdatasetbenchmarking}. Different from these works, we focus on extremely long contexts, with documents containing a total of more than 10,000 tokens. Another important aspect of our work is that all the documents that make up the context for an example are generated by an LLM from scratch, which reduces contamination risk. Moreover, our questions are designed to simulate everyday queries that users might ask an LLM-based agent. Correspondingly, the annotated ground-truth answers are at times very verbose, which make them impossible to objectively evaluate using automated metrics.

\paragraph{Long Context Reasoning.} \citet{needleinhaystack} introduced the Needle-In-a-Haystack (NIH) task in which a model is asked to retrieve a fact placed in a long context. However, this only assessed surface-level long-context retrieval abilities without much reasoning. In the past few months, many more long-context understanding benchmarks have been created. SCROLLS \cite{shaham-etal-2022-scrolls} and ZeroScrolls \citep{shaham-etal-2023-zeroscrolls} are long-context benchmarks created by aggregating and repurposing existing datasets. Compared to the QA datasets in ZeroScrolls which consist of contexts based on science and literature, \qaname{} focuses on real-world scenarios where a user is looking for information across multiple documents. Many benchmarks such as  LongBench \citep{bai-etal-2024-longbench}, RULER \citep{hsieh2024rulerwhatsrealcontext}, L-Eval \citep{an-etal-2024-l}, LooGLE \citep{li-etal-2024-loogle} and InfiniteBench \citep{zhang2024inftybenchextendinglongcontext} consist of long-context QA tasks. However, the relevant information for answering the question is comparatively much more centralized in the context, surrounded by distracting information. In contrast, the relevant information in \qaname{} is spread across multiple documents. Moreover, the irrelevant information in the other documents is still very closely related to the topics of the question, thereby making the task very difficult. Perhaps, the benchmark most similar to our work is Loong \citep{wang2024leavedocumentbehindbenchmarking}. Like \qaname{}, Loong also has evidences for answering questions scattered across a long context of documents. Unlike our benchmark, however, every document in Loong is known to be relevant for answering the question. In contrast \qaname{} models the more realistic scenario of searching a bunch of documents that are on very closely related topics, only some of which contain the answer. Moreover, unlike all these prior works, we create long-context reasoning benchmarks completely from scratch in which the long text contexts are also generated using LLMs. Further note that in contrast to these prior works that use manual annotation, our approach is highly scalable, capable of creating thousands of examples across diverse domains automatically in a much more cost-efficient manner.

\section{Error Analysis}\label{app:errors}

We provide examples for two types of errors, those made while solving the benchmarks, and those made while generating the benchmarks.

\subsection{Errors made while solving \dataname{} benchmarks.}

Figure \ref{error:qa_solve_gemini} provides an example of an error made by Gemini-1.5-Pro on a problem from \qaname{}. The model fails to mention two important points relevant for answering the question, which have been discussed in the documents. This provides a qualitative example of how even the most powerful models are unable to properly pay attention to all parts of a long-context and may miss some important information.

Figure \ref{error:code_solve_gemini} provides an example of an error made by Gemini-1.5-Pro in generating the correct code for a problem in \codename{}. The model generates most of the code correctly, but for a particular objective, it gets confused in choosing to call the right helper function from the long-context code repository. This example qualitatively illustrates that doing well on this task requires not only a good understanding of the user-specified objectives, but also requires an in-depth understanding of all parts of the code repository.

Figure \ref{error:math_solve_gemini} provides an example of an error made by Gemini-1.5-Pro in solving a math word problem from \mathname{}. The model executes most of the reasoning steps correctly but fails at the last one. This example qualitatively shows how even state-of-the-art LLMs start to struggle when we scale up the reasoning depth of such types of problems.

\subsection{Errors made in the generation process when using \dataname{}.}

In Figure \ref{error:qa_generate}, we show an error made in the generation process of \qaname{} by GPT-4o. In the document generation stage, the model generated a document which contained extra information that was directly relevant for answering the given question but was not included in the ground-truth answer. This is also a failure case of our verification engine (the one that uses the prompt in Figure \ref{prompt:qa_verify_extra}) which failed to detect the presence of this extra relevant information in the generated document. We believe such errors can be further reduced by using an ensemble of verifiers to carry out each verification task.

Figure \ref{error:math_generate} provides en example of an error made by GPT-4o-mini while generating the \mathname{} benchmark. The model's generated answer did not correspond to its generated problem. This is also a failure for both the generator and verifier. Such failures may be reduced by using an ensemble of stronger LLMs for verification instead.


\section{Prompts}\label{app:prompts}

\subsection{Prompts for \qaname{}}\label{app:prompts_qa}

In this section, we outline the exact prompts for all experiments related to \qaname{}:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Generating scenarios: Figure \ref{prompt:qa_scenarios}
    \item Generating QA pairs: Figure \ref{prompt:qa_qa}
    \item Generating irrelevant information: Figure \ref{prompt:qa_adversarial}
    \item Generating documents: Figure \ref{prompt:qa_documents}
    \item Verifying irrelevant information: Figure \ref{prompt:qa_verify_adv_cross_check}
    \item Verifying absence of relevant information: Figure \ref{prompt:qa_verify_extra}
    \item Verifying presence of answer: Figure \ref{prompt:qa_verify_presence}
    \item Solving the task: Figure \ref{prompt:qa_solve}
    \item LLM-as-a-judge: Figure \ref{prompt:qa_evaluation}
\end{itemize}


\subsection{Prompts for \codename{}}\label{app:prompts_code}

In this section, we outline the exact prompts for all experiments related to \codename{}:

\begin{itemize}
    \setlength\itemsep{0em}
    \item Generating helper functions: Figure \ref{prompt:code_helper}
    \item Generating problem statement and answer code: Figure \ref{prompt:code_problem}
    \item Generating test code: Figure \ref{prompt:code_test}
    \item Generating test for verifying if function executes: Figure \ref{prompt:code_verify_execution}
    \item Verify correctness of problem statement: Figure \ref{prompt:code_verify_problem}
    \item Solving the task: Figure \ref{prompt:code_solve}
\end{itemize}


\subsection{Prompts for \mathname{}}\label{app:prompts_math}

In this section, we outline the exact prompts for all experiments related to \mathname{}:

\begin{itemize}
    \setlength\itemsep{0em}
    \item Breaking down seed MWPs: Figure \ref{prompt:math_break}
    \item Creating continuation of MWP: Figure \ref{prompt:math_extend}
    \item Solving MWP: Figure \ref{prompt:math_solve}
    \item Solving MWP sentence-by-sentence: Figure \ref{prompt:math_solve_sentence}
\end{itemize}




% Configure tcolorbox style
\tcbset{
  colback=brown!15!white,   % Background color of the main box
  colframe=black,         % Border color of the box
  fonttitle=\bfseries,    % Bold font for the title
  boxrule=0.4mm,          % Border thickness          % Sharp corners for the box
  coltitle=white,         % Title text color (set to white)
  colbacktitle=purple!80!white,     % Background color of the title bar
}

% Configure Python code style
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  backgroundcolor=\color{brown!15!white},
  frame=none,
  showstringspaces=false,
  breaklines=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=5pt
}

\begin{figure}[ht]
\begin{tcolorbox}[title=Error Analysis: Solving \qaname{} - Gemini-1.5-Pro]

\textbf{Question:} What are the regulations and financial support options available for starting an artisanal cheese-making business in Vermont?

\rule{\linewidth}{1pt}\\

\textbf{Ground-truth Answer:}

- Obtain a Dairy Processing License from the Vermont Agency of Agriculture, Food \& Markets.\\
- Adhere to the Vermont Department of Environmental Conservation's (VT DEC) regulations regarding wastewater management.\\
- Apply for the Vermont Economic Development Authority (VEDA) Entrepreneurial Loan Program for startup financing.\\
- Comply with the Vermont Department of Taxes' requirements for sales and use tax on dairy products.\\
- Secure a Vermont Working Lands Enterprise Initiative Grant for equipment and facility improvement.

\rule{\linewidth}{1pt}\\

\textbf{Irrelevant Information:}

- Obtain a Microbrewery License from the New York State Liquor Authority (NYSLA).\\
- Adhere to the New York State Department of Environmental Conservation (NYSDEC) regulations regarding waste disposal.\\

[Redacted]

\rule{\linewidth}{1pt}\\

\textbf{Prediction:}

- Comply with the Vermont Department of Taxes' requirements for sales and use tax on dairy products, including registering for a sales tax permit, tracking sales, and remitting collected tax. (Document 10)\\
- Adhere to the Vermont Department of Environmental Conservation's (VT DEC) regulations regarding wastewater management, ensuring waste byproducts are treated and disposed of properly to prevent water contamination. (Document 13)\\
- Obtain a Dairy Processing License from the Vermont Agency of Agriculture, Food \& Markets. (Document 12)

\rule{\linewidth}{1pt}\\

\textbf{LLM Evaluation:}

Result: False. The prediction is missing two main points from the ground-truth answer:\\
1. Applying for the Vermont Economic Development Authority (VEDA) Entrepreneurial Loan Program for startup financing.\\
2. Securing a Vermont Working Lands Enterprise Initiative Grant for equipment and facility improvement.\\

These are crucial financial support options that are not mentioned in the prediction.

\end{tcolorbox}
\caption{Example of error made by Gemini-1.5-Pro on \qaname{}. The GPT-4o judge correctly evaluates the prediction to be wrong because it fails to mention some major points from the ground-truth answer which were discussed in the documents.}\label{error:qa_solve_gemini}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=Error Analysis: Solving \codename{} (Algorithms) - Gemini-1.5-Pro]

\textbf{Objectives:} 

[Redacted]

- For each coordinate from the previous sub-goal, calculate the Manhattan distance from the origin (0,0) for the interesting cells found, returning a list of tuples.

\rule{\linewidth}{1pt}\\

\textbf{Ground-truth Answer Code:}

\begin{lstlisting}
import numpy as np
from matrix_decomposition import svd_reconstruction
from entity_extractor import extract_entities
from meeting_scheduling import can_schedule_meetings
from grid_analyzer import high_value_coordinates

def complex_analysis(matrix, k, text, entities, intervals, threshold):
    reconstructed_matrix = svd_reconstruction(matrix, k)
    extracted_entities = extract_entities(text, entities)
    can_schedule = can_schedule_meetings(intervals, 0)
    interesting_cells = high_value_coordinates(reconstructed_matrix, threshold)
    interesting_cells_with_distances = [((i, j), abs(i) + abs(j)) for ((i, j), _) in interesting_cells]
    return reconstructed_matrix, extracted_entities, can_schedule, interesting_cells_with_distances
\end{lstlisting}

\rule{\linewidth}{1pt}\\

\textbf{Predicted Code:}

\begin{lstlisting}
import numpy as np
from matrix_decomposition import svd_reconstruction
from entity_extractor import extract_entities
from meeting_scheduling import can_schedule_meetings
from matrix_analyzer import find_interesting_cells

def complex_analysis(matrix, k, text, entities, intervals, threshold):
    reconstructed_matrix = svd_reconstruction(np.array(matrix), k)
    extracted_entities = extract_entities(text, entities)
    can_schedule = can_schedule_meetings(intervals, 0)
    interesting_cells = find_interesting_cells(reconstructed_matrix, threshold)
    interesting_cells_with_distances = [((row, col), abs(row) + abs(col)) for row, col in interesting_cells]
    return reconstructed_matrix, extracted_entities, can_schedule, interesting_cells_with_distances
\end{lstlisting}

\end{tcolorbox}
\caption{Example of error made by Gemini-1.5-Pro on an example from the \emph{algorithms} split of \codename{}. The prediction incorrectly calls the \texttt{find\_interesting\_cells} helper function (see line number $11$) which does not calculate manhattan distances as requested by the objective.}\label{error:code_solve_gemini}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=Error Analysis: Solving \mathname{} - 8-shot chain-of-thought - Gemini-1.5-Pro]

\textbf{Question:} 

Harry slept 9 hours last night. His friend James slept only 2/3 of what Harry slept. After their sleep, Harry decided to wake up early and went for a jog. He invited James to join him, and they jogged for 4 times the additional hours Harry slept compared to James. They both felt energized and decided to go for breakfast, where they calculated the total hours they spent jogging. After their breakfast, Harry and James sat down to review their fitness goals. They discussed their jogging session and decided it would be beneficial to repeat such sessions. However, they wanted to gauge how many smaller jogging sessions equal the time they previously spent jogging. James suggested that they break down their jogging time into segments of 3 hours to see how many complete segments they can fit within their total jogging time. After evaluating their jogging goals, Harry and James decided to increase the duration of their future jogging sessions. They considered adding an additional hour to each segment to further enhance their fitness regimen, while keeping the same total jog time of all sessions combined. After successfully completing their new jogging regimen for a month, Harry and James noticed that their stamina had increased significantly. They decided to celebrate this achievement by doubling their number of sessions for the future. They planned to increase from their current number of complete sessions to a total that is double their current sessions. They are excited to see how their increased devotion to jogging will impact their overall fitness. After a successful month of their increased jogging regimen, Harry and James wanted to keep track of their progress. They decided to add rest sessions into their training schedule to avoid burnout. For every 1/3 of the number of jogging sessions in their regimen, they would incorporate 1 rest session. If they have done 4 rest sessions so far, how many total jogging sessions have they finished?

\rule{\linewidth}{1pt}

\textbf{Ground-truth Answer:}

James slept 9 hours * 2/3 = 6 hours. So, Harry slept 9 - 6 = 3 hours more than James. They jogged for 4 times that amount, so 12 hours together. We then divide 12 by 3 to get 4 complete sessions. Now divide the 12 hours by the new session duration (4 hours) which yields 3 sessions. Doubling that quantity results 6. They add rest sessions for every 1/3 * 6 = 2 jogging sessions. So for 4 rest sessions, they have 4 * 2 = 8 jogging sessions.
\#\#\#\# 8.0

\rule{\linewidth}{1pt}

\textbf{Prediction:}

Let's break down this problem step-by-step:\\
1. **James's sleep:** James slept 2/3 of Harry's 9 hours, which is (2/3) * 9 = 6 hours.\\
2. **Additional hours Harry slept:** Harry slept 9 hours - 6 hours = 3 more hours than James.\\
3. **Jogging time:** They jogged for 4 times the additional hours Harry slept, meaning they jogged for 4 * 3 = 12 hours.\\
4. **Segments of jogging:** They want to break down their jogging time into 3-hour segments, so they have 12 hours / 3 hours/segment = 4 segments.\\
5. **Increased segment duration:** They increase each segment by 1 hour, making each segment 3 hours + 1 hour = 4 hours long.\\
6. **Total sessions:** Since they keep the same total jogging time, they still have 12 hours of jogging. With the new segment length, they have 12 hours / 4 hours/session = 3 sessions.\\
7. **Doubled sessions:** They double their number of sessions from 3 to 3 * 2 = 6 sessions.\\
8. **Rest sessions and total jog sessions:** They incorporate 1 rest session for every 1/3 of their jogging sessions. \textcolor{red}{Since they've done 4 rest sessions, they have completed 4 rest sessions * 3 jog sessions/rest session = 12 jogging sessions.}\\
The answer is 12. 

\end{tcolorbox}
\caption{Example of error made in the last step by Gemini-1.5-Pro on an example from \mathname{} when prompted with the default 8-shot chain-of-thought.}\label{error:math_solve_gemini}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=Error Analysis: Solving \mathname{} - 8-shot sentence-by-sentence - Gemini-1.5-Pro]

\textbf{Question:} 

James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. After a month of consistent training, James feels he needs to increase his speed. He decides to take a break from sprinting for one week to focus on recovery. During this recovery week, he runs a shorter distance of 30 meters for only 2 sprints, 3 times that week instead. After his recovery week, James feels rejuvenated and ready to resume his training. He decides to continue running but at double the distance he covered during his recovery week for each day the next week, aiming to improve his overall performance. He runs this increased distance for 7 days straight, maintaining consistency every day. After running consistently throughout the week, James notices a small injury starting to develop in his leg. Concerned about this, he decides to cut back on his total running distance for the following week. To ensure he allows for proper recovery and does not worsen the injury, he plans to reduce his total running distance from the prior week by 420 meters. How much distance will James run in the following week after the reduction?

\rule{\linewidth}{1pt}\\

\textbf{Ground-truth Answer:}

He sprints $3*3=<<3*3=9>>9$ times
So he runs $9*60=<<9*60=540>>540$ meters In the recovery week, James runs 2 sprints each day for 3 days, covering 30 meters each sprint. Therefore, the total distance covered during the recovery week is calculated as follows: he runs 2 sprints/day * 3 days = 6 sprints. Then, multiplying by the distance per sprint gives 6 sprints * 30 meters/sprint = 180 meters. To find out how much less he runs compared to his regular week, we subtract the total distance he runs that week from the distance he typically runs in a week, which is 540 - 180 = 360 meters less. James runs a distance that is double what he covered in the recovery week, which is 180 meters. Therefore, if he runs this doubled distance for 7 days, we first calculate the distance he covers in one day, which is \textcolor{blue}{180 meters * 2 = 360} meters. Over the course of 7 days, the total distance will be 360 meters/day * 7 days. To find the distance James will run in the following week after reducing his initial total distance by 420 meters, I will need to subtract 420 from the total distance he ran in the previous week, which was 2520.0 meters. Hence, the calculation will be: 2520.0 - 420 = 2100.0 meters.
\#\#\#\#2100.0

\rule{\linewidth}{1pt}\\

\textbf{Prediction:}

Sentence 1: James runs 3 sprints * 60 meters/sprint = 180 meters per day.\\
Sentence 2: He runs 180 meters/day * 3 days/week = 540 meters per week.\\
Sentence 3: Nothing to solve.\\
Sentence 4: During his recovery week, he runs 30 meters/sprint * 2 sprints = 60 meters per day.\\
Sentence 5: He runs this shorter distance 3 times a week, so 60 meters/day * 3 days/week = 180 meters. \\
Sentence 6: Nothing to solve.\\
Sentence 7: He doubles his recovery week distance, running \textcolor{red}{60 meters * 2 = 120} meters per day.\\
Sentence 8: He runs this distance for 7 days straight, totaling 120 meters/day * 7 days = 840 meters.\\
Sentence 9: Nothing to solve.\\
Sentence 10: He reduces his total running distance by 420 meters, meaning he'll run 840 meters - 420 meters = 420 meters. \\
The answer is 420. \\

\end{tcolorbox}
\caption{Example of error made by Gemini-1.5-Pro on an example from \mathname{} when prompted with 8-shot sentence-by-sentence solving examples.}\label{error:math_solve_gemini_sentence}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=Error Analysis: Solving \mathname{} - 8-shot sentence-by-sentence - Gemini-1.5-Pro]

\textbf{Question:} 

Rani has ten more crabs than Monic, who has 4 fewer crabs than Bo. If Bo has 40 crabs, calculate the total number of crabs the three have together. After counting their crabs, Rani, Monic, and Bo decide to share their crabs equally among themselves for a crab feast. However, before they can do that, Rani finds out that they must provide 2 additional crabs to a local aquarium as a donation. After their crab feast, Rani, Monic, and Bo decide to host another gathering for their friends. They plan to invite 5 additional friends to enjoy crabs. Each person, including Rani, Monic, Bo, and their friends, should receive an equal amount of crabs for this new gathering. They go and buy new crabs which are in total 3 times the number of crabs each person ate after the donation. How many crabs did they have altogether to share with the additional friends in the end?

\rule{\linewidth}{1pt}\\

\textbf{Ground-truth Answer:}

If Bo has 40 crabs, then Monic, who has 4 fewer crabs than Bo, has 40-4 = $<<40-4=36>>36$ crabs.
Bo and Monic have a total of 36+40 = $<<36+40=76>>76$ crabs.
Since Monic has 36 crabs, Rani's number of crabs is 36+ 10 = $<<36+10=46>>$46.
Together, the three have 76+46 = $<<76+46=122>>122$ crabs. First, calculate how many crabs remain after the donation: 122 - 2 = 120. Now, to find out how many crabs each person will receive, divide the remaining number of crabs (120) by 3, since there are three individuals: 120  3 = 40. Since each of them originally ate 40 crabs, if we multiply that number by 3, we calculate the total number of crabs available in the end.
\#\#\#\#120.0

\rule{\linewidth}{1pt}\\

\textbf{Prediction:}

Sentence 1: Monic has 40 - 4 = 36 crabs.\\
Sentence 2: Rani has 36 + 10 = 46 crabs.\\
Sentence 3: Bo has 40 crabs.\\
Sentence 4: Together they have 46 + 36 + 40 = 122 crabs.\\
Sentence 5: They donate 2 crabs, leaving them with 122 - 2 = 120 crabs.\\
Sentence 6: They share the 120 crabs equally, so each person gets 120 / 3 = 40 crabs.\\
Sentence 7: They invite 5 more friends, making a total of 3 + 5 = 8 people.\\
Sentence 8: They buy new crabs totaling 3 times the number each person ate after the donation, which is 3 * 40 = 120 crabs.\\
Sentence 9: \textcolor{red}{Including the 120 crabs they bought and the remaining crabs they shared after donating, they had 120 + 40*3 = 120 + 120 = 240 crabs altogether to share with the additional friends.}\\

The answer is 240.

\end{tcolorbox}
\caption{Example of error made by Gemini-1.5-Pro on an example from \mathname{} when prompted with 8-shot sentence-by-sentence solving examples. The model fails to understand that the 120 crabs they had earlier have already been eaten and that only the new 120 crabs bought in the end are available to share with the new friends.}\label{error:math_solve_gemini_sentence_2}
\end{figure}




\begin{figure}[ht]
\begin{tcolorbox}[title=Error Analysis: Generating \qaname{} - GPT-4o]

\textbf{Documents:} 

[Redacted]\\

Document 2:
Title: Semiconductor Advancements and Emerging Trends  
Text:  
The semiconductor industry has witnessed numerous advancements over the past few years, driven by the rapid pace of technological innovation. One of the most promising materials being explored for next-generation processors is Gallium Nitride (GaN). GaN is being investigated for its potential to significantly improve the speed and efficiency of processors. Its unique properties, such as high electron mobility and wide bandgap, enable faster electron transmission and greater thermal conductivity. This makes GaN an ideal candidate for applications requiring high power and high-frequency operation.\\

Developments in artificial intelligence algorithms for natural language processing continue to push the boundaries of what machines can understand and generate in human language. Meanwhile, the impact of 5G technology on mobile communications and IoT devices opens up new possibilities for high-speed, low-latency connectivity.\\

Additionally, \textbf{Graphene-based semiconductors hold the potential to revolutionize processor technology}. Graphene, a single layer of carbon atoms arranged in a hexagonal lattice, exhibits exceptional electrical properties. Its high electrical conductivity and mechanical strength make it an attractive material for creating faster and more efficient processors. Researchers are working on overcoming the challenges associated with mass production and integration of graphene into existing manufacturing processes.\\

[Redacted]

\rule{\linewidth}{1pt}\\

\textbf{Question:} What new semiconductor materials are being explored for next-generation processors, and which startups have recently secured funding to advance these technologies?

\rule{\linewidth}{1pt}\\

\textbf{Ground-truth Answer:}

- Gallium Nitride (GaN) is being investigated for its potential to improve the speed and efficiency of processors.\\
- Anokiwave, a pioneering startup in GaN-based semiconductors, has recently raised \$45 million in Series C funding.\\
- Paragraf, a startup developing electronic devices for the semiconductor industry, secured \$30 million in a recent funding round.\\
- The development of molybdenum disulfide (MoS2) as a semiconductor material could lead to significant advancements in processor miniaturization.\\

\end{tcolorbox}
\caption{Example of an error made in the generation process of \qaname{}. The model generated a document which contained additional relevant information about the question (the point about graphene-based semiconductors highlighted in bold in the text) which is not a part of the ground-truth answer.}\label{error:qa_generate}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=Error Analysis: Generating \mathname{} - GPT-4o]

\textbf{Question:} Rani has ten more crabs than Monic, who has 4 fewer crabs than Bo. If Bo has 40 crabs, calculate the total number of crabs the three have together. After counting their crabs, Rani, Monic, and Bo decide to share their crabs equally among themselves for a crab feast. However, before they can do that, Rani finds out that they must provide 2 additional crabs to a local aquarium as a donation. After their crab feast, Rani, Monic, and Bo decide to host another gathering for their friends. They plan to invite 5 additional friends to enjoy the remaining crabs. Each person, including Rani, Monic, Bo, and their friends, should receive an equal amount of crabs for this new gathering. They want to figure out how many crabs they have available for their friends while ensuring that each person gets an equal share. If they multiply the number of crabs each had after the donation by 3, how many crabs did they have altogether to share with the additional friends?

\rule{\linewidth}{1pt}\\

\textbf{Ground-truth Answer:}

If Bo has 40 crabs, then Monic, who has 4 fewer crabs than Bo, has 40-4 = $<<40-4=36>>36$ crabs.
Bo and Monic have a total of 36+40 = $<<36+40=76>>76$ crabs.
Since Monic has 36 crabs, Rani's number of crabs is 36+ 10 = $<<36+10=46>>46$.
Together, the three have 76+46 = $<<76+46=122>>122$ crabs. First, calculate how many crabs remain after the donation: 122 - 2 = 120. Now, to find out how many crabs each person will receive, divide the remaining number of crabs (120) \textbf{by 3, since there are three individuals}: 120 ÷ 3 = 40. Since each of them originally received 40 crabs, if we multiply that number by 3 individuals (Rani, Monic, and Bo), we calculate the total number of crabs available for sharing.
\#\#\#\#120.0

\end{tcolorbox}
\caption{Example of an error made in the generation process of \mathname{}. The reasoning in the answer wrongly considers the number of people for which to divide crabs to be $3$ while the context has added new information that there are $5$ additional friends to consider.}\label{error:math_generate}
\end{figure}




% Configure Python code style
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  backgroundcolor=\color{brown!15!white},
  frame=none,
  showstringspaces=false,
  breaklines=true
}

% Configure tcolorbox style
\tcbset{
  colback=brown!15!white,   % Background color of the main box
  colframe=black,         % Border color of the box
  fonttitle=\bfseries,    % Bold font for the title
  boxrule=0.4mm,          % Border thickness          % Sharp corners for the box
  coltitle=white,         % Title text color (set to white)
  colbacktitle=teal!70!white,     % Background color of the title bar
}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Generating Scenarios]

\textbf{System Prompt:} You are an expert generator of data.

\rule{\linewidth}{1pt}\\

You are a research scientist. You want to make data to test an advanced question answering system.\\

Give me 5 examples of real-life scenarios where a \texttt{USER\_PERSONA} may seek information in a \texttt{COLLECTION\_OF\_DOCS}. Do not consider educational or historical scenarios.\\

Some examples are:\\
\texttt{USER\_PERSONA}: College student\\
\texttt{COLLECTION\_OF\_DOCS}: Intranet on the university website\\

\texttt{USER\_PERSONA}: Intern doctor at a hospital\\
\texttt{COLLECTION\_OF\_DOCS}: Encyclopedia of diseases\\

\texttt{USER\_PERSONA}: Immigrant in NYC\\
\texttt{COLLECTION\_OF\_DOCS}: Laws on renting and subletting\\

\texttt{USER\_PERSONA}: HR manager at a top law firm\\
\texttt{COLLECTION\_OF\_DOCS}: Court and newspaper records\\

\texttt{USER\_PERSONA}: Scientist at an NGO\\
\texttt{COLLECTION\_OF\_DOCS}: Government website for Income Tax\\

Answer in the following format:\\
\texttt{USER\_PERSONA}:\\
\texttt{COLLECTION\_OF\_DOCS}:\\

\end{tcolorbox}
\caption{Prompt for generating diverse scenarios for \qaname{}.}\label{prompt:qa_scenarios}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Generating QA Pairs]

\textbf{System Prompt:} You are an expert generator of data. Do not use ** to start lines or denote points.

\rule{\linewidth}{1pt}\\

You are a research scientist. You want to make data to test an advanced question answering system.\\

Give me an example question and corresponding answer that a \texttt{\{USER\_PERSONA\}} may ask that compulsorily requires searching a \texttt{\{COLLECTION\_OF\_DOCS\}}. Make questions that cannot be answered directly with general knowledge but necessarily require some uncommon information that is present in some documents. The answer must be very specific and written in bullet points, so that it is easier to objectively evaluate. Depending on the question, the answer can have anything between 3-6 bullet points without any sub-points.\\

The answer to the question you create must be scattered across different documents (at least 3). Assign each point of the answer to a specific document in which that point will be discussed. You may assign multiple points to the same document, but each point must only be assigned to a single document. You must state the title and answer points assigned for each of the documents.\\

Answer in the following format:\\

Question: $<$Question$>$\\
Answer: $<$Answer$>$\\

Document 1 Title: $<$Title$>$\\
Document 1 Answer points assigned: $<$Points$>$\\

Document 2 Title: $<$Title$>$\\
Document 2 Answer points assigned: $<$Points$>$\\

and so on...
\end{tcolorbox}
\caption{Programmatic prompt for generating question-answer pairs for \qaname{}.}\label{prompt:qa_qa}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Generating Irrelevant Information QA Pairs]

\textbf{System Prompt:} You are an expert generator of data. Do not use ** to start lines or denote points.

\rule{\linewidth}{1pt}\\

You are a research scientist. You want to make hard data to test an advanced question answering system. You are given a question that a \texttt{\{USER\_PERSONA\}} might want answered, along with the corresponding answer, and information of documents from \texttt{\{COLLECTION\_OF\_DOCS\}} that are important for answering that question. \\

Original Question: \texttt{\{QUESTION\}}\\

Original Answer:\\
\texttt{\{ANSWER\}}\\

Original Documents Information:\\
\texttt{\{DOCS\_INFORMATION\}}\\

You must generate an adversarial question, adversarial answer, and corresponding adversarial documents that ask for something different but on similar topics or type so that it is difficult to answer the original question. Examples of how adversarial questions should look like are provided below:\\

Original Question: What are the best activities to do in Montreal, Canada during the winter season?\\
Adversarial Question: What activities should I look at when visiting Tokyo during the summer?\\

[Redacted]\\

Also provide an answer to the adversarial question, which is similar in style to the original answer, but differs significantly in information or specifics. The answer points for the adversarial question should be written in context of that adversarial question, so that they cannot be confused with the original question. Note that none of the points appearing in the original answer should be present in the answer to the adversarial question.\\

The answer to the adversarial question you craft must be scattered across different documents (at least 3) separate from the original answer documents. Assign each point of the adversarial answer to a specific document in which that point will be discussed. You may assign multiple points to the same adversarial document, but each point must only be assigned to a single adversarial document. You must state the title and adversarial answer points assigned for each of the adversarial documents. These adversarial documents should not have any overlapping information with the original answer documents.\\

Answer in the following format:\\

[Redacted]\\
\end{tcolorbox}
\caption{Programmatic prompt for generating irrelevant information question-answer pairs for \qaname{}.}\label{prompt:qa_adversarial}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Generating Documents]

\textbf{System Prompt:} You are an expert data generator. Following the instruction, you must generate long and correct documents.

\rule{\linewidth}{1pt}\\

You need to generate the documents for an example of a retrieval based Question Answering Task. \\

The task consists of n documents provided in English text that consist of information about different topics and a question. To answer the question correctly compulsorily requires using some of the information in some subset of the documents provided.\\

Given below is a situation faced by \texttt{\{USER\_PERSONA\}} when searching \texttt{\{COLLECTION\_OF\_DOCS\}}. The question-answer pair is:\\

Question: \texttt{\{QUESTION\}}\\
Answer: \texttt{\{ANSWER\}}\\

Given below are the assigned answer points for each document.\\

{\texttt{\{DOCS\_INFORMATION\}}}\\

Your job is to create long documents according to this information. For each document, first create 10-12 unique other points that are in no way related to the topic of the question and answer (different points for each document). These points should discuss very different things about a similar but different topic. Then use these points along with the assigned answer points to create a long document (at least 700 words long). The assigned answer points must be discussed taking into account the question. You must only discuss about these points and nothing else. Change the order of the points so that the answer points are embedded inside the document. Assign an appropriate title to the document. Do not summarize or conclude the document in the end.\\

Additionally, ensure that the documents you create do not have any information related to the following irrelevant question-answer pairs. You should create documents that discuss topics that are completely different from the following information.\\

\texttt{\{IRRELEVANT\_QUESTIONS\_ANSWERS\}}\\

Give output in the following format:\\
Document 1:\\
Title: $<$Title$>$\\
Question: \texttt{\{QUESTION\}}\\
Answer points assigned [Only these points must be covered with respect to the question]: $<$Points$>$\\
Other unrelated points created: $<$Points$>$\\
Text:\\
$<$Document Text$>$\\

[Redacted]

and so on...

\end{tcolorbox}
\caption{Programmatic prompt for generating documents for \qaname{}.}\label{prompt:qa_documents}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Verifying Irrelevant Information]

\textbf{System Prompt:} You are an expert at verifying data.

\rule{\linewidth}{1pt}\\

You are given a question and an answer. You must check whether the answer is even partially relevant for answering the question. If the answer is not relevant at all, output ``False" to ``Relevance". Otherwise, if and only if the answer discusses information that is at least partially necessary to answer the question, output ``True".\\

Question: \texttt{\{QUESTION\}}\\

Answer:\\
\texttt{\{IRRELEVANT\_ANSWERS\}}\\

Give output in the following format:\\
Relevance: True/False\\

\end{tcolorbox}
\caption{Programmatic prompt for verifying irrelevance of irrelevant information for \qaname{}.}\label{prompt:qa_verify_adv_cross_check}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Verifying Absence of Relevant Information]

\textbf{System Prompt:} You are an expert at verifying data.

\rule{\linewidth}{1pt}\\

You are given a document followed by a question and some answer points. You must check whether there are any additional major points in the document that provide relevant information for answering the question that are currently missing from the answer. Follow these instructions:\\

1. Do not look for exact phrases or explicit mentions since the answer can have points that are a paraphrase of the same broad information. \\

2. It is ok if the document provides more specifics or details about the points already in the answer or if it discusses them in more depth by introducing related information so you can ignore that. \\

3. Check if the document introduces a new ``major" idea or point that is crucial for answering the question and is not at all mentioned in the answer and is not an extension of the existing points in the answer.\\

4. Your job is not to check if the question can be sufficiently answered. You should ignore if the document or answer points are missing any points that are needed in the answer to the question.\\

If the document is not introducing major new points pertaining to the answer, output ``False" to ``Presence of Extra Points" without giving any explanation. Otherwise, if and only if the document discusses major additional points that are necessary to answer the question, output ``True" and mention only the extra major points discussed.\\

Document:\\
\texttt{\{Document\}}\\

Question: \texttt{\{QUESTION\}}\\

Answer Points:\\
\texttt{\{ANSWER\}}\\

Give output in the following format:\\
Presence of Extra Points: True/False\\
Extra Points Mentioned (if any):

\end{tcolorbox}
\caption{Programmatic prompt for verifying absence of relevant information in the documents for \qaname{}.}\label{prompt:qa_verify_extra}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Verifying Presence of Answer]

\textbf{System Prompt:} You are an expert at verifying data.

\rule{\linewidth}{1pt}\\

You are given a document followed by a question and an answer point. You must check two things:\\
1. Presence: Is the point mentioned in the document?\\
2. Relevance: Is the point discussed in a manner such that it can be used to partially answer the question?\\

Document:\\
\texttt{\{DOCUMENT\}}\\

Question: \texttt{\{QUESTION\}}\\

Answer Point:\\
\texttt{\{ANSWER\_POINT\}}\\

Give output in the following format:\\
Presence: True/False\\
Explanation for Presence:\\

Relevance: True/False\\
Explanation for Relevance:

\end{tcolorbox}
\caption{Programmatic prompt for verifying presence of ground-truth answer in the documents for \qaname{}.}\label{prompt:qa_verify_presence}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: Solving the Task]

\textbf{System Prompt:} You are an expert at answering questions based on documents.

\rule{\linewidth}{1pt}\\

You are given some documents followed by a question. You need to generate the answer for that question. Provide the answer in bullet points, so that it is easier to objectively evaluate. Answering the question correctly requires information from multiple documents. You must only generate the points necessary for answering the question, without mentioning anything irrelevant to the question. If you find no relevant information in the documents for answering the question, you must only generate ``No relevant information found in the documents." and nothing else.\\

Documents:
\texttt{\{DOCUMENTS\}}\\

Question: \texttt{\{QUESTION\}}\\

Answer:

\end{tcolorbox}
\caption{Programmatic prompt for solving examples in \qaname{}.}\label{prompt:qa_solve}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: LLM-as-a-Judge for Calculating Accuracy]

\textbf{System Prompt:} You are an expert evaluator.

\rule{\linewidth}{1pt}\\

You are given a question, irrelevant answers, the ground-truth answer, and a prediction. You need to evaluate whether the prediction is correct by matching against the ground truth answer. Do not look for exact phrases or words since the prediction can have points that are a paraphrase of the same information. Based on the question, check for the presence of the same ideas or main points in the prediction as in the ground-truth answer. All the main points in the ground-truth answer must be mentioned in the prediction. The order of points mentioned is irrelevant. It is allowed for the prediction to elaborate or provide more specifics or details over the major points in the ground-truth answer. However, the prediction should not contain additional major points that are contradictory or irrelevant for answering the question. Importantly, the prediction must not discuss any of the points mentioned in the ``irrelevant answers". The first word in your response must be either True or False. If False, explain why you think the prediction is wrong in detail.\\

Question: \texttt{\{QUESTION\}}\\

Irrelevant Answers: \texttt{\{IRRELEVANT\_ANSWERS\}}\\

Ground-truth Answer: \texttt{\{GROUND\_TRUTH\_ANSWER\}}\\

Prediction: \texttt{\{PREDICTION\}}\\

Result:

\end{tcolorbox}
\caption{Programmatic prompt for evaluating accuracy of predictions of models for problems in \qaname{}.}\label{prompt:qa_evaluation}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: LLM-as-a-Judge for Calculating K-Precision]

\textbf{System Prompt:} You are an expert evaluator.

\rule{\linewidth}{1pt}\\

You are given a question, an answer written in points, and some documents. You need to check whether the information in the answer points is discussed in the documents in a manner such that it can be used to at least partially answer the question. You do not need to think about the overall correctness of the answer points, just check whether or not a particular answer point is discussed in the documents. Your goal is to calculate precision, i.e., the percentage (out of 100) of answer points that have been adequately mentioned in the document. The first thing in your response must be ``Precision: " followed by the precision value in decimal form. If precision is less than 100\%, explain which answer points are not present in the document.\\

Documents: \texttt{\{DOCUMENTS\}}\\

Question: \texttt{\{QUESTION\}}\\

Answer Points: \texttt{\{ANSWER\_POINTS\}}\\

Result:

\end{tcolorbox}
\caption{Programmatic prompt for evaluating K-Precision of predictions of models for problems in \qaname{}.}\label{prompt:qa_precision}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\qaname{}: LLM-as-a-Judge for Calculating Recall]

\textbf{System Prompt:} You are an expert evaluator.

\rule{\linewidth}{1pt}\\

You are given a question, a statement, and some reference points. You need to check whether the information in the statement is discussed in the reference points in a manner such that it can be used to at least partially answer the question. It is okay if the reference points contain a lot more information, your goal is to only check whether the statement is included in the reference points. The first word in your response must be either True or False. If False, explain why in detail.\\

Question: \texttt{\{QUESTION\}}\\

Statement: \texttt{\{STATEMENT\}}\\

Reference Points: \texttt{\{REFERENCE\_POINTS\}}\\

Result:

\end{tcolorbox}
\caption{Programmatic prompt for evaluating recall of predictions of models for problems in \qaname{}.}\label{prompt:qa_recall}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\codename{}: Generating Helper Functions]

\textbf{System Prompt:} You are an expert generator of code data.

\rule{\linewidth}{1pt}\\

You are a research scientist. You want to make data to test an advanced code generation system. You are given a domain. Assume that there is a large python code base 'C' with at least 10 python files on that domain.\\

Domain: \texttt{\{DOMAIN\}}\\

You need to create 5 functions in this codebase for achieving various objectives. First define the parameters that will be input to the function. Then define the objective of the function. The objective must consist of 3-4 sub-goals, each of which must involve complex logic that make it very difficult to implement the function. However, each sub-goal must be well-specified such that there is only one way to implement the sub-goal. Then based on the objective, you need to create a single function (do not create other functions inside this).\\

Some examples are:\\

Parameters:\\
- data: pandas.DataFrame\\
- k: int\\
Objectives:\\
- In the dataframe ``data", find the ``frequency" of occurence of rows that have at least one string field with the number of letters divisible by ``k".\\

[redacted]\\

Function ``filter\_k\_frequency" in file ``string\_filters.py":
\begin{lstlisting}
import pandas as pd

def filter_k_frequency(data, k):
    [redacted]
    return frequency, filtered_df
\end{lstlisting}

Now you need to create 5 unique, diverse, and complex functions. Answer in the following format:\\

Function $<$Number$>$:\\
Parameters:\\
- $<$para\_name$>$: $<$data\_type$>$\\
...\\
Objectives:\\
- $<$sub\_goal$>$\\
...\\

Function ``function\_name" in file ``file\_name.py":\\
$<$import statements$>$\\

$<$function definition$>$

\end{tcolorbox}
\caption{Prompt for generating helper functions for \codename{}.}\label{prompt:code_helper}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=\codename{}: Generating Problem Statement and Answer Code]

\textbf{System Prompt:} You are an expert generator of code data.

\rule{\linewidth}{1pt}\\

You are a research scientist. You want to make data to test an advanced code generation system.\\

Below, you are given 10 functions from a codebase ``C" in the domain of \texttt{\{DOMAIN\}}.\\

Parameters:\\
- data: pandas.DataFrame\\
- k: int\\
Objectives:\\
- In the dataframe ``data", find the ``frequency" of occurence of rows that have at least one string field with the number of letters divisible by ``k".\\

[redacted]\\

Function ``filter\_k\_frequency" in file ``string\_filters.py":
\begin{lstlisting}
import pandas as pd

def filter_k_frequency(data, k):
    [redacted]
    return frequency, filtered_df
\end{lstlisting}

[redacted]\\

You need to create a complex function that calls at least 4 (but not more than 6) of these functions to achieve various objectives. Apart from just calling these functions, it should also implement some other pieces of complex logic. You first need to define the parameters that will be input to the function. Then you need to define the objective of the function. Follow these instructions for creating the objective:\\

1. The objective must consist of 6-8 sub-goals. Each sub-goal must be detailed and well-specified such that there is only one way to implement the sub-goal.\\
2. VERY IMPORTANT: The objective must not explicitly specify which functions should be called.\\
3. Always give names for variables you are talking about in the objective.\\
4. You must explicitly mention what parameters are to be used for a specific sub-goal by the name of the parameter.\\
5. Whenever a variable is obtained that must be returned by the function, you must explicitly state that in the sub-goal.\\
6. At least 2 of the sub-goals must involve some complex logic, apart from just calling helper functions that make it very difficult to implement the function.\\

Once you write down the objective, you need to create the function that achieves this objective. Import the required functions from the codebase ``C" and use them in your function.\\

You must give output in the following format:\\

[redacted]

\end{tcolorbox}
\caption{Programmatic prompt for generating problem statement and answer code for \codename{}.}\label{prompt:code_problem}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=\codename{}: Generating Test Code]

\textbf{System Prompt:} You are an expert tester of code systems.

\rule{\linewidth}{1pt}\\

You are given a function. You need to define an input-output test case for that function to exhaustively test all scenarios.\\

\texttt{\{ANSWER\_FUNCTION\}}\\

Follow these instructions:\\
1. You must output only a single long python code. \\
2. First initialize the input parameters for the function in python code. If the function reads data from files, you should create and store the necessary files with sample data in the corresponding filepath in the python code. Call the function and assign the return values to variables named as return\_$<$variable\_name$>$.\\
3. Then write new code to implement the exact logic of the function. This way, you need to simulate step-by-step how the values of the input parameters will be used to obtain the final return values. Call these values as correct\_$<$variable\_name$>$.\\
4. Finally, and most importantly use assert statements to compulsorily check if the returned outputs of the function (return\_$<$variable\_name$>$ variables) match with the ones you computed yourself (correct\_$<$variable\_name$>$ variables).\\

Give output in the following format:

\begin{lstlisting}
# Import statements if required
import <>
...
# Import function from file
from <filename> import <function_name>
...
# Initialize input parameters
<param1> = <value1>
...
# Call function with input parameters
return_<variable1>, return_<variable2>, ... = $<function_name>(<param1>, <param2>, ...)
# Step-by-step run-through of function to obtain intermediate outputs:
# Step 1
# Explanation: <>
<Code for step-1>

[redacted]

# Final Expected Output:
correct_<variable1> = <value1>
...
# Assert statements (compulsory) to check if the function returns the correct values:
assert return_<variable1> == correct_<variable1>
...

\end{lstlisting}

\end{tcolorbox}
\caption{Programmatic prompt for generating the test code for \codename{}.}\label{prompt:code_test}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\codename{}: Verifying if Function Executes]

\textbf{System Prompt:} You are an expert tester of code systems.

\rule{\linewidth}{1pt}\\

You are given a function in a file. You need to check whether the function correctly executes.\\

\texttt{\{FUNCTION\}}\\

Follow these instructions:\\
1. You must output only a single long python code.\\
2. First initialize the input parameters for the function in python code. If the function reads data from files, you should create and store the necessary files with sample data in the corresponding filepath in the python code.\\
3. Finally, call the function with the input parameters.\\

Give output in the following format:\\

\begin{lstlisting}
# Import statements if required
import <>
...

# Import function from file
from <filename> import <function_name>

# Initialize input parameters
<param1> = <value1>
...

# Call function with input parameters
return_<variable1>, return_<variable2>, ... = <function_name>(<param1>, <param2>, ...)
\end{lstlisting}

\end{tcolorbox}
\caption{Programmatic prompt for generating the test code for verifying if a function executes correctly for \codename{}.}\label{prompt:code_verify_execution}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\codename{}: Verifying Problem Statement]

\textbf{System Prompt:} You are an expert programmer.

\rule{\linewidth}{1pt}\\

You are given a codebase with some files and functions in the domain of {params[0]}. You need to write a single python function to achieve the objectives specified in the problem statement. You may call the functions in the codebase when necessary. Do not give any examples of usage or any explanations.\\

Codebase:\\

\texttt{\{RELEVANT\_FUNCTIONS\}}\\

Problem Statement:\\

\texttt{\{PROBLEM\_STATEMENT\}}\\

Give output in the following format:\\

\begin{lstlisting}
# Import statements if required
import <>
...

# Import necessary helper functions from their files
from <filename> import <function_name>

# Define the function
def <function_name>(<param1>, <param2>, ...):
	# Your code here
	...

	return <return_variable>

\end{lstlisting}

\end{tcolorbox}
\caption{Programmatic prompt for verifying if the problem statement sufficiently specifies the answer code for \codename{}.}\label{prompt:code_verify_problem}
\end{figure}




\begin{figure}[ht]
\begin{tcolorbox}[title=\codename{}: Solving the Task]

\textbf{System Prompt:} You are an expert programmer. You must output only python code.

\rule{\linewidth}{1pt}\\

You are given a codebase. You need to write a single python function to achieve the objectives specified in the problem statement. In your function, you should call some of the functions in the codebase to achieve specific objectives. Do not give any examples of usage or any explanations.\\

Codebase:\\

\texttt{\{CODEBASE\}}\\

Problem Statement:\\

\texttt{\{PROBLEM\_STATEMENT\}}\\

Give output in the following format:\\
\begin{lstlisting}
# Import statements if required
import <>
...

# Import necessary helper functions from their files
from <filename> import <function_name>

# Define the function
def <function_name>(<param1>, <param2>, ...):
	# Your code here
	...

	return <return_variable>

\end{lstlisting}

\end{tcolorbox}
\caption{Programmatic prompt for solving examples in \codename{}.}\label{prompt:code_solve}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\mathname{}: Breaking-down Seed MWPs]

\textbf{System Prompt:} You are an expert mathematician.

\rule{\linewidth}{1pt}\\

You are a research scientist. Your task is to create a hard math word problem to test an advanced math reasoning system. For that, you are given the following problem:\\

Q: \texttt{\{QUESTION\}}\\
A: \texttt{\{ANSWER\}}\\

Your job is to first divide up the problem into the ``context" and the ``question statement". Isolate the quantity that the problem is inquiring about by looking at the final question statement and the rest of the information provided becomes the context. Also form a brief answer statement by phrasing the answer in a complete sentence. Do not include the answer statement in the context.\\

Give output in the following format only:\\
Original context [without question statement]: $<>$\\
Question statement: $<>$\\
Original answer: $<>$\\
Original answer statement: $<>$

\end{tcolorbox}
\caption{Programmatic prompt for breaking down the seed MWP for \mathname{}.}\label{prompt:math_break}
\end{figure}





\begin{figure}[ht]
\begin{tcolorbox}[title=\mathname{}: Creating Continuation of MWP]

\textbf{System Prompt:} You are an expert mathematician.

\rule{\linewidth}{1pt}\\

You are a research scientist. Your task is to create a hard math word problem to test an advanced math reasoning system. For that, you are given the following problem:\\

Context: \texttt{\{CONTEXT\}}\\
Question statement: \texttt{\{QUESTION\_STATEMENT\}}\\
Answer: \texttt{\{ANSWER\}}\\
Answer statement: \texttt{\{ANSWER\_STATEMENT\}}\\

You need to further continue the problem over the answer quantity, by introducing a scenario and new question where you need to perform one more operation (such as +,-,/,*, etc.) over this quantity to get the final answer. Crucially, the new context must not mention the original answer - it still has to be inferred based on previous information. Do not make any calculation or inference in the new context. Try to make the new context challenging. Also provide a complete reasoning of how you reached the new answer (never round down or round up decimals).\\

Give output in the following format only:\\
New operation over original answer: $<>$\\
New context [Do not mention original answer]: $<>$\\
New question statement: $<>$\\
New answer reasoning: $<>$\\
New answer [Number only]: $<>$\\

\end{tcolorbox}
\caption{Programmatic prompt for extending the seed MWP for \mathname{}.}\label{prompt:math_extend}
\end{figure}



\begin{figure}[ht]
\begin{tcolorbox}[title=\mathname{}: Solving MWP - 8-shot chain-of-thought]

\textbf{System Prompt:} You are an expert mathematician. Your final statement must be of the form 'The answer is $<$answer$>$'.

\rule{\linewidth}{1pt}\\

Solve the final math word problem given below by thinking step-by-step. You should always work with exact numbers - never round down or round up decimals based on context. Give the final answer in the end by saying ``The answer is $<$number$>$".\\

Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\
A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\
A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\
A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\\
Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\
A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\\
Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\
A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\\
Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\
A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\\
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\
A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\\
Q: Olivia has \$23. She bought five bagels for \$3 each. How much money does she have left?\\
A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\\
Q: \texttt{\{QUESTION\}}\\
A:

\end{tcolorbox}
\caption{Programmatic prompt for solving an example in \mathname{} using chain-of-thought.}\label{prompt:math_solve}
\end{figure}


\begin{figure}[ht]
\begin{tcolorbox}[title=\mathname{}: Solving MWP - 8-shot sentence-by-sentence]

\textbf{System Prompt:} You are an expert mathematician. Your final statement must be of the form 'The answer is $<$answer$>$'.

\rule{\linewidth}{1pt}

You need to solve the given math word problem. You should break down the problem sentence by sentence, and solve each sentence, one at a time, from start to finish until you get the final answer. You should always work with exact numbers - never round down or round up decimals based on context. Give the final answer in the end by saying ``The answer is $<$number$>$".\\
Given below are illustrations of solving sentence-by-sentence:\\

Q: In a store, an Uno Giant Family Card costs \$12. When Ivan bought ten pieces, he was given a discount of \$2 for each. The store has a 8\% sales tax added to all purchases. Ivan decides to save 25\% of this expenditure for a future vacation. After saving, Ivan instead decides to split this amount between two of his friends who are always helping him out. One of the friends decided to split their received amount equally among the five children in Ivan's neighborhood who helped him move the previous day. How much will each child receive?\\
A: Sentence 1: Uno card costs \$12.\\
Sentence 2: Ivan bought 10 cards and there was a discount of \$2 each. So, 10 * \$12 = \$120 total cost of cards and, 10 * \$2 = \$20 discount.\\
Sentence 3: Sales tax is 8\% of (\$120 - \$20 = \$100). So, 8\% of \$100 = \$8.\\
Sentence 4: Ivan saves 25\% of (\$100 + \$8) = \$108. So, 25\% of \$108 = \$27.\\
Sentence 5: Ivan splits \$27 between 2 friends. So, \$27 / 2 = \$13.50 each.\\
Sentence 6: One friend splits \$13.50 among 5 children. So, \$13.50 / 5 = \$2.70 each.\\
The answer is 2.70.\\

[redacted]\\

Q; Carly is trying to get in shape to try out for the soccer team. She starts by running 2 miles a week. The second week, she runs twice as long plus 3 extra miles per week. The third week she runs 9/7 as much as she ran the second week. The week after that, she sprains her ankle and has to reduce her running time by 5 miles this week compared to the previous week. After a few weeks of recovering from her ankle injury, Carly starts to feel better and decides to gradually increase her running time. She starts with a shorter routine that is one quarter of the amount she ran the week she was injured. After a week of intense training, Carly decides to boost her speed and endurance by increasing her weekly running routine to 2.5 times longer. Carly is planning increase the weekly routine by 5 times now. How much is Carly planning to run every week?\\
A: Sentence 1: Nothing to solve.\\
Sentence 2: Carly runs 2 miles a week.\\
Sentence 3: Carly runs 2 * 2 + 3 = 7 miles in the second week.\\
Sentence 4: Carly runs 9/7 * 7 = 9 miles in the third week.\\
Sentence 5: Carly reduces her running time by 5 miles this week. So, 9 - 5 = 4 miles.\\
Sentence 6: Nothing to solve.\\
Sentence 7: Carly starts with 1/4 of 4 miles = 1 mile.\\
Sentence 8: Carly increases her running routine to 2.5 times longer. So, 1 * 2.5 = 2.5 miles.\\
Sentence 9: Carly wants to make her long run 5 times as long. So, 2.5 * 5 = 12.5 miles.\\
The answer is 12.5.\\

Q: \texttt{\{QUESTION\}}\\
A:

\end{tcolorbox}
\caption{Programmatic prompt for solving an example in \mathname{} sentence-by-sentence.}\label{prompt:math_solve_sentence}
\end{figure}


\end{document}
