\section{Theoretical Results for \shortname}\label{app:fgw}

This section provides algorithms, in pseudo-code, to describe \shortname (see Algorithms \ref{alg:fedgwcluster} and \ref{alg:fedgw_recursion}). Additionally, here we provide the proofs for the convergence results introduced in Section \ref{sec:theory}, specifically addressing the convergence (Theorems \ref{thm_main:1} and \ref{thm_main:weak_conv}) and the formal derivation on the variance bound of the Gaussian weights (Proposition \ref{prop_var_main}). \textcolor{black}{In addition, we also present a sufficient condition, under which is guaranteed that the overall sampling rate of the training algorithm does not increase and remain unchanged during the training process (Theorem \ref{thm:samplerate}).}

\begin{theorem}\label{thm:1}
Let $\{\alpha_t\}_{t = 1}^\infty$ be a sequence of positive real values, and $\{\Gamma_k^t\}_{t=1}^\infty$ the sequence of Gaussian weights. If $\{\alpha_t\}_{t = 1}^\infty \in l^2(\mathbb{N})/l^1(\mathbb{N})$, then $\Gamma_k^t$ converges in $L^2$. Furthermore, for $t\to\infty$, 
\begin{equation}
    \Gamma_k^t \longrightarrow \mu_k\,\,\, a.s.
\end{equation}
\end{theorem}
\begin{proof}
At each communication round, we compute the samples $r_i^{t,s}$ from $R_k^{t,s}$ via a Gaussian transformation of the observed loss in Eq. \ref{eq:reward}. Notice that, due to the linearity of the expectation operator, $\mathbb{E}[\Omega_k^t] = \mu_k$, that is the true, unknown, expected reward. The observed value for the random variable is given by $\omega_k^t = 1/S \sum_{s = 1}^S r_k^{t,s}$, which is sampled from a distribution centered on $\mu_k$.
Each client's weight is updated according to
\begin{equation}\label{weight_formula}
    \gamma_k^{t+1} = (1-\alpha_t)\gamma_t + \alpha_t \omega_k^t\,.
\end{equation}
Since such an estimator follows a Robbins-Monro algorithm, it is proved to converge in $L^2$. In addition, $\Gamma_k^t$ converges to the expectation $\mathbb{E}[\Omega_k^t] = \mu_k$ with probability 1, provided that $\alpha_t$ satisfies $\sum_{t\geq 1}|\alpha_t| = \infty$, and $\sum_{t\geq 1}|\alpha_t|^2 < \infty$ \citep{harold1997stochastic}.
\end{proof}

\begin{theorem}\label{thm:weak_conv}
Let $\alpha \in (0,1)$ be a fixed constant, then in the limit $t \to \infty$, the expectation of the weights converges to the individual theoretical reward $\mu_k$, for each client $k = 1,\dots, K$, \ie,
\begin{equation}
    \mathbb{E}[\Gamma_k^t]\longrightarrow \mu_k\,\,\,t\to\infty\,.
\end{equation}
\end{theorem}
\begin{proof}
Recall that $\gamma_k^{t+1} = (1-\alpha)\gamma_k^t + \alpha \omega_k^t$, where $\omega_k^t$ are samples from $\Omega_k^t$. If we substitute backward the value of $\gamma_k^t$ we can write
\begin{equation}
\gamma_k^{t+1} = (1-\alpha)^2\gamma_k^{t-1} + \alpha \omega_k^t + \alpha(1-\alpha)\omega_k^{t-1}\,.
\end{equation}
By iterating up to the initialization term $\gamma_k^0$ we get the following formulation: 
\begin{equation}\label{explicit}
    \gamma_k^{t+1} = (1-\alpha)^{t+1} \gamma_k^0+ \sum_{\tau = 0}^t \alpha (1-\alpha)^\tau \omega_k^{t-\tau}\,\,.
\end{equation}
Since $\omega_k^t$ are independent and identically distributed samples from $\Omega_k^t$, with expected value $\mu_k$, then the expectation of the weight at the $t$-th communication round would be
\begin{equation}
    \mathbb{E}[\Gamma_k^{t}] = \mathbb{E}\left[(1-\alpha)^{t}\gamma_k^0 + \sum_{\tau = 0}^t \alpha (1-\alpha)^\tau \Omega_k^{t-\tau-1}\right ]\,\,,
\end{equation}
that, due to the linearity of expectation, becomes
\begin{equation}
    \mathbb{E}[\Gamma_k^{t}] = (1-\alpha)^{t}\gamma_k^0 + \sum_{\tau = 0}^t \alpha (1-\alpha)^\tau \mu_k\,\,.
\end{equation}
If we compute the limit
\begin{equation}
    \lim_{t \to \infty}\mathbb{E}[\Gamma_k^{t}] =\lim_{t\to\infty}(1-\alpha)^{t}\gamma_k^0 + \sum_{\tau = 0}^\infty \alpha (1-\alpha)^\tau \mu_k\,\,,
\end{equation}
and since $\alpha\in(0,1)$, the first term tends to zero, and also the geometric series converges. Therefore, the expectation of the weights converges to $\mu_k$, namely
\begin{equation}
    \lim_{t \to \infty} \mathbb{E}[\Gamma_k^t] = \mu_k\,.
\end{equation}
\end{proof}

\begin{proposition}\label{prop_var}
The variance of the weights $\Gamma_k^t$ is smaller than the variance $\sigma_k^2$ of the theoretical rewards $R_k^{t,s}$.
\end{proposition} 
\begin{proof}
From Eq.\ref{explicit}, we can show that $\mathbb{V}ar(\Gamma_k^t)$ converges to a value that depends on $\alpha$ and the number of local training iterations $S$. Indeed
\begin{equation}
\begin{split}
\mathbb{V}ar(\Gamma_k^t) &= \mathbb{V}ar\left( (1-\alpha)^{t} \gamma_k^0 + \sum_{\tau = 0}^t \alpha (1-\alpha)^\tau \Omega_k^{t-\tau-1}\right) \\
&= \sum_{\tau = 0}^t \alpha^2 (1-\alpha)^{2\tau} \mathbb{V}ar(\Omega_k^t) = \dfrac{1}{S}\sum_{\tau = 0}^t \alpha^2 (1-\alpha)^{2\tau}\sigma_k^2
\end{split}
\end{equation}
since $\Omega_k^t = 1/S \sum_{s = 1}^S R_k^{t,s}$\,.

If we compute the limit, that exists finite due to the hypothesis $\alpha \in (0,1)$, we get
\begin{equation}
\lim_{t \to \infty}\mathbb{V}ar(\Gamma_k^t) = \dfrac{\alpha^2\sigma_k^2}{S}\sum_{\tau = 0}^\infty (1-\alpha)^{2\tau} = \dfrac{\alpha}{2-\alpha} \dfrac{\sigma_k^2}{S} <\dfrac{\sigma_k^2}{S}<\sigma_k^2\,\,.
\end{equation}
\end{proof}
We further demonstrate that the interaction matrix $P^t$ identified by \shortname is entry-wise bounded from above, as established in the following proposition.
\begin{proposition}\label{prop:bounded_matrix}
The entries of the interaction matrix $P^t$ are bounded from above, namely for any $t \geq 0$ there exists a positive finite constant $C_t > 0$ such that
\begin{equation}
    P_{kj}^t \leq C_t\,\,.
\end{equation}
And furthermore
\begin{equation}
    \lim_{t \to \infty} C_t = 1\,\,.
\end{equation}
\end{proposition}
\begin{proof}
Without loss of generality we assume that every client of the federation is sampled, and we assume that $\alpha_t = \alpha \in (0,1)$ for any $t \geq 0$. We recall, from Eq.\ref{inter_matrix}, that for any couple of clients $k,j \in \mathcal{P}_t$ the entries of the interaction matrix are updated according to 
\begin{equation}
    P_{kj}^{t+1} = (1-\alpha) P_{kj}^t + \alpha \omega_k^t\,.
\end{equation}
If we iterate backward until $P_{kj}^0$, we obtain the following update
\begin{equation}
     P_{kj}^{t+1} = (1-\alpha)^{t+1} P_{kj}^{0}+ \sum_{\tau = 0}^t \alpha (1-\alpha)^\tau \omega_k^{t-\tau}\,\,.
\end{equation}
We know that, by constructions, the Gaussian rewards $\omega_k^t < 1$  at any time $t$, therefore the following inequality holds
\begin{equation}
    P_{kj}^{t} = (1-\alpha)^{t} P_{kj}^{0}+ \sum_{\tau = 0}^t \alpha (1-\alpha)^\tau \omega_k^{t-\tau-1} \leq (1-\alpha)^{t} P_{kj}^{0}+ \sum_{\tau = 0}^t \alpha (1-\alpha)^\tau\,.
\end{equation}
At any round $t$ we can define the constant $C_t$, as
\begin{equation}
    C_t := (1-\alpha)^t P_{kj}^0 + \alpha \sum_{\tau = 0}^t(1-\alpha)^\tau = (1-\alpha)^t P_{kj}^0 + 1 -(1-\alpha)^{t+1} < \infty\,.
\end{equation}
Moreover, since $\alpha \in (0,1)$, by taking the limit we prove that 
\begin{equation}
    \lim_{t \to \infty} C_t = \lim_{t \to \infty} (1-\alpha)^t P_{kj}^0 + 1 -(1-\alpha)^{t+1} = 1\,.
\end{equation}
\end{proof}

\begin{theorem}{(Sufficient Condition for Sample Rate Conservation)}\label{thm:samplerate} Consider $K_{min}$ as the minimum number of clients permitted per cluster, \ie the cardinality $|\mathcal{C}_n| \geq K_{min}$ for any given cluster $n = 1,\dots, n_{cl}$, and $\rho \in (0,1]$ to represent the initial sample rate. There exists a critical threshold $n^* > 0$ such that, if $K_{min} \geq n^*$ is met, the total sample size does not increase.
\end{theorem}
\begin{proof}
Let us denote by $\rho_n$ the participation rate relative to the $n$-th cluster, \ie
\begin{equation}\label{eq:rho^n}
    \rho_n = \max \left\{\rho, \dfrac{3}{|\mathcal{C}_n|}\right\}
\end{equation}
because, in order to maintain privacy of the clients' information we need to sample at least three clients, therefore $\rho^n$ is at least $3$ over the number of clients belonging to the cluster. The total participation rate at the end of the clustering process is given by
\begin{equation}
    \rho^{\text{global}} = \sum_{n = 1}^{n_{cl}} \dfrac{K_n}{K}
\end{equation}
where $K_n$ denotes the number of clients sampled within the $n$-th cluster. If we focus on the term $K_n$, recalling Equation \ref{eq:rho^n}, we have that
\begin{equation}\label{eq:K_n}
    K_n = \rho_n |\mathcal{C}_n| = \max \left\{\rho, \dfrac{3}{|\mathcal{C}_n|}\right\}\times|\mathcal{C}_n| = \max\{\rho |\mathcal{C}_n|, 3\}\,\,.
\end{equation}
If we write Equation \ref{eq:K_n}, by the means of the positive part function, denoted by $(x)^+ = \max\{0,x\}$, we obtain that
\begin{equation}
    K_n = 3 + \max\{0, \rho |\mathcal{C}_n| - 3\} = 3 + (\rho |\mathcal{C}_n| - 3)^+\,\,.
\end{equation}
Observe that we are looking for a threshold value for which $\rho^{\text{global}} = \rho$, \ie the participation rate remains the same during the whole training process.\\
Let us observe that $K_n = \rho |\mathcal{C}_n| \iff \rho |\mathcal{C}_n| \geq 3 \iff |\mathcal{C}_n| \geq n^* = 3/\rho$. In fact, if we assume that $K_{min} \geq n^*$, then the following chain of equalities holds
\begin{equation*}
    \rho^{\text{global}} = \sum_{n = 1}^{n_{cl}} \dfrac{K_n}{K} = \dfrac{1}{K} \sum_{n = 1}^{n_cl} \rho|\mathcal{C}_{n}| = \dfrac{\rho}{K} \sum_{n = 1}^{n_{cl}}|\mathcal{C}_n| = \dfrac{\rho K}{K} = \rho
\end{equation*}
thus proving that $K_{min} \geq n^*$ is a sufficient condition for not increasing the sampling rate during the training process.
\end{proof}
\begin{algorithm}[t]
\caption{\texttt{FedGW\_Cluster}}\label{alg:fedgwcluster}
   \begin{algorithmic}[1]
     \STATE \textbf{Input:} $P, n_{max}, \mathcal{K}(\cdot,\cdot)$
     \STATE \textbf{Output:} cluster labels $y_{n_{cl}}$, and number of clusters $n_{cl}$
     \STATE Extract UPVs $v_k^j, v_j^k$ from $P$ for any $k,j$
     \STATE $W_{kj}\gets \mathcal{K}(v_k^j,v_j^k)$ for any $k,j$
     \FOR{$n = 2,\dots, n_{max}$}
     \STATE $y_{n} \gets \texttt{Spectral\_Clustering}(W,n)$
     \STATE $DB_n \gets \texttt{Davies\_Bouldin}(W,y_n)$
     \IF{$\min_n DB_n > 1$}
     \STATE $n_{cl} \gets 1$
     \ELSE 
     \STATE$n_{cl} \gets \arg \min_n DB_n$
     \ENDIF
    
    \ENDFOR
   \end{algorithmic}
\end{algorithm}

    
\begin{algorithm}[t]
  \caption{\shortname}\label{alg:fedgw_recursion}
  \begin{algorithmic}[1]
    \STATE \textbf{Input:} $K, T, S, \alpha_t, \epsilon, |\mathcal{P}_t|, \mathcal{K}$
    \STATE \textbf{Output:} $\mathcal{C}^{(1)},\dots, \mathcal{C}^{(N_{cl})}$ and $\theta_{(1)}, \dots, \theta_{(N_{cl})}$ 
    \STATE Initialize $N_{cl}^0\gets 1$
    \vspace{.1cm}
    \STATE Initialize $P^{0}_{(1)} \gets 0_{K\times K}$
    \STATE Initialize $\textrm{MSE}^{0}_{(1)} \gets 1$
    \vspace{.1cm}
    \FOR{$t = 0,\dots,T-1$}
    \STATE $\Delta N^t \gets 0$ for each iterations it counts the number of new clusters that are detected
    \vspace{.1cm}
    \FOR{$n = 1,\dots, N_{cl}^t$}
    
    \STATE Server samples $\mathcal{P}_t^{(n)} \in \mathcal{C}^{(n)}$ and sends the current cluster model $\theta_{(n)}^t$
    \STATE Each client $k \in \mathcal{P}_t^{(n)}$ locally updates $\theta_k^t$ and $l_k^t$, then sends them to the server
    \STATE $\omega_k^t \gets \texttt{Gaussian\_Rewards}(l_k^t, \mathcal{P}_t^{(n)})$, Eq. \ref{eq:reward}
    \STATE $\theta_{(n)}^{t+1}\gets \texttt{FL\_Aggregator}(\theta_k^t, \mathcal{P}_t^{(n)})$
    \STATE $P^{t+1}_{(n)}\gets\texttt{Update\_Matrix}(P^t_{(n)}, \omega_k^t, \alpha_t, \mathcal{P}_t^{(n)})$, according to Eq. \ref{inter_matrix}
    \vspace{.1cm}
    \STATE Update $\textrm{MSE}_{(n)}^{t+1}$
    \vspace{.1cm}
    \IF{$\textrm{MSE}^{t+1}_{(n)} < \epsilon$}
    \vspace{.1cm}
    \STATE Perform $\texttt{FedGW\_Cluster}(P_{(n)}^{t+1}, n_{max}, \mathcal{K})$ on $\mathcal{C}^{(n)}$, providing $n_{cl}$ sub-clusters
    \vspace{.1cm}
    \STATE Update the number of new clusters $\Delta N^t \gets \Delta N^t + n_{cl} -1 $ u
    \vspace{.1cm}
    \STATE Cluster server splits $P_{(n)}^{t+1}$ filtering rows and columns according to the new clusters
    \vspace{.1cm}
    \STATE Re-initialize MSE for new clusters to $1$
    \vspace{.1cm}
    \ENDIF
    \ENDFOR
    \STATE Update the total number of clusters$N_{cl}^{t+1} \gets N_{cl}^t + \Delta N^t$ 
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
\newpage
