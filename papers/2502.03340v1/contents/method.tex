\section{Problem Formulation}
\label{section:formulation}
Consider a standard FL scenario \citep{mcmahan2017communication} with $K$ clients and a central server. FL typically addresses the following optimization problem $\min_{\theta \in \Theta} \mathcal{L}(\theta) = \min_{\theta \in \Theta} \sum_{k=1}^K \frac{n_k}{n} \mathcal{L}_k(\theta),$
where $\mathcal{L}_k(\cdot)$ represents the loss function of client $k$, $n_k$ is the number of training samples on client $k$, $n = \sum_{k=1}^K n_k$ is the total number of samples, and $\Theta$ denotes the model's parameters space.  
At each communication round $t \in [T]$, a subset $\mathcal{P}_t$ of clients is selected to participate in training. Each participating client performs $S$ iterations, updating its local parameters using a stochastic optimizer, \eg Stochastic Gradient Descent (SGD).  
In clustered FL, the objective is to partition clients into non-overlapping groups $\mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n_\text{cl})}$ based on similarities in their data distributions, with each group having its own model, $\theta_{(1)}, \dots, \theta_{(n_\text{cl})}$.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/losses_violin.pdf}
    \vspace{-2.7em}
   {\caption{\small{Illustration of the Gaussian reward mechanism for two clients from Cifar100 (Dirichlet $\alpha = 0.05$, 10 sampled clients per round and $S = 8$ local iterations).
    The dashed line represents the average loss process $m^{t,s}$, with the blue region indicating the confidence interval $m^{t,s} \pm \sigma^{t,s}$ at fixed $t$, $s = 1,\dots, 8$. The green curve corresponds to an in-distribution client, whose loss remains within the confidence region, resulting in a high Gaussian reward. The red line represents an out-of-distribution client, whose loss lies outside the confidence region, resulting in a lower reward. } %
    }\label{fig:mechanism}

}\vspace{-2em}
\end{figure}
\section{\shortname Algorithm}\label{sec:algorithm_new}
In this section, we present the core components of \shortname in a progressive manner. First, Subsection \ref{section:gaussian_rewards} introduces the Gaussian Weighting mechanism, a statistical method that estimates how well each client's data distribution aligns with the overall federation. Subsection \ref{section:clustering} then explains how we model and detect clusters by analyzing interactions between client distributions. 
For clarity, we first present these concepts considering the federation as a single cluster, focusing on the fundamental mechanisms that enable client grouping. Finally, Subsection \ref{sec:alg_fedgw} introduces the complete algorithmic framework, including cluster indices and the iterative structure that allows \shortname to partition clients into increasingly homogeneous groups.

\subsection{Gaussian Weighting Mechanism}
\label{section:gaussian_rewards} 
To assess how closely the local data of each client aligns with the global distribution, we introduce the \textit{Gaussian Weights} $\gamma_k$, statistical estimators that capture the closeness of each clients' distribution to the main distribution of the cluster. A weight near zero suggests that the client's distribution is far from the main distribution. We graphically represent the idea of the Gaussian rewards in Figure \ref{fig:mechanism}. 

The fundamental principle of \shortname is to group clients based on the similarity of their empirical losses, which are used to assign a \textit{reward} between 0 and 1 to each client at each local iteration. A high reward indicates that a client's loss is close to the cluster's mean loss, while a lower reward reflects greater divergence. Gaussian weights estimate the expected value of these rewards, quantifying the closeness between each clientâ€™s distribution and the global one. 

Every communication round $t$, each sampled client $k \in \mathcal{P}_t$ communicates the server the empirical loss process $l_k^{t,s} = \mathcal{L}(\theta_k^{t,s})$, for $s = 1,\dots,S$, alongside the updated model $\theta_k^{t+1}$. The server computes the \textit{rewards} as
\begin{equation}\label{eq:reward}
r_k^{t,s} = \exp{\left(- \dfrac{(l_k^{t,s} - m^{t,s})^2}{2(\sigma^{t,s})^2}\right)} \in (0,1)\quad,
\end{equation}
where $m^{t, s} = 1/|\mathcal{P}_t| \sum_{k \in \mathcal{P}_t} l_k^{t,s}$ is the average loss process w.r.t. the local iterations $s = 1,\dots, S$, and $({\sigma}^{t,s})^2 = 1/(|\mathcal{P}_t|-1) \sum_{k \in \mathcal{P}_t} (l_k^{t,s} - m^{t, s})^2 $ is the sample variance. The rewards $r_k^{t,s} \to 1$ as the distance between $l_k^{t,s}$ and the average process $m^{t, s}$ decreases, while $r_k^{t,s} \to 0$ as the distance increases.
In Eq. \ref{eq:reward}, during any local iteration $s$, a Gaussian kernel centered on the mean loss and with spread the sample variance assesses the clients' proximity to the confidence interval's center, indicating their probability of sharing the same learning process distribution. Since the values of the rewards can suffer from stochastic oscillations, we reduce the noise in the estimate by averaging the rewards over the  $S$ local iterations, thus obtaining $\omega_k^t = 1/S \sum_{s \in [S]} r_k^{t,s}$ for each sampled client $k$. Instead of using a single sample, such as the last value of the loss as done in \cite{cho2022towards}, we opted for averaging across iterations to provide a more stable estimate. However, the averaged rewards $\omega_k^t$ depend on the sampled set of clients $\mathcal{P}_t$ and on the current round. Hence, we introduce the \textbf{Gaussian weights} $\gamma_k^t$  to keep track over time of these rewards. The Gaussian weights are computed via a running average of the instant rewards $\omega_k^t$. In particular, for each client $k$ the weight $\gamma_k^0$ is initialized to $0$ to avoid biases in the expectation estimate, and when it is randomly selected is update according to
\begin{equation}\label{eq:robbins_monro_weights}
    \gamma_k^{t+1} = (1-\alpha_t) \gamma_k^t + \alpha_t \omega_k^t
\end{equation}
for a sequence of coefficients $\alpha_t \in (0,1)$ for any $t \in [T]$. The weight definition in Eq.\ref{eq:robbins_monro_weights} is closely related to the Robbins-Monro stochastic approximation method \citep{robbins1951stochastic}. If a client is not participating in the training, its weight is not updated.
\shortname mitigates biases in the estimation of rewards by employing two mechanisms: (1) uniform random sampling method for clients, with a dynamic adjustment process to prioritize clients that are infrequently sampled, thus ensuring equitable participation across time periods; and (2) when a client is not sampled in a round, its weight and contribution to the reward estimate remain unchanged.\vspace{-1em}
\subsection{Modeling Interactions with Gaussian Weighs}\label{section:clustering}
\paragraph{Interaction Matrix.} Gaussian weights are scalar quantities that offer an absolute measure of the alignment between a client's data distribution and the global distribution. Although these weights indicate the conformity of each client's distribution individually, they do not consider the interrelations among the distributions of different clients.
Therefore, we propose to encode these interactions in an \textit{interaction matrix} $P^t \in \mathbb{R}^{K\times K}$ whose element $P_{kj}^t$ estimates the similarity between the $k$-th and the $j$-th client data distribution. The interaction matrix is initialized to the null matrix, \ie $P_{kj}^0 = 0$ for every couple $k,j \in [K]$.

Specifically, we define the update rule for the matrix $P^{t}$ as follows:
\begin{equation}\label{inter_matrix}
    P_{kj}^{t+1} = 
    \begin{cases}
        (1-\alpha_t) P_{kj}^t + \alpha_t \omega_k^t, & (k,j) \in \mathcal{P}_t \times \mathcal{P}_t\\
        P_{kj}^t, & (k,j) \notin \mathcal{P}_t \times \mathcal{P}_t
    \end{cases}
\end{equation}

where $\{\alpha_t\}_t$ is the same sequence used to update the weights, and $\mathcal{P}_t$ is the subset of clients sampled in round $t$.

Intuitively, in the long run, since $\omega_k^t$ measures the proximity of the loss process of client $k$ to the average loss process of clients in $\mathcal{P}_t$ at round $t$, we are estimating the \textit{expected perception} of client $k$ by client $j$ with $P_{kj}^t$, \ie a larger value indicates a higher degree of similarity between the loss profiles, whereas smaller values indicate a lower degree of similarity. For example, if $P_{kj}^t$ is close to $1$, it suggests that on average, whenever $k$ and $j$ have been simultaneously sampled prior to round $t$, $\omega_k^t$ was high, meaning that the two clients are well-represented within the same distribution.

To effectively extract the information embedded in $P$, we introduce the concept of \textit{unbiased perception vectors} (UPV). For any pair of clients $k, j \in [K]$, the UPV $v_k^j \in \mathbb{R}^{K-2}$ represents the $k$-th row of $P$, excluding the $k$-th and $j$-th entries. Recalling the construction of $P^t$, where each row indicates how a client is perceived to share the same distribution as other clients in the federation, the UPV $v_k^j$ captures the collective perception of client $k$ by all other clients, excluding both itself and client $j$. This exclusion is why we refer to $v_k^j$ as \textit{unbiased}. 

The UPVs encode information about the relationships between clients, which can be exploited for clustering. However, the UPVs cannot be directly used as their entries are only aligned when considered in pairs. Instead, we construct the \textit{affinity matrix} $W$ by transforming the information encoded by the UPVs through an RBF kernel, as this choice allows to effectively model the affinity between clients: two clients are considered \textit{affine} if similarly perceived by others. This relation is encoded by the entries of $W \in \mathbb{R}^{K \times K}$, which we define as:
\begin{equation}
    W_{kj} = \mathcal{K}(v_k^j, v_j^k) = \exp\left(-\beta\norm{v_k^j-v_j^k}^2\right).
\end{equation}
The spread of the RBF kernel is controlled by a single hyper-parameter $\beta>0$: changes in this value provide different clustering outcomes, as shown in the sensitivity analysis in Appendix \ref{app:sensitive}.
\vspace{-1em}
\paragraph{Clustering.}
The affinity matrix $W$, designed to be symmetric, highlights features that capture similarities between clients' distributions. Clustering is performed by the server using the rows of $W$ as feature vectors, as they contain the relevant information. We apply the spectral clustering algorithm \citep{ng2001spectral} to $W$ due to its effectiveness in detecting non-convex relationships embedded within the client affinities. Symmetrizing the interaction matrix $P$ into the affinity matrix $W$ is fundamental for spectral clustering as it refines inter-client relationship representation. It models interactions, reducing biases, and emphasizing reliable similarities. This improves robustness to noise, allowing spectral clustering to effectively detect the distributional structure underlying the clients' network \citep{von2007tutorial}.
During the iterative training process, the server determines whether to perform clustering by checking the convergence of the matrix $P^t$. Convergence is numerically verified when the mean squared error (MSE) between consecutive updates is less than a small threshold $\epsilon > 0$. Algorithm \ref{alg:fedgwcluster} summarizes the clustering procedure.
If the MSE is below $\epsilon$, the server computes the matrix $W^t$ and performs spectral clustering over $W^t$ with a number of clusters $n \in \{2, \dots, n_{max}\}$. For each clustering outcome, the Davies-Bouldin (DB) score \citep{davies1979cluster} is computed: DB larger than one means that clusters are not well separated, while if it is smaller than one, the clusters are well separated, a detailed description of the clustering metrics is provided in Appendix \ref{app:clustering}. We denote by $n_{cl}$ the optimal number of clusters detected by \shortname. If $\min_{n = 2,\dots, n_{max}} DB_n > 1$, we do not split the current cluster. Hence, the optimal number of clusters is $n_{cl}$ is one. In the other case, the optimal number of clusters is  $n_{cl} \in \arg \min_{n = 2, \dots, n_{max}} DB_{n}$. This requirement ensures proper control over the over-splitting phenomenon, a common issue in hierarchical clustering algorithms in FL which can undermine key principles of FL by creating degenerate clusters with very few clients. %
Finally, on each cluster $\mathcal{C}^{(1)},\dots, \mathcal{C}^{(n_{cl})}$ an FL aggregation algorithm is trained separately, resulting in models $\theta_{(1)}, \dots, \theta_{(n_{cl})}$ personalized for each cluster.

\subsection{Recursive Implementation}\label{sec:alg_fedgw} 
In the previous sections, we have detailed the splitting algorithm within the individual clusters.  In this section, we present the full \shortname procedure (Algorithm \ref{alg:fedgw_recursion}), introducing the complete notation with indices for the distinct clusters. We denote the clustering index as $n$, and the total number of clusters $N_{cl}$.

The interaction matrix $P^0_{(1)}$ is initialized to the null matrix $0_{K \times K}$, and the \textit{total number of clusters} $N_{cl}^0$, as no clusters have been formed yet, and $\textrm{MSE}_{(1)}^0$ are initialized to 1, in order to ensure stability in early updates, allowing a gradual decrease.  At each communication round $t$, and for cluster $\mathcal{C}^{(n)}$, where $n = 1, \dots, N_{cl}^t$, the cluster server independently samples the participating clients $\mathcal{P}_t^{(n)} \subseteq \mathcal{C}^{(n)}$. Each client $k \in \mathcal{P}_t^{(n)}$ receives the current cluster model $\theta_{(n)}^t$. After performing local updates, each client sends its updated model $\theta_k^{t+1}$ and empirical loss $l_k^t$ back to the cluster server. The server aggregates these updates to form the new cluster model $\theta_{(n)}^{t+1}$, computes the Gaussian rewards $\omega_k^t$ for the sampled clients, and updates the interaction matrix $P_{(n)}^{t+1}$ and $\textrm{MSE}_{(n)}^{t+1}$ according to Eq. \ref{inter_matrix}. If $\textrm{MSE}_{(n)}^{t+1}$ is lower than a threshold $\epsilon$, the server of the cluster performs clustering to determine whether to split cluster $\mathcal{C}^{(n)}$ into $n_{cl}$ sub-groups, as outlined in Algorithm \ref{alg:fedgwcluster}. The matrix $P_{(n)}^{t+1}$ is then partitioned into sub-matrices by filtering its columns and rows according to the newly formed clusters, with the MSE for these sub-matrices reinitialized to 1. This process results in a distinct model $\theta_{(n)}$ for each cluster $\mathcal{C}^{(n)}$. When the final iteration $T$ is reached we are left with $N_{cl}^T$ clusters with personalized models $\theta_{(n)}$ for $n = 1,\dots, N_{cl}^T$.

Thanks to the Gaussian Weights, and the iterative spectral clustering on the affinity matrices, our algorithm, \shortname, is able to autonomously detect groups of clients that display similar levels of heterogeneity.
The clusters formed are more uniform, \ie the class distributions within each group are more similar. These results are supported by experimental evaluations, discussed in Section \ref{sect:ablation}.
