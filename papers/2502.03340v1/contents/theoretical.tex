\section{Theoretical Results and Derivation of \shortname}\label{sec:theory}
In this section, we provide a formal derivation of the algorithm, discussing the mathematical properties of Gaussian Weights and outlining the structured formalism and rationale underlying \shortname.

The stochastic process induced by the optimization algorithm in the local update step, allows the evolution of the empirical loss to be modeled using random variables within a probabilistic framework. We denote random variables with capital letters (\eg , $X$), and their realizations with lowercase letters (\eg, $x$).

The observed loss process $l_k^{t,s}$ is the outcome of a stochastic process $L_k^{t,s}$, and the rewards $r_k^{t,s}$, computed according to Eq. \ref{eq:reward}, are samples from a random reward $R_k^{t,s}$ supported in $(0,1)$, whose expectation $\mathbb{E}[R_k^{t,s}]$ is lower for out-of-distribution clients and higher for in-distribution ones. To estimate the expected reward $\mathbb{E}[R_k^{t,s}]$ we introduce the r.v. $\Omega_k^t = 1/S \sum_{s \in [S]} R_k^{t,s}$, which is an estimator less affected by noisy fluctuations in the empirical loss. Due to the linearity of the expectation operator, the expected reward $\mathbb{E}[R_k^{t,s}]$ for the $k$-th client at round $t$, local iteration $s$ equals the expected Gaussian reward $ \mathbb{E}[\Omega_k^t]$ that, to simplify the notation, we denote by $\mu_k$. $\mu_k$ is the theoretical value that we aim to estimate by designing our Gaussian weights $\Gamma_k^t$ appropriately, as it encodes the ideal reward to quantify the closeness of the distribution of each client $k$ to the main distribution. Note that the process is stationary by construction. Therefore, it does not depend on $t$ but differs between clients, as it reaches a higher value for in-distribution clients and a lower for out-of-distribution clients.

To rigorously motivate the construction of our algorithm and the reliability of the weights, we introduce the following theoretical results. Theorems \ref{thm_main:1} and \ref{thm_main:weak_conv} demonstrate that the weights converge to a finite value and, more importantly, that this limit serves as an unbiased estimator of the theoretical reward $\mu_k$. The first theorem provides a strong convergence result, showing that, with suitable choices of the sequence $\{\alpha_t\}_t$, the expectation of the Gaussian weights $\Gamma_k^t$ converges to $\mu_k$ in $L^2$ and almost surely. In addition, Theorem \ref{thm_main:weak_conv} extends this to the case of constant $\alpha_t$, proving that the weights still converge and remain unbiased estimators of the rewards as $t \to \infty$. 
\begin{theorem}\label{thm_main:1}
Let $\{\alpha_t\}_{t = 1}^\infty$ be a sequence of positive real values, and $\{\Gamma_k^t\}_{t=1}^\infty$ the sequence of Gaussian weights. If $\{\alpha_t\}_{t = 1}^\infty \in l^2(\mathbb{N})/l^1(\mathbb{N})$, then $\Gamma_k^t$ converges in $L^2$. Furthermore, for $t\to\infty$, 
\begin{equation}
    \Gamma_k^t \longrightarrow \mu_k\,\,\, a.s.
\end{equation}
\end{theorem}
\begin{theorem}\label{thm_main:weak_conv}
Let $\alpha \in (0,1)$ be a fixed constant, then in the limit $t \to \infty$, the expectation of the weights converges to the individual theoretical reward $\mu_k$, for each client $k = 1,\dots, K$, \ie,
\begin{equation}
    \mathbb{E}[\Gamma_k^t]\longrightarrow \mu_k\,\,\,t\to\infty\,.
\end{equation}
\end{theorem}
Proposition \ref{prop_var_main} shows that Gaussian weights reduce the variance of the estimate, thus decreasing the error and enabling the construction of a confidence interval for $\mu_k$.
\begin{proposition}\label{prop_var_main}
The variance of the weights $\Gamma_k^t$ is smaller than the variance $\sigma_k^2$ of the theoretical rewards $R_k^{t,s}$.
\end{proposition} 

Complete proofs of Theorems \ref{thm_main:1}, \ref{thm_main:weak_conv}, and Proposition \ref{prop_var_main} are provided in Appendix \ref{app:fgw}. Additionally, Appendix \ref{app:fgw} includes further analysis of \shortname. Specifically, Proposition \ref{prop:bounded_matrix} demonstrates that the entries of the interaction matrix $P$ are bounded, while Theorem \ref{thm:samplerate} establishes a sufficient condition for conserving the sampling rate during the recursive procedure.


\section{Wasserstein Adjusted Score}\label{clustereing_metric}

In the previous Section we observed that when clustering clients according to different heterogeneity levels, the outcome must be evaluated using a metric that assesses the cohesion of individual distributions. In this Section, we introduce a novel metric to evaluate the performance of clustering algorithms in FL. This metric, derived from the Wasserstein distance \citep{kantorovich1942translocation}, quantifies the cohesion of client groups based on their class distribution similarities. 

We propose a general method for adapting clustering metrics to account for class imbalances. This adjustment is particularly relevant when the underlying class distributions across clients are skewed. The formal derivation and mathematical details of the proposed metric are provided in Appendix \ref{app:clustering}. We now provide a high-level overview of our new metric.

Consider a generic clustering metric $s$, e.g. Davies-Bouldin score \citep{davies1979cluster} or the Silhouette score \citep{rousseeuw1987silhouettes}. Let $C$ denote the total number of classes, and $x_i^k$ the empirical frequency of the $i$-th class in the $k$-th client's local training set. Following theoretical reasonings, as shown in Appendix \ref{app:clustering}, the empirical frequency vector for client $k$, denoted by $x_{(i)}^k$, is ordered according to the rank statistic of the class frequencies, \ie  $x_{(i)}^k \geq x_{(i+1)}^k$ for any $i = 1, \dots, C-1$.
The class-adjusted clustering metric $\tilde{s}$ is defined as the standard clustering metric $s$ computed on the ranked frequency vectors $x_{(i)}^k$.  Specifically, the distance between two clients $j$ and $k$ results in
\vspace{-.8em}
\begin{equation}\label{lab_dist_class}
    \dfrac{1}{C}\left(\sum_{i = 1}^C \left | x_{(i)}^k - x_{(i)}^j \right | ^2\right)^{1/2}\,.
\end{equation}
This modification ensures that the clustering evaluation is sensitive to the distributional characteristics of the class imbalance. As we show in Appendix \ref{app:clustering}, this adjustment is mathematically equivalent to assessing the dispersion between the empirical class probability distributions of different clients using the Wasserstein distance, also known as the Kantorovich-Rubenstein metric \citep{kantorovich1942translocation}. This equivalence highlights the theoretical soundness of using ranked class frequencies to better capture variations in class distributions when evaluating clustering outcomes in FL.
