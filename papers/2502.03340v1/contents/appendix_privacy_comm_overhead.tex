\section{Privacy of \shortname}\label{app:privacy}
In the framework of \shortname, clients are required to send only the empirical loss vectors $l_k^{t,s}$ to the server \citep{cho2022towards}. While concerns might arise regarding the potential leakage of sensitive information from sharing this data, it is important to clarify that the server only needs to access aggregated statistics, working on aggregated data. This ensures that client-specific information remains private. Privacy can be effectively preserved by implementing the Secure Aggregation protocol \citep{bonawitz2016practical}, which guarantees that only the aggregated results are shared, preventing the exposure of any raw client data.

\section{Communication and Computational Overhead of \shortname}
\label{app:communication-computation}
\shortname\ minimizes communication and computational overhead, aligning with the requirements of scalable FL systems \citep{mcmahan2016federated}. On the client side, the computational cost remains unchanged compared to the chosen FL aggregation, e.g. FedA, as clients are only required to communicate their local models and a vector of empirical losses after each round. The size of this loss vector, denoted by \( S \), corresponds to the number of local iterations (\ie the product of local epochs and the number of batches) and is negligible w.r.t. the size of the model parameter space, \( |\Theta| \). In our experimental setup, \( S = 8 \), ensuring that the additional communication overhead from transmitting loss values is negligible in comparison to the transmission of model weights. 


All clustering computations, including those based on interaction matrices and Gaussian weighting, are performed exclusively on the server. This design ensures that client devices are not burdened with additional computational complexity or memory demands. The interaction matrix $P$ used in \shortname\ is updated incrementally and involves sparse matrix operations, which significantly reduce both memory usage and computational costs.

These characteristics make \shortname\ particularly well-suited for cross-device scenarios involving large federations and numerous communication rounds.
Moreover, by operating on scalar loss values rather than high-dimensional model parameters, the clustering process in \shortname\ achieves computational efficiency while maintaining effective grouping of clients. The server-side processing ensures that the method remains scalable, even as the number of clients and communication rounds increases. Consequently, \shortname\ meets the fundamental objectives of FL by minimizing costs while preserving privacy and maintaining high performance.

