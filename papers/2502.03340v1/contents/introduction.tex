\section{Introduction}

Federated Learning (FL) \citep{mcmahan2017communication} has emerged as a promising paradigm for training models on decentralized data while preserving privacy. Unlike traditional machine learning frameworks, FL enables collaborative training between multiple clients without requiring data transfer, making it particularly attractive in privacy-sensitive domains \citep{bonawitz2019federated}. FL was introduced primarily to address two major challenges in decentralized scenarios: ensuring privacy \citep{kairouz2021advances} and reducing communication overhead \citep{hamer2020fedboost, asad2020fedopt}. 
In particular, FL algorithms must guarantee \textit{communication efficiency}, to reduce the burden associated with the exchange of model updates between clients and the central server, while maintaining \textit{privacy}, as clients should not expose their private data during the training process.

A core challenge in federated learning is data heterogeneity \citep{li2020federated}. This manifests in two key ways: through imbalances in data quantity and class distribution both within and across clients, and through the non-independent and non-identical distribution (non-IID) of data across the federation. In this scenario, a single global model often fails to generalize due to clients contributing with updates from skewed distributions, leading to degraded performance \citep{zhao2018federated, caldarola2022improving} compared to the centralized counterpart. Furthermore, noisy or corrupted data from some clients can further complicate the learning process \citep{cao2020fltrust,zhang2022fldetector}, while non-IID data often results in unstable convergence and conflicting gradient updates \citep{hsieh2020non, zhao2018federated}. Despite the introduction of various techniques to mitigate these issues, such as regularization methods \citep{li2020federated}, momentum \citep{mendieta2022local}, and control variates \citep{karimireddy2020scaffold}, data heterogeneity remains a critical unsolved problem.

In this work, we address the fundamental challenges of data heterogeneity and class imbalance in federated learning through a novel clustering-based approach. We propose \shortname (Federated Gaussian Weighting Clustering), a method that groups clients with similar data distributions into clusters, enabling the training of personalized federated models for each group. Our key insight is that clients' data distributions can be inferred by analyzing their empirical loss functions, rather than relying on model updates as most existing approaches do. Inspired by \citep{cho2022towards}, we hypothesize that clients with similar data distributions will exhibit similar loss landscapes.
\shortname implements this insight using a Gaussian reward mechanism to form homogeneous clusters based on an \textit{interaction matrix}, which encodes the pairwise similarity of clients' data distributions. This clustering is achieved efficiently by having clients communicate only their empirical losses to the server at each communication round. Our method transforms these loss values to estimate the similarity between each client's data distribution and the global distribution, using Gaussian weights as statistical estimators.
Each cluster then trains its own specialized federated model, leveraging the shared data characteristics within that group. This approach preserves the knowledge-sharing benefits of federated learning while reducing the negative effects of statistical heterogeneity, such as client drift \citep{karimireddy2020scaffold}. We develop a comprehensive mathematical framework for this approach and rigorously prove the convergence properties of the Gaussian weights estimators.

To evaluate our method, we introduce the \textit{Wasserstein Adjusted Score}, a new clustering metric tailored for assessing cluster cohesion in class-imbalanced FL scenarios. Through extensive experiments on both benchmark \citep{caldas2018leaf} and large-scale datasets \citep{hsu2020federated}, we demonstrate that \shortname outperforms existing clustered FL algorithms in terms of both accuracy and clustering quality. Furthermore, \shortname can be integrated with any robust FL aggregation algorithm to provide additional resilience against data heterogeneity. \\
\textbf{Contributions.}
\begin{itemize}[leftmargin=*]
\setlength{\itemsep}{-.5em} 
    \item We propose \shortname, an efficient federated learning framework that clusters clients based on their data distributions, enabling personalized models that better handle heterogeneity.
    \item We provide a rigorous mathematical framework to motivate the algorithm, proving its convergence properties and providing theoretical guarantees for our clustering approach.
    \item We introduce a novel clustering metric specifically designed to evaluate cluster quality in the presence of class imbalance. 
    \item We demonstrate through extensive experiments that 1) \shortname outperforms existing clustered FL approaches in terms of both clustering quality and model performance, 2) our method successfully handles both class and domain imbalance scenarios, and 3) the framework can be effectively integrated with any FL aggregation algorithm.
\end{itemize}
