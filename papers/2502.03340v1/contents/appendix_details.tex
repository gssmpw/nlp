\section{Datasets and implementation details} \label{app_details}

To simulate a realistic FL environment with heterogeneous data distribution, we conduct experiments on Cifar100 \citep{krizhevsky2009learning}. As a comparison, we also run experiments on the simpler Cifar10 dataset \cite{krizhevsky2009learning}. Cifar10 and Cifar100 are distributed among $K$ clients using a Dirichlet distribution (by default, we use $\alpha = 0.05$ for Cifar10 and $\alpha = 0.5$ for Cifar100) to create highly imbalanced and heterogeneous settings. By default, we use $K = 100$ clients with 500 training and 100 test images. The classification model is a CNN with two convolutional blocks and three dense layers. Additionally, we perform experiments on the Femnist dataset \cite{lecun1998mnist}, partitioned among $400$ clients using a Dirichlet distribution with $\alpha = 0.01$. In these experiments, we employ LeNet\-5 as the classification model \cite{lecun1998gradient}. Local training on each client uses SGD with a learning rate of $ 0.01$, weight decay of $4 \cdot 10^{-4}$, and batch size 64. The number of local epochs is 1, resulting in 7 batch iterations for Cifar10 and Cifar100 and 8 batch iterations for Femnist. The number of communication rounds is set to 3,000 for Femnist, 10,000 for Cifar10 and 20,000 for Cifar100, with a 10\% client participation rate per cluster. For \shortname we tuned the hyper-parameter $\beta \in \{0.1, 0.5, 1, 2, 4\}$, \ie the spread of the RBF kernel, and we set the tolerance $\epsilon$ to $10^{-5}$, constant value $\alpha_t = \alpha$ equal to the participation rate, \ie 10\%. \texttt{FeSEM}'s and \texttt{IFCA}'s number of clusters was tuned between 2,3,4, and 5.  Each client has its own local training and test sets. We evaluate classification performance using balanced accuracy, computed per client as the average class-wise recall. The overall federated balanced accuracy is then obtained by averaging client-wise balanced accuracy, optionally weighted by test set sizes, to account for heterogeneous data distributions.

Large Scale experiments are conducted on Google Landmarks \citep{weyand2020google} with $K = 823$ clients and \citep{van2018inaturalist} with $K = 2714$ clients, as partitioned in \citep{hsu2020federated}. For Landmarks and iNaturalist, we always refer to the Landmark-Users-160K and iNaturalist-Users-120K partition, respectively. The classification model is MobileNetV2 architecture \citep{sandler2018mobilenetv2} with pre-trained weights on ImageNet-1K dataset \citep{deng2009imagenet} optimized with SGD having learning rate of $0.1$. To mimic real world low client availability we employed 10 sampled clients per communication round, with a total training of 1000 and 2000 communication rounds, with 7 and 5 batch iterations respectively. For \shortname we tuned the hyper-parameter $\beta \in \{0.1, 0.5, 1, 2, 4\}$, \ie the spread of the RBF kernel, and we set the tolerance $\epsilon$ to $10^{-2}$ for iNaturalist and to $10^{-4}$ for Landmark, constant value $\alpha_t = \alpha = .1$. \texttt{IFCA}'s number of clusters was tuned between 2,3,4, and 5. Each client has its own local training and test sets. Performance in large scale scenarios are evaluated by averaging the accuracy achieved on the local test sets across the federation.
