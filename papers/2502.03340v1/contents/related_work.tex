\section{Related Work}

\paragraph{FL with Heterogeneous Data.} Handling data heterogeneity, especially class imbalance, remains a critical challenge in FL. \texttt{FedProx}\citep{li2020federated} was one of the first attempts to address heterogeneity by introducing a proximal term that constrains local model updates close to the global model. \texttt{FedMD} \citep{li2019fedmd} focuses on heterogeneity in model architectures, allowing collaborative training between clients with different neural network structures using model distillation. Methods such as \texttt{SCAFFOLD} \citep{karimireddy2020scaffold} and \texttt{Mime} \citep{karimireddy2020mime} have also been proposed to reduce client drift by using control variates during the optimization process, which helps mitigate the effects of non-IID data. Furthermore, strategies such as biased client selection \citep{cho2022towards} based on ranking local losses of clients and normalization of updates in \texttt{FedNova} \citep{wang2021novel} have been developed to specifically address class imbalance in federated networks, leading to more equitable global model performance.\vspace{-1.5em}
\paragraph{Clustered FL.} Clustering has proven to be an effective strategy in FL for handling client heterogeneity and improving personalization \citep{huang2022collaboration, duan2021fedgroup, briggs2020federated, Caldarola_2021_CVPR, ye2023personalized}. Clustered FL \citep{sattler2020clustered} is one of the first methods proposed to group clients with similar data distributions to train specialized models rather than relying on a single global one. Nevertheless, from a practical perspective, this method exhibits pronounced sensitivity to hyper-parameter tuning, especially concerning the gradient norms threshold, which is intricately linked to the dataset. This sensitivity can result in significant issues of either excessive under-splitting or over-splitting. Additionally, as client sampling is independent of the clustering, there may be privacy concerns due to the potential for updating cluster models with the gradient of a single client. An extension of this is the efficient framework for clustered FL proposed by \citep{ghosh2020efficient}, which strikes a balance between model accuracy and communication efficiency. Multi-Center FL \citep{long2023multi} builds on this concept by dynamically adjusting client clusters to achieve better personalization, however a-priori knowledge on the number of clusters is needed. Similarly, \texttt{IFCA} \citep{ghosh2020efficient} addresses client heterogeneity by predefining a fixed number of clusters and alternately estimating the cluster identities of the users by optimizing model parameters for the user clusters via gradient descent. However, it imposes a significant computational burden, as the server communicates all cluster models to each client, which must evaluate every model locally to select the best fit based on loss minimization. This approach not only increases communication overhead but also introduces inefficiencies, as each client must test all models, making it less scalable in larger networks. 

Compared to previous approaches, the key advantage of the proposed algorithm, \shortname, lies in its ability to effectively identify clusters of clients with similar levels of heterogeneity and class distribution through simple transformations of the individual empirical loss process. This is achieved without imposing significant communication overhead or requiring additional computational resources. Further details on the computational and communication overhead are provided in Appendix \ref{app:communication-computation}. Additionally, \shortname can be seamlessly integrated with any aggregation method, enhancing its robustness and performance when dealing with heterogeneous scenarios. 

