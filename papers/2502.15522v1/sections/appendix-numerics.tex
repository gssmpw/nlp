\paragraph{Data generation for the union of subspaces experiments.}
The union-of-subspaces model stipulates that each vector in the input
data belongs to one of $k$ subspaces. Formally, there exists a
collection $\mathcal{R} := \set{R_1, \dots, R_k}$, where $R_{i} \in O(\dout, s)$,
such that $x^{i} \in \bigcup_{j = 1}^k \range(R_{j})$ for all $i$.
In our experiments, we generate training samples from the union-of-subspaces model in the following manner:
\begin{itemize}
    \item Sample $Z \in \mathbb{R}^{\dout \times n}$ according to
    the procedure described in~\cref{sec:numerics}.
    \item For each $i = 1, \dots, n$:
    \begin{enumerate}
        \item Sample $R \sim \mathrm{Unif}(\mathcal{R})$.
        \item Set $X_{:, i} = R Z_{:, i}$.
    \end{enumerate}
\end{itemize}

\paragraph{Neural network architecture for the union-of-subspaces model.}
The inverse mapping for linear inverse problems with data from a union-of-subspaces model is in general nonlinear for $k > 1$ --- as
a result, deep linear networks are not a suitable choice for learning
the inverse mapping.
Nevertheless, it is known that the inverse mapping is approximated to arbitrary accuracy
by a piecewise-linear mapping~(see~\cite{GOW20}), which can be realized as a multi-index model of the form $g(V^{\T} x)$ for
suitable $V$ and vector-valued mapping $g$. Guided by this, we use a
neural network architecture defined as follows:
\begin{equation}
    f_{W_1, \dots, W_L}(x) =
    W_L \left( W_{L-1} W_{L-2} \cdots W_{1} x\right)_+,
    \label{eq:mixed-linear-relu-network}
\end{equation}
where $W_1, \dots, W_{L}$ are learnable weight matrices and
$[\cdot]_+$ denotes the (elementwise) positive part, equivalent to using a ReLU activation at the $(L-1)^{\text{th}}$ hidden layer.
Indeed, recent results~\cite{parkinson2023linear} suggest that neural
networks of the form~\eqref{eq:mixed-linear-relu-network} are biased
towards multi-index models such as the one sought to approximate the
inverse mapping. Finally, all the networks from~\cref{fig:gauss_noise_wd} were trained for $100000$ iterations
with learning rate $\eta = 10^{-3}$.


