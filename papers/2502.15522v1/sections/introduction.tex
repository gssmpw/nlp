Machine learning approaches, especially those based on deep neural networks, have risen to prominence for solving a broad class of inverse problems. In particular, deep learning approaches constitute the state of the art for various inverse problems arising in medical imaging (e.g. MRI or CT) \cite{gonzalez2002digital,lustig2008compressed, mccann2017convolutional,sriram2020end}, image denoising \cite{gonzalez2002digital, elad2023image}, and image inpainting \cite{bertalmio2000image, quan2024deep}. 
Despite its impressive performance for inverse problems, almost all the theoretical underpinnings of deep learning focus on regression or classification problems; see \cite{scarlett2022theoretical} for a summary of the theoretical results for deep neural networks for inverse problems.
On the other hand, there is a strong need for theory: understanding the behavior of deep neural networks is crucial when they are deployed in critical applications such as medical imaging.

A challenge is that neural networks are typically trained on a subset of all potential data points -- pairs of ``realistic'' signals and their measurements -- the distribution of which is not known a priori.
Nevertheless, one aims for robustness: perturbed measurements should yield approximate reconstructions, even if the perturbation no longer corresponds to a realistic signal passed through the forward model.
As suggested by a number of works \cite{antun2020instabilities, darestani2021measuring, genzel2022solving,krainovic2024learning}, the robustness of machine learning approaches is by no means automatic and requires special attention.
Even for the most fundamental model of a set of signals lying in a subspace, this effect is observed in numerical simulations. For example, \cref{fig:linear} shows that a linear network trained via vanilla gradient descent (i.e., with zero regularization, $\lambda = 0$) on synthetic data starting from a random initialization converges to a solver that is not very robust to perturbations (here, Gaussian noise). In \cref{fig:nonlinear} we can see the same effect for a non-linear ReLU network trained on data from a union of subspaces model; see \cref{sec:appendix numerical description}.

In this paper, we discuss ways out of this fundamental bottleneck focusing, as a proof of concept, on the aforementioned model of a high-dimensional signal lying in an (unknown) low-dimensional subspace.
Indeed, \cref{fig:gauss_noise_wd} shows that robustness considerably improves in the presence of $\ell_2$-regularization (also known as \emph{weight decay}),
a standard strategy in machine learning designed to promote simple parameter configurations~\cite{krogh1991simple,bos1996using}.
For the purposes of analysis, we address the case where the training data corresponds to solved linear inverse problems:
	\begin{align}
		X     & = \bmx{x^{1}                                                          & \dots & x^{n}} \in \Rbb^{d \times n},  \tag{signals}\\
		Y     & = \bmx{y^{1}                                                          & \dots & y^{n}} \in \Rbb^{m \times n}, \tag{measurements}\\
		\text{where} \quad
		y^{i} & = A x^{i}, \;\; x^{i} \in \mathrm{range}(R) \quad \text{for all $i$.}
		\label{eq:dataset-measurements}
	\end{align}
Here, $A \in \Rbb^{\din \times \dout}$ is a fixed measurement operator with $\din < \dout$ and $R \in \Rbb^{\dout \times s}$
is a (unknown) matrix with orthogonal columns that span a low-dimensional subspace (i.e., $s \ll \dout$).
One aims to solve the regularized minimization problem
\begin{align}\label{eq:l2regprob1}
	\min_{W_1, \dots, W_{L}} \frobnorm{f_{W_{L:1}}(Y)-X}^2 + \lambda \sum_{\ell =1}^L \frobnorm{W_{\ell}}^2
  = \min_{W_1, \dots, W_{L}} \sum_{i=1}^n \|f_{W_{L:1}}(y^i) - x^i\|^2 + \lambda \sum_{\ell =1}^L \frobnorm{W_{\ell}}^2.
\end{align}
Here, $f_{W_{L:1}}$ is a depth-$L$ neural network with weight matrices $W_1, \dots, W_L$ for some $L \in \mathbb{N}$. 
It is not hard to show that, for small $\lambda$, the global minimizer of this non-convex problem yields a robust solution -- namely, it
has the following two properties:
\begin{enumerate}
\item It is accurate (with error vanishing in the limit as $\lambda \rightarrow 0$) on the image of the signal subspace -- that is, it accurately reconstructs signals from their measurements.
	\item It is zero on the orthogonal complement of the image of the signal subspace -- that is, perturbations orthogonal to the model are eliminated (see \cref{lem:robustsolutionofoptproblem}).
\end{enumerate}
\begin{figure*}[h]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\linewidth]{figures/gaussian_noise_subspace_linear_s_5_m_100_d_200_dw_400_L_4_plot.pdf}
		\caption{Linear network}
		\label{fig:linear}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{figures/gaussian_noise_uos_non_linear_s_5_3_m_100_d_200_dw_400_L_4_plot.pdf}
		\caption{ReLU network with extra linear layers}
		\label{fig:nonlinear}
	\end{subfigure}
	\caption{
    Comparison of robustness against Gaussian noise for training with and without weight decay. All experiments use signals
    of dimension $\dout = 200$ and measurements of dimension $\din = 100$; all networks have $L = 5$ layers and hidden layer width
    $\dhid = 400$.
    In \cref{fig:linear}, 
    the model is a linear neural network trained on data 
    lying in a subspace of dimension $s=5$.
    In \cref{fig:nonlinear}, the model is a ReLU neural network with extra linear layers, similar to the setup of \citet{parkinson2023linear}, trained on data that lie in the union of three subspaces, each of dimension $s=5$. A detailed description of the numerical experiment can be found in \cref{sec:numerics,sec:appendix numerical description}.}
	\label{fig:gauss_noise_wd}
\end{figure*}

The remaining issue is that, due to the non-convexity of the problem~\eqref{eq:l2regprob1}, no algorithms with global convergence guarantees are available to date (to the best of our knowledge).
As a proxy, practitioners typically apply gradient descent or stochastic gradient descent to the regularized objective. However,
whether this approach produces a good approximation to the desired global minimizer (or \emph{any} point that shares the aforementioned properties)
remains unclear.

In this paper, we provide an answer for fully connected deep linear neural networks $f_{W_{L:1}}(Y) = W_L \cdots W_1 Y$ trained by gradient descent on the regularized objective \eqref{eq:l2regprob1}.
Our contributions can be summarized as follows (see~\cref{theorem:main-informal}).
\begin{enumerate}
	\item We show that gradient descent converges to an approximate solution that reconstructs signals from their measurements  with error vanishing in the limit as $\lambda \rightarrow 0$.
	\item We show that the part of the weights acting on the orthogonal complement of the image of the signal subspace is small after a finite number of iterations.
	\item We show that optimizing the regularized objective~\eqref{eq:l2regprob1} leads to a more robust solution than in the non-regularized case (see~\cref{sec:subsec:robustness}).
\end{enumerate}
