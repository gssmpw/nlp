\paragraph{Benefits of weight decay for generalization.}
It is believed that for understanding generalization properties of neural networks, “the size of the weights is more important than the size of the network”~\cite{bartlett1996valid}.
This idea has been studied in several works \cite{neyshabur2014search,neyshabur2015norm,wei2019regularization,daniely2019generalization,golowich2020size,parkinson2024depth},
and is especially notable in light of modern machine learning that operates in highly overparameterized regimes \cite{zhang2021understanding}.
Regularizing the $\ell_2$-norm of the parameters (i.e., weight decay) to encourage small-norm weight matrices is common practice in neural network training and has been empirically observed to improve generalization \cite{krogh1991simple,bos1996using,zhangthree,d2023we}.

Multiple works have addressed the properties of global minimizers of the $\ell_2$-regularized loss and of minimal-norm interpolants of the data \cite{savarese2019infinite,ongie2019function,ma2022barron,parhi2022near,boursier2023penalising}. Several works have found that such networks adapt to low-dimensional structure \cite{bach2017breaking,parkinson2023linear,Jacot23,kobayashi2024weight}.
In particular, minimal-norm linear deep neural networks are known to induce low-rank mappings \cite{shang2020unified,dai2021}.

\vspace*{-1em}
\paragraph{Convergence of gradient descent for deep linear networks.}
Several works study the dynamics of gradient descent for training deep linear neural networks in general regression tasks under different assumptions.
For example,~\citet{du2019width} show that gradient descent starting from a random Gaussian initialization will converge at a linear rate to a global minimizer of the \emph{unregularized} loss ($\lambda = 0$) as long as the hidden layer width
scales linearly in the input dimension and depth; a closely related work by~\citet{HXP20} demonstrates that the hidden layer width no longer needs to scale
with the network depth when weights are initialized according to an orthogonal
scheme.
Similarly,~\citet{arora2019convergence} study convergence of gradient descent when (i) weight matrices
at initialization are approximately balanced and (ii) the problem instance satisfies
a ``deficiency margin'' property ruling out certain rank-deficient solutions -- a condition later removed
by the analysis of~\citet{nguegnang2024convergence}.
On the other hand, \citet{xu2023linear} show that gradient descent converges to a global
minimum for linear neural networks with two layers and mild overparameterization \emph{without}
any assumptions on the initialization; however, their proof does not readily extend to
neural networks of arbitrary depth $L$.
\citet{shamir2019exponential} studies gradient descent on deep linear networks when the dimension and hidden width are both equal to one.
Other results include  \citet{kawaguchi2016deep} and ~\citet{LvB18}, who
show that under certain assumptions, all local minima are global.
Finally, a number of works focus on gradient flow~\citep{gidel2019implicit,JT19,eftekhari2020training,BRTW21,pesme2021implicit,JGS+21}, the continuous-time analog of gradient descent.

All the works mentioned so far study gradient descent or gradient flow
without any explicit regularization.
In contrast,~\citet{ACH18} study the $\ell_2$-regularized objective for deep linear networks but do not focus on the effects of the regularization in the analysis. Instead, they show that depth has a preconditioning effect that accelerates convergence. However, their analysis for the discrete-time setting relies on
near-zero initialization and small stepsizes.
\citet{lewkowycz2020training} study the regularization effect of
weight decay for \emph{infinitely wide} neural networks with positively homogeneous activations, finding that model performance peaks at approximately
$\lambda^{-1}$ iterations -- a finding also supported by our analysis (cf.~\cref{theorem:main-informal}). However, their theoretical analysis only covers
gradient flow updates. The works~\cite{YWH+23,yaras2024compressible}, inspired
by the LoRA technique \cite{hu2021lora}, show that gradient descent updates of deep linear networks traverse a ``small'' subspace when
the input data lies on a low-dimensional structure. Unfortunately, their proofs
(i) rely on an ``orthogonal initialization'' scheme and (ii) do not provide any
guarantees on the accuracy of the solution learned by gradient descent.
Finally,~\citet{wang2024implicit} study the implicit bias of (stochastic) gradient
descent for deep linear networks. They show that SGD with sufficiently small
weight decay initially converges to a solution that overestimates the rank of
the true solution mapping, but SGD will find a low-rank solution
with positive probability given a sufficiently large number of epochs (proportional to $O(\eta^{-1} \lambda^{-1})$). However, their work does not rule out the possibility that
the low-rank solution found by SGD is
a poor fit to the data.



