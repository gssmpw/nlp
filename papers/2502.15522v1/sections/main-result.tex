In this section, we present our main result as well as a proof sketch focusing
on the depth $L = 2$ case. Recall that we are interested in solving~\eqref{eq:l2regprob1},
for the special case where $f_{W_{L:1}}$ is a deep linear network, using gradient
descent (\cref{alg:gradient-descent}). Concretely,
we want to minimize the following loss function:
\begin{equation}
	\cL(\set{W_{\ell}}_{\ell = 1, \dots, L}; (X, Y)) :=
	\frac{1}{2} \frobnorm{W_{L} \cdots W_{1} Y - X}^2 + \frac{\lambda}{2} \sum_{\ell=1}^{L} \frobnorm{W_{\ell}}^2.
	\label{eq:loss-function}
\end{equation}
\begin{algorithm}[tb]
	\caption{Gradient descent}
	\label{alg:gradient-descent}
	\begin{algorithmic}
		\State \textbf{Input}: data $X$, $Y$, step-size $\eta > 0$, iterations $T$.
		\State \textbf{Initialize} weights $\set{W_{\ell}(0)}_{\ell=1}^{L}$.
		\For{$t = 0, 1, \dots, T-1$}
		\State $W_{\ell}(t+1) = W_{\ell}(t) - \eta \grad \cL(\set{W_{\ell}(t)}_{\ell=1}^{L}; (X, Y))$
		\EndFor
		\State \textbf{return} $\set{W_{\ell}(T)}_{\ell=1}^{L}$.
	\end{algorithmic}
\end{algorithm}
We consider weight matrices of the following sizes:
\begin{itemize}[itemsep=0ex]
	\item The weight matrix of the input layer $W_{1} \in \Rbb^{\dhid \times \din}$,
	      where $\dhid$ is a width common to all hidden layers.
	\item The weight matrix of the output layer $W_{L} \in \Rbb^{\dout \times \dhid}$.
	\item All other weight matrices $W_{2}, \dots, W_{L-1} \in \Rbb^{\dhid \times \dhid}$.
\end{itemize}
We will also write $W_{j:i}(t)$ for the following product of weight matrices at the $t^{\text{th}}$ iteration:
\begin{equation}
	W_{j:i}(t) := \prod_{\ell = j}^{i} W_{\ell}(t).
	\label{eq:folded-product}
\end{equation}
Having fixed the architecture, we introduce two mild assumptions under
which our results hold.
\begin{assumption}[Restricted Isometry Property]
	\label{assumption:rip}
	The measurement matrix $A$ from~\eqref{eq:dataset-measurements}
	satisfies the following: there exists $\delta > 0$ such that,
	for all vectors $x \in \range(R)$,
	\begin{equation}
		(1 - \delta) \norm{x}^2 \leq \norm{Ax}^2 \leq (1 + \delta) \norm{x}^2.
		\label{eq:rip}
	\end{equation}
\end{assumption}
\Cref{assumption:rip} is standard
in the compressed sensing literature~\cite{foucart2013invitation}, as it is
a sufficient condition that enables the solution of high-dimensional linear inverse
problems from few measurements. In our context,
\cref{assumption:rip} essentially states that the
training data has been sampled from inverse problems that are identifiable.

Our next assumption relates to the network initialization:
\begin{assumption}[Initialization]
	\label{assumption:initialization}
	The weight matrices $W_1, \dots, W_{L}$ at initialization are sampled from a scaled
	(``fan-in'') normal distribution:
	\begin{equation}
		[W_{\ell}(0)]_{ij} \iid \begin{cases}
			\cN\left(0, \frac{1}{\din}\right), & \ell = 1,           \\
			\cN(0, \frac{1}{\dhid}),           & \ell = 2, \dots, L.
		\end{cases}
		\label{eq:fan-in-initialization}
	\end{equation}
\end{assumption}
\Cref{assumption:initialization} is by no means restrictive: it
was introduced by~\cite{HZRS15} as a heuristic for stabilizing neural network training
and enjoys widespread adoption.\footnote{
	See, e.g., the \texttt{torch.nn.init.kaiming\_normal\_} initialization
	method in \texttt{Pytorch}.
}

We now present an informal version of our main result. The formal statement can be found in \cref{thm:mainresult-formal}, and the proof comprises \cref{sec:subsec:prelim,sec:subsec:main result formal,sec:subsec:Lemmas used for the proof,sec:subsec:Properties at initialization,sec:subsec:Step 1: Rapid early convergence,sec:subsec:Step 2: he error stays small,sec:subsec:Step 3: Convergence off the subspace}.
\begin{theorem}[Informal]
	\label{theorem:main-informal}
	Let~\cref{assumption:rip,assumption:initialization} hold and set
	the step size $\eta$ and weight decay parameter $\lambda$ as
	\begin{equation}
		\eta := \sfrac{\din}{L \cdot \sigma_{\max}^2(X)}, \quad
		\lambda := \gamma \sigma_{\min}^2(X) \sqrt{\sfrac{\din}{\dout}},
		\label{eq:informal-theorem-stepsize-and-wd}
	\end{equation}
	where $\gamma \in (0, 1]$ is a user-specified accuracy parameter. Moreover, define
	the times
	\begin{subequations}
		\begin{align}
			\tau & = \inf\set*{
				t \in \mathbb{N} \mid
				\frobnorm{W_{L:1}(t)Y - X} \leq
				\frac{80 \gamma \frobnorm{X}}{L}
			},             \label{eq:tau def}                                                            \\
			T    & = \frac{2 L \kappa^2 \log(\dhid)}{\gamma} \sqrt{\frac{\dout}{\din}}, \label{eq:T def}
		\end{align}
	\end{subequations}
	where $\kappa := \opnorm{X} \opnorm{X^{\dag}}$ denotes the condition number  of $X$.
	Finally, let $\sr(X) := \nicefrac{\frobnorm{X}^2}{\opnorm{X}^2}$ denote
	the \emph{stable rank} of $X$.
	Then, as long as the hidden layer width satisfies
	\[
		\dhid \gtrsim \dout \cdot \sr(X) \cdot \mathrm{poly}(L, \kappa),
	\]
	gradient descent (\cref{alg:gradient-descent})
	produces iterates that satisfy
	\begin{align}
		\frobnorm{W_{L:1}(t+1)Y - X}
		                                          & \leq
		\begin{cases}
			\left(1 - \frac{1}{32 \kappa^2}\right) \frobnorm{W_{L:1}(t)Y - X}, & t < \tau;         \\
			C_1 \gamma \frobnorm{X},                                           & \tau \le t \leq T
		\end{cases} \label{eq:thm-regression-error} \\
		\opnorm{W_{L:1}(T) P_{\range(Y)}^{\perp}} & \leq \left( \frac{1}{\dhid} \right)^{C_{2}},
		\label{eq:thm-generalization-error}
	\end{align}
	where $C_{1}$ and $C_{2}$ are universal positive constants. These guarantees hold with high probability
	over the random initialization.
\end{theorem}
\cref{eq:thm-regression-error} in \cref{theorem:main-informal} demonstrates that the reconstruction of $X$ from $Y$ can be made arbitrarily accurate using a suitably small choice of regularization parameter $\lambda$.
On the other hand, \cref{eq:thm-generalization-error} ensures that the component of the weights acting on the orthogonal complement of the signal subspace can be made small by increasing the hidden width of the model; this ensures robustness to noisy test data as discussed below in \cref{sec:subsec:robustness}.
\cref{theorem:main-informal} also highlights two distinct phases of gradient descent; during the first $\tau$ iterations, \cref{eq:thm-regression-error} suggests that the error in the reconstruction converges linearly up to the threshold specified in~\eqref{eq:tau def}. Upon reaching that threshold, the behavior changes:
while the reconstruction error can increase mildly from
iteration $\tau$ to $T$, the component of the weights acting on the orthogonal complement of the signal subspace shrinks to the level shown in \cref{eq:thm-generalization-error}.
The number of iterations $T$ of gradient descent required to achieve this behavior grows only logarithmically with the hidden width, but is highly sensitive to the targeted reconstruction accuracy -- and therefore the weight decay parameter $\lambda$.

\begin{remark}
	Plugging $\eta$ and $\lambda$ into~\cref{eq:T def} implies that $T = O(\nicefrac{1}{\eta \lambda})$; this is consistent with the
	results of~\citet{lewkowycz2020training,wang2024implicit}. The
	former work observes empirically that SGD without momentum
	attains maximum performance at roughly $O(\nicefrac{1}{\eta \lambda})$ iterations,
	while the latter work~\citep[Theorem B.2]{wang2024implicit}
	suggests that stochastic gradient descent requires a similar number of iterations to find a low-rank solution --- albeit one that might be a poor data fit.
\end{remark}

\begin{remark}
	\label{remark:small-eta}
	\cref{theorem:main-informal} remains valid when the step size $\eta$ is chosen to be smaller than the value specified in the theorem, albeit at the expense of an increased number of iterations $T$.
\end{remark}
