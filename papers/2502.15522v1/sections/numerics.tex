In this section, we present numerical experiments that corroborate our theoretical findings
and examine the sensitivity of the learned mapping to different parameters: the dimension of the latent subspace $s$ (\cref{sec:impact of s}, the depth of the neural network $L$ (\cref{sec:subsec:depth}),
and the regularization strength $\lambda$ (\cref{sec:subsec:wd}).
In our experiments, we track the regression and ``off-subspace''
errors across $t$:
\[
    \frac{\frobnorm{W_{L:1}(t) Y - X}}{\frobnorm{X}}
    \quad \text{and} \quad
    \opnorm{W_{L:1}(t) P_{\range(Y)}^{\perp}}.
\]
\paragraph{Experimental setup.}
For each experiment shown in \cref{fig:linear,fig:wd,fig:errors-by-depth,fig:plot_s}, we generate the measurement matrix $A$ by sampling a random
Gaussian matrix $G \in \Rbb^{\din \times \dout}$ and setting 
$A := \frac{1}{\sqrt{\din}} G$; such matrices satisfy
\cref{assumption:rip} with high probability as long as
$\din \gtrsim s \log(\dout)$~\cite{foucart2013invitation}.
To form the subspace basis matrix $R$, we
calculate the QR factorization of a $\dout \times s$ random Gaussian matrix and keep the orthogonal factor. 
Finally, we generate the signal matrix
$X \in \Rbb^{\dout \times n}$ as $X = RZ$, where $Z
\in \Rbb^{s \times n}$ is a full row-rank matrix of
subspace coefficients. Given a target condition number
$\kappa$ for $X$, we generate $Z$ via its SVD: we sample
the left and right singular factors at random and arrange
its singular values uniformly in the interval $[\frac{1}{\kappa}, 1]$. All our experiments use step sizes that
are covered by our theory but do not necessarily correspond to the value suggested by~\cref{theorem:main-informal} (see~\cref{remark:small-eta}). Similarly, each experiment uses a number of iterations that is sufficiently large but not necessarily equal to $T$. Finally, all weight decay parameters used correspond to a valid
$\gamma \in (0, 1)$, but for the sake of simplicity we
specify $\lambda$ directly.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/errors-by-subspace-dimension.pdf}
    \caption{Comparing the training error of a deep linear neural network for data of varying subspace dimensions $s$ using constant stepsize $\eta = \nicefrac{1}{10}$ and
    weight decay $\lambda = 10^{-3}$. The lines are the median over $10$ runs with independently sampled training data and weight initializations. The shaded region indicates one standard deviation around the median. 
    See \cref{sec:impact of s} for details.
    }
    \label{fig:plot_s}
\end{figure}

\subsection{Impact of latent subspace dimension $s$} \label{sec:impact of s}
The statement of~\cref{theorem:main-informal} suggests that the size of
the subspace $s$ does not affect the rate of (on-subspace) convergence
or the error achieved after $T$ iterations. To verify this numerically,
we generate several synthetic datasets with varying subspace dimension
$s \in \{2, 4, 8, 16, 32\}$, $\din = 128$, $\dout = 256$ and perfectly conditioned data (i.e., $\kappa = 1$). For each dataset, we train a deep linear network of width $\dhid = 512$ using $\eta = \nicefrac{1}{10}$ and $\lambda = 10^{-3}$ and compute the median reconstruction and off-subspace errors and standard deviation over $10$ independent runs, with each run using $n = 1000$ independently drawn samples. The results, depicted in~\cref{fig:plot_s},
suggest that the errors decay at the same rate; in the case of
the reconstruction error, the differences in magnitude are
negligible, while the off-subspace errors differ by a constant offset across subspace dimensions.

\subsection{Impact of neural network depth $L$}
\label{sec:subsec:depth}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/errors-by-depth.pdf}
    \caption{Normalized regression error and off-subspace error for deep linear nets of varying
    depths $L$, trained with gradient descent using
    constant stepsize $\eta = \sfrac{1}{10}$ and weight decay parameter $\lambda = 10^{-4}$. While
    the regression error drops to similar levels for
    all depths, larger $L$ confers a clear advantage with respect to the off-subspace error. See~\cref{sec:subsec:depth} for details.}
    \label{fig:errors-by-depth}
\end{figure*}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/errors-by-wd-depth=3.pdf}
    \caption{Normalized regression error and off-subspace errors for deep linear nets trained with
    gradient descent with stepsize $\eta = \sfrac{1}{10}$ and varying levels of weight decay $\lambda$. While high levels of weight decay reduce
    the off-subspace error faster, they lead to
    larger regression error. See~\cref{sec:subsec:wd} for details.}
     \label{fig:wd}
\end{figure}
In our next experiment, we examine how the neural network depth, $L$, affects convergence and generalization. We generate a dataset with subspace dimension $s = 4$,
measurement dimension $\din = 32$, signal dimension $\dout = 64$ and $n = 1000$ samples (using perfectly condition data; i.e., $\kappa = 1$) and train a deep linear network of width $\dhid = 1000$ using
gradient descent. We use the same stepsize $\eta = 10^{-1}$ and weight
decay parameter $\lambda = 10^{-4}$ across all configurations.

The results for both quantities of interest are depicted in~\cref{fig:errors-by-depth}.
The regression error first drops to similar levels, for all depths, before it starts increasing and
plateauing at roughly $20 \lambda$. However, higher depth $L$ confers a clear advantage with respect to the off-subspace error.


\subsection{Impact of weight decay parameter $\lambda$}
\label{sec:subsec:wd}
Our next experiment examines the impact of the weight decay parameter $\lambda$.
We use a similar setup as in~\cref{sec:subsec:depth}, where $s = 4$, $\din = 32$ and $\dout = 64$ with $n = 1000$ samples,
and train neural networks of width $\dhid = 1000$ and depth $L = 3$;
see~\cref{fig:wd}.
As~\cref{theorem:main-informal} suggests, larger weight decay values lead to
larger regression errors (approximately $10 \cdot \lambda$) but faster decaying off-subspace errors.




