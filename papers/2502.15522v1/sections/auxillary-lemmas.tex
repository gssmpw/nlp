In this section, we state and prove results used to prove the main result \cref{thm:mainresult-formal} or mentioned in the introduction. We start with a result showing that a global minimizer solution of the regularized optimization problem is zero on the orthogonal complement of the image.
\begin{lemma}\label{lem:robustsolutionofoptproblem}
    Suppose $f_{W_{L:1}}$ is a global minimizer of the regularized optimization problem~\eqref{eq:l2regprob1}.
    Then $f_{W_{L:1}}$ satisfies $W_1P_{\range(Y)}^{\perp}=0$, where $P_{\range(Y)}^{\perp}$ is the projection onto the orthogonal complement of $\range(Y)$.
\end{lemma}
\begin{proof}
Suppose that $f_{W_{L:1}}$ is a minimizer with $W_1P_{\range(Y)}^{\perp} \neq 0$. Then consider $f_{W_{L:1}P_{\range(Y)}}$, the neural network that coincides with $f_{W_{L:1}}$ except that its first-layer weights are right-multiplied by $P_{\range(Y)}$. We have
\begin{align*}
\frobnorm{f_{W_{L:1}P_{\range(Y)}}(Y)-X} 
= \frobnorm{f_{W_{L:1}}(P_{\range(Y)}Y)-X} 
= \frobnorm{f_{W_{L:1}}(Y)-X}. 
\end{align*}
Hence the first term in the objective in~\eqref{eq:l2regprob1} is the same for $f_{W_{L:1}}$ and $f_{W_{L:1}P_{\range(Y)}}$.
By the Pythagorean theorem, we have that
\begin{align*}
    \frobnorm{W_1}^2 &=
    \frobnorm{W_1 P_{\range(Y)} }^2 +
    \frobnorm{W_1 P_{\range(Y)}^{\perp}}^2
    > \frobnorm{W_1 P_{\range(Y)} }^2
\end{align*}
since $W_1P_{\range(Y)}^{\perp} \neq 0$ by
assumption. Thus the regularization term in the objective~\eqref{eq:l2regprob1} is strictly larger for $f_{W_{L:1}}$ than for $f_{W_{L:1}P_{\range(Y)}}$. Therefore $f_{W_{L:1}}$ cannot be the minimal-norm solution.
\end{proof}

\begin{lemma}
    \label{lemma:wide-gaussian-prod-tail}
    Let $A_1, A_2, \dots, A_{q}$ have i.i.d. Gaussian elements with $A_{i} \in \Rbb^{n_{i} \times n_{i-1}}$, $n_{0} = n$,
	and $n_{i} \gtrsim q$. Then
    \begin{align}
    	\expec{\norm{A_{q} \dots A_1 y}^2} & = \norm{y}^2 \cdot \prod_{i=1}^q n_{i},
			\label{eq:expec-prod}  \\
        \prob{\abs[\big]{\norm{A_{q} \cdots A_1 y}^2 - \norm{y}^2 \prod_{i=1}^q n_i}
        \geq 0.1 \norm{y}^2 \prod_{i=1}^q n_i}
        &\leq c_1 \exp\left(-\frac{c_2}{\sum_{ i = 1 }^q n_i^{-1}} \right),
        \label{eq:tail-bound-prod}
    \end{align}
    where $c_1, c_2 > 0$ are universal constants and $y$ is any fixed vector.
\end{lemma}
\begin{proof}
    We start with~\eqref{eq:expec-prod}. Note that for any $A_{i}$, we have
	\begin{align*}
		\norm{A_{i} y}^2 & = \sum_{j = 1}^{n_i} \ip{(A_{i})_{j, :}, y}^2                                      \\
		                 & \overset{(d)}{=} \sum_{j = 1}^{n_i} \norm{y}^2 g_{i}^2 \qquad (g_i \sim \cN(0, 1)) \\
		                 & \overset{(d)}{=} \norm{y}^2 Z_{i},
	\end{align*}
	where $Z_i \sim \chi^2_{n_i}$, a $\chi^2$-random variable with $n_i$ degrees of freedom. As a result,
	\[
		\expec{\norm{A_i y}^2} = \norm{y}^2 \expec{Z_{i}} = \norm{y}^2 \cdot n_{i}.
	\]
	Moreover, since $A_{1}, \dots, A_{q}$ are independent, we have
	\begin{align*}
		\expec{\norm{A_{q} \dots A_1 y}^2} & =
		\expec{\expec{\norm{A_{q} (A_{q-1} \dots A_1 y)}^2 \mid A_{1}, \dots, A_{q-1}}} \\
		                                   & =
		n_{q} \expec{\norm{A_{q-1} \dots A_{1} y}^2}                                    \\
		                                   & =
		n_{q} \expec{\expec{\norm{A_{q-1} \dots A_1 y}^2 \mid A_{1}, \dots, A_{q-2}}}   \\
		                                   & =
		n_{q} \cdot n_{q-1} \expec{\norm{A_{q-2} \dots A_1 y}^2}                        \\
		                                   & = \dots                                    \\
		                                   & = \prod_{j = 1}^q n_{i} \cdot \norm{y}^2,
	\end{align*}
	by iterating the above construction; this proves~\cref{eq:expec-prod}.

	\vspace{11pt}

	\noindent We now prove~\cref{eq:tail-bound-prod}. Let $\norm{y} = 1$ for simplicity;
	then $\norm{A_q \dots A_1 y}^2 \sim Z_{q} Z_{q-1} \dots Z_{1}$, where $Z_{i} \sim \chi^2_{n_i}$.
	The moments of a random variable $X \sim \chi^2_{k}$ satisfy
	\begin{align*}
		\mathbb{E}[X^{\lambda}] & = \frac{2^{\lambda} \Gamma(\frac{k}{2} + \lambda)}{\Gamma(\frac{k}{2})} \\
		                        & =
		\frac{2^{\lambda} \sqrt{\frac{4 \pi}{k + 2 \lambda}} \left(\frac{k + 2\lambda}{2e}\right)^{\frac{k}{2} + \lambda}}{
		\sqrt{\frac{4 \pi}{k}} \left(\frac{k}{2e}\right)^{\frac{k}{2}}
		} \left(1 + O(1 / k)\right),
	\end{align*}
	for all $\lambda > -k/2$, with the second equality furnished by a Stirling approximation.
	Following [Eq. (20)] in \cite{du2019width}, we obtain the following upper bound:
	\begin{equation}
		\mathbb{E}[X^{\lambda}] \leq
		\exp\left(
		\frac{2 \lambda^2}{k} - \frac{1}{2} \log\left(1 + \frac{2\lambda}{k}\right)
		+ \lambda \log k
		\right) \cdot \left(1 + O\left(\frac{1}{k}\right)\right),
		\quad \forall \lambda \geq -\frac{k}{4}.
		\label{eq:stirling-ub-chi2}
	\end{equation}
	To bound the upper tail in~\cref{eq:tail-bound-prod}, we argue that for any $\lambda > 0$,
	\begin{align}
		 & \prob{Z_q \dots Z_1 \geq \exp(c) \prod_{i=1}^q n_i}                                                           \notag \\
		 & \leq
		\exp\left(-\lambda c\right) \left(\prod_{i=1}^q n_i\right)^{-\lambda} \cdot
		\mathbb{E}[(Z_q \dots Z_1)^{\lambda}]                                                                            \notag \\
		 & =
		\exp\left(-\lambda c - \lambda \log\left( \prod_{i=1}^q n_i \right)\right) \mathbb{E}[(Z_q \dots Z_1)^{\lambda}] \notag \\
		 & \leq
		\exp\left(-\lambda c - \lambda \log\left( \prod_{i=1}^q n_i \right)
		+ \sum_{i=1}^q \frac{2 \lambda^2}{n_i} + \lambda \log(n_i) - \frac{1}{2} \log\left(1 + \frac{2\lambda}{n_i}\right)
		\right) \prod_{j=1}^q \left(1 + O\left(\frac{1}{n_i}\right)\right).
		\label{eq:Z-chernoff-ub}
	\end{align}
	Under our assumption that $n_i \gtrsim q$, the last term above satisfies
	\begin{align}
		\prod_{j=1}^q \left(1 + O\left(\frac{1}{n_i}\right)\right) & \lesssim
		\prod_{j=1}^q \left(1 + \frac{1}{q}\right) \notag                       \\
		                                                           & =
		\left(1 + \frac{1}{q}\right)^q \notag                                   \\
		                                                           & \leq
		\lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^{n} \notag             \\
		                                                           & = \exp(1),
		\label{eq:exp-defn-ub}
	\end{align}
	using the formal definition of the exponential. 
    For the upper tail, the exponent in~\eqref{eq:Z-chernoff-ub} can be simplified as follows:
    \begin{align*}
        -\lambda c + \sum_{i = 1}^q \frac{2 \lambda^2}{n_i} - \frac{1}{2} \log\left(1 + \frac{2 \lambda}{n_i}\right)
        \leq 
        -\lambda c + 2 \lambda^2 \sum_{i = 1}^q \frac{1}{n_i},
    \end{align*}
    using $\log(1 + u) \geq 0$ for any $u \geq 0$.
    Maximizing over $\lambda \geq 0$ yields
    \[
        \lambda_{\star} = \frac{c}{4 \cdot \sum_{i = 1}^q \frac{1}{n_i}}.
    \]
    Plugging the value of $\lambda_{\star}$ into the upper bound for the exponent leads to
    \begin{align*}
        & -\lambda_{\star} c + 2 \lambda_{\star}^2
        \sum_{i = 1}^q \frac{1}{n_i} \\
        &=
        -\frac{c^2}{4} \frac{1}{\sum_{i = 1}^q n_i^{-1}} +
        \frac{c^2}{8} \cdot
        \frac{\sum_{i = 1}^q \frac{1}{n_i}}{
        \left(\sum_{i = 1}^q \frac{1}{n_i}\right)^2
        } \\
        &=
        -\frac{c^2}{8} \cdot \frac{1}{\sum_{i = 1}^q n_i^{-1}}.
    \end{align*}
    Setting $c = \log(1.1)$ completes the proof.
    
    We now derive the lower bound in~\cref{eq:tail-bound-prod}.
    Given $\lambda < 0$, we have
	\begin{align*}
		 &\prob{Z_{q} \dots Z_1 \leq \exp(-c) \prod_{i=1}^q n_i} \\
		 & =\prob{(Z_{q} \dots Z_1)^{\lambda} \geq \exp(-\lambda c) \Big(\prod_{i=1}^q n_i\Big)^{\lambda}} \\
		 & \leq
		\exp\left(\lambda c - \lambda \log\Big(\prod_{i=1}^q n_i\Big)\right) \mathbb{E}[(Z_q \dots Z_1)^{\lambda}]
		\\
		 & \leq
		C_1 \exp\left(\lambda c - \lambda \sum_{i=1}^q \log(n_i)
		+ \sum_{i=1}^q \frac{2\lambda^2}{n_i} - \frac{1}{2} \log\left(1 + \frac{2\lambda}{n_i}\right)
		+ \lambda \log(n_i)
		\right)                                                                                            \\
		 & =
		C_1 \exp\left(
		\lambda c + 2 \lambda^2 \sum_{i=1}^q \frac{1}{n_i} - \frac{1}{2} \log\left(1 + \frac{2 \lambda}{n_i}\right)
		\right)
	\end{align*}
    In particular, the exponent in the preceding display satisfies
    \begin{align*}
        \lambda c + \sum_{i = 1}^q \frac{2 \lambda^2}{n_i} - \frac{1}{2} \log\Big(1 + \frac{2 \lambda}{n_i}\Big)
        & \leq
        \lambda c + 2 \lambda^2 \sum_{i = 1}^q 
        \frac{1}{n_i} - 2 \lambda \sum_{i=1}^q \frac{1}{n_i},
    \end{align*}
    using the inequaliy $\log(1 + 2x) \geq 4x$ valid for any $x > -\frac{1}{4}$. Setting $\lambda = -\frac{c}{4 \sum_{i=1}^q n_i^{-1}}$ yields
    \[
        -\frac{c^2}{4 \sum_{i=1}^q n_i^{-1}} +
        \frac{c^2}{8} \frac{\sum_{i=1}^q n_i^{-1}}{(\sum_{i=1}^q n_i^{-1})^2}
        +\frac{c}{2}
        =
        -\frac{c^2}{4 \sum_{i = 1}^q n_i^{-1}} + \frac{c}{2}.
    \]
    Setting $c = -\log(0.9)$ completes the proof.
\end{proof}

\begin{theorem}[Weierstrass]
	\label{thm:weierstrass}
	The following inequality holds:
	\begin{equation}
		1 - \sum_{i=1}^n w_{i} x_i \leq \prod_{i = 1}^n \left(1 - x_i\right)^{w_i}, \quad
		\text{for all $x \in [0, 1]$ and $w_i \geq 1$}.
		\label{eq:weierstrass}
	\end{equation}
\end{theorem}
\begin{proof}
    We prove the inequality by induction on the number of
    terms. For the base case $n = 1$, consider the function
    \[
        h(w) = (1 - x_1)^w - (1 - w x_1), \quad
        \text{with} \;\;
        h'(w) = (1 - x_1)^{w} \log(1 - x_1) + x_1
    \]
    Clearly $h(1) = 0$, so it suffices to show $h$ is increasing on $[1, \infty)$. Starting from the inequality
    \(
        \log(1 - x_1) \geq \frac{x_1}{x_1 - 1},
    \)
    we have
    \begin{align*}
        h'(w) &\geq
        \frac{x_1 (1 - x_1)^{w}}{x_1 - 1} + x_1 \\
        &=
        \frac{x_1 (1 - x_1)^{w} + x_1 (x_1 - 1)}{x_1 - 1} \\
        &=
        \frac{x_1 \left[(1 - x_1) - (1 - x_1)^w\right]}{1 - x_1} \\
        &\geq 0, \quad \text{for all $w \geq 1$,}
    \end{align*}
    since $(1 - x_1) \in (0, 1)$. This proves the claim
    for $n = 1$.

    Now suppose the claim holds up to some $n \in \mathbb{N}$. We have
    \begin{align*}
        \prod_{j = 1}^{n+1} (1 - x_j)^{w_j} &=
        (1 - x_{n+1})^{w_{n+1}} \prod_{j = 1}^n (1 - x_j)^{w_j} \\
        &\geq
        (1 - w_{n+1} x_{n+1}) \prod_{j = 1}^{n}
        (1 - x_j)^{w_j} \\
        &\geq
        (1 - w_{n+1} x_{n+1}) \left(1 - \sum_{j = 1}^n w_j x_j \right) \\
        &=
        1 - \sum_{j = 1}^{n+1} w_j x_j +
        w_{n+1} x_{n+1} \cdot \sum_{j = 1}^{n} w_j x_j \\
        &\geq
        1 - \sum_{j = 1}^{n+1} w_j x_j,
    \end{align*}
    where the first inequality follows from the base
    case, the second inequality follows by the inductive
    hypothesis and the last inequality follows from
    nonnegativity of $\set{w_j}_{j \geq 1}$ and $\set{x_j}_{j \geq 1}$. This completes the proof.
\end{proof}

\begin{lemma}
	\label{lemma:one-minus-folded-product}
	Under the assumptions of \cref{thm:mainresult-formal}, we have that
	\begin{equation}
		|1 - \call|
		\leq
		\frac{(L - 1) \eta \lambda}{d_w} +
		\frac{\eta \lambda}{m} \leq \frac{2 \eta \lambda}{m}.
		\label{eq:one-minus-folded-product}
	\end{equation}
\end{lemma}
\begin{proof}
	Since $\call < 1$, we have
	$|\call - 1| =
		1 - \prod_{i = 1}^{L}
		\left(1 - \frac{\eta \lambda}{d_i}\right)$.
	Now, let $x_i := \frac{\eta \lambda}{d_i}$ and $w_i := 1$ for $i = 1, \dots, L$. From~\cref{thm:weierstrass},
	it follows that
	\begin{align*}
		1 - \prod_{i = 1}^L
		\left(1 - \frac{\eta \lambda}{d_i}\right) & \leq
		\sum_{i = 1}^L \frac{\eta \lambda}{d_i} =
		\frac{(L - 1) \eta \lambda}{d_{w}} +
		\frac{\eta \lambda}{m}
		\leq \frac{2 \eta \lambda}{m},
	\end{align*}
    under the assumption that $\dhid \ge m(L-1)$.
\end{proof}
\begin{lemma}
    \label{lemma:truncated-geometric-series}
    For any $\alpha \leq \frac{1}{2}$ and $j, k \in \mathbb{N}$, it holds that
    \begin{equation}
        \sum_{i = j}^k \alpha^{i} \leq
        2 \alpha^j (1 - \alpha^{k - j + 1}).
    \end{equation}
\end{lemma}
\begin{proof}
    The claim follows from the geometric series formula:
    \begin{align*}
       \sum_{i = j}^k \alpha^i &=
       \alpha^j \sum_{i = 0}^{k- j } \alpha^i
       =
       \alpha^j \cdot \frac{1 - \alpha^{k - j + 1}}{1 - \alpha}
       \leq
       2 \alpha^j (1 - \alpha^{k - j + 1}),
    \end{align*}
    where the last inequality follows from  $\nicefrac{1}{1 - \alpha} \leq 2$.
\end{proof}
