Training a network on the regularized objective with gradient descent leads to a more robust solution than a network trained without regularization. The following Corollary, whose proof can be found in~\cref{sec:subsec:Robustness at test time}, formalizes this by considering a test instance with noisy measurements.
\begin{corollary}
	\label{cor:robustness}
	Let $(W_1(T), \dots, W_{L}(T))$ be the weight matrices of a deep linear
	network trained for $T$ iterations in the setting of~\cref{theorem:main-informal}.
	Consider a test data point $(x, y)$ satisfying $y=Ax+\epsilon$, where
	$\epsilon \sim \mathcal{N}(0, \sigma^2)$.
	Then, with high probability, the output of the network $W_{L:1}(T)(y)$ satisfies
	\begin{equation}\label{eq:robustnesslambda}
		\norm{W_{L:1}(T)y - x} \lesssim
		\gamma \kappa \sqrt{\sr(X)}  + \frac{1}{\dhid^{C_2}}
		+ \sigma \sqrt{s}.
	\end{equation}
	Conversely, let $(W^{\lambda=0}_1(t),...,W^{\lambda=0}_L(t))$
	be the weight matrices of a deep linear network trained in the setting of~\cref{theorem:main-informal} with $\lambda = 0$. Then, for any $\beta >0$, there exists an iteration $T$ such that the reconstruction error $\frobnorm{W_{L:1}^{\lambda = 0}(t)Y-X} \leq \beta \frobnorm{X}$ for all $t>T$. Moreover, with high probability, the test error satisfies
	\begin{equation}\label{eq:robustnessnolambda}
		\norm{W^{\lambda=0}_{L:1}(t)y-x} \gtrsim
		\sigma \left(\sqrt{\frac{\dout(\din - s)}{\din}} - \sqrt{s}\right) -
		\beta \kappa \sqrt{\sr(X)} \norm{y}.
	\end{equation}
\end{corollary}
The benefit of weight decay can be deduced from the qualitative behavior of the two bounds: on one hand,
the error in~\cref{eq:robustnesslambda} can be driven
arbitrarily close to $\sigma \sqrt{s}$ -- which is unimprovable in general -- by choosing $\gamma$ sufficiently small and $\dhid$ sufficiently large.
On the other hand, suppose that $\sfrac{(\din - s)}{\din} = \Omega(1)$
(a standard regime in compressed sensing tasks): in that case, training without weight decay \emph{always} incurs a
test error of at least $\sigma \sqrt{\dout} - \beta \kappa \sqrt{\sr(X)} \norm{y}$. For high-dimensional problem instances,
this bound is only vacuous if $\beta$ scales with the misspecification $\sigma$, the ambient dimension $\sqrt{\dout}$, or both -- in other words, the lower bound~\eqref{eq:robustnessnolambda} can be significantly larger than~\eqref{eq:robustnesslambda}
unless $W_{L:1}^{\lambda = 0}(t)$ is a poor fit
to the training data.
