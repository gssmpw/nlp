In this section, we provide a proof sketch for~\cref{theorem:main-informal}; full
proofs are deferred to the Appendix. For simplicity, the proof sketch focuses on the case
\[
	L = 2, \;\; \kappa = 1, \;\; \delta = \frac{1}{10}.
\]
Since normalization at initialization (\cref{assumption:initialization})
is essential to the proof, it is convenient to be explicit about normalization factors. We consider
the equivalent loss
\begin{equation}
	\cL(W_1, W_2) := \frac{1}{2} \frobnorm[\Big]{
		\frac{1}{\sqrt{\dhid \din}} W_{2} W_{1} Y - X
	}^2
	+ \frac{\lambda}{2} \left(
	\frac{\frobnorm{W_1}^2}{\din} + \frac{\frobnorm{W_2}^2}{\dhid}
	\right),
	\label{eq:reformulated-loss}
\end{equation}
under the assumption that $(W_1(0))_{ij}, (W_2(0))_{ij} \iid \cN(0, 1)$.
We will also use the shorthand notation
\[
	\Phi(t) := \frac{1}{\sqrt{\dhid \din}} W_{2:1}(t) Y - X.
\]
We refer to $\frobnorm{\Phi(t)}$ as the regression error.
Note that the gradient descent updates lead to the following decomposition:
\begin{align*}
	 & W_{2:1}(t+1)                                                                                                        \\
	 & = \begin{aligned}[t]
		      & \left(1 - \frac{\eta \lambda}{\dhid}\right)\left(1 - \frac{\eta \lambda}{\din}\right) W_{2:1}(t) + E_0(t) \\
		      & - \frac{\eta}{\sqrt{\dhid \din}}
		     \left(1 - \frac{\eta \lambda}{\dhid}\right) W_{2}(t) (W_{2}(t))^{\T} \Phi(t) Y^{\T}                          \\
		      & - \frac{\eta}{\sqrt{\dhid \din}}
		     \left(1 - \frac{\eta \lambda}{\din}\right) \Phi(t) Y^{\T} (W_{1}(t))^{\T} W_{1}(t),
	     \end{aligned}
\end{align*}
where $E_0(t) \in O(\eta^2)$ contains high-order terms. Multiplying both sides from the right by $(1 / \sqrt{\dhid \din}) Y$, subtracting
$X$ and taking norms, we obtain the following bound on the regression error at time $t+1$:
\begin{align}
	\frobnorm{\Phi(t+1)}
	\leq
	\opnorm{I - \eta P(t)} \frobnorm{\Phi(t)}
	+ O\left(\frac{\eta \lambda}{\din}\right) \cdot \frobnorm[\Big]{\frac{W_{2:1}(t) Y}{\sqrt{\dhid \din}}}
	+ \frobnorm[\Big]{\frac{1}{\sqrt{\dhid \din}} E_0(t) Y},
	\label{eq:proof-sketch-error-decomposition}
\end{align}
where $P(t)$ is an operator acting on matrix space whose matrix representation in terms of the vectorization is given by
\[
	P(t) :=
	\frac{1}{\dhid \din}
	\left(1 - \frac{\eta \lambda}{\din}\right) (W_1(t) Y)^{\T} (W_{1}(t) Y) \\
	+ \frac{1}{\dhid \din}
	\left(1 - \frac{\eta \lambda}{\dhid}\right) (Y^{\T} Y) \otimes ((W_{2}(t))^{\T} W_{2}(t)).
\]
In particular, one can show that the high-order terms from $E_0(t)$
can be ``folded'' into the first term in~\eqref{eq:proof-sketch-error-decomposition}, since
\begin{equation}
	\frobnorm[\Big]{\frac{1}{\sqrt{\dhid \din}} E_0(t) Y} \leq
	\frac{\eta \opnorm{P(t)}}{4} \frobnorm{\Phi(t)}.
\end{equation}
Consequently, it is the spectrum of $P(t)$ that controls the rate of convergence (up to error that vanishes as $\lambda \rightarrow 0$). Thanks to properties of the Kronecker product, controlling the
spectrum of $P(t)$ can be reduced to controlling the extremal singular values of $W_{1} Y$ and $W_2$ (see~\cref{lemma:P-k-spectrum} for the full statement).

The remainder of the proof outlines two phases for the convergence behavior of gradient descent. In the first phase,
the regression error is driven rapidly to a level that depends
on the regularization strength $\lambda$; in the second phase, the ``off-subspace'' error decreases while the regression error can fluctuate, albeit in a controlled manner.
\vspace*{-1em}
\paragraph{Phase 1: Rapid linear convergence.}
In the first phase, we show that the following properties hold by induction for $t < \tau$:
\begin{itemize}
	\item (\textbf{Singular value control}): For all $i$, it holds that
	      \begin{align*}
		      \frac{3}{4} \sqrt{\dhid}                  & \leq \sigma_{i}(W_{2}(t)) \leq \frac{5}{4} \sqrt{\dhid}                     \\
		      \frac{3}{4} \sqrt{\dhid} \sigma_{\min}(X) & \leq \sigma_{i}(W_{1}(t) Y) \leq \frac{5}{4} \sqrt{\dhid} \sigma_{\max}(X).
	      \end{align*}
	\item (\textbf{Small displacement}): We have
	      \begin{align*}
		      \opnorm{W_{1}(t) - \left(1 - \frac{\eta \lambda}{\din}\right)^{t} W_{1}(0)}  & \lesssim \sqrt{\dout \sr(X)}; \\
		      \opnorm{W_{2}(t) - \left(1 - \frac{\eta \lambda}{\dhid}\right)^{t} W_{2}(0)} & \lesssim \sqrt{\dout \sr(X)}.
	      \end{align*}
	\item (\textbf{Sufficient decrease in regression error}): We have
	      \begin{align}
		       & \frobnorm{\Phi(t+1)}                                                                                                                                  \label{eq:proof-outline-suff-decrease} \\
		       & \leq \left(1 - \frac{\eta \sigma_{\min}^2(X)}{8 \din}\right) \frobnorm{\Phi(t)} + \frac{5 \eta \lambda}{2\din} \sqrt{\frac{\dout}{\din}} \frobnorm{X}. \notag
	      \end{align}
\end{itemize}
A detailed argument can be found in~\cref{sec:subsec:Step 1: Rapid early convergence}.

While $t < \tau$, where $\tau$ is defined in~\cref{theorem:main-informal}, the second
term in the right-hand side of~\eqref{eq:proof-outline-suff-decrease} satisfies
\[
	\frac{5 \eta \lambda}{2 \din} \sqrt{\frac{\dout}{\din}} \frobnorm{X} \leq
	\frac{\eta \sigma_{\min}^2(X)}{16 \din} \frobnorm{\Phi(t)}.
\]
Consequently, we obtain the following recurrence for $t < \tau$:
\begin{equation}
	\frobnorm{\Phi(t+1)} \leq \left(1 - \frac{\eta \sigma_{\min}^2(X)}{16 \din}\right) \frobnorm{\Phi(t)}.
	\label{eq:phase-1-decrease}
\end{equation}
Plugging $\eta = \nicefrac{\din}{2 \sigma_{\max}^2(X)}$ into~\eqref{eq:phase-1-decrease} yields the bound~\eqref{eq:thm-regression-error} for $t < \tau$.
Iterating~\eqref{eq:phase-1-decrease} until the condition in the definition of $\tau$ fails reveals that the length of
phase 1 is at most
\begin{equation}
	\tau \lesssim \log\left(\frac{1}{\gamma} \sqrt{\frac{\dout}{\din}}\right) \;\;
	\text{iterations.}
	\label{eq:proof-sketch-tau-bound}
\end{equation}
Consequently, we achieve regression error $O(\gamma)$ within $O(\log \frac{1}{\gamma})$ iterations, a rate
commensurate with that achieved by gradient descent when minimizing the \emph{convex} objective
$\min_{W} \frobnorm{WY - X}^2$~\cite{du2019width}. Reducing the \emph{off-subspace} error,
$\opnorm{W_{2:1}(t) P_{\range(Y)}^{\perp}}$,
requires further work as outlined below.
\vspace*{-1em}
\paragraph{Phase 2: Off-subspace component reduction.}
During the first phase, the off-subspace error will generally
decrease but remain nontrivial, requiring additional iterations to bring to acceptable levels.
The challenge is that when $t > \tau$, the regression error $\frobnorm{\Phi(t)}$ is no longer monotonic.
To that end, we argue that the regression error \emph{remains} small
(up to a constant multiplicative factor) for the next $O\big(\frac{1}{\gamma}\big)$ steps,
subject to the same stepsize requirements; in turn, these steps are sufficient to reduce
the off-subspace error to $O(\mathrm{poly}(\dhid^{-1}))$.
Specifically, we argue that the following properties
hold (see~\cref{sec:subsec:Step 2: he error stays small}) for all iterations $t$ satisfying $\tau \le t \le O(\log(\dhid) / \lambda)$:
\begin{itemize}
	\item (\textbf{Singular value control II}): For all $i$, it holds that
	      \begin{align*}
		      \frac{5}{7} \sqrt{\dhid}  & \leq \sigma_{i}(W_{2}(t)) \leq \frac{9}{7} \sqrt{\dhid}; \\
		      \sigma_{\max}(W_{1}(t) Y) & \leq \frac{9}{7} \sqrt{\dhid} \sigma_{\max}(X).
	      \end{align*}
	\item (\textbf{Small displacement II}): We have that
	      \begin{align*}
		      \opnorm{W_{1}(t) - \left(1 - \frac{\eta \lambda}{\din}\right)^{t - \tau} W_{1}(\tau)}  & \lesssim \sqrt{\dout \sr(X)} \log(\dhid); \\
		      \opnorm{W_{2}(t) - \left(1 - \frac{\eta \lambda}{\dhid}\right)^{t - \tau} W_{2}(\tau)} & \lesssim \sqrt{\dout \sr(X)} \log(\dhid).
	      \end{align*}
	\item (\textbf{Small regression error}): We have
	      \begin{equation*}
		      \frobnorm{\Phi(t)} \lesssim
		      \frac{\lambda \frobnorm{X}}{\sigma_{\min}^2(X)} \sqrt{\frac{\dout}{\din}}
		      = O(\gamma \frobnorm{X}).
	      \end{equation*}
\end{itemize}
Equipped with the properties above, we show that the off-subspace error
satisfies the bound:
\begin{equation}
	\opnorm{W_{2:1}(t) P_{\range(Y)}^{\perp}} \lesssim \left(1 - \frac{\lambda}{2}\right)^{t}
	\sqrt{\frac{\dhid}{\din}},
	\label{eq:proof-sketch-off-subspace-bound}
\end{equation}
which is at most $\dhid^{-C_2}$ when
$t \geq \Omega\left(\frac{\log(\dhid)}{\lambda}\right)$.
See~\cref{sec:subsec:Step 3: Convergence off the subspace} for details.
