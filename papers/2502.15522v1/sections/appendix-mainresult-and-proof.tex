This section presents the formal version of the main result and the full proof. 
We start with fixing some notation and assumptions in \cref{sec:subsec:prelim}, and then state the main result in \cref{sec:subsec:main result formal}.
\cref{sec:subsec:Lemmas used for the proof} shows some general lemmas used in multiple proof steps. 
We show in \cref{sec:subsec:Properties at initialization} that certain properties hold at initialization.
The main proof then involves three steps.
First, we prove by induction in \cref{sec:subsec:Step 1: Rapid early convergence} that the regression error rapidly decreases during an initial phase of gradient descent.
Second, in \cref{sec:subsec:Step 2: he error stays small}, we again use induction to show that the regression error remains small during a subsequent phase of gradient descent.
Third, in \cref{sec:subsec:Step 3: Convergence off the subspace} we show that the ``off-subspace'' error becomes small during this period. 
We conclude by showing that this method is robust at test time in \cref{sec:subsec:Robustness at test time}.

Let us note that \cref{sec:subsec:Lemmas used for the proof,sec:subsec:Properties at initialization,sec:subsec:Step 1: Rapid early convergence} are based on the proof in \cite{du2019width} of the convergence of gradient descent for the convex problem $\min_W \frobnorm{ WY-X}$. Because of the additional regularization term in our setting, the proof is significantly different. For example, we cannot prove that the error converges towards $0$ or stays small for all iterations. Instead, we show in \cref{sec:subsec:Step 2: he error stays small,sec:subsec:Step 3: Convergence off the subspace} that the error stays less than $O(\lambda)$ for many iterations, during which time the ``off-subspace'' error shrinks, leading to good generalization.
\subsection{Preliminaries}
\label{sec:subsec:prelim}

\subsubsection{Notation}
\label{sec:subsec:subsubsec:notation}
We first establish some notation; let
\begin{subequations}
	\begin{align}
		W_{j:i} & = \prod_{t=i}^j W_{t} = \begin{cases}
			                                  W_j W_{j-1} \ldots W_i, & \text{if $i \leq j$}, \\
			                                  I,                      & \text{otherwise};
		                                  \end{cases} \\
		U       & = d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:1}Y.                        \\
		\Phi    & = U - X.
	\end{align}
\end{subequations}

The matrix $U$ corresponds to the network predictions,
while $\Phi$ corresponds to the matrix of training residuals.
To refer to a matrix at iteration $t$ of gradient descent, we write $W_{i}(t)$, $W_{j:i}(t), U(t), \Phi(t)$, etc.
When convenient, we write
\begin{equation}
	d_i = \begin{cases}
		d_w & \text{for $2\le i \le L$} \\
		m   & \text{for $i=1$.}
	\end{cases}
\end{equation}
With this notation at hand, our loss function becomes
\begin{equation}
	f(W_1, \ldots, W_L)
	= \frac{1}{2} \frobnorm{\Phi}^2
	+ \frac{\lambda}{2} \sum_{j=1}^{L}
    \frobnorm*{\frac{1}{d_i} W_i}^2 \\
	\label{eq:loss}
\end{equation}
We shall also write $\cprod{i}$ and $\call$ for the following products appearing in our proofs:
\begin{equation}
	\cprod{i} := \prod_{\substack{j = 1\\j \neq i}}^{L} \left(1 - \frac{\eta \lambda}{d_i} \right) \quad \text{and} \quad
	\call := \prod_{i = 1}^{L} \left(1 - \frac{\eta \lambda}{d_i}\right).
	\label{eq:c-prod-i}
\end{equation}
Finally, we let $\sr(X) \in [1, \rank(X)]$ denote the \emph{stable rank} of $X$, defined as 
\begin{equation}
  \sr(X) := \left( \frac{\frobnorm{X}}{\opnorm{X}} \right)^2 =
  \sum_{i \geq 1} \left( \frac{\sigma_{i}(X)}{\sigma_{1}(X)} \right)^2.
  \label{eq:stable-rank}
\end{equation}

\subsubsection{Initialization} \label{sec:subsec:subsubsec:initialization}
\begin{assumption}[Initialization]
	All the weight matrices $W_\ell$ are initialized according to:
	\[
		(W_{\ell})_{ij} \iid \cN(0, 1)
	\]
	with dimensions $W_{1} \in \Rbb^{d_w \times m}$, $W_{2}, \dots, W_{L-1} \in \Rbb^{d_w \times d_w}$,
	and $W_{L} \in \Rbb^{d \times d_w}$.
\end{assumption}
This is the same as \cref{assumption:initialization} since in the optimization problem in \cref{eq:loss} we have explicitly pulled out the normalization factor that comes from the ``fan-in" initialization.

\subsubsection{Gradient Descent Updates}
\label{sec:subsec:subsubsec:gd-updates}
The gradient of the regression error with respect to $W_{i}$ is equal to
\begin{equation}
	\grad_{W_i} \left[\frac{1}{2}\frobnorm{\Phi}^2 \right]
	= d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:i+1}^{\T} \Phi Y^{\T} W_{i-1:1}^{\T}.
	\label{eq:mse-gradient}
\end{equation}
The gradient of the $\ell_2$-regularization term is
\begin{equation}
	\grad_{W_i} \left[ \frac{\lambda}{2}\frobnorm*{\frac{1}{\sqrt{d_i}}W_i}^2 \right]
	= \frac{\lambda}{d_i} W_i.
\end{equation}
Hence the gradient descent iteration is as follows:
\begin{equation}
	W_i(t+1)
	= \left(1-\frac{\eta\lambda}{d_i}\right) W_i(t) - \eta
	d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:i+1}(t)^{\T} \Phi(t) Y^{\T} W_{i-1:1}(t)^{\T},
  \quad \text{for $1 \leq i \leq L$.}
	\label{eq:Wi-update}
\end{equation}

\subsubsection{Simplifying the number of samples}
\label{sec:subsec:subsubsec:number-of-samples}
We may assume that we have exactly $s$ input samples for the purpose of analysis.
Indeed,~\cref{claim:exact-rank-X} below shows that the gradient descent trajectories remain
unchanged when the number of samples $n > s$.
\begin{claim}
	\label{claim:exact-rank-X}
	Without loss of generality, we may assume $Z \in \Rbb^{s \times s}$, with $\rank(Z) = s$.
\end{claim}
\begin{proof}
	Since $X = RZ$ where $Z \in \Rbb^{s \times n}$ and $n \geq s$, the economic SVD of $Z$ yields
	\[
		X = R U_{Z} \Sigma_{Z} V_{Z}^{\T}, \quad
		U_{Z} \in O(s), \; \Sigma_{Z} = \diag(\sigma_1, \dots, \sigma_{s}),
		\; V_{Z} \in O(n, s).
	\]
	Since the Frobenius norm is unitarily invariant,
	\begin{align*}
		\frobnorm{\Phi}
		 & = \frobnorm{d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:1}Y - X}
		\\&= \frobnorm{(d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}}W_{L:1}AR - R) U_{Z} \Sigma_{Z} V_{Z}^\T}
		\\&= \frobnorm{d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}}W_{L:1}ARU_{Z}\Sigma_{Z} - RU_{Z} \Sigma_{Z}}.
	\end{align*}
	Moreover,
	\begin{align*}
		\grad_{W_i} \left[\frac{1}{2}\frobnorm{\Phi}^2 \right]
		 & = d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:i+1}^{\T} \Phi Y^{\T} W_{i-1:1}^{\T}                                                                                                                              \\
		 & = d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:i+1}^{\T} (d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:1}A - I)R U_{Z} \Sigma_{Z} \underbrace{V_{Z}^{\T} V_{Z}}_{I_{s}} \Sigma_{Z} U_{Z}^{\T} R^{\T} A^{\T} W_{i-1:1}^{\T} \\
		 & = d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:i+1}^{\T} (d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}}  W_{L:1}A - I)R U_{Z} \Sigma_{Z}^2 U_{Z}^{\T} R^{\T} A W_{i-1:1}^{\T}.
	\end{align*}
	Thus, without loss of generality, we can assume that $X = R U_{Z} \Sigma_{Z} \in \Rbb^{\dout \times s}$ since this assumption does not change the gradient descent trajectory or value of the loss function.
\end{proof}

Throughout the remainder of this section, we will assume that $n = s$.

\subsection{Main result}
\label{sec:subsec:main result formal}
Given the above assumptions and notation, we can now state the formal version of our main result.
\begin{theorem}
  \label{thm:mainresult-formal}
  Let~\cref{assumption:rip,assumption:initialization} hold
  with $\delta = \frac{1}{10}$. Furthermore, suppose the following conditions are true:
  \begin{equation}
      \lambda \leq
      \frac{L \sigma_{\min}^2(X)}{400 \cdot 35}, \;\;
      \dhid \gtrsim \dout \cdot \sr(X) \cdot \mathrm{poly}(L, \kappa), \;\;
      \eta \leq \frac{m}{L \sigma^2_{\max}(X)}, \;\; \text{and} \;\; \lambda = \gamma \sigma_{\min}^2(X) \sqrt{\frac{m}{d}},
      \label{eq:main-thm-assumptions}
  \end{equation}
  where $\gamma \in (0, 1]$ is a user-specified accuracy parameter.
  Moreover, define the times
	\begin{subequations}
		\begin{align}
			\tau & = \inf\set*{
				t \in \mathbb{N} \mid
				\frobnorm{\Phi(t)} \leq
				\frac{80 \gamma\frobnorm{X}}{L}
			},                                                                       \label{eq:tau def phi}\\
			T    & = \frac{2\log(\dhid)\sqrt{dm}}{\eta\gamma \sigma_{\min}^2(X)}. \label{eq:T def appendix}
		\end{align}
	\end{subequations}
Then with probability of at least $1-c_1 e^{-c_2 \dout}$  over the random initialization,
\begin{align}
		 \frobnorm{W_{L:1}(t+1)Y - X} \label{eq:thm-regression-error-formal}
		 & \leq
		\begin{cases}
			\left(1 - \frac{\eta L \sigma_{\min}^2(X)}{32 \din}\right) \frobnorm{W_{L:1}(t)Y - X}, & t < \tau;        \\
			C_1 \gamma \frobnorm{X} ,                                           & \tau \leq t \leq T.
		\end{cases}
		\\
		 \opnorm{W_{L:1}(T) P_{\range(Y)}^{\perp}}  
     &\leq \left( \frac{1}{\dhid} \right)^{C_2},
		\label{eq:thm-generalization-error-formal}
	\end{align}
    where $c_1, c_2$, $C_1$ and $C_2$ are positive universal constants.
\end{theorem}
\begin{remark}
    Throughout~\cref{sec:subsec:Lemmas used for the proof,sec:subsec:Properties at initialization,sec:subsec:Step 1: Rapid early convergence,sec:subsec:Step 2: he error stays small,sec:subsec:Step 3: Convergence off the subspace}, \cref{assumption:initialization,assumption:rip,eq:main-thm-assumptions} are in force.
\end{remark}
\begin{remark}
    The condition $\lambda \leq \nicefrac{L \sigma_{\min}^2(X)}{400 \cdot 35}$ is automatically satisfied for small enough $\gamma$.
\end{remark}

\subsection{Lemmas used for the proof}
\label{sec:subsec:Lemmas used for the proof}
The following lemma bounds the deviation of $W_{i}(t)$ from $\left(1-\frac{\eta \lambda}{d_i} \right)^t W_{i}(0)$.
\begin{lemma}
	\label{lemma:difference-norm}
  For any $i \in [L]$, any $t \in \mathbb{N}$, and any matrix norm $\norm{\cdot}$, we have
	\begin{align*}
     & \norm{W_{i}(t)- \left(1-\frac{\eta \lambda}{d_i} \right)^t W_{i}(0)} \\
		 & \leq
		\eta d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} \sum_{j=0}^{t-1}
		\left(1 - \frac{\eta \lambda}{d_i}\right)^{t-1-j}
		\norm{W_{L:i+1}(j)^{\T} \Phi(j) (W_{i-1:1}(j) Y)^{\T}}
		\label{eq:difference-norm-scaled}
	\end{align*}
\end{lemma}
\begin{proof}
	The proof follows from the update formula for $W_{i}$ in \cref{eq:Wi-update}.
	Writing
	\begin{equation}
		B_{t} := d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:i+1}(t)^{\T} \Phi(t) Y^{\T} W_{i-1:1}(t)^{\T},
	\end{equation}
	we rewrite~\cref{eq:Wi-update} as the recursion
	\begin{align*}
		W_i(t) & = \left(1 - \frac{\eta \lambda}{d_i}\right) W_i(t-1) - \eta B_{t-1} \notag \\
		       & = \left(1 - \frac{\eta \lambda}{d_i}\right)^2 W_i(t-2)
		- \eta \left(1 - \frac{\eta \lambda}{d_i}\right) B_{t-2} - \eta B_{t-1} \notag      \\
		       & \qquad \qquad \vdots \notag                                                \\
		       & = \left(1 - \frac{\eta \lambda}{d_i}\right)^t W_i(0)
		- \eta \sum_{j=0}^{t-1} \left(1 - \frac{\eta \lambda}{d_i}\right)^{t-1-j} B_{j}.
	\end{align*}
	Rearranging, taking norms and applying the triangle inequality yields the result.
\end{proof}
\subsubsection{Evolution of Product Matrix} \label{sec:iteration of WL1}
\begin{lemma}
	\label{lemma:prod-evolution-I}
	For an arbitrary iteration index $t$, it holds that
	\begin{align}
		W_{L:1}(t+1) &=
    \begin{aligned}[t]
         & \call W_{L:1}(t) + E_0(t) \\
         & - \eta d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}}
			     \sum_{i=1}^L
			     \cprod{i}
			     W_{L:i+1}(t) W_{L:i+1}^{\T}(t) \Phi(t) Y^{\T} W_{i-1:1}^{\T}(t) W_{i-1:1}(t),
    \end{aligned}
    \label{eq:prod-evolution-I}
	\end{align}
	with $E_0(t)$ containing all $O(\eta^2)$ terms.
\end{lemma}
\begin{proof}
	This is essentially the decomposition in~\citep[Section 5]{du2019width}, modified since
	\begin{equation}
		W_i(t+1)
		= W_i(t)\left(1-\frac{\eta\lambda}{d_i}\right) - \eta
		d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:i+1}(t)^{\T} \Phi(t) Y^{\T} W_{i-1:1}(t)^{\T}.
	\end{equation}
    For the sake of brevity, we do not repeat the argument here.
\end{proof}

\paragraph{Evolution of the Network Outputs and Residuals.}
\label{sec:evolution of res}
Armed with~\cref{lemma:prod-evolution-I}, we right-multiply
both sides of~\eqref{eq:prod-evolution-I} by $d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} Y$ to obtain
\begin{equation}
	U(t+1) = \begin{aligned}[t] 
    &\call U(t) + E(t)  \\
    & - \eta d_w^{-(L-1)}m^{-1}
	  \sum_{i=1}^L
	\cprod{i}
    W_{L:i+1}(t) W_{L:i+1}(t)^{\T} \Phi(t) Y^{\T} W_{i-1:1}^{\T}(t) W_{i-1:1}(t)Y
  \end{aligned}
	\label{eq:prod-evolution-I-U}
\end{equation}
where $E(t) := d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} E_0(t)Y$.

Vectorizing both sides and using the identity $\vect(AXB) = (B^{\T} \otimes A) \vect(X)$ yields
\begin{align}
	\vect (U(t+1))
	= \call \vect (U(t))
	-\eta P(t) \vect(\Phi(t))
	+ \vect(E(t)),
	\label{eq:prod-evolution-I-vec}
\end{align}
where we write $P(t)$ for the following matrix (dropping the
time index $t$ for brevity):
\begin{align}
	P
	= d_w^{-(L-1)}m^{-1}
	\sum_{i=1}^L
	\cprod{i}
	\left(Y^{\T} W_{i-1:1}^{\T} W_{i-1:1}Y \right)
	\otimes
	\left(W_{L:i+1} W_{L:i+1}^{\T}\right)
    \in \R^{sd \times sd}
	\label{eq:coeff-matrix}
\end{align}
We subtract $\vect(X)$ from both sides of
\cref{eq:prod-evolution-I-vec}; using the notation from~\cref{eq:coeff-matrix}, the result is equal to
\begin{align}
    \notag
	\vect(\Phi(t+1))
	 & =
	\begin{aligned}[t]
		 & \call \vect (U(t))
		- \vect(X)
		-\eta P(t) \vect(\Phi(t))
		+ \vect(E(t))
	\end{aligned} \\
	 & =
	\begin{aligned}[t]
		 & \left(\call - 1\right) \vect (U(t))
		+ (I-\eta P(t)) \vect(\Phi(t))
		+ \vect(E(t))
	\end{aligned}
	\label{eq:error-evolution-I}
\end{align}
Taking the Frobenius norm on both sides of~\eqref{eq:error-evolution-I}
and using the triangle inequality yields
\begin{align}
    \notag
	\frobnorm{\Phi(t+1)}
	 & \leq
	\opnorm{I - \eta P(t)} \frobnorm{\Phi(t)}
	+ \left|\call-1\right| \frobnorm{U(t)}
	+ \frobnorm{E(t)} \\
	 & \leq
	\left(1 - \eta \lambda_{\min}(P(t))\right)
	\frobnorm{\Phi(t)}
	+ \frobnorm{U(t)} \callbound
	+ \frobnorm{E(t)},
	\label{eq:error-evolution-II}
\end{align}
as long as $\eta \le \frac{1}{\lambda_{\max}(P(t))}$, using
\cref{lemma:one-minus-folded-product} in the second inequality.
Intuitively, \cref{eq:error-evolution-II} suggests that
bounding the spectrum of $P$ will allow us to get a recursive bound on the norm of the residual.


\subsubsection{Bounds on the spectrum of $P$}
In this paragraph, we furnish bounds on the spectrum of $P$ in terms of the spectrum of $W_{L:i+1}$ and $W_{i-1:1} Y$, for $i = 1 \ldots L$. In the following lemma, we drop the time index $t$ for
simplicity.
\label{sec:bounding spectrum of p}
\begin{lemma}
	\label{lemma:P-k-spectrum}
	We have the following inequalities:
	\begin{align}
		\lambda_{\max}(P)
		 & \leq d_w^{-(L-1)}m^{-1}
		\sum_{i=1}^L
		\cprod{i} \sigma_{\max}^2(W_{i-1:1} Y) \sigma_{\max}^2(W_{L:i+1}); \label{eq:lambda-max-pk} \\
		\lambda_{\min}(P)
		 & \geq d_w^{-(L-1)}m^{-1}
		\sum_{i=1}^L
		\cprod{i} \sigma_{\min}^2(W_{i-1:1} Y) \sigma_{\min}^2(W_{L:i+1}). \label{eq:lambda-min-pk}
	\end{align}
\end{lemma}
\begin{proof}
	The inequalities are straightforward to prove using the definition of $P$ in \cref{eq:coeff-matrix} and the following facts:
	\begin{enumerate}
		\item The largest (or smallest) eigenvalue of a sum of matrices is bounded above (or below) by the sum of the largest (or smallest) eigenvalues.
		\item The eigenvalues of a Kronecker product are the products of the eigenvalues of the individual factors.
		\item For any matrix $A$, $\lambda_{\max}(A^{\T} A) = \sigma_{\max}^2(A)$.
	\end{enumerate}
	Using these facts, the result is immediate.
\end{proof}
\subsection{Properties at initialization}
\label{sec:subsec:Properties at initialization}
Let us bound the extremal singular values of $W_{j:1}Y$, $W_{L:i}$, $W_{i:j}$ at initialization and bound $\frobnorm{\Phi}$ and $\frobnorm{U}$ at initialization.
\begin{lemma}	\label{lemma:restricted-singular-values}
	There are universal constants $c_1, c_2 > 0$ such that 
	\begin{subequations}
		\begin{align*}
			\prob{\max_{1 \leq i < L} d_{w}^{-\frac{i}{2}} \sigma_{\max}(W_{i:1}(0) Y) \leq \frac{6}{5} \sigma_{\max}(X)}
			 & \geq 1 - c_1 \exp\left(-\frac{c_2 \dhid}{L} \right), \\
			\prob{\min_{1 \leq i < L} d_{w}^{-\frac{i}{2}} \sigma_{\min}(W_{i:1}(0) Y) \geq \frac{4}{5} \sigma_{\min}(X)}
			 & \geq 1 - c_1 \exp\left(-\frac{c_2 \dhid}{L} \right).
		\end{align*}
	\end{subequations}
\end{lemma}
\begin{proof}
    Let $U \Sigma V^{\T}$ be the economic SVD of $Y = AX$; since
    $X \in \range(R)$, where $\dim(\range(R)) = s$, this implies
    $U \in O(m, s)$, $\Sigma = \diag(\sigma_1, \dots, \sigma_{s})$
    and $V \in O(n, s)$. Consequently, for all $1 \leq i < L$ we have
    \begin{align}
        \sigma_{\max}(W_{i:1}(0) Y) &=
        \sigma_{\max}(W_{i:1}(0) AX) \notag \\ 
        & \leq \sigma_{\max}(W_{i:1}(0) U) \cdot \sigma_{\max}(\Sigma V^{\T}) \notag \\
        &= \sigma_{\max}(W_{i:1}(0) U) \opnorm{U \Sigma V^{\T}} \notag \\
        &\leq \sqrt{1 + \delta} \cdot \sigma_{\max}(W_{i:1}(0) U) \cdot \sigma_{\max}(X),
        \label{eq:w1y-ub}
    \end{align}
    where the last inequality follows from~\cref{assumption:rip}.
    Similarly, we have
    \begin{align}
        \sigma_{\min}(W_{i:1}(0) Y) &=
        \sigma_{\min}(W_{i:1}(0) AX) \notag \\ 
        & \geq \sigma_{\min}(W_{i:1}(0) U) \cdot \sigma_{\min}(\Sigma V^{\T}) \notag \\
        &= \sigma_{\min}(W_{i:1}(0) U) \cdot \sigma_{\min}(AX) \notag \\
        &\geq \sqrt{1 - \delta} \cdot \sigma_{\min}(W_{i:1}(0) U) \cdot \sigma_{\min}(X).
        \label{eq:w1y-lb}
    \end{align}
	We proceed with bounding the singular values of $W_{i:1}(0)U$. Note that
	\begin{equation}
		W_{1}(0) U = \begin{bmatrix}
			\ip{(W_{1}(0))_{1, :}, U_{:, 1}} & \dots & \ip{(W_{1}(0))_{1, :}, U_{:, s}} \\
			\ip{(W_{1}(0))_{2, :}, U_{:, 1}} & \dots & \ip{(W_{1}(0))_{2, :}, U_{:, s}} \\
			\vdots                  &       & \vdots                  \\
			\ip{(W_{1}(0))_{\dhid, :}, U_{:, 1}} & \dots & \ip{(W_{1}(0))_{\dhid, :}, U_{:, s}}
		\end{bmatrix}
		\overset{(d)}{=} G \in \Rbb^{\dhid \times s}, \;\;
		G_{ij} \iid \cN(0, 1),
		\label{eq:AR-product}
	\end{equation}
	since any two components are Gaussian and uncorrelated.
    Indeed, we have that
	\begin{align*}
		\expec{\ip{(W_1(0))_{i, :}, U_{:, j}} \ip{(W_{1}(0))_{k, :}, U_{:, \ell}}} & =
		\mathsf{tr}\Big( U_{:, j}^{\T}\expec{(W_{1}(0))_{i, :} (W_{1}(0))_{k, :}^{\T}} U_{:, \ell} \Big) \\
		                                                           & =
		\begin{cases}
			0,                              & i \neq k \\
			\ip{U_{:, j}, U_{:, \ell}} = 0, & i = k
		\end{cases},
	\end{align*}
	using the fact that $W_{1}(0)$ has isotropic and
	$U$ has orthogonal columns. We now apply
    \cref{lemma:wide-gaussian-prod-tail} with
    \[
        A_1 = W_{1}(0) U, A_2 = W_2, \dots, A_{i} = W_{i}, \;\; \text{and} \;\;
        n_1 = n_{2} = \dots = n_{i} = \dhid.
    \]
    For these parameter choices, we have
    \(
        \sum_{j = 1}^{i} \frac{1}{n_j} =
        \frac{i}{\dhid}.
    \)
    Thus, for any fixed $y \in \Sbb^{s-1}$ and $i < L$,
    \cref{lemma:wide-gaussian-prod-tail} yields
	\begin{equation}
		\prob{\abs{ \norm{W_{i:1}(0) Uy}^2 - d_w^i} \geq \frac{1}{10} d_w^i}
		\leq c_1 \exp\left(-\frac{c_2 \dhid}{i} \right).
	\end{equation}
	Taking an $\varepsilon$-net $\cN_{\varepsilon}$ of $\Sbb^{s-1}$
	and using~\citep[Exercise 4.3.4]{Ver18}, we obtain for $i<L$
	\begin{align}
		\sup_{y \in \Sbb^{s-1}} \abs{\norm{W_{i:1}(0) U y}^2 - d_w^{i}} & \leq
		\frac{1}{1 - 2 \varepsilon} \sup_{y \in \cN_{\varepsilon}} \abs{\norm{W_{i:1}(0)Uy}^2 - d_w^i} \notag \\ &\leq
		d_w^i \cdot \frac{1}{10(1 - 2 \varepsilon)}, \label{eq:epsilon-net-bound}
	\end{align}
	where the last inequality holds with probability at least $1 - c_1 \abs{\cN_{\varepsilon}} \exp\left(-\frac{c_2 \dhid}{i} \right)$
	as a result of a union bound over $\cN_{\varepsilon}$.
	Hence for $i<L$,
	\begin{align}
		\sigma_{\max}^2(W_{i:1}(0) U) & = \sup_{x \in \Sbb^{s-1}}
		\norm{W_{i:1}(0)U x}^2 \notag                                                                                         \\
		                               & \leq d_w^i + \sup_{x \in \Sbb^{s-1}} \abs{\norm{W_{i:1}(0) Ux}^2 - d_w^i} \notag     \\
		                               & \leq d_w^i \left( 1 + \frac{1}{10(1 - 2 \varepsilon)} \right). \label{eq:w1y-ub-epsnet}
	\end{align}
	Similarly for $i<L$,
	\begin{align}
		\sigma_{\min}^2(W_{i:1}(0) U) & = \inf_{x \in \Sbb^{s-1}}
		\norm{W_{i:1}(0) U x}^2 \notag                                                                                       \\
		                               & \geq d_w^i - \sup_{x \in \Sbb^{s-1}} \abs{\norm{W_{i:1}(0) U x}^2 - d_w^i} \notag   \\
		                               & \geq d_w^i \left(1 - \frac{1}{10(1 - 2 \varepsilon)}\right).
                                       \label{eq:w1y-lb-epsnet}
	\end{align}
	Letting $\varepsilon = \sfrac{1}{10}$ in~\cref{eq:w1y-lb-epsnet,eq:w1y-ub-epsnet} and applying the bounds of~\cref{eq:w1y-ub,eq:w1y-lb} shows that the bound holds for each individual $i$ with probability at least
	\begin{align*}
		1 - c_1 \exp\left\{-\frac{c_2 \dhid}{i} + \log \abs{\cN_{\varepsilon}}\right\} & \geq
		1 - c_1 \exp\left\{-\frac{c_2 \dhid}{i} + s \log \left(1 + \frac{2}{\varepsilon}\right)\right\} \\
		                                                                         & \geq
		1 - c_1 \exp\left(-\frac{c_2 \dhid}{2i}\right),
	\end{align*}
	as long as $\dhid \gtrsim L s$, using the bound~\citep[Corollary 4.2.13]{Ver18}:
	\[
		\abs{\cN_{\varepsilon}} \leq \left(1 + \frac{2}{\varepsilon}\right)^{s}.
	\]
	Taking an additional union bound over $i = 1, \dots, L-1$ combined with the condition
    $\dhid \gtrsim L s \log(L)$ yields the claim.
\end{proof}

\begin{lemma}
	\label{lemma:norm-product-bounded}
	There exist constants $c,C > 0$ such that
	\begin{equation}
		\prob{
			\max_{1 < k \leq j < L}
			\dhid^{-\frac{j - k + 1}{2}} \opnorm{W_{j:k}(0)} \leq
			\sqrt{\frac{L}{c}}
		} \geq
		1 - \exp\left(
		-\frac{C d_w}{L}
		\right).
		\label{eq:norm-product-bounded-uniform}
	\end{equation}
\end{lemma}
\begin{proof}
	Since $W_{i} \in \Rbb^{n_i \times n_{i-1}}=\Rbb^{d_w \times d_w}$ for all $1 <i < L$,
	Lemma \ref{lemma:wide-gaussian-prod-tail} implies
	\[
		\prob{0.9 d_w^{j-k+1} \norm{y}^2 \leq \norm{W_j \dots W_k y}^2 \leq 1.1 d_w^{j-k+1} \norm{y}^2} \geq 1 - 2 \exp\left(
		-\frac{c_1 d_w}{j - k + 1}
		\right).
	\]
	In the following choose $y \in \Sbb^{d_w-1}$ and a small constant $c_2 < c_1$. We can partition $[d_w]$ into $\frac{L}{c_2}$ sets, each of size $ \frac{c_2 d_w}{L}$. Therefore we can write 
	\[
		[d_w] = S_1 \cup \dots \cup S_{\frac{L}{c_2}}.
	\]
  Let $\supp(u):=\{i \mid u_i \neq 0 \}$ and $U_{S_\ell} := \set{u \in \Sbb^{d_w-1} \mid \supp(u) \subset S_\ell}$.
  Taking a $\frac{1}{2}$-net $\mathcal{N}_{\ell}$ of $U_{S_{\ell}}$, we obtain:
	\begin{align*}
		\norm{W_{j:k}(0)u_{\ell}} \leq \sqrt{1.1} d_w^{\frac{j-k+1}{2}} \cdot \frac{3}{2} \leq 2 d_w^{\frac{j-k+1}{2}}, \;\; \text{for all $u_{\ell} \in U_{S_\ell}$},
	\end{align*}
	with the probability of failure at most
	\begin{align*}
		\abs{\cN_{\ell}} \exp\left(-\frac{c_1 d_{w}}{j - k + 1}\right) & \leq
		\log\left(1 + \frac{2}{\sfrac{1}{2}}\right)^{\abs{S_{\ell}}}
		\exp\left(-\frac{c_1 d_{w}}{j - k + 1}\right)                         \\
		                                                               & \leq
		\exp\left(
		- \frac{c_1 d_{w}}{j - k + 1} + \frac{c_2 d_{w}}{L} \log(5)
		\right)                                                               \\
		                                                               & \leq
		\exp\left(
		- \frac{d_{w}}{L} \left(c_1 - c_2 \log(5)\right)
		\right).
	\end{align*}
	The above inequality holds for all $\ell$ at the same time with probability of at least
	\begin{align*}
		1 - \frac{L}{c_2} \exp\left(-\frac{d_{w}}{L}(c_1 - c_2 \log(5))\right)
		\geq 1 - \exp\left(-\frac{C d_w}{L}\right),
	\end{align*}
	for some small constant $C > 0$, as long as $d_{w} \gtrsim L \log \frac{L}{c_2}$ and
	$c_2 \leq \frac{c_1}{2 \log(5)}$ (as a result of a union bound).
    
	Finally, note that we can write any unit vector $y \in \Sbb^{d_w - 1}$ as
	\[
		y = \sum_{\ell} \alpha_{\ell} u_{\ell}, \;\; u_{\ell} \in U_{S_{\ell}}, \;\;
		\sum_{\ell} \alpha_{\ell}^2 = 1.
	\]
	Using the triangle inequality and conditioning on the previous event, we obtain
	\begin{equation}
		\norm{W_{j:i}(0)y} \leq \sum_{\ell} \norm{ W_{j:i}(0)\alpha_{\ell} u_{\ell}} \leq 2  d_w^{\frac{j-k+1}{2}}\sum_{\ell} \abs{\alpha_{\ell}} \leq 2  d_w^{\frac{j-k+1}{2}} \sqrt{\frac{L}{c_1}\sum_{\ell} \alpha_{\ell}^2} \leq d_w^{j-k+1} \sqrt{\frac{L}{c}},
	\end{equation}
	where the last step is using norm equivalence,
	and relabeling $c := \frac{c_1}{4}$. This
	completes the proof of the first display in~\cref{lemma:norm-product-bounded}.

	Finally, to prove~\cref{eq:norm-product-bounded-uniform},
	we apply the union bound over at most $\binom{L}{2} = O(L^2)$
	pairs of indices $i, j$ and use the fact that $d_{w}
		\gtrsim L \log(\frac{L}{c_1})$.
\end{proof}

\begin{lemma}
	\label{lemma:restricted-WL_singular_values}
    There is
	a universal constant $C > 0$ such that
	\begin{subequations}
		\begin{align}
			\prob{
			\max_{1 < i \leq L}
			d_{w}^{-\frac{L - i + 1}{2}}
			\sigma_{\max}(W_{L:i}(0))
			\leq \frac{6}{5}
			} & \geq 1 - \exp\left(-\frac{C d_w}{L}\right),
			\label{eq:restricted-WL-sval-ub-unif}
			\\
			\prob{
			\min_{1 < i \leq L}
			d_{w}^{-\frac{L - i + 1}{2}} \sigma_{\min}(W_{L:i}(0))
			\geq \frac{4}{5}
			} & \geq 1 - \exp\left(-\frac{C d_w}{L} \right).
			\label{eq:restricted-WL-sval-lb-unif}
		\end{align}
	\end{subequations}
\end{lemma}
\begin{proof}
	Since $W_{i}^{\T} \in \Rbb^{d_w \times d_w}$ for $1< i < L$ and $W_{L}^{\T} \in \Rbb^{d_w \times d}$, it follows from Lemma \ref{lemma:wide-gaussian-prod-tail} that
	\begin{align*}
		 & \prob{\abs*{\norm{W_{L:i}^{\T}(0) y}^2 - d_w^{L-i+1}} \geq d_w^{L-i+1}\frac{1}{10}}
		\leq
		2\exp\left(-\frac{c_1 d_w}{L - i + 1}\right)
	\end{align*}
	for any $y \in \Sbb^{d-1}$ and some $c_1 > 0$. Taking an $\varepsilon$-net $\cN_{\varepsilon}$ of
	$\Sbb^{d-1}$ and using~\citep[Exercise 4.3.4]{Ver18}, we have
	\begin{equation}
		\sup_{y \in \Sbb^{d-1}} \abs*{\norm{W_{L:i}^{\T}(0) y}^2 - d_w^{L-i+1}} \leq
		\frac{1}{1 - 2 \varepsilon} \sup_{y \in \cN_{\varepsilon}} \abs*{\norm{W_{L:i}^{\T}(0)y}^2 - d_w^{L-i+1}} \leq
		\frac{d_w^{L-i+1}}{10  \cdot (1 - 2 \varepsilon)}, \label{eq:epsilon-net-bound-upper}
	\end{equation}
	where the last inequality holds with probability at least $1 - 2 \abs{\cN_{\varepsilon}} \exp\left\{-\frac{c d_w}{L - i + 1}\right\}$
	as a result of~\cref{lemma:wide-gaussian-prod-tail} and a union bound over $\cN_{\varepsilon}$.
	In light of~\cref{eq:epsilon-net-bound-upper}, we have
	\begin{align}
		\sigma_{\max}^2(W_{L:i}^{\T}(0)) & = \sup_{x \in \Sbb^{d-1}}
		\norm{W_{L:i}^{\T}(0) x}^2 \notag                                                                                                    \\
		                                 & \leq d_w^{L-i+1} + \sup_{x \in \Sbb^{d-1}} \abs*{\norm{W_{L:i}^{\T}(0) x}^2 - d_w^{L-i+1}} \notag \\
		                                 & \leq d_w^{L-i+1} \cdot \left(1 + \frac{1}{10(1 - 2 \varepsilon)}\right). \label{eq:wLi-ub-epsnet}
	\end{align}
	At the same time,~\cref{eq:epsilon-net-bound-upper} leads to the lower bound
	\begin{align}
		\sigma_{\min}^2(W_{L:i}^{\T}(0)) & = \inf_{x \in \Sbb^{d-1}}
		\norm{W_{L:i}^{\T}(0) x}^2 \notag                                                                                                      \\
		                                 & \geq d_w^{L-i+1} - \sup_{x \in \Sbb^{d-1}} \abs*{\norm{W_{L:i}^{\T}(0) x}^2 - d_w^{L-i+1}} \notag   \\
		                                 & \geq d_w^{L-i+1} \cdot \left( 1 - \frac{1}{10(1 - 2 \varepsilon)} \right). \label{eq:wLi-lb-epsnet}
	\end{align}
	Letting $\varepsilon = 0.25$ in~\cref{eq:wLi-lb-epsnet,eq:wLi-ub-epsnet} we obtain the bound for each individual $i$ with probability of at least
	\begin{align*}
		1 - 2 \exp\left\{-\frac{c d_w}{L-i+1} + \log \abs{\cN_{\varepsilon}}\right\} & \geq
		1 - 2 \exp\left\{-\frac{c d_w}{L-i+1} + d \log \left(1 + \frac{2}{\varepsilon}\right)\right\} \\
		                                                                           & \geq
		1 - 2\exp\left(-\frac{c d_w}{2L}\right),
	\end{align*}
	as long as $d_w \gtrsim L d$, using the bound from in~\citep[Corollary 4.2.13]{Ver18}:
	\[
		\abs{\cN_{\varepsilon}} \leq \left(1 + \frac{2}{\varepsilon}\right)^{d}.
	\]
    To prove \cref{eq:restricted-WL-sval-ub-unif,eq:restricted-WL-sval-lb-unif,}
	we apply a union bound over $1 < i \leq L$ and require that
	$\dhid \gtrsim L \dout \log (L)$.
\end{proof}

\begin{lemma}
	\label{lemma:initial-regression-error}
	At initialization, it holds that
	\begin{align}
		\frobnorm{\Phi(0)} \leq \left(\frac{6}{5}  \sqrt{\frac{d}{m}}  +1 \right) \frobnorm{X} \leq
		\left(\frac{11}{5}  \sqrt{\frac{\dout}{\din}}  \right) \frobnorm{X}
		\quad \text{ and } \quad
		\frobnorm{U(0)} \leq \frac{6}{5}  \sqrt{\frac{\dout}{\din}}  \frobnorm{X}
		\label{eq:initial-regression-error}
	\end{align}
	with probability at least $1 - c_1 \exp\left(-c_2 \dout\right)$ as long
	as $\din \gtrsim s$ and $\dhid \gtrsim L \din$. 
\end{lemma}
\begin{proof}
	Let $\bar{U} \bar{\Sigma} \bar{V}^{\T}$ denote the economic SVD of $AX$, with $\bar{U} \in O(m, s)$. We have
	\begin{align*}
		\frobnorm{U(0)}
		 & = \frobnorm{d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} W_{L:1}(0)Y} \\
    &= d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} \frobnorm{W_{L:1}(0)AX} \\
    &\leq \dhid^{-\frac{L-1}{2}} \din^{-\frac{1}{2}}
    \opnorm{W_{L:1}(0)\bar{U}}
    \frobnorm{\bar{\Sigma} \bar{V}^{\T}} \\
    &\leq \sqrt{1 + \delta} \cdot \dhid^{-\frac{L-1}{2}} \din^{-\frac{1}{2}}
    \opnorm{W_{L:1}(0)\bar{U}}
    \frobnorm{X},
	\end{align*}
	where the last inequality follows from~\cref{assumption:rip} and unitary invariance
    of the norm. Similarly, we have
	\begin{align*}
		\frobnorm{\Phi(0)}
		 & = \frobnorm{U(0) - X}
		\leq \frobnorm{U(0)} + \frobnorm{X}.
	\end{align*}
	Consequently, it suffices to bound
	$d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}}\opnorm{W_{L:1}(0)\bar{U}}$. To that end,
    we invoke~\cref{lemma:wide-gaussian-prod-tail} with
    \[
        A_{1} = W_{1}(0) \bar{U}, A_{2} = W_2, \dots, A_{L} = W_{L}, \;\; \text{with} \;\;
        n_{1} = n_{2} = \dots = n_{L} = \dhid, \text{ and } n_{L+1} = \dout.
    \]
    For these choices, the failure probability will depend on the term
    \[
        \sum_{i = 1}^L \frac{1}{n_i}
        = \frac{L-1}{\dhid} + \frac{1}{\dout} 
        \leq \frac{2}{\dout},
    \]
    under the assumption $\dhid \gtrsim L \cdot \dout$. Indeed,
    \cref{lemma:wide-gaussian-prod-tail} yields (for any fixed $y \in \Rbb^s$):
    \begin{equation}
        \prob{\abs{\norm{W_{L:1}(0)Uy}^2 - \dout \cdot \dhid^{L - 1} \norm{y}^2}
        \geq \frac{1}{10} \dout \cdot \dhid^{L-1} \norm{y}^2}
        \leq c_1 \exp(-c_2 \dout).
    \end{equation}
    Taking an $\varepsilon$-net of $\mathbb{S}^{s-1}$ and proceeding as in the proof of~\cref{lemma:restricted-singular-values}, we obtain
    \[
        \opnorm{W_{L:1}(0)U}^2 \leq \dout \cdot \dhid^{L-1} \left(
        1 + \frac{1}{10(1 - 2 \varepsilon)}
        \right)
    \]
    with probability at least $1 - c_1 \exp\left(-c_2 \dout + s \log(1 + \frac{2}{\varepsilon})\right) \geq
    1 - \exp\left(-c_2 \sfrac{\dout}{2}\right)$, since
    $\dout \geq \din \gtrsim s$ for~\cref{assumption:rip} to be valid. Finally, letting
    $\varepsilon = \sfrac{1}{10} = \delta$, we obtain
    \[
        \sqrt{\frac{1 + \delta}{\dhid^{L - 1} \din}} \opnorm{W_{L:1}(0) \bar{U}}
        \leq \frac{6}{5} \sqrt{\frac{\dout}{\din}},
    \]
    as expected. This completes the proof.
\end{proof}



Before we proceed with the proof, we note that a simple union bound shows that
all the bounds in~\cref{lemma:initial-regression-error,lemma:restricted-WL_singular_values,lemma:restricted-singular-values,lemma:norm-product-bounded} are fulfilled simultaneously with probability at least
$1 - c_1 \exp(-c_2 \dout)$, for appropriate universal constants $c_1, c_2 > 0$.


\subsection{Step 1: Rapid early convergence}
\label{sec:subsec:Step 1: Rapid early convergence}
The first step of our convergence analysis is 
showing a sufficient decrease in the regression error until time $\tau$ as defined in \cref{eq:tau def phi}.
We will prove the following theorem in this section. 
\begin{theorem}
\label{thm:step1-induction}
    For all $0 \leq t \leq \tau$, the following events hold with probability of at least $1-c_1 e^{-c_2 d}$, where $c_1, c_2 > 0$ are
    universal constants, over the random initialization:
    \begin{subequations}
      \begin{align}
          \cA(t) &:= \set*{
              \frobnorm{\Phi(t+1)} \leq
              \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{16 \din} \right)
              \frobnorm{\Phi(t)}
              + \frac{5 \eta \lambda}{2 \din} \sqrt{\frac{\dout}{\din}} \frobnorm{X}
          }
          \label{eq:event-A-prestop}
          \\
          \cB(t) &:= \left\{
          \begin{array}{lcll}
              \sigma_{\max}(W_{j:i}(t)) &\leq&
              2 \sqrt{\frac{L}{c}} \dhid^{\frac{j - i + 1}{2}}, &
              1 < i \leq j < L \\
              \sigma_{\max}(W_{i:1}(t) Y) &\leq&
              \frac{5}{4} \dhid^{\frac{i}{2}} \cdot \sigma_{\max}(X), &
              1 \leq i < L, \\
              \sigma_{\max}(W_{L:i}(t)) &\leq&
              \frac{5}{4} \dhid^{\frac{L - i + 1}{2}}, & 1 < i \leq L, \\
              \sigma_{\min}(W_{i:1}(t) Y) &\geq&
              \frac{3}{4} \dhid^{\frac{i}{2}} \cdot \sigma_{\min}(X), & 1 \leq i < L, \\
              \sigma_{\min}(W_{L:i}(t)) &\geq&
              \frac{3}{4} \dhid^{\frac{L - i + 1}{2}}, & 1 \leq i < L.
              \\
          \end{array} \right\},
          \label{eq:event-B-prestop}
          \\
          \cC(t) &:= \set*{
              \opnorm{W_{i}(t) - \left(
              1 - \frac{\eta \lambda}{d_i}
              \right)^t W_{i}(0)}
              \lesssim
              R
              \mid
              1 \leq i \leq L
          },
          \;\; \text{where} \;\;
          R := \frac{\kappa^2 \sqrt{d \sr(X)}}{L}.
          \label{eq:event-C-prestop}
      \end{align}
    \end{subequations}
\end{theorem}
We will prove the above theorem by induction, starting with $t=0$ (\cref{lemma:step-i-properties-at-initialization}).
We then proceed by showing that:
\begin{itemize}
  \item $\set{\cA(j)}_{j < t}$ and $\cB(t)$ imply $\cA(t)$ (\cref{lemma:step-i-all-else-implies-A,lemma:Bt-implies-bound-on-E});
  \item $\set{\cA(j), \cB(j)}_{j < t}$ imply $\cC(t)$ (\cref{lemma:Bj-implies-Ct-phase-1});
  \item $\cC(t)$ implies $\cB(t)$ (\cref{lemma:Ct-implies-Bt-phase-1}).
\end{itemize}
The proof of~\cref{thm:step1-induction} follows by iterating the above implications
until the stopping time $\tau$ is reached.
\begin{lemma}[Initialization]
    \label{lemma:step-i-properties-at-initialization}
    The events $\cA(0)$, $\cB(0)$ and $\cC(0)$ hold with probability at least $1-c_1 e^{-c_2 \dout}$, where $c_1, c_2 > 0$ are universal constants.
\end{lemma}
\begin{proof}
    The base case $\cC(0)$ is trivial. On the other hand, $\cB(0)$ follows from
    \cref{lemma:restricted-singular-values,lemma:norm-product-bounded,,lemma:restricted-WL_singular_values}. 
    Finally, we show in \cref{lemma:step-i-all-else-implies-A}
    that $\cB(t)$ implies $\cA(t)$ for all $t,$ including $t = 0$.
\end{proof}


\begin{lemma}
    \label{lemma:Bt-implies-bound-on-E}
    Fix $t \leq \tau$ and suppose that $\{\cA(j)\}_{j \leq t - 1}$ and $\{\cB(j)\}_{j \leq t}$ hold.
    Then
    \begin{equation}
        \frobnorm{E(t)}
        \leq \frac{17 \eta L \sigma_{\min}^2(X)}{1024 \din}
        \cdot \frobnorm{\Phi(t)}.
    \end{equation}
\end{lemma}
\begin{proof}
Note that each term
in $E(t)$ is the product of $2$ or more terms of the form
$\grad_{W_i} \frac{1}{2} \frobnorm{\Phi}^2$
and $L-2$ or fewer terms of the form $W_{i}(t)(1 - \nicefrac{\eta \lambda}{d_i})$.
When $\ell$ of these terms are from the former category, there are $\binom{L}{\ell}$
ways to choose their indices $(s_1, \dots, s_{\ell})$. Each such choice induces
a term $C_{(s_1, \dots, s_{\ell})}$, defined by
\begin{align*}
  & C_{(s_1, \dots, s_{\ell})}\\
  & := \eta^{\ell} 
  \widetilde{W}_{L:(s_{\ell}+1)} \left( \grad_{W_{s_{\ell}}} \frac{1}{2} \frobnorm{\Phi}^2 \right)
  \widetilde{W}_{(s_{\ell}-1):(s_{\ell-1} + 1)} \left(\grad_{W_{s_{\ell-1}}} \frac{1}{2} \frobnorm{\Phi}^2\right)
  \dots
  \left( \grad_{W_{s_1}} \frac{1}{2} \frobnorm{\Phi}^2 \right)
  \widetilde{W}_{(s_1-1):1}
\end{align*}
where we define the products $\widetilde{W}_{i:j}$ as 
$\widetilde{W}_{i:j} = W_{i:j} \prod_{k = i}^{j} \left(1 - \frac{\eta \lambda}{d_k}\right).
  \label{eq:tildeW-defn}
$
Each factor of the form $\grad_{W_k} \frac{1}{2} \frobnorm{\Phi}^2$ satisfies
\begin{align}
  \frobnorm{\grad_{W_k} \frac{1}{2} \frobnorm{\Phi}^2} &\leq
  d_{w}^{-\frac{L-1}{2}} \din^{-\frac{1}{2}}
  \opnorm{W_{L:(k+1)}(t)} \frobnorm{\Phi(t)} \opnorm{W_{(k-1):1}(t) Y} \notag \\
                                                   &\leq
  \frac{5}{4} d_{w}^{-\frac{L-1}{2}} \din^{-\frac{1}{2}}
  \cdot d_{w}^{\frac{L - k}{2}} \frobnorm{\Phi(t)}
  \frac{5}{4} d_{w}^{\frac{k-1}{2}} \opnorm{X} \notag \\
                                                   &=
                                                   \frac{25}{16 \sqrt{\din}} \frobnorm{\Phi(t)} \opnorm{X}.
  \label{eq:Et-decomp-loss-term-bound}
\end{align}
From $\cB(t)$, the factors $W_{(s_{\ell}-1):(s_{\ell-1} + 1)}$ satisfy
$
\frobnorm{W_{(s_{\ell}-1):(s_{\ell-1} + 1)}}
\le 2\sqrt{\frac{L}{c}} \dhid^{\frac{s_\ell - s_{\ell-1}-1}{2}}.
$
From $\cB(t)$ and \cref{assumption:rip}, we also get 
$
\frobnorm{W_{(s_{1}-1):1}Y}
\le \frac{5}{4} \dhid^{\frac{s_1-1}{2}} \sigma_{\max}(X).
$
Similarly, we have
$
\frobnorm{W_{L:s_{\ell} + 1}}
\le 2\sqrt{\frac{L}{c}} \dhid^{\frac{L - s_{\ell}}{2}}.
$
Consequently, $C_{(s_1, \dots, s_{\ell})}Y$
admits the following bound:
\begin{align*}
  & \frobnorm{C_{(s_1, \dots, s_{\ell})}Y} \\
  &\leq
  \eta^{\ell} 
  \left( \prod_{k \notin \set{s_1, \dots, s_{\ell}}} \left(1 - \frac{\eta \lambda}{d_k}\right) \right) \cdot
  \left( \frac{25}{16 \sqrt{\din}} \frobnorm{\Phi(t)} \opnorm{X}\right)^\ell \cdot
  \left[
    \frac{5}{4} d_{w}^{\frac{s_1 - 1}{2}} \opnorm{X}
    \cdot 2\sqrt{\frac{L}{c}} \dhid^{\frac{L - s_{\ell}}{2}}
    \prod_{k = 1}^{\ell-1}
    2 \sqrt{\frac{L}{c}} d_{w}^{\frac{s_{k+1} - s_{k} - 1}{2}} 
  \right].
\end{align*}
Note that the last
term equals
\begin{equation}
  \frac{5}{4} d_{w}^{\frac{s_1 - 1}{2}} \opnorm{X}
    \cdot 2\sqrt{\frac{L}{c}} \dhid^{\frac{L - s_{\ell}}{2}}
    \prod_{k = 1}^{\ell-1}
    2 \sqrt{\frac{L}{c}} d_{w}^{\frac{s_{k+1} - s_{k} - 1}{2}} 
    = \frac{5}{4} \opnorm{X}
  \left(2 \sqrt{\frac{L}{c}}\right)^{\ell} \cdot
  d_{w}^{\frac{L - \ell}{2}},
  \label{eq:Et-decomp-last-term-bound}
\end{equation}
and the first term, comprising products for indices different from $\set{s_1, \dots, s_{\ell}}$, satisfies:
\begin{equation}
  \prod_{k \notin \set{s_1, \dots, s_{\ell}}} \left(1 - \frac{\eta \lambda}{d_k}\right)
  \leq \left(1 - \frac{\eta \lambda}{d_w}\right)^{L - \ell},
  \label{eq:Et-decomp-first-term-bound}
\end{equation}
since $\dhid \geq \din$ by assumption.
Putting~\cref{eq:Et-decomp-first-term-bound,eq:Et-decomp-loss-term-bound,eq:Et-decomp-last-term-bound}
together, we obtain
\begin{align*}
  \frobnorm{E(t)} &=
  \frobnorm{d_{w}^{-\frac{L-1}{2}} \din^{-\frac{1}{2}} E_{0}(t) Y} \\
                 &\leq
  \frac{5}{4} \opnorm{X}  d_{w}^{-\frac{L-1}{2}} \din^{-\frac{1}{2}}
  \sum_{\ell = 2}^{L}
  \eta^{\ell}
  \binom{L}{\ell} \left(1 - \frac{\eta \lambda}{d_w}\right)^{L - \ell}
  \left(2 \sqrt{\frac{L}{c}}\right)^{\ell} d_{w}^{\frac{L - \ell}{2}}
  \left(\frac{25}{16 \sqrt{\din}} \frobnorm{\Phi(t)} \opnorm{X}\right)^{\ell}  \\
                 &\leq
 \frac{5}{4} \opnorm{X}  \sqrt{\frac{\dhid}{\din}}
  \sum_{\ell=2}^{L} \left(\frac{C \eta L^{\frac{3}{2}} \opnorm{X} \frobnorm{\Phi(t)}}{\sqrt{\din \cdot d_w} \cdot (1 - \frac{\eta \lambda}{d_w})}\right)^{\ell} \\
                 &\leq
                 \frac{5C \eta L^{\frac{3}{2}} \opnorm{X}^2 \frobnorm{\Phi(t)}}{4\din \cdot (1 - \frac{\eta \lambda}{d_w})}
                 \sum_{\ell = 1}^{L - 1} \left(
                   \frac{C \eta L^{\frac{3}{2}} \opnorm{X} \frobnorm{\Phi(t)}}{(\din d_w)^{1/2} (1 - \nicefrac{\eta \lambda}{d_w})}
                 \right)^{\ell},
\end{align*}
where the second to last inequality was obtained  from the following bounds:
\begin{itemize}
  \item for any $j \in \mathbb{N}$, we have $\binom{L}{j} \leq L^j$;
  \item for any $j \in \mathbb{N}$, we have $\left(1 - \nicefrac{\eta \lambda}{d_w}\right)^{j} \leq 1$;
  \item finally, we relabel $C := 2 \sqrt{\frac{1}{c}} \cdot \frac{25}{16} $ for simplicity.
\end{itemize}
Note that $\eta \lambda \leq \frac{d_w}{2}$ implies $\nicefrac{\eta}{(1 - \frac{\eta \lambda}{d_w})} \leq 2\eta$. Consequently,
\begin{align*}
 \frac{2C \eta L^{3/2} \opnorm{X} \frobnorm{\Phi(t)}}{\sqrt{\din \dhid}} &\leq
     \frac{2C L^{1/2} \sqrt{\din} \frobnorm{\Phi(t)}}{\sqrt{\dhid} \sigma_{\max}(X)} \\ 
                                                                         &\lesssim
    \frac{\sqrt{L} \frobnorm{X} \sqrt{d}}{\sigma_{\max}(X) \sqrt{d_w}} \\
     &\lesssim
 \sqrt{\frac{L \dout \sr(X)}{d_w}} \\
     &\leq \frac{1}{2},
\end{align*}
where the first inequality follows from the upper bound on $\eta$, the second inequality
follows from $\cA(0), \dots, \cA(t-1)$, which together with the definition of $\tau$
imply that $\frobnorm{\Phi(t)} \leq \frobnorm{\Phi(0)} \lesssim \sqrt{\frac{\dout}{\din}} \frobnorm{X}$,
the penultimate inequality follows from the definition of $\sr(X)$ and the last inequality follows
from the lower bound on $\dhid$.
Therefore, the sum is bounded by $1$, which we use in the second inequality in the following.
Putting everything together, we obtain
\begin{align*}
  \frobnorm{E(t)}
  &\leq
  \frac{2 C \eta L^{3/2} \opnorm{X}^2 \frobnorm{\Phi(t)}}{m}
  \sum_{\ell = 1}^{L - 1} \left(
    \frac{2 C \eta L^{3/2} \opnorm{X} \frobnorm{\Phi(t)}}{\sqrt{\din \dhid}}
  \right)^{\ell}
  \\
  &\leq
  \frac{4 C^2 \eta^2 L^3 \opnorm{X}^3 \frobnorm{\Phi(t)}^2}{\din^{3/2} \dhid^{1/2}}
  \cdot \frac{1}{1 - \frac{2 C \eta L^{3/2} \opnorm{X} \frobnorm{\Phi(t)}}{\sqrt{\din \dhid}}}
  \\
  &\lesssim
  \frac{ \eta^2 L^{3} \opnorm{X}^4 \frobnorm{\Phi(t)}}{ \din^2} \sqrt{\frac{\dout \sr(X)}{\dhid}}
  \\
  &\leq
  \frac{ \eta L^2 \opnorm{X}^2 \frobnorm{\Phi(t)}}{ \din} \sqrt{\frac{\dout \sr(X)}{\dhid}} \\
  &\leq
  \frac{17 \eta L \sigma_{\min}^2(X)}{1024 \din} \cdot \frobnorm{\Phi(t)},
\end{align*}
by using the bound on $\eta$ and
after choosing $\dhid$ to satisfy
\[
  \dhid \gtrsim \dout \cdot \sr(X) \cdot L^2 \cdot \kappa^4.
\]
This completes the proof of the Lemma.
\end{proof}
Note that $\cprod{i} \leq 1$ is trivially true. On the other hand, we have the
following lower bound.
\begin{lemma}
    \label{lemma:cprod-i-lb}
    We have that
    $\cprod{i} \geq \frac{1}{4}$ for all $1 \leq i \leq L$.
\end{lemma}
\begin{proof}
    From the definition of $\cprod{i}$ in \cref{eq:c-prod-i} and~\cref{thm:weierstrass}, we have $\cprod{i} = \prod_{j \neq i}^{L} \left(1 - \frac{\eta \lambda}{d_j}\right) \geq
    1 - \sum_{j \neq i}^{L} \frac{\eta \lambda}{d_j}$. Moreover, we have that
    \begin{align*}
      \sum_{j \neq i} \frac{\eta \lambda}{d_j} &\leq
      \sum_{j \neq i} \frac{\din}{d_j} \cdot \frac{\lambda}{L \sigma_{\max}^2(X)} \\
                                               &\leq
                                               \frac{\lambda}{L \sigma_{\max}^2(X)} + \sum_{j \notin \set{i, 1}} \frac{\lambda}{L \sigma_{\max}^2(X)} \frac{\din}{\dhid} \\
                                               &\leq
      \frac{\gamma}{L \kappa^2} \sqrt{\frac{\din}{\dout}} \cdot \left(
        1 + \sum_{j \notin \set{i, 1}} \frac{1}{L^2}
      \right) \\
                                               &\leq
                                               \frac{\gamma}{L} \sqrt{\frac{\din}{\dout}} \cdot \left(1 + \frac{1}{L}\right) \\
                                               &\leq \frac{3 \gamma}{4} \sqrt{\frac{\din}{\dout}} \\
                                               &\leq \frac{3}{4},
    \end{align*}
    where the first inequality follows from the upper bound on $\eta$, the second inequality follows
    from the fact that $d_j = \dhid$ for all $j > 1$ and $d_1 = \din$, with $\dhid > \din$, the
    third inequality follows from the lower bound $\dhid \geq L \cdot \din$, the penultimate inequality
    follows from the fact the function $L \mapsto \frac{1}{L} \left(1 + \frac{1}{L}\right)$ is
    decreasing in $L$ and equal to $\frac{3}{4}$ for $L = 2$, and the last inequality
    follows from the assumption that $\din \leq \dout$ and $\gamma \leq 1$.
\end{proof}
Before we prove the event $\cA(t)$, we prove the following Lemma.
\begin{lemma}
  \label{lemma:Pt-extreme-eigenvalues}
  Under the event $\cB(t)$, the following holds:
  \begin{align*}
    \lambda_{\min}(P(t)) &\geq
    \left(\frac{9}{32}\right)^2 \cdot \frac{L \sigma_{\min}^2(X)}{\din}, \quad
    \lambda_{\max}(P(t)) \leq
    \frac{3 L \sigma_{\max}^2(X)}{\din}.
  \end{align*}
\end{lemma}
\begin{proof}
  From~\cref{lemma:cprod-i-lb}, it follows that $\cprod{i} \in [\frac{1}{4}, 1]$. Consequently \cref{lemma:P-k-spectrum} yields
  \begin{align*}
    \lambda_{\min}(P(t)) &\geq \frac{1}{4 d_{w}^{L-1} \din} \sum_{i = 1}^L \sigma_{\min}^2(W_{L:(i+1)}(t)) \sigma_{\min}^2(W_{(i-1):1}(t)Y) \\
                         &\geq
                         \frac{1}{4 d_w^{L-1} \din} \sum_{i = 1}^L \left(\frac{3}{4} d_{w}^{\frac{L - i}{2}}\right)^2 \left(\frac{3}{4} d_{w}^{\frac{i-1}{2}} \sigma_{\min}(X) \right)^2 \\
                         &= \frac{1}{4 d_{w}^{L-1} \din} \sum_{i = 1}^L \left( \frac{9}{16} \right)^2 d_{w}^{L - 1} \sigma_{\min}^2(X) \\
                         &\geq \frac{81 L \sigma_{\min}^2(X)}{1024 \din},
  \end{align*}
  where the second inequality uses~\cref{eq:event-B-prestop}, the assumption that the event $\cB(t)$ holds.
 
  Similarly, we have
  \begin{align*}
    \lambda_{\max}(P(t)) &\leq
    \frac{1}{d_{w}^{L-1} \din} \sum_{i = 1}^L \sigma_{\max}^2(W_{L:(i+1)}(t)) \sigma_{\max}^2(W_{(i-1):1}(t)Y) \\
                         &\leq
    \frac{1}{d_{w}^{L-1} \din} \sum_{i = 1}^L
    \left(\frac{5}{4} d_w^{\frac{L - i}{2}}\right)^2
    \left(\frac{5}{4} d_w^{\frac{i-1}{2}} \sigma_{\max}(X) \right)^2 \\
                         &\leq
    \frac{L \sigma_{\max}^2(X)}{\din} \cdot \left( \frac{25}{16} \right)^2 \\
                         &\leq
    \frac{3L \sigma_{\max}^2(X)}{\din},
  \end{align*}
  which completes the proof.
\end{proof}



\begin{lemma}
    \label{lemma:step-i-all-else-implies-A}
    For any $0 \leq t < \tau$, $\set{\set{\cA(j)}_{j < t}, \set{\cB(j)}_{j \leq t}} \implies \cA(t)$. Moreover, we have
    \begin{equation}
        \cA(t) \implies
        \frobnorm{\Phi(t+1)} \leq
        \left(
        1 - \frac{\eta L \sigma_{\min}^2(X)}{32\din}
        \right) \frobnorm{\Phi(t)}.
        \label{eq:At-strengthened-step-1}
    \end{equation}
\end{lemma}
\begin{proof}
    Recall the decomposition of the error from \cref{eq:error-evolution-I}:
    \[
        \mathrm{vec}(\Phi(t+1)) = (I - \eta P(t))
        \mathrm{vec}(\Phi(t)) +
        \mathrm{vec}(E(t)) +
        (\call - 1) \mathrm{vec}(U(t)).
    \]
    Taking norms on both sides and invoking the
    bound on $\frobnorm{E(t)}$ from~\cref{lemma:Bt-implies-bound-on-E}, we obtain
 \begin{align}
      \|\mathrm{vec}(\Phi(t+1))\| &=
      \|(I - \eta P(t)) \mathrm{vec}(\Phi(t)) +
      \mathrm{vec}(E(t)) +
    (\call - 1) \mathrm{vec}(U(t))\| \notag \\
                                     &\leq
                                     \opnorm{(I - \eta P(t))} \frobnorm{\Phi(t)}
                                     + \frobnorm{E(t)} + \abs{\call - 1} \frobnorm{U(t)} \notag \\
                                     &\leq \left(1 - \left(\frac{9}{32}\right)^2 \frac{\eta L \sigma^2_{\min}(X)}{\din} + 
                                     \frac{17 \eta L \sigma^2_{\min}(X)}{1024 \din}\right) \frobnorm{\Phi(t)}
      + \abs{\call - 1} \frobnorm{U(t)} \notag \\
      &\leq \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{16\din}\right) \frobnorm{\Phi(t)}
      + \left[\frac{(L - 1) \eta \lambda}{\dhid} + \frac{\eta \lambda}{\din}\right] \frobnorm{U(t)} \notag \\
      &\leq \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{16\din}\right) \frobnorm{\Phi(t)}
      + \frac{5 \eta \lambda}{2\din} \cdot \frobnorm{X} \sqrt{\frac{\dout}{\din}},
      \label{eq:loss-evolution-decomposition}
    \end{align}
    where the penultimate inequality follows from~\cref{lemma:one-minus-folded-product} and
    \eqref{eq:loss-evolution-decomposition} follows from
    \[
      d_{w} \geq L \din \implies \frac{(L-1) \eta \lambda}{d_w} \leq \frac{\eta \lambda}{\din},
      \quad \text{and} \quad
      \frobnorm{U(t)} \leq \frac{5}{4} \frobnorm{X} \sqrt{\frac{\dout}{\din}}
    \]
    If $t < \tau$, then from the definition of the stopping time $\tau$ in~\eqref{eq:tau def phi} and the identity $\lambda = \gamma\sigma_{\min}^2(X) \sqrt{\sfrac{m}{d}}$,
    it follows that
    \begin{align*}
      \frobnorm{\Phi(t+1)}  &\leq
      \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{16 \din}\right) \frobnorm{\Phi(t)}  +
      \frac{5 \eta \lambda \sqrt{\dout}}{2\din \sqrt{\din}} \frobnorm{X} \\
      &\leq
      \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{32 \din}\right) \frobnorm{\Phi(t)},
     \end{align*}
    which proves the inequality in~\eqref{eq:At-strengthened-step-1}.
\end{proof}


\begin{corollary}
    \label{corollary:length-of-step-1}
    With high probability, the stopping time $\tau$ satisfies
    \[
        \tau \leq
        \frac{32 \din}{\eta L \sigma_{\min}^2(X)}
        \log\left(
          \frac{L \sigma_{\min}^2(X)}{35 \lambda}
        \right).
    \]
\end{corollary}
\begin{proof}
    For any $t < \tau$, \cref{lemma:step-i-all-else-implies-A} implies
    \begin{align*}
        \frobnorm{\Phi(t)} &\leq
        \left(
        1 - \frac{\eta L \sigma_{\min}^2(X)}{32\din}
      \right) \frobnorm{\Phi(t-1)} \\ &\leq
        \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{32\din} \right)^{t} \frobnorm{\Phi(0)} \\
        &\leq
        \exp\left(
        -\frac{t \eta L \sigma_{\min}^2(X)}{32 \din}
    \right) \frobnorm{\Phi(0)} \\ &\leq
        \exp\left(
        -\frac{t \eta L \sigma_{\min}^2(X)}{32 \din}
      \right) \frac{11}{5} \sqrt{\frac{\dout}{\din}} \frobnorm{X},
    \end{align*}
    where the penultimate inequality follows from the identity $1 - x \leq \exp(-x)$ and
    the last inequality follows from~\cref{lemma:initial-regression-error}. Finally,
    we obtain
    \[
      t \geq \frac{32 \din}{\eta L \sigma^2_{\min}(X)}
        \log\left(
          \frac{L \sigma_{\min}^2(X)}{35 \lambda}
        \right) \implies
        \frobnorm{\Phi(t)} \leq
        \frac{80 \lambda \frobnorm{X}}{
        L \sigma_{\min}^2(X)
      } \sqrt{\frac{\dout}{\din}}
      = \frac{80 \gamma \frobnorm{X}}{L},
    \]
    which implies the stated upper bound on
    $\tau$.
\end{proof}
Next we will prove the event $\cC(t)$ (\cref{eq:event-C-prestop}).
\begin{lemma}
  \label{lemma:Bj-implies-Ct-phase-1}
  For any $t \leq \tau$, we have that
  $\set{\cA(j), \cB(j)}_{j < t} \implies \cC(t)$: 
  \begin{equation}
    \frobnorm{W_{i}(t) - \left(1 - \frac{\eta \lambda}{d_i}\right)^{t} W_{i}(0)}
    \lesssim
    \frac{\kappa^2 \sqrt{d \sr(X)}}{L} := R
    , \quad
    \text{for all $i = 1, \dots, L$.}
    \label{eq:Wi-travel-distance-improved}
  \end{equation}
\end{lemma}
\begin{proof}
Given \cref{lemma:difference-norm} we obtain the bound
  \begin{align*}
     &\frobnorm{W_{i}(t) - \left(1 - \frac{\eta \lambda}{d_i}\right)^t W_{i}(0)}\\
    &\leq \eta \sum_{j = 0}^{t-1} \left(1 - \frac{\eta \lambda}{d_i}\right)^{t - 1 - j}
    d_{w}^{-\frac{L - 1}{2}} \din^{-\frac{1}{2}}
    \frobnorm{W_{L:(i+1)}(j)\Phi(j) W_{(i-1):1}(j) Y} \\
    &\leq \eta \sum_{j = 0}^{t-1} \left(1 - \frac{\eta \lambda}{d_i}\right)^{t - 1 - j}
    d_{w}^{-\frac{L - 1}{2}} \din^{-\frac{1}{2}}
    \opnorm{W_{L:(i+1)}(j)} \frobnorm{\Phi(j)} \opnorm{W_{(i-1):1}(j) Y} \\
    &\leq
    \eta \sum_{j = 0}^{t-1} \left(1 - \frac{\eta \lambda}{d_i}\right)^{t-1-j}
    \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{32 \din}\right)^{j}
    \left( \frac{5}{4} d_{w}^{\frac{L-i}{2}} \right) \left(\frac{5}{4} d_{w}^{\frac{i-1}{2}} \sigma_{\max}(X)\right)
    \frac{\frobnorm{\Phi(0)}}{d_{w}^{\frac{L-1}{2}} \din^{\frac{1}{2}}} \\
    &\leq
    \eta \frac{25 \frobnorm{\Phi(0)} \opnorm{X}}{16 \sqrt{\din}}
    \sum_{j = 0}^{t - 1} \left(1 - \frac{\eta L \sigma_{\min}^2(X)}{32 \din}\right)^{j} \\
    &\leq
    \eta \frac{25 \frobnorm{\Phi(0)} \opnorm{X}}{16 \sqrt{\din}}
    \cdot \frac{32 \din}{\eta L \sigma_{\min}^2(X)} \\
    &=
    \frac{C \frobnorm{\Phi(0)} \opnorm{X} \sqrt{\din} }{L \sigma_{\min}^2(X)}\\
    & \leq \frac{C\sqrt{\dout} \cdot \frobnorm{X} \opnorm{X}}{L \sigma^2_{\min}(X)}\\
    & \leq \frac{\kappa^2 \sqrt{d \sr(X)}}{L},
  \end{align*}
  with $C=50$,
  which is independent of the choice of layer $i$. This completes the proof.
\end{proof}
Finally we prove the event $\cB(t)$ (\cref{eq:event-B-prestop}).
\begin{lemma}\label{lem:step1-b(t)-proof}
  \label{lemma:Ct-implies-Bt-phase-1}
    We have that $\cC(t) \implies
    \cB(t)$ for any $t \leq \tau$.
\end{lemma}
\begin{proof}
  To prove $\cB(t)$, we need to control the extremal singular values of several matrix products.

  \paragraph{Bounding $\opnorm{W_{j:i}(t)}$.}
  Fix any $i > 1$ and $j \geq i$. We start with the following decomposition:
  \begin{align*}
    W_{j:i}(t) &=
    \prod_{\ell = j}^{i} W_{\ell}(t) \\ 
               &=
               \prod_{\ell=j}^{i} \bigg( \Big( 1 - \frac{\eta \lambda}{d_{\ell}} \Big)^{t} W_{\ell}(0) + \underbrace{W_{\ell}(t) - \Big(1 - \frac{\eta \lambda}{d_{\ell}}\Big)^{t} W_{\ell}(0)}_{\Delta_{\ell}(t)} \bigg) \\
               &= \begin{aligned}[t]
      & W_{j:i}(0) \cdot \prod_{\ell = j}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t}  + \sum_{s = 1}^{j - i + 1} \sum_{i \leq k_{1}, \dots, k_{s} \leq j} \widetilde{W}_{j:(k_{s} + 1)}(0) \Delta_{k_s}(t) \dots \Delta_{k_1}(t) \widetilde{W}_{(k_1 - 1):i}(0),
      \end{aligned}
  \end{align*}
  using a slight abuse of the notation for $\widetilde{W}_{j:i}$ introduced in~\eqref{eq:tildeW-defn}:
  \[
    \widetilde{W}_{j:i}(0) = W_{j:i} \cdot \prod_{\ell = j}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t}.
  \]
  Continuing, we have the following upper bound:
  \begin{align*}
     \opnorm{\widetilde{W}_{j:(k_{s}+1)}(0) \Delta_{k_{s}}(t) \dots \Delta_{k_{1}}(t) \widetilde{W}_{(k_1 - 1):i}(0)} &\leq
    R^{s}
    \bigg[ \prod_{\substack{\ell = j\\\ell \notin \set{k_{1}, \dots, k_{s}}}}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t} \bigg]
    \left( \sqrt{\frac{L}{c}}\right)^{s + 1} \dhid^{\frac{j - i + 1 - s}{2}} \\
    &\leq
     \sqrt{\frac{L}{c}} \dhid^{\frac{j - i + 1}{2}} \cdot
    \left( \frac{C R \sqrt{L}}{\sqrt{d_w}} 
     \right)^{s} 
  \end{align*}
  Summing up over all possible $k_{1}, \dots, k_{s}$ for all possible $s = 1$ to $s = j - i + 1$,
  we have
  \begin{align*}
      & \sum_{s = 1}^{j - i + 1} \sum_{i \leq k_{1}, \dots, k_{s} \leq j} \opnorm{ \widetilde{W}_{j:(k_{s} + 1)}(0) \Delta_{k_s}(t) \dots \Delta_{k_1}(t) \widetilde{W}_{(k_1 - 1):i}(0)} \\
      & \leq
       \sqrt{\frac{L}{c}} \dhid^{\frac{ j - i + 1 }{2}}
      \sum_{s = 1}^{j - i + 1} \binom{j - i + 1}{s} \cdot \left( \frac{C R \sqrt{L}}{\sqrt{d_w}} 
     \right)^{s}  \\
      &\leq
       \sqrt{\frac{L}{c}} \dhid^{\frac{j - i + 1}{2}}
      \sum_{s = 1}^{j - i + 1}
     \left( \frac{C R \sqrt{L}}{\sqrt{d_w}} 
     \right)^{s} ,
  \end{align*}
  where the last inequality follows from the bound
  \(
    \binom{j - i + 1}{s} \leq \binom{L}{s} \leq L^{s}.
  \)
  Finally, by~\cref{lemma:truncated-geometric-series},
  \begin{align*}
    \sum_{s = 1}^{j - i + 1}
     \left( \frac{C R L^{3/2}}{\sqrt{d_w}} 
     \right)^{s} 
    & \lesssim
    \left( \frac{C R L^{3/2}}{\sqrt{d_w}} 
     \right) \cdot
    \frac{1}{1 -\left( \frac{C RL^{3/2}}{\sqrt{d_w}} 
     \right) }
    \leq \frac{1}{3},
  \end{align*}
  as long as we choose $\dhid$ such that
  \[
    \dhid \gtrsim R^2 L^{3} \asymp
    \frac{L \dout \kappa^4}{\sigma_{\max}^2(X)}
    \Leftrightarrow
    \left( \frac{ R L^{3/2}}{\sqrt{d_w}} 
     \right) \lesssim \frac{1}{4}.
  \]
  Putting everything together, we arrive at
  \begin{align*}
    \opnorm{W_{j:i}(t)} & \leq
    \opnorm{W_{j:i}(0)} \prod_{\ell = j}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t}
    + \frac{1}{3} \sqrt{\frac{L}{c}} \dhid^{\frac{j - i + 1}{2}} \leq
    \frac{4}{3} \sqrt{\frac{L}{c}} \dhid^{\frac{j - i + 1}{2}},
  \end{align*}
  using $(1 - \nicefrac{\eta \lambda}{d_{\ell}}) \leq 1$ and~\cref{lemma:norm-product-bounded} in the last inequality. This proves the
  first bound in the definition of $\cB(t)$. \\
  \paragraph{Bounding $\sigma_{\min}(W_{i:1}Y)$ and $\opnorm{W_{i:1}Y}$.}
  To control the singular values of $W_{i:1}(t) Y$ for $i < L$, we write
  \begin{align*}
      W_{i:1}(t) Y &=
      \Big( \prod_{ \ell = i }^{1} W_{\ell}(t)
      \Big) Y \\
      &=
      \bigg[ \prod_{\ell = i}^{1} 
      \bigg( \Big(1 - \frac{\eta \lambda}{d_{\ell}}\Big)^{t} W_{\ell}(0) +
      \Big[ W_{\ell}(t) - \Big(1 - \frac{\eta \lambda}{d_{\ell}}\Big)^{t} W_{\ell}(0)
      \Big] \bigg) \bigg] Y \\
      &=
      \begin{aligned}[t]
        & W_{i:1}(0) Y \cdot
        \prod_{\ell = i}^{1} \Big(1 - \frac{\eta \lambda}{d_{\ell}}\Big)^{t}  +
        \sum_{s = 1}^{i}
        \sum_{1 \leq k_{1}, \dots, k_{s} \leq i}
        \widetilde{W}_{i:(k_{s}+1)}(0)
        \Delta_{k_{s}} \dots \Delta_{k_1}
        \widetilde{W}_{(k_1 - 1):1}(0) Y
      \end{aligned}
  \end{align*}
  From the above decomposition and Weyl's inequality, it follows that
  \begin{equation}
      \abs{\sigma_{j}(W_{i:1}(t) Y) - \sigma_{j}(\widetilde{W}_{i:1}(0) Y)}
      \leq \sum_{s = 1}^{i} \sum_{(k_1, \dots, k_{s})} \opnorm{
        \widetilde{W}_{i:(k_{s}+1)}(0)
        \Delta_{k_{s}} \dots \Delta_{k_1}
        \widetilde{W}_{(k_{1} - 1):1}(0) Y
      }.
      \label{eq:singular-value-bound-phase-I}
  \end{equation}
  We now turn to bound the terms in the sum on the RHS of~\eqref{eq:singular-value-bound-phase-I}.
  First, note that
  \begin{align*}
    &\opnorm{
      \widetilde{W}_{i:(k_{s}+1)}(0)
      \Delta_{k_{s}} \dots \Delta_{k_1}
      \widetilde{W}_{(k_{1} - 1):1}(0) Y
    }\\
    &=
    \prod_{\substack{\ell = i\\\ell \notin \set{k_1, \dots, k_{s}}}}^{1}
    \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t}
    \opnorm{
      W_{i:(k_{s}+1)}(0) \Delta_{k_{s}} \dots \Delta_{k_{1}} W_{(k_{1} - 1):1}(0) Y
    } \\
    &\leq
    \opnorm{W_{i:(k_{s}+1)}(0) \Delta_{k_{s}} \dots \Delta_{k_{1}} W_{(k_{1}-1):1}(0) Y} \\
    &\leq
    \left(R \cdot 2 \sqrt{\frac{L}{c}}\right)^{s} \dhid^{\frac{i - k_{1} + 1 - s}{2}} \cdot
    \frac{6}{5} \dhid^{\frac{k_{1} - 1}{2}} \sigma_{\max}(X) \\
    &=
    \frac{6}{5} \cdot \left(R \cdot 2 \sqrt{\frac{L}{c}}\right)^{s} \dhid^{\frac{i - s}{2}} \sigma_{\max}(X) \\
    &=
    \frac{6}{5} \left(R \cdot 2 \sqrt{\frac{L}{c \dhid}}\right)^{s} \dhid^{\frac{i}{2}} \cdot \sigma_{\max}(X).
  \end{align*}
  Again, summing over all possible $(k_{1}, \dots, k_{s})$ for $s = 1$ to $i$
  yields the upper bound
  \begin{align*}
     \frac{6\sigma_{\max}(X) \cdot \dhid^{\frac{i}{2}}}{5} \cdot \sum_{s = 1}^{i} \binom{i}{s}
    \left(R \cdot 2 \sqrt{\frac{L}{c \dhid}}\right)^{s} 
    &\leq
    \frac{6 \sigma_{\max}(X) \cdot \dhid^{\frac{i}{2}}}{5}
    \sum_{s = 1}^{i} \left(R \cdot 2 i \sqrt{\frac{L}{c\dhid}}\right)^{s} \\
    &\leq
    \frac{6 \sigma_{\max}(X) \cdot \dhid^{\frac{i}{2}}}{5}
    \frac{R \cdot 2 L \sqrt{\frac{L}{c \dhid}}}{1 - R \cdot  2L \sqrt{\frac{L}{c \dhid}}} \\ 
    &\leq
    \frac{6 \sigma_{\max}(X) \cdot \dhid^{\frac{i}{2}}}{5}
    \frac{R \cdot 4 L \sqrt{\frac{L}{c \dhid}}}{3} \\
    &\leq
    c_{\mathsf{b}} \cdot \sigma_{\min}(X) \cdot \dhid^{\frac{i}{2}},
  \end{align*}
  valid for any $\dhid$ satisfying the following identity:
  \[
    \frac{R \cdot 8L}{5} \sqrt{\frac{L}{c \dhid}} \leq \frac{c_{\mathsf{b}}}{\kappa}
    \Leftrightarrow
    \dhid \gtrsim  \kappa^2 R^2 L^3 c_{\mathsf{b}}^2
    \asymp
    L \dout \cdot \frac{\kappa^6 \sr(X)}{c_{\mathsf{b}}^2},
  \]
   where $c_{\mathsf{b}}$ is a free parameter. Plugging
  the derived bound into~\eqref{eq:singular-value-bound-phase-I} yields
  \begin{align*}
    \sigma_{\max}(W_{i:1}(t)Y) &\leq
    \sigma_{\max}(\widetilde{W}_{i:1}(0) Y) + c_{\mathsf{b}} \cdot \sigma_{\min}(X) \cdot \dhid^{\frac{i}{2}} \\
                               &\leq
                               \sigma_{\max}(W_{i:1}(0) Y) + c_{\mathsf{b}} \cdot \sigma_{\max}(X) \cdot \dhid^{\frac{i}{2}} \\
                               &\leq
                               \left( \frac{6}{5} + c_{\mathsf{b}} \right) \sigma_{\max}(X) \cdot \dhid^{\frac{i}{2}} \\
                               &\leq
                               \frac{5}{4} \sigma_{\max}(X) \cdot \dhid^{\frac{i}{2}},
  \end{align*}
  after choosing $c_{\mathsf{b}} \leq \frac{1}{20}$. This proves the second bound
  in the definition of $\cB(t)$.


  Similarly, we have the following lower bound:
  \begin{align*}
    \sigma_{\min}(W_{i:1}(t) Y) &\geq
    \sigma_{\min}(\widetilde{W}_{i:1}(0) Y) - c_{\mathsf{b}} \cdot \sigma_{\min}(X) \cdot \dhid^{\frac{i}{2}} \\
                                &\geq
                                \sigma_{\min}(W_{i:1}(0) Y) \cdot \prod_{\ell = i}^{1} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t}
    - c_{\mathsf{b}} \cdot \sigma_{\min}(X) \cdot \dhid^{\frac{i}{2}} \\
                                &\geq
                                \left[ \left(1 - \frac{1}{20L}\right)^{i} \cdot \frac{4}{5} - c_{\mathsf{b}} \right] \cdot \sigma_{\min}(X) \cdot \dhid^{\frac{i}{2}} \\
                                &\geq
                                \left[ \left(1 - \frac{1}{20L}\right)^{L} \cdot \frac{4}{5} - c_{\mathsf{b}} \right] \cdot \sigma_{\min}(X) \cdot \dhid^{\frac{i}{2}} \\
                                &\geq
    \left[ \frac{19}{20} \cdot \frac{4}{5} - c_{\mathsf{b}} \right] \cdot \sigma_{\min}(X) \cdot \dhid^{\frac{i}{2}} \\
                                &\geq
                                \frac{3 \sigma_{\min}(X)}{4} \cdot \dhid^{\frac{i}{2}},
  \end{align*}
  where the third inequality follows from~\cref{lemma:contraction-factor-small-phase-1}, the second to last inequality follows from \cref{lemma:small-contraction-factor-lower-bound} and the last
  inequality follows from choosing $c_{\mathsf{b}} \leq \frac{1}{100}$. This proves the fourth bound
  in the definition of $\cB(t)$.\\
  
  \paragraph{Bounding $\sigma_{\min}(W_{L:i})$ and $\opnorm{W_{L:i}}$.}
  We now furnish upper and lower bounds for singular values of $W_{L:i}(t)$, for $i > 1$. By an analogous
  argument to the one employed for $W_{j:i}(t)$, when $j < L$, we arrive at
  \begin{align}
      W_{L:i}(t) &= {W}_{L:i}(0) \prod_{\ell = L}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t} +
      \sum_{s = 1}^{L - i} \sum_{i \leq k_{1}, \dots, k_{s} \leq L}
      \widetilde{W}_{L:(k_{s}+1)}(0) \Delta_{k_{s}} \dots \Delta_{k_{1}}
      \widetilde{W}_{(k_{1} - 1):i}(0)
      \label{eq:step-I-decomp-WL-i}
  \end{align}
  As before, we bound each summand on the RHS of~\eqref{eq:step-I-decomp-WL-i}. We have
  \begin{align*}
      \opnorm{\widetilde{W}_{L:(k_{s}+1)}(0) \Delta_{k_{s}} \dots \Delta_{k_{1}}
      \widetilde{W}_{(k_{1} - 1):i}(0)}  &\leq
     R^s
      \cdot \frac{6}{5} \dhid^{\frac{L - k_{s}}{2}}
      \cdot \left(2 \sqrt{\frac{L}{c}} \right)^{s}
      \dhid^{\frac{k_{s} - i + 1 - s}{2}} \\ &=
      \frac{6}{5} \cdot
      \left( \frac{R L^{1/2}}{\sqrt{d_w}} \right)^{s} \cdot \dhid^{\frac{L - i + 1}{2}}
  \end{align*}
  Adding up all the summands yields the upper bound
  \begin{align*}
       \sum_{s = 1}^{L - i} \sum_{i \leq k_{1}, \dots, k_{s} \leq L}
      \opnorm{\widetilde{W}_{L:(k_{s}+1)}(0) \Delta_{k_{s}} \dots \Delta_{k_{1}}
      \widetilde{W}_{(k_{1} - 1):i}(0)}  &\leq
      \frac{6 \dhid^{\frac{L - i + 1}{2}}}{5} \cdot \sum_{s = 1}^{L - i} \binom{L - i}{s}
      \left( \frac{R L^{1/2}}{\sqrt{d_w}} \right)^{s} \\
      &\leq
      \frac{6 \dhid^{\frac{L - i + 1}{2}}}{5} \cdot
      \sum_{s = 1}^{L - i}
      \left( \frac{R L^{3/2}}{\sqrt{d_w}} \right)^{s} \\ &\leq
      \frac{6 \dhid^{\frac{L - i + 1}{2}}}{5} \cdot
      \frac{R L^{3/2}}{\sqrt{d_w}},
  \end{align*}
  where the penultimate inequality follows from $\binom{k}{i} \leq k^{i}$ and the last
  inequality follows from~\cref{lemma:truncated-geometric-series}. Again, we introduce a free parameter
  $\bar{c}_{\mathsf{b}}$ and require
  \[ \frac{R L^{3/2}}{\sqrt{d_w}} = 
    \frac{\kappa^2 \sqrt{L \dout \sr(X)}}{\sqrt{ \dhid}} 
    \lesssim \bar{c}_{\mathsf{b}} \Leftrightarrow
    d_{w} \gtrsim \frac{L \dout \sr(X) \kappa^4}{\bar{c}_{\mathsf{b}}^2}.
  \]
  Returning to~\eqref{eq:step-I-decomp-WL-i}, we obtain the upper bound
  \begin{align*}
      \sigma_{\max}(W_{L:i}(t))
      &\leq \sigma_{\max}(W_{L:i}(0)) +
      \bar{c}_{\mathsf{b}} \cdot \dhid^{\frac{L - i + 1}{2}} \leq \left( \frac{6}{5} + \bar{c}_{\mathsf{b}} \right) \cdot \dhid^{\frac{L - i + 1}{2}} \leq \frac{5}{4} \cdot \dhid^{\frac{L - i + 1}{2}},
  \end{align*}
  choosing $\bar{c}_{\mathsf{b}} \leq \frac{1}{20}$. This proves the third inequality
  in $\cB(t)$; similarly by using ~\cref{lemma:contraction-factor-small-phase-1} and \cref{lemma:small-contraction-factor-lower-bound} we get the lower bound
  \begin{align*}
     \sigma_{\min}(W_{L:i}(t)) &\geq 
     \sigma_{\min}(W_{L:i}(0)) \cdot \left(1 - \frac{1}{20 L}\right)^{L - i + 1}
     - \bar{c}_{\mathsf{b}} \cdot \dhid^{\frac{L - i + 1}{2}} \\
     & \geq
    \sigma_{\min}(W_{L:i}(0)) \cdot \left(1 - \frac{1}{20 L}\right)^{L}
     - \bar{c}_{\mathsf{b}} \cdot \dhid^{\frac{L - i + 1}{2}} \\
     &\geq
     \left(0.95 \cdot \frac{4}{5} - \bar{c}_{\mathsf{b}}\right) \cdot \dhid^{\frac{L - i + 1}{2}} \\
     &\geq \frac{3}{4} \cdot \dhid^{\frac{L - i + 1}{2}},
  \end{align*}
  after choosing $\bar{c}_{\mathsf{b}} \leq \frac{1}{100}$. This proves the final inequality
  making up the event $\cB(t)$.
\end{proof}




\begin{lemma}
  \label{lemma:contraction-factor-small-phase-1}
    For any $t \leq \tau$, it follows that
    \[
        \left(1 - \frac{\eta \lambda}{d_i} \right)^t \geq 1 - \frac{1}{20L}.
    \]
\end{lemma}
\begin{proof}
    From~\cref{thm:weierstrass}, it follows that
    \[
        \left( 1 - \frac{\eta \lambda}{d_i} \right)^{t} \geq
        1 - \frac{t \eta \lambda}{d_i}
        \geq 1 - \frac{\tau \eta \lambda}{d_i}.
    \]
    From~\cref{corollary:length-of-step-1}, the quantity above is at least
    \begin{align*}
        1 - \frac{\tau \eta \lambda}{d_i} &\geq
        1 - \frac{32 \din}{\eta \sigma_{\min}^2(X) L} \log\left(
          \frac{L \sigma_{\min}^2(X)}{35 \lambda}
        \right) \cdot \frac{\eta \lambda}{d_i} \\
        &=
        1 - 32 \cdot \frac{\lambda}{L \sigma_{\min}^2(X)} \cdot
        \frac{\din}{d_i} \cdot
        \log\left(
            \frac{L \sigma_{\min}^2(X)}{35 \lambda}
        \right).
    \end{align*}
    We now argue that for small enough $\lambda$,
    the last term is at most $1 - \frac{1}{20L}$.
    Indeed,
    \[
        \frac{35 \lambda}{L \sigma_{\min}^2(X)} \log \left(
          \frac{L \sigma_{\min}^2(X)}{35 \lambda}
        \right)
        \leq \frac{1}{20}
        \Leftrightarrow
        20
        \leq
        \frac{20 \cdot 35 \lambda}{L \sigma_{\min}^2(X)} \exp\left(\frac{L \sigma_{\min}^2(X)}{20 \cdot 35 \lambda}\right).
    \]
    The above inequality is itself implied by the assumption that $\lambda \leq 
        \frac{L \sigma_{\min}^2(X)}{400 \cdot 35}$, which implies
    \[
        \frac{20 \cdot 35 \lambda}{L \sigma_{\min}^2(X)} \exp\left(\frac{L \sigma_{\min}^2(X)}{20 \cdot 35 \lambda}\right)
        \geq
        \frac{L \sigma_{\min}^2(X)}{20 \cdot 35 \lambda} \geq 20,
    \]
    using the inequality $x \exp(1 / x) \geq \frac{1}{x}$ for all $x > 0$ above.
    Therefore,
    \[
        1 - \frac{\tau \eta \lambda}{d_i} \geq
        1 - \frac{32}{35} \frac{35 \lambda}{L \sigma_{\min}^2(X)} \log\left(\frac{L \sigma_{\min}^2(X)}{35 \lambda}\right)
        \frac{\din}{d_i}
        \geq
        1 - \frac{32}{35 \cdot 20 L} \geq 1 - \frac{1}{20 L},
    \]
    which completes the proof of the claim.
\end{proof}
\begin{lemma}
    \label{lemma:small-contraction-factor-lower-bound}
    For any $L \geq 2$, we have that
    \[
        \left(1 - \frac{1}{20 L} \right)^{L}
        \geq 0.95.
    \]
\end{lemma}
\begin{proof}
    The function $x \mapsto \left(1 - \frac{1}{20x}\right)^{x}$ is monotone increasing for
    all $x \geq 1$. Therefore, 
    \[
        \left(1 - \frac{1}{20L}\right)^{L} \geq \left( 1 - \frac{1}{40} \right)^{2}
        = \left( \frac{39}{40} \right)^2 > 0.95.
    \]
\end{proof}

\begin{proof}[Proof of \cref{thm:step1-induction}]
We prove this theorem by induction. The base case follows from~\cref{lemma:step-i-properties-at-initialization}.
Now, suppose that all events $\cA(t)$, $\cB(t)$ and $\cC(t)$ hold up to some arbitrary index $t < \tau$. Then:
\begin{itemize}
  \item The event $\cC(t+1)$ holds by~\cref{lemma:Bj-implies-Ct-phase-1};
  \item The event $\cB(t+1)$ holds by~\cref{lemma:Ct-implies-Bt-phase-1} and the previous item;
  \item Finally, the event $\cA(t+1)$ holds by~\cref{lemma:step-i-all-else-implies-A} and the preceding item.
\end{itemize}
This completes the proof of the theorem.
\end{proof}


\subsection{Step 2: Regression error stays small}
\label{sec:subsec:Step 2: he error stays small}
In \cref{sec:subsec:Step 1: Rapid early convergence} we have shown that after $\tau$ iterations our regression error is small; namely,
$
  \frobnorm{\Phi(\tau)} \leq \frac{80 \gamma}{L} \frobnorm{X}.
$
We now want to show that the regression error remains small until at least iteration $T$.
In particular, we will show that
$
  \frobnorm{\Phi(t)} \leq C_1 \gamma\frobnorm{X}
$
for all $\tau \leq t \leq T$.
This we will show again by induction over the events stated in the following theorem.
\begin{theorem}\label{thm:step2-induction}
 Given $\tau$ defined in \eqref{eq:tau def phi} and $T$ defined in \eqref{eq:T def appendix}, then for all $\tau \leq t \leq T$ the following events hold with probability of at least $1-e^{-\Omega(d)}$ over the random initialization,
    \begin{subequations}
	\begin{align}
    \cA(t) & := \set*{\frobnorm{\Phi(t)} \leq C_1\gamma\frobnorm{X}}
		\label{eq:induction-a} \\
		\cB(t) & := \left\{ \begin{array}{rclrr}
			            \sigma_{\max}(W_{j:i}(t))   & \leq & \left( 2\sqrt{\frac{L}{c}} \right) d_w^{\frac{j-i+1}{2}}, \;\; \forall 1 < i \leq j < L                 \\
			            \sigma_{\max}(W_{i:1}(t) Y)  & \leq & \frac{9}{7} d_w^{\frac{i}{2}} \sigma_{\max}(X),  \;\; \forall 1 \leq i < L\\
                        \sigma_{\max}(W_{L:i}(t) ) &\leq& \frac{9}{7} d_w^{\frac{L-i+1}{2}} , \;\; \forall 1 < i \leq  L\\
                        \sigma_{\min}(W_{L:i}(t) ) & \geq& \frac{5}{7} d_w^{\frac{L-i+1}{2}}  , \;\; \forall 1 < i \leq  L
                        \end{array} \right\} \label{eq:induction-b} \\
      \cC(t) & := \set*{
        \frobnorm{W_{i}(t) - \left(1 - \frac{\eta \lambda}{d_i}\right)^{t - \tau} W_{i}(\tau)}
        \leq \Delta_{\infty}
      },
      \quad
      \Delta_{\infty} := C \kappa^2 \sqrt{\dout \sr(X)} \log(\dhid) .
      \label{eq:induction-d-after-tao}
	\end{align}
\end{subequations}
where $C_1 > 0$ is a universal constant and $c>0$ is the constant from \cref{lemma:norm-product-bounded}.
\end{theorem}
The events are similar to those in the first phase. In this phase, the difference is that we cannot guarantee anymore that the smallest singular value of $\sigma_{\min}(W_{i:1}Y)$ gets arbitrarily small. 
Note that $\cA(\tau), \cB(\tau)$ are true by~\cref{thm:step1-induction} and $\cC(\tau)$ is trivially true.
Throughout this section, we will require $\dhid$ to satisfy the following inequality
\begin{equation}
  \dhid \gtrsim \Delta_{\infty}^2 L^3 = \mathcal{O}\left(L^3 \kappa^4 \dout \sr(X) \log^2(\dhid) \right).
  \label{eq:dhid-lb-step-2}
\end{equation}

\paragraph{Proof of $\cC(t)$.}
We start by proving $\cC(t)$ given $\set{\cA(j), \cB(j)}_{j < t}$.
\begin{lemma}
Given that the set  of events
$\set{\cA(j), \cB(j)}_{j = \tau}^{t-1}$ for $\tau \leq t \leq T$ hold, then $\cC(t)$ holds:
\[
        \frobnorm{W_{i}(t) - \left(1 - \frac{\eta \lambda}{d_i}\right)^{t - \tau} W_{i}(\tau)}
        \lesssim \kappa^2 \sqrt{\dout \sr(X)} \log(\dhid)
      .
\]
\end{lemma}
\begin{proof}
  From~\cref{lemma:difference-norm} and the trivial bound $(1 - \nicefrac{\eta \lambda}{d_i}) \leq 1$, we deduce that
  \begin{align*}
    & \frobnorm{W_{i}(t) - \left(1 - \frac{\eta \lambda}{d_i}\right)^{t - \tau}
    W_{i}(\tau)} \\
    &\leq
    \eta \sum_{j = 0}^{t - \tau - 1}
    \left(1 - \frac{\eta \lambda}{d_i}\right)^{t - \tau - j}
    \frac{1}{\sqrt{\dhid^{L - 1} \din}}
    \opnorm{W_{L:(i+1)}(j + \tau)}
    \frobnorm{\Phi(j + \tau)} \opnorm{W_{(i-1):1}(j + \tau) Y} \\
    &\leq
    \frac{\eta}{\sqrt{\dhid^{L-1} \din}}
    \sum_{j = 0}^{t - \tau - 1}
    \frac{9}{7} \dhid^{\frac{L - i}{2}}
    \cdot \frobnorm{\Phi(j + \tau)} \cdot
    \frac{9}{7} \dhid^{\frac{i - 1}{2}} \sigma_{\max}(X) \\
    &\lesssim
    \frac{ \eta \opnorm{X}}{\sqrt{\din}}
    \sum_{j = 0}^{t - \tau - 1}
    \frobnorm{\Phi(j + \tau)} \\
    &\lesssim
    \frac{ \eta \opnorm{X}}{ \sqrt{\din}}
    \cdot T\gamma\frobnorm{X}  \\
    &\lesssim \kappa^2 \sqrt{\dout \sr(X)} \cdot \log(\dhid),
  \end{align*}
  where the second inequality follows from $\set{\cB(j)}_{j = \tau}^{t-1}$, the fourth inequality
  follows from $\set{\cA(j)}_{j = \tau}^{t-1}$, and the last inequality follows from the
  upper bound on $\eta$ and the identity $\frobnorm{X} \opnorm{X} = \opnorm{X}^2 \sqrt{\sr(X)}$.
\end{proof}
We next prove that $\cB(t)$ is implied by $\cC(t)$.
\begin{lemma}
  Fix $t \in [\tau, T]$. Then $\cC(t) \implies \cB(t)$.
\end{lemma}
\begin{proof}
As in the proof of \cref{lem:step1-b(t)-proof}, we have the decomposition
\begin{align*}
  W_{j:i}(t) & =
  \prod_{\ell = i}^j \left(\left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau} W_{\ell}^{(\tau)} + \left(W_{\ell}^{(t)} - \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau} W_{\ell}^{(\tau)} \right)\right) \\
  &=
  \begin{aligned}[t]
    & W_{j:i}(\tau) \prod_{\ell = j}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau} \\ 
    & + \sum_{i \leq k_1, \dots, k_s \leq j} \left[ \prod_{\ell \notin \set{k_1, \dots, k_{s}}} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau} \right] W_{j:(k_s + 1)}(\tau) \Delta_{k_s}
		\dots \Delta_{k_1} W_{(k_1 - 1):i}(\tau),
  \end{aligned}
\end{align*}
where each term satisfies $\opnorm{\Delta_{k_{i}}} \leq \Delta_{\infty}$. Therefore,
\begin{align}
  \opnorm[\Big]{W_{j:i}(t) - W_{j:i}(\tau) \prod_{\ell = j}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}} & \leq
  \sum_{s = 1}^{j-i+1} \binom{j-i+1}{s} \left(\Delta_{\infty}\right)^{s} \left(2 \sqrt{\frac{L}{c}}\right)^{s+1} d_w^{\frac{j-i+1-s}{2}}                                                  \notag \\
		                                     & \leq
		\left(2\sqrt{\frac{L}{c}}\right) d_w^{\frac{j-i+1}{2}}
    \sum_{s = 1}^{j-i} \left(\frac{C \Delta_{\infty} L^{3/2}}{\sqrt{\dhid}}\right)^{s} \notag \\
		                                     & \leq
       \left(2\sqrt{\frac{L}{c}}\right) d_w^{\frac{j-i+1}{2}}
		\frac{1}{31}, 
	\end{align}
  using~\cref{lemma:truncated-geometric-series} and the assumed bound~\eqref{eq:dhid-lb-step-2}.
  Therefore,
  \begin{align*}
    \opnorm{W_{j:i}(t)} &\leq \opnorm{W_{j:i}(\tau)} \prod_{\ell = j}^{i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}
  + \opnorm[\Big]{W_{j:i}(t) - W_{j:i}(\tau) \prod_{i \leq \ell \leq j} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}} \\
    & \leq  \left(2\sqrt{\frac{L}{c}}\right) d_w^{\frac{j-i+1}{2}} + \left(2\sqrt{\frac{L}{c}}\right) d_w^{\frac{j-i+1}{2}}
		\frac{1}{31} \\
    &\leq  \left(2\sqrt{\frac{L}{c}}\right) d_w^{\frac{j-i+1}{2}},
  \end{align*}
  relabeling $c$ appropriately in the last step to absorb the $1 + \frac{1}{31}$ term.
  This proves the first bound in the event $\cB(t)$. Continuing with $W_{L:i}(t)$, we have
  \begin{align}
    \opnorm[\Big]{W_{L:i}(t) - W_{L:i}(\tau) \prod_{i \leq \ell \leq L} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}}
    & \leq
    \sum_{s = 1}^{L-i+1} \binom{i}{\ell}\left(\Delta_{\infty}\right)^{s}
    \left(2 \sqrt{\frac{L}{c}}\right)^{s} \dhid^{\frac{L-i+1-s}{2}} \frac{5}{4}
            \notag \\
                                          & \leq
      \frac{5}{4}  d_w^{\frac{L-i+1}{2}} 
      \sum_{s = 1}^{L-i+1} \left(\frac{C \Delta_{\infty} L^{3/2}}{\sqrt{\dhid}}\right)^{s} \notag \\
                                          & \leq
      \frac{1}{63}  \frac{5}{4}  d_w^{\frac{L-i+1}{2}}.
  \end{align}
  Again using the bound from~\eqref{eq:dhid-lb-step-2}, we deduce that
  \begin{align*}
    \opnorm{W_{L:i}(t)} &\leq \opnorm{W_{L:i}(\tau)} + \opnorm[\Big]{W_{L:i}(t) - W_{L:i}(\tau) \prod_{i \leq \ell \leq L}\left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}} \\
                      &  \leq  \frac{5}{4} d_w^{\frac{L-i+1}{2}} + \frac{1}{63} \frac{5}{4}  d_w^{\frac{L-i+1}{2}} \\ 
                      & = \frac{80}{63}  d_w^{\frac{L-i+1}{2}} \\
                      & \leq \frac{9}{7}  d_w^{\frac{L-i+1}{2}},
  \end{align*}
  which proves the second bound from the event $\cB(t)$, as well as
\begin{align*}
  \sigma_{\min}(W_{L:i}(t)) &\geq  \sigma_{\min}(W_{L:i}(\tau)) \prod_{i \leq \ell \leq L} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau} - 
  \opnorm[\Big]{W_{L:i}(t) - W_{L:i}(\tau) \prod_{i \leq \ell \leq L}\left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}} \\
                            & \geq  \frac{3}{4} d_w^{\frac{L-i+1}{2}} \cdot \prod_{i \leq \ell \leq L} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}
                            - \frac{1}{63} \frac{3}{4} d_w^{\frac{L-i+1}{2}} \\
                            &\geq \frac{3}{4} \dhid^{\frac{L - i + 1}{2}} \left[
                              \exp\Big(-2 \cdot \frac{(t - \tau)\eta \lambda}{d_{\ell}}\Big)^{L - i + 1} - \frac{1}{63}
                            \right] \\
                            &\geq \frac{3}{4} \dhid^{\frac{L - i + 1}{2}}
                            \left[
                              \exp\left(-2 \cdot \frac{L \cdot \log(\dhid) \cdot \din}{d_{\ell}}\right) - \frac{1}{63}
                            \right] \\
                            &\geq
                            \frac{3}{4} \dhid^{\frac{L - i + 1}{2}} \cdot \frac{60}{63} \\
                            &= \frac{5}{7} \dhid^{\frac{L - i + 1}{2}},
\end{align*}
using $1 - x \geq \exp(-2x)$ in the third inequality, $t - \tau \leq \nicefrac{ \log(\dhid) \cdot \din}{\eta \lambda}$
in the penultimate inequality, the fact that $d_{\ell} = \dhid$ for $\ell > 1$,
and choosing $d_{w} \geq \nicefrac{4L \log(\dhid) \cdot \din}{\log(\frac{63}{61})}$ in the last
inequality. This proves the third bound from $\cB(t)$.

Finally, we have the upper bound
\begin{align*}
   \opnorm{W_{i:1}(t)Y} &\leq \opnorm{W_{i:1}(\tau)Y} \prod_{1 \leq \ell \leq i} \left(1-\frac{\eta \lambda}{d_{\ell}}\right)^{t-\tau} +
  \opnorm[\Big]{W_{i:1}(t)Y - W_{i:1}(\tau)Y \prod_{1 \leq \ell \leq i} \left(1 - \frac{\eta \lambda}{d_{\ell}}\right)^{t - \tau}} \\
                        & \leq  \frac{5}{4} \dhid^{\frac{i}{2}} \opnorm{X}   +
  \frac{1}{63} \frac{5}{4}  \dhid^{\frac{i}{2}}  \sigma_{\max}(X) \\
                        &= \frac{80}{63}  \dhid^{\frac{i}{2}} \opnorm{X} \\
                        &\leq \frac{9}{7}  d_w^{\frac{i}{2}} \opnorm{X}.
\end{align*}
This proves the last bound from the event $\cB(t)$.
\end{proof}
\paragraph{Proof of $\cA(t)$.} We show in the following that the events $\cB(t), \cA(t)$ imply $\cA(t+1)$. Let us start by stating the Lemma.
\begin{lemma}
  For any $\tau \leq t \leq T$, we have that $\set{\set{\cA(j)}_{\tau \leq j \leq t-1}, \set{\cB(j)}_{\tau \leq j \leq t}} \implies \cA(t)$.
\end{lemma}
\begin{proof}
From~\cref{lemma:cprod-i-lb}, it follows that $\cprod{i} \in [\frac{1}{4}, 1]$. From this and $\mathcal{B}(t)$, it follows that
\begin{align*}
    \lambda_{\min}(P(t)) &\geq \frac{1}{4 d_w^{L-1}m} \sum_{i=1}^L  \sigma^2_{\min}(W_{L:(i+1)}(t)) \sigma^2_{\min}(W_{i-1:1}(t)Y)  \\
    &\geq \frac{1}{4 d_w^{L-1}m}  \sigma^2_{\min}(W_{L:2}(t)) \sigma^2_{\min}(Y)\\
    &\geq \frac{1}{4 d_w^{L-1}m}  d_w^{L-1} \left(\frac{5}{7}\right)^2 (1-\delta)^2 \sigma_{\min}^2(X)\\
    &\geq  \frac{1}{4 d_w^{L-1}m}  \frac{1}{2} d_w^{L-1} \frac{8}{10} \sigma_{\min}^2(X)\\
    &\geq \frac{ \sigma_{\min}^2(X) }{10 \din},
\end{align*}
given $\delta = \frac{1}{10}$.
Similarly, for the upper bound on $\lambda_{\max}(P(t))$, we get
\begin{align*}
    \lambda_{\max}(P(t)) &\leq \frac{1}{d_w^{L-1}m} \sum_{i=1}^L \sigma^2_{\max}(W_{L:(i+1)}(t)) \sigma^2_{\max}(W_{i-1:1}(t)Y)  \\
    &\leq \frac{1}{d_w^{L-1}m} \sum_{i=1}^L \left(\frac{9}{7}\right)^2 d_w^{L-i} \left(\frac{9}{7}\right)^2 d_w^{i-1} \sigma_{\max}^2(X)\\ 
    &\leq \frac{2 L  \sigma_{\max}^2(X) }{m}.
\end{align*}
Similarly to the first $\tau$ iterations, we obtain a bound on the higher-order terms:
\begin{align*}
    & \frobnorm{E(t)Y} \\
    &= \frobnorm{d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}}E_0(t)Y} \\
    & \leq d_w^{-\frac{L-1}{2}}m^{-\frac{1}{2}} \sum_{\ell=2}^L \eta^{\ell} \binom{L}{\ell} \left(1-\frac{\eta \lambda}{d_w}\right)^{L-\ell} \left(2 \sqrt{\frac{L}{c}} \right)^{\ell -1} d_w^{\frac{L-\ell}{2}} \left(\frac{3}{ \sqrt{m}}\frobnorm{\Phi(t)}\opnorm{X} \right)^{\ell} \opnorm{Y}\\
    &\leq
                 \frac{C \eta L^{\frac{3}{2}} \opnorm{X}^2 \frobnorm{\Phi(t)}}{\din}
                 \sum_{\ell = 1}^{L - 1} \left(
                   \frac{C \eta L^{\frac{3}{2}} \opnorm{X} \frobnorm{\Phi(t)}}{(\din d_w)^{1/2}}
                 \right)^{\ell} \\
    &\lesssim
  \frac{\eta^2 L^3 \opnorm{X}^3 \frobnorm{\Phi(t)}^2}{\din^{3/2} \dhid^{1/2}} \\
  &\lesssim
  \frac{\eta^2 L^{3} \lambda \opnorm{X}^4 \frobnorm{\Phi(t)}}{\din^2 \sigma_{\min}^2(X)} \sqrt{\frac{\dout}{\dhid}}
  \\
  &\leq
  \frac{\eta L^{2} \opnorm{X}^2  \frobnorm{\Phi(t)}}{\din} \sqrt{\frac{\din}{\dhid}} \\
  &\leq
  \frac{3 \eta \sigma_{\min}^2(X)}{80 \din} \cdot \frobnorm{\Phi(t)},
\end{align*}
where the second inequality follows by imitating the
argument in~\cref{lemma:Bt-implies-bound-on-E}, the third inequality follows from
\cref{lemma:truncated-geometric-series} and the inequality
 \begin{align*}
   \frac{C \eta L^{3/2} \opnorm{X} \frobnorm{\Phi(t)}}{\sqrt{\din \dhid}} \overset{(\eta \leq \nicefrac{\din}{L \sigma_{\max}^2(X)})}&{\leq}
     \frac{C L^{1/2} \sqrt{\din} \frobnorm{\Phi(t)}}{\sqrt{\dhid} \sigma_{\max}(X)} \\ 
                                                                          \overset{(\cA(t))}&{\leq}
    \frac{C \lambda L^{1/2} \sqrt{\sr(X)}}{\sigma_{\min}^2(X)} \sqrt{\frac{\dout}{\dhid}} \\
   \overset{(\lambda \lesssim L \sigma_{\min}^2(X))}&{\lesssim}
     C L^{3/2} \sqrt{\frac{\dout \sr(X)}{\dhid}} \\ 
   \overset{(\dhid \gtrsim L^3 \dout \sr(X))}&{\leq} \frac{1}{2},
 \end{align*}
the second to last inequality uses that $\eta \leq \frac{m}{L \sigma^2_{\max}(X)}$ and that $\lambda \leq \sigma^2_{\min}(X) \sqrt{\frac{m}{d}}$
and the last inequality
follows from $\dhid \gtrsim \din L^4 \kappa^4$.
Therefore, we arrive at the following bound on the regression error:
\begin{align*}
  \frobnorm{\mathrm{vec}(\Phi(t+1))} &= \opnorm{(I-\eta P(t))} \frobnorm{\Phi(t)} + \frobnorm{E(t)} + \abs{\call - 1} \frobnorm{U(t)}\\
                                 &\leq \left( 1- \frac{ \eta \sigma^2_{\min}(X)}{10\din} +\frac{3 \eta \sigma_{\min}^2(X)}{80\din}  \right) \frobnorm{\Phi(t)} + \abs{\call - 1} \frobnorm{U(t)}\\
    & \leq \left( 1- \frac{\eta \sigma^2_{\min}(X)}{16\din} \right)  \frobnorm{\Phi(t)} + \left[\frac{(L-1)\eta \lambda}{d_w} +\frac{\eta \lambda}{\din} \right] \left(\frac{5}{4}  \sqrt{\frac{d}{m}}  \right) \frobnorm{X} \\
   & \leq \left( 1- \frac{\eta \sigma^2_{\min}(X)}{16m} \right)  \frobnorm{\Phi(t)} +
   \frac{5 \sqrt{d} \eta \lambda}{2\din \sqrt{\din}} \frobnorm{X}.
\end{align*}
We can split the remaining analysis into two cases:
\begin{enumerate}
  \item If $\frac{40  \lambda \frobnorm{X}}{\sigma_{\min}^2(X)} \sqrt{\frac{\dout}{\din}} \leq \frobnorm{\Phi(t)} \leq \frac{80  \lambda \frobnorm{X}}{\sigma_{\min}^2(X)} \sqrt{\frac{\dout}{\din}}$, then
  \begin{align*}
    \frobnorm{\Phi(t+1)} &\leq \left( 1-  \frac{ \eta \sigma^2_{\min}(X)}{16\din} \right) \frobnorm{\Phi(t)}+ \frac{5  \sqrt{\dout} \eta \lambda}{2\din \sqrt{\din}} \frobnorm{X}\\
                       &\leq \frobnorm{\Phi(t)} -\left( \frac{ \eta \sigma^2_{\min}(X)}{16\din} \right)  \frac{40  \sqrt{\dout}\lambda \frobnorm{X}}{ \sigma_{\min}^2(X) \sqrt{\din}}
                       + \frac{5  \sqrt{\dout} \eta \lambda}{2\din \sqrt{\din}} \opnorm{X}\\
                       &\leq \frobnorm{\Phi(t)} -\left( \frac{40 \eta \lambda \frobnorm{X} \sqrt{\dout}}{ 16\din \sqrt{\din}} \right)  + \frac{5   \sqrt{d} \eta \lambda}{2 \din \sqrt{\din}} \frobnorm{X}\\
      &= \frobnorm{\Phi(t)}
      - \frac{\eta \lambda \frobnorm{X} \sqrt{\dout}}{\din \sqrt{\din}}
      \left[\frac{5}{2}- \frac{5}{2}\right] \\
      & \leq \frobnorm{\Phi(t)}.
  \end{align*}
  \item On the other hand, if
  $\frobnorm{\Phi(t)} \leq \frac{40 \lambda \frobnorm{X}}{\sigma_{\min}^2(X)} \sqrt{\frac{\dout}{\din}}$, then
  \begin{align*}
     \frobnorm{\Phi(t+1)}
      &\leq \left( 1- \frac{\eta \sigma^2_{\min}(X)}{16m} \right) \frobnorm{\Phi(t)} + \frac{5 \sqrt{d} \eta \lambda}{2\din \sqrt{\din}} \frobnorm{X}\\
      &\leq \left( 1- \frac{\eta \sigma^2_{\min}(X)}{16m} \right)
      \frac{40 \lambda \frobnorm{X}}{\sigma_{\min}^2(X)} \sqrt{\frac{\dout}{\din}}
      + \frac{5  \sqrt{d} \eta \lambda}{2\din \sqrt{\din}} \frobnorm{X}\\
      & \leq \frac{40 \lambda \frobnorm{X}}{ \sigma_{\min}^2(X)}
      \sqrt{\frac{\dout}{\din}} + \frac{5 \sqrt{\dout} \eta \lambda}{2\din \sqrt{\din}} \frobnorm{X}\\
      & \leq \frac{41\lambda \frobnorm{X}}{\sigma_{\min}^2(X)}
\sqrt{\frac{\dout}{\din}} \\
&\leq \frac{80\lambda \frobnorm{X}}{\sigma_{\min}^2(X)}
\sqrt{\frac{\dout}{\din}} ,
  \end{align*}
  where the penultimate inequality follows from the requirement $\eta \leq \frac{2m}{5\sigma_{\min}^2(X)}$.
  In particular, since we assumed $\cA(t)$ holds, which means $\frobnorm{\Phi(t)} \leq \frac{80 \lambda \frobnorm{X}}{\sigma^2_{\min}(X)} \sqrt{\frac{\dout}{\din}}$ then this also holds for $\frobnorm{\Phi(t+1)}$. 
\end{enumerate}
This shows that the event $\cA(t+1)$ holds.
\end{proof}
\begin{proof}[Proof of \cref{thm:step2-induction}]
    Taking the above Lemmas together, we have shown that the base case for all three events $\cA, \cB, \cC$ holds. Further we have shown by induction that $\set{\cA(j), \cB(j)}_{\tau \leq j<t} \implies \cC(t+1)$, $\cC(t) \implies \cB(t)$ and $\cA(t), \cB(t) \implies \cA(t+1)$. From this, the theorem follows.
\end{proof}

\subsection{Step 3: Convergence off the subspace}
\label{sec:subsec:Step 3: Convergence off the subspace}
In this section, we show that the off-subspace error depends on the hidden width. 
The on-subspace components of the weights act on the image of the subspace; the off-subspace components are in the orthogonal complement of the on-subspace components.
More formally, the projection onto the subspace is defined as
 $P^{\perp}_{\range(Y)} := YY^{\ddag}$. 
To determine the behavior off the subspace, we must consider the projection onto ${\range(Y)}^{\perp}$, which we denote $P_{\range(Y)}^{\perp}.$
Note that
\begin{align*}
	W_{1}(t+1) P_{\range(Y)}^{\perp} & =
	W_{1}(t)\left(1 - \frac{\eta \lambda}{\din}\right) P_{\range(Y)}^{\perp} -
	\eta \cdot \frac{1}{\sqrt{d_w^{L-1}m}} W_{L:2}^{\T} \Phi(t) Y^{\T} P_{\range(Y)}^{\perp}                    \\
	                        & =
	\left(1 - \frac{\eta \lambda}{\din}\right) W_{1}(t)P_{\range(Y)}^{\perp}                                    \\
	                        & = \left(1 - \frac{\eta \lambda}{\din}\right)^{t+1} W_{1}(0) P_{\range(Y)}^{\perp}
\end{align*}
using $Y^{\T} P_{\range(Y)}^{\perp} = Y^{\T} P_{\range(Y)^{\perp}} = Y^{\T} P_{\ker(Y^{\T})} = 0$.
By event $\cB(t)$ from~\cref{eq:induction-b}, we have
\begin{align}
	\opnorm{W_{L:1}(t) P_{\range(Y)}^{\perp}} & \leq
    \opnorm{W_{L:2}(t)} \opnorm{W_{1}(t) P_{\range(Y)}^{\perp}}
 \notag \\
    &\leq
    \frac{9}{7} \dhid^{\frac{L - 1}{2}}
    \cdot
	\left(1 - \frac{\eta \lambda}{\din}\right)^t \opnorm{W_{1}{(0)} P_{\range(Y)}^{\perp}}
	\label{eq:W-perp-bound-refine-i}
\end{align}
Normalizing on both sides we obtain
\begin{align}
	\opnorm[\bigg]{\frac{1}{\sqrt{\dhid^{L-1} \din}}W_{L:1}(t) P_{\range(Y)}^{\perp}} & \leq  2 \left(1 - \frac{\eta \lambda}{\din}\right)^t \frac{1}{\sqrt{\din}} \opnorm{W_{1}(0) P_{\range(Y)}^{\perp}}.
\end{align}
We now turn to bounding $\opnorm{W_{1}(0) P_{\range(Y)}^{\perp}}$. 
Let $V_{\perp} \in O(m, m-s)$ be a matrix whose columns span ${\range(Y)}^{\perp}$;
by orthogonal invariance of the operator norm and
the Gaussian distribution, we have
\begin{align*}
    \opnorm{W_{1}(0) P_{\range(Y)}^{\perp}} &=
    \opnorm{W_{1}(0) V_{\perp} V_{\perp}^{\T}} \\
    &=
    \opnorm{W_{1}(0) V_{\perp}},
\end{align*}
where $W_{1}(0) V_{\perp} \in \Rbb^{\dhid \times (\din - s)}$ is a matrix with standard Gaussian elements; indeed,
\[
    W_{1}{(0)} V_{\perp} = \begin{bmatrix}
        W_{1}(0) (V_{\perp})_{:, 1} &
        \dots &
        W_{1}(0) (V_{\perp})_{:, \din - s}
    \end{bmatrix} \overset{(d)}{=}
    \begin{bmatrix}
        \bar{g}_1 & \dots \bar{g}_{\din - s}
    \end{bmatrix},
    \quad \text{where} \;\;
    \bar{g}_i \iid \cN(0, I_{\dhid}).
\]
Therefore by~\citep[Corollary 7.3.3]{Ver18}, the following holds
with probability $1-2\exp(-c \dhid^2)$:
\begin{align*}
	\opnorm{W_{1}(0) P_{\range(Y)}^{\perp}} & \leq
    2\sqrt{\dhid} + \sqrt{\din - s}
    \lesssim \sqrt{\dhid}.
\end{align*}
By the preceding displays,
\begin{align}
	\opnorm[\bigg]{\frac{1}{\sqrt{\dhid^{L-1} \din}} W_{L:1}(T) P_{\range(Y)}^{\perp}}
	 & \lesssim \left(1 - \frac{\eta \lambda}{\din}\right)^{T}
	\cdot \sqrt{\frac{\dhid}{\din}}         \notag \\
	 & =  \left(1 - \frac{\eta \lambda}{\din}\right)^{T}
	\exp\left(\frac{1}{2} \log(\dhid / \din)\right) \notag \\
	 & \leq
	 \exp\left(-\frac{T \eta \lambda}{\din}
    + \frac{1}{2} \log(\dhid)
	\right)                                                       \notag \\
    &=  \dhid^{-\frac{3}{2}},
	\label{eq:W-perp-bound-refined}
\end{align} 
where the second inequality follows from the identity $1 - x \leq \exp(-x)$,
the penultimate inequality follows from the choice of $T = \frac{2\log(\dhid)\sqrt{dm}}{\eta\gamma \sigma_{\min}^2(X)}$ from \cref{eq:T def appendix} and the choice of $\lambda = \gamma \sigma_{\min}^2(X) \sqrt{\frac{m}{d}}$.


\subsection{Robustness at test time}
\label{sec:subsec:Robustness at test time}
Suppose we have trained our model for $T$ steps and $W_{L:1}(T) = W_{L}(T) \cdots W_{1}(T)$ are the weights of the model at the end of training.
In what follows, we suppress the iteration index $T$ for simplicity. By~\cref{theorem:main-informal},
\begin{subequations}
\begin{align}
    \frobnorm{W_{L:1}Y-X} &\leq
    C_1 \gamma \frobnorm{X};
    \label{eq:robust-on} \\
    \opnorm{W_{L:1} P_{\range(Y)}^{\perp}} &\leq
    \dhid^{-C_2}
    \label{eq:robust-off}
\end{align}
\end{subequations}
for universal constants $C_1, C_2 > 0$. Suppose that we receive a new test pair $(x, y)$ satisfying
\begin{align}
    y=Ax+\epsilon, \;\; x \in \range(R), \;\;
    \epsilon \sim \cN(0, \sigma^2 I_{m}).
    \label{eq:new-test-point}
\end{align}
The next corollary characterizes the estimation error $\norm{W_{L:1}y - x}$.
\begin{corollary}
    Let $(W_1, \dots, W_{L})$ be the weight matrices of a deep linear 
    network trained for $T$ iterations in the setting of~\cref{theorem:main-informal}.
    Consider a new data point $(x, y)$ satisfying~\eqref{eq:new-test-point}.
    Then the output of the network, $W_{L:1}y$, satisfies
    \begin{equation}
        \norm{W_{L:1}y - x} \lesssim
        \gamma \kappa \sqrt{\sr(X)}  + \frac{1}{\dhid^{C_2}}
        + \sigma \sqrt{s}
    \end{equation}
    with probability of at least $1 - c_1 \exp(-c_2 \dout) - \exp(-c_3 s)$.
    Conversely, let $(W^{\lambda=0}_1(t),...,W^{\lambda=0}_L(t))$ be the weight matrices of a deep linear network trained in the setting of~\cref{theorem:main-informal} with $\lambda = 0$. Then for any $\beta >0$, there exists an iteration $T$ such that the reconstruction error $\frobnorm{W^{\lambda=0}_{L:1}(t)Y-X} \leq \beta \frobnorm{X}$ for all $t>T$. Moreover, with probability at least $1 - c_1 \exp(-c_2 \dout) - \exp(-c_3 s)$, the test error satisfies
    \[
     \norm{W^{\lambda=0}_{L:1}(t)y-x} \gtrsim \sigma \left(
    \sqrt{\frac{\dout(\din - s)}{\din}} - \sqrt{s}
    \right) -
    \beta \kappa \sqrt{\sr(X)} \norm{y}.
    \]
\end{corollary}
\begin{proof}
    We start by bounding the error between the ``oracle'' solution mapping $XY^{\dag}$ and the trained neural network. We have
    \begin{align*}
        \opnorm{ W_{L:1}-XY^{\dag}} &=  \opnorm{ W_{L:1}YY^{\dag}-XY^{\dag}+  W_{L:1}(I-YY^{\dag})}\\
        & \leq \opnorm{ W_{L:1}Y-X} \opnorm{Y^{\dag}}+ \opnorm{ W_{L:1}(I-YY^{\dag})}\\
        & \leq \frobnorm{ W_{L:1}Y-X} \opnorm{Y^{\dag}}+ \opnorm{ W_{L:1}P_{\range(Y)}^{\perp}} \\
        & \leq
        C_1 \gamma \frobnorm{X} \opnorm{Y^{\dag}} + \dhid^{-C_2} \\
        & = \frac{C_1 \gamma \sqrt{\sr(X)} \opnorm{X}}{\sigma_{\min}(Y)}
        + \dhid^{-C_2} \\
        & \leq \frac{C_1 \gamma \sqrt{\sr(X)} \sigma_{\max}(X)}{(1 - \delta) \sigma_{\min}(X)}
        + \dhid^{-C_2} \\
        & \lesssim \gamma \kappa \sqrt{\sr(X)} + \dhid^{-C_2},
    \end{align*}
    where the third inequality follows from~\cref{eq:robust-on,eq:robust-off},
    the second equality follows from the definition of $\sr(X)$ and the identity
    $\opnorm{Y^{\dag}} = \frac{1}{\sigma_{\min}(Y)}$, the penultimate inequality
    follows from~\cref{assumption:rip} and the last inequality follows by
    substituting $\delta = \frac{1}{10}$. Consequently, we have
    \begin{align}
        \norm{W_{L:1}y - x} &=
        \norm{(W_{L:1} - XY^{\dag})y + XY^{\dag}y - x} \notag \\
        &\leq
        \opnorm{W_{L:1} - XY^{\dag}} \norm{y} +
        \norm{XY^{\dag}y - x} \notag \\
        &\lesssim
        \left(\gamma \kappa \sqrt{\sr(X)} + \dhid^{-C_2}\right) \norm{y}
        + \norm{XY^{\dag} y - x}.
        \label{eq:robust-proof-decomposition}
    \end{align}
    We now argue that the second term in~\eqref{eq:robust-proof-decomposition}
    is bounded by $\sigma \sqrt{s}$.
    Recall that $Y = AX$, $X = RZ$ for some $Z \in \mathbb{R}^{s \times n}$ with full row rank, and $x = Rz$ for some $z \in \Rbb^{s}$.
    Therefore, we have
    \begin{align*}
        XY^{\dag} y &=
        RZ (ARZ)^{\dag} y \\
        &=
        R ZZ^{\dag} (AR)^{\dag} y \\
        &=
        R (AR)^{\dag} (AR) z + R(AR)^{\dag} \epsilon \\
        &= Rz + R(AR)^{\dag} \epsilon \\
        &= x + R(AR)^{\dag} \epsilon.
    \end{align*}
    The second equality in the preceding display follows from the fact
    that $(M_1 M_2)^{\dag} = M_2^{\dag} M_1^{\dag}$ when $M_1$ and $M_2$ are full column-rank and full row-rank respectively; indeed, here $M_1 \equiv AR$ is
    full column-rank by~\cref{assumption:rip} and $M_2 \equiv Z$ is full
    row-rank by assumption. Similarly, the third and fourth inequalities
    follow from the full row rankness and full column rankness of $Z$ and $AR$, respectively. Consequently, we have the bound
    \begin{align*}
        \norm{XY^{\dag} y - x} &=
        \norm{R(AR)^{\dag} \epsilon} =
        \norm{(AR)^{\dag} \epsilon},
    \end{align*}
    since $R$ is a matrix with orthogonal columns. We now write
    \[
        AR = \bar{U} \bar{\Sigma} \bar{V}^{\T}, \quad \text{where} \;\;
        \bar{U} \in O(m, s), \;
        \bar{V} \in O(s), \;
        1 - \delta \leq \Sigma_{ii} \leq 1 + \delta,
    \]
    for the economic SVD of $AR$, where the inequalities on the singular
    values follow from~\cref{assumption:rip}. In particular,
    \begin{align*}
        \norm{(AR)^{\dag} \epsilon} &=
        \norm{\bar{V} \bar{\Sigma}^{-1} \bar{U}^{\T} \epsilon}
        \leq
        \frac{1}{\sigma_{\min}(\bar{\Sigma})}
        \norm{\bar{U}^{\T} \epsilon}
        \lesssim
        \norm{\bar{U}^{\T} \epsilon},
    \end{align*}
    using $\delta = \frac{1}{10}$ in the last inequality.
    Finally, by standard properties of the multivariate normal distribution,
    \[
        \bar{U}^{\T} \epsilon \sim \cN(0, \sigma^2 I_{s}) \implies
        \norm{\bar{U}^{\T} \epsilon} \lesssim
        \sigma \sqrt{s},
    \]
    with probability at least $1 - \exp(-c s^2)$~\citep[Theorem 3.1.1]{Ver18},
    for a universal constant $c > 0$.
    Returning to~\eqref{eq:robust-proof-decomposition}, we conclude that
    \[
        \norm{W_{L:1}y - x} \lesssim 
        \left( \gamma \kappa \sqrt{\sr(X)} + \dhid^{-C_2} \right) \norm{y}
        + \sigma \sqrt{s},
    \]
    with probability at least $1 - c_1 \exp(-c_2 \dout) - \exp(-c_3 s)$. This proves the first of the two claims.

    We now prove the lower bound for the reconstruction error for the weights $W_{i}^{\lambda = 0}(t)$. For simplicity, we write $\bar{W}_{L:1} := W_{L}^{\lambda = 0}(t) \dots W_{1}^{\lambda = 0}(t)$ and suppress the dependence on $t$. We obtain
    \begin{align}
        \norm{\bar W_{L:1}y-XY^{\dag}y} &=
        \norm{\bar W_{L:1}(I-YY^{\dag})y+(\bar W_{L:1}Y-X)Y^{\dag} y} \notag \\
        & \geq \norm{\bar W_{L:1}P_{\range(Y)}^{\perp}y}- \frac{\frobnorm{(\bar W_{L:1}Y-X)}} {\sigma_{\min}(Y)} \norm{y} \notag \\
        & \geq \norm{\bar W_{L:1}P_{\range(Y)}^{\perp}\epsilon}- \frac{\beta \frobnorm{X}} {\sigma_{\min}(Y)} \norm{y} \notag \\
        & \gtrsim  \sqrt{\frac{\dout}{\din}} \norm{P_{\range(Y)}^{\perp} \epsilon}- \frac{\beta \sqrt{\sr(X)} \sigma_{\max}(X)} {(1  - \delta) \sigma_{\min}(X)} \norm{y} \notag \\
        & \gtrsim \sigma \sqrt{\frac{\dout (\din - s)}{\din}}  - \beta \kappa \sqrt{\sr(X)} \norm{y},
        \label{eq:test-error-lb-1}
    \end{align}
    where the first inequality follows from the reverse triangle inequality and the identity $\norm{Y^{\dag}} = \nicefrac{1}{\sigma_{\min}(Y)}$,
    the second inequality follows by the assumption that $t > T$,
    the third inequality follows from~\cref{assumption:rip}, the definition of $\sr(X)$ and~\cref{lemma:wide-gaussian-prod-tail} combined with property $\cC(t)$ from~\cref{sec:subsec:Step 1: Rapid early convergence}, and the last inequality follows from the fact that
    \begin{equation}
        \label{eq:projected-noise}
        \norm{P_{\range(Y)}^{\perp} \epsilon}
        \gtrsim \sigma \sqrt{\din - s}, \quad
        \text{with probability at least $1 - \exp(-c(\din - s)^2)$.}
    \end{equation}
    To see~\eqref{eq:projected-noise}, let $V_{\perp} \in O(\din, \din - s)$ be a matrix whose columns span $\range(Y)^{\perp}$
    such that $P_{\range(Y)}^{\perp} = V_{\perp} V_{\perp}^{\T}$.
    By orthogonal invariance of the Gaussian distribution,
    \[
        V_{\perp}^{\T} \epsilon \overset{(d)}{=}
        \cN(0, \sigma^2 I_{\din - s}).
    \]
    Moreover, by orthogonal invariance of the Euclidean norm,
    \[
        \norm{P_{\range(Y)}^{\perp} \epsilon} =
        \norm{V_{\perp}^{\T} \epsilon}.
    \]
    Combining the two preceding displays with~\citep[Theorem 3.1.1]{Ver18} yields the inequality~\eqref{eq:projected-noise}.

Altogether, we get the following lower bound
\begin{align*}
    \norm{\bar W_{L:1}(y)-x}&= \norm{\bar W_{L:1}y-XY^{\dag}y+XY^{\dag}y-x} \\
    &\geq \norm{(\bar W_{L:1}-XY^{\dag}) y} - \norm{XY^{\dag}y-x}\\
    & \gtrsim
    \sigma \sqrt{\frac{\dout(\din - s)}{\din}}
    - \beta \kappa \sqrt{s} \norm{y} -
    \norm{XY^{\dag} Ax - x} - \norm{XY^{\dag} \epsilon} \\
    & \gtrsim \sigma \sqrt{\frac{\dout (\din - s)}{\din}} - \beta \kappa \sqrt{\sr(X)} \norm{y} - \sigma \sqrt{s} \\
    &= \sigma \left(
    \sqrt{\frac{\dout(\din - s)}{\din}} - \sqrt{s}
    \right) -
    \beta \kappa \sqrt{\sr(X)} \norm{y},
\end{align*}
where the first inequality follows from the reverse triangle inequality, the second inequality follows from
the bound~\eqref{eq:test-error-lb-1} and the last inequality follows from the fact that $XY^{\dag} Ax = x$ and the upper bound $\norm{XY^{\dag} \epsilon} \lesssim \sigma \sqrt{s}$, which follows from standard properties of the multivariate Gaussian distribution. This lower bound holds
with probability at least $1 - c_1 \exp(-c_2 \dout) - \exp(-c_3 s)$.
This concludes the proof of the lower bound.
\end{proof}
