
\documentclass[11pt]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[numbers]{natbib}
\usepackage{xfrac}
\usepackage{nicefrac}
\usepackage{aligned-overset}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[colorlinks=true, linkcolor=blue!50!black, citecolor=green!50!black]{hyperref}
\usepackage{enumitem}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm} 


\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\Crefname{assumption}{Assumption}{Assumptions}

\usepackage[textsize=tiny]{todonotes}

\input{macros}

\usepackage[margin=1.1in]{geometry}
\captionsetup{labelfont=bf}
\usepackage{authblk}

\title{Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay}
\author[1,2]{Hannah Laus\thanks{Corresponding author. Email: \href{mailto:hannah.laus@tum.de}{\texttt{hannah.laus@tum.de}}}}
\author[3]{Suzanna Parkinson}
\author[4]{Vasileios Charisopoulos}
\author[1,2,5]{Felix Krahmer}
\author[3,4,6,7,8,9]{Rebecca Willett}
\affil[1]{Department of Mathematics, Technical University of Munich}
\affil[2]{Munich Center for Machine Learning (MCML)}
\affil[3]{Committee on Computational and Applied Mathematics, University of Chicago}
\affil[4]{Data Science Institute, University of Chicago}
\affil[5]{Munich Data Science Institute (MDSI), Technical University of Munich}
\affil[6]{Department of Computer Science, University of Chicago}
\affil[7]{Department of Statistics, University of Chicago}
\affil[8]{NSF-Simons National Institute for Theory and Mathematics in Biology (NITMB)}
\affil[9]{NSF-Simons National Institute for AI in the Sky (SkAI)}
\renewcommand{\Affilfont}{\fontsize{9}{10.8}\footnotesize}
\date{}

\allowdisplaybreaks

\begin{document}


\maketitle

\begin{abstract}


	Machine learning methods are commonly used to solve inverse problems, wherein an unknown signal must be estimated from few measurements generated via a known acquisition procedure. In particular, neural networks perform well empirically but have limited theoretical guarantees. In this work, we study an underdetermined linear inverse problem that admits several possible solution mappings. A standard remedy (e.g., in compressed sensing) establishing uniqueness of the solution mapping is to assume knowledge of latent low-dimensional structure in the source signal. We ask the following question: do deep neural networks adapt to this low-dimensional structure when trained by gradient descent with weight decay regularization? We prove that mildly overparameterized deep linear networks trained in this manner converge to an approximate solution that accurately solves the inverse problem while implicitly encoding latent subspace structure. To our knowledge, this is the first result to rigorously show that deep linear networks trained with weight decay automatically adapt to latent subspace structure in the data under practical stepsize and weight initialization schemes. Our work highlights that regularization and overparameterization improve generalization, while overparameterization also accelerates convergence during training.


\end{abstract}

\section{Introduction}
\label{sec:submission}
\input{sections/introduction}

\subsection{Related work}
\label{sec:subsec:related-work}
\input{sections/related-work}

\subsection{Notation and basic constructions}
\label{sec:subsec:notation}
\input{sections/notation}

\section{Main result}
\label{sec:main-result}
\input{sections/main-result}

\subsection{Proof sketch}
\label{sec:subsec:proof-sketch}
\input{sections/proof-sketch}

\subsection{Robustness to noisy test data}
\label{sec:subsec:robustness}
\input{sections/robustness}

\section{Numerical experiments}
\label{sec:numerics}
\input{sections/numerics}

\section{Limitations and future directions}
\label{sec:discussion}

\paragraph{Depth and generalization.}
Our experiments in~\cref{fig:errors-by-depth} suggest that depth is beneficial for both the regression and the ``off-subspace'' errors: larger depth, at least up to a certain point, leads to faster convergence. This phenomenon is not covered by our main theoretical result, but constitutes an interesting direction for future work.

\vspace*{-1em}

\paragraph{Near-singular matrices and conditioning.}
Our main result (\cref{theorem:main-informal}) does not provide meaningful insights
for \emph{approximately} low-rank data; e.g., inputs $X$ that can
be decomposed as the sum of a well-conditioned low-rank component and a full-rank component with relatively small singular values, a pervasive property in data science applications~\cite{UT19}.
For such inputs, it is plausible that gradient descent with weight decay is able to rapidly converge to a solution mapping that provides a good approximation
to the ``low-rank'' component of the input $X$. We leave such an investigation
to future work.

\vspace*{-1em}

\paragraph{Adaptivity of deep non-linear networks.}
Our experiments in~\cref{fig:nonlinear} suggest that weight decay can lead to robust solutions beyond the simple linear inverse problem setting.
In particular, a natural next step would be to study the training dynamics of $\ell_2$-regularized
gradient descent for deep networks with several linear layers and a ReLU layer
(as considered in~\cite{parkinson2023linear}) under the assumption that the input
data is generated by the ``union-of-subspaces'' model used in~\cref{fig:nonlinear}.

\section*{Acknowledgements}
SP gratefully acknowledges the support of the NSF Graduate Research Fellowship Program NSF DGE-2140001. VC and RW gratefully acknowledge the support of NSF DMS-2023109, the NSF-Simons National Institute for Theory and Mathematics in Biology (NITMB) through NSF (DMS-2235451) and Simons Foundation (MP-TMPS-00005320), and the Margot and Tom Pritzker Foundation. FK gratefully acknowledges the support of the German Science Foundation (DFG) in the context of the priority program Theoretical Foundations of Deep Learning
(project KR 4512/6-1). HL and FK gratefully acknowledge the support of the Munich Center for Machine Learning (MCML).




\bibliographystyle{unsrtnat}
\bibliography{main}

\newpage
\appendix
\onecolumn
\section{Main result and proof}
\label{sec:mainresultandproof-appendix}
\input{sections/appendix-mainresult-and-proof}

\section{Auxiliary results}
\input{sections/auxillary-lemmas}

\section{Information on numerics for the union of subspaces model}
\label{sec:appendix numerical description}
\input{sections/appendix-numerics}

\end{document}
