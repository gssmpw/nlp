% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\newcommand{\di}[1]{\textcolor{red}{#1}}
\newcommand{\meng}[1]{\textcolor{blue}{MD:#1}} 
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsmath,amssymb,bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{fact}{Fact}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\usepackage{subcaption}
\usepackage{booktabs}

\title{Towards User-level Private Reinforcement Learning with Human
Feedback}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{Jiaming Zhang \\
%  Renmin University of China \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
  %\texttt{zhangjiaming2002@ruc.edu.cn} \\\And
  %Second Author \\
  %Affiliation / Address line 1 \\
 % Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
 % \texttt{email@domain} \\}

\author{
  \textbf{Jiaming Zhang\textsuperscript{1,2,*}},
  \textbf{Mingxi Lei\textsuperscript{4,*}},
  \textbf{Meng Ding\textsuperscript{2,4}},
  \textbf{Mengdi Li\textsuperscript{2,3}},
\\
  \textbf{Zihang Xiang\textsuperscript{2,3}},
  \textbf{Difei Xu\textsuperscript{2,3}},
  \textbf{Jinhui Xu\textsuperscript{4}},
  \textbf{Di Wang \textsuperscript{2,3}} 
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
\\
  \textsuperscript{1}Renmin University of China,
 \textsuperscript{2}Provable Responsible AI and Data Analytics (PRADA) Lab,
\\  \textsuperscript{3}King Abdullah University of Science and Technology,
 \textsuperscript{4} State
University of New York at Buffalo \\
 \textsuperscript{*} Equal Contributions \\
   \small{
    \textbf{Correspondence:} \href{mailto:di.wang@kaust.edu.sa}{di.wang@kaust.edu.sa}
  }
%  \textsuperscript{5}Affiliation 5
}

\begin{document}
\maketitle

\begin{abstract}
Reinforcement Learning with Human Feedback (RLHF) has emerged as an influential technique, enabling the alignment of large language models (LLMs) with human preferences. Despite the promising potential of RLHF, how to protect user preference privacy has become a crucial issue. Most previous work has focused on using  differential privacy (DP) to protect the privacy of individual data. However, they have concentrated primarily on item-level privacy protection and have unsatisfactory performance for user-level privacy, which is more common in RLHF. This study proposes a novel framework, AUP-RLHF, which integrates user-level label DP into RLHF. We first show that the classical random response algorithm, which achieves an acceptable performance in item-level privacy, leads to suboptimal utility when in the user-level settings. We then establish a lower bound for the user-level label DP-RLHF and develop the AUP-RLHF algorithm, which guarantees $(\varepsilon, \delta)$ user-level privacy and achieves an improved estimation error. Experimental results show that AUP-RLHF outperforms existing baseline methods in sentiment generation and summarization tasks, achieving a better privacy-utility trade-off. 
\end{abstract}


\section{Introduction}
The advent of large language models (LLMs) has significantly transformed the field of artificial intelligence, leading to widespread adoption and application in diverse domains~\cite{hong2024dissecting,yangmodel,chengmulti,zhang2024locate}, such as legal \cite{chalkidis2023retrieval,wu2023precedent}, medical \cite{thirunavukarasu2023large,garcia2024medical,yuan2024continued}, and coding assistant \cite{ross2023programmer,nam2024using,kazemitabaar2024codeaid}. A notable advancement in this area is the incorporation of Reinforcement Learning from Human Feedback (RLHF), a paradigm that improves the performance and alignment of language models by integrating human preferences and feedback into the training process~\cite{ouyang2022}.

Currently, there is a substantial body of research on RLHF, with methods broadly categorized into reward model-based and reward model-free approaches. Although SOTA methods in the academic domain typically lean toward reward model-free approaches, such as DPO~\cite{dpo}, systems such as ChatGPT~\cite{chatgpt} and Claude~\cite{claude} employ a reward model combined with reinforcement learning algorithms (e.g. PPO). This distinction arises because DPO tends to produce out-of-distribution biased responses, and its performance is susceptible to distribution shifts between model output and preference data set~\cite{dpobetter}. In experimental evaluations, PPO consistently outperforms DPO in tasks such as dialogue generation and code generation~\cite{dpobetter}.


Although RLHF has shown considerable promise, it also introduces substantial privacy concerns, particularly with regard to the
identity and preferences of human labelers. For example, in the financial domain, investment advisors can annotate and provide feedback on LLM-generated investment recommendations. These preference labels may encompass sensitive information such as individual investment strategies, risk preferences, and financial objectives. 
In addition, LLMs have the potential to infer individual user preferences, interests, or personal attributes by analyzing queries and interactions posed. Such inferences may lead to the generation of targeted advertisements, personalized suggestions, or other customized content, which in turn could inadvertently expose sensitive aspects of the user's private life~\cite{privacysurvey,llmprivacy1,llmprivacy2,hu2024differentially,huang2024private}.  


To address the aforementioned privacy concerns, Differential Privacy (DP) \cite{dp1}, which quantifies privacy protection by adding noise to ensure the anonymity of individual data in statistical analysis,  has become a common approach, effectively protecting user privacy by introducing noise into the data.  Recently, there have been an increasing number of studies focusing on DP-RLHF~\cite{rewarddp,userlevelrdplhf,rlhfdp23,PROPSdp}, demonstrating the feasibility of DP to RLHF. 
 However, traditional DP algorithms in RLHF focus on item-level privacy, where they ensure that the inclusion or exclusion of any single data point does not significantly affect the modelâ€™s output. In practical scenarios, a user typically contributes multiple pieces of data, rendering item-level privacy insufficient. Specifically, existing methods offer privacy guarantees that deteriorate as user participation increases or blindly introduce excessive noise, leveraging the group property of differential privacy, which severely impacts the performance of the model in deployment.~\cite{userleveldp1}. For example, in Section \ref{sec:RR}, we will show that the Randomized Response mechanism, which is the canonical way of protecting label privacy and achieves the nearly optimal rate in the item-level case~\cite{rewarddp}, will significantly degrade its utility in the user-level privacy scenario. Thus, there is a pressing need for developing user-level label DP-RLHF with a better privacy-utility tradeoff.
 

To tackle the challenges outlined above, motivated by the current developments of user-level DP supervised learning \cite{geyer2017differentially,bassily2023user,userleveldp1,liu2024user,ghazi2024user,zhao2024huber}, we propose a novel framework, namely AUP-RLHF, which satisfies user-level label DP and achieves a smaller estimation error. The key intuition is the use of the average gradient of the loss with respect to each user's preferences for parameter updates in the DP-SGD algorithm~\cite{abadi2016deep,su2024faster,shen2023differentially,xiao2023theory,hu2022high}. However, unlike the existing DP-SGD designed for item level, we leverage outlier removal and adaptive sampling processes to handle the high sensitivity to ensure that the gradient exhibits concentrated properties. Then, at each step of the SGD update, we add noise related to the concentrated parameters only to ensure privacy, which is significantly small.  Our contributions are as follows:

\begin{enumerate}
    \item We first demonstrate that under canonical assumptions, the classical randomized response method, which outperforms well in the item-level setting, exhibits a large estimation error \( O({\frac{d\sqrt{m}}{ \sqrt{n}\epsilon}}) \) in the user-level setting. Specifically, when the contribution \( m \) for each user is large, randomized response sacrifices utility to provide uniform privacy protection in the privacy-utility trade-off, rendering the algorithm unsuitable.  We also establish a lower bound \( \Omega\left(\frac{d}{\sqrt{nm}}+\frac{\sqrt{d}}{\sqrt{m} n \varepsilon }\right) \) for user-level DP-RLHF. 
    
    \item  To close the gap between the upper and lower bounds, we develop AUP-RLHF that satisfies \((\varepsilon, \delta)\)-user-level label privacy, with an upper bound of $O\left({\frac{d\sqrt{d}}{\sqrt{m}n \varepsilon}}\right)$ for the estimation error.  Additionally, we extend the algorithm to the \(K\)-wise case, where the upper bound of the estimation error remains \(O\left({\frac{K^2d\sqrt{d}}{\sqrt{m}n \varepsilon}}\right)\).
    
    \item We empirically validate AUP-RLHF through experiments on commonly studied tasks. We conducted controlled sentiment generation and summarization experiments. We show that, across base models of varying sizes and different privacy parameter configurations, AUP-RLHF  consistently outperforms the other user-level DP baselines. 
\end{enumerate}
\section{Related Work}
\noindent \textbf{User-level DP learning. }
User-level DP has garnered increasing attention due to its privacy protection, which aligns more closely with real-world scenarios. Several studies have explored various tasks of user-level DP, such as mean estimation~\cite{userleveldp1}, empirical risk minimization~\cite{userleveldp1}, and stochastic convex optimization~\cite{userleveldp2,userleveldp}. \citealt{userleveldp1} first introduced user-level DP, providing stricter privacy protection by safeguarding the entire contribution of users, based on a novel private mean estimation algorithm. \citealt{userleveldp2} further investigated how carefully selected first-order optimization methods can achieve optimal utility under local DP conditions. \citealt{userleveldp}, on the other hand, ensured query concentration through above-threshold and adaptive sampling techniques, thereby releasing the minimal number of user assumptions while still achieving optimal utility. However, none of them has considered RLHF. In this paper, we build the first theoretical results on user-level (label) DP-RLHF. 

\noindent \textbf{Privacy-preserving in RLHF.}
\citealt{kwise} provided a theoretical framework for RLHF under clean data conditions, demonstrating the properties of MLE and pessimistic MLE. However, due to privacy leakage issues \cite{li2023multi}, MLE cannot be applied directly. Building on this, \citealt{rewarddp} introduced a theoretical framework for incorporating DP into RLHF, designing an unbiased loss function based on random responses to achieve DP.   \citealt{PROPSdp} proposed a novel multi-stage mechanism that privately aligns the model with labels from previous stages combined with random responses. However, as we will show later, random responses cannot be easily extended to more practical user-level settings, which results in poor utility. Meanwhile, \citealt{rlhfdp23} utilized DP-SGD across the three stages of RLHFâ€”Supervised Fine-Tuning, Reward Model Learning, and Alignment with PPOâ€”to ensure privacy preservation. \citealt{userlevelrdplhf} explored the application of Group Privacy and User-wise DP-SGD to achieve user-level DP in RLHF. However, they provide only experimental results and lack rigorous theoretical guarantees. Our experiments will show that our proposal method has better performance than these methods.  


%Prior research either did not consider user-level settings or lacked rigorous theoretical proof for such settings.

\section{Preliminaries}

\subsection{RLHF}
% 
Let \( \mathcal{D} = (s_{i}, a_{i}^{0}, a_{i}^{1}, y_{i})_{i=1}^{n} \) be a dataset of \( n \) samples, where each sample has a state \( s_{i} \in \mathcal{S} \) (e.g., prompt given to a language model), two actions \( a_{i}^{0}, a_{i}^{1} \in \mathcal{A} \) (e.g., two responses from the language model), and label \( y_{i} \in \{0,1\} \) indicating which action is preferred by human experts. We assume that the state \( s_{i} \) is first sampled from some fixed distribution \( \rho \). The pair of actions \( (a_{i}^{0}, a_{i}^{1}) \) are then sampled from some joint distribution (i.e., a behavior policy) \( \mu \) conditioned on \( s_{i} \). Finally, the label \( y_{i} \) is sampled from a Bernoulli distribution conditioned on \( (s_{i}, a_{i}^{0}, a_{i}^{1}) \), i.e., for \( l \in \{0,1\} \),
% \fontsize{10.5pt}{12pt}\selectfont
\begin{align*}
&\mathbb{P}_{\theta^{*}}\left[y_{i}=l \mid s_{i}, a_{i}^{0}, a_{i}^{1}\right]\\&= 
\frac{\exp\left(r_{\theta^{*}}\left(s_{i}, a_{i}^{l}\right)\right)}{
    \exp\left(r_{\theta^{*}}\left(s_{i}, a_{i}^{0}\right)\right) + 
    \exp\left(r_{\theta^{*}}\left(s_{i}, a_{i}^{1}\right)\right)
} .
\end{align*}
Here, \( r_{\theta^{*}}(\cdot, \cdot) \) denotes the reward model parameterized by an unknown parameter \( \theta^{*} \), which we aim to estimate from the dataset \( \mathcal{D} \). This model is known as the Bradley-Terry-Luce (BTL) model. In this work, we specifically focus on a linear reward model as follows:
$$r_{\theta^{*}}(s, a) = \phi(s, a)^{\top} \theta^{*},$$
where \( \phi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^{d} \) is some known and fixed feature map. For instance, such a \( \phi \) can be constructed by removing the last layer of a pre-trained language model, and in that case, \( \theta^{*} \) corresponds to the weights of the last layer. With this model, one can equivalently write the probability of sampling \( y_{i}=1 \) given \( (s_{i}, a_{i}^{0}, a_{i}^{1}) \) as
\begin{align*}
    &\mathbb{P}_{\theta^{*}}\left[y_{i}=1 \mid s_{i}, a_{i}^{0}, a_{i}^{1}\right] \\
    &= \sigma\left(\left(\phi\left(s_{i}, a_{i}^{1}\right) - \phi\left(s_{i}, a_{i}^{0}\right)\right)^{\top} \theta^{*}\right),
\end{align*}
where \( \sigma(z) = \frac{1}{1 + e^{-z}} \) refers to the sigmoid function. Let \( x_{i} = \phi(s_{i}, a_{i}^{1}) - \phi(s_{i}, a_{i}^{0}) \) denote the differential feature of actions \( a_{i}^{1} \) and \( a_{i}^{0} \) at state \( s_{i} \). For any \( \theta \in \mathbb{R}^{d} \), we use this differential feature to define the predicted probability of the label $y_{i}$ given $x_i$ as follows (we omit dependence on \( \theta \) for brevity):
% the predicted probabilities of a label \( y_{i} \) given \( x_{i} \) as (we omit dependence on \( \theta \) for brevity)
% {\footnotesize
% \[
% \]
% }
\begin{align*}
    p_{i, 1} &:= \mathbb{P}_{\theta}\left[y_{i}=1 \mid x_{i}\right] = \sigma\left(x_{i}^{\top} \theta\right), \\
    p_{i, 0} & := 1 - p_{i, 1}.
\end{align*}

\subsection{Differential Privacy}
We use the notation \( Z_i := (s_{i,j}, a_{i,j}^{0}, a_{i,j}^{1}, {y}_{i,j})_{j=1}^{m}  \) to represent  user $i$'s contributions, and \\$z_i:= (s_{i,j}, a_{i,j}^{0}, a_{i,j}^{1}, {y}_{i,j})$  to represent  user $i$'s $j$th contributions. We use capital Z to denote one user and z to denote one item. We consider user-level DP, which protects all of a user's contributions, meaning that the output of an algorithm \( M \) operating on a dataset with $n$ users (thus, $mn$ samples in total) \( D=\left(Z_i\right)_{i=1}^{n}  \)  is `indistinguishable' when a single user's contributions in \( D \) are altered. A formal definition is provided below.

\begin{definition}\textbf{(User-Level Differential Privacy).}
A mechanism \( \mathcal{M}:\left(\mathcal{Z}^{m}\right)^{n} \rightarrow \mathbb{R}^{d} \) is \((\varepsilon, \delta)\) user-level differentially private, if for any neighboring datasets \( \mathcal{D}, \mathcal{D}^{\prime} \in \left(\mathcal{Z}^{m}\right)^{n} \) that differ in one user, and for any event \( O \) in the range of \( \mathcal{M} \), we have 
\[
\operatorname{Pr}[\mathcal{M}(\mathcal{D}) \in O] \leq e^{\varepsilon} \operatorname{Pr}[\mathcal{M}(\mathcal{D}^{\prime}) \in O] + \delta.
\]
\end{definition}
The original definition of DP~\cite{dp1} assumes that the whole dataset is private, which is quite strong for many scenarios in RLHF. As in RLHF, a user's preferences are only associated with the label of the \((prompt, response, label)\) tuple, while the prompt itself is not sensitive, as it is sampled from pre-collected datasets that are already considered public knowledge~\cite{rewarddp,PROPSdp}. Thus, 
in this paper, we consider the label DP~\cite{labeldp}. We assume that \((s, a^{0}, a^{1})\) is publicly accessible, while user preference data \(y \in \{0, 1\}\) are private. Therefore, the focus is on protecting the user's preference information \(y\). The definition of label DP at the user level is provided below. 

\begin{definition}\textbf{(User-Level Label Differential Privacy).}
A mechanism \( \mathcal{M}:\left(\mathcal{Z}^{m}\right)^{n} \rightarrow \mathbb{R}^{d} \) is \((\varepsilon, \delta)\) user-level label differentially private, if for any neighboring datasets \( \mathcal{D}, \mathcal{D}^{\prime} \in \left(\mathcal{Z}^{m}\right)^{n} \) that differ in the labels of one user, and for any event \( O \) in the range of \( \mathcal{M} \), we have 
$$
\operatorname{Pr}[\mathcal{M}(\mathcal{D}) \in O] \leq e^{\varepsilon} \operatorname{Pr}[\mathcal{M}(\mathcal{D}^{\prime}) \in O] + \delta.
$$
\end{definition}
Note that item-level label differential privacy is a specific case of this definition with \( m = 1 \).



\section{Sub-optimality of Random Response}\label{sec:RR}
To achieve label-level DP, a natural approach is to employ the random response (RR)~\cite{warner1965randomized} mechanism to each label, which achieves privacy by randomly flipping labels. In fact, such a simple idea has been shown to achieve satisfactory performance in the item-level DP-RLHF both theoretically and practically~\cite{PROPSdp,rewarddp}. However, the theoretical behavior of RR in the user-level setting remains unclear. 
% Thus, following the previous work~\cite{rewarddp}, 
In the following, we first investigate the theoretical behavior of RR in the user-level setting. We will then show how the upper bound of the estimation error degrades when each user contributes a large amount of data \( m \). 

\subsection{User-level Random Response}  
Let \(\varepsilon \geq 0\) , \(m\) be the sample numbers of a single user, and \(y \in \{0,1\}\) be the true label. By the group privacy property, to extend the classical RR to the user level, we need to flip each label to make it satisfy $\frac{\varepsilon}{m}$-DP. In detail, the outputs of the RR mechanism \(\tilde{y}\) follow the following probability distribution        
\begin{align*}
    \mathbb{P}[\tilde{y} = y] &= \frac{e^{\varepsilon/m}}{1 + e^{\varepsilon/m}} = \sigma(\varepsilon/m),\\
    \mathbb{P}[\tilde{y} \neq y] &= 1 - \sigma(\varepsilon/m).
\end{align*}
% {\footnotesize
% \[
% \]}
It is easy to show that \textbf{User-level RR} is \(\varepsilon\)-\textit{user-level} label DP. %We use \textbf{User-level RR} as \(\mathcal{M}\) in Definition 3.2.
Then, we consider the following loss on the perturbed data, which is unbiased to the original loss in (non-private) RLHF. We will present the design of this loss function in Appendix~\ref{sec:debias_loss}.

{\small
\begin{equation}
\begin{aligned}\label{eq:rrloss}
\widehat{l}_{\mathcal{D}, \varepsilon}(\theta) = -\sum_{i=1}^{n}&\sum_{j=1}^{m} \left[ \mathbbm{1}\left(\widetilde{y}_{i,j}=1\right) \log \widehat{p}_{i,j}^{1} \right. \\
&\quad + \left. \mathbbm{1}\left(\widetilde{y}_{i,j}=0\right) \log \widehat{p}_{i,j}^{0} \right],
\end{aligned}
\end{equation}
}
where the predicted scores of each randomized label \( \widetilde{y}_{i,j} \) given \( x_{i,j} \) are defined as:
{\footnotesize
\[
\widehat{p}_{i,j}^{1} = \frac{\sigma\left(x_{i,j}^{\top} \theta\right)^{\sigma(\varepsilon/m)}}{\left(1-\sigma\left(x_{i,j}^{\top} \theta\right)\right)^{(1-\sigma(\varepsilon/m))}},
\]}
{\footnotesize\[
\widehat{p}_{i,j}^{0} = \frac{\left(1-\sigma\left(x_{i,j}^{\top} \theta\right)\right)^{\sigma(\varepsilon/m)}}{\sigma\left(x_{i,j}^{\top} \theta\right)^{(1-\sigma(\varepsilon/m))}}.
\]}
Thus, our private model is defined as 
\begin{equation}\label{eq:1}
    \widehat{\theta}_{\mathrm{RR}} \in \operatorname{argmin}_{\theta \in \Theta_{B}} \widehat{l}_{\mathcal{D}, \varepsilon}(\theta), 
\end{equation}
where \( \widehat{\theta}_{\mathrm{RR}} \) satisfies \(\varepsilon\)-\textit{user-level} label DP due to RR and the post-processing property of DP. To get the estimation error of $\widehat{\theta}_{\mathrm{RR}}$, we make the following assumption, which is standard in the existing literature~\cite{shah2016,shin2023,rewarddp}. 

 
\begin{assumption}[Boundedness] \label{ass:1}
(a) \( \theta^{*} \) lies in the set {\( \Theta_{B} = \left\{ \theta \in \mathbb{R}^{d} \mid \langle \mathbf{1}, \theta \rangle = 0, \|\theta\| \leq B \right\} \)} with some constant $B$. Here the condition \( \langle \mathbf{1}, \theta \rangle = 0 \) ensures identifiability of \( \theta^{*} \).
(b) Features are bounded, i.e., for all $(s, a)$ we have $\|\phi(s, a)\| \leq L$ for some constant $B$. 

\end{assumption}
Let $\Sigma_{\mathcal{D}} := \frac{1}{n} \sum_{i=1}^{n} x_i x_i^\top$ denote the sample covariance matrix of differential features $x_i = \phi(s_i, a_i^1) - \phi(s_i, a_i^0)$.
\begin{theorem} \label{thm:1}
For any $\varepsilon>0$, the private model $\hat{\theta}_{\mathrm{RR}}$ is $\varepsilon$-DP. Moreover, under Assumption \ref{ass:1}, for any $\alpha>0$, with probability at least \( 1 - \alpha \), we have 
{\small\[
\left\|\widehat{\theta}_{\mathrm{RR}} - \theta^{*}\right\|_{2} \leq O(\frac{1}{\gamma \sqrt{\lambda_{\min}(\Sigma_{\mathcal{D}})}} \frac{e^{\varepsilon/m} + 1}{e^{\varepsilon/m} - 1} \sqrt{\frac{d + \log(1 / \alpha)}{n m }} ), 
\]}
where \(\gamma = \frac{1}{2 + e^{-2 L B} + e^{2 L B}}\), \(\lambda_{\min}(\Sigma_{\mathcal{D}})\) is the minimum eigenvalue of \(\Sigma_{\mathcal{D}}\).
\end{theorem}
Note that, the bound is non-trivial only when the sample covariance matrix is positive definite. However, this assumption is too strong due to the high dimensionality of the feature vector.  We can relax it by imposing that the population covariance matrix is positive definite by imposing a coverage assumption on the state-action feature space, which is commonly encountered in offline bandit and reinforcement learning settings~\cite{yin2022near}.
\begin{assumption}[Coverage of feature space]\label{ass:2} 
Defining the differential state-action features \(x = \phi\left(s, a^{1}\right) - \phi\left(s, a^{0}\right)\), and population covariance matrix 
$\Sigma = \mathbb{E}_{s \sim \rho(\cdot),\,(a^{0}, a^{1}) \sim \mu(\cdot \mid s)} \left[x x^{\top}\right]$. 
We assume that \(\lambda_{\min}(\Sigma) \geq \kappa\) for some \(\kappa > 0\).
\end{assumption}

\begin{corollary}\label{cor:1}
Under the same assumption in Theorem \ref{thm:1} and Assumption \ref{ass:2}, with probability at least \( 1 - \alpha \), we have 
\[
\left\|\widehat{\theta}_{\mathrm{RR}} - \theta^{*}\right\|_{2} \leq O(\frac{1}{\gamma \kappa }\frac{e^{\varepsilon/m} + 1}{e^{\varepsilon/m} - 1} \sqrt{\frac{1 + \log(1 / \alpha)}{n m }} ) 
\]
\end{corollary}

\begin{remark}\label{rmk:1} The estimation error in Corollary~\ref{cor:1} is influenced by the convergence parameter \(\kappa\), which implicitly depends on the dimensionality \(d\)~\cite{wang2020}. Since \(\|x\| \leq L\), it follows that \(\kappa \geq O\left(\frac{L^2}{d}\right)\) by Assumption~\ref{ass:1}. Thus, to implement user label level DP, the estimation error of estimator \( \widehat{\theta}_{\mathrm{RR}} \) is  \(O(\frac{1}{e^{\varepsilon/m} - 1}\sqrt{\frac{d^2}{ mn}})\). 
When \(m = 1\), i.e., when in the item-level case, our bound matches the nearly-optimal rate shown in \cite{rewarddp}.  However, when \(m\) is large, \(\frac{\varepsilon}{m}\) becomes a vanishingly small quantity. Consequently, the estimation error comes to  \(O(\frac{1}{\varepsilon}\sqrt{\frac{d^2 m}{ n}})\), which can be a large magnitude, rendering the estimator inefficient.
\end{remark}
\subsection{Lower Bound of User-level DP-RLHF}
On the other hand, we introduce the lower bound of estimation error for user-level DP-RLHF, which characterizes the worst-case performance of any user-level DP algorithms. A detailed discussion is deferred to the Appendix~\ref{app:lowerbound}. 
\begin{theorem}[Informal Statement]
    For any  $(\epsilon, \delta)$-user-level DP algorithm with output $\theta_{\text{priv}}$, there exists an instance of the BTL model with the underlying parameter $\theta^*$ such that 
    \begin{equation}
        \mathbb{E}\|\theta_{\text{priv}}-\theta^*\|_2\geq  \Omega\left(\frac{d}{\sqrt{nm}}+\frac{\sqrt{d}}{\sqrt{m} n \varepsilon }\right).  
    \end{equation}
\end{theorem}
\section{Main Method}
\begin{algorithm}[tb]
   \caption{AUP-RLHF}
   \label{alg:aup-rlhf}
\begin{algorithmic}[1]
   \State {\bf Input:} Dataset $D = (Z_1, \dots, Z_n) \in (\mathbb{Z}^m)^n$, privacy parameters $(\varepsilon, \delta)$, initial point $\theta_0$
   %\State Set $k = \lceil \log \log mn \rceil$
   \State Based on users, partition $D$ into $k$ disjoint datasets $\{D_i\}_{i \in [k]}$, where $D_i$ is of size $n_i := n / 2^{k+1-i}$
   \For{$i = 1, \dots, k$}
       \State Run Algorithm \ref{alg:aup-SGD} with ($\theta_{i-1}, D_i, \tilde{n}_i, T_i, \eta_i, \varepsilon, \delta,    \tau_i$)  and get its output $\theta_i$.
      % \State $\theta_i=\text{AdapUserPriv-SGD}(\mathcal{D}_i, \varepsilon, \delta, \theta_{i-1}, T_i, \eta_i, \tau_i)$
   \EndFor
   \State {\bf Output:} $\hat{\theta} = \theta_k$
\end{algorithmic}
\end{algorithm}


\begin{algorithm*}[tb]
   \caption{AdapUserPriv-SGD}
   \label{alg:aup-SGD}
\begin{algorithmic}[1]
   \State {\bf Input:} Initial model weights $\theta_0$, training set $\mathcal{D}$ of $n$ users and $m$ records, batch size $\tilde{n}$, learning rate $\eta$, iterations $T$, privacy parameters $(\varepsilon, \delta)$, concentration threshold $\tau$
      
   \State $\sigma \gets \text{PrivacyAccount}(\frac{\varepsilon}{2}, \frac{\delta}{2}, n, \tilde{n}, T)$ \Comment{Compute noise multiplier}
   \For{$t = 1, \dots, T$}
        \State Randomly draw $U_t$, a batch of $\tilde{n}$ users\Comment{Subsampling}
       \For{$i \in U_t$}
%           \State We define $\ell(\theta; z) :=  \left[ \mathbbm{1}( y_{z} = 1 ) \log p_{z,1} + \mathbbm{1}( y_{z} = 0 ) \log p_{z,0} \right]$ 
           \State  $g_{t,i} \gets \frac{1}{m} \sum_{j \in [m]} \nabla \ell(\theta_t, z_j)$ \Comment{Compute user-averaged gradient}
       \EndFor
       \State  $s^c_{t} \gets \frac{1}{\tilde{n}} \sum_{i, i' \in U_t} 1(\|g_{t,i} - g_{t,i'}\| \leq \tau)$\Comment{Compute concentration scores}
       \If{$\text{AboveThreshold}(s^c_t, \varepsilon/2, 4\tilde{n}/5) = \top$ in Alg.~\ref{alg:above_threshold}}\Comment{Run AboveThreshold with $s^c_{t}$}
           \State Set $B_t \gets \emptyset$
           \State Set $f_{t,i} \gets \sum_{i'} 1(\|g_{t,i'} - g_{t,i}\| \leq 2\tau)$
           \State Add $i$th user to $B_t$ with probability $p_{t,i}$, where \Comment{Remove the outliers}
           \[
           p_{t,i} = 
           \begin{cases} 
              0 & \text{if } f_{t,i} < \tilde{n}/2 \\
              1 & \text{if } f_{t,i} \geq 2\tilde{n}/3 \\
              \frac{f_{t,i} - \tilde{n}/2}{\tilde{n}/6} & \text{otherwise}
           \end{cases}
           \]
           \State Aggregate and add noise for the remaining users:
           \State Let $\widehat{g_t} = \frac{1}{|B_t|} \sum_{i \in B_t} g_{t,i}$ if $B_t$ is not empty, and 0 otherwise
           \State $\widetilde{g_t} \gets \widehat{g_t} + \nu_t$, where $\nu_t \sim \mathcal{N}(0, \frac{8\tau^2 \log(e^{\varepsilon}T/\delta) \sigma^2}{\tilde{n}^2})I_d$
           \State  $\theta_{t+1} \gets \theta_t - \eta \widetilde{g_t}$ \Comment{Update model}
       \Else
           \State Halt. \Comment{Halt the algorithm if it does not pass. }
       \EndIf
   \EndFor
   \State \Return $\widehat{\theta} = \frac{1}{T} \sum_{t \in [T]} \theta_t$

\end{algorithmic}
\end{algorithm*}
In the previous section, we have shown that although the RR-based mechanism for DP-RLHF in the user-level setting is simple, there are several issues. First, when the contribution of each user $m$ is large, we can see the privacy-utility trade-off in Corollary \ref{cor:1} is bad. This is because, intuitively, larger $m$ indicates we have more data, so the estimation error should be lower. However, a larger $m$  will introduce a larger error for the RR mechanism, which contradicts our expectations. Second, we can see there is a large gap between the lower bound and upper bound. Thus, a natural question is whether we can further fill in the gap. Finally, the private model $  \widehat{\theta}_{\mathrm{RR}}$ needs to be the exact minimizer of the loss in (\ref{eq:1}), which is impossible to get in practice due to the non-convexity of the loss. Thus, a practical and efficient DP algorithm is needed. 

In this section, we propose our AUP-RLHF method, which is based on DP-SGD~\cite{abadi2016deep,wang2017differentially,wang2019differentially,wang2019differentially1}. Instead of flipping labels, here we consider the original loss function in the BTL model with linear reward: 
$$\ell(\theta; z) :=  \left[ \mathbbm{1}( y_{z} = 1 ) \log p_{z,1} 
+ \mathbbm{1}( y_{z} = 0 ) \log p_{z,0} \right],$$
where $
p_{z,1}= \sigma\left(x^{\top} \theta\right), 
p_{z, 0} =\left(1-\sigma\left(x^{\top} \theta\right)\right).
$

In AUP-RLHF (Algorithm \ref{alg:aup-rlhf}), we first partition the data into several disjoint sets, and for each set, we will use a DP-SGD-based update, namely AdapUserPriv-SGD (Algorithm \ref{alg:aup-SGD}). In detail, during each iteration of parameter updates, the framework operates in three stages. First, a subset of users is selected via user-wise sampling, and the average gradient of the selected subset is computed as a query (Step 4-8). Second, the query is passed through a user-level private mean estimation oracle, which outputs a gradient satisfying user-level DP (Step 9-15). Third, the gradient obtained from the previous step is used to perform gradient descent for parameter updates.

The main difference between AUP-RLHF and DP-SGD is the private mean estimation oracle for aggregated gradients. If we directly extend DP-SGD to the user-level (Appendix Algorithm~\ref{alg:user_dp_sgd})\cite{userlevelrdplhf}
, for each user's data, we may aggregate their gradient and perform a clipping with some threshold $C$ (Step 9). In this case, the sensitivity of the average of clipped gradients is $O(\frac{1}{\tilde{n}})$, with $\tilde{n}$ as the number of subsampled users. However, such a method does not leverage information on each user's data, leading to unsatisfactory performance. In this work, we leverage the user-level private mean estimation oracle proposed by \cite{userleveldp}, which is an adaptive sampling procedure to remove outliers to let the aggregated user-level gradients add less noise. To achieve adaptive mean estimation for concentrated samples, we first introduce the concept of concentration. 

%In general, classical private mean estimation oracles follow a standard procedure. First, the query is norm-clipped to ensure it lies within a ball of radius \( C \). This guarantees bounded sensitivity, which measures the maximum impact of a single query perturbation on the result and is critical for setting the privacy parameters. The mean of the clipped queries is then computed, and Gaussian noise is added to the resulting gradient to ensure that individual user information remains private, thereby satisfying differential privacy requirements. \di{what are you doing, why you introduce DP-SGD?}

\begin{definition}
 A set of  random samples $\left\{X_{i}\right\}_{i \in [n]}$ is $(\tau, \gamma)$-concentrated if there exists a point $x \in \mathbb{R}^{d}$ such that with probability at least $1 - \gamma$,
\[
\max_{i \in [n]} \left\| X_{i} - x \right\| \leq \tau.
\]
\end{definition}
After adaptive sampling (Step 8-14), we can obtain a \(\tau\)-concentrated subset of users, with user-averaged gradients $\{g_{t,i}\}_{i\in U_t}$ as the random samples. Due to the concentration property, the sensitivity of the aggregated gradients will be bounded $O(\tau)$. Thus, there is no need to do a clipping and it allows the added noise to scale proportionally to \(\tau\) (which is $\tilde{O}(\frac{1}{\sqrt{m}}$) rather than the constant clipping radius \( C \), resulting in significantly smaller noise and achieving improved utility. 


Specifically, to get such a concentrated subset, the algorithm first employs an \textit{AboveThreshold} mechanism (Algorithm \ref{alg:above_threshold}) to compute a concentration score, which determines whether the average gradient of users is predominantly concentrated (Step 9). This ensures that the input dataset is approximately \(\tau\)-concentrated. Subsequently, outlier detection is performed by assigning each sample a score that quantifies its likelihood of being an outlier, i.e., not concentrated gradient. Each sample is then retained with a probability proportional to its score, effectively removing low-score outliers and yielding a high-quality, concentrated subset of data (Step 10-12).

\noindent {\bf PrivacyAccount.} Given the total number of users, batch size of users, number of iterations, and privacy budget, this subroutine can determine the noise $\sigma$ we need to add in Step 16 of Algorithm \ref{alg:aup-SGD} to ensure the algorithm satisfies $(\varepsilon,\delta)$ DP. We adopt the Poisson subsampling,  a common practice in the privacy accounting of DP-SGD, in Step 4. Thus, theoretically, we can show that  $\sigma=\tilde{O}(\frac{\varepsilon n}{\sqrt{T} \tilde{n}})$ can make the algorithm achieve $(\varepsilon,\delta)$-DP (\ref{thm:2}). However, in practice, we always use various
open source libraries to give more accurate calculations on such a noise, such as DP Accounting Library\footnote{\url{https://github.com/google/differential-privacy/tree/main/python/dp_accounting}}.  

\begin{theorem}\label{thm:2}
For any $0<\varepsilon, \delta<1$, if $\sigma=\tilde{O}(\frac{n}{\tilde{n}}  \frac{\varepsilon}{\sqrt{T\ln (1/\delta)}})$, Algorithm~\ref{alg:aup-rlhf} is \((\varepsilon, \delta)\) user-level (label) DP. Here, the Big-$\tilde{O}$ notation omits other logarithmic terms. 
\end{theorem}
In the following, we will show that it is possible to get an improved upper bound of estimation error with some specific hyper-parameters. 
\begin{theorem}\label{thm:3}
 Under Assumption~\ref{ass:1} and ~\ref{ass:2} and the condition in Theorem \ref{thm:2}, for any  \(0 \leq \varepsilon \leq 10\) and \( 0 \leq \delta \leq 1\), 
  in Algorithm \ref{alg:aup-rlhf} set  $k = \lceil \log \log mn \rceil$, \(\theta_0 = 0\),
  \(\tilde{n}_i=n_i\), \(\eta_i=\frac{B}{4L} \cdot \min \left\{\frac{\sqrt{m} n_i \varepsilon}{T_i \sqrt{d \log ^{2}(m n_i d / \delta)}}, \frac{1}{T_i^{3 / 4}}, \frac{\sqrt{n_i m}}{T_i}\right\}\), \(\tau_i=\frac{4L \log \left(n_i d m e^{\varepsilon} T_i / \delta\right)}{\sqrt{m}}\), and \(T_i=O\left(m^2 n_i^2 + m n_i \sqrt{d}\right)\). The output \(\hat{\theta} \) satisfies the following if \( n_i> \frac{\log (m d n_i) \log (m d n_i / \delta)}{\varepsilon} \), 
\begin{equation*}
    \mathbb{E} \|\hat{\theta} - \theta^*\|_2 \leq \widetilde{O}(\frac{L}{\kappa \gamma} \left[\frac{1}{\sqrt{mn}} + \frac{\sqrt{d}}{\sqrt{m}n\varepsilon}\right]). 
\end{equation*}
The Big-$\tilde{O}$ notation omits other logarithmic terms. 
\end{theorem} 

\begin{remark}\label{rmk:2}
    We discuss the case under the same condition as in Remark \ref{rmk:1} when \(\kappa = O\left(\frac{L^2}{d}\right)\). As we mentioned, when $m$ is a large quantity, the estimation error of the RR estimator is \(O(\frac{1}{\varepsilon}\sqrt{\frac{d^2 m}{ n}})\), while the upper bound of AUP-RLHF
    is \(\widetilde{O}\left({\frac{d\sqrt{d}}{\sqrt{m}n \varepsilon}}\right)\). We can see it improves a factor of $O(\frac{m\sqrt{n}}{\sqrt{d}})$. The factor of $O(\frac{1}{\sqrt{d}})$  is because our SGD algorithm adds noise to each dimension of the parameter \(\theta\). When \(m\sqrt{n}\) exceeds \(\sqrt{d}\), our algorithm exhibits better utility than the RR algorithm, which is also commonly observed in practical scenarios.
\end{remark}
In the previous section, we considered the case where the label is binary. In fact, our method can be directly extended to the general $K$-wise case. See Appendix \ref{sec:kwise} for details. 
\iffalse 
\begin{remark}\textbf{Applications in Contextual Bandits.}
In the conventional theoretical analysis of offline linear contextual bandits, a simple greedy policy is typically employed\cite{kwise}. In contrast, we adopt a more PPO-aligned approach, using the reverse-KL regularized contextual bandit to learn a policy with a KL-constraint\cite{zhang2023,klrlhf}. Furthermore, due to our convergence assumption, we do not require the use of a pessimistic policy, as is common in traditional MLE estimators\cite{kwise}. As a result, the policy we derive is 
{\small\begin{align*}
\pi_r(\cdot|s) &:= \arg\max_{\pi \in \Pi} \mathbb{E}_{a \sim \pi(\cdot|s)} 
         \left[ r(x, a) + \eta \log \frac{\pi_0(a|s)}{\pi(a|s)} \right] \\
       &\propto \pi_0(\cdot|s) \cdot \exp \left( \frac{1}{\eta} r(s, \cdot) \right).
\end{align*}}
where $r$ is the reward function \(\mathcal{S} \times \mathcal{A} \to \mathbb{R}\),  \(\pi_0\) is a reference policy and $\eta>0$ is the KL penalty coefficient. We know the policy in turn follows the Gibbs distribution\cite{zhang2023}. Consequently, our policy achieves a gap of \(\widetilde{O}\left(\frac{L}{\kappa \gamma} \sqrt{\frac{1}{mn} + \frac{d}{mn^2 \varepsilon^2}}\right)\) for \(J(\pi) = \mathbb{E}_{s \sim \rho} \mathbb{E}_{a \sim \pi(\cdot|s)} 
         \left[ r^*(s, a) + \eta \log \frac{\pi_0(a|s)}{\pi(a|s)} \right])\). We will defer the proof to the appendix.    
    
\end{remark}

\fi 


\section{Experiments}

\subsection{Experimental Setup}
\noindent \textbf{Tasks and Datasets.}
We evaluate our AUP-RLHF method on two different datasets with three different tasks. The IMDb dataset~\cite{imdb} is a widely used movie review dataset containing both positive and negative reviews. We perform a controlled sentiment generation task on this dataset. The TL;DR dataset~\cite{tldr} is a summarization dataset containing posts from Reddit along with their corresponding titles, on which we conduct a summarization task. 


\noindent \textbf{Base models.}
For each dataset and task, we use the pre-trained Gemma-2-2B\cite{gemma2} and LLaMA-2-7B \cite{llama} as our base models.   


\noindent \textbf{Baselines.}
We compare our algorithm with three baselines, which all are user-level DP-based RLHF algorithms: Random Response~(Section~\ref{sec:RR}), User-wise DP-SGD~\cite{userlevelrdplhf}, and Group Privacy~\cite{userlevelrdplhf}. Additionally, we compare our results with those obtained from RLHF under non-private settings.


\noindent \textbf{Evaluation.}
% We use accuracy as the evaluation metric for the performance of the reward model.
In the controlled sentiment generation task, we use the reward score as the evaluation metric. Additional details are provided in Appendix~\ref{app:imdb_reward}. For the summarization task, we assess our AUP-RLHF algorithm by comparing its win rate against each baseline strategy. Following previous work in RLHF~\cite{dpo}, we adopt GPT-4 as a proxy for human evaluation.

\noindent \textbf{Settings.}  We set each user's contributions at \( m = 10 \), with \( n \) set to 2500 for the sentiment generation task, and \( m = 50, n=1800 \) for the summarization task. We set privacy budgets as \( \varepsilon = 3 \) or \( \varepsilon = 8 \), with \( \delta \) fixed at $10^{-5}$. Due to the slower training speed of the SGD-based optimizer compared to Adam, we set the number of sub-datasets $k=1$ and the number of training epochs for the reward model to 5 instead of 1. Other hyperparameter details are provided in Appendix~\ref{app:A} Table~\ref{tab:sft_params} and Table~\ref{tab:ppo_params}. The experiments are carried out on machines
with one Nvidia A100 GPU card, 14-core Intel Xeon Gold 6348 CPUs, and 100GB of RAM. 

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{c c c c c}
        \toprule
        $m$ & 5 & 10 & 20 & 50 \\
        \midrule
        $\epsilon=1$ & 0.366 & 0.586 & 1.051 & 2.429 \\
        $\epsilon=3$ & 0.139 & 0.175 & 0.246 & 0.461 \\
        $\epsilon=8$ & 0.086 & 0.102 & 0.128 & 0.189 \\
        \bottomrule
    \end{tabular}
    \caption{Effective noise of AUP-RLHF on TL;DR dataset.}
    \label{tab:dataset1}
\end{table}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{c c c c c}
        \toprule
        $m$ & 5 & 10 & 20 & 50 \\
        \midrule
        $\epsilon=1$ & 1.101 & 1.284 & 1.631 & 2.402 \\
        $\epsilon=3$ & 0.726 & 0.809 & 0.928 & 1.182 \\
        $\epsilon=8$ & 0.531 & 0.578 & 0.639 & 0.750 \\
        \bottomrule
    \end{tabular}
    \caption{Effective noise of User-wise DPSGD on TL;DR dataset.}
    \label{tab:dataset2}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/imdb.png}
    \caption{IMDb Sentiment Generation (Llama-2-7b, $\epsilon=8$).}
    \label{fig:imdb_sum}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/tldr_gemma_eps_8.png}
    \caption{Win Rate Against the SFT Model for TL;DR Summarization (Gemma-2-2b, $\epsilon=8$).}
    \label{fig:tldr_sum}
\end{figure}
\subsection{Experimental Results}
\noindent \textbf{IMDb Sentiment.} For the sentiment generation task, we make the following observations based on the experimental results depicted in Figure~\ref{fig:imdb_sum}, \ref{fig:imdb-gemma}, and \ref{fig:imdb-llama} : (1) We observe that, under the same KL divergence, our AUP-RLHF outperforms other baselines in terms of the reward metric, demonstrating the superiority of our model. (2) Consistent with the theoretical analysis of differential privacy, relaxing the privacy budget leads to an improvement in the reward score metric. Specifically, we find that, for the same KL divergence, the reward values of AUP-RLHF and all baselines are higher when \(\epsilon = 8\) compared to when \(\epsilon = 3\). (3) By comparing the experimental results of Gemma-2-2B and Llama-2-7B, we observe that at \(\epsilon = 3\), the larger model yields improved performance, which is consistent with most DP fine-tuning papers \cite{yu2021differentially,rlhfdp23}. However, at \(\epsilon = 8\), the performance improvement in the larger model is less significant, possibly due to the lack of a sufficiently exhaustive hyperparameter search to find the optimal model performance. 
    %it is evident that Llama-2-7B converges faster than Gemma2-2B, which further supports the idea that scaling can enhance model performance.

\iffalse
{We aim to make the following discussions:

\begin{enumerate}
    \item The reward versus step comparison for different privacy budgets across various algorithms.
    
    \item Display the win rate of AUP-RLHF and other baselines under a specific privacy budget (using bar charts or percentage bar charts). In the appendix, we will present the win rates of AUP-RLHF and other baselines under all privacy budgets in a tabular format.
    
    \item Discuss the impact of the choice of \( m \) on the algorithm's performance, comparing the reward scores or win rates of RR and AUP-RLHF when \( m = 5, 10, 20, 50 \). \di{Also, you need to fix $m$ and discuss the number of users} 
    
    \item Compare the convergence rates of AUP-RLHF and user-wise DP-SGD.

    \di{check previous papers on user-level DP-SGD and DP-RLHF to see whether we need ablation study. }

\end{enumerate}}
\fi
\paragraph{TL;DR Summarization.} Similar to the findings in the sentiment generation task, in Figure~\ref{fig:tldr_sum} and \ref{fig:tldr-gemma} we observe that AUP-RLHF outperforms other baselines in terms of the win rate metric, across both model sizes and varying privacy budgets. Furthermore, when the privacy budget is set to \(\epsilon = 3\), the performance of all algorithms is suboptimal, with winrates below 50\%. However, when the privacy budget is relaxed to \(\epsilon = 8\), the win rates of all algorithms improve. Meanwhile, our AUP-RLHF achieves a significant increase in winrate, rising from 39\% to 63\% on the Gemma-2-2B model.


\paragraph{Analyzing AUP-RLHF.} As observed in Table~\ref{tab:dataset1}, as the number of user records \(m\) increases, the noise added to AUP-RLHF increases accordingly. This is because, based on our theory, the variance of the noise is $\tilde{O}(\frac{1}{m\tilde{n}^2})$. Thus, when the total size $mn$ and iteration number $T$ are fixed, $m\tilde{n}=\frac{mn}{T}$ is fixed, and increasing $m$ will make $\tilde{n}$ less, which makes the noise $\tilde{O}(\frac{1}{m\tilde{n}^2})$ larger. 


Additionally, we find that, under the same \(\epsilon\) and \(m\), the noise in AUP-RLHF is smaller than that in User-wise DP-SGD. This is due to the fact that our algorithm, based on the user-concentrated nature, requires the noise scale to be proportional to the concentration parameter \(\tau\) rather than the clipping radius \(C\). This reduction in noise allows our algorithm to converge more quickly.

\section{Conclusion}
In this work, we study RLHF with user-level privacy. We first demonstrate the sub-optimality of the RR method, followed by presenting the lower bound for user-level DP-RLHF. Then, we propose the AUP-RLHF algorithm, which ensures that the output satisfies user-label DP while achieving better utility. In our experiments, we validate the superiority of our algorithm across multiple datasets.  



\section{Limitations}
Although we present an innovative approach, there are some limitations to our method that motivate future research directions. First, our upper bound (Theorem~\ref{thm:3})  relies on a strong assumption that the loss function is both strongly convex and Lipschitz continuous.%, and future work could explore the relaxation of this assumption.
Second, this upper bound we prove is suboptimal, which has an additional factor of $d$ compared to lower bound. This originates from our coverage assumption.
%, and future research could focus on developing algorithms that closely match the lower bound. 
Furthermore, during experimentation, the SGD-based algorithm demonstrated relatively poor efficiency in complex tasks, and improving the convergence rate of the algorithm is another promising direction for future investigation.
\bibliography{custom}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Additional Experiment Details}\label{app:A}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/imdb-llama-eps-3.png}
        \caption{$\epsilon=3$}
    \end{subfigure}
    
    \vspace{10pt} % Adds some vertical spacing

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/imdb-llama-eps-8.png}
        \caption{$\epsilon=8$}
    \end{subfigure}

    % \vspace{10pt} % Adds some vertical spacing

    % \begin{subfigure}{\textwidth}
    %     \centering
    %     \includegraphics[width=1\textwidth]{figures/imdb-llama-nonprivate.png}
    %     % \caption{Caption for Image 2}
    % \end{subfigure}
    
    \caption{IMDb Sentiment Generation (Llama-2-7b).}
    \label{fig:imdb-llama}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/imdb-gemma-eps-3.png}
        \caption{$\epsilon=3$}
    \end{subfigure}
    
    \vspace{10pt} % Adds some vertical spacing

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/imdb-gemma-eps-8.png}
        \caption{$\epsilon=8$}
    \end{subfigure}

    
    \caption{IMDb Sentiment Generation (Gemma-2-2b).}
    \label{fig:imdb-gemma}
\end{figure}

\subsection{IMDb Sentiment Experiment Details}\label{app:imdb_reward}
We follow the setup in \cite{dpo}, where prefix prompts are sampled between 2 to 8 tokens. For reward model training, we use \texttt{cardiffnlp/twitter-roberta-base-sentiment}, a backbone model trained on Twitter data, which is further fine-tuned on IMDb sentiment analysis by updating the last layer only. Llama-2-7b and Gemma-2-2b serve as the base models. While each method fine-tunes the base models using its respective reward model, test performance is evaluated using a ground-truth reward model, \texttt{siebert/sentiment-roberta-large-english}, ensuring a fair comparison. On every 10 steps until convergence, we report the rewards and KL-divergence over a set of test prompts, shown in Fig~\ref{fig:imdb-llama} and Fig~\ref{fig:imdb-gemma}.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Gemma-2-2B} & \textbf{Llama-2-7B} \\
        \midrule
        Batch Size & 4 & 4 \\
        Learning Rate & 1e-4 & 1e-4 \\
        Warmup Ratios & 0.1 & 0.1 \\
        Learning Rate Scheduler & Cosine & Cosine \\
        Optimizer & AdamW & AdamW \\
        Training Epochs & 1 & 1 \\
        LoRA Rank & 16 & 16 \\
        LoRA Alpha & 32 & 32 \\
        \bottomrule
    \end{tabular}
    \caption{Parameter settings for Supervised Fine-Tuning (SFT) on Gemma-2-2B and Llama-2-7B.}
    \label{tab:sft_params}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Gemma-2B} & \textbf{Llama-2-7B} \\
        \midrule
        Batch Size & 64 & 64 \\
        Mini-batch Size & 64 & 64 \\
        Learning Rate & 1e-5 & 1e-5 \\
        Optimizer & RMSProp & RMSProp \\
        KL Penalty Coefficient & 0.15 & 0.15 \\
        LoRA Rank & 16 & 16 \\
        LoRA Alpha & 32 & 32 \\
        \bottomrule
    \end{tabular}
    \caption{PPO parameter settings for Gemma-2-2B and Llama-2-7B.}
    \label{tab:ppo_params}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Parameter} & \textbf{AUP-RLHF} & \textbf{User-wise DPSGD} & \textbf{Group Privacy} & \textbf{Random Response} \\
        \midrule
        User Batch Size & 50 & 50 & N/A & N/A \\
        Per-user Sample Size & 10 & 10 & N/A & N/A \\
        Sample Batch Size & N/A & N/A & 500 & 500\\
        Learning Rate & 1e-3 & 1e-3 & 1e-3 & 1e-3 \\
        Warmup Ratios & 0.2 & 0.2 & 0.2 & 0.2 \\
        Learning Rate Scheduler & Cosine & Cosine & Cosine & Cosine \\
        Epochs & 5 & 5 & 5 & 5 \\
        \bottomrule
    \end{tabular}
    \caption{Reward model parameter settings for Gemma-2-2B and Llama-2-7B.}
    \label{tab:ppo_params}
\end{table}

\subsection{TL;DR Summarization}\label{app:tldr_reward}

For reward model training, we use \texttt{tasksource/ModernBERT-base-nli}, a backbone model trained on Natural Language Inference, which is further fine-tuned on summarization preference by updating the last layer only.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/tldr_gemma_eps_3.png}
        \caption{$\epsilon=3$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/tldr_gemma_eps_8.png}
        \caption{$\epsilon=8$}
    \end{subfigure}

    \caption{Win Rate Against the SFT Model for TL;DR Summarization (Gemma-2-2b).}
    \label{fig:tldr-gemma}
\end{figure}

\section{Other Details}
\subsection{Algorithm}
\begin{algorithm}[tb]
   \caption{AboveThreshold}
   \label{alg:above_threshold}
\begin{algorithmic}[1]
   \State {\bf Input:} Dataset $\mathcal{D} = (Z_1, \dots, Z_n)$, threshold $\Delta \in \mathbb{R}$, privacy parameter $\varepsilon$
   \State Let $\hat{\Delta} := \Delta - \text{Lap}\left(\frac{2}{\varepsilon}\right)$
   \For{$t = 1$ {\bf to} $T$}
       \State Receive a new query $q_t: \mathbb{Z}^n \rightarrow \mathbb{R}$
       \State Sample $\nu_t \sim \text{Lap}\left(\frac{4}{\varepsilon}\right)$
       \If{$q_t(\mathcal{D}) + \nu_t < \hat{\Delta}$}
           \State Output: $a_t = \bot$
           \State Halt
       \Else
           \State Output: $a_t = \top$
       \EndIf
   \EndFor
\end{algorithmic}
\end{algorithm}

\iffalse
\begin{algorithm}[tb]
   \caption{Outlier-Removal Based Mean Estimation for Concentrated Data}
   \label{alg:outlier_removal}
\begin{algorithmic}
   \State {\bf Input:} Dataset $\mathcal{D} = (Z_1, \ldots, Z_n)$, privacy parameters $(\varepsilon, \delta)$, parameters $\tau$
   \For{$i = 1$ {\bf to} $T$}
      \State Receive a new mean estimation query $q_i: \mathcal{Z} \rightarrow \mathbb{R}^d$
      \State Define concentration score
      \[
         s_i^{\text{conc}}(\mathcal{D}, \tau) := \frac{1}{n} \sum_{Z \in \mathcal{D}} \sum_{Z' \in \mathcal{D}} \mathbb{I}(\| q_i(Z) - q_i(Z') \| \leq \tau)
      \]
      \If{AboveThreshold($s_i^{\text{conc}}$, $\varepsilon/2, 4n/5$) = T}
         \State Set $S_i = \emptyset$
         \For{{\bf Each User} $Z_j \in \mathcal{D}$}
            \State Set $f_{i,j} = \sum_{Z \in \mathcal{D}} \mathbb{I}(\| q_i(Z_j) - q_i(Z) \| \leq 2\tau)$
            \State Add $Z_j$ to $S_i$ with probability $p_{i,j}$, for $p_{i,j} = $
            \[
               p_{i,j} = 
               \begin{cases} 
                  0 & \text{if } f_{i,j} < n/2 \\ 
                  1 & \text{if } f_{i,j} \geq 2n/3 \\ 
                  \frac{f_{i,j} - n/2}{n/6} & \text{otherwise} 
               \end{cases}
            \]
         \EndFor
         \State Let $g_i = \frac{1}{|S_i|} \sum_{Z \in S_i} q_i(Z)$ if $S_i$ is not empty, and $0$ otherwise
         \State {\bf Output:} $\hat{g}_i = g_i + \nu_i$, where $\nu_i \sim \mathcal{N} \left(0, \frac{8\tau^2 T \log(e^\varepsilon T/\delta) \log(e^{\varepsilon/2}/\delta)}{n^2 \varepsilon^2} I_d \right)$
      \Else
         \State {\bf Output:} $g_i = 0$
         \State {\bf Halt}
      \EndIf
   \EndFor
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[tb]
   \caption{DP-SGD for user-level DP}
   \label{alg:dp_sgd}
\begin{algorithmic}
   \State {\bf Input:} Dataset $\mathcal{D} = (Z_1, \ldots, Z_n) \in (\mathbb{Z}^m)^n$, private parameters $(\varepsilon, \delta)$, initial point $\theta_0$, number of rounds $T$, stepsize $\eta_t$, concentration parameter $\tau$
   \For{$t = 1, \dots, T$}
      \State Define a query $q_t(Z_i) = \nabla l_{\mathcal{D}}(\theta_t; Z_i)$ for $Z_i \in \mathbb{Z}^m$
      \State Run Algorithm 2 with query $q_t$ and parameters $\mathcal{D}, \varepsilon, \frac{\delta}{2Tmnd}, \tau$
      \State Let $g_t$ be the output of Algorithm 2
      \If{$g_t \neq \bot$}
         \State Update $\theta_{t+1} \leftarrow \Pi(\theta_t - \eta_t g_t)$
      \Else
         \State {\bf Output:} Initial point $\theta_0$
         \State {\bf Halt}
      \EndIf
   \EndFor
   \State {\bf Return:} $\hat{\theta} = \frac{1}{T} \sum_{t \in [T]} \theta_t$
\end{algorithmic}
\end{algorithm}
\fi

\begin{algorithm}
\caption{User-wise DP-SGD}\label{alg:user_dp_sgd}
\begin{algorithmic}[1]
\State \textbf{Input:} Initial model weights $\theta_0$, training set $D$ of $n$ users and $m$ records, learning rate $\eta$, iterations $T$, user batch size $\tilde{n}$, privacy budget $\varepsilon$, number of records per user $\{k_i\}_{i=1}^{\tilde{n}}$ gradient norm bound $C$
\State $ \sigma \leftarrow \text{PRIVACYCOUNTING}(\varepsilon, \delta, n, \tilde{n}, T) $\Comment{Compute noise multiplier}

\State \text{(User-wise DP-SGD directly uses $D$)}\Comment{Prepare the dataset}

\For{$t = 1, \dots, T$}
    \State Randomly draw $U_t$, a batch of $\tilde{n}$ users
    \For{$i = 1, \dots, \tilde{n}$}
        \State Sample $z_j=\{x_j, y_j\}_{j \in [k_i]}$  $k_i$ records for the $i$th user in $U_t$
        \State $g_{t,i} \leftarrow \frac{1}{k_i} \sum_{j \in [k_i]} \nabla_{\theta_t} \ell(\theta, z_j)$\Comment{Compute user-averaged gradient}
        \State $\hat{g}_{t,i} \leftarrow g_{t,i} / \max(1, \frac{\|g_{t,i}\|_2}{C})$\Comment{Clip gradient}
    \EndFor
    \State $\tilde{g}_t \leftarrow \frac{1}{\tilde{n}} \sum_{j \in [\tilde{n}]} \hat{g}_{t,i} + \mathcal{N}(0, \sigma^2 C^2 I_d)$\Comment{Aggregate and add noise}
    \State $\theta_{t+1} \leftarrow \theta_t - \eta \tilde{g}_t$\Comment{Model update}
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{De-biased loss function}\label{sec:debias_loss}
We start with the BCE loss under randomized labels. Next, we discuss their drawback, which help us get intuition for a de-biased loss. For any \( \theta \in \mathbb{R}^{d} \), the predicted probabilities of a randomized label \( \widetilde{y}_{i} \) given \( x_{i} \) as
\[
\begin{array}{l}
\widetilde{p}_{i, 1} = \sigma(\varepsilon)Â·\sigma\left(x_{i}^{\top} \theta\right)  + (1-\sigma(\varepsilon))Â·\left(1 - \sigma\left(x_{i}^{\top} \theta\right)\right), \\
\widetilde{p}_{i, 0} = \sigma(\varepsilon)Â·\left(1 - \sigma\left(x_{i}^{\top} \theta\right)\right)  + (1-\sigma(\varepsilon))Â·\sigma\left(x_{i}^{\top} \theta\right).
\end{array}
\]

After applying random response to the dataset, it becomes \(\mathcal{\widetilde{D}}= \left((x_{i,j}, \widetilde{y}_{i,j})\right)_{j=1}^{m})_{i=1}^{n} \). The MLE \( \widetilde{\theta}_{\text{MLE}} \) computed on this dataset satisfies user-level privacy and can be obtained by minimizing the BCE loss (the negative log-likelihood). 
\[
l_{\mathcal{D}, \varepsilon}(\theta) = -\sum_{i=1}^{n}\sum_{j=1}^{m}\left[\mathbbm{1}\left(\widetilde{y}_{i,j}=1\right) \log \widetilde{p}_{i,j}^{1} + \mathbbm{1}\left(\widetilde{y}_{i,j}=0\right) \log \widetilde{p}_{i, j}^{0}\right].
\]


The drawback of BCE loss with randomized labels is that it introduces bias in loss compared with clear-text loss
\(
\mathbb{E}[l_{\mathcal{D}, \varepsilon}(\theta) ]\neq\mathbb{E}[{l}_{\mathcal{D}}(\theta)]
\).
At the same time, this leads to a discrepancy in the log-odds between the randomized dataset and the clear dataset, introducing bias in the preferences for \(a^1\) and \(a^0\)\cite{rewarddp}. This motivates us to design a de-bias loss function that ensures the log-odds are identical between the randomized and clear datasets. The following loss achieves this:
\[
\widehat{l}_{\mathcal{D}, \varepsilon}(\theta) = -\sum_{i=1}^{n}\left[\mathbbm{1}\left(\widetilde{y}_{i}=1\right) \log \widehat{p}_{i, 1} + \mathbbm{1}\left(\widetilde{y}_{i}=0\right) \log \widehat{p}_{i, 0}\right]
\]

where we define, for any \( \theta \in \mathbb{R}^{d} \), the predicted scores of each randomized label \( \widetilde{y}_{i} \) given \( x_{i} \) as

\[
\widehat{p}_{i, 1} = \frac{\sigma\left(x_{i}^{\top} \theta\right)^{\sigma(\varepsilon)}}{\left(1 - \sigma\left(x_{i}^{\top} \theta\right)\right)^{(1-\sigma(\varepsilon))}}, \quad \widehat{p}_{i, 0} = \frac{\left(1 - \sigma\left(x_{i}^{\top} \theta\right)\right)^{\sigma(\varepsilon)}}{\sigma\left(x_{i}^{\top} \theta\right)^{(1-\sigma(\varepsilon))}}.
\]

Although \( \widehat{p}_{i, 1} \) and \( \widehat{p}_{i, 0} \) are not probabilities, they satisfy our desired property:

\[
\log \frac{\widehat{p}_{i, 1}}{\widehat{p}_{i, 0}} = \log \frac{\sigma\left(x_{i}^{\top} \theta\right)}{1 - \sigma\left(x_{i}^{\top} \theta\right)} = \operatorname{logit}\left(p_{i, 1}\right).
\]

Hence, the loss function \( \widehat{l}_{\mathcal{D}, \varepsilon}(\theta) \) essentially de-biases the effect of randomization. 
\subsection{Extension to K-wise}\label{sec:kwise}
Let \( \mathcal{D} = (s_{i}, a_{i}^{1},..., a_{i}^{K}, y_{i})_{i=1}^{n} \) be a dataset of \( n \) samples, where each sample has a state \( s_{i} \in \mathcal{S} \) (e.g., prompt given to a language model), K actions \( a_{i}^{1},..., a_{i}^{K} \in \mathcal{A} \) (e.g., K responses from the language model), and label \( y_{i} \in \{1,2,...,K\} \) indicating which action is most preferred by human experts. We assume that the state \( s_{i} \) is first sampled from some fixed distribution \( \rho \). The K actions \( (a_{i}^{1},... ,a_{i}^{K}) \) are then sampled from some joint distribution (i.e., a behavior policy) \( \mu \) conditioned on \( s_{i} \).  Under the Plackett-Luce model\cite{pla75, luc12}, the label \(y\) is sampled according to the probability distribution:
{\small
\[
\mathbb{P}_{\theta^{*}}\left[y=k \mid s, a_{1}, \ldots, a_{K}\right] = \frac{\exp \left(r_{\theta^{*}}\left(s, a_{k}\right)\right)}{\sum_{j=1}^{K} \exp \left(r_{\theta^{*}}\left(s, a_{j}\right)\right)}
\]}

For the Plackett-Luce (PL) model \cite{pla75, luc12} for \(K\)-wise comparisons between actions, let \(\Pi\) be the set of all permutations \(\pi: [K] \rightarrow [K]\), which denotes the ranking of  \(K\) actions, where \(a_{\pi(j)}\) denotes the \(j\)-th ranked action. Under the PL model, we define the loss of a permutation \(\pi \in \Pi\) for a state \(s\) as
{\[
\ell(\theta ; s, \pi) = -\log \left(\prod_{j=1}^{K} \frac{\exp \left(r_{\theta}\left(s, a_{\pi(j)}\right)\right)}{\sum_{k^{\prime}=j}^{K} \exp \left(r_{\theta}\left(s, a_{\pi\left(k^{\prime}\right)}\right)\right)}\right).
\]}

\begin{theorem}\label{thm:4}

Under the same setting of Theorem~\ref{thm:3}, the output \(\hat{\theta} \) of Algorithm~\ref{alg:aup-rlhf} with loss function $\ell(\theta ; s, \pi)$  satisfies

\[
\mathbb{E} \|\hat{\theta} - \theta^*\| \leq \widetilde{O}(\frac{ K^2 L}{\kappa \gamma} \left[\frac{1}{\sqrt{mn}} + \frac{\sqrt{d}}{\sqrt{m}n\varepsilon}\right])
\]
The Big-$\tilde{O}$ notation omits other logarithmic terms. 
\end{theorem}

\begin{remark}
When \( K = 2 \) or  \( K = O(1) \), the PL model reduces to the pairwise comparison considered in the BTL model, yielding the same utility as in Theorem~\ref{thm:3}.  
%Furthermore, another option is to replace the joint distribution of \( K \)-ranking data with \( \frac{K(K - 1)}{2} \) pairs of pairwise comparisons, in which case the loss becomes the product of the marginals~\cite{kwise} \di{what do you want to say?}.
\end{remark}
\subsection{Lower Bound of User-level DP-RLHF}\label{app:lowerbound}
On the other side, we will study the lower bound of the estimation error for user-level DP-RLHF. 
Consider a family of distributions $\mathcal{P}$ over $(\mathcal{X}^m)^n$, where $\mathcal{X}$ represents the data universe, $m$ denotes the sample size, and $n$ refers to the user size. Our goal is to estimate a parameter $\theta$, which is a mapping $\theta: \mathcal{P} \rightarrow \theta$ that characterizes the underlying distribution.
To quantify the estimation error, we define a pseudo-metric $\rho: \Theta \times \Theta \rightarrow \mathbb{R}_{+}$, which serves as the loss function for evaluating the accuracy of the estimate. The minimax risk under the loss function $\rho$ for the class $\mathcal{P}$ is given by:
$$
R(\mathcal{P}, \rho):=\min _{\hat{\theta}} \max _{P \in \mathcal{P}} \mathbb{E}_{X \sim P}[\rho(\hat{\theta}(X), \theta(P))].
$$
In the following, we focus on estimating an unknown parameter ${\theta}$, within the BTL model under a use-level privacy framework. Data is collected from $n$ users, where each user provides $m$ samples. We consider a fixed design setup, meaning that the feature vectors $x_{i, j} \in \mathbb{R}^d$ for samples $i \in[n]$ from user $j \in[m]$ are known. Our objective is to infer $\theta$ based on a sequence of private responses $\tilde{y}_{i, j}$.

% Specifically, we will show that for any $(\varepsilon,\delta)$-user-level-DP algorithm, its output always achieves an estimation error of $\Omega(\cdot)$ to $\theta^*$. 
\begin{theorem}\label{thm:lower_bound}
    For a large enough $n, m$, any private estimator $\widehat{\theta}$ based on samples form the BTL model that satisfies $(\varepsilon, \delta)$-user-level-label DP has the estimation error lower bounded as
    $$
    \mathbb{E}\left[\left\|\widehat{\theta}-\theta^*\right\|\right] \geq \Omega\left(\frac{d}{\sqrt{nm}}+\frac{\sqrt{d}}{\sqrt{m} n(\varepsilon+\delta)}\right).
    $$
\end{theorem}
\begin{remark}
    The estimation bound in Theorem~\ref{thm:lower_bound} consists of two components: a non-private term and a private term. Compared to the item-level setting in \cite{rewarddp}, both the non-private and the private terms include an additional factor of $1/\sqrt{m}$. In particular, when $m=1$, our results reduce to those in \cite{rewarddp}, demonstrating consistency and generality with their findings.
\end{remark}

\section{Omitted Proofs} 
\subsection{{\bf Proof of Theorem \ref{thm:1}}}
\begin{proof}
First recall from our de-biased loss function ~\ref{eq:rrloss}
\[
\hat{l}_{D,\varepsilon}(\theta) = -\frac{1}{nm} \sum_{i=1}^{n} \sum_{j=1}^{m}\left[ 1(\tilde{y}_{i,j} = 1) \left( \sigma(\varepsilon/m) \log \sigma(\theta^\top x_{i,j}) - (1 - \sigma(\varepsilon/m)) \log (1 - \sigma(\theta^\top x_{i,j})) \right) \right.
\]
\[
\left. + 1(\tilde{y}_{i,j} = 0) \left( \sigma(\varepsilon/m) \log (1 - \sigma(\theta^\top x_{i,j})) - (1 - \sigma(\varepsilon/m)) \log \sigma(\theta^\top x_{i,j}) \right) \right].
\]

The gradient of the loss function is given by
\[
\nabla \hat{l}_{D,\varepsilon}(\theta) = -\frac{1}{nm} \sum_{i=1}^{n}\sum_{j=1}^{m} V_{\theta,i,j}\cdot x_{i,j} = -\frac{1}{nm} X^\top V_\theta,
\]
where
{\small\begin{align*}
V_{\theta,i,j} &= 1(\tilde{y}_{i,j} = 1) \left( \frac{\sigma'(\theta^\top x_{i,j})}{\sigma(\theta^\top x_{i,j})} \sigma\left(\frac{\varepsilon}{m})\right) + \frac{\sigma'(\theta^\top x_{i,j})}{1 - \sigma(\theta^\top x_{i,j})} (1 - \sigma\left(\frac{\varepsilon}{m})\right) \right) \\
&- 1(\tilde{y}_{i,j} = 0) \left( \frac{\sigma'(\theta^\top x_{i,j})}{1 - \sigma(\theta^\top x_{i,j})} \sigma\left(\frac{\varepsilon}{m})\right) + \frac{\sigma'(\theta^\top x_{i,j})}{\sigma(\theta^\top x_{i,j})} (1 - \sigma\left(\frac{\varepsilon}{m})\right)) \right).
\end{align*}}

It holds that
{\small
\[
\mathbb{E}_\theta[V_{\theta,i,j} | x_{i,j}] = \left( \sigma(\theta^\top x_{i,j}) \sigma(\varepsilon/m) + (1 - \sigma(\theta^\top x_{i,j})) (1 - \sigma(\varepsilon/m)) \right) \left( \frac{\sigma'(\theta^\top x_{i,j})}{\sigma(\theta^\top x_{i,j})} \sigma(\varepsilon/m) + \frac{\sigma'(\theta^\top x_{i,j})}{1 - \sigma(\theta^\top x_{i,j})} (1 - \sigma(\varepsilon/m)) \right)
\]
\[
- \left( (1 - \sigma(\theta^\top x_{i,j})) \sigma(\varepsilon/m) + \sigma(\theta^\top x_{i,j}) (1 - \sigma(\varepsilon/m)) \right) \left( \frac{\sigma'(\theta^\top x_{i,j})}{1 - \sigma(\theta^\top x_{i,j})} \sigma(\varepsilon/m) + \frac{\sigma'(\theta^\top x_{i,j})}{\sigma(\theta^\top x_{i,j})} (1 - \sigma(\varepsilon/m)) \right) = 0.
\]
}
Furthermore, we have
\[
|V_{\theta,i,j}|_{\tilde{y}_{i,j}=1} = \frac{\sigma'(\theta^\top x_{i,j})}{\sigma(\theta^\top x_{i,j})} \sigma(\varepsilon/m) + \frac{\sigma'(\theta^\top x_{i,j})}{1 - \sigma(\theta^\top x_{i,j})} (1 - \sigma(\varepsilon/m)),
\]
\[
|V_{\theta,i,j}|_{\tilde{y}_{i,j}=0} = \frac{\sigma'(\theta^\top x_{i,j})}{1 - \sigma(\theta^\top x_{i,j})} \sigma(\varepsilon/m) + \frac{\sigma'(\theta^\top x_{i,j})}{\sigma(\theta^\top x_{i,j})} (1 - \sigma(\varepsilon/m)).
\]
The first derivative of the logistic function $\sigma(\cdot)$ is given by $\sigma'(z) = \sigma(z)(1 - \sigma(z))$, which gives us
\[
|V_{\theta,i,j}|_{\tilde{y}_{i,j}=1} = (1 - \sigma(\theta^\top x_{i,j}))\sigma(\varepsilon/m) + \sigma(\theta^\top x_{i,j})(1 - \sigma(\varepsilon/m)) = \mathbb{P}_\theta[\tilde{y}_{i,j} = 0 | x_{i,j}]
\]
\[
|V_{\theta,i,j}|_{\tilde{y}_{i,j}=0} = \sigma(\theta^\top x_{i,j})\sigma(\varepsilon/m) + (1 - \sigma(\theta^\top x_{i,j}))(1 - \sigma(\varepsilon/m)) = \mathbb{P}_\theta[\tilde{y}_{i,j} = 1 | x_{i,j}].
\]
Therefore, it holds that $V_{\theta,i,j}$ is zero-mean and $\nu = 1$ sub-Gaussian under the conditional distribution $\mathbb{P}_\theta[ \cdot | x_{i,j}]$.

Now the Hessian of the loss function is given by
\[
\nabla^2 \hat{l}_{D,\varepsilon}(\theta) = \frac{1}{nm} \sum_{i=1}^{n} \sum_{j=1}^{m}\left[ 1(\tilde{y}_{i,j} = 1) \left( (1 - \sigma(\varepsilon/m)) \nabla^2 \log (1 - \sigma(\theta^\top x_{i,j})) - \sigma(\varepsilon/m) \nabla^2 \log \sigma(\theta^\top x_{i,j}) \right) \right.
\]
\[
\left. + 1(\tilde{y}_{i,j} = 0) \left( (1 - \sigma(\varepsilon/m)) \nabla^2 \log \sigma(\theta^\top x_{i,j}) - \sigma(\varepsilon/m) \nabla^2 \log (1 - \sigma(\theta^\top x_{i,j})) \right) \right],
\]
where
\[
\nabla^2 \log \sigma(\theta^\top x_{i,j}) = \frac{\sigma''(\theta^\top x_{i,j}) \sigma(\theta^\top x_{i,j}) - \sigma'(\theta^\top x_{i,j})^2}{\sigma(\theta^\top x_{i,j})^2} x_{i,j} x_{i,j}^\top,
\]
\[
\nabla^2 \log (1 - \sigma(\theta^\top x_{i,j})) = -\frac{\sigma''(\theta^\top x_{i,j}) (1 - \sigma(\theta^\top x_{i,j})) + \sigma'(\theta^\top x_{i,j})^2}{(1 - \sigma(\theta^\top x_{i,j}))^2} x_{i,j} x_{i,j}^\top.
\]

Now the second derivative of the logistic function $\sigma(\cdot)$ is given by $\sigma''(z) = \sigma'(z)(1 - 2\sigma(z))$, which gives us
\[
\nabla^2 \log \sigma(\theta^\top x_{i,j}) = \nabla^2 \log (1 - \sigma(\theta^\top x_{i,j})) = -\sigma'(\theta^\top x_{i,j}) x_{i,j} x_{i,j}^\top.
\]
Hence, the Hessian of the loss function takes the form
\[
\nabla^2 \hat{l}_{D,\varepsilon}(\theta) = \frac{1}{nm} \sum_{i=1}^{n}\sum_{j=1}^{m} \left[ 1(\tilde{y}_{i,j} = 1)(2\sigma(\varepsilon/m) - 1)\sigma'(\theta^\top x_{i,j}) + 1(\tilde{y}_{i,j} = 0)(2\sigma(\varepsilon/m) - 1)\sigma'(\theta^\top x_{i,j}) \right] x_{i,j} x_{i,j}^\top.
\]

Now, under Assumption~\ref{ass:1}, observe that $\sigma'(\theta^\top x_i) \geq \gamma$ for all $\theta \in \Theta_B$, where $\gamma = \frac{1}{2 + \exp(-2LB) + \exp(2LB)}$. This implies that $\hat{l}_{D,\varepsilon}$ is $\gamma_\varepsilon := \gamma(2\sigma(\varepsilon/m) - 1)$ strongly convex in $\Theta_B$ for all $\varepsilon > 0$ with respect to the semi-norm .
Since $\theta^* \in \Theta_B$, introducing the error vector $\Delta = \hat{\theta}_{RR} - \theta^*$, we conclude that
\[
\gamma_\varepsilon \|\Delta\|_{\Sigma_D}^2 \leqslant \left\| \nabla \hat{l}_{D,\varepsilon}(\theta^*) \right\|_{\Sigma_D^{-1}}^2 \|\Delta\|_{\Sigma_D }
\]
Introducing $M = \frac{1}{n^2} X \Sigma_D ^{-1} X^\top$, we now have $\left\| \nabla l_{D,\varepsilon}(\theta^*) \right\|_{\Sigma_D ^{-1}}^2 = V_{\theta^*}^\top M V_{\theta^*}$. Then, the Bernstein's inequality for sub-Gaussian random variables in quadratic form (see e.g. Theorem 2.1 in \cite{hsu2012tail}) implies that with probability at least $1 - \alpha$,
\[
\left\| \nabla \hat{l}_{D,\varepsilon}(\theta^*) \right\|_{\Sigma_D ^{-1}}^2 = V_{\theta^*}^\top M V_{\theta^*} \leqslant \upsilon^2 \left( \text{tr}(M) + 2\sqrt{\text{tr}(M^\top M) \log(1/\alpha)} + 2 \|M\| \log(1/\alpha) \right)
\]
\[
\leqslant C_1 \cdot \upsilon^2 \cdot \frac{d + \log(1/\alpha)}{nm}
\]
For some $C_1 > 0$. This gives us
\[
\gamma_\varepsilon \|\Delta\|_{\Sigma_D + } \leqslant \left\| \nabla \hat{l}_{D,\varepsilon}(\theta^*) \right\|_{\Sigma_D ^{-1}} \leqslant \sqrt{C_1 \cdot \upsilon^2 \cdot \frac{d + \log(1/\alpha)}{nm}}  .
\]

Solving for the above inequality, we get
\[
\|\Delta\|_{\Sigma_D} \leqslant C_2 \cdot \sqrt{\frac{\upsilon^2}{\gamma_\varepsilon^2} \cdot \frac{d + \log(1/\alpha)}{nm} }
\]
for some constant $C_2 > 0$. Now note that $\frac{\upsilon}{\gamma_\varepsilon} = \frac{1}{\gamma} \cdot \frac{e^{\varepsilon/m} + 1}{e^{\varepsilon/m} - 1}$. Hence, we get
\[
\left\| \hat{\theta}_{RR} - \theta^* \right\|_{\Sigma_D } \leqslant \frac{C}{\gamma} \cdot \frac{e^{\varepsilon/m} + 1}{e^{\varepsilon/m} - 1} \sqrt{\frac{d + \log(1/\alpha)}{nm}} ,
\]
Equivalently,
\[
\left\| \hat{\theta}_{RR} - \theta^* \right\| \leqslant \frac{C}{\gamma\sqrt{\lambda_{\min}(\Sigma_{\mathcal{D}})}} \cdot \frac{e^{\varepsilon/m} + 1}{e^{\varepsilon/m} - 1} \sqrt{\frac{d + \log(1/\alpha)}{nm}} ,
\]
for some $C > 0$, which holds for any $\varepsilon \in (0, \infty)$. This completes our proof.
\end{proof}
\subsection{{\bf Proof of Theorem \ref{thm:2}}}
\begin{proof}
As the subsets  $D_i$ are disjoint. It is sufficient to show Algorithm~\ref{alg:aup-SGD} is $(\varepsilon,\delta)$ user-level (label) DP.

To prove the privacy guarantee, note that there are two components related to privacy in each iteration of Algorithm \ref{alg:aup-SGD}, the AboveThreshold (step 9) and private mean oracle (step 10-15). For the AboveThreshold, as the query $s_t^c$. Thus, by~\cite{dwork2014algorithmic}, we can see it will be $\frac{\varepsilon}{2}$-DP for $T$ iterations in total. Thus, it is sufficient for us to show steps 10-15 are $(\frac{\varepsilon}{2},\delta)$-user-level (label) DP. 

 Since it consists of $T$-folds,  we first recall the advanced composition theorem \cite{dwork2014algorithmic}. 

    \begin{lemma}[Advanced Composition Theorem \cite{dwork2014algorithmic}]\label{lemma:adv}
Given target privacy parameters $0<\varepsilon <1$ and $0<\delta<1$, to ensure $(\varepsilon, T\delta'+\delta)$-DP over $T$ mechanisms, it suffices that each mechanism is $(\varepsilon',\delta')$-DP, where $\varepsilon'=\frac{\varepsilon}{2\sqrt{2T\ln(2/\delta)}}$ and $\delta'=\frac{\delta}{T}$.  
\end{lemma} 
Thus, we have to show each iteration satisfies $(\varepsilon',\delta')$-DP, where $\varepsilon'=\frac{\varepsilon}{4\sqrt{2T\ln(2/\delta)}}$ and $\delta'=\frac{\delta}{T}$. 
We then recall the following privacy amplification via Poison subsampling. 
\end{proof}
\begin{lemma}[\citep{bassily2014private,beimel2010bounds}]
Let $A$ be an $(\varepsilon, \delta)$-DP algorithm. Now we construct the algorithm $B$ as follows: On input $D=\{x_1, \cdots, x_n\}$, first we construct a new sub-sampled dataset $D_S$ where each $x_i\in D_s$ with probability $q$.  Then we run algorithm $A$ on the dataset $D_S$. Then $B(D)=A(D_S)$ is $(\tilde{\varepsilon}, \tilde{\delta})$-DP, where $\tilde{\varepsilon}=O(q\varepsilon)$ and $\tilde{\delta}=q\delta$. 
\end{lemma}
Note that when there is no subsampling, i.e., the batch size is $n$. Then when $\sigma=O(\frac{\varepsilon}{\sqrt{T\ln (1/\delta)}})$, it will be the private mean oracle as in  \cite{userleveldp}. Thus, it will be $(\varepsilon',\delta')$-user-level (label) DP and the $T$-fold composition is $(\varepsilon,\delta)$-user-level (label) DP. 

Thus, when there is Poison subsampling with the probability $\frac{\tilde{n}}{n}$, the whole algorithm (step 10-15) will be $(O(\frac{\tilde{n}}{n}\varepsilon), \frac{\tilde{n}}{n}\delta)$-user-level (label) DP. That is when we take $\sigma=\tilde{O}(\frac{n}{\tilde{n}}  \frac{\varepsilon}{\sqrt{T\ln (1/\delta)}})$, it will be $(\varepsilon',\delta')$-user-level (label)DP and the $T$-fold composition is $(\varepsilon,\delta)$-user-level (label) DP.


\subsection{{\bf Proof of Theorem~\ref{thm:3}}}
We begin with the definition of Lipschitz continues and strongly convexity.
\begin{definition}
    % Lipschitz Continuity
    \textbf{(Lipschitz Continuity)} A function \( f : \mathbb{R}^n \to \mathbb{R} \) is said to be Lipschitz continuous with constant \( L \) if there exists a constant \( L \geq 0 \) such that for all \( x, y \in \mathbb{R}^n \),
\[
\| f(x) - f(y) \| \leq L \| x - y \|
\]
where \( \| \cdot \| \) denotes the norm in \( \mathbb{R}^n \) (typically the Euclidean norm).
\end{definition}
\begin{definition}
    % Strong Convexity
\textbf{(Strong Convexity)} A function \( f : \mathbb{R}^n \to \mathbb{R} \) is said to be strongly convex with parameter \( \mu > 0 \) if there exists a constant \( \mu > 0 \) such that for all \( x, y \in \mathbb{R}^n \), we have:
\[
f(y) \geq f(x) + \nabla f(x)^\top (y - x) + \frac{\mu}{2} \| y - x \|^2
\]
where \( \mu \) is the strong convexity parameter.
\end{definition}
We can see when we use the full batch in Algorithm \ref{alg:aup-SGD}, i.e.,  \(\tilde{n}_i=n_i\), based on our parameters, Algorithm \ref{alg:aup-rlhf} will be the same as the method in \cite{userleveldp} for DP-SCO with strongly convex loss in the user-level DP setting.  Specifically, we have the following result by Theorem 4.11 in \cite{userleveldp} 

\begin{lemma}[Derived by Theorem 4.11 in \cite{userleveldp}]\label{lemma:2} 
 For  \(0<\varepsilon<10, 0<\delta<1\),  if the  population loss function 
 \[
l_{\mathcal{P}, \varepsilon}(\theta) = \mathbb{E}_{s \sim \rho(\cdot),\,(a^{0}, a^{1}) \sim \mu(\cdot \mid s)} \mathbb{I} \{ y  = 1 \}  \log \sigma(\theta^T x )  + \mathbb{I}\{y  = 0 \}  \log(1 -  \sigma(\theta^T x )) 
\]
is  \(\kappa \gamma\)-strongly convex  and $4L$-Lipschitz continuous, under the same parameters in Theorem \ref{thm:3}, Algorithm \ref{alg:aup-rlhf} outputs  $\widehat{\theta}$  such that
\[
\mathbb{E}\left[L_P(\widehat{\theta})-\min _{\theta^{\star} \in \Theta} L_P\left(\theta^{\star}\right)\right] \leq O\left(\frac{L^{2}}{\kappa \gamma } \cdot\left(\frac{1}{n m}+\frac{d \log ^{2}(n d m / \delta)}{n^{2} m \varepsilon^{2}}\right)\right)
\]
\end{lemma}
Thus, it is sufficient for us to show that  population loss function is  \(\kappa \gamma\)-strongly convex and $4L$-Lipschitz continuous under Assumption~\ref{ass:1} and ~\ref{ass:2}. 

\textbf{Proof of Theorem~\ref{thm:3}} 
By strongly convexity, we have 
\[
\mathbb{E} \|\hat{\theta}_k - \theta^{*}\|^2 \leq \frac{2}{\kappa \gamma} \mathbb{E}\left[\widehat{l}_{\mathcal{P}, \varepsilon}(\hat{\theta}_k)-\widehat{l}_{\mathcal{P}, \varepsilon}\left(\theta^{*}\right)\right]
\]
\[
\leq O\left(\frac{L^2}{\kappa^2 \gamma^2} \cdot\left(\frac{1}{n m}+\frac{d \log ^{2}(n d m / \delta)}{n^{2} m \varepsilon^{2}}\right)\right)
\]

The second inequality comes from Lemma~\ref{lemma:2}. 


\begin{lemma}\label{lemma:1}

Under Assumption~\ref{ass:1} and ~\ref{ass:2}, the loss function
\[
l_{\mathcal{P}, \varepsilon}(\theta) = \mathbb{E}_{s \sim \rho(\cdot),\,(a^{0}, a^{1}) \sim \mu(\cdot \mid s)} \mathbb{I} \{ y  = 1 \}  \log \sigma(\theta^T x )  + \mathbb{I}\{y  = 0 \}  \log(1 -  \sigma(\theta^T x )) 
\]
is \( \kappa\gamma \)-strongly convex and $4L$-Lipschitz continuous.
\end{lemma}

\begin{proof}[\textbf{Proof of Lemma~\ref{lemma:1}}]


\textbf{Lipschitz continuous}\\
%We let
%\[\nabla l_{\mathcal{P}, \varepsilon}(\theta) = \mathbb{E}_{s \sim \rho(\cdot),\,(a^{0}, a^{1}) \sim \mu(\cdot \mid s)} \mathbb{I} \{ \tilde{y}  = 1 \}  \log \sigma(\theta^T x )  + \mathbb{I}\{\tilde{y}  = 0 \}  \log(1 -  \sigma(\theta^T x )) \]

The gradient of the loss function is given by
\[
\nabla l_{\mathcal{P}, \varepsilon}(\theta) =  \mathbb{E}_{s \sim \rho(\cdot),\,(a^{0}, a^{1}) \sim \mu(\cdot \mid s)} \left[V_{\theta}x\right]
\]

where
\[
V_{\theta} = \mathbb{I}\{y  = 1\} \frac{\sigma'(\theta^T x )}{\sigma(\theta^T x )}  -  \mathbb{I}\{y  = 0\} \frac{\sigma'(\theta^T x )}{1 - \sigma(\theta^T x )}.
\]


Furthermore, we have
\[
\lvert V_{\theta} \rvert_{y = 1} = \frac{\sigma'(\theta^T x )}{\sigma(\theta^T x )}  = 1 - \sigma(\theta^T x ) \leq 1.
\]
\[
\lvert V_{\theta} \rvert_{y = 0} \leq 1.
\]

Therefore, by Assumption~\ref{ass:1}, it holds that 
\[
|\nabla l_{\mathcal{P}, \varepsilon}(\theta)| \leq 
 \lvert V_{\theta} \rvert \cdot |x | \leq 4 L
\]
This means \(l_{ \mathcal{P},\varepsilon}(\theta)\) is 4L - Lipschitz continuous\\
\textbf{Strongly convex}\\
Now, the Hessian of the loss function is given by

\[
\nabla^2 l_{\mathcal{P}, \varepsilon}(\theta) =  \mathbb{E}_{s \sim \rho(\cdot),\,(a^{0}, a^{1}) \sim \mu(\cdot \mid s)} \left[ \mathbb{I}\{y = 1\}  \sigma'(\theta^T x) + \mathbb{I}\{y = 0\}  \sigma'(\theta^T x) \right] x x^T.
\]

we observe that \( \sigma'(\theta^T x) \geq \gamma \) for all \(\theta \in \Theta_{B}\), where
\[
\gamma = \frac{1}{2 + \exp(-2 L B) + \exp(2 L B)}.
\]

by Assumption~\ref{ass:2},
\[
v^T \nabla^2l_{\mathcal{P}, \varepsilon}(\theta) v \geq  \kappa \gamma \|v\|_{2}^2.
\]

This implies that \(l_{\mathcal{P}, \varepsilon}\) is \(\kappa \gamma\)-strongly convex in \(\Theta_{B}\).

\end{proof} 






\iffalse 
\subsection{Proof of Remark~\ref{rmk:2}(follow Proof of Theorem 1 in \cite{klrlhf})}

\textbf{Proof:}  If we set $\hat{r}(s, a) = \langle \hat{\theta}, \phi(s, a) \rangle$, and take the policy by
\[
\hat{\pi} = \arg\max_{\pi \in \Pi} \left\{ \langle \hat{\theta}, \mathbb{E}_{s \sim \rho} \phi(s, \pi) \rangle  - \eta \cdot \mathbb{E}_{s \sim \rho} \left[ D_{\text{KL}}(\pi(\cdot|s) \| \pi_0(\cdot|s)) \right] \right\}.
\]

Then, we have
\[
 \langle \hat{\theta}, \mathbb{E}_{s \sim \rho} [\phi(s, \pi) - \phi(s, \hat{\pi})] \rangle 
  + \eta \cdot \mathbb{E}_{s \sim \rho} \left[ D_{\text{KL}}(\pi(\cdot|s) \| \pi_0(\cdot|s)) - D_{\text{KL}}(\hat{\pi}(\cdot|s) \| \pi_0(\cdot|s)) \right] \leq 0.
\]

For simplicity, we denote the LHS of Equation above as $(\star)$. We plugging this into the estimation of $J(\pi) - J(\hat{\pi})$:
\begin{align*}
\mathbb{E}_{\hat{\theta}}\left[J(\pi) - J(\hat{\pi})\right] &= \mathbb{E}_{\hat{\theta}}\left[\mathbb{E}_{s \sim \rho} \left[ \mathbb{E}_{a \sim \pi(\cdot|s)} [r^*(s, a) + \eta \log \frac{\pi_0(a|s)}{\pi(a|s)}] - \mathbb{E}_{a \sim \hat{\pi}(\cdot|s)} [r^*(s, a) + \eta \log \frac{\pi_0(a|s)}{\hat{\pi}(a|s)}] \right]\right] \\
&= \mathbb{E}_{\hat{\theta}}[(\star) + \langle \theta^* - \hat{\theta}, \mathbb{E}_{s \sim \rho} [\phi(s, \pi)] \rangle + \langle \hat{\theta} - \theta^*, \mathbb{E}_{s \sim \rho} [\phi(s, \hat{\pi})] \rangle \\
&\leq \mathbb{E}_{\hat{\theta}}[ \langle \theta^* - \hat{\theta}, \mathbb{E}_{s \sim \rho} [\phi(s, \pi) ] \rangle + \langle \hat{\theta} - \theta^*, \mathbb{E}_{s \sim \rho} [\phi(s, \hat{\pi}) ] \rangle \\
&\leq 2 L^2 \cdot \mathbb{E} \|\hat{\theta} - \theta^{*}\|^2,
\end{align*}
where the last inequality uses Cauchy-Schwarz inequality and Assumption~\ref{ass:1}.
\fi 
\subsection{{\bf Proof of Theorem~\ref{thm:4}}}
We follow the proof of Theorem~\ref{thm:3}. First, we demonstrate that the objective loss function \( \mathcal{L}_{\mathcal{P}}(\theta, \pi) \) is \( \kappa\gamma \)-strongly convex and \( 2LK^2 \)-Lipschitz continuous. Then, with the help of the lemma~\ref{lemma:2}, the proof is completed.
\begin{lemma}~\label{lemma:5}
We define \(\mathcal{L}_{\mathcal{P}}(\theta , \pi ) = \mathbb{E}_{s \sim \rho(\cdot),\, (a_1, \ldots, a_K) \sim \mu(\cdot \mid s)} \ell(\theta ; s, \pi) \). 
 Under Assumption~\ref{ass:1} and ~\ref{ass:2} , \(\mathcal{L}_{\mathcal{P}}(\theta , \pi)\) is \(\kappa\gamma\)-strongly convex and \(2LK^2\)- Lipschitz continuous, where \(\gamma = \exp(-4LB)/2\). 
\end{lemma}

\begin{proof}[\textbf{Proof of Lemma~\ref{lemma:5}}]
Let \( s \) be a state and \( a_1, \ldots, a_K \) be \( K \) actions to be compared at that state. Let the label/preference feedback \( y \in \{1, 2, \ldots, K\} \) indicate which action is most preferred by the human labeler. Let
\[
x_{i,j} = \phi(s, a_i) - \phi(s, a_j), \quad 1 \leq i \neq j \leq K
\]
be the feature difference between actions \( a_i \) and \( a_j \) at state \( s \). Define the population covariance matrix
\[
\Sigma_{i,j} = \mathbb{E}_{s \sim \rho(\cdot), (a_1, \ldots, a_K) \sim \mu(\cdot \mid s)}\left[x_{i,j} x_{i,j}^\top \right].
\]

\begin{assumption}(Coverage of feature space)\label{ass:3}
 The data distributions \( \rho \), \( \mu \) are such that
\[
\lambda_{\min} \left(\Sigma_{i,j}\right) \geq \kappa \quad \text{for some constant} \quad \kappa > 0 \quad \text{for all} \quad 1 \leq i \neq j \leq K.
\]   
\end{assumption} 



\[
\ell(\theta) = -\frac{1}{n} \sum_{i=1}^{n}\sum_{j=1}^{K} \log \left( \frac{\exp(\langle \theta, \phi(s^i, a_{\pi(j)}^i) \rangle)}{\sum_{k=j}^{K} \exp(\langle \theta, \phi(s^i, a_{\pi(k)}^i) \rangle)} \right)
\]


\textbf{Strongly convexity of} \(\mathcal{L}_{\mathcal{P}}(\theta , \pi )\)(follow Proof of Theorem 4.1 in \cite{kwise})




The Hessian of the negative log likelihood can be written as
{\small\[
\nabla^2 \mathcal{L}_{P}(\theta, \pi) =  \mathbb{E} \sum_{j=1}^{K} \sum_{k=j}^{K} \frac{\exp\left(\langle \theta, \phi(s , a_{\pi (j)}) + \phi(s , a_{\pi (k)}) \rangle\right)}{\left(\sum_{k'=j}^{K} \exp\left(\langle \theta, \phi(s , a_{\pi (k')}) \rangle\right)\right)^2} \cdot \left(\phi(s , a_{\pi (j)}) - \phi(s , a_{\pi (k)})\right) \left(\phi(s , a_{\pi (j)}) - \phi(s , a_{\pi (k)})\right)^{\top}
\]}

Since \(\exp(\theta, \phi)\) \(\in\) \([\exp(-LB), \exp(LB)]\), we know that the coefficients satisfy
\[
\frac{\exp\left(\langle \theta, \phi(s , a_{\pi (j)}) + \phi(s , a_{\pi (k)}) \rangle\right)}{\left(\sum_{k'=j}^{K} \exp\left(\langle \theta, \phi(s , a_{\pi (k')}) \rangle\right)\right)^2} \geq \frac{\exp(-4LB)}{2(K+1-j)^2}
\]

Set \(\gamma = \exp(-4LB)/2\). We can verify that for any vector \(v \in \mathbb{R}^K\), one has
\[
v^{\top} \nabla^2 \mathcal{L}_{P}(\theta, \pi) v \geq \mathbb{E} \gamma  v^{\top} \left[\sum_{j=1}^{K} \frac{1}{(K+1-j)^2} \sum_{k'=k}^{K}  \sum_{k=j}^{K} \left(\phi(s , a_{\pi (j)}) - \phi(s , a_{\pi })\right) \left(\phi(s , a_{\pi (j)}) - \phi(s , a_{\pi (k)})\right)^{\top}\right] v
\]

\[
\geq \mathbb{E} \gamma v^{\top} \left[ \min_{\pi \in \prod(K)}  \sum_{j=1}^{K} \frac{1}{(K+1-j)^2} \sum_{k=j}^{K} \sum_{k'=k}^{K} \left(\phi(s , a_{\pi (j)}) - \phi(s , a_{\pi (k)})\right)(\phi(s , a_{\pi (j)}) - \phi(s , a_{\pi (k)}))^{\top}\right] v
\]

\[
 \geq  \kappa \gamma \|v\|_{2}^2
\]

The last inequality uses Assumption ~\ref{ass:3}.\\
So, \(\mathcal{L}_{\mathcal{P}}(\theta , \pi )\) is \(\kappa\gamma\)-strongly convex with respect to \(\ell_{2}\)-norm.

\textbf{Lipschitz continuous}\\
The gradient of the negative log likelihood is
\[
\nabla \ell\left(\theta, \pi\right) = - \sum_{j=1}^{K} \sum_{k=j}^{K} \frac{\exp \left(\left\langle \theta, \phi\left(s , a_{\pi(k)} \right) \right\rangle \right)}{\sum_{k^{\prime}=j}^{K-1} \exp \left(\left\langle \theta, \phi\left(s , a_{\pi\left(k^{\prime}\right)} \right) \right\rangle \right)} \cdot \left(\phi\left(s , a_{\pi(j)} \right) - \phi\left(s , a_{\pi(k)} \right)\right).
\]

We set \( x_{j k}  = \phi\left(s , a_{j} \right) - \phi\left(s , a_{k} \right) \). \( X \in \mathbb{R}^{(K(K-1) / 2) \times d} \) has the differencing vector \( x_{j k}  \) as its \(( k + \sum_{l=K-j+1}^{K} l )^{\text{th}}\) row. We also define \( V_{j k}  \) to be the random variable of the coefficient of \( x_{j k}  \) in equation above under the PL model, i.e., conditioned on an arbitrary permutation \( \pi \).

\[
V_{jk}  = \begin{cases}
\frac{\exp \left(\left\langle\theta, \phi\left(s , a_{k} \right)\right\rangle\right)}{\sum_{k'=\pi^{-1}(j)}^{K} \exp \left(\left\langle\theta, \phi\left(s , a_{\pi(k')} \right)\right\rangle\right)}, & \text{if } \pi^{-1}(j) < \pi^{-1}(k) \\
-\frac{\exp \left(\left\langle\theta, \phi\left(s , a_{j}\right)\right\rangle\right)}{\sum_{k'=\pi^{-1}(k)}^{K} \exp \left(\left\langle\theta, \phi\left(s , a_{\pi(k')} \right)\right\rangle\right)}, & \text{otherwise.}
\end{cases}
\]

Here, \( \pi^{-1}(j) < \pi^{-1}(k) \) means that the \( j \)-th item ranks higher than the \( k \)-th item. Let \( V \in \mathbb{R}^{K(K-1)/2} \) be the concatenated random vector of \( \{V_{jk} \}_{1 \leq j < k \leq K} \).  Furthermore, since under any permutation, the sum of the absolute value of each element in \( V \) is at most \( K \). Thus,\\

\[
\left\|\nabla\ell(\theta, \pi)\right\|_{2}^{2} = V^{\top} X X^{\top} V \leq \|X\|_{2}^{2}\|V\|_{2}^{2} \leq 4 L^2 K^4 
\]

Thus, \(\nabla \ell\left(\theta, \pi\right)\) is \(2 K^2 L \)-Lipschitz continuous, and \(\mathcal{L}_{P}(\theta, \pi)\) is also \(2 K^2 L \)-Lipschitz continuous.
\end{proof}
The rest of the proof is the same as that of Theorem~\ref{thm:3}.

\subsection{\bf Proof of Theorem~\ref{thm:lower_bound}}
To establish our lower bounds, we begin by introducing key background concepts, notations, and properties.
Consider a family of distributions $\mathcal{P}$ over $(\mathcal{X}^m)^n$, where $\mathcal{X}$ represents the data universe, $m$ denotes the sample size, and $n$ refers to the user size. Our goal is to estimate a parameter $\theta$, which is a mapping $\theta: \mathcal{P} \rightarrow \theta$ that characterizes the underlying distribution.

To quantify the estimation error, we define a pseudo-metric $\rho: \Theta \times \Theta \rightarrow \mathbb{R}_{+}$, which serves as the loss function for evaluating the accuracy of the estimate. The minimax risk under the loss function $\rho$ for the class $\mathcal{P}$ is given by:
$$
R(\mathcal{P}, \rho):=\min _{\hat{\theta}} \max _{P \in \mathcal{P}} \mathbb{E}_{X \sim P}[\rho(\hat{\theta}(X), \theta(P))].
$$
In this study, we focus on estimating an unknown parameter, ${\theta}$, within the Bradley-Terry-Luce model under a use-level privacy framework. Data is collected from $n$ users, where each user provides $m$ samples. We consider a fixed design setup, meaning that the feature vectors $x_{i, j} \in \mathbb{R}^d$ for samples $i \in[n]$ from user $j \in[m]$ are predetermined and known. Our objective is to infer $\theta$ based on a sequence of private responses $\tilde{y}_{i, j}$.

Without privacy constraints, responses $y_{i, j}$ follow a logistic model:
\begin{equation}
    \begin{aligned}
    \mathbb{P}\left(y_{i, j}=1 \mid x_{i, j}\right)=\sigma\left(\theta^{\top} x_{i, j}\right)=\frac{1}{1+\exp \left(-\theta^{\top} x_{i, j}\right)}, \quad
    \mathbb{P}\left(y_{i, j}=0 \mid x_{i, j}\right)=1-\sigma\left(\theta^{\top} x_{i, j}\right).
\end{aligned}
\end{equation}
We refer to this distribution family as $\mathcal{P}_{\theta}$. Under the use-level privacy constraint, our goal is to construct an estimator $\hat{\theta}$ that closely approximates $\theta$ while preserving use-level-label differential privacy, ensuring that an entire user's labels remain protected. Depending on the estimation framework, the loss function $\rho$ is defined as either the squared $\ell_2$-norm or a squared semi-norm.

\begin{lemma}[Assouad's lemma \citealt{rewarddp}]\label{lem:assouad}
    Let $\mathcal{V} \subseteq (\mathcal{P})^m$ be a set of distributions indexed by the hypercube $\mathcal{E}_d=\{ \pm 1\}^{d}$. Suppose there exists a $\tau \in \mathbb{R}$ and $\alpha>0$, such that $\rho$ satisfies: (i) for all $u, v, w \in \mathcal{E}_d, \rho\left(\theta\left(P_u\right), \theta\left(P_v\right)\right) \geq 2 \tau \cdot \sum_{i=1}^d \mathbbm{1}\left(u_i \neq v_i\right)$ and (ii) $\rho\left(\theta\left(P_u\right), \theta\left(P_v\right)\right) \leq \alpha\left(\rho\left(\theta\left(P_u\right), \theta\left(P_w\right)\right)+\rho\left(\theta\left(P_v\right), \theta\left(P_w\right)\right)\right)$, i.e., $\alpha$-triangle inequality. For each $i \in[d]$, define the mixture distributions:
    $$
    P_{+i}:=\frac{2}{\left|\mathcal{E}_d\right|} \sum_{e\in \mathcal{E}_d: e_{i}=1} P_{e} \text { and } P_{+i}:=\frac{2}{\left|\mathcal{E}_d\right|} \sum_{e\in \mathcal{E}_d: e_{i}=-1} P_{e}.
    $$
    Then, we have
    $$
    R(\mathcal{P}, \rho) \geq \frac{\tau}{2 \alpha} \sum_{i=1}^d\left(1-\left\|P_{+i}-P_{-i}\right\|_{\mathrm{TV}}\right).
    $$
\end{lemma}

\begin{corollary}\label{cor: R(P,p)}
    Under the same conditions of Lemma~\ref{lem:assouad}, we have
    $$
    R(\mathcal{P}, \rho) \geq \frac{d \tau}{2 \alpha}\left[1-\left(\frac{1}{d} \sum_{i=1}^d \frac{1}{2^d} \sum_{e \in \mathcal{E}_d}\left\|P_e-P_{\bar{e}^i}\right\|_{\mathrm{TV}}^2\right)^{1 / 2}\right].
    $$
    where $\bar{e}^i$ is a vector in $\mathcal{E}_d$ that flips the $i$-th coordinate of $e$.
\end{corollary}

\begin{lemma}[Assouad's lemma \citealt{rewarddp}]
    Let the same conditions of Lemma~\ref{lem:assouad} hold. If for all $i \in[d]$, there exists a coupling $(X, Y)$ between $P_{+i}$ and $P_{-i}$ with $\mathbb{E}\left[d_{\operatorname{Ham}}(X, Y)\right] \leq D$ for some $D \geq 0$, then
    $$
    R(\mathcal{P}, \rho, \varepsilon, \delta) \geq \frac{d \tau}{2 \alpha} \cdot\left(0.9 e^{-10 \varepsilon D}-10 D \delta\right).
    $$
\end{lemma}

\begin{fact}\label{fact:kl}
    Let $p_a=\frac{1}{1+e^t}$ and $p_b=\frac{1}{1+e^b}$. Then, the sum of KL-divergences between the corresponding Bernoulli distributions satisfies:
    $$
    \mathrm{kl}\left(p_a \| p_b\right)+\mathrm{kl}\left(p_b \| p_a\right) \leq(a-b)^2,
    $$
    where $\mathrm{kl}(p \| q)$ represents the KL-divergence between Bernoulli distributions with parameters $p$ and $q$
    $
    D_{\mathrm{KL}}(\operatorname{Bernoulli}(p) \| \operatorname{Bernoulli}(q)) .
    $
\end{fact}




\begin{proof}[\bf Proof of Fact~\ref{fact:kl}]
By direct calculation, the sum of KL-divergences simplifies to:
$$
\mathrm{kl}\left(p_a \| p_b\right)+\operatorname{kl}\left(p_b \| p_a\right)=\left(p_a-p_b\right) \log \left(\frac{p_a}{1-p_a} \cdot \frac{1-p_b}{p_b}\right).
$$
From the definitions of $p_a$ and $p_b$, we substitute:
$$
\left(p_a-p_b\right) \log \left(\frac{p_a}{1-p_a} \cdot \frac{1-p_b}{p_b}\right)=\left(\frac{1}{1+e^a}-\frac{1}{1+e^b}\right) \cdot(b-a).
$$
Now, assuming without loss of generality that $b \geq a$, we bound the difference:
$$
\frac{1}{1+e^a}-\frac{1}{1+e^b} \leq \frac{e^b-e^a}{e^b}=1-e^{a-b}.
$$
Using the inequality $e^{a-b} \geq 1+(a-b)$, we obtain:
$$
1-e^{a-b} \leq b-a.
$$
Thus, combining these results gives the final bound:
$$
\mathrm{kl}\left(p_a \| p_b\right)+\mathrm{kl}\left(p_b \| p_a\right) \leq(a-b)^2.
$$
This completes the proof.
\end{proof}

Now, we proceed to prove Theorem~\ref{thm:lower_bound}, which we break down into two parts: the non-private component and the private component.

Non-private component. We begin by selecting a parameter $\Delta>0$ and defining $\theta_e=\Delta e$ for each $e \in \mathcal{E}_d=\{ \pm 1\}^{d }$. 
Each $P_e$ is a probability distribution associated with a specific vector $e \in \mathcal{E}_d$. This means that for every binary vector $e=\left(e_1, e_2, \ldots, e_d\right)$, there is a corresponding probability distribution $P_e$. Since we consider each user provides $m$ samples, thus here $\mathcal{P}_{\theta}^{\otimes m}$ is a $m$-product distribution.
Our goal is to verify the two conditions stated in Lemma~\ref{lem:assouad}.


First, we observe that the function $\rho=\|\cdot\|_2^2$ adheres to the 2-triangle inequality, meaning $\alpha=2$. Additionally, for any $u, v \in \mathcal{E}_d$, the squared norm difference satisfies
$$
\left\|\theta_u-\theta_v\right\|_2^2=4 \Delta^2 \sum_{i=1}^d 1\left(u_i \neq v_i\right),
$$
indicating that $\tau=2 \Delta^2$.
Now, let $P_e^{n}$ represent the distribution corresponding to $n$ independent samples from $m$ users of the (non-private) observations $y_{i, j}$ when $\theta=\theta_e$. Then, by applying Corollary~\ref{cor: R(P,p)}, we obtain
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2\right) \geq \frac{d \Delta^2}{2}\left[1-\left(\frac{1}{d} \sum_{i=1}^d \frac{1}{2^d} \sum_{e \in \mathcal{E}_d}\left\|P_e^{n}-P_{\bar{e}^i}^{n}\right\|_{\mathrm{TV}}^2\right)^{1 / 2}\right].
$$
Using Pinsker's inequality along with the chain rule for KL-divergence, we obtain the following bound by bounding the total variation (TV) distance for any $u, v \in$ $\mathcal{E}_d:$
$$
\begin{aligned}
\left\|P_u^{n}-P_v^{n}\right\|_{\mathrm{TV}}^2 & \leq \frac{1}{4}\left(D_{\mathrm{KL}}\left(P_u^{n} \| P_v^{n}\right)+D_{\mathrm{KL}}\left(P_v^{n} \| P_u^{n}\right)\right) \\
& =\frac{1}{4} \sum_{j=1}^m\sum_{k=1}^n\left(\mathrm{kl}\left(p_u\left(x_{k,j}\right) \| p_v\left(x_{k,j}\right)\right)+\mathrm{kl}\left(p_v\left(x_{k,j}\right) \| p_u\left(x_{k,j}\right)\right)\right).
\end{aligned}
$$
Next, applying Fact~\ref{fact:kl}, we derive an upper bound on the total variation distance:
$$
\left\|P_u^{n}-P_v^{n}\right\|_{\mathrm{TV}}^2 \leq \frac{\Delta^2}{4} \sum_{j=1}^m\sum_{k=1}^n\left(x_{k,j}^{\top}(u-v)\right)^2.
$$
Moreover, we have
$$
\begin{aligned}
\frac{1}{d 2^d} \sum_{i=1}^d \sum_{e \in \mathcal{E}_d}\left\|P_e^{n}-P_{\bar{e}^i}^{n}\right\|_{\mathrm{TV}}^2 
\leq  \frac{\Delta^2}{4 d} \frac{1}{2^d} \sum_{e \in \mathcal{E}_d} \sum_{i=1}^d \sum_{j=1}^m \sum_{k=1}^n\left(2 x_{k, j, i}\right)^2 .
\end{aligned}
$$
Rewriting in terms of the Frobenius norm,
$$
\frac{1}{d 2^d} \sum_{i=1}^d \sum_{e \in \mathcal{E}_d}\left\|P_e^{n}-P_{\bar{e}^i}^{n}\right\|_{\mathrm{TV}}^2 =\frac{\Delta^2}{d} \frac{1}{2^d} \sum_{e \in \mathcal{E}_d} \sum_{i=1}^d \sum_{j=1}^m \sum_{k=1}^n x_{k, j, i}^2=\frac{\Delta^2}{d} \frac{1}{2^d} \sum_{e \in \mathcal{E}_d}\|X\|_F^2.
$$
Here, $X \in \mathbb{R}^{m n \times d}$ represents the aggregated data matrix, where each row $x_{k, j}^{\top} \in \mathbb{R}^d$ corresponds to a sample from user $j$, and $\|\cdot\|_{\mathrm{F}}$ denotes the Frobenius norm. Using this bound, we derive the following lower bound:
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq \frac{d \Delta^2}{2}\left[1-\left(\frac{\Delta^2}{d}\|X\|_F^2\right)^{1 / 2}\right].
$$
To optimize the bound, we set:
$$
\Delta^2=\frac{d}{4\|X\|_F^2}.
$$
which simplifies the expression to:
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq \frac{d^2}{16\|X\|_{\mathrm{F}}^2}=\frac{d}{m n} \cdot \frac{1}{16 \frac{1}{d m n} \sum_{j=1}^m \sum_{k=1}^n\left\|x_{k, j}\right\|^2}.
$$
Given the assumption that $\left\|x_{k, j}\right\|^2 \leq L^2$, we further refine the bound as:
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq \Omega\left(\frac{d}{L^2} \cdot \frac{d}{m n}\right).
$$

Now we consider the {\bf Private component}. Similar to the non-private part, we verify $\Delta, \theta_e, \rho$. Since we consider user-level privacy, we will assume $\mathcal{P}_{\theta}^{\otimes m}$ is an m-product distribution. Applying Lemma~\ref{lem:assouad}, we obtain the following lower bound:
%Let $P_{+i}^{m}$ denote the product distribution of $P_{+i}$, with a similar definition for $P_{-\mathrm{i}}^{n}$. 
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq \frac{d \Delta^2}{2}\left(0.9 e^{-10 \varepsilon D}-10 D \delta\right).
$$
By using the fact that $e^z \geq 1+x$, we further derive:
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq \frac{d \Delta^2}{2}(0.9-10 D(\varepsilon+\delta)).
$$
Here, $D$ represents an upper bound on the expected Hamming distance between $(X, Y)$, where $(X, Y)$ is a coupling of $P_{+i}^{n}$ and $P_{-i}^{n}$. Our next step is to determine an appropriate bound for $D$, which requires analyzing the expected Hamming distance between these two product distributions.
For a lower bound, it suffices to consider $x_{k,j}=x \in \mathbb{R}^d$ where $\|x\|_{\infty} \leq 1$ for all $k \in[n], j \in [m]$. Applying the standard result on maximal coupling, we obtain:
$$
\mathbb{E}\left[d_{\mathrm{Ham}}(X, Y)\right]=n\left\|P_{+i}-P_{-i}\right\|_{\mathrm{TV}}.
$$
Using above and the joint convexity of TV distance, we derive:
$$
\begin{aligned}
\left\|P_{+i}-P_{-i}\right\|_{\mathrm{TV}} & =\left\|\frac{1}{\left|\mathcal{E}_d\right|} \sum_{e \in \mathcal{E}_e} P_{e,+i}-P_{e,-i}\right\|_{\mathrm{TV}} \\
& \leq \frac{1}{\left|\mathcal{E}_d\right|} \sum_{e \in \mathcal{E}_d}\left\|P_{e,+i}-P_{e,-i}\right\|_{\mathrm{TV}} \\
& \leq \max _{e \in \mathcal{E}_{d, i \in[d]}}\left\|P_{e,+i}-P_{e,-i}\right\|_{\mathrm{TV}} \\
& =\max _{e \in \mathcal{E}_b, i \in[d]}\left\|P_e-P_{\bar{e}^i}\right\|_{\mathrm{TV}}.
\end{aligned}
$$
Here, $\bar{e}^i$ represents a modified version of $e$ where its $i$-th coordinate is flipped. Applying Pinsker's inequality for any $i \in[d]$, we obtain:
$$
\left\|P_{e}-P_{\bar{e}^i}\right\|_{\mathrm{TV}}^2 \leq \frac{1}{4}\left(D_{\mathrm{KL}}\left(P_e \| P_{\bar{e}^i}\right)+D_{\mathrm{KL}}\left(P_e \| P_{\bar{e}^i}\right)\right) \leq m \Delta^2.
$$
The last inequality follows from Fact~\ref{fact:kl}, the assumption that $\|x\|_{\infty} \leq 1$ and the conclusion in \cite{userleveldp1}. Combining these results, we establish :
$$
\mathbb{E}\left[d_{\mathrm{Ham}}(X, Y)\right]= n\left\|P_{+i}-P_{-i}\right\|_{\mathrm{TV}} \leq \sqrt{m}n \Delta:=D.
$$
With the derived value of $D$, we now arrive at the following lower bound:
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq \frac{d \Delta^2}{2}(0.9-10 \sqrt{m}n \Delta(\varepsilon+\delta)).
$$
To optimize this bound, we select
$$
\Delta=\frac{0.04}{\sqrt{m}n(\varepsilon+\delta)}.
$$
Substituting this choice into the expression, we obtain:
$$
R\left(\mathcal{P}_{\theta}^{\otimes m},\|\cdot\|_2^2, \varepsilon, \delta\right) \geq c \cdot \frac{d}{m n^2(\varepsilon+\delta)^2}.
$$
for some universal constant $c$. Finally, combining this result with the non-private case completes the proof.
\end{document}
