\section{Related Work}
\noindent \textbf{User-level DP learning. }
User-level DP has garnered increasing attention due to its privacy protection, which aligns more closely with real-world scenarios. Several studies have explored various tasks of user-level DP, such as mean estimation~\cite{userleveldp1}, empirical risk minimization~\cite{userleveldp1}, and stochastic convex optimization~\cite{userleveldp2,userleveldp}. \citealt{userleveldp1} first introduced user-level DP, providing stricter privacy protection by safeguarding the entire contribution of users, based on a novel private mean estimation algorithm. \citealt{userleveldp2} further investigated how carefully selected first-order optimization methods can achieve optimal utility under local DP conditions. \citealt{userleveldp}, on the other hand, ensured query concentration through above-threshold and adaptive sampling techniques, thereby releasing the minimal number of user assumptions while still achieving optimal utility. However, none of them has considered RLHF. In this paper, we build the first theoretical results on user-level (label) DP-RLHF. 

\noindent \textbf{Privacy-preserving in RLHF.}
\citealt{kwise} provided a theoretical framework for RLHF under clean data conditions, demonstrating the properties of MLE and pessimistic MLE. However, due to privacy leakage issues \cite{li2023multi}, MLE cannot be applied directly. Building on this, \citealt{rewarddp} introduced a theoretical framework for incorporating DP into RLHF, designing an unbiased loss function based on random responses to achieve DP.   \citealt{PROPSdp} proposed a novel multi-stage mechanism that privately aligns the model with labels from previous stages combined with random responses. However, as we will show later, random responses cannot be easily extended to more practical user-level settings, which results in poor utility. Meanwhile, \citealt{rlhfdp23} utilized DP-SGD across the three stages of RLHF—Supervised Fine-Tuning, Reward Model Learning, and Alignment with PPO—to ensure privacy preservation. \citealt{userlevelrdplhf} explored the application of Group Privacy and User-wise DP-SGD to achieve user-level DP in RLHF. However, they provide only experimental results and lack rigorous theoretical guarantees. Our experiments will show that our proposal method has better performance than these methods.  


%Prior research either did not consider user-level settings or lacked rigorous theoretical proof for such settings.