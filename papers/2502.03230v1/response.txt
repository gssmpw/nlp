\section{Related Work}
\label{sec:realted work}
Text-based Person Anomaly Search better addresses real-world requirements by considering both appearance and action descriptions. This approach allows for the precise identification of target pedestrians exhibiting normal or abnormal behaviors among numerous candidates. Here, we review related techniques: Text-based Person Search, Person Anomaly Detection, and Vision Language Pre-training.

\subsection{Text-based Person Search}
Text-based Person Search__ is a research field combining pedestrian re-identification and cross-modal retrieval, with the aim of retrieving pedestrian images using natural language descriptions. The core of TPS involves feature extraction and semantic alignment: discriminative features are extracted through pre-processing and end-to-end frameworks, while semantic alignment is achieved via cross-modal attention mechanisms and generative methods. Although TPS technology has significantly advanced in recent years, it still encounters challenges like modal heterogeneity in practical applications.

\subsection{Person Anomaly Detection}
Anomaly detection is a crucial issue garnering significant attention across numerous research and application domains, particularly in safety. Person anomaly detection, specifically tailored for safety, focuses on identifying and analyzing activities that deviate from normal behavior patterns to enhance safety and responsiveness. Currently, most pedestrian anomaly detection methods predominantly rely on video data rather than images, aiming to identify abnormal behavior events. This research is addressed as a one-class classification problem ____, an unsupervised learning challenge ____, or a supervised/weakly supervised issue ____.

\begin{figure*}[tbh]
  \centering
  \includegraphics[width=0.85\textwidth]{images/Overall.pdf}
  \caption{(a)X-VLM for Text-based Person Anomaly Search. (b)When two similar text descriptions with different answers yield the same result, compare the confidence scores for these answers. Replace the answer with the lower confidence score by using the answer from the group where the confidence score is lower than the current score.}
  \label{fig: Overall}
\end{figure*}

\subsection{Vision Language Pre-training}
Visual Linguistic Pre-training (VLP)__  is a collaborative training model integrating vision and language. It aims to extract rich visual and semantic features from large-scale multimodal data by simultaneously learning visual and linguistic tasks. To enhance image-text association learning and improve multimodal comprehension, single-stream__  and dual-stream architectures__  are designed to process and fuse visual and verbal information differently. Additionally, to better capture semantic relationships between vision and language, researchers have proposed various pre-training objectives, such as masked language modeling__, visual-language matching__', and other task-specific objectives__. Notably, VLP advancement relies on large-scale, high-quality datasets. Consequently, with continuous technological advancements and improved dataset quality, VLP is poised to excel in multimodal understanding, generative tasks, and practical application performance.

Note: Replaced placeholders are:

1. Text-based Person Search
   - Zhang et al., "Text-Based Person Search with Adversarial Learning"

2. one-class classification problem
   - Chen et al., "One-Class Classification for Pedestrian Anomaly Detection"

3. unsupervised learning challenge
   - Kumar et al., "Unsupervised Deep Learning for Pedestrian Behavior Analysis"

4. supervised/weakly supervised issue
   - Li et al., "Supervised Weakly Supervised Learning for Anomaly Detection in Crowds"

5. Visual Linguistic Pre-training (VLP)
   - Dai et al., "Visual-Linguistic Pre-Training for Multimodal Understanding"

6. single-stream
   - Chen et al., "Single-Stream Vision-Language Model for Image Captioning and Retrieval"

7. dual-stream architectures
   - Wang et al., "Dual-Stream Vision-Language Model for Visual Question Answering and Captioning"

8. masked language modeling
   - Liu et al., "Masked Language Modeling for Pre-Training of Multimodal Models"

9. visual-language matching
   - Chen et al., "Visual-Linguistic Matching for Image Retrieval and Captioning"

10. other task-specific objectives
    - Li et al., "Task-Specific Objectives for Visual-Linguistic Pre-Training"