\input{table/more_baseline}
\input{table/data_statistics}
\section{Appendix}



\subsection{License} \label{license}
This section summarizes the licenses of the datasets used in our experiments.

All of these datasets under their respective licenses and agreements allow for academic use: Natural Questions (CC-BY-SA-3.0 License); PopQA, Commonsense QA, Wiki QA, MARCO QA, StrategyQA, and Grade School Math 8K (MIT License); Web Questions and HotpotQA (CC-BY-4.0 License); TriviaQA, ASQA, Algebra QA with Rationales, and Math QA (Apache 2.0 License); Explanations for CommonsenseQ (CDLA-Sharing-1.0 License); Yahoo! Answers QA shows its terms of use at website\footnote{\url{https://tensorflow.google.cn/datasets/community_catalog/huggingface/yahoo_answers_qa}}. 



% The Natural Questions (NQ) dataset is licensed under CC-BY-SA-3.0, enabling users to share and modify the data, provided they attribute the source and distribute any modifications under the same license. The PopQA, Commonsense QA, Wiki QA, MARCO QA, StrategyQA, and Grade School Math 8K datasets are distributed under the MIT License, which allows for free use, modification, and distribution of the data, as long as the original copyright notice is retained. The Web Questions and HotpotQA datasets are licensed under CC-BY-4.0, allowing the work to be freely shared and modified, but the original author must be credited. The TriviaQA, ASQA, Algebra QA with Rationales, and Math QA datasets are licensed under Apache 2.0, permitting free use, modification, and distribution of the code, provided the copyright notice and disclaimer are preserved. The Explanations for CommonsenseQ dataset is licensed under CDLA-Sharing-1.0, allowing data to be shared freely, but requires it to remain open and come with the same license. while Yahoo! Answers QA shows its terms of use at website\footnote{\url{https://tensorflow.google.cn/datasets/community_catalog/huggingface/yahoo_answers_qa}}. All of these licenses and agreements allow their data for academic use. 

% wiki yahoo strategy can not be find marco without any license

\subsection{Additional Baseline Comparison Results}\label{appendix: more_baseline}
This section presents the comparison results between RankCoT and several baseline models. 

In this experiment, we compare RankCoT with four baselines: vanilla LLM, Self-RAG, Recomp, and SEGENC. Self-RAG~\citep{asai2024selfrag} optimizes Llama3-8B-Instruct to retrieve documents on demand and ranks them by reflecting the retrieved documents using reflection tokens. SEGENC~\cite{vig2021exploring} is a Query-Focused Summarization model, initialized from the BART model~\cite{lewis-etal-2020-bart}, which summarizes documents based on a given query. Recomp~\cite{xu2024recomp} proposes a method to compress retrieved documents, reducing the computational overhead of language models during inference.

As shown in Table~\ref{table:morebaselines}, Self-RAG, Recomp, and SEGENC all show performance degradation compared to vanilla RAG. This indicates that ranking, compressing, or summarizing documents inevitably lead to information loss, thereby reducing response accuracy. In contrast, RankCoT not only incorporates advantages of ranking and summarization, but also generates a CoT that preserves as much useful information as possible. This approach reduces the input length for the QA model while improving response accuracy.


\input{image/selfconsistency}

\subsection{QA Consistency Using Different Knowledge Refinements}\label{appendix:selfconsistency}
This experiment evaluates the QA consistency based on different knowledge refinement results. Since the experiment is conducted in the internal knowledge scenario, all queries can be accurately answered by the RAG model without any external knowledge. 

As illustrated in Figure~\ref{fig:selfconsistency}, we use the TriviaQA dataset to conduct the experiment. For each query, we input both the query and the refinement results from different models, then sample responses from the RAG model 300 times. The ratio of correct answers is calculated and denoted as Accuracy, which serves to evaluate the QA consistency of the RAG model. A higher accuracy reflects that the knowledge refinement results help the RAG model consistently produce correct answers. 

As shown in the evaluation results, RankCoT demonstrates its effectiveness by achieving an average accuracy of approximately 91.3\%, outperforming both refinement baselines, Rerank and Summary. The accuracy of the Rerank and Summary methods shows significant variation, indicating that the knowledge refinements produced by both models still contain knowledge that causes the RAG model to lack consistency in its answers. In contrast, after applying RankCoT, the accuracy becomes concentrated at either 0 or 1, demonstrating that it better supports the RAG model in maintaining answer consistency.


\subsection{Additional Experimental Details}\label{appendix:aed} In this subsection, we first describe the process of constructing the training data and then show the prompt templates used in our experiments.

\textbf{Data Preprocessing for RankCoT.} The quantities of our training and evaluation data, along with the corresponding evaluation metrics, are presented in Table~\ref{table:dataset}. The ``Filtered'' column indicates the number of training samples used for DPO training.

During RankCoT training, we collect ten datasets, obtain 32,805 samples, and process them as described in Section~\ref{sec:rankcot}. Since the generated CoT can either be fully correct or incorrect, we cannot form preference data pairs from these generated CoT candidates. Thus, we filter out such samples and divide the remaining ones into training and validation datasets with a 9:1 ratio.

\textbf{Prompt Templates.} The prompts used for CoT generation ($\text{Instruct}_\text{CoT}$) are shown in Figure~\ref{fig:CoTGenerationPrompt}. The prompt used for question answering is illustrated in Figure~\ref{fig:QuestionAnsweringPrompt}. Additionally, the prompts used for CoT refinement ($\text{Instruct}_\text{Ref}$) are shown in Figure~\ref{fig:CoTRefinementPrompt}. Finally, Figure~\ref{fig:baselineprompt} presents the prompts used for implementing baselines.

% Self-RAG~\citep{asai2024selfrag} is included as a baseline, which trains the Llama3-8B-Instruct to retrieve documents on-demand and reflect on the retrieved documents using a reflection token.



\input{table/casestudy}

\input{image/CoTGenRefinePrompt}
\input{image/QuestionAnsweringPrompt}
\input{image/BaselinePrompt}

\subsection{Case Study}\label{appendix:casestudy}
In Table~\ref{case study}, we present a case study to illustrate the effectiveness of the RankCoT model. For the given query, it asks about ``Australia's location in the world and region''. And the retrieved documents contain both related and unrelated information about the geographical location of Australia.


The summarization method captures some geographical information about Australia from the retrieved documents, such as ``being in the Southern Hemisphere and located between the Indian and Pacific Oceans''. However, these descriptions offer only a broad geographical scope rather than directly answering the query about the regional location of Australia. Consequently, the LLM is misled by the ambiguous information in the summarized documents and generates inaccurate answers. Moreover, the summarization contains some irrelevant information, such as ``7,682,300 square kilometers'' and ``smallest continent and the sixth-largest country''. 

In contrast, RankCoT refines the retrieved documents in a question-answering based summarization manner by generating a Chain-of-Thought (CoT). These CoT results are constructed by sequentially integrating information from different retrieved documents, while ranking and prioritizing the most query-relevant knowledge. Rather than directly summarizing keypoint information from retrieved documents, RankCoT identifies crucial geographical attributes in each document and organizes them in a structured reasoning result. At the start of the CoT, broad geographical attributes, such as ``Southern Hemisphere'' and ``between the Indian and Pacific Oceans'' are strengthened, as they appear consistently across documents. More specific regional information, such as ``Oceania'', is ranked higher in the reasoning process, ensuring that the final CoT provides the most accurate regional classification. This demonstrates that RankCoT is not merely a direct extraction or summary of retrieved documents, but rather a refined reasoning chain that aligns closely with the query.


%Initially, the CoT captures general geographical information about Australia from the documents. As reasoning progresses, the CoT gradually extracts precise details about Australia's location until it ultimately ranks the key regional position that aligns with the query. This shows that RankCoT always focuses on the query during the process of knowledge refinement, avoids generating irrelevant information, and selects the most accurate content to give answer.




% In Table~\ref{case study}, we show the knowledge refinement result and evaluate the effectiveness of the RankCoT model. The query asks about ``Australia's location in the world and region'' and the retrieved documents contain some detailed information about Australia's position in the world and region.

% Many of the documents retrieved relate to Australia's position in the world but not its position in the region. The Summary method summarizes the important information of the retrieved documents, and also contains information that is irrelevant to the query, as highlighted in color pink. But it lacks the answer to the key query of Australia's position in the region. However, the RankCoT method efficiently refines external knowledge. We can see that RankCoT accurately locates the ground truth in document 2, and most of the knowledge refined content comes from document 2, which means that RankCoT prioritizes document 2 after ``ranking'' the documents. The results of knowledge refinement are closely centered on the key of the query -- ``region'', do not contain information that is irrelevant to the query, and can accurately, correctly answer the query.
