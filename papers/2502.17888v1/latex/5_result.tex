\input{table/overall}
\section{Evaluation Result}
In this section, we first evaluate the performance of various RAG methods, followed by ablation studies to examine the impact of self reflection module and different training strategies. We then investigate the characteristics of RankCoT by analyzing the knowledge utilization capabilities of RAG models using different knowledge refinement models. We also examine the effectiveness of the refined knowledge generated by RankCoT through answering consistency in Appendix~\ref{appendix:selfconsistency}. The case study is conducted in Appendix~\ref{appendix:casestudy}.

\subsection{Overall Performance}
This section presents the knowledge refinement performance of different models, as shown in Table~\ref{main result}. Additional baseline comparison results are shown in Appendix~\ref{appendix: more_baseline}.

The evaluation results reveal that these three knowledge refinement models, Rerank, Summary, and CoT, show distinct performance. Specifically, Rerank exhibits a slight improvement, while both Summary and CoT lead to a decrease in RAG performance. This highlights the challenge of effectively refining knowledge for RAG modeling. In contrast, RankCoT demonstrates a 2.5\% improvement over vanilla RAG model, indicating its effectiveness in providing more meaningful refinements that help LLMs better answer questions. Furthermore, RankCoT outperforms the Rerank model with a 1.8\% improvement and avoids the need to feed raw passages into the LLM twice for knowledge refinement and question answering.

Then we present the performance of RankCoT by applying it to different RAG systems implemented with LLMs of various scales. The results indicate that RankCoT maintains its effectiveness across different RAG configurations, yielding a 7.6\% improvement over the MiniCPM3-4B-based RAG model and a 4.1\% improvement over the RAG model implemented with Qwen2.5-14B-Instruct. These findings demonstrate that RankCoT has strong generalization ability in knowledge refinement, enabling LLMs of different scales to effectively leverage external knowledge.

\input{table/ablation}
\input{table/knowledge_usage}
\subsection{Ablation Study}
We conduct ablation studies to evaluate the effectiveness of various training strategies.

As shown in Table~\ref{ablation study result}, we first conduct the approach from prior work~\cite{lin2024radit}, where a fine-tuned QA model is used for knowledge refinement. We then train several knowledge refinement models--Rerank, Summary, and CoT--using the self-reflection mechanism introduced by RankCoT. Specifically, the self-reflection mechanism involves feeding the knowledge refinement results into LLMs to generate self-reflection results based on these inputs. In this setup, SFT methods select self-reflection results containing ground truth answers to train knowledge refinement models, while DPO methods select both positive and negative responses--those that contain ground truth answers and those that do not--for training.

After fine-tuning with QA supervision, the LLM is able to generate knowledge refinement results using different prompts. However, the evaluation results illustrate that these QA models present limited effectiveness compared to vanilla RAG model. We then train the knowledge refinement models using training signals refined by the LLM itself. Among the SFT-based models, Rerank achieves the best performance, illustrating that the reranking signals can be easily learned by LLMs through SFT. Using the DPO training method, both Summary and RankCoT show significant improvements over these knowledge refinement models using the SFT strategy. Furthermore, RankCoT outperforms all knowledge refinement models, demonstrating its effectiveness in producing effective knowledge refinement results to help LLMs generate more accurate answers. By replacing self-refined CoTs with the raw CoT outcomes during DPO training, RankCoT achieves a 1.3\% decline, showing the effectiveness of our self-reflection mechanism.


%In the first part, we directly use ground truth to fine-tune the llama3-8B-Instruct model using the SFT method, and then use the fine-tuned model to refine the retrieval results (including reranking, summarization, chain-of-thought reasoning), and then give the refined results to llama3-8B-Instruct to generate the final answer. We test its performance on NQ, HotpotQA, and TraviaQA datasets. Then, we independently feed the query and retrieved document to the LLM, asking it to generate different answers for different document. If the answer is correct, it is considered a positive answer, while the wrong answer is considered negative one. These data preference data are used to fine-tune llama3-8B-Instruct through DPO method. The test inference process is the same as above. 

%The second is to use the Self-Reflection signal to fine-tune the model. We use equation~\ref{eq:reranking}, equation~\ref{eq:CoTrank} but output summary, equation~\ref{eq:CoTrank} to refine the retrieval documents independently. Then we introduce the self-reflection mechanism to further correct the refinement results, if the reflection results contain the ground truth answer, they are considered a positive data, while the result that do not contain the ground truth answer are regarded as the negative. We use the positive data for SFT training and preference data for DPO training. For each fine-tuned model, we test its performance on NQ, HotpotQA, and TraviaQA datasets.


%As shown in Table~\ref{ablation study result}, the model test effect of using DPO fine-tuning method is also better than SFT method on average. Some models even have lower performance than vanilla RAG after SFT. It shows that blindly fitting the training data will lead to a decrease in model performance. Even though we use QA supervision signals as feedback and DPO to fine-tune the model, the improvement is minimal compared to vanilla RAG. This indicates that it is not enough to just let the model learn to answer questions and then refine the knowledge. However, using self-reflection data to fine-tune the model can bring significant improvements. Whether SFT or DPO is used to fine-tune the model, the model test effect of using Self-Reflection method is much better than using QA Supervision method on average. 


\subsection{Knowledge Usage Performance of Different Refinement Models}
In this experiment, we evaluate the ability of RankCoT to assist RAG models in leveraging external knowledge to generate final answers. We compare RankCoT with three knowledge refinement models: Rerank, Summary, and CoT.

As shown in Table~\ref{table:scenarios}, we conduct three testing scenarios to assess the effectiveness of the different knowledge refinement models: Has-Answer, Miss-Answer, and Internal Knowledge. The Has-Answer scenario involves cases where the retrieved documents contain the correct (golden) answer. This scenario evaluates whether the knowledge refinement model can effectively extract key information from these documents to aid the LLM in answering the question. The Miss-Answer scenario, on the other hand, deals with cases where the retrieved documents do not include the golden answer. This scenario further tests the ability of the knowledge refinement models to minimize the impact of retrieved noise. Finally, the Internal Knowledge scenario examines the ability of different knowledge refinement models to handle conflicts between internal and external knowledge.

As shown in the evaluation results, RankCoT outperforms all knowledge refinement models across all datasets in the Has-Answer scenario. This demonstrates the effectiveness of RankCoT in incorporating more query-relevant information from retrieved documents, thereby enhancing the accuracy of the RAG system in this scenario. In the Miss-Answer scenario, the performance of vanilla RAG models significantly drops compared to LLMs w/o RAG, indicating that query-irrelevant documents mislead LLMs into producing incorrect answers. However, the use of different knowledge refinement models mitigates this performance decline. Among these models, RankCoT exhibits the most substantial improvements, demonstrating its effectiveness in filtering out noisy information from retrieval and reducing the misleading information of irrelevant documents. In the internal knowledge scenario, knowledge refinement models--Rerank, Summary, and CoT--perform comparably or even worse than vanilla RAG model, illustrating that existing methods are less effective in addressing the knowledge conflict issue. In contrast, RankCoT outperforms these knowledge refinement models, demonstrating its ability to provide more tailored knowledge refinement results. The RankCoT-produced knowledge refinement results effectively alleviate knowledge conflicts, aiding LLMs in better utilizing both internal and external knowledge.



\input{image/refinedknowledge_quality}
\input{image/inputlength}
\subsection{Characteristics of the Refined Knowledge Produced by RankCoT}\label{sec:characteristics}
This experiment further explores the characteristics of RankCoT-produced knowledge refinement results by estimating both the quality and length of the refined knowledge. We conduct the experiment on the NQ and TriviaQA datasets, specifically in the Has-answer test scenario, where the retrieved documents contain the ground truth answers.

\textbf{Refinement Quality.} As shown in Figure~\ref{fig:refined knowledge quality}, we evaluate the quality of the knowledge refinement results based on query relevance and the hit rate of the golden answer. Specifically, the similarity scores between the query and the refined knowledge generated by three different knowledge refinement models are presented in Figure~\ref{fig:textsimquery}. RankCoT achieves the highest similarity score with the query, demonstrating its effectiveness in retaining more query-related information from the retrieved documents. Furthermore, we show the hit rate of the ground truth answer in Figure~\ref{fig:groundtruthin}. As indicated by the evaluation results, Rerank achieves the highest hit rates, while Summary performs the worst. This outcome likely stems from the fact that the Rerank model only selects the most relevant document, whereas the Summary model must extract key information, inevitably discarding some of the relevant contents that contain the ground truth answers. Although RankCoT is also a summarization-style knowledge refinement model, it achieves higher hit rates, showing that RankCoT can capture more ground truth answers in its refinement results.

\textbf{Length of Knowledge Refinement.} Subsequently, we present the results of knowledge refinement lengths for different models in Figure~\ref{fig:inputlength}. As shown in Figure~\ref{fig:inputlengthlog}, the summarization-style knowledge refinement methods, Summary and RankCoT, significantly reduce the length of the refined knowledge compared to the Rerank model. Notably, RankCoT achieves the shortest refinement length, demonstrating its effectiveness in minimizing the consumption of prompt inputs for LLMs~\cite{mu2023learning}. Additionally, we investigate the length change ratio across different training methods in Figure~\ref{fig:inputlengthratio}. As shown in the results, these SFT methods generally result in shorter knowledge refinement outputs, illustrating that SFT encourages the summarization-style knowledge refinement model to overfit training signals~\cite{li2024rag}. In contrast, the DPO training method helps these knowledge refinement models produce longer results, facilitating more flexible responses that incorporate more crucial knowledge.



% We argue that the RankCoT method retains both the role of reranking the documents and the role of summarize the documents, and does not need to send the rerank documents to the QA model for repeated encoding. Only a short CoT is needed to improve the effect of the QA model, while also reducing the input tokens. We computed the average length of the knowledge refinement results of different models trained with DPO on NQ, HotpotQA, TriviaQA dataset, as shown in Figure~\ref{fig:inputlengthlog}. Compared with the reranking method, the method of generating summary and CoT significantly reduces the input length of the QA model. CoT is even better with its shorter length. In addition, we also calculated the length change ratio of knowledge refinement results of different refinement methods under different training methods compared to vanilla RAG on the NQ, HotpotQA, TriviaQA dataset, as shown in Figure~\ref{fig:inputlengthratio}. Compared with SFT, the overall length of the refinement result will become longer after DPO fine-tuning. This indicates that DPO training does not simply fit the characteristics of the training data, which provides more possibilities for the diversity of generated data. 


%In addition, we also calculated the textual similarity between the CoT or Summary and the query and document generated by the ranker trained using the RankCoT method. As shown in Figure~\ref{fig:textsim}, compared with vanilla RAG, after SFT fine-tuning, the similarity between CoT or Summary and both query and document is reduced, but the similarity can be improved through DPO training. Although CoT and Summary can both answer questions, CoT generated after DPO training has a higher similarity with query than Summary, and has a higher accuracy in answering questions. Summary has a higher similarity with document than CoT, and is more about summarizing the relevant content of the generated document, lacking a certain reasoning process.



