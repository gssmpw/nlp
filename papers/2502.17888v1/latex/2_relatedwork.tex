\section{Related Work}
Retrieval-Augmented Generation (RAG) aims to enhance Large Language Models (LLMs) by enabling them to access external knowledge bases, providing up-to-date information during the generation process~\cite{shi-etal-2024-replug, ram-etal-2023-context}. This approach has demonstrated promising results across various NLP tasks, including open-domain question answering~\cite{izacard2022few}, code generation~\cite{zhou2023docprompting}, and dialogue~\cite{shuster2022blenderbot}. In these RAG models, retrieved documents are typically used as context to assist LLMs in generating more accurate responses~\cite{ram-etal-2023-context}. However, the conflict between the external knowledge and the parametric memory of LLMs often undermines the effectiveness of current RAG systems~\cite{asai2024reliable, xie2024adaptive, chen2024benchmarking}.

To mitigate the potentially negative impact of retrieved knowledge, existing models focus on refining the external knowledge through various modules designed to help LLMs generate more precise responses. Earlier works concentrate on reranking the retrieved documents~\cite{yu2023augmentation,shi-etal-2024-replug,yu2024rankrag}, while others employ query-focused summarization techniques~\cite{vig2021exploring,xu2023lmgqs} to reduce noise. However, reranking models often overlook noise within individual passages, and summarization models may fail to account for query-document relevance, sometimes incorporating misleading content in the summarization results. Chain-of-Note~\cite{yu-etal-2024-chain} attempts to instruct LLMs to generate query-related notes when answering a given query. This model incorporates the knowledge refinement process into the reasoning stage~\cite{wei2022chain} and heavily relies on the capabilities of LLMs, which may limit its applicability in RAG systems~\cite{gao2024modular}.

Modular RAG systems~\cite{gao2024modular,xu2024activerag} focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area.
For instance, Self-RAG~\cite{asai2024selfrag} uses different tags for adaptive retrieval~\cite{jiang2023active} and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering questions~\cite{yan2024corrective,trivedi2023interleaving}. \citet{yan2024corrective} introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation.

To further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models~\cite{yu2023augmentation, shi-etal-2024-replug, izacard2021distilling}. \citet{lin2024radit} adapt LLMs within the RAG setting by constructing instruction-tuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, \citet{li2024rag} use Direct Preference Optimization (DPO)~\cite{rafailov2024direct} to jointly optimize the modules in a RAG system, aligning their data preferences.

%Further research~\cite{lin2023ra,li2024rag} on RAG systems further uses instruct-tuning~\cite{chung2024scaling} or Direct Preference Optimization (DPO)~\cite{rafailov2024direct} for optimization. 


%How to optimize the RAG system to generate more accurate responses is an important issue. Many studies have adopted Supervised Fine-Tuning (SFT) in optimizing the LLM of RAG~\cite{lin2023ra,asai2023self}. However, these training methods focus on training the LLM to adapt to the training signal, which is prone to overfitting to the specific training data distribution, and the model may have difficulty generating diverse and reasonable responses. Instead of letting the model learn to imitate the output in the training data, Reinforcement Learning with Human Feedback (RLHF) optimizes the model through human feedback (such as preference comparison) to make its generated results more in line with user expectations. For example, Direct Preference Optimization (DPO) is widely used to optimize LLM to align it with human preferences~\cite{rafailov2024direct} and enhance the consistency of generated responses~\cite{putta2024agent}.


% And the retrieval knowledge also increases the input length of LLM, encoding lengthy retrieval knowledge causes additional delays and generative speed. To alleviate the interference of noise on the generation of large language models, many RAG models focus on building modular RAG pipelines to improve retrieval results~\cite{gao2024modular}. These models aim to refine the retrieval knowledge by estimating the relevance of each document to the query to re-rank the documents~\cite{asai2023self} or prompting LLMs to summarize the query-related knowledge from retrieved documents~\cite{yu2023chain}, employing a retrieval evaluator to trigger different knowledge refinement actions~\cite{yan2024corrective}, etc. 

% Although these RAG models have a certain ability to remove noise, they do not optimize the retrieval results from the perspective of model performance and may not be compatible with downstream models. The improved retrieval results we get may be misleading. When external knowledge conflicts with the model parametric knowledge of LLMs, the RAG models will give priority to external knowledge to generate answers~\cite{xie2023adaptive}, thereby generating incorrect answers.

% While multiple LLMs show great potential in enhancing task-solving  capabilities~\cite{abbasiantaeb2024let,qiu2024interactive,wang2024mixture} through interaction such as collaborative generation, question-answering dialogue, and knowledge transfer. The "LLM-to-LLM" approach can effectively leverage the generation or reasoning capabilities of one LLM to guide another LLM to obtain better outputs and thus improve model performance~\cite{xu2024unsupervised}. To the end, we propose RankCOT, inspired by the "LLM-to-LLM" approach, which uses the rewards of a downstream student model to guide the generation of a large language model.