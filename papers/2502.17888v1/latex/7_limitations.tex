\section*{Limitations}
Although RankCoT demonstrates its effectiveness in refining retrieved knowledge for RAG systems, the quality of the refinement is still constrained by the capabilities of LLMs. Specifically, RankCoT is optimized using the DPO method, which relies on LLMs to generate meaningful chosen and rejected pairs during optimization. Therefore, the generation of meaningful preference pairs for optimization still heavily depends on the performance of the LLMs. Additionally, RankCoT (Llama3-8B-Instruct) can be applied to different RAG systems that implemented with LLMs of varying scales and show its effectiveness. The improvements may be diminished when larger-scale LLMs are used as the generation model of RAG systems, due to the stronger knowledge refinement capabilities of LLMs of a larger scale. This further highlights the importance of aligning the parameter scale of the LLM used to build the RankCoT model with that of the generation model in RAG systems.

