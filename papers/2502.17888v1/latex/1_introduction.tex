\section{Introduction}
Retrieval-Augmented Generation (RAG)~\cite{NEURIPS2020_6b493230, guu2020retrieval, ram-etal-2023-context, shi-etal-2024-replug} empowers Large Language Models (LLMs) to access external knowledge, providing up-to-date information during the generation process. RAG models have demonstrated their effectiveness in mitigating the hallucination problem commonly encountered by LLMs~\cite{shuster2021retrieval}, enhancing the performance of LLMs, such as GPT-4~\cite{achiam2023gpt} and LLaMA~\cite{touvron2023llama}, in different NLP tasks.

\input{image/RankCoT_intro}
RAG models typically use dense retrieval methods~\cite{karpukhin2020dense, DBLP:conf/iclr/XiongXLTLBAO21} to retrieve query-relevant documents from external knowledge bases. These documents, along with the query, are then fed as the input context into LLMs~\cite{ram-etal-2023-context}. Thriving on their in-context learning capabilities~\cite{brown2020language,dong2022survey}, LLMs are able to extract relevant semantics from retrieved documents and generate appropriate responses to address the given query. However, the potential knowledge conflict between external knowledge and parameterized memory still poses a challenge for LLMs in generating precise responses~\cite{chen2024benchmarking, asai2024reliable}.


Many RAG models focus on building modular RAG pipelines to enhance retrieval performance~\cite{gao2024modular,asai2024selfrag}. These models primarily aim to refine the retrieved knowledge by assessing the relevance of each document to the query and subsequently filtering out irrelevant ones~\cite{yan2024corrective, asai2024selfrag}. However, the reranking model still requires feeding the remaining documents into LLMs, which means that query-unrelated content within a relevant document may still mislead the generators~\cite{xu2024recomp}. Some models address this problem by prompting LLMs to summarize query-relevant knowledge from the retrieved documents, thereby reducing the influence of irrelevant information~\cite{vig2021exploring, yu-etal-2024-chain, xu2024recomp}. This summarization approach often incorporates information from unrelated documents as part of the summaries, resulting in the introduction of noise. Both the reranking and summarization modules have advantages for knowledge refinement. However, in existing RAG systems, these modules are typically modeled separately by prompting the same LLMs.

%Recent work~\cite{yu2023chain} further combines the strengths of both reranking and summarization by instructing LLMs to evaluate the relevance between the query and documents, and then generate query-specific notes within the generated Chain-of-Thought~\cite{wei2022chain}. However, this work relies on the capabilities of LLMs, overlooking the feedback from downstream generators that could be leveraged to optimize the knowledge refinement model.





This paper presents RankCoT, a knowledge refinement method that combines the strengths of both ranking and summarization to effectively enhance the process for retrieval result refinement. As shown in Figure~\ref{fig:RankCoT_intro}, we feed both the query and all retrieved documents into the RankCoT model, which incorporates reranking signals in generating CoT-based summarization as knowledge refinements, thereby aiding LLMs in generating more accurate responses for answering the given query. During training RankCoT, we independently feed the query and retrieved document to the LLM, asking it to generate several Chain-of-Thought (CoT) responses to answer the question, which can be considered as summarization results. We then design the self-refinement model to prompt LLMs to answer the question according to these sampled CoTs, helping to refine the CoT results for more effective training. If the refined CoT contains the ground truth answer, it is considered a positive refinement result, while those that do not contain the ground truth answer are considered negative refinements. 



Our experiments demonstrate that RankCoT outperforms all baseline models, achieving over a 2\% improvement. Notably, RankCoT proves effective across LLMs of various scales. It generates shorter knowledge refinement results compared to both reranking and summarization methods, while enhancing the response accuracy of generator. Further analysis reveals that RankCoT successfully incorporates ground truth answers into the knowledge refinement results, while also including more query-relevant content. Additionally, RankCoT effectively extracts crucial semantics from the retrieved documents and alleviates the conflict between retrieved contents and internal knowledge.
