\section{Experimental Methodology}
In this section, we describe the datasets, baselines, evaluation metrics, and implementation details in our experiments. More experimental details are shown in Appendix~\ref{appendix:aed}.


\textbf{Datasets.}
In our experiments, we follow previous work~\cite{lin2024radit} and utilize the instruction tuning datasets to train and evaluate RAG models. For all datasets and baselines, we use BGE-large~\cite{chen2024bge} to retrieve documents from the MS MARCO V2.1 document collection~\cite{bajaj2016ms}. We select six datasets for evaluation, including NQ~\cite{kwiatkowski2019natural}, HotpotQA~\cite{yang2018hotpotqa}, Trivia QA~\cite{joshi2017triviaqa}, PopQA~\cite{mallen-etal-2023-trust}, ASQA~\cite{stelmakh-etal-2022-asqa}, and MARCO QA~\cite{bajaj2016ms}, which require models to retrieve factual knowledge or conduct more complex reasoning to help answer the given query. All data statistics are shown in Table~\ref{data statistics simple}.


\textbf{Baselines.}
In our experiments, we compare RankCoT with the Vanilla RAG (No Refinement) model and three knowledge refinement models, including Rerank, Summary, and CoT, which are described in Sec.~\ref{sec:preliminary}. For the vanilla RAG model, we follow previous work~\cite{ram-etal-2023-context} and feed 5 retrieved documents as context to answer the question. For the Rerank model, we prompt the LLM to evaluate the relevance between the query and retrieved documents~\cite{asai2024selfrag, li2024rag}. If the document is relevant to the question, it outputs ``YES'' and retains the document, otherwise, it outputs ``NO'' and discards the document. The Summary model and CoT model prompt the LLM to extract query-related knowledge from retrieved documents using summarization and Chain-of-Thought~\cite{wei2022chain} formats to conclude query-related knowledge from retrieved documents.

\input{table/data_statistics_simple}

\textbf{Evaluation Metrics.} Following~\citet{xu2024unsupervised}, we utilize Rouge-L as evaluation metric for MARCO QA task. Following~\citet{gao-etal-2023-enabling}, we utilize String-EM as evaluation metric for ASQA. For other tasks, we use the Accuracy metric for evaluation.


\textbf{Implementation Details.}
We implement our RankCoT model using Llama3-8b-Instruct~\cite{touvron2023llama} as the backbone model. To construct a training dataset for DPO training, we ask Llama3-8b-Instruct to generate CoTs using 10 retrieved documents independently and use the same model to refine CoTs. During training, we feed 5 relevant documents as external knowledge and ask the LLM to reproduce the refined CoT results. We use LoRA~\cite{hulora} method to fine-tune Llama3-8B-Instruct, with $\beta$ set to 0.1 and the learning rate set to 2e-5.

For the RAG model, we concatenate the generated CoT with the query to let Llama3-8B-Instruct generate the final answer. In addition, we also use LLMs of different scales, such as MiniCPM3-4B~\cite{hu2024minicpm} and Qwen2.5-14B-Instruct~\cite{yang2024qwen2}, to build the RAG model and evaluate the generalization ability of RankCoT.
