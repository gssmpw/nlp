\section{Methodology}
As illustrated in Figure~\ref{fig:RankCoT}, this section introduces the RankCoT method. First, we introduce the preliminary of knowledge refinement in Retrieval-Augmented Generation (RAG) systems (Sec.~\ref{sec:preliminary}). And then we describe how to optimize LLMs to produce more effective chain-of-thoughts for knowledge refinement (Sec.~\ref{sec:rankcot}).

\input{image/RankCoT}

\subsection{Preliminary of Knowledge Refinement in Retrieval-Augmented Generation Systems}\label{sec:preliminary}
Given a query $q$ and a set of retrieved documents $D = \{d_1, d_2, \ldots, d_n\}$, vanilla RAG system~\cite{ram-etal-2023-context} uses retrieved documents as context and leverages the in-context learning method to help the generation model $\mathcal{M}_\text{Gen}$ produce the answer $y_\text{Gen}$:
\begin{equation}
    (q, D) \rightsquigarrow \mathcal{M}_\text{Gen} \rightsquigarrow y_\text{Gen}.
\end{equation}
Instead of directly feeding retrieved documents to the generation model ($\mathcal{M}_\text{Gen}$), some RAG models~\cite{gao2024modular, asai2024selfrag} design various modules to refine retrieved documents $D$:
\begin{equation}\label{eq:refinement}
    (q,D) \rightsquigarrow \mathcal{M}_\text{KR} \rightsquigarrow y_\text{KR}.
\end{equation}
The refined knowledge $y_\text{KR}$ is then passed to the generation model to mitigate the negative impact of retrieval noise:
\begin{equation}
    (q, y_\text{KR}) \rightsquigarrow \mathcal{M}_\text{Gen} \rightsquigarrow y_\text{Gen}.
\end{equation}
In the rest of this subsection, we will introduce different methods for implementing the knowledge refinement model $\mathcal{M}_\text{KR}$ in Eq.~\ref{eq:refinement}, including reranking, summarization, and RankCoT.

\textbf{Reranking.} Following previous work~\cite{asai2024selfrag}, we prompt LLMs to evaluate the relevance of the $i$-th retrieved document $d_i$ with respect to the query $q$, and output a binary label $y_\text{Rerank}^i$ for filtering out noisy documents:
\begin{equation}
\label{eq:reranking}
    y_\text{Rerank}^i = \text{LLM} (\text{Instruct}_\text{Rerank}, q, d_i),
\end{equation}
where $\text{Instruct}_\text{Rerank}$ prompts the LLM to assess the relevance between $q$ and $d_i$. The prediction label $y_\text{Rerank}^i$ can be ``YES'' or ``NO'', indicating whether the $i$-th document $d_i$ is relevant or irrelevant to the query $q$. We then retain the documents predicted as ``YES'' and construct the filtered document set $\{d_1, \dots, d_k\}$. The knowledge refinement result $y_\text{KR}$ is then represented as:
\begin{equation}
    y_\text{KR} = d_1 \oplus \dots \oplus d_k,
\end{equation}
where $\oplus$ is the concatenation operation.

\textbf{Summarization.} Another approach for knowledge refinement is summarization, which aims to extract query-related content from the retrieved documents~\cite{vig2021exploring}. The knowledge refinement result can be obtained as:
\begin{equation}
    y_\text{KR} = \text{LLM} (\text{Instruct}_\text{Sum}, q, D),
\end{equation}
where $\text{Instruct}_\text{Sum}$ is the instruction prompting the LLM to generate a summary. Unlike reranking, summarization directly generates the refined knowledge, avoiding the need to feed raw documents to the generation model $\mathcal{M}_\text{Gen}$.

\textbf{RankCoT.} RankCoT further incorporates a Chain-of-Thought (CoT)~\cite{wei2022chain} into the knowledge refinement process:
\begin{equation}
    y_\text{KR} = \text{LLM} (\text{Instruct}_\text{CoT}, q, D),
\end{equation}
where $\text{Instruct}_\text{CoT}$ is the instruction that prompts the LLM to generate CoT. RankCoT incorporates the chain-of-thought reasoning as the knowledge refinement result $y_\text{KR}$ to extract relevant knowledge from retrieved documents $D$, thereby assisting RAG models in answering the query.  Unlike summarization, RankCoT integrates the reranking mechanism during the chain-of-thought generation (Sec.~\ref{sec:rankcot}) to mitigate the influence of noisy documents~\cite{liu2024lost, xie2024adaptive}. 





\subsection{Knowledge Refinement through Ranking Chain-of-Thoughts}\label{sec:rankcot}
To generate tailored knowledge refinement results $y_\text{KR}$ for RAG modeling, RankCoT optimizes the LLM ($\mathcal{M}$) by incorporating reranking into the CoT generation process. Furthermore, we introduce a self-reflection method to further refine the CoT results, mitigating the risk of overfitting to undesired CoT patterns during training RankCoT.

\subsubsection{Reranking Modeling in CoT Generation}
To learn the query-document relevance, we feed each document $d_i$ into the LLM ($\mathcal{M}$) and sample one CoT output $y_\text{CoT} (d_i)$:
\begin{equation}
\label{eq:CoTrank}
 y_\text{CoT} (d_i) \sim \mathcal{M} (q, d_i).
\end{equation}
Next, we gather all generated CoT results from each document in the retrieved document set $D$ to form the candidate CoT set $Y_\text{CoT}$:
\begin{equation}
 Y_\text{CoT} =  \{y_\text{CoT} (d_1), \dots, y_\text{CoT} (d_n)\}.
\end{equation}
We treat the CoT result $y_\text{CoT} \in Y_\text{CoT}$ that contains the ground truth answer as the positive $y_\text{CoT}^+$, while the result that does not contain the ground truth answer is regarded as the negative $y_\text{CoT}^-$.

Finally, we can optimize the LLM ($\mathcal{M}$) to assign higher generation probabilities to positive knowledge refinement results $y_\text{CoT}^+$ than the negative ones $y_\text{CoT}^-$. The training process is implemented with the Direct Preference Optimization (DPO) method~\cite{rafailov2024direct}:
\begin{equation}
\begin{aligned}
\label{eq:dpo}
& \mathcal{L} = 
- \mathbb{E}_{(q, y_\text{CoT}^+,y_\text{CoT}^-) \sim \mathcal{T}} \Big[ \log \sigma \Big(  \beta \log \\
&\frac{\mathcal{M}(y_\text{CoT}^+ \mid  q, D)}{\mathcal{M}^\text{Ref}(y_\text{CoT}^+ \mid  q, D)} - 
\beta \log \frac{\mathcal{M}(y_\text{CoT}^- \mid q, D)}{\mathcal{M}^\text{Ref}(y_\text{CoT}^- \mid q, D)} \Big) \Big],
\end{aligned}
\end{equation}
where $\beta$ is a hyperparameter and $\sigma$ is the Sigmoid function. $\mathcal{M}^\text{Ref}$ is the reference model, which remains frozen during training.

In DPO training, we input all documents $D$ into the model $\mathcal{M}$ and aim to assign a higher probability to the positive knowledge refinement result $y_\text{CoT}^+$, which is individually generated from one of the retrieved documents. This guides the model $\mathcal{M}$ to rerank the retrieved documents $D$ when generating chain-of-thought as the refinement.



\subsubsection{CoT Refinement through Self-Reflection}
While these generated CoTs help the RAG model generate more accurate answers, the generated CoT results of LLMs may contain undesired patterns, such as ``According to the document'' and ``the reasoning process is''. These training patterns can mislead the LLM ($\mathcal{M}$) to overfit these CoT results during training~\cite{gudibande2023false}. To address this problem, RankCoT proposes a self-reflection method to refine the CoT results $Y_\text{CoT}$.

Specifically, we first sample the CoT outputs $\Tilde{y}_\text{CoT} (d_i)$ by feeding the given query $q$ and each document $d_i$ to the LLM: 
\begin{equation}
 \Tilde{y}_\text{CoT} (d_i) \sim \mathcal{M} (\text{Instruct}_\text{CoT}, q, d_i),
\end{equation}
where $\text{Instruct}_\text{CoT}$ is used to prompt the LLM to generate a chain-of-thought. Then the CoT result $\Tilde{y}_\text{CoT} (d_i)$ is refined as $y_\text{CoT} (d_i)$ by using the same LLM ($\mathcal{M}$):
\begin{equation}
 y_\text{CoT} (d_i) = \mathcal{M} (\text{Instruct}_\text{Ref}, q, \Tilde{y}_\text{CoT} (d_i)),
\end{equation}
where the instruction $\text{Instruct}_\text{Ref}$ prompts the LLM ($\mathcal{M}$) to answer the given query $q$ based on the initial CoT result $\Tilde{y}_\text{CoT} (d_i)$. Such a self-reflection mechanism helps to extract more query-related contents from the initial CoT $\Tilde{y}_\text{CoT} (d_i)$, producing higher-quality data to optimize LLMs.
Finally, we collect the refined CoT results to form $Y_\text{CoT} (d_i) = \{y_\text{CoT} (d_1), \dots, y_\text{CoT} (d_n)\}$, which is used to train the LLM through DPO (Eq.~\ref{eq:dpo}).




% \noindent \textbf{Vanilla Retrieval-Augmented Generation}. For a query $Q$, a standard RAG system consists of these three main components.
% \textbf{1) Indexing}. Given documents $D = \{d_1, d_2, \ldots, d_n\}$, where $d_i$ represents the document chunk. Indexing is the process of converting $d_i$ into vectors through an embedding model $f_e$, and then storing vectors in a vector database.
% \begin{equation}
%     I = \{e_1, e_2, \ldots, e_n\} \; and \; e_i = f_e(d_i)
% \end{equation}

% \noindent\textbf{2) Retrieval}. Transform the query into a vector using the same encoding model, and then retrieve the top k document chunks that are most similar based on vector similarity.
% \begin{equation}
%     (Q, I) \xrightarrow{\text{retriever}} D_q 
% \end{equation}

% \noindent $D_q = \{d_1, d_2, \ldots, d_k\}$ represents the relevant documents for query Q. The similarity function $Sim$ usually used are dor product or cosine similarity.
% \begin{equation}
%     Sim(Q,d_i) = e_Q \cdot e_{d_i} \; or \; \frac{e_Q \cdot e_{d_i}}{\left\| e_Q \right\| \cdot \left\| e_{d_i} \right\|}
% \end{equation}

% \noindent \textbf{3) Generation}. The query $Q$ and the related document $D_q$ are concatenated together and fed into a LLM to generate the final answer.
% \begin{equation}
%     y = LLM(Q, D_q)
% \end{equation}

% \noindent \textbf{RankCoT}. The indexing and retrieval parts of RankCoT are the same as vanilla RAG. Now we define the generation part as follows. For a query $Q$, after retrieval we get the top k relevant documents $D_q$. First, we concatenate them and fed them into a ranking model (RM) to generate a chain of thought (CoT).
% \begin{equation}
%     CoT = RM(Q, D_q)
% \end{equation}
% Then the generated COT and the query are fed into the LLM to generate the final answer.
% \begin{equation}
%     y = LLM(Q, CoT)
% \end{equation}

% \subsection{RankCoT}\label{sec:rankcot}
% As illustrated in Figure~\ref{fig:RankCoT}, Our aim is to use the CoT generated by the ranking model for RAG. Unlike the summary method, CoT includes the reasoning process, rather than simply summarizing the useful information in the documents. Compared with the refinement method, which first determines whether the documents should be retained and then uses the retained documents for RAG, the ranking model generates a CoT by implicitly re-ranking the documents. This CoT can directly assist the LLM in generating answers to the query without having to use related documents again, which also reduces the input tokens of the LLM. Next, we will introduce how to train a ranking model.

% \subsubsection{Data Generation}\label{subsec:datagen}
% First, for each document $D_q = \{d_1, d_2, \ldots, d_k\}$ related to the given query $Q$, we concatenated them with the given query to form a query-document pair $(Q,\{d_i\}_{1}^{k})$. These query-documnet pairs are used as input to let the LLM generate different CoTs separately. So that we can obtain k CoTs for each query.
% \begin{equation}
%     (Q,\{d_i\}_{1}^{k}) \xrightarrow{\text{LLM}} \{ CoT_1, CoT_2, \ldots, CoT_k\}
% \end{equation}

% Subsequently, the obtained different CoTs are concatenated with the query to form query-COT pairs as the input of the LLM to generate the final answer. Similarly, each query has k answers.
% \begin{equation}
% \begin{aligned}
%     &(Q,\{CoT_i\}_{1}^{k}) \xrightarrow{\text{LLM}} \{ ANSWER_1, \\&ANSWER2, \ldots, ANSWER_k\}
% \end{aligned}
% \end{equation}
% \subsubsection{Train your own Ranker}
% After getting the final answers, divide these answers into those that can answer the query correctly and those that cannot answer the query correctly, or arrange them from high to low scores. Randomly select an answer from the correct answer category or take the answer with the highest score as $y^-$, and select an answer from the incorrect answer category or take the answer with the lowest score as $y^+$. Finally, we use the Direct Preference Optimization (DPO)~\cite{rafailov2024direct} training loss to train a ranking model:
% \begin{equation}
% \scalebox{0.8}{$
% \begin{aligned}
%     &\mathcal{L}_{\text{DPO}}(RM;RM_{\text{ref}}) = -\mathbb{E}_{(x, y^+, y^-) \sim \mathcal{D}} \\
%     & \left[ \log \sigma \left( \beta \log \frac{RM(y^+ \mid x)}{RM_{\text{ref}}(y^+ \mid x)} - \beta \log \frac{RM(y^- \mid x)}{RM_{\text{ref}}(y^- \mid x)} \right) \right]
% \end{aligned}
% $}
% \end{equation}
% where $\sigma$ is the Sigmoid function, $\beta$ is a hyperparameter. $\mathcal{D}$ is the dataset containing the input x and its corresponding preference data pairs $(y^+, y^-)$. $RM_{\text{ref}}$ is the reference model, which is frozen during training. 

