\section{Introduction}

The recognition of Activities of Daily Living (ADLs) in Smart Home environments is a widely studied research topic in the pervasive computing community~\cite{bouchabou2021survey}. Recognizing the daily activities that humans do in their daily life at  home (e.g., cooking, watering plants, taking medicines) has several important healthcare applications, including the early detection of cognitive decline~\cite{riboni2016smartfaber}.

The majority of the approaches in the literature are based on deep learning models, mainly due to their effectiveness in reaching high recognition rates~\cite{gu2021survey}. The most common architectures used for ADLs recognition are Convolutional~\cite{arrotta2022dexar} and Recurrent~\cite{liciotti2020sequential} neural networks. However, these approaches may not fully capture the spatiotemporal properties of sensor data. In the literature, Graph Neural Networks (GNNs) have emerged as a promising approach for time series classification~\cite{duan2022multivariate}. In GNNs, sensor data time windows are encoded as graphs capturing both spatial and temporal relationships between sensors. While the majority of existing studies focused on human activity recognition with mobile/wearable devices~\cite{mondal2020new}, only a few GNN-based approaches have been proposed for ADLs recognition in smart home environments~\cite{ye2023graph,s24123944}. 

In general, deep learning models are often considered as ``\textit{black boxes}'' mapping windows of sensor data into activities, and it is challenging to understand the rationale behind their decisions.
The field of eXplainable Artificial Intelligence (XAI) has the goal of mitigating this problem by providing human-understandable explanations to the output of machine learning models~\cite{arrieta2020explainable}. 

Since important decisions in ambient assisted living applications may rely on the output of ADLs recognition, inferring \emph{why} a specific ADL was predicted by the classifier is crucial to provide understandable, trusted, and transparent solutions~\cite{wolf2019explainability}. For instance, XAI would allow clinicians to increase their trust in decision support systems that rely on ADLs recognition (e.g., supporting early detection of cognitive decline).
Data scientists may also benefit from explanations to refine the recognition system, the sensing infrastructure, or the training set. 
%An explainable system would also make it possible to include the residents (and caregivers) in the loop, by showing them which ADLs are released to clinicians and how the system inferred their execution by the resident.

A few works proposed explainable ADLs recognition in smart homes environments~\cite{arrotta2022dexar,das2023explainable}. However, to the best of our knowledge,  eXplainable GNN approaches for sensor-based ADLs recognition in smart homes have not been explored yet. Therefore, in this paper we present \acronym{}, the first explainable GNN-based system for ADLs recognition. Specifically, \acronym{} dynamically constructs a graph starting from windows of environmental sensor data taking into account spatial and temporal aspects. Each graph is processed by a Graph Convolutional Network (GCN) for ADLs classification. An adapted state-of-the-art XAI method specifically designed for GNNs is in charge of determining the most important nodes and arcs of the input for activity classification. This information is finally used to generate an explanation in natural language.

To sum up, the contributions of this paper are the following:

\begin{itemize}
    \item We propose \acronym{}: the first Explainable Graph Neural Network system for Smart Home HAR.
    \item Starting from windows of raw sensor data, \acronym{} dynamically constructs a graph that a GNN processes to classify the most likely activity.
    \item For each prediction, \acronym{} leverages an eXplainable AI approach to produce explanations in natural language. 
    \item Our results show that \acronym{} generates superior explanations with respect to state-of-the-art explainable HAR methods, while slightly improving the recognition rate.
\end{itemize}