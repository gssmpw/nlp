\section{Experimental Evaluation}

\subsection{Datasets}

Two datasets have been used to evaluate the model proposed in this work. They are \textit{CASAS Milan}~\cite{cook2009assessing} and \textit{CASAS Aruba}~\cite{cook2010learning}. 
%Only movement and magnetic sensors have been considered, and temperature sensors for both datasets have been discarded since, in previous works, temperature sensors were observed to have a low impact on the prediction.

\subsubsection{CASAS Milan}    

CASAS Milan consists of data gathered from the home of a female adult volunteer living with a pet and where the woman's children visited periodically. The dataset contains about three months of recording and includes the following classes: Bed-to-Toilet, Chores, Desk Activity, Dining room Activity, Evening Medications, Guest Bathroom, Kitchen Activity, Leave Home, Master Bathroom, Meditate, Watch TV, Sleep, Read, Morning Medications, Master Bedroom Activity. Some of these activities are very under-represented in the dataset; for this reason, following what has been done in~\cite{arrotta2022dexar}, the less represented classes, that are Bed to Toilet, Chores, Meditation, Evening Medications and Morning Medications, are not taken into consideration. Moreover, Master Bathroom and Guest Bathroom have been fused obtaining a new class that contains similar activities in a number comparable with the other classes.

\subsubsection{CASAS Aruba} 
CASAS Aruba is a dataset collected in the home of a woman whose children and grandchildren visited regularly. The dataset contains Meal Preparation, Relax, Eating, Work, Sleeping, Wash Dishes, Bed to Toilet, Enter Home, Leave Home, Housekeeping and Resperate. Resperate and Bed To toilet classes have been dropped according to what has been done in~\cite{liciotti2020sequential}.

\subsection{Implementation details}
We implemented \acronym{} using Python 3.10.5, using Pytorch and Pytorch Geometrics for the models and the explainer.
Other libraries used include Scikit-learn for the evaluation, Networkx for graph visualization, and Pandas and Numpy for data processing.

\subsection{Evaluation}

\subsubsection{Baseline}
We decided to focus our comparison of \acronym{} only with state-of-the-art explainable ADL recognition methods, selecting the one that demonstrated the highest recognition accuracy in the literature.
For this reason, we chose DeXAR~\cite{arrotta2022dexar} as a baseline, since it is the method that meets these criteria. DeXAR converts sensor data into semantic images, leveraging XAI methods for computer vision to generate natural language explanations. Since \acronym{} uses a posthoc explanation method, we compare DeXAR when used with LIME~\cite{ribeiro2016should}.

The original DeXAR implementation also considered previously predicted ADLs as input, while this aspect is not captured by \acronym{}. Hence, we implemented a version of DeXAR not considering past activities.



\subsubsection{Experimental setup}

We consider a standard $70\%$-$20\%$-$10\%$ split to partition the datasets into training, test and validation sets.
The models have been trained using the early stopping strategy with a patience of $50$ epochs, the Adam optimizer with a learning rate of $0.0001$, and a CrossEntropy loss function.

For segmentation, we used the same hyper-parameters suggested in~\cite{arrotta2022dexar} for the CASAS datasets, where the window size is $360$ seconds with an overlap factor of $80\%$.
We discarded all the windows not corresponding to an activity label (i.e., transitions or other activities) as well as temporal windows without sensor events.


%\subsubsection{Classification metrics}

%Precision, recall, F1.

%\subsubsection{Explainability evaluation}
\subsection{Evaluation Metrics}

We use the standard metrics for precision, recall, and F1 score to assess the recognition rate of \acronym{}. 
These metrics provide a comprehensive understanding of the model's performance from different perspectives.

%\begin{itemize}
%    \item \textbf{precision}, corresponds to the accuracy of positive predictions and is computed as the number of true positive divided by the number of windows labeled as belonging to the positive class $$Precision=\frac{TP}{TP+FP}$$
%    Precision is low when there are many false positives compared to the true positives.
%    \item \textbf{recall}, measure the ability of the model to correctly recognize the window of the corresponding class; it is computed as the ratio among true positives and total elements of that class. $$Recall=\frac{TP}{TP+FN}$$
%    recall is low when the number of false negatives is high, with respect to the number of false positives.
%    \item \textbf{f1-score} is the harmonic mean among precision and recall. This means that the f1-score is high only if both precision and recall are high. This is more significant for an unbalanced dataset where one class or another is underrepresented. $$F1=2*\frac{Precision*Recall}{Precision+Recall}$$    
%\end{itemize}
%Another option is to evaluate the model by computing the \textbf{accuracy} as the number of items classified correctly divided by the total number of elements.

However, evaluating the effectiveness of explanations is more challenging. A standard way adopted in the literature involves user surveys~\cite{arrotta2022dexar,das2023explainable,jeyakumar2023x}. However, such method is time and money-consuming.
We leverage a recent work proposing LLMs to automatically compare alternative XAI methods, since it proved to be aligned with user surveys~\cite{fiori2024using}.
Specifically, we provide to an LLM the explanations generated by \acronym{} and DeXAR on the same window, asking the LLM to choose the best one (using the prompt proposed in~\cite{fiori2024using}).



%One way to do so is to analyze how humans react to AI explanations and how often they choose to rely on the system or not. In fact, in some settings like decision support systems, the user may decide whether to trust or not the model prediction; an explanation connected to the prediction may help the user to trust the AI system or detect when the system is making a wrong prediction. Thus, a way to evaluate the final explanation is to study the performance of the whole system composed of humans and AI.
%If we want to evaluate the explanation directly, there exists metrics that can provide a score based on common sense; with this approach, the difficulty is in integrating the "common sense" in an algorithm; a way to do so is to use an ontology as has been done in \cite{arrotta2022dexar}. 
%In this paper, it has been decided to evaluate the explanations given by the model by comparing them with state-of-the-art methods; in future works, more refined techniques, such as the ones described before, will be used for evaluation.
%To compare two or more types of explanations, a user survey asking the user which system they prefer can be done. Unfortunately, this technique is often unfeasible due to its high cost and time consumption.
%If we want to compare two or more XAI systems, a more direct option is to conduct a user survey asking the users which system they prefer; this technique is often unfeasible due to its high cost and time consumption.



\subsection{Results}

\subsubsection{Classification results}

Tables~\ref{tab:clf_milan} and ~\ref{tab:clf_aruba} compare \acronym{} and DeXAR considering the F1 score for each class.
We observe that our approach achieves slightly better recognition rates in the overall F1 score for both datasets.
By observing the confusion matrices in Figure~\ref{fig:cm_milan} and \ref{fig:cm_aruba}, both models struggle to distinguish activities taking place in the same room, like Bathroom and Dress Undress. 

Considering the CASAS Milan dataset, this is likely due to the fact that the wardrobe is located in the master bedroom near the bathroom entrance. To distinguish between these two activities, it is probably necessary to consider additional context information, such as past activities and time. Another remarkable difference between the two models is the higher f1 score of \acronym{} on Leave Home (see table~\ref{tab:clf_milan}). In fact, this ADL strongly depends the temporal order of sensor events, that is better captured by our GNN model.




\begin{figure*}
  \centering
  \begin{tabular}{c @{\qquad} c }
    \includegraphics[width=.46\linewidth]{figures/COG.png} &
    \includegraphics[width=.46\linewidth]{figures/COD.png} \\
    \small DeXAR & \small \acronym{}
  \end{tabular}
  \caption[Classification results on CASAS Milan]{Confusion Matrices for CASAS Milan.}
  \label{fig:cm_milan}
\end{figure*}



Considering the CASAS Aruba dataset, the GNN model performs better than DeXAR for almost all the activities. The main difference with respect to the results obtained in CASAS Milan is that two activities are completely misclassified by both \acronym{} and DeXAR: washing dishes and housekeeping. These two activities are the least represented in the dataset. Wash dishes is always confused with meal preparation. Similarly to CASAS Milan,  the activities that benefit more from the GNN model are entering home and leaving home.

\begin{table}
\begin{center}
\begin{tabular}{ccc}
\hline
& DeXAR~\cite{arrotta2022dexar} & \acronym{} \\
\hline
Bathroom & \textbf{0.55} & 0.53 \\
Dress/Undress & 0.26 & \textbf{0.37} \\
Eat & \textbf{0.67} & 0.61 \\
Kitchen activity & \textbf{0.77} & \textbf{0.77} \\
Leave Home & 0.46 & \textbf{0.74} \\
Read & 0.90 & \textbf{0.91} \\
Sleep & 0.85 & \textbf{0.87} \\
Watch TV & 0.84 & \textbf{0.89} \\
Work & \textbf{0.80} & 0.70 \\
\hline
weighted avg. & 0.77 & \textbf{0.81}\\
\hline
\hspace{5pt}
\end{tabular}
\caption{CASAS Milan: Classification results (F1 score).}
\label{tab:clf_milan}
\end{center}
\end{table}


\begin{table}
\begin{center}
\begin{tabular}{ccc}
\hline
& DeXAR~\cite{arrotta2022dexar} & \acronym{} \\
\hline
Eating & 0.69 & \textbf{0.75} \\
Enter Home & 0.53 & \textbf{0.76} \\
Housekeeping & 0.09 & \textbf{0.14} \\
Leave Home & 0.71 & \textbf{0.82} \\
Meal Preparation & 0.80 & \textbf{0.81} \\
Relax & 0.94 & \textbf{0.96} \\
Sleeping & 0.93 & \textbf{0.96} \\
Wash Dishes & \textbf{0.06} & 0.00 \\
Work & 0.79 & \textbf{0.87} \\
\hline
weighted avg. & 0.90 & \textbf{0.92}\\
\hline
\hspace{5pt}
\end{tabular}
\caption{CASAS Aruba: Classification results (F1 score).}
\label{tab:clf_aruba}
\end{center}
\end{table}


\begin{figure*}
  \centering
  \begin{tabular}{c @{\qquad} c }
    \includegraphics[width=.46\linewidth]{figures/CM3.png} &
    \includegraphics[width=.46\linewidth]{figures/CM4.png} \\
    \small DeXAR & \small \acronym{}
  \end{tabular}
  \caption[Classification results on CASAS Aruba]{Confusion Matrices for CASAS Aruba.}
  \label{fig:cm_aruba}
\end{figure*}


\subsubsection{Explainability results}

As we previously mentioned, we leverage an LLM-based approach to compare \acronym{} and DeXAR. However, due to the costs of LLM-based APIs requests, we sampled $30$ random windows for each activity. Similarly to \cite{arrotta2022dexar}, we only evaluate the quality of explanations associated to correct predictions, since evaluating explanations of wrong predictions is still an open problem and we will consider it for future work.
For each window, we provide the LLM with the explanations generated by the two models and we obtain as output the explanation preferred by the LLM.
Table~\ref{tab:llm-example} shows an example of how the LLM compares the explanations.

\begin{table}[h]
    \caption{Example of LLM-based explaination evaluation.}
    \label{tab:llm-example}
    \centering
\begin{tabular}{|p{2.5cm}|p{9.5cm}|} % Defines the width of each column
  \hline 
  GNNExplainer & I predicted leaving home mainly due to the following observations:  Bob approached the area between the dining room and the hall multiple times, and then the front door. \\
  \hline
  DeXAR &  I predicted leaving home mainly because Bob has been near the front door and between the dining room and the hall.\\
  \hline
  LLM answer &  Let's break down each explanation: \\ & \\ & 1) The first explanation mentions specific observations of Bob approaching the area between the dining room and the hall multiple times before heading to the front door. This sequence of movements aligns with the common behavior of someone preparing to leave home, such as gathering belongings or checking for keys before exiting. Therefore, this explanation provides a clear and logical connection between Bob's movements and the prediction of leaving home. \\ & \\ & 2) The second explanation is similar to the first one but lacks the detail of multiple approaches to the specific areas. While it still mentions Bob being near the front door and between the dining room and the hall, the repetition of movements in the first explanation adds more weight to the prediction of leaving home.  \\ & \\ & Considering the clarity and specificity of the observations provided in the first explanation, it offers a more detailed and plausible rationale for predicting leaving home based on Bob's movements. \\ & \\ & CHOICE=1.\\
  \hline
\end{tabular}
\end{table}

Figure~\ref{fig:BP0} shows, for both datasets, the percentage of times where the LLM preferred explanations from \acronym{} compared to the ones generated by DeXAR. We observe that the explanations generated by \acronym{} were preferred by the LLM in $80\%$ of the times for the CASAS Milan dataset and $69\%$ of the times for the CASAS Aruba dataset.

\begin{figure}
\centering   \includegraphics[width=0.6\textwidth]{figures/BP0_1.png}
    \caption[Explanation evaluation]{Overall percentage of preferences given by the LLM to explanations given by GNN-XAR and DeXar.}
    \label{fig:BP0}
\end{figure}

It is important to note that this preference is not uniform over all the classes classes. Indeed, Figures~\ref{fig:BP1} and~\ref{fig:BP2} shows that more dynamic activities, like entering and leaving home, eating, and preparing a meal achieve a higher score with respect to static activities like sleeping, reading, relaxing and watching TV.
This is reasonable since our graph encoding is better at capturing temporal relations in dynamic activities.
The only time where DeXAR explanations ``wins'' over the ones of \acronym{} is on the Sleep activity for the CASAS Aruba dataset. This is probably due to the fact that in this dataset there is a higher number of sensors in the bedroom, and a slight movement during sleep may trigger more sensors at once. Thus, the GNN may tend to explain the sequence of actions transmitting a false sense of movement from one sensor to the other. This probably can be fixed in future work by adding further heuristics to refine the explanations.
%For example, in a leaving-home activity, the explanation provided by the GNN model can better focus on the fact that the last action of the chain is movement near the door.

\begin{figure}
\centering   \includegraphics[width=0.8\textwidth]{figures/BP1_1.png}
    \caption[Explanation evaluation class by class on CASAS Milan]{Percentage of preferences (for each activity class) given by the LLM to explanations given by GNN-XAR and DeXar for CASAS Milan.}
    \label{fig:BP1}
\end{figure}

\begin{figure}
\centering   \includegraphics[width=0.8\textwidth]{figures/BP2_1.png}
    \caption[Explanation evaluation class by class on CASAS Aruba]{Percentage of preferences (for each activity class) given by the LLM to explanations given by GNN-XAR and DeXar for CASAS Aruba.}
    \label{fig:BP2}
\end{figure}

