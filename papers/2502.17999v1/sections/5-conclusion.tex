\section{Conclusion and Future Work}

In this paper we presented \acronym{}, an explainable Graph Neural Network framework for ADLs recognition in smart home environments.
Our results suggest that \acronym{} generates effective explanations by leveraging the structural properties of the graph representation.
While the results are promising, this work still has limitations, and we plan to extend it following several research directions.

%This paper is focused on the design and implementation of a Graph Neural Network (GNN) designed to be explainable for human activity recognition in smart homes. The project aimed to explore GNN's properties and analyze their compatibility with the HAR problem in smart homes. The final goal was to create a system capable of recognizing resident activity and providing a coherent and clear explanation that a non-expert could understand. 
%To represent sensor data, graphs have been built starting from the sensor multivariate time series, with each node representing a sensor event.
%Then, a GNN model was implemented, trained, and tested with the public datasets of CASAS Milan and CASAS Aruba. To obtain explanations, the algorithm GNNExplainer was used to get scores proportional to the importance of edges and nodes. Then, with some heuristics, a sequence of significant events was extracted. This sequence was later used to provide an explanation in natural language for the final user. The model's performance, both for classification and explainability, has been compared to that of DeXar using the explanations given by LIME. In classifying, the GNN performs slightly better than DeXar, while the GNN model outperforms LIME in generating an explanation.



%FUTURE WORKS


%this paper still leaves room for further studies. 

Currently, \acronym{} only considers binary environmental sensors. We will investigate how to also integrate continuous sensor data from mobile/wearable devices, information about past activities, and other context information.

Another limitation of \acronym{} is that it assigns an importance score to a node or arc within a graph structure, but it does not consider nodes or arcs features in the explanations. Therefore, another possible development might consist in improving the explainer algorithm to also provide such details.


%This work shows how promising GNN is, even in HAR, for smart homes where there isn't an intrinsic graph structure. This paper leaves space for further studies; for example, a way to integrate past activities and other context information must be developed to make the system competitive with other state-of-the-art systems.

%node features
%Further project development could enhance the system by incorporating an extra explanatory algorithm that provides insights into the node features. Currently, the system described in this thesis can allocate an importance score to a node or edge within a graph structure, but it does not offer any details about the value of the features within the node or edge.

%fixed number of events

Regarding the segmentation, in this work we considered fixed time sliding windows, that is the standard approach. In future work, we will investigate the impact of dynamic segmentation~\cite{aminikhanghahi2017using} on \acronym{}.

Finally, while we used LLMs to evaluate the explanations, we will investigate where it is possible to leverage them also to automatically generate explanations starting from the most important nodes and arcs obtained by GNNexplainer.

%This work can be further improved by using LLM, for example is possible to use the LMM to provide a numerical score for the explanations and use that score to fine-tune the threshold applied in the node/edge mask for better choosing what is important or not for the model or even involve the score in the model training. Unfortunately, an experiment in this direction needs a large number of calls to the LLM API that require a relevant amount of financial resources.