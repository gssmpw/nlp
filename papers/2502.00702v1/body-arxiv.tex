\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{pdf/intro.pdf}
    \caption{Online Cardiac Monitoring (OCM).}
    \label{fig:intro}
\end{figure}

\section{Introduction}

Video streaming has exploded in recent years, and its growth shows no signs of slowing down. From social platforms like TikTok that have turned live video sharing into a global phenomenon, to Zoom, which has become synonymous with remote work and learning, video streaming has woven itself into the fabric of our daily lives. The popularity of these platforms has not waned even after the COVID-19 pandemic. The market is booming steadily \cite{VideoStreamingSVoD}, reflecting our collective appetite for real-time, interactive, and accessible content.

Online Cardiac Monitoring (OCM) can be one intriguing enhancement for the next-generation video streaming platforms. 
The rich tapestry of video and audio in streaming not only provides the context of actions, movement, human activities, speech, etc., but it also embeds subtle cardiac events, which have been long neglected in contemporary video streaming systems. 
Uncovering such physiological information would bring various benefits. In the realm of remote health, physicians could remotely access real-time cardiac data without the need for specialized equipment \cite{HeartDiseaseSymptoms}. Similarly, in video gaming, displaying a player's heart rate during live streams could add a new layer of excitement and engagement for viewers \cite{PulsoidRealtimeHearta}. 
Notably, in the Paris Olympics 2024, NBC introduced heart-rate streaming to add a new "gamifying" element for creating compelling TV \cite{Paris2024Parents}. 
This technology could also play a pivotal role in online conferences or interviews, where emotional responses (including lies) inferred from cardiac data \cite{wang2020you, sun2022estimating} could enrich interactions, making them more nuanced and meaningful. 
Furthermore, the potential for this technology extends into security and fraud detection against digital impersonation techniques like deepfakes \cite{liu2019cardiocam, qi2020deeprhythm}. 
These multifaceted applications of OCM underscore its potential to revolutionize video streaming, making it not just a tool for communication and entertainment, but also a platform for health monitoring, affective computing, emotional intelligence, and security.

However, existing online cardiac monitoring either relies on specified hardware \cite{PolarH10Polar}(\eg, heartbeat belt) or introduces additional sensing modalities (\eg, Wi-Fi \cite{liu2015tracking}, mmWave \cite{yang2016monitoring}, and UWB \cite{chen2021movi} \etc.) which are not typically available in live streaming systems.
These approaches suffer from extra cost and are often misaligned with live streams. Moreover, sensing-based approaches necessitate active transmission of the sensing signals \cite{wang2023df, qian2018acousticcardiogram}, which is often impractical to force in live streaming applications. 
A video streaming system that seamlessly enables online cardiac monitoring in pervasive contexts without additional hardware still lacks. 

In this paper, we ask: \textit{Can we incorporate accurate and robust online cardiac monitoring into a video-streaming system without introducing additional hardware or modalities?} 
To build such a system, we answer the following key questions:

First, \textit{what information should we take from the video streaming system to monitor the cardiac activities?} 
Existing works \cite{chen2018deepphys, liu2020multi, niu2020video, li2023learning, yu2019remote,yu2019remoteCompress, yu2023physformer++, liu2023efficientphys, zou2024rhythmformer} on extracting heart rate from human faces focus on remote photoplethysmography (rPPG) which leverages solely video. These video-only solutions are more likely to suffer from low illumination conditions, head movement, and orientation. 
Recent progress in cardiac Vocal User Interfaces (VUIs) \cite{xu2022hearing} inspires us to infer heart rate from human speech. However, audio signals are usually sensitive to noise interference and lack contextual background information, rendering them less robust in real-life scenarios and requiring user calibration. 
Conceptually, video provides detailed visual context while sound exhibits resilience to varying light conditions and body motions. 
Consequently, they offer complementary advantages to enhance cardiac monitoring. This motivates us to move beyond video-only or audio-only solutions, and investigate new designs to combine the naturally co-existed video and audio streams. 

Second, \textit{how to tackle real-world problems to make this system robust and accurate?} Unveiling the cardiac activity from video and audio is challenging. The information is nuanced and easy to be overshadowed by more prominent body movements, environmental dynamics, and/or ambient noise. Previous works \cite{liu2020multi, niu2020video, li2023learning, yu2023physformer++, liu2023efficientphys, zou2024rhythmformer} primarily evaluate models on well-controlled datasets featuring static subjects under optimized light conditions and viewing angles, which simplifies the problems yet becomes unrealistic in real-world settings. 
The task gets even more challenging when deployed in live video streaming environments, due to the discrepancies in frame rates, degraded image quality, and presence of multiple individuals with mixed audio and video streams. 
To deliver an accurate and robust system in practice, novel techniques are desired to effectively discern subtle cardiac signals amidst various disturbances while combating fluctuating frame rates and drifted misalignment of the streams. 

Third, \textit{how to enable Service-On-Demand (SoD) cardiac monitoring in video streaming systems?} Despite the promise of the integration,
enabling SoD for users poses significant challenges due to the complexity of modern video streaming systems. These platforms vary widely, encompassing formats such as conferences \cite{SkypeStayConnected, TeamsChannelsMicrosoft, OnePlatformConnect}, Video-On-Demand (VoD) \cite{StreamTVMovies, NetflixSingaporeWatch}, live streaming \cite{YouTube, ExploreFindYour}, \etc, each with its own technical and operational nuances. 
These providers must balance the demands of real-time data processing with the need for immediate accessibility and minimal latency while not interfering with the original streams. At the same time, deploying our service on edge (\eg, browsers) benefits from preserving privacy, while getting access to the data yields another challenge. One naive way is to deploy our models over the WebRTC peers, but it lacks scalability and versatility. To this end, we are motivated to establish a plug-and-play service that can be seamlessly integrated into video streaming systems, whether hosted on servers or edges. 



In this paper, we present \sysname, the first-of-its-kind online cardiac monitoring system, that can continuously infer the heart rate in video streaming systems. At the core of \sysname, we design a novel audio-video deep learning network, \texttt{CardioNet}, that can effectively learn the nuanced cardiac activities from facial regions and human voices. Specifically, we combine the temporal difference network and a frequency-aware block to model the temporal-spectrum properties from videos. 
We directly exploit the raw audio to capture the cardiac activities by emulating the natural filtering effects of the human body. To handle the irregularly sampled data, we integrate time embeddings to provide temporal context. 
Finally, we fuse audio and visual features through a multi-head temporal attention mechanism, which synergistically combines the strengths of both modalities to produce a robust and precise cardiac monitoring solution.

We further devise systematic solutions to deploy \sysname as a middleware service to support the SoD online cardiac monitoring. 
We introduce practical techniques to handle issues like changing FPS and unsynchronized streams. 
Through in-depth analyses of mainstream video streaming architectures, we realize a \sysname service with effective data hooks and novel packet and buffer designs, which can be easily integrated with various video streaming systems. 

Extensive experiments have been done to validate the effectiveness of \sysname. We have self-collected data through 8 different devices and 10 users. Our evaluation results show that \sysname achieves a mean absolute error (MAE) of 1.79 BPM and root mean square error (RMSE) of 3.25 BPM, largely outperforming the video-only solutions by 69.2\% in MAE and 61.4\% in RMSE,  and the audio-only solution by 81.2\% in MAE and 76.8\% in RMSE. We demonstrate \sysname's generalizability to different environments, devices, and users.
As for \sysname service, we implement our system on two ends, a meeting platform (Zoom) and a content provider (YouTube), respectively. We achieve the overall throughput of 115.97 FPS and 98.16 FPS for each platform respectively, ensuring smooth updates without disrupting the original streams.
These results highlight the robustness and accuracy of \sysname, confirming its potential for widespread application in video streaming systems.

\head{Contributions} We conclude our contributions as follows:

\noindent\ding{182} To the best of our knowledge, we are the first to combine video and audio for cardiac monitoring in video streaming systems. Our solution outperforms video-only or audio-only approaches, especially under adverse conditions in practice.

% \com{In “inferring cardiac monitoring”, the word “inferring” is used incorrectly. “Performing” or “conducting” would be more appropriate.}

\noindent\ding{183} We develop \texttt{CardioNet}, a novel audio-video pipeline that can uncover the nuanced heart rate. Our experiments validate the robustness against different conditions.

\noindent\ding{184} We implement \sysname as a service-based plug-and-play middleware, that can seamlessly be integrated into mainstream platforms for real-time streaming.


\section{Design Scope}
In this section, we will discuss what potential benefits \sysname can bring about and the research scope of this paper. 


\head{Application Momentum}
Consider a scenario where users on platforms such as Zoom or YouTube can access real-time cardiac monitoring. With just a single click, users see their heart rate, providing immediate insights into their emotional and physiological states, including what others are thinking about, whether they are in good health, and how exciting the game is. By online cardiac monitoring, these platforms could significantly enhance user engagement and interactivity. Particularly, \sysname can provide unique and compelling benefits in the downstream applications:

\begin{itemize}
    \item \textbf{Accessibility:} In many video streaming scenarios, such as live product demonstrations on TikTok or Zoom interviews, using wearables or additional hardware is often impractical. OCM can overcome this problem by leveraging modalities that already exist within video streams, thereby increasing accessibility for audiences and facilitating broader engagement.  It also promises wider dissemination of remote health, offering device-free cardiac monitoring compared to the latest work \cite{chen2024exploring} that relies on earphones. 

    \item \textbf{Enhanced Analytical Abilities:} While there exist alternative approaches for tasks including affective computing \cite{mottelson2016affect, prajwal2023towards, wu2020emo, ahmad2024detecting} and deepfake detection \cite{yang2023avoid, demir2021deep}, the cardiac signal shows a strong correlation with them \cite{wascher2021heart, prajwal2023towards}, by capturing the subtle changes in heart rate. In this context, OCM provides an additional verification layer in a real-time and continuous manner, allowing experts to proceed to analyze behaviors. This analysis can help determine if someone is lying, happy, nervous, or engaging in deceptive behavior. 

    \item \textbf{Entertainment:} Our work also presents a distinct chance for augmented entertainment. With the rise of live streaming, the audience can access the heart rates of celebrities, which opens up a new world for the existing viewing experiences.  
\end{itemize}


Despite the potential, there are \textit{no} existing solutions capable of achieving this integration without additional hardware. 
In this work, we focus on addressing this gap by leveraging the co-existence of audio and video signals, specifically in scenarios where a speaker is talking. This can be common in both entertainment and telehealth use cases, including affective computing, remote health, deepfake detection, \etc. \textit{At the core of OCM is the accurate prediction of cardiac information.} Our system should robustly detect the heart rate from the video streaming systems by hooking the video and audio chunks. {Once cardiac data is acquired, it can be further analyzed for various downstream tasks, including affective computing, remote health monitoring, and deepfake detection.} Yet how cardiac monitoring is used for downstream tasks (\eg, emotions, lies, \etc) is not the focus of this paper. 

\head{Audio-Video Pair} We intend to integrate the video and audio information for cardiac monitoring. Leveraging the natural co-existence of audio and video modalities offers contemporary benefits as follows: 
\begin{itemize}
    \item Ubiquity: Video and audio streams are the most fundamental components in video streaming systems, while no additional hardware is needed.
    \item  Feasibility: Both video and audio data contain the cardiac information (discussed in \S\ref{subsec: principle}).
    \item  Complementarity: Audio and video offer different strengths and weaknesses. Audio is less interfered with by motion and light but is sensitive to noises. Video is more robust to noises but will fail in various body movements and non-optimized view angles. We will elaborate the detailed analyses in \S\ref{sec: cardionet}.
\end{itemize}
We argue that in our primary target application scenarios—such as video conferencing, live streaming, and remote healthcare—human speech is inherently present alongside video. Our goal is to fully leverage the potential of these naturally coexisting signals. Additionally, our system is welldesigned to seamlessly fall back to a video-only solution when audio quality degrades.

\head{\sysname as a service}
To deploy such an OCM system, a straightforward way is to build a self-hosted WebRTC service, which, however, does not scale to existing video streaming systems. 
The recent rise of Software as a service (SaaS) provides a scalable and robust software design paradigm to construct our system. 
Therefore, for the sake of versatility, we aim to establish a microservice to host \sysname for seamless integration with mainstream video streaming platforms.

\head{Privacy Concerns} Audio and video data are inherently sensitive and vulnerable to privacy breaches. However, in our proposed scenarios, privacy concerns are mitigated for several reasons. First, the primary purpose of audio and video data in this context is for communication. Therefore, participants are already receiving this data during the meetings, regardless of whether our system is activated or not. In other words, all participants have consented to share their audio and video within the video streaming applications, without requiring extra sensitive data inputs. Additionally, our system is implemented as a middleware solution within existing video streaming systems. These contemporary systems are subject to stringent privacy regulations. \sysname will operate in compliance with these established privacy frameworks.

In a nutshell, the audio-video pair appears to be an attractive choice for ubiquitous and practical OCM, yet it entails numerous challenges to build an accurate and robust multi-modal algorithm and system. We will present our model design in \S\ref{sec: cardionet} and leave the system implementation in \S\ref{sec: sys_design}.


\begin{figure*}[t]
\centering

% Subfigures for body movements, with inner caption
\begin{subfigure}{.99\linewidth}
    \centering
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{pdf/ddpm_l2m1_batch_1.pdf}
        \caption{Stationary}
        \label{subfig:a}
    \end{subfigure}\hfill
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{pdf/ddpm_l2m2_batch_1.pdf}
        \caption{Rotation}
        \label{subfig:b}
    \end{subfigure}\hfill
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{pdf/ddpm_l2m3_batch_1.pdf}
        \caption{Talking}
        \label{subfig:c}
    \end{subfigure}\hfill

    \label{fig:mmpd_motion}
\end{subfigure}\hfill
% Subfigures for illumination conditions, with inner caption

\begin{subfigure}{.99\linewidth}
    \centering
    \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/ddpm_l1m1_batch_1.pdf}
            \caption{Low Light}
            \label{subfig:d}
    \end{subfigure}\hfill
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{pdf/ddpm_l3m1_batch_1.pdf}
        \caption{Incandescence}
        \label{subfig:e}
    \end{subfigure}\hfill
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{pdf/ddpm_l4m1_batch_1.pdf}
        \caption{Nature Light}
        \label{subfig:f}
    \end{subfigure}
    % \caption{The performances of video-based approaches vary under different light conditions.}
    \label{fig:mmpd_light}
\end{subfigure}
\caption{The performances of video-based approaches vary under different body movements light conditions.}
\label{fig: mmpd}
\end{figure*}



% %\vspace{-3mm}
\section{\texttt{C\MakeLowercase{ardio}N\MakeLowercase{et}} Design}
\label{sec: cardionet}
In this section, we will present our design of  \texttt{CardioNet}. We will first describe the underlying fundamentals of inferring cardiac activity from video and audio. Then, we will illustrate our design of model. 


\subsection{Kinetics for Cardiac Learning}
\label{subsec: principle}
\head{Principles} 
% \com{Principles}
In this section, we introduce the principles of extracting cardiac information from video and audio data.  
The fundamental concept revolves around the variations in blood pressure caused by cardiac activities, which manifest as quasi-periodic deformations of blood vessels. 
Since blood vessels circulate blood throughout the body, including the face, lungs, and throat,  we can infer heart activity in these areas through video and audio analysis. 
Specifically, when a light illuminates the skin, subtle color variations caused by pulse-induced blood flow can be captured through video streams. Additionally, as the lungs supply airflow for vocal fold vibrations and the throat modulates voice production, subtle cardiovascular motions associated with these processes can be detected in human speech.

In video streams, when light hits the skin, subtle color changes from pulse-induced blood flow can be captured, as described by the Dichromatic Reflection Model (DRM) \cite{shafer1985using}. We define the Domain of Interest (DOI) of the facial areas as $\Pi \in \mathbb{R}^{N_v \times C \times H_f \times W_f}$, and $\Pi_{i,j} \in \Pi$ denotes the RGB pixels at the $i$-th row and the $j$-th column. To bridge the color with RGB values, we model the spectral relationship as:
\begin{equation}
\label{eq: video_principle}
    \Psi_{\Pi_{i,j}}(f) = I(f) \ast \Delta(f),
\end{equation}
where $I(f)$ is the illumination spectral components, $\ast$ is the convolution operation, and $\Delta(f)$ is the reflection modulator, comprising specular reflection $\Delta_s(f)$ and diffuse reflection $\Delta_d(f)$. Specular reflection occurs at the epidermis level, while diffuse reflection penetrates into the hypodermis, reflecting off capillaries and blood vessels, encapsulating physiological spectrum $H(f)$. We further decompose $I(f)$ and $\Delta_s(f)$ into static and dynamic components, where dynamic components are denoted as $\mu(H(f), O(f))$ and $\nu(H(f), O(f))$, respectively. $O(f)$ is a set of irrelevant signals. $\mu(\cdot)$ and $\nu(\cdot)$ are transfer functions without analytic expressions. Our goal is to infer $h(t)$ from $\Pi$, where $h(t)$ is the temporal counterpart of the spectral representation $H(f)$.

Speech is a complex auditory phenomenon that carries biological information. The airflow is produced from the lungs, which is then modulated by the vocal folds within the larynx to generate sound. This sound is further shaped by the movements and positions of the articulatory organs, such as the tongue and throat. Formally, the speech signal $\Xi$ can be formulated in the frequency domain as 
\begin{equation}
\label{eq: audio_principle}
    \Psi_{\Xi}(f) = L(f) \cdot R(f),
\end{equation}
where $L(f)$ is the sound energy source. $R(f)$ is an acoustic filter creating formant, affected by the vocal tract's physical attributes. Blood flow in surrounding vessels, particularly carotid arteries, influences the acoustic properties \cite{xu2022hearing}.These cardiovascular dynamics are encapsulated in the model by integrating the physiological signal $\hat{H}(f)$ into $R(f)$. 

\head{Observations}
Existing video-based solutions \cite{liu2020multi, chen2018deepphys, yu2019remote, liu2023efficientphys, zou2024rhythmformer, wang2016algorithmic}, though many, are trained on small datasets with controlled environments, \eg, PURE \cite{stricker2014non}. Their performances will degrade greatly when training and testing on more complicated datasets, \eg, MMPD \cite{10340857}. As can be seen from  \fig\ref{fig: mmpd}, the existing video-based solutions cannot effectively capture the cardiac semantics across different body movements and light conditions. These results present a grand challenge for cardiac learning. Meanwhile, different light conditions and body movements will degrade the performance from the video-based approaches, where audio can help \cite{xu2022hearing}. Therefore, our goal is to design a dedicated audio-visual network to extract those motions. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{pdf/cardionet.pdf}
    \caption{Overall Illustration of \texttt{CardioNet}}
    \label{fig:overall_cardionet}
\end{figure}
% \com{Should better cite Wenyao's paper somewhere in the above section.}

% %\vspace{-3mm}
\subsection{Model Design}
Given the underlying cardiac motions, we aim to devise a learning approach to extract $h(t)$. 
As shown in \fig\ref{fig:overall_cardionet}, the DOI pairs, \ie, frames $\Pi$ and audio clips $\Xi$, will be fed into video encoder $E_v$ and audio encoder $E_a$, respectively, to acquire the latent representation. Then we devise a fusion network to aggregate the two modalities. 
% The model will output the regular sampled.
We will elaborate on the details of the design below.  


\subsubsection{Video Branch Design}
We will first introduce $E_v$.

\head{Temporal Differential Block (TDB)} The input video frames $\Pi$  will first be processes as, \ie,
% %\vspace{-1mm}
\begin{equation}
    \dot{\Pi}_{i,j}^{t}=\Pi_{i,j}^t - \Pi_{i,j}^{t-1}.
    % %\vspace{-1mm}
\end{equation}
Note that in online learning, we only have past information, so we perform backward differentiation. The key idea is, we treat the psychological activities as tiny local "motions". It efficiently captures the changes between consecutive frames \cite{wang2021tdn}. Furthermore, TDB plays a crucial role in isolating dynamic features while suppressing static components present in the video data, as stated in \eqn\eqref{eq: video_principle}. 
Furthermore, temporal difference enhances the contrast of the cardiac signal $h(t)$ within the latent space, facilitating more effective feature extraction and subsequent analysis. 
Thereafter, they are fed into convolution networks and upsampled to meet the length of video features. It is also imperative to capture the static information inherent in the video frames. To this end, we integrate a parallel pathway to process the original video frames, allowing for a more comprehensive understanding of the environment. We then introduce lateral connections to facilitate fusion of static and dynamic information.



\head{Static and Dynamic Path}
In addition to dynamic modeling through the temporal difference block described earlier, it is imperative to capture the static information inherent in the video frames. While the temporal difference mechanism offers a means to extract low-resolution and sparse representations, it inevitably overlooks crucial static details such as appearance and illumination. Recognizing the significance of this static information, we integrate a parallel pathway to process the original video frames. This parallel pathway begins with passing the original video frames through a convolution block, followed by downsampling using max pooling
, \ie,
\begin{equation}
    \mathcal{S}(\bm{\Pi^{t}}) = \operatorname{Downsample}(\operatorname{CNN}(\bm{{\Pi}^{t}})).
\end{equation}
This can preserve static information, allowing for a more comprehensive understanding of the environment.
 To ensure mutual awareness and coherence between the dynamic and static pathways, we introduce lateral connections to facilitate fusion. Specifically, bidirectional connections are employed to merge features by downsampling the static branch and upsampling the dynamic branch
 , \ie,
 \begin{equation}
     \hat{\bm{\Pi}} =  \cdot \mathcal{D}(\bm{\Pi}) \oplus \cdot \mathcal{S}(\bm{\Pi})
 \end{equation}
 This fusion mechanism enables the model to effectively leverage insights gained from both temporal dynamics and static features, enhancing overall performance and interoperability.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pdf/video_encoder.pdf}
        \caption{Video Encoder}
        \label{fig:video_enc}
    \end{minipage}\hfill
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pdf/fcb.pdf}
        \caption{Frequency-Aware Convolution Block (FCB)}
        \label{fig:fcb}
    \end{minipage}\hfill
\end{figure}

\head{Motion-Aware Aggregation (MAA)}
The above design incorporates temporal information with static and dynamic modeling. After lateral fusion, we pass the intermediate latent to the bottleneck block
, which gradually reduces the channels of the feature while being effective to extract the spatial information and increase the expressive power. 
We recognize the importance of spatial modeling in mitigating the motion noise from head movement. Unlike video recognition tasks, where the relative location of the pixel is vital, we care more about how to track the variations of these pixels over time. To this end, we introduce a self-attention mechanism for frame-wise aggregation between consecutive frames. Our goal is to establish a mapping between temporal pixel variations and consecutive spatial information.  Given the latent space $\bm{\hat{\Pi}} \in \mathbb{R}^{\hat{T} \times \hat{C} \times \hat{H} \times \hat{W}} $, we query the one pixel at time $t$, \ie,  $\hat{\Pi}^{t}_{i,j}$ and compute the attention with previous frame,
\begin{equation}
    \rho^{t} = \operatorname{Softmax}\left(\frac{\hat{\Pi}^{t}_{i,j} \cdot \left(\hat{\Pi}^{t-1}_{i \pm \Delta i, j \pm \Delta j}\right)^T}{\sqrt{d_k}}\right).
\end{equation}
Here $\Delta i=\Delta j=k/2$, which is the perception grid size. $d_k$ is the dimension of $\hat{\Pi}^{t-1}_{i \pm \Delta i, j \pm \Delta j}$. $\rho^t$ captures the inter-frame pixel displacement, drawing attention to motion while enhancing temporal features between frames. Subsequently, we can get the weighted sum of temporal neighbor frames and aggregate with a query to enhance the original pixel:
\begin{equation}
    \check{\Pi}^{t}_{i,j} = \hat{\Pi}^{t}_{i,j} + \rho^{t} \cdot \hat{\Pi}^{t-1}_{i \pm \Delta i, j \pm \Delta j}.
\end{equation}
This mechanism scrutinizes pixel displacements across consecutive frames, akin to tracing the path of movement within a sequence of images. Each pixel's attention weight encapsulates its significance in depicting motion, allowing the model to recognize subtle shifts and fluctuations over time. 

This mechanism allows the model to selectively attend to relevant spatial features and enhance the temporal motion modeling. 
In essence, this scheme can be conceptualized as a motion vector, dynamically tracking and encoding the intricate temporal variations within video data. 
By leveraging the self-attention mechanism, 
The model scrutinizes pixel displacements across consecutive frames, akin to tracing the path of movement within a sequence of images. Each pixel's attention weight encapsulates its significance in depicting motion, allowing the model to recognize subtle shifts and fluctuations over time. Through this process, the model 
elucidates the underlying motion patterns embedded within the video stream. 

\head{Frequency-Aware Block (FAB)}
After applying motion attention aggregation, we acquire the enhanced feature $\bm{\check{\Pi}} \in \mathbb{R}^{T \times \hat{C} \times \hat{H} \times \hat{W}}$. Our previous focus has been on modeling video dynamics in the temporal domain.
These are very effective designs for cardiac time series learning. Moreover, given the intrinsic property of $h(t)$, which turns out to be a quasi-periodic signal,  it becomes imperative to incorporate frequency features into our analysis. Here, the term "frequency" does not merely refer to the spectrum of color space within the video; rather, we aim to capture the underlying frequency variations of pixels over time. 
Inspired by DTF \cite{long2022dynamic}, we 
attempt to explicitly incorporate FFT in our design. For each pixel $\bm{\check{\Pi}_{i,j}} \in \mathbb{R}^{T \times \hat{C}}$, we apply FFT along the temporal dimension to acquire the feature spectrum $\Psi_{\bm{\check{\Pi}_{i,j}}}(f)$. 
To capture the frequency information, we introduce a learnable frequency filter $\Psi_{G}(f) \in \mathbb{R}^{\hat{C} \times N_{f}}$. We use IFFT to get the modulated temporal feature.
With FCB, we can enlarge the receptive field and profile cardiac time series with frequency constraints.

\head{Irregular Sampled Time Embedding} 
Another challenge of online cardiac learning is the fluctuating FPS. 
To this end, we introduce the timestamp feature to handle the irregular sampled time learning. Technically, we can acquire the set of timestamps $\{t_i\}_{i=1}^{N_v}$ for each frame. We incorporate a timestamp embedding $E_t$ design and fuse it with $\bm{\Theta_{E_v}(\Pi)}$. Specifically, we employ a frequency embedding scheme, which computes triangle embedding based on a geometric progression of frequencies up to $f_m$. We first derive a set of frequencies with the size of embedding dimension $N_{t_d}$, \ie,
\begin{equation}
    \omega^{k} = \exp \left(\frac{2k}{N_{t_d}}\cdot \log (f_m)\right),
\end{equation}
where $k=1, \cdots N_{t_d}/2$. Then the angle for each timestamp $i$ is given by 
\begin{equation}
    \theta_{i}^{k} = t_i \cdot \omega^{k} \cdot 2\pi.
\end{equation}
Finally, the timestamps are embedded through trigonometric encoding by concatenating sine and cosine values for each angle. 




\subsubsection{Audio Branch Design}
We then introduce the design for the audio encode $E_a$. As discussed in \S\ref{subsec: principle}, human speech is modulated by the time-varying filter $R(f)$. And the cardiac series is encapsulated in $R(f)$. Inspired by this filtering process, we opt to emulate it within our design.  We target directly processing raw audio in our case. We will justify the rationale first, followed by our design.

\head{Raw Audio}  Traditional audio-based learning often leverages mel-spectrogram, a common practice for tasks like speech recognition. However, this method may not be suitable for our task. Our predictions, $h(t)$, manifest as quasi-periodic signals, ideally shown as straight lines on a mel-spectrum. But because cardiac activities are variable, these lines will exhibit randomness on a temporal-frequency map. Also, the location of the "straight" line has physical meanings, rather than a simple pattern. Therefore, we resort to learning from the raw audio signals directly.  
The key insight is, the process of producing speed from our vocal organs is composed of several acoustic filters, as indicated in \S\ref{subsec: principle}.  We can simulate the effect of filters and incorporate them in our design. 

\head{Temporal-Frequency Filter (TFF)} 
The temporal format of \eqn\eqref{eq: audio_principle} can be rewritten as $\xi(t) = l(t) \ast r(t)$,
where $\xi(t)$ is the speech signal. $l(t)$ represents the source of the sound while $r(t)$ is combination of source filters.  To this end, we adopt the SincNet  \cite{ravanelli2018speaker}, which can be expressed as,
\begin{equation}
    r_i(t, \theta) = 2 f_{i,2}^{\theta} \operatorname{sinc} (2 \pi f_{i,2}^{\theta} \cdot t) - 2 f_{i,1}^{\theta} \operatorname{sinc} (2 \pi f_{i,1}^{\theta} \cdot t).
\end{equation}
$f_{i,2}^{\theta}$ and $f_{i,1}^{\theta}$ denotes the two cutoff frequencies. We can treat the two cutoff frequencies as learnable parameters. 
We then perform convolution between $r_i(t)$ and raw audio $\xi(t)$. 
They will be fed into 1D convolution blocks for feature extraction.

\subsubsection{Fusion Block Design}
Until now, we have handled the video feature $\Theta_{E_v}(\Pi)$ and audio feature $\Theta_{E_a}(\Xi)$. We now present the design of the fusion network. We opt for the late-fusion scheme, as the relationships between audio and cardiac activity, as well as video and cardiac activity, are not initially apparent. Within the fusion block, we aim to address two challenges: 1) aligning the audio and video features along the temporal domain, and 2) handling the sampling rate mismatch between the audio and video features. To do so, we propose a multi-head temporal attention fusion block. Subsequently, the fused feature will be passed through linear fully connected layers. 
To achieve this, we adopt a temporal attention-based fusion scheme. Technically, we exploit video features as the query, and audio features as the key and value
, \ie,
\begin{equation}
    \Theta_{f}(\Pi, \Xi) = \operatorname{Softmax}\left(\frac{\Theta_{E_v}(\Pi) \cdot \Theta^T_{E_a}(\Xi)}{\sqrt{d_{E_v}}} \right) \cdot \Theta_{E_a}(\Xi).
\end{equation}
The fused feature $\Theta_f(\Pi, \Xi)$ will be fed to the output layer.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{pdf/streaming_arch.pdf}
    \caption{The architecture of a video streaming system.}
    \label{fig:streaming_arch}
\end{figure}
\subsubsection{Loss} 
In this part, we will elaborate our loss function design. 

In the time domain, we calculate the focal loss:
\begin{equation}
    \mathcal{L}_{\text{dis}} = \frac{1}{N}\sum_{i=1}^{N} \left(\epsilon \cdot \left(1-\operatorname{e}^{-\mathcal{M}\left(\hat{h(t_i)}, h(t_i)\right)}\right)^\eta \cdot \mathcal{M}\left(\hat{h(t_i)}, h(t_i)\right)\right),
\end{equation}
where $\hat{h(t)}$ is the predicted cardiac series. $\epsilon$ and $\eta$ are the balancing coefficient and focusing coefficient, respectively. $\mathcal{M}(\cdot, \cdot)$ represents the mean square error (MSE).
Given the inherent complexity and variability of physiological signals, focal loss offers a more robust framework for capturing peaks. Besides, we also introduce similarity loss,
\begin{equation}
    \mathcal{L}_{\text{sim}} = 1 -  \frac{\sum_{i=1}^N \left(\hat{h(t_i)}\cdot h(t_i)\right) }{\sqrt{\sum_{i=1}^N \hat{h(t_i)^2}}\cdot \sqrt{\sum_{i=1}^N h(t_i)^2}}.
\end{equation}
The range of $\mathcal{L}_{\text{sim}}$ is $[0,2]$, where $\mathcal{L}_{\text{sim}}=0$ represents perfect alignment and is the optimization goal. Additionally, as we are learning quasi-periodic signal, we incorporate spectral loss as well. Specifically, we calculate the FFT-MSE loss,
\begin{equation}
    \mathcal{L}_{\text{freq}} = \mathcal{M}\left(\Psi_{\hat{h(t)}}(f), \Psi_{h(t)}(f) \right),
\end{equation}
where $\Psi(\cdot)$ represents the spectrum function. To wrap up, we calculate the weighted sum of the above three losses,
\begin{equation}
    \mathcal{L}_{\text{all}} = \alpha \cdot \mathcal{L}_{\text{dis}} + \beta \cdot \mathcal{L}_{\text{sim}} + \gamma \cdot \mathcal{L}_{\text{freq}},
\end{equation}
where $\alpha$, $\beta$ and $\gamma$ are weights to balance the loss items. 
Thus far, we present the design of \texttt{CardioNet}. To our best knowledge, it is the first audio-visual network designed for cardiac learning. We will demonstrate the performance in \S\ref{sec:evaluation}. 

\section{\sysname Design}
\label{sec: sys_design}

In this section, we will introduce the design of \sysname. We will introduce the design goal of \sysname in \S\ref{subsec: design_goal}, followed by our detailed designs of service in \S\ref{subsec: buffer} and \S\ref{subsec: service}. We will introduce the preprocessing in \S\ref{subsec: preprocess}.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.8\linewidth}
    \begin{subfigure}{.48\linewidth}
            \includegraphics[width=\linewidth]{pdf/cardiolive_zoom_arch.pdf}
        \caption{In-app bot}
        \label{fig:zoom_arch}
        \end{subfigure}\hfill
        \begin{subfigure}{.48\linewidth}
            \includegraphics[width=\linewidth]{pdf/chrome_webgl_webaudio.pdf} % change
            \caption{Web extension}
            \label{subfig:chrome_av}
        \end{subfigure}
        % %\vspace{-0.1in}
    \caption{Data Hook Design.}
    \label{fig:data_hook}
    \end{minipage}
\end{figure}

% %\vspace{-3mm}
\subsection{Design Goal}
\label{subsec: design_goal}
Modern video streaming systems are complicated, and integrating OCM into them is non-trivial. 
As shown in \fig\ref{fig:streaming_arch}, the content is sent through cloud servers spanning across different locations globally. 
Besides running the data center and cloud computing, these video streaming systems offer a range of application services, such as content summarization, transcriptions, and AI-driven interactive features. 
Note that for content providers like YouTube, Netflix, and many VoD providers, integrating new features is relatively straightforward because they can preload resources in their data centers.  However, this does work well with streaming systems with live content generation and interactions.
On the other hand, deploying cardiac monitoring on edge devices is also valuable. Users will be concerned about how the sensitive data are communicated over the network. 

Therefore, to achieve SoD cardiac monitoring service, we need to both consider deploying the cardiac monitoring services on the edge ends, \eg, browsers, and application services. Notably, direct access to data that manufacturers possess is often restricted by stringent privacy regulations affecting external developers. 
To this end, we aim to package \sysname into a service, which both end users and manufacturers can readily access. 
At a high level, we are not concerned about specific implementations on specific platforms, but aim to develop \sysname as a \textit{microservice}. We utilize data hooks to capture video and audio streams, organizing them into buffer queues as data packets, which will be fed to the inference engines, as detailed below. 

\subsection{Buffer Design}
\label{subsec: buffer}
\head{Data Hook} We will first introduce data hooks to get the video and audio streams, namely \texttt{onVideoDataReceived()} and \texttt{onAudioDataReceived()}. Meeting platforms like Zoom usually support in-app bots that virtually participate in calls. We can leverage the bots to access the raw data streams, as shown in \fig\ref{fig:zoom_arch}. Meanwhile, increasingly more video streaming systems are based on web pages, \eg, YouTube, Bilibili, \etc. Direct accessing the video streams of this platform is rather complicated and violates the policies. To this end, we leverage WebGL and WebAudio that exist in modern browsers to get the data streams, as shown in \fig\ref{subfig:chrome_av}. The browsers usually provide the Document Object Model (DOM), a programming interface to manipulate the structure, style, and content of web content. Our service will first access the canvas, an element for graphics on a web page, through DOM. 
The canvas offers a bitmap where each pixel can be individually manipulated. We get the rendering context through WebGL, which operates as a rendering context of canvas using the underlying GPU. We create an offscreen canvas that is rendered off the main thread and read the pixels through WebGL, preventing it from interfering with the normal UI updates. Meanwhile, we capture the audio from the video element through WebAudio, a versatile framework to handle audio operations on the web. We record the timestamp of the audio and video as well. Through the data hook, we can acquire the video and audio streams. Then we will construct them into data packets and buffer queues.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{pdf/cardiolive_buffer_queue.pdf}
    \caption{Packet and Buffer Design}
    \label{fig:packet_design}
\end{figure}
\head{Data Packet}
Normally, audio and video are encoded in separate ways. In meeting platforms, the video frames are usually encoded in YUV format. They are designed for the best transmission efficiency. Encoding the data in YUV space allows fewer total bits of space in a video stream for the colors to be shown. 
% The YUV is composed of a physical brightness, blue projection and red projection layers. 
To recover the original RGB streams, we have first to decode the YUV streams. To reduce the cost of decoding, we adapt a streaming-based decoding pipeline from GStreamer \cite{GStreamer}. We set the \verb|appsink| property for receiving the RGB data and assign \verb|appsrc| for handling YUV encoding. We set the transformation in asynchronous mode so that the incoming frames will not conflict with the current operations. After that, we will construct the collected frames in buffers. 
 
Then we feed the video-audio pair into the forwarding packets. For audio and video streams, we apply the same packet format, which contains a unique header, an identifier, the data size, timestamps, and the encoded payload data, as illustrated in \fig\ref{fig:packet_design}. The unique header is designed to judge whether the packet is correctly constructed and not mixed with other packets.
The identifier is assigned to indicate whether it is audio or video data packets. We embed the received timestamps to denote the sequence of the video and audio, which will be further used for synchronization.


\begin{figure}[t]
    \centering
    % Row 1
    \begin{minipage}{0.8\linewidth}
        \begin{subfigure}{.48\linewidth}
            \includegraphics[width=\linewidth]{pdf/FPS_chrome.pdf}
            \caption{Chrome}
            \label{subfig:FPS_chrome}
        \end{subfigure}\hfill
        \begin{subfigure}{.48\linewidth}
            \includegraphics[width=\linewidth]{pdf/FPS_zoom.pdf} % change
            \caption{Zoom}
            \label{subfig:FPS_zoom}
        \end{subfigure}
    \end{minipage}
    %\vspace{-0.4cm}
    \caption{The FPS vary and change rapidly.}
    \label{fig: FPS}
\end{figure}



% %\vspace{-1mm}
\subsection{Service Design}
\label{subsec: service}
We abstract our system as a plug-and-play service. 
Our service first gets the hooked video and audio packets as the input. 
The data will be preprocessed and fed to the inference engine for output. 
Our design overcomes the two challenges: fluctuating FPS and unsynchronized audio and video streams. 

\head{Changing FPS} The fluctuating FPS will lead to two subproblems. 
Initially, the video streaming systems will ideally have 30 FPS but in reality undersampled at the receiver's end, as illustrated in \fig\ref{fig: FPS}, with some outliers present as well. 
Additionally, the frame rate is not constant, resulting in a varying number of frames within a given window. However, our model assumes a fixed 4-s input, with 120 frames of video (30 FPS) and 32000 samples of audio (8kHz). In other word, we have to adapt the real input size to the model.
To this end, instead of padding empty frames at the end, we duplicate one single frame circularly. For instance, if the actual FPS is 25, we insert an additional identical frame after every 5 frames to approximate a smoother transition to 30 FPS. Any remaining gaps at the end of the sequence are filled by repeating the last frame. As for overlarge FPS, we downsample the frames.  
For the audio clips, as 8kHz is much lower than the typical sampling rates (usually 32kHz or 44.1kHz) in modern video streaming systems, we can concatenate the received audio chunks and safely downsample them to 8kHz. 


\begin{figure}[t]
    \centering
    \begin{minipage}{0.8\linewidth}
        \begin{subfigure}{0.68\linewidth}
            \centering
            \includegraphics[width=\linewidth]{pdf/av-drifting.pdf}
            \caption{The temporal drifting}
            \label{fig:temporal_drifting}
        \end{subfigure}
        \begin{subfigure}{0.3\linewidth}
            \includegraphics[width=\linewidth]{pdf/sync_scheme-vertical.pdf} % change
            \caption{Sync}
            \label{subfig:sync_scheme}
        \end{subfigure}
    \end{minipage}
    %\vspace{-0.4cm}
    \caption{Audio-Video Synchronization Scheme.}
    \label{fig: res_dis}
\end{figure}
\head{Audio Video Synchronization}
The audio-visual misalignment is a more severe issue than changing FPS. 
As we are hooking audio and video from separate channels, they are likely to lose synchronization with the increase of time. As can be seen from \fig\ref{subfig:chrome_av}, the starting time of the audio and video will be misaligned quickly with accumulating drifts. To overcome this issue, we develop a scheme to ensure the audio and video chunks are synchronized before sending to the inference engine. Given the audio and video streams $S_a(t)$ and $S_b(t)$, they will be extended to the buffer queues $Q_a(t)$ and $Q_v(t)$, respectively. We also maintain $t_a$ and $t_v$ as the starting time of audio and video chunks, respectively. We denote $\Delta t_n = t_a^{k} - t_v^{k}$ as the temporal drift between audio and video streams at $k$-th trial. To mitigating the continuously increasing $\Delta t_n$, we align the start time at each step $k$ as, 
\begin{equation}
    t_{\text{start}}^{k} = \max(t_{a}^{k}, t_{v}^{k}),
    % %\vspace{-4mm}
\end{equation}
when $\Delta t_n$ is larger than the threshold $\epsilon_t$. We use $\epsilon_t=0.3$s. Then the ending time will be determined by $t_{\text{end}}^{k} = t_{\text{start}}^{k} + t_w$,
where $t_w$ is the window lengths. Note that we are adopting a sliding window scheme, with window length $t_w$ and step length $t_s$. For the next window, the start time will be updated by finding the timestamp closest to, \ie, $t_{a}^{k+1} = t_{a}^{k} + t_s$ and $t_{v}^{k+1} = t_{v}^{k} + t_s$.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{pdf/exp_setting.pdf}
    \caption{Experimental Setups}
    \label{fig:exp_set}
\end{figure}
 Meanwhile, we will pop the items that have been processed from the buffer queues, \ie, 
 \begin{equation}
     \begin{aligned}
         Q_{a}(t) &= Q_{a}(t) \textbackslash \{S_a(t) | t < t_{a}^{k+1}\} \\
         Q_{v}(t) &= Q_{v}(t) \textbackslash \{S_v(t) | t < t_{v}^{k+1}\}
     \end{aligned}
 \end{equation}
We then feed the synchronized pairs for inference.

\subsection{Preprocessing}
\label{subsec: preprocess}
In this section, we will discuss the preprocessing pipelines. 
We use OpenCV face detector to find the faces. We also perform voice activity detection to segment the talking period. Additionally, we need to separate multiple persons, if any, and match their audio and videos.

\head{Multi-person Separation}
We deduct the more challenging multi-user case into the single-user case by separating them. 
Initially, face detection can determine the number of participants. To ensure facial resolution, we focus on the largest $N_f$ faces, disregarding the others. 
Similarly, we will only consider $N_f$ speech clips with the largest power spectrum when separating audio. For efficiency, we choose $N_f=2$ in our paper. At this stage, the separated faces and speech segments may not correspond to each other. To address this mismatch, we proceed with audio-visual matching as described next.




\head{Audio-Visual Matching}
To realize the matching between speaking clips and facial hints, we adopt a cross-attention scheme \cite{tao2021someone, jiang2023target}. Specifically, after the encoders, we get two features $M_a$ and $M_v$. These features are expected to encapsulate relevant speaking activities by employing temporal encoders \cite{hu2018squeeze, afouras2018conversation}. To fuse the audio and video features, The audio features $M_a$ are integrated with the video data by treating $M_v$ as the target for querying through an attention framework. Conversely, the video features $M_v$ interact with $Q_a$, representing the audio query sequences. The outputs are concatenated together along the temporal direction.

\section{Evaluation}
\label{sec:evaluation}
In this section, we systematically evaluate \sysname. We aim to answer the following questions:
\begin{enumerate}
    \item To what extent does \sysname outperform existing video-only and audio-only solutions?
    \item How effectively does \sysname perform across various scenarios, including edge cases?
    \item How does the streaming service of \sysname perform when integrated into real-world video streaming systems?
\end{enumerate}

We perform comparison studies with the state-of-the-art (SOTA) video-based solutions and audio-based solutions. We mainly leverage our self-collected dataset. We use the following metrics to evaluate the accuracy of the model: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE).

\begin{figure*}[t]
    \centering
    % Row 1

    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pdf/av_legend_long.pdf}
    \end{minipage}

    \begin{minipage}{\linewidth}
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/dis_mae_mae.pdf}
            \caption{MAE}
            \label{subfig:dis_mae}
        \end{subfigure}\hfill
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/dis_mape_mape.pdf}
            \caption{MAPE}
            \label{subfig:dis_mape}
        \end{subfigure}\hfill
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/dis_rmse_rmse.pdf}
            \caption{RMSE}
            \label{subfig:dis_rmse}
        \end{subfigure}
    \end{minipage}
    %\vspace{-0.4cm}
    \caption{The performances for different distances.}
    \label{fig: res_dis}
\end{figure*}

\begin{figure*}[t]
    \centering
    % Row 1
    \begin{minipage}{\linewidth}
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/angle_mae.pdf}
            \caption{MAE}
            \label{subfig:angle_mae}
        \end{subfigure}\hfill
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/angle_mape.pdf}
            \caption{MAPE}
            \label{subfig:angle_mape}
        \end{subfigure}\hfill
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/angle_rmse.pdf}
            \caption{RMSE}
            \label{subfig:angle_rmse}
        \end{subfigure}
    \end{minipage}
    %\vspace{-0.4cm}
    \caption{The performances for different angles.}
    \label{fig: res_angle}
\end{figure*}

\head{Data Collection} There is no existing dataset that can fit our requirements, with audio-visual pairs and clear heart rate ground truth. Therefore, we self-collect the dataset through 8 commodity devices which span multiple mainstream platforms (iOS, Android, Windows, Mac), major brands, and device types (smartphones, tablets, laptops, and webcams) released between 2018-2024:  Logitech C930 Webcam, OPPO Reno 2Z, Redmi Note 5, Honor 20i, MacBook Air M2, iPad 2018, iPad 2023 Pro, and iPhone 14. We leverage Polar H10 \cite{PolarH10Polar} to collect the ground truth. We utilize Flutter to develop a cross-platform application capable of recording raw video and audio, while also establishing the connections to the ground truth via MQTT. Our dataset comprises recordings from 10 users of diverse genders and regions. They are requested to read 10 materials \cite{sun2021ultrase}, counting for 2,800 words. Each round lasts for 40 minutes. They wear the heartbeat belt when they are reading. We do not restrict users to a fixed distance from the recording devices. We leverage a tripod along with a ring light to cast different light sources on the users. We collected a total of 84,666 data clips, which are clipped into facial regions with 4-s windows. We resize the video frames to 72$\times$72$\times$3 and the audio is resampled to 8kHz. The missing frames will be duplicated adopting the same scheme as \S\ref{subsec: service} mentions. We gain IRB from our university board. Moreover, we also make use of two publicly available video-only datasets: PURE \cite{stricker2014non} and MMPD \cite{10340857} to validate the video-only solution.

\head{Software} We implement \texttt{CardioNet} through Pytorch 2.4.0. The model is trained via a single-card NVIDIA A100 80GB. We train the model with the learning rate of 1e-3,  AdamW optimizer, batch size of 16, OneCycle scheduler. We use Pytorch JIT to compile the model. We write 2000+ lines C++ code to implement the service in Zoom and 1500+ lines JavaScript code for developing the service in the extension. 


\head{Deployment}
We propose two deployment paradigms, web-based and app-based. For the web-based one, we develop a browser extension that operates \sysname in the background, which continuously captures audio and video data for processing, with results displayed on a canvas within the interface. In the app-based deployment, we register a bot in compliance with the policies of the video streaming companies. This bot joins the sessions as a member, similar to other participants, with the consent of all members. The data hook extracts audio and video towards inference engines. The processed results are delivered through a notification system. Notably, the inference can be performed either on the company's cloud server or locally on the user's device.  In our real-world evaluation, we perform inference on the user's device (Xiaoxin 16 Pro with AMD Ryzen 7 5800H), demonstrating the robustness and efficiency of our model.



% %\vspace{-3mm}
\subsection{Comparative Study}
We compare our \texttt{CardioNet} with various baselines. We choose the SOTA video-only baselines: TS-CAN \cite{liu2020multi}, DeepPhys \cite{chen2018deepphys}, PhysNet \cite{yu2019remote}, EfficientPhys \cite{liu2023efficientphys}, RhythmFormer \cite{zou2024rhythmformer}, POS \cite{wang2016algorithmic}. The last one represents the signal processing based rPPG approaches. We also reimplement VocalHR \cite{xu2022hearing}, the recent work that employs human speech for detecting heart rate. Through this study, we will justify our superior performances using both audio and video modalities.

\head{Distances} We first experiment with different distances, ranging from 0.5m to 2.5m. We apply the logarithmic scale to each graph, with the base of 10. 
As shown in \fig\ref{fig: res_dis},
while the error increases with distance for all methods, our approach consistently outperforms other baseline models at all tested distances. \texttt{CardioNet} achieves a MAE of just 1.40 BPM at 0.5m, significantly lower than the SOTA video-based baseline, \ie, RhythmFormer, by 73.7 \%, and 96.7\% lower than the worst-performing model, \ie, POS. Meanwhile, the audio-based model VocalHR has a MAE of 8.12 BPM at the same distance, which is 82.8\% higher than ours. 
Even at the maximum testing distance of 2.5 meters, \texttt{CardioNet} is still 63.1\% better than RhythmFormer and 77.9\% better than VocalHR. This demonstrates the fusion of audio and video signals in \texttt{CardioNet} significantly enhances the overall performance. Besides, we observe the identical patterns of MAE, MAPE and RMSE, we will mainly report MAE for simplicity.


\head{Angles}
We evaluate our model across a range of angles from 0° to $\pm$60° at a distance of 1 meter, as shown in \fig\ref{fig: res_angle}. As the viewing angle increases, video-based methods suffer from significant performance degradation due to reduced visibility of facial features. However, \texttt{CardioNet}, through audio-visual fusion, maintains robust performance across all angles. While the video quality deteriorates with extreme angles, audio signals remain largely unaffected by viewing angles. Even at extreme angles like $\pm$60°, where video signals typically falter, our model achieves up to 38.9\% lower MAE compared to baseline models, highlighting its superior resilience in challenging conditions. This result underscores the critical role of the audio modality in compensating for the loss of visual information at extreme angles.


\begin{figure}[t]
    \centering
    % Row 1
    \begin{minipage}{\linewidth}
        \begin{subfigure}{.19\linewidth}
            \includegraphics[width=\linewidth]{pdf/30_cdf.pdf}
            % \caption{}
            \label{subfig:noise_30}
        \end{subfigure}\hfill
        \begin{subfigure}{.19\linewidth}
            \includegraphics[width=\linewidth]{pdf/32_cdf.pdf}
            % \caption{MAPE}
            \label{subfig:noise_32}
        \end{subfigure}\hfill
        \begin{subfigure}{.19\linewidth}
            \includegraphics[width=\linewidth]{pdf/34_cdf.pdf}
            % \caption{RMSE}
            \label{subfig:noise_34}
        \end{subfigure}\hfill
        \begin{subfigure}{.19\linewidth}
            \includegraphics[width=\linewidth]{pdf/36_cdf.pdf}
            % \caption{MAE}
            \label{subfig:noise_36}
        \end{subfigure}\hfill
        \begin{subfigure}{.19\linewidth}
            \includegraphics[width=\linewidth]{pdf/38_cdf.pdf}
            % \caption{MAPE}
            \label{subfig:noise_38}
        \end{subfigure}
    \end{minipage}
    %\vspace{-0.7cm}
    \caption{CDF for different noise levels.}
    \label{fig: res_noise1}
\end{figure}

\begin{figure}[t]
    \centering
    % Row 1
    \begin{minipage}{\linewidth}
        \begin{subfigure}{.325\linewidth}
            \includegraphics[width=\linewidth]{pdf/Rain_cdf.pdf}
            % \caption{}
            \label{subfig:noise_rain}
        \end{subfigure}\hfill
        \begin{subfigure}{.325\linewidth}
            \includegraphics[width=\linewidth]{pdf/Music_cdf.pdf}
            % \caption{MAPE}
            \label{subfig:noise_music}
        \end{subfigure}\hfill
        \begin{subfigure}{.34\linewidth}
            \includegraphics[width=\linewidth]{pdf/TV-Show_cdf.pdf}
            % \caption{RMSE}
            \label{subfig:noise_tv}
        \end{subfigure}
    \end{minipage}
    %\vspace{-0.7cm}
    \caption{CDF for different noise sources.}
    \label{fig: res_noise2}
\end{figure}

\head{Noise Levels}
We test heart rate estimation under noise levels from 30 dB to 38 dB. As in \fig\ref{fig: res_noise1}, increasing noise leads to higher absolute error. Nonetheless, \texttt{CardioNet} consistently outperforms the SOTA audio-only model \texttt{VocalHR}. This can be attributed to our temporal frequency filter design and the video modality which provides complementary information that remains stable under acoustic noise. For instance, at 30 dB, our model achieves a MAE of 1.25 BPM, significantly lower than \texttt{VocalHR}'s 8.64 BPM, and maintains this advantage even at 38 dB. The fusion network learns to adaptively reduce reliance on noisy audio features while leveraging more stable visual cues. The CDF curves show that \texttt{CardioNet} achieves higher cumulative probabilities at lower error thresholds, indicating its resilience to noise.


\head{Noise Sources}
We analyze the impact of noise sources such as rain, music, and TV shows in \fig\ref{fig: res_noise2}. \texttt{CardioNet} demonstrates strong noise resilience, particularly with rain noise, where it significantly outperforms VocalHR, achieving a MAE of just 1.94 BPM compared to 12.93 BPM. 
Even with more complex noise like music and TV shows, our model maintains lower MAEs, showcasing its robustness in diverse acoustic environments. This highlights the effectiveness of video modalities when facing the ambient noises.






\head{Body Motions}
Body motion can significantly impact the performance of heart rate detection models. To validate the robustness of our approach under different body motion scenarios, we evaluate the model in three typical body movements: walking, left-right (LR) rotation, and up-down (UD) rotation, as in \fig\ref{fig: res_move}. 
Despite the motion artifacts, \texttt{CardioNet} maintains robust performance, achieving an MAE of 1.35 BPM in the UD scenario, and consistently outperforms baselines by significant margins in all motion types. Our model benefits from the unique design of the motion-aware aggregation and temporal differentiation block.
These prove the robustness of our model against body motions by effectively employing video plus audio modalities.


\head{Video-only Solutions} We evaluate our approach on open datasets that contain only video data. As shown in \fig\ref{fig: res_pure}, our method consistently ranks among the top among rPPG-based solutions. We achieve MAE errors of 2.09 and 1.12 BPM on PURE and MMPD datasets, respectively.
It is important to note that during evaluation, we disable the audio branch of \texttt{CardioNet}. This ensures that our video encoder independently captures heart-related activities. 
In scenarios where no audio is available (e.g., during silent periods), our model effectively transitions into a video-only solution. 


\subsection{Micro Benchmarks}

\head{Different Light Conditions}
We assess our model under varying lightness levels from 0.3702 to 0.3259 in Fig.\ref{subfig:light}, by adjusting the ring light. As lightness decreases, the MAE increases from 4.85 BPM to 8.16 BPM. This trend suggests that poorer conditions impact accuracy due to the reduced visibility of facial features. However, the model remains sufficiently robust, indicating that while lighting plays a role, the audio-visual fusion helps mitigate the negative effects.

\begin{figure}[t]
    \centering
    % Row 1
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pdf/videoonly_legend.pdf}
    \end{minipage}
    
    \begin{minipage}{\linewidth}
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/Walk_mae.pdf}
            \caption{Walk}
            \label{subfig:move_walk}
        \end{subfigure}\hfill
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/LR-Rotate_mae.pdf}
            \caption{Left-Right}
            \label{subfig:move_lr}
        \end{subfigure}\hfill
        \begin{subfigure}{.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/UD-Rotate_mae.pdf}
            \caption{Up-Down}
            \label{subfig:move_ud}
        \end{subfigure}
    \end{minipage}
    \caption{The performances of different body motions.}
    \label{fig: res_move}
\end{figure}
\begin{figure}[t]
    \centering
    % Row 1

    \begin{minipage}{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pdf/videoonly_legend.pdf}
    \end{minipage}

    \begin{minipage}{0.8\linewidth}
        \begin{subfigure}{.51\linewidth}
            \includegraphics[width=\linewidth]{pdf/PURE_mae.pdf}
            \caption{PURE}
            \label{subfig:res_pure}
        \end{subfigure}\hfill
        \begin{subfigure}{.49\linewidth}
            \includegraphics[width=\linewidth]{pdf/MMPD_mae.pdf}
            \caption{MMPD}
            \label{subfig:res_mmpd}
        \end{subfigure}\hfill
    \end{minipage}
    \caption{The performances on public datasets.}
    \label{fig: res_pure}
\end{figure}




\head{Different FPS}
We examine the model across various video frame rates (FPS), ranging from 30 FPS to 15 FPS, as shown in Fig.\ref{subfig:FPS}.  We interpolate the frame rate by adopting the principles discussed in \S\ref{subsec: service}. The model performs best at 30 FPS with an MAE of 1.75 BPM. Even at lower frame rates, particularly 15 FPS, the MAE increases to 4.56 BPM, while still remaining in the low level.  This robust performance is achieved through our frame interpolation scheme and the audio branch's ability to provide continuous cardiac information regardless of video frame rate. Also, our temporal differential block and irregular sampled time embedding block are equally vital to handle varying frame rates.


\head{Different Quality of Image}
We analyze the model's performance under different video compression qualities, from 100 (highest quality) to 40 (lowest quality), as shown in Fig.\ref{subfig:qoi}. 
Interestingly, the MAE does not consistently worsen with lower quality. At extreme compression levels, the model achieves the lowest MAE of 2.49 BPM, potentially due to smoothing effects that enhance key facial features. 
This suggests that while high compression degrades visual information, moderate to high levels of compression might benefit the model by reducing noise.

\begin{figure}[t]
    \centering
    % Row 1
    \begin{minipage}{0.8\linewidth}
        \begin{subfigure}{0.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/light_mae.pdf}
            \caption{Lightness}
            \label{subfig:light}
        \end{subfigure}\hfill
        \begin{subfigure}{0.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/FPS_mae.pdf}
            \caption{FPS}
            \label{subfig:FPS}
        \end{subfigure}\hfill
        \begin{subfigure}{0.32\linewidth}
            \includegraphics[width=\linewidth]{pdf/qoi_mae.pdf}
            \caption{Quality of Image}
            \label{subfig:qoi}
        \end{subfigure}
        %\vspace{-0.2cm}
        \caption{The performances under different light, FPS and image conditions.}
        \label{fig: res_light-FPS-qoi}
    \end{minipage}\hfill
\end{figure}
\begin{figure}[t]
\centering
    \begin{minipage}{0.8\linewidth}
        \begin{subfigure}{0.42\linewidth}
            \includegraphics[width=\linewidth]{pdf/env_mae.pdf}
            \caption{Environment}
            \label{subfig:env}
        \end{subfigure}\hfill
        \begin{subfigure}{.57\linewidth}
            \includegraphics[width=\linewidth]{pdf/fb_mae.pdf}
            \caption{Face Beauty}
            \label{subfig:fb}
        \end{subfigure}\hfill
    \end{minipage}
    %\vspace{-0.2cm}
    \caption{The performances under different environments and face beauty filters.}
    \label{fig: res_fb-env}
\end{figure}
\head{Different Environments}
Our model's performance is evaluated across various environmental settings, including Office, Outdoor, Conference Room, and Laboratory, as shown in Fig.\ref{subfig:env}. 
The model performs best in the Office environment with a MAE of 1.40 BPM. Notably, the latter three environments are not in the training set, yet the model maintains strong performance, demonstrating that our feature extraction generalizes well to unseen conditions.

\head{Different Face Filters}
We test various facial filters, including Smooth Face, Tint Skin, Adjust Brightness, Add Contrast, and Sharpen Face, as shown in Fig.\ref{subfig:fb}. 
The Tint Skin filter yields the best performance with an MAE of 2.38 BPM, while a more aggressive filter like Sharpen Face achieves an MAE of 8.69 BPM. It shows our model effectively handles appearance changes while maintaining accuracy.

\head{Different Devices}
We evaluate our model on various devices under inter-device and cross-device conditions, as shown in Fig.\ref{subfig:device}.
For inter-device testing, the average MAE is approximately 2.95 BPM. In cross-device scenarios, the average MAE is around 8.07 BPM. While there is a drop in accuracy, the model still delivers acceptable performance across different hardware platforms. 
This suggests that despite some variability, the model remains robust and capable of providing reliable heart rate estimates on a wide range of devices. 

\head{Different Users}
We evaluate our model's performance across a diverse set of users in Fig.\ref{subfig:people}. Our model's user generalization capability stems from learning universal cardiac patterns rather than user-specific features. The temporal-spectral modeling captures fundamental physiological characteristics that are consistent across individuals. Under inter-user conditions, the average MAE is about 1.93 BPM. In cross-user scenarios, the model still performs reasonably well, with an average MAE of 7.53 BPM. Despite the diversity, the model maintains a usable level of accuracy, underscoring its generalizability across different user groups. This demonstrates that our feature extraction pipeline effectively captures device-independent cardiac patterns.






\head{Multi-person Scenarios} We evaluate the multi-person scenarios to justify the effectiveness of our preprocessing. 
We set the maximum number of people to be separated as two and crop the face region to a size of 72$\times$72 pixels. 
In our test, two users read materials simultaneously while sitting next to each other. We apply the facial and sound separation and match their audio and face regions. The test results show an MAE of 7.83 BPM and 8.13 BPM for each person, respectively. 
Although we observe some performance drops, our method still effectively distinguishes between the two individuals. Notably, the heart rates of the two people vary over time, with average heart rates of 76.17 BPM and 68.55 BPM, respectively ,showing our system can track distinct physiological states simultaneously.

\begin{figure}[t]
    \centering
    % Row 1
    \begin{minipage}{0.5\linewidth}
         \includegraphics[width=\linewidth]{pdf/device_mae.pdf}
         %\vspace{-0.5cm}
        \caption{Different Devices}
        \label{subfig:device}
    \end{minipage}\hfill
    \begin{minipage}{0.49\linewidth}
        \includegraphics[width=\linewidth]{pdf/people_mae.pdf}
        %\vspace{-0.6cm}
        \caption{Different Users}
        \label{subfig:people}
    \end{minipage}\hfill
\end{figure}


\subsection{\sysname in the wild}
In this section, we will evaluate how \sysname works as a service. We assess the service on two ends: the meeting platform and the online content providers. 



\begin{figure}[t]
    \centering

    \begin{minipage}{0.8\linewidth}
        \begin{subfigure}{0.49\linewidth}
            \includegraphics[width=\linewidth]{pdf/latency-throughput_chrome.pdf}
            \caption{Chrome}
            \label{subfig:inference_chrome}
        \end{subfigure}\hfill
        \begin{subfigure}{0.49\linewidth}
            \includegraphics[width=\linewidth]{pdf/latency-throughput_zoom.pdf} % change
            \caption{Zoom}
            \label{subfig: inference_zoom}
        \end{subfigure}
    \end{minipage}
    %\vspace{-0.4cm}
    \caption{Latency \& throughput for Zoom and Chrome}
    \label{fig: res_througput}
\end{figure}
\head{Meeting Platforms}
We choose Zoom as one of online meeting platforms, which provides the external developers the SDK to acquire access to the raw data. The average FPS is 28.4. We exploit the data hooks to acquire the streams and leverage buffer queues to hold the packets, as described in \S\ref{subsec: buffer}. The model consumes on average in 850ms on CPU. We choose a step size of 1s, and a window size of 4s. It means every second, we feed the 4-s windows for inference. The overall system latency averages 1.03 seconds, as depicted in \fig\ref{subfig: inference_zoom}. Notably, latency was primarily elevated at the start due to the initial model warm-up period \cite{lion2016don}. This means our systems can run inference in real-time.
Furthermore, we calculate the throughput of the whole system. We measure the time since the last update of heart rate. As we are feeding 4-s window of video and audio frames, the throughput is calculated as the volume of video and audio data processed per update period. As in \fig\ref{subfig: inference_zoom}, the average throughput of the system is 115.97 FPS, which is prominently larger than the common video FPS. It means that our systems can hold the service robustly without any freezes.

\head{Online Content Providers}
Online content providers such as YouTube often host their services in the web browser. We implement such a service in a Chrome extension. We employ the data hook developed from WebGL and WebAudio to acquire the streams. The average FPS is 26.97. The overall latency of our service is 1.23s, comparable to our step size 1s, as can be observed from \fig\ref{subfig:inference_chrome}. Meanwhile, the average throughput is 98.16 FPS, with a maximum throughput of 114.41 FPS. These results also justify our service will run smoothly in the extensions.



% \subsection{Possible Application}

% \com{Check my messages in the Chat. Modify the figures and then we should be good with length.}
% %\vspace{-6mm}
\section{Related Work}
In this section, we will summarize the existing works.

\head{Cardiac Monitoring}
Cardiac information is crucial for health monitoring, affective computing \cite{yang2022survey, fairclough2020personal} and deception analysis \cite{bian2024ubihr}. 
Traditional approaches in hospitals, \eg, electrocardiograms (ECGs) and CT scans \cite{HeartDiseaseSymptoms}, provide the most accurate data but require professional operation and are prohibitively expensive and cumbersome for everyday use. Recent advancements have focused on more portable solutions.
Earable-based systems \cite{cao2023heartprint, chen2024exploring, fan2023apg} allow earpieces to detect cardiac information, but they either need specific probing signals or custom hardware, limiting their widespread adoption. Similarly, wearable solutions necessitate constant wear, which is not practical for all users. Wireless technologies, including Wi-Fi \cite{liu2015tracking}, mmWave \cite{yang2016monitoring}, and UWB \cite{chen2021movi}, \etc, are constrained by specific hardware which is not commonly available in video systems . 
Solutions using active acoustic sensing \cite{wang2023df, wang2022loear, qian2018acousticcardiogram, zhang2020your} with smart speakers rely on pseudo-inaudible signals, which can be intrusive to human hearing and increase hardware burden.
Video-based solutions use optical means to measure blood volume changes in tissues. Signal processing \cite{de2013robust, li2014remote, wang2016algorithmic, wang2015novel} and deep learning \cite{chen2018deepphys, liu2020multi, niu2020video, li2023learning, yu2019remote,yu2019remoteCompress, yu2023physformer++, liu2023efficientphys, zou2024rhythmformer} techniques have been developed to enhance these methods. Yet these solutions are sensitive to low light conditions, head/body movements, and typically perform poorly outside controlled environments. VocalHR \cite{xu2022hearing} proves the potential of extracting heart rate from human speech. Although it leverages human speech effectively, it is limited by range, requires pre-calibration, and cannot distinguish multiple individuals. 
Differently, \sysname is the first to combine the complementary and naturally co-existing audio and video modalities in online video streaming systems. Our video design incorporates temporal-frequency co-design and motion-aware aggregations for the first time in OCM to mitigate the light and body movement influence. The audio module employs the temporal acoustic filter for OCM. These designs are innovative and contribute to our performances.

\head{Video Streaming System} Video streaming systems have gained immense popularity due to their vast libraries of on-demand content, user-generated videos, and live streaming capabilities, catering to diverse viewer preferences, including YouTube, TikTok, Zoom, \etc. They can be further categorized into VoD systems, live streaming systems and video conferencing systems. Research efforts have been devoted to communication protocols \cite{hamadanian2023ekho, dhawaskar2023converge}, adaptive rate streaming algorithms  \cite{li2023dashlet, wen2023adaptivenet, zhou2019learning}, online learning \cite{tang2023lut, guan2023metastream, khani2023recl, yi2023boosting}, \etc. None of these works explore adding cardiac monitoring into modern video streaming systems. 
In contrast, \sysname stands out as the first work that creates a middleware service of OCM that can be seamlessly integrated into mainstream video streaming systems. 


\section{Discussion and Future Work}

\head{Audio-Video Pair} In our primary application scenarios (\eg, live streaming, online meetings, \etc), audio and video naturally coexist. 
In practice, only video data is available in some situations, where \sysname can be easily adapted to a video-only solution. Such periods can be detected through mature voice activity detection techniques \cite{wiseman2016python}. 
Our results shown in \fig\ref{fig: res_pure} have demonstrated that \sysname also performs well in video-only scenarios. 
\sysname not only introduces a novel approach to OCM by utilizing audio-visual pairs for the first time, but also integrates these capabilities into a practical system with flexibility and robustness.

\head{Impacts on Original Streams} Integrating additional services into standard streaming platforms has been a bottleneck for many previous solutions \cite{liu2020grad, mmdetection, du2020server}. In \sysname, we address this challenge with a dedicated design of data hook and middleware service. 
Our approach ensures that these additional services are isolated from the original streams.
With offscreen canvas, which operates independently in the extension, we avoid disrupting the original content. In meetings, our data hook duplicates data to the inference engine instantly, seamlessly, and without affecting the main video and audio streams. Our evaluations demonstrate that \sysname operates without causing any disruptions or interference to ongoing streams.

\head{Equality and Accessibility} \sysname is designed for equality and is devised to be flexible and adaptable, allowing it to be integrated into any platform without the need for specialized hardware. This significantly increases accessibility, making the technology available to a wider audience. Moreover, while companies can promote this service on cloud platforms, \sysname is crafted to ensure democratized access, preventing any hidden biases or preferential treatment. By enabling audiences to independently initiate the service, \sysname reduces the likelihood of companies manipulating the system for economic gains by altering the model.

\head{Use of Deep Learning} The relationship between video-audio information and cardiac activity is inherently implicit and complex. We evaluate our results against signal processing approaches in \fig\ref{fig: res_dis} and \fig\ref{fig: res_angle}, where our performances are significantly better. And our system evaluation validates real-time monitoring without introducing large latency. We identify the exploration of combining signal processing with increased explainability as a direction for future work.


% %\vspace{-3mm}
\section{Conclusion}
In this paper, we envision the attractiveness of Online Cardiac Monitoring (OCM) in video streaming 
and present \sysname, the first-of-its-kind system to fuse both audio and video streams for online cardiac monitoring in video streaming systems. We devise an effective audio-visual network that can robustly and accurately unveil the nuanced cardiac activities, achieving an average MAE of 1.79 BPM and outperforming the video-only and audio-only solutions by 69.2\% and 81.2\%, respectively. Furthermore, we design and implement \sysname as a plug-and-play microservice that can seamlessly be integrated into mainstream video streaming systems. We believe our work will significantly enhance the entertainment and healthcare value of video streaming and inspire a new direction in this field.
