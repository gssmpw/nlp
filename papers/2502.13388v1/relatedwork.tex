\section{Related Works}
\subsection{StarCraft-II Environment}
Prior to the application of AI to StarCraft II, AI research in RTS games had been going on for years. Early attempts focused on using rules and finite state machines to create simple game agents. These agents are often inflexible and adaptable, unable to handle the complex strategies of human players. 

In 2017, DeepMind and Blizzard Entertainment collaborated to release SC2L
E\cite{SC2LE}. It is a standardized reinforcement learning environment designed to facilitate AI research. SC2LE provides apis that allow researchers to control units and buildings in the game and access game state information. SC2LE provides a unified test bed for researchers, greatly advancing StarCraft II AI research. 

In 2019, DeepMind released AlphaStar, a landmark AI system that uses Multi-Agent Reinforcement Learning to compete against top human players in StarCraft II. In multi-agent reinforcement learning, there are also many methods used in StarCraft II environments, such as VDN\cite{VDN}, QMIX\cite{QMIX}, WQMIX\cite{WQMIX}, MAPPO\cite{MAPPO}, MADDPG\cite{MADDPG}. In recent years, there have been other studies of the StarCraft II environment through other methods, such as RLforSC\cite{RLforSC}.

However, existing methods cannot improve the interpretability of AI decision-making processes or help researchers understand and improve AI strategies.


\subsection{Large Language Model}
LLMS typically have complex architectures with billions or even hundreds of billions of parameters that are trained with vast amounts of data and computational resources. In June 2018, GPT-1 was the first generative pre-trained converter model released by OpenAI. It uses the converter architecture for unsupervised pre-training on large amounts of text data and then optimization on specific tasks through supervised fine-tuning. In November 2022, ChatGPT was released as a chat application. 

In recent years, there have also been a variety of large models in different directions, such as Claude-2\cite{Claude-2}, LLAMA-3\cite{LLAMA-3}, PaLM-2\cite{PaLM-2} and so on. Claude is a large-scale language model introduced by Anthropic that focuses on security and controllability to reduce harmful information in generated content. LLaMA-3 is a large-scale language model published by Meta to advance AI research and provide an open research platform. 

The development of large language models has also facilitated the study of StarCraft II environments, such as SwarmBrain\cite{SwarmBrain}, but these studies have not solved the problem of reflection and strategy iteration in complex environments with LLMs.


\subsection{LLM and reflection}
In order to make the large model continuously improve itself in the process of communication, the method of combining the large model and reflection is constantly proposed. A new framework, reflexition\cite{reflextion}, was proposed in 2023, which instead of updating weights, strengthens language agents through language feedback, allowing large models to learn quickly and efficiently from trial and error.In this approach, the focus is on action-level reflection, allowing the large model to store buffers in each chat, thereby inducing better decisions in subsequent exchanges.
In addition to action-level reflection, Agent-Pro\cite{Agent-Pro} methods are proposed to form strategy-level reflection and optimization, fine-tuning its overall strategy by reflecting on past trajectives and decisions.

However, these reflection methods do not take into account the adaptability of frameworks in complex environments such as StarCraft II. Therefore, we propose Reflection of Episodes to realize strategy-level reflection in complex environment.

%
%
%
%
\begin{figure}[htbp]
\centerline{\includegraphics[width=360pt]{figs/structure.pdf}}
\caption{\textbf{Reflection of Episodes Framework.} The framework consists of Text StarCraft 2 environment and reflection structure. After an episode, reflection structure generate new prompt and update it to the next game.} \label{fig2}
\end{figure}