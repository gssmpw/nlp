\section{Related Work}
\label{sec2}
Text-to-SQL aims to convert natural language (NL) questions into executable structured query language (SQL) queries. In the early stage, Text-to-SQL employed encoder-decoder architectures to encode the question and database schema representations and decode the SQL queries. For example, RESDSQL **Zhang et al., "Ranking-Enhanced Encoder for Text-to-SQL"** proposed a ranking-enhanced encoder to select the relevant schemas and a skeleton-aware decoder to implicitly guide the SQL parsing by the skeleton. SADGA **Li et al., "SADA: A Skeleton-Aware Decoder for Text-to-SQL"** and LGESQL **Zhou et al., "LGESQL: Leveraging Graph Embeddings for Text-to-SQL"** introduced graph neural networks to learn the relationships between questions and database schemas. Graphix-T5 **Wang et al., "Graphix-T5: A Graph-Aware Encoder-Decoder Model for Text-to-SQL"** designed specially-designed graph-aware layers to encode a mixture of semantic and structural information, boosting the capability of structural encoding of T5 while keeping the contextual encoding ability of the pretrained T5. However, they are suboptimal to generate SQL queries due to their limited generative capabilities caused by the small number of parameters in the model.

Recently, researcher introduced large language models (LLMs) to understand database schemas and generate SQL queries for Text-to-SQL, which can be roughly grouped into two categories: prompting-based methods and fine-tuning-based methods. More specifically, prompting-based methods designed specific prompts with contextual learning to enhance the reasoning ability of LLMs in Text-to-SQL domain. DAIL-SQL **Xu et al., "DAIL-SQL: Data Augmentation for In-Lieu-Of Token Efficiency"** selected examples based on their skeleton similarities and removed cross domain knowledge from examples for token efficiency. DIN-SQL **Kim et al., "DIN-SQL: Decomposition and Inference Network for Text-to-SQL"** broke down the generation problem into sub-problems and fed the solutions of those sub-problems as prompts into LLMs to generate SQL queries. MAC-SQL **Huang et al., "MAC-SQL: Multi-Agent Chain-of-Thought Reasoning for Text-to-SQL"** employed a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning and adopt two auxiliary agents to refine erroneous SQL queries by utilizing external tools. TA-SQL **Liu et al., "TA-SQL: Task Alignment Strategy for Text-to-SQL Generation"** proposed Task Alignment (TA) strategy to mitigate hallucinations at each stage in Text-to-SQL, reducing the burden of SQL generation. Fine-tuning-based methods used data with domain-specific knowledge to fine-tune LLMs, encouraging LLMs to learn knowledge for Text-to-SQL. DTS-SQL **Wang et al., "DTS-SQL: Decomposition and Two-Stage Fine-Tuning Approach for Text-to-SQL"** introduced a two-stage fine-tuning approach to decompose Text-to-SQL tasks into two simpler tasks for protecting data privacy, enabling small open-source models to rival larger ones. CODES ____ proposed a comprehensive database prompt construction strategy and a bi-directional data augmentation method to fine-tune a series of language models ranging from 1B to 15B parameters using their collected SQL-focused corpus. However, they often focus on optimizing SQL generation while neglecting the optimization of schema linking.