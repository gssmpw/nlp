\section{Related Work}
\label{sec2}
Text-to-SQL aims to convert natural language (NL) questions into executable structured query language (SQL) queries. In the early stage, Text-to-SQL employed encoder-decoder architectures to encode the question and database schema representations and decode the SQL queries. For example, RESDSQL \cite{ref2} proposed a ranking-enhanced encoder to select the relevant schemas and a skeleton-aware decoder to implicitly guide the SQL parsing by the skeleton. SADGA \cite{ref3} and LGESQL \cite{ref4} introduced graph neural networks to learn the relationships between questions and database schemas. Graphix-T5 \cite{ref5} designed specially-designed graph-aware layers to encode a mixture of semantic and structural information, boosting the capability of structural encoding of T5 while keeping the contextual encoding ability of the pretrained T5. However, they are suboptimal to generate SQL queries due to their limited generative capabilities caused by the small number of parameters in the model.

Recently, researcher introduced large language models (LLMs) to understand database schemas and generate SQL queries for Text-to-SQL, which can be roughly grouped into two categories: prompting-based methods and fine-tuning-based methods. More specifically, prompting-based methods designed specific prompts with contextual learning to enhance the reasoning ability of LLMs in Text-to-SQL domain. DAIL-SQL \cite{ref7} selected examples based on their skeleton similarities and removed cross domain knowledge from examples for token efficiency. DIN-SQL \cite{ref8} broke down the generation problem into sub-problems and fed the solutions of those sub-problems as prompts into LLMs to generate SQL queries. MAC-SQL \cite{ref9} employed a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning and adopt two auxiliary agents to refine erroneous SQL queries by utilizing external tools. TA-SQL \cite{ref10} proposed Task Alignment (TA) strategy to mitigate hallucinations at each stage in Text-to-SQL, reducing the burden of SQL generation. Fine-tuning-based methods used data with domain-specific knowledge to fine-tune LLMs, encouraging LLMs to learn knowledge for Text-to-SQL. DTS-SQL \cite{ref11} introduced a two-stage fine-tuning approach to decompose Text-to-SQL tasks into two simpler tasks for protecting data privacy, enabling small open-source models to rival larger ones. CODES \cite{ref12} proposed a comprehensive database prompt construction strategy and a bi-directional data augmentation method to fine-tune a series of language models ranging from 1B to 15B parameters using their collected SQL-focused corpus. However, they often focus on optimizing SQL generation while neglecting the optimization of schema linking.