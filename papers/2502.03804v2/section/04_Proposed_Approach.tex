% 1. how many questions did we specify and how? Now it said appropriate number of questions, but it's unclear what is "appropriate"
% 2. How did the question generated? based on what criteria? for instance, did we ask gpt to generate random questions? if not, what exact did gpt do? would be better to show one example of prompt in this part.

\section{Proposed LLM-Powered QA-Based Approach: ResQ}
\label{sec:Proposed_Approach}
% 本セクションでは、電子メール対応タスクをサポートするために提案されたアプローチ「ResQ」について説明する
This section describes the proposed approach, ResQ, for supporting email response tasks. 
\input{figures_tables/fig_overview_of_process}
\input{figures_tables/fig_interface}
Fig.~\ref{fig_overview_of_process} illustrates the overview of how a reply message is created using ResQ.
% (A) After the system detects users' initiation of the reply task, it generates multiple-choice questions using an LLM (in this study, GPT-4o~\cite{GPT4o}). 
% (B) Then, users communicate their reply strategy to the AI by responding to these questions.
% (C) When the system detects that users have pressed the ``Generate Reply'' button, it presents a draft of the reply to users.
% (D) Finally, users review and revise the reply draft generated by the LLM and send the response.
Fig.~\ref{fig_interface} shows the actual interface of ResQ.
The following sections describe the specific functions involved in each step of this process.

\paragraph{\textbf{A: Generate Questions}}
\label{sec:generate_questions}
% ResQは返信の必要性を検知すると、大規模言語モデル (本研究ではGPT-4o) を使用して、多肢選択式の質問を生成する
% また我々は、ユーザが質問をクリックすると、その質問が受信メールのどの部分に対応しているかがハイライトされるようにした
% 我々はこれらの機能を実現するために、まずモデルに対して、モデルの役割（ユーザに対する質問と有益そうな選択肢のペアを複数生成すること）と、作成する質問の目的（メールに含まれる全ての要求を抽出し、送信者がそれぞれに対してどのように返答したいかを明確にすること）をプロンプトとして与えた
% さらに、モデルに受信メールの文章、タイトル、送信者の情報（名前、メールアドレス）、受信メールの過去のやり取りの文章、ユーザの情報（名前、メールアドレス）を提供した
When a user first activates ResQ, the system uses an LLM (in this study, GPT-4o~\cite{GPT4o}) to generate multiple-choice questions (Fig.~\ref{fig_interface}-C). 
The LLM extracts all parts of the email that require a reply, generates corresponding questions and presents possible response options.
Additionally, if a user clicks on any generated question, the relevant part of the email is highlighted (Fig.~\ref{fig_interface}-A).

% To implement these features, we first provided the LLM with the email's text, subject, sender information, text from past email interactions, and the user's information (name and email address).
% プロンプトは~\cite{relatedwork}を参考に作成し、文脈を踏まえており、返信を作成する上で役に立ち、適切な数の（メールのすべての要件に対しては漏れなく）質問と、それに役立つ選択肢を生成するように指示した。
% またプロンプトには質問と選択肢の生成例を含めることで、生成の質を上げた。
\red{Following the approach described in~\cite{bsharat2023principled}, we designed a structured prompt that guides the LLM to determine how many questions are necessary and sufficient to cover all requirements in the incoming email without omission. 
Instead of pre-specifying a fixed number of questions, the prompt instructs the model to produce an ``appropriate'' number of questions, where ``appropriate'' is defined as the minimal set of questions needed to address all points raised by the sender while avoiding redundant or irrelevant inquiries.
To ensure that the questions were generated systematically rather than randomly, we provided explicit criteria within the prompt. 
These criteria included referencing the sender's intent, quoting relevant portions of the original email verbatim, and offering multiple-choice options where applicable. 
We also provided concrete examples within the prompt to illustrate the desired format and style of the generated questions and corresponding answer choices. 
By doing so, we ensured that the LLM's output was both well-grounded and easy for the recipient to answer.
The detailed prompt used to guide the LLM in this process is shown in the appendix.}

% We designed the prompt with reference to~\cite{bsharat2023principled}, incorporating contextual considerations to ensure it effectively supports reply composition. 
% We instructed the LLM to generate an appropriate number of questions that comprehensively cover all the email's requirements without omissions, along with relevant response options. 
% To further guide the model and improve output accuracy, we included examples of question and option generation within the prompt.

\paragraph{\textbf{B: Answer Questions}}
% 次にユーザは受信メールと生成された質問、選択肢を同時に見ながら、質問に回答する
% 我々は有益な選択肢がない場合を想定して、ユーザ自身が選択肢を追加できるようにした
% また、LLMにメールのcontextを伝えることができるように、送信者と受信者の関係性を記入できるboxを設置した
% さらに以前の研究に基づき~\cite{fu2024text}、ユーザがAIメールの文章をカスタマイズできるように、ユーザが期待する返信のトーンやスタイル、長さを調整するための選択肢を提供した
% また、ユーザがそれ以外のリクエストをAIに対してできるように、AIに対する自由記述欄を設置した
% ユーザはこれらの作業が完了するとGenerate Replyボタンを押す
Next, users view the incoming email (Fig.~\ref{fig_interface}-B) alongside the generated questions (Fig.~\ref{fig_interface}-C) and options and proceed to answer them. 
In anticipation of situations where none of the provided options are useful, we enabled users to add their own options (Fig.~\ref{fig_interface}-D). 
Additionally, to help the LLM better understand the context of the email, we introduced a box where users can specify the relationship between the sender and the recipient (Fig.~\ref{fig_interface}-E, top). 
Furthermore, following previous research~\cite{fu2024text}, we provided users with controls to adjust the tone, style, and length of the reply to match their preferences better, thereby giving them more flexibility in customizing the AI-generated response (Fig.~\ref{fig_interface}-E, middle). 
A free-text field was also included to allow users to make other specific AI requests (Fig.~\ref{fig_interface}-E, bottom). 
After completing these steps, users can click the ``Generate Reply'' button (Fig.~\ref{fig_interface}-F).

\paragraph{\textbf{C: Generate Reply Draft}}
% ResQはユーザが"Generate Reply"ボタンを押したことを検知すると、大規模言語モデルを使用して、返信のドラフトを作成する
% ユーザの期待するような返信のドラフトが出力されるように、我々はモデルに対して、受信メールとその関連情報、AIの質問とそれに対応するユーザの回答、ユーザが返信案に期待する他の要素（トーン、スタイル、長さ、その他の要望）、ユーザの情報を提供した
When the user clicks the ``Generate Reply'' button, ResQ detects the action and uses the LLM to generate a reply draft.
% To ensure that the draft aligns with users' expectations, we first provided the LLM with the information provided when Sec.~\ref{sec:generate_questions}, the generated questions, corresponding users' answers, and users' preferences (\textit{e.g.}, tone, style, length, and any additional requests).
% Then, the LLM generates a draft of the reply.
The prompt used for this function is shown in the appendix.

\paragraph{\textbf{D: Review Reply Draft}}
Once the draft reply is generated, users can review the draft in detail (Fig.~\ref{fig_interface}-G).
Moreover, if users find that extensive revisions are needed or if they want to explore alternative phrasing, they have the option to request the AI to regenerate a new draft based on updated input or preferences.
After completing these steps, users can click the ``Reply'' button (Fig.~\ref{fig_interface}-H).