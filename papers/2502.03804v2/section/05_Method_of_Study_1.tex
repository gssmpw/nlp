\section{Method of Study 1}
\red{To test our hypotheses and answer three research questions, we first conducted a controlled experiment, focusing on gaining a quantitative understanding.}
% original
% We conducted a controlled experiment (Study 1) and a field study (Study 2) to test our hypotheses and answer three research questions. 
% Study 1 was conducted in a controlled environment, focusing on gaining quantitative understanding. 
% To further examine the actual usage of our QA-based system, we conducted a field experiment for Study 2 to gain a qualitative understanding of how our QA-based method influenced the practice of email replies. 

\subsection{Experiment Design}
Study 1 was designed to quantitatively assess how ResQ influences the writing process (RQ1), the quality of email replies (RQ2), and the perceived relationships with others (RQ3) compared to scenarios without AI intervention and when using traditional AIMC tools. 
% 実験は日本人を対象に、日本語でやったことを明記する
\red{The experiment targeted Japanese participants and was conducted entirely in Japanese.}
% to ensure the tasks reflected natural communication in formal settings such as workplaces and schools.
This experimental context was designed as communication in formal settings, such as \red{office-related communication, research collaboration in academic institutions, and interactions with external organizations}. 
\red{It focused on time-consuming emails that were characterized as lengthy, containing multiple requests, or requiring detailed and polite responses. 
Simple and straightforward emails, such as those that can be answered with a single word or phrase (\textit{e.g.}, ``Understood''), were excluded from the scope.}

Participants were assigned the role of message recipients and required to craft replies based on the scenarios and supplementary information provided. 
\red{The messages used in the experiment were collected from 10 volunteers who provided real emails they had received in formal communication contexts.}
\red{These volunteers included office workers, graduate students, and teaching staff, all of whom were Japanese and engaged in email communication regularly (at least once per month).}
To ensure anonymity, identifying details were removed during the preparation process. 
\red{Based on the design principles of ResQ, we excluded extremely short emails and emails that could be replied to with a single word from the selection process.}
Each scenario included details about the sender, the recipient, and the context in which the message was received. 
Multiple scenarios were included in the experiment to minimize the influence of any single scenario and increase the variety. 
Additionally, supplementary information, such as the recipient’s schedule and potential questions, was provided to prevent excessive variability in the responses among participants. 

In total, we created twenty types of \red{emails}, with two assigned to the practice session and eighteen to the test session.
The length of the \red{emails} used in the test session averaged 404 \red{Japanese} characters, with the shortest being 135 characters and the longest being 925 characters.
\red{The scenarios covered a wide range of formal communication situations, including responding to a request for data submission in the workplace, answering a survey from a professor, asking questions based on guidance from a language school’s customer support team, and addressing a request for schedule adjustments as a part-time worker.}
\red{Additionally, these emails varied in structure, ranging from structured formats with bullet points to more non-structured, free-text formats.}
\red{The specific emails and examples of ResQ-generated questions and options used in the experiment can be referred to in the supplementary materials.}
% \red{The specific emails used in the experiment, including full message content, sender information, scenarios, and supplementary information, are provided as supplementary materials.}

\subsection{Experimental Conditions}
We employed a within-subject design with three conditions: QA-based, Prompt-based, and No-AI.
\red{This design was chosen to control for individual differences among participants, such as varying levels of language proficiency or familiarity with AI systems, ensuring a fair comparison across conditions.}
To illustrate each condition, consider the scenario of a participant who, as an employee of a company, is asked by their superiors to assume the role of a fixed asset committee member.

% QA-based method.
In the QA-based condition, participants created replies using the QA-based AI.
The system detected when participants navigated to the next email screen, inferred that they were initiating a reply, and then generated relevant questions.
For example, the system might ask, ``Would you be willing to take on the role of the fixed asset manager?'' ``Is there any issue with handling the annual inventory check?'' or ``Please let us know if you have any questions or concerns about the tasks.''
Participants could respond by selecting from provided options (\textit{e.g.}, ``yes,'' ``no''), adding their own options, or ignoring the questions entirely. 
After responding, they would press the ``Generate Reply'' button, which would produce an AI-generated draft in the reply box. 
Participants could then regenerate or modify the draft as needed to finalize their response.

% Prompt-based condition
In the Prompt-based condition, participants created replies using a prompt-based AI without the QA feature of the QA-based method.
Participants wrote prompts for the AI to generate a draft email response, which they then edited to create their replies.
For example, a participant might input a prompt such as, ``I want to convey my acceptance of the fixed asset committee role. ...''
Afterward, similar to the QA-based system, participants would press the ``Generate Reply'' button and, if necessary, either regenerate the draft or revise its content.

% No-AI condition
In the No-AI condition, participants created email replies manually without using AI assistance.

\subsection{Participants}
\input{figures_tables/tab_study1_participants}
Twelve participants (six males and six females, aged 20-57) were recruited via a local Japanese participant recruiting platform (see Tab.~\ref{tab_study1_participants}).
The average age of the participants was 29.6 (SD = 11.0). 
\red{The sample size $n=12$ was determined based on an a priori power analysis (effect size $f=0.4$, significance level $p=0.05$, power = 0.8, correlation among repeated measures = 0.5) as well as the previous study~\cite{Mu2024Whispering}.}
The participants were paid approximately \$21 USD for participation, and the experiment lasted around two hours.
This study was approved by the institute's ethical review board.

\subsection{Procedure}
The participants first read the study instructions and the right to participate and then consented to participate in the experiment. 
Next, they were given an explanation of the purpose of the experiment and the use of the AI systems (Prompt-based and QA-based systems). 
% \red{To avoid the AI nocebo/placebo effect~\cite{}, the explanations were carefully designed to use neutral and standardized language, avoiding any statements that could imply one system was superior or inferior to another. 
% This ensured that participants’ perceptions of the systems were not biased before engaging in the tasks.}
% Subsequently, participants were randomly assigned to reply to six emails per condition based on the Latin square design.
\red{Participants were then randomly assigned to reply to six emails per condition using a Latin square design, which counterbalanced the order of conditions and mitigated potential order effects\blue{~\footnote{\blue{We conducted analyses to examine the potential order effect. The results of this analysis are provided in the appendix.}}}.}
In each condition, participants first engaged in a practice session where they read and replied to two emails \red{to familiarize themselves with the system.}
Then, they read and replied to six emails, which were presented in a randomly assigned order \red{to further reduce any sequence-related biases.}
After replying to six emails for each condition, participants were asked to complete a questionnaire regarding their experience with the task. 
% \blue{To ensure participants could manage their workload during the study, they were allowed to take break between conditions. }
\blue{To ensure participants could manage their workload during the study, they were allowed to take a short break after completing tasks in each condition. 
% Additionally, participants had the flexibility to take breaks at their discretion during non-task periods within each condition, such as after the practice session or before the questionnaire.
}
After completing all conditions, they were asked to fill out a comparative questionnaire evaluating the three conditions. 
\red{In addition, follow-up interviews were conducted to gather deeper insights into their experiences and preferences.}
This study was conducted remotely for all participants and lasted approximately two \red{and a half} hours in total.

\subsection{\red{Evaluation Session}}
After completing the main experiment, we conducted an additional evaluation session to assess the quality of the email responses created by participants and the impressions of participants as email senders.
This session involved a group of eighteen Japanese evaluators (ten males and eight females, aged 20-57) recruited via a local participant recruiting platform~\footnote{Participants were recruited from Lancers.jp, an online freelancing platform.}. 
The average age of the evaluators was 40.6 (SD = 8.3).
% evaluatorsは、メールを使用したコミュニケーションを最低5年以上、平均18.7年経験していた。
% また1人を除いて、evaluatorsは月に一回以上、メールを使用したコミュニケーションを行なっていた。
\blue{The evaluators had a minimum of five years and an average of 18.7 years of experience in email-based communication. 
Furthermore, with the exception of one individual, the evaluators engaged in email-based communication at least once a month.}
Each evaluator assessed email replies written by twelve different participants for a specific scenario. 
The evaluators were paid approximately \$2.5 USD for their participation, and the evaluation session lasted around fifteen minutes.

\subsection{Measurements}
% We used multiple measurements to test our hypotheses.
% 参加者が返信タスクに取り組んでいる際の行動から、Efficiency, Prompt Character Countsを算出した。
% また参加者の実験後のアンケートの回答から、Cognitive Load, Difficulty in Understanding Email Content, Satisfaction with Completing Task, Future Preference, Difficulty in Initiating the Action for Replying to Emails, Sense of Agency, Sense of Control, Psychological Distance between Participants and Their Counterpartを算出した。
% さらに、Evaluation Sessionにおけるevaluatorのアンケートの回答から、Perceived Quality of the Email, Impression of Participants as Email Sendersを算出した。
\red{We used multiple measurements to test our hypotheses. 
From participants' behavior during the email reply task, we calculated two measures: efficiency and prompt character count. 
From their post-experiment questionnaire responses, we evaluated cognitive load, difficulty in understanding email content, satisfaction with completing the task, difficulty in initiating the action for replying to emails, sense of agency, sense of control, and psychological distance between participants and their counterparts. 
Additionally, from evaluators’ questionnaire responses during the evaluation session, we assessed the perceived quality of the email and the impression of participants as email senders.}
\subsubsection{Efficiency of Replying to Emails (H1-a)}
We calculated the efficiency of replying to emails using task completion time and total character count.
The efficiency of replying to emails is defined as the amount of text contributing to the final output that can be typed per second, where a higher score indicates better task efficiency.
For task completion time, we recorded the time participants took to reply to an email, starting from when the email appeared on the screen to when the participant pressed the send button.
For total character count, we considered the text in the reply box when the participant pressed the Reply button as the final response and counted its characters.

\subsubsection{Prompt Character Counts (H1-a)}
We also calculated the average number of characters typed by participants to have the AI generate email drafts as the prompt character counts in each condition.
Under the Prompt-based condition, we measured the number of characters participants typed in the free-text field for the AI. 
Under the QA-based condition, the prompt character counts included this number plus any additional characters typed by the participants when they added their own options.

\subsubsection{Cognitive Load for Replying to Emails (H1-b)}
We used the NASA-TLX~\cite{hart1988development} questionnaire to measure cognitive load across six subscales: mental demand, physical demand, temporal demand, performance, effort, and frustration and calculated the Raw-TLX~\cite{byers1989traditional}.
Participants answered the above items using a 10-point Likert scale.
The Raw-TLX score is calculated as the simple average of six scales, where higher scores indicate a greater cognitive load.

% Additionally, to assess cognitive load specifically related to understanding received emails, we conducted a survey using a 7-point Likert scale, where 1 indicated strongly disagree, 4 indicated neutral, and 7 indicated strongly agree. 
% Participants were asked to rate the perceived load of understanding emails under all conditions.
\subsubsection{\red{Difficulty in Understanding Email Content (H1-b)}}
\red{Additionally, to assess cognitive load specifically related to understanding received emails, we used a 7-point Likert scale. 
Participants rated their agreement with the statement, ``I found it difficult to understand the sender’s intentions or requests in the email,’’ where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree.}
% Specifically, for all conditions, we asked about the perceived load of understanding the received emails.

% Q. I was able to understand the sender's intent and request easily.

\subsubsection{Satisfaction with Completing Task (H1-c)}
We evaluated participants' satisfaction with completing their task using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree.
Specifically, the satisfaction of completing their task was evaluated based on their satisfaction with efficiency and their satisfaction with the quality of the email they created.
We asked the following questions: 
(1) I felt that I was able to create a high-quality response. 
(2) I felt that I was able to complete the response efficiently.
We averaged the scores from two items and treated them as an index of the satisfaction with completing their task.
% The Cronbach's Alpha for the two items is $0.889$.

% \subsubsection{\red{Future Preference (H1-c)}}
% \red{We evaluated participants' preferences for future use across all conditions using a 7-point Likert scale.
% Participants rated their agreement with the statement, ``I would prefer to use this approach for replying to emails in the future,'' where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree.
% }

\subsubsection{Difficulty in Initiating the Action for Replying to Emails (H1-d)}
We tested H1-d using a survey with a 7-point Likert scale (1 = strongly disagree, 4 = neutral, and 7 = strongly agree) to evaluate perceived barriers to task initiation. 
Specifically, we asked the following question: I felt a high barrier to initiating email response tasks.

\subsubsection{Sense of Agency and Control (H1-e)}
We evaluated participants' perceived sense of agency and control using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree.
Specifically, drawing on previous research~\cite{Fu2023Comparing, Draxler2024The}, the sense of agency was evaluated by assessing whether participants felt they were the ones who wrote the responses, while the sense of control was evaluated by whether they felt they had control over the content of the responses. 

\subsubsection{Perceived Quality of the Email by Evaluators (H2)}
\label{sec:method2_H2}
The quality of each email reply was evaluated \red{by evaluators} using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree.
It was evaluated on three aspects: politeness (whether it was politely written), readability (whether it had an easy-to-understand structure), and meeting demands (whether it appropriately addressed the recipient's demands). 
We averaged the scores from three items and treated it as an index of the perceived quality of the email.
% The Cronbach's Alpha for the three items is $0.846$.

\subsubsection{\blue{Perceived Impression of Participants by Evaluators (H3-a)}}
% \red{To evaluate the impression of the email senders (participants) as perceived by others, we recruited the same group of eighteen Japanese evaluators described in Sec.~\ref{sec:method2_H2}.
\red{Following a previous study~\cite{rau2009effects}, we asked the evaluators to read the email and assess their impressions of the senders (participants) based on two aspects: whether the participants were perceived as likable and whether they were perceived as kind, using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree. 
We averaged the scores from these two items to create an index of the impression of the email sender.}

% The same eighteen evaluators were also asked to assess their impression of the email sender who were participants. 
% Following a previous study~\cite{rau2009effects}, we asked the eighteen evaluators to read the email and rate participants' impression toward the email senders (participants) in two aspects: whether they were perceived as likable or kind, using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree.
% We averaged the scores from two items and treated it as an index of the impression of the email sender.
% Cronbach’s $\alpha$ for this index was as follows: $0.955$ for the No-AI condition, $0.950$ for the Prompt-based condition, and $0.917$ for the QA-based condition.
% The Cronbach's Alpha for the two items is $0.946$.

\subsubsection{Psychological Distance between Participants and Their Counterpart (H3-b)}
\input{figures_tables/fig_study1_IOS}
We evaluated the perceived psychological distance using the Inclusion of Other in the Self (IOS) scale~\cite{aron1992inclusion}.
Participants choose a pair of circles from seven with different degrees of overlap (\red{see Fig.~\ref{fig_study1_IOS}}). 
1 = no overlap; 2 = little overlap; 3 = some overlap; 4 = equal overlap; 5 = strong overlap; 6 = very strong overlap; 7 = most overlap. 
The number chosen is the participants’ score.
The higher the score was, the closer participants felt they were with the email sender.