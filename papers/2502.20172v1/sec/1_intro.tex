\section{Introduction}
\label{sec:intro}

Recent years have witnessed remarkable advancements in text-to-image generation, primarily driven by powerful diffusion models~\citep{2020DDPM,ldm,2024SD3,flux}. While these models excel at generating images that align with simple text prompts, they struggle to handle more complex instructions that interweave graphical and textual elements. Although condition augmentation methods like IP-Adapter~\citep{ye2023ip-adapter} and ControlNet~\citep{controlnet} enhance text-to-image models with additional low-level control signals such as canny edges, depth maps, or reference images, they lack the flexibility to process complex and high-level text-image interleaved instructions, for example, merging visual elements from multiple images using natural language descriptions. This inability restricts more creative image generation processes where users might want to precisely orchestrate visual compositions by combining and manipulating elements from multiple sources with simple text instructions.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{images/comparisons.pdf}
% \vspace{-2em}
\caption{\textbf{Overview Comparison.} Among all types of works connecting LMM and diffusion model, our~\model~adopts the simplest design yet achieves the best performance.}
\label{fig:models architecture}
\end{figure}

Meanwhile, Large Multimodal Models (LMMs)~\citep{chen2024tokenpredictionmultimodalintelligence,liu2023llava,QwenVL,Qwen2vl} have shown remarkable progress in understanding visual content and natural language instructions, enabling various tasks such as image captioning, visual question answering, and visual grounding. This advancement raises an intriguing question: \textit{Can we take advantage of the advanced visual language understanding capabilities of LMMs to improve diffusion-based image generation models, enabling more flexible text-image interleaved control?}

Several recent works have explored integrating LMMs with diffusion models to enhance image generation control. As shown in Figure~\ref{fig:models architecture}, Emu-1 and 2~\citep{sun2023emu1, emu2} incorporate a specialized regression head on the hidden output states of LMM tokens following multimodal input processing. Seed-Tokenizer~\citep{seed-tokenizer} expands the LMM vocabulary with discrete vision tokens, which serve as condition for the diffusion model during image generation. BLIP-Diffusion~\citep{li2023blipdiffusionpretrainedsubjectrepresentation} employs a multimodal query-transformer encoder to extract subject representations, which are then combined with text prompts to guide the generation process.

However, many of these approaches merely add text-to-image generation capabilities to LMMs without improving generation quality~\citep{zhao2024bridging} or expanding potential applications. Some methods~\citep{li2023blipdiffusionpretrainedsubjectrepresentation} are designed for specific tasks and can only process a single conditioning image, limiting their utility in scenarios involving multiple image inputs. To the best of our knowledge, no existing models can effectively perform compositional image generation tasks, indicating a gap in understanding text-image interleaved control, particularly when multiple images are involved.

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{images/structure.pdf}
% \vspace{-2em}
\captionof{figure}{\textbf{\model~architecture.}}
\label{fig:dream_engine}
\end{figure*}


Our insight is that the fundamental challenge lies in effectively representing multimodal interleaved control, where mapping both text and images into a unified semantic space is crucial for coherent alignment. In this work, we demonstrate that Large Multimodal Models inherently provide a unified representation space, eliminating the need for additional architectural components such as regression heads or specialized tokens. We propose \model, an efficient and effective framework for image generation that accepts arbitrary text-image interleaved control signals. Building upon open-source text-to-image diffusion models like Stable Diffusion v3.5~\citep{2024SD3}, we replace its text encoders with a LMM along with a lightweight projector layer to encode the text-image interleaved controls. We introduce a two-stage training paradigm that efficiently aligns the representation spaces between these backbone models, enabling the generation of images guided by interleaved text and image instructions. We also design a new task called objects driven generation, which leverages object detection and image captioning data to enable compositional generation. 

Our experiments demonstrate the effectiveness of our architecture design and training recipe. By fine-tuning only an MLP layer on 20 millions data during Stage-I training, our model achieves an overall score of 0.69 on the GenEval benchmark, matching the performance of state-of-the-art text-to-image models such as SDv3.5 (0.71) and surpassing FLUX.1 Dev (0.66). This result highlights the efficacy of our alignment tuning method and demonstrates that powerful multimodal encoders can replace text encoders without compromising the original diffusion model's image generation quality. Furthermore, our Stage-II model exhibits strong text-image interleaved instruction following, significantly outperforming comparable models like Emu2-gen with substantially less training data. Notably, it can even synthesize concepts from different input images based on the text prompt to generate a cohesive output image as shown in Figure~\ref{fig:teaser}. Our contributions are threefold:

\begin{itemize}
    \item We found that Large Multi-Modal Models can be easily adapted into the text encoder of the text-to-image diffusion models even without updating the parameters.
    \item We achieve object-driven generation, combining object detection and captioning for compositional generation.
    \item Our method allows for complex, interwoven guidance from both text and images, resulting in highly customized outputs and state-of-the-art quality.
\end{itemize} 











