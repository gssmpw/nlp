\section{Related Work}
\label{sec:related}
\subsection{Image Generation with Complex Control}
% blipdiffusion, kosmos-g, emu2, subject-diffusion, suti, control net
Recent progress in controlled image generation using diffusion models has been significant. Researchers have explored various conditioning strategies—ranging from low-level cues like canny edges and depth maps~\citep{ye2023ip-adapter, controlnet} to higher-level guidance provided by reference images~\citep{SDEdit}—to steer the generative process. For instance, methods such as IP-Adapter~\citep{ye2023ip-adapter} and ControlNet~\citep{controlnet} incorporate additional control signals into standard text-to-image frameworks, thereby allowing more precise manipulation of generated content.
In parallel, several works have leveraged visual elements from input images to further guide the generation process. DreamBooth~\citep{ruiz2023dreamboothfinetuningtexttoimage} and Textual Inversion~\citep{gal2022imageworthwordpersonalizing}, for example, adopt optimization-based approaches to adapt models to specific reference images. Although effective, these methods typically require extensive fine-tuning for each new input, limiting their practicality. To address these limitations, approaches like SuTI~\citep{suti} and Subject-diffusion~\citep{ma2024subjectdiffusionopendomainpersonalizedtexttoimage} have aimed to scale the fine-tuning process so that models can generalize across diverse reference images. However, these strategies still tend to be both time- and resource-intensive, highlighting the ongoing need for more efficient mechanisms for image generation with complex controls.


 
\subsection{Connecting LMMs with Diffusion Models}

Recent studies integrate LMMs with diffusion generators, leveraging the strengths of both paradigms. 
One straightforward approach employs LMMs to interpret complex text-image conditions and generate pure textual representations, which then guide image generation models~\citep{Mini-Gemini}.
Moreover, Seed-Tokenizer~\citep{seed-tokenizer} expands the LMM vocabulary by introducing discrete vision tokens that serve as robust conditioning signals for diffusion models, while Seed-LLama~\citep{seedllama} pre-trains a discrete image tokenizer that decodes visual codes into realistic images using pretrained diffusion models. Similarly, M-VADER~\citep{MVADER} aligns semantic consistency between language models and diffusion decoders through training on extensive image-text pair datasets. Methods such as GILL~\citep{koh2023GILL}, MiniGPT5~\citep{zheng2023minigpt5} Emu~\citep{sun2023emu1} further advance this integration by mapping the embedding spaces of language models to diffusion models, and NExT-GPT~\citep{nextgpt} and Any-GPT~\citep{Zhan2024AnyGPT} even broadens the scope to include modalities like audio and video. Additionally, DreamLLM~\citep{dreamllm}  employs a novel strategy by transferring differential gradients from image diffusion models to language models, thereby enabling free-form, interleaved content generation.
To enhance flexible control in image generation, BLIP-Diffusion~\citep{li2023blipdiffusionpretrainedsubjectrepresentation} leverages LMMs to jointly encode image and text inputs, projecting them into the text conditioning space of diffusion models to better handle complex instructions. 
Moreover, Kosmos-g~\citep{Kosmos-G} and Emu-2~\citep{emu2} explore the multimodal in-context control for image generation. 
Seed-X~\citep{2024SeedX} using a similar architecture to model multi-granularity visual semantics for better generation in the real world applications.
While promising, these approaches mainly extend text-to-image generation and often fall short in improving image quality or managing compositional tasks with arbitrary text-image interleaved control, limiting their real-world applicability.


