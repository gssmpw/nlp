\section{Methods}
\label{sec:method}

% $$
% h_I = (1-r)\cdot h_{llm} + r\cdot h_{vit}
% $$
We target at enabling diffusion models to take different text-image interleaved control in a unified manner by introducing a large multimodal model. We first concisely introduce LMM and MM-DiT architecture in Section~\ref{subsec:lmm_dit}, which are the foundational components of our method. Next, we introduce the structure design of~\model, explaining how do we align LMM with the diffusion model and how to maintain visual features consistency in Section~\ref{subsec:structure_model}. Last, we introduce the two-stage training recipe and how to curate data from different tasks in in Section~\ref{subsec:training_stages}.


\subsection{Preliminary: LMM and MM-DiT}
\label{subsec:lmm_dit}

\paragraph{Large Multimodal Models}
Large multimodal models typically include three major modules, a visual encoder usually a ViT~\citep{vit} structure, a large language model backbone and an alignment layer used to align the representation space between the visual encoder and LLM~\citep{liu2023llava,QwenVL,Qwen2vl,zhao2023mmicl,InternVL}. Given an input image \( i \), this data first passes through the visual encoder followed by the alignment layer, resulting in a transformed representation denoted as $h_{\mathrm{ViT}}$. Subsequently,   $h_{\mathrm{ViT}}$ is processed by the LLM backbone consisting of multiple transformer decoder blocks, evolving into the output image hidden states $h_{\mathrm{LLM}}$ used for text generation.



\begin{figure*}[t]
\vspace{5em}
\includegraphics[width=1\linewidth]{images/objects_generation_background.pdf}

\caption{Performance demonstration on Natural Object Background Merging where \model~ can understand complex text-image input. It can even set more than one object in the background (last line). }
\label{fig:example_front_background}
\end{figure*}





\paragraph{Multimodal Diffusion Transformer} MM-DiT structure~\citep{2024SD3}, is the basic structure for state-of-the-art text-to-image diffusion models such as SD3.5~\citep{2024SD3} and FLUX~\citep{flux}. It is built upon the Latent Diffusion Model (LDM)~\citep{ldm} and Diffusion Transformer (DiT)~\citep{DIT}. It concatenates textual conditioning information $c$ with noised latent embeddings $x$ into a unified sequence. Within the DiT module, MM-DiT employs distinct LayerNorm and MLP layers for each modality while merging their sequences during the attention mechanism. This design allows each representation to evolve within its own specialized space while still considering the influence of the other modality. We employ the embeddings of the timestep $t$ and pooled representation of text condition in the modulation mechanism of DiT. We use rectified flow matching~\citep{liu2022flowstraightfastlearning} as the training objectives to conduct text-to-image generation in Section~\ref{subsec:structure_model}.
 
\subsection{Model Design}
\label{subsec:structure_model}

We adopt the MM-DiT module from Stable Diffusion 3~\citep{2024SD3} model as the DiT model and Qwen2VL~\citep{Qwen2vl} as the LMM to compose \model.

\paragraph{Align LMM and MM-DiT}
As shown in Figure~\ref{fig:dream_engine}, we completely replace the text encoders including CLIP~\citep{radford2021clip} and T5~\citep{xue2021mt5} from the text-to-image diffusion models with the LMM to get a unified representation of text and image $c$. To align the representation space of pretrained LMM with that of previous encoders and enable the MM-DiT module to take image input, we add a straight-forward adapter layer consisting of a two-layer MLP. The adapter maps the output hidden states of LMM to the conditioning feature space of MM-DiT. We add the average pooling representations of the LMM condition and timestep embedding as the modulation embedding $y$ in the MM-DiT model. We remove the token length limit of original text encoders so that~\model~can take any sequence length of text-image input.
\paragraph{Blending Visual Feature for Better Consistency}

% \begin{figure}[t]
% \includegraphics[width=1\linewidth]{images/structure.pdf}
% % \vspace{-2em}
% \captionof{figure}{Caption}
% \label{fig:blended_visual}
% \end{figure}

To control the visual consistency in image editing and objects-driven generation tasks, we add a skip connection for visual features in the LMM to avoid visual information loss in the LMM backbone model. As shown in Figure~\ref{fig:dream_engine}, the final hidden states of image patches $h_I$ are a weighted sum of the LLM output hidden states $h_{\mathrm{LLM}}$ and ViT image features $h_{\mathrm{ViT}}$:

\begin{equation}
    h_I = (1-r)\cdot h_{\mathrm{LLM}} + r\cdot h_{\mathrm{ViT}}
\end{equation}

The adjustable blending ratio \( r \) allows for the control of image feature consistency between the input and output images, tailored to specific applications. During the training phase, \( r\in[0,1] \) adheres to a uniform distribution, which enables the flexibility to assign various values to \( r \) during the inference process as shown in Figure~\ref{fig:vision_blending}.

\paragraph{Training Objectives} We adopt rectified flows~\citep{liu2022flowstraightfastlearning} to learn the transition between the target data distribution $\mathbf{x}_0$ and a standard normal distribution $\boldsymbol{\epsilon}$, i.e.

\begin{equation}
    \mathbf{z}_t = (1 - t)\cdot \mathbf{x}_0 + t\cdot \boldsymbol{\epsilon},
\end{equation}

where $t \in [0,1]$ represents the timestep, and $\mathbf{z}_t$ denotes the corresponding distribution at the $t$-th step. At each step, based on the current distribution $\mathbf{z}_t$, a condition $\mathbf{c}$, and the timestep $t$, the model directly parameterizes the velocity $v_\theta (\mathbf{z}_t, \mathbf{c}, t)$. This velocity is expected to approximate $\mathbf{x}_0 - \boldsymbol{\epsilon}$ during the flow matching process. It is important to note that the condition $\mathbf{c}$ can include interleaved text-image control, as opposed to solely text-based information as seen in the original text-to-image diffusion models. The training objective is to minimize the expected L2 loss by updating the model parameters $v_\theta$, with a weight $w_t$ assigned to each timestep, i.e.

\begin{equation}
    \min_{v_\theta} \int_{0}^{1} \mathbb{E} \left[ w_{t} \left\| (\mathbf{x}_0 - \boldsymbol{\epsilon}) - v_\theta (\mathbf{z}_t, \mathbf{c}, t) \right\|^2 \right] dt
\end{equation}

We use the Euler Discrete Scheduler~\citep{karras2022elucidatingdesignspacediffusionbased} following SD3~\citep{2024SD3} to set the timestep. The target data distribution $\mathbf{x}_0$ comes from the latent representation of VAE, which is the same as the VAE of SD3 following a standard Latent Diffusion Model~\citep{ldm} training process.


\subsection{Training Stages}
\label{subsec:training_stages}

Given that~\model~comprises two individually pretrained components—the LMM and the DiT—it is crucial to ensure their alignment. Adhering to the established practices outlined in the LMM literature~\citep{liu2023llava,QwenVL,internvl15,Cambrian-1}, we have structured the training process into distinct phases, each designed to unfreeze specific model components to promote stable and effective training. As shown in Figure~\ref{fig:stages}, our approach involves two primary training stages, where each stage has its own training tasks and trainable modules. In the S1 stage, we focus on training only the adapter layer, which facilitates the alignment of the representation spaces between the LMM and the DiT. During the S2 stage, we train both the adapter and the DiT, allowing for more sophisticated control over the generation process. We also show the training examples of each task in Figure~\ref{fig:stages}.



% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \centering
% \includegraphics[width=1\linewidth]{images/stages.pdf}
% % \vspace{-2em}
% \captionof{figure}{Caption}
% \label{fig:stages}
% }]

\begin{figure*}[t]
\includegraphics[width=1\linewidth]{images/twostages.pdf}
% \vspace{-2em}
\caption{Training stages and tasks of \model.}
\label{fig:stages}
\end{figure*}

\paragraph{Stage 1: Joint Text and Image Alignment}

In the first stage, we focus on aligning the representation spaces of the LMM and DiT modules by training a dedicated adapter, while keeping the parameters of both the LMM and DiT frozen. This alignment process involves two complementary tasks. 

\begin{itemize}
    \item Task A: Text-to-Image Alignment. It leverages high-quality image-caption pairs to establish a foundational correspondence between textual descriptions and generated images, effectively replacing the original text encoders.
    \item  Task B: Image-to-Image Alignment. It is a self-supervised task that enables DiT to condition on image inputs. Specifically, DiT is trained to reconstruct input images based on the LMM’s image representations, thereby enhancing the consistency and fidelity of visual elements. 
\end{itemize}

Upon completing Stage 1 training, \model~acquires two core capabilities: \textbf{text-to-image generation} and \textbf{image variation}. Interestingly, we observe that the two tasks mutually reinforce each other. Even when trained solely on one task, the model demonstrates a certain degree of capability in the other task in a zero-shot manner. This finding suggests that the LMM inherently provides a unified representation space for text and images, which the DiT can effectively leverage during training. As shown in Table~\ref{tab:reconstruct}, our model trained without the Image-to-Image Alignment task can still achieve a relatively high (0.7+) CLIP score in the image reconstruction evaluation.


\paragraph{Stage 2: Interleaved Condition Instruction Tuning}


In the second stage, we unfreeze the DiT module and train it on two tasks that require interleaved image-text conditioning. 

\begin{itemize}
    \item Task C: Free-Form Image Editing. It takes an input image along with an editing instruction and outputs the edited image. We use the UltraEdit~\citep{ultraEdit} dataset as the dataset.
    \item Task D: Objects Driven Generation. It accepts multiple input images and a textual instruction, composing elements from the input images based on the given text to generate the output. For Task 4, we construct the training data using object detection datasets, such as COCO~\citep{lin2014mscoco}, pairing images with captions that describe the objects present.
\end{itemize}

After the second stage, the model gains the ability to handle interleaved image-text conditions during generation. Surprisingly, we observe emergent capabilities in \model. Notably, it can synthesize elements from different objects to generate cohesive images, as demonstrated in Figure~\ref{fig:example_object_mix}, despite such compositions not being explicitly present in the training data.




