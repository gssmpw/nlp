\section{Conclusion}

In this work, we introduced \model, a novel framework that enables sophisticated text-image interleaved control without complex architectural modifications. Our method bridges Large Multimodal Models and text-to-image diffusion models through a lightweight projector layer and efficient two-stage training paradigm. It demonstrates superior capabilities in handling multiple image inputs and compositional instructions while also achieving competitive performance on the GenEval benchmark (0.69). The success of \model~demonstrates that LMMs can effectively replace traditional text encoders in text-to-image diffusion models while expanding their capabilities to include sophisticated multimodal control. 

Looking ahead, our work opens up new possibilities for creative image generation applications where users can precisely orchestrate visual compositions through natural language instructions and multiple reference images. Future research directions could explore extending this framework to other modalities, such as video or 3D content, and investigating ways to further enhance the semantic understanding of complex multimodal instructions.