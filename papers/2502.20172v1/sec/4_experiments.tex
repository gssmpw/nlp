\begin{table*}[t]
    \centering
    \caption{\textbf{Performances on GenEval benchmark.} We split the methods to autoregressive and diffusion based models.
    }
    \resizebox{0.8\textwidth}{!}{
    % \begin{tabular}{clccccccc}
    \begin{tabular}{@{}cl
        >{\centering\arraybackslash}m{1.4cm}
        >{\centering\arraybackslash}m{1.4cm}
        >{\centering\arraybackslash}m{1.4cm}
        >{\centering\arraybackslash}m{1.4cm}
        >{\centering\arraybackslash}m{1.4cm}
        >{\centering\arraybackslash}m{1.4cm}
        >{\centering\arraybackslash}m{1.4cm}
        @{}}
        \toprule
         & \textbf{Method} & \textbf{Single Object} & \textbf{Two Object} & \textbf{Counting} & \textbf{Colors} & \textbf{Position} & \textbf{Attribute Binding} & \textbf{Overall} \\
        \midrule

        
        \multirow{6}{*}{\rotatebox{90}{\textit{Autoregressive}}}
        % \cdashline{2-10}
        & Chameleon~\cite{2024Chameleon}  & - & - & - & - & - & - & $0.39$ \\
        & LWM~\cite{2024LWM}  & $0.93$ & $0.41$ & $0.46$ & $0.79$ & $0.09$ & $0.15$ & $0.47$ \\
        & LlamaGen~\cite{2024llamagen} &  $0.71$ & $0.34$ & $0.21$ & $0.58$ & $0.07$ & $0.04$ & $0.32$ \\
        & Show-o~\cite{2024Showo}  & $0.95$ & $0.52$ & $0.49$ & $0.82$ & $0.11$ & $0.28$ & $0.53$ \\
        & Emu$3$-Gen ~\cite{2024emu3} &  $0.98$ & $0.71$ & $0.34$ & $0.81$ & $0.17$ & $0.21$ & $0.54$ \\
        & Janus \cite{2024Janus}  & $0.97$ & $0.68$ & $0.30$ & \underline{0.84} & \textbf{0.46} & $0.42$ & $0.61$\\ 

        \midrule
        
        \multirow{12}{*}{\rotatebox{90}{\textit{Diffusion}}} 
        
        & LDM~\cite{2022LDM}  & $0.92$ & $0.29$ & $0.23$ & $0.70$ & $0.02$ & $0.05$ & $0.37$ \\
        & SDv$1.5$~\cite{2022LDM}  & $0.97$ & $0.38$ & $0.35$ & $0.76$ & $0.04$ & $0.06$ & $0.43$ \\
        & PixArt-$\alpha$~\cite{2023Pixelartalpha} & $0.98$ & $0.50$ & $0.44$ & $0.80$ & $0.08$ & $0.07$ & $0.48$ \\
        & SDv$2.1$~\cite{2022LDM} & $0.98$ & $0.51$ & $0.44$ & $0.85$ & $0.07$ & $0.17$ & $0.50$ \\
        & DALL-E $2$~\cite{2022DALLE2} &  $0.94$ & $0.66$ & $0.49$ & $0.77$ & $0.10$ & $0.19$ & $0.52$ \\

        & SDXL~\cite{2023SDXL} & $0.98$ & $0.74$ & $0.39$ & \textbf{0.85} & $0.15$ & $0.23$ & $0.55$ \\
        & IF-XL~\cite{2023IF}& $0.97$ & $0.74$ & $0.66$ & $0.81$ & $0.13$ & $0.35$ & $0.61$ \\
        & DALL-E $3$~\cite{2023dalle3}& $0.96$ & $0.87$ & $0.47$ & $0.83$ & \underline{0.43} & $0.45$ & $0.67$ \\
        &SDv3 Medium~\cite{2024SD3} & 0.98 & 0.74 & 0.63 & 0.67 & 0.34 &0.36 & 0.62
        \\
        &Flux.1 Dev~\citep{flux}& 0.98&	0.81	&
        \textbf{0.74}&	0.79	&0.22&	0.45 & 0.66 \\
        &SDv3.5 Large~\cite{2024SD3} & \underline{0.98} & \underline{0.89} & \underline{0.73} & 0.83 & 0.34 & \underline{0.47} &  \textbf{0.71}
        \\
        &\textbf{\model} & \textbf{1.00} & \textbf{0.94} & 0.64 & 0.81 & 0.27 & \textbf{0.49} & \underline{0.69}
        \\

        \bottomrule
    \end{tabular}}
    \label{tab:exp-geneval}
\end{table*}

\section{Experiments}
\label{sec:experiment}

\subsection{Dataset}

Table~\ref{tab:dataset} provides an overview of the datasets used for training~\model, along with the number of examples drawn from each source. In Stage 1, for the Text-to-Image Alignment task, we compile public image-caption datasets, including real-world images from CC12M~\citep{changpinyo2021cc12m} and model-generated images from JourneyDB~\citep{2024JDB}. Additionally, we synthesize a subset of high-quality images using diverse prompts with open text-to-image models, such as Flux.1 dev~\citep{flux} and Stable-Diffusion v3.5 Large~\citep{2024SD3}. For the Image-to-Image Alignment task, we rely solely on images from JourneyDB, as lower-aesthetic-quality images, such as images from CC12M, tend to degrade overall image reconstruction and text-to-image performance. In Stage 2, we utilize the UltraEdit~\citep{ultraEdit} dataset for the Free-Form Image Editing task and an internal object detection dataset for Object-Driven Generation. For the latter, we randomly select three objects from each image. Additionally, the name of each selected object must appear in the text caption to compose the conditioning input.

\begin{table}[t!]
\centering
\caption{Details on datasets used in training \model~ within the two training stages. }
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{@{}cllc@{}}
\toprule
\textbf{Stage} & \textbf{Dataset} & \textbf{Task} & \textbf{Number} \\
\midrule
\multirow{4}{*}{1} & JourneyDB~\citep{2024JDB} & Text-to-Image Alignment & 4M \\
 & CC12M~\citep{changpinyo2021cc12m} & Text-to-Image Alignment & 4M \\
 & Synthetic Data & Text-to-Image Alignment & 4M \\
 & JourneyDB~\citep{2024JDB} & Image-to-Image Alignment & 4M \\
\midrule
\multirow{2}{*}{2}  & UltraEdit~\citep{ultraEdit} & Free Form Image Edit & 1M \\
 & Internal Data & Object-Driven Generation & 4M \\
\bottomrule
\end{tabular}}
\label{tab:dataset}
\end{table}

\subsection{Model and Training Details}

We initialize the LMM and DiT module of \model~from Qwen2VL-2B-Instruct~\citep{Qwen2vl} and Stable-Diffusion-3.5-Large~\citep{2024SD3}. The Adapter consists of a two-layer MLP with a middle projection dimension of 4,096 and uses SiLU as the activation function following the DiT module. In Stage 1, we freeze the parameters of the LMM and DiT modules and train the Adapter on the composed dataset for one epoch with a global batch size of 128. The learning rate is set to 1e-4, with 5\% warmup steps and a cosine learning rate scheduler. In Stage 2, we also fine-tune the DiT module using LoRA~\citep{hu2021lora} with a rank of 32 on all attention layers. The learning rate is set to 5e-5, while all other settings remain the same as in Stage 1. We do not fine-tune the LMM component of the model, thus preserving its original multimodal understanding capabilities. This design choice allows the model to be easily adapted into an omni-model, capable of performing both multimodal understanding and generation simultaneously. On the other hand, unfreezing the LMM during training has large potential in further improving the generation performance.


\subsection{Results and Comparisons}

\begin{table}[t]
\centering
\caption{Image reconstruction performance comparison on COCO and JourneyDB datasets.}
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{COCO}} & \multicolumn{2}{c}{\textbf{JourneyDB}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & CLIP~($\uparrow$) & L2~($\downarrow$) & CLIP~($\uparrow$) & L2~($\downarrow$) \\
\midrule
SeedTokenizer & 0.7760 & 0.5102 & 0.7921 & 0.5291 \\
EMU2-Gen & 0.8537 & \underline{0.3828} & \textbf{0.9299} & \underline{0.2869} \\
SEED-X & \underline{0.8595} & 0.4317 & 0.9017 & 0.4352 \\
\midrule
\model & \textbf{0.8714} & \textbf{0.2065} & \underline{0.9221} & \textbf{0.2052} \\
\quad - w/o I-to-I Alignment & 0.7184 & 0.6541 & 0.7536 & 0.6543 \\
\bottomrule
\end{tabular}}
\label{tab:reconstruct}
\end{table}


\begin{figure*}[t]
\vspace{5em}
\includegraphics[width=1\linewidth]{images/Example_object_mix.pdf}

\caption{ Performance demonstration on Object Driven Feature Mixing task. \model~ can understand the complex instruction while Emu2-Gen fails on the task.}
\label{fig:example_object_mix}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{images/edit.pdf}
% \vspace{-2em}
\caption{Performance demonstration on Free Form Image Editing task. \model~ outperforms the counterpart Emu2-Gen model in both instruction following and output image quality. }
\label{fig:example_freeform_edit}
\end{figure*}




\paragraph{\mbox{Text-to-Image Generation}}

We evaluate the text-to-image generation capability of \model on the GenEval~\citep{ghosh2023genevalobjectfocusedframeworkevaluating} benchmark following Stage 1 training. The results, including fine-grained scores, are presented in Table~\ref{tab:exp-geneval}. Built upon the SDv3.5~\citep{2024SD3} model, \model achieves a competitive overall score of 0.69, closely matching the original model’s 0.71 despite excluding its native text encoders. Moreover, \model outperforms all other counterparts, demonstrating the effectiveness and efficiency of our text-to-image alignment training in preserving instruction-following capabilities while replacing the original text encoders of diffusion models to enable more complex interleaved conditions.


\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{images/reconstruction_examples.pdf}
% \vspace{-2em}
\caption{Image reconstruction performance dynamics during training. We can see that there is a concept-to-detail transition during the training period.}
\label{fig:recon}
\end{figure*}





\paragraph{Image Reconstruction}
We introduce an Image Reconstruction Benchmark to evaluate the preservation of visual features in our Image-to-Image alignment task during Stage 1. This capability is essential for generating images conditioned on input images. To construct the benchmark, we randomly sample 100 images from the JourneyDB development set and 100 images from the COCO development set. We assess the similarity between the original and reconstructed images using the CLIP~\citep{radford2021clip} score and L2-Distance. As shown in Table~\ref{tab:reconstruct}, we compare the performance of \model~ against several baselines with similar architectures, including SeedTokenizer~\citep{seed-tokenizer}, EMU-2~\citep{emu2}, and SeedX~\citep{2024SeedX}, which also integrate LMMs and diffusion models for generation. The results demonstrate that our model achieves the best average image reconstruction performance across both subsets of the benchmark. Notably, it achieves outstanding performance on the L2 distance metric, which emphasizes pixel-level consistency, surpassing the second-best model by 46\% on the COCO subset and 28\% on the JourneyDB subset.




\paragraph{Generation with Text-Image Interleaved Control}






After Stage 2 training, \model~acquires the capability to incorporate text-image interleaved control during the image generation process. We showcase several applications of the model in this paper and compare its performance with Emu-2~\citep{emu2}, the most relevant baseline that also supports text-image interleaved control. 


\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{images/visionblending.pdf}
% \vspace{-2em}
\caption{The ablation on visual blending ratio in the ViT module. It reveals that a higher blending ratio results in greater consistency during image reconstruction tasks.}
\label{fig:vision_blending}
\end{figure*}


\begin{itemize}
    \item[1.] \textbf{Natural Object Background Merging:} Figure~\ref{fig:example_front_background} illustrates an example application where objects are merged into different backgrounds based on a provided hint image. The results demonstrate that \model can seamlessly place the main object into various backgrounds in a more natural manner even when there are multiple objects, rather than merely copying and pasting the objects.
    \item[2.] \textbf{Object Driven Feature Mixing:} Figure~\ref{fig:example_object_mix} demonstrates an emergent ability of \model~to generate images by combining visual features from given images based on text instructions—an area where the EMU-2 model fails to follow instructions accurately. Notably, these examples are not present in the training dataset, which was constructed directly from an object detection dataset. This highlights the significant potential of LMMs as unified multimodal instruction encoders for image generation. \model~ effectively decouples complex elements from different images using simple text prompts and produces a unified representation, showcasing its versatility and robustness.
    \item[3.] \textbf{Free Form Image Editing:} Figure~\ref{fig:example_freeform_edit} presents the results of our free-form image editing task. \model~consistently demonstrates superior ability to follow edit instructions compared to EMU-2. Notably, \model~can handle complex editing instructions, such as simultaneously modifying both the object and background. This further highlights the effectiveness of LMMs in providing a unified representation space that seamlessly integrates image and text conditions.
\end{itemize}








\subsection{Discussions}

\paragraph{Understand the training dynamics of \model}

To better understand how \model~leverages an LMM and a text-to-image model to achieve complex text-image instruction following ability during the training process, we examine the image reconstruction results at different training stages in Stage-1. The results are shown in Figure~\ref{fig:recon}. We observe a clear concept-to-detail progression during the training of \model. For example, as illustrated in the first two columns of Figure~\ref{fig:recon}, the model initially reconstructs the primary concepts in the images, such as ``girl,'' ``house with snow,'' and ``dog'' in the given examples. 

In the later stages, the model begins to learn to reconstruct more fine-grained details, such as colors, shapes, and poses. We believe this unique training dynamic stems from the nature of the LMM, which provides a unified representation space where images and text are well aligned. Thus, at the beginning of training, even if the text-to-image diffusion model has not yet seen image conditions, the image representations provided by the LMM are aligned with text features. This alignment enables the model to generate conceptually aligned images through the bridge of text.

\paragraph{Ablation on Balancing Visual Consistency}

We conduct an ablation study on the Blending Visual Feature mechanism within the Vision Transformer (ViT) module. As shown in Figure~\ref{fig:vision_blending}, varying the blending ratio in the image-image alignment task produces notably different results. Higher blending ratios lead to greater consistency in image reconstruction, while lower ratios introduce more variation in the output. This mechanism offers flexible control over object consistency, benefiting various downstream tasks such as image editing and object-driven feature mixing.