\section{Related Works}
\subsection{Video Generation}
  In recent years, the advent of technologies such as diffusion models \cite{jonathan2020ddpm, song2021ddim, karras2022edm, song2020score, liu2022reflow} has propelled the capabilities of generative models to a practically usable level. The latest advancements in image generation \cite{sd3, chen2024pixartdelta} produce results that are almost indistinguishable from reality. Consequently, a growing number of studies \cite{zhou2022magicvideo, zeng2024pxldance, hong2022cogvideo, yang2024cogvideox, openai2024sora, kong2024hunyuanvideo, polyak2024moviegen} are shifting their focus toward the field of video generation.
  Early text-to-video works primarily centered on training-free adaptations of pre-trained text-to-image models \cite{singer2022make, wu2023tune, qi2023fatezero} or integrated temporal layers with fine-tuning on limited video datasets \cite{guo2023animatediff, zhou2022magicvideo, wang2023modelscope}. However, due to the lack of extensive data, the video generation quality of these methods often remains unsatisfactory. To better exploit scaling laws and push the boundaries of video generation models, recent works \cite{openai2024sora, yang2024cogvideox, kong2024hunyuanvideo, polyak2024moviegen} have optimized in three major areas. First, they have collected larger-scale, high-quality video datasets, with the data volume increasing to (O(100M)) clips of high-resolution videos. Second, they employ 3D Causal VAE \cite{yu20233DVAE} to compress both spatial and temporal features of video data, thereby enhancing video modeling efficiency. Third, the foundational model structure has transitioned from UNet to Transformer, improving the modelâ€™s scalability. Additionally, these works utilize meticulously designed progressive training recipes and datasets to maximize the model's potential. For example, \cite{polyak2024moviegen, kong2024hunyuanvideo} first pre-train on a large volume of low-resolution images and videos, leveraging data diversity to enhance the model's generalization capabilities. They then perform fine-tuning on a subset of high-resolution, high-quality data to improve the visual quality of generated videos. Large-scale data has significantly improved the effectiveness of general video generation. However, progress in the field of human animation synthesis remains relatively slow.
  
  \subsection{Human Animation}
  As an important task of video generation, Human Animation synthesizes human videos using human images and driving conditions such as audios or videos. Early  GAN-based methods \cite{siarohin2019fomm, zhao2022tps, siarohin2021mraa, jiang2024mobileportrait, wang2021facev2v} typically employ small datasets \cite{nagrani2017voxceleb, siarohin2019fomm, xie2022vfhq, zhu2022celebv} consisting of tens of thousands of videos to achieve video-driven in a self-supervised manner. With the advancement of Diffusion models, several related works \cite{Disco, aa, champ, shao2024human4dit, zhang2024mimicmotion} have surpassed GAN-based methods in performance while using datasets of similar scale. Instead of using pixel-level videos, these methods employ 2D skeleton, 3D depth, or 3D mesh sequences as driving conditions.
  Audio-driven methods used to focus on portrait \cite{adnerf,GeneFace,zhang2023sadtalker, EMO, jiang2024loopy, hallo3, fada}. Despite some efforts \cite{VLogger, lin2024cyberhost, EchomimicV2, EMO2, diffted} to extend the frame to the full body, there are still challanges especially in hand quality. To bypass it, most approaches \cite{VLogger,  EchomimicV2, EMO2, diffted} adopt a two-stage hybrid driving strategy, utilizing gesture sequences as a strong condition to assist hand generation. CyberHost \cite{lin2024cyberhost} attempts to achieve one-stage audio-driven talking body generation through codebook design.
  Most notably, existing Human Animation methods typically focus on limited-scale datasets and limited-complexity structure, generally less than a thousand hours and 2B. Although FADA \cite{fada} employs a semi-supervised data strategy to utilize 1.4K hours of portrait videos, VLogger \cite{VLogger} meticulously collects 2.2K hours of half-body videos, and Hallo3 \cite{hallo3} initializes its weights derived from CogVideoX5B-I2V \cite{cogvideox}, their performance  does not exhibit the scaling law trends observed in other tasks such as LLMs \cite{ouyang2022training, touvron2023llama}, VLMs \cite{liu2024improved,bai2023qwen}, and T2I/T2V \cite{esser2024scaling, flux2023, kondratyuk2023videopoet}. Scaling effects in Human Animation  haven't been investigated effectively yet.

  

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/framework.jpg}
    \caption{\small \textbf{The framework of OmniHuman.} It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture and supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training strategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training allows the OmniHuman model to benefit from the scaling up of mixed data.}
    \label{fig:framework}
\end{figure*}