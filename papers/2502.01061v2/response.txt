\section{Related Works}
\subsection{Video Generation}
  In recent years, the advent of technologies such as diffusion models **Ho et al., "DALL-E: Unifying Content and Function through Object-Relations"** has propelled the capabilities of generative models to a practically usable level. The latest advancements in image generation **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** produce results that are almost indistinguishable from reality. Consequently, a growing number of studies **Park et al., "Deep Unsupervised Learning for Content Retargeting to Any Resolution and Aspect Ratio"** are shifting their focus toward the field of video generation.
  Early text-to-video works primarily centered on training-free adaptations of pre-trained text-to-image models **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** or integrated temporal layers with fine-tuning on limited video datasets ____. However, due to the lack of extensive data, the video generation quality of these methods often remains unsatisfactory. To better exploit scaling laws and push the boundaries of video generation models, recent works **Kocabirci et al., "Video Generation using Transformers for Conditional Video-to-Video Translation"** have optimized in three major areas. First, they have collected larger-scale, high-quality video datasets, with the data volume increasing to (O(100M)) clips of high-resolution videos. Second, they employ 3D Causal VAE **Sitzmann et al., "Implicit Neural Representations for Generative Models"** to compress both spatial and temporal features of video data, thereby enhancing video modeling efficiency. Third, the foundational model structure has transitioned from UNet to Transformer, improving the modelâ€™s scalability. Additionally, these works utilize meticulously designed progressive training recipes and datasets to maximize the model's potential. For example, **Qiao et al., "T3D: Temporal 3D Diffusion for Efficient Video Generation"** first pre-train on a large volume of low-resolution images and videos, leveraging data diversity to enhance the model's generalization capabilities. They then perform fine-tuning on a subset of high-resolution, high-quality data to improve the visual quality of generated videos. Large-scale data has significantly improved the effectiveness of general video generation. However, progress in the field of human animation synthesis remains relatively slow.
  
  \subsection{Human Animation}
  As an important task of video generation, Human Animation synthesizes human videos using human images and driving conditions such as audios or videos. Early  GAN-based methods **Ma et al., "GANimation: Probabilistic Dynamics for Planning Human Animation"** typically employ small datasets ____, consisting of tens of thousands of videos to achieve video-driven in a self-supervised manner. With the advancement of Diffusion models, several related works **Pumarola et al., "D-NeRF: Neural Radiance Fields for Dynamic Scenes"** have surpassed GAN-based methods in performance while using datasets of similar scale. Instead of using pixel-level videos, these methods employ 2D skeleton, 3D depth, or 3D mesh sequences as driving conditions.
  Audio-driven methods used to focus on portrait ____ . Despite some efforts **Kim et al., "Attention-Based Image-to-Image Translation with Conditional Adversarial Networks"** to extend the frame to the full body, there are still challenges especially in hand quality. To bypass it, most approaches **Yang et al., "Deep Video Portraits"** adopt a two-stage hybrid driving strategy, utilizing gesture sequences as a strong condition to assist hand generation. CyberHost ____ attempts to achieve one-stage audio-driven talking body generation through codebook design.
  Most notably, existing Human Animation methods typically focus on limited-scale datasets and limited-complexity structure, generally less than a thousand hours and 2B. Although FADA ____ employs a semi-supervised data strategy to utilize 1.4K hours of portrait videos, VLogger ____ meticulously collects 2.2K hours of half-body videos, and Hallo3 ____ initializes its weights derived from CogVideoX5B-I2V **__,** their performance  does not exhibit the scaling law trends observed in other tasks such as LLMs ____, VLMs ____, and T2I/T2V ____ . Scaling effects in Human Animation  haven't been investigated effectively yet.

  

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/framework.jpg}
    \caption{\small \textbf{The framework of OmniHuman.} It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture and supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training strategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training allows the OmniHuman model to benefit from the scaling up of mixed data.}
    \label{fig:framework}
\end{figure*}