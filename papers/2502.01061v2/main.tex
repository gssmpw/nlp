% CVPR 2024 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{float}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{textcomp}

%%%%%%%%% TITLE - PLEASE UPDATE

\newcommand*{\affaddr}[1]{#1} 
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\name}{OmniHuman}
\begin{document}
\title{OmniHuman-1: Rethinking the Scaling-Up of One-Stage  
\\ Conditioned Human Animation Models}



\author{
Gaojie Lin$^{*}$ \quad 
Jianwen Jiang$^{*\dagger}$ \quad 
Jiaqi Yang$^{*}$ \quad 
Zerong Zheng$^{*}$ \quad 
Chao Liang \vspace{2mm} \\
\affaddr{ByteDance} \\
\small{\href{https://omnihuman-lab.github.io/}{\ttfamily https://omnihuman-lab.github.io/}}
}





\twocolumn[{
\maketitle
\begin{center}
    \vspace{-10pt}
    \captionsetup{type=figure}
    \includegraphics[width=1\textwidth]{imgs/omni1.png}
    \captionof{figure}{\small \textbf{The video frames generated by OmniHuman based on input audio and image.} The generated results feature head and gesture movements, as well as facial expressions, that match the audio. OmniHuman generates highly realistic videos with any aspect ratio and body proportion, and significantly improves gesture generation and object interaction over existing methods, due to the data scaling up enabled by omni-conditions training. }
    \label{fig:vis1}
\end{center}
}\bigbreak



]

\footnotetext{~$^*$Equal contributions}
\footnotetext{~$^\dagger$Project lead}


% \footnote{~$^*$Equal contributions}
% \footnote{~$^\dagger$Project lead}





\begin{abstract}
End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the \href{https://omnihuman-lab.github.io/}{\ttfamily project page}.



\end{abstract}

\section{Introduction}
Since the emergence of the Diffusion Transformer-based (DiT) video diffusion models, the field of general video generation, including Text-to-Video and Image-to-Video~\cite{bar2024lumiere,svd,ayl,guo2023animatediff,zhou2022magicvideo,walt,wang2023modelscope,vdm,videogan,cvideogan,singer2022make,text2video,villegas2022phenaki,lin2025apt} has made significant progress in producing highly realistic video content. A key factor driving this advancement is the large-scale training data, typically formatted as video-text pairs. Expanding the training dataset enables DiT networks to learn motion priors for various objects and scenes, resulting in strong generalization capabilities during inference.


Building upon these pretrained video diffusion networks, end-to-end human animation models, either for pose-driven human animation or audio-driven talking human generation, have developed rapidly since last year~\cite{he2023gaia,tian2024emo,xu2024hallo,wang2024vexpress,chen2024echomimic,xu2024vasa,stypulkowski2024diffused,jiang2024loopy,lin2024cyberhost}. 
Despite achieving realistic results, these models are trained on highly filtered datasets to simplify the learning process, restricting their applicability to limited scenarios. 
For instance, most existing end-to-end audio-conditioned models are limited to facial or portrait animation, while most pose-conditioned models can only handle full-body images captured from a front-facing perspective with a static background. To date, no prior work has attempted to scale up training data for more generalizable human animation.


Scaling up human animation data may seem straightforward, but unfortunately it is not. Directly adding more data is not always beneficial for network training. 
Take audio-conditioned models as an example: audio is primarily associated with facial expressions and has little correlation with body poses, background motion, camera movement, or lighting changes. As a result, raw training data must be filtered and cropped to minimize the influence of these unrelated factors. Additionally, audio-conditioned models often undergo further data cleaning based on lip-sync accuracy, which is also important to stabilize training. Similarly, pose-conditioned models require extensive filtering, cropping, and cleaning. 
Unfortunately, these processes discard a substantial amount of data, making dataset scaling a futile effort, despite the fact that much of the discarded data contains valuable motion patterns essential for training data expansion.


In this paper, we address the challenges of scaling up human animation data and models. Our key insight is that incorporating multiple conditioning signals, such as text, audio, and pose, during training can significantly reduce data wastage. This approach offers two main advantages. On one hand, data that would otherwise be discarded for single-condition models (e.g., audio- or pose-conditioned) can be leveraged in tasks with weaker or more general conditions, such as text conditioning. Training on such data allows the model to learn more diverse motion patterns, mitigating the limitations imposed by data filtering. On the other hand, different conditioning signals can complement each other. For example, while audio alone cannot precisely control body poses, stronger conditions such as pose inputs can provide additional guidance. By integrating stronger conditioning signals alongside audio data during training, we aim to reduce overfitting and improve the generalization of generated results.



Based on the above considerations, we designed the omni-conditions training strategy, which follows two proposed training principles: (1) stronger conditioned tasks can leverage weaker conditioned tasks and their corresponding data to achieve data scaling up during the model training process, and (2) the stronger the condition, the lower the training ratio that should be used. To implement this strategy, we built a mixed conditioned human video generation model named OmniHuman, based on the advanced video generation model architecture, DiT~\cite{dit,sd3}. OmniHuman can train with three motion-related conditions (text, audio, and pose) from weak to strong. This approach addresses the data scaling up challenge in end-to-end frameworks, allowing the model to benefit from large-scale data training, learn natural motion patterns, and support various input forms.


Overall, our contributions can be summarized as follows:
\begin{enumerate}
    \item We propose the OmniHuman model, a mixed-conditioned human video generation model. It leverages our omni-conditions training strategy to integrate various motion-related conditions and their corresponding data. Unlike existing methods that reduce data due to stringent filtering, our approach benefits from large-scale mixed conditioned data.


    \item OmniHuman generates highly realistic and vivid human motion videos, supporting multiple modalities simultaneously. It performs well with different portrait and input aspect ratios. OmniHuman significantly improves gesture generation, a challenge for previous end-to-end models, and supports various image styles, significantly outperforming existing audio-conditioned human video generation methods.
\end{enumerate}



\section{Related Works}
\subsection{Video Generation}
  In recent years, the advent of technologies such as diffusion models \cite{jonathan2020ddpm, song2021ddim, karras2022edm, song2020score, liu2022reflow} has propelled the capabilities of generative models to a practically usable level. The latest advancements in image generation \cite{sd3, chen2024pixartdelta} produce results that are almost indistinguishable from reality. Consequently, a growing number of studies \cite{zhou2022magicvideo, zeng2024pxldance, hong2022cogvideo, yang2024cogvideox, openai2024sora, kong2024hunyuanvideo, polyak2024moviegen} are shifting their focus toward the field of video generation.
  Early text-to-video works primarily centered on training-free adaptations of pre-trained text-to-image models \cite{singer2022make, wu2023tune, qi2023fatezero} or integrated temporal layers with fine-tuning on limited video datasets \cite{guo2023animatediff, zhou2022magicvideo, wang2023modelscope}. However, due to the lack of extensive data, the video generation quality of these methods often remains unsatisfactory. To better exploit scaling laws and push the boundaries of video generation models, recent works \cite{openai2024sora, yang2024cogvideox, kong2024hunyuanvideo, polyak2024moviegen} have optimized in three major areas. First, they have collected larger-scale, high-quality video datasets, with the data volume increasing to (O(100M)) clips of high-resolution videos. Second, they employ 3D Causal VAE \cite{yu20233DVAE} to compress both spatial and temporal features of video data, thereby enhancing video modeling efficiency. Third, the foundational model structure has transitioned from UNet to Transformer, improving the model’s scalability. Additionally, these works utilize meticulously designed progressive training recipes and datasets to maximize the model's potential. For example, \cite{polyak2024moviegen, kong2024hunyuanvideo} first pre-train on a large volume of low-resolution images and videos, leveraging data diversity to enhance the model's generalization capabilities. They then perform fine-tuning on a subset of high-resolution, high-quality data to improve the visual quality of generated videos. Large-scale data has significantly improved the effectiveness of general video generation. However, progress in the field of human animation synthesis remains relatively slow.
  
  \subsection{Human Animation}
  As an important task of video generation, Human Animation synthesizes human videos using human images and driving conditions such as audios or videos. Early  GAN-based methods \cite{siarohin2019fomm, zhao2022tps, siarohin2021mraa, jiang2024mobileportrait, wang2021facev2v} typically employ small datasets \cite{nagrani2017voxceleb, siarohin2019fomm, xie2022vfhq, zhu2022celebv} consisting of tens of thousands of videos to achieve video-driven in a self-supervised manner. With the advancement of Diffusion models, several related works \cite{Disco, aa, champ, shao2024human4dit, zhang2024mimicmotion} have surpassed GAN-based methods in performance while using datasets of similar scale. Instead of using pixel-level videos, these methods employ 2D skeleton, 3D depth, or 3D mesh sequences as driving conditions.
  Audio-driven methods used to focus on portrait \cite{adnerf,GeneFace,zhang2023sadtalker, EMO, jiang2024loopy, hallo3, fada}. Despite some efforts \cite{VLogger, lin2024cyberhost, EchomimicV2, EMO2, diffted} to extend the frame to the full body, there are still challanges especially in hand quality. To bypass it, most approaches \cite{VLogger,  EchomimicV2, EMO2, diffted} adopt a two-stage hybrid driving strategy, utilizing gesture sequences as a strong condition to assist hand generation. CyberHost \cite{lin2024cyberhost} attempts to achieve one-stage audio-driven talking body generation through codebook design.
  Most notably, existing Human Animation methods typically focus on limited-scale datasets and limited-complexity structure, generally less than a thousand hours and 2B. Although FADA \cite{fada} employs a semi-supervised data strategy to utilize 1.4K hours of portrait videos, VLogger \cite{VLogger} meticulously collects 2.2K hours of half-body videos, and Hallo3 \cite{hallo3} initializes its weights derived from CogVideoX5B-I2V \cite{cogvideox}, their performance  does not exhibit the scaling law trends observed in other tasks such as LLMs \cite{ouyang2022training, touvron2023llama}, VLMs \cite{liu2024improved,bai2023qwen}, and T2I/T2V \cite{esser2024scaling, flux2023, kondratyuk2023videopoet}. Scaling effects in Human Animation  haven't been investigated effectively yet.

  

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/framework.jpg}
    \caption{\small \textbf{The framework of OmniHuman.} It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture and supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training strategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training allows the OmniHuman model to benefit from the scaling up of mixed data.}
    \label{fig:framework}
\end{figure*}
\section{Method}
\label{headings}

In this section, we introduce our framework, OmniHuman, which employs motion-related condition mixing during network training to scale up the training data. First, we provide an overview of the framework, including its inputs, outputs and key design elements. Next, we focus on the omni-conditions design, covering audio, pose, and reference conditions. We then detail the training strategy of OmniHuman, which leverages these omni-conditions for mixed data training, enabling the model to learn natural motion from large-scale datasets. Finally, we describe the implementation details for the inference phases of the OmniHuman model.

\subsection{Overview}
As illustrated in Figure~\ref{fig:framework}, our approach consists of two primary parts: the OmniHuman model, a multi-condition diffusion model and the Omni-Conditions Training Strategy. 
For model, The OmniHuman model begins with a pretrained Seaweed model~\cite{lin2025apt}, which uses MMDiT~\cite{sd3,dit} and is initially trained on general text-video pairs for text-to-video and text-to-image tasks. Given a reference image, the OmniHuman model aims to generate human videos using one or more driving signals including text, audio and pose. To achieve this, we employ various strategies to integrate frame-level audio features and pose heatmap features into the OmniHuman model. The detailed procedure is explained in the following subsections. OmniHuman model utilizes a causal 3DVAE~\cite{opensora} to project videos at their native size~\cite{navit} into a latent space and employs flow matching~\cite{lipman2022flow} as the training objective to learn the video denoising process.
We employ a three-stage mixed condition post-training approach to progressively transform the diffusion model from a general text-to-video model to a multi-condition human video generation model. As depicted on the left of Figure~\ref{fig:framework}, these stages sequentially introduce the driving modalities of text, audio, and pose according to their motion correlation strength, from weak to strong, and balance their training ratios.

\subsection{Omni-Conditions Designs}

  \textbf{Driving Conditions. }We adopted different approaches for injecting audio and pose conditions. Regarding audio condition, the wav2vec \cite{schneider2019wav2vec,baevski2020wav2vec2} model is employed to extract acoustic features, which are subsequently compressed using a MLP to align with the hidden size of MMDiT. The features of each frame are concatenated with the audio features from adjacent timestamps to generate audio tokens for the current frame. As depicted in Figure~\ref{fig:framework}, these audio tokens are injected into each block of MMDiT through cross-attention, enabling interaction between the audio tokens and the noisy latent representations.
  To incorporate pose condition, we use a pose guider to encode the driving pose heatmap sequence. The resulting pose features are concatenated with those of adjacent frames to acquire pose tokens. These pose tokens are then stacked with the noise latent along the channel dimension and fed into the unified multi-condition diffusion model for visual alignment and dynamic modeling. The text condition is retained as in the MMDiT text branch.


\textbf{Appearance Conditions. } The goal of OmniHuman is to generate video outputs that preserve both the subject’s identity and the background details from a reference image. To achieve this, previous research has proposed various strategies for injecting appearance representations into the denoising process. The most widely adopted approach involves using a reference network~\cite{jiang2024loopy,lin2024cyberhost,tian2024emo}, a parallel, trainable copy of the entire diffusion UNet or DiT that integrates with the self-attention layers of the original denoising Net. While effective at transferring appearance features to the denoising process, this method requires duplicating a full set of trainable parameters, which presents scalability challenges as model size increases.
To overcome this challenge, OmniHuman introduces a simple yet effective strategy for reference conditioning. Instead of constructing additional network modules, we reuse the original denoising DiT backbone to encode the reference image. Specifically, the reference image is first encoded into a latent representation using a VAE, and both the reference and noisy video latents are flattened into token sequences. These sequences are then packed together and simultaneously fed into the DiT, enabling the reference and video tokens to interact via self-attention across the entire network. To help the network distinguish between reference and video tokens, we modify the 3D Rotational Position Embeddings (RoPE)~\cite{rope} in the DiT by zeroing the temporal component for reference tokens, while leaving the RoPE for video tokens unchanged. This approach effectively incorporates appearance conditioning without adding extra parameters. In addition to the reference image, to support long video generation, we draw on previous methods by using motion frames~\cite{stypulkowski2024diffused}, concatenating their features with the noise features.

After introducing these conditions, the motion-related conditions now include text, reference image, audio, and pose. Text describes the current event, the reference image defines the range of motion, audio determines the rhythm of co-speech gestures, and pose specifies the exact motion. Their correlation strength with human motions can be considered to decrease in this order.

\subsection{Scaling up with Omni-Conditions Training}
Thanks to the multi-condition design, we can divide the model training into multiple tasks, including image and text to video, image and text, audio to video, and image and text, audio, pose to video. During training, different modalities are activated for different data, allowing a broader range of data to participate in the training process and enhancing the model's generation capabilities. 
After the conventional text-to-video pretraining phase, we follow two training principles for scaling up the conditioned human video generation task. \textbf{Principle 1}, stronger conditioned tasks can leverage weaker conditioned tasks and their corresponding data to achieve data scaling up during the model training process. Data excluded from audio and pose conditioned tasks due to filtering criteria like lip-sync accuracy, pose visibility, and stability can be used in text and image conditioned tasks, as they meet the standards for weaker conditions. 
Therefore, in the first stage 1, we drop the audio and pose conditions. \textbf{Principle 2}, the stronger the condition, the lower the training ratio that should be used. During training, stronger motion-related conditions, such as pose, generally train better than weaker conditions like audio due to less ambiguity. When both conditions are present, the model tends to rely on the stronger condition for motion generation, preventing the weaker condition from learning effectively. Therefore, we ensure that weaker conditions have a higher training ratio than stronger conditions. We construct stage 2 to drop only the pose condition, and in the final stage 3, use all conditions. Additionally, the training ratios for text, reference, audio, and pose are progressively halved. This approach assigns higher gradient weights to more challenging tasks and prevents overfitting to a single condition during overlapping condition training. Principle 1 allows us to significantly expand the training data, while Principle 2 ensures that the model fully utilizes the advantages of each motion-related condition during mixed conditions training and learns their motion generation capabilities. By combining Principles 1 and 2, OmniHuman can effectively train with mixed conditioned data, benefiting from data scaling up and achieving satisfactory results.



\subsection{Inference Strategies}
For audio-driven scenarios, all conditions except pose are activated. For pose-related combinations, all conditions are activated, but for pose-only driving, audio is disabled. Generally, when a condition is activated, all conditions with a lower motion-related influence are also activated unless unnecessary.
During inference, to balance expressiveness and computational efficiency, we apply classifier-free guidance (CFG)~\cite{cfg} specifically to audio and text across multiple conditions. However, we observed that an increased CFG results in pronounced wrinkles on the characters, whereas a decreased CFG compromises lip synchronization and motion expressiveness. To mitigate these issues, we propose a CFG annealing strategy that progressively reduces the CFG magnitude throughout the inference process, thereby significantly minimizing the appearance of wrinkles while ensuring that expressiveness.
OmniHuman is capable of producing video segments of arbitrary length within memory constraints based on the provided reference images and various driving signals. To ensure temporal coherence and identity consistency in long videos, the last five frames of the previous segment are utilized as motion frames.



\begin{figure*}
    \small
\captionsetup{type=table}
    \centering
        {
            \caption{\textbf{Quantitative comparisons with audio-conditioned portrait animation baselines.}}
  \label{Tab1}
  \centering
  \begin{tabular}{ccccccccccc}
    \toprule
     \multirow{2}{*}{Methods} & \multicolumn{5}{c}{CelebV-HQ} & \multicolumn{5}{c}{RAVDESS} \\ %  & \multicolumn{1}{c}{Average} \\
     % & \multirow{2}{*}{{FLOPs $\downarrow$}} \\
     \cmidrule(l){2-6}
     \cmidrule(l){7-11}
     % \cmidrule(l){12-12}
     &IQA $\uparrow$ & ASE$\uparrow$ & Sync-C$\uparrow$ & FID$\downarrow$ & FVD$\downarrow$ &IQA $\uparrow$ & ASE$\uparrow$ & Sync-C$\uparrow$ & FID$\downarrow$ & FVD$\downarrow$  \\ % & Rank$\downarrow$\\
    \midrule
         SadTalker \cite{zhang2023sadtalker} & 2.953 & 1.812 & 3.843 & 36.648 & 171.848 & 3.840 & 2.277 & 4.304 & 32.343 & 22.516  \\ % & 6/7/6/6/7  \\
         Hallo \cite{xu2024hallo} & 3.505 & 2.262 & 4.130 & 35.961 & 53.992 & 4.393 & 2.688 & 4.062 &19.826 & 38.471\\ %  & 3/3/5/3/4 \\
         VExpress \cite{wang2024v} & 2.946 & 1.901 &3.547 & 65.098 & 117.868 & 3.690 & 2.331 & 5.001 & 26.736 &  62.388 \\ % & 7/6/3/7/8  \\ 
         EchoMimic \cite{chen2024echomimic} & 3.307 & 2.128 & 3.136 & 35.373 & 54.715 & 4.504 & 2.742 & 3.292 & 21.058 & 54.115 \\ % & 4/4/7/4/5  \\
    Loopy \cite{jiang2024loopy} & 3.780 & 2.492 & 4.849 & 33.204 & 49.153 & 4.506 & 2.658 & 4.814 & 17.017 & 16.134 \\ % & 2/2/2/2/2 \\
    Hallo-3 \cite{hallo3} &
    3.451 & 2.257 & 3.933 & 38.481 & 42.125 & 4.006 & 2.462 & 4.448 & 28.840 & 26.029\\ %  & 5/5/4/5/3 \\
    
     OmniHuman & \textbf{3.875} & \textbf{2.656} & \textbf{5.199} & \textbf{31.435} & \underline{46.393} & \textbf{4.564} & \textbf{2.815} & \textbf{5.255} & \textbf{16.970} & \textbf{15.906} \\ % & 1/1/1/1/1 \\
  \bottomrule
  \end{tabular}
        }
        % \ffigbox[\FBwidth]
        {
            \caption{\textbf{Quantitative comparisons with audio-conditioned body animation baselines.}}
  \label{Tab2}
  \centering
  \begin{tabular}{cccccccc}
    \toprule
          Methods &IQA $\uparrow$ & ASE$\uparrow$ & Sync-C$\uparrow$ & FID$\downarrow$ & FVD$\downarrow$ &HKV $\uparrow$ & HKC$\uparrow$  \\
    \midrule
    DiffTED~\cite{diffted} & 2.701 & 1.703 & 0.926 & 95.455 & 58.871 & - & 0.769 \\
    DiffGest.~\cite{zhu2023taming}+MomicMo.~\cite{zhang2024mimicmotion} & 4.041 & 2.897 & 0.496 & 58.953 & 66.785  & 23.409  & 0.833  \\
    CyberHost~\cite{lin2024cyberhost} & 3.990 & 2.884 & 6.627  &  32.972 & 28.003 & 24.733 & 0.884 \\
    OmniHuman & \textbf{4.142} & \textbf{3.024} & \textbf{7.443} & \textbf{31.641} & \textbf{27.031} & \textbf{47.561} & \textbf{0.898} \\
  \bottomrule
  \end{tabular}
        }
        % \ffigbox[\FBwidth]
        {
\caption{\textbf{Subjective comparison of different training ratios for audio conditions.}}
  \label{Tab4}
  \centering
  \begin{tabular}{cccccc}
    \toprule
     Methods &Identity Consistency& Lip-sync Accuracy &  Visual Quality &  Action Diversity &  Overall \\
    \midrule
    $10\%$ Audio Training Ratio   &28.84 &11.59 &21.59 &11.59 &11.59 \\
    
    $50\%$ Audio Training Ratio  &\textbf{50.87} &\textbf{53.62} &\textbf{44.93} &\textbf{40.58}  &\textbf{69.57}\\
    $100\%$ Audio Training Ratio   &11.59 &30.43 &13.04 &36.23 &17.93  \\
    
    
  \bottomrule
  \end{tabular}
        }
\end{figure*}




\section{Experiments}
\subsection{Implementation Details}
  \textbf{Dataset.} By filtering based on aesthetics, image quality, motion amplitude, etc. (common criteria for video generation), we obtained 18.7K hours of human-related data for training. Of this, 13\% was selected using lipsync and pose visibility criteria, enabling audio and pose modalities. During training, the data composition was adjusted to fit the omni-condition training strategy. For testing, we conduct the evaluation following the portrait animation method Loopy \cite{jiang2024loopy} and the half-body animation method CyberHost \cite{lin2024cyberhost}. We randomly sampled 100 videos from public portrait datasets, including CelebV-HQ \cite{zhu2022celebv} (a diverse dataset with mixed scenes) and RAVDESS \cite {ravdess_dataset} (an indoor dataset including speech and song) as the testset for portrait animation. For half-body animation, we used CyberHost's test set, which includes a total of 269 body videos with 119 identities, encompassing different races, ages, genders, and initial poses.


  \textbf{Baselines.} To comprehensively evaluate OmniHuman's performance in different scenarios, we compare against portrait animation baselines including Sadtalker \cite{zhang2023sadtalker}, Hallo \cite{xu2024hallo}, Vexpress \cite{wang2024vexpress}, EchoMimic \cite{chen2024echomimic}, Loopy \cite{jiang2024loopy}, Hallo-3 \cite{hallo3}, and body animation baselines including DiffTED \cite{diffted}, DiffGest \cite{zhu2023taming} + Mimiction \cite{zhang2024mimicmotion}, CyberHost \cite{lin2024cyberhost}. 

  \textbf{Metrics.} For visual quality, FID \cite{heusel2017gans} and FVD \cite{unterthiner2019fvd} are used to evaluate the distance between the generated and labeled images and videos. We also leverage q-align \cite{wu2023q}, a VLM to evaluate the no-reference IQA(image quality) and ASE(aesthetics). For lip synchronism, we employ the widely-used Sync-C \cite{syncnet} to calculate the confidence between visual and audio content. Besides, HKC (hand keypoint confidence) \cite{lin2024cyberhost} and HKV (hand keypoint variance) \cite{lin2024cyberhost} are employed, to represent hand quality and motion richness respectively. 



\begin{figure*}[tbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/audio_ablation.pdf}
    \caption{\textbf{Ablation study on different audio condition ratios.} The models are trained with different audio ratios (top: 10\%, middle: 50\%, bottom: 100\%) and tested in an audio-driven setting with the same input image and audio.}
    \label{fig:audio_ratio}
\end{figure*}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/pose_ratio0.jpeg}
    \caption{\textbf{Ablation study on different pose condition ratios.} The models are trained with different pose ratios (top: 20\%, middle: 50\%, bottom: 80\%) and tested in an audio-driven setting with the same input image and audio.}
    \label{fig:pose_ratio0}
\end{figure*}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/pose_ratio1.jpeg}
    \caption{\textbf{Ablation study on different pose condition ratios.} The models are trained with different pose ratios (top: 20\%, middle: 50\%, bottom: 80\%) and tested in an audio-driven setting with the same input image and audio.}
    \label{fig:pose_ratio1}
\end{figure*}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/reference_ablation.pdf}
    % \vspace{-2mm}
    \caption{\small \textbf{Ablation study on reference condition ratios.} Comparisons of visualization results for 30s videos at different reference ratios.
}
    \label{fig:ref_ratio}

\end{figure*}


\subsection{Comparisons with Existing Methods}
As shown in the Table~\ref{Tab1} and \ref{Tab2}, overall, OmniHuman demonstrates superior performance compared to leading specialized models in both portrait and body animation tasks using a single model. For audio-driven animation, the generated results cannot be identical to the original video, especially when the reference image contains only a head. The model's varying preferences for motion styles across different scenarios complicate performance measurement using a single metric. By averaging the metrics across the dataset, OmniHuman achieves the best results across all evaluated metrics, reflecting its overall effectiveness. Additionally, OmniHuman excels across almost all metrics in specific datasets. Notably, existing methods use a single model for specific body proportions (portrait, half-body) with fixed input sizes and ratios. In contrast, OmniHuman supports various input sizes, ratios and body proportions with a single model, achieving satisfactory results. This advantage stems from its omni-conditions training, which learns from a large scale of diverse content and varying sizes during mixed data training.

\begin{figure*}[tbp]
        \centering
    \includegraphics[width=0.9\textwidth]{imgs/omni2.jpg}
    \caption{\small \textbf{The videos generated by OmniHuman based on input audio and images.} OmniHuman is compatible with stylized humanoid and 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.
}
    \label{fig:vis2}
\end{figure*}



\subsection{Ablation Studies on Omni-Conditions Training}
Here, we primarily analyze and explain principles 1 and 2 of the omni-condition training in OmniHuman. For the first principle, we compare training using only data that meets the requirements for audio and pose animation (i.e., 100\% audio training ratio) with training data for weaker conditions (i.e., text). 
Our experimental results demonstrate that the ratio of these two data parts significantly affects the final performance. From the visualizations in Figure~\ref{fig:audio_ratio}, it is evident that a high proportion of audio condition-specific data training reduces dynamic range and can cause failures with complex input images. Including weaker condition data at a 50\% ratio yields satisfactory results (e.g., accurate lip-syncing and natural motion). However, excessive weaker condition data can hinder training, resulting in poorer correlation with the audio. We also conducted a subjective evaluation to determine the optimal mix of these two data types during training. Specifically, we conducted a blind evaluation with 20 subjects who compared the samples across various dimensions to select the most satisfactory one, with an option for abstention. In total, 50 samples depicting diverse scenarios were evaluated. The results in Table~\ref{Tab4} were consistent with the conclusions drawn from the visualizations.
\par



The second principle can also be simultaneously validated with the principle 1 experiment, but we additionally conduct another experiment using different ratios of pose conditions to study the effects of pose condition ratios. Visual comparisons are presented in Figure~\ref{fig:pose_ratio0} and ~\ref{fig:pose_ratio1}. When the model is trained with a low pose condition ratio and tested with only audio conditions, the model tends to generate intense, frequent co-speech gestures, as is proven by the motion blur effects in the top row of Figure~\ref{fig:pose_ratio1} and the incorrect fingers in the top row of Figure~\ref{fig:pose_ratio0}. On the other hand, if we train the model with a high pose ratio, the model tends to rely on the pose condition to determine the human poses in the generated video. Consequently, given the input audio as the only driving signal, the generated results typically maintain a similar pose, as shown in the bottom rows of Figure~\ref{fig:pose_ratio0} and ~\ref{fig:pose_ratio1}. Therefore, we set the pose ratio to 50\% as our final training configuration. 
\par



Apart from analyzing the training ratios of new driving modalities in Stage 2 and Stage 3, the training ratio of the appearance condtion is equally important.
We investigated the impact of reference image ratios on the generation of 30-second videos through two experiments: (1) setting the reference image ratio to 70\%, lower than the text injection ratio but higher than audio; (2) setting the reference image ratio to 30\%, lower than the injection ratios for both audio and text. The comparative results are shown in Figure~\ref{fig:ref_ratio}, revealing that a lower reference ratio leads to more pronounced error accumulation, characterized by increased noise and color shifts in the background, degrading performance. In contrast, a higher reference ratio ensures better alignment of the generated output with the quality and details of the original image. This can be explained by the fact that when the reference image training ratio is lower than that of audio, the audio dominates the video generation, making it difficult to maintain the ID information from the reference image.



\subsection{Extended Visual Results}
In the Figure~\ref{fig:vis2} and Figure~\ref{fig:vis3}, we present more visual results to demonstrate OmniHuman's powerful capabilities in human animation, which are difficult to capture through metrics and comparisons with existing methods. OmniHuman is compatible with diverse input images and maintains the motion style of the input, such as preserving the characteristic mouth movements in anime. OmniHuman also excels in object interaction, generating videos of singing while playing different musical instruments and natural gestures while holding objects. Due to its compatibility with pose conditions during training, OmniHuman can perform pose-driven video generation or a combination of pose and audio-driven generation. More video samples can be seen on our project page (highly recommended).

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/omni3.jpg}
    \caption{\textbf{The videos generated by OmniHuman based on input audio and images.} These demonstrates OmniHuman's compatibility with various environments, objects, and camera angles, producing satisfactory results.}
    \label{fig:vis3}
\end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\textwidth]{imgs/omni4.jpg}
%     \caption{\small \textbf{The videos generated by OmniHuman based on input audio and images.} OmniHuman can generate highly realistic human motion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.}
%     \label{fig:vis4}
% \end{figure*}

\section{Conclusion}
We propose OmniHuman, an end-to-end multimodality-conditioned human video generation framework that generates human videos based on a single image and motion signals (e.g., audio, video, or both). OmniHuman employs a mixed data training strategy with multimodality motion conditioning, leveraging the scalability of mixed data to overcome the scarcity of high-quality data faced by previous methods. It significantly outperforms existing approaches, producing highly realistic human videos from weak signals, especially audio. OmniHuman supports images of any aspect ratio (portraits, half-body, or full-body) delivering lifelike, high-quality results across various scenarios.

\section*{Acknowledgments}
We thank Ceyuan Yang, Zhijie Lin, Yang Zhao, and Lu Jiang for their discussions and suggestions.


{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
