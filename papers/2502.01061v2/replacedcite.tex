\section{Related Works}
\subsection{Video Generation}
  In recent years, the advent of technologies such as diffusion models ____ has propelled the capabilities of generative models to a practically usable level. The latest advancements in image generation ____ produce results that are almost indistinguishable from reality. Consequently, a growing number of studies ____ are shifting their focus toward the field of video generation.
  Early text-to-video works primarily centered on training-free adaptations of pre-trained text-to-image models ____ or integrated temporal layers with fine-tuning on limited video datasets ____. However, due to the lack of extensive data, the video generation quality of these methods often remains unsatisfactory. To better exploit scaling laws and push the boundaries of video generation models, recent works ____ have optimized in three major areas. First, they have collected larger-scale, high-quality video datasets, with the data volume increasing to (O(100M)) clips of high-resolution videos. Second, they employ 3D Causal VAE ____ to compress both spatial and temporal features of video data, thereby enhancing video modeling efficiency. Third, the foundational model structure has transitioned from UNet to Transformer, improving the modelâ€™s scalability. Additionally, these works utilize meticulously designed progressive training recipes and datasets to maximize the model's potential. For example, ____ first pre-train on a large volume of low-resolution images and videos, leveraging data diversity to enhance the model's generalization capabilities. They then perform fine-tuning on a subset of high-resolution, high-quality data to improve the visual quality of generated videos. Large-scale data has significantly improved the effectiveness of general video generation. However, progress in the field of human animation synthesis remains relatively slow.
  
  \subsection{Human Animation}
  As an important task of video generation, Human Animation synthesizes human videos using human images and driving conditions such as audios or videos. Early  GAN-based methods ____ typically employ small datasets ____ consisting of tens of thousands of videos to achieve video-driven in a self-supervised manner. With the advancement of Diffusion models, several related works ____ have surpassed GAN-based methods in performance while using datasets of similar scale. Instead of using pixel-level videos, these methods employ 2D skeleton, 3D depth, or 3D mesh sequences as driving conditions.
  Audio-driven methods used to focus on portrait ____. Despite some efforts ____ to extend the frame to the full body, there are still challanges especially in hand quality. To bypass it, most approaches ____ adopt a two-stage hybrid driving strategy, utilizing gesture sequences as a strong condition to assist hand generation. CyberHost ____ attempts to achieve one-stage audio-driven talking body generation through codebook design.
  Most notably, existing Human Animation methods typically focus on limited-scale datasets and limited-complexity structure, generally less than a thousand hours and 2B. Although FADA ____ employs a semi-supervised data strategy to utilize 1.4K hours of portrait videos, VLogger ____ meticulously collects 2.2K hours of half-body videos, and Hallo3 ____ initializes its weights derived from CogVideoX5B-I2V ____, their performance  does not exhibit the scaling law trends observed in other tasks such as LLMs ____, VLMs ____, and T2I/T2V ____. Scaling effects in Human Animation  haven't been investigated effectively yet.

  

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/framework.jpg}
    \caption{\small \textbf{The framework of OmniHuman.} It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture and supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training strategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training allows the OmniHuman model to benefit from the scaling up of mixed data.}
    \label{fig:framework}
\end{figure*}