@inproceedings{Disco,
  title={Disco: Disentangled control for realistic human dance generation},
  author={Wang, Tan and Li, Linjie and Lin, Kevin and Zhai, Yuanhao and Lin, Chung-Ching and Yang, Zhengyuan and Zhang, Hanwang and Liu, Zicheng and Wang, Lijuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9326--9336},
  year={2024}
}

@inproceedings{aa,
  title={Animate anyone: Consistent and controllable image-to-video synthesis for character animation},
  author={Hu, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8153--8163},
  year={2024}
}

@inproceedings{adnerf,
  title={Ad-nerf: Audio driven neural radiance fields for talking head synthesis},
  author={Guo, Yudong and Chen, Keyu and Liang, Sen and Liu, Yong-Jin and Bao, Hujun and Zhang, Juyong},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={5784--5794},
  year={2021}
}

@article{bai2023qwen,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  volume={1},
  number={2},
  pages={3},
  year={2023}
}

@inproceedings{champ,
  title={Champ: Controllable and consistent human image animation with 3d parametric guidance},
  author={Zhu, Shenhao and Chen, Junming Leo and Dai, Zuozhuo and Dong, Zilong and Xu, Yinghui and Cao, Xun and Yao, Yao and Zhu, Hao and Zhu, Siyu},
  booktitle={European Conference on Computer Vision},
  pages={145--162},
  year={2025},
  organization={Springer}
}

@misc{chen2024pixartdelta,
      title={PIXART-Delta: Fast and Controllable Image Generation with Latent Consistency Models}, 
      author={Junsong Chen and Yue Wu and Simian Luo and Enze Xie and Sayak Paul and Ping Luo and Hang Zhao and Zhenguo Li},
      year={2024},
      eprint={2401.05252},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@inproceedings{diffted,
  title={DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures},
  author={Hogue, Steven and Zhang, Chenxu and Daruger, Hamza and Tian, Yapeng and Guo, Xiaohu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1922--1931},
  year={2024}
}

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{fada,
  title={FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation},
  author={Zhong, Tianyun and Liang, Chao and Jiang, Jianwen and Lin, Gaojie and Yang, Jiaqi and Zhao, Zhou},
  journal={arXiv preprint arXiv:2412.16915},
  year={2024}
}

@misc{flux2023,
    author={Black Forest Labs},
    title={FLUX},
    year={2023},
    howpublished={\url{https://github.com/black-forest-labs/flux}},
}

@article{guo2023animatediff,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}

@article{hallo3,
  title={Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks},
  author={Cui, Jiahao and Li, Hui and Zhan, Yun and Shang, Hanlin and Cheng, Kaihui and Ma, Yuqi and Mu, Shan and Zhou, Hang and Wang, Jingdong and Zhu, Siyu},
  journal={arXiv preprint arXiv:2412.00733},
  year={2024}
}

@article{hong2022cogvideo,
  title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  journal={arXiv preprint arXiv:2205.15868},
  year={2022}
}

@article{jiang2024loopy,
            title={Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency},
            author={Jiang, Jianwen and Liang, Chao and Yang, Jiaqi and Lin, Gaojie and Zhong, Tianyun and Zheng, Yanbo},
            journal={arXiv preprint arXiv:2409.02634},
            year={2024}
          }

@article{jiang2024mobileportrait,
  title={Mobileportrait: Real-time one-shot neural head avatars on mobile devices},
  author={Jiang, Jianwen and Lin, Gaojie and Rong, Zhengkun and Liang, Chao and Zhu, Yongming and Yang, Jiaqi and Zhong, Tianyun},
  journal={arXiv preprint arXiv:2407.05712},
  year={2024}
}

@inproceedings{jonathan2020ddpm,
    author      = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
    booktitle   = {Advances in Neural Information Processing Systems},
    editor      = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages       = {6840--6851},
    publisher   = {Curran Associates, Inc.},
    title       = {Denoising Diffusion Probabilistic Models},
    url         = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
    volume      = {33},
    year        = {2020}
}

@article{karras2022edm,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={26565--26577},
  year={2022}
}

@article{kondratyuk2023videopoet,
  title={Videopoet: A large language model for zero-shot video generation},
  author={Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and others},
  journal={arXiv preprint arXiv:2312.14125},
  year={2023}
}

@article{kong2024hunyuanvideo,
  title={HunyuanVideo: A Systematic Framework For Large Video Generative Models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603},
  year={2024}
}

@article{lin2024cyberhost,
            title={CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention},
            author={Lin, Gaojie and Jiang, Jianwen and Liang, Chao and Zhong, Tianyun and Yang, Jiaqi and Zheng, Yanbo},
            journal={arXiv preprint arXiv:2409.01876},
            year={2024}
          }

@article{liu2022reflow,
    title   = {Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
    author  = {Xingchao Liu and Chengyue Gong and Qiang Liu},
    journal = {ArXiv},
    year    = {2022},
    volume  = {abs/2209.03003},
    url     = {https://api.semanticscholar.org/CorpusID:252111177}
}

@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@article{nagrani2017voxceleb,
  title={VoxCeleb: a large-scale speaker identification dataset},
  author={Nagrani, A and Chung, J and Zisserman, A},
  journal={Interspeech 2017},
  year={2017},
  publisher={ISCA}
}

@article{openai2024sora,
  author = {Tim, Brooks and Bill, Peebles and Connorm Holmes and Will, DePue and Yufeim Guo and Li, Jing and David, Schnurr and Joe, Taylor and Troy, Luhman and Eric, Luhman and Clarence, Ng and Ricky, Wang and Aditya, Ramesh},
  title = {Video generation models as world simulators},
  year = {2024},
  note = {Accessed: 2024-02-15},
  url = {https://openai.com/index/sora/}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{polyak2024moviegen,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{qi2023fatezero,
        title={FateZero: Fusing Attentions for Zero-shot Text-based Video Editing}, 
        author={Chenyang Qi and Xiaodong Cun and Yong Zhang and Chenyang Lei and Xintao Wang and Ying Shan and Qifeng Chen},
        year={2023},
        journal={arXiv:2303.09535},
}

@inproceedings{sd3,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{shao2024human4dit,
  title={Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer},
  author={Shao, Ruizhi and Pang, Youxin and Zheng, Zerong and Sun, Jingxiang and Liu, Yebin},
  journal={arXiv preprint arXiv:2405.17405},
  year={2024}
}

@article{siarohin2019fomm,
  title={First order motion model for image animation},
  author={Siarohin, Aliaksandr and Lathuili{\`e}re, St{\'e}phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{siarohin2021mraa,
  title={Motion representations for articulated animation},
  author={Siarohin, Aliaksandr and Woodford, Oliver J and Ren, Jian and Chai, Menglei and Tulyakov, Sergey},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13653--13662},
  year={2021}
}

@article{singer2022make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@inproceedings{song2021ddim,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{wang2021facev2v,
  title={One-shot free-view neural talking-head synthesis for video conferencing},
  author={Wang, Ting-Chun and Mallya, Arun and Liu, Ming-Yu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10039--10049},
  year={2021}
}

@article{wang2023modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}

@inproceedings{wu2023tune,
    title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
    author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={7623--7633},
    year={2023}
}

@inproceedings{xie2022vfhq,
  title={Vfhq: A high-quality dataset and benchmark for video face super-resolution},
  author={Xie, Liangbin and Wang, Xintao and Zhang, Honglun and Dong, Chao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={657--666},
  year={2022}
}

@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@article{yu20233DVAE,
  title={Language Model Beats Diffusion--Tokenizer is Key to Visual Generation},
  author={Yu, Lijun and Lezama, Jos and Gundavarapu, Nitesh B and Versari, Luca and Sohn, Kihyuk and Minnen, David and Cheng, Yong and Birodkar, Vighnesh and Gupta, Agrim and Gu, Xiuye and others},
  journal={arXiv preprint arXiv:2310.05737},
  year={2023}
}

@inproceedings{zeng2024pxldance,
  title={Make pixels dance: High-dynamic video generation},
  author={Zeng, Yan and Wei, Guoqiang and Zheng, Jiani and Zou, Jiaxin and Wei, Yang and Zhang, Yuchen and Li, Hang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8850--8860},
  year={2024}
}

@inproceedings{zhang2023sadtalker,
  title={Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation},
  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8652--8661},
  year={2023}
}

@article{zhang2024mimicmotion,
  title={Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance},
  author={Zhang, Yuang and Gu, Jiaxi and Wang, Li-Wen and Wang, Han and Cheng, Junqi and Zhu, Yuefeng and Zou, Fangyuan},
  journal={arXiv preprint arXiv:2406.19680},
  year={2024}
}

@inproceedings{zhao2022tps,
  title={Thin-plate spline motion model for image animation},
  author={Zhao, Jian and Zhang, Hui},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3657--3666},
  year={2022}
}

@article{zhou2022magicvideo,
  title={Magicvideo: Efficient video generation with latent diffusion models},
  author={Zhou, Daquan and Wang, Weimin and Yan, Hanshu and Lv, Weiwei and Zhu, Yizhe and Feng, Jiashi},
  journal={arXiv preprint arXiv:2211.11018},
  year={2022}
}

@inproceedings{zhu2022celebv,
  title={CelebV-HQ: A large-scale video facial attributes dataset},
  author={Zhu, Hao and Wu, Wayne and Zhu, Wentao and Jiang, Liming and Tang, Siwei and Zhang, Li and Liu, Ziwei and Loy, Chen Change},
  booktitle={European conference on computer vision},
  pages={650--667},
  year={2022},
  organization={Springer}
}

