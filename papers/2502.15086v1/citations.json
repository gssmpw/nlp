[
  {
    "index": 0,
    "papers": [
      {
        "key": "ganguli2022red",
        "author": "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others",
        "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      },
      {
        "key": "zhang2023safetybench",
        "author": "Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie",
        "title": "Safetybench: Evaluating the safety of large language models with multiple choice questions"
      },
      {
        "key": "zhuo2023red",
        "author": "Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang",
        "title": "Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity"
      },
      {
        "key": "sun2023safety",
        "author": "Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie",
        "title": "Safety assessment of chinese large language models"
      },
      {
        "key": "zhang2024agent",
        "author": "Zhang, Zhexin and Cui, Shiyao and Lu, Yida and Zhou, Jingzhuo and Yang, Junxiao and Wang, Hongning and Huang, Minlie",
        "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents"
      },
      {
        "key": "xie2024sorry",
        "author": "Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and others",
        "title": "Sorry-bench: Systematically evaluating large language model safety refusal behaviors"
      },
      {
        "key": "tedeschi2024alert",
        "author": "Tedeschi, Simone and Friedrich, Felix and Schramowski, Patrick and Kersting, Kristian and Navigli, Roberto and Nguyen, Huu and Li, Bo",
        "title": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ganguli2022red",
        "author": "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others",
        "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2023safetybench",
        "author": "Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie",
        "title": "Safetybench: Evaluating the safety of large language models with multiple choice questions"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xie2024sorry",
        "author": "Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and others",
        "title": "Sorry-bench: Systematically evaluating large language model safety refusal behaviors"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2024agent",
        "author": "Zhang, Zhexin and Cui, Shiyao and Lu, Yida and Zhou, Jingzhuo and Yang, Junxiao and Wang, Hongning and Huang, Minlie",
        "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents"
      },
      {
        "key": "hua2024trustagent",
        "author": "Hua, Wenyue and Yang, Xianjun and Jin, Mingyu and Li, Zelong and Cheng, Wei and Tang, Ruixiang and Zhang, Yongfeng",
        "title": "Trustagent: Towards safe and trustworthy llm-based agents through agent constitution"
      },
      {
        "key": "yin2024safeagentbench",
        "author": "Yin, Sheng and Pang, Xianghe and Ding, Yuanzhuo and Chen, Menglan and Bi, Yutong and Xiong, Yichen and Huang, Wenhao and Xiang, Zhen and Shao, Jing and Chen, Siheng",
        "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2023safetybench",
        "author": "Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie",
        "title": "Safetybench: Evaluating the safety of large language models with multiple choice questions"
      },
      {
        "key": "xie2024sorry",
        "author": "Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and others",
        "title": "Sorry-bench: Systematically evaluating large language model safety refusal behaviors"
      },
      {
        "key": "tedeschi2024alert",
        "author": "Tedeschi, Simone and Friedrich, Felix and Schramowski, Patrick and Kersting, Kristian and Navigli, Roberto and Nguyen, Huu and Li, Bo",
        "title": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ganguli2022red",
        "author": "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others",
        "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ganguli2022red",
        "author": "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others",
        "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jang2023personalized",
        "author": "Jang, Joel and Kim, Seungone and Lin, Bill Yuchen and Wang, Yizhong and Hessel, Jack and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Choi, Yejin and Ammanabrolu, Prithviraj",
        "title": "Personalized soups: Personalized large language model alignment via post-hoc parameter merging"
      },
      {
        "key": "cheng2023everyone",
        "author": "Cheng, Pengyu and Xie, Jiawen and Bai, Ke and Dai, Yong and Du, Nan",
        "title": "Everyone deserves a reward: Learning customized human preferences"
      },
      {
        "key": "wu2023fine",
        "author": "Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh",
        "title": "Fine-grained human feedback gives better rewards for language model training"
      },
      {
        "key": "li2024personalized",
        "author": "Li, Xinyu and Zhou, Ruiyang and Lipton, Zachary C and Leqi, Liu",
        "title": "Personalized language modeling from personalized human feedback"
      },
      {
        "key": "chakraborty2024maxmin",
        "author": "Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Huang, Furong and Manocha, Dinesh and Bedi, Amrit and Wang, Mengdi",
        "title": "Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences"
      },
      {
        "key": "lee2024aligning",
        "author": "Lee, Seongyun and Park, Sue Hyun and Kim, Seungone and Seo, Minjoon",
        "title": "Aligning to thousands of preferences via system message generalization"
      },
      {
        "key": "zhang2024personalization",
        "author": "Zhang, Zhehao and Rossi, Ryan A and Kveton, Branislav and Shao, Yijia and Yang, Diyi and Zamani, Hamed and Dernoncourt, Franck and Barrow, Joe and Yu, Tong and Kim, Sungchul and others",
        "title": "Personalization of large language models: A survey"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lee2024aligning",
        "author": "Lee, Seongyun and Park, Sue Hyun and Kim, Seungone and Seo, Minjoon",
        "title": "Aligning to thousands of preferences via system message generalization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "vijjini2024exploring",
        "author": "Vijjini, Anvesh Rao and Chowdhury, Somnath Basu Roy and Chaturvedi, Snigdha",
        "title": "Exploring Safety-Utility Trade-Offs in Personalized Language Models"
      }
    ]
  }
]