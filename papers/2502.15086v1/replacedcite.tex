\section{Related Work}
\label{sec:related}

\noindent \textbf{LLM Safety Benchmark.} \@ 
With increasing attention on the safety of LLM agents, recent studies ____ have introduced various instruction datasets designed to assess the safety of modern instruction-following LLMs by eliciting potentially unsafe behavior. ____ pioneered this effort by leveraging red team members to generate diverse and creative instructions, forming one of the most foundational safety evaluation datasets. Many subsequent works have expanded on such foundations by unifying and reorganizing existing datasets or refining safety risk taxonomies. For example, SafetyBench ____ converts existing instructions into multiple-choice questions to enable easy-to-use and efficient evaluation. SorryBench ____ introduces a more fine-grained safety risk taxonomy than prior works and collects questions that responsible LLM agents should decline to answer. Beyond assessing the safety of QA agents—ensuring LLMs do not generate unsafe responses to questions—recent research ____ has also emphasized the safety of autonomous LLM agents, evaluating how safely they interact within various environments. 

Despite extensive research in this area, no existing work addresses user-specific safety, leaving a critical gap in safety evaluation benchmarks and limiting the real-world applicability of LLM agents. In this work, we introduce \proposed~, a novel benchmark system for assessing user-specific safety in terms of both QA and autonomous agent tasks.


% These datasets are curated by building upon prior works ____ or by leveraging red team members to generate diverse and creative instructions ____. 

% With growing attention on the safety of LLM agents, researchers have introduced numerous benchmarks  to assess various aspects of LLM safety, such as toxicity, harmfulness, trustworthiness, and refusal behaviors. ____ curates a red-teaming instruction dataset that might trigger unsafe behavior of LLM agents. 

% addresses the red-teaming instructions  are the most foundational safety evaluation datasets. Recently, many works, such as SafetyBench, Chinese blah, Do-Not-Answer, SorryBench,  redefine the safety taxonomy to be more finegrained and comprehensive based on the foundational red-teaming datasets. Moreover, 

% \noindent \textbf{LLM Safety Alignment.} \@ Do I need to write this section?

\noindent \textbf{LLM Personalization.} \@  As LLM agents are adopted by diverse users, developing models that adapt to individual preferences and characteristics has become essential, prompting numerous studies on LLM personalization via post-training ____. For instance, ____ introduced a novel post-training approach that integrates user-specific preferences via system messages, allowing LLM agents to better align with individual user preferences. While these studies focus on enhancing personalization to match user-specific preferences, they largely overlook the need to personalize safety considerations to individual users, i.e., user-specific safety. Meanwhile, ____ found that the safety and helpfulness of LLM responses vary significantly given user demographics and proposed strategies to mitigate these discrepancies. While it highlights the influence of the given user information on LLM responses, it does not account for the fact that safety standards can also vary depending on user profiles.

To bridge this gap, in this paper, we explore the safety standards within the context of personalization and introduce a novel safety concept, user-specific safety.
% introduce \proposed~to explore the user-specific safety of LLM agents, which is  as a novel safety paradigm within the context of personalization.