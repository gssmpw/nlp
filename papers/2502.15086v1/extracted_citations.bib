@inproceedings{chakraborty2024maxmin,
  title={Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences},
  author={Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Huang, Furong and Manocha, Dinesh and Bedi, Amrit and Wang, Mengdi},
  booktitle={ICML 2024 Workshop on Models of Human Feedback for AI Alignment},
  year={2024}
}

@article{cheng2023everyone,
  title={Everyone deserves a reward: Learning customized human preferences},
  author={Cheng, Pengyu and Xie, Jiawen and Bai, Ke and Dai, Yong and Du, Nan},
  journal={arXiv preprint arXiv:2309.03126},
  year={2023}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@inproceedings{hua2024trustagent,
  title={Trustagent: Towards safe and trustworthy llm-based agents through agent constitution},
  author={Hua, Wenyue and Yang, Xianjun and Jin, Mingyu and Li, Zelong and Cheng, Wei and Tang, Ruixiang and Zhang, Yongfeng},
  booktitle={Trustworthy Multi-modal Foundation Models and AI Agents (TiFA)},
  year={2024}
}

@article{jang2023personalized,
  title={Personalized soups: Personalized large language model alignment via post-hoc parameter merging},
  author={Jang, Joel and Kim, Seungone and Lin, Bill Yuchen and Wang, Yizhong and Hessel, Jack and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Choi, Yejin and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2310.11564},
  year={2023}
}

@article{lee2024aligning,
  title={Aligning to thousands of preferences via system message generalization},
  author={Lee, Seongyun and Park, Sue Hyun and Kim, Seungone and Seo, Minjoon},
  journal={arXiv preprint arXiv:2405.17977},
  year={2024}
}

@article{li2024personalized,
  title={Personalized language modeling from personalized human feedback},
  author={Li, Xinyu and Zhou, Ruiyang and Lipton, Zachary C and Leqi, Liu},
  journal={arXiv preprint arXiv:2402.05133},
  year={2024}
}

@article{sun2023safety,
  title={Safety assessment of chinese large language models},
  author={Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2304.10436},
  year={2023}
}

@article{tedeschi2024alert,
  title={ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming},
  author={Tedeschi, Simone and Friedrich, Felix and Schramowski, Patrick and Kersting, Kristian and Navigli, Roberto and Nguyen, Huu and Li, Bo},
  journal={arXiv preprint arXiv:2404.08676},
  year={2024}
}

@article{vijjini2024exploring,
  title={Exploring Safety-Utility Trade-Offs in Personalized Language Models},
  author={Vijjini, Anvesh Rao and Chowdhury, Somnath Basu Roy and Chaturvedi, Snigdha},
  journal={arXiv preprint arXiv:2406.11107},
  year={2024}
}

@article{wu2023fine,
  title={Fine-grained human feedback gives better rewards for language model training},
  author={Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={59008--59033},
  year={2023}
}

@article{xie2024sorry,
  title={Sorry-bench: Systematically evaluating large language model safety refusal behaviors},
  author={Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and others},
  journal={arXiv preprint arXiv:2406.14598},
  year={2024}
}

@article{yin2024safeagentbench,
  title={SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents},
  author={Yin, Sheng and Pang, Xianghe and Ding, Yuanzhuo and Chen, Menglan and Bi, Yutong and Xiong, Yichen and Huang, Wenhao and Xiang, Zhen and Shao, Jing and Chen, Siheng},
  journal={arXiv preprint arXiv:2412.13178},
  year={2024}
}

@article{zhang2023safetybench,
  title={Safetybench: Evaluating the safety of large language models with multiple choice questions},
  author={Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2309.07045},
  year={2023}
}

@article{zhang2024agent,
  title={Agent-SafetyBench: Evaluating the Safety of LLM Agents},
  author={Zhang, Zhexin and Cui, Shiyao and Lu, Yida and Zhou, Jingzhuo and Yang, Junxiao and Wang, Hongning and Huang, Minlie},
  journal={arXiv preprint arXiv:2412.14470},
  year={2024}
}

@article{zhang2024personalization,
  title={Personalization of large language models: A survey},
  author={Zhang, Zhehao and Rossi, Ryan A and Kveton, Branislav and Shao, Yijia and Yang, Diyi and Zamani, Hamed and Dernoncourt, Franck and Barrow, Joe and Yu, Tong and Kim, Sungchul and others},
  journal={arXiv preprint arXiv:2411.00027},
  year={2024}
}

@article{zhuo2023red,
  title={Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv preprint arXiv:2301.12867},
  year={2023}
}

