\section{Experiments} \label{sec:experiments}

We perform a comprehensive evaluation of our proposed method across both reasoning-intensive as well as structured application-specific tasks using pre-trained models of multiple scales. 

\textbf{Datasets.} \label{all_tasks} To assess reasoning capabilities, we adopt a comprehensive strategy by training on the instruction-tuning dataset Alpaca~\cite{touvron2023alpaca} and evaluating performance across multiple held-out human instruction test sets, including Koala~\cite{koala2023}, Self-Instruct~\cite{wang2022selfinstruct}, WizardLM~\cite{xu2023wizardlm}, and MT Bench~\cite{bai2024mtbench}. These datasets encompass a wide spectrum of instruction styles, task complexities, and domain-specific reasoning challenges, such as multi-turn interactions (Koala), open-ended problem-solving (Self-Instruct), step-by-step reasoning (WizardLM), and multi-dimensional evaluation of instruction-following capabilities (MT Bench). Beyond reasoning-oriented tasks, we further evaluate our approach on structured application-specific tasks, including Structured API Generation, Text Summarization, and Meaning Representation. For Structured API Generation, we leverage the sql-create-context dataset, which is constructed using WikiSQL~\cite{zhongSeq2SQL2017} and SPIDER~\cite{yu2018spider}; for Text Summarization, we utilize Dialogsum~\cite{chen-etal-2021-dialogsum}; and for Meaning Representation, we employ the e2e-nlg dataset~\cite{dusek_etal2020_csl}. These tasks assess the model's ability to produce well-formed outputs for practical AI assistant applications, ensuring both efficiency and applicability in real-world deployment.  



\textbf{Models and Baselines.} We evaluate our proposed approach on open-source, dense transformer models of varying scales, including Phi-3-mini-4k-instruct (3.8B) \cite{phi3_report} and Gemma (7B) \cite{gemma2024}, as well as the recently introduced sparse Mixture-of-Experts (MoE) model, OlMoE (1B-7B) \cite{muennighoff2024olmoe}. To benchmark our method in dynamic compute scenarios, we compare it against several state-of-the-art dynamic compute techniques, including LITE \cite{varshney-etal-2024-investigating}, an extension of CALM \cite{schuster2022confident}, as well as skip decoding \cite{delcorro2023skipdecode} and Mixture of Depths (MoD) \cite{raposo2024mixture}. For speculative decoding scenarios, we use both standard draft-target speculative decoding method ~\cite{spector2023accelerating} and single-model baselines such as Medusa \cite{medusa}, DEED  \cite{Tang2024} and LookAhead Decoding ~\cite{fu2023lookahead}. To assess the performance of our approach in MoE configurations with Ahead-of-Time (AoT) expert loading, we conduct comparisons using several baselines. These include fixed expert configurations, LRU-based caching, LRU caching combined with random expert speculation, and expert prediction based on previous hidden states.

\textbf{Metrics} We report the trade-off between wall-time speedups and generation quality metrics on the held-out test sets to compare dynamic compute approaches. In contrast, for evaluating speculative decoding setups, we focus on wall-time speedups and acceptance rates, as speculative decoding does not affect generation quality ~\cite{spector2023accelerating}. To assess the effectiveness of our method in Ahead-of-Time (AoT) MoE expert loading, we report both expert speculation hit rate and decoding latency.

For reasoning-oriented tasks, we evaluate response quality on human instruction test sets such as Self-Instruct~\cite{wang2022selfinstruct}, Koala~\cite{geng2023koala}, WizardLM~\cite{xu2023wizardlm}, and MT Bench~\cite{zheng2023judging} using GPT-4~\cite{openai2023gpt4}. To mitigate position bias in LLM judgments~\cite{wang2022selfinstruct}, we assess response pairs in both possible orderings and aggregate the judgment scores. The prompt used for comparing response quality between models is provided in~\cref{m2r2_prompt}.  For structured application-specific tasks, we use Exact Match (EM) accuracy as the generation metric for the Structured Query task, while for Dialog Summarization, we employ the Rouge-LSum metric~\cite{wolf2020transformers}.  




\textbf{Inference.} All inference experiments were conducted on a single Nvidia A100-80GB GPU, with a batch size of 1, using float16 precision and greedy decoding (temperature T = 0) to emulate common configurations for on-device AI assistants.

% For additional analyses, please refer to \cref{sec:batching} for the impact of batching, \cref{supp_ablations} for ablation studies on top-k sampling and temperature \(T = 1\), and \cref{supp_expt_details} for further experimental details.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=4cm]{sections/figures/alignment_koala.png}
        \caption{Early alignment performance on the Koala test set, comparing traditional early exiting with M2R2, both with and without cache sharing between slow and accelerated residuals.}
        \label{fig:koala_alignment}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=4cm]{sections/figures/parameter_overhead_dynamic_compute_mode.png}
        \caption{Parameter overhead across different dynamic compute approaches, highlighting additional trainable parameters in routers, projection layers, accelerator adapters, and ARLA.}
        \label{fig:parameter_overhead}
    \end{subfigure}
    \caption{Alignment of early exited tokens and trainable parameter overhead associated with different dynamic computing approaches.}
    \label{fig:side_by_side}
\end{figure}





\subsection{Results}

\subsubsection{Dynamic Residual Transformation}
\textbf{Early Alignment and ARLA Effectiveness} \label{early_alignment}
We begin by analyzing the alignment of tokens exited at intermediate gates with those exited at the final layer using the Koala instruction set~\cite{koala2023}. As shown in \cref{fig:koala_alignment}, we observe that accelerated residuals achieve significantly higher alignment compared to conventional early exit approaches, such as those proposed in~\cite{schuster2022confident, chen2023eellm, varshney-etal-2024-investigating}. This difference in alignment is particularly pronounced at lower gates, demonstrating that accelerated residual streams more effectively capture the features of the final-layer slow residual stream than applying a projection layer on intermediate slow residuals. Additionally, we find that sharing the KV cache between slow and accelerated residuals does not significantly impact alignment. Cache sharing allows for substantial reductions in runtime memory, and in the subsequent experiments detailed in this section, we share the cache between slow and accelerated residual streams. We also compare ROC curves obtained from confidence scores that are used to make exiting decisions in \cite{schuster2022confident} and our approach. As observed in \cref{fig:roc_arla}, ARLA described in \cref{method_arla}, is consistently effective in  optimally determining decision boundaries than classifier-based routers that operate on latest slow residual state at each gate in \cite{schuster2022confident, varshney-etal-2024-investigating}. 

\begin{figure}[ht]
    \centering
    % Row 1
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/dialog_sumv2.pdf}
        \caption{\small Dialog Summarization}
        \label{fig:ea_dialog}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/meaning_representation.pdf}
        \caption{\small E2E-NLG}
        \label{fig:ea_meaning}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/sql.pdf}
        \caption{\small SQL Generation}
        \label{fig:ea_sql_generation}
    \end{subfigure}

    % Row 2
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/self_instructv2.pdf}
        \caption{\small Self Instruct}
        \label{fig:ea_self_instruct}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/wizardv2.pdf}
        \caption{\small WizardLM}
        \label{fig:ea_wizardlm}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/koalav2.pdf}
        \caption{\small Koala}
        \label{fig:ea_koala}
    \end{subfigure}
    
    \caption{Generation Metric vs Speedup trade-off of different dynamic computing approaches with Phi-3 model on instruction and application specific test sets.}
    \label{fig:ea_comparison}
\end{figure}


 \textbf{Dynamic compute speedups}
We evaluated both LITE ~\cite{varshney-etal-2024-investigating} and our approach with various early exiting thresholds, examining generation metrics across a wide range of speedup trade-offs. For Mixture of Depths~\cite{raposo2024mixture}, we conducted experiments with varying layer capacities to assess generation performance as a function of layer capacity and in turn speedup. In the case of Skip Decoding ~\cite{delcorro2023skipdecode}, we adapted exit points according to sequence length to achieve similar trade-offs between generation performance and speedup. 
 Benefits of better alignment (see ~\cref{fig:koala_alignment}) and decision boundaries (see ~\cref{fig:roc_arla}) translate into significantly better generation metrics vs speedup trade-offs in case of M2R2 relative to other approaches as observed in ~\cref{fig:ea_comparison}. This trend is also consistent in instruction tuning setup where our approach achieves better generation metrics to speedup trade-off across all instruction test sets. 
 Interestingly we observe that approaches that are shown to perform well during pre-training such as Mixture of Depths ~\cite{raposo2024mixture} and Skip decoding ~\cite{delcorro2023skipdecode} tend to perform poorly during supervised fine-tuning and instruction-tuning setups. In ~\cref{mod_discountinuity} we provide some empirical reasoning that may lead to this suboptimal behavior. We also measured trainable parameter overhead associated with routers, projection layers and ARLA mechanism described in ~\cref{method_arla} of all approaches to evaluate inference feasibility on resource constrained devices as shown in ~\cref{fig:parameter_overhead}. Our approach uses significantly less parameters since accelerator adapters work fairly well with lower ranks (see ~\cref{fig:adapter_rank_ablation}), while ARLA latent dimesion is substantially low (see ~\cref{method_arla}). Parameter overhead of LITE ~\cite{varshney-etal-2024-investigating} and Skip Decode ~\cite{delcorro2023skipdecode} approaches is dominated by projection layer while that of MoD ~\cite{raposo2024mixture} is dominated by router parameters which include a linear projection and binary classifier at each layer. 

% Furthermore, we note that on compute bound hardware settings where flops are a bottleneck, training change described in \cite{method_training} enables our approach to achieve better flops to speedup trade-off despite consuming more flops per layer demonstrating that benefits of better alignment outweigh flop overhead per layer required accelerate residual streams and obtain better alignment. 


\begin{figure}[ht!]
    \centering
    % First subfigure
    \begin{subfigure}{0.52\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=4cm]{sections/figures/spec_decode_results_revised.pdf}
        \caption{Acceptance rates and speedups of different speculative decoding architectures on Nvidia-A100.}
        \label{fig:sd_speedup}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=4cm]{sections/figures/sd_param_overhead.pdf}
        \caption{Overhead of trainable parameters of different speculative decoding architectures.}
        \label{fig:sd_overhead}
    \end{subfigure}
    \caption{Effectiveness of M2R2 in Speculative Decoding settings.}
    \label{fig:sd_results}
\end{figure}


\subsection{Speculative Decoding} Improved early alignment significantly boosts acceptance rates when using accelerated residual streams for speculative candidate sampling in speculative decoding settings. As shown in \cref{fig:sd_results}, we compare acceptance rates by sampling tokens from the first $k = 4$ layers of Gemma-7B ~\cite{gemma2024} with our approach, which employs a residual transformation rate $R = N/k$, where $N$ denotes number of layers of base model. Baselines include draft-model-based speculative decoding \cite{leviathan2023fast, chen2023accelerating} that utilizes Gemma-2B \cite{gemma2024} as the draft model, as well as single-model methods such as Medusa \cite{medusa},  DEED \cite{Tang2024} and LookAhead Decoding ~\cite{fu2023lookahead}. For these experiments, we fix the candidate speculation length $\gamma = 3$ and set $k = 4$.

While aligned draft model in 2-model setup achieves high acceptance rates, it does not yield generation speedups on A100 GPUs; the latency of speculative candidate generation outweighs the benefits of parallel acceptance for multiple tokens. In contrast, Medusa performs candidate speculation in a non-autoregressive (NAR) manner, minimizing speculation overhead; however, the acceptance rates are suboptimal due to the lack of dependency between generated speculative tokens \cite{hydra, bhendawade2024speculative}. The method proposed in DEED \cite{Tang2024} generates candidate speculations using only a few layers, making each token dependent on the previous, which incurs low candidate generation costs since only a subset of the model layers is engaged. Our approach addresses lower acceptance rates of DEED ~\cite{Tang2024} by leveraging accelerated residuals, which improve both alignment and generation speedups. Furthermore, as shown in \cref{fig:sd_overhead}, parameter overhead of our approach for speculative draft generation is substantially lower than other baselines  making it an ideal approach for resource-constrained scenarios. 


 

% \begin{figure}[h!]
%     \centering
%     % First subfigure
%     \begin{subfigure}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{sections/figures/moe_hit_rate_koala.png}
%         \caption{Expert speculation hit rate on Koala}
%         \label{fig:subfigure1}
%     \end{subfigure}
%     \hfill
%     % Second subfigure
%     \begin{subfigure}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{sections/figures/moe_inference_time_per_token_koala.png}
%         \caption{MoE inference time per step on Koala}
%         \label{fig:subfigure2}
%     \end{subfigure}
%     \caption{Overall caption for both images}
%     \label{fig:mainfigure}
% \end{figure}



\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/moe_hit_rate_mt_bench.png}
        \caption{Expert speculation hit rate of different expert speculation strategies on measured on MT Bench.}
        \label{fig:subfigure1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/moe_aot_inference_time_mt_bench.png}
        \caption{Inference latency per decode step using different expert pre-loading strategies measured on MT Bench.}
        \label{fig:subfigure2}
    \end{subfigure}
    \caption{Effectiveness of M2R2 for speculative expert pre-loading on sparse MoE Transformers }
    \label{fig:MoE_mt_bench}
\end{figure}

%  \subsection{AoT MoE Expert Loading} We evaluate the efficacy of Ahead-of-Time (AoT) expert loading in resource-constrained scenarios where not all experts can be stored in high-bandwidth memory (HBM). If the MoE gate selects an expert not residing in HBM, it must be loaded from low-bandwidth memory (LBM). To simulate this on an A100 GPU, we limit HBM occupancy to 16GB, requiring experts beyond this limit to be loaded from LBM. While our experiments use GPU DRAM as HBM and disk as LBM, the setup is applicable when SRAM is HBM and DRAM is LBM, as is the case in most accelerators.


% We compare five strategies for efficient expert loading: (1) fixed experts in HBM, (2)  LRU-based eviction strategy where least recently used experts are replaced, (3) LRU strategy coupled with randomly speculating experts not residing in cache (4) speculative loading using residual states from the previous layer~\cite{moe_offloading_paper}, and (5) our approach that uses accelerated residual states for earlier expert speculation, detailed in \cref{method_aot_expert_loading}. Since we limit HBM cache size to 16GB, only 32 out of 64 experts of each MLP layer in OlMOE model can be resided in GPU cache, while remaining experts are kept on LBM disk (Need to confirm 16 experts == 16GB).  

% As shown in \cref{fig
% }, our method and~\cite{moe_offloading_paper} achieve significantly higher hit rates than LRU and fixed-expert approaches. Furthermore, we observe that, miss rates of last few layers are higher than initial layers with LRU based caching strategy (see ~\cref{fig:moe_miss_rate_per_layer}). So when previous hidden state is used to speculate experts of next layer, even though hit rates are high, not all experts of layer $i+1$ can be loaded during compute time of layer $i$. On the other hand, since out approach allows to load experts of layer $2i+2$ at layer $i$, most of the experts of last layer are available prior to compute and good hit rate of our method transforms effectively into inference time savings. 

 

 

\subsection{Ahead-of-Time (AoT) MoE Expert Loading} \label{experiments_aot}

We evaluate efficacy of Ahead-of-Time (AoT) expert loading in resource-constrained environments, where limited high-bandwidth memory (HBM) restricts the number of experts that can be stored in fast-access memory. When an MoE gate selects an expert that is not in HBM, it must be loaded from low-bandwidth memory (LBM), introducing latency. To simulate this scenario on an A100 GPU, we restrict HBM capacity to 8GB, requiring experts beyond this limit to be loaded from LBM.\footnote{In our experimental setup, GPU DRAM is treated as HBM and disk as LBM, though the setup generalizes to architectures where SRAM serves as HBM and DRAM as LBM, a common design in many accelerators ~\cite{jouppi2017tpu, keckler2011gpu, lane2020apple}}.

We compare five strategies to maximize compute efficiency and reduce latency. The first approach fixes experts in HBM without replacement. In the second strategy, we employ a Least Recently Used (LRU) eviction policy, where the least accessed experts are dynamically replaced when new ones are needed. The third method extends the LRU approach by adding speculative caching and randomly pre-loading experts. The fourth strategy uses speculative loading based on residual states from the previous layer. Finally, our proposed approach, described in \cref{method_aot_expert_loading}, leverages accelerated residual states to more effectively speculate and pre-load experts in advance. Using the OLMoE model, which has 64 experts per MLP layer, our 8GB HBM capacity allows only 32 experts to be cached in high-speed memory, while the remaining 32 reside in LBM. 

% \textbf{[Clarification Needed: Confirm if 32 experts per layer correspond to 16GB]}

As shown in \cref{fig:MoE_mt_bench}, both our method and the approach of speculating experts based on residual state of previous layer achieve significantly higher hit rates compared to the fixed and LRU-based strategies. Our method operates on accelerated residuals at a rate of 2X, initiating speculative pre-loading of experts at layers $2i+2$ and $2i+3$ while the GPU kernel is engaged in computing the attention and MLP transformations for layer $i$. Starting pre-loading earlier proves advantageous, as we observe that miss rates tend to increase in the final layers when using LRU caching strategies (see ~\cref{fig:miss_rates}). If speculative pre-loading is initiated only one layer before the current layer, it often results in insufficient loading time, preventing all the necessary experts for layer $i$ from being fully loaded during the computation of layer $i-1$. By pre-loading ahead, our method ensures that most speculated experts are readily available, thereby reducing latency and improving inference efficiency. While we demonstrate the effectiveness of operating at a 2X rate for initiating expert pre-loading, the optimal extent of early pre-loading necessary to maximize inference performance remains an open question. We leave this exploration for future work.










% We also perform ablations on hit rates using a linear mapping to speculate experts early relative to using accelerated residuals in ~\cref{ablations:moe_speculation}. 

