\appendix  % Start the appendix

\section{Dynamic Computing}
\subsection{Gradient conflict resolution} \label{sec:grad_conflict}

Traditional early exiting strategies frequently encounter issues related to gradient conflicts ~\cite{predictive_exit_conflicts, gradient_projection_exit}, where multiple exit points induce conflicting gradients during the training phase. This phenomenon leads to optimization instability and challenges in convergence, as gradients computed from divergent branches may not align effectively, and the presence of early exits can perturb the gradient flow, potentially resulting in the incomplete training of lower early exit heads. To illustrate this problem, consider a trainable parameter \( w_j \) situated between gates \( E_j \) and \( E_{j+1} \). For the loss associated with the early exit at gates \( E_{j+1 ... n} \), the parameter update required in \( w_j \) can be expressed as:

\begin{equation}
    \label{eq_grad_propagation_vaniila-ea}
\Delta w_{j} = -\eta \sum_{k=j+1}^{n} \beta_k \frac{\partial L_{E_k}}{\partial w_k}
\end{equation}

where \( \beta_k \) is the backward transformation coefficient for the gradient from gate \( E_k \) to reach parameter \( w_{j} \) and \( \eta \) is the learning rate. Conversely, since accelerated residuals at gate $E_j$ are initialized from slow residuals $H_j$ which are trained with base adapters, when base adapters are frozen, gradient propagation is limited to parallel adapter parameters from gate $E_j$ to gate $E_{j+1}$ thus ensuring every parallel adapter parameter is optimized for specific exit as shown in ~\cref{fig:m2r2_main}. Formally speaking, the update of accelerator adapter parameter $w_j$  within our proposed framework is delineated as:


\begin{equation}
    \label{eq_grad_propagation_ss}
    \Delta w_{j} = 
\begin{cases} 
 -\eta \hat{\beta}_{j+1} \frac{\partial L_{E_{j+1}}}{\partial w_j} & \text{if } E_j < w_j < E_{j+1} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

where \( \hat{\beta}_{j+1} \) is the backward transformation coefficient for the gradient from gate \( E_{j+1} \) to reach parameter \( w_{j} \) of accelerator adapter. This formulation mitigates gradient conflicts arising from gradients associated with top gates, thereby enhancing the stability of the optimization process.\footnote{For simplicity, we focus on cross-entropy loss in this discussion; however, the same reasoning extends to distillation loss as detailed in ~\cref{method_training}.}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/adapter_rank_ablation.png}
        \caption{Alignment of early-exited tokens with those from the final layer improves as adapter rank increases, but tends to plateau beyond a rank of 8. }
        \label{fig:adapter_rank_ablation}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/miss_rates.pdf}
        \caption{Expert speculation miss rates increase in later layers with LRU policy, while initial layers exhibit lower miss rates. Thus, accurate speculative pre-loading benefits later layers more. We leverage accelerated residuals to speculate and pre-load experts for these layers during the computation of earlier layers.   }
        \label{fig:miss_rates}
    \end{subfigure}
    \caption{(a) Adapter Rank Ablation on Dialog Summarization (b) Expert speculation miss Rates}
    \label{fig:overall_fig}
\end{figure}



 \subsection{Discontinuity in Mixture of Depths and Skip Decoding} \label{mod_discountinuity}

To get a deeper understanding of discontinuity leading to suboptimal performance of architectures like MoD and Skip decoding during instruction tuning and fine-tuning phases, we pass a diverse set of prompts through the models and observe residual stream transformation. Residual streams in pre-trained dense transformers tend to undergo significant changes in both direction and magnitude in first few layers than later layers as depicted in \cref{fig:residual_change}. Since Mixture of Depth (MoD) relies on skipping the residual transformation for some of the tokens determined by router, skipping early transformations makes it harder to obtain final residual state closer to that obtained from full dense transformers even when MoD parameters are explicitly trained to alleviate this discontinuity. Skip decoding on the other hand approximates skipping residual transformation of first few layers with a linear projection while  ignoring non-linearities and context, leading to sub-otpimal performance as well. 
    

\section{MoE Speculation Continued}

In this section, we detail the expert transfer process between High Bandwidth Memory (HBM) and Low Bandwidth Memory (LBM) on the A100 GPU. We employ CUDAâ€™s multi-stream functionality ~\cite{cuda_programming_guide} to establish distinct compute and memory-loading streams, both of which operate concurrently during each forward pass. The load stream is scheduled ahead of the compute stream to ensure efficient memory management: while the compute stream processes layer \(i\), the load stream transfers the least recently used experts of layer \(2i+2\) and \(2i+3\) to LBM and loads speculated experts into HBM. This approach leverages the accelerated residual at layer \(i\), which exhibits strong similarity to the slow residuals at layers \(2i+2\) and \(2i+3\) (see ~\cref{fig:m2r2_residual_sim}), enabling effective expert speculation as shown in ~\cref{fig:moe_expert_aot_loading}. Before executing the MLP experts, we verify whether all required experts are available on HBM; if not, the load stream initiates prioritized, on-demand loading for the experts necessary for MLP computation at layer \(i\). Coordination between the load and compute streams is managed using CUDA primitives.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{sections//figures/moe_cuda_trace.jpg}
    \caption{A100 GPU trace demonstrating overlap of computation and expert transfer between LBM and HBM. }
    \label{fig:moe_aot_cuda_trace}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{sections/figures/flop_overhead_m2r2.pdf}
        \caption{FLOPs overhead of generating speculative draft for different approaches on Gemma-7B. The optimized M2R2 approach incorporates FLOPs optimization techniques described in \cref{sec:flops_optimization}}
        \label{fig:flops_optmization}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{sections/figures/num_heads_ablation.png}
        \caption{Ablation study on attention heads and M2R2 alignment benefits. Using 4 to 8 heads in the accelerated residual stream reduces FLOPs with minimal alignment degradation.}
        \label{fig:m2r2_num_heads_ablation}
    \end{subfigure}
    \caption{FLOPs overhead of M2R2 and optimization based on Attention head pruning.}
\end{figure}




% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.5\linewidth]{sections/figures/adapter_rank_ablation.png} 
%     \caption{Adapter Rank Ablation on Dialog Summarization}
%     \label{fig:adapter_rank_ablation}
% \end{figure}



% \section{FLOP Efficient Mode} \label{appendix:flop_efficient_mode}
% Transformer-based decoding is generally memory-bound on most mainstream accelerators ~\cite{liang2020transformer}; however, there exist specific deployment scenarios where reducing floating-point operations (FLOPs) is essential. For example, in on-device settings, power consumption is often directly proportional to the number of FLOPs per decoding step, making FLOP reduction a critical factor in minimizing energy consumption. 
% Since M2R2 uses a parallel faster stream during each forward pass, FLOPs especially during attention computation increase substantially, resulting in overall more flops than SOTA approaches as shown in ~\cref{fig:m2r2_flops_overhead}. To operate in Flop efficient mode, we propose a slimmer accelerated residual stream that uses a fraction of attention heads as that of slower residual stream. Specifically, we use first $\hat{n}_h$ heads of query, key and value projections of base model to process a slicker accelerated stream. ~\cref{fig:m2r2_num_heads_ablation} indicates effect of using a slicker stream on alignment. As depicted, using $\hat{n}_h = 8$ offers a good trade-off between alignment and FLOPs overhead. 

% So we used $\hat{n}_h = 4$ for all experiments described in ~\cref{sec:experiments}
 




\textbf{Accelerator Adapter Rank Ablation} \label{m2r2_adapter_rank}
To minimize parameter overhead from accelerator adapters, we conduct an ablation study on adapter rank to identify the optimal rank that achieves strong alignment without substantially increasing parameter load. As illustrated in~\cref{fig:adapter_rank_ablation}, a rank of 8 offers an effective trade-off, with alignment performance showing a steep improvement up to rank 8, beyond which the benefit curve begins to plateau.

\newpage
\textbf{Prompt for Evaluation of Dynamic Compute Responses} \label{m2r2_prompt}

To assess the responses generated by our approach alongside baseline models, we utilize the following prompt for GPT-4 oracle. Note that the baseline and target responses are randomly assigned to either Assistant 1 or Assistant 2 in the template below.

\begin{lstlisting}[basicstyle=\ttfamily, breaklines=true]
Human: You are a helpful and precise assistant for evaluating the quality of an answer.
[Question]
{question}
[The Start of Assistant 1's Answer]
{answer_1}
[The End of Assistant 1's Answer]
[The Start of Assistant 2's Answer]
{answer_2}
[The End of Assistant 2's Answer]

We request your feedback on the performance of both AI assistants in response to the user question above. Please rate their responses based on helpfulness, relevance, accuracy, and level of detail.

Assign each assistant an overall score on a scale of 1 to 10, where a higher score reflects better performance.

Please provide a single line output with only two values, representing the scores for Assistant 1 and Assistant 2, respectively, separated by a space.

Assistant:
\end{lstlisting}

% \subsubsection{Train Inference Mismatch: TODO Should we keep this section ?} \label{m2r2_train_inference_mismatch} 

% Transformer-based decoding is generally memory-bound on most mainstream accelerators; however, there exist specific deployment scenarios where reducing floating-point operations (FLOPs) is essential. For example, in on-device settings, power consumption is often directly proportional to the number of FLOPs per decoding step, making FLOP reduction a critical factor in minimizing energy consumption. Vanilla early exiting techniques offer a mechanism for reducing FLOPs but suffer from a fundamental mismatch between training and inference due to the variable exit depths of tokens. 

% Consider a token $T_t$ that exits at layer $E_i$ and another token $T_{t+k}$ that exits at a deeper layer $E_j$ with $E_i < E_j$. In this case, the hidden state $H_{t+k}^l$ does not have a corresponding hidden state $H_t^l$ to attend to for any intermediate layer $l$ where $E_i < l \leq E_j$. A common strategy in the literature is to utilize the last available hidden state, denoted as $H_t^{E_j}$, for subsequent attention computations. However, this approach is sub-optimal and often degrades generation quality \cite{ref}. 

% To address the mismatch while still reducing FLOPs, we propose an alternative strategy where the router is trained to adaptively manage attention masking. Specifically, the attention mask between the token $T_{t+k}$ and any preceding token $T_{<t+k}$ is defined as:

% \begin{equation}
%     a_{T_{t+k}, T_{<t+k}} =
%     \begin{cases}
%       1, & \text{if } E_{T_{<t+k}} \geq E_{T_{t+k}} \\
%       0, & \text{otherwise}
%     \end{cases}
% \end{equation}

% This adaptive attention mask ensures that the router takes into account the exit status of each token, aligning the training and inference phases. Since the attention mechanism during decoding is identical to that during training, the impact on generation quality is minimized, as evidenced by the results shown in \cref{fig:gen_quality_with_and_without_recompute_attention_show_flops}. 

% While methods such as Mixture of Decoder (MoD) do not suffer from the training-inference mismatch, they exhibit discontinuities between pre-training and supervised fine-tuning, resulting in sub-optimal perplexity scores. In contrast, our proposed method does not require pre-training, avoids these discontinuities, and achieves superior perplexity metrics in both supervised fine-tuning and instruction-tuning settings, as demonstrated in \cref{fig:Mod_vs_m2r2_loss_curves}.



% \textbf{Optional : Impact of Batching} \label{m2r2_batching_impact}
% - SD batching 
% - Dynamic compute batching 


% \textbf{Optional : Speculative Decoding Top-k Sampling} \label{m2r2_sd_topk_sampling}
% - SD topk sampling

