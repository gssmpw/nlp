
-------------------------------------------------------
Let's consider residual stream $h_i$ traverses through interval $E_j$ to $E_{j+1}$ and undergoes residual transformation given by 
\begin{equation}
h_{E_{j+1}} = h_{E_j} \prod_{i=E_j}^{E_{j+1}} \left( 1 + \delta_i \right)    
\end{equation}

where $\delta_i$ denotes non-linear transformation added by layer $i$. Each non-linear transformation of layer $i$ is a function of previous contextual representation, $c_i$ and input residual stream $h_i-1$ as
$\delta_i = f(c_i, h_{i-1})$ 

One way to exit early at exit $E_j+1$ is to assume that residual transformation from $E_j+1$ to final layer $N-1$ can be approximated by a linear function $\phi$ as $h_{N-1} \sim \Phi(h_{E_j+1})$ and most conventional approaches such as \todo{cite EA papers} use this approach. In other words, 

\begin{equation}
\Phi(h_{E_j+1} \sim h_{E_j+1} \prod_{i=E_j+1}^{N} \left( 1 + \delta_i \right)   
\end{equation}

This approach suffers from two primary issues, linearity assumption from $h_E_j+1$ to $H_N-1$ if often incorrect, particularly when $E_j << N$. More importantly, linear transformation $\Phi$ doesn't consider effect of context $C_i$. M2R2  effectively addresses these issues as accelerated residual stream at interval $E_j+1$ can be represented as 

\begin{equation}
r_{E_{j+1}} = r_{E_j} \prod_{i=E_j}^{E_{j+1}} \left( 1 + \gamma_i \right)    
\end{equation}

where $\gamma_i$ denotes non-linear transformation added by layer $i$ to previous accelerated residual $r_i-1$. Similar to $\delta_i$, non-linear transformation $\gamma_i$ considers context $C_i$ as 
$\gamma_i = g(c_i, r_{i-1})$. So in summary, slow residual transformation is approximated by accelerated residual as: 

\begin{equation}
h_{E_j} \prod_{i=E_j}^{N} \left( 1 + \delta_i \right) \sim h_{E_j} \prod_{i=E_j}^{E_j+1} \left( 1 + \gamma_i \right)
\end{equation}

It's worth noting that accelerated residual $r_i$ and slow residual $h_i$ are processed concurrently at layer $i$ by constructing proper attention mask such as attention of slow residual is represented as 

$MHA(H_it, H_{i<=t}, H_{i<=t}$ while attention of fast residual is computed as 

$MHA(r_it, H_{i<=t}, H_{i<=t}$ where $MHA(q,k,v$ denotes multi head attention between query, $q$, key $k$ and value $v$.


------------------------------------------------------------------

Vertical latent attention on accelerated residual is computed as 
$MHA(S_mt, S(Ej<=i<=m)t, S(Ej<=i<=m)t)$ where $Smt$ denotes query/key/value projection in latent domain at layer $m$ at time $t$. 
------------------------------------------------------------------

Gradient conflict Avoidance: 

Let's consider $w_j$ is a trainable parameter that belongs to a layer between $E_j$ and $E_j+1$. Consider early exit loss at gate $E_j+1$, $L_j+1$, gradient propagation of $w_j$ at another trainable parameter $w_j-n$ can be gives as 

$\sum_{k=E_j-n}^{E_j} \beta_k \frac{\partial L_{E_k}}{\partial w_k}$

where $\beta_j$ denotes backward transformation coefficient for weight $w_j$ to reach gate $E_j$. 
 
On the other hand, gradient propagation in proposed approach can be represented as 

\[
\frac{\partial L_{E_j}}{\partial w_j} = 
\begin{cases} 
\beta_j \frac{\partial L_{E_j}}{\partial w_j} & \text{if } E_j \leq w_j \leq E_{j+1} \\
0 & \text{otherwise}
\end{cases}
\]

