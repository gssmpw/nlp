\section{Introduction}
\label{sec:intro}
Large Language Models (LLMs) have become a cornerstone of contemporary natural language processing (NLP) systems, exhibiting exceptional performance in tasks requiring the understanding and generation of complex, structured language across diverse domains \cite{brown2020language, radford2019language, vaswani2017attention}. Their success largely stems from their ability to capture long-range dependencies in text, enabling the modeling of intricate linguistic patterns and semantics. A critical architectural component enabling this capability is the residual transformation mechanism, which introduces a pathway for the direct flow of information across layers. By preserving representations from earlier layers and allowing new transformations to build on them, residual connections help mitigate the degradation of critical information, thereby expanding the model’s expressive power to handle more abstract and nuanced features \cite{he2016identity, ba2016layer, vaswani2017attention}. This architecture empowers LLMs to model complex dependencies over extended sequences, thereby improving feature extraction and representational depth. However, the use of a static residual transformation for all tokens creates a rigid balance between inference efficiency and generation quality \cite{shen2021dynamic, garncarek2021dynamic}. This approach fails to account for the inherent variability in token complexity, leading to inefficiencies in dynamic compute scenarios.


\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/vanilla_ea_residual.pdf}
        \caption{Residual similarity in traditional early exiting}
        \label{fig:vanilla_ea_residual_sim}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/m2r2_residual.pdf}
        \caption{Residual similarity with M2R2}
        \label{fig:m2r2_residual_sim}
    \end{subfigure}
    \caption{Traditional early exiting approaches approximate the final residual state with context-independent mapping, $\mathcal{T}$, applied on intermediate hidden state, resulting in discontinuities in transformations and lower similarity with final residual state. In contrast, M2R2 progressively enhances residual transformation velocity at each layer, enabling more robust and uniform early alignment.}
    \label{fig:residual_sim}
\end{figure}


Recent initiatives to rectify this limitation have centered on the introduction of adaptive computation mechanisms. Approaches such as Early Exiting \cite{schuster2022confident, varshney-etal-2024-investigating, chen2023eellm}, Skip Decoding \cite{delcorro2023skipdecode}, Mixture-of-Depth \cite{raposo2024mixture} aim to mitigate computational overhead by dynamically modulating the depth of residual transformations based on the complexity of individual tokens. While these methodologies have demonstrated promise in enhancing inference efficiency, they predominantly focus on the distance that a token traverses through the model layers—specifically, the number of layers a token traverses prior to exiting. This distance-centric perspective neglects a critical dimension of residual transformations: the velocity at which token representations evolve within the model. In this context, velocity denotes the rate at which the residuals transform as tokens navigate the network, a factor we hypothesize can be leveraged to achieve an optimal balance between generation quality and computational efficiency.

In this paper, we introduce a novel framework, \textit{Mixture of Multi-rate Residuals}, which emphasizes the modulation of residual transformation velocity to improve early residual alignment. This enhancement boosts efficiency across diverse inference paradigms, including dynamic computing, speculative decoding, and Mixture-of-Experts (MoE) Ahead-of-Time (AoT) loading. In dynamic compute settings, rather than solely adjusting the computational depth for tokens ~\cite{chen2023eellm, schuster2022confident, delcorro2023skipdecode, Tang2024}, our method explicitly accelerates the rate of residual transformation, allowing for faster alignment of token representations at earlier stages of computation. In self-speculative decoding ~\cite{leviathan2023fast, chen2023accelerating}, accelerated residual streams facilitate the generation of speculative tokens, which are validated in parallel against slower residual streams. This parallelism enables the the advancement of multiple tokens per forward pass at the expense of loading all model parameters. Furthermore, our method also enables efficient inference of Mixture-of-Experts (MoE) models ~\cite{shazeer2017outrageously, fedus2022switch} in resource constrained setups. While sparse MoE architectures decrease the number of active parameters per decoding step, the experts necessary for a token can only be identified immediately before computation via routers that utilize the residual state to output the required expert probabilities. This necessitates expert loading from low bandwidth memory (LBM) to the accelerator, which becomes a significant bottleneck ~\cite{lepikhin2020gshard, fedus2022switch}. In contrast, our method allows for the early speculation of experts through accelerated residuals, enabling temporal overlap between computation and memory transfer, thereby reducing latency associated with expert switching during inference. 

% Our empirical results demonstrate a \textbf{<>}\% improvement in decoding speed, rendering our approach highly effective in resource-constrained environments.

To substantiate our claims, we conduct comprehensive evaluations across a spectrum of reasoning oriented and application specific tasks. Our findings illustrate that the Mixture of Multi-rate Residuals framework provides practical benefits in various inference optimization scenarios, including dynamic computation, speculative decoding, and the inference of Mixture-of-Experts (MoE) models. The contributions of this paper are as follows:
\begin{itemize}
    \item We introduce \textit{Mixture of Multi-rate Residuals} (M2R2), a novel framework that dynamically modulates residual velocity to optimize early alignment, enhancing inference efficiency and achieving a superior trade-off between decoding speedup and generation quality across diverse reasoning-oriented tasks—all without requiring costly pre-training.
    \item We empirically demonstrate that M2R2 surpasses state-of-the-art self-speculative decoding methods, attaining a 2.8$\times$ speedup on MT Bench.
    \item We further establish the effectiveness of M2R2 for on-device Mixture-of-Experts (MoE) models with Ahead-of-Time (AoT) Expert Loading, which mitigates the latency associated with on-demand expert retrieval by overlapping memory transfers with computation, yielding a 2.9$\times$ speedup over traditional expert loading methods~\citep{lepikhin2020gshard, fedus2022switch}.
\end{itemize}
