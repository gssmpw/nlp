\section{conclusion}
In this paper, we introduced the Mixture of Multi-rate Residuals (M2R2) framework, which dynamically modulates the velocity of residual transformations to optimize early residual alignment, improving inference efficiency in diverse inference setups. Unlike traditional methods that focus on the "distance" tokens traverse within the model layers, M2R2 enhances the rate at which residuals evolve, allowing for faster alignment of intermediate representations. Our empirical results demonstrate that M2R2 outperforms state-of-the-art dynamic computation methods offering better generation metrics to speedup trade-off. Furthermore, it achieves 2.8X speedup in lossless self-speculative decoding setup. In Mixture-of-Experts (MoE) models, with ahead-of-time expert loading, M2R2 reduces decoding latency by overlapping memory transfers with computation and achieves a throughput improvement of up to 2.9X compared to traditional expert loading methods. Overall, M2R2 offers an effective solution for optimizing inference in resource-constrained environments, enhancing both dense transformer and sparse MoE models.