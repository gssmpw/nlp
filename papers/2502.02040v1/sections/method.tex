% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{Move_teaser.pdf}
%     \caption{Comparison of different dynamic compute approaches. length of arrow indicates residual transformation per token while width indicates velocity of transformation.}
%     \label{fig:enter-label}
% \end{figure}

\section{Method}
\label{sec:method}
Residual connections play a crucial role in shaping token representations, yet their dynamics remain underexplored in the context of efficient decoding. In this work, we delve deeper into transformer residual dynamics and investigate how modulating residual transformation velocity can improve inference efficiency in token-level processing, optimizing both dense and sparse MoE transformers.


\subsection{Residual Dynamics and Motivation for Multi-rate Residuals} \label{sec:motivation}

To analyze how hidden representations evolve across different layers of a transformer architecture, it's crucial to consider the effect of residual connections. Each transformer decoder layer typically has residual connections across attention and MLP submodules. As the residual stream $h_i$ traverses from interval $E_j$ to $E_{j+1}$, it undergoes a residual transformation given by:  
% \begin{equation}
% \label{eq:slow_residual_transformation}
% H_{E_{j+1}} = H_{E_j} \prod_{i=E_j}^{E_{j+1}} \left( I + \mathcal{A}_i \right) \left( I + \mathcal{M}_i \right) \quad \text{where} \quad \mathcal{A}_i = f(c_i, h_{i}), \mathcal{M}_i = g(h_i)
% \end{equation}

\begin{equation} \label{eq:slow_residual_transformation}
h_{E_{j+1}} = h_{E_j} + \sum_{i=E_j}^{E_{j+1}-1} \left( \mathcal{A}_i(h_i) + \mathcal{M}_i(h_i + \mathcal{A}_i(h_i)) \right) \quad \text{where} \quad \mathcal{A}_i = f(c_i, h_{i}), \mathcal{M}_i = g(h_i). 
\end{equation}

Here, \( \mathcal{A}_i \) denotes the non-linear transformation introduced by the multi-head attention mechanism at layer \( i \), while \( \mathcal{M}_i \) corresponds to the non-linear transformation of the MLP block at the same layer. These transformations depend on the input residual stream \( h_i \) and, in the case of \( \mathcal{A}_i \), the previous contextual representation \( c_i \).\footnote{Normalization layers are typically applied in practice but are omitted here for simplicity of the argument.}


% For easy tokens, the magnitude and direction of this delta transformation become progressively smaller with each successive layer as shown in \cref{fig:delta_transformation}. Consequently, it is feasible to predict these tokens after only a few residual connections, whereas harder tokens necessitate more extensive processing through additional layers.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/residual_change.pdf}
        \caption{}
        \label{fig:residual_change}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sections/figures/alignment_wrt_dedicated_model.pdf}
        \caption{}
    \label{fig:alignment_wrt_dedicated_model}
    \end{subfigure}
    \caption{(a) As residual streams propagate through the model, the directional shifts in the residuals become progressively smaller. (b) A dedicated model with $k$ layers achieves a faster rate of change in residual streams and higher alignment than base model leveraging early exit mechanisms at layer $k$.}
    \label{fig}
\end{figure}


To examine whether residual transformations can be accelerated across layers, we conducted experiments using a diverse set of prompts on a pre-trained Phi3 model~\cite{phi3_report}. As illustrated in \cref{fig:residual_change}, we measured the directional shift in residual states as \( 1 - \mathcal{C}(h_{i-1}, h_i) \), where \(\mathcal{C}\) denotes normalized cosine similarity. This shift is notably higher in the initial layers, gradually decreasing in subsequent layers. This behavior allows traditional early exit approaches to effectively accelerate decoding by enabling earlier exits for simpler tokens. However, these approaches typically rely on a distance-based approximation, where the full residual transformation of the model is approximated by the residual transformations of the initial layers. To gain deeper insights into the distance versus velocity aspects of residual transformation, we conducted a comparative study. Specifically, we trained an early exit head at layer $k$ of the Phi3 model, which consists of 32 layers, restricting the distance traveled by each token. To accelerate the residual transformation relative to number of layers, we trained a smaller model consisting of only $k$ layers, while keeping all other hyperparameters consistent. We then compared the next-token prediction accuracy of the early exit head of the base model with that of the smaller model. To ensure an equal number of trainable parameters, we inserted low-rank adapters into the smaller model and trained only these adapters, whereas, in the distance-based approach, we trained solely the early exit head. In addition, to accelerate the residual transformation in smaller model, we distilled the residual streams from the larger model by incorporating a distillation loss ~\cite{sanh2019distilbert} between the residual state at layer \(i\) of the smaller model and the residual state at layer \(4 \times i\) of the larger model. As shown in ~\cref{fig:alignment_wrt_dedicated_model} the smaller model demonstrates a significantly faster rate of change in residual streams, leading to higher next token prediction accuracy after $k$ layers compared to the base model that employs traditional early exit mechanisms after $k$ layers \cite{schuster2022confident, chen2023eellm, varshney-etal-2024-investigating}. This experimental setup, which modifies only the rate of change in residual streams while keeping other factors constant, suggests that dense transformers, trained with a fixed number of layers, may inherently possess a slow residual transformation bias.

This observation raises an intriguing question: if the rate of change in residual streams could be accelerated relative to the number of layers, is it possible to facilitate earlier alignment for a greater proportion of tokens? Earlier alignment would be beneficial to not only facilitate dynamic computation but also for generating speculative tokens efficiently with high acceptance rates in speculative decoding setups ~\cite{leviathan2023fast, chen2023accelerating}. 

%thereby enhancing the efficiency of early exiting? 
 % This bias likely constrains the effectiveness of early exiting, particularly for easier tokens. By addressing this limitation through accelerated residual transformations, we hypothesize that it is possible to substantially improve the efficiency and accuracy of early exit strategies in transformer models.

\subsection{Multi-Rate Residual Transformation} \label{m2r2_method}

To address the slow residual transformation bias described in ~\cref{sec:motivation}, we introduce \textit{accelerated residual streams} that operate at rate $R$ relative to original slow residual stream. We pair slow residual stream, $h$ with an accelerated residual stream, $p$, which has an intrinsic bias towards earlier alignment. Relative to ~\cref{eq:slow_residual_transformation}, accelerated residual transformation from interval $E_j$ to $E_{j+1}$ can be represented as: 

% \begin{equation}
% \label{eq:fast_residual_transformation}
% P_{E_{j+1}} = P_{E_j} \prod_{i=E_j}^{E_{j+1}} \left( I + \hat{\mathcal{A}_i} \right) \left( I + \hat{\mathcal{M}_i} \right) \quad \text{where} \quad \hat{\mathcal{A}_i} = \hat{f}(c_i, P_{i}), \hat{\mathcal{M}_i} = \hat{g}(P_{i})
% \end{equation}


\begin{equation} \label{eq:fast_residual_transformation}
p_{E_{j+1}} = p_{E_j} + \sum_{i=E_j}^{E_{j+1}-1} \left( \hat{\mathcal{A}_i}(p_i) + \hat{\mathcal{M}_i}(p_i + \hat{\mathcal{A}_i}(p_i)) \right) \quad \text{where} \quad \hat{\mathcal{A}_i} = \hat{f}(c_i, p_{i}), \hat{\mathcal{M}_i} = \hat{g}(h_i), 
\end{equation}



where $\hat{\mathcal{A}_i}$ and $\hat{\mathcal{M}_i}$ denote non-linear transformation added by layer $i$ to previous accelerated residual $p_{i}$. Similar to $\mathcal{A}_i$, non-linear transformation $\hat{\mathcal{A}_i}$ attends to same context $c_i$ but uses a different transformation $\hat{f}$ for accelerating $p_{E_j}$ relative to $h_{E_j}$. 

We integrate accelerated residual transformation directly into the base network using parallel accelerator adapters such that rank of accelerator adapters $R_p << d$ where $d$ denotes base model hidden dimension. This setup allows the slow residual stream $h_{E_j}$ to pass through the base model layers while the accelerated residual stream $p_{E_j}$ utilizes these parallel adapters as shown in ~\cref{fig:m2r2_main}. Both slow and accelerated residuals are processed in same forward pass via attention masking and incur negligible additional inference latency in memory bound decoding setups, while in compute bound decoding setups where FLOPs optimization is essential, accelerated residual stream utilizes a fraction of attention heads that of slow residual (see ~\cref{sec:flops_optimization}). Additionally, to maximize the utility of accelerated residual transformations without introducing dedicated KV caches, we propose a shared caching mechanism between the slow and accelerated streams which minimally impact alignment benefits of our approach while offering substantial memory savings (see ~\cref{fig:koala_alignment}). Specifically, the attention operation on the slow residuals \( \text{MHA}(h_t, h_{\leq t}, h_{\leq t}) \) is redefined for accelerated residuals as 
\[
\hat{\mathcal{A}} = MHA(p_t, h_{<t} \oplus p_t, h_{<t} \oplus p_t),
\]
where the accelerated residual at time-step $t$, \( p_t \) attends to the slow residual’s KV cache, facilitating the reuse of contextual information across both residual streams without incurring additional caching costs. Here, \(MHA(q, k, v) \) represents multi-head attention between query \( q \), key \( k \), and value \( v \).

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{sections//figures/m2r2_main2.pdf}
    \caption{Multi-rate Residuals Framework: Slow residual stream of base model is accompanied by a faster stream that operates at a $2-(J+1)\times$ rate relative to the slow stream, undergoing transformations via accelerator adapters as detailed in \cref{m2r2_method}, where J denotes number of early exit intervals. Colors within the slow and fast residual streams indicate similarity, with matching colors representing the most closely aligned residual states. At the beginning of the forward pass and at each exit point, the accelerated residual state is initialized from the corresponding slow residual state to avoid gradient conflict during training (see ~\cref{sec:grad_conflict}). Early exiting decisions are informed by the Accelerated Residual Latent Attention (ARLA) mechanism, described in \cref{method_arla}, which evaluates residual dynamics across consecutive exit gates.}
    \label{fig:m2r2_main}
\end{figure}

% Furthermore. to maximize the benefits of fast residual transformations without using dedicated KV caches, we propose sharing the fast network’s cache with the slow network. Formally speaking, We modify attention operation on slow residuals $MHA(H_t, H_{<=t}, H_{<=t})$ as $MHA(P_{t}, H_{<t} \oplus P_t, H_{<t}  \oplus P_t)$ such that accelerated residuals attend to previous slow context KV cache, where $MHA(q,k,v)$ denotes multi head attention between query, $q$, key $k$ and value $v$.


\subsection{Enhanced Early Residual Alignment}
Early residual alignment is instrumental in optimizing early exiting, speculative decoding, and Mixture-of-Experts (MoE) inference mechanisms. In this section, we provide a detailed analysis of how accelerated residuals enhance these inference setups.

% By aligning the residual states of intermediate layers with the final output representations, the model can maintain high prediction accuracy even when computations are truncated at earlier layers. This enables more reliable early exiting, reducing the overall computational cost while preserving performance. Additionally, in speculative decoding, early residual alignment allows the model to make confident predictions using faster, partial computations, thereby accelerating inference without sacrificing output quality.


\subsubsection{Early Exiting} \label{method_early_exiting}

A prevalent strategy for enabling early exiting at an intermediate layer $E_{j}$ involves approximating the residual transformation between $E_{j}$ and the final layer $N-1$ using a linear, context independent mapping, $\mathcal{T}$, such that $H_{N-1} \approx \mathcal{T}(H_{E_{j}})$. This approximation has been extensively employed in conventional approaches ~\cite{schuster2022confident, chen2023eellm, varshney-etal-2024-investigating}, providing a computationally efficient means to project the output of deeper layers from intermediate states. Specifically, residual state of layer $N-1$ with this approximation can be expressed as:


% \begin{equation}
% \label{eq: vanila_ea_assumption}
% \Phi(H_{E_{j}}) \sim H_{E_{j}} \prod_{i=E_{j}}^{N}\left( I + \mathcal{A}_i \right) \left( I + \mathcal{M}_i \right) \quad \text{where} \quad \Phi \perp C
% \end{equation}

\begin{equation} \label{eq:early_exiting}
h_{E_j} + \sum_{i=E_j}^{N-1} \left( \mathcal{A}_i(h_i) + \mathcal{M}_i(h_i + \mathcal{A}_i(h_i)) \right) \sim \mathcal{T}(h_{E_{j}})  \quad \text{where} \quad \mathcal{T} \perp c. 
\end{equation}


Here, $\mathcal{A}_i$ and $\mathcal{M}_i$ represent the residual contributions of the multi-head attention and MLP layers, respectively, while $\mathcal{T}$ remains independent of $c$, the preceding context.

This approach is inherently limited by two major factors: first, the assumption of linearity between $h_{E_{j}}$ and $h_{N-1}$ may not hold uniformly for all tokens, particularly when $E_j \ll N$. Second, the linear transformation $\mathcal{T}$ disregards the influence of the context $c$ and fails to account for the latent representations of previous contextual states. In contrast, M2R2 accelerated residual states mitigate both of these challenges by approximating the slow residual transformation of all layers via a faster residual transformation of fewer layers as:
% \begin{equation}
% H_{E_j} \prod_{i=E_j}^{N}\left( I + \mathcal{A}_i \right) \left( I + \mathcal{M}_i \right) \sim P_{E_j} \prod_{i=E_j}^{E_j+1}\left( I + \hat{\mathcal{A}_i} \right) \left( I + \hat{\mathcal{M}_i} \right)
% \end{equation}


\begin{equation} \label{eq:m2r2_approximating_ea}
h_{E_j} + \sum_{i=E_j}^{N-1} \left( \mathcal{A}_i(h_i) + \mathcal{M}_i(h_i + \mathcal{A}_i(h_i)) \right) \sim p_{E_j} + \sum_{i=E_j}^{E_{j+1}-1} \left( \hat{\mathcal{A}_i}(p_i) + \hat{\mathcal{M}_i}(p_i + \hat{\mathcal{A}_i}(p_i)) \right), 
\end{equation}

% \begin{equation} \label{eq:fast_residual_transformation}
% p_{E_{j+1}} = p_{E_j} + \sum_{i=E_j}^{E_{j+1}-1} \left( \hat{\mathcal{A}_i}(p_i) + \hat{\mathcal{M}_i}(p_i + \hat{\mathcal{A}_i}(p_i)) \right) \quad \text{where} \quad \hat{\mathcal{A}_i} = \hat{f}(c_i, p_{i}), \hat{\mathcal{M}_i} = \hat{g}(h_i) 
% \end{equation}






where $p_{E_j}$ is initialized from the slow residual state $h_{E_j}$ at each early exit interval $E_j$ using an identity transformation (see ~\cref{fig:m2r2_main}). As shown in ~\cref{fig:m2r2_residual_sim}, accelerated residuals offer a smoother, more consistent shift in residual direction across layers, in contrast to the abrupt changes typically seen at early exit points in standard early exit methods. Moreover, the normalized cosine similarity between accelerated states at early exit intervals and final residual states is substantially higher compared to traditional early exit techniques, highlighting improved alignment with final layer representations. Traditional adaptive compute methods are constrained by two principal factors: the number of tokens eligible for early exit at intermediate layers and the precision of early exit decision. If residual streams fail to saturate early, the majority of tokens remain ineligible for exit, thereby diminishing potential speedups. Additionally, imprecise delineations between tokens suitable for early exit can lead to underthinking (premature exits that adversely affect accuracy) or overthinking (unnecessary processing that compromises efficiency) ~\cite{zhou2020self, dai2020dynamic}. Enhanced early alignment using ~\cref{eq:m2r2_approximating_ea} helps to address  first issue. To address the second issue we introduce Accelerated Residual Latent Attention, which dynamically assesses the saturation of the residual stream, allowing for a more precise differentiation between tokens that can exit early and those requiring further processing.

% This results in uniform change in residual direction    
% % We keep $\mathcal{A} = \hat{\mathcal{A}}$, while $\hat{\mathcal{M}}$ is accelerated by a factor of $2 - (N_{E}+1)X$ relative to the slower residual transformation $\mathcal{M}$, where $N_E$ represents number of early exiting intervals.
% Figure~\cref{fig:rate_change_comparison} illustrates the comparative rate of change between these transformation streams.



% fig:rate_change_comparison
% - grid plot x axis -> layer id (0, 8) , y axis -> layer id -> dark color cell for max similarity , lighter for lower 
% \input{sections/equations.tex}





% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\textwidth, height=5cm]{rate_change_comparison.png}
%     \caption{Rate of change comparison between fast and slow residual streams.}
%     \label{fig:rate_change_comparison}
% \end{figure}

%vary k and and plot EA accuracy for larger and smaller models. 

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.5\textwidth,height=5cm]{sections/figures/alignment_comparison_dialogsum.pdf}
%     \caption{Alignment of exited tokens for different early exit layers using traditional early exiting heads, dedicated faster networks, and faster residuals.}
%     \label{fig:small_model_early_exiting}
% \end{figure}


\textbf{Accelerated Residual Latent Attention} \label{method_arla}

In the context of residual streams, we observe that the decision to exit at a given layer can be more effectively informed by analyzing the dynamics of residual stream transformations, instead of solely relying on a classification head applied at the early exit interval $E_j$. To capture the subtle dynamics of residual acceleration, we propose a \textit{Accelerated Residual Latent Attention} (ARLA) mechanism. This approach involves making the exit decision at gate $E_j$ by attending to the residuals spanning from gate $E_{j-1}$ to $E_j$, rather than considering only the residual at gate $E_j$. To minimize the computational overhead associated with exit decision-making, the attention mechanism operates within the latent domain as depicted in ~\cref{fig:arla_arch}. Formally, for each interval $[E_j, E_{j+1}]$, the accelerated residuals are projected into Query ($Q^s_{E_j}, \ldots, Q^s_{E_{j+1}}$), Key ($K^s_{E_j}, \ldots, K^s_{E_{j+1}}$), and Value ($V^s_{E_j}, \ldots, V^s_{E_{j+1}}$) vectors, with latent dimension $d^s$ for $Q^s$, $K^s$, and $V^s$ being significantly smaller than hidden dimension of $p$.\footnote{We use $d^s = 64$ for experiments described in ~\cref{sec:experiments}.} Notably, when the router is allowed to make exit decisions at gate $E_j$ based on residual change dynamics, we observe that the attention is not confined to the residual state at $E_j$ but is distributed across residual states from $E_{j-1}$ to $E_j$, %as illustrated in Figure~\ref{fig:vertical_latent_attention_dynamics}. 
This broader focus on residual dynamics significantly reduces decision ambiguity in early exits, as demonstrated in Figure~\ref{fig:roc_arla}, which contrasts routers based on the last hidden state, and the proposed ARLA router.

%show R -> S transformation. 
%show parameter and flop overhead as compared to adapter on last hidden state.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.5\textwidth,height=5cm]{sections/figures/roc_arla.pdf}
%     \caption{ROC curves of early exit decision strategies: confidence-based methods (CALM/LITE), routers based on the accelerated hidden state, and latent attention routers.}
%     \label{fig:decision_making_comparison}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.5\textwidth,height=5cm]{vertical_latent_attention.png}
%     \caption{Vertical latent attention mechanism for optimizing early exit decisions by considering residuals from gate \(M\) through \(M-1\).}
%     \label{fig:vertical_latent_attention}
% \end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.52\textwidth}
        \centering
        \includegraphics[width=\textwidth, height = 4cm]{sections/figures/arla_arch.pdf}
        \caption{Accelerated Residual Latent Attention (ARLA): Accelerated residuals between early exit gates are projected into latent domain and attention over residual states within the interval is computed to capture residual dynamics and exit decision is made based on residual saturation.}
        \label{fig:arla_arch}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, height = 4.5cm]{sections/figures/vla_roc.pdf}
        \caption{ROC classification curves of early exit decision strategies using a linear router used on last residual state ~\cite{schuster2022confident, varshney-etal-2024-investigating, chen2023eellm}  and using ARLA approach that considers residual dynamics. }
        \label{fig:roc_arla}
    \end{subfigure}
    \caption{Effectiveness of ARLA in capturing residual dynamics for early exiting decisions.}


\end{figure}



% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=1\textwidth,height=5cm]{sections/figures/arla.pdf}
%     \caption{fig that plots 32 rows 2 cols heatmap showing attention at each gate}
%     \label{fig:vertical_latent_attention_dynamics}
% \end{figure}

\subsubsection{Self Speculative Decoding} \label{method_self_speculative_decoding}

An alternative means to exploit the early alignment properties of our approach is through the use of accelerated residual states for speculative token sampling to accelerate autoregressive decoding. Speculative decoding aims to speed up memory-bound transformer inference by employing a lightweight draft model to predict candidate tokens, while verifying speculated tokens in parallel and advancing token generation by more than one token per full model invocation \cite{leviathan2023fast, chen2023accelerating, xia2023speculative, miao2023specinfer}. Despite its effectiveness in accelerating large language models (LLMs), speculative decoding introduces substantial complexity in both deployment and training. A separate draft model must be specifically trained and aligned with the target model for each application, which increases the training load and operational complexity ~\cite{chen2023accelerating}. Additionally, this approach is resource-inefficient, as it requires both the draft and target models to be simultaneously maintained in memory during inference \cite{leviathan2023fast, chen2023accelerating}. 

One strategy to address this inefficiency is to leverage the initial layers of the target model itself to generate speculative candidates, as depicted in ~\cite{Tang2024}. While this method reduces the autoregressive overhead associated with speculation, it suffers from suboptimal acceptance rates. This occurs because the linear transformation employed for translating hidden states from layer $k$ to the final layer $N$ is typically a poor approximation, as discussed in ~\cref{sec:motivation} and ~\cref{method_early_exiting}. Our approach resolves this limitation by utilizing accelerated residuals, which demonstrate higher fidelity to their slower counterparts. By utilizing accelerated residuals operating at a rate of $N/k$, where $k$ denotes the number of layers used for candidate speculation, we are able to efficiently generate speculative tokens for decoding.\footnote{We typically set $k = 4$ to balance the trade-off between autoregressive drafting overhead and acceptance rate, as discussed in~\cref{sec:experiments}.}
 This technique not only obviates the need for multiple models during inference but also improves the overall efficiency and effectiveness of speculative decoding.

\begin{figure}
    \centering    \includegraphics[width=1\linewidth]{sections/figures/m2r2_aot_loading.pdf}
    \caption{Ahead-of-Time Expert Loading: M2R2 accelerated residual stream predicts experts required for future layers, reducing reliance on on-demand lazy loading. Speculative pre-loading is efficiently overlapped with computation of multi-head attention (MHA) and MLP transformations. Only incorrectly speculated experts are loaded lazily, resulting in faster inference steps and improved computational efficiency. Here, H indicates LBM Host while D indicates HBM Device.}
    \label{fig:moe_expert_aot_loading}
\end{figure}


\subsubsection{Ahead of Time Expert Loading:} \label{method_aot_expert_loading}

Recent advancements in sparse Mixture-of-Experts (MoE) architectures ~\cite{shazeer2017outrageously, fedus2022switch, artetxe2019massively, lepikhin2020gshard, zoph2022designing} have introduced a paradigm shift in token generation by dynamically activating only a subset of experts per input, achieving superior efficiency in comparison to dense models, particularly under memory-bound constraints of autoregressive decoding \cite{fedus2022switch, zoph2022designing}. This sparse activation approach enables MoE-based language models to generate tokens more swiftly, leveraging the efficiency of selective expert usage and avoiding the overhead of full dense layer invocation. In dense transformer models, pre-loading layers is a common strategy to enhance throughput, as computations of current layer can be overlapped with pre-loading of next layer parameters ~\cite{narayanan2021efficient, shoeybi2020megatron}. However, MoE models face a unique challenge: expert selection occurs dynamically based on previous layer’s output, making it infeasible to preload next layer’s experts in parallel. This limitation results in inherent latency, as expert loading becomes a sequential, on-demand process ~\cite{lepikhin2020gshard, fedus2022switch}.

To address this inefficiency, our method introduces a mechanism with \textit{accelerated residuals}, which not only captures key characteristics of base slower residual states but also exhibit high cosine similarity with their final counterparts (as illustrated in \cref{fig:m2r2_residual_sim}). By employing accelerated residual streams, we can effectively predict the necessary experts for future layers well in advance of their actual invocation. Specifically, using a $2\times$ accelerated residual, the experts needed for layers $2i+2$ and $2i+3$ can be identified while still computing in layer $i$, thus overcoming the bottleneck of sequential, on-demand expert selection and mitigating latency in the decoding pipeline, as shown in \cref{fig:moe_expert_aot_loading}. Note that, we use fixed set of accelerator adapters for transforming accelerated residuals (as discussed in ~\cref{m2r2_method}) while slow residual is transformed via expert routing mechanism. 

Furthermore, our approach integrates a Least Recently Used (LRU) caching strategy, which enhances memory efficiency by replacing the least recently used experts with speculated experts that are anticipated to be needed in upcoming layers. This hybrid approach of preemptive expert loading with LRU caching yields substantial improvements over traditional on-demand loading or standalone caching strategies. By minimizing cache misses and efficiently managing memory, this approach addresses both compute and memory bottlenecks, leading to faster, more resource-efficient token generation in MoE architectures. A comprehensive evaluation of this strategy, in relation to state-of-the-art methods, is provided in \cref{experiments_aot}, and the compute and memory traces on an A100 GPU are detailed in \cref{fig:moe_aot_cuda_trace}.



% Recent advancements in sparse Mixture-of-Experts (MoE) architectures have introduced the concept of utilizing distinct computational paths for different tokens \cite{shazeer2017outrageously}. This approach, wherein only a subset of experts are activated per input, enables MoE-based language models to generate tokens more swiftly compared to their dense counterparts due to memory-bound nature of auto-regressive decoding. In dense models, pre-loading layers in advance is a common strategy to enhance computational efficiency. However, this technique is not applicable to MoE models, where expert selection occurs dynamically based on the outputs of previous layers, preventing parallel pre-fetching of experts.

% Our proposed method addresses this inefficiency. Accelerated residuals, which are highly similar to their slower counterparts (see \cref{fig:similarity}), can reliably predict the necessary experts ahead of time. For instance, by utilizing $2X$ accelerated residual stream, we can predict the experts needed for the layer $2i+1$ and $2i+3$ while carrying out computation in layer $i$. This enables us to commence expert loading significantly earlier, as illustrated in \cref{expert_loading}, effectively mitigating the delays observed with the naive on-demand expert loading. Additionally, our method benefits from incorporating a Least Recently Used (LRU) strategy, where speculated experts replace those that are least recently utilized, resulting in improved performance compared to using either strategy alone. For a comprehensive evaluation, refer to \cref{moe_trace}, which provides a CUDA compute and memory trace of our approach executed on <>.



% A naive solution involves using the residual state of the previous layer along with the gating function of the next layer to predict which experts need to be loaded, and initiating the expert loading process in parallel with the attention computation of the next layer. Yet, as shown in \cref{fig:MOE_attn_vs_loading_time}, the attention computation for medium to long contexts is considerably faster than the expert loading time, making this approach inefficient.




\subsection{Training} \label{method_training}
% This approach is feasible due to the absence of gradient conflicts, as discussed in \cref{sec:grad_conflict}.

To accelerate residual streams, we employ parallel accelerator adapters as described in \cref{m2r2_method}.  For the early exiting use-case outlined in \cref{method_early_exiting}, we define the training objective for these adapters using the following loss function, which combines cross-entropy loss at each exit $E_j$ with distillation loss at each layer $i$. Loss weights coefficients $\alpha_0$ and $\alpha_1$ are employed to balance contribution of corresponding losses.

\begin{align} \label{eq:mr_loss}
L_{\text{m2r2}} = \underbrace{-\alpha_0 \sum_{j=1}^{J} \sum_{t=1}^{T} \log p_{\theta} \left( \hat{y}_t^{E_j} \mid y_{<t}, x \right)}_{\text{cross-entropy loss}} 
+ \underbrace{\alpha_1\sum_{i=1}^{E_{J-1}} \sum_{t=1}^{T} \| \mathbf{p}_{t}^{i} - \mathbf{h}_{t}^{((i - E_{j(i)}) \cdot R_i) + E_{j(i)})} \|^2}_{\text{distillation loss}}.
\end{align}

where $\hat{y}_t^{E_j}$ denotes the predictions from the accelerated residual stream at layer $E_j$ and time step $t$, $y_t$ represents the corresponding ground truth tokens, and $x$ indicates previous context tokens. The distillation loss at each layer $i$ is computed by comparing accelerated residuals at layer $i$ with slow residuals at layer $(i - E_{j(i)}) \cdot R_i + E_{j(i)}$, where $R_i$ denotes the rate of accelerated residuals at layer $i$ while $E_{j(i)}$ represents the most recent gate layer index such that $E_{j(i)} <= i$. \( J \) represents the total number of early exit gates, N denotes number of hidden layers and $E_j$ denotes layer index corresponding to gate index $j$ and \( T \) denotes the sequence length. 

In dynamic compute settings, after training of accelerator adapters, we optimize the query, key, and value parameters governing the ARLA routers (see ~\cref{method_arla}) across all exits in parallel on binary cross entropy loss between predicted decision and ground truth exiting decision. The ground truth labels for the router are determined based on whether the application of the final logit head on $\hat{y}_t^{E_j}$ yields the correct next-token prediction. 


% The objective for this optimization is defined by the following loss function:


%TODO are equations required ? 
% \begin{equation} \label{eq:arla_loss_combined}\small
%     L_{\text{arla}} = -\frac{1}{N} \sum_{t=1}^{T} \left( \sum_{j=1}^{E_n} \left[ O_t^{E_j} \log(\hat{O}_t^{E_j}) + (1 - O_t^{E_j}) \log(1 - \hat{O}_t^{E_j}) \right] \right), \quad \text{where} \quad 
%     O_t^{E_j} = \begin{cases} 
%     1, & \text{if } L(\hat{y}_t^{E_j}) = y_t^{E_j} \\
%     0, & \text{otherwise}
%     \end{cases}
% \end{equation}

% where $\hat{O}_t^{E_j}$ represents the binary predicted logits produced by the vertical latent attention router, as described in \cref{sec:arla}, at gate $E_j$ and time step $t$, and $O_t^{E_j}$ denotes the corresponding ground truth labels. The ground truth labels for the router are determined based on whether the application of the logit head on $\hat{y}_t^{E_j}$ yields the correct next-token prediction. The parameters controlling vertical latent attention are trained concurrently to ensure consistency and efficient use of computational resources.

For self-speculative decoding, as described in \cref{method_self_speculative_decoding}, the training objective remains the same as \cref{eq:mr_loss}, but with the number of intervals set to $J = 1$ and the rate of residual transformation set to $R_n = N/k$, where the first $k$ layers generate speculative candidate tokens. In the context of Ahead-of-Time Expert Loading for Mixture-of-Experts (MoE) models (see \cref{method_aot_expert_loading}), setting the rate of residual transformation to $R_n = 2$ typically offers a good trade-off between the accuracy of expert speculation and AoT pre-loading of experts. 

% Thus, we set $J = 1$ and $E_1 = 16$.


~\subsection{FLOPs Optimization} \label{sec:flops_optimization}

Naively implemented, M2R2 incurs higher FLOP overhead compared to traditional speculative decoding and early exiting approaches such as ~\cite{medusa, schuster2022confident, Tang2024}. However, modern accelerators demonstrate compute bandwidth that exceeds memory access bandwidth by an order of magnitude or more~\cite{databricksLLMInference2023, jouppi2021ten}, meaning increased FLOPs do not necessarily translate to increased decoding latency. Nevertheless, to ensure fair comparison and efficiency in compute bound scenarios, we introduce targeted optimizations.

~\textbf{Attention FLOPs Optimization} For medium-to-long context lengths, attention computation dominates FLOPs in the self-attention layer, surpassing the contribution from MLP layers. Specifically, matrix multiplications involving queries, cached keys, and cached values scale with $l_{kv} * l_{q}$ where $l_{kv}$ denotes previous context length and $l_q$ denotes current query length. Since M2R2 pairs accelerated residuals with slow residuals, a naive implementation results in twice the FLOPs consumption compared to a standard attention layer. To address this, we limit the attention of accelerated residual stream to selectively attend to the top-k most relevant tokens, identified by the slow residual stream based on top attention coefficients\footnote{We set to k = 64 and attend to top 64 tokens as identified by the slow residual stream.}. This is possible since slow and accelerated residual streams are processed in same forward pass and accelerated streams have access to attention coefficients of slow stream. Note that, the faster residual stream still retains the flexibility to assign distinct attention coefficients to these tokens. Furthermore, we design the faster residual stream to employ only 8 attention heads, compared to the 32 heads used in the slow residual stream of the Phi-3 model, reducing query, key, value, and output projection FLOPs by a factor of 1/4. ~\cref{fig:m2r2_num_heads_ablation} indicates effect of using a slicker stream on alignment. As depicted, using $\hat{n}_h = 8$ offers a good trade-off between alignment and FLOPs overhead. 

~\textbf{MLP FLOPs Optimization} The accelerator adapters operating on the accelerated residual stream are intentionally designed with lower rank than their counterparts in the base model. This reduces FLOP overhead by a factor proportional to $hiddenSize / rank$. Additionally, since the faster residual stream uses only 8 attention heads (compared to 32 in the slow residual stream of Phi-3), the subsequent MLP layers process a smaller set of activations, further reducing FLOPs by another factor of 1/4.

These optimizations significantly reduce the FLOP overhead per speculative draft generation, as illustrated in ~\cref{fig:flops_optmization}. Notably, while traditional early-exiting speculative approaches such as DEED require propagating the full slow residual state through the initial layers, incurring substantial computational costs, M2R2 achieves efficient token generation via slimmer, low-rank faster residual streams. In contrast, Medusa introduces considerable FLOP overhead due to per-head computations scaling with $d^2+dv$\footnote{Here $d$ denotes hidden state dimension while $v$ denotes vocab size.}, whereas M2R2 employs low-rank layers for both MLP and language modeling heads, maintaining computational efficiency. All experiments involving the M2R2 approach, as detailed in ~\cref{sec:experiments}, are conducted using these FLOPs optimizations.









% \[
% O_t^{E_j} = 
% \begin{cases} 
% 1, & \text{if } L(\hat{y}_t^{E_j}) = y_t^{E_j} \\
% 0, & \text{otherwise}
% \end{cases}
% \]




%add distillation
% We train accelerator adapters described in \cref{m2r2_method} to accelerate residual streams on next token prediction all in parallel since there are no gradient conflict issues as described in \cref{sec:grad_conflict}.

% \begin{align} \label{eq:mr_loss}
% L_{mr} =  & -\sum_{j = 1}^{E_n} (\sum_{t=1}^{T}\log p_{\theta} (\hat{y}_t^{E_j} | \hat{y}_{<t}, x)) \nonumber
% \end{align}

% where $\hat{y_t^{E_j}}$ denotes predicted logits obtained from accelerated residual stream at gate $E_j$ and time-step $t$ while $y_t^{E_j}$ denotes corresponding truth tokens. 

% Upon training of adapters responsible for accelerating residual streams, we train query, key, value parameters responsible for vertical latent attention of all gates in parallel as

% \begin{equation} \label{eq:arla_loss}
%     L_{arla} = -\frac{1}{N} (\sum_{t=1}^{T}(1\sum_{j=1}^{E_n} \left[ O_t^{E_j} \log(\hat{O}_t^{E_j}) + (1 - o_t^{E_j}) \log(1 - \hat{o_t}_{E_j}) \right]))
% \end{equation}

% where $\hat{O_t^{E_j}}$ denotes binary predicted logits obtained from vertical latent attention router described in \cref{sec:arla} at gate $E_j$ and timestep $t$ while $O_t^{E_j}$ denotes corresponding truth label. Truth labels for router are obtained by computing whether logit head application on $\hat{y}_t^j$ results in true next token prediction. Formally speaking, 

% $O_t^{E_j} = 1 if L(\hat{y_t^{E_j}}) == y_t^{E_j} , 0 otherwise$. 

% Parameters responsible for vertical latent attention are also trained in parallel as well. 

%todo: training slow and fast residuals together and distillation can be two training mdoes. 
%Distillation can be an ablation. 




% Although transformer decoding is memory bound on most mainstream accelerators, there could be scenarios where flop savings are crucial. For instance, on on-device settings power consumption is directly correlated with flops per decoding step and reducing flops does help with overall energy consumption. Vanilla early exiting methods help with flop reduction but suffer from mismatch between training and inference due to early exited tokens. If token at decoding step $t$, $T_t$ exited at layer $E_i$, while token $T_{t+k}$ exits at layer $E_j$ such that $E_i < E_j$, hidden state $H_{t+k}l$ does not have corresponding hidden state $H_tl$ to attend to where $E_i < l <= E_j$. One solution that's often used in literature is to rely on last hidden state available, $H_t{E_j}$, however it tends to be sub-optimal and does affect generation quality \cite{ref}.  To alleviate this mismatch while reducing flops, we train router such that attention mask between token $T_{t+k}$ and token $T_{<t+k}$ is given by: 

% \begin{equation}
%     a_{T_{{t+k}{T_{<t+k}}} = 1 if  E_{T_{<t+k}} >= E{T_{t+k}}
%     else 0
% \end{equation}

% This attention mask enables router to account for exited tokens and get trained accordingly. Since attention mechanism during decoding remains exactly same as that during training, impact on generation quality tends to be minimal as noted in \cref{fig:gen_auality_with_and_without_recompute_attention_show_flops}.  Although MoD does not suffer from training and inference mismatch, we observe that it suffers from discountinuity between pre-training and super-vised fine-tuning resulting in sub-optimal perplexity. On the other hand, our method doesn't not require pre-training , doesn't suffer from discountinuity, and achieves much better perplexity in super-vised fine-tuning and instruction tuning setups as shown in \cref{fig:Mod_vs_m2r2_loss_curves}.






% Our techniques are directly applicable in such scenarios.    




%expert loading with cuda streams in experiments