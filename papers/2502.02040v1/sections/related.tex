\section{Related Work} \label{sec:related}

The inference speed of large language models (LLMs) is often constrained by the sequential nature of auto-regressive decoding, which necessitates a complete forward pass of the network for each token generated. To mitigate the high inference latency associated with LLMs, various strategies have been proposed to reduce their memory footprint. Techniques such as model quantization \cite{frantar2022gptq,yao2022zeroquant,dettmers2023spqr}, knowledge distillation to smaller models \cite{gu2023knowledge,agarwal2023gkd}, and pruning \cite{frantar2023sparsegpt,sun2023simple} have emerged as effective solutions. However, these strategies often neglect the variational complexity inherent in each token, resulting in a reliance on static computation for all tokens. To better address this issue, several early exiting approaches have been developed to facilitate dynamic computation. These methods focus on terminating residual transformations early for simpler tokens, achieving significant speedups in embedding models \cite{xin2020deepspeed,hou2020dynabert,varshney2022model}.
In the context of sequence generation models, techniques like Confident Adaptive Language Modeling (CALM) \cite{schuster2022confident} and Depth-Adaptive Transformers \cite{elbayad2020depthadaptive} have effectively employed early exiting by integrating classifiers into the decoder layers. However, these approaches are constrained by key-value (KV) cache mismatches that arise between the training and inference phases, as KV states are not accessible for tokens that are early-exited. To mitigate these limitations, skip decoding \cite{delcorro2023skipdecode} has been introduced. This method allows for bypassing a progressively increasing number of layers based on the tokenâ€™s position in the decoded sequence. While this approach effectively circumvents KV mismatches, the pre-defined limitations on the number of bypassed layers can lead to suboptimal generation quality.

Another promising direction involves conditioning residual transformations at each layer through the use of a router. For example, CoLT5 \cite{ainslie2023colt5} employs conditional routing to determine whether a token should follow a heavy or light computational pathway for each feedforward layer in encoder-decoder models. Mixture-of-depths \cite{raposo2024mixture} builds upon this idea by introducing a predictive router at each layer, which enables efficient inference for conditional computation in decoder-only models. Although conditional routing demonstrates potential during pre-training, as illustrated in \cref{sec:experiments}, its effectiveness during supervised fine-tuning and instruction tuning remains limited. This restricts the applicability of this technique across a wider array of publicly available pre-trained models.

Speculative decoding (SD) has also emerged as a potent method for accelerating autoregressive inference. Techniques such as the original SD framework \cite{leviathan2023fast,chen2023accelerating} utilize a smaller draft model to generate token candidates, which are subsequently validated by the target model, achieving speedups of 2-3x. However, this dual-model approach complicates deployment, as it necessitates hosting both models in memory, which can be resource-intensive in constrained environments. Alternatives like Medusa offer single-model solutions but are limited by their inability to account for token dependencies. In contrast, our approach introduces dependencies between speculative tokens, resulting in more coherent and efficient speculation, thereby achieving higher decoding speedups.

The recent proliferation of Mixture-of-Experts (MoE) language models builds on a long-established concept ~\cite{jacobs1991adaptive, jordan1994hierarchical} of training ensembles of specialized models or ``experts,'' and employing a gating function to select the appropriate expert for a given task. Shazeer et al.~\cite{shazeer2017outrageously} further this idea by developing a sparsely gated Mixture-of-Experts language model. Numerous studies have since explored the application of MoE architectures in Transformer-based models for tasks such as machine translation \cite{lepikhin2021gshard}, masked language modeling \cite{fedus2021switch}, and general-purpose LLMs \cite{du2022glam}. Recently, state-of-the-art sparse Mixture-of-Experts models, such as Mixtral-8x7B \cite{jiang2024mixtral} and OLMoE \cite{muennighoff2024olmoe}, have been released, outperforming their open-source dense transformer counterparts across several benchmarks.

% For efficient inference in MoE models, expert caching strategies, such as least-recently-used (LRU) caching, have been proposed to optimize expert utilization. For example, \cite{eliseev2023fast} utilizes residual states from previous layers to inform expert selection; however, the resultant speedups are limited by their reliance on earlier layers for initialization. Expert loading for the current layer can only be activated after the attention computation of the previous layer is complete. Our approach overcomes these constraints by enabling anticipatory expert selection mechanisms through the use of accelerated residuals.




