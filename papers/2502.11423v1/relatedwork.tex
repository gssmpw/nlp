\section{Related Works}
\label{sec:related}

\subsection{Personalized Dialogue Generation}
\label{sec:related.1} 
Personalized dialogue systems have increasingly focused on leveraging user-specific information for more contextually aligned interactions~\cite{li2016persona, zhang2018personalizing, zhang2019dialogpt, roller2020recipes}. Early approaches typically involved training generative models with VAE~\cite{lee2022improving} to ensure dialogue coherence or NLI components~\cite{song2021bob, chen2023learning, zhou2023simoap} to capture persona representations. These persona representations often defined as descriptive sentences~\cite{zhang2018personalizing, dinan2020second} or key-value attributes~\cite{qian2017assigning, gao2023livechat}. With the advent of large language models and subsequent instruction tuning methods~\cite{ouyang2022training}, persona information can now be more flexibly embedded directly into system prompts~\cite{yang2023palr, wang2023rolellm}.

Recent research has also advanced multi-agent~\cite{park2023generative} or dual-persona~\cite{xu2022cosplay, jandaghi2023faithful} strategies, enabling two distinct personas to converse within a single session. This approach enhances personalized-chatbot capabilities~\cite{shuster2022blenderbot, lee2023p5} and supports large-scale synthetic data generation~\cite{jandaghi2023faithful} for further personalization. 
%Our work aligns with these trends by examining how persona polarity—particularly positive, negative, and ambiguous sentiments—influences dialogue quality in LLM-driven systems.

\subsection{Dialogue Evaluation}
\label{sec:related.2} 
Evaluating open-domain dialogue is inherently multifaceted, reflecting diverse aspects such as coherence, fluency, and persona consistency~\cite{wang2024learning, samuel2024personagym}. Traditional metrics like ROUGE~\cite{lin2004rouge} and BLEU~\cite{papineni2002bleu} often fail to capture higher-level qualities. Consequently, specialized metrics leveraging pretrained models—including C score~\cite{madotto2019personalizing} (for consistency), QuantiDCE~\cite{ye2021towards}, and PairEval~\cite{park2024paireval} (for coherence)—have gained traction~\cite{ghazarian2022deam, li2024dialogues}. We adopt these automated metrics for quantifiable evaluation.

In parallel, LLM-based evaluation strategies have rapidly emerged as a cost-effective alternative to human annotation~\cite{chiang2023can}. Leveraging Chain-of-Thought prompting~\cite{wei2022chain} further enhances evaluative transparency, allowing models to articulate their reasoning~\cite{liu2023g}. In our work, we integrate both traditional and LLM-based metrics to comprehensively assess persona-driven dialogue.

\subsection{Sentimental Sensitivity in LLMs}
\label{sec:related.3} 
Large Language Models (LLMs) are known to be highly sensitive to a variety of factors, including prompt order, language, cultural context, and sentiment~\cite{lu2021fantastically, dang2024rlhf, shen2024understanding, kwok2024evaluating}. As a difference perspective from previous works, we focus on sentimental sensitivity in LLMs in our work.
Although instruction tuning~\cite{ouyang2022training} and RLHF~\cite{dang2024rlhf} can mitigate these effects, recent studies still show that contextual sentiment can strongly influence model outputs~\cite{liu2024large, wu2024evaluating}. However, while much of the existing research focuses on sentiment that arises naturally in generated text, less work has considered sentimental polarity embedded naturally in explicit persona definitions. This underexplored avenue is central to our investigation, as it can profoundly affect both the coherence and consistency of persona-driven dialogues.