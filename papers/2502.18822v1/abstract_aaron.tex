In this project, our goal is to determine how to leverage the world-knowledge of pretrained large language models for efficient and robust learning in multiagent decision making.  We examine this in a taxi routing and assignment problem where agents must decide how to best pick up passengers in order to minimize overall waiting time.  While this problem is situated on a graphical road network, we show that with the proper prompting zero-shot performance is quite strong on this task. Furthermore, with limited fine-tuning along with the roll out algorithm for look ahead, LLMs can out-compete existing approaches with drastically fewer environmental interactions.  We also explore the benefits of various automated prompting approaches and show that including certain easy-to-compute signals in the prompt significantly improves performance.  Finally, we demonstrate additional benefits of the built-in semantic understanding of the LLM, and show that it can easily account for different environmental factors with simple prompting.  For example by including a simple description of current weather conditions in the prompt, the LLM can automatically update it's expectations for the number of incoming requests without further training.