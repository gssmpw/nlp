%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{colortbl} % Add this for coloring cells
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{lightblue}{rgb}{0.5, 0.8, 0.9} % Light blue color
\usepackage{tcolorbox}
% Define a style for a text box
\newtcolorbox{mytextbox}[2][]{ % #1: Optional parameters, #2: Title text
    colback=lightblue!10,       % Background color (light blue)
    colframe=lightblue!90!black,     % Border color (dark blue)
    fonttitle=\bfseries,        % Title font
    title=#2,                   % Title text
    coltitle=white,             % Title text color (white)
    sharp corners,              % Optional: sharp corners
    #1                          % Optional parameters
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{ Data-Efficient Multi-Agent Spatial Planning with LLMs \\}

% Multi-agent
% Spatial
% Resource Allocation
% LLMs

% On the spatial reasoning abilities of LLMs in resources allocation problems

% Planning
% RL Framework

% Multi-agent spatial planning with LLMs

% Rapidly training LLMs for spatial planning in multi-agent reinforcement learning

% Efficient learning in LLMs for spatial planning in multi-agent reinforcement learning

% LLMs are efficient planners in multi-agent reinforcement learning

% Efficient tailoring of LLMs for multi-agent planning/decision-making/reinforcement learning

% Data-efficient multi-agent spatial planning with LLMs

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Huangyuan Su}{yyy,sch}
\icmlauthor{Aaron Walsman}{yyy,sch}
\icmlauthor{Daniel Garces }{sch}
\icmlauthor{Sham Kakade}{yyy,sch}
\icmlauthor{Stephanie Gil}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{John A. Paulson School Of Engineering And Applied Sciences, Harvard University, 150 Western Ave, United States}
\icmlaffiliation{sch}{Kempner Institute, 150 Western Ave, United States}

\icmlcorrespondingauthor{Huangyuan Su}{csu@g.harvard.edu}
\icmlcorrespondingauthor{Stephanie Gil}{sgil@g.harvard.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%g bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{} 

\begin{abstract}
% In this project, our main goal is to answer the following question:

% \begin{quote}
% \textit{How can we leverage the world-knowledge of pre-trained large language models for efficient and robust learning in multi-agent decision making.}
% \end{quote}

In this project, our goal is to determine how to leverage the world-knowledge of pretrained large language models for efficient and robust learning in multiagent decision making. We examine this in a taxi routing and assignment problem where agents must decide how to best pick up passengers in order to minimize overall waiting time. While this problem is situated on a graphical road network, we show that with the proper prompting zero-shot performance is quite strong on this task. Furthermore, with limited fine-tuning along with the one-at-a-time rollout algorithm for look ahead, LLMs can out-compete existing approaches with 50 times fewer environmental interactions. We also explore the benefits of various linguistic prompting approaches and show that including certain easy-to-compute information in the prompt significantly improves performance. Finally, we highlight the LLM’s built-in semantic understanding, showing its ability to adapt to environmental factors through simple prompts.
% For example by including a simple description of current weather conditions in the prompt, the LLM can automatically update it's expectations for the cost of traveling a road without further training. 
\end{abstract}
% \label{submission}
% \begin{center}
% % \textbf{\texttt{http://icml.cc/}}
% \end{center}

% \input{intro_aaron}
\section{Introduction}

Resource allocation in rapidly changing dynamic environments is an important logistics and controls problem with huge economic impact \cite{kondor2022cost, vazifeh2018addressing}.  However, many approaches to these problems train models for specific environments from scratch using model-based or model-free reinforcement learning (RL), which can be challenging in settings where data collection is expensive.
Recently, as large language models (LLMs) have increased in capability \cite{ahn2022can, o1}, they have been increasingly deployed on problems outside their traditional domain by developing prompting strategies that convert other data modalities into natural language.  In examples ranging from biology \cite{jumper2021highly} to chess \cite{zhang2024transcendence} to robotics \cite{ahn2022can, yang2023foundation}, researchers have shown that LLMs can accelerate learning and in some cases improve capabilities due to the inherent world knowledge that is built into them during pretraining.

In this work, we explore the application of LLMs to the challenging resource allocation problem of multi-agent taxi routing in a graph-based road network. Specifically, for each agent, we provide textual descriptions of the underlying environment, the current state of all agents, and all outstanding requests as input to the LLM. The LLM outputs its reasoning and final decision for where to travel next or pickup a passenger for that agent. We show that Llama3-8B-Instruct \cite{dubey2024llama} not only offers strong zero-shot performance in this setting, but also that when fine-tuned and combined with the rollout algorithm \cite{bertsekas2021multiagent, bertsekas1999rollout}, it can outperform previous state-of-the-art approaches with fewer total environment interactions during training.  This efficiency is important in real-world problems that are difficult to simulate and where data collection is expensive.

% We also provide guidance for prompt engineering for graph-based data, and show that incorporating additional  information that is easy-to-compute from the underlying physical environment, such as shortest paths to passengers can significantly improve performance.

We also offer guidance on prompt engineering for graph-based data and demonstrate that incorporating easily computable information from the underlying physical environment—such as shortest paths to passengers—can significantly enhance performance. 

In a preliminary case study, we demonstrate the ability of LLMs to enhance planning performance by incorporating additional contextual information: they can react to semantic descriptions of the environment to anticipate relevant changes in dynamics. For example, an LLM prompted with the additional information that it is currently raining can predict increased cost of traveling and react accordingly. We include an example of this in the Appendix \cref{sec: semantic-info}.

Finally, we show that the use of LLMs in this setting is not without drawbacks. In some scenarios the LLM will hallucinate and provide controls that are impossible given the current graph structure and outstanding requests. We show that these can be largely mitigated by finetuning or incorporating feasibility checking-then-resampling.

Our primary contributions are:
\begin{enumerate}
\item Through experiments, we demonstrate that even in zero-shot settings, Llama 3-8B-Instruct performs remarkably well. Moreover, our finetuned rollout policy surpasses previous state-of-the-art approaches while requiring significantly less training data. Finetuning also effectively eliminates spatial hallucinations observed in the prompting-based methods.
% Something about zero-shot capability and new best performance.
\item We conduct a comprehensive ablation study on prompting techniques, offering valuable insights into how LLMs can serve as planners for multi-agent tasks.
% Something about prompt engineering guidance.
\item We demonstrate that our prompting-based methods generalize effectively to a significantly larger map with more agents, featuring a quadratic increase in the number of nodes and roads.
% Something about avoiding hallucinations.
% \item 
% Something about adapting to new semantic info (raining example)
% Am I missing anything?
\end{enumerate}


% The rest of the paper is organized as follows: In Section 2, we present the formulation of the multiagent taxicab routing problem, the online play with offline approximation rollout framework, and our language models-based approach. In Sec. 3, we present our numerical results on the San Francisco taxicab pickup dataset\footnote{https://ieee-dataport.org/open-access/crawdad-epflmobility}. In Sec. 4, we review relevant literature. Finally, we discuss the limitations and future works and conclude this paper.


% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% ``-G'' dvips to use the config.pdf file.


% $\mathtt{epackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% \verb|\icmltitlerunning{...}|


\section{Problem Formulation}

\begin{figure}[ht]  % Start the figure environment
    \centering  % Center the image within the figure environment
    \includegraphics[width=0.6\columnwidth]{figs/fig_1_directed.png}  % Specify the file and width
    \caption{In this multi-agent setting, taxis must decide where to move and pickup passengers in order to minimize overall passenger wait time. At each time step, taxis can move to any neighboring intersection, where arrows indicate one-way road segments.  In this example there are two outstanding requests, one in the top left and another in the bottom right.  While the green taxi is closest to the request in the bottom right, the task of allocating the red and yellow taxi is more challenging.  Both are four stops away from the request in the top left, but which one is sent, and where the other moves in the meantime will determine how prepared the taxis are for picking up future requests.}  % Add a caption
    \label{fig:fig_1}  % Add a label for referencing
    \vskip -0.1in
\end{figure}

\subsection{Environment} We study the problem through a multi-agent taxicab routing problem introduced in \cite{garces2023multiagent}. Given an urban environment with a fixed street topology, represented as a directed graph, $\mathbb{G} = (\mathbb{V}, \mathbb{E})$, where $\mathbb{V} = [n]$ is the set of indices of the street intersections, and $\mathbb{E} \subseteq \{(i, j)|i, j \in \mathbb{V} \}$ is the set of directed streets connecting adjacent intersections. We define $\mathcal{N}(i) = \{j|j \in \mathbb{V},(i,j) \in \mathbb{E}\}$ as the neighboring intersections for intersection $i$.   

Requests stochastically enter the system according to unknown distributions. A \textbf{request} $r$ is defined by the closest intersections to its desired pickup and dropoff locations, the time at which it enters the system; and if it has been assigned to any agent.  We define a \textbf{scenario} as a fixed map topology and a sequence of requests.

\subsection{State and Action} We assume there are a total of $m$ agents and each agent can perfectly observe all requests, and agents’ locations and occupancy status. We use letters $s, u$ to denote numerical states and controls, and letters $x, y$ to denote language input and output sequences. The state $s_k$ at time $k$ contains locations for all $m$ agents in terms of node indices, the time remaining in the currently assigned trip for all agents, and the set of outstanding requests at time $k$. If the remaining time for an agent $l$ is zero, the agent is considered available, and new requests can be assigned to it. 

We define the action space for an agent $l$ at state $s_k$ as $U^l_k(s_k)$. At each time step, this action space contains the following actions. If an agent is not currently assigned, the agent may move to one of the neighboring intersections, remain in its current position, or pick up a passenger at the current location. On the other hand, if it is assigned, it must move along the shortest path to the passenger's current destination. 

Our formulation of the action space allows for separable control constraints for each agent. For this reason, we express the joint control space at time $k$, $U_k(s_k)$, as the Cartesian product of the local control sets $U^1_k(s_k) \times \cdots \times U^m_k (s_k)$. As the control space grows exponentially with the number of agents, finding an optimal policy becomes intractable, necessitating the search for suboptimal solutions. 
% $\hat{\mu}_k$ is a function that maps $F(x_k,l)$ to the rollout control for agent $l$.
% $\tau_k^l \in N^+$

% We assume there are a total of $m$ agents and each agent can perfectly observe all requests, and all agents’ locations and occupancy status. We use letters $s, u$ to denote numerical states and controls and letters $x, y$ to denote language input and output sequences. The state at time $k$ is $s_k = (\mathbf{\nu}_k, \mathbf{\tau}_k, \mathbf{r}_k)$. $\nu_k = [\nu_k^1, ..., \nu_k^m]$ lists locations for all $m$ agents at time $k$ represented by the index
% of the graph location
% %of the closest intersection
% %to the geographical coordinates
% of each agent. $\tau_k = [\tau_k^1 , \dots, \tau_k^m ]$, is a list of time remaining in the currently assigned trip for all agents at time $k$, and $\mathbf{r}_k$ is the set of outstanding requests at time $k$. If $\tau_k^l = 0$, then agent $l$ is available and new requests can be assigned to the agent, otherwise $\tau_k^l \in N^+$. 
 

% Mathematically, agent $l$'s control space at time $k$ is defined as follows:
% \[
% U^l_k (x_k) = \begin{cases} \{\mathcal{N}(\nu _k^l ), \nu_k^l, \zeta\} \text{, if } \tau_k^l = 0 \text{ (agent is available)} \\
% \{h\} \text{, otherwise (must complete current trip)}
% \end{cases}
% \]
% where $\zeta$ is a special pickup control available when a request $r \in \mathbf{r}_k$, such that $\rho_r = \nu_k^l$ and $h$ is the next hop in the shortest path (via Dijkstra’s algorithm) from $\nu_k^l$ to the dropoff location $\delta_r$ for the agent’s assigned request $r$.
% $\tau : V \times V \rightarrow N^+$ is a function that gives the time taken to traverse the shortest path between two locations. 


\subsection{Stochastic Dynamic Programming} This routing problem can be formulated as a finite horizon, stochastic Markov Decision Process (MDP). 
The global \textbf{objective} of all agents is to find a pickup strategy that minimizes the total wait time of passengers
%. The cost of policy \(\pi\) starting from state \(s_1\) at time \(1\) is
starting from the initial state $s_1$. A policy \(\pi = \{\mu_1, \dots, \mu_N \}\) is a list of functions, where $\mu_k$ maps state $s_k$ into control \(u_k = \mu_k(x_k) \in U_k(x_k)\). We define the cost of a policy $\pi$ starting at state $s_1$ as:
\[J_{\pi}(s_1) = \mathbb{E} \left[g_N(s_N) + \sum_{k=1}^{N - 1} g_k(s_k, \mu_k(s_k))\right].\]

In the equation above, $s_{k+1}$ is the state at time $k+1$ after application of control $u_k$ at time $k$ from the current state $s_k$, and $g_k(s_k, u_k)$ is the per-step cost, which is the number of current outstanding requests.
% \eta, \rho, \delta
% Notice that actions taken by the agents do not affect the requests coming up external environment.
Note that the agents' actions do not affect the distribution of incoming requests.

\section{Background and Preliminaries}

\subsection{(One-at-a-time) Rollout and Offline Approximation} 
\label{sec: rollout-offline}
We adopt the \textbf{one-at-a-time rollout} framework from \cite{bertsekas2021multiagent}, which solves multiple smaller lookahead optimizations to derive a policy that improves upon an easy to compute heuristic known as the base policy. Agent $l$’s one-at-a-time rollout control is conditioned on the actions of other agents, where the controls for agents $1, \dots, l-1$  are already determined, and the actions for agents $l+1, \dots, m$ are computed using the base policy,  $i.e.$, .
\begin{align}
\label{eqn}
    \Tilde{u}_k \in \argmin_{u_k^l \in U_k^l(s_k)} \mathbb{E} \left[g_k(s_k, u_k) + \Tilde{J}_k(s_{k+1}) \right]
    % \eta, \rho, \delta
\end{align}

where \(u_k = (\Tilde{u}_k^1, \dots, \Tilde{u}_k^{l-1}, u_k^l, \mu_k(s_k), \dots, \mu_k(s_k) )\). We use \(\in\) instead of \(=\) as the optimal control is generally not unique. This process is illustrated in greater detail in Fig. \ref{fig: rollout}. 

Notably, this approach ensures that the control space scales linearly with the number of taxis, rather than exponentially. This is crucial given the high computational cost of LLM inference, as it avoids an exponential increase in inference calls. This rollout also allows for faster generation of high-quality training samples for policy evaluation. Furthermore, we use certainty equivalence \cite{bertsekas1999rollout} approximation to reduce the number of samples required to approximate the expectation, without changing the rollout algorithm. \noindent In our implementation of the CE, we fix the disturbances in distribution parameters across all the rollout steps, only preserving the stochasticity of the order in which requests arrive and the pairing between pickup and dropoff locations for each request.  
% $\eta, \rho$ and $\delta$

\begin{figure}[ht]
    \centering
    \centerline{\includegraphics[width=\columnwidth]{figs/rollout.png}}
    \caption{We use multi-agent rollout \cite{bertsekas2021multiagent} for policy improvement. In this setting, we compute updated actions for each agent by computing many rollouts for each possible action it could take, and we choose the action resulting in the lowest average cost. In order to avoid computing rollouts for all combinations in the exponential joint multi-agent action space, agent actions are computed one at a time. In the example shown here, we are estimating an improved action for the red taxi. Each of its actions are combined with the best action found for the green taxi which has already been computed, and an action chosen by the base policy of the yellow taxi, which has not yet been updated by rollout.}
    \label{fig: rollout}
    \vskip -0.1in
\end{figure}


\subsection{Online Play and Approximate Policy Iteration}
Assuming our finetuned offline model successfully approximates the rollout policy, we then apply the offline trained models as base policy in online play, which according to recent theory \cite{bertsekas2022lessons} serves as an approximate Newton step to further improve the performance of the learned policy. Online play’s control $u_k$ at state $s_k$ is given by \cref{eqn}, with $\bar{u} = (u_k^1, \dots, \bar{u}^{l - 1}, u^l_k, \hat{\mu}(F(s_k, l+1)), \dots, \hat{\mu}(F(s_k, m)))$.

If the policy approximation, $\hat{\pi}$, correctly approximates the rollout policy $\tilde{\pi}$ on the current demand model, we expect online play policy $\bar{\pi}$ to outperform $\tilde{\pi}$ following the cost improvement property of the approximate policy iteration~\cite{bertsekas2019reinforcement}. If $\hat{\pi}$ fails to approximate the rollout policy $\tilde{\pi}$, the cost improvement property will not hold and the online play will not provide a significant improvement \cite{garces2023multiagent}.

% because of a change in the current demand model
Then, an \textbf{offline approximation} $\hat{\pi}$ of this one-at-a-time rollout-based RL policy is achieved by supervised finetuning of Llama 3-8B model. To train the policy approximation, we generate a dataset of random initial taxi and request locations. The training feature at state $s_k$ for an agent contain the state and other agent’s actions, \textit{i.e.}, $l \in \{1,...,m\}$ is $F(x_k, l) = (x_k, u_k^1, \dots, u_k^{l-1}, \mu^{l+1}_k(x_k), \dots, \mu^m_k(x_k))$ and the training label is $\mu_k^l$, which are the corresponding one-at-a-time rollout controls. 

% \textbf{Approximate Policy Iteration. } We take one more policy iteration step to improve the performance.

\section{Our Method}

\subsection{Language Models as Base Policies}
\label{sec: prompt-methods} 
We study what physical information is pertinent to LLM decision making, how to encode that information in language for better LLM decision making, and how to use physical information to alleviate hallucinations~\cite{gil2023physicality}. 

We use \(p\) to denote a pre-trained LLM. For language model policies, we use chat template \cite{chat} to format the inputs and encapsulate the environmental information in the system prompt. An example of the system prompt is given in \cref{sec: sys_prompt}. For all policies that use LLMs, we use prompts to convert state and action information (of other agents) as inputs to the LLMs: \(x_k = \text{prompt}(s_k)\) and parse the outputs of the LLMs to get the actions: \(u_k = \text{parse}(y_k)\). The prompting function wraps input $x$ with task instructions. Furthermore, we physically ground the LLMs by also providing the shortest paths to outstanding requests, which is fast to compute from the environment. A concrete example is given in \cref{sec: cot_prompt}. We denote logically-coherent intermediate reasoning steps by \(z_1, \dots, z_n\). Furthermore, we define \textbf{hallucinations} as when the LLM outputs a next position that is neither a neighbor of the current position nor on the paths to one of the requests. In addition to zero-shot prompting, we also experiment with the following common techniques for improving performance.

% \item known actions of agents before `me' and expected actions of agents after `me'

\textbf{Few-shot.} 
We provide five examples consisting of input-output sequence pairs that illustrate the desired behavior across various states.

% \verb+\icmlauthor{...}+ \verb+\icmlaffiliation{...}+ 
\textbf{Chain-of-Thoughts} \citep{wei2022chain} (CoT) enhances language models' ability to solve complex, multi-step problems by prompting the model to generate the reasoning steps explicitly. \(z_i \sim p^{CoT}(z_i| x, z_{1\dots i-1})\). \(y \sim p^{CoT}(y|x, z_{1\dots n})\) for  reasoning by encouraging step-by-step problem-solving and making the reasoning process more interpretable. An example of our CoT prompt is provided in \cref{sec: cot_prompt}. 

\textbf{Self-Consistency} \citep{wang2022self} \textbf{w/ CoT} (CoT-SC) samples $c$ independent chains of thought: \(\forall i\in [c]\), \([z^{(i)}_{1\dots n}, y^{(i)}] \sim p^{CoT}(y|x)\), and returns the most frequent output. It enhances CoT by exploring diverse thought processes for more reliable decisions. However, it lacks local exploration within chains, and its “most frequent” heuristic works best in limited output spaces.

\textbf{Tree-of-Thoughts} \cite{yao2024tree}(ToT). Instead of generating thoughts in a linear sequence, ToT explores a tree-like structure of possible thoughts or reasoning steps. Each node \(s^{\text{ToT}} =[x, z_{1\dots i}]\) represents a partial solution, allowing the model to identify the most promising global outcomes by breadth-first or depth-first search with lookahead and backtracking. $S^{\text{ToT}}$ is the state space of all such nodes. We design a value prompt \(V(p, S^{\text{ToT}}) \sim p^{value}(v|s^{\text{ToT}})\), \(s^{\text{ToT}} \in S^{\text{ToT}}\) to reason about the cost of taking an action. We restrict the tree depth to 1, making the search algorithm simply selects the branch with the highest value (or lowest cost). An example of our ToT prompt is provided in \cref{sec: tot_prompt}. 

\textbf{Zero-shot w/ Hallucination Checking }(ZS-HC). We design a checker that detects hallucinations based on the underlying environment and incorporates this information into the prompt for reprompting. 



 % In addition, to performing approximate policy iteration, we do an additional round of data collection using rollout with the finetuned \textbf{Llama 3-8B} as the base policy.



% For both of them, ReLU activation and initializes weights using Kaiming and Xavier methods. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
% This section provides detailed benchmark results. 
% We first describe our hardware. We then describe baseline and evaluation metrics. Finally, we discuss results and analysis.

\subsection{Implementation Details}

Our main models are zero-shot \textbf{Llama 3-8B} with/without rollout, finetuned \textbf{Llama 3-8B} with/without rollout and zero-shot \textbf{Llama 3.2-3B} \cite{llama32} with/without rollout. The Llama 3 models \cite{llama3} are state-of-the-art open-source large language models that improve upon the Llama 3 herd of models \cite{dubey2024llama} in terms of reasoning capabilities, context length, and tool use. The 8B and the 70B models can satisfy a large range of use cases. For our experiments, we use the 8B model because it has reasonable zero-shot performance on our problem and can be finetuned within reasonable time and infrastucture resources.

For fine-tuning, we adapt Llama Cookbook~\cite{llamarecipes}, the official codebase released for fine-tuning and building applications. Finetuning data is collected using rollout with an instantaneous reassignment policy (IA-RA, see Section \ref{subsec:baselines} for details) as the base policy and 2000 Monte Carlo (MC) simulations.  The details for finetuning using one-at-a-time rollout, are detailed in \cref{sec: rollout-offline}. We set the learning rate to \(5 \times 10^{-5}\), the batch size to \(8\), and adhered to the default settings in Llama Recipes for all other configurations. 

The Llama 3.2-3B models are expected to perform worse on this planning problem due to their optimization for multilingual dialogue tasks and their smaller sizes. Nonetheless, we use this smaller model as a baseline to study the trade-off between speed and performance.


% Reducing uncertainty in disturbances enables us to use fewer (2000) Monte-Carlo simulations per leaf node.

\subsection{Baselines Used for Comparison}
\label{subsec:baselines}

We compare our approach with the following baselines:

\textbf{Greedy policy}: Each taxi moves towards its closest request without coordinating with other taxis. This method does not consider future demand.

\textbf{Instantaneous assignment (IA-RA)}: It matches available taxis and outstanding requests at every time step using an auction algorithm \cite{bertsekas1979distributed}. This method also does not consider future demand.

\textbf{Graph Neural Networks (GNNs) }\cite{garces2023multiagent}: Graph neural networks are an intuitive choice for this problem since the underlying environment is a road graph. Therefore, it serves as a good baseline that is of a different model architecture from transformers. For each load level, we train a separate pair of GNNs. The first GNN determines if an available agent should pick up a request in its current location, and the second GNN determines the next intersection towards which the agent should move. The move model is a multi-classifier while the pick up model is a binary classifier. 

\textbf{Rollout with Different Base Policies.} We also compare against rollout with greedy policy, IA-RA, and GNNs as the base policies. Rollout with GNNs as base policy is the previous state-of-the-art on this multi-agent routing problem.



\begin{figure*}[ht]
    \centering
    \centerline{\includegraphics[width=2\columnwidth]{figs/fig3.png}}
    \caption{Here we show two policies in the same scenario, greedy on the top row and Rollout using LLM as the base policy on the bottom.  Each column shows a time range, and the location of each taxi at the end of this range.  Colored lines show the route taken by the taxi during this interval.  In the first three frames on the far left, the LLM already starts to move the taxis into a more central location.  The first request (blue) arrives in the center of the map on frame 3, and the LLM picks it up one step earlier.  Eleven frames later on frame 14, two more requests (orange and purple) appear, then a third (pink) one frame later.  While the LLM is slower to pick up the purple request, it is able to pick up the orange and pink requests much faster and wins by three points over all.}
    \label{fig: }
\end{figure*}

    

    
    % with trained LoRA: inspired by \cite{zhong2024algorithmic}
    % \item \textbf{Human}: calculated from imitation policy trained from historical data from the San Francisco taxicab dataset.
    
    




% https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html
% apply a mask

\subsection{Hardware} We conduct experiments on Nvidia H100 or A100 GPUs. 
% Evaluation objectives (research questions)
We use two techniques to enhance inference speed: (1) running all LLMs in FP8 precision, and (2) employing tensor parallelism across four GPUs within the same node, as these approaches provide results with negligible performance loss.

\subsection{Performance Benchmark}
We mainly consider a fixed horizon of \(60\) steps, which is equivalent to \(60\) minutes. We consider a \(400\times 400 m^2\) section of San Francisco with \(42\) nodes and \(125\) edges, and \(m = 3\) taxicabs. We consider 1-minute edge travel time. For rollout, we set the planning horizon to $t_h = 10$. 

We create a test set comprised of 20 scenarios. We create one set for each load level: low, medium, and high. The primary focus of our experiments is to evaluate the performance of the methods in terms of \textbf{cost}, defined as the total waiting time (in minutes) for all riders. Additionally, we report the occurrence of hallucinations alongside \textbf{cost}. Final performance is calculated as an average across all test scenarios.


\begin{table*}[t]
    \centering
  \caption{Performance comparison of methods in terms of total waiting time in minutes. We use {\color{lightblue!40} blue} to highlight the minimum cost and \textbf{bold} the minimum cost across all base policies. For \textbf{Llama 3-8B} zero-shot, we use the best number among all zero-shot prompting methods. $\dagger$: running with number of MC simulations set to 2000; $\ddagger$: running with number of MC simulations set to 200.}
    \label{tab:comparison}
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{llcccccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Version}} & \multicolumn{2}{c}{\textbf{Low}} & \multicolumn{2}{c}{\textbf{Medium}} 
        &\multicolumn{2}{c}{\textbf{High}} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
       &  & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$} & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$} & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$} \\
             \midrule
       \rowcolor{gray!20}  \multicolumn{8}{c}{\textbf{Heuristic}}  \\
\multirow{2}{*}{\textbf{Greedy}} & Base & 10.16 & N.A. & 34.70 & N.A. & 61.28 & N.A. \\
  & Rollout$^\dagger$ & 8.95 & N.A. & 30.74 & N.A. & 52.39 & N.A. \\
\midrule
\multirow{2}{*}{\textbf{IA-RA}} & Base & 10.26 & N.A.\textbf{} & 31.78 & N.A. & 58.21 & N.A. \\
  & Rollout$^\dagger$ & 8.11 & N.A. & 27.22 & N.A. & 50.05 & N.A. \\
\midrule
\multirow{2}{*}{\textbf{Llama 3-8B }Zero-shot} & Base & $8.84$ & $3.95$  & 30.94 & 3.28 & 56.53 & 3.58  \\
  & Rollout$^\ddagger$ & 8.89 & 2.71 & 29.94 & ${2.91}$ & $56.63$ & $3.55$ \\
\midrule
\multirow{2}{*}{\textbf{Llama 3.2-3B }Zero-shot} & Base & $12.88$ & 3.47  & $37.88$ & $4.72$ & $65.89$ & $4.26$\\
  & Rollout$^\ddagger$ & $10.68$ & $3.44$ & $33.78$ & $4.93$ & $67.11$ & $5.06$ \\
               \midrule
       \rowcolor{gray!20}  \multicolumn{8}{c}{\textbf{Learning-based}}  \\
       \multirow{2}{*}{\textbf{GNN}} & Base & \textbf{8.37} & N.A. &  $30.04$ &  N.A. & $58.71$ & N.A. \\
  & Rollout$^\dagger$ & $8.23$ & N.A. & $27.38$ & N.A. & $47.32$ & N.A. \\
\midrule
\multirow{2}{*}{\textbf{Llama 3-8B} Finetuned} & Base & $8.68$ & 0  & \textbf{29.22} & 0  & $\mathbf{51.00}$ & 0 \\
  & Rollout$^\ddagger$ & \cellcolor{lightblue!40 }$7.05$ & 0 & 27.12\cellcolor{lightblue!40 } & 0 & \cellcolor{lightblue!40 }$47.11$ & 0\textbf{} \\
  
        \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
\end{table*}

% {\textbf{GPT-o1}} & Base & \cellcolor{lightblue!40 } &   & \cellcolor{lightblue!40 } \textbf{} &   & \cellcolor{lightblue!40 } &   \textbf{} \\
% \midrule

% \multirow{2}{*}{\textbf{Llama 3-70B}} & Base &  & 0\cellcolor{lightblue!40 }  & \textbf{} & 0  &  &  \textbf{} \\
%   & Rollout & \textbf{} &  &  &  &  & \textbf{} \\

% MAYBE ADD MARL METHODS




\subsection{Main Results}

We present our main results in \cref{tab:comparison} and discuss our observations and analysis below.

%\textbf{Evaluating Total Cost} First, the outperforming over all methods suggesting the effectiveness of finetuning and our offline and online play scheme. Second, we finetune the same one model across all load levels, suggesting better generalizable than GNN-based models. Third, outperforming both finetuning and rollout with 8B model suggests that it is necessary to perform both the offline approximation and online play steps. 
\textbf{Best Performing Model} Finetuning Llama, along with our offline and online play scheme outperforms all other models, including the previous state of the art GNN+rollout.  Importantly, unlike the GNN-based models, which trained a separate policy for each load level, we were able to finetune a single model for all load levels, suggesting better generalization.  Finally, the performance benefits of finetuning and rollout suggest that it is necessary to perform both the offline approximation and online play steps. While each pair of GNN models were trained with 6400 trajectories, we finetune a single Llama 3-8B model for all load levels with only 128 trajectories collected using Rollout with IA-RA as  base policy. With its larger capacity compared to GNN models, it demonstrates superior generalization capabilities. As a result, our approach achieves significantly greater \textbf{data efficiency} than the previous GNN-based method. % Approximate policy iteration: we use the finetuned Llama 3-8B model to collect another 128 trajectories, and finetune the model on the newly collected again.
% The model after approximate policy iteration is the best-performing one.

% We found that even the zero-shot achieves good performance (only slightly worse than rollout with greedy or IA-RA policy as the base policy and better than greedy/IA-RA without rollout) in all load levels. This may be explained by qualitative observations of their behavior: go to nodes of highest index (connected to most number of other nodes), and tendency in moving around to nearby nodes to increase the chance of meeting new requests.  

We also found that even the zero-shot approach performs well, slightly trailing rollout with greedy or IA-RA base policies but outperforming greedy/IA-RA without rollout across all load levels. This may be due to its inherent behaviors: prioritizing high-index nodes and navigating to nearby nodes to maximize encounters with new requests.

Zero-shot methods can exhibit spatial and reachability hallucinations, leading to suboptimal behaviors (see \cref{sec: hallu}).  While cost grows as the level of load grows, this is not the case for the number of hallucinations. Furthermore, when comparing in a large map, we see that the number of hallucinations grows linearly with the number of agents. This suggests that the hallucination rate is an inherent property of the LLMs, which is not obviously affected by load levels in the environment.
% they tendency in following the behavior of other agents (when there is no requests);

\textbf{Rollout vs. Fine-tuning. } We also observed that, through only one round of finetuning, we are able to remove spatial hallucinations.  With the improvement of the cost and alleviating hallucinations, we conclude that finetuning is better than rollout with LLMs as base policies at improving the behavior of the LLMs.
%Furthermore, when zero-shot prompting LLM is the base policy, comparing the performance of rollout policy to the base policy, we saw that applying rollout to zero-shot models hurts performance, we hypothesize that the this is because hallucinations happened frequently in MC sampling, which biased the expected cost computed from the MC samples.
When zero-shot prompted LLMs are used as the base-policy for rollout, we see that costs for rollout increase.  We hypothesize that this is caused by excess hallucinations during MC sampling, which biased the computed expected future cost. In other words, the LLM is not able to correctly approximate future costs and hence the rollout is not able to act as a Newton Step.

%  1) hallucinations can make the planning performance worse; and 2) 

\subsection{Prompting Methods}
In this section, we present the results of applying various prompting techniques \cref{sec: prompt-methods} to Llama 3-8B, aiming to identify optimal zero-shot performance (without any training or fine-tuning). For zero-shot, CoT, and ToT, we set the temperature of the LLM to 0. While for self-consistency, we set the temperature to 0.7 as that technique requires diverse samples. For ZS-HC, we set the maximum number of reprompt to 5. 
% Additionally, this analysis helps identify the most effective format for collecting data to support fine-tuning.

\begin{table*}[t]
    \centering
  \caption{Performance comparison of prompting methods in terms of total waiting time in minutes. We \textbf{bold} the best number across all base policies and use {\color{lightblue!40} blue} to denote best number across all methods.}
    \label{tab:prompt}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{llcccccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Version}} & \multicolumn{2}{c}{\textbf{Low}} & \multicolumn{2}{c}{\textbf{Medium}} 
        &\multicolumn{2}{c}{\textbf{High}} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
       &  & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$} & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$} & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$} \\
             \midrule
       \rowcolor{gray!20}  \multicolumn{8}{c}{\textbf{}}  \\
{\textbf{Zero shot}} & Base & 8.84 & 4.37 & 32.75 & 3.28 &  56.53 & 3.58 \\
\midrule
{\textbf{Few shot}} & Base & 9.26 & 2.58 & 32.06 & 3.00 & 57.68 & 1.79 \\
\midrule
{\textbf{CoT-SC}} & Base & 9.37 & 2.21 & 32.83 & 2.50 &  55.84 & 2.72 \\
\midrule
{\textbf{CoT}} & Base & 8.68  & 2.32 & 30.06 \textbf{} & 3.94 & 56.95 & 3.24 \\
\midrule
{\textbf{ToT}} & Base & 8.63 & 1.79  & \cellcolor{lightblue!40}29.50 & 2.33 & 55.63 & \textbf{}2.11 \\
\midrule
{\textbf{ZS-HC}} & Base & \cellcolor{lightblue!40}8.42 & \textbf{1.67} & 30.89  & \textbf{1.35} & \cellcolor{lightblue!40 }55.05 & \textbf{1.32} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

We present the complete numerical results of all prompting methods in \cref{tab:prompt}. We observe that CoT, ToT have the effect of alleviating hallucinations and decreasing cost. The worse performance of ToT than CoT may be because that we ask the Llama 3-8B model to be the evaluator, which is too challenging for this model. 

However, from the results, we see that CoT and ToT offer only limited improvement of the total cost under heavy load levels. Therefore, we need better methods to improve LLMs' spatial reasoning abilities. We collect data using the simple zero-shot prompt, to ensure a fair comparison between prompting methods and finetuning/rollout methods.

Surprisingly, the results also indicate that few-shot and CoT-SC do not perform better than zero-shot prompting. %This is probably because few-shot prompting, making wrong generalization (extrapolation) from the given examples.
This may be caused by the model making improper generalization (extrapolation) from the given examples.

\subsection{Ablations on Test-time Efficiency}
Compute consumption increases linearly with the number of MC simulations, which can be viewed as a hyperparameter of the rollout algorithm. This raises a critical question: how many MC simulations are necessary? To address this, we analyze the trade-off between computational cost and the number of MC simulations, using our fine-tuned 8B model. \cref{sample-complexity} shows that decreasing the number of MC simulations only leads to slightly worse performance. 

\begin{figure}[ht]
% \vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{num_sims.pdf}}
\caption{Cost (total waiting time) versus the number of MC futures sampled under low, medium, and high load levels.}
\label{sample-complexity}
\end{center}
\vskip -0.1in
\end{figure}
 % \def\tmplabel#1{[#1]}
 \vspace{-10pt}
 
\subsection{Ablations on Scalability}
We study the scalability through two sets of experiments. First, we present performance on the most difficult scenario in our test in \cref{tab:hard}. We observe that the finetuned LLM and rollout with finetune LLM as the base policy are better in these hardest scenarios, suggesting that they are better at generalization. Second, we scale our environment to a $1000\times1000 m^2$ map and 10 agents in \cref{tab:large-map}. The numbers show that LLMs can scale to this setting.

\begin{table}[t]
    \centering
  \caption{Performance comparison of methods in the two hardest scenarios. Color coding is the same as in \cref{tab:comparison}.}
    \label{tab:hard}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{llccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Version}} & \multicolumn{2}{c}{\textbf{Cost}$\downarrow$}  \\
        \cmidrule(lr){3-4} 
       &  & Scenario 12 & Scenario 14    \\
             \midrule
             \multirow{2}{*}{\textbf{Greedy}} & Base &  172  & 121 \\
  & Rollout & 161 & 109  \\
             \midrule
             \multirow{2}{*}{\textbf{IA-RA}} & Base & 183 & 117 \\
  & Rollout & 160 & 115  \\
             \midrule
       \rowcolor{gray!20}  \multicolumn{5}{c}{\textbf{Prompting} Llama 3-8B}  \\
\multirow{2}{*}{\textbf{Zero-shot}} & Base & 187 & \textbf{105}  \\
  & Rollout & \textit{167} & \textit{108} &  \\
\midrule
\textbf{Few-shot} & Base & 188 & 113  \\
\midrule
\textbf{CoT}  & Base & 188 & {108} & \\
               \midrule
\textbf{CoT-SC} & Base & 163 & 108\textbf{} & \\
               \midrule
\textbf{ToT} & Base & 184 & 116 & \\
               \midrule
               \rowcolor{gray!20}  \multicolumn{5}{c}{\textbf{Finetuned/Trained}}  \\
       \multirow{2}{*}{\textbf{GNN}} & Base & 161 &  108\textbf{}  \\
  & Rollout & 159\textbf{} & \textbf{} 105  \\
  \midrule
    \multirow{2}{*}{\textbf{Llama 3-8B} Finetuned} & Base & \textbf{160} &  114 \\
  & Rollout &  151\cellcolor{lightblue!40 } & \textbf{}  \cellcolor{lightblue!40 }104  \\
        \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
\end{table}


% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

\begin{table}[t]
    \centering
  \caption{Performance comparison of methods in terms of total waiting time in the large map with more agents. Color coding is the same as in \cref{tab:comparison}. }
    \label{tab:large-map}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{llcccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Version}} & \multicolumn{2}{c}{\textbf{Low}} & \multicolumn{2}{c}{\textbf{Medium}} 
        \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
       &  & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$} & \textbf{Cost $\downarrow$} & \textbf{Hallucination  $\downarrow$}   \\
             \midrule
       \rowcolor{gray!20}  \multicolumn{6}{c}{\textbf{}}  \\
       \multirow{2}{*}{\textbf{Greedy}} & Base & 13.05 & N.A. & 37.40 & N.A.   \\
        & Rollout & 8.30 & N.A. & 25.05 & N.A.  \\
\midrule
\multirow{2}{*}{\textbf{IA-RA}} & Base & 12.45 & N.A. & 31.20 & N.A.   \\
 & Rollout & \cellcolor{lightblue!40 } 8.20 & N.A. & 24.70 & N.A. \\
\midrule
{\textbf{Zero-shot}} & Base & 10.30 & 12.95 & 27.15 & 20.80 \\
\midrule
{\textbf{CoT-SC}} & Base & \textbf{8.58} & 9.94 & \cellcolor{lightblue!40 } 
 \textbf{24.20} & 20.05  \\
% \midrule
% {\textbf{Few-shot}} & Base & \textbf{} & \textbf{} &  &   \\
        \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
\end{table}

% \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

% \begin{table}[t]
% \caption{.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}




% \begin{definition}
% \label{def:inj}
%  $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
%  \cref{def:inj}  result:
% \begin{proposition}
% If $f$ mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
%  to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
%  (cf. \cref{ass:xfinite}) .
% \end{remark}

\section{Related Works}
% TODO: Include small paragraph describing why you chose the three areas of related works that you are focusing on
\subsection{Multi-agent Decision-making}
Multi-agent decision-making algorithms allow for the optimization of resource allocations in real-world applications such as taxi routing \cite{garces2023multiagent} and drone deliveries \cite{lee2022autonomous}. Optimal solution for these problems is intractable since it requires considering multiple scenarios of potential future requests and all relevant agent actions at each decision point, in extremely large state and control spaces that grow exponentially with the number of agents. Hence, many methods have been proposed to find competitive sub-optimal solutions. Most intuitive solutions are greedy policies, but they can be far from optimal--for example, when two taxis go to serve the same request, leaving other requests unattended. Studies \cite{kondor2022cost, vazifeh2018addressing} show that with non‐coordinating greedy policies, each additional ride‐hailing company in the market can largely increase the total number of circulating vehicles than necessary. Heuristic-based methods \cite{bertsekas1979distributed, bertsimas2019online, croes1958method} often generate myopic policies due to the lack of consideration for future demand. RL methods have been proposed in both offline \cite{ulmer2019offline, farazi2021deep} and online \cite{silver2010monte, somani2013despot, bent2004scenario} regimes; however, they either lack robustness to distribution shifts or are computationally expensive. Furthermore, sample efficiency remains a pervasive challenge in deep RL \cite{li2021breaking, li2023deep} due to sparse reward signals or large state spaces. As a result, deep Rl training from scratch typically requires millions of trajectories, which is expensive or impossible to collect in some cases. Given the substantial model sizes and pretraining data of LLMs, their rich world knowledge can enhance sample efficiency \cite{ahn2022can, morad2024language, ajay2024compositional, zhang2024improving} and robustness against distribution shift of the environment \cite{ge2024openagi, wang2023robustness}. For example, the previous art for this multi-agent routing problem \cite{garces2023multiagent} requires training separate a different network for each distinct representative demand distribution.


\subsection{Foundation models for Planning and Control}
The most common class of foundation models used for planning and control are vision-language-action models (VLAs) \cite{kim2024openvla, zhen20243d, brohan2023rt}, which leverage pretrained vision \cite{kim2024openvla,oquab2023dinov2}, language \cite{touvron2023llama,chowdhery2023palm}, or multimodal~\cite{zhai2023sigmoid,driess2023palm} models to interpret visual inputs, understand textual instructions, and generate contextually appropriate actions within dynamic environments. 

More relevant to us are those works using foundation models for spatial reasoning and path planning~\citep{yang2024diffusion, yang2024thinking, liu2023can}. \citeauthor{yang2024thinking} proposes a video-based benchmark to probe the spatial reasoning ability of multimodal LLMs (MLLMs). Their conclusions are rather negative: first, MLLMs are competitive but subhuman; second, linguistic prompting techniques are harmful for spatial reasoning. Instead of testing their ability in free-form environments, we take a step back, and test if they can reason spatially in an environment that can be represented as a graph. \citeauthor{liu2023can} applies Word2Vec models on real-world delivery route optimization \cite{merchan20242021} by drawing an analogy between delivery routes and sentences in language. Based on language descriptions of single/multi-robots routing tasks, \citeauthor{huang2024words} asks LLMs to generate Python code to solve single or multiple robots problems. Their evaluations are limited to prompting methods. They also perform task verification by generating unit tests. \citeauthor{deng2024can} investigates using LLMs as path planners and curriculum generators to mitigate hallucinations. The study leverages LLMs and Python to convert maze descriptions into Gym environments, where LLMs generate intermediate waypoints to simplify paths. Guided by LLMs, Q-learning iteratively plans paths, outputting the planning history and Q-table as the final policy.
% use \{B\}ayesian or \{L\}ipschitz  .bib file.

% Success of using LLMs for multi-agent decision-making is related to their reasoning abilities, as effective decision-making in dynamic environments often requires sophisticated reasoning processes that are essential for LLMs to understand and generate contextually appropriate responses. 
The success of LLMs in multi-agent decision-making depends on their reasoning abilities, which are crucial for generating context-aware responses in dynamic environments. Techniques have been proposed for enhancing the reasoning capabilities of LLMs, including those based on CoT \cite{wei2022chain}, ToT \cite{yao2024tree}, Best-of-N \cite{lightman2023let, wang2023math}, Monte Carlo Tree Search (MCTS) \cite{gao2024interpretable, zhang2024rest, wang2023math, wang2024towards}, or search against learned verifiers \cite{cobbe2021training}. However, these approaches focus on single-agent scenarios and do not extend to multi-agent contexts, where the actions of one agent must be conditioned on the actions of others. Moreover, MCTS incurs significant computational costs during the search process \cite{wang2024q,ye2022spending}, severely restricts its applicability, especially in multi-agent scenarios. 


% TODO - rework the flow of the related works

% \section{Discussion}
\section{Limitations and Future Directions} 
The primary limitation of our work lies in the slower inference time of LLMs compared to GNN models or heuristic-based methods. 
In an online setting, this makes it impractical to use LLMs with a high number of MC samples, potentially hurting the performance.
% test with more models

As future work, We are especially interested in the following directions. First,  LLMs could be used to predict future requests or as value functions to evaluate generated plans. Second, while RL traditionally addresses environments with costly interactions, the compute expense of large-model inference may now surpass that of environmental interactions, necessitating new frameworks for this paradigm. Third, Diverse, challenging tasks are needed to test LLMs’ capabilities beyond traditional textual benchmarks, bridging the gap between current models and general robotics applications.
% RL is often applied in environments where interactions with environmental dynamics are costly. However, we may now be entering a paradigm where the computational expense of inference with large models outweighs that of environmental interactions. This shift could pave the way for new frameworks or methods better suited to this emerging regime.


% we need more diverse and harder tasks to test the abilities of LLMs, to complement traditional textual tasks or ARC-AGI \cite{chollet2019measure}, especially to connect current model to general robotics.

\section{Conclusion.} We have demonstrated that LLMs can plan effectively in structured spatial environments, even in zero-shot settings, when provided with well-crafted information and descriptions. Furthermore, we have shown that finetuning through rollout, which encourages models to consider multiple possible actions and future states, significantly enhances their performance. Based on prompting results, we conclude that adressing spatial hallucinations is crucial for further improving LLMs' spatial planning capabilities. We hope this work highlights the potential of leveraging pre-trained knowledge in LLMs to achieve greater efficiency in learning and executing complex multi-agent tasks. Additionally, we advocate for the development of more diverse and challenging multi-agent benchmarks to deepen our understanding of the capabilities embedded within these models.

% \section*{Accessibility}
% \url{http://icml.cc/}.


% \section*{Acknowledgements}
% Kempner Institute computing resources enabled this work. 




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}













% view actions taken by agents $1, \cdots, l-1$ as communications
% The core contributions of this work are as follows:
% \begin{itemize}
%     \item 
% \end{itemize}

% \aaron{Overall: All the material you have here looks like a great start, but I may suggest some restructuring in order to be more concise and make your high-level points earlier and faster.  It seems to me like the high-level idea is: see how much performance and sample complexity we can gain from using a pretrained/fine-tuned LLM for this taxi routing problem.  We want to explore the two high-level axes we talked about last time with Stephanie: zero-shot vs. fine-tuned, and allowing the model to act without intervention vs. using the rollout algorithm to shape the policy.  Then it is also worth discussing the bit about providing extra context (it is rainy today) to see if the model's world knowledge helps it more rapidly adjust/fine-tune to these types of scenarios. \\ \\
% In general, I see a tendency with people when writing papers (and proposals in this context) to try to justify all the decisions they are making right away.  It is good to be thorough, and show the reader that you have done your homework, but the very first thing needs to be making absolutey sure your reader has a good idea of what your project looks like.  Starting with your framing question is a GREAT way to do this, but then I think you jump into the details and issues with other methods too soon.  Linger slightly longer on the high-level premise, and THEN talk more about what everybody else has done and how they don't adequately cover the work you want to do here. \\ 
% It may also be worth briefly mentioning a little bit of the mechanics of how to convert these problems to text that the model can understand towards the end.
% }
% \subsection{Physicality-informed Hallucination Mitigation}

\bibliography{paper}
\bibliographystyle{icml2025}

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \subsection{Low-rank Adaptation through Policy Gradient}
% We propose to update the low-rank matrices $A$ and $B$ using policy gradient \cite{kakade2001natural}, i.e., by applying the gradient of the expected reward with respect to the LoRA parameters.


% \begin{align}
% \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R_t \right],
% \end{align}
% where 
\section{Bar Charts for Standard Deviation}
Within each load level, the difficulty of scenarios can vary significantly depending on when and where requests occur. Therefore, we present error bars representing the standard deviation of costs in the $400\times400 m^2$ map across all scenarios within each load level to illustrate this variability.
\vskip -0.5in
\begin{figure}[ht]
\vskip -0.1in
\begin{center}
\centerline{\includegraphics[height=2.2in]{low_Load.pdf}}
\caption{Average cost and error bars in terms of standard deviation of methods over test set under the low load level. }
\label{sample-complexity}
\end{center}
\vskip -0.1in
\end{figure}

\vskip -1.1in

\begin{figure}[ht]
\vskip -0.2in
\begin{center}
\centerline{\includegraphics[height=2.2in]{medium_Load.pdf}}
\caption{Average cost and error bars in terms of standard deviation  of methods over test set under the medium load level.}
\label{sample-complexity}
\end{center}
\vskip -1in
\end{figure}

\vskip -0.1in

\begin{figure}[ht]
\vskip -0.2in
\begin{center}
\centerline{\includegraphics[height=2.2in]{high_Load.pdf}}
\caption{Average cost and error bars in terms of standard deviation  of methods over test set under the high load level.}
\label{sample-complexity}
\end{center}
\vskip -0.1in
\end{figure}
 
% \textbf{Monte-Carlo Rollout.} Starting from the newly added node $s{\prime}$, a default policy  $\pi_{\text{rollout}}$  is used to simulate a complete trajectory (sequence of actions) until a terminal state  $s_T$  is reached. The cumulative cost along this trajectory is computed as: \(G = \sum_{}^{} \gamma c(s)\).

\section{Prompts}
\label{sec: prompts}

\subsection{System and User Prompts}
\label{sec: sys_prompt}
\begin{mytextbox}{System and User Prompts as State}
\textbf{System:}  "You are a taxi driver in a multi-taxicab team on a map described by roads and intersections. Nodes (intersections) are listed by index with coordinates (longitude, latitude): $65293741$: (-122.4097034, 37.7817636), 65293743: (-122.4092587, 37.7814105), 65303533: (-122.4038718, 37.7898332), $\dots$

Roads are expressed as connections between nodes in the form 'from node to node': 65293741 to 65293743, 65293741 to 65318282, 65293741 to 1723738829, 65293743 to 65293741, 65293743 to 65306931, 65303533 to 65303538, 65303538 to 6378899319, 65303538 to 1271001348, 65303541 to 65303544, 65303541 to 1271001343, $\dots$

Your goal is to minimize the waiting time of all riders. Make your decisions based on the following rules:

1. A taxi can only pick up an active request if it is idle.

2. If no request exists or you choose not to pick up, you must decide where to move next."
\vspace{1em}

\textbf{User:} You are Taxi 0. You may only pick up a request if there is active request in the system. Taxi 0 is idle at location 6925582021. Taxi 1 is idle at location 1578907668. Taxi 2 is idle at location 65306810. Next action for taxi 1: do not pickup, go to 1578907668. Next action for taxi 2: do not pickup, go to 65306810. Please provide your next action as a tuple in the format: (pickup: True or False, next position: [numeric value]).
\vspace{1em}

\textbf{Assistant:} As Taxi 0, I am idle at location 6925582021. I will check for active requests.

There are no active requests in the system. Therefore, I will decide where to move next. I will move to the closest idle taxi to reduce waiting time.

The closest idle taxi is Taxi 2, which is at location 65306810. I will move to location $65306810$.

My next action is: (pickup: False, next position: 65306810)

% ablations: DON'T GIVE EXPECTED ACTIONS
\end{mytextbox}
\subsection{Chain-of-Thought}
\label{sec: cot_prompt}
\begin{mytextbox}{Chain-of-Thought}
\textbf{User:} [Zero-shot User Prompt as in \cref{sec: sys_prompt}]

Before deciding your next action, consider the following steps: \\
1. Identify all outstanding requests.\\
2. Determine which request you should prioritize picking up, aiming to minimize the total waiting time. 
 
  \quad - You should pick up a request immediately if you are at the same location/one step away from the request. 
  
  \quad - You can move to a node and pick up the request at that node in the same step.

3. Evaluate which requests other agents are already closer to or actively moving toward, based on their known or expected actions. Adjust your action if necessary.\\
After reasoning through these steps, provide your next action as a tuple in the format: (pickup: True or False, next position: a numeric value).\\
Remember, your goal is to minimize total waiting time and avoid targeting requests that are better suited for other agents unless no alternative exists.

\vspace{1em}

% \textbf{Assistant:} 
\end{mytextbox}

\subsection{Tree-of-Thoughts}
\label{sec: tot_prompt}
\begin{mytextbox}{Tree-of-Thoughts}
\textbf{System:} [Zero-shot User Prompt as in \cref{sec: sys_prompt}]

Before deciding your next action, think through the following tree of possibilities:\\
1.	Identify Requests:

    \quad - Which requests are outstanding?
    
	\quad - Which are closest or most urgent?
    
	\quad - Are other agents closer or already handling them?\\
→ Rank requests by priority based on proximity, urgency, and agent competition.\\
2.	Evaluate Actions:

	\quad - Action 1: Pick up a request if you're at/one step away. You can move to a node and pick up the request at that node in the same step.
    
	\quad - Action 2: Move toward a high-priority request.
    
	\quad - Action 3: Stay if no better option exists.\\
→ Simulate the impact of each action on total waiting time.

3.	Decide:

    \quad - Which action minimizes waiting time?
    
    \quad - Does it avoid unnecessary conflicts with other agents?\\
→ Choose the best action as: (pickup: True/False, next position: numeric value).

% \vspace{1em}

% \textbf{Assistant:} 

% My next action is: (pickup: False, next position: 65306810)

\end{mytextbox}

\vskip -1em
\subsection{Few-shot with CoT}
\begin{mytextbox}{Few-Shot}
\textbf{User:} Here are three examples: 
$\langle|$start\_header\_id$|\rangle$user$\langle|$end\_header\_id$|\rangle$ You are Taxi 0. You are idle at location 65334120. Taxi 1 is idle at location 1580501206. Taxi 2 is idle at location 1580501206. Currently there are outstanding requests in the system:

pickup\_location: 65314158

  \quad Taxi 0 shortest route: $[65334120, 65314158]$ (length: 1)
  
  \quad Taxi 1 shortest route: [1580501206, 65334120, 65314158] (length: 2)
  
  \quad Taxi 2 shortest route: [1580501206, 65334120, 65314158] (length: 2)
  
 Expected next action for taxi 1: go to 65334120. Expected next action for taxi 2: go to 65334120. You should prefer picking up requests closer to you. You should not follow other agents. Please provide your next action as a tuple in the format: (pickup: True or False, next position: a numeric value).
 
$\langle|$start\_header\_id$|>$assistant$\langle|$end\_header\_id$|>$ I'm the closest to the request at 65314158 among all taxis. I will pick it up. My next action is: (pickup: True, next position: 65314158). \\

 
$\langle|$start\_header\_id$|\rangle$user$\langle|$end\_header\_id$|\rangle$ You are Taxi 1. Taxi 0 is idle at location 65314156. You are idle at location 65317939. Taxi 2 is idle at location 65314156. Currently there are outstanding requests in the system:

  pickup\_location: 1580501214
  
    \quad Taxi 1 shortest route: [65317939, 1580501214] (length: 2)
    
    \quad Taxi 0 shortest route: [65314156, 6988532585, 386885670, 1271001343, 6988532615, 2936165726, 65317939, 1580501214] (length: 8)
    
    \quad Taxi 2 shortest route: [65314156, 6988532585, 386885670, 1271001343, 6988532615, 2936165726, 65317939, 1580501214] (length: 8)
    
  Known next action for taxi 0: go to 65314156. Expected next action for taxi 2: go to 6988532585. Provide your next action as a tuple in the format: (pickup: True or False, next position: a numeric value).
  
$\langle|$start\_header\_id$|\rangle$assistant$\langle|$end\_header\_id$|\rangle$ I'm the closest to the request at 1580501214 among all taxis. I will pick it up. My next action is: (pickup: True, next position: 1580501214). \\

$\langle|$start\_header\_id$|\rangle$user$\langle|$end\_header\_id$|\rangle$ You are Taxi 1.  Taxi 0 is idle at location 1308305528. You are idle at location 552853360.  Taxi 2 is idle at location 65313133.  Currently there are outstanding requests in the system:

  pickup\_location: 6988532585
  
    \quad Taxi 0 shortest route: [1308305528, 6988532585] (length: 1)
    
    \quad Taxi 1 shortest route: [552853360, 1308305528, 6988532585] (length: 2)
    
    \quad Taxi 2 shortest route: [65313133, 65313138, 1578907668, 552853360, 1308305528, 6988532585] (length: 5)
    
  Known next action for taxi 0: go to 6988532585. Expected next action for taxi 2: go to 65313138. You should prefer picking up requests closer to you. You should not follow other agents. Please provide your next action as a tuple in the format: (pickup: True or False, next position: a numeric value).
  
$\langle|$start\_header\_id$|\rangle$assistant$\langle|$end\_header\_id$|\rangle$ Since there is only one request at 6988532585 and Taxi 0 is closer to it and trying to pick it up, I will not try to pick it up. My next action is: (pickup: False, next position: 552853360).\\


Now, [Zero-shot User Prompt as in \cref{sec: sys_prompt}]
% \vspace{1em}

% \textbf{Assistant:} 

\end{mytextbox}
% \begin{quote}
%\begin{center}
% \rule{0.92\textwidth}{1pt}
%\end{center}
% \textbf{System:} \textit{``You are a taxi driver in a multi-taxicab team on a map described by roads and intersections. Nodes (intersections) are listed by index with coordinates (longitude, latitude): $65293741$: (-122.4097034, 37.7817636), 65293743: (-122.4092587, 37.7814105), 65303533: (-122.4038718, 37.7898332), $\dots$}

% \textit{Roads are expressed as connections between nodes in the form 'from node to node': 65293741 to 65293743, 65293741 to 65318282, 65293741 to 1723738829, 65293743 to 65293741, 65293743 to 65306931, 65303533 to 65303538, 65303538 to 6378899319, 65303538 to 1271001348, 65303541 to 65303544, 65303541 to 1271001343, $\dots$}

% \textit{Your goal is to minimize the waiting time of all riders. Make your decisions based on the following rules:}

% \textit{1. A taxi can only pick up an active request if it is idle.}

% \textit{2. If no request exists or you choose not to pick up, you must decide where to move next."}

% \textbf{User:} \textit{You are Taxi 0. You may only pick up a request if there is active request in the system. Taxi 0 is idle at location 6925582021. Taxi 1 is idle at location 1578907668. Taxi 2 is idle at location 65306810. Next action for taxi 1: do not pickup, go to 1578907668. Next action for taxi 2: do not pickup, go to 65306810. Please provide your next action as a tuple in the format: (pickup: True or False, next position: [numeric value]).}
% \vspace{1em}

% \textbf{Assistant:} \textit{As Taxi 0, I am idle at location 6925582021. I will check for active requests.}

% \textit{There are no active requests in the system. Therefore, I will decide where to move next. I will move to the closest idle taxi to reduce waiting time.}

% \textit{The closest idle taxi is Taxi 2, which is at location 65306810. I will move to location $65306810$.}

% \textit{My next action is: (pickup: False, next position: 65306810)}

% \rule{0.92\textwidth}{1pt}
% \end{quote}


% \subsection{Self-consistency with CoT}

% \begin{mytextbox}{Self-consistency with CoT}

% \textbf{User:} 

% \vspace{1em}

% \textbf{Assistant:} 

% \end{mytextbox}






\section{Examples of Hallucination}
\label{sec: hallu}
\begin{mytextbox}{Hallucination}

% \vspace{1em}

\textbf{Assistant:} To minimize the total waiting time of all riders, I will follow the given constraints and prioritize picking up requests closer to my current location.

My current location is 65328690. 

The shortest route to the first request 65343958 is [65328690, 2936165726, 65317939, 65371286, 65332806, 65313133, 65326742, 65326744, 65343958]. 

The shortest route to the second request 65303546 is [65328690, 2936165726, 3902413693, 552853360, 1308305528, 6988532585, 65303544, 65303546]. 

Since the first request 65343958 is closer to me, I will pick it up. The second request 65303546 is farther away, so I will not pick it up.

My next action is to pick up the request at location 65343958. (pickup: True, next position: 65343958)

\end{mytextbox}

In this example, the LLM successfully identified the shortest paths to the two outstanding requests by itself. However, it mistakenly concluded that the first request was closer, while the second was actually nearer—a case of \textbf{spatial} hallucination. Furthermore, despite heading toward the first request, it should have chosen Node 2936165726 as the next step. Instead, it exhibited a \textbf{reachability} hallucination by selecting Node 65343958.


\section{Examples of Additional Semantic Information}
\label{sec: semantic-info}
\begin{mytextbox}{Semantic Information: Conditioning on Weather}


\textbf{User:}  … Current weather in San Francisco: heavy rain with temperature: 11.8 ℃ and wind speed: 7.9 km/h.   
 Please provide your next action as a tuple in the format: (pickup: True or False, next position: a numeric value). 

\vspace{1em}

\textbf{Assistant:} Given the current weather conditions, we can assume that the travel time will be increased by 1 unit due to the heavy rain… 


\end{mytextbox}

% \subsection{Ablation with parameter-efficient finetuning schemes}

% LoRA is a method to adapt pre-trained LLMs by injecting low-rank decompositions into their weight matrices. Instead of fine-tuning all the weights (which can be computationally expensive and storage-intensive), LoRA modifies only a few additional parameters in the form of low-rank matrices that are added to the original weights. For a weight matrix $W \in \mathbb{R}^{d \times k}$, LoRA decomposes it as: $W + \Delta W, \quad \Delta W = A B^\top$ 
\end{document}

% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


\subsection{Theoretical Analysis of Stability of Finetuned LLM Policy}

we can treat hallucinations as a feature of the distributions induced by the policies.

\begin{definition}[Stable Policy]
\label{def}
we will consider the policy $\mu$ to be stable if $J_{\Tilde{\mu}}$ is real-valued, i.e., $J_\mu(x) < \infty$, for all $x \in X$.
\end{definition}



\begin{lemma}[Lipschitz Continuity]
\label{Lipschitz}
The per-step cost function \(c: \mathcal{X} \times \mathcal{A}\rightarrow \mathbb{R}\) for the multi-agent routing problem immediate cost at time $ $ when action $ $ is taken in state $ $. It is Lipschitz Continuous, \textit{i.e.}, \(|c(q=[s, a]) - c(q'=[s, a'])| \leq L_c \|q - q'\|\) for all $q, q' \in \mathcal{A}$.
\end{lemma}

Let \(\pi_{Rollout}\) be the Rollout Policy with IA-RA as the base policy. From \cite{}, we know it is stable. Let \(\pi_{L}\) be the policy derived from LLM fintuned by data collected using \(\pi_{Rollout}\). 

\begin{lemma}
\label{ass: bounded}
The first Wasserstein distance between \(\pi_{L}\) and \(\pi_{Rollout}\) is bounded by \(nD\), where \(n\) and \(D\) is the ,  i.e., \begin{align}
    W_1(\pi_L, \pi_{Rollout}; s) &= \inf_{\gamma \in \Gamma(\pi_{L}, \pi_{R})} \int_{\mathcal{Q}\times\mathcal{Q}} d(q_L, q_R) \mathrm{d} \gamma(q_L, q_R) \\
    % &= \sup_{\|f\|_{\text{Lip}} \leq 1}  \left|\int_{\mathcal{Q}} f d\pi_L(\cdot, s) - \int_{\mathcal{Q}}d\pi_{R} (\cdot,s) \right| \\
    &= \sup_{\|f\|\leq 1} \mathbb{E}_{q\sim }[f(q)] - \mathbb{E}_{q\sim}[f(q)] \\
    &\leq nD,
\end{align} where the last equality follows from Kantorovich-Rubinstein Duality the supremum is taken over all 1-Lipschitz continuous functions, i.e. \(\|\| \leq 1\).
\end{lemma}

We are then interested in proving the following result:
% We define LLM policy distribution \[p(x, a, h) = \pi(a\mid x, h) d(x, h)\] policy distribution is defined similarly.

\begin{theorem}[Stability of Finetuned LLM Policy]
\label{thm}
   If Assumption \ref{ass: bounded} holds and the per-step cost function is Lipschitz continuous with Lipschitz constant $L_c$, then the expected total request waiting time under $\pi_L$ satisfies: \(J_T(\pi_{L}) \leq J_T(\pi_{R}) + TL_c \epsilon\).

  Therefore, \(\pi_L\) remains a stable policy \ref{def} as long as \(\epsilon\) is sufficiently small.
\end{theorem}

\begin{proof}[Proof of Theorem \ref{thm}]
Since $c$ is $L_c$-Lipschitz, the function $f=c/L_c$ is 1-Lipschitz.
\begin{align}
    \left|\mathbb{E}_{\pi}\left[\frac{c(s, )}{L_c}\right] - \mathbb{E}_{\pi}\left[\frac{c(s, )}{L_c}\right]\right| \leq W_1(\pi_{}, \pi_{};s) &\leq \epsilon \\
    \left|\frac{1}{L_c}\mathbb{E}_{\pi_{}}{c(s, \pi)} - \frac{1}{L_c}\mathbb{E}_{\pi}[c(s, \pi_{})]\right| &\leq \epsilon \\
    \left|\mathbb{E}_{\mathcal{R}, \pi_{}}{c(s, \pi_{}(s))} - \mathbb{E}_{\pi_{}}[c(s, \pi(s))]\right| &\leq L_c\epsilon
\end{align}
At each time step, the difference in expected cost between the two policies is:
\begin{align}
     \Delta_t &= \left|\mathbb{E}_{q_t }[c(q_t=(s_t, a_t))] - \mathbb{E}_{q_t}[c(q_t(s_t, a_t))]\right| \\
     &\leq L_c\epsilon, 
\end{align}
where the first inequality follows from the definition of the first Wasserstein distance (also known as the earth mover's distance).

The expected total request waiting time under $\pi_L$ is then:
 \[J_T(\pi) = J_T(\pi) + \sum_{t=1}^T \Delta_t \leq mT + TL_c \epsilon\]

As long as $\epsilon$ is sufficiently small (i.e., the learned policy is close to the base policy in Wasserstein distance), 
$\pi_L$ remains stable with a total request waiting time bounded above by $mT+TL_{c} \epsilon$.
\end{proof}