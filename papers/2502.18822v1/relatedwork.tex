\section{Related Works}
% TODO: Include small paragraph describing why you chose the three areas of related works that you are focusing on
\subsection{Multi-agent Decision-making}
Multi-agent decision-making algorithms allow for the optimization of resource allocations in real-world applications such as taxi routing \cite{garces2023multiagent} and drone deliveries \cite{lee2022autonomous}. Optimal solution for these problems is intractable since it requires considering multiple scenarios of potential future requests and all relevant agent actions at each decision point, in extremely large state and control spaces that grow exponentially with the number of agents. Hence, many methods have been proposed to find competitive sub-optimal solutions. Most intuitive solutions are greedy policies, but they can be far from optimal--for example, when two taxis go to serve the same request, leaving other requests unattended. Studies \cite{kondor2022cost, vazifeh2018addressing} show that with non‐coordinating greedy policies, each additional ride‐hailing company in the market can largely increase the total number of circulating vehicles than necessary. Heuristic-based methods \cite{bertsekas1979distributed, bertsimas2019online, croes1958method} often generate myopic policies due to the lack of consideration for future demand. RL methods have been proposed in both offline \cite{ulmer2019offline, farazi2021deep} and online \cite{silver2010monte, somani2013despot, bent2004scenario} regimes; however, they either lack robustness to distribution shifts or are computationally expensive. Furthermore, sample efficiency remains a pervasive challenge in deep RL \cite{li2021breaking, li2023deep} due to sparse reward signals or large state spaces. As a result, deep Rl training from scratch typically requires millions of trajectories, which is expensive or impossible to collect in some cases. Given the substantial model sizes and pretraining data of LLMs, their rich world knowledge can enhance sample efficiency \cite{ahn2022can, morad2024language, ajay2024compositional, zhang2024improving} and robustness against distribution shift of the environment \cite{ge2024openagi, wang2023robustness}. For example, the previous art for this multi-agent routing problem \cite{garces2023multiagent} requires training separate a different network for each distinct representative demand distribution.


\subsection{Foundation models for Planning and Control}
The most common class of foundation models used for planning and control are vision-language-action models (VLAs) \cite{kim2024openvla, zhen20243d, brohan2023rt}, which leverage pretrained vision \cite{kim2024openvla,oquab2023dinov2}, language \cite{touvron2023llama,chowdhery2023palm}, or multimodal~\cite{zhai2023sigmoid,driess2023palm} models to interpret visual inputs, understand textual instructions, and generate contextually appropriate actions within dynamic environments. 

More relevant to us are those works using foundation models for spatial reasoning and path planning~\citep{yang2024diffusion, yang2024thinking, liu2023can}. \citeauthor{yang2024thinking} proposes a video-based benchmark to probe the spatial reasoning ability of multimodal LLMs (MLLMs). Their conclusions are rather negative: first, MLLMs are competitive but subhuman; second, linguistic prompting techniques are harmful for spatial reasoning. Instead of testing their ability in free-form environments, we take a step back, and test if they can reason spatially in an environment that can be represented as a graph. \citeauthor{liu2023can} applies Word2Vec models on real-world delivery route optimization \cite{merchan20242021} by drawing an analogy between delivery routes and sentences in language. Based on language descriptions of single/multi-robots routing tasks, \citeauthor{huang2024words} asks LLMs to generate Python code to solve single or multiple robots problems. Their evaluations are limited to prompting methods. They also perform task verification by generating unit tests. \citeauthor{deng2024can} investigates using LLMs as path planners and curriculum generators to mitigate hallucinations. The study leverages LLMs and Python to convert maze descriptions into Gym environments, where LLMs generate intermediate waypoints to simplify paths. Guided by LLMs, Q-learning iteratively plans paths, outputting the planning history and Q-table as the final policy.
% use \{B\}ayesian or \{L\}ipschitz  .bib file.

% Success of using LLMs for multi-agent decision-making is related to their reasoning abilities, as effective decision-making in dynamic environments often requires sophisticated reasoning processes that are essential for LLMs to understand and generate contextually appropriate responses. 
The success of LLMs in multi-agent decision-making depends on their reasoning abilities, which are crucial for generating context-aware responses in dynamic environments. Techniques have been proposed for enhancing the reasoning capabilities of LLMs, including those based on CoT \cite{wei2022chain}, ToT \cite{yao2024tree}, Best-of-N \cite{lightman2023let, wang2023math}, Monte Carlo Tree Search (MCTS) \cite{gao2024interpretable, zhang2024rest, wang2023math, wang2024towards}, or search against learned verifiers \cite{cobbe2021training}. However, these approaches focus on single-agent scenarios and do not extend to multi-agent contexts, where the actions of one agent must be conditioned on the actions of others. Moreover, MCTS incurs significant computational costs during the search process \cite{wang2024q,ye2022spending}, severely restricts its applicability, especially in multi-agent scenarios. 


% TODO - rework the flow of the related works

%