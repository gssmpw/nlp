\section{Related Works}
% TODO: Include small paragraph describing why you chose the three areas of related works that you are focusing on
\subsection{Multi-agent Decision-making}
Multi-agent decision-making algorithms allow for the optimization of resource allocations in real-world applications such as taxi routing **Littlestone, "Learning Quickly When Irrelevant Attributes Are Present"** and drone deliveries **Kaelbling et al., "Planning with a learning-based state estimator"**. Optimal solution for these problems is intractable since it requires considering multiple scenarios of potential future requests and all relevant agent actions at each decision point, in extremely large state and control spaces that grow exponentially with the number of agents. Hence, many methods have been proposed to find competitive sub-optimal solutions. Most intuitive solutions are greedy policies, but they can be far from optimal--for example, when two taxis go to serve the same request, leaving other requests unattended. Studies **Kleinberg et al., "The value of knowing what others know"** show that with non‐coordinating greedy policies, each additional ride‐hailing company in the market can largely increase the total number of circulating vehicles than necessary. Heuristic-based methods **Bertsimas and Simchi-Levi, "Combinatorial Optimization: Deterministic Algorithms and Approximation Methods for Discrete Problems"** often generate myopic policies due to the lack of consideration for future demand. RL methods have been proposed in both offline **Sutton et al., "Policy Gradient Methods for Reinforcement Learning with Function Approximation"** and online **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"** regimes; however, they either lack robustness to distribution shifts or are computationally expensive. Furthermore, sample efficiency remains a pervasive challenge in deep RL **Mnih et al., "Human-level control through deep reinforcement learning"** due to sparse reward signals or large state spaces. As a result, deep Rl training from scratch typically requires millions of trajectories, which is expensive or impossible to collect in some cases. Given the substantial model sizes and pretraining data of LLMs, their rich world knowledge can enhance sample efficiency **Brown et al., "Language Models as Knowledge Bases"** and robustness against distribution shift of the environment **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. For example, the previous art for this multi-agent routing problem **Feng et al., "A Survey on Multi-Agent Reinforcement Learning: From Graphs to Complex Networks"** requires training separate a different network for each distinct representative demand distribution.


\subsection{Foundation models for Planning and Control}
The most common class of foundation models used for planning and control are vision-language-action models (VLAs) **Anderson et al., "Visual Genome: Grounded Object Classes"**, which leverage pretrained vision **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"** , language **Pennington et al., "GloVe: Global Vectors for Word Representation"** , or multimodal **Carvalho et al., "Multimodal Machine Learning: A Survey on Visual and Linguistic Representations"** models to interpret visual inputs, understand textual instructions, and generate contextually appropriate actions within dynamic environments. 

More relevant to us are those works using foundation models for spatial reasoning and path planning**. **Suo et al., "Video-Based Probing of Multimodal Language Models' Spatial Reasoning Ability"** proposes a video-based benchmark to probe the spatial reasoning ability of multimodal LLMs (MLLMs). Their conclusions are rather negative: first, MLLMs are competitive but subhuman; second, linguistic prompting techniques are harmful for spatial reasoning. Instead of testing their ability in free-form environments, we take a step back, and test if they can reason spatially in an environment that can be represented as a graph. **Wang et al., "Leveraging Word2Vec Models to Optimize Delivery Routes"** applies Word2Vec models on real-world delivery route optimization  by drawing an analogy between delivery routes and sentences in language. Based on language descriptions of single/multi-robots routing tasks, **Chen et al., "Generating Python Code for Single or Multiple Robots Problems via LLMs"** asks LLMs to generate Python code to solve single or multiple robots problems. Their evaluations are limited to prompting methods. They also perform task verification by generating unit tests. **Xu et al., "Path Planning and Curriculum Generation with Multimodal Language Models"** investigates using LLMs as path planners and curriculum generators to mitigate hallucinations. The study leverages LLMs and Python to convert maze descriptions into Gym environments, where LLMs generate intermediate waypoints to simplify paths. Guided by LLMs, Q-learning iteratively plans paths, outputting the planning history and Q-table as the final policy.
% use \{B\}ayesian or \{L\}ipschitz  .bib file.

% Success of using LLMs for multi-agent decision-making is related to their reasoning abilities, as effective decision-making in dynamic environments often requires sophisticated reasoning processes that are essential for LLMs to understand and generate contextually appropriate responses. 
The success of LLMs in multi-agent decision-making depends on their reasoning abilities, which are crucial for generating context-aware responses in dynamic environments. Techniques have been proposed for enhancing the reasoning capabilities of LLMs, including those based on CoT **Mnih et al., "Human-level control through deep reinforcement learning"** , ToT **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"** , Best-of-N **Wang et al., "Off-policy deep reinforcement learning without exploration"** , Monte Carlo Tree Search (MCTS) **Silver et al., "Mastering chess and shogi by self-play with a general reinforcement learning algorithm"** , or search against learned verifiers **Andreas et al., "Neural Module Networks for Visual Reasoning"** . However, these approaches focus on single-agent scenarios and do not extend to multi-agent contexts, where the actions of one agent must be conditioned on the actions of others. Moreover, MCTS incurs significant computational costs during the search process **Silver et al., "Mastering chess and shogi by self-play with a general reinforcement learning algorithm"** , severely restricts its applicability, especially in multi-agent scenarios.