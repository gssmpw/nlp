
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}



\usepackage{hyperref}   
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional -quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{appendix}
\usepackage{adjustbox}
% \usepackage{newclude}
% \usepackage[table]{xcolor}
\usepackage{subcaption}
\usepackage{caption}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[theorem]
\newcommand{\change}[1]{\textcolor{blue}{#1}}
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\todo}[1]{\noindent\textcolor{red}{[#1]}}
\newcommand{\notes}[1]{\noindent\textcolor{blue}{[#1]}}
\newcommand{\kaustubh}[1]{\noindent\textcolor{brown}{[#1]}}
\newcommand{\LP}[1]{\noindent\textcolor{green}{LP: #1}}
\newcommand{\AC}[1]{\noindent\textcolor{purple}{AC: #1}}
\newcommand{\VM}[1]{\noindent\textcolor{orange}{VM: #1}}
\newcommand{\ours}{RIPL\xspace}

\newcommand{\state}{\mathcal{S}}
\newcommand{\action}{\mathcal{A}}
\newcommand{\actionprob}{[0,1]^\action}
% \newcommand{\R}{\mathbb{R}}
\newcommand{\risk}{\mathfrak{S}}
% \newcommand{\risk}{S2C}

\newtheorem{assumption}{Assumption}



\graphicspath{ {./imgs/} }
\title{Safety Representations for Safer Policy Learning}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.



\author{Kaustubh Mani$^1$, Vincent Mai$^2$, Charlie Gauthier$^1$, Annie Chen$^3$,  Samer Nashed$^1$, Liam Paull$^{1,4}$\\
$^1$Mila, Université de Montréal, Quebec, Canada\\
$^2$ Hydro-Québec Research Center, Quebec, Canada\\
$^3$ Standford University\\
$^4$ Canada CIFAR AI Chair \\
\texttt{kaustubh.mani@mila.quebec} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

% Old Abstract
% \begin{abstract}
% Reinforcement learning algorithms typically need extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Current safe exploration methods address this challenge by enforcing constraints during learning, but these approaches often result in overly conservative behaviour and inefficient learning. This paper introduces a novel approach that exploits the observation that risk is often unevenly distributed across states in many environments. We propose a risk-informed policy learning strategy that integrates risk information directly into the state representation as a feature vector. By augmenting the state with this risk vector, our method naturally encourages exploration in safer states, leading to more efficient and secure policy learning in safety-critical scenarios. Extensive empirical evaluations across diverse environments demonstrate that our approach significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety.
% \end{abstract}

% New abstract
\begin{abstract}
Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety.
\end{abstract}

\section{Introduction}
%% Paragraph1: 
%1. Introduce the problem of learning RL agents in safety-critical applications. 
%2. Introduce the concept of risk-reward tradeoff 
\captionsetup{font=footnotesize} % Use small font for captions


Reinforcement learning (RL) has achieved notable success across various domains, from game playing~~\citep{shao2019survey} to robotics~\citep{zhang2021reinforcement,singh2022reinforcement}. However, training RL agents directly in safety-critical environments remains challenging partly due to the presence of failure events that are deemed undesirable or unacceptable both during training and deployment.  To manage these failures, RL algorithms typically rely on failure penalties or impose safety constraints~\citep{when_to_ask_for_help, ai-grid, cpo, pid_lag}. While these methods help minimize unsafe behaviours during learning, they often result in overly cautious policies, restricting the agent's ability to explore leading to sub-optimal performance. The key challenge, therefore, lies in designing RL algorithms that can effectively balance the risks of exploration with the reward of task completion.


%% Paragraph2:
%1. Highlight the reasons behind RL agents learning conservative behaviors, that is
% a) Overfitting on limited data. 
% b) overfitting on action sequences rather than learning state-dependent features. 
%2. Conclude by stating that "state-based risk representation learning forms a good inductive bias"

% One of the reasons RL agents tend to exhibit conservative behaviors is overfitting. Training on limited and growing datasets makes RL agents prone to overfitting to the specific samples they encounter during training \citep{overfitting_robust_bengio, overfitting_regularization}. In safety-critical environments, this issue is exacerbated as agents are discouraged from exploring states with high uncertainty due to potential risks. Instead of learning robust policies conditioned on state features, agents may memorize specific action sequences \citep{arcade, overfitting_action}, leading to narrow exploration focused on safer regions of the state space. Consequently, the learned policies are locally optimal but overly conservative, failing to generalize well to other parts of the environment where better policies might exist.


% One of the key reasons RL agents often exhibit conservative behaviour is overfitting~\citep{overfitting_robust_bengio, overfitting_regularization}. When exploration is limited, the agent gathers data from only a small portion of the state space, causing it to overfit to these narrow regions and fail to generalize to other, potentially more rewarding parts of the environment. This issue is further magnified in safety-critical applications, where exploration in uncertain regions of the state space is discouraged due to potential risks. Consequently, RL agents tend to develop locally optimal but overly conservative policies, sacrificing performance for safety by failing to explore safer yet more efficient behaviours in the broader environment.


%% Paragraph3: 
% 1. Highlight the importance of learning state-centric risk representations. 
% 2. Why learning such representations directly from rewards or constraints is difficult?
% 3. Conclude by saying that to facilitate learning of such representations we propose learning risk as an inductive bias.

% In such settings, learning state-conditioned representations of risk becomes crucial for effective management of the risk-reward trade-off. Relying purely on reward signals to infer risk can be insufficient or infeasible, especially when failure events are rare or the environment provides sparse feedback on risky behaviours. In this paper, we propose to learn a state-conditioned risk representation as an inductive bias to guide policy learning. By incorporating risk information directly into the learning process, the agent can make more informed decisions that balance exploration and safety.

% In high-dimensional observation spaces, RL agents often exploit the environment’s determinism by memorizing open-loop action sequences instead of learning meaningful state representations~\cite {overfitting_robust_bengio, arcade, overfitting_action}. This is largely due to the challenge of learning good state representations from reward signals alone.  Several methods have proposed auxiliary objectives to facilitate better representation learning. In this work, we focus on learning state-centric risk representations as an inductive bias for policy learning in safety-critical applications. We show that in the absence of good risk representation, RL algorithms tend to learn conservative policies by getting stuck in local optima. By incorporating risk information directly into the learning process, the agent can make more informed decisions that balances exploration and safety.



%%%% Vincent proposal: merging above paragraphs
% Original 
% One of the key reasons RL agents often exhibit conservative behaviour is overfitting~\citep{overfitting_robust_bengio, overfitting_regularization}. Indeed, learning good state representations from reward signals alone is challenging. It has been shown that, in high-dimensional observation spaces, RL agents often exploit the environment’s determinism by memorizing open-loop action sequences instead of learning meaningful state representations~\cite {overfitting_robust_bengio, arcade, overfitting_action}. In safety-critical applications, agents can overfit on strong negative rewards, failing to capture the state space elements which would allow it to differentiate the risk of one state from another. This often discourages exploration early, reducing the available data and thus the possibility to generalize. Consequently, RL agents tend to develop locally optimal but overly conservative policies, sacrificing performance for safety by failing to explore safer yet more efficient behaviours in the broader environment. 
% In this work, we show that, by incorporating a good representation of risk information directly into the learning process, the agent can make more informed decisions that balance exploration and safety. We then propose to learn such a representation from the agent's experience, through an inductive bias.




%% Maybe add a paragraph describe in more detail the issue of conservativeness cause due to overfitting on early experience, this can be described in terms of the related work 


One significant factor contributing to conservative behaviour in RL agents is "primacy bias"~\citep{primacy}, where early experiences exert a lasting influence on the agent’s learning trajectory. In safety-critical applications, severe penalties for constraint violations encountered early in the training process can disproportionately shape the agent’s policy, leading to overly cautious decision-making and hindered exploration. As a result, agents learn safety representations that overestimate the risk of failure, discouraging further exploration. 
This often results in agents with a narrow view of the state space and locally optimal yet overly conservative policies that sacrifice performance for safety. 
While a considerable body of research has explored some notion of safety estimation for safer policy learning~\citep{wcpg, risk_constrained, efficient_risk_averse}, most approaches focus on failure prevention by implicitly or explicitly restricting agent exploration. For example, methods such as~\citet{csc} and \citet{learning_to_be_safe} 
% LP estimate the risk to 
filter out actions with the likelihood of failure above a specific threshold.
In this work, we demonstrate that by incorporating accurate safety representations into the learning process, agents can make more informed decisions, balancing exploration with safety. 
% LP We propose to learn this safety representation as an inductive bias from the agent's experience.



%% Paragraph 4: 
% 1. Explain the intuition behind learning a state-centric risk representation.
% 2. Introduce the idea of learning risk representation as an inductive bias for policy learning in safety-critical applications. 
% 3. What does the risk representation encode? -> Explain how the risk representation can be valuable for trading off risk and reward. 


% We propose to learn a state-conditioned risk representation as an inductive bias, based on the intuition that risk is often unevenly distributed across states. For instance, driving in the opposite lane on a two-lane road is inherently risky, independent of the agent’s policy. This state-centric view of risk is crucial for exploration, as the agent must explore states with high epistemic uncertainty while using risk information to guide exploration. By encoding this risk representation, the agent can effectively navigate toward safer states during policy learning, without overly conservative behavior.




%% Paragraph 5: 
% Contrast with related work that uses risk 
% 1. Need to talk about risk-aware methods that estimate the risk to constrain exploration 
% 2. Filtering-based methods use risk to filter out states with a high likelihood of failure. 
% 3. Saute RL using state augmentation to convert CMDPs to MDPs by formulating state-based penalties.
 
%LP However, by discouraging exploration, these methods tend to produce safer but overly conservative policies, often resulting in sub-optimal task performance.
















%% Paragraph 6:
% Contributions or Hypotheses of the paper. 

Specifically, we present \textit{Safety Representations for Policy Learning} (SRPL), a framework that augments an RL agent's state representation by integrating a state-conditioned safety representation derived from the agent's experiences during the learning process. SRPL is grounded in the understanding that risk is often unevenly distributed among states. For instance, driving in the wrong lane on a two-lane road is inherently unsafe, independent of the specific policy the agent follows. The safety representation is captured by a steps-to-cost (\textit{S2C}) model, which for any given state estimates the distribution over the proximity to unsafe or cost-inducing states. Additionally, by learning safety representations that are state-centric utilizing data from the agent's experience (as opposed to just the current policy), we encourage the generalizability of the safety representations across tasks.




% In RIPL, the agent’s state representation is augmented with risk estimates provided by the risk model. This leads to a \emph{Risk-Informed MDP} where the agent learns a policy in an environment augmented with risk information. The risk model estimates the probability that the agent will encounter an unsafe state within a horizon of \( k \) steps, where \( k \in \{1, \dots, H\} \). This enables the agent to assess the relative risk of different states and actions, making decisions that trade-off risk and reward more effectively. Additionally, RIPL extends naturally to constrained environments, allowing for seamless integration with Constrained Markov Decision Processes (CMDPs).

% RIPL distinguishes itself from existing methods in two key ways. First, traditional approaches for safe or risk-aware exploration often rely on external safety critics, such as filtering or shielding mechanisms, to evaluate and conditionally execute actions. These methods can lead to overly conservative policies that may hinder learning. In contrast, RIPL integrates risk information directly into the policy learning process, allowing for more effective management of high-risk situations. Second, unlike previous risk-aware RL methods that model risk as a scalar value~\citep{intrinsic_fear,csc,sauteRL}, RIPL represents risk as a probability distribution over relative risk values derived from policy rollouts. This probabilistic approach captures more detailed risk information, enabling more nuanced risk-informed decision-making and more accurately approximating important values such as expected returns and constraint violations.





To summarize, we study three primary hypotheses addressing the key challenges outlined:
\begin{itemize}
    \item By directly integrating safety information into the state representation, the safety and efficiency of RL agents during the learning process are significantly enhanced.
    \item Safety representations can be efficiently learned online using the experiences generated during RL training, resulting in improved performance without requiring prior or additional data.
    \item When learned from an agent's entire experience that includes a diversity of policies, safety representations can be generalized across various tasks, acting as an effective prior for learning new tasks.
\end{itemize}



The SRPL framework can be used to augment any RL algorithm and we show results for several on-policy and off-policy baselines in Sec~\ref{sec:results}. We evaluate SRPL agents on several simulated robotic tasks, including manipulation, navigation, and locomotion. Our results show that by leveraging safety information, SRPL agents are significantly more sample-efficient while being safer during learning compared to baselines. Additionally, safety information transfers well across tasks, providing a useful prior for learning new policies. 
% LP We also show that SRPL agents can better optimize the risk-reward tradeoff compared to baselines.


\section{Preliminaries}


% \subsection{Markov Decision Processes} 

\textbf{Markov decision processes (MDPs)} are defined as a tuple $\langle S, A, T, R, d, \gamma \rangle$, where $S$ is a set of states, $A$ is a set of actions, $R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function indicating the immediate reward for executing action $a$ in state $s$ and resulting in state $s'$, $T: S \times A \times S \rightarrow [0, 1]$ is the forward dynamics model indicating the probability of transitioning to state $s'$ after executing action $a$ in state $s$,  $d: S \rightarrow [0, 1]$ represents the probability of starting in a state $s \in S$, and $\gamma \in [0, 1)$ is the discount factor.
The solution to an MDP is an optimal policy $\pi^*$ that maximizes the expected discounted cumulative reward, $J(\pi)$:
\begin{equation}
    J(\pi) = \mathbb{E}_{s_0 \sim d(s)} \Big [ \mathbb{E}_{\tau\sim\pi} \Big [ \sum_{t=0}^{H} \gamma^t R(s_t, a_t, s_{t+1)} \Big ] \Big ].
\end{equation}

Here, $H$ is the length of the horizon. 

% Executing a policy $\pi$ induces a value function $V^{\pi}: S \rightarrow \mathbb{R}$ indicating the expected cumulative discounted reward that an agent would experience if it executed policy $\pi$ beginning in state $s$. The optimal policy $\pi^*$ induces the maximum value function for each state $V^*$.

% \subsection{Constrained Markov Decision Processes} 

\textbf{Constrained MDPs (CMDPs)} are defined as a tuple $\langle S, A, T, R, d, \gamma, \mathcal{C}, \beta \rangle$, where $\langle S, A, T, R, d, \gamma \rangle$ is an MDP, $\mathcal{C}: S \rightarrow \{0,1\}$ is a cost function, and $\beta$ is the constraint threshold or a maximum, cumulative cost that is acceptable in expectation. Intuitively, we can view the constraints imposed by $\mathcal{C}$ and $\beta$ as reducing the set of possible policies from all policies $\Pi$ to those satisfying the constraints $\Pi_{\mathcal{C}} \subseteq \Pi$. 
The solution to a CMDP is a policy $\pi^*$ where
\begin{equation}
    \begin{array}{cc}
    \pi^* =& \operatorname*{argmax}_{\pi \in \Pi_C} J(\pi), 
    \Pi_\mathcal{C} = \bigl\{\pi \in \Pi: J_{\mathcal{C}}(\pi) \leq \beta \bigl\}.\\
     & \text{where}\hspace{2mm} J_{\mathcal{C}}(\pi) = \mathbb{E}_{s_0 \sim d(s)} \Big [ \mathbb{E}_{\tau \sim \pi} \Big [ \displaystyle \sum_{t=0}^{H} \gamma^t \mathcal{C}(s_t) \Big ] \Big ] \\
    \end{array}
\end{equation}

% \begin{equation}
%     \begin{array}{cc}
%         J(\pi) =&\hspace{-2mm}\mathbb{E}_{s_0 \sim d(s)} \Big [ \mathbb{E}_{\tau_{s_0} ^{\pi}} \Big [ \displaystyle \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1)} \Big ] \Big ]. \\
%         \text{where} & \mathbb{E}_{s_0 \sim d(s)} \Big [ \mathbb{E}_{\tau_{s_0} ^{\pi}} \Big [ \displaystyle \sum_{t=0}^{\infty} \gamma^t \mathcal{C}(s_t) \Big ] \Big ] \leq \beta. \\
%     \end{array}
% \end{equation}



% \footnote{While $\pi^*$ is deterministic for MDPs, it is generally stochastic in CMDPs as this allows agents to fully exploit $\beta$ in expectation.}

To find an optimal policy, online RL algorithms allow an agent to explore the environment while simultaneously using these trajectory rollouts to optimize the policy. In deep reinforcement learning (DRL), it is typically impossible to guarantee that an agent will never execute a policy $\pi \notin \Pi_{\mathcal{C}}$ during training. Therefore, in the absence of any prior information, the agent will likely violate the constraints during exploration. Given this, we often care about both the cumulative constraint violations incurred during training as well as the expected constraint violations of the final policy.

% \subsection{Terminology and Assumptions}

% In this section, we attempt to clarify as well as set the standard for terminologies used in the rest of the paper. Although not specific to our method, we highlight some of the assumptions commonly used in Safe Reinforcement Learning research.


In the remainder of this paper, we will refer to states that we aim to avoid, such as sink states in MDPs and cost-inducing states in CMDPs, as ``unsafe'' states. We've also used "distance to unsafe" and "steps to unsafe" interchangeably. % LP While a state may be at high risk due to its proximity to an unsafe state, it is not necessarily classified as unsafe itself. 
We define a failure as an event where a constraint is violated, and we will use the terms ``failure'' and ``constraint violation'' interchangeably. Furthermore, following common practices in safe RL literature \citep{cpo, pid_lag, sauteRL}, we assume that unsafe states can be identified either through the termination of an episode or via the cost signal.




% % \textbf{Terminology:} We attempt to clarify terminology used in the paper. 

% In this paper we do not attempt to unify, classify, or intentionally reify any particular terms adopted from colloquial speech, though the general concept of ``risk'' comes closest to the intuition used to design RIPL. In the remainder of the paper, we will collectively refer to states we wish to avoid (terminal states in the unconstrained case, and cost-inducing states in the constrained case) as ``unsafe'' states. A state of course may have high risk in that it is near an unsafe state, but may not be unsafe itself.

% Despite the terminology, RL researchers have remained fairly consistent in the assumptions they make about their operating environments. We follow two of the most common in the literature~\citep{learning_to_be_safe,ldm,csc}: (1) \emph{Identifiability}: Unsafe states are identifiable as such, either via episode termination or immediate accumulation of cost or reward. (2) \emph{Binary Safety}: The categorization of a state as safe or unsafe is binary; varying degrees of safety are not considered and we use strictly $\mathcal{C}: \mathcal{S} \rightarrow \{0, 1\}$.

% Though they do exclude some decision-making models, these assumptions admit a large, informal class of MDPs. For example, any CMDP for which there is some subset of states $S_{unsafe} \subset S$ which induces a non-zero, constant cost. Or, any MDP with absorbing states or sink states $S_{unsafe} \subset S$ that, when the agent enters, terminates the episode prematurely and may or may not produce negative reward. Although most of the example domains in safety gym \citep{safety-gym} and other popular RL environments represent safety as a physical property, the concept of unsafe states, and thus the RIPL framework, may be applied to any MDP following the above assumptions, whether or not the label of ``unsafe'' is accurate in a colloquial sense. \todo{add robotics examples (non-ergodic state, reset state, etc.)}



\section{Safety Representations for Policy Learning}

%% What this section should talk about? 
% 1. Importance of state-centric risk representation for learning in safety critical environments. 
% 2. RL algorithms learn to be overly conservative failing to learn state-centric risk representations essential for managing tradeoffs b/w risk and reward. 
% 3. Discuss Risk as an Inductive Bias: Different choices of risk functions and their pros and cons. 
% 4. Discuss the design choice we take and what the risk representation is encoding. (aleatoric uncertainty, ambiguity about the policy, ambiguity in action space.)
% 5. Briefly describe the whole framework and some implementation details. 

In this section, we will begin by motivating the usefulness of safety information in a toy example where we assume that this information is somehow provided. Subsequently, we will formalize our choice of safety representation and describe the SRPL framework. 

\subsection{Motivating Example}
\label{sec:motivating_example}

%% What points we want to get across? 

% 


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_islandnav_main.png}
    \vspace{-1em}
    \caption{To motivate the benefit of learning state-conditioned safety representations in safety-critical applications, we perform experiments on the \textit{Island Navigation} environment~\cite {ai-grid}. We assume access to the Manhattan distance from the nearest water cell as ground truth (GT) safety information. \textbf{(Col 1)} shows that without this information, penalties due to failure early in the learning process bias the agent toward overly conservative behaviour resulting in suboptimal policies that avoid water but fail to complete the task. \textbf{(Col 2)} compares the Q-value estimates of a DQN agent with and without GT safety information across all states over multiple episodes. Without safety information, the agent fails to distinguish between risky and less risky states, producing uniformly low Q-values across all states. This highlights the inability of RL agents to learn good safety representations using reward signals alone. \textbf{(Col 3)} examines state visitation patterns over episodes, showing that while both agents initially explore the environment, the agent without safety information quickly reduces exploration, oscillating between two states to avoid failure but failing to reach the goal state while the agent with safety information is able to explore a larger region of the state-space. More detailed discussion in Sec.~\ref{sec:motivating_example}}
    \label{fig:motivating_example_curves}
\end{figure*}


To 
% LP better understand the issue of overfitting as a result of poor 
demonstrate the usefulness of state-centric safety representations, we perform experiments on \textit{Island Navigation}~\citep{ai-grid}, a grid world environment designed for evaluating safe exploration approaches. The agent's goal is to navigate the island without entering the water cells. Entering a water cell is considered unsafe and leads to a failure penalty in the form of a negative reward and episode termination. The agent is only rewarded when it visits the goal state. The input to the RL agent is the image of the entire grid. Instead of experimenting with a single environment with a fixed start and goal state~\citep{ai-grid} where the agent can simply memorize action sequences, we create four different versions (Fig. \ref{fig:islandnavenvs} (in the appendix)) of the \textit{Island Navigation} environment with different start and goal positions as well as locations of the water tiles. Each episode begins with randomly selecting an environment. Thus the agent needs to learn good state-conditioned representations of safety to solve the task as well as minimize failures during learning.

In this environment, a reasonable proxy for safety associated with every cell is its Manhattan distance to the nearest water cell. To investigate how this priviliged safety information can be useful to the policy, we provide the RL agent with this distance (referred to as GT safety
% LP\footnote{Safety is encoded as the Manhattan distance to water cells. Smaller distances will correspond to riskier states, larger distance corresponds to safer states.}
) by adding it to the state representation. 
% LPfor illustrative purposes. 
We train DQN\citep{dqn} agents with and without this safety information, along with variants of DQN that model aleatoric (c51 \cite{dist_rl}) and epistemic (BootstrapDQN \cite{bootDQN}) uncertainty. 



% From Fig. \ref{fig:motivating_example_curves}, we see that as we increase the failure penalty associated with entering the water cells, which is equivalent to making the system more safety-critical, traditional RL approaches like DQN, c51, and BootstrapDQN incur fewer failures during learning but also converge to suboptimal policies. On the other hand, risk-informed DQN leverages risk information to avoid getting stuck in local minima and learns effective policies even for high failure penalties. While learning effective policies that solve the task, risk-informed agents also incur significantly fewer failures during learning. 


%% Vincent's proposal
Fig. \ref{fig:motivating_example_curves} (col 2) shows the evolution of the Q-value distributions for all state-action pairs for a particular instance of the environment during training, for DQN agents with and without safety information. In the presence of safety information, the DQN agent is able to efficiently learn a correlation between safety information and reward, outputting low Q-values for risky states (GT safety = 1) and high Q-values for safe states (GT safety = 3), as a result efficiently exploring the state-space and ultimately converging to an optimal policy. On the other hand, the DQN agent without this information fails to learn accurate internal representations of safety and instead overestimates risk for all states resulting in uniformly low Q-values (Fig. \ref{fig:motivating_example_curves} (col 2) (row 1)), further discouraging exploration and as a result converging to a sub-optimal policy (Fig. \ref{fig:motivating_example_curves} (col 1) (row 1)). Fig. \ref{fig:motivating_example_curves} (col 3) (row 1) shows that the policy learned by the DQN agent in the absence of safety information ensures safety of the agent by oscillating between two or three states near the start state\footnote{Episodes terminate when the agent reaches the goal state or enters a water cell. However, oscillating between two states causes episodes to truncate at the max-steps limit of 100, resulting in high state-visitation counts} but fails to consistently reach the goal state resulting in low average return (Fig. \ref{fig:motivating_example_curves} (col 1)). 

This example demonstrates that having access to additional information about the safety of a state can be 
% safety representation of the state is 
useful for RL agents to overcome the bias created by negative experiences early in learning that result in an overestimation of risk leading to low Q-values and, as a result, yield conservative policies with sub-optimal performance. 
However, typically this information is not provided to a learning agent and must be inferred from the agent's observation of the environment. We propose explicitly learning safety representations using agent experience during learning.
% LP by introducing this as an inductive bias into policy learning.




%% Vincent proposes to remove these two last paragraphs:
% We can conclude from this example that this specific proxy for risk is useful auxiliary information. However, we cannot assume that this information is provided to our agent in general.  

% In safety-critical environments, where exploration risks a high likelihood of failure and penalties, reinforcement learning agents often adopt overly cautious behavior, such as staying in the start state to avoid failure altogether. This prevents them from learning the necessary risk information. However, when provided with explicit risk information, such as the likelihood of failure near hazardous areas like water cells, the agent can overcome local minima and learn to complete the task. By integrating risk information that correlates with failure probabilities and negative returns, the agent avoids high-risk states while still learning an effective policy, leading to a better balance between exploration and safety. Thus, learning the risk information directly from the agent's experience can enable safer and more efficient learning for safety-critical applications.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/SRPL_diagram.png}
    \vspace{-2em}
    \caption{\textit{SPRL Framework:} SPRL explicitly learns safety representations for states as distribution over proximity to unsafe states (cost-inducing states) through a steps-to-cost (\textit{S2C}) model and uses this information to implicitly guide policy learning towards exploring safer regions of the state space.
    }
    % \vspace{-2em}
    \label{fig:framework}
\end{figure*}





\subsection{State-Centric Safety Representations}
\label{subsec:modelling_risk}
% Para1: 
% 1. Why learning state-centric risk representations is important?
% 2. Why is it difficult to learn such representations directly by optimizing return? 
% 3. 
We propose to learn state-conditioned safety representations as an inductive bias to enable safety-informed agents by leveraging the agent’s prior experience. This raises two key questions: (1) What constitutes an ideal state-centric representation of safety? (2) How should we train such a representation? 


\begin{wrapfigure}{H}{0.6\textwidth}
    
    \begin{center}
    \vspace{-2em}
    \includegraphics[width=0.6\textwidth]{figs/risk_island.png}
    \end{center}
    \vspace{-1em}
    \caption{\textit{Safety Representation:} We demonstrate the \textit{S2C} model's output on two different states ($A$ \& $B$). A being farther from the water cells (indicated in blue) has its peak at 3 (distance from the unsafe set) while $B$ is a more risky state and has its peak at 1. }
    \label{fig:risk_dist}
    \vspace{-1em}
\end{wrapfigure}

An effective safety representation must capture the immediate likelihood of failure in the current state as well as reflect potential risks in future states. This representation should incorporate both the risks associated with exploration from a given state, the uncertainties in the environment’s dynamics, and the ambiguities in policy choices for action selection following the current state. A simple scalar safety representation~\citep{csc, learning_to_be_safe}, lacks the expressiveness necessary to capture these complexities. Therefore, we propose modelling safety as a probability distribution over distances to cost-inducing states. 
% LP learned from the agent’s experience. 
To avoid learning representations that overfit data from a narrow region of the state space induced by a particular policy, we propose to learn safety representations over the entirety of the agent's experience instead of policy-specific rollouts. 

Formally, we model safety as a function $\risk: \mathcal{S} \to \Delta^{H_s}$, where $\mathcal{S}$ is the state space and $\Delta^{H_s}$ represents a probability simplex over the safety horizon $H_s$\footnote{$H_s << H$, where H is the MDP time horizon.}. Given the experience of an agent, the model $\risk_t(s)$ aims to capture the conditional probability of entering an unsafe state in exactly $t\in \{1, 2, \ldots, H_s\}$ steps given that the agent is in state $s$ (Fig. \ref{fig:risk_dist}). Here, $\risk_{H_s}(s)$ represents the probability that the agent remained safe throughout the safety horizon $H_s$ without encountering an unsafe state based on the agent's past experience. This model can be learned from the set of trajectories which represent the experience of the agent, as we will describe next. 
% LP Each trajectory, $\tau$, which goes through a state $s$ can be labelled with the number of actions $\delta_{\tau}(s)$ taken before entering an unsafe state, or with $H_s$ if no unsafe state is visited by $\tau$ after visiting $s$.

% For any state $s \in \mathcal{S}$, $\risk(s)$ denotes the distribution over the number of actions $t \in \{1, 2, \ldots, H\}$ that will be taken before the agent encounters an unsafe state. Specifically, let $\tau$ be a trajectory starting from state $s$, and $\delta_{\tau}(s)$ denote the number of actions taken before entering an unsafe state $s' \in S_{unsafe}$. Specifically, given an agent's experience, risk function $\risk(t|s)$ aims to capture conditional probability of entering an unsafe state in exactly $t$ steps given that the agent is in state $s$. 


% This distributional approach allows the risk model to account for different sources of uncertainty—both aleatoric uncertainty in state transitions and ambiguity in policy actions—resulting in a more nuanced and informative risk representation. Such a model enables the agent to make more informed decisions during exploration, striking a balance between safety and performance without being overly conservative.



% Vincent's proposal to replace previous paragraph

%We propose to represent risk as a state-conditioned discrete distribution $\risk: S \rightarrow \Delta^H$. For a given state $s$, $\risk(s)$ provides the probability that an unsafe state will be encountered after precisely $0 < t < H$ actions, where $H$ is the horizon. That is $\risk(s)$ estimates a distribution over $\delta_{\tau}(s)$, the number of actions executed from $s$ in trajectory $\tau$ before encountering an unsafe state, and $\risk_t(s)$ is the estimated probability that $\delta_{\bar{\tau}}(s) = t$ for $\bar{\tau}$. This probability is specified over the likely exploration policy taken by the agent. Probability mass on $\risk_H(s)$ indicates the probability of the state being completely safe within the finite horizon $H$. 

% FINISH THIS SECTION HERE. Next section: \subsection{Learning the Risk Model}







% In Appendix \ref{app:theory}, we show that under certain conditions the value of a particular policy $\pi$ for a given state $s$ can be written as a simple dot product between the risk model output and a constant vector. We demonstrate that risk models learned from non-stationary policies can compute high probability lower bounds on the value function and provide bounds on expected cost via a similar dot product. Additionally, we find that risk models improve expected utility in transfer learning with exposure to more tasks, leading to a complex meta-reasoning problem due to the interplay between limited data and narrowing state distributions during non-stationary policy learning.



\subsection{SRPL Framework}

 To learn the state-centric safety representation 
% LP  described in Section \ref{subsec:modelling_risk}
 , we train a neural network, referred to as the ``steps-to-cost'' or the \textit{S2C} model, which takes the state as input and outputs the corresponding safety representation. This representation is modelled as a discrete distribution, implemented as the softmax output of a neural network $\risk^{\nu}$ parameterized by $\nu$. 
%
%
% \todo{Add a paragraph to describe what the ideal risk representation would look like... and discuss what we can do given that we don't have infinite data.}
%
% and trained on a dataset $\mathcal{D}_{risk}$:

% \begin{equation} \risk_t(s) = \frac{e^{\theta_t(s)}}{\sum_{t=1} ^H e^{\theta_t(s)}}, \end{equation}
%
% where $\risk_t(s)$ denotes the probability that an unsafe state will be encountered exactly $t$ steps after being in state $s$, and $H$ is the horizon length.
%
The safety representation is learned alongside the RL policy by constructing a separate replay buffer $\mathcal{D}_{\textit{S2C}}$ in the case of on-policy algorithms, which contains trajectories $\tau = { (s_0, \delta_{\tau}(s_0)), \ldots, (s_n, \delta_{\tau}(s_n)) }$ from policy rollouts. In the case of off-policy algorithms like CSC~\citep{csc} and CVPO~\cite{cvpo}, the off-policy buffer is used to store the ``steps-to-cost'' information corresponding to states for each trajectory. At the end of each episode, every state $s$ in the trajectory $\tau$ is labelled with its corresponding ``steps-to-cost'' value $\delta_{\tau}(s)$, which is the number of actions taken before encountering an unsafe state. If no unsafe state is encountered during the episode, the distance to unsafe for all states in the trajectory is set to the safety horizon length ($H_s$) to indicate safety of the state within the safety horizon. In this way, for every trajectory $\tau\in \mathcal{D}_{S2C}$ and for every state $s \in \tau$, we have a label $\delta_\tau(s)$ which we can use to train our \textit{S2C} model by minimizing the following negative log-likelihood loss\footnote{For categorical distributions, NLL loss is equivalent to Cross-Entropy loss.}: 

\begin{equation}
    \mathcal{L}_{\text{\textit{S2C}}}((s,\tau); \nu) = - \sum_{t=1}^{H_s} \mathbb{I}[\delta_\tau(s) = t] \log(\risk^\nu_t(s))
    \label{eq:cross-entropy}
\end{equation}
% \vspace{1em}
where $\mathbb{I}[\delta(s) = t]$ is an indicator function that takes value $1$ if $\delta_\tau(s) =t$ and $0$ otherwise.
% The parameters of the risk model are updated by minimizing the Negative log likelihood (NLL) loss\footnote{For categorical distributions, NLL loss is equivalent to Cross-Entropy loss.} between the predicted distribution and observed distance to failures sampled from the replay buffer $\mathcal{D}_{risk}$. The loss function is defined as:

% \begin{equation}
%     \mathcal{L}_{\text{risk}}(s) = - \sum_{t=1}^{H} \mathbb{I}[\delta(s) = t] \log(\risk_t(s))
%     \label{eq:cross-entropy}
% \end{equation}
% \vspace{1em}

% where $\mathbb{I}[\delta(s) = t]$ is an indicator function that is 1 if the distance to failure for state $s$ is $t$ steps, and $0$ otherwise.

 In the SRPL framework, (Fig.~\ref{fig:framework}), the output of the \textit{S2C} model $\risk^\nu(s)$ is incorporated into the agent’s state by augmenting the original state with the learned safety representation. The augmented state is defined as $s' = \{s, \risk^\nu(s)\}$. For high-dimensional observations, such as raw images, the safety representation is concatenated with the encoded feature representation of the observation, such that the augmented observation becomes $o' = \{\mathcal{F}(o), \risk^\nu(o)\}$, where $\mathcal{F}$ is the feature encoder.



%\subsection{Implementation Details}


%% What do we want to say? 
% 1. In practice modelling over the entire horizon is not feasible because of the issues of dimensionality as well as far as safety is concerned being accurate about safety of a state in near term or is more important because more the time the agent has to make decisions more safe it can be. So introduce the concept of safety Horizon. H_s
% 2. Introduce the bin size variable, rather than modelling the entire ditribution over timesteps from 1...H_s we instead split it into bins in order to reduce the dimensionality of the resulting distribution further. We've shown analysis over the choices of bin size and safety horizon in the Appendix.
% 3. Talk more about the way replay buffer is populated. We can talk about the tradeoff between completely on-policy to complete off-policy. 

\paragraph{Implementation details: }
%In practice, modelling the distribution over the entire horizon can be infeasible, especially for large horizon lengths or infinite horizon MDPs. Instead, 
We model the safety distribution over a fixed safety horizon $H_s << H$, relying on the assumption that information about near-term safety is more important and an agent can safely navigate the state space with this information. Instead of modelling the distribution over all time steps between $[1, H_s]$, we split this range into bins to further reduce the dimensionality of the safety representation. An ablation over the choices of bin size and safety horizon $H_s$ is included in the Appendix \ref{sec:safety-horizon}. For on-policy algorithms, a separate off-policy replay buffer is maintained which stores policy rollouts along with the distance to unsafe values. To preserve only relevant experiences about policies similar to the agent's current policy, the replay buffer throws away samples from older policies. More thorough implementation details are provided in Appendix \ref{app:implementation-details} 


% \todo{Write a conclusion to this section}
% \todo{refer to the framework diagram }

\section{Experiments}

% In this section, we will detail the tasks, the environments, and the baselines chosen for demonstrating the effectiveness of our SRPL framework. 

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/risk_from_scratch.png}
%     \vspace{-2em}
%     \caption{\textbf{Risk training alongside policy:} (\emph{Island Navigation}) DQN with ground truth risk information outperforms all baselines, while RIPL(DQN) learns to estimate the same information and reaches similar performance. (\emph{FrozenLake}) RIPL(DQN) outperforms DQN, BootstrapDQN, and Intrinsic Fear significantly. (\emph{CarGoal2}) While CSC and Intrinsic Fear remain overly conservative, and  RIPL(PPO) is the most sample efficient. (\emph{CarButton2}) Similar to CarGoal2, RIPL(PPO) finds stronger policies much more quickly than any baseline.}
%     \label{fig:risk_from_scratch}
%     \vspace{-2mm}
% \end{figure*}



% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c|cc|cc|}
%           &                                         &    \multicolumn{2}{c|}{AdroitHandPen-v1}        &          \multicolumn{2}{c|}{Ant-v4}             \\
%             \hline
%           &                                          Methods      &    Average Return    &  Total Cost       &  Average Return  &   Total Cost           \\
%          \hline
%          \multirow{4}{*}{\rotatebox{90}{Vanilla}} &  CPO          &   $4154 \pm 1798$    &  $6667 \pm 363$   & $3507 \pm 785$   & $1444 \pm 276$    \\
%          &                                          TRPO-PID      &   $4809 \pm 2641$    &  $6629 \pm 445$   & $5109 \pm 312$   & $1626 \pm 81$    \\
%          &                                             CSC        &   $2461 \pm 1612$    &  $4120 \pm 148$   & $2466 \pm 588$   & $1214 \pm 148$    \\
%          &                                            WCPG        &   $1487 \pm 1878$    &  $3189 \pm 346$   & $1968 \pm 220$   & $1364 \pm 247$    \\
%          \hline
%          \multirow{4}{*}{\rotatebox{90}{RIPL}} &         CPO      &   $7176 \pm 1392$    &  $4797 \pm 931$   & $4300 \pm 438$   & $1396 \pm 275$    \\
%          &                                          TRPO-PID      &   $5963 \pm 1384$    &  $5311 \pm 741$   & $5064 \pm 309$   & $1466 \pm 123$    \\
%          &                                             CSC        &   $3916 \pm 1491$    &  $3327 \pm 219$   & $3184 \pm 317$   & $1104 \pm 152$    \\
%          &                                            WCPG        &   $2164 \pm 1741$    &  $2884 \pm 268$   & $2750 \pm 199$   & $1296 \pm 221$    \\
%          \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}



% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c|cc|cc|}
%           &                                         &    \multicolumn{2}{c|}{PointGoal1}        &          \multicolumn{2}{c|}{PointButton1}             \\
%             \hline
%           &                                          Methods      &    Average Return    &  Cost Rate       &  Average Return  &   Cost Rate           \\
%          \hline
%          \multirow{4}{*}{\rotatebox{90}{Vanilla}} &  CPO          &   $6.25 \pm 3.28$    &  $1.61 \pm 0.12$   & $0.91 \pm 0.70$   & $1.64 \pm 0.06$    \\
%          &                                          TRPO-PID      &   $2.96 \pm 2.61$    &  $1.09 \pm 0.02$   & $0.15 \pm 0.73$   & $1.13 \pm 0.06$    \\
%          &                                             CSC        &   $4.07 \pm 1.08$    &  $1.12 \pm 0.02$   & $0.71 \pm 0.61$   & $1.09 \pm 0.02$    \\
%          &                                            WCPG        &   $1.01 \pm 0.98$    &  $1.03 \pm 0.02$   & $-0.25 \pm 0.42$  & $1.01 \pm 0.01$    \\
%          \hline
%          \multirow{4}{*}{\rotatebox{90}{RIPL}} &         CPO      &   $11.63 \pm 2.28$   &  $1.49 \pm 0.03$   & $2.21 \pm 1.16$   & $1.53 \pm 0.05$    \\
%          &                                          TRPO-PID      &   $7.31 \pm 2.85$    &  $1.07 \pm 0.02$   & $1.38 \pm 1.19$   & $1.08 \pm 0.02$    \\
%          &                                             CSC        &   $6.28 \pm 1.72$    &  $1.09 \pm 0.01$   & $1.81 \pm 0.56$   & $1.06 \pm 0.02$    \\
%          &                                            WCPG        &   $1.87 \pm 0.66$    &  $1.01 \pm 0.02$   & $0.86 \pm 0.53$   & $1.01 \pm 0.01$    \\
%          \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}



% We conduct experiments on several continuous control tasks to answer the following questions:
% % Through experiments on several continuous control tasks, we aim to answer the following questions: 

% \begin{itemize}
%     \item How does adding risk information impact the safety and efficiency of the learning algorithm during training? 
%     \item Can risk-informed RL agents trade-off safety and task performance better than the baselines? 
%     \item Can we transfer risk representation across tasks to act as a prior for safer and more efficient learning on the new task? 
% \end{itemize}


% In this section, we evaluate the efficacy of RIPL in both constrained and unconstrained MDPs. Concretely, we address the following questions: (1) In constrained and unconstrained MDPs, how does RIPL impact returns, costs, and violations when wrapping generic RL baselines? (2) How well do policies and their associated frozen pre-trained risk models generalize to new tasks? (3) How impactful is a pre-trained risk model for training a policy from scratch? Below, we first describe our experimental setup before presenting our results regarding the questions 1-3.
% \vspace{-2em}
\subsection{Experimental Setup}
\label{sec:experimental_setup}
% \vspace{-1em}
We perform our experiments on four tasks in three distinct environments. First is a manipulation task \textit{AdroitHandPen}~\citep{adroit} where a 24-degree of freedom Shadow Hand agent needs to learn to manipulate a pen from a start orientation to a randomly sampled goal orientation. Dropping the pen from the hand is considered a failure or constraint violation.  Next, we have an autonomous driving environment \textit{SafeMetaDrive}~\citep{metadrive}, where an RL agent is learning to drive on the road while avoiding traffic, each collision incurs a cost and the cost threshold is set to 1 ($\beta=1$).  Finally, we evaluate our method on the Safety Gym \citep{safety-gym} environment on tasks \textit{PointGoal1} and \textit{PointButton1}. For the \textit{PointGoal1} task we have a point agent that is tasked with reaching a random goal position from a random start position, the environment consists of regions that are unsafe and accumulate cost. For the \textit{PointButton1} task, the agent needs to press a sequence of buttons in the correct order, pressing the wrong button incurs a cost. In addition, there are static hazards and dynamic objects that the agent needs to avoid. The goal in these environments is to do the task while incurring costs lower than a threshold ($\beta=10$). Additionally, we also show results on Mujoco locomotion environments (Ant, Hopper and Walker2d) in the Appendix. The goal in these tasks is for the robot to move as fast as possible while avoiding falling on the ground which is considered a failure or a constraint violation.


\begin{figure}[t]
    \centering
    \vspace{-2em}
    \includegraphics[width=\linewidth]{figs/ripl_constrained_mdps_main.png}
    \vspace{-2em}
    
    \caption{Performance of SRPL agents (denoted SR-*) on four different tasks. For these experiments, both the \textit{S2C} model and the policy have been randomly initialized so no prior information has been provided to the agent. SRPL agents consistently outperform their baseline counterparts on both safety during learning as well as sample efficiency. Results were obtained by averaging the training runs across five seeds. The input to the RL agent is state-based in the form of joint states or LiDAR points. }
    \label{fig:on-policy-results-main}
\end{figure}



Further, we evaluate the transferability of learned safety representations from one task to another. For this, we use \textit{PointButton1} as the source task and \textit{PointGoal1} as the target task.  We augment the state representation of the Safety Gym environments to ensure the same state dimensionality for \textit{PointGoal1} and \textit{PointButton1} (More details in Appendix \ref{app:environment_details}). The \textit{S2C} model is trained on the source task and frozen for the target task and we study the performance of the \textit{S2C} model at transferring information in terms of safety and efficiency on the target task. Additionally, we show that the \textit{S2C} model provides a good initialization for fine-tuning representations on the target task in the case of transferring both policy and safety representations.  




% \begin{itemize}
%     \item \textbf{Hard Constraints:} These are environments in which cost is binary and cost-threshold is set to $0$ (i.e. constraint satisfaction means no failures). We evaluate two environments: AdroitHandPen-v1 (\cite{adroit}) where the objective of the agent is to manipulate a pen without dropping it (dropping the pen is considered as failure (cost=1)) and reach a goal pen position and orientation, the agent receives a higher reward for being closer to the goal configuration and reaching the goal configuration faster and Ant-v4 where the objective of the agent to run as fast as possible without falling on the ground (failure mode cost = 1) with control costs in the reward functions that incentivizes lower torques on the joints.

%     \item \textbf{Soft Constraints:} In these environments, the cost takes an integer value and can be accumulated throughout the episode. The goal in these environments is to do the task while incurring costs lower than a threshold ($\beta=10$). The results are evaluated on two environments from Safety Gym (\cite{safety-gym}). In PointGoal1-v0, the point agent is tasked with reaching a goal location (marked in green) while avoiding hazards in the environment. For PointButton1-v0, the agent needs to press a sequence of buttons in the correct order, pressing the wrong button incurs a cost also there are static hazards and dynamic objects that the agent needs to avoid as well. 
% \end{itemize}

% \vspace{-1em}
% \textbf{Constrained MDPs:} Here, we also use the CarGoal1 environment from Safety-Gym, this time formulated as constrained MDPs. CarGoal1 is a simpler version of CarGoal2 with less number of unsafe regions in the state-space. This means that the reward function and transition function are decoupled from the cost function (i.e. entering an unsafe state no longer results in termination or negative reward). The task of the agent is to maximize return while satisfying the cost constraint. In this project, the cost threshold $\beta = 10$.  

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/environments.png}
%     \vspace{-2em}
%     \caption{Environments: \textit{(left)} Island Navigation environment for safe exploration tasks \cite{ai-grid}, \textit{(middle)} safety-gym environments: CarGoal \& CarButton \textit{(right)} Humanoid-velocity environment}
%     \label{fig:domains}
% \end{figure}

\vspace{-1em}
\subsection{Baselines \& Comparison}
% \vspace{-1em}

The SRPL framework can be integrated with any RL algorithm that accounts for risk sensitivity or operates in safety-critical environments. We evaluate its effect on feature learning by comparing several baseline algorithms with their SRPL-augmented versions. We compare against both off-policy and on-policy safe RL algorithms. On-policy baselines include Constrained Policy Optimization (CPO)~\citep{cpo}, a second-order method for enforcing constraints, TRPO-PID~\citep{pid_lag} a Lagrangian-based method that uses a PID controller for stable learning, SauteRL~\citep{sauteRL} a state-augmentation method that stores the remaining constraint budget as part of the state information, CRPO~\citep{crpo} a primal approach which updates the policy by alternating between reward maximization and constraint satisfaction. Off-policy baselines include Conservative Safety Critics (CSC)~\citep{csc} which learns a safety critic using a safe Bellman operator and Constraint Variational Policy Optimization (CVPO)~\citep{cvpo} which formulates the constrained MDP problem as an Expectation Maximization (EM) problem.


\textbf{Evaluation Metrics.} For \textit{AdroitHandPen} environment, we evaluate SRPL and baselines on episodic return (sample efficiency) and total failures/costs incurred during the training of the RL agent (safety). For the \textit{SafeMetaDrive} environment, we evaluate the algorithms on task performance and safety in terms of the success rate and total cost, respectively. For the Safety Gym environments, we evaluate the algorithms on the task performance and safety in terms of episodic return and episodic cost, respectively. Additionally, we measure the task performance and safety at the end of training for the Safety Gym environments in terms of average return and cost rate respectively. Cost-rate is measured by dividing the total cost incurred during training by the number of actions/steps taken in the environment. 


% In Table~\ref{tab:hard_constraints} and \ref{tab:sensor_modes}, cost-rate values are multiplied by 100. 


\vspace{-1em}
\section{Results}
\label{sec:results}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/risk_from_scratch.png}
%     % \vspace{-2em}
%     \caption{\textbf{Unconstrained MDPs:} (\emph{Island Navigation}) DQN with ground truth risk information outperforms all baselines, while RIPL(DQN) learns to estimate the same information and reaches similar performance. (\emph{FrozenLake}) Both C51 and RIPL(DQN) outperform DQN, and Intrinsic Fear significantly. (\emph{CarGoal2}) While CSC and Intrinsic Fear remain overly conservative, and  RIPL(PPO) is the most sample-efficient. (\emph{CarButton2}) Similar to CarGoal2, RIPL(PPO) finds stronger policies much more quickly than any baseline.}
%     \label{fig:risk_from_scratch}
%     \vspace{-2mm}
% \end{figure*}




% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/ripl_constrained_mdps_main.png}
%     % \vspace{-1em}
%     \caption{\textbf{Constrained MDPs:} RIPL policies consistently improve the performance of the baseline methods on all four environments in terms of Episodic Return and constraint satisfaction. }
%     \label{fig:constrained-MDPs}
% \end{figure}





\begin{table}[t]
    \begin{adjustbox}{width=\columnwidth,center}

    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
                                             &    \multicolumn{2}{c|}{\textit{AdroitHandPen}}        &          \multicolumn{2}{c|}{\textit{SafeMetaDrive}} &   \multicolumn{2}{c|}{\textit{PointGoal1}} &  \multicolumn{2}{c}{\textit{PointButton1}}          \\
            % \hline
                                                    Methods      &    Return ($\uparrow$)   &   \#failures ($\downarrow$)     &   Success Rate ($\uparrow$) &   Total Cost ($\downarrow$) &  Return ($\uparrow$) & Cost-Rate ($*1e^2$) ($\downarrow$) & Return ($\uparrow$) & Cost-Rate ($*1e^2$) ($\downarrow$)      \\
         \hline
        %  \rowcolor{gray!10}
           CPO         &   $4154 \pm 1798$    &  $6667 \pm 363$   &   $0.088 \pm 0.1$    &  $2715 \pm 366$ &   $6.25 \pm 3.28$    &  $1.61 \pm 0.12$ & $0.91 \pm 0.70$   & $1.64 \pm 0.06$    \\
          %  \rowcolor{gray!10}
            TRPO-PID     &   $4809 \pm 2641$    &  $6629 \pm 445$   &   $0.076 \pm 0.17$   &  $2471 \pm 1326$  &   $2.96 \pm 2.61$    &  $1.09 \pm 0.02$  & $0.15 \pm 0.73$   & $1.13 \pm 0.06$   \\
            % \rowcolor{gray!10}
            SauteRL      &   $3831 \pm 1783$    &  $9100 \pm 2557$  &   $0.08 \pm 0.09$     &  $10170 \pm 1196$  &   $3.63 \pm 1.73$    &  $1.08 \pm 0.02$   & $-1.45 \pm 0.56$   & $1.09 \pm 0.02$   \\
            % \rowcolor{gray!10}
           CRPO     &   $7893 \pm 2576$    &  $5752 \pm 1146$   &   $0.22 \pm 0.24$    &  $2966 \pm 732$   &   $7.49\pm 1.56$    &  $1.58 \pm 0.13$   & $1.47 \pm 0.68$  & $1.58 \pm 0.05$   \\
           
         \hline
         
%  \rowcolor{blue!10}
          SR-CPO    & 7176 $\pm$ 1392   & $4797 \pm 931$   & $0.47 \pm 0.34$   & $\textbf{1997} \pm \textbf{151}$  &   \textbf{11.63} $\pm$ \textbf{2.28}   &  $1.49 \pm 0.03$  & \textbf{2.21} $\pm$ \textbf{1.16}   & $1.53 \pm 0.05$   \\
% \rowcolor{blue!10}
    SR-TRPO-PID      & $5963 \pm 1384$   & $5311 \pm 741$    & $0.12 \pm 0.29$  & $2001 \pm 1014$  &   $7.31 \pm 2.85$    &  $1.07 \pm 0.02$  & $1.38 \pm 1.19$   & $1.08 \pm 0.02$   \\
    % \rowcolor{blue!10}
    SR-SauteRL      &   $4094 \pm 1007$    &  $6316 \pm 1634$   &   $0.15 \pm 0.12$     &  $8682 \pm 814$  &   $4.46 \pm 1.25$    &  $\textbf{1.05} \pm \textbf{0.01}$   & $-1.01 \pm 0.61$   & $\textbf{1.04} \pm \textbf{0.01}$   \\
    %  \rowcolor{blue!10}
    SR-CRPO     &   $\textbf{8800} \pm \textbf{985}$    &  $\textbf{4626} \pm \textbf{447}$   &   $\textbf{0.53} \pm \textbf{0.18}$    &  $2889 \pm 549$   &   $10.49 \pm 4.64$    &  $1.51 \pm 0.02$   & $ \textbf{3.12}\pm \textbf{0.99}$  & $1.53\pm 0.02$   \\
         \hline
         % \hline
    \end{tabular}
    \end{adjustbox}
    % \vspace{-1em}
    \caption{Performance of the RL policies at the end of the training. SRPL versions of the baseline algorithms (denoted by SR-*) consistently reduce the number of failures or cost-rate while significantly improving return. Training details for SRPL and baselines are provided in Appendix \ref{app:implementation-details}. The input to the RL agent is state-based in the form of joint states or LiDAR points.}
    \label{tab:hard_constraints}

\end{table}


\subsection{Learning Safety Representations Alongside Policy}


% Describe how SRPL performs:
% 1. On policy results: Discuss CPO, CRPO and Saute RL. 
% 2. Off Policy results: Discuss CVPO and CSC. Why CSC is more conservative but results in better constraint satisfaction on the other hand CVPO is much more sample efficient. Also mention that off-policy methods are way more sample efficient than off-policy ones because Off-policy SRPL-CVPO achieves same performance as SRPL-CRPO in only 2 million steps for safety gym environments.




We study the performance of SRPL agents when jointly training the \textit{S2C} model and policy (Fig. \ref{fig:on-policy-results-main}). In this setting, the \textit{S2C} model does not contain prior information and thus must learn state-conditioned safety representations using rollouts generated by the agent during training.

% Results for both Vanilla and SRPL versions of the baseline algorithms are shown in Fig. \ref{fig:constrained-MDPs} and \ref{fig:off_policy}. Table \ref{tab:hard_constraints} shows results in terms of average return and total cost at the end of training for \textit{AdroitHandPen} environment, success rate and total cost for \textit{SafeMetaDrive} environment and in terms of average return and cost rate for Safety Gym~\citep{safety-gym} environments (\textit{PointGoal1} and \textit{PointButton1}). Cost-rate is computed by dividing the total cost experienced during training by the number of steps taken in the environment. 



Table \ref{tab:hard_constraints} shows the results for on-policy safe RL algorithms along with their SRPL counterparts. Learning safety representations alongside the policy greatly enhances the sample efficiency of the baseline algorithms on all tasks, while enabling faster constraint satisfaction or fewer failures or costs during training. From Fig. \ref{fig:on-policy-results-main}, we can see that SauteRL~\citep{sauteRL} performs the best in terms of constraint satisfaction in the case of Safety Gym \citep{safety-gym} environments where the constraint threshold is greater than $1$ ($\beta > 1$) and fails to do the same for tasks like \textit{AdroitHandPen} and \textit{SafeMetaDrive}. We hypothesize that this limitation arises from how SauteRL encodes the remaining safety budget into the state. In these environments, the safety budget remains $0$ until a constraint violation occurs, rendering it ineffective for accurately representing safety throughout the task. 



Fig. \ref{fig:off_policy} shows the results for off-policy baselines and their SRPL counterparts on Safety Gym environments \textit{PointGoal1} and \textit{PointButton1} over 2M timesteps\footnote{Off-policy methods are generally more sample efficient and require fewer training samples}. By filtering out actions that might cause a constraint violation through the use of a safety critic, CSC~\citep{csc} enables better constraint satisfaction but results in conservative or suboptimal task performance. On the other hand, CVPO~\citep{cvpo} is significantly more sample efficient than CSC as well as the on-policy counterparts. SRPL significantly improves the sample efficiency of both CVPO and CSC baselines reaching similar or higher performance in comparison to SR-CVPO and SR-CPO in just $2M$ steps.



% For both \textit{AdroitHandPen} and \textit{Ant} we observe that RIPL incurs fewer failures during training without sacrificing task performance (i.e. Average Return). In the case of \textit{AdroitHandPen}, RIPL significantly improves the task performance compared to baselines. CSC also results in lower costs but sacrifices performance by filtering actions which prevents exploration. WCPG is also very conservative as a result of optimizing for worst-case failures. 


% For both \textit{PointGoal1} and \textit{PointButton1}, we observe that RIPL results in faster constraint satisfaction in terms of Episodic cost compared to baselines. RIPL also more than doubles the task performance (i.e. Episodic Return) of the baseline algorithms displaying the ability to do safer and more effective exploration. 





\begin{figure}[t] % 'h' for placing figures near here in the text
  \centering
  \begin{minipage}{0.48\textwidth} % First figure, takes up 48% of the text width
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_off_policy.png}
    \vspace{-1em}
    \caption{Off-policy results for CSC and CVPO and their SRPL counterparts on Safety Gym environments over $2M$ timesteps. While CSC has better constraint satisfaction it also leads to suboptimal performance, CVPO has better sample efficiency which is further improved by SRPL. }
    \label{fig:off_policy}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth} % Second figure, also takes up 48% of the text width
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_transfer.png} % Replace with your image
    \caption{\textit{Transferring the Safety Representations:}  Transferring the policy as well as learned safety representations from \textit{PointButton1} to \textit{PointGoal1}. Zero-shot transfer of the (frozen) safety representation leads to significant improvement in terms of sample efficiency over vanilla CPO with or without policy transfer. Fine-tuning the safety representations on \textit{PointGoal1} further improves the performance while leading to better constraint satisfaction for policy transfer.}
    \label{fig:ripl_transfer}
  \end{minipage}

\end{figure}



% We also analyze cost and return tradeoffs for different algorithms in Fig. \ref{fig:risk_reward}. We see that RIPL can greatly improve the ability of algorithms to do effective exploration to better tradeoff risk and reward.






% \begin{wrapfigure}[t]{0.4\textwidth}
%     \begin{center}
%         \includegraphics[width=\textwidth{figs/ripl_risk_reward_tradeoff.png}
%     \end{center}
% \end{wrapfigure}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.40\textwidth}
%         \centering
%         \vspace{-2cm}
%         \includegraphics[width=\textwidth]{figs/ripl_risk_reward_tradeoff.png}

%     \caption{\textbf{Risk-Reward Tradeoff:} Results for different levels of constraint enforcement and how different algorithms trade costs / incur more risk in order to explore and gain more task performance. }

%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.56\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/ripl_transfer.png}
%     \caption{\textbf{Policy and Risk transfer across tasks.} a) For Constrained MDPs, we observe that the risk model transfers information across tasks and significantly improves task performance with similar constraint satisfaction. b) For standard MDPs, without the Risk Model, PPO both trains and transfers worse than if we transfer both the Risk Model and PPO or only the Risk Model.}
%     \end{subfigure}


%     \caption{a) We study the Risk-reward tradeoff b) We study generalization across tasks in safety gym environment PointGoal1}
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/ripl_transfer.png}
%     \vspace{-1.5em}
%     \caption{\textbf{Policy and Risk transfer across tasks.} a) For Constrained MDPs, we observe that the risk model transfers information across tasks and significantly improves task performance with similar constraint satisfaction. b) For standard MDPs, without the Risk Model, PPO both trains and transfers worse than if we transfer both the Risk Model and PPO or only the Risk Model.}
%     \vspace{-1em}
%     \label{fig:risk-transfer}
% \end{figure}

% \begin{wrapfigure}{H}{0.6\textwidth}
% `   \begin{center}
%     \includegraphics[width=0.6\textwidth]{figs/ripl_risk_reward_tradeoff.png}
%     \end{center}
    
%     \caption{\textbf{Risk-Reward Tradeoff:} Results for different levels of constraint enforcement and how different algorithms trade costs / incur more risk in order to explore and gain more task performance. }
%     \vspace{2em}
%     \label{fig:choice_of_risk_function}
% \end{wrapfigure}
% \begin{wrapfigure}{lt}{0.4\textwidth} % Left figure, taking about half the width
%   \centering
%   \includegraphics[width=\linewidth]{figs/ripl_risk_reward_tradeoff.png} % Replace with your image file
%   \caption{\textbf{Risk-Reward Tradeoff:} Results for different levels of constraint enforcement and how different algorithms trade costs / incur more risk in order to explore and gain more task performance. }
%   \label{fig:fig1}
% \end{wrapfigure}
% \hfill
% \begin{wrapfigure}{rt}{0.48\textwidth} % Right figure, taking about half the width
%   \centering
%   \includegraphics[width=\linewidth]{figs/ripl_transfer.png} % Replace with your image file
%   \caption{\textbf{Policy and Risk transfer across tasks.} a) For Constrained MDPs, we observe that the risk model transfers information across tasks and significantly improves task performance with similar constraint satisfaction. b) For standard MDPs, without the Risk Model, PPO both trains and transfers worse than if we transfer both the Risk Model and PPO or only the Risk Model.}
%   \label{fig:fig2}
% \end{wrapfigure}


% \vspace{-2em}
\subsection{Risk-Reward Tradeoff}


% \begin{wrapfigure}{H}{0.5\textwidth}
% `   \begin{center}
%     \includegraphics[width=0.5\textwidth]{figs/ripl_risk_reward_tradeoff.png}
%     \end{center}
    
%     \caption{\textbf{Risk-Reward Tradeoff:} Results for different levels of constraint enforcement and how different algorithms trade costs / incur more risk in order to explore and gain more task performance. }
%     \vspace{2em}
%     \label{fig:risk_reward}
% \end{wrapfigure}

In CMDPs, safety and task performance can be treated as separate objectives, framing the problem as a multi-objective optimization task. Depending on the safety-critical nature of a system, different levels of tolerance for constraint violations during learning can be set, effectively prioritizing one objective over the other. Emphasizing safety during the learning process tends to discourage exploration, which can subsequently reduce task performance, a phenomenon often referred to as the risk-reward or safety-performance tradeoff. To investigate the effect that learning safety representations has on this tradeoff, we conduct experiments comparing the risk-reward tradeoff capabilities of SPRL with baseline algorithms.


% \begin{wrapfigure}{H}[0.5\textwidth]
%     \includegraphics[width=0.5\linewidth]{figs/ripl_risk_reward_tradeoff.png}
%     % \vspace{-1em}
%     \caption{\textit{Risk-Reward Tradeoff:} To highlight the importance of learned risk representations we study their effect on the risk-reward tradeoff on \textit{AdroitHandPen} and \textit{Ant} tasks.  }
%     \label{fig:risk_reward}

% \end{wrapfigure}









% \begin{wrapfigure}{H}{0.5\textwidth}
%     \vspace{-1em}
% `   \begin{center}
%     \includegraphics[width=0.5\textwidth]{figs/ripl_risk_reward_tradeoff.png}
%     \end{center}
    
%     \caption{\textit{Risk-Reward Tradeoff:} To highlight the importance of learned safety representations we study their effect on the risk-reward tradeoff on \textit{AdroitHandPen} and \textit{Ant} tasks.  }
%     \vspace{2em}
%     \label{fig:risk_reward}
% \end{wrapfigure}

In algorithms that approach the CMDP problem through a Lagrangian formulation (CSC in Fig.~\ref{fig:risk_reward}), prioritizing safety corresponds to increasing the value of the initial Lagrange multiplier. A higher Lagrange multiplier value reduces exploration and enhances safety during learning. In algorithms like CPO~\citep{cpo} that enforce exact constraint satisfaction at each step, we adjust the constraint threshold to modulate this tradeoff. Our experiments are conducted in two distinct environments: \textit{AdroitHandPen} and \textit{Ant}. In Fig. \ref{fig:risk_reward}
, each point represents the policy's total cost incurred during learning (x-axis) and its final performance (y-axis), measured as the average return, for various safety requirement settings (either Lagrange multiplier or constraint threshold). The ellipses represent the variance across both the x and y directions. As we increase the priority of safety while learning, the number of failures reduces along with the task performance for all RL agents.

Figure \ref{fig:risk_reward} clearly illustrates that SPRL improves baseline algorithms' ability to balance task performance and safety during the learning process. This indicates that the SPRL framework facilitates safer learning for a given level of task performance or enhances task performance for a desired level of safety. Additionally, as expected, the figure shows that safety information becomes increasingly valuable as the safety-criticality of the objective rises. 







% \begin{wrapfigure}{t}{0.5\textwidth}
% `   \begin{center}
%     \includegraphics[width=0.5\textwidth]{figs/total_cost_avg_return.png}
%     \end{center}
%     \caption{\textbf{Risk-Reward Tradeoff:} Results for different levels of constraint enforcement and how different algorithms trade costs / incur more risk in order to explore and gain more task performance. }
%     \label{fig:risk_reward}
% \end{wrapfigure}




% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figs/islandnav_main.png}
%     \caption{Island Navigation: DQN with groundtruth information about the distance to nearest unsafe state outperforms all algorithms without this priviliged input. RiskDQN is able to approximate the same information slowly and reaches similar performance level. }
%     \label{fig:island-nav-main}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figs/frozenlake_main.png}
%     \caption{FrozenLake: RiskDQN outperforms DQN and BootstrapDQN\cite{bootDQN} significantly in terms of success rate} 
%     \label{fig:frozen-lake-main}
% \end{figure}



% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figs/cargoal2_scratch.png}
%     \caption{CarGoal2: Risk training alongside policy.}
%     \label{fig:cargoal2_scratch}
% \end{figure}



% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figs/carbutton2_scratch.png}
%     \caption{CarButton2: Risk training alongside policy.}
%     \label{fig:carbutton2_scratch}
% \end{figure}


%\subsection{Ablations}

\vspace{-1em}
\subsection{SRPL as an Effective Prior}
\label{sec:generalization}
%% Target is two fold:
% 1. To show that learned risk representation on one task can be transferred to another task without additional finetuning and can lead to better sample efficiency and constraint satisfaction. 
% 2. Learned risk represnetaiton can act as an effective prior for finetuning on the target task in comparision to learning from scratch, resulting in improved sample efficiency. Describe how the results for policy transfer are shown for 2M steps since the policy already converges and finetuning results in significant improvement. Also mention the fact that finetuning the safety representation in case of not transferring the policy doesn't improve the performance comparing it to learning risk from scratch, although it does lead to faster constraint satisfaction. This can be due to the requirement of the policy to learn how to use the safety information. (reason is not super clear).




As described in Sec.~\ref{sec:experimental_setup}, to study the generalizability of the learned safety representations across tasks, we use \textit{PointButton1} as the source task and \textit{PointGoal1} as the target task. We present two sets of results 1) where we transfer the safety representations but not the policy (Fig.~\ref{fig:ripl_transfer}(left)), training policy from scratch on the target task) and 2) where we transfer both the policy and the safety representations to the target task. Additionally, for both these cases, we study the effect of freezing the \textit{S2C} model (i.e. safety representations) as well as finetuning the safety representations on the target task.

From Fig. \ref{fig:ripl_transfer} (left), we see that transferring safety representations without finetuning them on the target task (SR-CPO (safety transfer) (frozen)) improves the sample efficiency as well as enables faster constraint satisfaction on the target task. However, training the safety representations directly on the target task from scratch (SR-CPO (no transfer)) leads to better performance at convergence, due to the fact that the source and the target environments are not identical in their distribution of cost-inducing states. We further see that fine-tuning the safety representations (SR-CPO (safety transfer) (finetuned)) achieves similar performance at convergence and faster constraint satisfaction in comparison to learning representations from scratch. Fig. \ref{fig:ripl_transfer} (right) shows the results when transferring the policy from source to target task. We see that CPO with policy transfer is able to be more sample efficient compared to CPO trained from scratch. Transferring both the learned safety representations and the policy keeping the safety representations frozen (SR-CPO (policy transfer) (frozen)) leads to significant improvement in sample efficiency over CPO (policy transfer). Additionally, finetuning the safety representations on the target task (SR-CPO (policy transfer) (finetuned)) leads to even more sample-efficient agents as well as better constraint satisfaction.


% \begin{wrapfigure}{rt}{0.48\textwidth} % Right figure, taking about half the width
%   \centering
%   \includegraphics[width=\linewidth]{figs/ripl_transfer.png} % Replace with your image file
%   \caption{\textbf{Policy and Risk transfer across tasks.} a) For Constrained MDPs, we observe that the risk model transfers information across tasks and significantly improves task performance with similar constraint satisfaction. b) For standard MDPs, without the Risk Model, PPO both trains and transfers worse than if we transfer both the Risk Model and PPO or only the Risk Model.}
%   \label{fig:fig2}
% \end{wrapfigure}

% In this section, we study the generalizability of risk models and risk-informed policies to other tasks and environments. We do this by pre-training risk models and policies on task A and fine-tuning the policies on task B while freezing the risk model (policy transfer). Additionally, we show results for risk-model transfer with randomly initialized policies (risk transfer), where a new policy is trained from scratch on task B while conditioned on the output of a frozen risk model trained on task A. The questions we want to answer are the following: 1) Does the risk model learn environment-centric information that can be transferred across tasks in similar environments? 2) Do risk-informed policies lead to better transfer? 3) Do risk models transfer better than policies?



% Since, the risk-model is trained in a policy-agnostic way, its expected to learn environment-centric risk information. We do the following experiments to test this hypothesis. We first train the risk-model on task A alongside policy learning for task A,  the resultant risk model is then used to condition policy-learning for task B (risk transfer). Importantly, the risk model is frozen i.e. is not updated during the policy learning on task B. We also show results for policy transfer, i.e. we pre-train both the vanilla and risk-conditioned policies on Task A and then fine-tune on Task B. 

% Fig. \ref{fig:risk-transfer}(a) shows that DQN pre-trained on FrozenLake-v1 is unable to transfer environment-specific information across tasks, while RIPL(DQN) (policy transfer) is able to leverage the risk-information from FrozenLake-v1 to be more sample efficient and achieve higher performance on FrozenLake-v2.
% From Fig. \ref{fig:transfer}, we can see that the risk model transfers well across environments and results in significant improvement in task performance. Since CPO uses a cost critic which models the expected cost for the current policy, we see that transferring the policy also helps in this case and leads to better constraint satisfaction and faster learning, transferring both the policy and risk model while keeping the risk model frozen and updating the policy results in the most sample-efficient transfer.  




\vspace{-1em}
\section{Ablations \& Analysis}
\begin{figure}[t] % 'h' for placing figures near here in the text
  \centering
  \begin{minipage}{0.48\textwidth} % First figure, takes up 48% of the text width
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_risk_reward_tradeoff.png}
    % \vspace{-1em}
    \caption{\textit{Risk-Reward Tradeoff:} SR-CPO is able to better tradeoff risk and reward in comparison to vanilla CPO thanks to learned safety representations. }
    \label{fig:risk_reward}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth} % Second figure, also takes up 48% of the text width
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_ablation_choice_of_risk.png} % Replace with your image
    \caption{\textit{Alternative Safety Representations:} An analysis of the effect of different choices in modelling safety representations on overall performance in terms of safety and efficiency of the algorithm. }
    \label{fig:choice_of_risk_function}
  \end{minipage}

\end{figure}

% \subsection{Risk-Reward Tradeoff}


\subsection{Alternative Safety Representations}



%% Objectives
%% Discuss the issue of learning safety representations on-policy or off-policy. Take information from what Reviewer 4 has described and discuss it here. Use the results in this section to illustrate that relying on agent's entire experience leads to better safety representations that results in improved risk-reward tradeoff.


 The choice of modelling the safety representation depends on the properties we desire our safety representations to encode. For the safety representation to be state-centric we need it to be trained in a policy-agnostic way (i.e. unlike the value function the safety representation encodes the likelihood of failure from data collected from a diverse set of policies experienced during learning). In this section, we study some of the alternate choices of safety representations:
 \begin{enumerate}[label=(v\arabic*)]
 \item modelling safety representation as the expected likelihood of entering an unsafe/cost-inducing state for the current policy~\citep{csc},
 \item policy-dependent safety representation as a distribution over proximity to unsafe states trained using on-policy rollouts,
 \item learning the safety representation, as proposed here, from the past experience of the agent, across a diverse set of policies.
 \end{enumerate}


% \begin{wrapfigure}{t}{0.6\textwidth}
%     \vspace{-2.5em}
% `   \begin{center}
%     \includegraphics[width=0.6\textwidth]{figs/ripl_ablation_choice_of_risk.png}
%     \end{center}
    
%     \caption{\textbf{Choice of Risk function:} We analyze the effect of different choices in modelling risk on overall performance in terms of safety and efficiency of the algorithm. }
%     % \vspace{-1em}
%     \label{fig:choice_of_risk_function}
% \end{wrapfigure}

 As shown in Fig. \ref{fig:choice_of_risk_function}, SRPL outperforms all other safety representation models. 
 The safety representation learned in (v1) does not improve the performance of the RL agent, as this information is already captured by the cost-critic in CPO~\citep{cpo}.
 Although the policy-dependent safety representation (v2) enhances the base algorithm's performance, it does not surpass SRPL. We attribute this to two key factors: (1) SRPL utilizes data from previous policies, allowing the safety representation to encode information about a broader region of the state space, while the policy-dependent safety representation is confined to on-policy rollouts, a common limitation in on-policy versus off-policy reinforcement learning~\citep{sac}; (2) by learning state-centric features, SRPL promotes more stable dynamics during RL training, in contrast to the policy-dependent representations, which fluctuates with policy updates.



\subsection{Effect of safety representations for high-dimensional observations}




% \begin{tabular}{|c|c|c|c|c|c|c|}
%   \hline
%    & \multicolumn{3}{c|}{\textit{Return}} &  \multicolumn{3}{c|}{\textit{Cost-Rate}} \\ 
%   % \hline
%   Sensor Modality & CPO & RI-CPO & change (\%) & CPO & RI-CPO & change (\%) \\ 
%   \hline
%   LiDAR & $6.45 \pm 2.91$ & $11.63 \pm 2.28$ & $+77.69$ & $1.61 \pm 0.12$ & $1.49 \pm 0.03$ & $-19.67$  \\ 
%   % \hline
%   Depth & $3.78 \pm 2.41$ & $9.44 \pm 3.86$ & $+149.73$ & $1.91 \pm 0.48$ & $1.66 \pm 0.1$ & $-27.47$ \\ 
%   % \hline
%   RGB  & $2.54 \pm 1.838$ & $8.134 \pm 4.46$ & $+219.98$ & $2.0 \pm 0.334$ & $1.7 \pm 0.374$ & $-30.01$\\ 

%   \hline
  
% \end{tabular}



% \begin{table}[t]
%     \centering
%         \begin{adjustbox}{width=\columnwidth,center}
%     \begin{tabular}{|c|c|c|c|c|c|c|}
%       \hline
%        & \multicolumn{3}{c|}{\textit{Return} ($\uparrow$)} &  \multicolumn{3}{c|}{\textit{Cost-Rate} ($*1e^2$) ($\downarrow$)} \\ 
%       % \hline
%       Sensor Modality & CPO & RI-CPO & change (\%) & CPO & RI-CPO & change (\%) \\ 
%       \hline
%       LiDAR & $6.45 \pm 2.91$ & $11.63 \pm 2.28$ & $+77.69$ & $1.61 \pm 0.12$ & $1.49 \pm 0.03$ & $-19.67$  \\ 
%       % \hline
%       Depth & $3.78 \pm 2.41$ & $9.44 \pm 3.86$ & $+149.73$ & $1.91 \pm 0.48$ & $1.66 \pm 0.1$ & $-27.47$ \\ 
%       % \hline
%       RGB  & $2.54 \pm 1.838$ & $8.134 \pm 4.46$ & $+219.98$ & $2.0 \pm 0.334$ & $1.7 \pm 0.374$ & $-30.01$\\ 
    
%       \hline
%     \end{tabular}
%     \end{adjustbox}
%     \caption{Risk representation learning improves agent performance (return) and reduces constraint violations (cost-rate) in higher-dimensional observation spaces, where representation learning is typically more challenging. }
%     \vspace{-1em}
%     \label{tab:sensor_modes}
% \end{table}


To investigate the effects of learning low-dimensional state-centric safety representations from high-dimensional observations, we conducted experiments in the Safety-Gym \textit{PointGoal1} environment \cite{safety-gym}. We examined how different sensor modalities impact task performance and safety during learning. As shown in Table \ref{tab:sensor_modes}, increasing the dimensionality of sensor observations, from LiDAR to depth to RGB, leads to a decline in both safety and task performance. This degradation is likely due to the increased complexity involved in learning effective representations from higher-dimensional data. Additionally, Table \ref{tab:sensor_modes} emphasizes that as the dimensionality of observations rises, learning safety representations as an inductive bias becomes increasingly critical for ensuring safe and efficient policy learning.


% \subsection{Studying the effect of convergence of Risk model on policy learning}


% Hypothesis: Risk model is learning policy-agnostic environment-centric features, so it should converge faster than policy or the value function chasing a moving target. The fact that the risk model becomes reliable early on in policy learning might help with the performance of the learned policy. Caveat: On the flip side, learning risk model too fast i.e. overfitting on data sampled by the initial policies can lead to poor generalization. (maybe we can show some results for this also in the appendix). 
\begin{wraptable}{r}{0.6\textwidth} % 'r' for right alignment, width=60% of text width
\vspace{-2cm}
    \centering
    \begin{adjustbox}{width=\linewidth,center} % Adjust table width to fit the wraptable area
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
       Sensor & \multicolumn{3}{c|}{\textit{Return} ($\uparrow$)} &  \multicolumn{3}{c|}{\textit{Cost-Rate} ($*1e^2$) ($\downarrow$)} \\ 
       Modality & CPO & RI-CPO & change (\%) & CPO & RI-CPO & change (\%) \\ 
      \hline
      LiDAR & $6.45 \pm 2.91$ & $11.63 \pm 2.28$ & $+77.69$ & $1.61 \pm 0.12$ & $1.49 \pm 0.03$ & $-19.67$  \\ 
      Depth & $3.78 \pm 2.41$ & $9.44 \pm 3.86$ & $+149.73$ & $1.91 \pm 0.48$ & $1.66 \pm 0.1$ & $-27.47$ \\ 
      RGB  & $2.54 \pm 1.838$ & $8.134 \pm 4.46$ & $+219.98$ & $2.0 \pm 0.334$ & $1.7 \pm 0.374$ & $-30.01$ \\ 
      \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Safety representation learning improves agent performance (return) and reduces constraint violations (cost-rate) in higher-dimensional observation spaces, where representation learning is typically more challenging.}
    \vspace{-0.2cm}
    \label{tab:sensor_modes}
\end{wraptable}

\section{Related Work}
% Push the related work section to the end just before Conclusion. 

%% What this section should talk about? 
% 1. Existing Methods for Safe Exploration 
% 2. Risk-aware RL or Risk averse RL methods
% 3. Representation Learning and Inductive Bias for Reinforcement Learning 
% 4. 

\textit{Representation Learning for RL:} RL algorithms often need to learn effective policies based on observations of the environment, rather than having direct access to the true state. These observations can come from sensors like RGB cameras, LiDAR, or depth sensors. Learning representations that capture the essential aspects of the environment or task can significantly enhance efficiency and performance. To achieve this, various methods employ auxiliary rewards or alternative training signals \citep{sutton2011horde, jaderberg2016reinforcement,riedmiller2018learning, lin2019adaptive}. One effective approach is learning to predict future latent states, which has proven valuable in both model-free \citep{munk2016learning,schwarzer2020data,ota2020can} and model-based \citep{watter2015embed,ha2018world} settings. In this paper, we've focused on learning representations for state-conditioned safety that can enable more informed decision-making in safety-critical applications. 


% \textit{Risk-sensitive RL:} Several methods have looked at the safe RL problem through the lens of risk sensitivity. These methods generally model the aleatoric uncertainty associated with the environment~\citep{dist_rl, morimura2010nonparametric, tamar2015policy} (transition dynamics or reward uncertainty) and propose algorithms that are robust to these uncertainties. 




\textit{Safe Exploration:} Safe exploration~\citep{cpo, cvpo, sauteRL, pid_lag, sauteadj, gu2024review} approaches need to contend with both the aleatoric uncertainty of the environment and the epistemic uncertainty associated with the exploration of unseen parts of the state-space. These methods commonly achieve this by restricting exploration to parts of the state space with low epistemic uncertainty. Bayesian model-based methods \citep{thesis_berkenkamp}, represent uncertainty within the model via Gaussian processes, favouring exploration in states with low uncertainty. ~\citep{huang2023safe} incorporate lagrangian-methods into world models. \citet{cscadj}, \citet{learning_to_be_safe}, \citet{ldm} and \citet{csc} use a safe Bellman operator (called the safety critic) to evaluate the risk of failure from a given state taking a particular action and use it to restrict exploration by filtering out actions with high risk of failure or formulating constraints according to the safety critic. \citep{sauteRL, sauteadj} use accumulated cost as a proxy for risk associated with a state and use it to augment the state space.



% The common theme of disincentivizing exploration often results in these methods being overly conservative and sample inefficient. In this paper, we instead focus on informing the RL agent directly about the risks in the environment by proposing an inductive bias thus empowering the agent to tradeoff safety and reward more efficiently.


% \todo{Add a paragraph to descibe all the recent work along with the new baselines added... can be included in the existing paragraph}



\vspace{-1em}
\section{Discussion \& Conclusion}  
\vspace{-1mm}



While SRPL offers a valuable approach for improving safety representations in RL agents, it comes with certain limitations, which, though typical of many RL methods, are still worth noting. First, SRPL has difficulty capturing long-horizon causal mechanisms related to safety. For example, a state at the beginning of a long single-way track leading to an unsafe state is actually very risky for the agent but would be represented by the safety representation as low risk or safe, as the number of actions separating the current and final, unsafe state is large. While challenging, such examples are not typical for most embodied or otherwise high-risk agents. Second, our chosen definition of safety has practical limits. We do not consider degrees of safety, and in general defining safety, harm, danger, or any other bad outcome typically involves substantial nuance in real-world settings which most safe RL methods, including SRPL, struggle to capture completely.

In summary, this paper addresses the problem of reinforcement learning (RL) agents becoming overly conservative as a result of penalties due to safety violations early in training in safety-critical environments, leading to suboptimal policies. To tackle this, we propose a framework that learns state-specific safety representations from the agent's experiences. By integrating this safety information into the state representation, our approach enables more informed and balanced decision-making. 


\section*{Acknowledgements}

The authors thank Yann Pequignot and Adriana Knatchbull-Hugessen for insightful discussions and useful suggestions on the early draft. This work was supported by the DEEL Project funded by the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Consortium for Research and Innovation in Aerospace in Québec (CRIAQ). 

% Empirical evaluations demonstrate that SRPL agents outperform baseline algorithms by improving task performance while also reducing constraint violations during learning. Furthermore, we show that SRPL agents effectively optimize the risk-reward tradeoff and improve the transfer of knowledge across tasks.

% In summary, RIPL is a general-purpose RL method that leverages distributional estimates of the distance to unsafe states given by a risk model trained either online with the RL policy or frozen and subsequently transferred to new tasks. We find RIPL leads to better constraint satisfaction, improved sample efficiency, and increased transfer performance on a range of tasks including both constrained and unconstrained MDPs in both discrete and continuous state and action spaces. In the future, we are interested in finding the decision-theoretic optimal stopping point for training the risk model, possibly decoupling its optimization from the RL policy. Additionally, given an optimal risk model, we are interested in sim2real transfer onto robot morphologies, possibly using the risk model as part of model-based RL.







% \subsubsecti


\bibliography{references}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\include{rebuttal}


\section{Appendix}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_islandnav_reset.png}
    \caption{\textit{DQN with resets}: Resetting the DQN agent to overcome conservatism results in significantly more failures during training.}
    \label{fig:island-resets}
    \vspace{-1em}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/cpo_primacy_bias.png}
    \caption{\textit{Conservatism in CPO}: As the cost-limit is reduced CPO agent struggles to overcome the bias induced by initial experiences where the agent encountered heavy penalties due to constraint violation, resulting in overly conservative behaviours. }
    \label{fig:cpo-primacy}
    \vspace{-1em}
\end{figure}


\subsection{Primacy Bias in Safe RL}
\label{app:primacy-bias}

RL agents overfitting on early experience is a well-studied problem. \cite{primacy} first showed that outcomes of early experience can have long-lasting effects on subsequent learning of RL agents. \cite{overfitting_robust_bengio} identified poor data diversity caused by limited exploration as the primary reason for overfitting. In safety-critical systems, penalties imposed on RL agents due to constraint violations early in the training can further disincentivize exploration limiting the data diversity further. As a result RL agent tends to learn from samples from a narrow region of the state space resulting in conservative behaviors and suboptimal performance. \cite{primacy} proposed to reset the agent to encourage exploration and avoid primacy bias but for safety-critical applications resetting the RL agent can lead to catastrophic consequences. As shown in Fig.~\ref{fig:island-resets}, DQN with resets lead to significantly more failures than vanilla DQN, while not resulting in significant improvement in performance. In this paper, we've proposed to deal with this problem by learning state-centric representations of safety that force the agent to learn safety representations from limited data and overcome conservative behaviour.


To illustrate the extent of overfitting on negative experiences experienced by the agent early in training, we study the impact of penalties caused by constraint violation on the behaviour of the agent. We experiment on \textit{AdroitHandPen} with CPO~\citep{cpo} for different cost thresholds. A lower cost limit in the case of CPO is essentially equivalent to penalizing failures during learning more and more. From Fig.~\ref{fig:cpo-primacy} We see that as we reduce the cost limit the number of failures reduces but also the resultant policy becomes more and more conservative, ultimately for cost limit = 0.01, the agent fails to learn anything about the task and settles for a local optima in which it doesn't move.







\subsection{Implementation Details}
\label{app:implementation-details}


% 1. Choice of the S2C model architecture etc. 
% 2. Expound on Safety Horizon and bin sizes. 
% 3. Expound on Replay buffer and how its populated for different environments.
% 4. Add more hyperparameter details 
% 5. Training details for on-policy vs off-policy for the S2C model. 
% 6. Explain how you can't use the cost-limit of 0 with CPO and CRPO. 




% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{figs/ant_risk_fig.png}
%     \caption{\textit{Safety Representation (Ant)}: (col 1) Safety representation outputs a high probability for low proximity to an unsafe state. The agent has three of its feet in the air and it's about to topple. (col 2) Safety representation predicts a high probability for steps to cost around 20-30 steps, indicating a moderately safe state. (col 3) Safety representation predicts a large probability for large values of steps to cost indicating a safe and stable state.}
%     \label{fig:ant-risk}
%     \vspace{-1em}
% \end{figure}





In this section, we aim to clarify some of the design choices and implementation details used for training SRPL agents for on-policy and off-policy baselines. Our implementations were based on top of FSRL~\citet{fsrl} and Omnisafe~\citet{omnisafe} codebases.

\subsubsection{Learning the Safety Representation}

The safety representation aims to characterize a state with respect to its proximity to the unsafe states and models a distribution over steps-to-cost based on the agent's past experience. Empirically, in the case of discrete state-spaces like \textit{Island Navigation}, for a given state safety representation this is equivalent to maintaining the normalized frequencies over proximity to unsafe states based on the agent's past experience or policy rollouts in the past.

Here we face a dilemma about how to learn such a representation. should we use on-policy rollouts or should we use off-policy data? There is an inherent tradeoff in this choice: if we train the safety representation using on-policy rollouts to ensure coverage we would need to collect lots of samples which will make the algorithm highly sample inefficient and also unsafe. This is particularly infeasible in the case of off-policy baselines like CVPO~\citep{cvpo} and CSC~\citep{csc}, where the policy is updated at every step.  On the other hand, if we learn the safety representations completely using all of the agent's past experience, at some point the representation will become irrelevant to the current policy because it is storing information about policies that might be too different from the current policy. To manage this tradeoff, we learn the safety representation using off-policy data from the agent's past but recent experience, we maintain a separate replay buffer in case of on-policy algorithms or reuse the existing replay buffer of off-policy algorithms to store steps-to-cost $\delta_{\tau}(s)$ for a state encountered in a particular trajectory $\tau$. We keep a fixed replay buffer size and experiences that are generated by the older policies are discarded when the replay buffer gets full in a First In First Out (FIFO) manner. This allows us to ensure that the safety representation is learned with enough data to ensure coverage over the state space while also keeping the representation relevant for the current policy.

% Fig.~\ref{fig:ant-risk} shows the output of the \textit{S2C} model (i.e. the safety representation) at the end of $10M$ timesteps for different positions of the ant robot. The model predicts high density for low proximity to failure in the state where the Ant robot has only one foot touching the ground (Fig.~\ref{fig:ant-risk}-left) representing that there is a high likelihood that the agent will flip over. Conversely, in (Fig.~\ref{fig:ant-risk}-right) the model predicts high probabilities to higher values of steps-to-cost indicating that the agent's likelihood of failure is low based on its past experience.



\subsection{Training the \textit{S2C} model}


Because the output of the \textit{S2C} model is given to condition policy learning, there are some considerations while training the \textit{S2C} model to stabilize the training of the SRPL agent. In the case of on-policy baselines, since the policy is frozen while collecting on-policy rollouts, the dynamics of learning the safety representation alongside the policy is more stable and we update the safety representation (i.e., the \textit{S2C}) model at a higher frequency than the policy. On the other hand, doing this in case of off-policy algorithms like CSC~\cite{csc} and CVPO~\cite{cvpo} really destabilizes training because the policy is being updated at a very high frequency making the dynamics really complex. To solve this, we train the safety representations at a significantly lower frequency than the policy. This ensures that the \textit{S2C} model is frozen while the policy is being updated. The idea is similar to the use of target networks proposed in~\cite{dqn} to stabilize training.



\subsubsection{Practical Considerations}


There are some practical considerations in order to show results on environments like \textit{AdroitHandPen} and Mujoco (\textit{Ant}, \textit{Hopper}, \textit{Walker2d}). Training these tasks with a cost-limit of $0$ leads to overly conservative policies that learn nothing about the task as demonstrated in Fig.~\ref{fig:risk_reward} and Fig.~\ref{fig:cpo-primacy}, for a cost-limit of $0$, the \textit{\# failures} is very low but also the performance degrades. To address this issue we went with the cost limit of $0.1$ for our experiments for both SRPL and vanilla versions of the baseline algorithms.



\subsubsection{Hyperparameter Details}

As described earlier we chose a bin size of $4$ and safety horizon $H_s = 80$  for \textit{PointGoal1} and \textit{PointButton1} environments and a bin size of $4$ and safety horizon $H_s = 40$ for all the other environments. The batch size for training the S2C model was chosen between $512$ and $5000$ and we found that a batch size of $5000$ led to better performance in Safety Gym environments and $512$ for all the other environments. Additionally, we optimize for hyperparameters like when to update the S2C model $update-freq$ which was set to $100$ for on-policy baselines and $20000$ for off-policy baselines. Additionally, we used a learning rate of $1e-6$ or $1e-5$ for on-policy experiments and a learning rate of $1e-3$ for off-policy baselines. The well-optimized hyperparameters of the baseline algorithms can be found in open-source implementations such as Omnisafe~\cite{omnisafe}\footnote{https://github.com/PKU-Alignment/omnisafe} (for CPO, CRPO, TRPO-PID, SauteRL) and FSRL~\cite{fsrl}\footnote{https://github.com/liuzuxin/FSRL} (for CVPO). For CSC~\cite{csc} we used the hyperparameters specified in the original paper. We faced difficulty in training on-policy baseline algorithms on \textit{SafeMetaDrive} with default hyperparameters from Omnisafe, fine-tuning hyperparameters revealed that lower target-kl stabilizes the baseline algorithms. $target-kl=0.0005$. Additionally, we faced difficulty in stabilizing CVPO~\citet{cvpo} on \textit{AdroitHandPen} and \textit{Ant} tasks with the default hyperparameters. We couldn't stabilize the training with a limited hyperparameter search. The S2C model has the same network architecture as the policy which in most cases is an MLP with two hidden layers of size $64$.









% Thus the objective of safety representation is not to model the likelihood of failure of the current policy which requires us to collect several policy rollouts to ensure coverage, instead similar to off-policy algorithms we use off-policy rollouts to learn the safety representations ensuring that the replay buffer keeps agent's experience from recent past but discard really old experience which might be irrelevant to the current policy. This enables us to learn safety representations more efficiently by bootstrapping from the agent's prior experience. 

% \subsection{Off-Policy }




% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/environments.png}
%     \caption{\textit{Environments}: We perform experiments on four continuous control tasks in three environments. They are displayed in the following order from left to right: \textit{AdroitHandPen}, \textit{Ant}, \textit{PointGoal1}, \textit{PointButton1}. }
%     \label{fig:environments}
%     \vspace{-1em}
% \end{figure}

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figs/islandenvs_all.png}
    \vspace{-2em}
    \caption{\textit{Island Navigation:} Instead of experimenting with a single environment with fixed start and goal state where the agent can simply memorize action sequences. We create four different versions of the Island Navigation environment with different start and goal positions as well as locations of the water tiles.}
    \label{fig:islandnavenvs}
\end{figure}




\subsection{Environmental Details}
\label{app:environment_details}



% Sepecify state-space action-space and reward for each environment, as well as cost-inducing states.


\begin{itemize}
    \item \textit{Island Navigation~\citep{ai-grid}} is a grid-world environment explicitly designed for evaluating safe exploration algorithms. The environment consists of an agent marked $A$, trying to navigate an island to reach a goal $G$ while avoiding water cells (marked in blue). Entering a water cell is considered a failure and the agent receives a heavy reward penalty along with episode termination. The state observation consists of the image of the island at a given time (as such the state is completely observable). The action space is $["left", "right", "up", "down"]$, so there is no action to stay in place. Because its a deterministic environment with a small distance between the start state and goal state, its not difficult for RL agents to just remember the action sequences instead of learning an accurate state-conditioned value function. In order to avoid such a situation, we instead created 4 copies of the same environment with different positions of start state and goal state as well as the location of the water cells (Fig. \ref{fig:islandnavenvs}).

    \item \textit{AdroitHandPen~\citep{adroit}} is a manipulation environment, where a 24-degree of freedom Shadow Hand agent is learning to manipulate a pen from a randomly initialized start orientation to a randomly specified goal orientation. The input to the RL agent is the angular positions of the finger joints, the pose of the palm of the hand, as well as the current pose of the pen and the target pose of the pen. The action space is continuous and are the absolute angular position of the hand joints ($24$-dimensional). The pen falling on the ground is considered a failure and induces a cost. Since the cost-inducing state is also non-ergodic (irrecoverable), maximum cost for an episode cannot be greater than 1. The agent is provided a dense reward based on the similarity of the current pose to the target pose of the pen.

    \item \textit{SafeMetaDrive~\citep{metadrive}} is an autonomous driving environment, where an agent is tasked with navigating the traffic containing static and dynamic participants. The agent has to frequently change lanes or apply brakes to avoid incurring costs. Any collision induces cost and the cost-limit is set to $1$. The observation space consists of LiDAR information about the surrounding objects, ego-vehicle's pose, and safety indicators along with route points. The agent is rewarded for following the waypoints/route as well as for keeping the lane and for driving at high velocity. The action space consists of steering, brake and throttle values.

    \item \textit{Safety-Gym~\citep{safety-gym}} is a navigation environment, we evaluate \textit{SRPL} on \textit{PointGoal1} and \textit{PointButton1} tasks. In \textit{PointGoal}, a randomly initialized point agent is trying to navigate to a randomly sampled goal position while avoiding hazards in the environment which are the cost-inducing states, so that the total cost for an episode doesn't exceed $10$. In \textit{PointButton1}, the agent is tasked with pressing the specific orange button, pressing the wrong button induces a cost and there are hazards as well as dynamic obstacles in the environment which are cost-inducing. Cost threshold is set to $10$. In the original version of safety gym, the agent's observation consists of LiDAR information about every object including hazards, buttons and goal position individually. This prevents us from doing the generalization experiment since the dimensionality of the observation for \textit{PointGoal1} and \textit{PointButton1} are different because they have different objects in the environment. To address this we aggregate the Lidar information for all objects into one LiDAR observation that for every LiDAR point provides the distance to the nearest object as well as the corresponding object label. The action space for the point agent is the two-dimensional (force applied to move the point agent and the velocity about the z-axis). The reward function for the agent is based on the distance to the goal state.

    \item \textit{Mujoco environments~\citep{mujoco}:} We evaluate \textit{SRPL} on three locomotion tasks (\textit{Ant}, \textit{Hopper} and \textit{Walker2d}). The goal in all three tasks is to run as fast as possible while not falling on the ground. Falling on the ground is considered a failure and induces a cost. We treat the agent fallen on the ground as a non-ergodic state and thus the maximum cumulative cost for an episode can be $1$. The agent's state input is the joint position and velocity of the robot's body parts as the centre of mass-based external forces acting on the body. The action space is the torques applied to each joints. The agent's reward is proportional to the velocity of the agent, and the agent is penalized for applying high torque on its joints.


\end{itemize}
















% \subsection{Alternative Choice of Modelling Safety}



\begin{figure}[t]
    \centering
     \includegraphics[width=\linewidth]{figs/ripl_mujoco.png}
    % \vspace{-1em}
    \caption{We report the performance of safety-informed  SRPL agents (denoted SR-*) on Mujoco environments (Ant, Hopper and Walker2d). For these experiments, both the S2C model and the policy have been randomly initialized so no prior information has been provided to the agent. Safety-informed agents consistently outperform their vanilla counterparts on both safety during learning as well as sample efficiency. Results were obtained by averaging the training runs across five seeds. The input to the RL agent is the joint state and velocity.}
    \label{fig:additional-results}
\end{figure}

\subsection{Additional Results}


In addition to the results presented in the main paper, we perform experiments on Mujoco environments (\textit{Ant}, \textit{Hopper} and \textit{Walker2d}) and show results for CPO~\citep{cpo} and CRPO~\citep{crpo} along with their SRPL counterparts. From Fig.~\ref{fig:additional-results}, we can see for the \textit{Ant} environment, CRPO significantly outperforms CPO in terms of task performance (i.e. return) but also leads to more failures during training. SR-CRPO has similar performance in terms of return but significantly lower total failures during training thus making the algorithm safer. On the other hand, SR-CPO leads to an improvement in the performance in terms of return in comparison to CPO, while incurring similar but fewer failures. Similarly, for \textit{Hopper} SR-CRPO outperforms all other baselines in terms of task performance as well as leads to the least total failures during training. SR-CPO also shows improvement over CPO in terms of safety by incurring fewer failures during learning. Finally, on \textit{Walker2d}, SR-CPO outperforms all the baselines in terms of task performance while incurring the same number of failures as SR-CRPO. We observe a consistent improvement in either the safety or efficiency of the SRPL versions of the algorithms in comparison to their vanilla counterparts.

To provide more clarity into the results presented in the paper, we have added Fig.~\ref{fig:on-policy-epcost}, which highlights the episodic cost for AdroitHandPen and SafeMetaDrive environments as well as the respective cost-limits. In our experiments we observed that setting the cost-limit to $0$ for AdroitHandPen or the Mujoco environments was leading to the policy failing to learn anything, so we set the cost-limit to $0.1$ ($\beta = 0.1$) for AdroitHandPen as well as Mujoco experiments (Fig.~\ref{fig:additional-results}). For clarity we've also added pairwise plots for baseline algorithms with their SRPL versions on \textit{AdroitHandPen} Fig.~\ref{fig:adroit-pairwise}, \textit{SafeMetaDrive} Fig.~\ref{fig:metadrive-pairwise}, \textit{PointGoal1} Fig.~\ref{fig:pointgoal-pairwise} and \textit{PointButton1} Fig.~\ref{fig:pointbutton-pairwise}.




\begin{figure}[t] % 'h' for placing figures near here in the text
  \centering
  \begin{minipage}{0.48\textwidth} % First figure, takes up 48% of the text width
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_bin_sizes_metadrive.png}
    % \vspace{-1em}
    \caption{\textit{Choice of bin size:} Results on SafeMetaDrive show that lowering the bin size better the performance of the model.}
    \label{fig:bin_sizes}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth} % Second figure, also takes up 48% of the text width
    \centering
    \includegraphics[width=\linewidth]{figs/ripl_safety_horizon_metadrive.png} % Replace with your image
    \caption{\textit{Choice of safety horizon $H_s$} Results on MetaDrive show that higher safety horizon $H_s$ leads to improvement in performance but above a threshold the curve plateaus. }
    \label{fig:safety_horizon}
  \end{minipage}

\end{figure}


\subsection{Additional Ablations}

To further analyze the safety representations proposed in the paper, we perform additional ablations that study the effect of design choices like safety horizon ($H_s$) and bin size for learned distribution. We also study the generalization ability of the learned safety representations across constraint thresholds.


\subsubsection{Effect of Safety Horizon and Bin Size}
\label{sec:safety-horizon}
% LP Modelling the safety distribution over the entire horizon becomes infeasible because of high dimensionality in the case of long horizon MDPs and completely impractical for infinite horizon MDPs, instead, we model the safety representation over a safety horizon $H_s$, which is significantly smaller than true horizon length $H_s << H$. This is done to ensure that the dimensionality of the risk distribution is manageable. Additionally, to further reduce the dimensionality of the safety representation we split the safety horizon into bins and model the safety representation over the bins. 

In Fig.~\ref{fig:bin_sizes} and \ref{fig:safety_horizon} we study the effect of different choices of bin size and safety horizon on the performance of the policy both in terms of safety (in terms of total cost) and efficiency (in terms of success rate) for \textit{SafeMetaDrive} environment. In Fig.~\ref{fig:safety_horizon}, we analyze the effect of varying safety horizons for a fixed bin size of $bin-size = 2$. We can see that both success rate and total cost almost plateau post the bin size of around $50$, the dimensionality of the safety representation can be the cause behind the slight deterioration in performance beyond $H_s = 50$. Fig.~\ref{fig:bin_sizes} studies the effect of bin sizes given a fixed safety horizon $H_s = 40$, from the figure it is clear that smaller bin sizes lead to better performance both in terms of safety and efficiency of the SRPL agents.


\begin{figure}[t]
    \centering
     \includegraphics[width=\linewidth]{figs/ripl_across_cost_thresholds.png}
    % \vspace{-1em}
    \caption{\textit{Generalization across Cost thresholds}: Since safety representations are learned to model the proximity to unsafe states, they can be generalized across cost thresholds. Here we show that safety representation learnt on \textit{PointButton1} with cost-limit = $10$ can be transferred to \textit{PointGoal1} task for cost-limits other than 10 without the need for fine-tuning and results in improved sample efficiency. }
    \label{fig:ablation-cost-threshold}
\end{figure}



\subsubsection{Generalization of Safety Representation Across Cost Thresholds}

Instead of modelling the likelihood of constraint violation, safety representation encodes the distribution over steps to cost or distance to cost-inducing/unsafe states. This design choice is based on the principle that we are interested in modelling state-centric features that are generalizable and don't depend on the task definition (for e.g., the cost threshold definition). We perform experiments on \textit{PointGoal1} environment for the transferability of safety representation across constraint thresholds. For this experiment, we mimic the structure of the generalization experiments shown in Sec.~\ref{sec:generalization}. We treat \textit{PointButton1} as the source task and \textit{PointGoal1} as the target task where the safety representation as well as policy for both CPO and SR-CPO is trained on \textit{PointButton1} task. In order to study the generalizability of safety representations across cross thresholds we don't fine-tune the safety representation on the target task for different cost thresholds thus freezing the \textit{S2C} model. From Fig.~\ref{fig:ablation-cost-threshold}, we can see that SR-CPO (policy transfer) (frozen safety) significantly outperforms CPO (policy transfer), thus transferring the policy and frozen safety representation across tasks enables SRPL agents to learn more samples efficiently while ensuring constraint satisfaction for different constraint thresholds.






% \subsection{Experimental Results}








\begin{figure}[H]
    \centering
    % \vspace{-2em}
    \includegraphics[width=\linewidth]{figs/ripl_on_policy_episodic_cost.png}
    \vspace{-2em}
    
    \caption{The figure shows Episodic Cost / Episodic Failure for AdroitHandPen and SafeMetaDrive experiments}
    \label{fig:on-policy-epcost}
    
\end{figure}



\begin{figure}[H]
    \centering
     \includegraphics[width=\linewidth]{figs/ripl_adroit_pairwise.png}
    % \vspace{-1em}
    \caption{\textit{AdroitHandPen}: We plot all the baselines and their SRPL counterparts }
    \label{fig:adroit-pairwise}
\end{figure}


\begin{figure}[H]
    \centering
     \includegraphics[width=\linewidth]{figs/ripl_metadrive_pairwise.png}
    % \vspace{-1em}
    \caption{\textit{SafeMetaDrive}: We plot all the baselines and their SRPL counterparts}
    \label{fig:metadrive-pairwise}
\end{figure}


\begin{figure}[H]
    \centering
     \includegraphics[width=\linewidth]{figs/ripl_pointgoal1_pairwise.png}
    % \vspace{-1em}
    \caption{\textit{PointGoal1}: We plot all the baselines and their SRPL counterparts}
    \label{fig:pointgoal-pairwise}
\end{figure}

\begin{figure}[H]
    \centering
     \includegraphics[width=\linewidth]{figs/ripl_pointbutton1_pairwise.png}
    % \vspace{-1em}
    \caption{\textit{PointButton1}: We plot all the baselines and their SRPL counterparts}
    \label{fig:pointbutton-pairwise}
\end{figure}


% \subsection{Implementation Details}
% \label{app:implementation_details}


% To reduce the search space of hyperparameters for RIPL agents, we first find the optimal hyperparameters for the baseline algorithms and then iterate over RIPL hyperparameters which are learning rate (risk-lr) and batch-size (risk-batch-size) for risk model training. In practice, we split the risk distributions into bins, representing distances, typically we use a bin size of $4$ and total bins $= 20$.

% \begin{tabular}{l*{6}{c}r}
% % \textbf{CarGoal1 (CMDP) (Fig. 5)}\\
% \hline
% Hyperparameter              & CPO & CSC & WCPG & RI-CPO & RI-CSC & RI-WCPG  &\\
% \hline
% Network architecture          &[64,64]& [64,64]& [64,64]& [64,64]& [64,64]& [64,64]& \\
% learning-rate                & 0.00005 & 0.00005 & 0.00005 &  0.00005 & 0.00005 &  0.00005 &  \\
% batch-size                   & 128 & 128 & 128 & 128 & 128 & 128 &  \\
% target-kl                     & 0.01 & 0.01 & 0.01 & 0.01  & 0.01 & 0.01 &\\
% risk-batch-size               & -    & - & - & 5000  & 5000 & 10000   &\\
% risk-lr                       & -    & - & - & 1e-6 & 1e-6 & 1e-5 &\\
% % risk-levels                   & -    & - & - & 20   &    &\\
% csc-threshold ($\chi$)        & -    & 0.05 & -    & - & 0.05 & - \\
% cvar-wcpg                     & - & - & 0.8 & - & - & 0.8 \\
% gamma                         &0.99  & 0.99 & 0.99 & 0.99 & 0.99 & 0.99\\
% Damping Coefficient           & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
% Backtrack Coefficient        &  0.8 & 0.8 & 0.8 & 0.8 & 0.8 & 0.8\\
% Backtrack iterations         & 15   & 15 & 15 & 15 & 15 & 15 \\
% Activation                   & ReLU & ReLU & ReLU & ReLU & ReLU & ReLU\\
% \hline
% \end{tabular}







\end{document}
