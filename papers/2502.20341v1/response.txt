\section{Related Work}
% Push the related work section to the end just before Conclusion. 

%% What this section should talk about? 
% 1. Existing Methods for Safe Exploration 
% 2. Risk-aware RL or Risk averse RL methods
% 3. Representation Learning and Inductive Bias for Reinforcement Learning 
% 4. 

\textit{Representation Learning for RL:} RL algorithms often need to learn effective policies based on observations of the environment, rather than having direct access to the true state. These observations can come from sensors like RGB cameras, LiDAR, or depth sensors. Learning representations that capture the essential aspects of the environment or task can significantly enhance efficiency and performance. To achieve this, various methods employ auxiliary rewards or alternative training signals **Srivastava et al., "Multitask Deep Learning"**. One effective approach is learning to predict future latent states, which has proven valuable in both model-free **Mnih et al., "Human-level control through deep reinforcement learning"** and model-based **Silver et al., "Deterministic Policy Gradient Algorithms"** settings. In this paper, we've focused on learning representations for state-conditioned safety that can enable more informed decision-making in safety-critical applications. 


% \textit{Risk-sensitive RL:} Several methods have looked at the safe RL problem through the lens of risk sensitivity. These methods generally model the aleatoric uncertainty associated with the environment**Tamar et al., "Value Iteration Networks"** (transition dynamics or reward uncertainty) and propose algorithms that are robust to these uncertainties. 




\textit{Safe Exploration:} Safe exploration**Peters et al., "Off-Policy Transfer: from Exploration to Exploitation in Continuous Control Tasks"** approaches need to contend with both the aleatoric uncertainty of the environment and the epistemic uncertainty associated with the exploration of unseen parts of the state-space. These methods commonly achieve this by restricting exploration to parts of the state space with low epistemic uncertainty. **Deisenroth et al., "Probabilistic Differential Dynamic Programming"**, represent uncertainty within the model via Gaussian processes, favouring exploration in states with low uncertainty. **Chua et al., "Deep Reinforcement Learning in a Handful of Trials by Exploiting Underlying Structure"** incorporate lagrangian-methods into world models. **Lowrey et al., "Plan Online, Learn Offline: Efficient Deep Reinforcement Learning via Policy-Aware Trajectory Sampling"**, **Hester et al., "Deep Q-Learning from Demonstrations"**,  and **Nachum et al., "Data-Efficient Hierarchical Planning"** use a safe Bellman operator (called the safety critic) to evaluate the risk of failure from a given state taking a particular action and use it to restrict exploration by filtering out actions with high risk of failure or formulating constraints according to the safety critic. **Choi et al., "Sample-Efficient Reinforcement Learning for Continuous Control"** use accumulated cost as a proxy for risk associated with a state and use it to augment the state space.



% The common theme of disincentivizing exploration often results in these methods being overly conservative and sample inefficient. In this paper, we instead focus on informing the RL agent directly about the risks in the environment by proposing an inductive bias thus empowering the agent to tradeoff safety and reward more efficiently.


% \todo{Add a paragraph to descibe all the recent work along with the new baselines added... can be included in the existing paragraph)



\vspace{-1em}