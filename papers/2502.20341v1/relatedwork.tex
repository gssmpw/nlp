\section{Related Work}
% Push the related work section to the end just before Conclusion. 

%% What this section should talk about? 
% 1. Existing Methods for Safe Exploration 
% 2. Risk-aware RL or Risk averse RL methods
% 3. Representation Learning and Inductive Bias for Reinforcement Learning 
% 4. 

\textit{Representation Learning for RL:} RL algorithms often need to learn effective policies based on observations of the environment, rather than having direct access to the true state. These observations can come from sensors like RGB cameras, LiDAR, or depth sensors. Learning representations that capture the essential aspects of the environment or task can significantly enhance efficiency and performance. To achieve this, various methods employ auxiliary rewards or alternative training signals \citep{sutton2011horde, jaderberg2016reinforcement,riedmiller2018learning, lin2019adaptive}. One effective approach is learning to predict future latent states, which has proven valuable in both model-free \citep{munk2016learning,schwarzer2020data,ota2020can} and model-based \citep{watter2015embed,ha2018world} settings. In this paper, we've focused on learning representations for state-conditioned safety that can enable more informed decision-making in safety-critical applications. 


% \textit{Risk-sensitive RL:} Several methods have looked at the safe RL problem through the lens of risk sensitivity. These methods generally model the aleatoric uncertainty associated with the environment~\citep{dist_rl, morimura2010nonparametric, tamar2015policy} (transition dynamics or reward uncertainty) and propose algorithms that are robust to these uncertainties. 




\textit{Safe Exploration:} Safe exploration~\citep{cpo, cvpo, sauteRL, pid_lag, sauteadj, gu2024review} approaches need to contend with both the aleatoric uncertainty of the environment and the epistemic uncertainty associated with the exploration of unseen parts of the state-space. These methods commonly achieve this by restricting exploration to parts of the state space with low epistemic uncertainty. Bayesian model-based methods \citep{thesis_berkenkamp}, represent uncertainty within the model via Gaussian processes, favouring exploration in states with low uncertainty. ~\citep{huang2023safe} incorporate lagrangian-methods into world models. \citet{cscadj}, \citet{learning_to_be_safe}, \citet{ldm} and \citet{csc} use a safe Bellman operator (called the safety critic) to evaluate the risk of failure from a given state taking a particular action and use it to restrict exploration by filtering out actions with high risk of failure or formulating constraints according to the safety critic. \citep{sauteRL, sauteadj} use accumulated cost as a proxy for risk associated with a state and use it to augment the state space.



% The common theme of disincentivizing exploration often results in these methods being overly conservative and sample inefficient. In this paper, we instead focus on informing the RL agent directly about the risks in the environment by proposing an inductive bias thus empowering the agent to tradeoff safety and reward more efficiently.


% \todo{Add a paragraph to descibe all the recent work along with the new baselines added... can be included in the existing paragraph}



\vspace{-1em}