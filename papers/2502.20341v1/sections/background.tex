\section{Preliminaries}

RIPL may be applied to both constrained and unconstrained MDPs. Here, we review the notation and central concepts for these decision-making models.

\subsection{Markov Decision Processes} 

Markov decision processes (MDPs) are defined as a tuple $\langle S, A, T, R, d, \gamma \rangle$, where $S$ is a set of states; $A$ is a set of actions; $R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function indicating the immediate reward for executing action $a$ in state $s$ and resulting in state $s'$; $T: S \times A \times S \rightarrow [0, 1]$ is the forward dynamics model indicating the probability of achieving state $s'$ after executing action $a$ in state $s$; $d: S \rightarrow [0, 1]$ represents the probability of starting in a state $s \in S$; and $\gamma \in [0, 1)$ is a discount factor.

A solution to an MDP is a policy $\pi: S \rightarrow A$ that prescribes an action $\pi(s)$ for each state $s \in S$. Policies may be stochastic, written as $\pi(a|s)$, indicating the probability of choosing action $a$ when in state $s$. The optimal solution to an MDP is an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward, $J(\pi)$:
\begin{equation}
    J(\pi) = \mathbb{E}_{s_0 \sim d(s)} \Big [ \mathbb{E}_{\tau_{s_0} ^{\pi}} \Big [ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1)} \Big ] \Big ].
\end{equation}

Here, $\tau_{s_0} ^{\pi}$ is a sequence of states generated by executing policy $\pi$ starting from state $s_0$. Executing a policy $\pi$ induces a value function $V^{\pi}: S \rightarrow \mathbb{R}$ indicating the expected cumulative discounted reward that an agent would experience if it executed policy $\pi$ beginning in state $s$. The optimal policy $\pi^*$ induces the maximum value function for each state $V^*$.

\subsection{Constrained Markov Decision Processes} 

Constrained MDPs (CMDPs) are defined as a tuple $\langle S, A, T, R, d, \gamma, \mathcal{C}, \beta \rangle$, where $S$, $A$, $T$, $R$, $d$, and $\gamma$ retain their MDP definitions. $\mathcal{C}: S \rightarrow \mathbb{R}^+$ is a cost function, mapping each state to a real-valued, non-negative cost. $\beta$ is budget, or a maximum, cumulative cost that is acceptable in expectation. The optimal solution to a CMDP is a policy $\pi^*$ that, again, maximizes the expected cumulative discounted reward, but also accumulates expected discounted cost less than   $\beta$:
\begin{equation}
    \begin{array}{cc}
        J(\pi) =&\hspace{-2mm}\mathbb{E}_{s_0 \sim d(s)} \Big [ \mathbb{E}_{\tau_{s_0} ^{\pi}} \Big [ \displaystyle \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1)} \Big ] \Big ]. \\
        \text{s.t.} & \mathbb{E}_{s_0 \sim d(s)} \Big [ \mathbb{E}_{\tau_{s_0} ^{\pi}} \Big [ \displaystyle \sum_{t=0}^{\infty} \gamma^t \mathcal{C}(s_t) \Big ] \Big ] \leq \beta. \\
    \end{array}
\end{equation}

Intuitively, we can view the constraints imposed by $\mathcal{C}$ and $\beta$ as reducing the set of possible policies from all policies $\Pi$ to those satisfying the constraints $\Pi_{\mathcal{C}} \subseteq \Pi$. While $\pi^*$ is deterministic for MDPs, it is generally stochastic in CMDPs as this allows agents to fully exploit $\beta$ in expectation. \todo{Samer can find a reference for this?... also move to footnote}

In deep reinforcement learning (DRL), due to approximation errors, it is typically impossible to guarantee that the agent will never execute a policy $\pi \notin \Pi_{\mathcal{C}}$ during training. Therefore, in the absence of any prior information, the agent will likely violate the constraints during exploration. Given this, we often care about both the cumulative cost incurred during training as well as the expected cost of the final policy.

\subsection{``Safety'', ``Risk'', and Important Assumptions}

The reinforcement learning community has adopted several terms to describe the behaviour of agents exhibiting some sense of self-preservation or some concept of the broader consequences of their actions, ``risk-aware'' and ``safe'' among the most prominent\footnote{This is separate from notions of ``ethical'' or ``fair'' decision making, though the consensus is still evolving~\citep{nashed2023fairness}.}. Even among RL researchers these terms often index subtly different meanings or reasoning mechanisms that may differ from their use in other areas of sequential decision-making and may depart entirely from their non-technical denotation. In this paper we do not attempt to unify, classify, or intentionally reify any particular terms adopted from colloquial speech, though the general concept of ``risk'' comes closest to the intuition used to design RIPL. In the remainder of the paper, we will collectively refer to states we wish to avoid (terminal states in the unconstrained case, and cost-inducing states in the constrained case) as ``unsafe'' states. A state of course may have high risk in that it is near an unsafe state, but may not be unsafe itself.

Despite the state of terminology, RL researchers have remained fairly consistent in the assumptions they make about their operating environments. We follow two of the most common in the literature~\citep{learning_to_be_safe,ldm,csc}: (1) \emph{Identifiability}: Unsafe states are identifiable as such, either via episode termination or immediate accumulation of cost or reward. (2) \emph{Binary Safety}: The categorization of a state as safe or unsafe is binary; varying degrees of safety are not considered and we use strictly $\mathcal{C}: \mathcal{S} \rightarrow \{0, 1\}$.

Though they do exclude some decision-making models, these assumptions allow a large, informal class of MDPs. For example, any CMDP for which there is some subset of states $S_{unsafe} \subset S$ which induce a non-zero, constant cost. Or, any MDP with absorbing states or sink states $S_{unsafe} \subset S$ that, when the agent enters, terminate the episode prematurely and may or may not produce negative reward. Although most of the example domains in safety gym \citep{safety-gym} and other popular RL environments represent safety as a physical property, the concept of unsafe states, and thus the RIPL framework, may be applied to any MDP following the above assumptions, whether or not the label of ``unsafe'' is accurate in a colloquial sense. \todo{add robotics examples (non-ergodic state, reset state, etc.)}
