\section{Related Work}


Reinforcement learning (RL) agents must efficiently explore the state space to discover optimal policies for specific tasks. Common exploration strategies include stochastic methods like the $\epsilon$-greedy approach, where a random action is chosen with a probability $\epsilon$, and entropy regularization techniques, such as MaxEnt RL, which encourages exploration by maximizing the policyâ€™s entropy~\citep{hao2023exploration,neu2017unified}. Additionally, curiosity-driven approaches provide supplementary rewards for encountering novel situations or reducing model uncertainty~\citep{curiosity_org,rnd,doctor2022learning}. While these methods enhance exploration, they do not explicitly address the challenge of safe exploration, which is essential for deploying RL in real-world applications.

Reinforcement learning (RL) algorithms often need to develop effective policies based on observations of the environment, rather than having direct access to the true state. These observations can come from sensors like RGB cameras, LiDAR, or depth sensors. Learning representations that capture the essential aspects of the environment or task can significantly enhance the performance of RL algorithms. To achieve this, various methods employ auxiliary rewards or alternative training signals \citep{sutton2011horde, jaderberg2016reinforcement,riedmiller2018learning, lin2019adaptive}. One effective approach is learning to predict future states, which has proven valuable in both model-free \citep{munk2016learning,schwarzer2020data,ota2020can} and model-based \citep{watter2015embed,ha2018world} settings in latent space. Additionally, some approaches have explored joint representations for state-action pairs \citep{fujimoto2024sale,liu2021return}.

In this paper, we focus on learning representations that capture safety-related aspects of the environment based on the agent's prior experience. We argue that in most environments, safety or constraints are inherent to the environment or state, rather than the task itself. This means that safety-related information can be learned in a task-agnostic manner, enabling these representations to generalize across different tasks.

The concept of risk has been an object of interest in Markov decision processes since the early 1970s~\citep{howard1972risk}, and specifically in reinforcement learning since the early 2000s (see~\citep{garcia2015comprehensive} for a nice overview). While there are many competing definitions for ``safety'' and ``risk'' within the RL literature~\cite {garcia2015comprehensive}, many find common ground in their focus on the exploration process within RL. 


% There has been considerable research on many sub-problems in RL, including both sample efficiency and safety. Methods for efficient exploration generally fall into one of three categories. Curiosity-driven approaches \cite{curiosity_org, rnd} use a curiosity term in their loss function to encourage exploring novel states; Bayesian \cite{bootDQN, rpf} approaches estimate the posterior distribution of the value function and encourage exploring states with high uncertainty; and distributional RL\cite{dist_rl} approaches reformulate the Bellman update over distributions, allowing algorithms to implicitly model uncertainties in the learning process. While these methods do increase sample efficiency, they make no improvements with respect to safety, and in many cases may be less safe.

% \kaustubh{  
% P3: Safe exploration methods commonly formulate the Rl problem as a constrained MDP. Describe different design choices made by different methods. 
% P4: Our paper is most similar to Conservative Safety Critics, SQRL and Saute RL. What do they do and what do we do differently and why it makes sense. The idea of RIPL can also be compared to methods in Meta-RL that use a context variable to condition policy learning. We are also aiming to learning something that generalizes across tasks (centric to the environment). }


Methods for safe exploration are often formulated as constrained Markov decision processes \citep{cpo,cvpo}. Alternatively, Bayesian model-based methods \citep{thesis_berkenkamp}, represent uncertainty within the model via Gaussian processes, favoring exploration in states with low uncertainty. Other safe exploration methods try to avoid unsafe states by explicitly modeling risk using a safety critic. These so-called filtering or shielding approaches \citep{learning_to_be_safe,ldm,csc} prevent actions during exploration that the critic deems high risk and attempt to guide the agent to remain within the bounds of its experienced data distribution. Due to their cautious approach, these methods are often inefficient and can fail to ensure complete state-space coverage --- a requirement for deriving an optimal policy.

In this paper we relax some of the inherent tradeoffs between sample efficiency and safety by delegating this problem to the agent itself via state-augmentation, allowing the network to figure out how best to use risk information. State-augmentation most often includes either \emph{extra information} or \emph{re-processed information}. Extra information may be added to give the agent access to privileged information as in goal-conditioned RL \citep{liu2022goal}, to retain the Markov property, for example by including action histories when solving MDPs with stochastic delays \citep{nath2021revisiting} or Langrangian weights when solving certain CMDPs as the Lagrangian update is itself Markovian \citep{calvo2023state}. It can also be used in combination with changes to the reward function, such as to reduce the risk of vanilla Q-learning when the objective is sensitive to higher order moments in the distribution of returns \citep{ma2019state}, or in SauteRL \citep{sauteRL}, where states are augmented with the remaining constraint budget and the reward function is modified to reflect safety considerations.

Here, we avoid augmenting our state with extra information as this may not always be available, and instead use re-processing. This strategy has proven effective in meta-RL \cite{bhatia2023rl}, where Q-value augmentation leads to both faster adaptation and better generalization. In the context of safe reinforcement learning, the intrinsic fear approach \cite{intrinsic_fear} trains a model to predict the probability that a given state could reach an unsafe state within $k$ actions. They then modify the agent's loss function by a Lagrange term based on the model output. While this work bounds the sub-optimality of the resulting policy, it assumes optimal policies rarely visit near-failure states, which may in practice be unrealistic (e.g. most robots are often just a few commands from damaging themselves). RIPL is distinct from prior safe exploration methods in several important ways: 1) We do not perturb the original objective; 2) We do not completely prevent exploration of any state; 3) We do not require any domain knowledge; 4) We model risk as a distribution rather than a scalar.


meta-RL relation: risk model is a type of context variable. 

