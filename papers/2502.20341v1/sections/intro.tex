\section{Introduction}


% \begin{itemize}
%     \item In nature, organisms have a model of the riskiness of states learned through play, self play, social, innate, whatever. This helps them learn new envs, operate safely, learn more efficiently, etc.
%     \item We devise a machinery to endow RL agents with such a risk model. Indeed, we <description of the machinery>
%     \item We then review the impact of this additional system on commonly-studied aspects of RL artificial life. For instance: safety during training, safety after training, efficiency of training, transferrability to new tasks.
% \end{itemize}

% \begin{itemize}
%     \item SN todo:
%     \item P6: Summary of results... possibly combine with above.
% \end{itemize}



%details about the training and such can be moved to the later section. I think what Liam said about Implicit bias and generalization those two points can be emphasised

Reinforcement Learning (RL) environments often incorporate the notion of failure, where certain events are deemed undesirable during both training and deployment. To manage these failures, RL frameworks 
employ negative rewards or enforce constraints that penalize the agent for unsafe actions. The primary challenge lies in training an agent that can navigate an uncertain and potentially hazardous environment while balancing the risk of failure against the reward for task completion. During the exploration phase, the agent must contend with the inherent epistemic uncertainty of the environment, making it particularly difficult to avoid failures that are not yet fully understood. This challenge is further compounded in scenarios where safety and task objectives are misaligned, such as in the Safety Gym environment or autonomous driving, where optimizing for speed may compromise safety measures like avoiding collisions.

In environments where safety and task objectives are more closely aligned, such as robotic manipulation tasks, the learning process presents a different set of challenges. Here, the agent may quickly learn that certain actions are risky, leading to overly conservative behaviours that inhibit further exploration. This premature convergence to a risk-averse policy not only results in sample inefficiency but also hinders the agent's ability to learn more complex, state-conditioned representations that are crucial for achieving the task. Consequently, the agent’s exploration is curtailed, resulting in a learning process that is both inefficient and unsafe. This issue underscores the broader difficulty of deploying RL algorithms in real-world settings, where the need for both efficient learning and adherence to safety constraints is paramount.





We propose to ease the learning of state-conditioned risk by introducing an implicit bias. An additional low-dimensional state risk representation is added to the agent's observations. This representation allows for faster learning of state-conditioned risk in the critic and policy, allowing the agent to reach higher performance faster and avoid unsafe states during training. This representation is learned explicitly using the experience gathered during training as a quantile distribution over distance to failure.  We show that this leads to significantly higher performance in both cases of safety/task relationships. This uses the insight that risk is state-conditioned. This leads to the ability to transfer across tasks in the same environment.   


%%% Sept 2nd 2024
%Reinforcement learning (RL) has shown promise as a paradigm for solving sequential decision-making problems in a variety of domains, including games~\citep{shao2019survey}, recommender systems~\citep{afsar2022reinforcement}, infrastructure control~\citep{arwa2020reinforcement,chen2022reinforcement,degrave2022magnetic}, and robotics~\citep{zhang2021reinforcement,singh2022reinforcement}. Nonetheless, there are several key challenges that prevent more widespread adoption of RL-based systems, of which we highlight three. (1) \emph{Sample efficiency:} RL algorithms require more data to train than is feasible in many scenarios, especially in robotics. (2) \emph{Transferability / Generalization:} It is often difficult to extract task-agnostic information from a policy trained to complete a particular task in order to learn a new task in the same environment. (3) \emph{Safety during exploration:} While the process of exploration is inherently risky and there is some amount of risk that is irreducible, state-of-the-art ``safe'' RL algorithms still make more mistakes than many applications can afford\footnote{In many applications, the risks of taking the wrong action are relatively benign: a user is served an inaccurate ad or a movie recommendation they have no interest in, or the agent dies in a video game and must start the level over again. However, action selection for agents operating in the physical world often presents significant risk: robotic systems, such as autonomous vehicles, can cause severe injury to themselves or their human counterparts, power-grid management systems can disrupt peoples' access to reliable electricity, and healthcare agents could dramatically change patients' conditions. Although such systems often have access to simulators in which they may conduct a large amount of low-risk training, the so-called sim-to-real gap will likely remain a challenge for many applications~\citep{salvato2021crossing}.}. 

%One aspect of learning hypothesized to be important is disentangling task- and environment-specific information~\citep {mao2018universal,kim2019curiosity,zhou2019environment}. This is built largely on the intuition that, in many environments, states possess action-independent properties that are correlated with their induced value (or cost) under many policies. For example, driving in the opposite lane on a two-lane road is more risk-prone regardless of the policy the agent follows. In fact, the concept of state-dependent risk may guide human skill acquisition, in part through an awareness that certain behaviours or elements in our environment pose inherent risks \citep{simpson1996neither}, and is thought to have broad utility in learning complex, dangerous tasks \citep{keating2008adolescent}. Regardless of the task at hand, we instinctively avoid certain states with the \emph{potential} for harm, enabling us to navigate novel scenarios judiciously when learning \citep{crawford2002learning}.


% Motivated by these insights from human learning, we introduce a novel approach to represent the underlying structure of hazards in the environment via the acquisition of an independent \emph{risk model} which is simultaneously (or subsequently) used to condition the RL agent.



%Motivated by these insights from human learning, we propose a novel approach to learning a representation that models the risk associated with a state, introducing an implicit bias into the policy learning process. By embedding risk information directly into the state representation, we enable risk-aware learning, allowing the agent to make more informed and cautious decisions in high-risk situations. This approach is particularly advantageous in high-dimensional state spaces, where effective risk modelling becomes increasingly critical. Assuming that the cost function remains consistent across tasks—meaning the inherent characteristics of objects or states do not change—we propose a policy-agnostic risk modelling strategy. This approach allows the development of task-invariant risk features that can generalize effectively across different environments.



Concretely, we propose Risk-Informed Policy Learning (RIPL), a framework where a state-centric risk model is used to guide policy learning. RIPL augments the RL agent’s state representation by integrating risk information from the risk model alongside the state description. This framework is formalized as a \emph{Risk-informed MDP}, where the original state is enhanced with risk metrics provided by the risk model. Additionally, we extend this framework to incorporate constraints, thereby adapting it for use with Constrained Markov Decision Processes (CMDPs).  The risk model is trained to estimate the probability that the agent will encounter an "unsafe" state after exactly \( k \) actions, where \( k \in \{ 1, \ldots, H \} \).

RIPL distinguishes itself from existing methods in two key ways. First, traditional approaches for safe or risk-aware exploration often rely on external safety critics—such as filtering or shielding mechanisms—to evaluate and conditionally execute actions. These methods can lead to overly conservative policies that may hinder learning. In contrast, RIPL integrates risk information directly into the policy learning process, allowing for more effective management of high-risk situations. Second, unlike previous risk-aware RL methods that model risk as a scalar value~\citep{intrinsic_fear,csc,sauteRL}, RIPL represents risk as a probability distribution over relative risk values derived from policy rollouts. This probabilistic approach captures more detailed risk information, enabling more nuanced risk-informed decision-making and more accurately approximating important values such as expected returns and constraint violations.






% Concretely, we propose Risk-Informed Policy Learning (RIPL), a framework in which a state-centric risk model is used to inform policy learning. In practice, RIPL is a form of state augmentation in which we condition the RL agent on the output of a risk model in addition to the state description. This notion is formalized as a \emph{Risk-informed MDP} that, given a risk function, augments the original state to include the risk information provided by the risk model. The risk model itself is trained to predict, for a given state, the probability that the agent will encounter an ``unsafe'' state after exactly $k$ actions where $k \in \{ 1, \ldots, H \}$. Though simple, the RIPL framework is distinct from past approaches in two important ways. First, existing approaches for safe or risk-aware exploration largely attempt to reduce the hazards of exploration using safety critics~\citep{learning_to_be_safe,ldm,csc}, where, through mechanisms like filtering or shielding that are external to the policy, actions selected by the policy during exploration are only conditionally executed by the agent pending their evaluation by the safety critic. However, by explicitly restricting the policy from exploring certain states during learning, these approaches often converge to overly conservative solutions, many times preventing learning entirely. Second, unlike previous risk-aware RL methods, RIPL represents risk as a probability distribution over relative risk values learned from data generated by policy rollouts. Modelling risk as a distribution rather than a scalar (e.g. \citep{intrinsic_fear,csc,sauteRL}) captures more nuanced information with which important values such as expected return or expected constraint violations may be bounded or approximated. This representation also naturally allows risk-informed decision-making, rather than simply risk-averse.

% \todo{incorporate below into previous paragraph... near mention of formalizing the risk model}
% Due to its simplicity, RIPL is also broadly applicable. It may be used in both constrained and unconstrained MDPs, uses only the data already available to, or generated by, the RL agent, and requires no additional assumptions or privileged data beyond what is typical in the safe-RL literature. Moreover, RIPL may be used with any underlying RL algorithm allowing essentially all common RL algorithms to be converted to their risk-informed variants.





%Further, training risk-informed agents is very flexible as the risk model may be trained in several ways, including jointly from scratch alongside the policy, pre-trained on a given task and transferred to agents learning new tasks in similar environments, or continually updated over the agent's lifetime. 
%In particular, the risk model may be learned using rollouts from any set of policies. 


Empirically, we investigate two primary hypotheses addressing the key challenges outlined:

\begin{itemize}
    \item Integrating risk information directly into the state representation enhances both the safety and efficiency of reinforcement learning (RL) agents during the learning process.
    \item Since the risk information is learned in a state-centric or policy-agnostic manner, it can be generalized across different tasks, leading to safer and more efficient learning when applied to new tasks.
\end{itemize}


% describe breadth / depth of experiments (domains, etc.)

% highlight outcomes w.r.t. H1-H3

We empirically evaluate risk-informed agents across four distinct simulated robotic environments, encompassing tasks such as manipulation, navigation, and locomotion. Our results demonstrate that risk-informed agents can develop policies that are up to twice as safe during training and up to twice as sample-efficient compared to baseline methods. Additionally, we show that risk information is transferable across different tasks, serving as an effective prior for learning policies in new tasks. We further investigate the risk-reward tradeoff exhibited by risk-informed agents relative to the baselines. We also conduct additional analyses and ablations to validate key design decisions.

% list ablation studies (what knobs did we try turning?)



