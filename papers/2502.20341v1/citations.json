[
  {
    "index": 0,
    "papers": [
      {
        "key": "sutton2011horde",
        "author": "Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina",
        "title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction"
      },
      {
        "key": "jaderberg2016reinforcement",
        "author": "Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray",
        "title": "Reinforcement learning with unsupervised auxiliary tasks"
      },
      {
        "key": "riedmiller2018learning",
        "author": "Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias",
        "title": "Learning by playing solving sparse reward tasks from scratch"
      },
      {
        "key": "lin2019adaptive",
        "author": "Lin, Xingyu and Baweja, Harjatin and Kantor, George and Held, David",
        "title": "Adaptive auxiliary task weighting for reinforcement learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "munk2016learning",
        "author": "Munk, Jelle and Kober, Jens and Babu{\\v{s}}ka, Robert",
        "title": "Learning state representation for deep actor-critic control"
      },
      {
        "key": "schwarzer2020data",
        "author": "Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R Devon and Courville, Aaron and Bachman, Philip",
        "title": "Data-efficient reinforcement learning with self-predictive representations"
      },
      {
        "key": "ota2020can",
        "author": "Ota, Kei and Oiki, Tomoaki and Jha, Devesh and Mariyama, Toshisada and Nikovski, Daniel",
        "title": "Can increasing input dimensionality improve deep reinforcement learning?"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "watter2015embed",
        "author": "Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin",
        "title": "Embed to control: A locally linear latent dynamics model for control from raw images"
      },
      {
        "key": "ha2018world",
        "author": "Ha, David and Schmidhuber, J{\\\"u}rgen",
        "title": "World models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dist_rl",
        "author": "Bellemare, Marc G and Dabney, Will and Munos, R{\\'e}mi",
        "title": "A distributional perspective on reinforcement learning"
      },
      {
        "key": "morimura2010nonparametric",
        "author": "Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki",
        "title": "Nonparametric return distribution approximation for reinforcement learning"
      },
      {
        "key": "tamar2015policy",
        "author": "Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie",
        "title": "Policy gradient for coherent risk measures"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "cpo",
        "author": "Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter",
        "title": "Constrained policy optimization"
      },
      {
        "key": "cvpo",
        "author": "Liu, Zuxin and Cen, Zhepeng and Isenbaev, Vladislav and Liu, Wei and Wu, Steven and Li, Bo and Zhao, Ding",
        "title": "Constrained variational policy optimization for safe reinforcement learning"
      },
      {
        "key": "sauteRL",
        "author": "Sootla, Aivar and Cowen-Rivers, Alexander I and Jafferjee, Taher and Wang, Ziyan and Mguni, David H and Wang, Jun and Ammar, Haitham",
        "title": "{Saut{\\'e} RL: A}lmost surely safe reinforcement learning using state augmentation"
      },
      {
        "key": "pid_lag",
        "author": "Stooke, Adam and Achiam, Joshua and Abbeel, Pieter",
        "title": "Responsive safety in reinforcement learning by pid lagrangian methods"
      },
      {
        "key": "sauteadj",
        "author": "Jiang, Hao and Mai, Tien and Varakantham, Pradeep and Hoang, Minh Huy",
        "title": "Solving Richly Constrained Reinforcement Learning through State Augmentation and Reward Penalties"
      },
      {
        "key": "gu2024review",
        "author": "Gu, Shangding and Yang, Long and Du, Yali and Chen, Guang and Walter, Florian and Wang, Jun and Knoll, Alois",
        "title": "A Review of Safe Reinforcement Learning: Methods, Theories and Applications"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "thesis_berkenkamp",
        "author": "Berkenkamp, Felix",
        "title": "Safe exploration in reinforcement learning: {T}heory and applications in robotics"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "huang2023safe",
        "author": "Huang, Weidong and Ji, Jiaming and Zhang, Borong and Xia, Chunhe and Yang, Yaodong",
        "title": "Safe dreamerv3: Safe reinforcement learning with world models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "cscadj",
        "author": "Fisac, Jaime F and Lugovoy, Neil F and Rubies-Royo, Vicen{\\c{c}} and Ghosh, Shromona and Tomlin, Claire J",
        "title": "Bridging hamilton-jacobi safety analysis and reinforcement learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "learning_to_be_safe",
        "author": "Srinivasan, Krishnan and Eysenbach, Benjamin and Ha, Sehoon and Tan, Jie and Finn, Chelsea",
        "title": "Learning to be safe: {D}eep {RL} with a safety critic"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ldm",
        "author": "Kang, Katie and Gradu, Paula and Choi, Jason J and Janner, Michael and Tomlin, Claire and Levine, Sergey",
        "title": "Lyapunov density models: {C}onstraining distribution shift in learning-based control"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "csc",
        "author": "Bharadhwaj, Homanga and Kumar, Aviral and Rhinehart, Nicholas and Levine, Sergey and Shkurti, Florian and Garg, Animesh",
        "title": "Conservative safety critics for exploration"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "sauteRL",
        "author": "Sootla, Aivar and Cowen-Rivers, Alexander I and Jafferjee, Taher and Wang, Ziyan and Mguni, David H and Wang, Jun and Ammar, Haitham",
        "title": "{Saut{\\'e} RL: A}lmost surely safe reinforcement learning using state augmentation"
      },
      {
        "key": "sauteadj",
        "author": "Jiang, Hao and Mai, Tien and Varakantham, Pradeep and Hoang, Minh Huy",
        "title": "Solving Richly Constrained Reinforcement Learning through State Augmentation and Reward Penalties"
      }
    ]
  }
]