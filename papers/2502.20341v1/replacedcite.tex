\section{Related Work}
% Push the related work section to the end just before Conclusion. 

%% What this section should talk about? 
% 1. Existing Methods for Safe Exploration 
% 2. Risk-aware RL or Risk averse RL methods
% 3. Representation Learning and Inductive Bias for Reinforcement Learning 
% 4. 

\textit{Representation Learning for RL:} RL algorithms often need to learn effective policies based on observations of the environment, rather than having direct access to the true state. These observations can come from sensors like RGB cameras, LiDAR, or depth sensors. Learning representations that capture the essential aspects of the environment or task can significantly enhance efficiency and performance. To achieve this, various methods employ auxiliary rewards or alternative training signals ____. One effective approach is learning to predict future latent states, which has proven valuable in both model-free ____ and model-based ____ settings. In this paper, we've focused on learning representations for state-conditioned safety that can enable more informed decision-making in safety-critical applications. 


% \textit{Risk-sensitive RL:} Several methods have looked at the safe RL problem through the lens of risk sensitivity. These methods generally model the aleatoric uncertainty associated with the environment____ (transition dynamics or reward uncertainty) and propose algorithms that are robust to these uncertainties. 




\textit{Safe Exploration:} Safe exploration____ approaches need to contend with both the aleatoric uncertainty of the environment and the epistemic uncertainty associated with the exploration of unseen parts of the state-space. These methods commonly achieve this by restricting exploration to parts of the state space with low epistemic uncertainty. Bayesian model-based methods ____, represent uncertainty within the model via Gaussian processes, favouring exploration in states with low uncertainty. ____ incorporate lagrangian-methods into world models. ____, ____, ____ and ____ use a safe Bellman operator (called the safety critic) to evaluate the risk of failure from a given state taking a particular action and use it to restrict exploration by filtering out actions with high risk of failure or formulating constraints according to the safety critic. ____ use accumulated cost as a proxy for risk associated with a state and use it to augment the state space.



% The common theme of disincentivizing exploration often results in these methods being overly conservative and sample inefficient. In this paper, we instead focus on informing the RL agent directly about the risks in the environment by proposing an inductive bias thus empowering the agent to tradeoff safety and reward more efficiently.


% \todo{Add a paragraph to descibe all the recent work along with the new baselines added... can be included in the existing paragraph}



\vspace{-1em}