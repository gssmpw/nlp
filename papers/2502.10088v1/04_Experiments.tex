\section{User Study}\label{Sec:UserStudy}

To evaluate the impact of conversational virtual agent and different mixed reality visualizations on patient acceptance and comfort during an autonomous robotic ultrasound procedure, we conducted a $1\times4$ within-subject user study. \add{The study was approved by the Ethics Committee of Technical University of Munich under protocol number 2022-87-S-KK.}


\subsection{Hypotheses}

Based on the previous findings of virtual agent and effects of level of immersion, we propose the following three hypotheses:
\begin{itemize}
    \item[\textbf{\textit{H1.}}]\replaced{The presence of a virtual agent will reduce discomfort and increase acceptance. We hypothesize that, compared to the baseline robotic ultrasound procedure (without any visualization), the appearance of a virtual human assistant guiding the procedure will reduce patient discomfort by providing a more humanized and reassuring experience.}{The presence of a virtual agent will improve patient comfort and acceptance. Compared to the baseline robotic ultrasound procedure, incorporating a virtual human assistant during the procedure will enhance the overall patient experience.}
    \item[\textbf{\textit{H2.}}] \replaced{Reduced visibility of the robot will reduce stress and increase trust. We hypothesize that when the robot is not visible to the patient, such as in the \add{AV and }VR environments, the procedure will be perceived as less intimidating compared to conditions where the robot is fully visible.}{The visibility of the robot will influence patient trust and stress levels. The extent to which the robot is visible during the procedure will affect how intimidating the procedure is perceived by the patient, impacting their trust and stress levels.}
	\item[\textbf{\textit{H3.}}] \replaced{The level of immersion will influence patient workload and usability. We hypothesize that AR, \replaced{VR with passthrough}{AV}, and fully immersive VR will have varying effects on usability and mental workload. Specifically, AR will demonstrate better usability due to maintaining better situational awareness, while fully immersive VR may induce higher cognitive.}{The level of immersion will affect patient usability and workload. Different levels of immersion across AR, AV, and VR environments will have varying impacts on usability and mental workload during the procedure.}
\end{itemize}

\subsection{Experimental Variables and Measures}

The study was designed with one independent variable: the mode of visualization applied during the ultrasound procedure. Participants experienced four different conditions. The baseline condition, Robotic Ultrasound (\textbf{RUS}), involved no visualizations. In contrast, the three proposed conditions included immersive visualization modalities: AR Virtual Agent Guidance (\textbf{AR-VG}), \revised{AV} Virtual Agent Guidance (\textbf{\revised{AV}-VG}), and Fully Immersive VR Virtual Agent Guidance (\textbf{FV-VG}).

Several dependent variables were measured to evaluate the effects of these visualization modes. Stress levels were objectively recorded using an ECG sensor, providing physiological data on participant stress before and during the procedure under each condition. In addition, participants’ mental workload was assessed using the NASA Task Load Index (NASA-TLX)~\cite{hart2006nasa}. The user-friendliness and usability of the visualizations was measured using the System Usability Scale (SUS)~\cite{brooke1996sus}. Finally, participants’ trust in the robotic system was evaluated using the HRI Trust Score, a scale originally developed by Schaefer et al.~\cite{schaefer2016measuring} and later modified by Eilers et al.~\cite{eilers2023importance}, to measure the level of trust participants placed in the robotic system during each condition.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/flow3.png}
    \caption{\textbf{User Setup (top) and User Study Flow Chart (bottom)}. The setup includes the patient interacting with the conversational virtual agent and the robotic ultrasound system, using different immersive visualizations provided by the HMD. The flow chart outlines the user study procedure: the orange (left column) indicates the visualization module, the purple (middle column) represents the user, and the green (right column) depicts the robotic ultrasound module. Dashed lines represent the interactions between these three main components.}
    \label{fig:flow}
\end{figure}


\subsection{Setup}

The proposed visualizations are developed using the Unity game engine (Unity Technologies, San Francisco, US), version 2022.3.18f1. Communication between the Unity-based visualizations and the robotic ultrasound system, controlled via the Robot Operating System (ROS), is facilitated through the ROS TCP Endpoint v0.7.0~\cite{unity2022rostcp}. The robotic system is powered by a KUKA LBR IIWA R800 robotic arm (KUKA AG, Augsburg, Germany), which is controlled via IIWA Stack~\cite{hennersperger2017towards} running on ROS Noetic. The ultrasound system used is the Siemens ACUSON Juniper (Siemens Healthineers, Erlangen, Germany), with the transducer mounted to the robotic manipulator using a custom-printed holder. A frame grabber was used to capture the images from the ultrasound machine and stream them directly to the Unity application. Meta Quest 3 (Meta, Menlo Park, US) with firmware version 67.0 is chosen for deployment of the Unity solution for its versatility in supporting varying levels of immersion, which is critical for testing the effectiveness of each visualization approach under the same hardware conditions. Additionally, we utilized the Depth API~\cite{depthapi} to provide more believable visual rendering that handles occlusion between virtual and physical components. 

The virtual human avatar used in the proposed visualizations is created using Ready Player Me~\cite{readyplayerme}. For this study, we designed the avatar as a female figure wearing a doctor’s coat to enhance its professional and comforting presence\add{, as previous research has shown that anthropomorphic features, such as human-like appearance and professional attire, can foster trust and positive attitudes toward avatar-assisted therapy~\cite{moriuchi2023looking}}. The avatar’s animations consist of both baked and procedural elements. Idle animations, such as sitting and breathing, eye blinking, and mouth movement during speech, are baked into the avatar’s model. In contrast, procedural animations—like the avatar reaching to grab the ultrasound probe when it enters its proximity, or turning to face the patient during interaction—are dynamically generated using the inverse kinematics solver Final IK 2.3~\cite{finalik}.
Speech-to-text is implemented via the whisper.unity 1.3.2~\cite{whisperunity}, which runs locally on the system. We selected the Whisper tiny model (OpenAI, San Francisco, US) for its balance between performance and speed, ensuring rapid transcription during patient interaction. The transcribed text is processed using the LLM for Unity 2.2.0~\cite{llmunity}. We opted for the Phi-3 3B model (Microsoft, Redmond, US), which is also run locally, achieving a similar balance between speed and performance to ensure responsiveness during real-time interactions. Once a response is generated by the LLM module, it is fed into the TTS module, implemented using the Meta Quest Voice SDK 67 with Wit.AI for speech synthesis, providing natural and consistent vocal output from the avatar.

To create a virtual environment identical to the real-world setting, we utilized an iPhone 13 Pro equipped with the Luma AI app~\cite{lumaai} to perform a detailed scan of the room. This process generated a 3D Gaussian Splatting model of the environment, capturing the room’s geometry and textures with high accuracy. The resulting model was then imported into Unity, where it was rendered using the Gaussian Splatting VR Viewer~\cite{gaussianvr}. Finally, the visualization system runs on a PC equipped with an NVIDIA RTX 2060 Super GPU, an Intel i7-10700F CPU, and 16GB RAM. The Meta Quest 3 was connected to this PC via Quest Link using a USB-C cable.


\subsection{Participants}

A total of 14 participants (5 female, 9 male) took part in this study, with ages ranging from 23 to 64 years (M = 31.6, SD = 9.9). Participants were required to have no conditions that could hinder the use of AR and VR, although the use of corrective glasses or contact lenses was allowed. \add{Regarding prior experience with AR and VR, 50$\%$ of participants rated themselves below 3 on a 5-point scale, indicating limited experience, while the other 50$\%$ rated themselves 3 or higher.} In terms of familiarity with robotic procedures, 28.6$\%$ of participants reported being not at all familiar to somewhat familiar, 28.6$\%$ were moderately familiar, and 42.8$\%$ were very familiar to expert level.
Participation in the study was entirely voluntary, and participants were free to withdraw at any time. The study adhered to the ethical principles outlined in the Declaration of Helsinki. To ensure confidentiality, all data collected during the study was fully anonymized.

\subsection{Procedure}

The study procedure, as depicted in Fig.~\ref{fig:flow}, began with a registration process prior to the arrival of participants. Using the Meta Quest 3 controller, we placed virtual spheres at predefined points on both the real robot and the room for registration. 
Upon arrival, participants were informed about the study procedure and provided with a consent form outlining the purpose of the study, their role, their rights, and assurances of data confidentiality. Once informed consent was obtained, participants filled out a demographic form. Afterward, they were fitted with a biosignalsplux 3-lead ECG sensor (PLUX Biosignals, Lisbon, Portugal) following a tutorial. The ECG sensor was connected via Bluetooth to a MacBook Pro running OpenSignals software to record physiological data.
Participants were seated in a chair and instructed to place their right arm on an ultrasound exam table. An experienced operator applied ultrasound gel to the participant's arm, and they were asked to keep it still while the robot, equipped with the ultrasound probe, was manually positioned by the operator to record the start and end points of the scanning path. The robot arm was then lifted away from the participants to prepare for the procedure.

Once preparations were complete, ECG recording began, and the visualizations were activated. The virtual avatar greeted the participants, prompting them to engage in conversation. Participants were free to speak with the avatar, raise concerns, or ask questions. They could also instruct the avatar to begin the procedure at their discretion. During the ultrasound procedure, which was performed with the robot arm set to apply the force of 8 N in the Z direction with stiffness of 500 N/m, participants were encouraged to continue interacting with the avatar if desired.
Upon completion, the virtual assistant informed the participant that the procedure was finished, and they could relax and move their arm. The visualization was then deactivated, and the ECG sensor recording was stopped. The presentation order of the different visualization methods—baseline (\textbf{RUS}), \textbf{AR-VG}, \textbf{\revised{AV}-VG}, and \textbf{FV-VG}—was randomized for each participant to avoid bias.
After each task, participants completed the SUS, NASA-TLX, and the HRI Trust Score questionnaire. Upon finishing all tasks for both the baseline and the proposed visualizations, participants were asked to rank the visualizations based on their preferences and provide feedback. In total, each user study session lasted between 30 to 40 minutes.

\subsection{\add{Statistical Analysis}}

\add{To analyze the data collected during the study, we employed a range of statistical tests to evaluate differences across the visualization conditions. 
For stress levels derived from ECG-based heart rate variability (HRV), non-parametric tests were used: Wilcoxon Signed-Rank test for within-condition comparisons (e.g., resting vs. execution phases), and the Kruskal-Wallis test for between-condition comparisons. These methods were selected due to the non-normal distribution of the data as confirmed by the Shapiro-Wilk test.
For subjective measures, a Friedman test was conducted to determine if there were significant differences among the four visualization methods. Post-hoc Dunn-Sid{\'a}k pairwise comparisons were performed to identify specific group differences when the Friedman test indicated significance. Additionally, effect sizes were calculated for pairwise comparisons using Cohen’s $d$ and $\eta^2$ for the Kruskal-Wallis test to quantify the magnitude of observed differences.
All statistical analyses were performed using Python and appropriate libraries, with significance levels set at $\alpha = 0.05$.}
