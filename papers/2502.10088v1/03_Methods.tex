\section{Methods}\label{sec:Methods}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/Pipeline2.png}
    \caption{\textbf{Overview of the Proposed Pipeline.} The pipeline adds a visualization module, including augmented reality and virtual reality modalities, enhanced by a virtual human assistant. The system architecture facilitates real-time interaction and synchronization between the robotic ultrasound system, the virtual agent, and the patient, providing a more engaging and patient-centered experience.}
    \label{fig:overview}
\end{figure*}


In this work, we propose a pipeline, as illustrated in Fig.~\ref{fig:overview}, that introduces a visualization module into the existing robotic ultrasound system (RUS), enhancing the patient experience through conversational virtual agent and immersive visualizations. 
In the following subsections, we detail the components and theoretical frameworks behind the conversational virtual agent, the different visualization modalities (AR, \revised{AV}, and fully immersive VR), as well as the registration process that ensures the seamless integration of virtual and real-world elements.

\subsection{Force-Compliant Robotic Ultrasound System}

Our robotic ultrasound system employs a force-compliant control approach based on impedance control~\cite{jiang2020automatic,jiang2021autonomous,jiang2021deformation}, which ensures safe and accurate probe positioning during the ultrasound procedure by regulating both contact force and probe orientation. This control system operates as a spring-like mechanism with predefined stiffness values, ensuring that if an obstacle, such as the patient’s body, is encountered, the robot will either bypass with a reduced force or stop at the location to avoid applying excessive force.

Impedance control modulates the interaction forces between the robotic arm and the patient’s tissue by adjusting the robot’s stiffness, damping, and inertia parameters. The control law is defined as:
\begin{equation}
\tau = J^T \left[ F_d + K_m e + D \dot{e} + M \ddot{e} \right]
\end{equation}
where $\tau$ represents the computed joint torques, $J^T$ is the transposed Jacobian matrix, and $e$ is the pose error between the current and target positions. The stiffness, damping, and inertia matrices are denoted by $K_m$, $D$, and $M$, respectively. The desired force $F_d$ controls the applied contact force. To ensure stable force along the probe’s centerline, the robot uses a 1-DOF compliant controller for force control and a 5-DOF position controller for precise positioning.

This force-compliant control \revised{follows the same formulation used in Hennersperger et al.\cite{hennersperger2017towards} and Jiang et al.\cite{jiang2021autonomous}, which} allows the system to adapt to soft tissue deformations and variations in probe position, preventing excessive force that could cause discomfort or harm. \add{To enable communication between the RUS and the visualization module, messages are serialized and passed between the two. This ensures that the visualization module is always aware of the robot’s state and the current phase of the procedure. Additionally, commands produced through user interactions with the conversational virtual agent in the visualization module can be sent back to the RUS via serialized messages. This bidirectional communication enables real-time synchronization and seamless integration of robotic control and immersive visualizations.}


\subsection{Conversational Virtual Agent}

The conversational virtual agent serves as a central component of the proposed visualizations, aiming to enhance patient interaction by natural communication and reduce the sense of isolation commonly associated with robotic procedures~\cite{almeras2019operating}, therefore creating a more humanized and engaging experience.

At the core of the agent’s functionality is a speech-to-text (STT) system that transcribes the patient’s spoken words into textual input. The transcribed text is then processed by a large language model (LLM), which generates contextually appropriate responses based on pre-configured prompt. The use of an LLM ensures that the agent provides responses that are both relevant to the patient’s situation and emotionally supportive, fostering a sense of trust and ease during the procedure.
Once the response is generated, it is converted back into audible speech through a text-to-speech (TTS) engine, which provides natural output. The TTS system matches the assistant’s appearance and personality, ensuring consistent and realistic verbal interactions. \add{Similar technical pipelines of the conversational intelligent agent have been used in domains such as virtual museums~\cite{garcia2024speaking} and educational VR applications~\cite{yang2024effects}.}

In addition to the agent’s verbal responses, various animations are employed to enhance its realism and human-like presence~\cite{yu2021avatars}. These include eye blinking, mouth movements synchronized with speech generated by the TTS system, and a subtle idle breathing animation to give the avatar a more natural appearance. Beyond these baseline animations, inverse kinematics (IK) is used to further enhance realism by controlling the assistant’s head and hand movement. The IK algorithm calculates natural, human-like gestures in response to the patient’s position and the state of the robot. \revised{Studies have shown that non-verbal cues such as gestures, head movements, and subtle body language significantly enhance user immersion and realism in virtual environments~\cite{etienne2023perception, beck2012emotional}. Adapting these findings to our robotic ultrasound use case, we implemented a set of non-verbal behaviors to increase the agent’s presence and engagement.} For example, the assistant’s head turns toward the patient when they speak, and its hand animates to hold the ultrasound probe when the robot moves it within the agent’s arm reach, as though the virtual assistant were guiding the procedure. This visual and behavioral consistency strengthens the impression of the virtual agent being physically present and actively engaged with the patient.

Together, these elements form the foundation of the conversational virtual agent framework. The integration of real-time speech, intelligent language processing, and naturalistic physical interaction ensures that the agent serves as a comforting presence, enhancing the overall patient experience.

\subsection{Augmented Reality Visualization}

The AR visualization, illustrated in Fig.~\ref{fig:teaser}a, is designed to blend virtual elements with the patient’s real-world environment, allowing for seamless integration of the guidance from virtual human agent within a familiar, physical space. This approach maintains situational awareness by allowing the patient to see both their surroundings and the robotic arm at work while interacting with the assistant. 
In this visualization, the avatar appears seated next to the patient, engaging with them through real-time conversation. Through the HMD, the patient can see both the virtual assistant and the ultrasound probe, which the avatar is holding and guiding over the patient’s body. As the robotic arm moves the ultrasound probe in the physical world, the avatar’s virtual hand mimics these motions synchronously. This visual configuration provides the patient with the comforting illusion that the human-like avatar is controlling the robotic procedure, reducing the sense of detachment associated with the automated process.


\subsection{\revised{Augmented Virtuality} Visualization}

The \replaced{VR Passthrough}{AV} visualization, as demonstrated in Fig.~\ref{fig:teaser}b, combines an immersive virtual environment with selective real-world visibility through a passthrough window. This allows the patient to remain visually engaged with the area of their body being scanned while still interacting with the virtual human assistant in the virtual world. The virtual environment mirrors the physical room, but the robotic ultrasound system itself is not visible; only the ultrasound probe becomes visible when it enters the passthrough window, allowing the patient to see it during the scan.
Within the virtual environment, the patient can also see a virtual representation of the ultrasound probe, which mirrors the position of the real probe. As the robotic arm drives the real ultrasound probe toward the patient, the virtual probe moves correspondingly, providing a visual cue that the probe is approaching. When the probe transitions into the passthrough window, the virtual probe seamlessly aligns with and transitions into the real probe, ensuring continuity and reassuring the patient that the procedure is progressing as expected.

In this visualization, the virtual human assistant interacts with the patient in the same way as in the AR setting. The assistant is positioned to give the impression of guiding the robotic ultrasound. The patient’s view of the scan area remains in the passthrough window.
This visualization preserves the patient’s sense of presence and control by allowing them to observe their body during the scan. At the same time, the virtual environment and virtual human assistant help to reduce anxiety with guidance and interaction throughout the procedure.

\subsection{Fully Immersive Virtual Reality Visualization}

The fully immersive VR visualization, as shown in Fig.~\ref{fig:teaser}c, places the patient entirely in a virtual environment, removing any direct visual connection to the real world. In this setting, the virtual environment mirrors the physical room, but the robotic ultrasound system is absent as well, reinforcing the illusion that the virtual assistant is in full control of the procedure. The patient perceives the virtual agent as the sole operator, guiding the ultrasound probe, which enhances the feeling of human involvement and control.

In this visualization, the patient sees a virtual replica of their own body. As the robot moves the ultrasound probe on the patient’s actual body, the virtual probe in the VR environment moves in sync on the virtual body. This synchronization ensures that the patient feels a consistent tactile and visual connection between what they see in the virtual space and what they feel in the real world.
This immersive environment is designed to reduce anxiety by eliminating the often sterile, impersonal atmosphere of traditional robotic procedures, replacing it with a comforting virtual experience where the patient feels human involvement and control throughout. By maintaining real-time synchronization with their physical body and providing a strong sense of presence through the virtual human assistant, the visualization offers a fully immersive alternative that fosters comfort and trust in the procedure.

\subsection{Registration of Virtual and Real Components}

\begin{figure}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/registration.png}
    \caption{\add{\textbf{Registration of Virtual and Real Robot Using Predefined Points.} Predefined points on the virtual robot are shown in green. We marked the corresponding points on the real robot with the HMD, shown in orange. The dashed lines represent the transformation matrix $W_T$ to be solved, which aligns the two point sets.}}
    \label{fig:registration}
\end{figure}


Accurate registration between the virtual and real-world components is crucial for maintaining spatial coherence across all visualizations. To achieve this, we utilize a set of predefined, ordered points on both the virtual and physical elements to perform 3D-to-3D registration, similar to the approaches used in previous works~\cite{song2022happy,yu2022duplicated}. For instance, several key points $P_{virtual}(i)$ are selected on the virtual robot, which correspond to matching points marked on the real robotic system $P_{real}(i)$. Similarly, for the virtual environment, corner points of the physical room are used to align the virtual representation of the room with the real one. The transformation matrix  $W_T$  between these point sets is computed to minimize the difference between their positions. This can be expressed mathematically as:

\begin{equation}
	W_T = \arg\min_{\widehat{W}_T} d\left(\widehat{W}_T P_{virtual}(i), P_{real}(i)\right)
\end{equation}
where $W_T$ is the estimated transformation matrix between the real and virtual spaces, and $d$ represents the distance between corresponding points in the two spaces. \add{The registration process is visually illustrated in Fig.~\ref{fig:registration}.} We further decompose this transformation into a rotation component $W_R$ and a translation component $W_t$, solving each sequentially using the Kabsch algorithm~\cite{kabsch1993automatic}.

After the initial alignment is achieved, we can leverage the spatial anchor feature offered by modern HMDs to maintain this alignment between sessions. This means the calibration procedure only needs to be performed once, unless fine-tuning or refinements are required in future sessions. By using spatial anchors, the system ensures persistent alignment, enhancing the immersive experience for the patient without the need for repeated calibrations.
