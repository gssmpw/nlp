\section{Results}\label{Sec:Results}

The system’s performance was evaluated across key metrics, including latency, frame rate, and resolution. Latency was measured for the key components of the virtual human assistant interaction: STT exhibited a latency of $46 \pm 5$ ms, the LLM processing took $552 \pm 187$ ms, and the TTS synthesis had a latency of $1281 \pm 188$ ms.
The visual output was rendered at a resolution of $4128 \times 2208$ on the HMD, with frame rate recorded to assess the visual fluidity of each visualization modality. The \textbf{AR-VG} visualization maintained a consistent average frame rate of 72 FPS. Both \textbf{\revised{AV}-VG} and \textbf{FV-VG} operated at an average frame rate of 36 FPS.


\subsection{Stress Level}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/hrv.png}
    \caption{\textbf{HRV during the Resting and Execution Phases.} In \textbf{RUS}, the RMSSD shows the steepest drop between the two phases, indicating a higher stress level compared to the proposed visualizations. \textbf{AR-VG} and \textbf{\revised{AV}-VG} perform similarly, while \textbf{FV-VG} exhibits highest RMSSD value and the smallest change between phases, suggesting that less stress is induced during the execution.}
    \label{fig:hrv}
\end{figure}

To assess stress levels during the robotic ultrasound procedure, we derived HRV from the ECG sensor data, focusing on the Root Mean Square of the Successive Differences (RMSSD), a commonly used measure of stress~\cite{shaffer2017overview}. Lower RMSSD values generally indicate higher stress levels. The analysis was performed using the HeartPy~\cite{van2019heartpy} Python package.
We analyzed HRV during two phases of the procedure: the resting phase, where the robot remained stationary and participants were free to interact with the virtual agent, and the execution phase, during which the robot performed the ultrasound scan. The HRV data for these phases are shown in Fig.~\ref{fig:hrv}.

Given the non-normal distribution of the data observed by the Shapiro-Wilk test, we used the Wilcoxon Signed-Rank Test for within-condition comparisons, assessing differences between the resting and execution phases for each visualization method. Although we observed a trend of lower RMSSD values during the execution phase compared to the resting phase, in \textbf{RUS} ($z = 25.0, p = 0.846\add{, d = 0.291}$), \textbf{AR-VG} ($z = 13.0, p = 0.547\add{, d = 0.141}$), \textbf{\revised{AV}-VG} ($z = 38.0, p = 0.970\add{, d = 0.180}$), and \textbf{FV-VG} ($z = 28.0, p = 0.700\add{, d = 0.032}$), the results did not indicate significance.
To compare HRV across the different visualization methods during both the resting and execution phases, we employed the Kruskal-Wallis Test. The analysis for the resting phase showed no significant difference in HRV across the visualization methods ($H = 0.485, p = 0.922\add{, \eta^2 = 0.012}$). During the execution phase, the test also yielded no significant difference between methods ($H = 3.430, p = 0.330\add{, \eta^2 = 0.086}$).



\subsection{Subjective Ratings}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hri.png}
        \caption{Trust in Human Robot Interaction}
        \label{fig:hri}
    \end{subfigure}
    \hfill % optional; add some horizontal spacing
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sus.png}
        \caption{System Usability Score}
        \label{fig:sus}
    \end{subfigure}
    \hfill % optional; add some horizontal spacing
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/tlx.png}
        \caption{Perceived Workload}
        \label{fig:tlx}
    \end{subfigure}
    \caption{\textbf{Subjective Measurements for Trust Score, Usability, and Workload.} All proposed immersive visualizations with the conversational agent significantly increase the HRI trust score compared to \textbf{RUS}. \textbf{AR-VG} receives the highest trust score, the best usability, and the lowest workload among all methods. Statistical significance is indicated as $\star \left( p<0.05 \right)$, $\star \star \left( p<0.01 \right)$, and $\star \star \star \left( p<0.001 \right)$.}
    \label{fig:subjective}
\end{figure*}

HRI Trust scores under each condition for the robotic ultrasound were as follows: \textbf{RUS} ($M = 3.12, SD = 0.62$), \textbf{AR-VG} ($M = 4.33, SD = 0.42$), \textbf{\revised{AV}-VG} ($M = 4.29, SD = 0.38$), and \textbf{FV-VG} ($M = 4.06, SD = 0.68$). The results are visualized in Fig.~\ref{fig:hri}.
Statistical analysis using the Friedman test revealed a significant difference in trust scores across the visualization methods ($\chi^2(3) = 26.95, p = 6.02 \times 10^{-6}$). Post-hoc Dunn-Sid{\'a}k  pairwise comparisons further emphasized these differences. Significant differences were observed between \textbf{RUS} and \textbf{AR-VG} ($p = 0.000316\add{, d = 2.272}$), \textbf{RUS} and \textbf{\revised{AV}-VG} ($p = 0.00035\add{, d = 2.272}$), and \textbf{RUS} and \textbf{FV-VG} ($p = 0.012\add{, d = 1.428}$).

The SUS scores for each condition\revised{, normalized to a 0-1 scale,} are shown in Fig.~\ref{fig:sus}. A Friedman test revealed a significant difference in usability across the visualization methods ($\chi^2(3) = 16.60, p = 0.000854$). Post-hoc Dunn-Sid{\'a}k  pairwise comparisons indicated that the significant difference lies between \textbf{RUS} and \textbf{AR-VG} ($p = 0.037\add{, d = 1.343}$).

The NASA-TLX scores, \add{normalized to a 0-1 range}, are presented in Fig.~\ref{fig:tlx}. A Friedman test revealed a significant difference in task load across the visualization methods($\chi^2(3) = 9.03, p = 0.02$).
Although there was a tendency for both \textbf{AR-VG} and \textbf{\revised{AV}-VG} to show lower task load scores compared to \textbf{RUS}, Dunn-Sid{\'a}k  pairwise comparisons did not reveal any statistically significant differences between the visualization methods.


\subsection{User Preference and Feedback}

%\begin{figure}
%    \centering
%    \includegraphics[width=\columnwidth]{figures/rank.png}
%    \caption{\textbf{Preference Ranking.} \textbf{AR-VG} was the most preferred, followed by \textbf{\revised{AV}-VG}, \textbf{FV-VG}, and \textbf{RUS}.}
%    \label{fig:rank}
%\end{figure}


The results showed that \textbf{AR-VG} was the most preferred visualization, with 72$\%$ of participants ranking it as their top choice, 14$\%$ ranking it second, and 14$\%$ ranking it third. \textbf{\revised{AV}-VG} followed, with 21$\%$ of participants ranking it as the most preferred, 43$\%$ ranking it second, and 36$\%$ ranking it third. For \textbf{FV-VG}, 36$\%$ of participants ranked it in their top three choices. Finally, no participants ranking \textbf{RUS} as their first choice. However, 22$\%$ ranked it second, 42$\%$ ranked it third, and 36$\%$ ranked it as their least preferred visualization.

The qualitative feedback from participants provided further insight into their preferences. Participants in general appreciated the conversational abilities of the virtual assistant across \textbf{AR-VG}, \textbf{\revised{AV}-VG} and \textbf{FV-VG}. They noted that talking to the avatar felt natural and gave them more control over the procedure. In addition, several participants remarked that the hand animation of the virtual assistant taking control of the probe “made me trust the system more.” However, due to technical limitation, the avatar’s hand was not visible in the \textbf{\revised{AV}-VG} passthrough window, which led to some confusion about the interaction.
Concerns about the accuracy of VR visualizations were also raised. Participants noted that due to tracking error, sometimes misalignment between their real and virtual arms in \textbf{FV-VG} caused uncertainty about the success of the scan. Participants raised concerns about the robot’s actions, particularly when they could not see the real robot.
%, leading to uncertainty about the procedure.

Overall, the feedback indicated that participants favored the visualizations that offered a balance between immersion and real-world visibility and integrating a friendly, responsive avatar can improve patient trust and comfort in robotic ultrasound procedures.
