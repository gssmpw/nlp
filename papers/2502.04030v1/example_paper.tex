%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{verbatim}
\usepackage{rotating}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
%\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\title{Fine, I'll Merge It Myself: A Multi-Fidelity Framework \\for Automated Model Merging}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{A Multi-Fidelity Framework for Automated Model Merging}



\begin{document}

\twocolumn[
% \icmltitle{Automating Merging with HPO}
% \icmltitle{A framework for model merging using multi-fidelty HPO}
% \icmltitle{Efficient Improvements due to automated Model Merges [...]}
% \icmltitle{Model Merging should be automated: A framework for efficient HPO with multi...}
% \icmltitle{Finding Difficult Merges automatically}
% \icmltitle{[Funny meme]: What is actually happening}
% \icmltitle{Many Merges: }
% \icmltitle{[Funny meme]: What is actually happening}

% \icmltitle{Fine, I'll Merge It Myself: An Automated, Multi-Fidelity Framework for Fine-Grained Model Merging}


% \icmltitle{Fine, I'll Merge It Myself: An Automated, Multi-Fidelity Framework \\for Model Merging}


%\icmltitle{Fine, I'll Merge It Myself: A Multi-Fidelity Framework \\for Automated Model Merging}
\date{}
\maketitle
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.
\hypersetup{
    pdftitle={Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated Model Merging},
    }
% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}
%\author{\centering
%Guinan Su$^{2}$ \and Jonas Geiping$^{1,2,3}$ \\[4mm]
%\normalsize $^1$Tübingen AI Center,\ \ $^2$Max Planck Institute for Intelligent Systems,\ \ $^3$ELLIS Institute Tübingen\\[4mm]
%}
\vspace{-1cm}
\author{\centering
\large Guinan Su$^{1}$  \and \qquad Jonas Geiping$^{1,2,3}$ \\[4mm]
\normalsize $^1$Max Planck Institute for Intelligent Systems,\ \ $^2$ELLIS Institute Tübingen,\ \ $^3$Tübingen AI Center\\[4mm]
}
%\date{}


%\begin{icmlauthorlist}
%\icmlauthor{Guinan Su}{yyy}
%\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
%\icmlauthor{Jonas Geiping}{comp}
%\icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\end{icmlauthorlist}

%\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Tübingen AI Center, Max Planck Institute for Intelligent Systems, ELLIS Institute Tübingen}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

%\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reasoning capabilities represent a critical frontier for large language models (LLMs), but developing them requires extensive proprietary datasets and computational resources. One way to efficiently supplement capabilities with is by model merging, which offers a promising alternative by combining multiple models without retraining.  
However, current merging approaches rely on manually-designed strategies for merging hyperparameters, limiting the exploration of potential model combinations and requiring significant human effort. We propose an Automated Model Merging Framework that enables fine-grained exploration of merging strategies while reducing costs through multi-fidelity approximations. We support both single and multi-objective optimization and introduce two novel search spaces: layer-wise fusion (LFS) and depth-wise integration (DIS). 
% Evaluations across five benchmarks demonstrate our auto-merge framework's effectiveness in efficient model integration by leveraging complementary strengths of individual models. 
Evaluating across a number of benchmarks, we find that the search autonomously finds 1) Merges that further boost single-objective performance, even on tasks the model has already been finetuned on, and 2) Merges that optimize multi-objective frontiers across tasks. Effective merges are found with limited compute, e.g. within less than 500 search steps. The code is available at.\footnote{\url{https://github.com/Guinan-Su/auto-merge-llm}}  %\url{https://github.com/Guinan-Su/auto-merge-llm}
% This study presents a systematic framework integrating multiple search spaces and optimization objectives, advancing cost-efficient model merging for enhanced reasoning in language models.





% Reasoning capabilities, particularly in mathematics, coding, and general logical inference, represent a critical yet challenging frontier for large language models (LLMs). While these models show promise, their development is constrained by the need for extensive proprietary datasets and substantial computational resources, which are increasingly becoming a bottleneck. In this study, we propose an automated merging framework that combines multiple expert models into a unified LLM, aiming to enhance reasoning capabilities while maintaining computational efficiency.  Our approach supports both single-objective and multi-objective optimization and introduces two novel search spaces: a fine-grained layer-wise fusion search space and a depth-wise integration search space. We evaluated the effectiveness of our method on five benchmarks. Extensive experimental results demonstrate that our proposed auto-merge framework achieves highly efficient model integration, leveraging the potential complementary strengths of individual models. Overall, this study aims at investigting a systematic framework that integrates multiple search spaces and optimization objectives, providing insights into the potential of model merging to enhance reasoning capabilities in language models.
\end{abstract}





\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=1.0 \linewidth]{fig_version_0/fig1.pdf}
    \vspace{-.2cm}
    \caption{Overview of our Proposed Merging Framework: Search space: layer-wise fusion (LFS) and depth-wise integration (DIS), search strategy optimization with single/multi-objective approaches, and multi-fidelity performance estimation}
    \vspace{-.1cm}
    \label{fig:fig0}
\end{figure}

% Recent years have witnessed unprecedented advancements in large-scale pre-trained models, driven by breakthroughs in algorithmic development and computational capabilities. Models such as GPT-4 \citep{achiam2023gpt}, LLAMA \citep{touvron2023llama}, DALL·E \citep{ramesh2021zero}, and Imagen \citep{saharia2022photorealistic} have demonstrated remarkable capabilities across diverse domains. In the context of Large Language Models (LLMs), reasoning capabilities—encompassing logical inference, complex relationship comprehension, and multi-step problem solving—have become increasingly crucial.
% While specialized LLMs have emerged excelling in specific reasoning tasks, such as instruction following \citep{xu2023wizardlm,jiang2023mistral,bi2024deepseek}, code generation \citep{luo2023wizardcoder,guo2024deepseek,zhu2024deepseek}, and mathematical problem-solving \citep{luo2023wizardmath}, developing models with enhanced comprehensive reasoning capabilities remains challenging. A naive approach would be to aggregate training datasets from these specialized models to develop a more comprehensive model. However, this strategy faces significant practical constraints, including computational limitations and substantial human resource requirements. Moreover, the proprietary nature and sensitivity of training data often restrict access, limiting the viability of data-centric approaches.

% These challenges have spurred interest in model-centric methods that can efficiently enhance large models by leveraging existing pre-trained models, focusing on capability improvement without requiring retraining or access to original training data. Model merging has emerged as a promising direction. Beginning with foundational approaches like weight averaging \citep{utans1996weight}, the field has evolved to include more sophisticated techniques such as Linear Mode Connectivity (LMC) \citep{garipov2018loss}, which enables merging models trained from common initializations. Recent advances, including Task Arithmetic \citep{ilharco2022editing}, have extended these principles through weight averaging and task-specific vector operations. Furthermore, cutting-edge approaches such as TIES \citep{yadav2024ties} and Drop And REscale (DARE) \citep{yu2024language} address the challenges of permutation symmetries, enabling the alignment of models with diverse initializations.

% However, the specific challenges of model merging for enhancing reasoning capabilities in LLMs remain inadequately explored. In this paper, we propose a cost-efficient automated framework for enhancing reasoning through model merging, as illustrated in Figure \ref{fig:fig0}. Our approach leverages Multi-Fidelity Optimization (MFO) \citep{peherstorfer2018survey} to optimize the model merging process, efficiently utilizing evaluations across different fidelity levels to achieve effective search processes. We introduce two comprehensive search spaces operating at both depth-wise and layer-wise levels, enabling fine-grained optimization of model merging strategies.

% Our key contributions include:
% \begin{itemize}
% \item We propose an automated model merging framework specifically designed to enhance both single and multi-objective reasoning capabilities, optimized for resource-constrained scenarios, which eliminates manual design.
% \item To enhance search efficiency, a multi-level fidelity searching strategy is leveraged to enable efficient exploration of the configuration space while maintaining robust evaluation quality.
% \item We further explore novel search spaces through layer-wise and depth-wise integration, enabling flexible layer fusion and pathway optimization.
% \item We show that, even with limited budgets of only 500 model evaluations on subsets of a target data set, we can find improved model merges in our newly designed search spaces.
% \item We find that we can effectively find multi-objective merges, merging different capabilities [+X average across the multiple objectives]. Yet, we also show that we can even improve on single-objective merges, where model already finetuned on, e.g. GSM8k can further improve in performance [+4\%....



Recent advancements in large-scale pre-trained models like GPT-4 \citep{achiam2023gpt}, LLaMA \citep{touvron2023llama}, DALL·E \citep{ramesh2021zero}, and Imagen \citep{saharia2022photorealistic} have demonstrated remarkable capabilities across diverse domains. Within Large Language Models (LLMs), specialized models have emerged excelling in specific tasks such as instruction following \citep{xu2023wizardlm,jiang2023mistral,bi2024deepseek}, code generation \citep{luo2023wizardcoder,guo2024deepseek,zhu2024deepseek}, and mathematical problem-solving \citep{luo2023wizardmath}.
However, developing models with comprehensive reasoning capabilities remains challenging. A straightforward solution would be to combine training data from specialized domains for retraining or finetuning. However, this data-centric approach faces practical limitations: it requires substantial resources, and many training datasets remain proprietary or restricted. Researchers have turned to model-centric approaches that enhance capabilities by leveraging existing pre-trained models without requiring training or data access.

Model merging has emerged as a promising solution in this space. Beginning with simple weight averaging for models sharing initialization \citep{utans1996weight}, more advanced parameter-based techniques were subsequently developed. Parameter-based methods like Task Arithmetic \citep{ilharco2022editing} and SLERP \citep{white2016sampling} advanced the field through parameter difference computation and spherical interpolation. Recent sparsity-based approaches like TIES-Merging \citep{yadav2024ties} and DARE \citep{yu2024language} leverage neural network over-parametrization, using magnitude-based selection and rescaling to further improve merging effectiveness.
However, these model merging methods rely on manually-tuned hyperparameters, applied uniformly across the entire model. This approach relies heavily on manual hyperparameter tuning, and its coarse granularity makes finding optimal solutions challenging. 
In this paper, we propose a \textbf{cost-efficient automated framework} for enhancing reasoning through model merging. Our approach leverages Multi-Fidelity Optimization (MFO) \citep{peherstorfer2018survey} to optimize the merging process through low-fidelity approximations, reducing computational cost. We support both single and multi-objective optimization and introduce layer-wise and depth-wise search spaces for finer-grained merging. Our key contributions include:
\begin{itemize}
\item We propose an automated model merging framework that enhances both single and multi-objective reasoning capabilities while reducing computational costs through multi-level fidelity optimization.
\item We introduce two novel search spaces: Layer-wise Fusion Space (LFS) for fine-grained layer merging and Depth-wise Integration Space (DIS) for optimizing inference pathways, enabling comprehensive model integration strategies.
\item We demonstrated efficient optimization across different scenarios: in mathematical reasoning using LFS, only 17\% of trials required the full search budget within 500 trials; in general reasoning using DIS, only 18.6\% of trials required the full budget within 1000 trials.

% \item We demonstrate efficient optimization by achieving improved model merges using less than 20\% of the full computational budget within 500-1000 trials through our newly designed search spaces.
\item We achieve significant performance gains, including a 6.86\% average improvement in multi-objective scenarios and a 4.24\% improvement on the challenging GSM8K task, with consistent effectiveness across various reasoning benchmarks.
\end{itemize}
% \vspace{-.15cm}

% \begin{itemize}
% \item An automated model merging framework designed to enhance single and multi-objective reasoning, reducing manual effort and further minimizing search costs through multi-level fidelity optimization.

% \item Novel search spaces include both  layer-wise and depth-wise integration, enabling flexible and fine-grained searching

% \item  We show that, even with limited budgets of only 500 model evaluations on subsets of a target data set, we can find improved model merges in our newly designed search spaces.


% \item We find that multi-objective merging can effectively combine different capabilities, yielding an average improvement of 6.86\% across multiple objectives. Moreover, we show that even single-objective merges can be further enhanced. For instance, a model already fine-tuned on GSM8k can achieve an additional 4\% performance improvement.

% \end{itemize}

% We propose a cost-efficient automated model merging framework specifically designed to enhance both single and multi-objective reasoning capabilities, optimized for resource-constrained scenarios, which eliminates manual design.

% To enhance search efficiency, a multi-level fidelity searching strategy is leveraged to enable efficient exploration of the configuration space while maintaining robust evaluation quality.

% We further explore novel search spaces through layer-wise and depth-wise integration, enabling flexible layer fusion and pathway optimization.


% We also investigate novel search spaces (i.e., layer-wise and depth-wise integration) for structural optimization, enabling searching for precise and flexible layer fusion and pathway.


\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{fig_version_0/fig3new1.pdf}
    \caption{Illustrations of two merging search spaces: (a) Layer-wise Fusion Search (LFS), which merges layers across layer groups with different methods and hyper-parameters, and (b) Depth-wise Integration Search (DIS), which optimizes inference paths through block-wise searching with permutation and layer selection from different source models.}
    \label{fig:fig1}
\end{figure*}

\section{Related Work}

\subsection{Model Merging}
Model merging has emerged as an efficient approach to enhance model capabilities without additional training data or extensive computation. The field has evolved from simple weighted parameter averaging to increasingly sophisticated methods. Early methods employed weighted parameter averaging  \citep{utans1996weight} for models fine-tuned from a shared base model. Despite being simple to implement, these approaches often yielded suboptimal results. More advanced parameter-based techniques like Task Arithmetic \citep{ilharco2022editing} and SLERP \citep{white2016sampling} introduced parameter differences computation and spherical interpolation respectively. Later developments leveraged neural network sparsity, with TIES-Merging \citep{yadav2024ties} selectively retaining parameters based on magnitude while addressing sign conflicts, and DARE \citep{yu2024language} combining magnitude-based sparsification with parameter rescaling. Recent advances include Evolutionary model merging \citep{akiba2024evolutionary}, which optimizes merging coefficients through evolutionary search. In this study, our framework also focuses on automatic model merging, while we propose a novel framework that leverages Multi-fidelity optimization \citep{peherstorfer2018survey} based on SMAC \citep{JMLR:v23:21-0888}, optimizing merging recipes through layer fusion and pathway optimization for enhanced reasoning capabilities.


% Early methods focused on weighted parameter averaging \citep{utans1996weight} for models fine-tuned from a shared base model. While straightforward, these approaches often yielded suboptimal results. More advanced parameter-based techniques  Task Arithmetic \citep{ilharco2022editing}, which computes parameter differences between fine-tuned and pre-trained models, and SLERP \citep{white2016sampling}, which employs spherical interpolation for parameter combination. A significant advancement came through sparsity-based methods, which leverage the inherent over-parameterization of neural networks. TIES-Merging \citep{yadav2024ties} exploits this characteristic by selectively retaining parameters based on magnitude while addressing sign conflicts to reduce interference. Similarly, Drop And REscale (DARE) \citep{yu2024language} combines magnitude-based sparsification with parameter rescaling to enhance merging effectiveness. The field further evolved with automated optimization approaches, exemplified by Evolutionary model merging\citep{akiba2024evolutionary}, which introduces an efficient evolutionary search strategy for determining optimal merging coefficients. While our framework also focuses on automatic model merging,  We propose a novel automated model merging framework that leverages Multi-fidelity optimization based on SMAC\citep{JMLR:v23:21-0888}, optimizing merging recipes through both layer fusion and pathway optimization for enhanced reasoning capabilities.




% Model merging techniques can be broadly categorized into two paradigms: homogeneous merging, which combines models trained on identical tasks to enhance generalization, and heterogeneous merging, which synthesizes capabilities across models specialized in distinct tasks. Our work advances heterogeneous merging through an automated framework that optimizes both specific reasoning abilities and comprehensive reasoning capabilities through systematic model combination.


% \subsection{Neural Architecture Search}
% Neural Architecture Search (NAS) has revolutionized the landscape of deep learning by enabling the discovery of customized architectures that might elude human intuition. NAS frameworks typically comprise three fundamental components: a search space defining possible neural architectures, architecture optimization methods, and model evaluation strategies. The architecture optimization landscape encompasses various approaches, including reinforcement learning (RL) \citep{pham2018efficient}, evolutionary algorithms (EA) \citep{akiba2024evolutionary}, gradient descent (GD) \citep{liu2018darts}, and Surrogate Model-Based Optimization (SMBO) \citep{hutter2011sequential, falkner2018bohb}.
% While model merging has emerged as a promising approach to democratize model development, current methods heavily rely on manual intuition and domain expertise. As the ecosystem of open-source models and tasks continues to expand, there is a pressing need for more systematic and automated approaches to model merging, drawing inspiration from NAS methodologies.
% Our approach adopts the principles of BOHB \citep{falkner2018bohb}, which elegantly combines Hyperband optimization with Bayesian Optimization (BO). In this framework, the surrogate model is fitted on the highest budget level with sufficient observations. We extend this concept to automate the optimization of mathematics-reasoning enhanced model merging recipes across both parameter and layer space. Notably, our method differs fundamentally from traditional NAS in its evaluation strategy. Since we work with pre-trained transformer blocks, we can directly evaluate candidate architectures without the computational overhead of training, significantly accelerating the optimization process.

\subsection{Hyperparameter Optimization}
\textbf{Bayesian Optimization} has demonstrated remarkable success across various applications, from achieving state-of-the-art results on CIFAR-10~\citep{snoek2012practical} to winning multiple datasets in the 2016 AutoML challenge~\citep{mendoza2016towards}. Although Gaussian processes remain the predominant probabilistic model in Bayesian optimization due to their well-calibrated uncertainty estimates, they face limitations in scalability, flexibility, and robustness. Alternative models such as random forests \citep{hutter2011sequential} and Bayesian neural networks \citep{snoek2015scalable, springenberg2016bayesian, perrone2017multiple} offer better scalability for high-dimensional spaces.

\textbf{Hyperband} is one of the most widely-used multi-fidelity optimization \citep{peherstorfer2018survey} methods. It dynamically allocates resources across random configurations, while applying successive halving \citep{jamieson2016non} to eliminate poor options early. Although this method demonstrates superior performance and scalability compared to traditional Bayesian optimization, its random sampling strategy fails to leverage information from previous evaluations, potentially limiting its performance. 

Our optimizer builds upon SMAC \citep{JMLR:v23:21-0888}, combining Hyperband (HB) and Bayesian Optimization (BO) to harness both efficient resource allocation and learning capabilities through surrogate modeling for effective multi-fidelity optimization. We integrate this approach into our framework to enable effective and efficient searching.


% Our optimizer builds upon SMAC~\citep{JMLR:v23:21-0888}, which combines Hyperband (HB)~\citep{li2018hyperband} and Bayesian Optimization (BO)~\citep{snoek2012practical}, which fits a surrogate model on the highest budget level with sufficient observations, effectively combining the efficiency of Hyperband with the learning capabilities of Bayesian optimization. We extend this optimization strategy to our automatic model merging framework to achieve both effective and efficient searching.






\section{Method}
\subsection{Overview}

% Our framework explores model merging through two search spaces: Layer-wise Fusion Search Space (LFS) and Depth-wise Integration Search Space (DIS). We employ an efficient multi-fidelity optimization approach that operates in two modes: single-objective optimization maximizes performance on specific tasks, while multi-objective optimization identifies Pareto-optimal solutions across multiple reasoning dimensions. 
% This approach facilitates both specialized model optimization for targeted applications and balanced solutions that maintain strong performance across diverse reasoning tasks. The subsequent sections elaborate on our search space formulation and optimization methodology.

% Our framework explores model merging through two search spaces: Layer-wise Fusion Search Space (LFS) enables fine-grained model combination while preserving the original architecture, allowing precise control over how knowledge from source models is integrated at each layer, while Depth-wise Integration Search Space (DIS) leverages the expressive power of increased transformer depth by intelligently stacking layers from source models, offering a compute-efficient alternative for those seeking enhanced model capabilities without training deeper architectures from scratch. To optimize search efficiency, we implement a multi-fidelity approach with two modes: single-objective optimization maximizes performance on specific tasks, while multi-objective optimization identifies Pareto-optimal solutions across multiple reasoning dimensions.

To define a hyperparameter optimization pipeline for model merging, we need three parts, a search space, a target objective, and a search strategy.
Our framework introduces two model merging search spaces: A Layer-wise Fusion Search Space (LFS) and a Depth-wise Integration Search Space (DIS). LFS provides fine-grained layer-wise merging, merging weights at corresponding layers from multiple models according to an optimal merge operation. However, finetuned models may not always contain corresponding layers that can be merged, even if they contain complementary information. The DIS space addresses this limitation by maintaining individual layer weights while optimizing their sequential arrangement, enabling the discovery of optimal inter-layer relationships, e.g. by picking up both copies of a corresponding layer from a model pair and placing both in an optimal order in the merged model. 
To accelerate the search process, we implement multi-fidelity optimization that reduces search costs while efficiently finding both task-specific optimal solutions and Pareto-optimal configurations across multiple reasoning dimensions.

\subsection{Search Space}

% Layer-wise Fusion Search Space (LFS)
% Depth-wise Integration Search Space (DIS)


\subsubsection{Layer-wise Fusion Search Space}
%  

% uniform merging methods and hyperparameters across all layers  -- > performance variation --> our search space

Layer-wise merging combines corresponding layers from multiple models to create a new model. While effective, current approaches typically apply uniform merging methods and hyperparameters across all layers, using the entire set of candidate models, which might be too coarse-grained and potentially problematic. To illustrate this concern, we conduct an analysis using TIES merge \citep{yadav2024ties}, one of the most robust merging methods, on the GSM8K benchmark. As shown in Figure \ref{fig:fig2} (a), when applying TIES merge with same hyperparameters across different model combinations, the accuracy varies dramatically from 1\% to 64.5\%. Furthermore, Figure \ref{fig:fig2} (b) demonstrates that even with a fixed model combination (Math+Code), different hyperparameter settings lead to substantial performance variations, ranging from 34.9\% to 64.5\%. These results reveal two critical challenges in layer-wise merging: the selection of candidate models and the determination of hyperparameters. Both factors significantly impact the final performance, even for well-established methods like TIES. Manual tuning of these choices is not only labor-intensive but also makes it challenging to find optimal configurations.



\looseness -1 To address these limitations and motivated by how different layers in neural networks serve distinct purposes, from basic feature extraction to complex task-specific processing, we design a fine-grained layer-wise merging search space (LFS). Our search space is illustrated in Figure \ref{fig:fig1} (a), we partition the model's $L$ layers into $G$ consecutive groups, where layers within each group share the same merging coefficients. These coefficients determine: (1) the selection of source models from the candidate pool, (2) the choice of merging algorithms, and (3) the corresponding hyperparameters for the chosen merging method. Furthermore, we introduce a component-wise decomposition strategy. Specifically, we partition the parameters within each Transformer layer into $C$  component groups. When $C=1$, the entire layer is treated as a single unit. When $C=3$, we decompose the layer into three groups: MLP-related parameters, attention mechanism parameters, and layer normalization parameters. This decomposition allows for the application of component-specific hyperparameters during the merging process.

We define the merging coefficients $x \in \mathbb{R}^{G \times C \times (1 + H)}$, where $G$ represents the number of layer groups, $C$ denotes the number of components per layer, and $1 + H$ dimensions specify the merging method selection and hyperparameters of all candidate merging methods. We use four well-established merging methods: Task Arithmetic, TIES-Merging, SLERP, and Linear Merging. See Section \ref{sec:a0} for more descriptions of the methods. LFS provides a fine-grained and flexible search space for model merging, which not only enables precise optimization of the fusion but also maximizes the potential of layer-wise merging.
% The optimization process is guided by predefined objectives, where we evaluate the merged model's performance according to these objectives to obtain feedback that directs our optimization. 
% The Layer-wise Fusion Search Space (LFS) provides a fine-grained and flexible search space for model merging, which not only enables precise optimization of the fusion process but also maximizes the potential of layer-wise model merging.


% \textbf{Task Arithmetic} enhance model capabilities through vector operations by leveraging weighted combinations of task-specific knowledge. Given a base model with weights $\theta_{\text{pre}}$ and task-specific fine-tuned weights $\{\theta_{t}^{\text{ft}}\}_{t=1}^n$, task vectors are defined as:

% \begin{equation}
% \tau_t = \theta_{t}^{\text{ft}} - \theta_{\text{pre}}
% \end{equation}

% The merged weights are then computed through:

% \begin{equation}
% \theta_{\text{Merge}} = \theta_{\text{pre}} + \lambda \sum_{t=1}^n \tau_t
% \end{equation}

% where $\lambda$ controls the magnitude of task-specific adaptations.

% \textbf{TIES-Merging} is a parameter conflict resolution approach that operates in three stages. First, select the top $k\%$ parameters by magnitude of each task vector $\tau_t$:

% \begin{equation}
% \hat{\tau}_t = \text{TopK}(\tau_t, k)
% \end{equation}

% Next, Generating a consensus sign vector by examining the aggregate direction of parameter changes across all tasks:

% \begin{equation}
% \hat{\gamma} = \text{sgn}\left(\sum_{t=1}^n \hat{\tau}_t\right)
% \end{equation}

% Finally, computing the average update magnitude considering only those task vectors whose signs align with the consensus direction:

% \begin{equation}
% \tilde{\tau} = \text{Average}(\{\hat{\tau}_t : \text{sgn}(\hat{\tau}_t) = \hat{\gamma}\})
% \end{equation}

% The final merged model weights are then computed as:

% \begin{equation}
% \theta_{\text{Merge}} = \theta_{\text{pre}} + \tilde{\tau}
% \end{equation}

% \textbf{SLERP} (Spherical Linear Interpolation ) computes optimal geodesic paths between model weights through:

% \begin{equation}
% \text{SLERP}(\theta_1, \theta_2, t) = \frac{\sin((1-t)\omega)}{\sin(\omega)}\theta_1 + \frac{\sin(t\omega)}{\sin(\omega)}\theta_2
% \end{equation}

% where $\omega = \arccos\left(\frac{\langle\theta_1, \theta_2\rangle}{\|\theta_1\|\|\theta_2\|}\right)$ and $t \in [0,1]$ is the interpolation parameter.

% \textbf{Linear Merging} implements straightforward weighted aver-aging:

% \begin{equation}
% \theta_{\text{Linear}} = \sum_{t=1}^n w_t\theta_t
% \end{equation}

% where $\sum_{t=1}^n w_t = 1$ and $w_t \geq 0$.



% Our framework provides flexibility in two dimensions: different layers can employ distinct merging algorithms, and within the same layer, different network structures can utilize merging algorithm with varying hyperparameters. This comprehensive exploration of layer-wise merging possibilities enables fine-grained optimization of the model fusion process and fully exploits the potential of layer-wise model merging.




% \subsubsection{Scale-Factor Search Space}

% Growing research focuses on identifying task-specific neurons through various methods: gradient-based approaches (\citep{panigrahi2023task}) provide precise insights but are computationally intensive, while methods leveraging forward pass information through activation analysis (\citep{tang2024language}) or combined weight-activation importance calculations (\citep{christ2024math}) offer greater efficiency. These neuron-specific studies enhance model interpretability and demonstrate practical benefits: fine-tuning only task-specific neurons (\citep{zhang2024interpreting}) or simply scaling them (\citep{christ2024math}) can improve task performance.
% Inspired by these findings, we introduce a Scale-Factor Search Space (SFS) that enables efficient task-specific optimization by applying scaling factors to both weights and layer outputs, as illustrated in Figure \ref{fig:search_space}(b). 




% Specifically, for each transformer layer, we introduce scaling parameters for different weight matrices (query, key, value matrices in attention modules and weight matrices in MLPs), with all scaling factors constrained to the range $[0,2]$.

%, totaling $k$ scaling parameters per layer, along with one scaling parameter for each layer's output, yielding a search space dimension of $d = (k+1)L$ where $L$ denotes the number of layers. Through empirical evaluation, 





\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig_version_0/fig2new.pdf}
    \vspace{-.25cm}
    \caption{Performance of TIES merges on GSM8K: (a) comparison of different source model combinations and (b) various configurations with Math and Code as source models. We set the scaling term and parameter retention ratio in reasonable ranges of [0.5, 1.0] and [0.5, 0.7, 0.9], respectively. See Table~\ref{tab:ties_performance} for detailed configuration information.}
    \label{fig:fig2}
    \vspace{-.25cm}
\end{figure}


\subsubsection{Depth-wise Integration Search Space}
% In transformer architectures, each layer shares an identical structure but differs in its position and parameters. Recent research has provided significant insights into how knowledge is distributed across transformer layers \citep{meng2022locating, geva2022transformer}. \citet{lad2406remarkable} investigated the robustness of transformer-based Large Language Models through layer deletion and swapping. \citet{sun2024transformer} conceptualized middle layers as an assembly line of painters, offering new insights into layer behavior. \citet{kim2023solar} showed that straightforward depth-wise scaling can effectively create deeper models.

Large Language Models (LLMs) exhibit hierarchical language understanding, with knowledge transformation progressing sequentially from word-level comprehension to abstract conceptual understanding. Recent research has increasingly focused on the behavior of transformer layers. \citet{meng2022locating} and \citet{geva2022transformer} explored the distribution of knowledge across different layers.  \citet{lad2406remarkable} examined the robustness of transformer-based large language models by analyzing the effects of layer deletion and swapping. \citet{sun2024transformer} show that lower and final layers differ from middle layers, which demonstrate uniformity and robustness to reordering and parallelization for certain tasks. This hierarchical relationship between layers remain unexplored in the context of model merging. To fill this gap, we introduce the Depth-wise Integration Search Space (DIS), which preserves the original weights of individual layers while optimizing the inference pathway.


% Inspired by these developments, we introduce the Depth-wise Integration Search Space (DIS) as shown in Figure \ref{fig:fig1}(b), a framework for investigating the impact of layer manipulation in pretrained transformers. Unlike Layer-wise Feature Search (LFS) space, Depth-wise Integration Search Space (DIS) preserves the original weights of individual layers while optimizing the inference pathway through which tokens traverse the network.

As depicted in Figure \ref{fig:fig1} (b), DIS is characterized by three parameters: depth granularity $D$, number of candidate models $M$, and repeat factor $R$. These parameters partition transformer layers into $B = L/D$ consecutive blocks, where each block encompasses $D \times M \times R$ candidate layers. The search space is characterized by merging coefficients $x = \{(\mathbf{s}^{(i)}, \mathbf{p}^{(i)})\}_{i=1}^B$, where selection vector $\mathbf{s}^{(i)} \in \{0,1\}^{M \times D \times R}$ determines layer activation and permutation vector $\mathbf{p}^{(i)} \in \{0, 1, ..., P-1\}$ with $P = (D \times M \times R)!/(R!)^{M \times D}$ specifies layer ordering.   When no layers are selected in a block ($\mathbf{s}^{(i)} = 0$), we implement a layer retention strategy that preserves model depth by defaulting to base model layers. To maintain architectural coherence, we introduce scaling factors $\{o_1, ..., o_B\}$ to normalize the output of each block \citep{akiba2024evolutionary}.





% layer retention strategy 

This parametrization enables a rich set of architectural operations, including layer pruning, stacking, repetition, and reordering, with the scope of these operations controlled by the granularity parameter $D$. At $D=1$, the search space emphasizes layer-wise interactions at corresponding positions across candidate models. As $D$ increases, the framework accommodates more complex integration patterns and cross-depth layer interactions. We hypothesize that LFS and DIS might serve complementary roles. Our intuition is that LFS, with its fine-grained feature fusion, could be better suited for combining models with similar concepts, while DIS's preservation of processing blocks might be more effective when merging models from complementary domains.
% We argue that LFS and DIS serve as complementary search spaces with distinct strengths. Compared to LFS which boosts performance by combining models with similar concepts through fine-grained feature fusion, DIS is more effective for models trained on complementary domains by preserving processing blocks and optimizing sequential pathways.



% add some compare

 % However, it is important to note that the search space complexity grows exponentially with increasing depth granularity $D$. This rapid expansion presents a significant challenge in determining appropriate hyperparameters for optimization algorithms, necessitating careful consideration of the trade-off between search space expressivity and optimization efficiency. 

% LFS and DIS represent complementary search spaces with distinct advantages. LFS excels when models have learned similar concepts with varying layer-level strengths, enabling fine-grained combination of layer-specific features. In contrast, DIS is particularly effective when models are trained on different but complementary domains, as it preserves complete processing blocks and enables optimization of diverse sequential processing paths. 







% We define search variables $x \in \Lambda$ where $\Lambda = \{\mathbf{s}^{(1)}, \mathbf{p}^{(1)}, ..., \mathbf{s}^{(B)}, \mathbf{p}^{(B)}\}$, with $B = L/D$ blocks, selection vectors $\mathbf{s}^{(i)} \in \{0,1\}^{M \times D \times R}$, and permutation vectors $\mathbf{p}^{(i)} \in \mathbb{Z}^{D \times M \times R}$ subject to ordering constraints.

% We define search variables $x \in \{\mathbf{s}^{(1)}, \mathbf{p}^{(1)}, ..., \mathbf{s}^{(B)}, \mathbf{p}^{(B)}\}$, where $B = L/D$ is the number of blocks, $\mathbf{s}^{(i)} \in \{0,1\}^{M \times D \times R}$ specifies layer activation states, and $\mathbf{p}^{(i)} \in \{0, 1, ..., P-1\}$ with $P = (D \times M \times R)!/(R!)^{M \times D}$ determines layer ordering within each block $i$.

% Formally, for a transformer with $L$ layers and $m$ candidate models, the search space $\mathcal{S}$ is defined as $\mathcal{S} = {(\mathbf{s}, \mathbf{p}) | \mathbf{s} \in {0,1}^{n \times g}, \mathbf{p} \in \mathcal{P}(n \times g)}$, where $\mathcal{P}(n \times g)$ denotes valid permutations.  This formulation preserves the original weights of individual layers while optimizing the inference pathway through which tokens traverse the network


\subsection{Multi-Fidelity Optimization}
Although optimization of model merging requires less computation compared to searching for optimal hyper-parameters for neural network structures and the model training process~\citep{yu2020hyper,elsken2019neural}, evaluating large language models on extensive validation datasets remains computationally intensive. We optimize the process using cost-efficient Multi-Fidelity Optimization (MFO)~\citep{peherstorfer2018survey}, leveraging evaluations across different fidelity levels from fast but less accurate low-fidelity to slow but more accurate high-fidelity. 

The optimization objective can be formulated as:
% \begin{equation}
%     \lambda^* \in \arg\min_{\lambda \in \Lambda} c(\lambda, b_{\text{max}}) = \arg\min_{\lambda \in \Lambda} \mathcal{L}(\mathcal{D}_{\text{val}}; \lambda, b_{\text{max}})
% \end{equation}
\begin{equation}
x^* \in \arg\min_{x \in \Lambda} c(x, b) = \arg\min_{x \in \Lambda} \mathcal{L}(\mathcal{D}_{\text{val}}; x,b)
\end{equation}

Here, we use evaluation samples as fidelity types. Each configuration is evaluated with varying budgets $b_{\text{min}} \leq b \leq b_{\text{max}}$, where the budget determines the validation dataset size. Using smaller budgets provides a cheaper proxy of the true cost function (measured at $b_{\text{max}}$).
% where configurations can be evaluated with varying budgets $b_{\text{min}} \leq b \leq b_{\text{max}}$ (representing validation dataset size) to obtain a cheap proxy of the true cost function at $b_{\text{max}}$.


Our implementation extends SMAC~\citep{JMLR:v23:21-0888} by establishing a hierarchical evaluation framework parameterized by $b_{\text{max}}$, $b_{\text{min}}$, and spacing factor $\eta$. Using Successive Halving \citep{jamieson2016non}, each bracket $i$ starts with $n_i$ configurations at budget $b_i$, iteratively halving configurations and increasing budgets by $\eta$ until reaching $b_{\text{max}}$. Higher brackets begin with fewer configurations but larger budgets. A Random Forest surrogate model~\citep{breiman2001random} guides configuration selection through Expected Improvement, balancing exploration at low fidelities with exploitation at high fidelities. See Section \ref{sec:a1} for more descriptions of the optimization.

% Our implementation extends SMAC~\citep{JMLR:v23:21-0888} by integrating Hyperband~\citep{li2018hyperband} with Bayesian Optimization (BO)~\citep{snoek2012practical}. The framework determines evaluation brackets based on budget parameters $b_{\text{max}}$, $b_{\text{min}}$, and spacing $\eta$. A Random Forest\citep{breiman2001random} surrogate model guides configuration selection through Expected Improvement, progressively transitioning from exploration with low-fidelity evaluations to exploitation with high-fidelity results. Please see Section \ref{sec:a2} for more descriptions of the optimization.

% Let $b_{\text{max}}$ and $b_{\text{min}}$ denote the maximum and minimum budgets respectively, and $\eta > 1$ be a budget spacing parameter. The framework determines $s_{\text{max}} = \lfloor\log_\eta(b_{\text{max}}/b_{\text{min}})\rfloor$ brackets, where $s_{\text{max}}$ represents the maximum number of successive halving iterations.

% The algorithm executes Successive Halving through $s+1$ rounds. At round $l$, where $l$ denotes the current iteration index, it evaluates all remaining configurations with budget $b_l = b_{\text{max}} \cdot \eta^{l-s-1}$, retains the top $\lfloor \frac{n_s}{\eta^l} \rfloor$ performing configurations, and increases the budget by a factor of $\eta$ for surviving configurations.
%% still too long 
% The Random Forest surrogate model maintains a performance history of configuration-budget pairs across all evaluations and updates before each Successive Halving iteration using multi-fidelity data, while prioritizing observations from higher budget evaluations. Configuration selection is guided through Expected Improvement (EI) maximization. As the optimization progresses, the accumulation of high-fidelity evaluations enables the surrogate model to refine its predictions, gradually transitioning from exploration guided by lower-fidelity evaluations to exploitation based on high-fidelity results. This multi-fidelity optimization framework enables efficient exploration of the configuration space while maintaining robust evaluation quality.


% Our implementation builds upon SMAC~\citep{JMLR:v23:21-0888}, which combines Hyperband (HB)~\citep{li2018hyperband} and Bayesian Optimization (BO)~\citep{snoek2012practical}, utilizing Random Forest~\citep{breiman2001random} as the surrogate model. Given a budget spacing parameter $\eta$, SMAC determines $s_{\text{max}} = \lfloor\log_\eta(R)\rfloor$ brackets. Each bracket $i$ begins with $n_i = \lfloor\eta^{s_{\text{max}}-i} \cdot \frac{\eta}{\eta-1}\rfloor$ configurations and executes Successive Halving across $\lfloor \log_\eta(\frac{n_i}{n_{\text{min}}}) \rfloor + 1$ rounds.

% Each round evaluates all configurations with budget $b$, retains the top $\lfloor \frac{n_i}{\eta^l} \rfloor$ performers, and increases their budget to $\eta b$ for the next round, where $l$ denotes the current round. This process continues until reaching either a single configuration or $b_{\text{max}}$.

% The Random Forest model incorporates configuration-performance pairs from previous evaluations, updating before each Successive Halving iteration using evaluations from all budget levels while prioritizing data from the largest available budget. The model guides configuration selection through Expected Improvement, enabling efficient exploration while maintaining evaluation quality.

% As optimization progresses, more configurations undergo evaluation at higher budgets, allowing the model to overcome potential misguided conclusions from lower-fidelity evaluations by ultimately relying on high-fidelity results. This integration of Hyperband's resource allocation with Bayesian optimization's surrogate modeling enables efficient exploration of the configuration space while maintaining evaluation quality through principled multi-fidelity optimization.



\subsubsection{Single objective Optimization} 

We use single-objective optimization to maximize task-specific performance. The cost function for each task is defined as:

% We employ single-objective optimization to maximize model performance, where the optimization is performed on a singleton set $\mathcal{S}_i = {\mathcal{D}_i}$, where $\mathcal{D}_i$ is one of GSMPlus \citep{li2024gsm}, MBPP \citep{austin2021program}, or MMLU \citep{hendrycks2020measuring} datasets. The optimization objective is formulated as:
% \begin{equation}
% c_{\mathcal{S}i}(x) = \arg\min{x \in \Lambda} \mathcal{L}(\mathcal{D}i; x, b{\text{min}}, b_{\text{max}}), \quad |\mathcal{S}_i| = 1
% \end{equation}




% We employ single-objective optimization to maximize model performance on a specific reasoning capability from dataset set $\mathcal{S} = {\mathcal{D}{\text{GSMPlus}}, \mathcal{D}{\text{MBPP}}^{\text{val}}, \mathcal{D}_{\text{MMLU}}^{\text{val}}}$ \citep{li2024gsm,austin2021program,hendrycks2020measuring}. The optimization objective is formulated as:

% \begin{equation}
% c_{\mathcal{D}}(x) = \arg\min_{x \in \Lambda} \mathcal{L}(\mathcal{D}; x, b_{\text{min}}, b_{\text{max}}), \quad \mathcal{D} \in \mathcal{S}
% \end{equation}

% where $c_{\mathcal{D}}(x)$ represents the optimization objective over the parameter space $x$ for dataset $\mathcal{D}$.


\begin{equation}
\begin{aligned}
c_i(x) &= \arg\min_{x \in \Lambda} \mathcal{L}(\mathcal{D}_i; x, b_{\text{min}}, b_{\text{max}})
\end{aligned}
\end{equation}

where $c_i(x)$ represents an optimization objective over the parameter space $x$ for a specific task dataset $\mathcal{D}_i$.  The loss function $\mathcal{L}$ measures the model's performance on the target task dataset. 

We define different optimization objectives using task-specific datasets: GSMPlus \citep{li2024gsm} for mathematical reasoning, MBPP \citep{austin2021program} validation set for programming capabilities, and MMLU \citep{hendrycks2020measuring} validation set for general reasoning.

% \begin{equation}
% \begin{aligned}
% c_i(x) &= \arg\min_{x \in \Lambda} \mathcal{L}(\mathcal{D}_i; x, b_{\text{min}}, b_{\text{max}}) \\
% \mathcal{D}_i &\in \{\mathcal{D}_{\text{GSMPlus}}, \mathcal{D}_{\text{MBPP}}^{\text{val}}, \mathcal{D}_{\text{MMLU}}^{\text{val}}\}
% \end{aligned}
% \end{equation}


% \begin{equation}
% c_1(x) = \arg\min_{x \in \Lambda} \mathcal{L}(\mathcal{D}_{\text{GSMPlus}}; x, b_{\text{min}}, b_{\text{max}})
% \end{equation}
% \begin{equation}
% c_2(x) = \arg\min_{x \in \Lambda} \mathcal{L}(\mathcal{D}_{\text{MBPP}}^{\text{val}}; x, b_{\text{min}}, b_{\text{max}})
% \end{equation}
% \begin{equation}
% c_3(x) = \arg\min_{x \in \Lambda} \mathcal{L}(\mathcal{D}_{\text{MMLU}}^{\text{val}}; x, b_{\text{min}},b_{\text{max}})
% \end{equation}


% where $c_1$ evaluates mathematical reasoning capabilities using the GSMPlus \citep{li2024gsm} dataset,  $c_2$ assesses programming proficiency through the MBPP \citep{austin2021program} validation dataset, measuring the model's ability to understand and generate functional code solutions, $c_3$ quantifies general reasoning capabilities using the MMLU \citep{hendrycks2020measuring} validation dataset, evaluating performance across a diverse range of academic subjects and knowledge domains. Each cost function $c_i(x)$ represents an optimization objective over the parameter space $x$.

\subsubsection{Multi objective Optimization} 
To develop comprehensive reasoning models, we employ ParEGO \citep{knowles2006parego} for multi-objective optimization to identify Pareto-optimal solutions across different objectives. The algorithm converts different cost values into a single cost using a parameterized scalarizing weight vector. By varying this weight vector at each iteration, ParEGO gradually builds an approximation of the entire Pareto front. Initially, the algorithm normalizes the $k$ cost functions to the $[0,1]$ interval.

In each step, the algorithm randomly selects a weight vector $\lambda$ from a set of uniformly distributed vectors, defined as:
%
%
%
% \begin{equation}
% \Lambda = \left\{\lambda = (\lambda_1, \lambda_2, ..., \lambda_k) \middle|
% \sum_{j=1}^k \lambda_j = 1 \wedge \forall j, \lambda_j = \frac{l}{s}, l \in \{0,\ldots,s\} \right\}
% \end{equation}
%
\begin{equation*}
\begin{aligned}
 \Lambda=\biggl\lbrace\boldsymbol{\lambda}=\left(\lambda_1, \lambda_2, \ldots, \lambda_k\right) \mid & \sum_{j=1}^k \lambda_j=1 \wedge \forall j, \\
 & \lambda_j=\frac{l}{s}, l \in\{0, \ldots, s\}\biggr\rbrace
\end{aligned}
\end{equation*}
%
The size of this set is determined by $|\Lambda| = \binom{s+2}{2}$, where $s$ controls the total number of possible vectors. The scalar cost for each solution is calculated using the augmented Tchebycheff function, where $\rho$ represents a small positive constant:
%
% \begin{equation}
% c_\lambda(x) = \max_{j=1}^k(\lambda_j \cdot c_j(x)) + \rho\sum_{j=1}^k \lambda_j \cdot c_j(x)
% \end{equation}
%
%c_{\text{agg}}
%
\begin{equation}
c_{\text{agg}}(x;\boldsymbol{\lambda}) = \max_{j=1}^k(\lambda_j \cdot c_j(x)) + \rho\sum_{j=1}^k \lambda_j \cdot c_j(x) 
\end{equation}
\vspace{-0.3cm}
%

% The augmented Tchebycheff function is particularly advantageous due to its dual properties: its nonlinear component facilitates the discovery of solutions in nonconvex regions of the Pareto front, enabling the identification of nonsupported solutions, while its linear component maintains proper ordering by ensuring that solutions weakly dominated by Pareto-optimal solutions receive appropriately lower valuations. Through this scalarization approach, the algorithm evaluates the scalar costs of previously explored solutions and directs the search toward promising areas within the solution space.


% \begin{table*}[htbp]
%     \centering
%     \caption{Performance of merging decoder-based WizardLM-13B (LM), WizardMath-13B (Math), and llama-2-13b-codealpaca (Code) on all the datasets.}
    
%     \begin{tabular}{l|c|c|c|c|c|c}
%         \hline
%         & \multirow{2}{*}{\textbf{Models}} & \multicolumn{1}{c|}{\textbf{Common}} & \multicolumn{2}{c|}{\textbf{Mathematical}} & \multicolumn{2}{c}{\textbf{Code-generating}} \\
%         & & \textbf{Reasoning} & \multicolumn{2}{c|}{\textbf{Reasoning}} & \multicolumn{2}{c}{} \\
%         \cline{3-7}
%         \multicolumn{1}{l|}{\textbf{Merging Objective}} & & MMLU & GSM8K & MATH & MBPP & Human Eval \\
%         \hline
%         \multirow{3}{*}{\textbf{Base}} 
%         & MATH & 52.04 & 64.22 & 13.70 & 18.20 & 7.32 \\
%         & CODE & 52.79 & 0 & 0 & 27.20 & 23.17 \\
%         & LM & 53.43 & 3.79 & 0 & 33.4& 38.41\\
%         \hline
%         \multirow{3}{*}{\textbf{Base Merge}} 
%         & Ties & 54.67 &  61.56 & 10.58 & 27.40 & 23.17 \\
%         & Slerp & - & - & - & - & - \\
%         & Task Arithmetic & 54.85 & 57.99  & 12.06 & 26.22 & 24.04 \\
%         & Linear Merging & 55.13 & 57.09 & 9.98 & 29.80 & 18.90\\
%         \hline
%         \multirow{8}{*}{\textbf{Single Objective}} 
%         & MATH-LFS & 54.52 & \underline{68.46} & 10.42 & 28.2& 17.07\\
%         & CODE-LFS & 53.36 &  49.73 &  9.3 &  \underline{33.6} & 14.63\\
%         & GEN-LFS & \underline{55.31} & 33.81 & 3.82 & 30.8 & 16.46\\
%         %& MATH-SFS & 52.03 & 63.91 & 13.58 & 18.2 & 7.32 \\
%         %& CODE-SFS & 51.91 & 0 & 0 & 28.6 & 24.39 \\
%         %& GEN-SFS & 53.63 & 6.52 & 0.08 & 30.4 & 34.15 \\
%         & GEN-DIS-0 & \underline{54.72} & 16.98 & 1.14 & 12.4 & 9.76 \\
%         & GEN-DIS-1 & \underline{54.76} & 1.74 & 0.06 & 16.8 & 18.29 \\
%         %& GEN-DIS-2 & \underline{53.68} & 1.82 & 0.02 & 9.8 & 4.88 \\
%         \hline
%         \multirow{5}{*}{\textbf{Multiple Objectives}} 
%         & MULTI-LFS-0 & 55.03 & 63.08 & 11.76 & 32.6 & 21.95 \\
%         & MULTI-LFS-1 & 54.70 & 66.94 & 11.38 & 30.6 & 23.78 \\
%         & MULTI-LFS-2 & 54.67 & 65.13 & 11.06 & 30.4 & 20.73 \\
%         & MULTI-LFS-3 & 54.99 & 65.50 & 9.42 & 30.2 & 23.17 \\
%         & MULTI-LFS-4 & 54.77 & 66.79 & 12.06 & 31.8 & 24.4 \\
%         \hline
%     \end{tabular}
%     \label{tab:table0}
% \end{table*}


% \begin{table*}[htbp]
% \centering
% \small
% \setlength{\tabcolsep}{5pt}
% %\caption{Performance of merging decoder-based WizardLM-13B (LM), WizardMath-13B (Math), and llama-2-13b-codealpaca (Code)}
% \caption{Performance comparison of merged models combining WizardLM-13B (LM), WizardMath-13B (Math), and llama-2-13b-codealpaca (Code). For single-objective optimization, improvements over corresponding best performance over base models are shown in blue parentheses.}
% %\begin{tabular}{@{}l|l|ccccc|c@{}}
% \begin{tabular}{@{}l|l|lllll@{}}
% \toprule
% & & \multicolumn{1}{c}{\textbf{Common}} & \multicolumn{2}{c}{\textbf{Mathematical}} & \multicolumn{2}{c}{\textbf{Code-generating}}  \\
% \cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
% \textbf{Method} & \textbf{Model} & MMLU & GSM8K & MATH & MBPP & HumanEval \\
% \midrule
% \multirow{3}{*}{\textbf{Base}} 
% & MATH & 52.04 & 64.22 & \textbf{13.70} & 18.20 & 7.32\\
% & CODE & 52.79 & -- & -- & 27.20 & 23.17  \\
% & LM & 53.43 & 3.79 & -- & \textbf{33.40} & \textbf{38.41} \\
% %\addlinespace[0.5em]
% \midrule
% %\addlinespace[0.5em]
% \multirow{3}{*}{\textbf{Basic Merge}} 
% & Ties & 54.67 & 61.56 & 10.58 & 27.40 & 23.17  \\
% & Task Arithmetic & 54.85 & 57.99 & \textbf{12.06} & 26.22 & \textbf{24.04} \\
% & Linear Merging & \textbf{55.13} & 57.09 & 9.98 & 29.80 & 18.90 \\
% %\addlinespace[0.5em]
% \midrule
% %\addlinespace[0.5em]
% \multirow{5}{*}{\textbf{Single Objective}} 
% & MATH-LFS & 54.52 & \textbf{68.46} \small\textcolor{blue}{(+4.24)} & 10.42 & 28.20 & 17.07  \\
% & CODE-LFS & 53.36 & 49.73 & 9.30 & \textbf{33.60}\small\textcolor{blue}{(+0.20)} & 14.63  \\
% & GEN-LFS & \textbf{55.31} \small\textcolor{blue}{(+1.88)}& 33.81 & 3.82 & 30.80 & 16.46  \\
% & GEN-DIS-0 & 54.72 \small\textcolor{blue}{(+1.29)}& 16.98 & 1.14 & 12.40 & 9.76  \\
% & GEN-DIS-1 & 54.76 \small\textcolor{blue}{(+1.33)} & 1.74 & 0.06 & 16.80 & 18.29  \\
% %\addlinespace[0.5em]
% \midrule
% \multirow{5}{*}{\textbf{Multiple Objectives}} 
% & MULTI-LFS-0 & \textbf{55.03} & 63.08 & 11.76 & \textbf{32.60} & 21.95 \\
% & MULTI-LFS-1 & 54.70 & \textbf{66.94} & 11.38 & 30.60 & 23.78  \\
% & MULTI-LFS-2 & 54.67 & 65.13 & 11.06 & 30.40 & 20.73  \\
% & MULTI-LFS-3 & 54.99 & 65.50 & 9.42 & 30.20 & 23.17 \\
% & MULTI-LFS-4 & 54.77 & \textbf{66.79} & \textbf{12.06} & 31.80 & \textbf{24.40} \\
% \bottomrule
% \end{tabular}
% \label{tab:table0}
% \end{table*}
% GEN-DIS-0(47)  GEN-DIS-1(Repeat 47)  GEN-DIS-2(Repeat delete 42)


% \begin{table*}[htbp]
% \centering
% \small
% \setlength{\tabcolsep}{4pt}
% \caption{Performance comparison of merged models combining WizardLM-13B (LM), WizardMath-13B (Math), and llama-2-13b-codealpaca (Code). For single-objective optimization, improvements over corresponding best performance over base models are shown in blue parentheses.}
% \begin{tabular}{@{}c|c|c|lllll@{}}
% \toprule
% & & & \multicolumn{1}{c}{\textbf{Common}} & \multicolumn{2}{c}{\textbf{Mathematical}} & \multicolumn{2}{c}{\textbf{Code-generating}}  \\
% \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
% \textbf{Method} & \textbf{Model} & \textbf{Source Models} & MMLU & GSM8K & MATH & MBPP & HumanEval \\
% \midrule
% \multirow{3}{*}{\textbf{Base}} 
% & MATH & -- & 52.04 & 64.22 & \textbf{13.70} & 18.20 & 7.32\\
% & CODE & -- & 52.79 & 0.00 & 0.00 & 27.20 & 23.17  \\
% & LM & -- & 53.43 & 3.79 & 0.00 & \textbf{33.40} & \textbf{38.41} \\
% \midrule
% \multirow{3}{*}{\textbf{Basic Merge}} 
% & Ties & Math+Code+LM & 54.67 & 61.56 & 10.58 & 27.40 & 23.17  \\
% & Task Arithmetic & Math+Code+LM & 54.85 & 57.99 & \textbf{12.06} & 26.22 & \textbf{24.04} \\
% & Linear Merging & Math+Code+LM & \textbf{55.13} & 57.09 & 9.98 & 29.80 & 18.90 \\
% \midrule
% \multirow{5}{*}{\textbf{Single Objective}} 
% & MATH-LFS & Math+Code+LM & 54.52 & \textbf{68.46} \small\textcolor{blue}{(+4.24)} & 10.42 & 28.20 & 17.07  \\
% & CODE-LFS & Math+Code+LM & 53.36 & 49.73 & 9.30 & \textbf{33.60}\small\textcolor{blue}{(+0.20)} & 14.63  \\
% & GEN-LFS & Math+Code+LM & \textbf{55.31} \small\textcolor{blue}{(+1.88)}& 33.81 & 3.82 & 30.80 & 16.46  \\
% & GEN-DIS-0 & Math+Code+LM & 54.72 \small\textcolor{blue}{(+1.29)}& 16.98 & 1.14 & 12.40 & 9.76  \\
% & GEN-DIS-1 & LM & 54.76 \small\textcolor{blue}{(+1.33)} & 1.74 & 0.06 & 16.80 & 18.29  \\
% \midrule
% \multirow{5}{*}{\textbf{Multiple Objectives}} 
% & MULTI-LFS-0 & Math+Code+LM & \textbf{55.03} & 63.08 & 11.76 & \textbf{32.60} & 21.95 \\
% & MULTI-LFS-1 & Math+Code+LM & 54.70 & \textbf{66.94} & 11.38 & 30.60 & 23.78  \\
% & MULTI-LFS-2 & Math+Code+LM & 54.67 & 65.13 & 11.06 & 30.40 & 20.73  \\
% & MULTI-LFS-3 & Math+Code+LM & 54.99 & 65.50 & 9.42 & 30.20 & 23.17 \\
% & MULTI-LFS-4 & Math+Code+LM & 54.77 & \textbf{66.79} & \textbf{12.06} & 31.80 & \textbf{24.40} \\
% \bottomrule
% \end{tabular}
% \label{tab:table0}
% \end{table*}



\begin{table*}[htbp]
\centering
\small
\setlength{\tabcolsep}{3.5pt}
\caption{Performance comparison of merged models combining WizardLM-13B (LM), WizardMath-13B (Math), and llama-2-13b-codealpaca (Code). For single-objective optimization, improvements over source models are shown in blue. For multi-objective optimization, improvements over the best base model (MATH) are shown in blue in the Average column.}
\begin{tabular}{@{}c|c|c|c|lllll|c@{}}
\toprule
& & & & \multicolumn{1}{c}{\textbf{Common}} & \multicolumn{2}{c}{\textbf{Math}} & \multicolumn{2}{c}{\textbf{Code}} & \\
\cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
\textbf{Method} & \textbf{Model} & \textbf{Source\textsuperscript{*}} & \textbf{Search\textsuperscript{*}} & MMLU & GSM8K & MATH & MBPP & HumanEval & Average \\
\midrule
\multirow{3}{*}{\textbf{Base}} 
& MATH & -- & -- & 52.04 & 64.22 & \textbf{13.70} & 18.20 & 7.32 & 31.10 \\
& CODE & -- & -- & 52.79 & 0.00 & 0.00 & 27.20 & 23.17 & 20.63 \\
& LM & -- & -- & 53.43 & 3.79 & 0.00 & \textbf{33.40} & \textbf{38.41} & 25.81 \\
\midrule
\multirow{3}{*}{\textbf{Basic}} 
& Ties & M+C+L & -- & 54.67 & 61.56 & 10.58 & 27.40 & 23.17 & 35.48 \small\textcolor{blue}{(+4.38)} \\
& Task Arith & M+C+L & -- & 54.85 & 57.99 & \textbf{12.06} & 26.22 & \textbf{24.04} & 35.03 \small\textcolor{blue}{(+3.93)} \\
& Linear & M+C+L & -- & \textbf{55.13} & 57.09 & 9.98 & 29.80 & 18.90 & 34.18 \small\textcolor{blue}{(+3.08)} \\
\midrule
\multirow{5}{*}{\textbf{Single-Obj}} 
& MATH-LFS & M+C+L & 1 & 54.52 & \textbf{68.46} \small\textcolor{blue}{(+4.24)} & 10.42 & 28.20 & 17.07 & -- \\
& CODE-LFS & M+C+L & 2 & 53.36 & 49.73 & 9.30 & \textbf{33.60} \small\textcolor{blue}{(+0.20)} & 14.63 & -- \\
& GEN-LFS & M+C+L & 3 & \textbf{55.31} \small\textcolor{blue}{(+1.88)}& 33.81 & 3.82 & 30.80 & 16.46 & -- \\
& GEN-DIS-0 & M+C+L & 3 & 54.72 \small\textcolor{blue}{(+1.29)}& 16.98 & 1.14 & 12.40 & 9.76 & --  \\
& GEN-DIS-1 & L & 3 & 54.76 \small\textcolor{blue}{(+1.33)} & 1.74 & 0.06 & 16.80 & 18.29 & -- \\
\midrule
\multirow{5}{*}{\textbf{Multi-Obj}} 
& MULTI-LFS-0 & M+C+L & 1-3 & \textbf{55.03} & 63.08 & 11.76 & \textbf{32.60} & 21.95 & 36.88 \small\textcolor{blue}{(+5.78)} \\
& MULTI-LFS-1 & M+C+L & 1-3 & 54.70 & \textbf{66.94} & 11.38 & 30.60 & 23.78 & 37.48 \small\textcolor{blue}{(+6.38)} \\
& MULTI-LFS-2 & M+C+L & 1-3 & 54.67 & 65.13 & 11.06 & 30.40 & 20.73 & 36.40 \small\textcolor{blue}{(+5.30)} \\
& MULTI-LFS-3 & M+C+L & 1-3 & 54.99 & 65.50 & 9.42 & 30.20 & 23.17 & 36.66 \small\textcolor{blue}{(+5.56)} \\
& MULTI-LFS-4 & M+C+L & 1-3 & 54.77 & \textbf{66.79} & \textbf{12.06} & 31.80 & \textbf{24.40} & 37.96 \small\textcolor{blue}{(+6.86)} \\
\bottomrule 
\multicolumn{10}{l}{\textsuperscript{*}Dataset Index: 1=GSMPlus, 2=MBPP\textsubscript{val}, 3=MMLU\textsubscript{val}}\\
\multicolumn{10}{l}{\textsuperscript{*}M+C+L = Math+Code+LM}\\
 % M+C+L denotes the combination of Math+Code+LM models, 
\end{tabular}
\label{tab:table0}
\vspace{-.1cm}
\end{table*}






\section{Experiments}
\subsection{Experimental Setting}

\textbf{Source Models}
We use LLaMA-family models \citep{touvron2023llama} as our base model set, including WizardLM-13B \citep{xu2023wizardlm}, WizardMath-13B \citep{luo2023wizardmath}, and llama-2-13b-code-alpaca \citep{chaudhary2023code}. All these models are fine-tuned from Llama-2-13b, ensuring a shared loss landscape. We exclude WizardCoder-Python-13B~\citep{luo2023wizardcoder} as it uses CodeLlama-13b-Python~\citep{roziere2023code} as its pre-trained backbone, resulting in a different loss landscape.

\textbf{Datasets}
We select separate datasets for search and evaluation. For searching, we use GSMPlus \citep{li2024gsm} for mathematical reasoning, MBPP \citep{austin2021program} samples for code understanding, and MMLU \citep {hendrycks2020measuring}validation samples for general knowledge. For evaluation, we employ established benchmark test sets: GSM8K and MATH for mathematical reasoning, MBPP and HumanEval \citep{chen2021codex} for code generation, and the MMLU test set for general knowledge assessment. See Section \ref{sec:a2} and \ref{sec:a3} for more details.




% \textbf{Evaluation Metrics and details} We evaluate using domain-specific metrics: accuracy for MMLU multiple choice questions based on loglikelihood, zero-shot accuracy for GSM8K and MATH, and Pass@1 for HumanEval and MBPP. We run all evaluations using LM Evaluation Harness\citep{eval-harness} with vLLM\citep{kwon2023efficient} acceleration. For consistency, we use fixed parameters across all tests: batch size $16$, temperature $0.0$ for greedy decoding, and maximum generation length of $1,024$ tokens for GSM8K and $2,048$ tokens for other datasets. All experiments run on NVIDIA Tesla A100 GPUs.


\textbf{Search Spaces} 
% Within LFS, we implement four merging methods: Task Arithmetic, TIES-Merging, Linear-merging and sleep.
As described in the method section, within LFS we implement four merging methods: Task Arithmetic, TIES-Merging, Linear-merging, and Slerp. We set the number of layer groups to $G=4$ and the number of component groups per layer to $C=3$. For DIS, we evaluate two configurations: Configuration $1$ with depth granularity $D=1$, number of candidate models $M=3$, and repeat factor $R=1$; Configuration $2$ with $D=1$, $M=1$, and $R=2$. Both configurations keep $D=1$ to manage optimization complexity. We focus on studying layer-wise interactions between corresponding positions across candidate models. See \ref{sec:a4} for more details.
% Both configurations maintain $D=1$ as optimization complexity needs to be managed, our focus on studying layer-wise interactions between corresponding positions across candidate models. See \ref{sec:a4} for more details.

\textbf{Optimization} 
% budget， trails， init points
Our implementation builds upon SMAC~\citep{JMLR:v23:21-0888}, with domain-specific budget allocations for optimization tasks. Mathematical reasoning tasks receive 100-1000 samples to explore complex solution spaces, code reasoning uses 300 samples (200 training, 100 validation), and we sample 50\% of MMLU validation data within 100-700 sample bounds. These budget ranges remain consistent when conducting multi-objective optimization across multiple datasets within these domains. We configured the search trials based on the complexity of each search space. We allocated $500$ search trials for LFS and $1000$ for the broader DIS search space, using initial candidate models as starting points to improve optimization efficiency.
%For LFS, we set $500$ search trials. The DIS search space, with its broader scope, was assigned $1000$ search trials. To improve optimization efficiency, we used initial candidate models as starting points across all search spaces, providing better initialization states and potentially faster convergence to optimal solutions.



% % For single-objective optimization, we set budgets range based on task type: $(100-1000)$ samples for mathematical reasoning tasks, $(100-300)$ samples for code reasoning tasks, and $(100-700)$ samples for general reasoning tasks. When conducting multi-objective optimization across multiple datasets from these domains, we maintained these same budget allocations.

% We configured the search trials based on the complexity of each search space. For Layer-wise Fusion Search Space (LFS), we set $500$ search trials. The Depth-wise Integration Search (DIS) space, with its broader scope, was assigned $1000$ search trials. To improve optimization efficiency, we used initial candidate models as starting points across all search spaces, providing better initialization states and potentially faster convergence to optimal solutions.

%By setting reward to zero to ensure the resulting network size remained bounded at a maximum of $55$ layers.






\subsection{Results}


\subsubsection{Single objective optimization}

\textbf{Layer-wise Fusion} Using three source models (Math, Code, and LM) optimized for mathematical, coding, and general reasoning tasks respectively, we developed three specialized models through LFS: MATH-LFS, CODE-LFS, and GEN-LFS.  Table~\ref{tab:table0} presents the performance of these models across five benchmarks. Our results demonstrate that MATH-LFS achieves a 4.24\% improvement over the best performance of source models on GSM8k, CODE-LFS shows modest gains on MBPP, and GEN-LFS exhibits a 1.88\% improvement on MMLU. These MATH-LFS gains are especially surprising, given that the base model was already finetuned for improve GSM8k performance - it appears that its arithmetic performance could be further improved through merging with the coding and instruction tuning model.

Beyond these task-specific enhancements, we observe that LFS-searched models also demonstrate improved performance in other reasoning capabilities, with MATH-LFS showing particular strength in both common reasoning and code generation tasks. In comparison with alternative merging methods including TIES, Task Arithmetic, and linear merging (with hyperparameter search ranges detailed in Table~\ref{tab:table6}), our LFS method achieves superior performance on the targeted optimization objectives, demonstrating the effectiveness of our approach in single task optimization. The architecture of MATH-LFS, as illustrated in Figure  \ref{fig:fig3} (a), divides the model layers into four groups (G=4), where Task Arithmetic merge is employed for groups 1 and 2, while the subsequent layer groups 3 and 4 utilize SLERP merge with different model combinations - Math-Code and Math-LM respectively. The full architecture parameters can be found in Table~\ref{tab:table9}. Notably, we found that HumanEval~\citep{chen2021codex} is very sensitive to parameter changes. When merging the LM with other models, performance dropped for all methods. This may be due to the nature of the task and the small test set of only 164 samples.













% compare to traditional merge
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{fig_version_0/fig5_version2.pdf}
    \caption{Visualization of LFS-searched models}
    \label{fig:fig3}
    \vspace{-.3cm}
\end{figure}

% \begin{table*}[h]
% \centering
% \small
% \caption{Performance comparison of Source Models and Fusion Strategies}
% \begin{tabular}{@{}l|llll|lll@{}}
% \toprule
% \multirow{2}{*}{\textbf{Benchmark}} & \multicolumn{4}{c|}{\textbf{Source Models}} & \multicolumn{3}{c}{\textbf{Fusion Results}} \\
% \cmidrule{2-8}
% & \textbf{Math} & \textbf{Code} & \textbf{LM} & \textbf{LM\_JA} & \textbf{LFS} & \textbf{DFS} & \textbf{LFS+DFS} \\
% \midrule
% LogiQA          & \textbf{29.65} & 25.81 & 28.57 & -- & 30.88 & 29.34 \small\textcolor{gray}{(-0.31)} & \\
% OpenBookQA      & 34.20 & \textbf{34.80} & 34.40 & -- & 37.60 \small\textcolor{blue}{(+2.80)}& \textbf{37.40} \small\textcolor{blue}{(+2.60)} &  38.6 \small\textcolor{blue}{(+3.80)}\\
% OpenBookQA+f    & 39.20 & 44.20 & \textbf{46.20} & -- & 47.00  \small\textcolor{blue}{(+0.80)} & \textbf{47.40} \small\textcolor{blue}{(+1.20)} & 47.8\small\textcolor{blue}{(+1.60)}\\
% PIQA            & 79.77 & 79.92 & \textbf{79.95} & -- & -- & 79.02 \small\textcolor{gray}{(-0.93)}  &  \\
% SocialIQA       & 46.82 & 46.77 & \textbf{51.09} & -- & -- & 50.50 \small\textcolor{gray}{(-0.59)} &  \\
% MGSM\_JA        & 8.00  & 4.00  & \textbf{10.40} & -- & \textbf{17.60} \small\textcolor{blue}{(+7.20)}  & \textbf{15.20} \small\textcolor{blue}{(+4.80)}  &  \\
% MGSM\_JA        & 8.00  & --  & -- & \textbf{8.80} & \textbf{16.40} \small\textcolor{blue}{(+7.60)} & \textbf{21.60} \small\textcolor{blue}{(+12.80)} & \\
% \bottomrule
% \end{tabular}
% \label{tab:table2}
% \end{table*}


% \begin{table*}[h]
% \centering
% \small
% \caption{Performance comparison of Source Models and Fusion Strategies}
% \begin{tabular}{@{}l@{\hspace{0.7em}}|@{\hspace{0.7em}}c@{\hspace{0.7em}}c@{\hspace{0.7em}}c@{\hspace{0.7em}}c|@{\hspace{1.2em}}c@{\hspace{1.2em}}c@{\hspace{1.2em}}c@{}}
% \toprule
% \multirow{2}{*}{\textbf{Benchmark}} & \multicolumn{4}{@{\hspace{0.7em}}c|@{\hspace{1.2em}}}{\textbf{Source Models}} & \multicolumn{3}{c}{\textbf{Merging Results}} \\
% \cmidrule[0.4pt]{2-5} \cmidrule[0.4pt]{6-8}
% & \makebox[1.2cm]{\textbf{Math}} & \makebox[1.2cm]{\textbf{Code}} & \makebox[1.2cm]{\textbf{LM}} & \makebox[1.2cm]{\textbf{LM\_JA}} & \makebox[1.4cm]{\textbf{LFS}} & \makebox[1.4cm]{\textbf{DFS}} & \makebox[1.6cm]{\textbf{LFS+DFS}} \\
% \midrule
% LogiQA          & 29.65 & 25.81 & 28.57 & -- & 30.88\small\textcolor{blue}{(+1.23)} & 29.34 \small\textcolor{gray}{(-0.31)} & \textbf{31.64}\small\textcolor{blue}{(+1.99)}\\
% OpenBookQA      & 34.20 & \textbf{34.80} & 34.40 & -- & 37.60 \small\textcolor{blue}{(+2.80)}& 37.40 \small\textcolor{blue}{(+2.60)} & \textbf{38.6} \small\textcolor{blue}{(+3.80)}\\
% OpenBookQA+f    & 39.20 & 44.20 & \textbf{46.20} & -- & 47.00  \small\textcolor{blue}{(+0.80)} & 47.40\small\textcolor{blue}{(+1.20)} & \textbf{47.8}\small\textcolor{blue}{(+1.60)}\\
% PIQA            & 79.77 & 79.92 & \textbf{79.95} & -- & 80.93\small\textcolor{blue}{(+0.98)} & 79.02 \small\textcolor{gray}{(-0.93)} & \\
% SocialIQA       & 46.82 & 46.77 & \textbf{51.09} & -- & 51.14 \small\textcolor{blue}{(+0.05)}& 50.50 \small\textcolor{gray}{(-0.59)} & \\
% MGSM\_JA        & 8.00  & 4.00  & \textbf{10.40} & -- & 17.60\small\textcolor{blue}{(+7.20)} & 15.20\small\textcolor{blue}{(+4.80)} & \textbf{22.00}\small\textcolor{blue}{(+10.60)}\\
% MGSM\_JA        & 8.00  & --  & -- & \textbf{8.80} & 16.40\small\textcolor{blue}{(+7.60)} & \textbf{21.60} \small\textcolor{blue}{(+12.80)} & \\
% \bottomrule
% \end{tabular}
% \label{tab:table2}
% \end{table*}


\begin{table*}[h]
\centering
\small
\caption{Performance of models merged via LFS and DIS on other Reasoning Tasks. LFS+DIS represents using LFS-merged model as an additional source model for DIS search. Numbers in blue indicate improvements over the best source model.}
\begin{tabular}{@{}l@{\hspace{0.7em}}|@{\hspace{0.7em}}c@{\hspace{0.7em}}c@{\hspace{0.7em}}c@{\hspace{0.7em}}c|@{\hspace{1.2em}}l@{\hspace{1.2em}}l@{\hspace{1.2em}}l@{}}
\toprule
\multirow{2}{*}{\textbf{Benchmark}} & \multicolumn{4}{@{\hspace{0.7em}}c|@{\hspace{1.2em}}}{\textbf{Source Models}} & \multicolumn{3}{l}{\textbf{Merging Results}} \\
\cmidrule[0.4pt]{2-5} \cmidrule[0.4pt]{6-8}
& \makebox[1.2cm]{\textbf{Math}} & \makebox[1.2cm]{\textbf{Code}} & \makebox[1.2cm]{\textbf{LM}} & \makebox[1.2cm]{\textbf{LM\_JA}} & \makebox[1.4cm]{\textbf{LFS}} & \makebox[1.4cm]{\textbf{DIS}} & \makebox[1.6cm]{\textbf{LFS+DIS}} \\
\midrule
LogiQA          & 29.65 & 25.81 & 28.57 & -- & 30.88 \small\textcolor{blue}{(+1.23)} & 29.34 \small\textcolor{gray}{(-0.31)} & \textbf{31.64} \small\textcolor{blue}{(+1.99)}\\
OpenBookQA      & 34.20 & \textbf{34.80} & 34.40 & -- & 37.60 \small\textcolor{blue}{(+2.80)}& 37.40 \small\textcolor{blue}{(+2.60)} & \textbf{38.60} \small\textcolor{blue}{(+3.80)}\\
OpenBookQA+f    & 39.20 & 44.20 & \textbf{46.20} & -- & 47.00  \small\textcolor{blue}{(+0.80)} & 47.40 \small\textcolor{blue}{(+1.20)} & \textbf{47.80} \small\textcolor{blue}{(+1.60)}\\
PIQA            & 79.77 & 79.92 & \textbf{79.95} & -- & \textbf{80.93} \small\textcolor{blue}{(+0.98)} & 79.02 \small\textcolor{gray}{(-0.93)} & 80.03 \small\textcolor{blue}{(+0.08)}\\
SocialIQA       & 46.82 & 46.77 & \textbf{51.09} & -- & \textbf{51.14} \small\textcolor{blue}{(+0.05)}& 50.50 \small\textcolor{gray}{(-0.59)} & 50.17 \small\textcolor{gray}{(-0.92)}\\
MGSM\_JA        & 8.00  & 4.00  & \textbf{10.40} & -- & 17.60 \small\textcolor{blue}{(+7.20)} & 15.20 \small\textcolor{blue}{(+4.80)} & \textbf{22.00} \small\textcolor{blue}{(+10.60)}\\
MGSM\_JA        & 8.00  & --  & -- & \textbf{8.80} & 16.40 \small\textcolor{blue}{(+7.60)} & \textbf{21.60} \small\textcolor{blue}{(+12.80)} & 19.60 \small\textcolor{blue}{(+10.80)}\\
\bottomrule
\multicolumn{8}{l}{\textsuperscript{*}LM\_JA denotes ELYZA-japanese-Llama-2-13b}\\
\end{tabular}
\label{tab:table1}
\vspace{-.2cm}
\end{table*}



\textbf{Depth-wise Integration} We evaluated two DIS search configurations: Configuration 1 using three candidate models (Code, Math, and LM) with depth granularity 1 and repeat factor 1; and Configuration 2 using a single candidate model (the corresponding source model for specific objective) with depth granularity 1 and repeat factor 2. Using these configurations, we similarly optimized for mathematical, coding, and general reasoning tasks respectively. DIS demonstrated effectiveness only in general reasoning tasks, yielding GEN-DIS-0 and GEN-DIS-1. Our results, as presented in Table~\ref{tab:table0}, demonstrate that both models achieved improvements,  exceeding 1\% over the source models on the MMLU benchmark. GEN-DIS-0 exhibited enhanced performance not only on the optimization objective but also demonstrated benefits on auxiliary tasks, showing improvement on the GSM8K. A detailed breakdown of performance across MMLU categories is presented in Table \ref{tab:table7}.

% In our exploration of the DIS search space across mathematical, code, and general reasoning tasks, we evaluated two search configurations: Configuration $1$, utilizing three candidate models (Code, Math, and LM) with depth granularity $1$ and a repeat factor of $1$; and Configuration $2$, employing a single candidate model (LM) with depth granularity $1$ and a repeat factor of $2$. Using these configurations with our layer retention strategy, we obtained GEN-DIS-0 and GEN-DIS-1 respectively. Our results, as presented in Table~\ref{tab:table0}, demonstrate that both models achieved improvements, exceeding 1\% over the baseline LM model on the MMLU benchmark. GEN-DIS-0 exhibited enhanced performance not only on the primary optimization objective but also demonstrated benefits on auxiliary tasks, including GSM8k. A detailed breakdown of performance across specific MMLU categories is presented in Table \ref{tab:table6}.


% \begin{table}[htbp]
% \centering
% \caption{Performance of task-specific Merged models searched by DIS on corresponding benchmarks.}
% \setlength{\tabcolsep}{4pt}  
% \begin{tabular}{l|c c c c}
% \hline
% Benchmark & Math & Code & LM & DIS-Searched \\
% \hline
% LogiQA & 29.65 & 25.81 & 28.57 & 29.34 \\
% OpenBookQA & 34.20 & 34.80 & 34.40 & \underline{37.4} \\
% OpenBookQA+f & 39.20 & 44.20 & 46.20 & \underline{47.4} \\
% %MMLU\_Gen & 49.92 & 45.95 & 47.52 & 50.13 (53.50)\\
% PIQA & 79.77 & 79.92 & 79.95 & 79.02\\
% %CommonsenseQA &  &  &  & \\
% MGSM\_JA & 8.00 & 4.00 & 10.40 & \underline{12.40} \\
% SocialIQA & 46.82 & 46.77 &  51.09 & 50.50 \\
% \hline
% \end{tabular}

% \label{tab:tabel4}
% \end{table}


% \begin{table}
%    \centering
%    \small
%    \caption{Ablation study on granularity ($G$) and component groups ($C$) for MATH-LFS model.}
%    \label{tab:table1}
%    \setlength{\tabcolsep}{2em}  % Increase column spacing
%    \begin{tabular}{c|cc}
%        \toprule
%        \multirow{2}{*}{\textbf{Layer Groups} ($G$)} & \multicolumn{2}{c}{\textbf{Component Groups}} \\
%        \cmidrule{2-3}
%        & $C$=3 & $C$=1 \\
%        \midrule
%        10 & 66.79 & 66.41 \\
%        4 & \textbf{68.46} & 68.08 \\
%        2 & 67.82 & 67.24 \\
%        1 & 66.41 & 66.41 \\
%        \bottomrule
%    \end{tabular}
% \end{table}




% \begin{table}[htbp]
% \centering
% \caption{Performance of task-specific Merged models searched by DIS on corresponding benchmarks}
% \begin{tabular}{@{}l|cccc@{}}
% \toprule
% \textbf{Benchmark} & \textbf{Math} & \textbf{Code} & \textbf{LM} & \textbf{DIS-Searched} \\
% \midrule
% LogiQA          & \textbf{29.65} & 25.81 & 28.57 & 29.34 \small\textcolor{gray}{(-0.31)} \\
% OpenBookQA      & 34.20 & \textbf{34.80} & 34.40 & \textbf{37.40} \small\textcolor{blue}{(+2.60)} \\
% OpenBookQA+f    & 39.20 & 44.20 & \textbf{46.20} & \textbf{47.40} \small\textcolor{blue}{(+1.20)} \\
% PIQA            & 79.77 & 79.92 & \textbf{79.95} & 79.02 \small\textcolor{gray}{(-0.93)} \\
% MGSM\_JA        & 8.00  & 4.00  & \textbf{10.40} & \textbf{15.20} \small\textcolor{blue}{(+4.80)} \\
% SocialIQA       & 46.82 & 46.77 & \textbf{51.09} & 50.50 \small\textcolor{gray}{(-0.59)} \\
% \bottomrule
% \end{tabular}
% \label{tab:table2}
% \end{table}



We visualize the network in Figure~\ref{fig:fig4}. The complete set of architectural parameters can be found in Table~\ref{tab:table10}. GEN-DIS-0 exhibited a predominantly stable selection of LM layers in early layers, gradually incorporating other source model layers with stacking behavior in middle and later layers.  Similarly, GEN-DIS-1 demonstrated no layer repetition in early layers but showed emergence of layer repetition patterns after layer $15$. Notably, although improvements were observed in common reasoning task, we found no superior configurations for mathematical and code reasoning tasks, suggesting varying sensitivity to layer ordering across different tasks.  To further explore our methodology, we extend our investigation to additional tasks.

%Notably, the effectiveness of DIS search varied across different optimization tasks.



% \begin{table}[htbp]
%     \centering
%     \caption{Benchmark performance of GEN-DIS Search with layer deletion}
%     \setlength{\tabcolsep}{4pt}  % 减小列间距
%     \begin{tabular}{l|ccc}
%         \hline
%         Benchmark & GEN-DIS-2 & GEN-DIS-3 \\
%         \hline
%         MMLU & 53.43 & 53.68 \\
%         GSM8K & 3.79  & 1.82 \\
%         MATH & 0.00  & 0.02 \\
%         MBPP & 33.6 & 9.8 \\
%         Human Eval & 38.41 & 4.88 \\
%         \hline
%     \end{tabular}
    
%     \label{tab:table3}
% \end{table}



%We hypothesize this disparity stems from MMLU's perplexity-based metric, which differs fundamentally from mathematical and code task evaluations. A detailed analysis of these findings is presented in the analysis section.



% \begin{table}[htbp]
% \centering
% \caption{Performance of task-specific Merged models searched by DIS on corresponding benchmarks.}
% \setlength{\tabcolsep}{4pt}  
% \begin{tabular}{l|c c c c}
% \hline
% Benchmark & Math & Code & LM & DIS-Searched \\
% \hline
% LogiQA & 29.65 & 25.81 & 28.57 & 29.34 \\
% OpenBookQA & 34.2 & 34.8 & 34.4 & \underline{37.4} \\
% OpenBookQA\_wf & 39.2 & 44.2 & 46.2 & \underline{47.4} \\
% %MMLU\_Gen & 49.92 & 45.95 & 47.52 & 50.13 (53.50)\\
% PIQA & 79.77 & 79.92 & 79.95 & 79.02\\
% %CommonsenseQA &  &  &  & \\
% SocialIQA & 46.82 & 46.77 &  51.09 & 50.50 \\
% \hline
% \end{tabular}

% \label{tab:tabel4}
% \end{table}








% \begin{table*}[htbp]
% \centering
% \small
% \caption{Performance comparison of Source Models and Fusion Strategies}
% \begin{tabular}{@{}l|cccc|ccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{Benchmark}} & \multicolumn{4}{c|}{\textbf{Source Models}} & \multicolumn{3}{c}{\textbf{Fusion Results}} \\
% \cmidrule{2-8}
% & \textbf{Math} & \textbf{Code} & \textbf{LM} & \textbf{LM\_JA} & \textbf{LFS} & \textbf{DFS} & \textbf{LFS+DFS} \\
% \midrule
% LogiQA          & \textbf{29.65} & 25.81 & 28.57 & -- & 30.88 & 29.34 \small\textcolor{gray}{(-0.31)} & \\
% OpenBookQA      & 34.20 & 34.80 & 34.40 & -- & 37.60 & \textbf{37.40} \small\textcolor{blue}{(+2.60)} &  \\
% OpenBookQA+f    & 39.20 & 44.20 & 46.20 & -- & 47.00 & \textbf{47.40} \small\textcolor{blue}{(+1.20)} &  \\
% PIQA            & 79.77 & 79.92 & 79.95 & -- & -- & 79.02 \small\textcolor{gray}{(-0.93)}  &  \\
% SocialIQA       & 46.82 & 46.77 & 51.09 & -- & -- & 50.50 \small\textcolor{gray}{(-0.59)} &  \\
% MGSM\_JA        & 8.00  & 4.00  & 10.40 & -- & \textbf{17.60} \small\textcolor{blue}{(+7.20)}  & \textbf{15.20} \small\textcolor{blue}{(+4.80)}  &  \\
% MGSM\_JA        & 8.00  & --  & -- & 8.80 & \textbf{16.40} \small\textcolor{blue}{(+7.60)} & \textbf{21.60} \small\textcolor{blue}{(+12.80)} & \\
% \bottomrule
% \end{tabular}
% \label{tab:table2}
% \end{table*}






\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig_version_0/fig4new.pdf}
    \vspace{-.35cm}
    \caption{Visualization of DIS model architecture. The x-axis represents block indices from 1 to 40, and the y-axis indicates the selected layer in each block. }
    % The dashed lines denote layer connectivity in sequential order. For detailed model parameter configurations, please refer to Table 2 in the Appendix.
    \label{fig:fig4}
    \vspace{-.35cm}
\end{figure}

\textbf{Expanding to other tasks} We further evaluated our method on diverse reasoning tasks including LogicQA~\citep{liu2020logiqa}, OpenBookQA~\citep{mihaylov2018can}, PIQA~\citep{bisk2020piqa}, SocialIQA~\citep{sap2019socialiqa}, and MGSM Japanese~\citep{sap2019socialiqa}, using validation datasets for searching and test sets for evaluation. For OpenBookQA, we tested with and without relevant facts in the prompt. We incorporated ELYZA-japanese-Llama-2-13b~\citep{elyzallama2023} alongside our base models (LM, Math, Code) for MGSM Japanese tasks.  See Section~\ref{sec:a6} for implementation details.  Results are presented in Table~\ref{tab:table1}. For most tasks, LFS+DFS shows promising results with significant improvements across benchmarks, notably 3.80\% on OpenBookQA and up to 10.8 points on MGSM\_JA. In addition to LFS that consistently delivers positive improvements, DFS shows occasional degradation in LogiQA and PIQA. The results particularly highlight the effectiveness of our method in cross-lingual reasoning tasks, DIS demonstrates notable effectiveness with improvements up to 12.80 points, supporting our hypothesis that DIS excels when handling models trained on complementary domains. The architectural parameters can be found in Tables~\ref{tab:table11} and~\ref{tab:table12}.

% LFS and DIS represent complementary search spaces with distinct advantages. LFS excels when models have learned similar concepts with varying layer-level strengths, enabling fine-grained combination of layer-specific features. In contrast, DIS is particularly effective when models are trained on different but complementary domains, as it preserves complete processing blocks and enables optimization of diverse sequential processing paths. 


%  We evaluated our method on diverse tasks including LogicQA\citep{liu2020logiqa}, OpenBookQA\citep{mihaylov2018can}, PIQA\citep{bisk2020piqa}, SocialIQA\citep{sap2019socialiqa}, and MGSM Japanese\citep{sap2019socialiqa}. For OpenBookQA, we tested with and without relevant facts in the prompt. We use the corresponding validation datasets as searching datasets and performed evaluations on the test sets. For source model selection, in addition to the models we introduced before (LM, Math, Code), we incorporated a Japanese language model  ELYZA-japanese-Llama-2-13b \citep{elyzallama2023} for MGSM Japanese tasks. Please see Section~\ref{sec:a7} for implementation details. Results are presented in Table~\ref{tab:table1}. For most tasks, LFS+DFS shows promising results with significant improvements across benchmarks. Notable examples include OpenBookQA with a 3.80\% improvement and MGSM\_JA with gains up to 12.80 points. While DFS occasionally shows slight degradation in LogiQA and PIQA, LFS consistently delivers positive improvements. DIS is particularly effective in cross-lingual reasoning tasks, showing significant improvements across configurations with gains up to 12.80 points.  which proved the 、eDIS
% proves more effective for models trained on complementary
% domains by preserving processing blocks and optimizing
% sequential pathways.




\subsubsection{Multi objective Optimization} Given the task-specific nature of DIS search and the more robust merging capabilities of LFS, we conducted our multi objective search on LFS with three optimization objectives: mathematical reasoning, code generation, and general reasoning. Our optimization yielded five Pareto-optimal solutions, denoted as MULTI-LFS-0 to MULTI-LFS-4, with their performance metrics presented in Table~\ref{tab:table0}. These solutions achieved significant improvements, ranging from 5.30 to 6.86 points on average compared to the best base model. When comparing our results with existing merging approaches such as Ties, task arithmetic, and linear merging, our method achieves better trade-offs along the Pareto frontier while maintaining higher average performance across all benchmarks. This demonstrates the effectiveness of our multi-objective optimization strategy in finding superior model configurations that balance diverse task requirements.

% When further compared to other merging approaches, our method consistently outperforms existing approaches, achieving better trade-offs along the Pareto frontier while maintaining higher average performance across all benchmarks. This demonstrates the effectiveness of our multi-objective optimization strategy in finding superior model configurations that balance diverse task requirements.

As visualized in Figure~\ref{fig:fig3} (b), our analysis of MULTI-LFS-4 reveals an interesting layer-wise merging pattern: Task Arithmetic is optimal for groups 1 and 2, while groups 3 and 4 employ Ties and linear merging strategies. Unlike the single-objective search-derived MATH-LFS, MULTI-LFS-4 shows a preference for incorporating layers from all source models during the merging process, resulting in better preservation of comprehensive information.

% In our multi-objective optimization, we identify five Pareto-optimal solutions, denoted as MULTI-LFS-0 to MULTI-LFS-4, with their performance metrics presented in Table~\ref{tab:table0}. Compared to single-objective optimization, these multi-objective solutions demonstrate more comprehensive capabilities, maintaining competent performance across all five benchmarks. The results validate that our multi-objective optimization approach effectively balances performance across diverse tasks while preserving model competencies in each domain. When compared to alternative merging approaches, our method consistently outperforms existing approaches, achieving better trade-offs along the Pareto frontier while maintaining higher average performance across all benchmarks. This demonstrates the effectiveness of our multi-objective optimization strategy in finding superior model configurations that balance diverse task requirements.

% Our visualization of MULTI-LFS-4 in Figure  \ref{fig:fig4}  reveals that Task Arithmetic is optimal for layers $0-10$ and $0-20$, while layers $20-30$ and $30-40$ adopt Ties and linear merging strategies. In contrast to the single-objective search-derived MATH-LFS, MULTI-LFS-4 shows a preference for incorporating layers from all source models during the merging process, resulting in better preservation of comprehensive information.










\begin{table}[htbp]
\centering
\small
\caption{Distribution of searching trials across different budget levels for MATH-LFS and GEN-DIS-1.}
\begin{tabular}{@{}l|c@{\hspace{5pt}}c@{\hspace{5pt}}c|c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{}}
\toprule
& \multicolumn{3}{c|}{\textbf{MATH-LFS}} & \multicolumn{4}{c}{\textbf{GEN-DIS-1}} \\
& 100 & 300 & 1000 & 0 & 100 & 300 & 700 \\
\midrule
Trail Count & 263 & 152 & 85 & 196 & 311 & 307 & 186 \\
Percentage (\%) & 52.6 & 30.4 & \textbf{17.0} & 19.6 & 31.1 & 30.7 & \textbf{18.6} \\
\bottomrule
\end{tabular}
\label{tab:table2}
\end{table}


\subsubsection{Efficiency Analysis}

% Our Multi-Fidelity Optimization (MFO) framework dynamically adjusts the budget allocation during the search process, where the budget is defined as the validation dataset size used for evaluation. Analysis of the distribution of search trials for MATH-LFS and GEN-DIS-1 (detailed in Table~\ref{tab:search_cost}) reveals that only $17.0\%$ of search trials utilized the full budget in MATH-LFS. For DIS-1, considering that DIS search tends to explore deeper architectures, we imposed a maximum layer constraint of $50$ to manage computational costs. Configurations exceeding this depth constraint were assigned a reward of $0$ without evaluation, resulting in zero budget allocation. Consequently, out of the initial $1000$ trials in DIS, $800$ were effective search trials, with only $18.6\%$ utilizing the full evaluation budget.

Our Multi-Fidelity Optimization (MFO) framework dynamically adjusts the budget allocation during the search process, where the budget is defined as the validation dataset size used for searching. We implemented different search iterations and budget ranges based on the search space size and specific tasks. Here, we analyze the budget distribution during the search process using MATH-LFS and GEN-DIS-1 as examples (see Table \ref{tab:table8} for complete budget details).
As shown in Table~\ref{tab:table2}, for MATH-LFS, we conducted 500 search trials with a budget range of (100, 1000), divided into three levels: 100, 300, and 1000. Our analysis reveals that only 17\% of the search trials utilized the full budget, while over 52\% of the evaluations were conducted with the minimum budget of 100.
For GEN-DIS-1, considering that DIS search typically explores deeper architectures, we imposed a maximum layer constraint of $50$ to manage computational costs. Configurations exceeding this depth constraint were assigned a reward of $0$ without evaluation, resulting in zero budget allocation. Consequently, out of the initial $1000$ trials in DIS, $804$ were effective search trials, with only $18.6\%$ utilizing the full evaluation budget, showing that our multi-fidelity approach significantly reduced computational resource consumption during the search process while consistently discovering high-quality solutions.


\section{Ablations and Analysis}



% \subsection{Search on other models}
% We expanded our investigation to include a diverse set of tasks: LogicQA\citep{liu2020logiqa}, OpenBookQA\citep{mihaylov2018can}, PIQA\citep{bisk2020piqa}, SocialIQA\citep{sap2019socialiqa}, and MGSM Japanese\citep{sap2019socialiqa}. Please see Section \ref{sec:a7} for more details. Results are presented in Table \ref{tab:table2}. For OpenBookQA, we conducted two sets of experiments: one with relevant facts included in the prompt (OpenBookQA+F) and one without (OpenBookQA). The DIS approach demonstrated positive outcomes in several cases, achieving improvements of 3\% on OpenBookQA, 1\% on OpenBookQA with fact in prompt, and 2\% on MGSM\_JA. However, for the remaining evaluated tasks, the search did not yield superior configurations, highlighting the task-dependent nature of DIS effectiveness.


% To further evaluate the potential of the DIS search space, we conducted additional experiments using MGSM-JA (Mathematical Grade School Problems - Japanese) as our optimization objective. We modified our source models to ELYZA-japanese-Llama-2-13b\citep{elyzallama2023} and Wizard Math. This change was motivated by our observation of Wizard LM's limited Japanese language capabilities. As shown in our results table \ref{tab:table4}, our approach achieved a 5\% improvement in performance.

\subsection{Impact of granularity in LFS}
To examine the effect of granularity in layer-wise fusion search space, we conducted ablation studies to examine the effect of granularity in LFS by varying the number of layer groups ($G$) and component groups ($C$). Table~\ref{tab:table3} shows that increasing $G$ from $1$ to $4$ consistently improves GSM8K accuracy, demonstrating the benefits of more fine-grained layer control. However, performance decreases when $G$ reaches $10$, likely due to the growth in search space exceeding our search algorithm's capability within the given trials. Our analysis also shows that increasing $C$ from $1$ to $3$ improves performance, though these gains are smaller compared to those from layer-wise refinement.
\begin{table}[t]
   \centering
   \small
   \caption{Ablation study on granularity ($G$) and component groups ($C$) for MATH-LFS search.}
   \begin{tabular}{c|cccc}
       \toprule
       \multirow{2}{*}{\textbf{Component Groups}} & \multicolumn{4}{c}{\textbf{Layer Groups} ($G$)} \\
       \cmidrule{2-5}
       & 10 & 4 & 2 & 1 \\
       \midrule
       $C$=3 & 66.79 & \textbf{68.46} & 67.82 & 66.41 \\
       $C$=1 & 66.41 & 68.08 & 67.24 & 66.41 \\
       \bottomrule
   \end{tabular}
\label{tab:table3}
\vspace{-.3cm}
\end{table}

\begin{table}[t]
\centering
\caption{Ablation study on layer retention for GEN-DIS-1 model. GEN-DIS-1-NR denotes GEN-DIS-1 without layer retention.}
\setlength{\tabcolsep}{2pt}
%\renewcommand{\arraystretch}{0.8}
\small
\begin{tabular}{c|lllll}
\toprule
\textbf{Model} & \textbf{MMLU} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} \\
\midrule
LM & \textbf{53.43} & 3.79 & 0.00 & 33.40 & 38.41 \\
GEN-DIS-1 & \textbf{54.76}\textcolor{blue}{↑}  & 1.74 & 0.06 & 16.80 & 18.29 \\
GEN-DIS-NR & \textbf{53.68}\textcolor{blue}{↑}  & 1.82 & 0.02 & 9.80 & 4.88 \\
\bottomrule
\end{tabular}
\label{tab:table4}
\vspace{-.2cm}
\end{table}
\subsection{Impact of layer retention in DIS}

% \begin{table}[htbp]
% \centering
% \caption{Benchmark Performance of GEN-DIS Search with Layer Deletion}
% \small 
% \setlength{\tabcolsep}{3pt} 
% \begin{tabular}{l|ccc}  % 增加一列
% \toprule
% \textbf{Benchmark} & \textbf{Base} & \textbf{GEN-DIS-2} & \textbf{GEN-DIS-3} \\
% \midrule
% MMLU & 53.43 & 53.43 \small\textcolor{gray}{(+0.00)} & 53.68 \small\textcolor{blue}{(+0.25)} \\
% GSM8K & 4.21 & 3.79 & 1.82 \\
% MATH & 0.03 & -- & 0.02 \\
% MBPP & 35.20 & 33.60 & 9.80 \\
% Human Eval & 40.12 & 38.41 & 4.88 \\
% \bottomrule
% \end{tabular}
% \label{tab:table7}
% \end{table}




To explore the layer retention strategy in our DIS search space, we conducted a comparative experiment. Specifically, we modified the DIS-GEN-1 configuration by replacing the layer retention strategy with direct layer deletion, resulting in the DIS-GEN-RN.
As shown in Table \ref{tab:table4},  although DIS-GEN-RN marginally outperformed the baseline language model on MMLU tasks, its performance still fell short compared to our proposed layer retention approach. This result shows that directly deleting layers degrades model performance while retaining layers is more effective.

\begin{table}[t]
\centering
\small
\caption{Performance comparison of source models and SFS-search results}
\begin{tabular}{@{}l@{\hspace{5pt}}|@{\hspace{5pt}}l@{\hspace{5pt}}l@{\hspace{5pt}}l@{\hspace{5pt}}|@{\hspace{5pt}}l@{\hspace{5pt}}l@{\hspace{5pt}}l@{}}
\toprule
\textbf{Benchmark} & \multicolumn{3}{c|}{\textbf{Source Models}} & \multicolumn{3}{c}{\textbf{SFS Results}} \\
\cmidrule{2-7}
& \textbf{Math} & \textbf{Code} & \textbf{LM} & \textbf{SFS-0} & \textbf{SFS-1} & \textbf{SFS-2} \\
\midrule
MMLU & 52.04 & 52.79 & \textbf{53.43} & 52.03 & 51.91 & \textbf{53.63}\textcolor{blue}{↑}\\
GSM8K & \textbf{64.22} & 0.00 & 3.79 & \textbf{63.91}\textcolor{gray}{↓} & 0.00 & 6.52 \\
MATH & 13.70 & 0.00 & 0.00 & 13.58 & 0.00 & 0.08 \\
MBPP & 18.20 & \textbf{27.20} & 33.40 & 18.20 & \textbf{28.60}\textcolor{blue}{↑} & 30.40 \\
HumanEval & 7.32 & 23.17 & 38.41 & 7.32 & 24.39 & 34.15 \\
\bottomrule
\end{tabular}
\label{tab:table5}
\vspace{-.3cm}
\end{table}
\subsection{Search only on scales}
%
% Growing research focuses on identifying task-specific neurons through various methods: gradient-based approaches \citep{panigrahi2023task} provide precise insights but are computationally intensive, while methods leveraging forward pass information through activation analysis \citep{tang2024language} or combined weight-activation importance calculations \citep{christ2024math} offer greater efficiency. These neuron-specific studies enhance model interpretability and demonstrate practical benefits: fine-tuning only task-specific neurons \citep{zhang2024interpreting} or simply scaling them \citep{christ2024math} can improve task performance.
%
%
%
%
% Recent studies have identified task-specific neurons through gradient-based methods~\citep{panigrahi2023task}, activation analysis~\citep{tang2024language}, and weight-activation calculations~\citep{christ2024math}. 
Several findings show that fine-tuning~\citep{zhang2024interpreting} or scaling~\citep{christ2024math} task-specific neurons can improve model performance. To evaluate these claims and to verify that our DIS search space (which includes scales) is not simply re-scaling layers, we further evaluate Scale-Factor Search Space (SFS) that optimizes tasks by searching scaling factors to weights and layer outputs. As before, we apply our framework and obtain three SFS models: SFS-MATH, SFS-CODE, and SFS-GEN, each initialized from specialized base models. As shown in Table~\ref{tab:table5}, Results show a decline in mathematical performance and slight improvements in code and reasoning tasks, though gains are modest compared to LFS and DIS, showing that scale optimization alone is not sufficient to explain the DIS effectiveness.

% Recent research has identified task-specific neurons through various approaches, including computationally intensive gradient-based methods \citep{panigrahi2023task}, efficient forward-pass activation analysis \citep{tang2024language}, and balanced weight-activation importance calculations \citep{christ2024math}. These neuron-specific studies demonstrate practical benefits: fine-tuning task-specific neurons \citep{zhang2024interpreting} or scaling them \citep{christ2024math} can enhance model performance. Motivated by these findings, we propose a Scale-Factor Search Space (SFS) that enables efficient task-specific optimization by applying scaling factors to both weights and layer outputs. We evaluate three SFS variants initialized from different base models: MATH, CODE, and GEN, optimized for mathematical, code, and general reasoning tasks, respectively. Our experimental results in Table~\ref{tab:table3} indicate that while mathematical task performance remains unchanged, code and reasoning tasks show marginal improvements. However, these gains are notably modest compared to the LFS search space.






\section{Conclusions and Future Work}
% forward searching， slow converge speed, gradient-based method
% large search space， better optimization

In this work, we presented a Multi-Fidelity Framework for Automated Model Merging that introduces two complementary search spaces: Layer-wise Fusion Search Space (LFS) and Depth-wise Integration Search Space (DIS). LFS enables fine-grained layer-wise merging, and DIS optimizes sequential layer arrangements while preserving individual layer weights. We show automated model merging not only works, but is quite effective, demonstrating strong performance in both single-objective and multi-objective scenarios, achieving a 4.24\% improvement on the GSM8k challenge task with only 17\% of the full budget within 500 trials, and a 6.86\% improvement in multi-objective performance using 18.6\% of the full budget within 1000 trials. When extended to various benchmarks, our method consistently shows promising results without any additional tuning. Overall, our work provides an efficient and flexible framework for automated model merging that achieves effective improvements with reduced computational costs.

% \section*{Impact Statement}
% This work introduces an efficient model merging framework that reduces the need for additional fine-tuning. Our framework facilitates reducing computational costs and manual effort, offering advantages in accessibility and environmental sustainability. However, users should be aware that merged models may inherit biases from source models. We encourage thorough evaluation across diverse scenarios before deployment, particularly when combining models from different domains.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Detailed Experimental Settings}
\subsection{Details of TIES Configuration for Math and Code Model Merging on GSM8K} \label{sec:a}
\begin{table}[htbp]
\centering
\caption{Performance Comparison with Different Parameters of Ties Merging Method}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|cccccc}
\toprule
\textbf{Parameters} & \multicolumn{6}{c}{\textbf{Configuration}} \\
\midrule
\textbf{Ratio to retain parameters} & 0.7 & 0.5 & 0.7 & 0.5 & 0.9 & 0.9 \\
\textbf{Scaling Coefficient} & 1.0 & 0.5 & 0.5 & 1.0 & 0.5 & 1.0 \\
\midrule
\textbf{Performance on GSM8k} & 61.94 & 52.08 & 46.78 & 64.52 & 34.87  & 49.05  \\
\bottomrule
\end{tabular}
\label{tab:ties_performance}
\end{table}
\subsection{Descriptions of Existing Model Merging Methods in Layer-wise Fusion Search Space (LFS)} \label{sec:a0}
\textbf{Task Arithmetic} enhance model capabilities through vector operations by leveraging weighted combinations of task-specific knowledge. Given a base model with weights $\theta_{\text{pre}}$ and task-specific fine-tuned weights $\{\theta_{t}^{\text{ft}}\}_{t=1}^n$, task vectors are defined as:

\begin{equation}
\tau_t = \theta_{t}^{\text{ft}} - \theta_{\text{pre}}
\end{equation}

The merged weights are then computed through:

\begin{equation}
\theta_{\text{Merge}} = \theta_{\text{pre}} + \lambda \sum_{t=1}^n \tau_t
\end{equation}

where $\lambda$ controls the magnitude of task-specific adaptations.

\textbf{TIES-Merging} is a parameter conflict resolution approach that operates in three stages. First, select the top $k\%$ parameters by magnitude of each task vector $\tau_t$:

\begin{equation}
\hat{\tau}_t = \text{TopK}(\tau_t, k)
\end{equation}

Next, Generating a consensus sign vector by examining the aggregate direction of parameter changes across all tasks:

\begin{equation}
\hat{\gamma} = \text{sgn}\left(\sum_{t=1}^n \hat{\tau}_t\right)
\end{equation}

Finally, computing the average update magnitude considering only those task vectors whose signs align with the consensus direction:

\begin{equation}
\tilde{\tau} = \text{Average}(\{\hat{\tau}_t : \text{sgn}(\hat{\tau}_t) = \hat{\gamma}\})
\end{equation}

The final merged model weights are then computed as:

\begin{equation}
\theta_{\text{Merge}} = \theta_{\text{pre}} + \lambda * \tilde{\tau}
\end{equation}

\textbf{SLERP} (Spherical Linear Interpolation ) computes optimal geodesic paths between model weights through:

\begin{equation}
\text{SLERP}(\theta_1, \theta_2, t) = \frac{\sin((1-t)\omega)}{\sin(\omega)}\theta_1 + \frac{\sin(t\omega)}{\sin(\omega)}\theta_2
\end{equation}

where $\omega = \arccos\left(\frac{\langle\theta_1, \theta_2\rangle}{\|\theta_1\|\|\theta_2\|}\right)$ and $t \in [0,1]$ is the interpolation parameter.

\textbf{Linear Merging} implements straightforward weighted averaging:

\begin{equation}
\theta_{\text{Linear}} = \sum_{t=1}^n w_t\theta_t
\end{equation}

where $\sum_{t=1}^n w_t = 1$ and $w_t \geq 0$.

% \begin{table}[htbp]
%     \centering
%     \caption{Performance Comparison with Different Parameters of Ties Merging Method}
%     \begin{tabular}{ccccccc}
%     \toprule
%     \textbf{Parameters} & \multicolumn{6}{c}{\textbf{GSM8k}} \\
%     \midrule
%     \textbf{Mask Rate} & 0.7 & 0.5 & 0.7 & 0.5 & 0.9 & 0.9 \\
%     \textbf{Scaling Coefficient} & 1.0 & 0.5 & 0.5 & 1.0 & 0.5 & 1.0 \\
%     \textbf{Score} & 61.94 & 52.08 & 46.78 & 64.52 & 34.87 & 49.05 \\
%     \bottomrule
%     \end{tabular}
%     \label{tab:ties_performance}
% \end{table}


\subsection{Descriptions of SMAC-based Multi-Fidelity Optimization} \label{sec:a1}
Our implementation builds upon SMAC~\citep{JMLR:v23:21-0888}, which combines Hyperband (HB)~\citep{li2018hyperband} and Bayesian Optimization (BO)~\citep{snoek2012practical}, utilizing Random Forest~\citep{breiman2001random} as the surrogate model.
Let $b_{\text{max}}$ and $b_{\text{min}}$ denote the maximum and minimum budgets respectively, and $\eta > 1$ be a budget spacing parameter.  SMAC determines $s_{\text{max}} = \lfloor\log_\eta(R)\rfloor$ brackets. Each bracket $i$ begins with $n_i = \lfloor\eta^{s_{\text{max}}-i} \cdot \frac{\eta}{\eta-1}\rfloor$ configurations and executes Successive Halving across $\lfloor \log_\eta(\frac{n_i}{n_{\text{min}}}) \rfloor + 1$ rounds. Each round evaluates all configurations with budget $b$, retains the top $\lfloor \frac{n_i}{\eta^l} \rfloor$ performers, and increases their budget to $\eta b$ for the next round, where $l$ denotes the current round. This process continues until reaching either a single configuration or $b_{\text{max}}$. The Random Forest model incorporates configuration-performance pairs from previous evaluations, updating before each Successive Halving iteration using evaluations from all budget levels while prioritizing data from the largest available budget. The model guides configuration selection through Expected Improvement, enabling efficient exploration while maintaining evaluation quality. As optimization progresses, more configurations undergo evaluation at higher budgets, allowing the model to overcome potential misguided conclusions from lower-fidelity evaluations by ultimately relying on high-fidelity results. This integration of Hyperband's resource allocation with Bayesian optimization's surrogate modeling enables efficient exploration of the configuration space while maintaining evaluation quality through principled multi-fidelity optimization.


\subsection{Details of Datasets Information} \label{sec:a2}
For searching, We use data from three reasoning domains: $1,000$ GSMPlus~\citep{li2024gsm} samples, an adversarial variant of GSM8K with mathematical perturbations for testing math reasoning; $300$ MBPP~\citep{austin2021program} samples ($200$ training/$100$ validation) for code understanding; and $700$ samples from MMLU validation\citep{hendrycks2020measuring} for general reasoning. These datasets support both single-objective optimization when used separately and multi-objective optimization when combined. For comprehensive performance evaluation, we employ established benchmark test sets across three key domains: mathematical reasoning using the complete test sets from GSM8K~\citep{cobbe2021training} and MATH~\citep{hendrycksmath2021}, code generation using the standard test splits from MBPP~\citep{austin2021program} and HumanEval~\citep{chen2021codex}, and general knowledge using the  MMLU test set~\citep{hendrycks2020measuring}, which spans diverse knowledge domains. 


\subsection{Evaluation Metrics and details} \label{sec:a3}
We evaluate using domain-specific metrics: accuracy for MMLU multiple choice questions based on loglikelihood, zero-shot accuracy for GSM8K and MATH, and Pass@1 for HumanEval and MBPP. We run all evaluations using LM Evaluation Harness\citep{eval-harness} with vLLM\citep{kwon2023efficient} acceleration. For consistency, we use fixed parameters across all tests: batch size $16$, temperature $0.0$ for greedy decoding, and maximum generation length of $1,024$ tokens for GSM8K and $2,048$ tokens for other datasets. All experiments run on NVIDIA Tesla A100 GPUs.

\subsection{Details of Search Spaces} \label{sec:a4}
Within LFS, we define specific parameter ranges for each merging method: Task Arithmetic utilizes task vector weights $\lambda \in [0,1]$; TIES-Merging combines task vector weights $\lambda \in [0,1]$ with a ratio to retain parameters $k \in [0.1,0.99]$; Linear-merging optimizes model coefficients $w_t \in [0,1]$ subject to $\sum_i w_t = 1$; and Slerp employs an interpolation parameter $t \in [0,1]$. 


\subsection{Details of Grid Search on Hyperparameters of base Model Merging Methods} \label{sec:a5}

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{0.8}  
\caption{Search Ranges of Hyperparameters for Different Model Merging Methods}
\begin{tabular}{l|p{10cm}}
\toprule
\textbf{Model Merging Methods} & \textbf{Search Ranges of Hyperparameters} \\
\midrule
Task Arithmetic & Scaling term to merge parameters: [0.5, 1.0] \\
\midrule
Linear Merging & Scaling term to merge parameters: $[1/n]$ ($n$: number of models) \\
\midrule
TIES-Merging & \begin{tabular}[t]{@{}l@{}}
Scaling term to merge parameters: [0.5, 1.0]\\[0.5ex]
Ratio to retain parameters with largest-magnitude values: [0.5, 0.7, 0.9]
\end{tabular} \\
\bottomrule
\end{tabular}
\label{tab:table6}
\end{table}

\subsection{Details of search on other Reasoning Tasks} \label{sec:a6}
\textbf{LogiQA} is derived from the logical comprehension section of China's National Civil Servants Examination, specifically designed to evaluate candidates' critical thinking and problem-solving capabilities. For our search implementation, we utilize the validation dataset with a budget range of $100-651$.

\textbf{OpenBookQA} is used to measure deep understanding of both subject matter and language comprehension. The dataset comes with an "open book" of fundamental facts. We conducted experiments both with and without facts in the prompt. Our search employs the validation dataset with a budget range of $100-500$.

\textbf{PIQA (Physical Interaction: Question Answering)} serves as a benchmark dataset for physical commonsense reasoning, with a particular focus on everyday situations and unconventional solutions. We have sampled 1,000 examples from the validation dataset for our search purposes, setting the budget range at $100-1,000$.

\textbf{SocialIQA} stands as a comprehensive benchmark for testing social commonsense intelligence, this dataset evaluates understanding of human actions and their social implications in everyday situations. Our search implementation uses a 1,000-sample subset from the validation data, with a budget range of $100-1,000$.

\textbf{MGSM (Multilingual Grade School Math Benchmark)} is a benchmark of grade-school math problems The same 250 problems from GSM8K are each translated via human annotators in 10 languages. 
we use 1,069 mathematics problems and solutions translated to japanese from the GSM8K test set by Sakana AI for searching ,set a budget range of $100-1000$.


\section{Additional Experimental Results.}
\subsection{Detailed breakdown of performance across specific MMLU categories of GEN-DIS-0 and GEN-DIS-1} \label{sec:b0}

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{0.95}  
\caption{Performance Comparison on MMLU Subject Categories between LM and GEN-DIS}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|lll}
\toprule
\textbf{MMLU Category} & \textbf{LM} & \textbf{GEN-DIS-0} & \textbf{GEN-DIS-1} \\
\midrule
Social Sciences & 62.24 & 63.24 \small\textcolor{blue}{(+1.00)} & \textbf{63.69} \small\textcolor{blue}{(+1.45)} \\
Humanities & 49.52 & \textbf{51.31} \small\textcolor{blue}{(+1.79)} & 51.09 \small\textcolor{blue}{(+1.57)} \\
STEM & 42.82 & 43.67 \small\textcolor{blue}{(+0.85)} & \textbf{44.37} \small\textcolor{blue}{(+1.55)} \\
Other & 61.31 & \textbf{62.66} \small\textcolor{blue}{(+1.35)} & 62.12 \small\textcolor{blue}{(+0.81)} \\
\bottomrule
\end{tabular}
\label{tab:table7}
\end{table}

\subsection{Additional result of Budget Distribution } \label{sec:b1}
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{0.95}  
\caption{Budget Distribution Across Models}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l|r|r|r|r|r|r}
\toprule
\textbf{Budget} & \textbf{MATH-LFS} & \textbf{CODE-LFS} & \textbf{Multi-LFS} & \textbf{GEN-LFS} & \textbf{GEN-DIS-0} & \textbf{GEN-DIS-1} \\
\midrule
100 & 263 (47.2\%) & 295 (59.0\%) & 240 (48.0\%) & 207 (41.4\%) & 163 (16.3\%) & 311 (31.1\%) \\
300 & 152 (30.4\%) & 205 (41.0\%) & 161 (32.2\%) & 181 (36.2\%) & 165 (16.5\%) & 307 (30.7\%) \\
1000 & 85 (17.0\%) & - & 99 (19.8\%) & 112 (22.4\%) & 109 (10.9\%) & 186 (18.6\%) \\
0 & - & - & - & - & 563 (56.3\%) & 196 (19.6\%) \\
\bottomrule
\end{tabular}
\label{tab:table8}
\end{table}



% \begin{table}[htbp]
% \centering
% \caption{Budget Distribution Across Models}
% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{l|r|r}
% \toprule
% \textbf{Model} & \textbf{Budget} & \textbf{Count (\%)} \\
% \midrule
% \multirow{3}{*}{MATH-LFS} & 100 & 263 (47.2\%) \\
% & 300 & 152 (30.4\%) \\
% & 1000 & 85 (17.0\%) \\
% \midrule
% \multirow{2}{*}{CODE-LFS} & 100 & 295 (59.0\%) \\
% & 300 & 205 (41.0\%) \\
% \midrule
% \multirow{3}{*}{Multi-LFS} & 100 & 240 (48.0\%) \\
% & 300 & 161 (32.2\%) \\
% & 1000 & 99 (19.8\%) \\
% \midrule
% \multirow{3}{*}{GEN-LFS} & 100 & 207 (41.4\%) \\
% & 300 & 181 (36.2\%) \\
% & 1000 & 112 (22.4\%) \\
% \midrule\midrule
% \multirow{4}{*}{GEN-DIS-0} & 100 & 163 (16.3\%) \\
% & 300 & 165 (16.5\%) \\
% & 1000 & 109 (10.9\%) \\
% & 0 & 563 (56.3\%) \\
% \midrule
% \multirow{4}{*}{GEN-DIS-1} & 100 & 311 (31.1\%) \\
% & 300 & 307 (30.7\%) \\
% & 1000 & 186 (18.6\%) \\
% & 0 & 196 (19.6\%) \\
% \bottomrule
% \end{tabular}
% \end{table}



% \begin{table}[htbp]
% \centering
% \caption{Performance comparison on MMLU subject categories between LM and GEN-DIS, with improvements over baseline shown in parentheses.}
% \setlength{\tabcolsep}{4pt}  
% \begin{tabular}{lrrrr}
% \hline
% MMLU\_Sub & LM & GEN-DIS-0 & GEN-DIS-1 \\
% \hline
% social\_sciences & 62.24 & 63.24 (+1.00) & 63.69 (+1.45) \\
% humanities & 49.52 & 51.31 (+1.79) & 51.09 (+1.57) \\
% stem & 42.82 & 43.67 (+0.85) & 44.37 (+1.55) \\
% other & 61.31 & 62.66 (+1.35) & 62.12 (+0.81) \\
% \hline
% \end{tabular}
% \label{tab:table2}
% \end{table}


\begin{sidewaystable}
\centering
\caption{Configuration parameters of architectures Searched by LFS}
\label{tab:lfsmodels_hyper}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll|l|cccccccc}
\toprule
\textbf{Group} & \textbf{Method} & \textbf{Metric} & \textbf{MATH-LFS} & \textbf{GEN-LFS} & \textbf{MULTI-LFS-0} & \textbf{MULTI-LFS-1} & \textbf{MULTI-LFS-2} & \textbf{MULTI-LFS-3} & \textbf{MULTI-LFS-4} \\
\midrule
\multirow{6}{*}{Group (1-10)} 
& \multirow{3}{*}{task\_arithmetic} & mlp & 0.983 & -- & -- & 0.303 & 0.303 & 0.303 & 0.293 \\
& & att & 0.182 & -- & -- & 0.948 & 0.948 & 0.948 & 0.881 \\
& & other & 0.791 & -- & -- & 0.997 & 0.997 & 0.997 & 0.997 \\
\cmidrule{2-10}
& \multirow{3}{*}{linear} & mlp & -- & [0.118, 0.324, 0.558] & [0.351, 0.344, 0.304] & -- & -- & -- & -- \\
& & att & -- & [0.430, 0.224, 0.346] & [0.162, 0.298, 0.540] & -- & -- & -- & -- \\
& & other & -- & [0.317, 0.257, 0.426] & [0.288, 0.371, 0.340] & -- & -- & -- & -- \\
\midrule
\multirow{6}{*}{Group (11-20)} 
& \multirow{3}{*}{task\_arithmetic} & mlp & 0.982 & -- & 0.395 & 0.395 & 0.395 & 0.395 & 0.395 \\
& & att & 0.604 & -- & 0.842 & 0.842 & 0.842 & 0.842 & 0.862 \\
& & other & 0.329 & -- & 0.300 & 0.380 & 0.300 & 0.300 & 0.300 \\
\cmidrule{2-10}
& \multirow{3}{*}{linear} & mlp & -- & [0.376, 0.258, 0.366] & -- & -- & -- & -- & -- \\
& & att & -- & [0.414, 0.357, 0.229] & -- & -- & -- & -- & -- \\
& & other & -- & [0.263, 0.532, 0.205] & -- & -- & -- & -- & -- \\
\midrule
\multirow{9}{*}{Group (21-30)} 
& \multirow{3}{*}{slerp(Math, Code)} & mlp & 0.594 & -- & -- & -- & -- & -- & -- \\
& & att & 0.566 & -- & -- & -- & -- & -- & -- \\
& & other & 0.323 & -- & -- & -- & -- & -- & -- \\
\cmidrule{2-10}
& \multirow{3}{*}{ties} & mlp & -- & -- & 0.778/0.583 & 0.778/0.583 & 0.819/0.583 & -- & 0.724/0.583 \\
& & att & -- & -- & 0.394/0.312 & 0.394/0.312 & 0.373/0.312 & -- & 0.488/0.312 \\
& & other & -- & -- & 0.324/0.606 & 0.349/0.606 & 0.324/0.606 & -- & 0.371/0.606 \\
\cmidrule{2-10}
& \multirow{3}{*}{linear} & mlp & -- & -- & -- & -- & -- & [0.4739, 0.4728, 0.0533] & -- \\
& & att & -- & -- & -- & -- & -- & [0.3916, 0.3649, 0.2435] & -- \\
& & other & -- & -- & -- & -- & -- & [0.0342, 0.2826, 0.6832] & -- \\
\midrule
\multirow{6}{*}{Group (31-40)} 
& \multirow{3}{*}{slerp(LM, Math)} & mlp & 0.469 & -- & -- & -- & 0.113 & -- & -- \\
& & att & 0.437 & -- & -- & -- & 0.339 & -- & -- \\
& & other & 0.549 & -- & -- & -- & 0.58 & -- & -- \\
\cmidrule{2-10}
& \multirow{3}{*}{linear} & mlp & -- & -- & [0.354, 0.382, 0.264] & [0.375, 0.392, 0.234] & -- & [0.373, 0.390, 0.237] & [0.354, 0.382, 0.264] \\
& & att & -- & -- & [0.568, 0.141, 0.292] & [0.556, 0.168, 0.276] & -- & [0.562, 0.159, 0.279] & [0.548, 0.156, 0.296] \\
& & other & -- & -- & [0.344, 0.183, 0.473] & [0.345, 0.183, 0.473] & -- & [0.345, 0.183, 0.473] & [0.345, 0.183, 0.473] \\
\bottomrule
\end{tabular}
}
\label{tab:table9}
\end{sidewaystable}


\begin{table}[htbp]
\centering
\caption{DIS-Optimized Architecture Parameters for General Reasoning}
\begin{tabular}{c||c|c|c|c||c|c|c||c|c|c}
\hline
\multirow{2}{*}{block} & \multicolumn{4}{c||}{GEN-DIS-0} & \multicolumn{3}{c||}{GEN-DIS-1} & \multicolumn{3}{c}{GEN-DIS-NR} \\
\cline{2-11}
& layer\_0 & layer\_1 & layer\_2 & scale & layer\_0 & layer\_1 & scale & layer\_0 & layer\_1 & scale\\
\hline
1 &LM & -- & -- & 1.00& LM& -- &  0.99 & LM& -- & 1.07\\
2 & Math& -- & -- &1.00 & Base & -- & 0.78& LM& -- &1.00\\
3 & LM& -- & -- &1.00 & LM & -- & 0.93 & LM& -- &1.00\\
4 & LM& -- & -- &1.00 & LM& -- & 1.01 & LM& -- &1.00\\
5 & LM& -- & -- &1.00 & LM& -- & 1.00& LM& -- &1.00\\
6 &LM & -- & -- &1.00 & LM& -- & 0.93& LM& -- &1.00\\
7 & LM& -- & -- &1.00 & Base& -- & 1.10 & LM & -- &1.00\\
8 & LM& -- & -- &1.00 & LM& -- &1.00 & LM& -- &1.00\\
9 & LM& -- & -- &1.00 & LM& -- &1.14 & LM & -- &1.00\\
10 &LM & -- & -- & 1.01& LM& -- & 1.00& LM & -- &1.00\\
11 & LM& -- & -- &1.00 & Base& -- &1.00 & LM& -- & 0.97\\
12 & Code& -- & -- &1.00 & LM & -- &0.95 & LM& -- &1.00\\
13 & LM& -- & -- &1.00 & LM& -- & 0.99& LM& -- & 1.12\\
14 & LM& -- & -- &1.00& LM& -- & 1.00& LM& -- &1.00\\
15 & LM& -- & -- &1.00 & LM& -- & 0.92 & LM& -- & 1.14\\
16 & LM& -- & -- & 1.07& LM& -- & 0.89 & LM& -- & 0.89\\
17 & LM& -- & -- & 1.00& Base& -- & 1.00& LM& -- &1.00\\
18 & Code&LM & -- & 1.00& LM&LM & 1.00& LM & -- &1.00\\
19 & LM& -- & -- & 1.00& LM&LM &1.00 & LM& -- &1.00\\
20 & Code& -- & - &1.00 & LM& -- & 1.00& LM& -- &1.00\\
21 & Base& -- & -- &1.00 & Base& -- &1.00 & -- & -- & 1.15\\
22 & LM& -- & -- & 1.00&  LM& -- &1.00 & LM& -- &1.00\\
23 & Math& LM& -- & 1.00& LM& -- &1.00 & LM & -- & 1.04 \\
24 & Base& -- & -- &1.00 &LM &LM & 1.00& LM& -- &1.00\\
25 & Code& LM& -- &1.00 & LM& -- &1.00 & -- & -- & 1.00\\
26 & Base& -- & -- & 1.00& LM& -- &1.00 & -- & -- & 0.93\\
27 & Base& -- & -- & 1.05 & LM& -- & 1.05& LM& -- & 1.00\\
28 & Math& LM& -- &1.00 & LM& LM& 1.00& LM & LM& 1.06\\
29 & LM& -- & -- & 1.07 & LM& -- & 1.00& LM& -- &1.00\\
30 & Math& LM&  -- &0.87 &LM &LM &1.00 & LM& -- & 1.21\\
31 & Base & -- & -- &1.00 & LM&LM &1.00 &LM & LM&1.00\\
32 & Math&LM & -- & 1.00& LM& &1.05 & LM & -- & 1.10\\
33 & LM& -- & -- & 1.00& Base & -- &1.00 & LM & LM & 0.99\\
34 & LM& -- & -- & 1.00& LM & LM &1.00 & LM & -- & 1.00\\
35 & LM& -- & -- &1.00 & LM & -- &1.00 & LM & -- &1.08\\
36 & LM& -- & -- &1.20 & Base & -- & 1.00& LM & -- &1.07\\
37 & Base& -- & -- &1.00 & LM & -- & 1.13& LM & LM &1.32\\
38 &Code & -- & -- &1.00 & LM & -- &1.03 & LM& LM &1.00\\
39 &Math &LM & -- &1.00 & LM & -- &1.00 &LM & -- &1.00\\
40 &Code & -- & -- & 1.00& LM& --  &1.01 & LM& -- &1.00\\
\hline
\end{tabular}
\label{tab:table10}
\end{table}





\begin{table}[htbp]
\centering
\caption{DIS-Optimized Architecture Parameters for OpenbookQA}
\begin{tabular}{c||c|c|c|c||c|c|c|c}
\hline
\multirow{2}{*}{block} & \multicolumn{4}{c||}{OpenbookQA} & \multicolumn{4}{c}{OpenbookQA+F} \\
\cline{2-9}
& layer\_0 & layer\_1 & layer\_2 & scale & layer\_0 & layer\_1 & layer\_2 & scale \\
\hline
1 & LM & -- & -- & 1.00 & Base & -- & -- & 0.96 \\
2 & Code & -- & -- & 1.00 & Code & LM & -- & 1.00 \\
3 & LM & -- & -- & 1.00 & LM & -- & -- & 1.00 \\
4 & LM & -- & -- & 1.00 & LM & -- & -- & 0.97 \\
5 & LM & -- & -- & 1.06 & LM & Code & -- & 1.00 \\
6 & LM & -- & -- & 1.00 & Base & -- & -- & 1.00 \\
7 & LM & -- & -- & 1.00 & Base & -- & -- & 1.00 \\
8 & Math & LM & -- & 1.00 & LM & -- & -- & 1.19 \\
9 & LM & -- & -- & 1.00 & Code & -- & -- & 1.00 \\
10 & LM & -- & -- & 1.00 & LM & -- & -- & 1.00 \\
11 & LM & -- & -- & 1.00 & Math & -- & -- & 1.00 \\
12 & Base & -- & -- & 1.00 & Base & -- & -- & 1.00 \\
13 & LM & -- & -- & 1.00 & LM & -- & -- & 0.91 \\
14 & LM & -- & -- & 1.00 & Base & -- & -- & 1.00 \\
15 & LM & -- & -- & 1.00 & LM & -- & -- & 1.08 \\
16 & LM & -- & -- & 1.00 & LM & -- & -- & 1.00 \\
17 & LM & -- & -- & 1.00 & LM & -- & -- & 0.99 \\
18 & LM & -- & -- & 0.85 & LM & -- & -- & 1.00 \\
19 & LM & -- & -- & 1.00 & LM & -- & -- & 1.00 \\
20 & LM & -- & -- & 1.00 & Code & -- & -- & 1.07 \\
21 & LM & -- & -- & 1.00 & Base & -- & -- & 1.21 \\
22 & LM & -- & -- & 1.00 & LM & -- & -- & 0.98 \\
23 & LM & -- & -- & 1.00 & Code & -- & -- & 1.00 \\
24 & LM & -- & -- & 1.00 & Base & -- & -- & 0.92 \\
25 & Code & LM & -- & 1.00 & LM & -- & -- & 1.00 \\
26 & Base & -- & -- & 1.00 & LM & -- & -- & 0.93 \\
27 & Code & LM & -- & 1.00 & Math & LM & -- & 1.08 \\
28 & Code & LM & -- & 0.96 & Code & -- & -- & 1.18 \\
29 & LM & -- & -- & 1.00 & LM & -- & -- & 1.00 \\
30 & Math & LM & -- & 1.00 & Code & LM & -- & 1.01 \\
31 & LM & -- & -- & 1.00 & LM & -- & -- & 0.89 \\
32 & LM & -- & -- & 1.00 & Base & -- & -- & 1.05 \\
33 & LM & -- & -- & 1.00 & LM & -- & -- & 1.00 \\
34 & LM & -- & -- & 1.00 & Math & LM & -- & 0.83 \\
35 & LM & -- & -- & 1.00 & Code & LM & -- & 1.12 \\
36 & LM & -- & -- & 1.20 & LM & -- & -- & 1.00 \\
37 & LM & -- & -- & 1.00 & Math & -- & -- & 1.13 \\
38 & LM & -- & -- & 1.00 & LM & -- & -- & 1.03 \\
39 & LM & -- & -- & 1.00 & Math & -- & -- & 1.00 \\
40 & LM & -- & -- & 1.00 & Base & -- & -- & 1.01 \\
\hline
\end{tabular}
\label{tab:table11}
\end{table}


\begin{table}[htbp]
\centering
\caption{DIS-Optimized Architecture Parameters for MGSM\_JA}
\begin{tabular}{c||c|c|c|c||c|c|c}
\hline
\multirow{2}{*}{block} & \multicolumn{4}{c||}{ MGSM\_JA\_0} & \multicolumn{3}{c}{MGSM\_JA\_1} \\
\cline{2-8}
& layer\_0 & layer\_1 & layer\_2 & scale & layer\_0 & layer\_1 & scale \\
\hline
1 & Base & -- & -- & 0.99 & Math & -- & 0.98 \\
2 & Math & -- & -- & 1.00 & Base & -- & 1.12 \\
3 & Math & -- & -- & 0.96 & LM\_JA & -- & 1.00 \\
4 & Math & -- & -- & 1.00 & Base & -- & 1.00 \\
5 & Math & -- & -- & 1.00 & Math & LM\_JA & 1.00 \\
6 & Math & -- & -- & 1.00 & Math & -- & 0.93 \\
7 & Math & LM & -- & 1.00 & Math & -- & 1.00 \\
8 & Math & -- & -- & 1.11 & Math & -- & 1.00 \\
9 & Math & LM & -- & 1.00 & Math & -- & 1.00 \\
10 & Math & -- & -- & 1.10 & Math & LM\_JA & 1.00 \\
11 & Math & -- & -- & 1.00 & LM\_JA & -- & 1.00 \\
12 & Math & LM & -- & 1.06 & Math & LM\_JA & 1.00 \\
13 & LM & Math & -- & 1.00 & Math & -- & 1.00 \\
14 & LM & Math & -- & 0.83 & Math & -- & 0.97 \\
15 & Math & -- & -- & 0.86 & Base & -- & 1.00 \\
16 & Math & -- & -- & 1.00 & Base & -- & 0.99 \\
17 & Code & -- & -- & 1.00 & Math & -- & 1.09 \\
18 & Math & -- & -- & 1.00 & Math & -- & 1.00 \\
19 & Base & -- & -- & 1.00 & Base & -- & 0.90 \\
20 & Math & -- & -- & 1.00 & Math & -- & 1.26 \\
21 & Base & -- & -- & 1.00 & Base & -- & 1.24 \\
22 & Base & -- & -- & 1.00 & Base & -- & 1.00 \\
23 & Math & -- & -- & 0.96 & Math & -- & 0.82 \\
24 & Base & -- & -- & 0.85 & Math & -- & 1.08 \\
25 & Math & -- & -- & 1.01 & Base & -- & 1.00 \\
26 & Code & -- & -- & 1.00 & Base & -- & 1.00 \\
27 & Math & -- & -- & 1.00 & LM\_JA & -- & 1.00 \\
28 & Math & -- & -- & 1.00 & Math & -- & 1.07 \\
29 & Math & -- & -- & 1.00 & Base & -- & 1.00 \\
30 & Base & -- & -- & 1.00 & LM\_JA & Math & 0.87 \\
31 & Code & -- & -- & 1.04 & Math & LM\_JA & 0.84 \\
32 & Math & LM & -- & 0.89 & Math & LM\_JA & 0.99 \\
33 & Math & -- & -- & 1.00 & Math & -- & 1.00 \\
34 & Base & -- & -- & 1.00 & Math & LM\_JA & 0.73 \\
35 & LM & -- & -- & 1.00 & Math & -- & 1.00 \\
36 & Math & -- & -- & 0.98 & LM\_JA & -- & 1.00 \\
37 & Math & -- & -- & 1.00 & Math & -- & 1.00 \\
38 & Base & -- & -- & 1.00 & Math & LM\_JA & 1.00 \\
39 & Math & -- & -- & 0.95 & Math & -- & 1.00 \\
40 & Base & -- & -- & 1.00 & Math & -- & 1.00 \\
\hline
\end{tabular}
\label{tab:table12}
\end{table}
% Required packages in preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{array}
% \usepackage{graphicx}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
