\section{Related Work}
\subsection{Model Merging}
Model merging has emerged as an efficient approach to enhance model capabilities without additional training data or extensive computation. The field has evolved from simple weighted parameter averaging to increasingly sophisticated methods. Early methods employed weighted parameter averaging  \citep{utans1996weight} for models fine-tuned from a shared base model. Despite being simple to implement, these approaches often yielded suboptimal results. More advanced parameter-based techniques like Task Arithmetic \citep{ilharco2022editing} and SLERP \citep{white2016sampling} introduced parameter differences computation and spherical interpolation respectively. Later developments leveraged neural network sparsity, with TIES-Merging \citep{yadav2024ties} selectively retaining parameters based on magnitude while addressing sign conflicts, and DARE \citep{yu2024language} combining magnitude-based sparsification with parameter rescaling. Recent advances include Evolutionary model merging \citep{akiba2024evolutionary}, which optimizes merging coefficients through evolutionary search. In this study, our framework also focuses on automatic model merging, while we propose a novel framework that leverages Multi-fidelity optimization \citep{peherstorfer2018survey} based on SMAC \citep{JMLR:v23:21-0888}, optimizing merging recipes through layer fusion and pathway optimization for enhanced reasoning capabilities.


% Early methods focused on weighted parameter averaging \citep{utans1996weight} for models fine-tuned from a shared base model. While straightforward, these approaches often yielded suboptimal results. More advanced parameter-based techniques  Task Arithmetic \citep{ilharco2022editing}, which computes parameter differences between fine-tuned and pre-trained models, and SLERP \citep{white2016sampling}, which employs spherical interpolation for parameter combination. A significant advancement came through sparsity-based methods, which leverage the inherent over-parameterization of neural networks. TIES-Merging \citep{yadav2024ties} exploits this characteristic by selectively retaining parameters based on magnitude while addressing sign conflicts to reduce interference. Similarly, Drop And REscale (DARE) \citep{yu2024language} combines magnitude-based sparsification with parameter rescaling to enhance merging effectiveness. The field further evolved with automated optimization approaches, exemplified by Evolutionary model merging\citep{akiba2024evolutionary}, which introduces an efficient evolutionary search strategy for determining optimal merging coefficients. While our framework also focuses on automatic model merging,  We propose a novel automated model merging framework that leverages Multi-fidelity optimization based on SMAC\citep{JMLR:v23:21-0888}, optimizing merging recipes through both layer fusion and pathway optimization for enhanced reasoning capabilities.




% Model merging techniques can be broadly categorized into two paradigms: homogeneous merging, which combines models trained on identical tasks to enhance generalization, and heterogeneous merging, which synthesizes capabilities across models specialized in distinct tasks. Our work advances heterogeneous merging through an automated framework that optimizes both specific reasoning abilities and comprehensive reasoning capabilities through systematic model combination.


% \subsection{Neural Architecture Search}
% Neural Architecture Search (NAS) has revolutionized the landscape of deep learning by enabling the discovery of customized architectures that might elude human intuition. NAS frameworks typically comprise three fundamental components: a search space defining possible neural architectures, architecture optimization methods, and model evaluation strategies. The architecture optimization landscape encompasses various approaches, including reinforcement learning (RL) \citep{pham2018efficient}, evolutionary algorithms (EA) \citep{akiba2024evolutionary}, gradient descent (GD) \citep{liu2018darts}, and Surrogate Model-Based Optimization (SMBO) \citep{hutter2011sequential, falkner2018bohb}.
% While model merging has emerged as a promising approach to democratize model development, current methods heavily rely on manual intuition and domain expertise. As the ecosystem of open-source models and tasks continues to expand, there is a pressing need for more systematic and automated approaches to model merging, drawing inspiration from NAS methodologies.
% Our approach adopts the principles of BOHB \citep{falkner2018bohb}, which elegantly combines Hyperband optimization with Bayesian Optimization (BO). In this framework, the surrogate model is fitted on the highest budget level with sufficient observations. We extend this concept to automate the optimization of mathematics-reasoning enhanced model merging recipes across both parameter and layer space. Notably, our method differs fundamentally from traditional NAS in its evaluation strategy. Since we work with pre-trained transformer blocks, we can directly evaluate candidate architectures without the computational overhead of training, significantly accelerating the optimization process.

\subsection{Hyperparameter Optimization}
\textbf{Bayesian Optimization} has demonstrated remarkable success across various applications, from achieving state-of-the-art results on CIFAR-10~\citep{snoek2012practical} to winning multiple datasets in the 2016 AutoML challenge~\citep{mendoza2016towards}. Although Gaussian processes remain the predominant probabilistic model in Bayesian optimization due to their well-calibrated uncertainty estimates, they face limitations in scalability, flexibility, and robustness. Alternative models such as random forests \citep{hutter2011sequential} and Bayesian neural networks \citep{snoek2015scalable, springenberg2016bayesian, perrone2017multiple} offer better scalability for high-dimensional spaces.

\textbf{Hyperband} is one of the most widely-used multi-fidelity optimization \citep{peherstorfer2018survey} methods. It dynamically allocates resources across random configurations, while applying successive halving \citep{jamieson2016non} to eliminate poor options early. Although this method demonstrates superior performance and scalability compared to traditional Bayesian optimization, its random sampling strategy fails to leverage information from previous evaluations, potentially limiting its performance. 

Our optimizer builds upon SMAC \citep{JMLR:v23:21-0888}, combining Hyperband (HB) and Bayesian Optimization (BO) to harness both efficient resource allocation and learning capabilities through surrogate modeling for effective multi-fidelity optimization. We integrate this approach into our framework to enable effective and efficient searching.


% Our optimizer builds upon SMAC~\citep{JMLR:v23:21-0888}, which combines Hyperband (HB)~\citep{li2018hyperband} and Bayesian Optimization (BO)~\citep{snoek2012practical}, which fits a surrogate model on the highest budget level with sufficient observations, effectively combining the efficiency of Hyperband with the learning capabilities of Bayesian optimization. We extend this optimization strategy to our automatic model merging framework to achieve both effective and efficient searching.