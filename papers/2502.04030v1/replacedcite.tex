\section{Related Work}
\subsection{Model Merging}
Model merging has emerged as an efficient approach to enhance model capabilities without additional training data or extensive computation. The field has evolved from simple weighted parameter averaging to increasingly sophisticated methods. Early methods employed weighted parameter averaging  ____ for models fine-tuned from a shared base model. Despite being simple to implement, these approaches often yielded suboptimal results. More advanced parameter-based techniques like Task Arithmetic ____ and SLERP ____ introduced parameter differences computation and spherical interpolation respectively. Later developments leveraged neural network sparsity, with TIES-Merging ____ selectively retaining parameters based on magnitude while addressing sign conflicts, and DARE ____ combining magnitude-based sparsification with parameter rescaling. Recent advances include Evolutionary model merging ____, which optimizes merging coefficients through evolutionary search. In this study, our framework also focuses on automatic model merging, while we propose a novel framework that leverages Multi-fidelity optimization ____ based on SMAC ____, optimizing merging recipes through layer fusion and pathway optimization for enhanced reasoning capabilities.


% Early methods focused on weighted parameter averaging ____ for models fine-tuned from a shared base model. While straightforward, these approaches often yielded suboptimal results. More advanced parameter-based techniques  Task Arithmetic ____, which computes parameter differences between fine-tuned and pre-trained models, and SLERP ____, which employs spherical interpolation for parameter combination. A significant advancement came through sparsity-based methods, which leverage the inherent over-parameterization of neural networks. TIES-Merging ____ exploits this characteristic by selectively retaining parameters based on magnitude while addressing sign conflicts to reduce interference. Similarly, Drop And REscale (DARE) ____ combines magnitude-based sparsification with parameter rescaling to enhance merging effectiveness. The field further evolved with automated optimization approaches, exemplified by Evolutionary model merging____, which introduces an efficient evolutionary search strategy for determining optimal merging coefficients. While our framework also focuses on automatic model merging,  We propose a novel automated model merging framework that leverages Multi-fidelity optimization based on SMAC____, optimizing merging recipes through both layer fusion and pathway optimization for enhanced reasoning capabilities.




% Model merging techniques can be broadly categorized into two paradigms: homogeneous merging, which combines models trained on identical tasks to enhance generalization, and heterogeneous merging, which synthesizes capabilities across models specialized in distinct tasks. Our work advances heterogeneous merging through an automated framework that optimizes both specific reasoning abilities and comprehensive reasoning capabilities through systematic model combination.


% \subsection{Neural Architecture Search}
% Neural Architecture Search (NAS) has revolutionized the landscape of deep learning by enabling the discovery of customized architectures that might elude human intuition. NAS frameworks typically comprise three fundamental components: a search space defining possible neural architectures, architecture optimization methods, and model evaluation strategies. The architecture optimization landscape encompasses various approaches, including reinforcement learning (RL) ____, evolutionary algorithms (EA) ____, gradient descent (GD) ____, and Surrogate Model-Based Optimization (SMBO) ____.
% While model merging has emerged as a promising approach to democratize model development, current methods heavily rely on manual intuition and domain expertise. As the ecosystem of open-source models and tasks continues to expand, there is a pressing need for more systematic and automated approaches to model merging, drawing inspiration from NAS methodologies.
% Our approach adopts the principles of BOHB ____, which elegantly combines Hyperband optimization with Bayesian Optimization (BO). In this framework, the surrogate model is fitted on the highest budget level with sufficient observations. We extend this concept to automate the optimization of mathematics-reasoning enhanced model merging recipes across both parameter and layer space. Notably, our method differs fundamentally from traditional NAS in its evaluation strategy. Since we work with pre-trained transformer blocks, we can directly evaluate candidate architectures without the computational overhead of training, significantly accelerating the optimization process.

\subsection{Hyperparameter Optimization}
\textbf{Bayesian Optimization} has demonstrated remarkable success across various applications, from achieving state-of-the-art results on CIFAR-10____ to winning multiple datasets in the 2016 AutoML challenge____. Although Gaussian processes remain the predominant probabilistic model in Bayesian optimization due to their well-calibrated uncertainty estimates, they face limitations in scalability, flexibility, and robustness. Alternative models such as random forests ____ and Bayesian neural networks ____ offer better scalability for high-dimensional spaces.

\textbf{Hyperband} is one of the most widely-used multi-fidelity optimization ____ methods. It dynamically allocates resources across random configurations, while applying successive halving ____ to eliminate poor options early. Although this method demonstrates superior performance and scalability compared to traditional Bayesian optimization, its random sampling strategy fails to leverage information from previous evaluations, potentially limiting its performance. 

Our optimizer builds upon SMAC ____, combining Hyperband (HB) and Bayesian Optimization (BO) to harness both efficient resource allocation and learning capabilities through surrogate modeling for effective multi-fidelity optimization. We integrate this approach into our framework to enable effective and efficient searching.


% Our optimizer builds upon SMAC____, which combines Hyperband (HB)____ and Bayesian Optimization (BO)____, which fits a surrogate model on the highest budget level with sufficient observations, effectively combining the efficiency of Hyperband with the learning capabilities of Bayesian optimization. We extend this optimization strategy to our automatic model merging framework to achieve both effective and efficient searching.