\section{Implementation details}
\label{sec:implementation-details}
In this section, we describe relevant details omitted in the text above.

\subsection{Enumeration process}
\label{sec:dead-sector-construction-neighbors}
Here we describe the implementation details omitted from \cref{sec:edge-enumeration}.

\subsubsection{Precise activation distances}
\label{sec:precise-activation-distances}
Before resorting to the simplified activation distance computation described in \cref{sec:dead-sectors},
we attempted to compute precise upper bounds on the approximation distance using the elliptic arc and interval arithmetic as follows.
We begun by finding parameters $a,b,c,d,e,f$ that describe the ellipse using the equation $ax^2 + bxy + cy^2 + dx + ey + f = 0$,
based on $5$ points on the ellipse; this involves solving a linear system with $6$ variables and unknowns.

We then continued by a bisection search to find the maximum distance from $p$ to the full ellipse
on which the elliptic arc bounding the dead sector lies.
In each iteration of this procedure, we have to check whether a circle with center $p$ and some radius $r$ 
intersects the ellipse, and if the intersection is possibly between the two rays that form the other bounds of the dead sector.

The intersection points of this circle with the ellipse are the roots of a quartic equation.
Therefore, in each iteration, we have to check whether a quartic equation has a real root.
We found that this computation is costly and numerically unstable;
using interval arithmetic to compute a safe upper bound often produced overly conservative results.
This was mostly due to strong amplification of minute uncertainties contained in $a,\ldots,f$ when evaluating
the 6th-degree discriminant of the quartic equation to check for real roots;
even in cases that were not deliberately constructed to be numerically challenging,
we observed that the computation of the activation distance yielded results that were sometimes worse
than what the much cheaper bounding rectangle-based approach gave us.

\subsubsection{Dead sector construction}
We now discuss which other points we use to construct dead sectors when we encounter a point $t$.
During preliminary experiments, we found it insufficient to consider the closest points to $t$ w.r.t.\ the polar angle around $p$.
Points that are angularly near still can have high Euclidean distance, leading to a large activation distance or an empty dead sector.
For efficiency reasons, it may also be costly to consider large sets, such as all previously encountered points.
We only use a number that is $O(1)$ on average for each point $t$;
this is also needed to guarantee that the overall runtime of the algorithm is at most $O(n^2\log n)$:
otherwise, we might have significantly more dead sectors than points.

We use the following heuristics to determine \emph{interesting neighbors} of a point $t$ with which we attempt to construct dead sectors.
Firstly, we do consider a small constant number of previously encountered points $q$ that are close to $t$ in terms of polar angle.
Secondly, before running the actual enumeration process, we determine for each point a small number of interesting neighbors:
aside from the six closest neighbors of $t$, we also consider all neighbors of $t$ in the Delaunay triangulation.

\subsection{Exact algorithm}
In this section, we describe details omitted from \cref{sec:exact-algorithms}.

\subsubsection{Dilation path separation in practice}
\label{sec:practical-dilation-path-sep}
Recall that solving the dilation path separation problem is a crucial part of our exact algorithm.
We need to repeatedly and exactly compute shortest $s$-$t$-paths on our triangulation supergraph $G$
to build a set of edges $E' \subseteq E \setminus T$ without which there is no sufficiently short $s$-$t$-path;
we then return $\bigvee_{e \in E'} x_e$ as a clause that is violated by the current solution but satisfied by any solution with dilation less than $\rho$.

In order to speed up the computation, we begin by enumerating the vertices that may be on a sufficiently short $s$-$t$-path.
We start by running a bounded version of Dijkstra's algorithm from $s$ to enumerate the set $Q$ of all vertices $v$ that are reachable from $s$ in distance less than $\rho \cdot d(s,t) - d(v,t)$.
If this fails to reach $t$, we know that no sufficiently short path exists.
In this case, we can immediately return the empty clause, which is always violated, as no solution with dilation less than $\rho$ exists.

Otherwise, we further restrict $Q$ by running a second bounded Dijkstra's algorithm from $t$,
this time only considering vertices $v$ reachable from $t$ in distance less than $\rho \cdot d(s,t) - |\pi_G(s,v)|$,
using the shortest path distances computed by the run from $s$.

We also restrict the set of possible edges by checking for each edge between two possible vertices $u, v \in Q$ 
whether either of the two paths $\pi_G(s,v),\pi_G(u,t)$ or $\pi_G(s,u),\pi_G(v,t)$ is shorter than $\rho \cdot d(s,t)$.

Afterwards, we can repeatedly run a bidirectional version of Dijkstra's algorithm,
which simultaneously runs the algorithm from $s$ and $t$, restricted to the possible vertices and edges to find a shortest path $\pi$ between $s$ and $t$;
after each run, if the path is shorter than $\rho \cdot d(s,t)$, we add an edge $e \notin T$ from $\pi$ to $E'$,
ignoring $e$ in all subsequent bidirectional Dijkstra runs; see \cref{fig:path-enumeration} for an illustration.
As soon as the shortest path exceeds $\rho \cdot d(s,t)$ or does not exist, we have found a suitable $E'$.

\begin{figure}
    \centering
    \includegraphics{figures/dilation_path_sep.pdf}
    \caption{Sketch of the dilation path separation between $s$ and $t$;
             the red path represents the shortest $s$-$t$-path in the current triangulation $T$.
             The ellipse represents the points with total distance $\rho \cdot d(s,t)$ to $s,t$.
             Red and gray edges represent the current triangulation $T$ and are a subset of $E$,
             blue edges are possible edges in $E$, but not in $T$.
             The green point is found from $s$ but eliminated when running Dijkstra from $t$;
             the purple points are the points in $Q$ that may be on sufficiently short $s$-$t$-paths.
             The clause we generate contains the bold blue edges $sp$ and $rt$; 
             without these edges, there is no sufficiently short $s$-$t$-path in $E$.}
    \label{fig:path-enumeration}
\end{figure}

In practice, we have no efficient oracle for square root sum comparisons;
see our remarks on computing exact shortest paths in \cref{sec:exactness-implementation-issues} for details on how we handle this.

\subsubsection{Incremental SAT solving}
\label{sec:incremental-sat-solving}
Many modern SAT solvers are incremental, at least to some extend.
Among other things, this means that we can add clauses to the solver after solving a formula and check whether the new set of clauses is satisfiable.
This is often much more efficient than reconstructing the solver and re-solving the formula from scratch.
Among other things, the solver gets to keep learned clauses and often also parts of the current state of the search;
we see this reflected in the fact that, when we add clauses in \incmdt{}, the majority of the triangulation usually remains unchanged
even though the solver could theoretically change large parts of the triangulation.

However, we must be more careful in \binmdt{}.
As soon as we have used an incremental SAT solver on a bound that cannot be achieved, we have to construct a new solver object.
This is because clauses may have been added to the solver that are not valid for less tight bounds.
We therefore maintain a pool of clauses that we have generated;
this allows us to quickly reconstruct a new SAT solver without losing such clauses.
All clauses that we generate during the current iteration are only added to that pool
once we have found a triangulation that satisfies the current bound.

We could alternatively generate tentative clauses by adding an \emph{assumption literal} $a_i$ on a new variable to all clauses generated by the $i$th iteration of \binmdt{}.
We could then tell the incremental solver to temporarily \emph{assume} that $a_i$ is false, which would effectively enforce the clauses.
If we find a triangulation satisfying the bound, we can make this assumption permanent by adding $\lnot a_i$ as clause.
If we instead find the bound to be too tight, we can disable the clauses by removing the assumption and adding $a_i$ as clause.

We did not study this aspect further, since the time spent in the SAT solver was negligible compared to the time spent computing dilations.

\subsection{Exactness}
\label{sec:exactness-implementation-issues}
Ultimately, we aim for solutions that are exactly optimal, including all numerical issues, as if we were using infinite precision for all our computations.
In this section, we describe the practical challenges this presents and our approach to handling them.

\subsubsection{Dilation and shortest paths}
Achieving this requires us to exactly compute the dilation.
We thus rely on an exact implementation of Dijkstra's algorithm for Euclidean distances.
We run Dijkstra's algorithm once for each source point $p$, computing the ratio between the shortest path and the Euclidean distance for $p$ and all other points.
The theoretical runtime for this is $O(n^2\log n)$ assuming uniform cost for each computational operation; we parallelize the runs for different source points.
We represent each dilation value as a combination of a path defining the dilation and an interval containing the true dilation value.
In most cases, interval arithmetic is sufficient to decide which path defines the dilation;
however, in particular in some non-randomly generated instances that include symmetries, we encounter cases where intervals do not suffice.
In fact, the path defining the dilation is often non-unique in these instances.

In a first prototype, we simply resorted to directly constructing an instance of CGAL's exact number type with support for square roots, representing the precise dilation values in order to compare them if interval arithmetic was insufficient.
However, the comparisons resulted in unacceptable performance issues; it turned out to be crucial to perform extensive preprocessing before resorting to such expensive operations:
even single comparisons of sums consisting of less than $10$ square roots each often took several seconds or minutes.

Let $\pi_1 = (p_1, \ldots, p_k)$ and $\pi_2 = (q_1, \ldots, q_m)$ be two paths for which we want to decide which one incurs a worse detour.
In other words, we want to determine whether
\[\frac{\sum\limits_{i = 2}^k d(p_{i-1}, p_i)}{d(p_1, p_k)} = \underbrace{\sum\limits_{i = 2}^k \frac{d(p_{i-1}, p_i)}{d(p_1, p_k)}}_{(*)} \overset{?}{<} \frac{\sum\limits_{i = 2}^m d(q_{i-1}, q_i)}{d(q_1, q_m)} = \underbrace{\sum\limits_{i = 2}^m \frac{d(q_{i-1}, q_i)}{d(q_1, q_m)}}_{(**)}.\]
Observe that each entry of the sums $(*)$ and $(**)$ is the square root of a rational number;
we can compute and compare these rational numbers at much lower cost than dealing with the square roots.
This allows us to eliminate equal terms from the sums $(*)$ and $(**)$.
We also scan for horizontal or vertical segments, which have rational length for rational input coordinates,
and collect them in a single term on each side.

We use these rational numbers to compute more precise intervals for each of the terms,
summing up $(*) - (**)$ in interval arithmetic in order of increasing absolute value.
If the resulting interval does not contain $0$, we can decide which path has higher dilation.
If it is precisely $[0,0]$, e.g., because we eliminated all terms, we know both paths have the same dilation.
Otherwise, we repeat the computation using intervals of 1024-bit mantissa floating point values using the MPFR~\cite{DBLP:journals/toms/FousseHLPZ07} library instead of double-precision values.
Only if all these steps fail do we construct the dilation values using CGAL's exact number type and compare them;
in all cases we observed, at this point, the resulting values were actually equal.
We apply similar techniques to compare the length of paths instead of dilations in our implementation of Dijkstra's algorithm.

\subsubsection{Candidate edge enumeration}
Not only the computation of shortest paths and the dilation require attention to numerical issues,
but these are the only cases necessarily involving the comparison of sums of square roots.
The incremental search procedure is also exact, i.e., events regarding points and nodes are handled in precisely the correct order,
while activation distance events are handled based on upper bounds on the exact activation distance.
We order all events by squared distances instead of distances and thus maintain rationality,
allowing us to augment interval arithmetic with rational computations to ultimately decide the order of events.
Most other parts of the enumeration process only require exact predicates, such as intersections checks;
these are provided by CGAL and also make use of interval arithmetic to only resort to exact computations if necessary.

Another situation where the use of interval arithmetic is important and not straightforward is the representation of dead sectors.
Since working with true polar angles involves the costly trigonometric function \texttt{atan2},
which is not readily available with correct rounding for use in interval arithmetic,
we instead use \emph{pseudo-angles} as described by Moret and Shapiro~\cite{10.5555/102912}.
In essence, we measure angles by computing distances along the $\ell_1$ unit circle instead of the $\ell_2$ unit circle.
Pseudo-angles are rational and only use operations that are available in correctly-rounded fashion on modern CPUs;
this allows us to use interval arithmetic to handle the polar angle intervals of dead sectors.
In order to allow us to merge polar angle intervals that touch in a single point,
which would not be possible using interval arithmetic alone, we also store, for each polar angle interval,
the lowest and highest point (w.r.t.\ the pseudo-angle around $p$) in the interval.


\section{Empirical evaluation}

In this section, we provide additional details on the experiments conducted in the context of the empirical evaluation,
together with additional figures and data tables; 
note that all experiment data are archived and publicly accessible; see the supplement.

\begin{description}
    \item[QA1] How does the runtime of the dilation computation scale with the number of points in the input instance?
\end{description}

\subsection{QA1: Dilation computation}
\label{sec:experiments-dilation-computation}

\begin{figure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/experiments/02_dilation_computation/runtimes.pdf}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/experiments/04_mdt_comparison/public_instance_set_improvement_initial_solution.pdf}
    \end{subfigure}
    \caption{\textbf{(Left)}~The dilation computation time increases significantly for larger instances. 
             \textbf{(Right)}~Runtime impact of working with the Delaunay triangulation ($t_{\operatorname{del}}$) directly vs.~using the initial improvement algorithm ($t_{\operatorname{imp}}$).
                     Introducing initial improvements reduces the search time of \binmdt{} significantly but can come at the cost of a higher overall runtime.}
    \label{fig:dilation-computation}
    \label{fig:initial-improvement}
\end{figure}

The proposed algorithms rely on the computation of the dilation for some given triangulation.
Our implementation uses an exact version of a bidirectional Dijkstra algorithm to compute the dilation in $O(n^2 \log n)$, see~\cref{sec:exactness-implementation-issues} for details.
The experiment was conducted on the \emph{random} instance set with up to \num{5000} points where the input triangulation is the Delaunay triangulation.
To obtain a reasonable average runtime, we computed the dilation for each instance \num{100} times, see~\cref{fig:dilation-computation}.
A dilation query for an instance with \num{5000} points takes on average slightly more than \qty{0.4}{s}.
As expected, the runtime increases with the number of points in the instance with a quadratic dependency.
In our experiments on the \emph{public} instance set we observed that the runtime of the dilation computation can be significantly higher. This is due to degenerate instances containing structures like regular grids that have a lot of edges and paths with the same length.
Doing exact comparisons of square roots in these cases can be costly and lead to higher runtimes.

%\subsection{Q3: Comparison to state-of-the-art}

%This section contains further details on the comparison of the proposed algorithms to the state-of-the-art algorithms and should be seen as an extension to~\cref{sec:experiments-comparison-to-existing}.
%Sattari~and~Izadi~\cite{DBLP:journals/jgo/SattariI17} proposed an exact algorithm for the MDT problem that is based on a custom branch and bound algorithm.
%In their paper they compared their approach to existing algorithms (approximations and exact algorithms) and showed that their algorithm outperforms the integer programming approach from Brandt et al.~\cite{DBLP:conf/cccg/BrandtGSR14}.
%We compare our exact algorithms to the algorithm from Sattari and Izadi~\cite{DBLP:journals/jgo/SattariI17} on the TSPLIB~\cite{reinelt1991} instances that were part our \emph{public} benchmark set.
%The results are summarized in~\cref{tab:tsplib-comparison}.

\begin{table}[h!t]
    \centering
    \input{tables/tsplib}
    \caption{Solutions and runtimes for small TSPLIB instances.}
    \label{tab:tsplib-comparison}
\end{table}

\subsection{Q4: Algorithm comparison}
\label{sec:appendix-algorithm-comparison}

This section contains additional details on the comparison of the proposed algorithms to each other and should be seen as an extension to~\cref{sec:experiments-algorithm-comparison}.
\cref{fig:experiments-public-instance-set} shows the runtime of the \binmdt{} with two different initial solution strategies on the \emph{public} benchmark set with up to \num{10000} points.

We provide an additional information in~\cref{fig:initial-improvement} that shows the relative runtime change when using the different initial solution strategies.
Let $t_{\mathrm{del}}$ be the runtime of \binmdt{} when starting with the Delaunay triangulation as initial solution and $t_{\mathrm{imp}}$ be the runtime when starting with the improvement Delaunay triangulation.
We distinguish between the overall runtime (including calculating the initial solution) and the search time (the time spent in finding the optimal solution).
What we compare is the relative improvement, i.e. $\frac{t_{\mathrm{del}}-t_{\mathrm{imp}}}{t_{\mathrm{del}}}$.
A positive value indicates that the improved Delaunay triangulation reduces the runtime and a negative value indicates that the initial improvement increases the runtime.
Regarding the search time we observe that the improved solution reduces the time it takes to find and verify the optimal solution. This is expected as the heuristic is known to produce better solutions than the Delaunay triangulation and in fact can never be worse than the Delaunay triangulation, see~\cref{sec:experiments-initial-solutions}.
The overall runtime however can sometimes be higher when using the improved triangulation as the initial solution. This is due to the fact that testing different constrained Delaunay triangulations require multiple dilation computations which can be costly.
Still we conclude that using the improved initial solution is beneficial in most cases and is used as the default setting for the \binmdt{} algorithm.

%\clearpage
\section{Additional remarks on regular \texorpdfstring{$n$}{n}-gons}
\label{sec:appendix-n-gons}
It was already noted by Euler (see~\cite{stanley2015catalan}) that the number of triangulations of convex $n$-gons corresponds to the Catalan numbers
$C_{n-2}$, which grow asymptotically like $\frac{4^n}{n^{3/2}\sqrt{\pi}}$, illustrating the futility of performing 
a brute-force enumeration for finding an MDT. 

In \cref{sec:n-gon-lb} we answer the open question posed by Dumitrescu and Ghosh~\cite[Problem 1]{DBLP:journals/ijcga/DumitrescuG16} that originated from Bose, Smit and Kanj~\cite{DBLP:journals/comgeo/BoseS13, DBLP:conf/iccit/Kanj13}.
With the help of our exact algorithm we can obtain a lower bound for the dilation of regular $n$-gons.
It is known that the worst-case dilation of a regular $n$-gon is between $1.4308$ and $1.4482$.

\begin{theorem}[\cite{DBLP:journals/ijcga/DumitrescuG16,DBLP:journals/comgeo/SattariI19}]
    \label{theorem:old-bounds}
    The worst-case dilation of a regular $n$-gon $\rho^*$ is bounded by
    \[
        1.4308 \approx 
            \frac{2\sin(\frac{2\pi}{23}) + \sin(\frac{8\pi}{23})}{\sin(\frac{11\pi}{23})} \leq \rho^* < 1.4482.
    \]
\end{theorem}

\begin{figure}[t!p]
    \centering
    \includegraphics[width=\linewidth]{figures/experiments/06_special_instances/ngon_runtimes_rho.pdf}
    \caption{Dilations and runtimes for regular $n$-gons for $4 \leq n \leq 100$.
             Red dots improve the current lower bound on the dilation of regular $n$-gons of $1.4308$ that comes from the regular $23$-gon.
             The dashed black lines mark the known upper bound of $1.4482$ and the previous best lower bound of $1.4308$.}
    \label{fig:n-gon-rho-runtimes}
\end{figure}

\begin{figure}[t!p]
    \centering
    \includegraphics[width=\linewidth]{figures/experiments/06_special_instances/n_gons.pdf}
    \caption{Optimal MDT of regular $n$-gon with floating point coordinates. All solutions improve the current lower bound for regular $n$-gons.} %The optimal solution has a dilation between $1.43832128843529$ and $1.438321288435301$.
    \label{fig:n-gon-examples}
\end{figure}

We ran our exact algorithm for regular $n$-gons for $4 \leq n \leq 100$. 
\Cref{fig:n-gon-rho-runtimes} highlights the previous bounds
from~\cref{theorem:old-bounds} and shows that many regular $n$-gons improve the
current lower bound of $1.4308$. 
\Cref{fig:n-gon-examples} shows some examples of such solutions. 
One can observe that the solution structures appear to be quite intricate, 
highlighting that it would have been quite challenging
to establish these results with only the help of manual analysis. Moreover,
this intricacy may be an indication that the underlying problem does not
lend itself to a simple polynomial-time algorithm.

These difficulties are also visible in the necessary runtimes for $n$-gons.
%Naturally, the runtime of the algorithm increases with the number of points in the instance.
It is remarkable that the growth (which appears to be exponential in $n$)
is significantly faster than for our results on 
instances from the \emph{uniform} and \emph{public} benchmark sets (see~\cref{sec:experiments-algorithm-comparison}),
with already more than \qty{10}{h} for $n = 84$ points.
(See~\cref{fig:n-gon-rho-runtimes} and~\cref{tab:n-gons} for detailed results.)
An intuitive reason may be that any point in the interior can be used as a Steiner point for shortcuts, 
while also allowing local reduction techniques such as the ellipse property, which fails
to provide a reduction for the vertices of a regular $n$-gon. Conversely, any chord
through the interior constitutes an obstacle for all separated point pairs, producing
cascading sets of constraints. Moreover, 
%The runtime of the \binmdt{} algorithm seems to grow exponentially with the number of points.
%One reason for this increase might be that the 
any solution contains many symmetries that make finding an optimal solution (among many others with similar or same dilation) significantly harder.
As all points are on the boundary of a circle, quadtree queries in the enumeration algorithm are not as effective as in other instance sets.
Finally, regular $n$-gons contain a maximum number of intersections between edges, which makes intersection constraints weaker.
It is this combination of difficulties that make regular $n$-gons all the more interesting,
as they are prime candidates for the worst-case dilation bound.

\begin{conjecture}
\label{con:worst}
The worst-case bound for Minimum Dilation Triangulations corresponds to the 
asymptotic value for regular $n$-gons. This implies that the correct 
value is $1.44\ldots$
\end{conjecture}

\begin{table}[t!p]
    \centering
    \input{tables/n_gons}
    \caption{Dilations of regular $n$-gons for $4 \leq n \leq 100$ and the runtime of the \binmdt{} algorithm. Bold values improve the current lower bound on the dilation of regular $n$-gons of $1.4308$ that arises from the regular $23$-gon.}
    \label{tab:n-gons}
\end{table}

\clearpage

\section{Additional tables}
Here we provide a full overview of additional experiment data in table form.

\begin{table}[h!]
    \centering
    \input{tables/large_benchmark}
    \caption{Solutions and runtimes of \binmdt{} for large instances of the \emph{public} benchmark set. Columns also provide the number of full and sampled dilation computations that are the bottleneck in the exact algorithm.}
    \label{tab:large-comparison}
\end{table}

\clearpage

\input{tables/public_instances}
