\section{Related Work}
\subsection{Multimodal Large Language Models}
% Building on the success of large language models (LLMs) \citep{yao2024tree, glm2024chatglm, achiam2023gpt, touvron2023llama, brown2020language}, multimodal large language models (MLLMs) \citep{liu2024improved, li2023blip, zhu2023minigpt, wang2023cogvlm, liu2024visual} extend these capabilities by integrating vision and text processing, achieving remarkable performance in tasks involving images, videos, and multimodal reasoning. However, handling visual data poses computational challenges due to the redundancy and low information density of high-resolution tokens \citep{liang2022evit} and the quadratic scaling of attention mechanisms \citep{vaswani2017attention}.
% For instance, models like LLaVA \citep{liu2023improvedllava} and mini-Gemini-HD \citep{li2024mini} encode high-resolution images into thousands of tokens, while video-based models such as VideoLLaVA \citep{lin2023video} and VideoPoet \citep{kondratyuk2023videopoet} allocate even more tokens to process multiple frames. These challenges highlight the need for more efficient token representations and longer context lengths to enable scalability. Recent advancements, such as Gemini \citep{geminiteam2023gemini} and LWM \citep{liu2024world}, have focused on addressing these issues by optimizing token efficiency and extending the context length, paving the way for more scalable and effective MLLMs.

The remarkable success of large language models (LLMs) \citep{radford2019language, brown2020language} has spurred a growing trend of extending their advanced reasoning capabilities to multi-modal tasks, leading to the development of vision-language models (VLMs) \citep{huang2023languageneedaligningperception, driess2023palmeembodiedmultimodallanguage, liu2024visual, Qwen-VL}. These VLMs typically consist of a visual encoder \citep{radford2021learning} that serializes input image representations and an LLM responsible for text generation. To enable the LLM to process visual inputs, an alignment module is employed to bridge the gap between visual and textual modalities. This module can take various forms, such as a simple linear layer, an MLP projector, or a more complex query-based network. While this integration allows the LLM to gain visual perception, it also introduces significant computational challenges due to the long sequences of visual tokens.

Moreover, existing VLMs often exhibit limitations, such as visual shortcomings or hallucinations, which hinder their performance. Efforts to enhance VLM capabilities by increasing input image resolution have further exacerbated computational demands. For instance, encoding higher-resolution images results in a substantial increase in the number of visual tokens. A model like LLaVA-1.5 \citep{liu2024improved} generates 576 visual tokens for a single image, while its successor, LLaVA-NeXT \citep{liu2024llavanext}, produces up to 2880 tokens at double the resolution, far exceeding the length of typical textual prompts.
Optimizing the inference efficiency of VLMs is thus a critical task to facilitate their deployment in real-world scenarios with limited computational resources.

\subsection{Visual Token Compression}
% Visual tokens often exceed text tokens by tens to hundreds of times, with visual signals being more spatially redundant compared to information dense text \citep{marr2010vision}.
% Various methods have been proposed to address this issue. For instance, LLaMA-VID \citep{li2023llama} uses a Q-Former with context tokens, and DeCo \citep{yao2024deco} applies adaptive pooling to downsample visual tokens at the patch level.
% However, these approaches require modifying model components and additional training, increasing computational and training costs.
% ToMe~\citep{bolya2022tome} reduces tokens without training by adding a token merge module to ViTs, but this disrupts early cross-modal interactions in language models~\citep{xing2024PyramidDrop}. FastV~\citep{chen2024image} selects important visual tokens using attention scores, while SparseVLM~\citep{zhang2024sparsevlm} incorporates text guidance via cross-modal attention.
% However, these methods forgo flash-attention~\citep{dao2022flashattention, dao2023flashattention2} and primarily focus on token importance, overlooking the impact of token duplication.
% In our work, we preserve hardware acceleration compatibility, including flash attention, while considering both token importance and duplication for token reduction.

Visual tokens are often significantly more numerous than text tokens, with higher spatial redundancy and lower information density. To address this issue, various methods have been proposed for reducing visual token counts in vision language models. For instance, some approaches modify model components, such as using context tokens in Q-Former \citep{li2023llama} or applying adaptive pooling at the patch level, but these typically require additional training and increase computational costs. Other techniques, like Token Merging (ToMe) \citep{bolya2022tome} and FastV \citep{chen2024image}, focus on reducing tokens without retraining by merging tokens or selecting important ones based on attention scores. SparseVLM \cite{zhang2024sparsevlm} incorporates text guidance through cross-modal attention to refine token selection. However, these methods often overlook hardware acceleration compatibility and fail to account for token duplication alongside token importance. Furthermore, while token pruning has been extensively explored in natural language processing and computer vision to improve inference efficiency, its application to VLMs remains under-explored. Existing pruning strategies, such as those in FastV and SparseVLM, rely on text-visual attention within large language models (LLMs) to evaluate token importance, which may not align well with actual visual token relevance.

