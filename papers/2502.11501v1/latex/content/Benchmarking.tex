\section{Benchmarking}

% Conducting a comprehensive benchmarking study across a diverse range of methods necessitates substantial effort in the design, execution, and allocation of computational resources. To begin, we present an overview of the datasets and methodologies included in our study, along with the rationale for their selection. Subsequently, we elaborate on the experimental setup and provide guidance on interpreting the results presented in our reports. Lastly, we examine the findings by emphasizing notable observations and offering insights that may inform future research endeavors.

% Conducting a comprehensive benchmark of a diverse range of methods demands considerable effort in designing experiments. 

We begin by presenting the datasets, models, and pruning methods included in our study, along with the rationale behind selection.
Next, we outline the experimental setup and provide guidance on interpreting the results reported in our study. Finally, we analyze the findings, emphasizing notable patterns and offering insights that may inform future research in this area.


\subsection{Models}
We selected several representative open-source MLLMs, including LLaVA-1.5-7B \cite{liu2024improved}, LLaVA-Next-7B \citep{liu2024llavanext}, and Qwen2-VL \citep{wang2024qwen2} series (7B-Instruct and 72B-Instruct).
LLaVA-1.5-7B integrates CLIP and LLaMA for vision-language alignment via end-to-end training,
employing MLP connectors to fuse visual-text features for multimodal reasoning.
LLaVA-Next-7B enhances data efficiency and inference robustness with dynamic resolution and hierarchical feature integration,
improving fine-grained visual understanding.
Qwen2-VL series  excel in high-resolution input processing and instruction-following,
supporting complex tasks like document analysis and cross-modal in-context learning through unified vision-language representations.


\subsection{Datasets}

To evaluate the impact of pruning on different tasks, we selected a diverse set of datasets, including visual understanding tasks including GQA \citep{hudson2019gqa}, MMBench (MMB) \citep{liu2025mmbench}, MME \citep{fu2023mme}, POPE \citep{li2023evaluating}, ScienceQA \citep{lu2022learn}, VQA$^{\text{V2}}$ (VQA V2) \citep{goyal2017making} and VQA$^{\text{Text}}$ (TextVQA) \citep{singh2019towards}, grounding task RefCOCO \citep{yu2016modelingcontextreferringexpressions, mao2016generationcomprehensionunambiguousobject} and object retrieval task Visual Haystack \citep{wu2025visual}, . We briefly introduce these datasets in Table \ref{tab:datasets}.

% We have conducted empirical analysis and research on token pruning across multiple benchmarks, including GQA \citep{hudson2019gqa}, MMBench (MMB) and MMB-CN \citep{liu2025mmbench}, MME \citep{fu2023mme}, POPE~\citep{li2023evaluating}, VizWiz \citep{bigham2010vizwiz}, SQA \citep{lu2022learn}, VQA$^{\text{V2}}$ (VQA V2) \citep{goyal2017making}, VQA$^{\text{Text}}$ (TextVQA) \citep{singh2019towards}, and OCRBench~\citep{liu2024ocrbench}.
% todo: refine again

% \input{latex/figure/compared_visual_cases}

\subsection{Token Pruning Method}

To rigorously evaluate the properties of visual token pruning, we select three representative and high-performing methods: FastV \citep{chen2024image}, SparseVLM \cite{zhang2024sparsevlm}, and MustDrop \citep{liu2024multi}.
FastV \cite{chen2024image} optimizes computational efficiency by learning adaptive attention patterns in early layers and pruning low-attention visual tokens post-layer 2 of LLMs, effectively reducing redundancy.
SparseVLM \citep{zhang2024sparsevlm} introduces a text-guided, training-free pruning mechanism that leverages self-attention matrices between text and visual tokens to assess importance. It maximizes sparsity while preserving semantically relevant tokens without additional parameters or fine-tuning.
MustDrop \citep{liu2024multi} addresses token redundancy across the entire model lifecycle. It merges spatially similar tokens during vision encoding, employs text-guided dual-attention filtering in prefilling, and implements output-aware KV cache compression during decoding. This multi-stage approach ensures balanced retention of critical tokens while enhancing inference efficiency.
These methods exemplify diverse strategies for token pruning, spanning adaptive attention, text-guided sparsity, and lifecycle-aware optimization.



% \subsection{Implementation Details}





% \subsection{Main Results}



\input{latex/content/table/image_understanding}




