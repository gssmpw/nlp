\section{Language in  Visual Token Pruning: When and Why Does Language Matter?}



Token pruning methods for multimodal models can be broadly categorized into two types: those guided by textual information (e.g., FastV \cite{chen2024image}, SparseVLM \cite{zhang2024sparsevlm}, MustDrop \cite{liu2024multi}) and those that rely solely on visual information (e.g., FasterVLM \cite{zhang2024clsattentionneedtrainingfree}). While both approaches achieve comparable performance on common benchmarks, however, we hypothesize: Could it be that the importance of language information is not evident simply because there has been a lack of testing on tasks where language information is especially critical? To validate our hypothesis, we select a typical scenario: Visual Haystack.

% we argue that their effectiveness is task-dependent. Specifically, tasks requiring strong textual guidance benefit more from text-guided pruning, whereas multi-turn dialogue tasks reveal a trade-off due to the bias introduced by early-stage text information.

\subsection{Visual Token Pruning in Strongly Text-Guided Tasks}

% Tasks such as grounding (RefCOCO \citep{yu2016modelingcontextreferringexpressions}) and visual haystack \citep{wu2025visual} (needle-in-a-haystack) are inherently text-driven. In RefCOCO, the model identifies bounding boxes based on descriptive phrases, while in visual haystack, it determines whether an object matching a textual description exists within a stack of images. These tasks demand precise alignment between textual and visual modalities. To evaluate the impact of text-guided pruning, we conducted experiments using the LLaVA-1.5-7B model on these datasets.


Tasks such as Visual Haystack \citep{wu2025visual} (needle-in-a-haystack task on visual scenario) are inherently text-driven. In Visual Haystack task, the MLLM needs to select an image from a set of confusing images with an anchor phrase, and determine whether an object matching a target textual description exists within the selected image. These tasks demand precise alignment between textual and visual modalities. To evaluate the impact of text-guided pruning, we conducted experiments using the LLaVA-1.5-7B model on the VH dataset.

\input{latex/content/table/Visual_Haystack}


To validate the importance of text guidance, we modified FastV to operate without textual information and denote it FastV$_{\texttt{VIS}}$. Originally, FastV calculates the importance of visual tokens based on the attention score with the last text token. FastV$_{\texttt{VIS}}$ computes with the last visual token instead, thereby eliminating the influence of text information while preserving the essence of the method. Our results in Table \ref{tab:visual_haystack} show that this modificaiton FastV$_{\texttt{VIS}}$ reveals a significant drop in performance, confirming the importance of leveraging textual cues in strongly text-guided tasks. The comparison of different pruning methods also reveals that approaches utilizing visual information exhibit significantly better overall performance. It is noteworthy that SparseVLM, guided by text information, achieves a compression rate of 77.8\% while maintaining nearly identical accuracy to the uncompressed model, particularly in scenarios with a higher number of confusing images.

However, there are also recent works methods \citep{zhang2024clsattentionneedtrainingfree, liu2025compressionglobalguidancetrainingfree} that perform pruning solely in ViT without textual information and reports better performance than FastV and SparseVLM in common VQA benchmarks. 

Therefore, for tasks with high reliance on language information, pruning strategies should be tailored to incorporate textual guidance effectively, and how to balance the use of linguistic information still requires further research.


% For instance, FastV \citep{chen2024image}, a representative text-guided method, demonstrates superior performance in terms of accuracy and efficiency. 



% \subsection{The Trade-Off in Multi-Turn Dialogue Tasks}
% In contrast, multi-turn dialogue tasks present a different challenge. Current pruning methods often perform token pruning during the prefilling stage of the first dialogue turn, relying either on visual information alone or combining it with available textual context. However, text-guided pruning introduces a bias toward the initial text tokens, which may not align with future dialogue turns. This premature pruning leads to the loss of visually generic yet informative tokens, negatively impacting subsequent dialogue rounds.

% To investigate this phenomenon, we tested various pruning methods on the MMDU dataset \citep{liu2024mmdu}, a multi-image, multi-turn dialogue benchmark averaging 15 dialogue turns per sample. Our findings indicate that text-guided pruning methods exhibit a noticeable decline in performance during later dialogue turns compared to vision-only pruning approaches. This suggests that while text-guided pruning excels in single-turn scenarios, it sacrifices generality for specificity, resulting in suboptimal performance in multi-turn settings. Therefore, there exists a trade-off between leveraging textual information for immediate gains and maintaining visual generality for sustained dialogue quality.

% \subsection{Wrap-up}
% Through our experiments, we demonstrated that the utility of textual information in visual token pruning is highly task-dependent. In strongly text-guided tasks like grounding and visual haystack, incorporating textual cues enhances performance. However, in multi-turn dialogues, the bias introduced by early-stage text information leads to a decline in later turns due to the trade-off between specific textual alignment and visual generality. Consequently, pruning strategies must be carefully designed and adapted to the requirements of specific tasks.
% \textcolor{red}{\textbf{TLDR:} Text-guided pruning improves performance in text-heavy tasks but introduces bias in multi-turn dialogues, trading generality for specificity; pruning methods should adapt to task needs.}
\vspace{-2mm}
\begin{takeaways}
\ \paragraph{Summary 2.} 
    \emph{Text-guided pruning improves performance in text-heavy tasks. Pruning methods should adapt to task needs.}
    % \emph{Text-guided pruning improves performance in text-heavy tasks but introduces bias in multi-turn dialogues, trading generality for specificity; pruning methods should adapt to task needs.}
\end{takeaways}

