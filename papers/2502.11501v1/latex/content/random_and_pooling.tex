\input{latex/figure/position_bias_frequency}
\section{Token Pruning Revisited: Are Simple Methods Better?}
When considering token pruning in multimodal large language models, two very basic methods naturally come to mind: random token pruning (hereafter referred to as \textbf{Random}) and token pooling (hereafter referred to as \textbf{Pooling}). Comparison with these two simple baselines is reliable evidence to demonstrate the significance of a well-designed token pruning method, yet has been ignored by most previous works. To address this gap, we investigated these two simple approaches in detail. Specifically, we conducted experiments on multiple widely-used benchmarks under pruning ratios of 75\% and 87.5\%, comparing Random and Pooling\footnote{In the experiments, Pooling specifically refers to applying a pooling operation to the visual tokens at the second layer of the language model. Please refer to the specific implementation in Algorithm~\ref{alg:pooling}.} with several recent token pruning methods (\emph{e.g.}, FastV and SparseVLM). 
As shown in Table~\ref{tab:random_and_pooling}, surprisingly, Random and Pooling outperformed carefully designed methods on nearly $2/3$ benchmarks.
These surprising results shocked us since its inferior performance compared with random selection may demonstrate that we are on the wrong road toward the ideal token pruning methods.



\subsection{Token Distribution: Spatial Uniformity Outperforms Position Bias}
\input{latex/figure/visual_cases}
\noindent We further explored the underlying reasons behind this phenomenon. Taking FastV~\citep{chen2024image} as an example, this method leverages the attention scores assigned to visual tokens by the last token to evaluate the importance of each visual token, which may introduce the basis for token pruning.

Using 8,910 samples from the POPE dataset, we conducted a statistical analysis of the visual tokens retained by FastV. As illustrated in Figure~\ref{fig:position_bias}, tokens located toward the end of the visual token sequence were assigned significantly higher attention scores and were retained far more frequently than tokens in other positions. This indicates that methods relying on attention scores to select visual tokens inherently suffer from a severe position bias during token reduction. In contrast, tokens retained by Random or Pooling exhibit a naturally uniform spatial distribution. We argue that this spatial uniformity may be the key reason why some existing methods underperform Random and Pooling.

\subsection{Validating the Hypothesis: From Position Bias to Spatial Uniformity}
To validate our hypothesis, we proposed a modification to FastV, introducing a variant called Window FastV. Specifically, we incorporated a sliding window mechanism into the original FastV framework. Within each window, a predetermined reduction ratio and window size were used to select a fixed number of visual tokens. For the specific implementation of Window FastV, please refer to Algorithm~\ref{alg:window_fastv} in Appendix~\ref{app:algorithms}.
Compared to Vanilla FastV, Window FastV ensures the spatial uniformity of the retained tokens, as shown in Figure~\ref{fig:intro_graph}.


We evaluated both Vanilla FastV and Window FastV across eight benchmarks. As shown in Table~\ref{tab:random_and_pooling}, under the setting where 75\% of visual tokens are reduced, Window FastV exhibits an average performance drop that is \textbf{3.4\%} less than that of Vanilla FastV. When adopting a more aggressive reduction ratio (\(\downarrow 88.9\%\)), this gap widens to \textbf{9\%}. These results not only validate our hypothesis but also inspire us to consider strategies that encourage the spatial uniformity of retained tokens when designing token pruning methods. 
% \textcolor{red}{\textbf{TLDR:} The position bias in the distribution of retained visual tokens is a key factor affecting the performance of some existing token pruning methods. This insight suggests that ensuring the spatial uniformity of retained tokens should be an important consideration when designing token pruning strategies.}

To further investigate the impact of token pruning on spatial position understanding, we selected the RefCOCO \citep{yu2016modelingcontextreferringexpressions} dataset, which requires the MLLM to generate a bounding box for a specified object phrase within an image. We consider this dataset to be an effective atomic benchmark for evaluating the spatial understanding capabilities of MLLMs. Our evaluation criterion is that a prediction is considered correct if the Intersection over Union (IoU) between the predicted bounding box and the ground truth area exceeds 0.5. As shown in Table \ref{fig:refcoco_grounding}, compared to conventional tasks, various token pruning methods exhibit a significant degradation in performance when applied to precise object localization. Notably, there is a marked difference between globally uniform attention-based pruning methods (e.g., Window FastV, or even naive approaches like Random and Pooling) and spatially non-uniform strategies (FastV, SparseVLM). This indicates that current token pruning techniques, particularly those that are spatially non-uniform, still possess substantial limitations in comprehending the spatial positioning of objects within images.

\input{latex/content/table/grounding_refcoco}

\vspace{-2mm}
\begin{takeaways}
\ \paragraph{Summary 1.} 
    \emph{The position bias in the distribution of retained visual tokens is a key factor affecting the performance of some existing token pruning methods. This insight suggests that ensuring the spatial uniformity of retained tokens should be an important consideration when designing token pruning strategies.}
\end{takeaways}

