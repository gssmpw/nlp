\section{Limitations and Challenges in Token Pruning Evaluation}
Token pruning has emerged as a promising technique to improve the efficiency of MLLMs. However, despite its potential, the evaluation of token pruning methods remains fraught with challenges. 
In this section, we critically examine two key issues that hinder the accurate and meaningful assessment of token pruning techniques: \textbf{(i)} the over-reliance on FLOPs as a proxy for speed gains, and \textbf{(ii)} the failure to account for training-aware compression in some advanced MLLMs. 
We argue that addressing these challenges is crucial for developing more robust and reliable token pruning approaches.

\subsection{Beyond FLOPs: Shifting the Focus to Actual Latency Gains}
\input{latex/content/table/efficiency}
\noindent\textbf{Phenomenon.} Many existing token pruning approaches tend to measure the speedup of their methods by calculating or estimating the reduction in FLOPs resulting from token reduction, or even directly using the token reduction ratio as a metric. However, can FLOPs or token reduction ratios truly reflect the actual acceleration achieved?

To investigate this question, we examined the speedup effects reported by several works. Our findings reveal that even when different methods exhibit identical or similar reduction ratios and FLOPs, their measured speeds can vary significantly. 
Table~\ref{tab:efficiency} presents the efficiency-related experimental results of these methods on LLaVA-Next-7B\footnote{\url{https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b}}. Specifically, under the same setting, SparseVLM's FLOPs are only \textbf{2.8\%} higher than those of FastV, yet its latency is \textbf{26.8\%} greater. 
This strongly suggests that relying on FLOPs to evaluate acceleration effects of proposed methods is inadequate. When assessing speed gains, it is imperative to \emph{shift our focus to actual latency measurements}.

\noindent\textbf{Reason.} We also conducted a detailed analysis of the design intricacies of the three methods to uncover the underlying reasons for their performance differences. Specifically, FastV, SparseVLM, and MustDrop all fail to support the efficient Flash Attention operator~\citep{dao2022flashattention, dao2023flashattention2}, as they rely on the complete attention map to select visual tokens. However, FastV performs token pruning in only one layer of the language model, whereas the other two methods conduct pruning across four layers. This implies that, compared to FastV, these methods have more layers that are forced to use the traditional attention operator with $O(N^2)$ memory costs. This could be one of the key factors contributing to their slower speeds.
Additionally, performing pruning layer by layer requires more complex operations to handle token selection. If the runtime overhead introduced during this stage becomes significant, it may offset the speed gains achieved by shortening the token sequence. Moreover, some of the transformer layers where these methods perform pruning are located deeper within the model. Pruning tokens in such deep layers may yield limited benefits, as the impact of token reduction diminishes at later stages of the network.

\noindent\textbf{Appeal.} This insight motivates us to consider the compatibility with efficient attention operators when designing token pruning methods. Additionally, it encourages us to implement the token pruning process as early as possible in the shallow layers using simpler approaches, avoiding the risk of excessive runtime overhead that could otherwise overshadow the intended acceleration benefits.
\vspace{-2mm}
\begin{takeaways}
\ \paragraph{Summary 4.} 
    \emph{(i) FLOPs are not a reliable metric for evaluating speed gains; greater emphasis should be placed on actual latency. (ii) We advocate for the implementation of token pruning in the shallow layers of MLLMs using simple or efficient operations, while ensuring compatibility with Flash Attention.}
\end{takeaways}

\subsection{The Overlooked Role of Training-Aware Compression in MLLMs}
\input{latex/content/table/qwen2vl}
In recent years, some of the latest MLLMs have adopted various advanced techniques during the training phase to enhance their efficiency. For instance, Qwen2-VL employs token merging strategy during training, consolidating four adjacent patches into a single visual token. Similarly, MiniCPM-V-2.6 incorporates a learnable query within its resetting module, mapping variable-length segment features into more compact representations.

This raises an intriguing question: \emph{If MLLMs already implement training-aware compression techniques, should we take this into account when designing and evaluating token pruning methods for the inference stage?} Given that the visual tokens encoded by these models possess higher information density, removing the same number of visual tokens could result in greater information loss compared to traditional approaches.

To this end, we selected a representative MLLM that employs training-aware compression, Qwen2-VL-7B-Instruct\footnote{\url{https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct}} and conducted a series of experimental analyses. Specifically, we applied FastV in two sets of experiments: one disregarding the token compression performed during Qwen2-VL's training phase, and the other taking it into account:
Let $\mathcal{P}$ denote the original number of image patches, and after processing with PatchMerger, the number of visual tokens $\mathcal{V}$ is:
\begin{equation}
    \mathcal{V} = \frac{\mathcal{P}}{\text{TACR}},
\end{equation}
where TACR means training-aware compression ratio and the value is 4. 

Finally, the Token Reduction Rate (TRR) can be formally defined as:
\begin{equation}
    \text{TRR}(\text{FastV}^\dagger) \triangleq 
    \underbrace{\text{TACR}}_{\text{Training-aware}} \times 
    \underbrace{\text{TFRR}}_{\text{Training-free}}.
    \vspace{-1mm}
\end{equation}

Surprisingly, as shown in Table~\ref{app_tab:qwen2vl}, we find that when taking training-aware compression into account, the same token pruning method achieves performance on par with the vanilla model across multiple benchmarks, even under varying reduction ratios.
This observation prompts us to reflect: \emph{perhaps more research effort should be directed toward training-aware token compression techniques}. Even in cross-model comparisons, such as between LLaVA-1.5-7B (Vanilla FastV) in Table~\ref{tab:random_and_pooling}, which does not employ training-aware compression, and Qwen2-VL-7B-Instruct (FastV$^\dagger$), the latter clearly demonstrates less performance degradation. 
\vspace{-2mm}
\begin{takeaways}
\ \paragraph{Summary 5.} 
    \emph{Training-aware token compression techniques deserve more research attention due to their potential for delivering superior performance guarantees.}
\end{takeaways}

