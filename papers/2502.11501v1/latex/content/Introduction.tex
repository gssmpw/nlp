\section{Introduction}
% 引入mllm
Multi-modal language models (MLLMs) \citep{huang2023languageneedaligningperception, driess2023palmeembodiedmultimodallanguage, liu2024visual, Qwen-VL}, especially the vision-language models have demonstrated powerful effectiveness in various tasks. However, the extremely high computational and storage costs have limited the application of MLLMs in real-time applications, which is caused by not only the enormous parameters inherited from LLMs but also a large number of tokens from the large visual information such as high-resolution images and multi-frame videos. 


To solve this problem, abundant efforts have been made in \textbf{token pruning} \citep{chen2024image, zhang2024sparsevlm, liu2024multi}, which aims to reduce the number of input tokens in MLLMs. Usually, token pruning methods first introduce a carefully-designed criterion to measure the importance of a vision token, and then prune the redundant tokens, or merge the redundant tokens into fewer tokens. As a result, the following computation of the pruned tokens or the merged tokens can be removed or reduced, bringing efficiency in both computation and storage. For instance, some recent works show that more than 70\% tokens can be pruned with tolerant loss in accuracy \citep{chen2024image}. Most attractively, thanks to the natural ability of MLLMs to process tokens in different lengths, token pruning can be applied to most existing MLLMs with no need for additional training, and thus attracts great attention from both academic researchers and industrial developers. 

\input{latex/figure/intro_hist}

However, despite the popularity of token pruning, numerous foundational questions have long been overlooked and remain largely unexplored, giving rise to several surprising phenomena. For instance, Figure \ref{fig:intro_hist} demonstrates the comparison between two classical token pruning methods including FastV \citep{chen2024image} and SparseVLM \citep{zhang2024sparsevlm}, and two naive baselines, including random token selection and direct average pooling on tokens. \textbf{Surprisingly, the two baselines outperform the two well-designed token pruning methods in most benchmarks by a clear margin}. This counterintuitive phenomenon may demonstrate that the current understanding of so-called important tokens is far away from the truth. Unfortunately, most recent works just focus on purchasing higher performance, while ignoring these questions, which may hinder the long-term development of token pruning.


In this paper, we have conducted massive experiments and analyses to dive into the fundamental problems of token pruning, with the main takeaways as follows.
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
    \item Attention-based token selection methods suffer from position bias, where vision tokens in the later positions are more likely to be retained. Reducing position bias in these methods can benefit their performance.
    \item Language information is helpful in token pruning only when a given task strongly correlates with the language information.
    \item Both the importance and uniqueness (low similarity) of tokens
    have a significant influence on the performance of token pruning and their influence varies from different tasks.
    \item FLOPs and the number of retained tokens are unreliable metrics for token pruning methods. Compatibility with hardware has a significant influence on real acceleration performance.
    \item Training-aware token pruning which directly merges tokens in spatially adjacent positions may bring more benefits than carefully-designed training-aware pruning.
\end{itemize}

% \input{latex/figure/intro_graph}

We hope that this paper can provide insights into the future design of token pruning, and correct the long-neglected evaluation issues in this field.
%For instance, the performance of many classic token pruning methods falls short of that achieved through random token deletion or direct average pooling of tokens. Some methods, despite eliminating a substantial number of tokens, fail to deliver real inference benefits. Why do many methods that consider both language and visual information underperform compared to those relying solely on visual data? The ignorance of these questions may hinder token pruning methods to move forward.


