% \section{The \protect\boldmath{$\alpha$} Dilemma: Importance vs. Redundancy in Token Pruning}
% In this section, we focus on another noteworthy issue in token pruning for multimodal large language models: \emph{should we remove redundant, similar tokens, or should we eliminate less important ones?}

% \paragraph{Redundancy criteria}
% Based on the redundancy criterion, the underlying goal is to eliminate redundant and repetitive information within the token sequence, ensuring that the amount of information contained in the sequence remains largely unchanged before and after token pruning, thereby minimizing information loss.



% \paragraph{Important criteria}
% The motivation behind the importance criterion is to retain only those tokens that make significant contributions to the final generation and prediction. Essentially, this approach aims to ensure that, after token pruning, the model can still achieve nearly the same generation and prediction performance using the remaining tokens.

% ------------------- v2 -------------------------
% \section{The \protect\boldmath{$\alpha$} Dilemma: Importance vs. Redundancy in Token Pruning}
% In this section, we focus on another noteworthy issue in token pruning for multimodal large language models: \emph{should we remove redundant, similar tokens, or should we eliminate less important ones?}

% \paragraph{Redundancy criteria}
% Based on the redundancy criterion, the underlying goal is to eliminate redundant and repetitive information within the token sequence, ensuring that the amount of information contained in the sequence remains largely unchanged before and after token pruning, thereby minimizing information loss. 

% From the perspective of mutual information theory, this criterion can be formally expressed as maximizing the \emph{\textbf{information preservation rate}} between the original token set $X$ and the retained token set $X'$:
% \begin{equation}
%     \max_{\mathcal{P}} I(X; X') = \mathcal{H}(X) - \mathcal{H}(X|X'),
% \end{equation}
% where $\mathcal{P}$ denotes the pruning operator, $\mathcal{H}(\cdot)$ is the Shannon entropy. The objective implies that the retained tokens $X'$ should maintain maximum mutual dependence with the original sequence $X$, subject to the length reduction constraint $\|X'\| = \|X\| - \Delta L$. This guarantees that the conditional entropy $\mathcal{H}(X|X')$ (\emph{i.e.}, the uncertainty about $X$ given $X'$) is minimized.

% \paragraph{Important criteria}
% The motivation behind the importance criterion is to retain only those tokens that make significant contributions to the final generation and prediction. Essentially, this aims to ensure that, after token pruning, the model can still achieve nearly the same generation and prediction performance using the remaining tokens.

% Therefore, the criterion requires preserving the \emph{\textbf{predictive sufficiency}} between the retained tokens $X'$ and the target output $Y$:
% \begin{equation}
%     I(X'; Y) \geq I(X; Y) - \epsilon,
% \end{equation}
% where $\epsilon \geq 0$ is the tolerable information loss threshold. Expanding this via the chain rule of mutual information:
% \begin{equation}
%     \underbrace{I(X; Y)}_{\text{Original}} = \underbrace{I(X'; Y)}_{\text{Pruned}} + \underbrace{I(X\setminus X'; Y|X')}_{\text{Discarded}}.
% \end{equation}
% The importance criterion essentially enforces $I(X\setminus X'; Y|X') \leq \epsilon$, meaning the conditional mutual information between discarded tokens $X\setminus X'$ and target $Y$ (given retained tokens $X'$) must be bounded, which captures the intuition that truly important tokens should have non-decomposable predictive information about $Y$.


% ------------------------ v3 ----------------------
% \section{The \protect\boldmath{$\alpha$} Dilemma: Importance vs. Redundancy in Token Pruning}
% In this section, we focus on another noteworthy issue in token pruning for multimodal large language models: \emph{should we remove redundant, similar tokens, or should we eliminate less important ones?}

% \subsection{Redundancy criteria} % NEW: 增加task-agnostic属性标注
% \label{sec:Redundancy_criteria}
% This \emph{task-agnostic} criterion focuses solely on the input itself. The underlying goal is to eliminate redundant information within the token sequence, ensuring that the amount of information contained in the sequence remains largely unchanged before and after token pruning, thereby minimizing information loss and preserving structural integrity.

% From the perspective of mutual information theory~\citep{latham2009mutual, kraskov2004estimating}, this criterion can be formally expressed as maximizing the \emph{information preservation rate} between the original token set $X$ and the retained token set $X'$:
% \begin{equation}
%     \max_{\mathcal{P}} I(X; X') = \mathcal{H}(X) - \mathcal{H}(X|X'),
% \end{equation}
% where $\mathcal{P}$ denotes the pruning operator, $\mathcal{H}(\cdot)$ is the Shannon entropy. The objective implies that the pruned sequence $X'$ should maintain maximum mutual dependence with the original sequence $X$, subject to the length reduction constraint $\|X'\| = \|X\| - \Delta L$. This guarantees that the conditional entropy $\mathcal{H}(X|X')$ (\emph{i.e.}, the uncertainty about $X$ given $X'$) is minimized.

% % NEW: 增加与信息瓶颈理论的联系
% This aligns with the \emph{compression phase} in the information bottleneck principle~\citep{tishby2000information}, where the minimal sufficient statistics are preserved without considering downstream tasks. The optimal pruning operator $\mathcal{P}^*$ can be viewed as finding the minimal representation:
% \begin{equation}
%     \mathcal{P}^* = \argmin_{\mathcal{P}} \|X'\| \quad \text{s.t.} \ I(X;X') \geq \gamma,
% \end{equation}
% where $\gamma$ is the minimally acceptable mutual information threshold.

% \subsection{Important criteria} % NEW: 增加task-oriented属性标注
% \label{sec:Important_criteria}
% As a \emph{task-oriented} criterion, this approach explicitly considers the target output $Y$ (\emph{e.g.}, generated text or prediction labels). The motivation is to retain only those tokens that make significant contributions to the final generation and prediction. Essentially, this approach aims to ensure that, after token pruning, the model can still achieve nearly the same generation and prediction performance using the remaining tokens.

% Therefore, the criterion requires preserving the \emph{predictive sufficiency} between the retained tokens $X'$ and the target output $Y$:
% \begin{equation}
%     I(X'; Y) \geq I(X; Y) - \epsilon,
% \end{equation}
% where $\epsilon \geq 0$ is the tolerable information loss threshold. Expanding this via the chain rule of mutual information:
% \begin{equation}
%     \underbrace{I(X; Y)}_{\text{Original}} = \underbrace{I(X'; Y)}_{\text{Pruned}} + \underbrace{I(X\setminus X'; Y|X')}_{\text{Discarded}}
% \end{equation}
% The importance criterion essentially enforces $I(X\setminus X'; Y|X') \leq \epsilon$, meaning the conditional mutual information between discarded tokens $X\setminus X'$ and target $Y$ (given retained tokens $X'$) must be bounded. This mathematically captures the intuition that truly important tokens should have non-decomposable predictive information about $Y$.

% % NEW: 增加任务依赖性的数学描述
% The task-dependent nature can be further characterized by the \emph{information plane}:
% \begin{equation}
%     \mathcal{R}(\beta) = \max_{X'} \left[ I(X';Y) - \beta^{-1}I(X;X') \right]
%     \label{eq:banlance}
% \end{equation}
% where $\beta$ controls the trade-off between the two criteria, revealing that importance-centric pruning corresponds to $\beta \to \infty$ while redundancy-centric pruning to $\beta \to 0$.


% \subsection{Empirical Validation of Criteria Balance}\label{sec:Empirical_Validation}
% To operationalize the theoretical framework discussed above, %in Sec.\ref{sec:Redundancy_criteria} and Sec.~\ref{sec:Important_criteria}, 
% Motivated by Eq.~\ref{eq:banlance}, we implement an adaptive scoring mechanism that balances the two criteria through a tunable parameter $\alpha \in [0,1]$:
% % \begin{equation}
% %     \text{Final Score}(x_i) = \alpha \cdot \underbrace{I(x_i; Y|x_{\setminus i})}_{\text{importance}} + (1-\alpha) \cdot \underbrace{[1 - I(x_i; X_{\setminus i})]}_{\text{redundancy}},
% %     \label{eq:alpha_balance}
% % \end{equation}
% % \begin{align}
% %     \text{Final Score}(x_i) \\
% %     &\hspace{-5.5em} = \alpha \cdot \underbrace{I(x_i; Y|x_{\setminus i})}_{\text{importance}} \nonumber 
% %                              + (1-\alpha) \cdot \underbrace{[1 - I(x_i; X_{\setminus i})]}_{\text{redundancy}},
% %     \label{eq:alpha_balance}
% % \end{align}
% \begin{align}
%     \text{Final Score} (x_i) \nonumber \\
%     &\hspace{-5.5em} = \alpha \cdot \underbrace{I(x_i; Y|x_{\setminus i})}_{\text{importance}} 
%                              + (1-\alpha) \cdot \underbrace{[1 - I(x_i; X_{\setminus i})]}_{\text{redundancy}},
%     \label{eq:alpha_balance}
% \end{align}
% where $I(x_i; Y|x_{\setminus i})$ quantifies the \emph{unique predictive information} of token $x_i$ about target $Y$, and $1 - I(x_i; X_{\setminus i})$ measures its \emph{non-redundancy} with respect to other tokens. 
% The experimental results in Table~\ref{tab:similarity_vs_attention} reveal three key patterns:
% % \begin{itemize}
% %     \item \textbf{Task-Specific Optimality}: For perception-dominant tasks like MME and POPE, maximal performance occurs at $\alpha=0.3$ (MME: 1706) and $\alpha=0.0$ (POPE: 82.8), confirming their preference for \emph{redundancy-first} pruning. This aligns with our theoretical analysis -- preserving structural integrity ($I(X;X')$) proves crucial for visual grounding tasks.
    
% %     \item \textbf{Reasoning-Sensitive Tradeoff}: In knowledge-intensive benchmarks (SQA, VQA$^\text{Text}$), optimal $\alpha$ shifts toward 0.8-0.9 range (SQA: 65.7 at $\alpha=0.9$), demonstrating the necessity of \emph{importance-biased} pruning for semantic coherence. This empirically validates the chain rule decomposition in Eq.(3) -- retaining tokens with high $I(X';Y)$ becomes critical when reasoning chains dominate.
    
% %     \item \textbf{The Pitfall of Extremes}: The monotonic performance drop in POPE (71.8 at $\alpha=1.0$ vs. 82.8 at $\alpha=0.0$) and suboptimal results at $\alpha=1.0$ across all tasks underscore our core thesis -- pure importance criteria risk overpruning \emph{structurally critical redundancies}, while pure redundancy criteria may retain \emph{predictively irrelevant} tokens.
% % \end{itemize}

% Perception tasks (\emph{e.g.}, MME, POPE) peak at $\alpha=0.1$ and $0.0$, respectively, favoring redundancy-first pruning to preserve structural integrity ($I(X; X')$). \\
% Knowledge-intensive tasks (\emph{e.g.}, SQA, VQA$^\text{Text}$) perform best with $\alpha=0.8$-$0.9$, emphasizing importance-biased pruning for semantic coherence ($I(X';Y)$). \\
% Suboptimal results at $\alpha=1.0$ (\emph{e.g.}, POPE drops to 71.8 vs. 82.8 at $\alpha=0.0$) highlight the risks of over-pruning critical redundancies or retaining irrelevant tokens.


% The experimental findings resonate with our information-theoretic formulation. Let $\mathcal{L}(\alpha) = \gamma I(X;X') + (1-\gamma)I(X';Y)$ be the Lagrangian dual of Eq.~\ref{eq:alpha_balance}, where $\gamma \propto (1-\alpha)$. The empirical $\alpha$ sweet spots (0.3-0.9 across tasks) reveal intrinsic \emph{task-dependent information budgets} -- each benchmark demands unique balancing between:
% \begin{equation}
%     \underbrace{\mathcal{H}(Y|X')}_{\text{predictive uncertainty}} \leftrightarrow \underbrace{\mathcal{H}(X|X')}_{\text{structural uncertainty}}
% \end{equation}
% This duality suggests that optimal pruning requires \emph{jointly} preserving task-agnostic structural information and task-specific predictive information, with $\alpha$ acting as the control parameter between these competing objectives.




% --------------------- v4 ------------------------
\section{The \protect\boldmath{$\alpha$} Dilemma: Importance vs. Redundancy in Token Pruning}
\input{latex/content/table/similarity_vs_attention}
In this section, we systematically analyze the fundamental tension in token pruning for multimodal large language models: \emph{should we prioritize removing redundant tokens to preserve structural patterns, or eliminate less important tokens to maintain predictive capacity?}

\subsection{Redundancy Criteria} % 改进：增加副标题说明视角
\label{sec:Redundancy_criteria}
This criterion adopts a \emph{task-agnostic} perspective, focusing exclusively on input patterns. The core objective is to eliminate redundant tokens while preserving the input's \emph{structural integrity} and minimizing information loss - analogous to finding the minimal sufficient statistics in information theory.

Through the lens of mutual information~\citep{latham2009mutual}, we formulate this as maximizing information preservation between original tokens $X$ and retained tokens $X'$: % 改进：增加理论解释
\begin{equation}
    \max_{\mathcal{P}} I(X; X') = \mathcal{H}(X) - \mathcal{H}(X|X'),
\end{equation}
where $\mathcal{P}$ denotes the pruning operator. This ensures the $X'$ retains maximal dependence on $X$ under length constraint $\|X'\| = \|X\| - \Delta L$. The formulation directly connects to the \emph{compression phase} of the information bottleneck principle~\citep{tishby2000information}, where $\mathcal{P}^*$ solves:
\begin{equation}
    \mathcal{P}^* = \argmin_{\mathcal{P}} \|X'\| \quad \text{s.t.} \ I(X;X') \geq \gamma,
\end{equation}
with $\gamma$ as the minimal acceptable mutual information. This preserves structural patterns without task-specific considerations.

\subsection{Importance Criteria} % 改进：增加副标题说明视角
\label{sec:Important_criteria}
In contrast, this \emph{task-oriented} criterion explicitly considers the target output $Y$. The goal shifts to preserving tokens critical for \emph{prediction accuracy}, formalized through predictive sufficiency: % 改进：明确对比前文
\begin{equation}
    I(X'; Y) \geq I(X; Y) - \epsilon,
\end{equation}
where $\epsilon$ is the tolerable information loss. Expanding via the chain rule:
\begin{equation}
    \underbrace{I(X; Y)}_{\text{Original}} = \underbrace{I(X'; Y)}_{\text{Pruned}} + \underbrace{I(X\setminus X'; Y|X')}_{\text{Discarded}}.
\end{equation}
The bound $I(X\setminus X'; Y|X') \leq \epsilon$ implies that discarded tokens provide negligible additional information about $Y$ when conditioned on retained tokens. This captures the essence of importance - truly critical tokens contain non-decomposable predictive information. % 改进：增加直观解释

The task dependence manifests in the information plane:
\begin{equation}
    \mathcal{R}(\beta) = \max_{X'} \left[ I(X';Y) - \beta^{-1}I(X;X') \right],
    \label{eq:banlance}
\end{equation}
where $\beta$ controls redundancy-importance tradeoff. 
% This reveals our criteria as two extremes: pure importance ($\beta \to \infty$) vs. pure redundancy ($\beta \to 0$).

\subsection{Empirical Validation of Adaptive Criteria Balancing}
\label{sec:Empirical_Validation}
Building on Eq.~\ref{eq:banlance}, we implement an adaptive scoring mechanism with tunable parameter $\alpha$: 
\begin{align}
    \text{Score}(x_i) = \nonumber \\
    &\hspace{-3em} \alpha \cdot \hspace{-0.7em}\underbrace{I(x_i; Y|x_{\setminus i})}_{\text{Predictive Criticality}} \hspace{-0.7em}+ (1-\alpha) \cdot \underbrace{[1 - I(x_i; X_{\setminus i})]}_{\text{Pattern Uniqueness}}.
    \label{eq:alpha_balance}
\end{align}
Here $I(x_i; Y|x_{\setminus i})$ measures a token's unique predictive value, while $1 - I(x_i; X_{\setminus i})$ quantifies its pattern distinctiveness.

Specifically, FastV is a typical token pruning method that follows the importance criterion by selecting important visual tokens based on the attention scores of the last token in the sequence. We modify this approach by introducing a redundancy criterion, which calculates the cosine similarity between each visual token and the last token to derive a similarity score\footnote{Notably, since the similarity score and attention score are on different scales, we apply min-max normalization to both before computing the final score.}. Ultimately, the final score in Eq.~\ref{eq:alpha_balance} is obtained by balancing these two metrics with a parameter $\alpha$.
Our experiments results in Table~\ref{tab:similarity_vs_attention} reveal two key insights: % 改进：统一术语

\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
    \item \textbf{Perception-Dominant Tasks} (MME, POPE) achieve peak performance at $\alpha=0.1$ and $0.0$, respectively, favoring redundancy-first pruning to maintain structural integrity ($\uparrow I(X;X')$).
    
    \item \textbf{Knowledge-Intensive Tasks} (SQA, VQA$^\text{Text}$) achieve optimal performance with $\alpha=0.8\sim0.9$, favoring importance-first pruning to enhance semantic coherence ($\uparrow I(X';Y)$).
\end{itemize}
\vspace{-2mm}
\begin{takeaways}
\ \paragraph{Summary 3.}
\emph{Prune by task: Redundancy-first preserves structural fidelity for perception tasks, while importance-first prioritizes predictive power for knowledge reasoning.}
\end{takeaways}


% \item \textbf{The Peril of Extremes}: Catastrophic performance drops at $\alpha=1.0$ (POPE: 71.8 vs 82.8 at $\alpha=0.0$) demonstrate how pure importance pruning destroys structural criticalities, while pure redundancy pruning retains predictively irrelevant tokens.











% The Lagrangian dual $\mathcal{L}(\alpha) = \gamma I(X;X') + (1-\gamma)I(X';Y)$ reveals an intrinsic \emph{task-dependent information budget}, where optimal pruning balances: % 改进：加强理论联系
% \begin{equation}
%     \underbrace{\mathcal{H}(Y|X')}_{\text{Predictive Uncertainty}} \leftrightarrow \underbrace{\mathcal{H}(X|X')}_{\text{Structural Uncertainty}}
% \end{equation}
% This duality establishes $\alpha$ as the control parameter between preserving structural patterns and predictive signals - a fundamental tradeoff requiring explicit optimization for each task type.