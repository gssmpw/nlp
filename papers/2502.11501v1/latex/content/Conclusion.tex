\section{Conclusion}
% Our exploration of token pruning in MLLMs reveals critical insights: simple methods often outperform complex ones, position bias significantly affects performance, and language information's utility varies by task. We highlight the need to balance token importance and redundancy while emphasizing actual latency over FLOPs for evaluation. These findings pave the way for more effective and efficient token pruning strategies, fostering long-term advancements in MLLM optimization.

% Our systematic investigation into token pruning for MLLMs reveals critical yet overlooked issues in current methodologies. While existing approaches prioritize attention-based scoring and language-guided strategies, our experiments demonstrate that naive spatial uniformity—achieved through random selection or pooling—often surpasses complex designs due to inherent positional biases.
% Notably, the utility of linguistic guidance hinges on task alignment: text-driven scenarios benefit from cross-modal attention, whereas vision-centric tasks risk performance degradation.
% We also shed light on the essence of token pruning based on redundancy and importance from an information-theoretic perspective.
% Furthermore, the conventional reliance on FLOPs for efficiency evaluation proves inadequate, while latency offers a more practical and meaningful measure.
% We hope that these insightful findings will help guide the design of future token pruning methods.

Our systematic investigation into token pruning for MLLMs reveals several critical yet overlooked issues. While existing methods prioritize attention-based scoring and language-guided strategies, we demonstrate that naive spatial uniformity, achieved through random selection or pooling, often outperforms complex designs due to inherent positional biases in visual tokens. Notably, the effectiveness of linguistic guidance depends on task alignment: it enhances performance in text-driven scenarios through cross-modal attention but risks degradation in vision-centric tasks. 
From an information-theoretic perspective, we shed light on the core principles of token pruning, \emph{i.e.,} the pursuit of structural integrity versus prediction accuracy. Furthermore, we challenge the conventional reliance on FLOPs for efficiency evaluation, showing that latency serves as a more practical and meaningful metric. 
These findings provide a refined framework to guide the development of future token pruning methods, balancing simplicity, effectiveness, and task-specific adaptability.