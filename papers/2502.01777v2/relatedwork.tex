\section{Related Work}
\paragraph{Robustness to distribution shifts}
Prior work categorizes distribution shifts as domain generalization~\citep{quinonero2008dataset, santurkar2020breedsbenchmarkssubpopulationshift, hendrycks2021many}, where train and test data domains have no overlap, or subpopulation shifts~\citep{dixon2018measuring, oren-etal-2019-distributionally, groupdro20}, where train and test data come from the same domains, but do not necessarily appear in the same proportions~\citep{koh2021wilds}. Our experimental setup is an example of a subpopulation shift, as all test languages are included in the training data for the models.

Methods for robust generalization are commonly categorized into three groups. Domain invariance methods aim to learn feature representations that are consistent across domains (groups) by encouraging similar feature distributions across domains~\citep{tzeng2014deep, long2015learning, ganin2016domain, coral16}. Other approaches use invariant prediction methods~\citep{meinshausen2014maximin, peters2016causal, arjovsky2019invariant, rothenhausler2021anchor} from causal inference literature. In contrast, \texttt{DRO} explicitly minimizes the worst-case loss over an uncertainty set, which is typically defined as a divergence ball around the training distribution~\citep{namkoong2016stochastic, mohajerin2018data, bertsimas2018data, duchi2019variance, oren-etal-2019-distributionally, groupdro20}. Our work builds upon \orig{}~\citep{groupdro20}, since it has outperformed other approaches in settings with subpopulation shifts~\citep{koh2021wilds}.

\paragraph{Robust ASR}
Prior work on robustness in ASR primarily focuses on quantifying or addressing biases related to accent, age, dialect, gender, and race ~\citep{tatman-2017-gender, koenecke2020, ngueajio2022hey, markl, martin2023, feng2024101567, harris-etal-2024-modeling}.
Methods to mitigate these biases include data balancing~\citep{dheram2022toward} and fairness-promoting training methods~\citep{sari, zhang2022mitigating, veliche}.
These methods are not appropriate for reducing ASR language disparities, as they either require large amounts of training data, which may not be available for most languages, or improve performance for certain groups at a substantial cost to others.
\citet{gao2022domain} explored \texttt{DRO} for training language-independent speech recognition models, and reported  negative results. To the best of our knowledge, our work is the first to propose a robust optimization method that successfully reduces cross-lingual performance disparities in ASR.