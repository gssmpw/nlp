%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Others
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
% \usepackage{graphicx}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CTC-DRO: Reducing Language Disparities in Speech Recognition}

\newcommand{\ours}{\texttt{CTC-DRO}} % system name here
\newcommand{\orig}{\texttt{group DRO}}
% Redefine comment style to look like Python comments
\renewcommand{\algorithmiccomment}[1]{\hfill // #1}

\begin{document}

\twocolumn[
\icmltitle{CTC-DRO: Robust Optimization for Reducing Language Disparities\\ in Speech Recognition}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Martijn Bartelds}{equal,stanford}
\icmlauthor{Ananjan Nandi}{equal,stanford}
\icmlauthor{Moussa Koulako Bala Doumbouya}{stanford}
\icmlauthor{Dan Jurafsky}{stanford}
\icmlauthor{Tatsunori Hashimoto}{stanford}
\icmlauthor{Karen Livescu}{ttic}
\end{icmlauthorlist}

\icmlaffiliation{stanford}{Department of Computer Science, Stanford University, Stanford, USA}
\icmlaffiliation{ttic}{Toyota Technological Institute at Chicago, Chicago, USA}

\icmlcorrespondingauthor{Martijn Bartelds}{bartelds@stanford.edu}
\icmlcorrespondingauthor{Ananjan Nandi}{ananjan@stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Distributionally Robust Optimization, Deep learning, Robustness, Speech recognition}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Modern deep learning models often achieve high overall performance, but consistently fail on specific subgroups.
Group distributionally robust optimization (\orig{}) addresses this problem by minimizing the worst-group loss, but it fails when group losses misrepresent performance differences between groups.
This is common in domains like speech, where the widely used connectionist temporal classification (CTC) loss scales with input length and varies with linguistic and acoustic properties, leading to spurious differences between group losses.
We present \ours{}, which addresses the shortcomings of the \orig{} objective by smoothing the group weight update to prevent overemphasis on consistently high-loss groups, while using input length-matched batching to mitigate CTC's scaling issues.
We evaluate \ours{} on the task of multilingual automatic speech recognition (ASR) across five language sets from the \texttt{ML-SUPERB 2.0} benchmark.
\ours{} consistently outperforms \orig{} and CTC-based baseline models, reducing the worst-language error by up to $47.1\%$ and the average error by up to $32.9\%$.
\ours{} can be applied to ASR with minimal computational costs, and offers the potential for reducing group disparities in other domains with similar challenges.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
State-of-the-art deep learning models are often highly accurate on training data populations, while consistently underperforming on specific subpopulations or groups~\citep{pmlr-v80-hashimoto18a, duchi2022}.
One practical setting where this issue has very large effects is multilingual automatic speech recognition (ASR), where performance varies substantially between languages~\citep{radford2023robust, pratap24_mms, ml-superb2}.  Such models, which jointly perform language identification (LID) and ASR in many languages, could help improve accessibility and increase digital participation for speakers worldwide~\citep{besacier2014automatic}.

Distributionally robust optimization (\texttt{DRO}), particularly \orig{}~\citep{groupdro20}, has the potential to mitigate language disparities in multilingual ASR.  \orig{} improves group robustness by dynamically up-weighting high-loss groups during training.
However, it requires comparable training losses between groups to perform well~\citep{oren-etal-2019-distributionally, groupdro20}, and this condition is often not met in ASR model training,
%.  This issue exists in many ASR training settings, 
because of differences in input length and speaker and acoustic characteristics across language-specific datasets.  In this paper we focus on a training approach that has been successful in recent multilingual ASR benchmarks:  pre-trained self-supervised models fine-tuned with the connectionist temporal classification (CTC;~\citealp{graves2006connectionist}) objective~\citep{rouditchenko23_interspeech, chen-etal-2024-towards-robust, pratap24_mms}.
Differences in CTC-based training losses due to length, speaker, and acoustics may lead to varying magnitudes and irreducible components of losses across different groups.
As a result, some groups with disproportionately high losses may dominate training with \orig{}, causing under-training of the other groups, and ultimately negatively impacting overall downstream performance~\citep{pmlr-v151-slowik22a}.

To address these issues, we present \ours{}, which optimizes a generalization of the \orig{} objective, specifically by smoothing the up-weighting of high-loss groups.
This new objective prevents overemphasis on groups with consistently and disproportionately high training losses.
Also, we use length-matched group losses to mitigate the scaling properties of CTC.
We evaluate \ours{} using language subsets randomly selected from the \texttt{ML-SUPERB~2.0}~\citep{ml-superb2} benchmark.
In this setting, \ours{} models outperform both \orig{} and CTC-based baseline models across five language sets.
Specifically, \ours{} models reduce the error rate of the worst-performing language in all of the five sets, with improvements of up to $47.1\%$, while also improving the average performance across all languages by up to $32.9\%$.
Our code and newly trained models are publicly available at \url{https://github.com/Bartelds/ctc-dro}.


\section{Background}
\label{sec:bg}

\subsection{Group DRO}
\label{sec:group-dro}
Given a family of models $\Theta$, loss function $l$ and training data $(x,y)$ drawn from empirical distribution $\hat{P}$, the standard training procedure for label prediction involves minimizing the expected loss over the training data:
\begin{equation}
\label{eq:1}
\min_{\theta \in \Theta} \mathbb{E}_{(x, y) \sim \hat{P}} \left[\ell(\theta; (x, y))\right].
\end{equation}
In contrast, \orig{} aims to minimize the worst-case expected loss over a set of pre-defined groups or sub-distributions $\{\hat{P}_g : g \in G\}$ in the training data:
\begin{equation}
\label{eq:2}
\min_{\theta \in \Theta} 
\Big\{ \max_{g \in G} \mathbb{E}_{(x, y) \sim \hat{P}_g} 
\left[\ell(\theta; (x, y))\right] \Big\}.
\end{equation}
Following \citet{groupdro20}, this objective can be rewritten as:
\begin{equation}
\label{eq:3}
\min_{\theta \in \Theta} 
\Big\{ \sup_{q \in \Delta_{|G|}} \sum_{g \in G} q_g 
\mathbb{E}_{(x, y) \sim \hat{P}_g} \left[\ell(\theta; (x, y))\right] \Big\},
\end{equation}
where $\Delta_{|G|}$ is the $|G|$-dimensional probability simplex, and $q_g$ is a weight for group $g \in G$.
\citet{groupdro20} propose an online algorithm to optimize this objective, treating the problem as a minimax game and interleaving gradient ascent updates on $q = \{q_g : g \in G\}$ with gradient descent updates on $\theta$ for training data mini-batches (Algorithm~\ref{alg:1}).  

\begin{algorithm}
\caption{Online optimization algorithm for \orig{}.  $\theta$ represents the model parameters.}
\label{alg:1}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Step sizes $\eta_q, \eta_\theta$; loss function $l$; batch size $B$; groups $G = \{g\}$; training data $(x,y,g) \sim D$; number of training steps $T$
\STATE Initialize $\theta^{(0)}$ and $\{q_g\}$
\FOR{$t = 1$ to $T$}
    \STATE Sample $\mathcal{B} = \{\,(x_i, y_i, g_i)\,\}_{i=1}^B \sim D$
    \FOR{$g \in G$}
        \STATE $\mathcal{L}_g \gets \emptyset$
        \FOR{$i = 1$ to $B$}
            \IF{$g_i == g$}
                \STATE $\mathcal{L}_g \leftarrow \mathcal{L}_g \cup \{ l(\theta^{(t-1)}; (x_i, y_i)) \}$
            \ENDIF
        \ENDFOR
        \STATE $\bar{\mathcal{L}}_g = \dfrac{\sum_{\mathcal{L} \in \mathcal{L}_g} \mathcal{L}}{|\mathcal{L}_g|}$ 
        \STATE $q'_g \gets q_g \exp(\eta_q \bar{\mathcal{L}}_g)$
    \ENDFOR
    \FOR{$g \in G$}
    \STATE $q_g \gets \dfrac{q'_g} {\sum_{g'} q'_{g'}}$ 
    \COMMENT{gradient ascent on $q$}
    \ENDFOR
    \STATE $\mathcal{L} \gets \sum_{g \in G} {q_g \bar{\mathcal{L}}_g}$
    \STATE $\theta^{(t)} \gets \theta^{(t-1)} - \eta_\theta q^{(t)}_g \nabla \mathcal{L}$
    \COMMENT{gradient descent on $\theta$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{CTC}
\label{sec:background:ctc_prop}
The CTC objective~\cite{graves2006connectionist} defines a method to learn a mapping between an input sequence 
$X = (x_1, x_2, \ldots, x_D)$ and an output sequence $Y = (y_1, y_2, \ldots, y_U)$ 
without requiring a known alignment between them, but assuming $U \leq D$ and a monotonic alignment.
CTC uses a blank output token $\epsilon$ to handle $x_d \in X$ that do not map to any output symbol.
Consider $\mathcal{Z}$, which is the set of all sequences of length $D$ that are composed of tokens from $Y$, and $\epsilon$.
Each sequence $Z \in \mathcal{Z}$ is a potential alignment between $X$ and $Y$.
CTC defines a collapsing function that merges consecutive, identical symbols and removes $\epsilon$ in an alignment $Z$.
The set of alignments $Z \in \mathcal{Z}$ that collapse to $Y$ using this function forms the set of valid alignments $\mathcal{A}(X,Y)$.
For example, a possible alignment $Z \in \mathcal{A}(X,Y)$ for $D = 2U + 2$ could be: $[\epsilon, y_1, \epsilon, y_2, y_2, \epsilon, \ldots, \epsilon, y_U, \epsilon]$.
The conditional probability $P_{CTC}(Z|X)$ for any alignment $Z$ is computed as:
\begin{equation}
\label{eq:4}
P_{CTC}(Z|X) = \prod_{d=1}^{D} p(z_d|X),
\end{equation}
where $Z = (z_1, z_2, \ldots, z_D)$ and $p(z_d|X)$ is the model's predicted probability for symbol $z_d \in Z$ at time $d$.
The predicted probability of the output sequence $Y$, $P_{CTC}(Y|X)$, is then computed by marginalizing over the probabilities of all valid alignments $Z \in \mathcal{A}(X,Y)$:
\begin{equation}
\label{eq:5}
P_{CTC}(Y|X) = \sum_{Z\in\mathcal{A}(X,Y)} P_{CTC}(Z|X).
\end{equation}
The CTC loss function for $(X,Y)$ is then defined as:
\begin{equation}
\label{eq:6}
\mathcal{L}_{CTC} = -\log P_{CTC}(Y \mid X).
\end{equation}

\subsection{Limitations of Group DRO Applied to CTC}
\label{sec:limitations}
The CTC loss, as defined in equation~\ref{eq:6}, scales with the length of the input sequence $D$ and the length of the output sequence $U$.
This scaling behavior occurs because $P_{CTC}(Y|X)$ is a marginalization over all valid alignments $Z\in\mathcal{A}(X,Y)$.
Each alignment is a sequence of length $D$, which collapses to an output sequence of length $U$.
As $D$ increases relative to $U$, the number of valid alignments increases as well~\citep{graves2006connectionist}.
As each alignment's probability is the product of $D$ per-element probabilities, its value typically decreases as $D$ increases.
Therefore, their sum $P_{CTC}(Y|X)$ remains relatively low, as the per-alignment probabilities typically decrease faster than the number of valid alignments increases.
In practice, this often results in a higher CTC loss for longer sequences.

Therefore, differences in the distribution of $D$ or $U$ between groups can result in CTC losses that are not directly comparable.
For example, a long audio sample (large $D$) may have fewer errors overall, but a higher loss than a short audio sample (small $D$) if their transcription lengths $U$ are similar.
In Figure~\ref{fig:hist}, we illustrate the need to address this challenge, showing that there are large differences in the distribution of audio sample lengths $D$ across various groups (in this case, languages) included in our experimental setup, which we further detail in Section~\ref{sec:exp}.
In this example, Spanish has a high proportion of long utterances, resulting in higher CTC losses.
We find that the \orig{} algorithm assigns a larger weight to this group, even though it is among the best groups in terms of downstream performance in our experiments, as shown in Section~\ref{sec:results}.

\begin{figure}[h]
\setlength\belowcaptionskip{-5pt}
    \centering
    \includegraphics[width=0.75\columnwidth]{hist.png}\vspace{-0.05in}
    \caption{Distribution of audio sample lengths across groups (languages) in our experimental setup.} 
    \label{fig:hist}
\end{figure}

However, normalizing the impact of length differences on the CTC loss alone would be insufficient to address the problem of incomparable CTC losses across languages.
The CTC loss also varies due to differences in linguistic and acoustic properties across the pre-defined groups.
This may cause variance in the irreducible component of the training loss~\citep{NEURIPS2018_3ea2db50}.

In line with observations made in past work~\citep{oren-etal-2019-distributionally, pmlr-v151-slowik22a}, we show that this inherent incomparability of losses across groups poses a critical challenge for \orig{}.
From Algorithm~\ref{alg:1}, we compute the gradient ascent update to $q_g$, given group losses $\mathcal{L}_g$, as:
\begin{equation}
    q_{g} \leftarrow \frac{q_{g} \cdot \exp(\eta_q \mathcal{L}_g)}{\sum_g \left(q_{g} \cdot \exp(\eta_q \mathcal{L}_g)\right)}.
\end{equation}
This is equivalent to the Hedge algorithm~\citep{MAL-068} update for the following maximization objective:
\begin{equation}
\label{eq:obj}
 \max_{q \in \Delta_{|G|}} \sum_{g \in G} q_g \mathcal{L}_g.
\end{equation}
Now consider a situation where one of the groups $g'$ consistently has the highest training losses among all groups during training,  presumably due to long audio samples or lengthy transcriptions, as well as the highest irreducible loss.
This will result in its weight $q_{g'}$ consistently receiving the largest increases $\delta q_g$ during training, as:
\begin{equation}
    \delta q_g \propto q_{g} \exp(\eta_q \mathcal{L}_g).
\end{equation}

As a result, $q_{g'}$ will grow disproportionally large over the course of training, eventually drawing all the weight away from the other groups. This can result in other groups being under-trained, which will cause a substantial decrease in their downstream performance (see Section~\ref{sec:results}).

This result highlights the problems caused by the fundamental mismatch between the computed loss and the ideal loss for use in \orig{}.
The ideal loss would measure only the excess loss beyond each group’s irreducible component and be length-normalized.
However, in our setting neither the irreducible component of the training loss nor a direct normalization of the CTC loss is easy to estimate.
Existing solutions, such as calibrating group losses or approximating disparities between groups with simpler models~\citep{oren-etal-2019-distributionally, pmlr-v151-slowik22a}, would either require a substantial increase in computational cost or a proxy for group difficulty, for which there is no reliable model for speech to our knowledge.
Therefore, CTC remains inherently incompatible with \orig{}.

\section{CTC-DRO}
\label{sec:new-dro}
To address the identified challenges, we propose a new training algorithm: \ours{} (Algorithm~\ref{alg:2}).
This algorithm computes length-matched losses across groups to mitigate the scaling properties of CTC, and uses a generalization of the \orig{} objective that introduces a new smoothed maximization objective for the group weights to prevent overemphasis on groups with consistently high training losses. Like \orig{}, \ours{} has minimal computational costs, only keeping track of a single scalar weight for every group in the training data.

\subsection{Length-Matched Group Losses}
To address incomparable CTC losses across groups due to different distributions of audio lengths, we ensure that the CTC loss for each group is computed using roughly equal total audio durations.
Specifically, we create a new batch sampler that selects batches of audio samples and corresponding transcripts $(x_i, y_i)$, all from a single, randomly-selected group $g$, while ensuring that their total audio duration is as close to a fixed value (set as a hyperparameter) as possible.\footnote{Group utterances are iteratively added to a batch until the total duration meets or slightly exceeds the set target duration.}
However, batches with a larger number of shorter audio samples tend to have a lower CTC loss per audio sample than batches with few, longer, audio samples.
Therefore, we sum the utterance-level CTC losses in a batch (see line 10 in Algorithm~\ref{alg:2}) and update the group weights using this sum instead of the mean loss used in the \orig{} algorithm.
During training, these summed losses are tracked for each group, and a group weight update is performed only after at least one batch has been processed for every group.
If a group is sampled multiple times before the update, the corresponding summed losses are averaged.
This approach effectively increases the batch size for computing the group weight update.

Also, we multiply the losses by the total number of groups (line 21 in Algorithm~\ref{alg:2}) before performing gradient descent on the model parameters.
This ensures that the training losses with \ours{} are comparable to a model trained without \ours{}, removing the need to tune shared hyperparameters, such as the learning rate, separately for both training algorithms.

\begin{algorithm}[ht]
\caption{Optimization algorithm for \ours{}.  $\theta$ represents the model parameters.}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Step sizes $\eta_q, \eta_\theta$; smoothing parameter $\alpha$; loss function $l$; duration of each batch $d$; groups $G = \{g\}$; training data $(x,y,g) \sim D$; number of training steps $T$
\STATE Initialize $\theta^{(0)}$, $\{q_g\}$
\STATE Initialize \text{gr\_losses}[g] $= \emptyset$ $ \forall g$
\FOR{$t = 1$ to $T$}
    \STATE Sample $g \sim G$
    \STATE Sample $\mathcal{B} = \{(x_i, y_i, g)\}_{i=1}^{B_t} \sim D$ \COMMENT{selected such that $\Sigma_{i=1}^{B_t}$ $duration(x_i) \approx d$}
    \FOR{$i = 1$ to $B_t$}
    \STATE $\ell_i = \ell(\theta^{(t-1)}; (x_i, y_i))$ % \textbf{for} $i = 1$ to $B_t$
    \ENDFOR
    \STATE $\text{gr\_losses}[g] \leftarrow \text{gr\_losses}[g] \cup \left\{ \sum_{i=1}^{B_t} \ell_i \right\}$
    \IF{$\text{gr\_losses}[g] \neq \emptyset$  $\forall g$} \label{line:acc1}
        \FOR{\textbf{each} group $g$}
            \STATE $\bar{\ell}_g = \dfrac{\sum_{\mathcal{L} \in \text{gr\_losses}[g]} \mathcal{L}}{|\text{gr\_losses}[g]|}$ \label{line:sum}
            \STATE $q'_g \leftarrow q_g \times \exp\left( \dfrac{\eta_q \bar{\ell}_g}{q_g + \alpha} \right)$ \label{line:update}
            \STATE $\text{gr\_losses}[g] \leftarrow \emptyset$
        \ENDFOR
        \FOR{\textbf{each} group $g$}
            \STATE $q_g \leftarrow \dfrac{q'_g}{\sum_{g'} q'_{g'}}$
            \COMMENT{gradient ascent on $q$}
        \ENDFOR
    \ENDIF \label{line:acc2}
    \STATE $\tilde{\mathcal{L}} = \dfrac{q_g . |G|}{B_t} \sum_{i=1}^{B_t} \ell_i$
    \COMMENT{all data from same group $g$}
    \STATE $\theta^{(t)} \leftarrow \theta^{(t-1)} - \eta_\theta \nabla_{\theta} \tilde{\mathcal{L}}$
    \COMMENT{gradient descent on $\theta$}
\ENDFOR
\end{algorithmic}
\label{alg:2}
\end{algorithm}

\subsection{Smoothed Maximization Objective}
We propose a new method for updating the group weights, which addresses \orig{}'s tendency to assign a disproportionately large weight to groups with consistently high training losses.
This approach also helps mitigate the scaling properties of CTC related to transcription length, which our initial experiments showed cannot be adequately resolved by normalizing for transcript length.

We propose a generalization of the \orig{} maximization objective (Equation~\ref{eq:obj}), containing a new smoothing hyperparameter $\alpha$:
\begin{equation}
\label{eq:newobj}
 \max_{q \in \Delta_{|G|}} \sum_{g \in G} \log(q_g + \alpha) \mathcal{L}_g.
\end{equation}

Following the Hedge algorithm~\citep{MAL-068} for this objective, the new update rule is (line 14--19 in Algorithm~\ref{alg:2}):
\begin{equation}
q_g \leftarrow \frac{q_g . \exp ({\eta_q \frac{\mathcal{L}_g}{q_g + \alpha}})}{\sum_{g \in G} (q_g . \exp ({\eta_q \frac{\mathcal{L}_g}{q_g + \alpha}}))}.
\end{equation}
As $\alpha \to 0$, the update becomes increasingly more sensitive to the current group weight relative to the group loss. This causes groups with higher weights to receive smaller updates, resulting in a more uniform distribution of the group weights. 
In contrast, as $\alpha$ increases, updates depend more on the group loss compared to the group weight, increasing the group weights more strongly for groups with higher losses. In fact, when $\alpha \to \infty$, the update rule reduces to:
\begin{equation}
% \label{eq:2}
q_g \leftarrow \frac{q_g . \exp ({\eta_q \frac{\mathcal{L}_g}{\alpha}})}{\sum_{g \in G} (q_g . \exp ({\eta_q \frac{\mathcal{L}_g}{\alpha}}))},
\end{equation}
recovering the \orig{} update and confirming that the new objective is indeed a generalization.

This update rule has several desirable properties.
First, the updates to $q_g$ are smoother, because they are inversely proportional to the current $q_g$, while still being proportional to the loss $\mathcal{L}_g$.
This discourages any single group from having a disproportionately large weight $q_g$ relative to its group loss, leading to a more balanced distribution of the group weights.
Second, the update rule adjusts for differences in group weights when the CTC losses are similar.
Specifically, if two groups with different $q_g$ have similar CTC losses, the group with the lower $q_g$ receives a larger update.
This helps prevent under-training of lower-weighted groups by reducing the gap between the group weights over time.

Along with these desirable properties, we demonstrate that our new objective does not change the fundamental behavior of the \orig{} objective, assigning higher weights to groups with higher losses.
Expanding the conditions for the probability simplex $\Delta_{|G|}$ and taking the Lagrangian of equation~\ref{eq:newobj}, we obtain:
\begin{equation}
\sum_{g \in G} \log(q_g + \alpha) \mathcal{L}_g + \lambda (1 - \sum_{g \in G} q_g) - \sum_{g \in G} \lambda_g q_g,
\end{equation}
where $\lambda$ and $\lambda_g$ are Lagrangian multipliers and $\lambda_g \geq 0$ for all $g$.
To find the optimal $q_g$, we calculate the partial derivative of $\mathcal{L}$ with respect to $q_g$ and set it to 0:
\begin{equation}
\frac{\mathcal{L}_g}{q_g + \alpha} - \lambda - \lambda_g = 0.
\end{equation}
This equation then rearranges to:
\begin{equation}
q_g \propto \frac{\mathcal{L}_g}{\lambda + \lambda_g} - \alpha.
\end{equation}
Thus, the optimal weight for a group is directly proportional to its loss, after subtracting $\alpha$ and enforcing non-negativity.


\section{Experiments}
\label{sec:exp}
We fine-tune the existing, self-supervised multilingual \texttt{XLS-R} and \texttt{MMS} models~\citep{babu22_interspeech, pratap24_mms} on the task of multilingual ASR (formulated as a joint task of ASR and LID) using data from the \texttt{ML-SUPERB 2.0} benchmark (more on the data in~\ref{sec:data}).
Following the setup of \texttt{ML-SUPERB~2.0}, we add two Transformer layers and a softmax layer on top of the pre-trained models to predict a language token followed by character sequences using CTC. We do not use a separate LID head or loss, and update all model parameters.
The models we choose have shown the best performance on \texttt{ML-SUPERB~2.0}~\cite{ml-superb2}, outperforming other models like Whisper~\citep{radford2023robust}.
The two pre-trained models share the same architecture and training objective~\citep{baevski2020wav2vec}, but their training data differs:
\texttt{XLS-R} is pre-trained on roughly 436K hours of speech in 128 languages, while \texttt{MMS} is pre-trained on 491K hours of speech in 1406 languages.

We train the models using three approaches.
First, our baseline models use the joint ASR and LID training setup adopted in \texttt{ML-SUPERB~2.0}, as described above.
Second, we fine-tune models using our proposed \ours{} algorithm.
Third, we train models using the \orig{} algorithm for comparison.
When training both \ours{} and \orig{} models, the groups correspond to the languages in our training datasets (see Section~\ref{sec:data}).

We mostly follow the hyperparameters used by \citet{babu22_interspeech}, \citet{pratap24_mms}, and in \texttt{ML-SUPERB~2.0}, but train for 40 epochs, retaining the model checkpoint with the lowest loss on the development data, accumulate gradients across 16 batches, and tune the learning rate of the baseline models on our development data.
We also use this learning rate to train models with \ours{} and \orig{}.
Lastly, for the \ours{} and \orig{} models, we tune the \texttt{DRO}-specific hyperparameters on the development set as well, specifically $\eta_q \in \{10^{-3}, 10^{-4}\}$ and $\alpha \in \{0.1, 0.5, 1\}$.

\subsection{Dataset}
\label{sec:data}
We use the \texttt{ML-SUPERB~2.0} dataset for our experiments.
This dataset belongs to an established benchmark where a number of multilingual ASR models have already been compared.
It has broad coverage of 141 languages sourced from 15 corpora, and contains more natural speech compared to smaller, translation focused corpora, such as \texttt{FLEURS}~\citep{fleurs23}.
For each language-corpus pair, there is at least one hour of training data available, as well as 10 minutes each for development and test data.
While the amount of training data for many of the languages is limited to one hour, 62 languages occur in more than one corpus, increasing their training data up to nine hours.

For our main experiments, we use a balanced data setup by randomly selecting five sets from \texttt{ML-SUPERB~2.0} that consist of six language-corpus pairs (Table~\ref{tab:langs}, dataset statistics in Appendix~\ref{appendix:more-data}).
We thus have one hour of training data, and 10 minutes of development and test data available for each language-corpus pair in each set.
The selection of language-corpus pairs is based on the character error rates (CERs) of the best-performing model configuration from \texttt{ML-SUPERB~2.0}.
Specifically, for each set, we randomly select two language-corpus pairs from the bottom 10 percentile of CERs, two language-corpus pairs from the top 10 percentile of CERs, and two language-corpus pairs with CERs between the 10th and 90th percentiles.

For the first two language sets, we also investigate the effect of using additional training data (unbalanced setup), as most languages in these sets have more than one hour of training data available.
We list these sets and the corpora included in Appendix~\ref{appendix:more-data}.

\begin{table}[t]
\caption{Overview of the included language sets, which are originally obtained from CommonVoice (CV; \citealp{ardila2019common}), FLEURS, Googlei18n open-source project (GOP; \citealp{sodimana18_sltu, kjartansson-etal-2020-open, he-etal-2020-open}), Living Audio dataset (LAD; \citealp{braude19_interspeech}), M-AILABS Speech Dataset (MSD; \citealp{m-ailabs}), NCHLT Speech Corpus (NCHLT; \citealp{barnard14_sltu}), and VoxForge (VF; \citealp{voxforge}).}
\label{tab:langs}
\vskip 0.15in
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{sc}
\begin{tabular}{cl}
\toprule
\textbf{Set \#} & \textbf{Languages (ISO code, Corpus)} \\
\midrule
1 & Czech (ces, CV), Mandarin (cmn, Fleurs) \\
           & Min Nan (nan, CV), Polish (pol, MSD) \\
           & Romanian (ron, Fleurs), Spanish (spa, VF) \\
\midrule
2 & Cantonese (yue, Fleurs), Croatian (hrv, Fleurs) \\
           & English (eng, LAD), Italian (ita, Fleurs) \\
           & Persian (fas, CV), Slovak (slk, Fleurs) \\
\midrule
3 & Khmer (khm, Fleurs), Korean (kor, Fleurs) \\
           & Northern Kurdish (kmr, CV), Nynorsk (nno, CV) \\
           & Southern Ndebele (nbl, NCHLT), Tatar (tat, CV) \\
\midrule
4 & Sindhi (snd, Fleurs), Slovenian (slv, CV) \\
           & Southern Sotho (sot, GOP), Spanish (spa, MSD) \\
           & Urdu (urd, Fleurs), Western Mari (mrj, CV) \\
\midrule
5 & English (eng, VF), German (deu, VF) \\
           & Hebrew (heb, Fleurs), Japanese (jpn, Fleurs) \\
           & Russian (rus, Fleurs), Spanish (spa, Fleurs) \\
\bottomrule
\end{tabular}
\end{sc}
}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Evaluation}
We compare the performance of \ours{} models to the baseline and \orig{} models.
The models are evaluated in terms of the standard metrics of CER and LID accuracy on the test sets from the five language sets.
The CER can be computed by comparing the system generated and reference transcripts using the formula:
\begin{equation}
 \text{CER} = \frac{I+S+D}{N} \times 100,
\end{equation}
where $I$ is the number of insertions, $S$ the number of substitutions, and $D$ the number of deletions in a minimum edit distance alignment between the reference and system output, and $N$ is the number of characters in the reference transcript.
We report the CER of the worst-performing language, as well as the average CER across languages.
For the \ours{} and \orig{} models, we report the performance of the model checkpoint with the largest CER improvement on the worst-performing language relative to the baseline on the development set.


\section{Results}
\label{sec:results}
\begin{table}[ht!]
\centering
\caption{CER of the worst-performing language (\textsc{\texttt{Max CER}}, ISO code for the worst-performing language provided as \textsc{\texttt{ISO}}), as well as the average CER (\textsc{\texttt{Avg CER}}) and LID accuracy (\textsc{\texttt{LID}}) across languages (in \%) for the baseline models, \orig{} models, and \ours{} models on the test sets from the five language sets (indexed by the ``\#" column). The best results for each language set and model are highlighted.}
\label{tab:results-ft}
\resizebox{\columnwidth}{!}{
% \begin{small}
\begin{sc}
\begin{tabular}{@{}cllrcc@{}}
\toprule
\textbf{Set}        & \textbf{Model}        & \textbf{Type}        & \textbf{Max CER} & \textbf{Avg CER}& \textbf{LID} \\ 
\textbf{\#} & & & \textbf{(ISO)} ($\downarrow$) & ($\downarrow$) & ($\uparrow$) \\ \midrule
\multirow{6}{*}{1} & \multirow{3}{*}{MMS}  & Baseline             & 60.8 (nan)         & 23.4         & \textbf{97.4}         \\
                   &                       & \orig{}              & 86.6 (nan)         & 30.5         & 78.7         \\
                   &                       & \ours{}              & \textbf{56.8} (nan)         & \textbf{22.9}         & 95.8         \\ \cmidrule(l){2-6} 
                   & \multirow{3}{*}{XLS-R} & Baseline             & 64.9 (cmn)        & 25.2         & \textbf{92.6}         \\
                   &                       & \orig{}              & 78.4 (nan)         & 30.0         & 87.8         \\
                   &                       & \ours{}              & \textbf{57.6} (nan)         & \textbf{22.5}         & 89.5         \\ \midrule
% \textbf{\#}        & \textbf{Model}        & \textbf{Type}        & \textbf{Max (ISO)} & \textbf{CER} & \textbf{LID} \\ \midrule
\multirow{6}{*}{2} & \multirow{3}{*}{MMS}  & Baseline             & 49.4 (yue)         & 15.8         & \textbf{98.4}         \\
                   &                       & \orig{}              & 55.5 (yue)         & 20.7         & 98.2         \\
                   &                       & \ours{}              & \textbf{44.4} (yue)         & \textbf{15.0}         & 96.2         \\ \cmidrule(l){2-6} 
                   & \multirow{3}{*}{XLS-R} & Baseline             & 68.8 (yue)         & 19.0         & \textbf{94.2}         \\
                   &                       & \orig{}              & 58.8 (yue)         & 21.6         & 87.0         \\
                   &                       & \ours{}              & \textbf{45.0} (yue)         & \textbf{15.8}         & 89.3         \\ \midrule
% \textbf{\#}        & \textbf{Model}        & \textbf{Type}        & \textbf{Max (ISO)} & \textbf{CER} & \textbf{LID} \\ \midrule
\multirow{6}{*}{3} & \multirow{3}{*}{MMS}  & Baseline             & 34.2 (kor)         & 16.1         & 98.5         \\
                   &                       & \orig{}              & 34.0 (kor)         & 22.0         & \textbf{98.7}         \\
                   &                       & \ours{}              & \textbf{31.3} (khm)         & \textbf{15.3}         & \textbf{98.7}         \\ \cmidrule(l){2-6} 
                   & \multirow{3}{*}{XLS-R} & Baseline             & 33.2 (khm)         & \textbf{17.0}         & \textbf{99.2}         \\
                   &                       & \orig{}              & 38.0 (khm)         & 25.1         & 97.2         \\
                   &                       & \ours{}              & \textbf{32.2} (khm)         & 17.7         & 97.9         \\ \midrule
% \textbf{\#}        & \textbf{Model}        & \textbf{Type}        & \textbf{Max (ISO)} & \textbf{CER} & \textbf{LID} \\ \midrule
\multirow{6}{*}{4} & \multirow{3}{*}{MMS}  & Baseline             & 24.0 (snd)         & 14.4         & \textbf{87.9}         \\
                   &                       & \orig{}              & 21.8 (urd)         & 14.9         & 91.9         \\
                   &                       & \ours{}              & \textbf{18.4} (urd)         & \textbf{12.9}         & 87.3         \\ \cmidrule(l){2-6} 
                   & \multirow{3}{*}{XLS-R} & Baseline             & 29.7 (urd)         & 14.6         & 88.4         \\
                   &                       & \orig{}              & 25.6 (slv)         & 18.6         & 83.5         \\
                   &                       & \ours{}              & \textbf{24.2} (urd)         & \textbf{13.7}         & \textbf{88.9}         \\ \midrule
% \textbf{\#}        & \textbf{Model}        & \textbf{Type}        & \textbf{Max (ISO)} & \textbf{CER} & \textbf{LID} \\ \midrule
\multirow{6}{*}{5} & \multirow{3}{*}{MMS}  & Baseline             & 90.0 (jpn)         & 26.0         & \textbf{96.3}         \\
                   &                       & \orig{}              & 62.2 (jpn)         & 29.2         & 67.0         \\
                   &                       & \ours{}              & \textbf{57.5} (jpn)         & \textbf{24.3}         & 90.5         \\ \cmidrule(l){2-6} 
                   & \multirow{3}{*}{XLS-R} & Baseline             & 114.8 (jpn)        & 29.9         & 89.0         \\
                   &                       & \orig{}              & 92.9 (jpn)         & 36.8         & 57.7         \\
                   &                       & \ours{}              & \textbf{71.5} (jpn)         & \textbf{23.8}         & \textbf{91.0}         \\ \bottomrule
\end{tabular}%
\end{sc}
% \end{small}
}
\end{table}

We present the results of our experiments using balanced and additional training data in Table~\ref{tab:results-ft} and Table~\ref{tab:results-ft-extra}, respectively (more detailed results in Appendix~\ref{appendix:more-results}).
In line with previous work (e.g., \citealp{pratap24_mms} and \citealp{ml-superb2}), we find substantial performance differences between languages for our baseline models trained without \orig{} or \ours{}, as shown by the large difference between the CER of the worst-performing language and the average CER across languages (for more details see Appendix~\ref{appendix:results-ft-full}).
This finding applies to each of the evaluated sets, regardless of whether the training data for each of the languages is balanced or unbalanced.

For each language set, \ours{} models achieve a lower CER for the worst-performing language compared to the baseline and \orig{} models.
The largest improvement is obtained on set 2 using \texttt{XLS-R} using all available data, showing a relative CER reduction of $47.1\%$ compared to the baseline model.
Note that \ours{} also results in the best average CER in 13 out of 14 settings (seven sets with two models each) compared to both the baseline and \orig{} models, leading to relative CER reductions up to $32.9\%$.
The only exception is for \texttt{XLS-R} in balanced set 3, where the average CER is slightly worse with \ours{} (17.7\%) than the baseline (17.0\%).
In terms of LID accuracy, \ours{} models improve over the baseline models in seven out of 14 settings.
In the remaining settings, the LID accuracy already exceeds 87\%, leaving little room for further improvement with \ours{}.

In contrast, \orig{} worsens the CER of the worst-performing language in seven out of 14 settings compared to the baseline model, with the highest relative CER increase of $57.5\%$ on set 2 using \texttt{MMS} trained on all available training data.
Also, \orig{} increases the average CER compared to the baseline in all settings.  This finding shows the ineffectiveness of the original \orig{} formulation in this challenging setting, and the substantial added robustness of the modifications in \ours{}.

In four settings, the worst-performing language changes between the baseline and \ours{} models. For example, in set 3 with \texttt{MMS} trained on balanced data, it shifts from Korean to Khmer.
As shown in Table~\ref{tab:results-ft-full}, the \ours{} model reduces the CER for Korean from 34.2 to 27.6, while the CER for Khmer remains unchanged at 31.3.
Overall, \ours{} consistently improves the performance of the worst-performing language, occasionally at a small cost to the performance of other languages, while still achieving a lower CER on average (see Appendix~\ref{appendix:more-results}).

\begin{table}[ht!]
\centering
\caption{CER of the worst-performing language (\textsc{\texttt{Max CER}}, ISO code for the worst-performing language provided as \textsc{\texttt{ISO}}), as well as the average CER (\textsc{\texttt{Avg CER}}) and LID accuracy (\textsc{\texttt{LID}}) across languages (in \%) for the baseline models, \orig{} models, and \ours{} models on the test sets from the first two language sets using additional training data where available. Best results are highlighted.}
\label{tab:results-ft-extra}
\resizebox{\columnwidth}{!}{
% \begin{small}
\begin{sc}
\begin{tabular}{@{}cllrcc@{}}
\toprule
\textbf{Set }        & \textbf{Model}        & \textbf{Type}        & \textbf{Max CER} & \textbf{Avg CER} & \textbf{LID} \\ 
\textbf{\#} & & & \textbf{(ISO)} ($\downarrow$) &  ($\downarrow$) & ($\uparrow$) \\ \midrule
\multirow{6}{*}{1} & \multirow{3}{*}{MMS}  & Baseline             & 67.5 (nan)         & 25.6         & 98.1         \\
                   &                       & \orig{}              & 96.3 (nan)         & 37.8         & 83.9         \\
                   &                       & \ours{}              & \textbf{62.8} (nan)         & \textbf{22.8}         & \textbf{98.5}         \\ \cmidrule(l){2-6} 
                   & \multirow{3}{*}{XLS-R} & Baseline             & 92.1 (cmn)         & 35.6         & 96.4         \\
                   &                       & \orig{}              & 90.8 (nan)         & 38.1         & 72.3         \\
                   &                       & \ours{}              & \textbf{67.5} (nan)         & \textbf{26.9}         & \textbf{97.1}         \\ \midrule
% \textbf{\#}        & \textbf{Model}        & \textbf{Type}        & \textbf{Max (ISO)} & \textbf{CER} & \textbf{LID} \\ \midrule
\multirow{6}{*}{2} & \multirow{3}{*}{MMS}  & Baseline             & 66.9  (yue)        & 19.5         & 99.0        \\
                   &                       & \orig{}              & 105.4 (yue)        & 38.8         & 81.0         \\
                   &                       & \ours{}              & \textbf{48.1}  (yue)        & \textbf{16.4}         & \textbf{99.1}         \\ \cmidrule(l){2-6} 
                   & \multirow{3}{*}{XLS-R} & Baseline             & 97.2  (yue)        & 28.0         & 98.2         \\
                   &                       & \orig{}              & 102.9 (yue)        & 44.0         & 80.8         \\
                   &                       & \ours{}              & \textbf{51.4}  (yue)        & \textbf{18.8}         & \textbf{98.6}         \\ \bottomrule
\end{tabular}%
\end{sc}
% \end{small}
}
\end{table}


\section{Analysis}
Next, we present an ablation study to measure the contributions of the length-matched group losses and smoothed maximization objective introduced in \ours{} (Section~\ref{sec:analysis:ablation}).
To this end, we train \ours{} models with each of these components removed on balanced training data from set 5, which showed the largest reduction in CER for the worst-performing language (Table~\ref{tab:results-ft}).
We also describe and compare how the group weights of \ours{} and \orig{} models change throughout training (Section~\ref{sec:analysis:viz}).
For brevity, we focus on the \texttt{XLS-R} models trained on the same set, showing that \ours{} results in more stable training.

\begin{figure*}[ht!]
\setlength\belowcaptionskip{-5pt}
    \centering
    \begin{minipage}{0.49\textwidth}
        \setlength\belowcaptionskip{-5pt}
        \centering
        \includegraphics[width=0.905\columnwidth]{q_values_asr_train_asr_xlsr_aleb_dro_0.0001_base.png}
        \captionsetup{skip=-2pt}
        \caption*{(a) \orig{}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \setlength\belowcaptionskip{-5pt}
        \centering
        \includegraphics[width=0.905\columnwidth]{q_values_asr_train_asr_xlsr_aleb_dro_0.0001_la_0.1.png}
        \captionsetup{skip=-2pt}
        \caption*{(b) \ours{}}
    \end{minipage}
    \caption{Group weights for each language throughout training of an \texttt{XLS-R} model trained with \orig{} or \ours{} on balanced data from set 5.}
    \label{fig:q_values}
\end{figure*}

\subsection{Ablation Study}
\label{sec:analysis:ablation}
We find that removing either component from \ours{} leads to a substantial decrease in performance (see Table~\ref{tab:ablation}, detailed results in Appendix~\ref{appendix:more-results}).
Specifically, the CER of the worst-performing language increases by up to $171.6\%$ and the average CER by up to $302.9\%$ compared to a model trained using the complete \ours{} algorithm.
The ablation study also shows that the smoothed maximization objective has the strongest effect on reducing both the CER of the worst-performing language and the average CER.

\begin{table}[ht!]
\centering
\caption{CER of the worst-performing language (\textsc{\texttt{Max CER}}), as well as the average CER (\textsc{\texttt{Avg CER}}) and LID accuracy (\textsc{\texttt{LID}}) across languages (in \%) in set 5 for a subtractive ablation of \ours{}, removing the length-matched group losses (\textsc{\texttt{Dur}}) and smoothed maximization objective (\textsc{\texttt{Smooth}}). Best results are highlighted.}
\label{tab:ablation}
\resizebox{\columnwidth}{!}{
% \begin{small}
\begin{sc}
\begin{tabular}{@{}llrcc@{}}
\toprule
\textbf{Model}        & \textbf{Type}        & \textbf{Max CER} & \textbf{Avg CER } & \textbf{LID } \\ 
& & \textbf{(ISO)} ($\downarrow$) & ($\downarrow$) & ($\uparrow$) \\ \midrule
\multirow{3}{*}{MMS}  & Baseline             & 90.0 (jpn)         & 26.0         & \textbf{96.3}         \\
                                          & \ours{}              & \textbf{57.5} (jpn)         & \textbf{24.3}         & 90.5         \\
                                          & \quad - Dur              & 84.6 (jpn)         & 29.5         & 66.1         \\ 
                                          & \quad - Smooth              & 102.1 (jpn)         & 97.9         & 13.2         \\
                                          \midrule 
                   \multirow{3}{*}{XLS-R} & Baseline             & 114.8 (jpn)         & 29.9         & 89.0         \\
                                          & \ours{}              & \textbf{71.5} (jpn)         & \textbf{23.8}         & \textbf{91.0}         \\
                                          & \quad - Dur              & 115.2 (nan)         & 50.6         & 54.4         \\ 
                                          & \quad - Smooth              & 194.2 (nan)         & 61.4         & 43.2         \\
                                          \bottomrule
\end{tabular}%
\end{sc}
% \end{small}
}
\end{table}

\subsection{Comparison of Group Weights}
\label{sec:analysis:viz}
To analyze the behavior of \orig{} and \ours{} models during training, we plot the group weights for all languages in set 5 throughout training (Figure~\ref{fig:q_values}).
We find that the group weights of the \orig{} model fluctuate substantially during training, reaching values close to 0 or 1 at various stages of training.
For extended periods of training with \orig{}, the group weights are heavily concentrated on a single language, causing the weights for all other languages to reach values close to 0.

In contrast, the group weights of the \ours{} model are distributed more evenly across all languages throughout training.
The group weights for each language also fluctuate substantially less during training compared to \orig{}.
The only languages with group weights dropping below 0.1 at any point are German and English, both of which have low CERs on the development set (results not shown).
Importantly, the weight of Japanese (worst-performing) consistently remains among the top two highest group weights.


\section{Related Work}
\paragraph{Robustness to distribution shifts}
Prior work categorizes distribution shifts as domain generalization~\citep{quinonero2008dataset, santurkar2020breedsbenchmarkssubpopulationshift, hendrycks2021many}, where train and test data domains have no overlap, or subpopulation shifts~\citep{dixon2018measuring, oren-etal-2019-distributionally, groupdro20}, where train and test data come from the same domains, but do not necessarily appear in the same proportions~\citep{koh2021wilds}. Our experimental setup is an example of a subpopulation shift, as all test languages are included in the training data for the models.

Methods for robust generalization are commonly categorized into three groups. Domain invariance methods aim to learn feature representations that are consistent across domains (groups) by encouraging similar feature distributions across domains~\citep{tzeng2014deep, long2015learning, ganin2016domain, coral16}. Other approaches use invariant prediction methods~\citep{meinshausen2014maximin, peters2016causal, arjovsky2019invariant, rothenhausler2021anchor} from causal inference literature. In contrast, \texttt{DRO} explicitly minimizes the worst-case loss over an uncertainty set, which is typically defined as a divergence ball around the training distribution~\citep{namkoong2016stochastic, mohajerin2018data, bertsimas2018data, duchi2019variance, oren-etal-2019-distributionally, groupdro20}. Our work builds upon \orig{}~\citep{groupdro20}, since it has outperformed other approaches in settings with subpopulation shifts~\citep{koh2021wilds}.

\paragraph{Robust ASR}
Prior work on robustness in ASR primarily focuses on quantifying or addressing biases related to accent, age, dialect, gender, and race ~\citep{tatman-2017-gender, koenecke2020, ngueajio2022hey, markl, martin2023, feng2024101567, harris-etal-2024-modeling}.
Methods to mitigate these biases include data balancing~\citep{dheram2022toward} and fairness-promoting training methods~\citep{sari, zhang2022mitigating, veliche}.
These methods are not appropriate for reducing ASR language disparities, as they either require large amounts of training data, which may not be available for most languages, or improve performance for certain groups at a substantial cost to others.
\citet{gao2022domain} explored \texttt{DRO} for training language-independent speech recognition models, and reported  negative results. To the best of our knowledge, our work is the first to propose a robust optimization method that successfully reduces cross-lingual performance disparities in ASR.

\section{Conclusion}
\ours{}, our robust optimization approach motivated by multilingual ASR, addresses \orig{}'s inability to handle group losses that do not accurately reflect performance differences between groups.
When applied to data from a multilingual ASR and LID benchmark, \ours{} outperformed baseline CTC-based and \orig{} models, reducing the worst-language CER across all sets and improving average CER and LID accuracy in almost all cases.
Our analysis showed that this result can be attributed to the smoothed maximization objective and length-matched batching that balance and stabilize the group weights.

While performance disparities are reduced in our approach, they are not eliminated.
The improvements may be sufficient to make ASR useful for more languages than before, but additional work is needed before ASR is truly practical for many more languages.
Such work could include automatically learning data groupings, removing the need for pre-defined groups that may be unknown or incomplete, as well as applying \ours{} to pre-training.

Also, we believe the principles underlying \ours{} have broader applicability.
Other tasks that use variable-length sequences as input data and therefore face similar challenges, such as language modeling and video transcription, could potentially benefit from our algorithm, enabling more inclusive processing of other data modalities as well.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.

Our \ours{} approach reduces performance differences between languages in modern multilingual ASR models with minimal computational costs, ensuring it can be readily adopted.
Our work therefore has the potential to positively impact speakers of many languages worldwide, including digitally underrepresented languages and varieties, by improving their access to information and services.
However, several challenges remain.
The performance of multilingual ASR needs to further improve before it can be deployed in real-world settings for many languages.
In addition, improved ASR for underrepresented languages and varieties calls for careful, community-driven evaluation to ensure modern technology is aligned with local interests.
In this process, it is important to evaluate \ours{}'s impact in varied real-world settings to ensure our algorithm benefits all language communities.  For example, many practical settings may include types of linguistic and acoustic variation that we have not encountered in our data, and in some settings CER may not be the most useful performance measure (e.g., it may not correlate well with the usefulness of the transcripts for the target use cases).


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

\section{Datasets}
\label{appendix:more-data}
Table~\ref{tab:langs-extra} shows the first two language sets, listing all available corpora for each language in \texttt{ML SUPERB 2.0}.
In Table~\ref{tab:data_stats}, we show the number of samples, along with the average duration and transcript length for each language in each language set in the balanced data setting.

\begin{table*}[ht!]
\caption{Overview of the additional corpora available for the first two sets, which are originally obtained from CV, Fleurs, LAD, Multilingual Librispeech (MLL; \citealp{pratap2020mls}), MSD, NCHLT, Spoken Wikipedia corpus (SWC; \citealp{baumann_spoken_2019}), VF, and Voxpopuli (VP; \citealp{wang-etal-2021-voxpopuli}).}
\label{tab:langs-extra}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.72\columnwidth}{!}{
\begin{tabular}{clll}
\toprule
\textbf{Set \#} & \textbf{Language} & \textbf{ISO code} & \textbf{Corpus} \\
\midrule
\multirow{6}{*}{1} & Czech            & ces & CV, Fleurs, VP \\
                   & Mandarin         & cmn & CV, Fleurs \\
                   & Min Nan          & nan & CV \\
                   & Polish           & pol & CV, Fleurs, MSD, MLL, VP \\
                   & Romanian         & ron & CV, Fleurs, VP \\
                   & Spanish          & spa & CV, Fleurs, MSD, MLS, VF, VP \\
\midrule
\multirow{6}{*}{2} & Cantonese        & yue & CV, Fleurs \\
                   & Croatian         & hrv & Fleurs, VP \\
                   & Italian          & ita & CV, Fleurs, LAD, MSD, MLS, NCHLT, SWC, VF, VP \\
                   & English          & eng & CV, Fleurs, MSD, MLS, VF, VP \\
                   & Persian          & fas & CV, Fleurs \\
                   & Slovak           & slk & CV, Fleurs, VP \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table}[ht!]
\centering
\caption{Dataset statistics for the training set of each of the language subsets used in our experiments, in the balanced data setting. ISO codes are used for the languages, duration is presented in seconds, and transcript length is in number of characters. Averages and standard deviations are reported.}
\label{tab:data_stats}
\begin{sc}
\resizebox{0.45\columnwidth}{!}{
\begin{tabular}{clcrr}
\toprule
\multirow{2}{*}{\textbf{Set} \textbf{\#}}     & \multirow{2}{*}{\textbf{ISO}}        & \textbf{Number of}        & \multirow{2}{*}{\textbf{Duration}} & \textbf{Transcript} \\ 
 & & \textbf{Data Points} & & \textbf{Length} \\ \midrule
\multirow{6}{*}{1} & ces  &    908       & $4.0 \pm 1.7$  &  $23.8 \pm 22.1$     \\
& cmn & 322 & $10.4 \pm 3.5 $& $36.8 \pm 13.9$ \\
& nan & 1406 & $2.6 \pm 0.7$ & $3.4 \pm 1.9$ \\ 
& pol & 482 & $7.5 \pm 3.0$ & $104.6 \pm 46.3$ \\ 
& ron & 274 & $12.6 \pm 3.1$ & $136.1 \pm 45.1$ \\ 
& spa & 445 & $8.1 \pm 2.2$ & $91.1 \pm 26.4$ \\ \midrule 
\multirow{6}{*}{2} & eng  &   647      &  $4.7 \pm 1.5$  & $63.7 \pm 25.4$      \\
& fas & 693  &  $5.2 \pm 1.7 $& $34.4 \pm 18.2$ \\
& hrv & 291 & $11.7 \pm 3.3$ & $116.3 \pm 35.7$ \\ 
& ita & 326 & $10.7 \pm 3.2$ & $140.4 \pm 42.3$ \\ 
& slk & 330 & $10.6 \pm 3.3$ & $116.2 \pm 38.6$ \\ 
& yue & 243 & $12.2 \pm 3.7$ & $31.7 \pm 10.$2 \\ \midrule 
\multirow{6}{*}{3} & khm  &    206     &  $13.7 \pm 3.4$  &  $122.5 \pm 36.5$     \\
& kmr & 723 &  $5.0 \pm 1.6$ & $30.8 \pm 15.0$ \\
& kor & 269 & $12.5 \pm 3.0$ &  $45.8 \pm 14.1$\\ 
& nbl & 744 & $4.8 \pm 1.9$ & $31.3 \pm 10.0$ \\ 
& nno & 709 & $4.5 \pm 1.2$ & $41.2 \pm 17.3$ \\ 
& tat & 835 &$ 4.3 \pm 1.8$ & $33.2 \pm 20.8 $\\ \midrule 
\multirow{6}{*}{4} & mrj  &    707     & $5.1 \pm 2.0$   &  $40.8 \pm 22.8$     \\
& slv & 918 &  $3.9 \pm 1.1$ &  $30.2 \pm 12.3$\\
& snd & 263 & $12.0 \pm 3.6$ & $105.4 \pm 31.2$\\ 
& sot & 655 & $5.5 \pm 2.0$ & $51.0 \pm 23.6$ \\ 
& spa & 550 & $6.6 \pm 3.4$ & $87.2 \pm 50.2$ \\ 
& urd & 299 & $11.3 \pm 3.4$ & $119.9 \pm 37.1$ \\ \midrule 
\multirow{6}{*}{5} & deu  &    745     &  $4.8 \pm 1.6$  &   $43.3 \pm 16.1$    \\
& eng & 712 & $5.0 \pm 1.5$  & $47.7 \pm 17.4 $\\
& heb & 345 & $10.2 \pm 3.3$ & $91.9 \pm 29.8$ \\ 
& jpn & 290 & $11.5 \pm 3.1$ & $50.0 \pm 15.8$ \\ 
& rus & 318 &$ 10.8 \pm 3.4$ & $125.6 \pm 42.2$ \\ 
& spa & 311 & $11.1 \pm 3.4$ & $144.6 \pm 50.0$ \\ 
\bottomrule
\end{tabular}
}
\end{sc}
% \end{small}
\end{table}


\section{Results}
\label{appendix:more-results}
\label{appendix:results-ft-full}
For each language set, we present the language-specific results of our experiments using balanced training data in Table~\ref{tab:results-ft-full}.
Table~\ref{tab:results-ft-extra-full} shows the language-specific results for the first two sets based on experiments using all available training data in \texttt{ML-SUPERB~2.0}.

\begin{table*}[ht!]
\begin{center}
\begin{small}
\begin{sc}
\centering
\caption{Results of the baseline models, \orig{} models, and \ours{} models on the different language sets, where languages are indicated by their ISO code. We show the CER on the individual languages, CER averaged across languages (\textsc{\texttt{Avg CER}}), and LID accuracy (\textsc{\texttt{LID}}) for fine-tuned \texttt{MMS} and \texttt{XLS-R} models. Best LID and CER results are highlighted, and the CERs for the worst-performing languages are underlined.}
\label{tab:results-ft-full}
\vskip 0.15in
\begin{tabular}{@{}cllrrrrrrcc@{}}
\toprule
\textbf{Set \#}        & \textbf{Model} & \textbf{Type} & \textbf{ces} & \textbf{cmn} & \textbf{nan} & \textbf{pol}   & \textbf{ron} & \textbf{spa} & \textbf{Avg CER} & \textbf{LID} \\
& & & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  & ($\uparrow$) \\ \midrule
\multirow{6}{*}{1} & \multirow{3}{*}{MMS}   & Baseline                 & 8.4  & 52.4  & \underline{60.8} & 3.6   & 13.3 & 1.8  & 23.4 & \textbf{97.4} \\
      & & \orig{}                  & 20.6 & 48.6  & \underline{86.6} & 4.3   & 16.7 & 6.2  & 30.5 & 78.7 \\
      & & \ours{}                 & 10.5 & 46.1  & \underline{56.8} & 3.7   & 17.9 & 2.3  & \textbf{22.9} & 95.8 \\ \cmidrule(l){2-11}
& \multirow{3}{*}{XLS-R}  & Baseline                  & 7.3 & \underline{64.9} & 60.8 & 3.1  & 13.4 & 1.8 & 25.2 & \textbf{92.6} \\
      & & \orig{}                  & 27.4 & 48.9  & \underline{78.4} & 3.7  & 14.9 & 6.6  & 30.0 & 87.8 \\
      & & \ours{}                  & 7.8 & 50.7  & \underline{57.6} & 3.0   & 14.2 & 1.8  & \textbf{22.5}  & 89.5 \\  \midrule
& \textbf{Model} & \textbf{Type}  & \textbf{eng} & \textbf{fas}  & \textbf{hrv} & \textbf{ita}  & \textbf{slk} & \textbf{yue} & \textbf{Avg CER} & \textbf{LID} \\
& & & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  & ($\uparrow$) \\ \midrule
\multirow{6}{*}{2} & \multirow{3}{*}{MMS}   & Baseline                  & 0.2  & 21.8  & 9.0 & 5.9   & 8.2 & \underline{49.4} & 15.8 & \textbf{98.4} \\
      & & \orig{}                   & 11.8 & 29.7  & 10.8 & 6.2   & 10.2 & \underline{55.5} & 20.7 & 98.2\\
      & & \ours{}                   & 0.5  & 22.1  & 8.8  & 5.5   & 8.6  & \underline{44.4} & \textbf{15.0} & 96.2\\ \cmidrule(l){2-11}
& \multirow{3}{*}{XLS-R}  & Baseline                  & 0.1  & 20.6  & 10.9  & 4.6   & 8.9  & \underline{68.8} & 19.0 & \textbf{94.2}\\
      & & \orig{}                   & 12.7 & 28.5  & 14.4 & 5.1   & 10.2 & \underline{58.8} & 21.6 & 87.0\\
      & & \ours{}                   & 0.5  & 21.5  & 12.6 & 5.2   & 10.0  & \underline{45.0} & \textbf{15.8} & 89.3\\ \midrule
& \textbf{Model} & \textbf{Type}  & \textbf{khm} & \textbf{kmr} & \textbf{kor} & \textbf{nbl}   & \textbf{nno} & \textbf{tat} & \textbf{Avg CER} & \textbf{LID}\\ 
& & & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  & ($\uparrow$) \\ \midrule
\multirow{6}{*}{3} & \multirow{3}{*}{MMS}   & Baseline                  & 31.3 & 12.2  & \underline{34.2} & 7.4   & 2.5  & 9.0  & 16.1 & 98.5 \\
      & & \orig{}                   & 33.2 & 19.1  & \underline{34.0} & 22.4  & 9.8 & 13.5 & 22.0 & \textbf{98.7}\\
      & & \ours{}                   & \underline{31.3} & 12.0  & 27.6 & 8.1   & 2.3  & 10.2 & \textbf{15.3} & \textbf{98.7} \\ \cmidrule(l){2-11}
& \multirow{3}{*}{XLS-R}  & Baseline                  & \underline{33.2} & 13.3  & 32.3 & 8.7   & 3.7  & 11.0 & \textbf{17.0} & \textbf{99.2} \\
      & & \orig{}                   & \underline{38.0} & 23.9  & 35.5 & 26.6  & 11.9 & 14.9 & 25.1 & 97.2 \\
      & & \ours{}                   & \underline{32.2} & 14.8  & 31.9 & 10.1   & 5.0  & 12.0 & 17.7 & 97.9 \\ \midrule
& \textbf{Model} & \textbf{Type}  & \textbf{mrj} & \textbf{slv} & \textbf{snd} & \textbf{sot}  & \textbf{spa} & \textbf{urd} & \textbf{Avg CER} & \textbf{LID}\\
& & & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  & ($\uparrow$) \\ \midrule
\multirow{6}{*}{4} & \multirow{3}{*}{MMS}   & Baseline                  & 14.8 & 6.9  & \underline{24.0} & 14.4  & 5.9  & 20.1 & 14.4 & 87.9\\
      & & \orig{}                   & 13.1  & 14.4  & 19.0 & 17.1  & 3.8  & \underline{21.8} & 14.9 & \textbf{91.9}\\
      & & \ours{}                   & 17.7  & 8.1  & 17.5 & 11.4  & 4.4  & \underline{18.4} & \textbf{12.9} & 87.3\\ \cmidrule(l){2-11}
& \multirow{3}{*}{XLS-R}  & Baseline                  & 14.0 & 4.8   & 23.3 & 11.6  & 4.2  & \underline{29.7} & 14.6 & 88.4\\
      & & \orig{}                   & 19.5 & \underline{25.6}  & 18.5 & 23.0  & 3.9  & 21.1 & 18.6 & 83.5\\
      & & \ours{}                   & 11.9 & 6.7   & 21.0 & 13.8  & 4.8  & \underline{24.2} & \textbf{13.7} & \textbf{88.9}\\ \midrule
& \textbf{Model} & \textbf{Type}  & \textbf{deu} & \textbf{eng} & \textbf{heb} & \textbf{jpn}  & \textbf{rus} & \textbf{spa} & \textbf{Avg CER} & \textbf{LID}\\
& & & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  & ($\uparrow$) \\ \midrule
\multirow{6}{*}{5} & \multirow{3}{*}{MMS}   & Baseline                  & 5.4  & 11.1  & 30.2 & \underline{90.0}  & 12.0 & 7.2  & 26.0 & \textbf{96.3}\\
      & & \orig{}                   & 27.6 & 27.0  & 32.6 & \underline{62.2}  & 17.6 & 8.4  & 29.2 & 67.0\\
      & & \ours{}                  & 10.9 & 15.4  & 39.2 & \underline{57.5}  & 13.2 & 9.3  & \textbf{24.3}  & 90.5\\ \cmidrule(l){2-11}
& \multirow{3}{*}{XLS-R}  & Baseline                 & 4.8  & 9.2  & 33.2 & \underline{114.8} & 10.5 & 7.1  & 29.9  & 89.0\\
      & & \orig{}                   & 29.1 & 26.8  & 46.1 & \underline{92.9}  & 16.5 & 9.3 & 36.8 & 57.7\\
      & & \ours{}                   & 5.7  & 9.6  & 38.6 & \underline{71.5}  & 10.1 & 7.3  & \textbf{23.8} & \textbf{91.0}\\ \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table*}

\begin{table*}[ht!]
\caption{Results of the baseline models, \orig{} models, and \ours{} models on the first two language sets using additional amounts of training data per language, where languages are indicated by their ISO code. We show the CER on the individual languages, CER averaged across languages (\textsc{\texttt{Avg CER}}), and LID accuracy (\textsc{\texttt{LID}}) for fine-tuned \texttt{MMS} and \texttt{XLS-R} models. Best LID and CER results are highlighted, and the CERs for the worst-performing languages are underlined.}
\label{tab:results-ft-extra-full}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{@{}cllrrrrrrcc@{}}
\toprule
\textbf{Set \#} & \textbf{Model} & \textbf{Type}  & \textbf{ces} & \textbf{cmn} & \textbf{nan} & \textbf{pol} & \textbf{ron} & \textbf{spa} & \textbf{Avg CER} & \textbf{LID}\\ 
& &  & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  & ($\uparrow$)\\ \midrule
\multirow{6}{*}{1} & \multirow{3}{*}{MMS} & Baseline  & 9.1 & 58.9 & \underline{67.5} & 6.0 & 7.1 & 5.0 & 25.6 & 98.1 \\
 & & \orig{}  & 13.8 & 92.1 & \underline{96.3} & 6.7 & 11.9 & 5.8 & 37.8 & 83.9\\
 & & \ours{}  & 8.7 & 45.9 & \underline{62.8} & 6.2 & 7.5 & 5.3 & \textbf{22.8} & \textbf{98.5}\\ \cmidrule(l){2-11} 
& \multirow{3}{*}{XLS-R} & Baseline  & 13.0 & \underline{92.1} & 78.3 & 9.8 & 12.0 & 8.5 & 35.6 & 96.4\\
 & & \orig{}  & 18.9 & 86.4 & \underline{90.8} & 5.7 & 21.6 & 5.0 & 38.1 & 72.3\\
 & & \ours{}  & 12.9 & 52.5 & \underline{67.5} & 9.0 & 11.9 & 7.8 & \textbf{26.9} & \textbf{97.1}\\ \midrule
& \textbf{Model} & \textbf{Type}  & \textbf{eng} & \textbf{fas} & \textbf{hrv} & \textbf{ita} & \textbf{slk} & \textbf{yue} & \textbf{Avg CER} & \textbf{LID}\\
& & & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  & ($\uparrow$) \\ \midrule
\multirow{6}{*}{2} &\multirow{3}{*}{MMS} & Baseline  & 9.6 & 16.9 & 8.5 & 6.8 & 8.0 & \underline{66.9} & 19.5 & 99.0\\
 & & \orig{}  & 10.1 & 70.0 & 24.3 & 7.9 & 14.8 & \underline{105.4} & 38.8 & 81.0\\
 & & \ours{}  & 9.7 & 18.1 & 8.3 & 6.6 & 7.3 & \underline{48.1} & \textbf{16.4} & \textbf{99.1}\\ \cmidrule(l){2-11} 
& \multirow{3}{*}{XLS-R} & Baseline  & 11.9 & 32.2 & 9.6 & 8.1 & 9.2 & \underline{97.2} & 28.0 & 98.2\\
& & \orig{}  & 8.8 & 88.2 & 33.9 & 6.7 & 23.3 & \underline{102.9} & 44.0 & 80.8\\
& & \ours{}  & 11.6 & 23.2 & 9.3 & 8.2 & 8.9 & \underline{51.4} & \textbf{18.8} & \textbf{98.6}\\ \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\section{Ablation Study}
We present the language-specific results of our ablation study in Table~\ref{tab:ablation-full}.

\begin{table*}[ht!]
\caption{Results of the baseline models and \ours{} models for subset 5, along with a subtractive ablation study where we remove the length-matched group losses (\textsc{\texttt{Dur}}) and smoothed maximization objective (\textsc{Smooth}). We show the CER averaged across languages (\textsc{\texttt{Avg CER}}) as well as the CER on the individual languages and the LID accuracy (\textsc{\texttt{LID}}) for fine-tuned \texttt{MMS} and \texttt{XLS-R} models. Best LID and CER results are highlighted, and the CERs for the worst-performing languages are underlined.}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{@{}llrrrrrrcc@{}}
\toprule
\textbf{Model} & \textbf{Type}  & \textbf{deu} & \textbf{eng} & \textbf{heb} & \textbf{jpn} & \textbf{rus} & \textbf{spa} & \textbf{Avg CER} & \textbf{LID}\\ 
&  & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$) & ($\downarrow$)  &  ($\uparrow$)\\ \midrule
\multirow{3}{*}{MMS} & Baseline   & 5.4 & 11.1 & 30.2 & \underline{90.0} & 12.0 & 7.2 & 26.0 & \textbf{96.3} \\
 & \ours{}  & 10.9 & 15.4 & 39.2 & \underline{57.5} & 13.2 & 9.3 & \textbf{24.3} & 90.5\\ 
 & \quad - Dur   & 19.4 & 21.2 & 30.9 & \underline{84.6} & 12.9 & 8.3 & 29.6 & 66.1\\ 
 & \quad - Smooth   & 95.6 & 96.0 & 98.8 & \underline{102.1} & 97.4 & 97.3 & 97.9 & 13.2\\ \cmidrule(l){2-10}
\multirow{3}{*}{XLS-R} & Baseline   & 4.8 & 9.2 & 33.2 & \underline{114.8} & 10.5 & 7.1 & 29.9 & 89.0 \\
 & \ours{}   & 5.7 & 9.6 & 38.6 & \underline{71.5} & 10.1 & 7.3 & \textbf{23.8} & \textbf{91.0}\\ 
  & \quad - Dur   & 35.6 & 36.5 & 72.9 & \underline{115.2} & 27.4 & 15.9 & 50.6 & 54.4\\ 
 & \quad - Smooth  & 18.5 & 24.5 & 69.9 & \underline{194.2} & 41.2 & 19.9 & 61.4 & 43.2\\ \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\label{tab:ablation-full}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
