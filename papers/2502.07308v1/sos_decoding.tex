\section{Decoding using SoS up to the generalized Singleton bound}

Until now, we have proved that when the inner and outer codes as well as the graph used in AEL amplification are suitably chosen, then the list of codewords around an arbitrary center (corrupted codeword) is of small size. 
In this section, we describe a polynomial time algorithm that takes as input the corrupted codeword, and outputs this list. 


This algorithm is based on the Sum-of-Squares (SoS) hierarchy of semidefinite programs, which gives a systematic way of tightening convex relaxations.  SoS has been used before for decoding algorithms for codes constructed using spectral expanders in \cite{AJQST20, JQST20, RR23, JST23}. 
Among these, \cite{JST23} used the SoS hierarchy to give a list decoding algorithm for AEL up to the Johnson bound, yielding rate $R$ codes efficiently decodable up to $1-\sqrt{R}-\eps$ for any $\eps>0$. 
We will heavily rely on their framework but will improve the decoding radius to $1-R-\eps$ by proving an SoS analog of the generalized Singleton bound for appropriately instantiated AEL amplification.

%
Before going into the proof, we describe additional preliminaries for the SoS hierarchy. Readers familiar with the general terms and concepts can skip ahead to \cref{sec:sos_proof} where we define a specific SoS relaxation for the AEL code, and prove an SoS analog of the generalized Singleton bound. Finally, \cref{sec:sos_algo} describes the decoding algorithm in detail.

%\snote{Just wrote some text. Some of this will go to intro, some will get scattered. Mix above with the next para to produce something reasonable.}

\subsection{Additional Preliminaries: Sum-of-Squares Hierarchy}\label{sec:sos_prelims}
\input{sos_prelims}

\subsection{Pseudocodewords achieve the generalized Singleton bound}\label{sec:sos_proof}

We will show that the proof of list decodability established for integral codewords can be extended to SoS relaxations of these codewords, under certain average-case low covariance conditions. 
First, we describe what an SoS relaxation of an AEL codeword looks like. This relaxation will be such that true codewords are always feasible solutions, but there may be other feasible solutions. These feasible solutions are therefore called \textit{pseudocodewords}.

The main goal will be to show that for some large constant $t$ depending on $k$ and $\eps$, but independent of $n$, the degree-$t$ SoS relaxation is tight enough to decode up to radius $\frac{k-1}{k}(1-R-\eps)$. We will actually work with $k$-tuples of pseudocodewords, and the show that this tuple---after some random conditioning---satisfies a generalized Singleton bound just like a list of true (distinct) codewords.


\paragraph{SoS relaxations for AEL codewords}
Let $G(L,R,E)$ be the bipartite $(n,d,\lambda)$-expander on which the AEL code is defined, and let $\calC_\inn\subseteq \Sigma_\inn^d$ be the inner code. 

Before going into the details of the SoS relaxation for AEL codes, let us set up some convenient notation. For sets $S\sub [k]$ and $F \sub E$, we use $\zee_{S,F}$ to index their Cartesian product $\zee_{S \times F}$. Further, since we will be often dealing with the case when $F=N(\li)$ or $F=N(\ri)$, we will use $\zee_{S,\li}$ as a shorthand for $\zee_{S,N(\li)}$, and similarly use $\zee_{S,\ri}$ as a shorthand for $\zee_{S,N(\ri)}$. For example,
\[
	\tildeVar{\zee_{[k],\li}} = \tildeVar{ \zee_{[k] \times N(\li)}}.
\]


\begin{definition}[$k$-tuple of pseudocodewords]\label{def:k_tuple}
A \emph{$k$-tuple of psueocodewords of degree-$t$} is a pseudoexpectation operator $\tildeEx{\cdot}$ of SoS-degree-$t$ defined on the variables $\{Z_{i,e}\}_{i\in [k], e\in E}$ over the alphabet $\Sigma_{\inn}$ that respects the following constraints:
\[
	\forall i\in [k], \forall \li \in L,~~~ \zee_{i,N(\li)} \in \calC_{\inn}
\] 
\end{definition}

It is easy to see that any $k$ strings $f_1,\cdots ,f_k$ in $\calC_{\inn}^L$ can be used to define a $k$-tuple of pseudocodeword, by simply setting $\zee_{i,e}$ to be $(f_i)_e$ for all $i \in [k], e\in E$. Note that these strings need not be distinct, as written. However, the set of $k$-tuples of pseudocodewords is more general that just integral strings, and in particular, is convex. However, we can still generalize notions like distance for these objects.

\begin{definition}[Distances and pseudocodewords]
	For an $i\in [k]$, we define the left and right distances between the $i^{th}$ component of a $k$-tuple of pseudocodeword $\tildeEx{\cdot}$ of SoS-degree $t\geq 2d$ and any string $g \in \Sigma_{\inn}^E$ as
	\begin{align*}
		\tildeEx{\Delta_L(g, \zee_i)} &\defeq \Ex{\li \in L}{\tildeEx{\indi{g_{\li} \neq \zee_{i, \li}}}} \\
%	\dis(\tildeEx{\cdot},h) &\defeq \Ex{e}{\tildeEx{\indi{\zee_e \neq h_e}}} \\
		\tildeEx{\Delta_R(g,\zee_i)} &\defeq \Ex{\ri\in R}{\tildeEx{\indi{g_{\ri} \neq \zee_{i, \ri}}}}.
	\end{align*}
\end{definition}

The above can be seen as counting the fraction of errors between $\zee_i$ and $g$. We will also need another piece of notation which will make it easier to track the fraction of errors that are common to an entire set $S\sub [k]$.
\begin{align*}
	\tildeEx{\Delta_R(g,\zee_S)} \defeq \Ex{\ri \in R}{\tildeEx{\Pi_{i\in S} \indi{g_{\ri} \neq \zee_{i,\ri}}}}
\end{align*}

We will also need a notion of distance between two pseudocodewords within a $k$-tuple.
\begin{definition}[Distances between pseudocodewords in a $k$-tuple]
Let $\tildeEx{\cdot}$ be a $k$-tuple of pseudocodewords. Let $i,i' \in [k]$ be indices, then the distance between $i^{th}$ and $(i')^{th}$ pseudocodewords can be defined as
\begin{align*}
		\tildeEx{\Delta_L(\zee_i, \zee_{i'})} &\defeq \Ex{\li \in L}{\tildeEx{\indi{\zee_{i, \li} \neq \zee_{i', \li}}}} \\
%	\dis(\tildeEx{\cdot},h) &\defeq \Ex{e}{\tildeEx{\indi{\zee_e \neq h_e}}} \\
		\tildeEx{\Delta_R(\zee_i,\zee_{i'})} &\defeq \Ex{\ri\in R}{\tildeEx{\indi{\zee_{i,\ri} \neq \zee_{i', \ri}}}}.
	\end{align*}
\end{definition}

\paragraph{$\eta$-goodness} Instead of working with arbitrary pseudocodewords, we will work with those  that have small pseudocovariances across a typical edge in $G$. This key property, called $\eta$-goodness, has been used in prior works and we extend its definition from \cite{JST23} to $k$-tuple of pseudocodewords as follows:

\begin{definition}[$\eta$-good pseudocodeword]
    A $k$-tuple of pseudocodewords $\tildeEx{\cdot}$ of degree at least $2kd$ is called $\eta$-good if
    \[
        \Ex{\li,\ri}{\tildeCov{\zee_{[k]\times N(\li)}}{\zee_{[k]\times N(\ri)}}} \leq \eta.
    \]
\end{definition}

The key upshot of having this property is~\cref{lem:avg_corr} which we use in our proof.  Moreover, one can obtain such $\eta$-good pseudocodewords by randomly conditioning an SoS solution. We relegate the proof of these details to \cref{sec:appendix} as they are an adaptation of earlier proofs.

%
%\begin{definition}[AEL Pseudocodewords]
%	For $t\geq 2d$, we define a degree-$t$ AEL pseuocodeword to be a degree-$t$ pseudoexpectation operator $\tildeEx{\cdot}$ on $\zee$ respecting the following constraints:
%	\begin{align*}
%		\forall \li \in L,\, i\in [k_0]\quad \zee_{i, \li} \in \calC_\inn
%	\end{align*}
%\end{definition}
%
%Next we define the distances between a pseudocodeword and a codeword of $\AELC$. 


%\tnote{Moved the definition of eta-good here. Is the notation of Z consistent?}

The main result of this section is the following generalization of~\cref{lemma:common-error-bound} to $\eta$-good pseudocodewords.

\begin{theorem}\label{thm:sos_main}
	Let $k_0\geq 1$ be an integer and let $\eps > 0$. Let $\AELC$ be a
code obtained using the AEL construction using $(G,\calC_{\out}, \calC_{\inn})$, where $\calC_{\inn}$ is $(\delta_0, k_0, \eps/2)$ average-radius list decodable with erasures, and the graph $G$ is a $(n,d,\lambda)$-expander. 
	Let $\tildeEx{\cdot}$ be an $\eta$-good $k_0$-tuple of pseudocodewords that satisfies the following pairwise distance property on the left:
	\[
		\forall i,i'\in [k_0] \text{ with } i\neq i', \qquad \tildeEx{\Delta_L(\zee_i, \zee_{i'})} \geq \beta\mper
	\]
	Further, assume that $\lambda \leq \frac{\beta}{12k_0^{k_0}}\cdot \eps$ and $\eta \leq \frac{\beta}{12k_0^{k_0}}\cdot \eps$.	Then, for any $g \in (\Sigma_{\inn}^d)^R$,
%	, and any $K \sub [k_0]$ with $|K|=k$, 
	\begin{align*}
	\sum_{i\in [k_0]} \tildeEx{\Delta_R(g,\zee_i)} ~&\geq~ (k_0-1)(\delta_0-\eps) + \Ex{\ri \in R}{\tildeEx{ \Pi_{i\in [k_0]} \indi{g_{\ri} \neq \zee_{i,\ri}} }} \\
	~&=~ (k_0-1)(\delta_0 -\eps) + \tildeEx{\Delta_R(g,\zee_{[k_0]})}.
%		\Ex{i\in [k]}{\tildeEx{\Delta_R(g,\zee_i)}} \geq \frac{k-1}{k} \inparen{ \Delta - \Ex{\ri \in R}{\tildeEx{ \Pi_{i\in[k]} \indi{g_{\ri} \neq \zee_{i,\ri}} }} }
	\end{align*}
%
\end{theorem}
%
%\begin{proof}
%By induction on $k$. The case $k=1$ is trivial.
Just like in the proof of our main theorem in \cref{sec:avg-singleton}, we need preparatory claims about the existence of a nice partition and (a variant of) $L^*$. We start by proving analytic generalizations of \cref{lem:type_arg} and \cref{lemma:inductive}. Let $\Tau$ denote the set of all partitions\footnote{Note that now we work with partitions of $[k]$ rather a list of codewords $\calH$.} of $[k]$. For an $\li \in L$ and $\tau \in \Tau$, define the local function, 
	\[
		\Lambda_{\li, \tau}(\zee) \defeq \indi{ (\zee_{1, \li}, \zee_{2, \li}, \cdots , \zee_{k, \li}) \text{ induces partition }\tau}.
	\]
	By definition, for every $\li \in L$,
	\[
		\sum_{\tau \in \Tau} \Lambda_{\li, \tau}(\zee) = 1.
	\]
	Let $\tau_1 \in \Tau$ be the trivial partition with only 1 part. In the integral proof, we said that there must be a non-trivial partition that is induced on a significant fraction of left vertices, and used it to define the set $L^*$. For tuples of pseudocodewords, each left vertex will be inducing a distribution over all possible partitions, and we will argue that there is a partition $\tau^*$ that receives a significant mass among these distributions on average over all $\li \in L$. The indicator of this partition $\Lambda_{\li,\tau^*}(\zee)$ will then play the role of the indicator of the set $L^*$ in integral proof.
	\begin{claim}[Generalization of \cref{lem:type_arg}]\label{claim:best_partition}
		There exists a $\tau^*\in \Tau \setminus \{\tau_1\}$ such that 
		\[
			\tildeEx{\Ex{\li \in L}{\Lambda_{\li, \tau^*}(\zee)}} \geq \frac{\beta}{k^k}.
		\]
	\end{claim}
	\begin{proof}
		If $(\zee_{1, \li}, \zee_{2, \li}, \cdots , \zee_{k, \li})$ induces the partition $\tau_1$, then $\zee_{1,\li} = \zee_{2,\li}$. That is,
		\[
			\Lambda_{\li,\tau_1}(\zee) \leq \indi{\zee_{1,\li} = \zee_{2,\li}}.
		\]
		We bound the contribution from the trivial partition as 
		\[
			\tildeEx{\Ex{\li}{\Lambda_{\li,\tau_1}(\zee)}} \leq \tildeEx{\Ex{\li}{\indi{\zee_{1,\li} = \zee_{2,\li}}}} = \tildeEx{1-\Delta_L(\zee_1,\zee_2)} \leq 1-\beta
		\]
%		Since $\tildeEx{\Delta_L(\zee_1,\zee_2)} \geq \beta$,
%		\begin{align*}
%			&\tildeEx{\Ex{\li}{\indi{\zee_{1, \li} \neq \zee_{2, \li}}}} ~\geq~ \beta \\
%			\implies &~\tildeEx{\Ex{\li}{\indi{\zee_{1, \li} = \zee_{2,\li}}}} ~\leq~ 1-\beta \\
%			\implies &~\tildeEx{\Ex{\li}{\Lambda_{\li,\tau_1}(\zee)}} ~\leq~ 1-\beta.
%		\end{align*}
		The above shows that the partition $\tau_1$ cannot be too common. Next, we use $\sum_{\tau\in \Tau} \Lambda_{\li,\tau}(\zee) =1$ to show that:
		\begin{align*}
			1 = \tildeEx{\Ex{\li}{\sum_{\tau \in \Tau}{\Lambda_{\li,\tau}(\zee)}}} = \tildeEx{\Ex{\li}{\Lambda_{\li,\tau_1}(\zee)}} + \sum_{\tau \in \Tau \setminus \{\tau_1\}} \tildeEx{\Ex{\li}{\Lambda_{\li,\tau}(\zee)}} \\
			\implies\sum_{\tau \in \Tau \setminus \{\tau_1\}} \tildeEx{\Ex{\li}{\Lambda_{\li,\tau}(\zee)}} = 1 - \tildeEx{\Ex{\li}{\Lambda_{\li,\tau_1}(\zee)}} \geq 1-(1-\beta) = \beta.
		\end{align*}
		By averaging, we can use $|\Tau| \leq k^k$ to conclude that there is a $\tau^* \in \Tau \setminus \{\tau_1\}$ such that,
		\[
			\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}} ~\geq~ \frac{1}{|\Tau|} \sum_{\tau \in \Tau \setminus \{\tau_1\}} \tildeEx{\Ex{\li}{\Lambda_{\li,\tau}(\zee)}} ~\geq~ \frac{\beta}{k^k}.\qedhere
		\]
	\end{proof}
	
	\paragraph{Capturing common error locations}
	Suppose the partition $\tau^*$ is given by $[k] = (K_1, \cdots, K_p)$ for some $1<p<k$.  Henceforth, we will be working with this fixed partition. 	We define a function, $\dd_{S,\ri}(g,\zee)$, which is an indicator of whether $\ri \in R$ is a common error location for the pseudocodewords indexed by $S\subseteq [k]$. 
%	Let us define two shorthands for some $S\subseteq [k]$:
	\begin{align*}
		\dd_{S,\ri}(g, \zee) &= \Pi_{i\in S} \indi{g_{\ri} \neq \zee_{i,\ri}} \\
		\dd_{S,e}(g, \zee) &= \dd_{S,\ri}(g, \zee), \text{ where } e=(\li,\ri).
	\end{align*}
Of course, if a vertex $\ri $ is a common error location for all $k$ pseudocodewords, then it is also a common error location for a subset, implying
	\[
		\dd_{S,\ri}(g,\zee) ~\geq~ \dd_{[k],\ri}(g,\zee).
	\]
	We will also need to track the error locations common to $S$ but not to $[k]$, so we define two additional shorthands:
	\begin{align*}
		\fd_{S,\ri}(g, \zee) ~&=~ \dd_{S,\ri}(g,\zee) ~-~ \dd_{[k],\ri}(g,\zee) \\
		\fd_{S,e}(g, \zee) ~&=~ \fd_{S,r}(g, \zee), \text{ where } e=(\li,\ri).
	\end{align*}
		

\paragraph{Key Claims}		
		We will now prove SoS versions of the three main claims from the integral proof; ~\cref{lemma:inductive}, \cref{claim:local-bound}, and \cref{claim:sampling_erasure}. 
	%To begin, let us define the SoS generalizations of the terms in the lemma. 

%Finally, we use that the local snapshots of common errors are upper bounded by the global common errors. This will be another AEL argument.

%	The following table illustrates how the above functions capture...  
%\tnote{Suppresing the dictionary for now.}
%		\begin{table}[h]
%\begin{center}
%  \begin{tabular}{l|c}
%   Codeword & SoS Generalization \\
%    \hline\\
%   $\Delta(g_\ell, \fjl)$ & $\tildeEx{\Ex{e\in N(\li)}{\fd_{K_j,e}(g,\zee)}}$ \\
%    $\indi{\ell\in L^*}$  & $\tildeEx{\Lambda_{\li,\tau^*}(\zee)}$\\
%   	$\abs{L^*}/n$ & $\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}$  \\[2pt]
%   	$\Ex{\ell \in L^*}{\Delta(\gl, \fjl)}$ & $\frac{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \Ex{e\in N(\li)}{\fd_{K_j,e}(g,\zee)}}} }{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}}$
%  \end{tabular}
%  \caption{Dictionary between}
%\end{center}
%\end{table}

The first of these showed that errors observed on $L^*$ serve as a lower bound for global errors on the right. The analog of "averaging over $L^*$ can be carried out by reweighing according to the indicator function $\Lambda_{\li,\tau^*}$ for the $\tau^*$ partition.
	
\begin{lemma}[Generalization of \cref{claim:sampling_erasure}]\label{lemma:local_erasures_upper_bound} 
%For any functions $\Gamma(\ell, \zee), \Psi(\zee) $ of  pseudocodewords, the following holds for $\eta$-good pseudocodeword $\zee$.
%\[
%	\tildeEx{\Ex{\li \sim \ri}{\Gamma(\zee) \cdot \Psi(\zee)}} ~\leq~ \tildeEx{\Ex{\li}{\Gamma(\zee)}} \tildeEx{\Ex{\ri}{\Psi(\zee)}} + \lambda +\eta.			
%\]
Assume that $\lambda \leq \frac{\beta}{12k^{k}}\cdot \eps$ and $\eta \leq \frac{\beta}{12k^{k}}\cdot \eps$.
For the functions $\fd, \dd$, and any set $S\subseteq [k]$ we have,
\begin{align*}
	\frac{\tildeEx{\Ex{\li \sim \ri}{\Lambda_{\li,\tau^*}(\zee) \cdot \dd_{S,\ri}(g,\zee)}}}{\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}} ~\leq~ \tildeEx{\Ex{\ri}{\dd_{S,\ri}(g,\zee)}} + \frac{\eps}{6}.\\
		\frac{\tildeEx{\Ex{\li \sim \ri}{\Lambda_{\li,\tau^*}(\zee) \cdot \fd_{S,\ri}(g,\zee)}}}{\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}} ~\leq~ \tildeEx{\Ex{\ri}{\fd_{S,\ri}(g,\zee)}} + \frac{\eps}{6}.
		\end{align*}
	\end{lemma}
\begin{proof} The proof is based on an AEL-like argument and is identical for either case. The first step uses the expander mixing lemma for pseudocodewords (\cref{lem:eml_sos}), and the second utilizes the $\eta$-good property (\cref{lem:avg_corr}),
	\begin{align*}
%		\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee) \cdot \Ex{e\in N(\li)}{D_{[k],e}(g,\zee)}}} &= 
		\tildeEx{\Ex{\li\sim \ri}{\Lambda_{\li,\tau^*}(\zee) \cdot \dd_{S,\ri}(g,\zee)}}	~&\leq~ \tildeEx{\Ex{\li , \ri}{\Lambda_{\li,\tau^*}(\zee) \cdot \dd_{S,\ri}(g,\zee)}} + \lambda \\
		~&\leq~ \tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}} \cdot \tildeEx{\Ex{\ri}{\dd_{S,\ri}(g,\zee)}} + \lambda + \eta.
	\end{align*}
	To obtain the final consequence we divide by $\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}$ and use~\cref{claim:best_partition}.
		\begin{align*}
		\frac{\tildeEx{\Ex{\li\sim \ri}{\Lambda_{\li,\tau^*}(\zee) \cdot \dd_{S,\ri}(g,\zee)}}}{\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}} ~&\leq~ \tildeEx{\Ex{\ri}{\dd_{S,\ri}(g,\zee)}} + \frac{\lambda + \eta}{\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}} \\
%		~&\leq  \tildeEx{\Ex{\ri}{D_{[k],\ri}(g,\zee)}} + \frac{\lambda + \eta}{(\beta/k^k)} \\
		~&\leq~  \tildeEx{\Ex{\ri}{\dd_{S,\ri}(g,\zee)}} + \frac{\eps}{6}. \qedhere
%		~&=~  \tildeEx{\Delta_R(g,\zee_{[k]})} + \frac{\eps}{6}.
	\end{align*}
\end{proof}

	
%	Finally, define $\lstar := \tildeEx{\Ex{\li \in L}{\Lambda_{\li, \tau^*}(\zee)}}$
%	\snote{Move these definitions outside theorem? $\fd$ is a macro, feel free to change.}
	
	
	
%	\begin{align*}
%	\Delta(\gl, \fjl) ~&\mapsto~~   \tildeEx{\Ex{e\in N(\li)}{\fd_{K_j,e}(g,\zee)}}\;,	\\
%	\indi{\ell\in L^*}  ~&\mapsto~~ \tildeEx{\Lambda_{\li,\tau^*}(\zee)}\;, \\
%	\abs{L^*}/n ~&\mapsto~~ \tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}.
%	\end{align*}
	
%	It is easy to see that when $Z$ is an integral codeword, the two definitions coincide. Combining these, 
%	\begin{align*}
%	\Ex{\ell \in L^*}{\Delta(\gl, \fjl)} = \frac{\Ex{\ell}{\indi{\ell\in L^*} \cdot\Delta(\gl, \fjl)}}{|L^*|} ~&\mapsto~~   \frac{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \Ex{e\in N(\li)}{\fd_{K_j,e}(g,\zee)}}} }{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}} .
%	\end{align*}
%	\snote{Maybe above is not a great idea since we want to discourage $\erase{g}$ in the SoS section.}
	
%	$\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \Ex{e\in N(\li)}{\indi{g_e \neq \zee_{i,e}} \cdot (1-D_{[k],e}(g,\zee))}} $
%	
%	$\Ex{\ell \in L^*}{\Delta(\gl, \fl_j)}$

The second lemma showed that the number of common error locations increases when only considering a subset of indices $K_j \sub [k]$, and this increase can be lower bounded in terms of the distance between $g_{\li}$ (actually, $\gl$) and the common local projections of $K_j$, averaged over $L^*$. The second term on the RHS in the lemma below is the analog of these errors between $\gl$ and common local projections $f_j$, averaged over $L^*$.
	\begin{lemma}[Generalization of  \cref{lemma:inductive}]\label{lemma:more_common_errors}
	For any part $K_j$ in the partition $\tau^*$, we have,
	%, and an arbitrary $i\in H_j$,
	\begin{align*}
		\tildeEx{\Delta_R(g,\zee_{K_j})}
%		&= \Ex{\ri}{\tildeEx{D_{K_j,\ri}(g,\zee)}}
%		&\geq \Ex{\ri}{\tildeEx{D_{[k],\ri}(g,\zee)}} + \frac{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \Ex{e\in N(\li)}{\fd_{K_j,e}(g,\zee)} }}}{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}} - \frac{\eps}{6}\\
		~\geq~ \tildeEx{\Delta_R(g,\zee_{[k]})}  + \frac{\tildeEx{\Ex{\li \sim \ri}{\Lambda_{\li,\tau^*}(\zee) \cdot \fd_{K_j,\ri}(g,\zee)}}}{\tildeEx{\Ex{\li}{\Lambda_{\li,\tau^*}(\zee)}}} - \frac{\eps}{6} .
	\end{align*}
	\end{lemma}
	
	\begin{proof} By using the definition of the functions $\dd, \fd$ and some rearragement, we get,
	\begin{align*}
		\tildeEx{\Delta_R(g,\zee_{K_j})} ~&=~ \Ex{\ri}{\tildeEx{\dd_{K_j,\ri}(g,\zee)}} \\
		~&=~ \Ex{\ri}{\tildeEx{\dd_{[k],\ri}(g,\zee) \cdot \dd_{K_j,\ri}(g,\zee) + (1-\dd_{[k],\ri}(g,\zee)) \cdot \dd_{K_j,\ri}(g,\zee)}} \\
		~&=~ \Ex{\ri}{\tildeEx{\dd_{[k],\ri}(g,\zee)}} + \Ex{\ri}{\tildeEx{\fd_{K_j,\ri}(g,\zee)}}.
	\end{align*}
	
The proof finishes by replacing the second term by the bound from~\cref{lemma:local_erasures_upper_bound}.
%a typical AEL argument. The first step uses the expander mixing lemma (\cref{lem:eml_sos}), and the second utilizes the $\eta$-good property,
%	\begin{align*}
%		\tildeEx{\Ex{\li \sim \ri}{\Lambda_{\li, \tau^*}(\zee) \cdot \fd_{K_j,\ri}(g,\zee)}} \leq~&~ \tildeEx{\Ex{\li, \ri}{\Lambda_{\li, \tau^*}(\zee) \cdot \fd_{K_j,\ri}(g,\zee)}} + \lambda \\
%		\leq~&~ \tildeEx{\Ex{\li}{\Lambda_{\li, \tau^*}(\zee)}} \cdot \tildeEx{\Ex{\ri}{\fd_{K_j,\ri}(g,\zee)}} + \lambda + \eta\\
%		=~&~ \lstar \cdot \tildeEx{\Ex{\ri}{\fd_{K_j,\ri}(g,\zee)}} + \lambda + \eta.
%	\end{align*}
%	
%Dividing both sides by $\lstar$, we get 
%%\cref{eq:more_common_errors_lower_bound} and \cref{eq:more_common_errors_upper_bound}, we get
%	\begin{align*}
%		\tildeEx{\Ex{\ri}{\fd_{K_j,\ri}(g,\zee)}} &~\geq~ \frac{1}{\lstar}\cdot {\tildeEx{\Ex{\li \sim \ri}{\Lambda_{\li, \tau^*}(\zee) \cdot \fd_{K_j,\ri}(g,\zee)}}}- \frac{\lambda+\eta}{\lstar} \\
%		&~\geq~ \frac{1}{\lstar}\cdot {\tildeEx{\Ex{\li \sim \ri}{\Lambda_{\li, \tau^*}(\zee) \cdot \fd_{K_j,\ri}(g,\zee)}}}- \frac{\eps}{6}.\qedhere
%	\end{align*}
\end{proof}


	Finally, we state the local inequality needed from the inner code. Since this inequality is only valid for vertices in $L^*$, we multiply the required equation by indicator $\Lambda_{\li,\tau^*}(\zee)$ so that it is trivial when the indicator is 0. Subject to this indicator being 1, the inequality says that the generalized Singleton bound with erasures holds for the inner code. Note that this bound holds for each vertex in $L^*$ unlike previous sampling lemmas which only work in an average sense over $L^*$.
%		\tnote{Shashank:Check}
	\begin{lemma}[Generalization of \cref{claim:local-bound}]\label{lemma:local_avg_singleton}
		For every $\li \in L$ and for every $j\in [p]$,
		\[
			\Lambda_{\li, \tau^*}(\zee) \parens[\bigg]{ \sum_{j = 1}^p \Ex{e\in N(\li)}{\fd_{K_j, e}(g,\zee)}} ~\geq~ \Lambda_{\li, \tau^*}(\zee) \cdot (p-1) \parens[\Big]{ \delta_0 - \Ex{e\in N(\li)}{\dd_{[k],e}(g,\zee)}  - \frac{\eps}{2}}.
		\]
	\end{lemma}
	\begin{proof}
	Let $M_{\li} \sub E$ be the union of edge neighborhoods over vertices in $R$ that are adjacent to $\li$. That is, 
	\[
		M_{\li} = \bigcup_{\ri 
		\sim \li}  N(\ri)
	\]
	Let us consider the following two local functions that depend on $[k] \times M_{\li}$.
	\begin{align*}
		\mu_1(\cdot) ~&=~ \Lambda_{\li, \tau^*}(\cdot) \;\parens[\bigg]{\; \sum_{j\in [p]} \Ex{e\in N(\li)}{\fd_{K_j, e}(g,\cdot)}\,} \\
		\mu_2(\cdot) ~&=~ \Lambda_{\li, \tau^*}(\cdot) (p-1) \inparen{ \delta_0 - \Ex{e\in N(\li)}{\dd_{[k],e}(g,\cdot)}  - \frac{\eps}{2}}
	\end{align*}
	We will prove this inequality pointwise by showing that for any $\alpha \in \Sigma_{\inn}^{[k] \times M_{\li}}$, $\mu_1(\alpha) \geq \mu_2(\alpha)$.
	
	If $\alpha$ is such that $\Lambda_{\li,\tau^*}(\alpha) = 0$, then the inequality is trivially true. Henceforth, we assume that $\Lambda_{\li,\tau^*}(\alpha) = 1$. This means that $(\alpha_{1,\li},\alpha_{2,\li},\cdots ,\alpha_{k,\li})$ induces the partition $\tau^*$. 
	By definition of $K_j$, for any $i,i'\in K_j$, we get that $\alpha_{i,\li} = \alpha_{i',\li}$. Let us denote this common codeword in $\calC_{\inn}$ as $\fjl$.
	
	Let $g_{\li} \in \Sigma_{\inn}^{d}$ be the local projection of $g$ to the edge neighborhood of $\li$. For every $e\in N(\li)$ that satisfies $\dd_{[k],e}(g,\alpha) = 1$, we replace the corresponding coordinate in $g_{\li}$ by an erasure to obtain $\gl\in \inparen{ \Sigma_{\inn} \cup \{\bot\} }^{N(\li)}$. 
	The fraction of erasures in $g_{\li}$ is
	\[
		s_{\li} = \Ex{e\in N(\li)}{\dd_{[k],e}(g,\alpha)}.
	\]
	Next, we calculate,
	\begin{align*}
		\Ex{e\in N(\li)}{\fd_{K_j, e}(g,\alpha)} ~&=~ \Ex{e\in N(\li)}{\dd_{K_j, e}(g,\alpha) \cdot \inparen{1-\dd_{[k],e}(g,\alpha)}} \\
		~&=~ \Ex{e\in N(\li)}{\dd_{K_j, e}(g,\alpha)} - \Ex{e\in N(\li)}{\dd_{[k],e}(g,\alpha)} \\
		~&=~ \Ex{e\in N(\li)}{\indi{g_{e} \neq f_{j,e}}} - s_{\li} \\
		~&=~ \Delta(g_{\li},\fjl) - s_{\li} \\
		~&=~ \Delta(\gl, \fjl).
	\end{align*}
	Applying the $(\delta_0,k,\eps/2)$-average radius list decodability with erasures of inner code to $\gl$ and the set of codewords $\{\fjl\}_{j\in [p]}$, we get,
	\[
		\sum_{j=1}^p \Delta(\gl,\fjl) ~\geq~ (p-1) \parens[\Big]{\delta_0 - s_{\li} - \frac{\eps}{2}}.
	\]
	Substituting $\Delta(\gl, \fjl) = \Ex{e\in N(\li)}{\fd_{K_j, e}(g,\alpha)}$ and $s_{\li} = \Ex{e\in N(\li)}{\dd_{[k],e}(g,\alpha)}$ gives
	\begin{align*}
		\sum_{j=1}^p \Ex{e\in N(\li)}{\fd_{K_j, e}(g,\alpha)} ~\geq~ (p-1) \cdot\parens[\bigg]{\delta_0 -  \Ex{e\in N(\li)}{\dd_{[k],e}(g,\alpha)} - \frac{\eps}{2}}.\qedhere
	\end{align*}
\end{proof}



	
\subsubsection{Proof of Main Theorem}	

We restate the main theorem and finish the proof using the above lemmas.

\begin{theorem}[Restatement of \cref{thm:sos_main}]
	Let $k_0\geq 1$ be an integer and let $\eps > 0$. Let $\AELC$ be a
code obtained using the AEL construction using $(G,\calC_{\out}, \calC_{\inn})$, where $\calC_{\inn}$ is $(\delta_0, k_0, \eps/2)$ average-radius list decodable with erasures, and the graph $G$ is a $(n,d,\lambda)$-expander. 
	Let $\tildeEx{\cdot}$ be an $\eta$-good $k_0$-tuple of pseudocodewords that satisfies the following pairwise distance property on the left:
	\[
		\forall i,i'\in [k_0] \text{ with } i\neq i', \qquad \tildeEx{\Delta_L(\zee_i, \zee_{i'})} \geq \beta\mper
	\]
	Further, assume that $\lambda \leq \frac{\beta}{12k_0^{k_0}}\cdot \eps$ and $\eta \leq \frac{\beta}{12k_0^{k_0}}\cdot \eps$.	Then, for any $g \in (\F_q^d)^R$,
	\begin{align*}
	\sum_{i\in [k_0]} \tildeEx{\Delta_R(g,\zee_i)} ~&\geq~ (k_0-1)(\delta_0-\eps) + \Ex{\ri \in R}{\tildeEx{ \Pi_{i\in[k_0]} \indi{g_{\ri} \neq \zee_{i,\ri}} }} \\
	~&=~ (k_0-1)(\delta_0 -\eps) + \tildeEx{\Delta_R(g,\zee_{[k_0]})}.
%		\Ex{i\in [k]}{\tildeEx{\Delta_R(g,\zee_i)}} \geq \frac{k-1}{k} \inparen{ \Delta - \Ex{\ri \in R}{\tildeEx{ \Pi_{i\in[k]} \indi{g_{\ri} \neq \zee_{i,\ri}} }} }
	\end{align*}
%
\end{theorem}

\begin{proof}
The proof, as before, is by induction on $k_0$. The base case, $k_0 =1$ is trivial. So we assume the statement holds upto $k-1$, and the goal is to prove it for $k_0=k$. Fix a partition $\tau^* = (K_1,\cdots, K_p)$ as guaranteed by~\cref{claim:best_partition}.  For each part $K_j$, we may apply the induction hypothesis to the sub-tuple defined by it as the pairwise distance property is already assumed. Using this we get, 
	\begin{align*}
\sum_{i=1}^k \tildeEx{\Delta_R(g,\zee_i)} 
		~&=~ \sum_{j=1}^p \sum_{i\in K_j} \tildeEx{\Delta_R(g,\zee_i)} \\
		~&\geq~ \sum_{j=1}^p \inparen{(|K_j|-1)(\delta_0 -\eps) + \tildeEx{\Delta_R(g,\zee_{K_j})}}.
		\end{align*}

The first term is simply,   
$
\sum_{j=1}^p (|K_j|-1)(\delta_0 -\eps) ~=~ (k-p)(\delta_0 -\eps) . 
$ The goal is now to show that, 
\[\sum_{j=1}^p\tildeEx{\Delta_R(g,\zee_{K_j})}	 ~\geq~ (p-1)(\delta_0 -\eps) + \tildeEx{\Delta_R(g,\zee_{[k]})}.\]
 
 For a fixed $j \in [p]$, we apply \cref{lemma:more_common_errors} to obtain, 

		\[
		\tildeEx{\Delta_R(g,\zee_{K_j})}	~\geq~  \tildeEx{\Delta_R(g,\zee_{[k]})}  + \frac{\Ex{\li \sim \ri}{\;\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \fd_{K_j,\ri}(g,\zee)}\,} }{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}} - \frac{\eps}{6}.
		\]

The term $c := \tildeEx{\Delta_R(g,\zee_{[k]})}  - \frac{\eps}{6}$ is independent of $j$.
Summing the RHS over $j \in [p]$, 

\begin{align}
\sum_{j=1}^p	\tildeEx{\Delta_R(g,\zee_{K_j})} - p\cdot c ~&\geq~ \sum_{j=1}^p \frac{\Ex{\li \sim \ri}{\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \fd_{K_j,\ri}(g,\zee)}} }{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}}\\
%~&=~ \sum_{j=1}^p \frac{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \inparen{ \Ex{e\in N(\li)}{ \fd_{K_j,e}(g,\zee)}}}} }{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}}\\
~&=~   { \frac{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee) \cdot \sum_{j=1}^p \inparen{\Ex{e\in N(\li)}{\fd_{K_j,e}(g,\zee)}} }}}{ \Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}} } }\label{eq:main}
		\end{align}

We can now apply \cref{lemma:local_avg_singleton} to the RHS
\begin{align}		
	~&\geq~ { \frac{\Ex{\li}{\tildeEx{\Lambda_{\li, \tau^*}(\zee) \cdot (p-1) \inparen{ \delta_0 - \Ex{e\in N(\li)}{\dd_{[k],e}(g,\zee)}  - \frac{\eps}{2}} }}}{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}}} \\
	~&=~	(p-1)(\delta_0 - \frac{\eps}{2}) -(p-1) \frac{\Ex{\li}{\tildeEx{\Lambda_{\li, \tau^*}(\zee) \cdot \inparen{\Ex{e\in N(\li)}{\dd_{[k],e}(g,\zee)} } }}}{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}}.\label{eq:second}
		 \end{align}

		
To bound the term on the right, we use~\cref{lemma:local_erasures_upper_bound},			\begin{align*}
				\frac{\Ex{\li}{\tildeEx{\Lambda_{\li, \tau^*}(\zee) \cdot \inparen{\Ex{e\in N(\li)}{\dd_{[k],e}(g,\zee)} } }}}{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}} ~&=~ \frac{\Ex{\li\sim \ri}{\tildeEx{\Lambda_{\li, \tau^*}(\zee) \cdot \dd_{[k],\ri}(g,\zee)  }}}{\Ex{\li}{\tildeEx{\Lambda_{\li,\tau^*}(\zee)}}}\\[1em]
\text{(\cref{lemma:local_erasures_upper_bound})}\;\;				~&\leq~  \tildeEx{\Ex{\ri}{\dd_{S,\ri}(g,\zee)}} + \frac{\eps}{6}\\
				~&=~	\tildeEx{\Delta_R(g,\zee_{[k]})} + \frac{\eps}{6}.  
				\end{align*}

Plugging this bound in~\cref{eq:second} and then back in~\cref{eq:main},  we get,		
\begin{align*}	
		\tildeEx{\Delta_R(g,\zee_{K_j})}  ~&\geq~ p\cdot c + (p-1)\parens[\Big]{\delta_0 - \frac{\eps}{2}} -  (p-1) \inparen{\tildeEx{\Delta_R(g,\zee_{[k]})} + \frac{\eps}{6}}\\
		~&=~ \tildeEx{\Delta_R(g,\zee_{[k]})}  +(p-1)\parens[\Big]{\delta_0 - \frac{\eps}{2}} - (2p-1)\cdot \frac{\eps}{6} \\
		~&\geq~ \tildeEx{\Delta_R(g,\zee_{[k]})}  + (p-1)(\delta_0 - \eps) \;. \qedhere
		\end{align*}
%	\begin{align*}
%		&= (k-1)(\Delta - \eps) + \Delta_R(g,\zee_{[k]}) +(p-1)\frac{\eps}{2} - (2p-1)\cdot k^k\inparen{\frac{\lambda+\eta}{\beta}} \\
%		&\geq (k-1)(\Delta - \eps) + \Delta_R(g,\zee_{[k]})
%	\end{align*}
\end{proof}

%\paragraph{Proof of the three lemmas}



\subsection{The final algorithm}\label{sec:sos_algo}

\input{sos_final_alg}
