\section{Introduction}
%
An error-correcting code $\calC \subseteq \Sigma^n$ is a collection of strings (codewords) over a finite alphabet $\Sigma$, such that every pair of codewords is well separated. 
%
Two important parameters associated with a code $\calC$ are its distance $\delta(\calC)$ and rate $\rho(\calC)$, which measure respectively the worst-case error correction capability of a code, and the amount of redundancy present in the code. These are defined as
%
\[
\delta(\calC) 
~=~ \min_{\underset{f\neq g}{f,g \in \calC}} \frac{1}{n} \cdot \abs{\inbraces{i \in [n] ~|~ f_i \neq g_i}}
\qquad \text{and} \qquad
\rho(\calC) ~\defeq~ \frac{1}{n} \cdot \log_{\abs{\Sigma}}\abs{\calC} \mper
\]
%

As may be expected, the separation between codewords (distance) involves a tradeoff with the number of codewords (rate). This is captuted by the well-known Singleton bound, (the asymptotic version of) which implies that for any code $\calC$, we must have $\delta(\calC) \leq 1 - \rho(\calC)$. 
%
The study of the achievable tradeoffs between rate $\rho$ and distance $\delta$ is a fundamental problem in the theory of error-correcting codes, with the tradeoffs given by the Singleton bound being optimal in the regime of large alphabets $\Sigma$. 

Several code families are known to achieve the Singleton bound including the celebrated Reed-Solomon codes~\cite{RS60}, multiplicity codes~\cite{KSY14, Kop15}, and many others~\cite{GRS23}. 
%
While the above codes achieving the Singleton bound often require an alphabet size growing with $n$, a particularly relevant construction by Alon, Edmonds, and Luby~\cite{AEL95} also yields near-optimal tradeoffs with a \emph{constant} alphabet size. 
%
In particular, this construction gives $\delta \geq 1 - \rho - \eps$ using an alphabet of size $2^{1/\eps^{O(1)}}$. 

Moreover, the above code constructions with near-optimal distances also enable highly efficient algorithms for the task of unique decoding, which is the task of recovering a codeword uniquely from, say a $\beta$ fraction of adversarial errors. 
%
For a code with distance $\delta$, we must have $\beta < \delta/2$, and it is indeed possible to achieve this algorithmically using the structure of several code constructions above. 
%
However, unique recovery is no longer possible when $\beta \geq \delta/2$, since the corruptions may be consistent with more than one codewords.  

\vspace{-5 pt}
\paragraph{List decoding capacity.}
%
The notion of list decoding defined by Elias~\cite{E57} and Wozencraft~\cite{W58} allows the output to be a small \emph{list} of codewords, when the fraction of errors is higher than the unique-decoding radius of $\delta/2$.
%
In this setting, both the \emph{combinatorial} question of bounding the number of codewords in the list, and the \emph{algorithmic} one of finding the list, are quite non-trivial and have been the subject of extensive research. Algorithms for list decoding were considered in the foundational works of Guruswami and Sudan~\cite{Sudan97, GS98} who gave the first list decoding algorithms for decoding Reed-Solomon codes up to a threshold known as the Johnson bound, where the list size is known to be bounded for any code.
%
Since then, there has been tremendous progress on both the combinatorial and algorithmic aspects of list decoding, and we refer the reader to excellent references by Guruswami, Rudra, and Sudan~\cite{Guruswami06, GRS23} for more details.
%
The constructions and algorithms for list-decodable codes have also had numerous applications in the areas of complexity theory~\cite{Sudan00, Trevisan05} and pseudorandomness~\cite{GUV09, Vadhan12}, leading to several deep and beautiful connections between these areas.


%
In terms of combinatorial bounds for list decoding \emph{beyond} the Johnson bound, volume arguments can be used to show that for the lists to be subexponential in $n$, we must have $\rho \leq 1 - H_q(\beta)$, where $q = \abs{\Sigma}$ and $H_q(x)$ denotes the $q$-ary entropy function (which approximately equals $x$ for large $q$). This bound is called the list decoding capacity, and is known to be achievable using random codes~\cite{GRS23} and random linear codes~\cite{GHK11}.

A breakthrough result of Guruswami and Rudra~\cite{GR06, GR08}, building on the work of Parvaresh and Vardy~\cite{PV05}, gave the first explicit construction of codes achieving list decoding capacity. 
%
Their construction was based on folded Reed-Solomon codes, and yielded a list of size $n^{O(1/\eps)}$ up to an error radius of $1 - \rho - \eps$, using an alphabet of size $n^{O(1/\eps^2)}$. 
%
Since then, there has been a series of exciting works on reducing the list size and alphabet size, combining folded Reed-Solomon codes with pseudorandom objects like \textit{subspace evasive sets}~\cite{Gur11, DL12,GW13}, expanders~\cite{KRZSW18} and \textit{subspace designs}~\cite{GK16, GRZ21}. 
% 
A series of recent works have also led to significantly better bounds on the list sizes for folded Reed-Solomon codes via a deeper understanding of their structure, starting with beautiful work of Kopparty \etal~\cite{KRZSW18,KRSW23} and Tamo~\cite{Tamo24} yielding list sizes $(1/\eps)^{O(1/\eps)}$, followed by a bound of $O(1/\eps^2)$ by Srivastava~\cite{Sri25}, and culminating in an asymptotically optimal bound of $O(1/\eps)$ by Chen and Zhang~\cite{CZ24}.

In addition to folded Reed-Solomon codes and their variants, there has also been several other explicit constructions of codes achieving list decoding capacity using algebraic-geometric codes~\cite{Gur09, GX13, GRZ21, GX22}, multiplicity codes~\cite{Kop15}, polynomial ideal codes~\cite{BHKS23} and subcodes of Reed-Solomon codes~\cite{GX13, BST24}. 
%
The algebraic constructions above also come with efficient list decoding algorithms using polynomial interpolation methods, which have been a cornerstone of techniques in the area of list decoding. 
%
However, the explicit construction known to achieve a list size of $O(1/\eps)$ at radii $\eps$-close to list decoding capacity, are folded Reed-solomon and multiplicity codes, via the very recent result of Chen and Zhang~\cite{CZ24}. In fact, they show that these codes which in fact achieve two additional strengthenings of the notion of list decoding, as discussed below. 


\vspace{-5 pt}
\paragraph{Average-radius list decoding.}
%
A stronger form of combinatorial list decoding was considered by Guruswami and Narayanan~\cite{GN14}, which shows that the list size at radius $\beta$ is bounded by $k$ by considering a strengthening of the contrapositive. 
%
Note that the bound on the list size is true if for any $g \in \Sigma^n$ and $h_1, \ldots, h_{k} \in \calC$, we have $\max_{i \in [k]} \Delta(g,h_i) > \beta$, where $\Delta(g,h)$ denotes the fractional Hamming distance as before.
%
The notion of average-radius list decoding requires establishing that in fact $\Ex{i \in [k]}{\Delta(g,h_i)} > \beta$ which also bounds the list size for a stronger reason. 

Starting with the beautiful works of Rudra and Wootters~\cite{Wootters13, RW14} which introduced techniques from high-dimensional probability, there have been a sequence of results that random linear codes, and random puncturings of structured codes, can in fact achieve list decoding capacity~\cite{FKS22, GST23, GLSTW24}, with several of the results even applicable for the average-radius list decoding~\cite{BGM23, GZ23, AGL24}.  
%

\vspace{-5 pt}
\paragraph{Generalized Singleton bounds.}
%
Shangguan and Tamo~\cite{ST20} (and independently Roth~\cite{Rot24}) considered a generalization of the Singleton bound in the context of combinatorial list decoding for Reed-Solomon codes, showing (in the asymptotic version) that for any code $\calC$, if the list size at error-radius $\beta$ is always bounded by $k-1$, then one must have $\beta \leq \frac{k-1}{k} \cdot (1 - \rho(\calC))$. 
%
While $k=2$ recovers the Singleton bound on the unique-decoding radius and hence the distance, the above bound also shows that lists at radius $1 - \rho - \eps$ must have size $(1-\rho - \eps)/\eps$. which they conjectured to be optimal.
%
The generalized Singleton bound also gives a different strengthening of the notion of list decoding, this time with a more fine-grained tradeoff for every list size.

Shangguan and Tamo~\cite{ST20}  also give an explicit construction of a Reed-Solomon code achieving the generalized Singleton bound for $k=3$, with alphabet size $q = 2^{2^n}$. 
%
Recent works on random puncturings of Reed-Solomon and related codes show that these in fact achieve the \emph{average-distance version} of (a relaxed) generalized Singleton bound, satisfying for any $g \in \Sigma^n$ and $h_1, \ldots, h_{k} \in \calC$,
\[
\Ex{i \in [k]}{\Delta(g,h_i)} ~\geq~ \frac{k-1}{k} \cdot (1 - \rho - \eps) \mper
\]
In a sequence of works, the alphabet size requirement for the above random constructions has been brought down from exponential in $n$~\cite{BGM23} to polynomial~\cite{GZ23} to linear in $n$~\cite{AGL24}.
%
It was also shown by \cite{BDG24} that exponential alphabet size is \emph{necessary} for the exact version of the Singleton bound, and thus, the relaxation of the bound by $\eps$ is required for smaller alphabet sizes.
%
It was also proved by Alrabiah, Guruswami, and Li~\cite{AGL24} that random linear codes with alphabet size $2^{1/\eps^{O(1)}}$ can achieve the average-radius relaxed generalized Singleton bound for each fixed $k$, and that a dependence of $2^{O_k(1/\eps)}$~\cite{AGL24:alphabet} is necessary even for the relaxed version.

Very recently, Chen and Zhang~\cite{CZ24} proved that folded Reed-Solomon codes satisfy the average-radius (relaxed) generalized Singleton bound.
%
Their construction is explicit and algorithmic, and achieves the $\eps$-relaxed bound with alphabet size $n^{O(k/\eps)}$.


% \fnote{Perhaps, at this point the reader has enough background to understand the main result. Here, it might be possible to state the main result in a very clean way
%   without forcing the reader to have to digest AEL first. Perhaps, the reader might just want to understand what the famility of codes achieves (say to use in their
%   application).

%   \paragraph{Our Main Result (informal version).} The main result of this paper is the first \emph{explicit} construction of codes achieving the average-radius (relaxed)
%                                                  generalized Singleton bound over \emph{constant} sized alphabets. An informal version of our main result is given below. 

% \begin{theorem}[Informal version of main result]
% %
% For every $\rho, \eps \in (0,1)$ and $k \in \N$, there exist an \emph{explicit} family of codes $\mathcal{C} \subseteq \F^n$, such that
% for any $g \in  \F^n$ and any $\calH \subseteq \mathcal{C}$ with $\abs{\calH} \leq k$ 
% \[
% \Ex{h \in \calH}{\Delta(g,h)} ~\geq~ \frac{\abs{\calH}}{\abs{\calH}+1} \cdot (1 - \rho - \eps) \mper
% \]
% Moreover, the alphabet size $\abs{F}$ of the code $\mathcal{C}$ can be taken to be $2^{O_k(1/\eps^{O(1)})}$.
% %
% \end{theorem}

% The key ingredient behind this new family of explicit codes, over constant size alphabets,
% is expander graphs. More precisely, we will use a celebrated expander-based distance amplification
% technique, for which we give a fine-grained analysis showing that it can be used to achieve the
% generalized Singleton bound. We recall this amplification technique next, and then we give a more precise
% version of our main result.
% }

% \mnote{Fair enough. I will think a little more about wether we should first include an informal version, before launching into AEL. I was first including AEL because I wanted to think of our main result as not the code condtruction but the local-to-global phenomenon (which needs AEL), but will think more about this.}

\subsection{The Alon-Edmonds-Luby (AEL) construction and our results}
%
% We show that the expander based distance amplification procedure of Alon, Edmonds and Luby~\cite{AEL95} also gives an elementary construction of linear codes satisfying the average-radius ($\eps$-relaxed) generalized Singleton bound, for every fixed list size $k$, with alphabet $2^{(k^k/\eps)^{O(1)}}$.
%
We give an explicit construction of codes satisfying the average-radius ($\eps$-relaxed)
generalized Singleton bound, for every fixed list size $k-1$, with a constant alphabet size
depending only on $\eps$ and $k$ (we denote the list size by $k-1$ since it makes it easier to state the average-distance inequality for $k$ points).
%
%\snote{Maybe this is a good place to stress that we actually give a procedure to start from any high rate code and do this. And then, when instantiated appropriately with LDPC codes,} 
%\mnote{Did now in the abstract and remarks. Let me know if you want to also stress it here.}
%
Our codes also satisfy the low-density parity check (LDPC) property \ie they can described as kernels of sparse (parity-check) matrices with constant sparsity in each row. While random LDPC codes are known to achieve list decoding capacity~\cite{MosheiffRRSW19}, no explicit constructions of such codes were previously known.
%
\begin{theorem}[Informal version of  \cref{cor:ael_instantiation}]\label{thm:result-intro}
%
For every $\rho, \eps \in (0,1)$ and $k \in \N$, there exist an \emph{explicit} family of codes $\mathcal{C} \subseteq \Sigma^n$, such that $\rho(\calC) \geq \rho$ and
for any $g \in  \Sigma^n$ and any $\calH \subseteq \mathcal{C}$ with $\abs{\calH} \leq k$ 
\[
\Ex{h \in \calH}{\Delta(g,h)} ~\geq~ \frac{\abs{\calH}-1}{\abs{\calH}} \cdot (1 - \rho - \eps) \mper
\]
Moreover,  the code $\mathcal{C}$ has alphabet size $2^{(k^k/\eps)^{O(1)}}$ and  is characterized by parity checks of size $O((k^k/\eps)^{O(1)})$.
%
% the alphabet size $\abs{\Sigma}$ of the code $\mathcal{C}$ can be taken to be $2^{(k^k/\eps)^{O(1)}}$ and $\mathcal{C}$ is characterized by parity checks of size at most $O((k^k/\eps)^{O(1)})$.
%
\end{theorem} 
%
By choosing $k = O(1/\eps)$ in  the above theorem, we also get codes which are average-radius list
decodable up to capacity, with the list size at error radius $1 - \rho - \eps$ bounded by $O(1/\eps)$.
%
\begin{corollary}
For every $\rho, \eps \in (0,1)$, there exist an \emph{explicit} family of codes $\mathcal{C} \subseteq \Sigma^n$, such that $\rho(\calC) \geq \rho$ and
for any $g \in  \Sigma^n$, the list $\calL(g,1-\rho-\eps)$ at radius $1-\rho - \eps$ satisfies
\[
\abs{\calL(g,1-\rho-\eps)} ~=~ \abs{\inbraces{h \in \cC ~\mid~ \Delta(g,h) \leq 1 - \rho - \eps}} ~=~
O(1/\eps) \mper
\]
%
Moreover,  the code $\mathcal{C}$ has alphabet size  $2^{2^{(1/\eps)^{O(1)}}}$ and  is characterized by parity checks of size $2^{(1/\eps)^{O(1)}}$.
%
%
% Moreover, the alphabet size $\abs{\Sigma}$ of the code $\mathcal{C}$ can be taken to be $2^{2^{(1/\eps)^{O(1)}}}$ and $\mathcal{C}$ is characterized by parity checks of size at most $2^{(1/\eps)^{O(1)}}$.
\end{corollary}
%

Our construction is elementary in nature, and relies on a celebrated distance amplification technique of Alon, Edmonds, and Luby~\cite{AEL95} (AEL) based on expander graphs.
%
We derive the above theorem as a consequence of a more general result about a ``local-to-global'' amplification for average-radius list decodability via the AEL construction.
%
We note that in contrast to previous algebraic constructions of codes achieving list decoding
capacity, our arguments do not need to rely on polynomial interpolation or other algebraic
phenomena, and can be proved using simple combinatorial and spectral properties of expander graphs.

%
Before describing our results in detail, we recall how the AEL construction can be used to produce an infinite family of codes arbitrarily close to the Singleton bound for distance.
%
\vspace{-5 pt}
\paragraph{The AEL construction.}
%
Given a balanced $d$-regular bipartitite graph $G=(L,R,E)$ with $\abs{L} = \abs{R} = n$, we call it an $(n,d,\lambda)$-expander if the second singular value of the (normalized) biadjacency matrix is at most $\lambda$. 
%
%
\begin{figure}[htb]
\begin{center}	
\begin{tikzpicture}[scale = 0.7]
\begin{scope}
\draw[] (0,0) ellipse (1.5cm and 3cm);
\node[below] at (0,-3.1) {$L$};
\draw[] (6,0) ellipse (1.5cm and 3cm);
\node[below] at (6,-3.1) {$R$};

%\rotatebox{90}{$\,=$}\cC_\inn \ni \varphi(f^*(\ell))=  

\node[fill,circle,red] (a1) at (0,2) {};
\node[fill,circle,red] (a2) at (0,1) {};
\node[left,darkred] at (-0.2,1) {$\cC_\inn \ni \parens[\big]{f(e_1),f(e_1), f(e_3)} = f_{\ell}  $};
%\node[left] at (-1.9,0.55) {$\Large \rotatebox{90}{\,=} $};
%\node[left] at (-1.2,0.2) {$\cC_\inn \ni \varphi(f^*(\ell))  $};
\node[fill,circle,red] (a3) at (0,0) {};
\node[fill,circle,red] (a4) at (0,-1) {};
\node[fill,circle,red] (a5) at (0,-2) {};
%label={$(f_{\ell}(e_2),f_{\ell'}(e_3), f_{\ell''}(e_2))$}
\node[fill,circle,blue] (b1) at (6,2) {};
\node[fill,circle,blue] (b2) at (6,1) {};
\node[fill,circle,blue] (b3) at (6,0) {};
\node[below,darkblue] at (10,0.45) {{$f_r = \parens[\Big]{f(e_2),f(e_4), f(e_5)} \in \Sigma_\inn^d$}};
\node[fill,circle,blue] (b4) at (6,-1) {};
\node[fill,circle,blue] (b5) at (6,-2) {};

\draw[](a2)--node[pos=0.45,sloped, above] {\small{$f(e_1)$}}(b1);
\draw[](a2)--node[pos=0.45,sloped, above] {\small $f(e_2)$}(b3);
\draw[](a2)--node[pos=0.45,sloped, above] {\small $f(e_3)$}(b5);
\draw[] (a4)--node[pos=0.17,sloped, above] {\small{$f(e_4)$}}(b3);
\draw[] (a5)--node[pos=0.17,sloped, above] {\small{$f(e_5)$}}(b3);

%\node[below] at (2.8,-3.8) {Illustration of the AEL procedure};
\end{scope}
\end{tikzpicture}
\vspace{-10 pt}
\end{center}
\caption{Illustration of the AEL procedure}
\label{fig:ael}
\end{figure}
%

The AEL construction works with such an expanding graph, an ``inner code'' $\calC_{\inn} \subseteq \Sigma_{\inn}^d$ and an ``outer code'' $\calC_{\out} \subseteq \Sigma_{\out}^n$, with $\Sigma_{\out} \subseteq \Sigma_{\inn}^d$. 
%
%
We view a codeword of $\calC_{\out}$ as being written on the left vertices (formally, $\calC_{\out} \subseteq \Sigma_{\out}^L$. The symbol at each vertex $\ell \in L$ is then viewed as an element of $\Sigma_{\inn}^d$, which gives a labeling of the edges (with an ordering fixed in advance) and thus an element of $\Sigma_{\inn}^E$. 
%
Each right vertex $r \in R$ the
n collects the $d$ symbols from the incident edges, forming a symbol in $\Sigma_{\inn}^d$.
%
The procedure can be viewed as using $\calC_{\inn}$ to map $\calC_{\out}$ to a new code $\AELC \subseteq (\Sigma_{\inn}^d)^R \cong (\Sigma_{\inn}^d)^n$.
%
As illustrated in \cref{fig:ael}, we can think of symbols being on the edges of $G$, which form a codeword in $\AELC$ when grouped from the right. We will need to consider both the fraction of left and right vertices in which two codewords differ, and will denote these distances using $\Delta_L(\cdot, \cdot)$ and $\Delta_R(\cdot , \cdot)$ respectively.

An easy computation shows that $\rho(\AELC) \geq \rho(\calC_{\out}) \cdot \rho(\calC_{\inn})$. Moreover, an application of the expander mixing lemma can be used to show that 
%
\[
\delta(\AELC) ~\geq~ \delta(\calC_{\inn}) - \frac{\lambda}{\delta(\calC_{\out})} \mper
\]
%
Thus, when $\rho(\calC_{\out}) \geq 1 - \eps$ and $\lambda \leq \eps \cdot \delta(\calC_{\out})$, this gives a ``local-to-global'' amplification of the rate-distance tradeoffs obtained by the code $\calC_{\inn}$:  we get $\rho(\AELC) \geq \rho(\calC_{\inn}) - \eps$ and $\delta(\AELC) \geq \delta(\calC_{\inn}) - \eps$. 
%
In particular, instantiating the construction with a ``constant-sized'' code $\calC_{\inn}$ achieving the Singleton bound, yields an infinite family of codes (corresponding to a family of expander graphs) which achieve a tradeoff $\eps$-close to the Singleton bound. 

In addition to giving a local-to-global amplification, the AEL procedure is also highly versatile and has been adapted for a number of applications, particularly because it also preserves structural properties of the outer code $\calC_{\out}$. 
%
It is easy to see $\calC_{\out}$ and $\calC_{\inn}$ are both linear over $\F_q$, then so is the code $\AELC$. 
%
It also preserves the property of being LDPC, being locally testable or locally correctable~\cite{GKORZS17, KMRZS16}, linear time unique decodability \cite{GI05}, and also the duality properties needed for quantum codes~\cite{BGG24}. 

Hemenway and Wootters~\cite{HW15} also used the AEL procedure to construct linear-time decodable codes achieving list decoding capacity for \textit{erasures}. We refer the reader to the excellent discussion in \cite{KMRZS16} on the various applications of this procedure.
%
\vspace{-5 pt}
\paragraph{Local-to-global results for generalized Singleton bound.}
%
In the subsequent discussion, we will only consider relaxed versions of the generalized Singleton bound (and will not mention the qualifier explicitly), since it is necessary for having alphabet sizes to be subexponential in $n$.
% 
Similar to the rate-distance tradeoffs discussed above, we show that the AEL procedure also gives a ``local-to-global'' amplification for (a slight strengthening of) the property of satisfying the average-radius generalized Singleton bound. 
%

For a code $\calC \subseteq \Sigma^n$ with distance $\delta$, and $S \subseteq n$ with $\abs{S}/n = s$, considered the ``erased'' code $\calC_S$, where all coordinates in $S$ are replaced by a special symbol $\bot$ and thus do not contribute to distances. The relative distance of the resulting code (which still has $n$ coordinates) is at least $\delta - s$. 
%
If the erased code $\calC_S$ is list decodable with list size $k$ at error radius $\beta = (k/(k+1)) \cdot (\delta - s - \eps)$, we would have for any $g$, and $h_1, \ldots, h_{k+1} \in \calC_S$
\[
\sum_{i \in [k+1]} \Delta(g,h_i) ~\geq~ k \cdot (\delta - s - \eps) \mper
\]
%
We say that the code $(\delta, k_0, \eps)$ average-radius list decodable \emph{with erasures} if the above inequality is true for \emph{all erased codes} $\calC_S$ and all $k < k_0$. 
%
Note that this property can equivalently be stated by simply erasing symbols in coordinates from $S$ in the center $g \in \Sigma^n$ to form the erased word $\erase{g} \in (\Sigma \cup \{\bot\})^n$ and not counting the $\bot$ symbols towards distances from codewords in $\calC$ (which is how we define it in \cref{def:gen_singleton}). 
%
We show that the AEL procedure gives a local-to-global amplification for the above property.
%
\begin{theorem}[Restatement of \cref{thm:main_technical_avg}]\label{thm:main-intro}
%
Let $k\geq 1$ be an integer and let $\eps > 0$. Let $\AELC$ be a
code obtained using the AEL construction using  $(G, \calC_{\out}, \calC_{\inn})$, where $\calC_{\inn}$ is $(\delta_0, k, \eps/2)$ average-radius list decodable with erasures, and $G$ is a $(n,d,\lambda)$-expander for $\lambda \leq \frac{\delta_{\out}}{6{k}^{k}} \cdot \eps$. 
%
Then, $\AELC$ is $(\delta_{0}, k,\eps)$ average-radius list decodable with erasures.
\end{theorem}
%
A reader may notice that instead of the erased codes $\calC_S$, we could have equivalently considered the punctured codes obtained by \emph{restricting} to coordinates in $\bar{S}$, which would yield the same inequality up to normalization factors of $1-s$. 
%
This view can be used to show that several known results for random codes and random linear codes also yield average radius list-decodability with erasures (via a simple union bound over puncturings). 
%
This proves the existence of the inner codes $\calC_{\inn}$, satisfying the necessary conditions of \cref{thm:main-intro} with $\delta_0 = 1 - \rho$, which can then be used to construct codes $\AELC$ satisfying the generalized Singleton bound. 
%
Moreover, if one wants even the inner codes to be fully explicit, they can also be chosen to be folded Reed-Solomon codes, using the results of Chen and Zhang~\cite{CZ24}.
%
%
Instantiating \cref{thm:main-intro} with any of the above choices for $\calC_{\inn}$, and appropriate families of graphs $G$ and codes $\calC_{\out}$, immediately yields the family of codes promised in \cref{thm:result-intro}.
%
%
% \mnote{will remove the following}
% %
% \begin{corollary}[Informal version of \cref{cor:ael_instantiation}]
% %
% For every $\rho, \eps \in (0,1)$ and $k \in \N$, there exist explicit inner codes $\calC_{\inn}$ and an infinite family of explicit codes $\AELC \subseteq (\F_q^d)^n$ obtained via the AEL construction satisfying $\rho(\AELC) \geq \rho$, and 
% for any $g \in  (\F_q^d)^n$ and any $\calH \subseteq \AELC$ with $\abs{\calH} \leq k$ 
% \[
% \sum_{h \in \calH} \Delta(g,h) ~\geq~ (\abs{\calH}-1) \cdot (1 - \rho - \eps) \mper
% \]
% Moreover, the alphabet size $q^d$ of the code $\AELC$ can be taken to be $2^{O(k^{3k}/\eps^7)}$.
% %
% \end{corollary}

%
\vspace{-10 pt}
\paragraph{Remarks.}
%
Given the versatility of the AEL framework, we can also obtain additional structural properties for the codes resulting from our constructions.
%
\begin{itemize}
%
\item[-] As discussed earlier, if both $\calC_{\inn}$ and $\calC_{\out}$ are $\F_q$-linear, then so
  is the resulting code $\AELC$. Note that the alphabet size for $\AELC$ is $q^d$ and thus, we
  obtain linearity only over a \emph{subfield}, as is also the case for other capacity-achieving
  codes such as folded Reed-Solomon.
%
\item[-] Our construction only needs the outer code $\calC_{\out}$ to have high rate and (arbitrarily small) constant distance. In particular, we do not need the outer code to be list decodable for our structural or algorithmic results. Moreover, starting from outer codes with additional properties (such as being LDPC), the AEL construction can often inherit these properties.
%
\item[-] If the code $\calC_{\out}$ is LDPC with parity checks of size at most $w$, then $\AELC$ is LDPC with parity checks of size at most $w \cdot d$ \emph{over the subfield} $\F_q$. 
%
Instantiating with an LDPC code $\calC_{\out}$, we thus also get an explicit construction of LDPC codes achieving list decoding capacity, with parity checks of size $2^{(1/\eps)^{O(1)}}$, where $\eps$ is the (arbitrarily small) gap to capacity. Examples of such codes were only known via random constructions~\cite{MosheiffRRSW19}.
%
\item[-] We note that while we state our results in terms of the average-radius generalized Singleton bound, the AEL construction can also be used to obtain local-to-global results directly for list decoding capacity~\cite{Sri24:thesis}, relying only on a bound on the list size for the inner code.
%
However, the statement in terms of the average-radius bounds leads to a simpler proof and stronger guarantees, and also directly leads to list decoding algorithms via the ``proofs to algorithms''
paradigm~\cite{FKP19}.
%
\end{itemize}
%
\vspace{-10 pt}
\paragraph{Algorithms.}
%
The proofs for our theorems rely on simple spectral inequalities for expander graphs, which can then
be turned into decoding algorithms, using the Sum-of-Squares (SoS) hierarchy of semidefinite
programs.  
%
Our algorithms rely on the ``proofs to algorithms'' framework~\cite{FKP19} adapted to the setting of codes~\cite{JST23}.
%
This framework was used in \cite{JST23} to obtain list decoding algorithms for AEL codes, and in
this work we show how this can be extended to obtain list decoding algorithms matching the
generalized Singleton bound. 
%
%\fnote{Perhaps, it is worth highlighting more this point somewhere. From a combinatorial code construction achieving certain parameters, it is not always immediate that efficient decoding can be done up to them.}
%
\begin{theorem}[Informal version of  \cref{cor:algo-main}]\label{thm:algo-intro}
%
For every $\rho, \eps \in (0,1)$ and $k \in \N$, there exist an \emph{explicit} family of codes $\mathcal{C} \subseteq \Sigma^n$, such that $\rho(\calC) \geq \rho$ and
for any $g \in  \Sigma^n$ and any $\calH \subseteq \mathcal{C}$ with $\abs{\calH} \leq k$ 
\[
\Ex{h \in \calH}{\Delta(g,h)} ~\geq~ \frac{\abs{\calH}-1}{\abs{\calH}} \cdot (1 - \rho - \eps) \mper
\]
%
The code  $\mathcal{C}$ has alphabet size  $2^{(k^k/\eps)^{O(1)}}$, and can be list decoded from
radius $\frac{k-1}{k}(1-\rho-\eps)$ deterministically in time $n^{2^{(k^k/\eps)^{O(1)}}}$ with a
list of size at most $k-1$.
%
\end{theorem} 
%
Broadly speaking, the SoS framework requires one to prove properties of an object by writing the
object as a set of formal variables, and expressing any desired inequalities as sums-of-squares of
low-degree polynomials in these formal variables.
%
The properties of the SoS hierarchy then allow one to write a convex relaxation to search for such an object,
and to deduce these properties also hold for ``convex proxy'' for the object obtained as a solution
to the relaxation.

We obtain the above theorem, by reasoning about a convex proxy for (real embeddings of) the entire list $\calH =
\inbraces{h_1, \ldots, h_k}$, replacing codewords by vectors of formal variables $\inbraces{\zee_1,
  \ldots, \zee_k}$.
%
The spectral expansion of a graph $G$ can be expressed in the form $A_G - u_0
u_0^{\T} \preceq \lambda \cdot I$, where $A_G$ is the normalized adjacency matrix and $u_0$ is the
(known) top eigenvector.
%
A spectral inequality relying on expansion can often be reduced to understanding a quadratic form
$\ip{\zee}{(\lambda \cdot I - A_G + u_0 u_0^{\T}) \zee}$ which can be viewed as a sum-of-squares
expression in the variables $\zee$ since the quadratic form $\ip{\zee}{M\zee}$ of a positive
semidefinite matrix $M = \sum_{\sigma_i} v_i v_i^{\T}$ is $\sum_{i} \sigma_i \ip{v_i}{\zee}^2$.

The key contribution of our work is the ``proof'' part of this
proofs-to-algorithms implementation.
%
Adapting the framework from \cite{JST23} to our setting does require some nontrivial technical ideas (discussed in the proof overview
below) but a reader interested in understanding the code construction and properties can safely skip the algorithmic aspects of our result (everything after \cref{sec:inner-code}) on a first reading.

\subsection{An overview of proofs and techniques}
%
We now give a brief overview of the proof of \cref{thm:main-intro}. 
%
Consider $h \in \AELC$ and $g \in (\Sigma_{\inn}^d)^n$, where we measure the distance
$\Delta_R(g,h)$ as the fraction of right vertices $r \in R$ on which $g$ and $h$ differ.
%
Since the goal is to deduce inequalities about ``global'' distances of the form $\Delta_R(g,h)$
using ``local'' inequalities for the inner code $\calC_{\inn}$, we also consider the local views
$g$ and $h$ from each vertex $\ell \in L$.
%
Formally, we can also view $g, h \in \Sigma_{\inn}^E$ as labeling the edges, which defines for each
$\ell \in L$ local views $g_{\ell}, h_{\ell} \in \Sigma_{\inn}^d$.  For $g_{\ell}$ and $h_{\ell}$ we meaure local
distances $\Delta(g_{\ell}, h_{\ell})$ as a fraction of $d$ edges incident on $\ell$.

Recall that the local distance inequalities show that for any $\ell \in L$, $g_{\ell} \in
\Sigma_{\inn}^d$ and distinct $h_{1,\ell}, \ldots, h_{k,\ell}$
\[
\sum_{i \in [k]} \Delta(g_{\ell}, h_{i,\ell}) ~\geq~ (k-1) \cdot (\delta_0 - \eps) \mcom
\]
and our goal is to deduce a similar inequality for the global distances (possibly with $\eps'$ instead of
$\eps$).
%
Also, note that by taking
$k=2$ and $g_{\ell} = h_{1,\ell}$, we also get that $\Delta(h_{1,\ell},h_{2,\ell}) \geq \delta_0 -
\eps$, and we can treat $\delta_0$ essentially as the distance of $\calC_{\inn}$, and by the AEL
bound, also for the code $\AELC$.
%
In the argument below, we will treat $\eps$ as an arbitrarily small error.

\vspace{-10 pt}
\paragraph{Sampling bound.}
A first observation, which is basically the distance proof of AEL,  is that the local distances
\emph{for most} $\ell \in L$ provide a lower bound on the global distances. 
%
Indeed, let $\Delta(g_{\ell}, h_{\ell}) \geq \theta$ for a set $L' \subseteq L$ of size (say) $\eps
\cdot n$.
%
Each $\ell \in L'$ thus has $\theta \cdot d$ ``error edges'' incident to it, on which $g_{\ell}$ and
$h_{\ell}$ don't match.
%
Let $R' \subseteq R$ be the right endpoints of these error edges and note that each $r \in R'$ must be a vertex
where $g, h$ differ.
%
The expander mixing lemma (\cref{lem:eml}) then shows these that large (greater than $\theta$)
local distances even at $\eps$ fraction of $\ell \in L$ imply large global distance
\[
\abs{L'} \cdot (\theta \cdot d) ~\leq~ \abs{E(L',R')} ~\leq~ \frac{d}{n} \cdot \abs{L'} \cdot
\abs{R'} + \lambda \cdot n \cdot d
\quad \implies \quad
\Delta_R(g,h) ~\geq~ \frac{\abs{R'}}{n} ~\geq~ \theta - \lambda \cdot \frac{n}{\abs{L'}} \mper
\]
By contrapositive, for (say) $\lambda \leq \eps^2$, we have $\Pr{\ell \in L}{\Delta(g_{\ell}, h_{\ell}) \leq
  \Delta_R(g,h) + \eps} \geq 1 - \eps$.
%
For a given $g \in (\Sigma_{\inn}^d)^R$ and codewords $h_1, \ldots, h_k \in \AELC$ and, we can say
using a union bound that
\[
\Pr{\ell \in L}{\forall i \in [k]~~ \Delta(g_{\ell}, h_{i,\ell}) \leq \Delta_R(g,h_i) + \eps} ~\geq~ 1 -
k\eps \mper
\]
%
\vspace{-20 pt}
\paragraph{A wrong proof.}
%
Applying the sampling bound for any ``good'' $\ell$ where all local distances $\Delta(g_{\ell},h_{i,\ell})$
lower bound the global distances $\Delta_R(g,h_i)$, one could guess that we are
already done, since $g_{\ell} \in \Sigma_{\inn}^d$ and the local codewords $h_{1,\ell}, \ldots,
h_{k,\ell}$ satisfy
%
\[
\sum_{i \in [k]} \Delta(g_{\ell}, h_{i,\ell}) ~\geq~ (k - 1) \cdot (\delta_0 - \eps) 
~\implies~
\sum_{i \in [k]} \Delta_R(g,h_i) ~\geq~ (k-1) \cdot (\delta_0 - \eps) - k \cdot \eps
\]
%
Of course the ``bug'' in the above proof is that the local projections $h_{1, \ell}, \ldots,
h_{k,\ell}$ are not necessarily distinct even if the codewords $h_1, \ldots, h_{\ell}$ are, and thus we
cannot apply a local distance inequality for $k$ \emph{distinct} codewords.
%
However, we can get around this issue by carefully examining the consequences of the local
projections being equal for two codewords.
%

\vspace{-5 pt}
\paragraph{Partitions and erasures.} 
%
We can think of each left vertex as inducing a partition $\tau_{\ell} =
(\calH_{1}, \ldots, \calH_{p_{\ell}})$ of the set $\calH$ of codewords, where $h, h'$ are in the
same part if and only if $h_{\ell} = h_{\ell}'$.
%
Since the total number of partitions is at most $k^k$, we fix a
partition $\tau^* = (\calH_{1}, \ldots, \calH_{p})$ which occurs for at least $k^{-k}$ fraction of
$\ell \in L$, forming (say) the set $L^*$, which we still think of as somewhat large if
$\lambda \ll k^{-k}$.

Consider the special case when $\tau^* = (\{h_1\}, \ldots, \{h_{k-2}\}, \{h_{k-1}, h_k\})$ \ie
for all $\ell \in L^*$, it is only the last two codewords that have the same local projection (note
that the projections still depend on $\ell$, just the partition is fixed).
%
Let $\{f_{1,\ell}, \ldots, f_{k-1, \ell}\}$ denote the set of local projections (for $\ell \in L^*$)
where we have $f_{k-1, \ell} = h_{k-1,\ell} = h_{k,\ell}$ for all $\ell \in L^*$.
%
The local distance inequality then gives 
\[
\sum_{i \in [k-1]} \Delta(g_{\ell}, f_{i,\ell}) ~\geq~ (k-2) \cdot (\delta_0 - \eps)
~\implies~
\sum_{i \in [k-2]} \Delta_R(g_{\ell},h_{i}) + \Delta(g_{\ell}, f_{k-1, \ell}) ~\geq~ (k-2) \cdot
(\delta_0 - 2\eps)
\]
%
They key difference is now we no longer think of the term $\Delta(g_{\ell},f_{k-1,\ell})$ as a lower bound
on $\Delta(g,h_{k-1, \ell})$ and $\Delta(g,h_{k,\ell})$, but on the fraction of positions where both
$h_{k-1}$ and $h_k$ \emph{simultaneuosly} differ from $g$.
%
This can again be seen by applying the expander mixing lemma argument above to the set 
$L' = \{\ell \in L ~|~ \Delta(g_{\ell}, f_{k-1, \ell}) \geq \theta\}$. 
%
Each $\ell \in L'$ then has $\theta \cdot d$
``simultaneous error edges'',  with the right endpoints being error locations for both $h_{k-1}$ and $h_{k}$.
%

Now consider the ``erased code'' $\calC_S^{\AEL}$ where we erase the set $S \subseteq R$ of size (say) $s
\cdot n$ of locations where $g_{r} \neq h_{k-1, r}$ and $g_r \neq h_{k,r}$, and don't count the
erased locations towards the distance. 
%
Since the distance of $\AELC$ is at least $\delta_0 - \eps$, the distance of $\calC_S^{\AEL}$ is at least
$\delta_0 - s - \eps$. Also, $\Delta_S(g,h_{k-1}) = \Delta_R(g,h_{k-1}) - s$ and similarly for
$h_k$, since they both do differ from $g$ in the erased locations. Triangle inequality then gives
\[
\Delta_S(g,h_{k-1}) + \Delta_S(g,h_k) ~\geq~ \delta_0 - s ~\implies~ \Delta_R(g,h_{k-1}) +
\Delta_R(g,h_{k}) ~\geq~ \delta_0 -\eps + s
\]
%
Adding the two inequalities above, and using the sampling lower bound $\Delta(g_{\ell},f_{k-1,\ell}) \leq s +
\eps$, then proves the average distance inequality for $h_1, \ldots, h_k$.
%
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{partition}
\vspace{-10 pt}
\caption{Local projections and partitions}
\vspace{-10 pt}
\label{fig:partition}
\end{figure}
%
\vspace{-5 pt}
\paragraph{Induction.}
%
The above idea of using erasures for common error locations, can also be extended to arbitrary
partitions $\tau^* = (\calH_1, \ldots, \calH_p)$. 
%
We now apply the local distance inequalities for codewords $f_{1,\ell}, \ldots, f_{p,\ell}$ corresponding to the different parts, at a ``good'' vertex $\ell \in L^*$ where the sampling bound holds. This gives
%
\[
\sum_{j \in [p]} \Delta(g_{\ell}, f_{j, \ell}) ~\geq~ (p-1) \cdot (\delta_0 - \eps)
\quad \implies \quad 
\sum_{j \in [p]} s_j ~\geq~ (p-1) \cdot (\delta_0 - \eps) - p \eps \mcom
\]
%
where $s_j$ now denotes the fraction of simultaneous errors for all $h \in \calH_j$.

For each part $\calH_j$ we also consider an ``erased'' code with $s_j$ fraction of erasures, corresponding to common error locations of all codewords in $\calH_j$. When $\abs{\calH_j} > 2$, the triangle inequality needs to be replaced by an average distance inequality (for global codewords in) the erased code. 
%
To arrange this, we prove our results by induction on $k$, which gives that (assuming $\abs{\calH_j} \leq k-1$ for all parts), 
\[
\forall j \in [p] \quad \sum_{h \in \calH_j} \Delta_R(g,h) ~\geq~ \inparen{\abs{\calH_j} - 1} \cdot (\delta_0 - \eps) + s_j \mper
\]
%
Adding the above inequalities and the lower bound on $\sum_j s_j$ then yields the result, since $\sum_j \abs{\calH_j} = k$. (Actually, we need a slight strenghtening of the inequalities above to correctly carry out the induction step, for which the details can be found in \cref{sec:avg-singleton}.)

% which means that such inequalities are
% available for all $\abs{\calH_j} \leq k-1$.

Finally, we also need to rule out the trivial partition with only one part of size $k$, since we needed $\abs{\calH_j} \leq k-1$ for all $j \in [p]$ to use induction. 
%
However, this cannot happen as this would mean that $h_{1,\ell} = \cdots = h_{k,\ell}$ for (essentially) all $\ell$. But $h_1, \ldots, h_k$ are distinct codewords in $\AELC$ and thus their
projections must differ in at least $\delta_{\out}$ fraction of left vertices.
%
\vspace{-5 pt}
\paragraph{Discussion.}
%
As mentioned earlier, our proof techniques rely on elementary combinatorial and spectral arguments,
instead of polynomial interpolation, which has been the workhorse for list size bounds in all
previous explicit constructions achieving list decoding capacity. 
%
As compared to previous applications of AEL for list decoding, the key difference here is a
fine-grained analysis of the structure of the local lists. 
%
While many previous analyses treated the local lists as a black box and relied on stronger (list
recovery) properties of the outer code for algorithmic applications, the idea of keeping track of
their structure, such as in terms of common error locations, may also be useful for other applications.

\vspace{-5 pt}
\paragraph{Connections to interleaved codes.}
%
The idea of using error locations of some parts of a code as erasures for the remaining code was already used in the work of Gopalan, Guruswami, and Raghavendra \cite{GGR09}. However, there the focus was to show that when starting with an already good list decodable code, its list size does not degrade too much upon interleaving. The interleaving operation can be seen as the dense analog of AEL, but since it does not change the blocklength, the results in \cite{GGR09} did not give an infinite family of codes starting from a single (inner) code as we do.

In more detail, an order-$t$ interleaving of a code $\calC$ of blocklength $n$ can be seen as applying the AEL procedure with $\calC$ as the inner code and $\calC^t$ as the trivial outer code, on the complete bipartite graph $K(t,n)$. By replacing all uses of the expander mixing lemma in the proof by $E(S,T) = |S|\cdot |T|$ (or equivalently, $\lambda=0$), one can verify that if the code $\calC$ achieves generalized Singleton bound with erasures, so does its order-$t$ interleaving. This also improves upon the list size that one could obtain via a black-box application of \cite{GGR09} on a code that lies close to the generalized Singleton bound.

The idea of using error locations as erasures can also be used to prove weak non-trivial bounds on list size of interleaved codes in general, and follows the same structure as the proof of Schwartz-Zippel lemma. In particular, both the list codewords for the interleaving of \emph{arbitrary} codes and the roots of a multivariate polynomial are contained in sets with a common structure, which immediately gives list size upper bounds. Such results were known before for the interleaving of Reed-Solomon codes, which can be derived using multivariate interpolation combined with the Schwartz-Zippel lemma \cite{CS03, PV05, GX13}. Our argument using erasures is however applicable to general codes and uses only on their distance, which is surprising because for non-algebraic codes the analog of multivariate interpolation is not clear at all. We believe this combinatorial handle on interleaved codes sheds light on interpolation-based decoders, and may find further applications. The interested reader is referred to \cite{Sri24:thesis} for details.

\vspace{-5 pt}
\paragraph{Outline of our algorithm.}
%
As discussed earlier, our algorithm for list decoding a given $g \in \Sigma^n$ searches for ``convex
proxies'' given by the SoS hierarchy, for the entire list of codewords in a ball around $g$.
%
We consider vectors $\zee_1, \ldots, \zee_k$ corresponding to (real embeddings of) the codewords
and the solutions to the SoS relaxation can be described as objects $\tildeEx{\zee_1}, \ldots,
\tildeEx{\zee_k}$ containing values corresponding to all low-degree monomials in the variables,
which we refer to as ``pseudocodewords''.
%
One can also extend $\tildeEx{\cdot}$ by linearity to form a linear operator which yields values for
all polynomials. The constraints of the SoS hierarchy imply that $\tildeEx{P(\zee)^2} \geq 0$ for all
low-degree polynomials $P$, and thus all inequalities that can be expressed as sums-of-squares of
low-degree polynomials are satisfied by the solution.

Our algorithm simply searches for the largest $k'$ for which there exists a tuple $(\tildeEx{\zee_1}, \ldots,
\tildeEx{\zee_{k'}})$ of pseudocodewords satisfying two conditions:
%
\begin{enumerate}
\item For all $i \neq j$, $\zee_i$ and $\zee_j$ are far (just as codewords are):
  $\tildeEx{\Delta_L(\zee_i, \zee_j)} \geq \delta'$.
\item For all $i$, $\zee_i$ is within the required error radius of $g$ (just as list elements are):
  $\tildeEx{\Delta_R(g,\zee_i)} \leq \beta$.
\end{enumerate}
%
Note that we use above that the distance $\Delta_L$ and $\Delta_R$ can be expressed as low-degree
polynomials in the variables $\inbraces{\zee_i}_{i \in [k']}$ (which is easy diffcult to show).
%
The analysis for the algorithm now consists of two parts. 
%

We first show that for a maximum $k'$ where the convex relaxation for $(\tildeEx{\zee_1}, \ldots,
\tildeEx{\zee_{k'}})$ is feasible, but the one for $k'+1$ codewords is not, any \emph{true} codeword
$h \in \AELC$ with $\Delta_R(g,h) \leq \beta$ must (essentially) satisfy $\tildeEx{\Delta_L(\zee_i, h)}
\leq \delta'$ for some $i \in [k]$. 
%
This is because if $h$ is far from all $\zee_i$s, we can augment the solution to a $k'+1$ tuple by
adding (low-degree monomials in) $h$ as $\tildeEx{\zee_{k'+1}}$. 
%
Since SoS is a relaxation, true codewords are also valid pseudocodewords, and thus $k'$ cannot
be maximum.
%
%
Given this ``covering'' property for the maximum $k'$ and by choosing $\delta'$ small enough, we can
obtain any $h$ in the list by ``unique-decoding'' the corresponding codeword of $\calC_{\out}$ from
the vector $\tildeEx{\zee_i}$ for some $i \in [k]$.
%
This argument is presented in \cref{sec:sos_algo}.

The second part of the proof consists in showing that \emph{there exists a maximum} $k'$. 
%
This follows from extending the average distance inequality to pseudocodewords, using the fact that
the proof can be expressed as sums-of-squares, and is proved in \cref{sec:sos_proof}. 
%
Formally, we get that for any $k'$-tuple of pseudocodewords as above, we must have
\[
\sum_{i \in [k']} \tildeEx{\Delta_R(g, \zee_i)} ~\geq~ (k' - 1) \cdot (\delta_0 - \eps) \mper
\]
%
However, for $\beta \leq ((k-1)/k) \cdot (\delta_0 - \eps)$, this contradicts condition (2) above
unless $k' \leq k$. 
%
Using codes with $\delta_0 \geq 1 - \rho - \eps$ and taking $k = O(1/\eps)$, we can use the above
algorithm to decode the code $\AELC$ arbitrarily close to list decoding capacity.


% \subsection{Intro things to remember}
% Compiling a list of points to emphasize in the introduction, so we don't forget anything later.

% \begin{itemize}
% 	\item We should make sure that the reader appreciates that our technique is not born because of the recent results on generalized Singleton bound, and that we would have built capacity achieving codes even in 2017 for example.
% 	\item This AEL as sparsified interleaving view is not commonly known, even among experts like Guruswami. We can emphasize it more.
% 	\item The decoding statement can be seen as independent of the presence of outer code. "We can always find $k-1$ centers such that any true codeword is super-close to one of them." The outer code just picks out at most 1 from each of these centers. Such a generalization is pleasing as it captures interleaving as a special case.
% 	\item The idea of tracking common error locations may find further applications. In some sense, once you know that we need to track common error locations, the proof writes itself.
% 	\item Can be made LDPC, LTC, linear-time unique-decodable etc. \cite{KMRZS16}
% 	\item We use fine grained information from local lists instead of black box list recovery. Does this have further applications?
% 	\item Explanation for exponential alphabet size of AEL. AGL explains AEL. Time to search for poly-sized alphabet codes via expanders to match AG codes. \snote{Actually, I take this back. I don't think this is true.}
% 	\item I think our codes are truly linear-time encodable. Is it even worth talking about this?
%         \item \fnote{List decoding capacity seems to imply capacity of q-ary BSC~\cite{PSW24}. Perhaps, worth mentioning this connection? In itself, list decoding capacity was already there. What does the generalized Singleton give on top of it?}
%         \item \fnote{Can any new explicit expander construction be deduced from explicitly achieving the generalized Singleton bound? Was GUV using list recovery?}
%         \item \fnote{Does this imply any new extractors? It seems that Xin Li has a new paper on codes using extractors. Is some form of converse true?}
% \end{itemize}


%!TEX root=main.tex

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
