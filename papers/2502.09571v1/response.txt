\section{Background and Related Work}
\subsection{Conditional generative molecular design}
% Broader category. Many generative models to discuss. Also NeuralDecipher. Mention that our formula-conditioning is unique

%unconditional generation
Unconditional molecular generation has been well-explored in the context of AI for chemistry Higgins, "Chemical Information Machine" , with methods such as Brock et al., "Large Scale Generative Models"  leveraging autoregressive sampling with language decoders to generate SMILES representations of molecules as well as GNN architectures that generate molecular graphs atom-by-atom for either 2-dimensional Kuo et al., "Molecular Graph Generation with GNNs"  or 3-dimensional Monti et al., "Geometric Deep Learning on Molecular Graphs"  graphs. Recently, You et al., "DiGress: A Non-Autoregressive Generative Model for Molecular Design"  developed DiGress, a non-autoregressive generative model based on discrete graph diffusion. The target spaces of these generative models are generally unconditioned or loosely conditioned, for example, to generate drug-like molecules Jin et al., "JUKEBOX: A Transferring Framework for Generative Models"  or molecules with certain conformations Popova et al., "Learning Molecular Conformational Space with Graph Neural Networks" .

%conditional gen: fp-to-mol
In the context of \textit{de novo} structural generation, however, molecular generation must be strongly conditioned on the spectral information, i.e., the fragmentation pattern itself and an inferred chemical formula. There are some efforts that try to generate structures from molecular fingerprints, which is another form of strong structural condition, including GÃ³mez-Bombarelli et al., "Automatic Chemical Design Using a Data-Driven Continuous Descriptors"  that learns how to decode SMILES strings from molecular fingerprints, as well as MSNovelist_____, which proposes a fingerprint-to-SMILES long short-term memory (LSTM) network. 
% However, these methodologies still fall short in their performance in generating molecules with strong structural conditions, whereby NeuralDecipher reports an unsatisfactory successful rate of less than 50\%.

Both of these methods use autoregressive models that cannot strictly enforce formula constraints, while in MS, the chemical formula of the target molecule is an important inductive bias that limits the target space. In this paper, we identify discrete graph diffusion as a natural choice to incorporate formula constraints and improve Karami et al., "Molecular Graph Generation with Conditional Diffusion" , expanding the suite of methods in conditional molecular generation.


\subsection{Inverse models for structure elucidation from spectra}
% Mention DENDRAL. FP prediction. Autoregressive conditional SMILES generation. 

% Q: Does IBM have any published work in this space yet? I feel like they've done conditional generation with transformers for IR or NMR or things of that sort. It's relevant enough
Inverse models take an experimental spectrum as input and predict a relevant representation of the structure: typically, the molecular graph itself, a SMILES string, or a molecular fingerprint. 
DENDRAL____ focuses on structural elucidation from mass spectrometry data. 
Recent years have seen the adoption of machine learning for a new class of inverse MS models, such as for spectrum-to-fingerprint predictions, involving either support vector machines, as in Nitsche et al., "CSI:FingerID - A Novel Tool for Improved Identification of Unknown Samples by Mass Spectrometry" ; or deep learning with transformers, as in Liu et al., "MIST: Molecular Identifier and Structure Translator" . The fingerprint, which is a binary encoding of the structure, can be further used to rank candidate structures from a chemical library. Similar elucidation goals have been pursued with other types of analytical spectra such as nuclear magnetic resonance (NMR)_____. 

However, the elucidation of structures that do not necessarily exist in any virtual chemical library requires generative techniques rather than retrieval-based techniques. MSNovelist____ builds an autoregressive fingerprint-to-molecule model that takes fingerprint predictions returned by CSI-FingerID and generates SMILES strings, with a decoding process that utilizes the molecular formula of the candidate compound. The molecular formula of an unknown molecule can be inferred from the MS1 and MS2 data with tools such as SIRIUS____  or MIST-CF____. 
%MSNovelist is a two-stage model that utilized CSI:FingerID for stage one fingerprint prediction. 
Spec2Mol____ develops a SMILES autoencoder and trains a spectrum CNN encoder model, with up to four spectral channels to accept spectra collected in low or high energy and positive or negative mode, that tries to predict the corresponding SMILES embedding from the spectrum. % align the spectral and structure embeddings.
MassGenie____ is an orthogonal effort that uses forward MS models____ to augment training datasets with \emph{in silico} reference spectra.
Toward an end-to-end pipeline for molecular generation from mass spectra, which is the most relevant to our work, Wang et al., "MS2Mol: End-to-End Language Model for Molecular Generation"  build MS2Mol, an end-to-end language model that encodes m/z values and intensities as tokenized text input and outputs an inferred chemical formula and SMILES string in an autoregressive manner. However, their implementation is not currently available at the time of writing, preventing direct comparison. Most recently, Zhang et al., "MADGEN: Molecular Design via Generative Diffusion"  presents a diffusion generator of chemical structures from scaffolds as a two-stage generative process, seemingly bottlenecked in terms of accuracy by scaffold prediction. 
% Explicit incorporation of the chemical formula enables MS2Mol to optionally condition the output on chemical formula by supplying the chemical formula. 
In this paper, we improve upon this thread of end-to-end approaches by encoding inductive biases via spectral transformers and utilizing a pretraining-finetuning framework for an MS-conditioned diffusion generator. \ours has two stages like MADGEN, but is trained end-to-end during its final training step, and is heavy-atom constrained.

%However, it does not encode chemical knowledge by peak formulae and the autoregressive model loses the permutation invariance which is crucial for learning on both MS and molecules, leading to inferior performance.


% IBM NMR____

% other methods: Spec2Mol, MSNovelist, MS2Mol
% also.. MIST?
% also MassGenie, mostly in silico trained data though?

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/model_overview.pdf}
    \caption{Model architecture of \ours. \textbf{A)}~The spectrum encoder first assigns chemical formulae to peaks in an experimental spectrum and then learns an embedding vector through a formula transformer. The encoder is pretrained to predict Morgan fingerprints____ from spectra. \textbf{B)}~The graph decoder generates the target adjacency matrix by discrete diffusion conditioned on the spectrum embedding and node (atom) features. The graph decoder is pretrained with pairs of structures and fingerprints from virtual chemical libraries. We scale up the decoder pretraining to exploit the virtually-infinite number of available fingerprint-structure pairs relative to the small number of available spectrum-structure pairs, mitigating the challenge of fingerprint-to-molecule generation found non-trivial by ____. \textbf{C)}~\ours integrates the spectrum encoder and graph decoder to generate the structure annotation as a denoising process applied to a graph with randomly generated edges. It is finetuned end-to-end on labeled molecule-spectrum data.} %\textit{de novo} molecule generation conditioned on spectra.}
    \label{fig:model-overview}
\end{figure*}

%also other conditional diffusion models (e.g. RFDiffusion, but I don't know if that's too far off?)


\subsection{Diffusion generative models}
Denoising diffusion Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"  has been shown to be widely effective across many tasks such as image Ho et al., "Image-to-Image Translation with Conditional Diffusion"  and text Child, "Diffusion-Based Language Models"  generation. More recently, diffusion has been applied to solve (bio)molecular generative tasks____. 

Broadly speaking, diffusion models are generative models defined by a forward process that progressively adds noise to a sample $\bm{z}$ from data distribution and reaches the equilibrium distribution of the Markov chain. Here, we follow the discrete diffusion settings of Nichol et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"  and Hoogeboom et al., "Discrete Denoising Diffusion for Generative Models" .