\section{Background and Related Work}
\subsection{Conditional generative molecular design}
% Broader category. Many generative models to discuss. Also NeuralDecipher. Mention that our formula-conditioning is unique

%unconditional generation
Unconditional molecular generation has been well-explored in the context of AI for chemistry____, with methods such as ____ leveraging autoregressive sampling with language decoders to generate SMILES representations of molecules as well as GNN architectures that generate molecular graphs atom-by-atom for either 2-dimensional____ or 3-dimensional____ graphs. Recently, ____ developed DiGress, a non-autoregressive generative model based on discrete graph diffusion. The target spaces of these generative models are generally unconditioned or loosely conditioned, for example, to generate drug-like molecules____ or molecules with certain conformations____.

%conditional gen: fp-to-mol
In the context of \textit{de novo} structural generation, however, molecular generation must be strongly conditioned on the spectral information, i.e., the fragmentation pattern itself and an inferred chemical formula. There are some efforts that try to generate structures from molecular fingerprints, which is another form of strong structural condition, including Neuraldecipher____ that learns how to decode SMILES strings from molecular fingerprints, as well as MSNovelist____, which proposes a fingerprint-to-SMILES long short-term memory (LSTM) network. 
% However, these methodologies still fall short in their performance in generating molecules with strong structural conditions, whereby NeuralDecipher reports an unsatisfactory successful rate of less than 50\%.

Both of these methods use autoregressive models that cannot strictly enforce formula constraints, while in MS, the chemical formula of the target molecule is an important inductive bias that limits the target space. In this paper, we identify discrete graph diffusion as a natural choice to incorporate formula constraints and improve ____, expanding the suite of methods in conditional molecular generation.


\subsection{Inverse models for structure elucidation from spectra}
% Mention DENDRAL. FP prediction. Autoregressive conditional SMILES generation. 

% Q: Does IBM have any published work in this space yet? I feel like they've done conditional generation with transformers for IR or NMR or things of that sort. It's relevant enough
Inverse models take an experimental spectrum as input and predict a relevant representation of the structure: typically, the molecular graph itself, a SMILES string, or a molecular fingerprint. 
DENDRAL, arguably the first expert system that applied AI to science, focuses on structural elucidation from mass spectrometry data____. 
Recent years have seen the adoption of machine learning for a new class of inverse MS models, such as for spectrum-to-fingerprint predictions, involving either support vector machines, as in CSI:FingerID____; or deep learning with transformers, as in MIST____. The fingerprint, which is a binary encoding of the structure, can be further used to rank candidate structures from a chemical library. Similar elucidation goals have been pursued with other types of analytical spectra such as nuclear magnetic resonance (NMR)____. 

However, the elucidation of structures that do not necessarily exist in any virtual chemical library requires generative techniques rather than retrieval-based techniques. MSNovelist____ builds an autoregressive fingerprint-to-molecule model that takes fingerprint predictions returned by CSI-FingerID and generates SMILES strings, with a decoding process that utilizes the molecular formula of the candidate compound. The molecular formula of an unknown molecule can be inferred from the MS1 and MS2 data with tools such as SIRIUS ____
 or MIST-CF ____. 
%MSNovelist is a two-stage model that utilized CSI:FingerID for stage one fingerprint prediction. 
Spec2Mol____ develops a SMILES autoencoder and trains a spectrum CNN encoder model, with up to four spectral channels to accept spectra collected in low or high energy and positive or negative mode, that tries to predict the corresponding SMILES embedding from the spectrum. % align the spectral and structure embeddings.
MassGenie____ is an orthogonal effort that uses forward MS models____ to augment training datasets with \emph{in silico} reference spectra.
Toward an end-to-end pipeline for molecular generation from mass spectra, which is the most relevant to our work, ____ build MS2Mol, an end-to-end language model that encodes m/z values and intensities as tokenized text input and outputs an inferred chemical formula and SMILES string in an autoregressive manner. However, their implementation is not currently available at the time of writing, preventing direct comparison. Most recently, MADGEN____ presents a diffusion generator of chemical structures from scaffolds as a two-stage generative process, seemingly bottlenecked in terms of accuracy by scaffold prediction. 
% Explicit incorporation of the chemical formula enables MS2Mol to optionally condition the output on chemical formula by supplying the chemical formula. 
In this paper, we improve upon this thread of end-to-end approaches by encoding inductive biases via spectral transformers and utilizing a pretraining-finetuning framework for an MS-conditioned diffusion generator. \ours has two stages like MADGEN, but is trained end-to-end during its final training step, and is heavy-atom constrained.

%However, it does not encode chemical knowledge by peak formulae and the autoregressive model loses the permutation invariance which is crucial for learning on both MS and molecules, leading to inferior performance.


% IBM NMR____

% other methods: Spec2Mol, MSNovelist, MS2Mol
% also.. MIST?
% also MassGenie, mostly in silico trained data though?

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/model_overview.pdf}
    \caption{Model architecture of \ours. \textbf{A)}~The spectrum encoder first assigns chemical formulae to peaks in an experimental spectrum and then learns an embedding vector through a formula transformer. The encoder is pretrained to predict Morgan fingerprints____ from spectra. \textbf{B)}~The graph decoder generates the target adjacency matrix by discrete diffusion conditioned on the spectrum embedding and node (atom) features. The graph decoder is pretrained with pairs of structures and fingerprints from virtual chemical libraries. We scale up the decoder pretraining to exploit the virtually-infinite number of available fingerprint-structure pairs relative to the small number of available spectrum-structure pairs, mitigating the challenge of fingerprint-to-molecule generation found non-trivial by ____. \textbf{C)}~\ours integrates the spectrum encoder and graph decoder to generate the structure annotation as a denoising process applied to a graph with randomly generated edges. It is finetuned end-to-end on labeled molecule-spectrum data.} %\textit{de novo} molecule generation conditioned on spectra.}
    \label{fig:model-overview}
\end{figure*}

%also other conditional diffusion models (e.g. RFDiffusion, but I don't know if that's too far off?)


\subsection{Diffusion generative models}
Denoising diffusion ____ has been shown to be widely effective across many tasks such as image ____ and text ____ generation. More recently, diffusion has been applied to solve (bio)molecular generative tasks 
____. 

Broadly speaking, diffusion models are generative models defined by a forward process that progressively adds noise to a sample $\bm{z}$ from data distribution $q(\bm{z}^0)$ such that $q(\bm{z}^T | \bm{z}^0)$ converges to a known prior distribution $p(\bm{z}^T)$ as $T \to \infty$. We additionally require that the noising process be Markovian such that $q(\bm{z}^1, \dots, \bm{z}^T | \bm{z}^0) = \prod_{t=1}^Tq(\bm{z}^t | \bm{z}^{t-1})$. Finally, we select the forward process such that we can efficiently sample from $q(\bm{z}^t | \bm{z}^0)$.

A neural network is then trained to reverse this noising process. However, instead of predicting $p_{\theta}(\bm{z}^{t-1} | \bm{z}^t)$, as long as $p_{\theta}(\bm{z}^{t-1} | \bm{z}^t) = \int q(\bm{z}^{t-1} | \bm{z}^t, \bm{z}^0)dp_{\theta}(\bm{z}^0)$ is tractable, we can train the model to directly predict the denoised sample $p_{\theta}(\bm{z}^0 | \bm{z}^t)$. To generate new samples from the model, we sample random noise from the prior distribution $p(\bm{z}^T)$, and iteratively sample from $p_{\theta}(\bm{z}^{t-1} | \bm{z}^t)$ until reaching $\bm{z}^0$.

Many works have also studied conditional generation with diffusion. Conditional diffusion models typically fall into two categories: classifier guidance ____ and classifier-free guidance ____. Classifier guidance uses the gradients of the log likelihood of a classifier function $p_{\phi}(y | \bm{z}^t)$ to guide the diffusion towards samples with class $y$. On the other hand, classifier-free guidance trains the denoising network directly to generate samples conditioned on class $y$ and does not require any external classifier function. \ours falls under classifier-free guidance.   

While diffusion models were originally designed to operate in continuous spaces, recent works have adapted denoising diffusion for discrete data modalities ____ and graph structured data ____, both of which are relevant for molecule generation. Here, we follow the discrete diffusion settings of ____ and ____.