% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{booktabs} % 导言区
\usepackage{multirow} % 导言区
% \usepackage{adjustbox}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{array} % 确保加载 array 包
\usepackage{caption} % 支持表格标题
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\red}[1]{\textcolor{blue}{#1}}

% \title{A Framework for Low-Resource Language Pretraining Based on Shared Weights: Application to Chinese Minority Languages}
\title{Multilingual Encoder Knows more than You Realize:\\Shared Weights Pretraining for Extremely Low-Resource Languages}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{%
  Zeli Su\textsuperscript{1,2}   \
  Ziyin Zhang\textsuperscript{3}  \
  Guixian Xu\textsuperscript{1,2 $\dagger$}  \
  Jianing Liu\textsuperscript{2} \AND
  % Ziyin Zhang\textsuperscript{3}\thanks{Zeli ran all the experiments and drafted the paper. Guixian is Zeli's graduate school supervisor. Ziyin supervised this project and revised the paper.}
  XU Han\textsuperscript{1,2} \
  Ting Zhang\textsuperscript{1,2} \
  Yushuang Dong\textsuperscript{1,2}\
  \vspace{6pt}\\
  \textsuperscript{1}Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE \\
  \textsuperscript{2}Minzu University of China \
  \textsuperscript{3}Shanghai Jiao Tong University \\
  \texttt{\{rickamorty,guixian\_xu,hanxu,jianing\_liu,yushuangdong\}@muc.edu.cn}\\
  \texttt{daenerystargaryen@sjtu.edu.cn} \
  \texttt{tozhangting@126.com} \\
   \textsuperscript{$\dagger$} Corresponding author
}


% \author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
% \\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
% \\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
% }

\begin{document}
\maketitle
\begin{abstract}
While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.

\end{abstract}

%  long abstract
% Unlike resource-rich languages that achieve remarkable performance in various natural language processing (NLP) tasks, low-resource languages often perform poorly in many tasks due to data scarcity and frequently lack effective model support. In recent years, the XLM-R model, as one of the benchmarks for multilingual pretraining, has led to the development of a series of related models. However, in the application to low-resource languages, especially in tasks such as text generation and understanding, despite some progress, their performance remains limited, particularly in data-scarce scenarios. To address this issue, we propose a novel pretraining framework. This framework, based on existing encoder-only models, utilizes a shared weight mechanism to reuse the parameters of the encoder layers, mapping them to the corresponding decoder layers. This enables the decoder to directly leverage the rich semantic space, avoiding the need to train from scratch. This innovative design allows the model to learn and generalize more efficiently in data-limited settings. Based on this architecture, we pretrained a model called XLM-SWCN for Chinese minority language tasks. Experimental results show that XLM-SWCN significantly outperforms traditional baseline models and demonstrates faster convergence during training.

\input{sec1-intro}
\input{sec2-related}
\input{sec3-method}
\input{sec4-experiments}
\input{sec5-ablation}
\input{sec6-conclusion}

\section{Limitations}
Due to the availability of pretrained language models for Chinese minority languages and high-quality corpora, our study focused on only four minority languages. Our single-language finetuning experiments are further constrained to Tibetan given the lack of relevant datasets, limiting the scope of our exploration.

Thus, we hope that future work will put more focus on the development of high-quality datasets in these minority languages and beyond, enabling a more thorough exploration of underrepresented languages in the LLM era.
% As more data becomes available and the model's capabilities continue to improve, the exploration of these languages will become a key direction for future research.







% \begin{table*}[ht]
%   \centering
%   \begin{tabular}{c c c c c c}
%     \toprule
%     {\textbf{X Value}} & {\textbf{Decoder Layers}} & {\textbf{Dataset Size}} 
%     & \multicolumn{1}{c}{\textbf{Sum}} 
%     & \multicolumn{1}{c}{\textbf{Mrc}} 
%     & \multicolumn{1}{c}{\textbf{Mt}} \\[0.5ex]
%     \midrule
%     \textbf{1} & 24 & 10,000
%     & 16 
%     & 13 
%     & 15 \\[0.5ex]
%     \textbf{2} & 12 & 20,000
%     & 8 
%     & 7  
%     & 11 \\[0.5ex]
%     \textbf{3} & 16 & 50,000
%     & \textbf{25} 
%     & \textbf{16} 
%     & \textbf{24} \\[0.5ex]
%     \textbf{4} & 16 & 50,000
%     & \textbf{25} 
%     & \textbf{16} 
%     & \textbf{24} \\[0.5ex]
%     \textbf{6} & 16 & 50,000
%     & \textbf{25} 
%     & \textbf{16} 
%     & \textbf{24} \\[0.5ex]
%     \bottomrule
%   \end{tabular}
%   \caption{\label{single}
%     Performance metrics of the experiments with different X values, evaluated using the F1-score. 
%     The "Decoder Layers" column indicates the number of layers in each model’s decoder, and the "Dataset Size" column shows the number of training samples used.
%   }
% \end{table*}



\bibliography{reference}

\input{appendix}


\end{document}
