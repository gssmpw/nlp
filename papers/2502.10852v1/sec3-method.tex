\section{Method}\label{sec:method}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/share-weight.pdf}
    \caption{The weight initialization schemes for the CustomDecoderLayer. The colored arrows indicate the initialization of weights between the different components.}
    \label{fig:decoder-initialization}
\end{figure}


\subsection{Adapting Encoders to Text Generation}



\subsubsection{Framework Overview}
In this section, we introduce the Shared Weights Framework, which leverages shared weights between the encoder and decoder for efficiently adapting multilingual encoders to text generation in low-resource languages. 
% The model architecture is designed to enhance performance in low-resource languages, addressing the challenges of efficient training under data-scarce conditions, aiming to maximize the transfer of knowledge across different languages.

The overall pipeline is visually summarized in Figure~\ref{fig:overview-model}. Starting from CINO~\citep{cino}, a continual-pretrained version of XLM-R for Chinese minority languages, we copy its weight to initialize the decoder layers for knowledge transfer, and tie some of the weights between encoder and dedocer to enable efficient training. This model, which we name XLM-SWCM, is pretrained on the MC2 corpus and then applied to downstream tasks, including both single-language finetuning and cross-lingual transfer.

\subsubsection{Model Architecture}
Like the vanilla Transformer, the proposed model has two main components:

\paragraph{Encoder:} a pre-trained encoder-only model, specifically CINO, a variant of XLM-R enhanced for Chinese minority languages.

\paragraph{Decoder:} a transformer decoder stack with a specialized weight transfer mechanism. To balance the knowledge acquired during the encoder's large-scale pretraining and new knowledge required for downstream generation tasks, we introduce two types of decoder layers: NormalDeocderLayer and CustomDecoderLayer, both maintaining the same hidden dimension, intermediate size, and number of attention heads as the encoder.
    
% \begin{itemize}
\textbf{NormalDecoderLayer}: A standard transformer decoder layer with randomly initialized weights. It follows a conventional architecture with sequential self-attention, cross-attention, and feed-forward network. These layers enable the model to learn generation-specific features from scratch, complementing the knowledge transfered from the encoder.

\textbf{CustomDecoderLayer}: A modified transformer decoder layer that inherits pre-trained weights from the encoder. It features an enhanced structure with two strategically positioned feed-forward networks: FFN1 between self-attention and cross-attention, and FFN2 following cross-attention, each with its own layer normalization and residual connection, as shown in Figure~\ref{fig:decoder-initialization}. CustomDecoderLayer inherits all its weights from the pre-trained encoder to reuse learned representations. 
% With two feed-forward networks (FFN), the model can more effectively refine and utilize the pre-trained knowledge from the encoder during the decoding phase.
% Specifically, FFN1, located between self-attention and cross-attention, further adjusts the features passed from the encoder to better align with the decoder's task requirements. FFN2, positioned after cross-attention, helps fine-tune the output from cross-attention, optimizing the final generation results. This design allows the model to process information more accurately during decoding, fully leveraging the strengths of the pre-trained encoder, and ultimately improves overall performance.
 
% \end{itemize}

    
    
\subsubsection{Weight Sharing Mechanism}
    
In our framework, the pre-trained encoder consists of only self-attention and feed-forward blocks, while the decoder layers require both self-attention and cross-attention mechanisms for effective generation. Thus, special schemes are designed to initialize and reuse the weights, as shown in Figure~\ref{fig:decoder-initialization}.

For weight initialization of CustomDecoderLayers, weights of both self-attention and cross-attention in the decoder are initialized from the encoder's self-attention blocks. Similarly, weights of both two FFN blocks in a decoder layer are initialized from the FFN block in the corresponding encoder layer. This mechanism reduces the effective number of parameters to be learned, accelerating convergence and enabling effective transfer of linguistic knowledge from the pre-trained encoder while maintaining model stability.

% The self-attention mechanism in our decoder directly inherits weights from the encoder's self-attention layers, maintaining the same multi-head configuration and preserving learned attention patterns while allowing for fine-tuning. The cross-attention mechanism is initialized using the encoder's self-attention weights, enabling effective information flow between encoder and decoder components. Each CustomDecoderLayer contains two FFN components (FFN1 and FFN2), both initialized with weights from the encoder's FFN layers. Specifically, the weights and biases of the input dense layer are cloned from the corresponding middle layer, while the weights and biases of the output dense layer are cloned from the corresponding output layer

% The weight transfer is implemented through a careful initialization process, with non-strict state dictionary loading to allow for architectural flexibility. 

A key architectural decision in our framework is the insertion pattern of these layers. After every $X$ CustomDecoderLayers, we insert one NormalDecoderLayer, so that an encoder with $n$ layers would correspond to a decoder with $n + \lfloor n / X\rfloor$ layers. The value of $X$ significantly impacts the modelâ€™s generalization capabilities, and its optimal value varies across different model scales. Through extensive experimentation, we find that $X=3$ yields the best performance, and a detailed analysis of how this choice affects the model's performance is discussed in Section~\ref{sec:5.3.2}.


\subsection{Pretraining}
\subsubsection{Pretraining Tasks}
We adopte a multi-task training approach for pretraining. The primary task involves self-supervised learning using mBART's \textbf{denoising auto-encoding (DAE)} strategy. This strategy helps with the model's transition from the encoder's word-level cloze tasks to sequence generation tasks by predicting the masked portions of the input sequence with a decoder.

Additionally, we incorporate \textbf{machine translation} as an auxiliary objective, particularly focusing on translation between Mandarin Chinese and various Chinese minority languages. Specifically, the training data includes bidirectional translation pairs between Mandarin Chinese and the minority languages. This auxiliary objective improves the model's cross-lingual transfer capability, thereby enhancing the model's performance in various low-resource language processing tasks.

\subsubsection{Training Data}
\textbf{THUCNews}~\cite{thucnews} is a Chinese news dataset, derived from historical data from the Sina News RSS feed between 2005 and 2011 and containing approximately 740,000 news articles. From this dataset, we extracted a subset of Simplified Chinese news articles.

\textbf{MC2}~\cite{mc2} provides multilingual data for several Chinese minority languages, including Tibetan, Uyghur, Kazakh, and Mongolian. The specific data volumes are described in detail in Appendix \ref{sec:Dataset Detail}. Together with THUCNews, these monolingual datasets serve as training data for the DAE task.

For machine translation, we leveraged Google Translate to create bidirectional translation pairs between Chinese and the minority languages (Tibetan, Uyghur, Kazakh, and Mongolian). These translations were verified by native speakers to ensure accuracy. A total of 2,000 sentence pairs from each language pair were selected to form the supplementary training data.

Combining these three corpora, the integrated dataset allows the model to effectively handle both high-resource and low-resource languages, improving its cross-lingual transfer and multilingual capabilities.