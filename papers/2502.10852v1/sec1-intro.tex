\section{Introduction}\label{sec:intro}

In recent years, with the development of multilingual pretrained models such as XLM-R~\citep{xlm-r}, mBART~\citep{mbart}, and mT5~\citep{mt5}, language models have achieved significant progress in multilingual tasks, especially for high-resource languages. However, low-resource languages like Tibetan, Uyghur, Kazakh, and Mongolian—spoken by millions of people in China—remain critically underserved. Among these languages, Tibetan has over 10 million speakers, Uyghur over 11 million, Kazakh approximately 3 million, and Mongolian around 7 million, yet their representation in existing multilingual corpora is vastly inadequate. As illustrated in Figure~\ref{fig:1_plot}, there is a significant disparity between the population sizes of these languages and the amount of available data in popular multilingual corpora such as OSCAR~\citep{OSCAR}. The situation is especially dire for Kazakh and Mongolian, with virtually zero usable data, hindering their inclusion in mainstream multilingual models.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/Figure_1-3.pdf}
    \caption{The relationship between population size and dataset size in OSCAR (y-axis, in MB) for various high-, middle-, and low-resource languages.}
    \label{fig:1_plot}
\end{figure}


Despite claims of multilingual support for hundreds of languages, models like mBART and mT5 are not trained on Chinese minority languages. In comparison, more advanced multiglingual large language models such as LLaMA~\citep{llama} and Qwen~\citep{qwen2} support even fewer languages. 
% One model does support these languages - CINO \cite{cino}, an encoder-only model continual-pretrained from XLM-R. However, limited by its architecture, it remains insufficient for text generation.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figure/figure_1_new.pdf} % 使用 \textwidth 以适应双栏布局
    \caption{An overview of the shared weight framework for efficiently adapting multilingual encoders to text generation in low-resource languages.}
    \label{fig:overview-model}
\end{figure*}

This gap underscores the need for targeted solutions to address the challenges of text generation in extremely low-resource languages. To tackle this challenge, we propose a novel framework for efficiently extending a multilingual encoder into an encoder-decoder architecture. To address the scarce training data in low-resource languages, we introduce a weight-sharing mechanism between the encoder and the decoder by interleaving weights transferred from the encoder with randomly initialized ones, allowing for efficient adaptation to text generation in low-resource settings.

% Instead of randomly initializing a decoder - which may results in significant undertraining for extremely low-resource languages like Arabic Kazakh and Traditional Mongolian -  we introduce a shared weight mechanism between the encoder and decoder. By carefully interleaving weights transferred from the encoder with randomly initialized layers, our approach ensures effective parameter reuse while allowing for gradual development of task-specific feature representations. This design enables the decoder to efficiently learn from very limited data instead of constructing the semantic space from scratch, significantly improving performance in low-resource settings.

Extensive experiments on the aforementioned four Chinese minority languages demonstrate the convincing advantages of our proposed method, with both faster convergence, better generalization, and strong cross-lingual transfer capabilities. Our model, \textbf{XLM-SWCM} (XLM-Shared Weight for Chinese Minorities), outperforms an mBART baseline by up to 199\% on text summarization, 108\% on reading comprehension, and also bests the much larger MC2-LLaMA 13B~\citep{mc2} in cross-lingual transfer settings.

In summary, the main contributions of this paper are:

1) a weight-sharing framework for efficiently adapting multilingual encoders to text generation in low-resource languages;
% a progressive weight copying mechanism that allows for the gradual development of unique decoder feature representation capabilities, while maintaining knowledge sharing from a pretrained encoder through the interleaving of standard decoder layers and shared layers, enhancing model generalization, convergence rate, and training stability, especially in extremely low-resource settings;

2) a model XLM-SWCM trained with this method for multiple Chinese minority languages;

3) extensive experiments showcasing the superior performance of XLM-SWCM compared with similar-sized baselines and much larger LLMs, confirming the feasibility of our framework.

Our code and models will be released upon publication.


% This gap underscores the need for targeted solutions to address the challenges of data scarcity and performance limitations. To tackle this challenge, we propose a novel approach that builds upon CINO — a secondary pretraining model based on XLM-R—and extends it with a tailored framework specifically designed for Chinese minority languages. While CINO leverages specific corpora to improve performance on these languages, it remains insufficient for tasks requiring robust text generation and understanding capabilities due to architectural constraints.

% One key limitation of CINO is its encoder-only architecture, which is ill-suited for text generation tasks that require a decoder to generate sentences. Incorporating CINO as an encoder within a seq2seq architecture partially addresses this issue, but the decoder must learn relevant semantic representations from scratch, leading to increased training complexity and suboptimal performance. These challenges are exacerbated by the scarcity of corpora for Chinese minority languages. Differences in writing systems—such as Kazakh using Cyrillic, Uyghur using Arabic script, and Mongolian having both Cyrillic and traditional scripts—further complicate the adaptation of existing multilingual datasets. This renders current large-scale models inadequate for these languages, limiting their scalability and performance.

% To address these challenges, we propose a novel framework that incorporates a shared weight mechanism between the encoder and decoder. By transferring the self-attention layers and feed-forward network parameters from the CINO encoder to the decoder, our approach ensures effective parameter reuse while introducing transformer layers that gradually develop task-specific feature representations. This design enables the decoder to learn efficiently from limited data, significantly improving performance in low-resource settings. Experimental results highlight the advantages of this framework, including faster convergence, better generalization, and strong zero-shot learning capabilities, making it a practical solution for natural language processing in Chinese minority languages.

% With this architecture, the decoder no longer needs to learn the semantic space from scratch. Instead, it can directly fine-tune on very limited downstream task data, effectively learning the most relevant features. Experimental results convincingly demonstrate the significant advantages of our proposed method under low-resource conditions: firstly, the weight-sharing strategy enables rapid convergence on smaller datasets; secondly, the model exhibits robust generalization capabilities in challenging tasks such as zero-shot learning. This provides a practical and efficient solution for natural language processing of low-resource languages.

% The main contributions of this paper can be summarized as follows:

% 1.Designs a progressive weight independence mechanism that allows for the gradual development of unique feature representation capabilities, while maintaining knowledge sharing through the insertion of standard transformer layers between the shared layers. This design enhances model generalization and ensures stability.

% 2.Proposes an innovative sequence-to-sequence pre-training framework that constructs the decoder through weight copying and sharing from the pre-trained encoder. This approach significantly improves the model’s ability to learn effectively from limited data, enabling faster convergence while maintaining performance, providing a novel solution for rapid adaptation to low-resource language models.

% 3.Introduces a model trained using a new pre-training framework that supports multiple Chinese minority languages. Experimental results show that this model outperforms baseline models, achieving better results on downstream tasks. Additionally, the model demonstrates some few-shot learning capabilities, making it a valuable approach for low-resource languages. This work establishes a solid baseline for future research in Chinese minority language processing, facilitating the development of more reliable foundation models for a wide range of downstream applications.