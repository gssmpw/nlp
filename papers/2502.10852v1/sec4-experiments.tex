\section{Experiments}\label{sec4:experiments}
\subsection{Pretraining}
% For the pre-training phase, we conducted experiments using three variants of our model (small, base, and large) with carefully tuned hyperparameters.

\paragraph{Training Configuration}
The models are trained for 8 epochs with a peak learning rate of 1e-4, AdamW~\citep{2017AdamW} optimizer, global batch size 600, and a linear learning rate scheduler with a warmup proportion of 0.1. The maximum sequence length is set to 256 tokens, and mixed-precision is enabled to optimize memory usage and training efficiency. To ensure training stability, the norms of gradients are clipped to 1.0. The models are trained on two NVIDIA A800 GPUs, each with 80GB of memory, and the training process takes 92 hours.

\paragraph{Balanced Sampling Strategy}
To address the inherent data imbalance across different languages, we implemente a balanced sampling strategy similar to XLM-R. The sampling probability for each language is calculated as

\begin{equation}
    p_i = \frac{q_i^\alpha}{\sum_j q_j^\alpha},
\end{equation}

\noindent where $q_i$ represents the original proportion of language $i$ in the dataset, and $\alpha$ (set to 0.3) is a smoothing parameter that balances between uniform sampling and size-proportional sampling. This approach ensures that low-resource languages receive adequate representation in the training process while maintaining the influence of larger datasets.
    
    
\paragraph{Model Adaptations}
We extende the model's vocabulary with special language tokens (<bo>, <kk>, <mn>, <ug>, <zh>) to handle our target languages (Tibetan, Kazakh, Mongolian, Uyghur, and Chinese). These language identifiers are directly added after the bos token <s> in the model inputs. This modification ensures that the model can effectively process and distinguish between different languages during both pre-training and downstream task finetuning. The same approach is consistently applied in all subsequent experiments.

% \paragraph{Loss Computation}
% To handle memory constraints when processing large sequences, we implemented a chunked cross-entropy loss calculation method. This approach divides the computation into manageable chunks of 1,000,000 tokens, enabling efficient training even with limited GPU resources while maintaining numerical stability.

Based on the aforementioned settings, we trained a new seq2seq model - XLM-SWCM, utilizing CINO-base-v2 as the encoder, with 457 million parameters. The detailed architectural configuration is provided in Appendix \ref{sec:Training Detail}.




\subsection{Downstream Tasks}

\subsubsection{Experiment Setting}\label{sec:experiments-downstream-setting}

To evaluate the capabilities of XLM-SWCM, we conduct fine-tuning experiments on three downstream tasks in both low-resource and high-resource languages: Text Summarization, Machine Reading Comprehension (MRC), and Machine Translation. These tasks are chosen to cover diverse areas of text generation in NLP.

\paragraph{Single-Language Fine-tuning} Due to the scarcity of labeled data for low-resource languages, we focus primarily on Tibetan for single-language fine-tuning, which has several publicly available datasets:

- \textbf{Text Summarization}: For this task, we utilize the Ti-Sum dataset~\cite{ti-sum} with 20,000 pairs of titles and articles.

- \textbf{MRC}: We mainly use the TibetanQA dataset~\cite{tibetanqa} for this task, which claims to contain 20K examples. However, only 2K examples are publicly available. Thus we enrich it by integrating 5K examples from the TibetanSFT Corpus\footnote{\href{https://huggingface.co/datasets/shajiu/ParallelCorpusSFT}{https://huggingface.co/datasets/shajiu/ParallelCorpusSFT}} and 3K examples translated from a Chinese MRC dataset \cite{chinese-mrc} using Google Translate. This approach enables us to create a comprehensive dataset consisting of 10K examples.

- \textbf{Machine Translation}: For Machine Translation, we also use the TibetanSFT Corpus, which is cleaned to generate 50,000 parallel Chinese-Tibetan sentence pairs.

\paragraph{Cross-lingual Transfer} In addition to single-language fine-tuning, we also conduct cross-lingual transfer experiments to test XLM-SWCMâ€™s ability to generalize across multiple low-resource languages. This experiment aims to assess the model's performance in Tibetan, Uyghur, Mongolian, and Kazakh after being fine-tuned on a high-resource language (Simplified Chinese) and a very small number of samples in the target languages.

- \textbf{Text Summarization}: For Mandarin Chinese, we use the publicly available LCSTS dataset~\citep{lcsts}, which contains 100K samples scraped from various Chinese portals. For the four minority languages, approximately 3K cleaned samples per language are scraped from language-specific news portals, using the news titles as their summarization.
    
- \textbf{MRC}: For Chinese, we employ the CMRC 2018 dataset~\citep{cmrc2018}, which consists of 10K samples. For Tibetan, we use 500 samples extracted from the publicly available TibetanQA dataset. For the other three minority languages (Uyghur, Mongolian, Kazakh), we utilize machine translation tools to translate and clean MRC data, ultimately selecting 500 samples per language.

\paragraph{Baseline Models}
We employ two baseline models to ensure broad coverage and robust performance in handling Chinese minority languages. The first model builds upon LLaMA2-Chinese and is fine-tuned on the MC2 dataset, resulting in the \textit{MC2-LLaMA-13B} model. The second baseline, referred to as \textit{mBART-CM}, is an adaptation of mBART-cc25. Its vocabulary is expanded to include tokens specific to our minority languages, followed by further pretraining on MC2.

\paragraph{Training settings}
Both XLM-SWCM and mBART-CM are sequence-to-sequence models that are fine-tuned using standard training configurations. Each of these models is trained for 50 epochs with a batch size of 200 samples to ensure comprehensive learning and optimal performance. MC2-LLaMA-13B model is trained using LoRA~\citep{lora} with a rank of 8 for 3 epochs.


\begin{table*}[ht]
  \centering
  \begin{tabular}{c c ccc ccc ccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Size}}
    & \multicolumn{3}{c}{\textbf{Sum}}
    & \multicolumn{3}{c}{\textbf{MRC}}
    & \multicolumn{3}{c}{\textbf{MT}} \\[0.5ex]
    \cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}
    & 
    & \textbf{F} & \textbf{P} & \textbf{R}
    & \textbf{F} & \textbf{P} & \textbf{R}
    & \textbf{F} & \textbf{P} & \textbf{R}\\[0.5ex]
    \midrule
    MC2-LLaMA-13B & 13B
    & 16.1 & 12.3 & 15.5
    & 13.2 & 11.7 & 13.1
    & 15.1 & 12.2 & 16.8 \\[0.5ex]
    mBART-CM & 611M
    & 8.6 & 11.2 & 15.2
    & 7.9  & 6.1 & 5.6
    & 11.5 & 7.3 & 9.3 \\[0.5ex]
    XLM-SWCM (ours) & 457M
    & \textbf{25.7} & \textbf{29.1} & \textbf{24.2}
    & \textbf{16.4} & \textbf{29.5} & \textbf{16.2}
    & \textbf{24.5} & \textbf{26.3} & \textbf{24.3} \\[0.5ex]
    \bottomrule
  \end{tabular}
  \caption{\label{single}
    Performance metrics of the baseline models, evaluated using three ROUGE-L sub metrics: 
    \textbf{F} (F1-score), \textbf{P} (precision), 
    and \textbf{R} (recall). Size refers to the number of parameters in each model.
  }
\end{table*}


\begin{table*}[ht]
    \centering
    \begin{tabular}{l cc cc cc cc cc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} 
        & \multicolumn{2}{c}{\textbf{Zh}} 
        & \multicolumn{2}{c}{\textbf{Bo}}
        & \multicolumn{2}{c}{\textbf{Ug}} 
        & \multicolumn{2}{c}{\textbf{Mn}} 
        & \multicolumn{2}{c}{\textbf{Kk}} \\[0.5ex]
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
        & \textbf{Sum} & \textbf{MRC}
        & \textbf{Sum} & \textbf{MRC}
        & \textbf{Sum} & \textbf{MRC}
        & \textbf{Sum} & \textbf{MRC}
        & \textbf{Sum} & \textbf{MRC} \\[0.5ex]
        \midrule
        MC2-LLaMA-13B    & 47.1 & 43.5 & 9.5 & 6.1 & 3.5 & 2.4 & 3.7 & 2.2 & 2.6 & 3.9 \\[0.5ex]
        MC2-LLaMA-13B*   & \textbf{47.3} & \textbf{44.7} & 13.1 & \textbf{11.5} & 11.7 & 10.1 & 9.7 & \textbf{10.2} & 2.9 & 4.6 \\[0.5ex]
        mBART-CM     & 32.7 & 25.6 & 6.8 & 2.1 & 2.7 & 2.2 & 3.1 & 1.7 & 0.2 & 0.1 \\[0.5ex]
        XLM-SWCM (ours)     & 33.1 & 23.5 & \textbf{17.1} & 11.1 & \textbf{12.5} & \textbf{11.1} & \textbf{13.5} & 7.2 & \textbf{5.6} & \textbf{6.9} \\[0.5ex]
        \bottomrule
    \end{tabular}
    \caption{\label{fewshot}
     Cross-lingual Transfer performance of different models on Text Summarization (Sum) and Machine Reading Comprehension (MRC) tasks, evaluated using ROUGE-L. The best results for each task are highlighted. *~indicates explicitly prompting MC2-LLaMA-13B with the language to be used in the response during evaluation.
    }
\end{table*}



\subsubsection{Experimental Results}

As illustrated in Table~\ref{single}, XLM-SWCM consistently outperforms the baseline models across all three tasks. Despite having fewer parameters, XLM-SWCM demonstrates a substantial margin of superiority over mBART-CM and even surpasses the much larger MC2-LLaMA-13B.

Notably, XLM-SWCM achieves an impressive \textbf{198.8\% improvement in F1-score for Text Summarization} over mBART-CM, along with a significant \textbf{107.6\% F1 improvement in MRC}. These remarkable gains are a direct result of XLM-SWCM's efficient weight sharing framework to maximize the utilization of pre-trained encoder features in resource-constrained scenarios. Even under equivalent seq2seq structures and identical training corpora, XLM-SWCM demonstrates greater efficiency and learning capacity.

In comparison to MC2-LLaMA-13B, which benefits from richer pretraining corpora and larger-scale parameters, XLM-SWCM achieves a \textbf{59\% higher F1-score in Text Summarization}, a \textbf{24.1\% F1 improvement in MRC}, and a \textbf{62.3\% higher F1-score in MT}. These results underscore the effectiveness of XLM-SWCM's shared weight framework in resource-constrained environments, making it a superior choice for tasks involving Chinese minority languages.

Table~\ref{fewshot} highlights the performance of XLM-SWCM and baseline models in cross-lingual transfer settings. For the primary source language (Zh), the baseline models demonstrate better performance, which stems from their larger parameter sizes and more extensive pretraining corpora in Simplified Chinese. However, when it comes to \textbf{generalization to minority languages}, XLM-SWCM showcases exceptional adaptability, significantly outperforming the baseline models. mBART-CM, for instance, struggles to distinguish between languages and often defaults to outputs in the primary language (Zh), even when language-specific labels are present. Similarly, MC2-LLaMA-13B exhibits language-related errors, though its performance improves when explicitly informed of the current language type, as seen with MC2-LLaMA-13B*.

In Text Summarization, XLM-SWCM outperforms all baselines. Specifically, XLM-SWCM achieves significant improvements of \textbf{30.5\%, 6.8\%, and 39.1\%} for Tibetan (Bo), Uyghur (Ug), and Mongolian (Mn) respectively over MC2-LLaMA-13B*, the best-performing baseline. For MRC, XLM-SWCM also demonstrates competitive performance across most languages, being only slightly weaker than MC2-LLaMA-13B* for Tibetan and Mongolian. 

Overall, these experiments indicate that XLM-SWCM can effectively leverage the shared weight mechanism to maximally reuse the semantic space of the pre-trained encoder, demonstrating excellent performance in Chinese minority language applications with limited data and parameter size.

\begin{table}[ht]
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{c c c c c}
    \toprule
    \textbf{Removing Module} & \textbf{Sum} & \textbf{MRC} & \textbf{MT} \\[0.5ex]
    \midrule
    None (XLM-SWCM) & \textbf{25.7} & \textbf{16.4} & \textbf{24.5} \\[0.5ex]
    MT & 25.6 & 15.1 & 20.3 \\[0.5ex]
    DAE & 22.4  & 12.2 & 18.7 \\[0.5ex]
    WS & 17.1  & 11.7 & 18.2 \\[0.5ex]
    MT + DAE & 22.5 & 12.3 & 17.7 \\[0.5ex]
    MT + WS  & 17.5 & 11.3 & 18.4 \\[0.5ex]
    DAE + WS & 15.2  & 11.9  & 17.1 \\[0.5ex]
    MT + DAE + WS & 15.9 & 10.8 & 16.5 \\[0.5ex]
    \bottomrule
  \end{tabular}
  }
  \caption{\label{ablation-single}
    Objective ablation results, evaluated using ROUGE-L.
    The experiments involve removing different combinations of training components, such as Machine Translation (MT), DAE (Denoising Auto-Encoding), and Weight Sharing (WS).
  }
\end{table}