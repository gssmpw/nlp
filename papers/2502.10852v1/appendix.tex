\clearpage
\appendix

\section{Dataset Details}\label{sec:Dataset Detail}

For pretraining of XLM-SWCM and other baseline models, we used a combination of Simplified Chinese data from THUCNews and minority languages from MC2. The breakdown of their distribution is given in Table~\ref{dataset-table}.

\begin{table}[ht]
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{>{\centering\arraybackslash}m{2.3cm} >{\centering\arraybackslash}m{2.3cm} >{\centering\arraybackslash}m{2.3cm}}
    \toprule
    \textbf{Language} & \textbf{Data Size} & \textbf{Number of Samples} \\[0.5ex]
    \midrule
    Tibetan & 2.2 GB & 184,045 \\[0.5ex]
    Uyghur & 736 MB & 90,441 \\[0.5ex]
    Kazakh & 937 MB & 57,827 \\[0.5ex]
    Mongolian & 970 MB & 171,847 \\[0.5ex]
    Simplified Chinese & 2.1 GB & 836,075 \\[0.5ex]
    \bottomrule
  \end{tabular}
  }
 \caption{\label{dataset-table}
   Statistics of our pretraining dataset.
}
\end{table}



\section{Training Details}\label{sec:Training Detail}
In addition to the settings presented in the main paper, here we detail other parameters used during pre-training XLM-SWCM for complete reproduction:

\subsection*{Hardware and Software Configuration}
\phantom{.}\vspace{-14pt}

- \textbf{Hardware:} NVIDIA Tesla A800 GPU, 80 GB RAM * 2, Intel i7 CPU.

- \textbf{Software:} Ubuntu 20.04, CUDA 11.7, PyTorch 2.3

\subsection*{Training Configurations}
\phantom{.}\vspace{-14pt}

- \textbf{Total Training Samples:} 1,340,235

- \textbf{Local Batch Size:} 75

- \textbf{Gradient Accumulation Steps:} 4

- \textbf{Global Batch Size:} 600

- \textbf{Epochs:} 8

- \textbf{Total Training Steps:} 17,864

- \textbf{Optimizer:} AdamW with $\beta_1 = 0.9$, $\beta_2 = 0.999$

- \textbf{Learning Rate:} 1e-4

- \textbf{Warm-up:} Linear warm-up for the first  epoch, gradually increasing the learning rate from 1e-5 to 1e-4.

- \textbf{Scheduled Sampling:} In the first epoch, teacher forcing is applied to guide the model. Subsequently, the teacher forcing ratio is gradually decreased in a linear fashion, transitioning to scheduled sampling~\citep{2015scheduled-sampling}.


