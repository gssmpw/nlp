\section{Related Works}
\label{sec:related-work}
\subsection{Multilingual Corpus}
The evolution of multilingual large language models (LLMs) has been enabled by the release of extensive multilingual corpora such as CC100, mC4, OSCAR, CulturaX, and Madlad-400 ____. While these resources cover a selection of low-resource languages to some extend, there remains a recognized gap in the representation for China's minority languages, primarily due to significant differences in writing systems.
   
China’s minority languages often use different writing systems from the same language family used elsewhere in the world. For example, Uyghur is primarily written in the Arabic script (UEY—Uyghurche Ereb Yëziqi) in China, with the Latin script (ULY—Uyghurche Latin Yëziqi) used as a supplementary form. In contrast, Uyghur in Russia and Central Asia is written in the Cyrillic script (USY—Uyghurche Shilir Yëziqi). When collecting data for minority languages, the aforementioned multilingual corpora either do not distinguish between such different writing systems, or only contain data from one system, as shown in Figure~\ref{fig:1_plot}.
   
Recently, the release of the Multilingual Corpus of Minority Languages in China (MC2,____) breaks the gap in the availability of Chinese minority language pretraining corpora, covering four underrepresented languages: Tibetan, Uyghur, Kazakh, and Mongolian. This dataset is used as the primary pretraining corpus in our work.

    
\subsection{Development of Multilingual Language Models}
In the past few years, multilingual variants of pretrained language models have been proposed in the NLP community, such as mBART____ and mT5____, supporting up to 100 languages and demonstrating powerful cross-lingual transfer capabilities. More recently, the emergence of large language models (LLMs) has revolutionized multilingual natural language processing. Models like PaLM____ and BLOOM____ have made significant strides in multilingual capabilities, while the LLaMA family____ and its multilingual variants have democratized access to multilingual LLMs. Some specialized models represented by XGLM and NLLB____ have focused on expanding language coverage and improving cross-lingual transfer capabilities across hundreds of low-resource languages. However, few of these models support Chinese minority languages.

% pre-trained language models have achieved significant breakthroughs in natural language processing, represented by BERT, RoBERTa, and T5 ____. 

\subsection{NLP for Minority Languages in China}
To enhance the accessibility of minority languages in China, prior studies have primarily focused on curating annotated datasets for various NLP tasks. These efforts have mainly concentrated on three key task categories: text classification ____, question answering ____, and machine translation ____. Prominent models specifically trained for these languages include CINO____, MiLMo____, and TiBert____. However, despite such progress, none of these models have released their pre-training corpora, and there is still a notable gap in the availability of models capable of text generation in these languages.


    
% Despite these advancements, significant challenges persist in effectively adapting models to text generation tasks for Chinese minority languages, particularly in low-resource settings. Currently, there is no unified model that can effectively handle text generation while leveraging existing resources for these languages. To address this gap, we propose \textbf{XLM-SWCM} (XLM-Shared Weight Chinese Minority), a novel seq2seq architecture that utilizes weight-sharing transfer learning from existing encoder models, offering a more efficient approach to handling text generation tasks across Chinese minority languages.