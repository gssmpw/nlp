% \begin{table*}[ht]
%     \centering
%     \begin{tabular}{l cc cc cc cc cc}
%         \toprule
%         \multirow{2}{*}{\centering \textbf{Removing Module}} 
%         & \multicolumn{2}{c}{\textbf{Zh}} 
%         & \multicolumn{2}{c}{\textbf{Bo}}
%         & \multicolumn{2}{c}{\textbf{Ug}} 
%         & \multicolumn{2}{c}{\textbf{Mn}} 
%         & \multicolumn{2}{c}{\textbf{Kk}} \\[0.5ex]
%         \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
%         & \textbf{Sum} & \textbf{MRC}
%         & \textbf{Sum} & \textbf{MRC}
%         & \textbf{Sum} & \textbf{MRC}
%         & \textbf{Sum} & \textbf{MRC}
%         & \textbf{Sum} & \textbf{MRC} \\[0.5ex]
%         \midrule
%          -MT  & 31.9 & 22.3 & 16.5 & 11.2 & 12.4 & 10.8 & 13.5 & 7.2 & 5.6 & 6.9 \\[0.5ex]
%          -MLM & 29.1 & 21.2 & 17.1 & 11.1 & 12.5 & 11.1 & 13.5 & 7.2 & 5.6 & 6.9 \\[0.5ex]
%          -WS  & 32.1 & 23.2 & 17.1 & 11.1 & 12.5 & 11.1 & 13.5 & 7.2 & 5.6 & 6.9 \\[0.5ex]
%         -MT + MLM  & 32.1 & 23.2 & 17.1 & 11.1 & 12.5 & 11.1 & 13.5 & 7.2 & 5.6 & 6.9 \\[0.5ex]
%         -MT + WS & 32.1 & 23.2 & 17.1 & 11.1 & 12.5 & 11.1 & 13.5 & 7.2 & 5.6 & 6.9 \\[0.5ex]
%         -MLM + WS & 32.1 & 23.2 & 17.1 & 11.1 & 12.5 & 11.1 & 13.5 & 7.2 & 5.6 & 6.9 \\[0.5ex]
%         -MT + MLM + WS  & 32.1 & 23.2 & 17.1 & 11.1 & 12.5 & 11.1 & 13.5 & 7.2 & 5.6 & 6.9 \\[0.5ex]
%         XLM-SWCM (ours) & 33.1 & 23.5 & \textbf{17.1} & 11.1 & \textbf{12.5} & \textbf{11.1} & \textbf{13.5} & 7.2 & \textbf{5.6} & \textbf{6.9} \\[0.5ex]
%         \bottomrule
%     \end{tabular}
%     \caption{\label{fewshot}
%      Few-shot performance of different models on Text Summarization (Sum) and Machine Reading Comprehension (MRC) tasks, evaluated using ROUGE-L as the metric. The best results for each task are highlighted. The asterisk (*) indicates that MC2-LLaMA-13B was explicitly prompted with the language to be used in the response during evaluation.
%     }
% \end{table*}


\section{Ablation Studies}\label{sec:ablation}
In this section, we present a series of ablation experiments aimed at evaluating the impact of key components in our framework that play essential roles in enhancing the model’s multilingual capabilities and improving its generalization to low-resource languages. We perform ablation experiments on the Tibetan finetuning tasks, maintaining a consistent finetuning setting with Section~\ref{sec:experiments-downstream-setting}.

\subsection{Objective  Ablation}
We first focus on three critical aspects of the model: DAE pretraining, machine translation, and weight initialization by removing each and combinations of them. The results are shown in Table~\ref{ablation-single}. Removing any of the three components is detreimental to performance, specifically:

- Machine Translation (MT): Removing machine translation has a relatively small impact on performance across tasks, as shown by both individual removal (maintaining 25.6 in Sum) and combined removals (MT+DAE vs DAE showing similar scores);
    
- Denoising Auto-Encoding (DAE): The removal of DAE pretraining causes considerable performance drops across all three downstream tasks, and its impact becomes more pronounced in combined removals (DAE+WS), indicating its fundamental importance in establishing the model's basic text generation capabilities.

- Weight Sharing (WS): The removal of weight sharing demonstrates the most significant impact among all modules, showing the largest performance drops in individual removal and maintaining this substantial negative effect across all combined removal scenarios, establishing it as the most crucial component for the model's effectiveness in low-resource settings.

In short, while all three components contribute positively to the model's performance, weight sharing emerges as the most critical component. This finding highlights the importance of weight sharing as a key architectural choice for multilingual models, especially in resource-constrained scenarios.

% These experiments demonstrate that shared weights are particularly beneficial for low-resource language tasks, enabling better generalization from limited data. 


\subsection{Structure Ablation}
We also perform experiments to evaluate the impact of different structural components in our proposed framework. These experiments aim to understand how the initialization of decoder weights and the insertion of normal layers affect model performance.

\subsubsection{Impact of Weight Initialization}
Firstly, we train a baseline model called \textbf{Cino-Transformer}. Unlike XLM-SWCM, the decoder of this model is randomly initialized, and also matches the number of encoder layers. The model is pretrained using the same DAE and MT tasks as XLM-SWCM but without weight sharing, and then finetuned on downstream tasks in the same setting as XLM-SWCM.

\begin{table}[ht]
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{c c c c c}
    \toprule
    \textbf{Model} & \textbf{Sum} & \textbf{MRC} & \textbf{MT} \\[0.5ex]
    \midrule
    Cino-Transformer & 18.9 & 13.5 & 18.7 \\[0.5ex]
    XLM-SWCM (ours) & \textbf{25.7} & \textbf{16.4} & \textbf{24.5} \\[0.5ex]
    \bottomrule
  \end{tabular}
  }
 \caption{\label{ablation-structure}
   Performance metrics of the Ablation of Weight Initialization, evaluated using the ROUGE-L metric. 
}
\end{table}

\begin{table}[ht]
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{c c c c c}
    \toprule
    \textbf{Model} & \textbf{Sum} & \textbf{MRC} & \textbf{MT} \\[0.5ex]
    \midrule
    BASE-A & 13.7 & 10.3 & 15.7 \\[0.5ex]
    BASE-B & 16.3 & 14.1 & 21.1 \\[0.5ex]
    XLM-SWCM (ours) & \textbf{25.7} & \textbf{16.4} & \textbf{24.5} \\[0.5ex]
    \bottomrule
  \end{tabular}
  }
\caption{\label{ablation-Normal-Layers}
   Performance metrics of the Ablation of Normal Layers, evaluated using the ROUGE-L metric. 
   \textbf{BASE-A} has fewer layers and does not include any normal layers, while \textbf{BASE-B} maintains the same number of layers as XLM-SWCM but uses weight duplication instead of normal layers. 
}

\end{table}

The results in Table \ref{ablation-structure} demonstrate the effectiveness of our weight initialization scheme. By transferring weights from the encoder to the decoder, XLM-SWCM can be efficiently adapted to text generation with limited training data, outperforming Cino-Transformer on all tasks.


\subsubsection{Impact of Randomly Initialized Layers}

Secondly, we explore the impact of inserting normal layers among the custom layers in the decoder. To assess the effectiveness of this modification, we use two baseline models for comparison:

- \textbf{Baseline A (XLM-SWCM without normal layers)}: This model is identical to XLM-SWCM but without any normal layers inserted into the custom layer architecture. The absence of normal layers leads to a reduced total number of layers in the decoder.

- \textbf{Baseline B (Weight duplication model)}: Instead of inserting normal layers, this model simply copies the weights of the preceding layer to maintain consistency in the number of model parameters. This results in identical weights across consecutive layers, allowing us to isolate the impact of inserting randomly initialized normal layers.

The results in Table~\ref{ablation-Normal-Layers} demonstrate the significant impact of inserting normal layers into the decoder. BASE-A, which has fewer layers, performs the worst across all tasks. BASE-B, which maintain the same number of layers as XLM-SWCM but lacks randomly initialized weights, shows some improvement but still underperforms.

Overall, these findings indicate that randomly initialized normal layers is also crucial for adapting encoders to text generation.

\subsubsection{Impact of Insertion Frequency of Normal Layers}
\label{sec:5.3.2}
Thirdly, we thoroughly investigate the impact of insertion frequency of normal layers in the decoder, and how this interacts with varying dataset sizes. This experiment is designed along two dimensions:

- \textbf{Insertion Frequency of Normal Layers}: we explore values of \( X \) where a normal layer is inserted after every \( X \) custom layers, with \( X \) ranging from 1 to 6. All these models are pretrained in the same setting as XLM-SWCM.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/line_plot.pdf}
    \caption{ROUGE-L scores on Tibetan summarization for different X-values (insertion frequency of normal layers). The three lines correspond to different dataset sizes.}
    \label{fig:enter-label}
\end{figure}

- \textbf{Effect of Finetuning Dataset Size}: we evaluate the model’s performance on datasets of varying sizes, including 10K, 20K, and 50K samples. As the existing Ti-SUM dataset only has 20K samples, we supplement it by crawling and cleaning 30K additional news articles from various major Chinese websites. This dimension allows us to examine the interaction between the amount of available data and the frequency of normal layers.

The results are plotted in Figure~\ref{fig:enter-label}:

- For the small dataset (10k), larger $X$ results in better performance, as smaller decoders generalize more effectively when data is limited. In contrast, smaller $X$ (i.e. larger decoders) leads to overfitting.

- For the medium dataset (20k), performance peaks at \( X = 3 \). This indicates that a moderate decoder size strikes a balance between capacity and data availability.

- For the large dataset (50k), smaller $X$ achieve the highest F1-scores, as the larger decoder capacity enables the model to fully exploit the larger dataset.

Overall, these results demonstrate the flexibility of our framework, where the insertion frequency of normal layers can be adjusted based on the task-specific dataset size. Larger $X$ (fewer layers) is better suited for small datasets, while smaller $X$ (more layers) performs best on larger datasets.
% For medium datasets, a balance can be achieved with moderate X-values. Based on the experimental results and considering the data scarcity typically found in low-resource languages, our model adopts \( X = 3 \). This configuration achieves a balanced performance across datasets of all sizes, making it a suitable choice for both small and large data scenarios.