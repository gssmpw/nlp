\section{Conclusion}\label{sec:conclusion}
In this work, we proposed a novel pretraining framework tailored for low-resource languages, with a particular focus on Chinese minority languages. Our framework leverages a shared weight mechanism between the encoder and decoder, which allows for the efficient adaptation of multilingual encoders to generation tasks without the need to start from scratch. Experimental results demonstrate that our model XLM-SWCM significantly outperforms traditional baselines on various text generation tasks for Tibetan, Uyghur, Kazakh, and Mongolian, which have long been underserved in NLP research. Our approach opens up new possibilities for developing robust models for these extremely low-resource languages, and also provides a promising method for the integration of resources across similar languages.

% We envision extending the shared weight mechanism to a broader range of languages, refining the pretraining process, and adapting the framework to other NLP tasks. Such advancements will help bridge the gap between high- and low-resource languages, ultimately fostering the development of more inclusive, universal language models.