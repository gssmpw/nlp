\section{Practical Considerations} \label{sec:considerations}
In this section, we discuss practical issues that needed to be considered when deploying Data Guard at-scale in LinkedIn. \\
1. {\bf View Schema Evolution: } \
In a large warehouse, tables often undergo changes to schema and new field additions (either top-level columns or nested fields) to tables are very common. As Data Guard data-masking views are schema-preserving,
the schema of the view at creation time may be different from the schema of the view at the consumption time due to newly added fields. This means that newly added fields may accidentally be consumed in user queries, 
resulting in non-compliant accesses. To prevent this data leakage, we dynamically rewrite the data-masking view at the view consumption time via the ViewShift $getView()$ API to set all newly added field values to $NULL$. \\ 
%The list of these non-compliant fields are computed by comparing schema of the underlying table with the compliant schema of the view, which was registered to the catalog at the view creation time.\\
2. {\bf Handling non-nullable fields: } \
The data-masking function described thus far replaces fields to be masked with $NULL$ values and thus, assumes that fields are nullable. 
In large warehouses, it is common to define schemas containing non-nullable fields. 
To ensure compliance, our approach when rolling out Data Guard at LinkedIn, has been to over-filter data and filter out the entire row from the result set when a non-nullable field needs to be masked. It is possible to extend this behavior to provide non-NULL replacement values which are defined system-wide. This would however require changes by data consumers to handle such values in their applications. 
An area of ongoing work is to enhance Data Guard's APIs to provide data consumer-specified replacement values for non-nullable fields in order to avoid over-filtering data and maximize data utility. \\
3. {\bf Enforcement Verification: } \ 
In addition to proactive enforcement of policies at the time of data access, Data Guard also monitors access logs for all data accesses to the warehouse data. 
Data Guard's monitoring sub-system relies on raw access logs from underlying storage system (i.e. HDFS in our case) along with query execution logs from data processing engines 
like Spark and Trino to detect any inadvertent access to warehouse data that bypasses Data Guard enforcement. 
Data Guard also detects accesses to stale versions of Data Guard views from the access logs. 
In each scenario, Data Guard notifies consumers about potential access violations and provides them with steps to remediate their accesses. \\
4. {\bf Result Caching: }\ 
Commercial warehouses commonly support result set caching, even though this functionality is not available currently in LinkedIn's warehouse. One of the conditions for result caching is that queries should not include non-deterministic SQL functions such as {\em CURRENT\_TIMESTAMP()}. Thus, the data-masking views described in this paper are not eligible for result set caching. However, caching results of data-masking views has limited value in our environment because the view evaluation depends on data subject consents which are frequently updated. In scenarios where such updates are infrequent, result set caching can indeed be effective. To enable reuse of cached results, alternatives such as defining data-masking views as parametrized views that use timestamp as a parameter passed to the view or defining the masking views as materialized views appear viable.  \\
5. {\bf Engine optimizations: }\ An important limitation of implementing the data-masking operator using UDFs is that it prevents engines from applying optimizations such as predicate pushdown or nested column pruning. Our primary focus in implementing and rolling out Data Guard at-scale inside LinkedIn has been on the APIs for enforcing policies and ensuring a friction-free developer experience. While we implemented a number of performance optimizations to ensure that the cost of adopting data-masking views remains acceptable, we recognize the opportunity for further optimizations in the future. 