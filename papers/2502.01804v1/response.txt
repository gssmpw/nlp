\section{Related Work}
\label{sec:related}
The simultaneous growth of training set sizes, computational budgets and parameter count 
has yield large language models (LLMs) strong at addressing a wide variety of tasks **Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"**.

The effectiveness of these generalist models comes at a high inference and training cost.
To improve inference effectiveness, significant effort has been invested in knowledge distillation **Kim et al., "Distilling the Knowledge in a Neural Network"**, model compression **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**.
Specialization and sparsification are also important, complementary strategies for efficient inference.

Specialization trades generality for inference efficiency: a small model trained on data close to the targeted domain can be strong on this domain. Since many tasks only provide little in-domain data, fine-tuning a generalist model for a few steps is a common strategy. Data selection is a complementary approach that resamples the pretraining set to emphasize the most relevant parts for the target domain, 
allowing a specialist model to be trained from scratch. Data selection based on gradient alignment **Huang et al., "Gradient Alignment for Unsupervised Domain Adaptation"** and importance sampling **Kunstner et al., "Importance Weighted Adversarial Regularization"** are the main strategies for task-aware pretraining.

Sparsification improves inference efficiency with mixture of experts (MoE). These models avoid using all parameters for all inputs. They jointly learn expert modules providing specialized weights for different data slices, and routing models deciding which weights to use **Shazeer et al., "Outrageously Large Neural Networks: The Growing Need for Models with 100,000 Parameters"**.
MoEs focus on the computational cost of inference but not on its memory cost **Frankle & Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"**: their routing decision are performed once the input is available, which means that they still require access to all the weights in memory. Model pruning **Li et al., "Pruning Neural Networks Using the YOLOv3 Algorithm"** focuses on task-dependent sparsification as a fine tuning step, which allows memory saving. However, such pruning strategies are difficult since their pretraining has no incentive to discover sparse structures.

This work proposes an alternative. Like an MoE we train a large number of parameters but still achieve efficient inference.
Unlike MoEs, we specialize the model to a given test domain and provide a small, stand-alone model. Unlike fine-tuning or task-aware pretraining, 
the specialized model does not require knowing the test domain at training time: specialization is not the result of optimization as the 
specialized parameters results from a closed form merging of the pretrained parameters.
%
An interesting future avenue of research is to combine MoEs and Soup-of-Experts, where the base architecture in the Soup-of-Experts is itself a MoE. It would increase the performance of the Soup-of-Experts without sacrificing its latency.

This work takes inspiration from litterature on the merging of fine-tuned models, aka task-arithmetic. This litterature observes that models
fine-tuned from a common ancestor can be linearly combined without retraining **Wang et al., "Task-Arithmetic: Learning to Combine Multiple Pre-trained Models"**. It has also been proposed to further fine-tune merge models **Ding et al., "EnsembleDistil: Ensemble Distillation for Deep Neural Networks"**
Our work extends this merging strategy beyond fine-tuning and incorporates it into the pretraining phase.  

**He et al., "AMC: AutoML for Model Compression and Acceleration on Resource-Constrained Devices"** also propose an architecture that dynamically mixes weights according to the input task, but there are key differences with our work. 
First, they consider tasks that share the data but differ in their losses, while we consider tasks that have different data distributions with the same loss.
Second, their framework does not allow having a different number of experts than domains, while we use a MLP to map histograms to experts.
Finally, we introduce shared parameters in addition to the experts, which allows us to amortize between tasks.
We discuss the differences with this work in more detail in \autoref{app:sec:related}.