\section{Related Work}
\label{sec:related}
The simultaneous growth of training set sizes, computational budgets and parameter count 
has yield large language models (LLMs) strong at addressing a wide variety of tasks____.

The effectiveness of these generalist models comes at a high inference and training cost.
To improve inference effectiveness, significant effort has been invested in knowledge distillation____ and model compression____.
Specialization and sparsification are also important, complementary strategies for efficient inference.

Specialization trades generality for inference efficiency: a small model trained on data close to the targeted domain can be strong on this domain. Since many tasks only provide little in-domain data, fine-tuning a generalist model for a few steps is a common strategy. Data selection is a complementary approach that resamples the pretraining set to emphasize the most relevant parts for the target domain, 
allowing a specialist model to be trained from scratch. Data selection based on gradient alignment____ and importance sampling____ are the main strategies for task-aware pretraining.

Sparsification improves inference efficiency with mixture of experts (MoE). These models avoid using all parameters for all inputs. They jointly learn expert modules providing specialized weights for different data slices, and routing models deciding which weights to use____. 
MoEs focus on the computational cost of inference but not on its memory cost____: their routing decision are performed once the input is available, which means that they still require access to all the weights in memory. Model pruning____ focuses on task-dependent sparsification as a fine tuning step, which allows memory saving. However, such pruning strategies are difficult since their pretraining has no incentive to discover sparse structures.

This work proposes an alternative. Like an MoE we train a large number of parameters but still achieve efficient inference.
Unlike MoEs, we specialize the model to a given test domain and provide a small, stand-alone model. Unlike fine-tuning or task-aware pretraining, 
the specialized model does not require knowing the test domain at training time: specialization is not the result of optimization as the 
specialized parameters results from a closed form merging of the pretrained parameters.
%
An interesting future avenue of research is to combine MoEs and Soup-of-Experts, where the base architecture in the Soup-of-Experts is itself a MoE. It would increase the performance of the Soup-of-Experts without sacrificing its latency.

This work takes inspiration from litterature on the merging of fine-tuned models, aka task-arithmetic. This litterature observes that models
fine-tuned from a common ancestor can be linearly combined without retraining____. It has also been proposed to further fine-tune merge models____
Our work extends this merging strategy beyond fine-tuning and incorporates it into the pretraining phase.  

____ also propose an architecture that dynamically mixes weights according to the input task, but there are key differences with our work. 
First, they consider tasks that share the data but differ in their losses, while we consider tasks that have different data distributions with the same loss.
Second, their framework does not allow having a different number of experts than domains, while we use a MLP to map histograms to experts.
Finally, we introduce shared parameters in addition to the experts, which allows us to amortize between tasks.
We discuss the differences with this work in more detail in \autoref{app:sec:related}.