[
  {
    "index": 0,
    "papers": [
      {
        "key": "brown2020gpt3",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral 7B"
      },
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "gu2024minillm",
        "author": "Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie",
        "title": "{MiniLLM}: Knowledge Distillation of Large Language Models"
      },
      {
        "key": "riviere2024gemma",
        "author": "Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\\'e}, Alexandre and others",
        "title": "Gemma 2: Improving open language models at a practical size"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2024quantized",
        "author": "Shiyao Li and Xuefei Ning and Luning Wang and Tengxuan Liu and Xiangsheng Shi and Shengen Yan and Guohao Dai and Huazhong Yang and Yu Wang",
        "title": "Evaluating Quantized Large Language Models"
      },
      {
        "key": "wan2024efficient",
        "author": "Zhongwei Wan and Xin Wang and Che Liu and Samiul Alam and Yu Zheng and Jiachen Liu and Zhongnan Qu and Shen Yan and Yi Zhu and Quanlu Zhang and Mosharaf Chowdhury and Mi Zhang",
        "title": "Efficient Large Language Models: A Survey"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "fan2023doge",
        "author": "Fan, Simin and Pagliardini, Matteo and Jaggi, Martin",
        "title": "Doge: Domain reweighting with generalization estimation"
      },
      {
        "key": "fan2024dynamic",
        "author": "Fan, Simin and Grangier, David and Ablin, Pierre",
        "title": "Dynamic Gradient Alignment for Online Data Mixing"
      },
      {
        "key": "grangier2024adaptive",
        "author": "Grangier, David and Ablin, Pierre and Hannun, Awni",
        "title": "Adaptive Training Distributions with Scalable Online Bilevel Optimization"
      },
      {
        "key": "wang2024greats",
        "author": "Wang, Jiachen (Tianhao) and Wu, Tong and Song, Dawn and Mittal, Prateek and Jia, Ruoxi",
        "title": "{GREATS}: Online Selection of High-Quality Data for LLM Training in Every Iteration"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "grangier2024task",
        "author": "Grangier, David and Fan, Simin and Seto, Skyler and Ablin, Pierre",
        "title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "shazeer2017moe",
        "author": "Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      },
      {
        "key": "jiang2024mixtral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others",
        "title": "Mixtral of experts"
      },
      {
        "key": "dai2024deepseekmoe",
        "author": "Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"
      },
      {
        "key": "abnar2025parameters",
        "author": "Abnar, Samira and Shah, Harshay and Busbridge, Dan and Ali, Alaaeldin Mohamed Elnouby and Susskind, Josh and Thilak, Vimal",
        "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "pan2024dense",
        "author": "Pan, Bowen and Shen, Yikang and Liu, Haokun and Mishra, Mayank and Zhang, Gaoyuan and Oliva, Aude and Raffel, Colin and Panda, Rameswar",
        "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xia2022pruning",
        "author": "Xia, Mengzhou  and\nZhong, Zexuan  and\nChen, Danqi",
        "title": "Structured Pruning Learns Compact and Accurate Models"
      },
      {
        "key": "xia2023sheared",
        "author": "Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi",
        "title": "Sheared llama: Accelerating language model pre-training via structured pruning"
      },
      {
        "key": "ma2023llmpruner",
        "author": "Ma, Xinyin and Fang, Gongfan and Wang, Xinchao",
        "title": "{LLM-Pruner}: On the Structural Pruning of Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wortsman2022soups",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      },
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      },
      {
        "key": "huang2023lorahub",
        "author": "Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min",
        "title": "Lorahub: Efficient cross-task generalization via dynamic lora composition"
      },
      {
        "key": "ortiz2023tangent",
        "author": "Ortiz-Jimenez, Guillermo and Favero, Alessandro and Frossard, Pascal",
        "title": "Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models"
      },
      {
        "key": "tam2024realistic",
        "author": "Tam, Derek and Kant, Yash and Lester, Brian and Gilitschenski, Igor and Raffel, Colin",
        "title": "Realistic evaluation of model merging for compositional generalization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "choshen2022fusing",
        "author": "Choshen, Leshem and Venezian, Elad and Slonim, Noam and Katz, Yoav",
        "title": "Fusing finetuned models for better pretraining"
      },
      {
        "key": "rame2023ratatouille",
        "author": "Rame, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, Leon and Lopez-Paz, David",
        "title": "Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "dimitriadis2023pareto",
        "author": "Dimitriadis, Nikolaos and Frossard, Pascal and Fleuret, Fran{\\c{c}}ois",
        "title": "Pareto Manifold Learning: Tackling multiple tasks via ensembles of single-task models"
      }
    ]
  }
]