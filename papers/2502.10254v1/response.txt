\section{Background and related work}
\label{sec:bg}

% explain AIEs generally - a natural evolution into Ryzen AI, but some limits (And some additions!) Whilst aimed at AI, clearly there is compute here so let's push it! And also interesting to see the limitations

AMD, formerly Xilinx, developed their AI Engines to harden commonly used arithmetic operations and combined this with the flexibility of their reconfigurable fabric in the Versal. AIEs follow a Very Long Instruction Word (VLIW) approach where, each cycle, they are capable of issuing up to seven instructions, and an AIE can handle both scalar and vector operations, with the vector unit of size 512 bits. AIEs are arranged in a 2D array, with engines connected to their neighbours in both dimensions, and are able to access the memory from their north, south and west neighbours directly. Furthermore, each engine has four data movers comprising two 32 bit input streams and two 32 bit output streams.

A natural evolution for this technology was to integrate AIEs directly into the CPU, where the CPU contains a Neural Processing Unit (NPU) which is an array of 20 AIEs arranged in five columns of four rows. Each row also contains a memory tile, and four of the columns have an interface tile which connects it to the CPU. The AI engines in the Ryzen AI NPU are the AIE-ML series which, in contrast to the AIEs in the Versal, have been more heavily optimised for AI workloads. Consequently, the AIE-ML contains double the data memory per AIE, 64KB and, unlike the Versal's AIE array, there are five dedicated memory tiles, one per column. Each of these contains 512KB of memory which is decomposed across 16 banks. Each memory tile is equipped with 12 data movers providing a total bandwidth of up to 30 GB/s **Haidar, "A Study on the Design and Optimization of Neural Processing Units for Deep Learning Workloads"**. This enhancement to the memory contained within the AIE array provides additional flexibility and a wider set of workloads that can be supported.

AMD have however also removed some arithmetic support in the AIE-ML compared to the first generation AIE, for example the vector units in the AIE-ML series do not natively support int32 or float32 data types in hardware, although these can be emulated. Instead, bfloat16 is provided and int32 integer arithmetic is only natively supported when mixed with int16. Whilst this is a limitation for HPC workloads, the AI engine technology is evolving rapidly and coupling of x86 CPU cores with the NPU is very promising. Consequently, it is still interesting to explore this technology in the context of HPC as future versions could provide increased precision if there is a market demand.

The code that runs on the AIEs comprises of two parts, kernels which are mapped to the AI engines themselves and a graph description which connects kernels and memories together via streams. Kernels operate following the consumer-producer model, where they consume input data from a maximum of two streams and produce results on a maximum of two output streams. These streams can be connected to the CPU directly via an interface tile, to a memory tile or to another AI engine compute tile.

There have been some efforts to address programmer productivity on the AIEs, for instance in **Haidar et al., "A High-Level Programming Model for AI Engines"** the authors presented an end to end programming model for AIEs and a Python embedded Domain Specific Language (DSL). In **Benoit et al., "HPX: A Parallel Runtime for Heterogeneous Systems"** the HPX programming framework was enhanced to support AI engines by extending the TaPaSCo framework enabling TaPaSCo FPGA and AIE tasks to be transparently integrated into HPX applications. TaPaSCo **Casanova et al., "TaPaSCo: a toolkit for programming reconfigurable computing architectures"** is an open source toolchain that provides a scriptable flow for the construction of dataflow designs on FPGAs, and an APIs that provides task parallel computing on FPGAs. This tool was enhanced **Kraemer et al., "TaPaSCo: enabling FPGA-based accelerators in HPX applications"** to also support programming AIEs on the Versal and integrates with the existing FPGA approach. However, all these tools share the same limitation that programmers must learn new technologies and then manually port their codes, potentially requiring rewriting in new languages. By contrast, our view is that programmers must be able to leverage these technologies without any changes necessary to their existing codes or specialised knowledge on their behalf.

\subsection{LLVM and MLIR}

LLVM **Lattner et al., " LLVM: a compilation framework for heterogeneous architectures"** provides reusable compiler and tool chain technologies the enable the development of compilers across different languages and hardware. There are many language frontends provided by LLVM and support for a range of hardware backends, with these connected via LLVM-IR. Consequently, an LLVM frontend such as Clang, which provides C and C++, that generates LLVM-IR is able to target any backend, and support for a wide range of architectures including CPUs, GPUs, and FPGAs has been developed. However, LLVM-IR is low level, and it requires significant work by each frontend to target LLVM-IR and results in duplication of compilation infrastructure between frontends.

MLIR, which was first developed by Google and then released open source in 2019, aims to address this issue of duplication by providing many IR dialects and transformations between them. Consequently, instead of targeting the low level LLVM-IR, frontends can translate to a mix of higher level intermediate representations and then leverage existing transformations within MLIR to lower to LLVM-IR. The IR follows a Static Single Assignment (SSA) form, and one of the major strengths of MLIR is that dialects can be mixed and manipulated separately, enabling progressive lowering of the abstraction level ultimately to LLVM-IR.  Because this involves existing dialects and transformations, the MLIR approach enables a much greater sharing of compiler infrastructure between frontends, significantly reducing the overall software effort in developing compilers. MLIR also provides a framework for defining bespoke dialects and transformations. 

MLIR is a sub-project of LLVM, and there are many IR dialects provided as standard including \emph{memref} for memory management and data access, \emph{func} to represent functions and calling between them, and \emph{linalg} which express linear algebra operations. All of these ultimately lower to the \emph{llvm} MLIR dialect, from which LLVM-IR is generated by the \emph{mlir-translate} tool. A considerable community has grown up around MLIR with involvement from many vendors. AMD have invested heavily in this technology with their own fork of MLIR which targets the AIEs. AMD developed several dialects, including \emph{aie} that for streaming connections between AIE compute tiles and direct memory access, \emph{aievec} for vector arithmetic operations, and \emph{adf} to express AMD's Adaptive Data Flow (ADF) graph that connects tiles. For each of these, transformations and optimisations have been developed which ultimately results in a set of instructions that execute across the AIE array.

\subsubsection{xDSL}

One of the disadvantages of MLIR is that programmers must initially learn a range of complex LLVM concepts, and then work with the Tablegen format to describe dialects and keep track of the fast evolving MLIR repository. By contrast, xDSL **Renganarayanan et al., "xDSL: a Python-based compiler design toolkit for MLIR"** is a Python based compiler design toolkit which is 1-1 compatible with MLIR. Providing the majority of standard MLIR dialects, as well as numerous additional experimental ones too, these are all expressed in **Renganarayanan et al., "xDSL: a domain-specific language for describing MLIR dialects"**. 

In this paper we focus on Fortran intrinsics, which are built in procedures defined by the Fortran standard to provide utility functionality. Given Fortran's lineage in scientific computing, a range of intrinsics are defined that undertake calculations and examples include the \emph{sum} procedure which sums all numbers in an array. Whilst the Flang compiler maps these directly to function calls in the Flang runtime library, the work undertaken in **Kraemer et al., "Flang: a Fortran frontend for MLIR"** instead maps these to the linear algebra, \emph{linalg}, dialect. Operations in this \emph{linalg} dialect are then lowered using the existing MLIR infrastructure, for instance to be optimised for the CPU, and the rich source of information about the linear algebra operation can be leveraged to target other architectures, in this work AMD's AIEs.