\section{Related work}
\subsection{Contrastive learning}

Contrastive learning is a self-supervised learning technique that learns effective feature representations by contrasting positive and negative pairs without the need for explicit data labeling, which is particularly important when labeled data is scarce or costly to obtain. It enables models to extract feature representations from the structure of the data itself, which can be applied to various downstream tasks such as classification, detection, and segmentation.

Recent work has applied contrastive learning to abnormal classification tasks in respiratory sounds. Combining contrastive learning with multiple instance learning, studies \cite{song2023patch} \cite{song2021contrastive} divide each sample into multiple patches, categorizing patches into different subclasses based on the types of respiratory sounds they contain. Since patch-level labels are often unavailable, they estimate patch-level labels using a multiple instance learning (MIL) approach, minimizing the distance between features of patches from the same subclass and maximizing the distance between features of patches from different subclasses, forming a contrastive loss for learning. Another paper \cite{bae2023patch}  proposes the use of pre-trained weights on large-scale visual and audio datasets and introduces a Patch-Mix contrastive learning method to improve respiratory sound classification performance. Combining metadata and labels, study \cite{moummad2023pretraining} proposes a supervised contrastive learning method to extract useful representations of respiratory sounds, demonstrating that using supervised contrastive learning combined with metadata learning is superior to cross-entropy training. \cite{kim2024stethoscope} suggested a stethoscope-guided contrastive learning method to reduce the model's dependency on stethoscope data



\subsection{Deep Clustering}
Clustering aims to group given objects such that the similarity among objects within the same cluster is higher than those in other clusters. It is an unsupervised learning method widely applied in various practical applications like image classification and data visualization. Traditional clustering methods such as K-means (KM), Gaussian Mixture Model (GMM), and Spectral Clustering (SC) are known for their speed and broad applicability. However, these methods often rely on predefined similarity measures, which become ineffective when dealing with high-dimensional data. To address this issue, researchers have employed dimensionality reduction techniques to project the original data into a lower-dimensional space before clustering, thereby facilitating effective clustering. Nevertheless, the limited expressive power of low-dimensional features restricts the representation ability of the data.


Compared to traditional methods, deep neural networks (DNNs) excel in extracting high-level features through nonlinear embeddings, benefiting clustering tasks. Approaches like Deep Embedded Clustering (DEC) \cite{xie2016unsupervised} and Deep K-means (DKM) \cite{fard2020deep} typically employ a two-stage learning process, separating feature extraction and clustering. However, this separation overlooks potential relationships, limiting performance. To address this, researchers explore joint unsupervised learning \cite{yang2016joint} and propose models like deep adaptive clustering (DAC) \cite{chang2017deep} and Adaptive Self-Paced Clustering (ASPC) \cite{guo2019adaptive}. These models prioritize high-confidence samples and incorporate constraints into deep clustering processes to enhance performance. Constraints include pairwise, instance hardness, triplet, and cardinality constraints \cite{zhang2021framework}. Additionally, instance-level must-link and cannot-link constraints \cite{gonzalez2020agglomerative} leverage pairwise distances between instances to enhance constrained clustering capability.




Inspired by the aforementioned works, this paper integrates deep clustering with contrastive learning and bring them into the task of respiratory sound classification. Unlike previous studies, we introduce a cosine similarity-based constraint loss in deep clustering to further reduce inter-cluster correlations and enhance clustering performance. In contrastive learning, we modify the encoding of spectrograms by grouping them directly at the frame level instead of patch encoding. The features of different samples within the same batch are mixed using group mix to generate new mixed samples for contrastive learning.