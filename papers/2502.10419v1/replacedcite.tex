\section{Related Work}
\label{section2}
In this section, we examine the relevant research on the deployment of \textit{MLLMs}, focusing on FL and edge systems. We will also discuss related studies on the integration of \textit{MLLMs} with swarm intelligence.

\subsection{Deployment of MLLMs and LLMs}
\textit{MLLMs} and \textit{LLMs} are extensively used in the AI community for their ability to process enormous amounts of data. Their foundation on attention mechanisms enables them to efficiently handle long data sequences while preserving dependencies within the data. Notable \textit{LLMs} in use currently include BERT ____, GPT and its variants ____, CLIP ____, and various LLAMA models ____, among others. However, challenges related to scalability and privacy remain major obstacles when deploying \textit{LLMs}, particularly \textit{MLLMs} ____. Federated learning is one of the emerging solutions to address this issue, as it allows \textit{MLLMs} to be deployed on edge devices that collaboratively train a shared model without exchanging their local data ____. However, \textit{MLLMs} are known for their high memory and computational demands, making deployment particularly challenging on edge devices with limited consumer-level GPUs. 

Various approaches have been proposed to address the issue of computing resources, aiming to enhance the flexibility and reusability of \textit{MLLMs}/\textit{LLMs} while ensuring secure and private data handling. For instance, solutions like task scheduling or organizing machine learning tasks have been explored to facilitate this process ____, along with solutions that utilize model compression techniques and Parameter-Efficient Fine-Tuning methods ____.
%like LoRA (Low-Rank Adaptation) , smaller-sized adapters %. 
At the same time, other approaches have been proposed to avoid deploying resource-intensive models on edge devices and instead use alternatives that effectively leverage these powerful models in constrained environments. For example, in the context of federated learning systems, the authors in ____ propose using Knowledge Distillation and a Prompt Generator to generates knowledge from the combined data of multiple IoT devices, allowing the model to be updated without the need for full deployment on edge devices, thereby keeping sensitive data private. Similarly, in ____, the authors propose pretraining \textit{LLMs} on the server before fine-tuning them on edge devices. This approach aims to minimize both computational resource usage and memory consumption related to deploying large models on edge devices. In a related work ____, the authors also suggest dividing the computational tasks of \textit{LLMs} between the server and edge devices to address the limited capabilities of these devices, while another work ____ suggests distributing sensitive layers of the \textit{LLM} on client devices while offloading non-sensitive layers to the server.

The deployment of large models is not the sole issue, the communication between edge devices and the cloud server also presents challenges. To address the communication overhead, the authors in ____ suggest deploying the large visual model on the cloud server and allowing vehicles to share only the learned features instead of the entire model parameters. This strategy enables each vehicle to keep its training data local while simultaneously reducing communication demands.
Given the ongoing efforts to manage the deployment of \textit{MLLMs}/\textit{LLMs} in decentralized systems like federated learning, including approaches that involve server deployment or sharing layers and data between edge devices and servers, there is still a lack of effective strategies for edge device selection and communication optimization. Therefore, to address this gap, we propose deploying pre trained \textit{MLLMs} on edge devices, where the models are initially trained in the cloud and subsequently fine-tuned on the edge. 

While various approaches in the literature have explored supervised learning, unsupervised learning, and reinforcement learning to optimize federated learning processes ____, these methods face challenges in handling communication overhead, non-IID data, and the dynamic nature of the environment. Swarm intelligence, with its decentralized structure, ability to select devices based on available resources and data relevance, and capability to optimize communication between edge devices and the cloud, presents a promising solution to these issues. However, there is limited research on the application of swarm intelligence techniques for device selection in federated learning ____. In this paper, we propose leveraging swarm intelligence techniques, including \textit{PSO} and \textit{ACO}, to optimize the deployment strategy of MLLMs, focusing on improving the efficiency of model updates between the edge and the cloud, while considering the resource availability and data relevance of edge devices.

\subsection{MLLMs/LLMs and Swarm Intelligence}
Swarm intelligence is an emerging approach in the field of AI that leverages individual collaboration to achieve a common and overarching goal. In the context of \textit{LLMs}, swarm intelligence is increasingly being used to optimize the performance of these models through multi-\textit{LLM} collaboration. For instance, the authors of this work ____ have developed a swarm model inspired by \textit{PSO} that enables multiple \textit{LLMs} to work together. They explore the weight space based on successful training checkpoints to enhance performance and make the models easily adaptable to various tasks.
In a different context, the authors in ____ have developed an approach based on swarm intelligence and visual question answering mechanisms to enable multiple Large Vision-Language Models (LVLMs) to collaborate on the geo-localization task. This solution allows for linking images with specific geographic locations without requiring a large database of geo-tagged images. At the same time, it leverages the network retrieval capabilities of multiple models, allowing them to collaborate and share knowledge effectively.
Given the extensive variety of swarm intelligence algorithms, practitioners often find it difficult to select the most suitable method for specific tasks, such as designing new metaheuristic algorithms ____. To address this challenge, the authors in ____ propose a study that uses a \textit{LLM} like GPT-4 to assist in the design process of novel metaheuristics through the hybridization of various swarm intelligence algorithms. However, the authors also highlight several challenges associated with the use of \textit{LLMs}, including their inability to consistently produce accurate and reliable outputs, necessitating human intervention. Additionally, they address the ethical and social concerns that arise from the application of these models.

From our literature review, we found examples of using swarm intelligence to enhance the collaborative use of \textit{LLMs}, making them more adaptable to various tasks or improving their efficiency for specific applications. Additionally, there are instances where \textit{LLMs} are employed to optimize the performance of swarm intelligence algorithms, such as in the design of metaheuristics. However, we noted a lack of research focused on applying swarm intelligence to improve the deployment of \textit{LLMs}, as well as a notable scarcity of studies addressing \textit{MLLMs}. Therefore, in this paper, we propose utilizing hybrid swarm intelligence, combining \textit{PSO} and \textit{ACO}, to optimize the deployment of \textit{MLLMs} specifically in federated learning systems within smart edge-cloud computing environments. Our approach aims to facilitate better resource allocation, improve communication between models, and optimize the overall performance of \textit{MLLMs} in dynamic environments.