

\documentclass[3p,times,authoryear]{article} % For LaTeX2e
%\usepackage{moderncvcompatibility} 
\usepackage{iclr2021_conference,times}
\usepackage{makecell, multirow}
\usepackage{graphicx}
\usepackage[tableposition=below]{caption}
\usepackage{longtable}
\captionsetup[longtable]{skip=1em}
\input{math_commands.tex}
\usepackage{enumitem}
\setlength{\parskip}{0.2mm}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{hyperref}
\hypersetup{colorlinks,allcolors=black}

\usepackage{fancyhdr}
\usepackage{array}
\usepackage{url}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{float}
%\usepackage{natbib}
\setcitestyle{round,authoryear,comma}
\usepackage{geometry}

%\usepackage{amsmath,amsfonts,bm}
%% For ESWA journal you need to use APA style

\usepackage{array,ragged2e}
\newcolumntype{M}[1]{>{\Centering\arraybackslash}m{#1}}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{0ex}%
   {1mm}%
   {1mm}%
   {\normalfont\normalsize\bfseries}}
\setcounter{secnumdepth}{4}
\newcommand\mymaketitle{%
  \begin{titlepage}
    \null\vfil\vskip 40\p@
    \begin{center}
      {\@title}
      \vskip 2.5em
      {\large \lineskip .75em \@author \par}
      \vskip 1.5em
      {\large \@date \par}
    \end{center}\par
    \@thanks
    \vfil\null
  \end{titlepage}
}
\makeatother

\usepackage{fancyhdr}



%\pagestyle{plain}

% \title{A Multi-Modal Machine Learning Framework for Denoising Biomedical Signals during CPR}
% \title{A Multi-Modal Machine Learning Unsupervised Method for Biomedical Signal Processing during CPR}
\title{A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large Language Models Deployment in Edge-Cloud-based Federated Learning Environments}
% \title{Enhancing CPR with Multi-Modal Machine Learning: A Framework for Denoising Biomedical Signals}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Gaith Rjoub$^4$, Hanae Elmekki$^1$, Saidul Islam$^1$, Jamal Bentahar$^{2,1,*}$, Rachida Dssouli$^1$ \\
$^1$Concordia Institute for Information Systems Engineering, 
Concordia University, Montreal, Canada \\
$^2$Department of Computer Science,
Khalifa University, Abu Dhabi, UAE\\
$^3$Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada\\
$^4$Faculty of Information Technology, Aqaba University of Technology, Aqaba, Jordan\\
\\\\
%
%
\textbf{Contributing Authors' Emails:}\\ jamal.bentahar@concordia.ca;\\ grjoub@aut.edu.jo;\\ saidul.islam@concordia.ca;\\ rachida.dssouli@concordia.ca\\\\
\textbf{$^*$Corresponding Author's Email:}  jamal.bentahar@concordia.ca\\\\
The authors contributed equally to this work.
%
% \texttt{\{robot,net\}@wits.ac.za} \\}
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\geometry{top=1.0in,left=1.0in,bottom=1.0in,right=1.0in}

\usepackage{fancyhdr}
\fancyhf{}


\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\fancypagestyle{first}{%
\fancyhf{} % clear all header and footer fields

%\lhead{Information Science (Elsevier)} % except the center
\setlength{\headsep}{0.5in}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}
\pagestyle{fancy}     
\fancyhf{}
\fancyfoot[C]{\vspace{0.4cm} \thepage}
\begin{document}

  \begin{center}
\maketitle
\thispagestyle{first}
\end{center}
\begin{abstract}


%Cardiopulmonary resuscitation (CPR) is a critical intervention aimed at restoring vital blood circulation and breathing in individuals experiencing cardiac arrest or respiratory failure, representing a critical aspect of emergency medical care. Numerous biomedical signals are associated with CPR automation, operations, and monitoring systems from the start of the ambulance journey to the intensive care unit (ICU) at the hospital. However, CPR signals often suffer from various forms of noise and artifacts, posing challenges to accurate clinical interpretation and decision-making using intelligent systems. Conventional denoising methods employing various filters exhibit limitations in parameter sensitivity and addressing the complex noise characteristics inherent in CPR signal scenarios. However, achieving high precision is desirable in CPR scenarios, considering it as a matter of life and death in medical emergencies. In this context, Machine Learning (ML) is known for handling complex data characteristics. Though a few supervised ML approaches have been proposed, a dedicated unsupervised ML approach for denoising CPR signals is yet to be introduced, which is demanding considering the applicability in real-life scenarios. To this end, a novel ML methodology is proposed for denoising biomedical signals during CPR in this paper, which is capable of denoising multiple signals in an unsupervised manner utilizing a multi-modality approach. It demonstrates notable enhancements in noise removal and signal fidelity without labeled data, achieving high values of signal-to-noise ratio (SNR) and peak signal-to-noise ratio (PSNR) compared to existing ML methods and filters in an unsupervised context. Additionally, the methodology preserves signal correlations at a significant level of 0.9993 for downstream tasks. Finally, the proposed methodology enhances CPR monitoring and decision-making by being adaptable to automated CPR systems and extendable for denoising various biomedical signals beyond CPR scenarios.


The combination of Federated Learning (FL), Multimodal Large Language Models (\textit{MLLMs}), and edge-cloud computing enables distributed and real-time data processing while preserving privacy across edge devices and cloud infrastructure. However, the deployment of \textit{MLLMs} in FL environments with resource-constrained edge devices presents significant challenges, including resource management, communication overhead, and non-IID data. 
To address these challenges, we propose a novel hybrid framework wherein \textit{MLLMs} are deployed on edge devices equipped with sufficient resources and battery life, while the majority of training occurs in the cloud. To identify suitable edge devices for deployment, we employ Particle Swarm Optimization (\textit{PSO}), and Ant Colony Optimization (\textit{ACO}) is utilized to optimize the transmission of model updates between edge and cloud nodes.
This proposed swarm intelligence-based framework aims to enhance the efficiency of \textit{MLLM} training by conducting extensive training in the cloud and fine-tuning at the edge, thereby reducing energy consumption and communication costs. Our experimental results show that the proposed method significantly improves system performance, achieving an accuracy of $92\%$, reducing communication cost by $30\%$, and enhancing client participation compared to traditional FL methods. These results make the proposed approach highly suitable for large-scale edge-cloud computing systems.



\textit{Keywords}: Federated Learning, Large Multimodal Models, Swarm Intelligence, Particle Swarm Optimization, Ant Colony Optimization, Edge-Cloud Computing, Resource Optimization.

\end{abstract}


\section{Introduction}
\label{sec1}

\subsection{Context and Motivation}

Internet of Things (IoT) devices have become increasingly prevalent, generating vast amounts of data that necessitate real-time processing and intelligent decision-making. To effectively analyze this diverse and extensive data, deep learning approaches have emerged, particularly Multimodal Large Language Models (\textit{MLLMs}), which excel in processing and understanding various data types, including text, images, audio, and sensor readings. These models have contributed significantly to advancements in several emerging domains \cite{huang2024large}, such as autonomous systems \cite{aldeen2024initial}, smart healthcare \cite{wang2023accelerating}, and industrial IoT \cite{fan2024new, usecase_04}. 
In this context, the integration of edge-cloud computing systems with Federated Learning (FL) has garnered substantial attention for its ability to facilitate decentralized training of the deep learning model while ensuring data privacy and security \cite{Akhtarshenas24, bao2022federated, wu2024topology, trindade2022resource,sagar2024hierarchical}. This approach allows for model training directly on IoT devices, thereby preserving sensitive data and enhancing privacy. Fig. \ref{fig: FL_MLLM_architecture} illustrates a general architecture for such integration. In this setup, a central cloud server maintains a global model (\textit{MLLM}), while edge devices perform local fine-tuning for specific tasks (e.g., self-driving cars and drones). Only model updates, rather than raw data, are shared with the central cloud, enhancing the global model while preserving data privacy.

Despite these potential benefits, the deployment of \textit{MLLMs} in edge-cloud systems presents several critical challenges. First, resource constraints on edge devices \cite{sajnani2024secure}, such as limited computational power and battery life, restrict their ability to participate continuously in FL training rounds. Second, the communication overhead associated with transmitting model updates between distributed edge devices and cloud infrastructure can overwhelm networks, particularly in large-scale deployments. Third, the non-IID (non-independent and identically distributed) nature of data across edge devices adds complexity to the model training process, as local data often varies significantly in structure and relevance.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.8\textwidth, height=.65\textwidth]{MLLM_ASO_PCO.drawio.pdf}
    \caption{Edge-Cloud-based \textit{MLLM} Architecture with Federated Learning for UAV Networks}
    \label{fig: FL_MLLM_architecture}
\end{figure}

To address the limitations of edge devices, which often lack the necessary resources and capacities for large models like \textit{MLLMs}, we introduce a framework that deploys pretrained \textit{MLLMs} on these devices for fine-tuning, while primary training is conducted in the cloud. In this framework, Particle Swarm Optimization (\textit{PSO}) is employed to efficiently select suitable devices for \textit{MLLM} deployment to ensure that only the most appropriate devices, which have sufficient resources and relevant data, are involved in each FL training round. This selection process helps to tackle the challenges posed by non-IID data of edge devices by providing a more representative sample of the overall data distribution. As a result, it facilitates effective updates to the \textit{MLLM} and enhances its generalization capabilities, even despite the heterogeneity of local data. 
Additionally, Ant Colony Optimization (\textit{ACO}) is employed to optimize communication pathways between the selected edge devices and cloud nodes, facilitating the efficient sharing of model updates \cite{abualigah2023swarm, sun2020survey}. This optimization not only minimizes bandwidth usage but also reduces latency and conserves energy, thereby enhancing the overall performance of the framework. 

The choice of \textit{PSO} and \textit{ACO} is motivated by their proven efficacy in solving complex optimization problems in distributed and dynamic environments. \textit{PSO} is well-suited for optimizing continuous, high-dimensional search spaces, as it efficiently converges towards near-optimal solutions with fewer iterations, making it ideal for edge device selection where multiple criteria must be considered \cite{cao2024computational}. On the other hand, \textit{ACO} excels in discrete optimization problems and has been shown to effectively find optimal or near-optimal paths in dynamic networks by mimicking the natural foraging behavior of ants. This makes \textit{ACO} particularly suitable for optimizing communication pathways in the dynamic and resource-constrained edge-cloud environment, where path quality may change frequently due to varying network conditions \cite{jiang2024evolutionary}.
By combining \textit{PSO} and \textit{ACO}, our hybrid approach aims to balance the computational load across edge devices while ensuring efficient communication between the edge and the cloud. This dual optimization process facilitates the effective deployment of \textit{MLLMs} in edge-cloud computing environments, improving system scalability and performance. In this paper, we evaluate the effectiveness of the proposed hybrid model through a series of experiments conducted in a simulated edge-cloud system. Our results demonstrate significant improvements in energy efficiency, communication overhead, and model accuracy compared to traditional FL approaches.


\subsection{Contributions}

In this paper, we present a novel hybrid swarm intelligence approach to optimize the deployment of \textit{MLLMs} in FL within smart edge-cloud computing systems. Our key contributions are summarized as follows:

\begin{enumerate}
    \item \textbf{Hybrid \textit{PSO}-\textit{ACO} Optimization Framework for Efficient Deployment of \textit{MLLMs}:} We propose a hybrid optimization framework that integrates \textit{PSO} and \textit{ACO}. In this framework, \textit{PSO} is utilized for the efficient selection of edge devices for deploying pretrained \textit{MLLMs}, based on criteria such as resource availability, data relevance, and network stability. \textit{ACO} is employed to optimize communication pathways and facilitate the sharing of \textit{MLLM} updates between the selected edge devices and cloud servers, minimizing communication overhead and latency.

   \item \textbf{Addressing Non-IID Data}: The proposed \textit{PSO}-based method tackles the challenges associated with non-IID data by intelligently selecting a subset of edge devices to ensure diverse contributions to the global model. This approach enhances the generalization and accuracy of the \textit{MLLM} despite the heterogeneity present in the local data distributions.

    \item \textbf{Energy Efficiency and Model Performance Trade-off Optimization:} Our approach balances energy consumption with model accuracy, achieving significant reductions in total energy usage while maintaining or improving the performance of the \textit{MLLM}. This is particularly important for prolonging the operational life of edge devices in resource-constrained environments. 

    \item \textbf{Scalability and Adaptability in Dynamic Environments:} The framework is designed to scale effectively and adapt to real-time variations in edge-cloud environments, managing large numbers of devices and varying network conditions while ensuring optimal system performance.

    \item \textbf{Extensive Experimental Validation:} We conducted comprehensive simulations to evaluate the performance of our hybrid \textit{PSO}-\textit{ACO} framework. The results demonstrate notable improvements in energy efficiency, communication overhead reduction, and model accuracy compared to relevant benchmarks, including deep and reinforcement learning-based approaches.

\end{enumerate}

These contributions collectively provide a robust solution to the challenges of deploying \textit{MLLMs} in FL systems within smart edge-cloud computing environments. Our hybrid \textit{PSO}-\textit{ACO} framework enhances the efficiency, scalability, and performance of FL, making it more viable for real-world applications involving large-scale, resource-constrained edge networks.

The paper is structured as follows: Section \ref{Problem_Formulation} presents the problem formulation, describing the objectives and constraints of the federated learning scenario with the integration of swarm intelligence over the edge-cloud computing environment. Section \ref{Background} provides a brief background on \textit{MLLMs}, including their training process and fine-tuning methods, which are crucial for understanding the implementation of our proposed solution. Section \ref{section2} gives an overview of the related work in FL, swarm intelligence, and edge-cloud computing, establishing the foundation for the proposed approach. Section \ref{section3} details the proposed hybrid \textit{PSO}-\textit{ACO} framework, explaining the integration of particle swarm optimization and ant colony optimization to enhance model performance. The experimental setup and simulation results are presented in Section \ref{results}, where we compare the performance of the proposed framework against relevant benchmarks, including traditional and learning-based models. Finally, Section \ref{section6} concludes the paper by summarizing the findings and discussing potential future research directions.






\section{Problem Formulation} \label{Problem_Formulation}

Deploying \textit{MLLMs} in \textit{FL} systems within \textit{smart edge-cloud computing environments} poses several critical challenges. These challenges arise due to the decentralized nature of FL, the limited resources of edge devices, and the inherent complexities in managing communication and model training across a large and diverse network of devices. In this section, we outline the key challenges that must be addressed to enable the efficient deployment of \textit{MLLMs} in FL systems.

\subsection{Resource Constraints on Edge Devices}

One of the primary challenges in deploying \textit{MLLMs} in FL is the limited computational and energy resources available on edge devices, such as IoT sensors, cameras, and mobile devices. These devices often have restricted battery life and processing power, making it impractical for all devices to participate in every round of FL training. Given the high computational demands of training \textit{MLLMs}, selecting a subset of edge devices that can contribute effectively to the global model without exhausting their resources is essential.

The total energy consumption \( E_i \) for each device \( i \) includes the energy required for both model training \( E_{\text{train}} \), communication \( E_{\text{comm}} \) and time \( E_{\text{time}} \):

\begin{equation}
E_i = E_{\text{train}} + E_{\text{comm}} + E_{\text{time}}
\label{eq1}
\end{equation}

\noindent where \( E_{\text{train}} \) is the energy consumed during training, \( E_{\text{comm}} \) and \( E_{\text{train}} \) represents the energy and time required to transmit model updates to a central server or cloud coordinator, respectively. This central server aggregates local model updates from multiple edge devices. This aggregation step is crucial in FL for updating the global model, which is then shared back with the participating devices.

In FL, training takes place over multiple rounds, where each round consists of three main steps: \textbf{(1)} Global Model Distribution: The central server sends the current global model to a selected set of edge devices. \textbf{(2)} Local Training: Each selected device trains the model on its local data for a specified number of iterations, using its own computational resources. \textbf{(3)} Model Update Communication: The locally updated models are then transmitted back to the central server for aggregation. The process is repeated for several rounds, with the goal of gradually improving the global model's accuracy. The challenge is to minimize the total energy consumption across the selected devices during these rounds, while still ensuring meaningful contributions to the global model updates:

\begin{equation}
\min_{S \subseteq D} \sum_{i \in S} E_i
\label{eq2}
\end{equation}

\noindent where \( S \) represents the subset of devices selected from the entire device set \( D \).

\subsection{Communication Overhead and Latency}

Another significant challenge is the communication overhead associated with transmitting large model updates between edge devices and the cloud or edge servers. In FL, frequent communication is required to synchronize local model updates with the global model, which can result in substantial bandwidth usage, network congestion, and increased latency. This is particularly problematic when deploying \textit{MLLMs}, which typically have large model sizes.

The communication cost \( C_{\text{comm}} \) for each device \( i \) can be modeled as follows:

\begin{equation}
C_{\text{comm}} = \sum_{i \in S} \frac{M_i}{B_i} \cdot d_i
\end{equation}

\noindent where \( M_i \) is the size of the model update, \( B_i \) is the available bandwidth, and \( d_i \) represents the distance between device \( i \) and the nearest server. Reducing communication overhead and minimizing latency is crucial to maintaining the efficiency of the FL process, particularly in large-scale deployments.

\subsection{Non-IID Data Distribution Across Devices}

In FL systems, data is distributed across edge devices in a non-IID (non-independent and identically distributed) manner, meaning that each device collects data that reflects its specific environment or context. This leads to significant variability in the type and distribution of data across the network, which can negatively affect the performance and generalization of the global model, especially when training \textit{MLLMs} that rely on diverse and balanced data.

The local loss function \( \mathcal{L}_i(w) \) for each device \( i \) is computed based on its local data, where \( w \) represents the model parameters:

\begin{equation}
\mathcal{L}_i(w) = \frac{1}{n_i} \sum_{j=1}^{n_i} \ell(f(x_j; w), y_j)
\end{equation}

\noindent where \( n_i \) is the number of local data points, \( \ell(f(x_j; w), y_j) \) is the loss function between the model prediction \( f(x_j; w) \) and the true label \( y_j \). The global objective in FL is to minimize the weighted sum of the local losses across all selected devices:

\begin{equation}
\mathcal{L}(w) = \sum_{i \in S} \frac{n_i}{n} \mathcal{L}_i(w)
\end{equation}

\noindent where \( n \) is the total number of data points across all devices. The challenge here is to ensure that the global model generalizes well despite the heterogeneity in the local data distributions.

\subsection{Energy and Time Efficiency vs. Model Performance Trade-off}

A critical challenge in FL systems is balancing the energy and time consumption of edge devices with the overall performance of the \textit{MLLM}. Edge devices with limited energy and time resources may still hold valuable data that could significantly improve the global model, however, their continuous participation could lead to battery depletion, increased latency, and potential system failures. Conversely, prioritizing solely on energy and time efficiency may result in the exclusion of devices with high-quality data essential for effective model training. Balancing these factors is crucial to ensuring both sustainable resource usage and optimal model performance across the network.

The total energy consumed by a device can be formulated as shown in Eq.\ref{eq1}.
The optimization problem involves minimizing a weighted combination of energy consumption and model performance as follows:

% \begin{equation}
% \min_{S \subseteq D} \alpha \sum_{i \in S} E_i + \beta \mathcal{L}(w),
% \end{equation}

\begin{equation}
\min_{S \subseteq D} \alpha \sum_{i \in S} E_i + \gamma \sum_{i \in S} T_i + \beta \mathcal{L}(w) 
\end{equation}

\noindent where \( \alpha \), \( \beta \), and \( \gamma \) are weighting factors that balance the trade-off between energy efficiency and model accuracy.

\subsection{Scalability and Adaptability in Dynamic Edge-Cloud Environments}

As edge-cloud systems scale to include thousands or even millions of devices, the need for scalability and dynamic adaptability becomes increasingly important. Edge devices are often mobile and subject to changing environmental conditions, leading to variability in resource availability and connectivity. This dynamic nature requires FL systems to continuously adapt to changes in the network, ensuring efficient use of resources and maintaining high model performance.

Scalability challenges arise in ensuring that the system can manage the increasing number of devices without overwhelming network resources or introducing significant delays. Dynamic adaptability involves efficiently handling devices joining or leaving the network, adjusting communication paths, and reallocating computational tasks in real time.

\section{MultiModal Large Language Model (MLLM) Background}\label{Background}

\textit{MLLM} are advanced machine learning models capable of processing and integrating data from multiple modalities, such as text, images, audio, and structured sensor data \cite{MLLM_02}. The structure and architecture of \textit{MLLM} center on integrating a Large Language Model (\textit{LLM}) with additional components to handle other data modalities, like images, audio, or video. \textit{MLLMs} primarily have a language-centric architecture where other modalities are treated as extensions, allowing the model to understand and generate responses based on complex, multimodal input \cite{MLLM_01}. Fig. \ref{fig:MLLM_architecture} depicts the architectural overview of \textit{MLLMs} illustrating core components and workflow.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth, height=.28\textwidth]{MLLM_diagram.pdf}
    \caption{Architectural Overview of \textit{MLLM}. }
    \label{fig:MLLM_architecture}
\end{figure}


\subsection{Backbone of MLLM}

\textbf{LLM Foundation:} The core of an \textit{MLLM} is an \textit{LLM} such as GPT, PaLM, or BERT, trained on extensive text datasets \cite{MLLM_00}. The language model is typically a transformer-based model with self-attention mechanisms that enable it to understand and generate text \cite{Transformer}.\\

\textbf{Pretrained Text Understanding:}  \textit{MLLMs} leverage the language model’s strong natural language understanding and generation capabilities. This foundation ensures that the model can perform text-based tasks and serves as the central processing unit for interpreting other modalities \cite{MLLM_03}.

% The core of a \textit{MLLM} is a large language model based on the transformer architecture \cite{vaswani2017attention}. Let \( x \) represent a sequence of tokens (words or subwords in the text) \cite{MLLM_02}. For a text input \( x = \{x_1, x_2, \ldots, x_n\} \):

% \begin{itemize}
%     \item \textbf{Token Embedding}: Each token \( x_i \) is mapped to a continuous vector space:
%     \begin{equation}
%     \text{Embed}(x_i) = e_i \in \mathbb{R}^d
%     \end{equation}
%     where \( d \) is the embedding dimension.

%     \item \textbf{Positional Embedding}: Since transformers do not have an inherent sense of sequence, a positional encoding \( p_i \) is added to each token embedding:
%     \begin{equation}
%     h_i^{(0)} = e_i + p_i
%     \end{equation}

%     \item \textbf{Self-Attention Mechanism}: For each layer \( l \) in the transformer, self-attention is used to capture contextual dependencies among tokens \cite{Transformer}.
%     \begin{equation}
%     \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
%     \end{equation}
%     where:
%     \begin{itemize}
%         \item \( Q = h^{(l)} W_Q \), \( K = h^{(l)} W_K \), \( V = h^{(l)} W_V \)
%         \item \( W_Q \), \( W_K \), and \( W_V \) are learnable weight matrices.
%         \item \( d_k \) is the dimension of \( K \).
%     \end{itemize}

%     This attention mechanism forms the basis for processing multimodal embeddings in the \textit{MLLM}.

%     \item \textbf{Layer Output}: After the attention mechanism, the output is passed through a feed-forward neural network:
%     \begin{equation}
%     h_i^{(l+1)} = \text{FFN}(\text{Attention}(Q, K, V))
%     \end{equation}
% \end{itemize}

\subsection{Modality-Specific Encoders}

To handle non-textual data like images or audio, modality-specific encoders are used. The encoders transform each modality into embeddings that can be processed by the language model \cite{MLLM_modality}.

\begin{itemize}
    \item \textbf{Image Encoder}: Given an image \( I \), it is divided into patches, and each patch is flattened into a vector \cite{MLLM_image}. These patch vectors are linearly embedded into a vector space, similar to text token embeddings:
    \begin{equation}
    f_i = \text{Embed}(\text{patch}_i) \quad \text{for each patch}_i
    \end{equation}
    This results in image embeddings \( F = \{f_1, f_2, \ldots, f_m\} \), where each \( f_i \in \mathbb{R}^d \).

    \item \textbf{Audio Encoder}: Audio data \( A \) is processed into a sequence of feature vectors $a_j$ using an encoder like wav2vec \cite{MLLM_audio}.
    \begin{equation}
    a_j = \text{wav2vec}(A_j) \quad \text{for each audio segment}~j
    \end{equation}
    This results in audio embeddings \( A = \{a_1, a_2, \ldots, a_k\} \), with each \( a_j \in \mathbb{R}^d \).

    \item \textbf{Sensor Data Encoder}: Sensor data \( S \) is processed into a sequence of feature vectors $S_j$ using an encoder module of Transformer model \cite{Transformer}.
    \begin{equation}
    S_z = \text{Transformer-Encoder}(S_z) \quad \text{for each sequential data segment}~z
    \end{equation}
    This results in audio embeddings \( S = \{s_1, s_2, \ldots, s_k\} \), with each \( S_z \in \mathbb{R}^d \).
\end{itemize}

\subsection{Multimodal Embedding Layer}

The outputs of the modality-specific encoders (e.g., image and audio embeddings) are mapped to a unified embedding space compatible with the language model. These embeddings are typically augmented with special tokens to identify their modality \cite{MLLM_embadding}.

Let \( E_{\text{text}} \) represent text embeddings, \( E_{\text{image}} \) represent image embeddings, \( E_{\text{audio}} \) represent audio embeddings. Each modality is projected into the shared space by learned projections:
\begin{equation}
E'_{\text{modality}} = W_{\text{modality}} \times E_{\text{modality}}
\end{equation}
where \( W_{\text{modality}} \) is a learnable weight matrix that aligns each modality to the language model’s embedding space.

\subsection{Fusion Mechanism}

\begin{itemize}
    \item \textbf{Attention-Based Fusion}: 
    \textit{MLLMs} typically leverage the transformer’s self-attention mechanism to fuse multimodal information. During processing, each token (whether from text, image, or audio) attends to every other token, allowing the model to integrate information across modalities. This attention mechanism enables the model to focus on relevant aspects of each modality for the task at hand. The multimodal tokens (from text, image, audio) are combined in a single self-attention mechanism, allowing all tokens to interact \cite{MLLM_attention}.
    \begin{equation}
    h_{\text{multi}}^{(l+1)} = \text{Attention}(Q_{\text{multi}}, K_{\text{multi}}, V_{\text{multi}})
    \end{equation}
    This fused representation \( h_{\text{multi}} \) integrates information from all modalities.
    
    \item \textbf{Cross-Attention Layers}: In some \textit{MLLMs}, additional cross-attention layers are introduced to explicitly integrate visual, audio, or other embeddings into the language model. Cross-modal attention directs the model to selectively focus on the most relevant information from non-text modalities in context with the text. For cross-attention, each modality (e.g., text, image) has separate attention layers to capture relationships \cite{MLLM_cross}.
    \begin{equation}
    \text{Attention}(Q_{\text{lang}}, K_{\text{modality}}, V_{\text{modality}}) = \text{softmax}\left(\frac{Q_{\text{lang}} K_{\text{modality}}^\top}{\sqrt{d_k}}\right) V_{\text{modality}}
    \end{equation}
    where \( Q_{\text{lang}} \) represents the language model’s query vectors, and \( K_{\text{modality}} \) and \( V_{\text{modality}} \) are the key and value vectors from the other modality.
\end{itemize}

\subsection{Modality-to-Text Generation}

Once all modalities are aligned in the embedding space, the language model backbone generates text outputs based on multimodal context \cite{MLLM_align}.

\begin{itemize}
    \item \textbf{Language Modeling Objective}: The language model produces text by predicting the next token \( y_t \) given previous tokens and the multimodal context \cite{MLLM_align_lang}.
    \begin{equation}
    P(y_t | y_{<t}, h_{\text{multi}})
    \end{equation}
    where \( h_{\text{multi}} \) represents the fused multimodal embedding.

    \item \textbf{Text Output Generation}: Using the language model’s autoregressive nature, it generates a sequence of tokens \( \{y_1, y_2, \ldots, y_T\} \), forming a coherent output based on multimodal inputs \cite{MLLM_align_generation}.
\end{itemize}

\subsection{Training Process}

Training of \textit{MLLMs} typically involves two stages.

\begin{itemize}
    \item \textbf{Text Pretraining}: The language model is pre-trained on a large corpus of text to establish language understanding, optimizing \cite{MLLM_pretrain}.
    \begin{equation}
    \min \mathcal{L}_{\text{text}} = -\sum_{t} \log P(y_t | y_{<t})
    \end{equation}

    \item \textbf{Multimodal Fine-Tuning}: After text pretraining, the model is fine-tuned on multimodal datasets with paired text and other modalities, optimizing for cross-modal tasks \cite{MLLM_finetune}.
    \begin{equation}
    \min \mathcal{L}_{\text{multi}} = -\sum_{t} \log P(y_t | y_{<t}, h_{\text{multi}})
    \end{equation}
    The objective is to maximize the likelihood of generating correct responses based on multimodal context.
\end{itemize}


% MLLMs are advanced machine learning models capable of processing and integrating data from multiple modalities, such as text, images, audio, and structured sensor data. By handling multimodal inputs, MLLMs provide a more comprehensive and contextually rich understanding, which is especially valuable in complex applications like autonomous vehicles, medical diagnosis, and human-computer interaction \cite{LMM_00}. This section outlines the core structure of LMMs, including modality-specific processing, feature fusion, and prediction, which allow the model to learn from and make decisions based on diverse data sources.

% \subsection{Modality-Specific Encoding}

% Each modality in an LMM is handled by a dedicated module designed to capture that data type’s unique features and structure. Let \( M = \{ x^{(1)}, x^{(2)}, \dots, x^{(m)} \} \) represent the multimodal input, where \( x^{(k)} \) is the input for the \( k \)-th modality, and \( m \) is the total number of modalities. Each modality-specific module processes its input to produce a feature representation \( z^{(k)} \), which encapsulates the learned features of that modality \cite{LMM_01}. For example:

% \begin{itemize}
%     \item \textbf{Text Module:} For textual data, transformer-based models like BERT or GPT are commonly used \cite{text_encode}. The text module produces a feature vector \( z_{\text{text}} \) as follows:
    
%     \begin{equation}
%     z_{\text{text}} = f_{\text{text}}(x_{\text{text}}; \theta_{\text{text}}),
%     \end{equation}
%     where \( f_{\text{text}} \) represents the transformer model, and \( \theta_{\text{text}} \) denotes its parameters.
    
%     \item \textbf{Image Module:} Image data is often processed using transformer-based vision models such as ViT or Swin Transformer \cite{image_encode}. The feature vector \( z_{\text{image}} \) for image data is computed as:
%     \begin{equation}
%     z_{\text{image}} = f_{\text{image}}(x_{\text{image}}; \theta_{\text{image}}),
%     \end{equation}
%     where \( f_{\text{image}} \) is the image processing model, and \( \theta_{\text{image}} \) represents its parameters.
    
%     \item \textbf{Audio Module:} For audio data, transformers tailored for sequential data, like Conformer or Whisper, are used \cite{Transformer}. The audio feature representation \( z_{\text{audio}} \) is obtained as:
%     \begin{equation}
%     z_{\text{audio}} = f_{\text{audio}}(x_{\text{audio}}; \theta_{\text{audio}}),
%     \end{equation}
%     where \( f_{\text{audio}} \) represents the audio processing model, and \( \theta_{\text{audio}} \) denotes its parameters.
% \end{itemize}

% \subsection{Multimodal Feature Fusion}
% Once each modality-specific module has produced its feature vector, the next critical step is feature fusion, where these representations are combined into a single multimodal representation. Fusion methods vary and may include simple concatenation, late fusion, or attention-based mechanisms that weigh each modality according to its relevance to the task \cite{LMM_02}.

% % In a simple \textbf{concatenation-based fusion}, the feature vectors \( \{ z_{\text{text}}, z_{\text{image}}, z_{\text{audio}} \} \) are concatenated as follows:
% % \begin{equation}
% % z_{\text{fusion}} = \left[ z_{\text{text}}, z_{\text{image}}, z_{\text{audio}} \right].
% % \end{equation}

% For applications where certain modalities may carry more weight, \textbf{attention-based fusion} is often used. In attention-based fusion, each modality is assigned an attention weight \( \alpha_k \), learned through an attention mechanism \cite{LMM_03}. The multimodal representation is then computed as:
% \begin{equation}
% z_{\text{fusion}} = \sum_{k=1}^{m} \alpha_k z^{(k)},
% \end{equation}
% where \( \alpha_k \) is the attention weight for modality \( k \), dynamically adjusted based on the relevance of each modality to the specific input.

% \subsection{Multimodal Decoder and Output Generation}

% Once a unified representation \( z_{\text{fusion}} \) is obtained, it is passed to a multimodal decoder that generates the final output \cite{LMM_05}. For classification tasks, a fully connected layer with a softmax activation function can be used:
% \[
% \hat{y} = \text{softmax}(W_{\text{fusion}} z_{\text{fusion}} + b_{\text{fusion}}),
% \]
% where \( W_{\text{fusion}} \) and \( b_{\text{fusion}} \) are the weight matrix and bias term for the prediction layer, and \( \hat{y} \) is the predicted class probabilities.\\
% For a regression task, the prediction is computed as:
% \begin{equation}
% \hat{y} = W_{\text{pred}} z_{\text{fusion}} + b_{\text{pred}},
% \end{equation}
% where \( \hat{y} \) represents the continuous output value.\\

% For generative tasks, a generative language or vision model can process \( z_{\text{fusion}} \) to generate sequential outputs:
% \begin{equation}
% \hat{y}_t = \text{Decoder}(z_{\text{fusion}}, \hat{y}_{<t}; \theta_{\text{dec}}),
% \end{equation}
% where \( \hat{y}_t \) is the model's output at timestep \( t \), conditioned on both the fused representation and previously generated outputs \( \hat{y}_{<t} \) \cite{LMM_06}.

% \subsection{Training Objective}

% The training objective of an LMM depends on the target task. For supervised learning, the model typically minimizes a cross-entropy loss:
% \[
% \mathcal{L} = -\sum_{c} y_c \log(\hat{y}_c),
% \]
% where \( y_c \) and \( \hat{y}_c \) represent the ground truth and predicted probabilities for class \( c \), respectively \cite{LMM_07}.\\
% For generative tasks, the model may minimize the negative log-likelihood of the predicted sequence:
% \[
% \mathcal{L} = -\sum_{t} \log P(\hat{y}_t | \hat{y}_{<t}, z_{\text{fusion}}).
% \]
% Through this training process, the LMM learns to balance contributions from each modality, enhancing its ability to generate accurate and coherent predictions from multimodal data \cite{LMM_08}. \\
% The LMM’s ability to integrate multiple data types allows it to capture richer and more comprehensive insights from complex environments, making it particularly valuable for applications where decisions need to be made based on a diverse set of data sources. By leveraging modality-specific processing and advanced fusion techniques, LMMs achieve high performance in scenarios requiring nuanced, contextually aware decision-making.

% \section{Overview of Multimodal Large Language Models (LMMs)}

% Multimodal Large Language Models (LMMs) are designed to process and integrate data from multiple modalities—such as text, images, audio, and structured sensor data—by creating a unified representation that captures the complementary information present across these inputs. This capability is achieved through a combination of modality-specific encoders, a feature fusion layer, and a decoder that produces a final output. This section provides an overview of the core architecture and functioning of a typical LMM.

% \subsection{Modality-Specific Encoders}

% Each data modality has unique characteristics and therefore requires a specialized encoder to process it effectively. Let the multimodal input set be represented as:
% \[
% M = \{x_{\text{text}}, x_{\text{image}}, x_{\text{audio}}, \dots, x_{\text{sensor}}\},
% \]
% where \( x_{\text{text}} \), \( x_{\text{image}} \), \( x_{\text{audio}} \), and \( x_{\text{sensor}} \) denote inputs from text, image, audio, and sensor modalities, respectively. Each modality \( x^{(k)} \) is transformed into a feature representation \( z^{(k)} \) by the corresponding encoder \( f_k \):
% \[
% z^{(k)} = f_k(x^{(k)}; \theta_k),
% \]
% where \( \theta_k \) represents the parameters of the \( k \)-th encoder. This stage extracts modality-specific features that capture the essential information within each input.

% For instance:
% - \textbf{Text Encoder} (e.g., transformer-based models like BERT):
%   \[
%   z_{\text{text}} = f_{\text{text}}(x_{\text{text}}; \theta_{\text{text}})
%   \]
% - \textbf{Image Encoder} (e.g., convolutional or transformer-based models like Vision Transformers):
%   \[
%   z_{\text{image}} = f_{\text{image}}(x_{\text{image}}; \theta_{\text{image}})
%   \]
% - \textbf{Audio Encoder} (e.g., transformer or RNN models like Whisper or Conformer):
%   \[
%   z_{\text{audio}} = f_{\text{audio}}(x_{\text{audio}}; \theta_{\text{audio}})
%   \]

% \subsection{Feature Fusion}

% After encoding, each modality is represented as a feature vector \( z^{(k)} \) in the same latent space. The next step is to combine these features into a unified representation, \( z_{\text{fusion}} \), through a fusion mechanism. Various fusion techniques, such as concatenation, attention-based fusion, and cross-attention fusion, can be used, depending on the complexity and requirements of the task.

% One common approach is \textbf{concatenation-based fusion}, where the feature vectors are stacked into a single vector:
% \[
% z_{\text{fusion}} = [z_{\text{text}}; z_{\text{image}}; z_{\text{audio}}; \dots].
% \]

% In \textbf{attention-based fusion}, learned weights \( \alpha_k \) are assigned to each modality, highlighting the most relevant features:
% \[
% z_{\text{fusion}} = \sum_{k} \alpha_k z^{(k)},
% \]
% where \( \alpha_k \) is computed by an attention mechanism that dynamically adjusts based on the importance of each modality for the current task.

% \subsection{Multimodal Decoder and Output Generation}

% Once a unified representation \( z_{\text{fusion}} \) is obtained, it is passed to a multimodal decoder that generates the final output. For classification tasks, a fully connected layer with a softmax activation function can be used:
% \[
% \hat{y} = \text{softmax}(W_{\text{fusion}} z_{\text{fusion}} + b_{\text{fusion}}),
% \]
% where \( W_{\text{fusion}} \) and \( b_{\text{fusion}} \) are the weight matrix and bias term for the prediction layer, and \( \hat{y} \) is the predicted class probabilities.

% For generative tasks, a language model (such as GPT or a transformer-based decoder) can process \( z_{\text{fusion}} \) to generate sequential outputs:
% \[
% \hat{y}_t = \text{Decoder}(z_{\text{fusion}}, \hat{y}_{<t}; \theta_{\text{dec}}),
% \]
% where \( \hat{y}_t \) is the model's output at timestep \( t \), conditioned on both the fused representation and previously generated outputs \( \hat{y}_{<t} \).

% \subsection{Training Objective}

% The training objective of an LMM depends on the target task. For supervised learning, the model typically minimizes a cross-entropy loss:
% \[
% \mathcal{L} = -\sum_{c} y_c \log(\hat{y}_c),
% \]
% where \( y_c \) and \( \hat{y}_c \) represent the ground truth and predicted probabilities for class \( c \), respectively. For generative tasks, the model may minimize the negative log-likelihood of the predicted sequence:
% \[
% \mathcal{L} = -\sum_{t} \log P(\hat{y}_t | \hat{y}_{<t}, z_{\text{fusion}}).
% \]
% Through this training process, the LMM learns to balance contributions from each modality, enhancing its ability to generate accurate and coherent predictions from multimodal data.


% \section{MultiModal Large Language Models (MLLM) and Workflow Scenario}
% \label{lmm}

% \textit{MultiModal Large Language Models (MLLM)} is designed to process diverse data types, such as images, text, audio, and sensor data. It consists of specialized modules for each modality, which are fused at a later stage to provide a unified prediction. This section outlines the architecture of the LMM, how it processes multimodal data, and the technical details of feature fusion and model optimization.

% \subsection{Modality-Specific Modules}

% % The LMM consists of separate modules for each data modality. Each module is optimized to handle the specific characteristics of its input data. Let \( M = \{x^1, x^2, \dots, x^m\} \) represent the multimodal input, where \( x^k \) is the input for the \( k \)-th modality, and \( m \) is the number of modalities. The output of each module is a feature vector \( z^k \), representing the learned features for that modality.

% The LMM is composed of distinct modules, each tailored to handle the specific characteristics of its respective data modality. Let \( M = \{x^1, x^2, \dots, x^m\} \) denote the multimodal input, where \( x^k \) represents the input from the  \( k \)-th modality, and \( m \) is the total number of modalities. Each module processes its input to produce a feature vector \( z^k \), encapsulating the learned features of that modality. Once these feature vectors are obtained, the next crucial step is to combine them effectively through multimodal feature fusion, which integrates the information across modalities.

% % Each modality-specific module processes its input as follows:
% % \begin{itemize}
% %     \item \textbf{Text Module}: Processes textual data using a transformer-based model:
% %     \begin{equation}
% %     z_{\text{text}} = f_{\text{text}}(x_{\text{text}}; w_{\text{text}}),
% %     \end{equation}
% %     where \( f_{\text{text}} \) is the transformer model, and \( w_{\text{text}} \) represents the model parameters.
    
% %     \item \textbf{Image Module}: Processes image data using a convolutional neural network (CNN): \textcolor{blue}{we should mention about attention/transformer based model as well like Swin, TNT, DETR models}
% %     \begin{equation}
% %     z_{\text{image}} = f_{\text{image}}(x_{\text{image}}; w_{\text{image}}),
% %     \end{equation}
% %     where \( f_{\text{image}} \) is the CNN, and \( w_{\text{image}} \) represents the CNN parameters.

% %     \item \textbf{Audio Module}: Processes audio data using a recurrent neural network (RNN):\textcolor{blue}{we should mention attention/transformer based model as well like like Con-
% %     former, speech transformer or whisper, and so on}
% %     \begin{equation}
% %     z_{\text{audio}} = f_{\text{audio}}(x_{\text{audio}}; w_{\text{audio}}),
% %     \end{equation}
% %     where \( f_{\text{audio}} \) is the RNN, and \( w_{\text{audio}} \) represents the RNN parameters.
    
% %     \item \textbf{Sensor Module}: Processes structured sensor data using a fully connected neural network:
% %     \begin{equation}
% %     z_{\text{sensor}} = f_{\text{sensor}}(x_{\text{sensor}}; w_{\text{sensor}}),
% %     \end{equation}
% %     where \( f_{\text{sensor}} \) is the fully connected network, and \( w_{\text{sensor}} \) represents the network parameters.
% % \end{itemize}
% \subsection{Multimodal Feature Fusion}

% The outputs from each modality-specific module are combined in a fusion layer to generate a unified representation. There are several fusion techniques that can be used, including late fusion, attention-based fusion, or concatenation.

% For example, in \textbf{late fusion}, the outputs of the modality-specific modules are combined as follows:

% \begin{equation}
% z_{\text{fusion}} = g\left( z_{\text{text}}, z_{\text{image}}, z_{\text{audio}}, z_{\text{sensor}} \right),
% \end{equation}

% where \( g(\cdot) \) is the fusion function. A common approach is concatenation:

% \begin{equation}
% z_{\text{fusion}} = \left[ z_{\text{text}}, z_{\text{image}}, z_{\text{audio}}, z_{\text{sensor}} \right],
% \end{equation}

% which stacks the feature vectors from all modalities into a single vector.

% Alternatively, in our work \textbf{attention-based fusion} applies learned attention weights to each modality:

% \begin{equation}
% z_{\text{fusion}} = \sum_{k=1}^{m} \alpha_k z^k,
% \end{equation}

% where \( \alpha_k \) is the attention weight assigned to modality \( k \), learned through an attention mechanism that highlights the most relevant features from each modality.


% % \subsection{Prediction Layer}

% % Once the multimodal features are fused, the final prediction is made using a fully connected layer followed by a softmax function for classification tasks or a regression layer for continuous outputs.

% % For a classification task, the prediction can be written as:

% % \begin{equation}
% % \hat{y} = \text{softmax}(W_{\text{pred}} z_{\text{fusion}} + b_{\text{pred}}),
% % \end{equation}

% % where \( W_{\text{pred}} \) and \( b_{\text{pred}} \) are the weight matrix and bias of the prediction layer, and \( \hat{y} \) represents the predicted class probabilities.

% % For a regression task, the prediction can be expressed as:

% % \begin{equation}
% % \hat{y} = W_{\text{pred}} z_{\text{fusion}} + b_{\text{pred}},
% % \end{equation}

% % where \( \hat{y} \) is the predicted continuous output.\\

% % \textcolor{blue}{LLM or LMM does a variety of work besides classification or regression. Like generation, summarization, and so on. Specially generative is one of the major tasks by LLM Or LMM for instance Retrieval Augmented Generation(RAG) is very popular. So, rather we should cover those things or we can skip this and focus more on the technique we are trying to focus in this paper which is optimization using PSO and ACO. This suggestion also goes with backdpop and optimization subsec.}

% % \subsection{Optimization and Training Algorithm}

% % The LMM is trained by minimizing a loss function over the entire multimodal dataset. For classification tasks, the loss function \( \ell(\hat{y}, y) \) is typically the cross-entropy loss:

% % \begin{equation}
% % \ell_{\text{cross-entropy}} = -\sum_{c=1}^{C} y_c \log(\hat{y}_c),
% % \end{equation}

% % where \( C \) is the number of classes, \( y_c \) is the ground truth label, and \( \hat{y}_c \) is the predicted probability for class \( c \).

% % For regression tasks, the loss function can be the mean squared error (MSE):

% % \begin{equation}
% % \ell_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2,
% % \end{equation}

% % where \( n \) is the number of data points, \( y_i \) is the ground truth value, and \( \hat{y}_i \) is the predicted value.

% % \subsection{Backpropagation and Gradient Descent}

% % The LMM is trained using backpropagation to compute the gradients of the loss function with respect to the model parameters \( w \). The parameters are updated using gradient descent as follows:

% % \begin{equation}
% % w^{(t+1)} = w^{(t)} - \eta \nabla_w \ell(w^{(t)}),
% % \end{equation}

% % where \( \eta \) is the learning rate, and \( \nabla_w \ell(w^{(t)}) \) is the gradient of the loss function with respect to the model parameters at iteration \( t \).


% % The training of the LMM is performed by iteratively processing batches of multimodal data, computing the gradients, and updating the model parameters. The Algorithm \ref{algLMM} below describes the full process of training the LMM on a multimodal dataset.


% % \begin{algorithm}[H]
% % \caption{Training Large Multimodal Model (LMM)}
% % \label{algLMM}
% % \begin{algorithmic}[1]
% % \State \textbf{Input:} Multimodal dataset \( \mathcal{D} \), model parameters \( w \), learning rate \( \eta \)
% % \State Initialize model parameters \( w \)
% % \For{each epoch}
% %     \For{each batch of data \( (x, y) \) in \( \mathcal{D} \)}
% %         \State Compute modality-specific features: \( z_{\text{text}}, z_{\text{image}}, z_{\text{audio}}, z_{\text{sensor}} \)
% %         \State Fuse features to obtain \( z_{\text{fusion}} \)
% %         \State Compute prediction \( \hat{y} \) based on \( z_{\text{fusion}} \)
% %         \State Compute loss \( \ell(\hat{y}, y) \)
% %         \State Compute gradients \( \nabla_w \ell(w) \)
% %         \State Update model parameters \( w = w - \eta \nabla_w \ell(w) \)
% %     \EndFor
% % \EndFor
% % \State \textbf{Output:} Trained LMM parameters \( w \)
% % \end{algorithmic}
% % \end{algorithm}


% % The training algorithm begins by initializing the model parameters \( w \). The LMM is trained over multiple epochs, with each epoch involving the processing of multiple batches of multimodal data \( (x, y) \), where \( x \) is the input data (which can include text, images, audio, and sensor data) and \( y \) is the ground truth label.

% % For each batch of data:
% % \begin{itemize}
% %     \item The model first processes each modality through its respective modality-specific module (e.g., the text data is processed by a transformer model, image data by a CNN, etc.). The outputs of these modules are feature vectors \( z_{\text{text}}, z_{\text{image}}, z_{\text{audio}}, z_{\text{sensor}} \).
% %     \item The feature vectors from all modalities are then fused into a single vector \( z_{\text{fusion}} \) through a fusion technique such as concatenation or attention-based fusion.
% %     \item The fused feature vector is passed through a prediction layer to generate the prediction \( \hat{y} \).
% %     \item The loss function \( \ell(\hat{y}, y) \) is computed based on the difference between the predicted value \( \hat{y} \) and the ground truth \( y \).
% %     \item Using backpropagation, the gradients of the loss function with respect to the model parameters are computed. These gradients \( \nabla_w \ell(w) \) are then used to update the model parameters via gradient descent: \( w = w - \eta \nabla_w \ell(w) \).
% % \end{itemize}

% % The model is trained over several epochs until the loss function converges or reaches a satisfactory level of performance. At the end of the training process, the model parameters \( w \) are optimized to accurately process multimodal data and make predictions based on the learned features.

% % The LMM's ability to handle multimodal data is a key advantage in scenarios where edge devices collect different types of data (e.g., text, images, sensor data), and the model must integrate these diverse data streams to make predictions.


\section{Related Work}
\label{section2}
In this section, we examine the relevant research on the deployment of \textit{MLLMs}, focusing on FL and edge systems. We will also discuss related studies on the integration of \textit{MLLMs} with swarm intelligence.

\subsection{Deployment of MLLMs and LLMs}
\textit{MLLMs} and \textit{LLMs} are extensively used in the AI community for their ability to process enormous amounts of data. Their foundation on attention mechanisms enables them to efficiently handle long data sequences while preserving dependencies within the data. Notable \textit{LLMs} in use currently include BERT \cite{tang2023fusionai, mudvari2024splitllm, jiang2024low, liu2023differentially}, GPT and its variants \cite{tang2023fusionai, mudvari2024splitllm, kou2024pfedlvm, liu2023differentially}, CLIP \cite{atapour2024leveraging, zhang2024mllm}, and various LLAMA models \cite{dong2024fine, zhao2024llm, liu2023differentially, ye2024openfedllm, wu2024fedbiot, kuang2024federatedscope}, among others. However, challenges related to scalability and privacy remain major obstacles when deploying \textit{LLMs}, particularly \textit{MLLMs} \cite{krishnamoorthy2024integrating, kurkute2023scalable}. Federated learning is one of the emerging solutions to address this issue, as it allows \textit{MLLMs} to be deployed on edge devices that collaboratively train a shared model without exchanging their local data \cite{kurkute2023scalable,yao2024federated,ye2024openfedllm}. However, \textit{MLLMs} are known for their high memory and computational demands, making deployment particularly challenging on edge devices with limited consumer-level GPUs. 

Various approaches have been proposed to address the issue of computing resources, aiming to enhance the flexibility and reusability of \textit{MLLMs}/\textit{LLMs} while ensuring secure and private data handling. For instance, solutions like task scheduling or organizing machine learning tasks have been explored to facilitate this process \cite{tang2023fusionai}, along with solutions that utilize model compression techniques and Parameter-Efficient Fine-Tuning methods \cite{dong2024fine,jiang2024low,liu2023differentially,kim2023efficient,wu2024fedbiot,kuang2024federatedscope}.
%like LoRA (Low-Rank Adaptation) , smaller-sized adapters %. 
At the same time, other approaches have been proposed to avoid deploying resource-intensive models on edge devices and instead use alternatives that effectively leverage these powerful models in constrained environments. For example, in the context of federated learning systems, the authors in \cite{atapour2024leveraging} propose using Knowledge Distillation and a Prompt Generator to generates knowledge from the combined data of multiple IoT devices, allowing the model to be updated without the need for full deployment on edge devices, thereby keeping sensitive data private. Similarly, in \cite{zhang2024mllm}, the authors propose pretraining \textit{LLMs} on the server before fine-tuning them on edge devices. This approach aims to minimize both computational resource usage and memory consumption related to deploying large models on edge devices. In a related work \cite{mudvari2024splitllm}, the authors also suggest dividing the computational tasks of \textit{LLMs} between the server and edge devices to address the limited capabilities of these devices, while another work \cite{zhao2024llm} suggests distributing sensitive layers of the \textit{LLM} on client devices while offloading non-sensitive layers to the server.

The deployment of large models is not the sole issue, the communication between edge devices and the cloud server also presents challenges. To address the communication overhead, the authors in \cite{kou2024pfedlvm} suggest deploying the large visual model on the cloud server and allowing vehicles to share only the learned features instead of the entire model parameters. This strategy enables each vehicle to keep its training data local while simultaneously reducing communication demands.
Given the ongoing efforts to manage the deployment of \textit{MLLMs}/\textit{LLMs} in decentralized systems like federated learning, including approaches that involve server deployment or sharing layers and data between edge devices and servers, there is still a lack of effective strategies for edge device selection and communication optimization. Therefore, to address this gap, we propose deploying pre trained \textit{MLLMs} on edge devices, where the models are initially trained in the cloud and subsequently fine-tuned on the edge. 

While various approaches in the literature have explored supervised learning, unsupervised learning, and reinforcement learning to optimize federated learning processes \cite{mattoo2024device, eid2024federated, rjoub2021improving, muhammad2020deep, aradi2020survey, rjoub2024trust}, these methods face challenges in handling communication overhead, non-IID data, and the dynamic nature of the environment. Swarm intelligence, with its decentralized structure, ability to select devices based on available resources and data relevance, and capability to optimize communication between edge devices and the cloud, presents a promising solution to these issues. However, there is limited research on the application of swarm intelligence techniques for device selection in federated learning \cite{xing2023efficient, supriya2023particle}. In this paper, we propose leveraging swarm intelligence techniques, including \textit{PSO} and \textit{ACO}, to optimize the deployment strategy of MLLMs, focusing on improving the efficiency of model updates between the edge and the cloud, while considering the resource availability and data relevance of edge devices.

\subsection{MLLMs/LLMs and Swarm Intelligence}
Swarm intelligence is an emerging approach in the field of AI that leverages individual collaboration to achieve a common and overarching goal. In the context of \textit{LLMs}, swarm intelligence is increasingly being used to optimize the performance of these models through multi-\textit{LLM} collaboration. For instance, the authors of this work \cite{feng2024model} have developed a swarm model inspired by \textit{PSO} that enables multiple \textit{LLMs} to work together. They explore the weight space based on successful training checkpoints to enhance performance and make the models easily adaptable to various tasks.
In a different context, the authors in \cite{han2024swarm} have developed an approach based on swarm intelligence and visual question answering mechanisms to enable multiple Large Vision-Language Models (LVLMs) to collaborate on the geo-localization task. This solution allows for linking images with specific geographic locations without requiring a large database of geo-tagged images. At the same time, it leverages the network retrieval capabilities of multiple models, allowing them to collaborate and share knowledge effectively.
Given the extensive variety of swarm intelligence algorithms, practitioners often find it difficult to select the most suitable method for specific tasks, such as designing new metaheuristic algorithms \cite{van2024llamea,pluhacek2023leveraging}. To address this challenge, the authors in \cite{pluhacek2023leveraging} propose a study that uses a \textit{LLM} like GPT-4 to assist in the design process of novel metaheuristics through the hybridization of various swarm intelligence algorithms. However, the authors also highlight several challenges associated with the use of \textit{LLMs}, including their inability to consistently produce accurate and reliable outputs, necessitating human intervention. Additionally, they address the ethical and social concerns that arise from the application of these models.

From our literature review, we found examples of using swarm intelligence to enhance the collaborative use of \textit{LLMs}, making them more adaptable to various tasks or improving their efficiency for specific applications. Additionally, there are instances where \textit{LLMs} are employed to optimize the performance of swarm intelligence algorithms, such as in the design of metaheuristics. However, we noted a lack of research focused on applying swarm intelligence to improve the deployment of \textit{LLMs}, as well as a notable scarcity of studies addressing \textit{MLLMs}. Therefore, in this paper, we propose utilizing hybrid swarm intelligence, combining \textit{PSO} and \textit{ACO}, to optimize the deployment of \textit{MLLMs} specifically in federated learning systems within smart edge-cloud computing environments. Our approach aims to facilitate better resource allocation, improve communication between models, and optimize the overall performance of \textit{MLLMs} in dynamic environments.



\section{Proposed Framework} 
\label{section3}

In this section, we present the proposed hybrid \textit{PSO-ACO} framework, designed to optimize the deployment of \textit{MLLMs} in \textit{FL} environments within smart edge-cloud systems. The framework addresses three critical optimization tasks: \textbf{(1)} selecting the most suitable subset of edge devices for participation in FL rounds, \textbf{(2)} minimizing the communication overhead by optimizing the paths through which model updates are transmitted to cloud servers, and \textbf{(3)} handling the Non-IID nature of data across devices. The model leverages \textit{PSO} for edge device selection and \textit{ACO} for communication path optimization.

\subsection{Overview of the Hybrid \textit{PSO-ACO} Framework}

Swarm intelligence is an evolving approach that leverages individual collaboration of agents to achieve a common goal. In the context of \textit{MLLMs}, swarm intelligence has begun to play a significant role in optimizing \textit{MLLM} performance through collaborative strategies. In this work, a hybrid framework based on two swarm intelligence techniques, \textit{PSO} and \textit{ACO}, is proposed to optimize the deployment of \textit{MLLMs} in FL edge-cloud environments. This hybrid approach is organized into three main stages: 

\begin{itemize} 
    \item \textbf{Edge Device Selection via \textit{PSO}}: In each FL round, \textit{PSO} selects a subset of edge devices, each equipped with a pretrained \textit{MLLM}, based on their available resources, data relevance, and energy constraints. This ensures that only devices capable of contributing meaningfully to the global model while minimizing energy consumption are chosen and mitigate the impact of Non-IID data distributions across devices.
      
    \item \textbf{Communication Path Optimization via \textit{ACO}}: Following device selection, each chosen device fine-tunes its corresponding \textit{MLLM}. Then, \textit{ACO} is used to optimize the communication of these fine-tuned model updates between the selected edge devices and the cloud, aiming to minimize communication latency and bandwidth usage to ensure efficient transmission.
    
    \item \textbf{Global Model Update}: The aggregated updates of the fine-tuned \textit{MLLMs} from all selected devices are sent to the cloud through optimized communication paths to update the global \textit{MLLM}.
\end{itemize}

Each stage is essential for ensuring the efficient deployment of \textit{MLLMs} in resource-constrained, distributed edge-cloud environments.
The proposed solution, outlining the three stages, is shown in Fig.  \ref{fig:uvs_architecture}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{LMM_PSO_ACO.drawio_crop.pdf}
    \caption{Overview of the Proposed Framework for Optimizing \textit{MLLM} Deployment Using Swarm Intelligence Techniques \textit{PSO} and \textit{ACO}}
    \label{fig:uvs_architecture}
\end{figure}

\subsection{\textit{PSO} for Edge Device Selection} 

\textit{PSO} is a population-based stochastic optimization technique inspired by the social behavior of birds flocking or fish schooling. In \textit{PSO}, a group of particles (potential solutions) explores the search space by adjusting their positions based on their own experience and that of neighboring particles. Each particle has a position and a velocity, which are iteratively updated to find the optimal solution. The updates are influenced by the particle's best-known position and the global best position found by the entire swarm, leading the swarm to converge towards optimal or near-optimal solutions.

The first step in the proposed framework is to select the optimal subset of edge devices using \textit{PSO}. The goal is to minimize energy consumption and ensure that selected devices contribute valuable local updates to the global model.
Each particle in the \textit{PSO} algorithm represents a potential solution, i.e., a subset of devices \( S \subseteq D \). The fitness function is designed to minimize the energy consumption \( E_i \) of the selected devices while maximizing the relevance of their local data for \textit{MLLM} fine-tuning. The fitness function is defined as follows:
 \begin{equation}
   \text{Fitness}(S) = \alpha \sum_{i \in S} E_i + \beta \sum_{i \in S} (1 - R_i) + \gamma \sum_{i \in S} D_i
   \label{eq:fitness}
   \end{equation}


\noindent where \( E_i \) represents the energy consumption of device \( i \), \( R_i \) denotes the relevance of the local data to the global model, and \( D_i \) measures the data diversity contributed by device \( i \). The weighting factors \( \alpha \), \( \beta \), and \( \gamma \) balance the trade-offs between energy efficiency, data relevance, and diversity, respectively. The velocity and position of each particle are updated based on the standard \textit{PSO} update equations:

\begin{equation}
v_i(t+1) = \omega v_i(t) + c_1 r_1 (p_i - x_i(t)) + c_2 r_2 (g_i - x_i(t))
\label{eq8}
\end{equation}

\begin{equation}
x_i(t+1) = x_i(t) + v_i(t+1)
\label{eq9}
\end{equation}

\noindent where \( v_i(t) \) is the velocity of particle \( i \) at time $t$, \( x_i(t) \) is its current position of particle \( i \), \( p_i \) is its personal best position for particle \( i \), \( g_i \) is the global best position of particle \( i \), $\omega$ is the inertia weight controlling the influence of the previous velocity, \( c_1, c_2 \) are acceleration coefficients, and \( r_1, r_2 \)  are random values between $0$ and $1$. The algorithm converges to an optimal subset of devices that can participate in the FL round.

Algorithm \ref{algpso} applies \textit{PSO} to select the most appropriate subset of edge devices for participation in a distributed task. The selection is based on both energy consumption and the relevance of the data on each device. By optimizing for these factors, the algorithm ensures that only the most suitable edge devices are selected to participate in a given task, balancing energy efficiency with task relevance.


\begin{algorithm}[H]
\caption{\textit{PSO} for Edge Device Selection}
\label{algpso}
\begin{algorithmic}[1]
\State \textbf{Input:} Set of devices \( D \), energy thresholds \( E_{\text{threshold}} \), relevance metrics \( R_i \)
\State \textbf{Output:} Selected subset of devices \( S \)
\State Initialize particles representing different subsets of devices
\State Initialize velocities and positions of particles
\For{each iteration}
    \For{each particle}
        \State Calculate fitness of each particle based on energy and data relevance
        \State Update particle velocity and position using Equations (\ref{eq8}) and (\ref{eq9})
    \EndFor
    \State Update the global best particle
\EndFor
\State Return the optimal subset of devices \( S \)
\end{algorithmic}
\end{algorithm}


\subsection{\textit{ACO} for Communication Path Optimization}

\textit{ACO} is a nature-inspired metaheuristic algorithm that mimics the foraging behavior of ants to solve complex optimization problems, particularly in finding optimal paths. In nature, ants deposit a chemical substance called pheromone along their paths while searching for food. Other ants tend to follow paths with higher pheromone concentrations, reinforcing those paths over time, which eventually leads to the discovery of the shortest or most efficient route to the food source.

Once the optimal subset of edge devices is selected, \textit{ACO} is applied to optimize the communication paths between the selected edge devices and the cloud server. The primary objective is to minimize both communication latency and bandwidth usage, ensuring efficient transmission of model updates in a distributed edge-cloud environment.

In the \textit{ACO} algorithm, each ant represents a potential communication path between an edge device and the cloud server. The decision-making process for each ant is influenced by two key factors:
(1). Pheromone levels \( \tau_{ij} \): A higher pheromone level indicates a better communication path based on previous ants’ experiences; 
(2). Heuristic information \( \eta_{ij} \): This represents problem-specific knowledge such as path distance or bandwidth, guiding the ants towards more efficient paths.

The probability \( P_{ij} \) of an ant selecting edge \( (i,j) \) is computed using a combination of the pheromone levels and heuristic information:

\begin{equation}
P_{ij} = \frac{\tau_{ij}^\alpha \cdot \eta_{ij}^\beta}{\sum_{k \in \text{neighbors}} \tau_{ik}^\alpha \cdot \eta_{ik}^\beta}
\end{equation}

\noindent where \( \tau_{ij} \) is the pheromone level on edge \( (i,j) \), \( \eta_{ij} \) is the heuristic information, such as bandwidth or distance on edge \( (i,j) \), \( \alpha \) and \( \beta \) are parameters that control the influence of the pheromone level and heuristic information, respectively.

The heuristic information \( \eta_{ij} \) can be defined based on the characteristics of the communication path: if distance or latency is the critical factor, \( \eta_{ij} \) is inversely proportional to the distance \( d_{ij} \) between nodes \( i \) and \( j \):
  \[
  \eta_{ij} = \frac{1}{d_{ij}}
  \]
where shorter distances or lower latencies are preferred.
  

Alternatively, if bandwidth is the primary factor for path selection, the heuristic information \( \eta_{ij} \) can be expressed in terms of the available bandwidth \( B_{ij} \) on the communication link between nodes \( i \) and \( j \). In this case, \( B_{ij} \) refers to the maximum data transmission capacity of the link, typically measured in megabits per second (Mbps) or gigabits per second (Gbps). To model the influence of bandwidth on path selection, the heuristic information is directly proportional to the available bandwidth:

\[
\eta_{ij} = \frac{1}{B_{ij}}
\]

\noindent where \( B_{ij} \) is the available bandwidth on the communication link between nodes \( i \) and \( j \), paths with smaller values of \( B_{ij} \) (i.e., lower bandwidth) will result in larger \( \eta_{ij} \), making those paths less likely to be chosen by the ants.

This inverse relationship ensures that paths with higher bandwidth are assigned lower heuristic values \( \eta_{ij} \), thereby encouraging the selection of paths that can support greater data transmission rates. The incorporation of bandwidth into the heuristic information guides the ants towards paths that are more efficient for communication, improving overall network throughput and reducing congestion.


The pheromone levels \( \tau_{ij} \) are updated after each iteration to reinforce better communication paths. The update equation for \( \tau_{ij} \) is:

\begin{equation}
\tau_{ij}(t+1) = (1-\rho) \tau_{ij}(t) + \Delta \tau_{ij}
\end{equation}

\noindent where \( \rho \) is the pheromone evaporation rate, which ensures that previously chosen paths are not overly reinforced, allowing the algorithm to explore new paths.
\( \Delta \tau_{ij} \) is the pheromone deposit, which is proportional to the quality of the path based on factors such as latency or bandwidth.


\begin{algorithm}[H]
\caption{\textit{ACO} for Communication Path Optimization}
\label{algaco}
\begin{algorithmic}[1]
\State \textbf{Input:} Selected devices \( S \), pheromone levels \( \tau_{ij} \), heuristic \( \eta_{ij} \)
\State \textbf{Output:} Optimized communication paths
\For{each ant}
    \State Construct a path from device \( i \) to the cloud using probability \( P_{ij} \)
    \State Evaluate the path based on bandwidth usage and latency
    \State Update pheromone levels on path edges based on path quality
\EndFor
\State Return the optimal communication paths
\end{algorithmic}
\end{algorithm}

The \textit{ACO} algorithm (Algorithm \ref{algaco}) iterates through a number of devices, each exploring different communication paths between the selected edge devices and the cloud. Paths that offer higher bandwidth and lower latency are favored, and over successive iterations, the devices collectively converge on the most efficient paths. This leads to an optimized communication network that minimizes both bandwidth usage and latency, ensuring that model updates are transmitted efficiently and effectively.

\subsection{Global Model Aggregation and Update}

After local model fine-tuning is completed on the selected edge devices, the optimized communication paths are used to transmit the local updates to the cloud. The cloud server aggregates the local updates to form the global model. The aggregation process is defined as follows:

\begin{equation}
w_{\text{global}} = \sum_{i \in S} \frac{n_i}{n} w_i
\label{eq12}
\end{equation}

\noindent where \( w_i \) is the local model update from device \( i \), \( n_i \) is the number of data samples on device \( i \), and \( n \) is the total number of data samples across all devices. The global model is then updated, and the process repeats for subsequent FL rounds.


\section{Experimental Setup and Evaluation}
\label{results}

This section presents the experiments carried out to assess the proposed hybrid framework, including a detailed description and analysis of the results obtained. For the purposes of this work, the Unmanned Vehicle System (UVS) was selected as the use case.

\subsection{Use Case Scenario \& operational flow}

UVSs are autonomous robotic vehicles that function without the need for onboard human control. These systems are capable of performing various tasks, such as navigation and surveillance, in both terrestrial and aerial domains, and have seen recently significant progress in terms of technological development \cite{dong2024communication}. 
A typical UVS is equipped with several sensors that collect diverse types of data, such as images, LIDAR, audio, IMU, and GPS data. These sensors may consist of:
    \begin{itemize}
        \item \textit{Cameras}: Capture real-time images and videos used for obstacle detection and lane following.
        \item \textit{LIDAR Sensors}: Generate 3D maps for object distance measurement and spatial awareness.
        \item \textit{Microphones}: Collect environmental audio cues and voice commands.
        \item \textit{IMU}: Tracks the vehicle’s orientation, speed, and acceleration.
        \item \textit{GPS}: Provides precise geolocation data for navigation and route planning.
    \end{itemize}

This variety of sensors generates large quantities of data, which require sophisticated models for the processing of extensive, real-time multimodal data. To enable optimized resource selection and efficient communication for decision-making in the UVS, a well-designed processing architecture is essential. In this context, \textit{MLLM} is particularly well-suited due to its robustness and advanced capabilities.

As discussed in Section \ref{Background}, \textit{MLLMs} integrate different data types through a language-centric framework that seamlessly incorporates other modalities, such as images, audio, and structured sensor data. This integration enables \textit{MLLMs} to generate responses and make decisions based on complex multimodal inputs, like those provided by UAVs. By leveraging these advanced architectures, UVSs can achieve real-time processing and efficient data integration, optimizing decision-making for tasks such as navigation, object recognition, and environmental awareness.

%A comprehensive use case scenario has been depicted for an \textit{UVS} that integrates various cutting-edge technologies, including multimodal data processing using a \textit{MLLM}, \textit{Swarm Intelligence} (PSO and ACO), and \textit{Edge-Cloud Computing} in the IoT environment, \textcolor{red}{figure [ ]}. This architecture requires real-time data processing, optimized resource selection, and efficient communication for decision-making in the UVS \cite{usecase_00}.
% \subsection{ of the UVS}

% The operational flow of the UVS begins with the collection of multimodal data from the various sensors. This data is then processed in parallel by the respective modules (Transformer-based vision and language models, Point Cloud, IMU Neural Networks), and the resulting features are fused in the Feature Fusion Layer of the MLLM. Next, the PSO algorithm selects the edge devices that are most suitable for real-time data processing based on their computational resources and energy levels. Then, ACO optimizes the communication paths between the selected edge devices and the cloud, minimizing latency and bandwidth usage. Finally, the fused data is used for real-time decision-making, such as obstacle avoidance, lane following, and route planning. The UVS continuously communicates with the cloud for global model updates and enhanced decision-making capabilities.

In the \textit{MLLM}-driven operational flow of the UVS, the process, as presented in Section \ref{Background}, starts with the collection of multimodal data from various sensors. This multimodal data is then independently processed by specialized modules within the \textit{MLLM} framework: transformer-based models handle vision and language data, while point cloud and IMU data are processed by dedicated neural networks \cite{usecase_01}. Each module extracts essential features from its modality, which are then fused in the \textit{MLLM}’s Feature Fusion Layer to create a comprehensive, unified representation of the environment \cite{usecase_multimodal}. 

In this scenario, the UVS functions within a hybrid edge-cloud computing framework where the local data processing occurs on the edge devices, while global model updates and more complex computations are handled in the cloud. 
To optimize the deployment of \textit{MLLM} on edge devices, \textit{PSO} is employed to select the most appropriate devices based on factors such as resource availability (e.g., battery life, computational capacity) and data relevance. This ensures that the most capable devices contribute to the real-time processing tasks of the UVS.
Once the devices are selected, the fine-tuned \textit{MLLM} models are transferred to the cloud server. At this stage, \textit{ACO} is used to optimize the communication paths between the selected edge devices and the cloud. The objective is to minimize communication latency and bandwidth consumption, ensuring the efficient transmission of model updates and real-time data.\cite{usecase_ACO,usecase_01, usecase_02}.
Following this architecture, the fused multimodal representation, processed and distributed across optimal pathways, enables the \textit{MLLM} to support real-time decision-making tasks directly at the edge such as obstacle avoidance, lane following, and route planning. Continuous communication with the cloud allows the \textit{MLLM} to access global model updates, which further enhances the UVS’s ability to make accurate and adaptive decisions in dynamic environments \cite{usecase_03,usecase_edge}.
The architecture of the use case solution is illustrated in Fig. \ref{fig:uvs_architecture1}.

%\begin{figure}[h!]
%    \centering
%    \includegraphics{UVS.drawio.pdf}
%    \caption{The architecture of the use case solution}
%    \label{fig:uvs_architecture1}
%\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth, height=.53\textwidth ]{UVS.pdf}
    \caption{The Architecture of the Use Case Solution}
    \label{fig:uvs_architecture1}
\end{figure}

%\subsection{System Architecture Overview}

%In the IoT environment, the UVS is equipped with several sensors that collect diverse types of data, such as images, LIDAR, audio, IMU, and GPS data. These sensors provide real-time inputs to the multimodal data processing framework. The key components of the system architecture are as follows \cite{usecase_UAV_sensor}.
% The architecture of the system is illustrated in Figure \ref{fig:uvs_architecture}
%\begin{itemize}
    %\item \textbf{Sensors}: 
    %\begin{itemize}
     %   \item \textit{Cameras}: Capture real-time images and videos used for obstacle detection and lane following.
     %   \item \textit{LIDAR Sensors}: Generate 3D maps for object distance measurement and spatial awareness.
     %   \item \textit{Microphones}: Collect environmental audio cues and voice commands.
     %   \item \textit{IMU}: Tracks the vehicle’s orientation, speed, and acceleration.
     %   \item \textit{GPS}: Provides precise geolocation data for navigation and route planning.
    %\end{itemize}
    %     \item \textbf{Edge-Cloud Integration with Swarm Intelligence}: 
%     The UVS operates in a hybrid edge-cloud environment. Local data processing occurs on edge devices, while complex computations and global model updates happen in the cloud.
% % \end{itemize}
%     \item \textbf{Multimodal Data Processing}: 
%     The sensor data is fed into modality-specific processing modules. Each module is tailored to process a specific type of input.
    % \begin{itemize}
    %     \item \textit{Image Processing (CNN)}: Processes camera data for obstacle detection and lane following.
    %     \textcolor{blue}{Here is the place where we should use transformer-based models, like TNT, SWIN and DETR so on for object detection purposes}
    %     \item \textit{Point Cloud Processing (LIDAR)}: Generates 3D maps for environment perception.
    %     \item \textit{Audio Processing (RNN)}: Interprets audio inputs from microphones for detecting environmental sounds or voice commands.
    %     \textcolor{blue}{Here is the place where we should use transformer-based models, like Conformer, speech transformer or whisper and so on}
    %     \item \textit{IMU Neural Network}: Processes IMU sensor data to track the vehicle’s speed and orientation.
    % \end{itemize}
    
    %\item \textbf{Multimodal Data Processing}: Processes sensor data using modality-specific modules, tailored to handle each data type optimally.The outputs of these modality-specific modules are fused in the \textit{Feature Fusion Layer} to create a comprehensive understanding of the environment \cite{usecase_multimodal}.

 %   \item \textbf{Edge-Cloud Computing Integration}: 
 %   The UVS operates in a hybrid edge-cloud computing environment. Local data processing takes place on the edge devices, while global model updates and more complex computations occur in the cloud. The UVS continuously communicates with both the edge devices and the cloud to ensure timely decision-making and updates \cite{usecase_edge}.

%    \item \textbf{PSO for Edge Device Selection}: 
%    The \textit{PSO} algorithm is used to select the most suitable edge devices based on their resource availability (e.g., battery life, computational power) and data relevance. This ensures that the most capable devices contribute to the UVS's real-time processing tasks \cite{usecase_PSO}.

%    \item \textbf{ACO for Communication Path Optimization}: 
%    \textit{ACO} optimizes the communication paths between the selected edge devices and the cloud server. The goal is to minimize communication latency and bandwidth usage, ensuring that model updates and real-time data are transmitted efficiently \cite{usecase_ACO}.

    %\item \textbf{Final Decision-Making}: 
    %The processed multimodal data is used to make real-time decisions, such as obstacle avoidance, route planning, and emergency maneuvers, ensuring the safe and efficient operation of the UVS \cite{usecase_final}.
%\end{itemize}

\subsection{Simulation Environment}

To evaluate the effectiveness of the UVS model, we designed a simulation environment that closely mimics real-world urban scenarios. This environment includes multiple edge devices with diverse computational capacities and battery levels, reflecting the heterogeneous nature of real-world IoT networks. Additionally, the cloud infrastructure was simulated to assess the edge-cloud communication efficiency under various conditions.
The Key parameters of the simulation environment are as follows:
\begin{itemize}
    \item \textbf{Number of Edge Devices:} $50$ to $200$ devices, simulating onboard and nearby devices in a vehicular network.
    \item \textbf{Edge Device Capabilities:} Devices have varying levels of processing power (CPU, GPU) and battery life.
    \item \textbf{Cloud Infrastructure:} A centralized cloud server for model updates and complex computations.
    \item \textbf{Communication Range:} $100$ to $500$ meters between edge devices, depending on the simulation scenario.
    \item \textbf{Bandwidth:} Varying bandwidths between $10$ and $100$ Mbps for different communication channels.
\end{itemize}

\subsubsection{Datasets: Multimodal Data Sources}

The \textit{Leddar PixSet} dataset \footnote{https://leddartech.com/solutions/leddar-pixset-dataset/} is used for LIDAR, camera, radar, IMU, and GPS data. Since the dataset does not provide audio data, we complement it with the UrbanSound8K dataset \footnote{https://urbansounddataset.weebly.com/urbansound8k.html}, which includes high-quality audio recordings. This combination of datasets enables full multimodal testing of the UVS system.

\begin{itemize}
    \item \textbf{Leddar PixSet Dataset:} This dataset contains the following information:
    \begin{itemize}
        \item \textbf{LIDAR:} Captures 3D point cloud data for object detection and spatial mapping.
        \item \textbf{Cameras:} Provides RGB images for obstacle detection, lane tracking, and object classification.
        \item \textbf{Radar:} Adds depth perception and robustness in poor weather conditions.
        \item \textbf{IMU:} Provides inertial data to estimate orientation, acceleration, and vehicle dynamics.
        \item \textbf{GPS:} Supplies precise geolocation data for real-time navigation and path planning.
    \end{itemize}

    \item \textbf{UrbanSound8K Dataset:} This dataset provides labeled sound excerpts for common urban sounds, including vehicle horns, engine noises, emergency sirens, and street noise. It is ideal for testing environmental sound recognition within the UVS.
\end{itemize}

The Leddar PixSet dataset offers over $29,000$ frames and $97$ sequences in urban traffic scenarios, while the UrbanSound8K dataset provides thousands of real-world urban sound recordings, allowing for the evaluation of the UVS system across both visual, spatial, inertial, and auditory modalities.


\subsubsection{Baseline Methods}

%\begin{table}[H]
%  \centering
%  \begin{tabular}{|p{4.0cm}|p{6.0cm}|}
%\hline
%\rowcolor{red!30}
%   LMM/LLM & References \\ \hline

% BERT & \cite{tang2023fusionai,mudvari2024splitllm,jiang2024low,liu2023differentially}  \\
%\hline

% GPT \& Variants & \cite{tang2023fusionai,mudvari2024splitllm,kou2024pfedlvm,liu2023differentially}  \\
%\hline

% CLIP & \cite{atapour2024leveraging,zhang2024mllm}  \\
%\hline

% LLAMA \& Variants & \cite{dong2024fine,zhao2024llm,liu2023differentially,ye2024openfedllm,wu2024fedbiot,kuang2024federatedscope}  \\
%\hline

% LongFormer & \cite{zhao2024llm}  \\
%\hline

% ChatGLM & \cite{liu2023differentially}  \\
%\hline

 %ViT & \cite{kim2023efficient}  \\
%\hline


%\end{tabular}
%  \caption{Overview LMM/LLM Applications in Federated Learning in the Literature}
\label{tab:video_summarization}
%\end{table}

To evaluate the effectiveness of the proposed \textit{UVS} model, we compare its performance against several existing models and algorithms in the field of autonomous vehicle systems and resource optimization. These comparisons will show how our model, which integrates \textit{MLLM}, \textit{Swarm Intelligence (PSO and ACO)}, and \textit{Edge-Cloud Integration}, performs better in terms of accuracy, energy efficiency, latency, and bandwidth usage.

\begin{itemize}
    \item \textbf{Conventional Rule-Based System:}
    Traditional rule-based systems use predefined rules for decision-making based on sensor data. While these systems are efficient, they lack the adaptability and learning capabilities of models like our UVS, which leverages data-driven decision-making through machine learning and Swarm Intelligence. Comparing against rule-based systems highlights the advantages of real-time learning and dynamic optimization in our model.

    \item \textbf{Deep Learning-Based Autonomous Driving System (\textit{CNN-LSTM}):}
    A common approach in autonomous driving systems is to use deep learning, particularly \textit{Convolutional Neural Networks (CNNs)} for image processing and \textit{Long Short-Term Memory (LSTM)} networks for temporal sequence prediction. These models are primarily focused on visual data, making them limited in multimodal environments. By comparing our UVS model, which integrates LIDAR, GPS, IMU, and audio with image data, we demonstrate improvements in accuracy and robustness \cite{eid2024federated,rjoub2021improving,muhammad2020deep}. 

    \item \textbf{Deep Reinforcement Learning (DRL) for Autonomous Vehicles:}
    Deep Reinforcement Learning models, such as \textit{Deep Q-learning (DQN)}\cite{mnih2013playing} or \textit{Proximal Policy Optimization (PPO)} \cite{schulman2017proximal}, are commonly used for decision-making in autonomous systems. However, they typically focus on visual and spatial data without optimizing resource usage. In contrast, our UVS model integrates Swarm Intelligence (\textit{PSO} and \textit{ACO}) to reduce energy consumption and communication latency, providing a significant improvement in real-time decision-making and efficiency \cite{aradi2020survey,rjoub2024trust}. 

%    \item \textbf{FL-Based Models:}FL enables decentralized model training across edge devices, helping preserve privacy and reduce bandwidth usage. However, FL-based models often face challenges related to communication latency and uneven data quality across devices \cite{rjoub2020trust,wahab2022federated}. Our UVS model addresses these challenges by using \textit{PSO} to select the most relevant edge devices and \textit{ACO} to optimize communication paths, reducing overall latency and energy consumption while improving accuracy.

%    \item \textbf{LiDAR-Centric Models (PointNet/PointRCNN):}These models specialize in processing 3D point cloud data from LIDAR for object detection and classification (e.g., PointNet, PointRCNN). While these models perform well with spatial data, they lack the ability to incorporate other modalities like images or GPS \cite{rjoub2022one,lee2024lidar}. Comparing these models with our UVS model highlights how multimodal data fusion improves performance in complex environments by offering a richer understanding of the surroundings.
    
%   \item \textbf{Hybrid Swarm Intelligence Models (\textit{PSO} and \textit{ACO} in Distributed Systems):} Hybrid Swarm Intelligence models that combine \textit{PSO} and \textit{ACO} are used in distributed systems to optimize resource allocation and communication. These models typically focus on optimizing network resources but do not integrate multimodal data fusion or edge-cloud processing \cite{zhang2024novel,rjoub2017cloud}. Our UVS model extends these capabilities by incorporating multimodal data processing and real-time decision-making, demonstrating improvements in overall system performance.\


\end{itemize}

The comparison of these baseline models with our proposed UVS model emphasizes how the integration of \textit{MLLM}, \textit{Swarm Intelligence (PSO and ACO)}, and \textit{Edge-Cloud Integration} results in superior performance in terms of accuracy, energy efficiency, latency, and bandwidth usage.

\subsection{Simulation Results}

In This section, we present the performance metrics of various models, including Traditional Rule-Based, \textit{CNN-LSTM}, \textit{DRL}, and the proposed \textit{PSO-ACO} framework. The metrics evaluated include Accuracy, Precision, Recall, and F1 Score, providing a comprehensive comparison of how each model performs under the same experimental conditions.

\begin{figure}[htbp]
    %\centering
    \hspace{-1.05cm}
    \includegraphics[width=1.1\textwidth, height=0.45\textheight]{Picture1.jpg}
    \caption{Performance Metrics Comparison for Different Models}
    \label{fig:comparison}
\end{figure}

In Fig. \ref{fig:comparison}, we present the performance metrics of various models, including Traditional Rule-Based, CNN-LSTM, DRL, and the proposed PSO-ACO framework. The metrics evaluated include Accuracy, Precision, Recall, and F1 Score, providing a comprehensive comparison of how each model performs under the same experimental conditions.

From the figure, we observe that the proposed PSO-ACO framework consistently outperforms the other models across all metrics. The PSO-ACO framework achieves the highest accuracy of $92\%$, which reflects its ability to make highly correct decisions in the simulated environment. This high level of accuracy is a direct result of the hybrid optimization of PSO and ACO, which allows for the efficient selection of parameters and adaptation to changing conditions. The accuracy also demonstrates the framework's capability to generalize effectively across diverse scenarios, ensuring that the decisions made are consistently optimal. This is particularly important in environments with high complexity, where maintaining a high level of correctness is crucial for system reliability. The high accuracy is complemented by precision and recall values nearing $90\%$, indicating both precise and consistent correct predictions. The F1 Score of PSO-ACO further validates its balanced performance in terms of precision and recall, reaching $91\%$.
The DRL model also demonstrates strong performance, with accuracy, precision, recall, and F1 Score values slightly below those of PSO-ACO. Specifically, the DRL model achieves an accuracy of $85\%$ and an F1 Score of $84\%$. This highlights its capacity for adaptive learning, but the lack of hybrid optimization techniques limits its ability to reach the same level of efficiency as PSO-ACO. The DRL's accuracy indicates that it can learn from the environment and make correct decisions, but it lacks the swarm intelligence mechanism that further optimizes decision-making as seen in PSO-ACO.
The CNN-LSTM model shows moderate performance across all metrics, achieving an accuracy of $80\%$, with similar precision and recall values. This suggests that while the CNN-LSTM model is capable of handling complex data patterns, particularly temporal dependencies, it lacks the optimization techniques and adaptability found in DRL and PSO-ACO, resulting in lower overall performance. The accuracy achieved by CNN-LSTM indicates that it can learn from the data, but its performance plateaus earlier compared to PSO-ACO and DRL, highlighting its limitations in effectively scaling with the complexity of the environment.

The Traditional Rule-Based model, however, exhibits the lowest performance across all metrics, with an accuracy of $66\%$ and an F1 Score of $62\%$. This indicates that fixed-rule systems struggle in dynamic environments where adaptability and learning are crucial for optimal decision-making. The comparatively lower accuracy suggests that the Traditional Rule-Based model is unable to effectively adapt to new or changing conditions, leading to consistently incorrect decisions. The lower F1 Score highlights limitations in both precision and recall, emphasizing the need for more advanced approaches like PSO-ACO and DRL to achieve higher performance in complex environments.
In summary, the proposed PSO-ACO framework demonstrates superior performance across all key metrics, highlighting its effectiveness in optimizing decision-making through the integration of swarm intelligence and edge-cloud collaboration. The DRL and CNN-LSTM models also provide reasonable performance but are outclassed by the hybrid approach of PSO-ACO. The Traditional Rule-Based model, while functional, lacks the adaptability required to achieve competitive performance in this context.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth, height=0.4\textheight]{Picture2.jpg}
    \caption{Convergence Analysis (Training Loss over Epochs)}
    \label{fig:Loss}
\end{figure}

In Fig. \ref{fig:Loss}, we examine the convergence behavior of different models over $20$ epochs by plotting the training loss during each epoch. The models compared include Traditional Rule-Based, \textit{CNN-LSTM}, \textit{DRL}, and the proposed \textit{PSO-ACO} framework. Training loss here represents the degree of error between predicted and actual outcomes, which decreases as the model learns from the data. From the figure, we observe that the proposed \textit{PSO-ACO} framework achieves the fastest convergence, with its training loss declining sharply in the initial epochs and eventually stabilizing at a low level near $0.2$. This rapid decline indicates that the \textit{PSO-ACO} effectively optimizes its parameters early on, allowing it to minimize errors efficiently and converge to a solution that generalizes well.
Alternatively, the \textit{DRL} model shows a competitive convergence rate, with training loss reaching values slightly above that of \textit{PSO-ACO}. This indicates that \textit{DRL} can effectively learn from the environment, though not as efficiently as the hybrid approach of \textit{PSO-ACO}.
Regarding the \textit{CNN-LSTM} model, it shows moderate convergence, with its training loss stabilizing at around $0.4$. This implies that although it can reduce error, it takes longer to reach a stable state, potentially due to the complexity of learning temporal patterns.
In contrast, the Traditional Rule-Based model, on the other hand, exhibits the slowest convergence and highest final loss, plateauing at around $0.5$. This suggests that without the ability to adaptively learn from data, rule-based systems struggle to optimize effectively, leading to consistently higher error rates compared to the learning-based models.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth, height=0.38\textheight]{Picture3.jpg}
    \caption{Communication Cost Reduction over Rounds}
    \label{fig:Cost}
\end{figure}

In Fig. \ref{fig:Cost}, we illustrate the communication cost reduction achieved by different models over $20$ communication rounds. The models compared are Traditional Rule-Based, \textit{CNN-LSTM}, \textit{DRL}, and the proposed \textit{PSO-ACO} framework. Communication cost here refers to the data exchanged between clients and the central server during each round, a critical metric in evaluating the efficiency of federated learning approaches. From the figure, we observe that the proposed \textit{PSO-ACO} framework achieves the most significant reduction in communication cost over time. Starting from an initial cost of $100$ MB per client, the \textit{PSO-ACO} framework reduces the communication overhead to around $40$ MB by the $20$th round. This highlights the efficiency of \textit{PSO-ACO} in optimizing the communication paths and selecting the most suitable clients, which minimizes redundant data exchanges.
The \textit{DRL} model also demonstrates a notable reduction in communication cost, stabilizing at approximately $72$ MB by the $20$th round. While effective, the \textit{DRL} approach is not as efficient as the hybrid \textit{PSO-ACO}, indicating that integrating swarm intelligence further enhances the communication optimization.
In contrast, the \textit{CNN-LSTM} model shows moderate performance, reducing the communication cost to around $86$ MB. This suggests that while the model can learn to improve communication efficiency, it lacks the sophisticated optimization mechanisms present in \textit{PSO-ACO} and \textit{DRL}.
The Traditional Rule-Based model, however, shows the least reduction in communication cost, remaining close to its initial value of $100$ MB throughout the rounds. This is expected, as rule-based systems lack the adaptive capabilities required to minimize communication efficiently in a dynamic federated environment.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth, height=0.45\textheight ]{Picture4.jpg}
    \caption{Impact of Non-IID Data Distribution on Model Accuracy}
    \label{fig:noniid}
\end{figure}

In Fig. \ref{fig:noniid}, we analyze the impact of non-IID data distribution on the model accuracy for various approaches, including Traditional Rule-Based, \textit{CNN-LSTM}, 	\textit{DRL}, and the proposed 	\textit{PSO-ACO} framework. In this context, accuracy represents the model's ability to make correct predictions despite the inconsistencies in data distribution across clients, which is a critical aspect in federated learning environments with highly diverse data. Non-IID data refers to data that is not independently and identically distributed across clients, which poses a significant challenge in federated learning environments due to the differences in data distribution among clients.

From the figure, we observe that the proposed \textit{PSO-ACO} framework maintains a high level of accuracy even under non-IID data conditions, with a median accuracy of around $88\%$. This demonstrates the robustness of \textit{PSO-ACO} in dealing with data heterogeneity, which is crucial for real-world federated learning scenarios where client data is often highly diverse. The high accuracy under non-IID conditions indicates that \textit{PSO-ACO} is able to effectively manage the inconsistencies and variations in the data distribution, leveraging its hybrid optimization to make adaptive and robust decisions.
The \textit{DRL} model also performs relatively well, achieving a median accuracy of $84\%$ under non-IID conditions. This indicates that \textit{DRL} has the capacity to adapt to varying data distributions, although it is not as effective as \textit{PSO-ACO} in maintaining accuracy. The drop in accuracy compared to \textit{PSO-ACO} suggests that \textit{DRL}, while adaptive, is more affected by the irregularities in the data distribution due to the absence of the enhanced optimization provided by \textit{PSO} and \textit{ACO}.

Alternatively, the \textit{CNN-LSTM} model shows a noticeable drop in accuracy under non-IID data, with a median accuracy of approximately $79\%$. This suggests that the model struggles with data heterogeneity, likely due to its reliance on temporal patterns, which may not be consistent across clients. The accuracy drop highlights the difficulty \textit{CNN-LSTM} faces in generalizing across varying data distributions, making it less suitable for federated learning environments with diverse client data.
However, the Traditional Rule-Based model exhibits the lowest accuracy under non-IID data conditions, with a median accuracy of around $66\%$. This highlights the limitations of rule-based systems in handling non-IID data, as they lack the adaptive learning mechanisms necessary to address the challenges posed by diverse data distributions. The fixed nature of the rules prevents the model from adjusting to the heterogeneity of the data, resulting in consistently low accuracy and highlighting the need for more dynamic, learning-based approaches like \textit{PSO-ACO}.

Overall, the figure illustrates that the \textit{PSO-ACO} framework is the most robust in maintaining high accuracy under non-IID conditions, showcasing its ability to effectively handle data heterogeneity. The \textit{DRL} and \textit{CNN-LSTM} models also show some capacity to adapt, but their performance is noticeably lower compared to \textit{PSO-ACO}. The Traditional Rule-Based model struggles the most, reaffirming the importance of adaptive learning approaches like \textit{PSO-ACO} in complex, real-world federated learning scenarios.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth, height=0.38\textheight ]{Picture5.jpg}
    \caption{Client Participation Rate over Rounds for All Models}
    \label{fig:rate}
\end{figure}

In Fig. \ref{fig:rate}, we present the client participation rate over multiple communication rounds for different models, including Traditional Rule-Based, \textit{CNN-LSTM}, \textit{DRL}, and the proposed \textit{PSO-ACO} framework. The participation rate refers to the number of clients actively involved in each round of federated learning, which directly impacts the overall model performance and resource utilization.
From the figure, we observe that the proposed \textit{PSO-ACO} framework maintains the highest client participation rate, increasing from $40$ to $65$ clients over $20$ communication rounds. This reflects the adaptive and efficient client selection strategy employed by the \textit{PSO-ACO} framework, allowing it to consistently attract and involve the most suitable clients throughout the learning process, resulting in improved performance.

The \textit{DRL} model also shows a steady increase in client participation, reaching up to $60$ clients by the final round. This indicates that \textit{DRL} effectively incentivizes client participation, though it does not match the optimization level achieved by \textit{PSO-ACO}.
The \textit{CNN-LSTM} model demonstrates moderate client participation, growing from $40$ to $55$ clients over the communication rounds. This suggests that while the model has some capacity for learning to optimize client selection, it lacks the advanced capabilities of \textit{PSO-ACO} and \textit{DRL}, leading to a more limited increase in participation.
Finally, the Traditional Rule-Based model shows the lowest client participation, starting at $40$ clients and increasing only to $48$ clients by the end of the rounds. This is consistent with expectations, as rule-based approaches do not adapt to changing conditions, resulting in suboptimal client selection and lower overall participation compared to the learning-based models.

\subsection{Ablation Analysis}


In this section, we conduct an ablation analysis to assess the contribution of each component of the proposed \textit{PSO-ACO} framework to the overall performance. The ablation study involves systematically removing or altering individual components, such as \textit{PSO}, \textit{ACO}, or the combination of edge-cloud integration, to evaluate their individual impact on key metrics such as accuracy, communication cost, client participation, and training loss.

%\subsection{Ablation Results}


\begin{table}
\centering
\scriptsize
\caption{Ablation Analysis Results}
\label{table7}
\begin{tabular}{|p{2.2 cm}|p{1.4cm}|p{2.4cm}|p{1.4cm}|p{1.9cm}|p{1.5cm}|}
\hline
\textbf{Component Configuration} & \textbf{Accuracy (\%)} & \textbf{Communication Cost (MB)} & \textbf{Latency (ms)} & \textbf{Client \newline Participation} & \textbf{Final Training Loss} \\ \hline
Full Model \newline (\textit{PSO-ACO}) & 92 & 50 & 400 & 60 & 0.2 \\ \hline
Without \textit{PSO} & 85 & 50 & 450 & 50 & 0.3 \\ \hline
Without \textit{ACO} & 83 & 65 & 550 & 55 & 0.35 \\ \hline
Without \newline Edge-Cloud \newline Integration & 83 & 70 & 600 & 48 & 0.4 \\ \hline

\end{tabular}
\end{table}

The ablation results, shown in Table \ref{table7}, highlight each component's impact on the proposed model's performance. In fact, when the \textit{PSO} component is removed, the model experiences a significant decline in both client participation rate and accuracy. The absence of \textit{PSO} means that clients are not selected optimally based on resource availability, which leads to suboptimal data quality and fewer clients participating. This results in an accuracy drop from $92\%$ to $85\%$. The training loss also shows slower convergence, with higher fluctuations and a final value of approximately $0.3$.
In contrast, removing \textit{ACO} from the framework leads to an increase in communication cost. Without \textit{ACO}, the model loses the optimization of communication paths, causing redundant data exchanges and increased latency. The communication cost rises by approximately $30\%$, and latency increases from $400$ ms to $550$ ms. The training loss decreases more slowly, with the final value stabilizing at around $0.35$.
The edge-cloud integration also plays a crucial role in the proposed framework. 

Without Edge-Cloud Integration refers to a scenario where the framework operates solely on edge devices without cloud support. In this setup, all data processing and model training are handled locally, leading to increased latency and reduced scalability due to limited resources. The absence of cloud integration results in lower performance, especially in environments with many devices or complex computational needs.


Indeed,  without edge-cloud integration, the model's scalability is compromised. The latency and energy consumption increase significantly, especially when the number of devices surpasses $100$. The accuracy also drops to $83\%$, indicating the importance of cloud resources in supporting edge devices with limited computational capabilities. The training loss shows the slowest decline, stabilizing at around $0.4$, reflecting the reduced computational support available to edge devices.
In conclusion, the ablation study reveals that the complete model, with all components integrated, demonstrates the highest accuracy ($92\%$), lowest communication cost, and optimal client participation. The edge-cloud integration, combined with \textit{PSO} and \textit{ACO}, ensures that resources are utilized efficiently, leading to faster convergence, lower latency, and improved overall performance. The training loss declines smoothly and stabilizes at a low value near $0.2$, indicating effective learning and generalization.
%\begin{enumerate}
 %   \item \textbf{Without \textit{PSO}}: When the \textit{PSO} component is removed, the model experiences a significant decline in both client participation rate and accuracy. The absence of \textit{PSO} means that clients are not selected optimally based on resource availability, which leads to suboptimal data quality and fewer clients participating. This results in an accuracy drop from $92\%$ to $85\%$. The training loss also shows slower convergence, with higher fluctuations and a final value of approximately $0.3$.

    %\item \textbf{Without \textit{ACO}}: Removing \textit{ACO} from the framework leads to an increase in communication cost. Without \textit{ACO}, the model loses the optimization of communication paths, causing redundant data exchanges and increased latency. The communication cost rises by approximately $30\%$, and latency increases from $400$ ms to $550$ ms. The training loss decreases more slowly, with the final value stabilizing at around $0.35$.

   % \item W\textbf{ithout Edge-Cloud Integration}: Without edge-cloud integration, the model's scalability is compromised. The latency and energy consumption increase significantly, especially when the number of devices surpasses $100$. The accuracy also drops to $83\%$, indicating the importance of cloud resources in supporting edge devices with limited computational capabilities. The training loss shows the slowest decline, stabilizing at around $0.4$, reflecting the reduced computational support available to edge devices.

    %\item \textbf{Full Model (\textit{PSO-ACO} with Edge-Cloud Integration)}: The complete model, with all components integrated, demonstrates the highest accuracy ($92\%$), lowest communication cost, and optimal client participation. The edge-cloud integration, combined with \textit{PSO} and \textit{ACO}, ensures that resources are utilized efficiently, leading to faster convergence, lower latency, and improved overall performance. The training loss declines smoothly and stabilizes at a low value near $0.2$, indicating effective learning and generalization.
%\end{enumerate}

The ablation analysis confirms that each component of the proposed framework plays a crucial role in its success, contributing to higher accuracy, efficient communication, effective resource management, and faster convergence. The hybrid approach of integrating \textit{PSO} and \textit{ACO}, along with edge-cloud collaboration, significantly outperforms configurations that omit any of these elements.



\section{Conclusion}
\label{section6}


In this study, we proposed a hybrid optimization-based framework for the deployment of \textit{MLLMs} in a FL environment with resource-constrained edge-cloud computing systems. Our approach, integrating \textit{PSO} and \textit{ACO}, demonstrated its efficacy in optimizing resource allocation, reducing latency, and maintaining a balance between computational demands and energy efficiency.
The experimental results reveal significant improvements in model accuracy and training efficiency, highlighting the potential of hybrid swarm intelligence techniques for complex edge-cloud scenarios. The findings emphasize the suitability of our proposed approach for real-time applications where multimodal data fusion and low-latency decision-making are critical.

Future work will focus on addressing security and privacy concerns inherent in FL environments, enhancing scalability for larger networks, and exploring other hybrid optimization strategies to further boost system performance. Model checking MLLMs deployment leveraging reduction-based techniques \cite{El-MenshawyBD10,Al-SaqqarBSWA15} is another plan for future investigation. The applicability of our framework to other domains, such as Healthcare Systems and smart cities, will also be investigated to broaden its impact.

\bibliographystyle{model5-names}
{\footnotesize
\bibliography{iclr2021_conference.bib}}

\clearpage


\end{document}
