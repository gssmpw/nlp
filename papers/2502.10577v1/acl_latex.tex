\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[preprint]{acl}

\newcommand{\repourl}{%
    {\url{https://github.com/spidersouris/llm_masc_gen}} %
}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}


\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{changepage}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{tabularx,ragged2e}
\usepackage{tablefootnote}
\usepackage{enumitem}
\usepackage{float}
\usepackage{longtable}
\usepackage{caption}
\usepackage{listings}
\lstset{literate=%
{é}{{\'e}}{1}%
{è}{{\`e}}{1}%
{à}{{\`a}}{1}%
{ç}{{\c{c}}}{1}%
{œ}{{\oe}}{1}%
{ù}{{\`u}}{1}%
{É}{{\'E}}{1}%
{È}{{\`E}}{1}%
{À}{{\`A}}{1}%
{Ç}{{\c{C}}}{1}%
{Œ}{{\OE}}{1}%
{Ê}{{\^E}}{1}%
{ê}{{\^e}}{1}%
{î}{{\^i}}{1}%
{ô}{{\^o}}{1}%
{û}{{\^u}}{1}%
{ä}{{\"{a}}}1
{ë}{{\"{e}}}1
{ï}{{\"{i}}}1
{ö}{{\"{o}}}1
{ü}{{\"{u}}}1
{û}{{\^{u}}}1
{â}{{\^{a}}}1
{Â}{{\^{A}}}1
{Î}{{\^{I}}}1,
showstringspaces=false,
basicstyle=\rmfamily,
linewidth=.99\textwidth,
xrightmargin=0.15cm,
breaklines=true
}

\newlist{questions}{enumerate}{2}
\setlist[questions,1]{label=RQ\arabic*.,ref=RQ\arabic*}
\setlist[questions,2]{label=(\alph*),ref=\thequestionsi(\alph*)}

\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}

\makeatletter
\setlength\@fptop{0pt}             %
\setlength\@fpsep{2\baselineskip}  %
\makeatother

\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother

\usepackage[roman]{parnotes} %
\makeatletter
\def\parnoteclear{%
    \gdef\PN@text{}%
    \parnotereset }
\makeatother

\usepackage{gb4e}
\noautomath %

\makeatletter
\renewcommand{\@subex}[2]{\settowidth{\labelwidth}{#1}\itemindent\z@\labelsep#2%
         \topsep0\p@\itemsep0\p@%
         \parsep\p@\partopsep0\p@%
         \leftmargin\labelwidth%
         \ifnum\the\@xnumdepth=1
         \else\advance\leftmargin#2\relax\fi}
\makeatother

\title{Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias}


\author{Enzo Doyen \\
  University of Strasbourg \\
  \texttt{enzo.doyen@unistra.fr}
  \And 
  Amalia Todirascu \\
  University of Strasbourg\\
  \texttt{todiras@unistra.fr}}


\newcommand\green[1]{{\color{green}#1}}
\newcommand\cut[1]{{\color{orange}#1}}
\newcommand\red[1]{{\color{red}#1}}
\newcommand\rose[1]{{\color{pink}#1}}



\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) have been shown to propagate and even amplify gender bias, in English and other languages, in specific or constrained contexts. However, no studies so far have focused on gender biases conveyed by LLMs' responses to generic instructions, especially with regard to masculine generics (MG). MG are a linguistic feature found in many gender-marked languages, denoting the use of the masculine gender as a "default" or supposedly neutral gender to refer to mixed group of men and women, or of a person whose gender is irrelevant or unknown. Numerous psycholinguistics studies have shown that MG are not neutral and induce gender bias. This work aims to analyze the use of MG by both proprietary and local LLMs in responses to generic instructions and evaluate their MG bias rate. We focus on French and create a human noun database from existing lexical resources. We filter existing French instruction datasets to retrieve generic instructions and analyze the responses of 6 different LLMs. Overall, we find that $\approx$39.5\% of LLMs' responses to generic instructions are MG-biased ($\approx$73.1\% across responses with human nouns). Our findings also reveal that LLMs are reluctant to using gender-fair language spontaneously. 
\end{abstract}

\section{Introduction}

Masculine generics (MG) are a linguistic feature found in many gender-marked languages, among which French, German, or Dutch. MG denote the use of the masculine gender in gendered languages as a "default" or supposedly neutral gender to refer to either a) a mixed group of men and women, or b) a person whose gender is irrelevant or unknown within the context, as in the following examples in French:

\begin{exe}
\exi{(a)} \textbf{Les étudiants} ont participé à l'atelier. \\ (Students participated to the workshop.) \label{ex:1}
\\
\exi{(b)} \textbf{Un athlète} doit s'entrainer régulièrement pour progresser. \\ (An athlete needs to train regularly to progress.) \label{ex:2}
\end{exe}

While such usage of the masculine gender is supposed to act as a neutralizer and contrast with the "specific" interpretation of the masculine (i.e., referring exclusively to men), empirical research in psycholinguistics has revealed that this supposed genericness is in fact not processed as such by native speakers \citep{gygaxGenericallyIntendedSpecifically2008,rothermundRemindingMayNot2024}, thereby exposing individuals to cognitive biases that amplify male-centric mental representations \citep{braunCognitiveEffectsMasculine2005,gygaxMasculineFormIts2012}.

The rapid development of large language models (LLMs) and their remarkable capabilities across various textual tasks, such as translation \citep{alvesTowerOpenMultilingual2024,zhuMultilingualMachineTranslation2024}, summarization \citep{puSummarizationAlmostDead2023}, and other text generation tasks, have made them indispensable tools for numerous applications. Relatedly, the release of ChatGPT by OpenAI and similar chatbots through user-friendly web applications have made it extremely trivial for anyone to generate textual content. However, these models inherently reflect the biases to be found in the training data, including those related to gender representation.

Even though the detection of text generated by artificial intelligence (AI) remains a challenging task \citep{tangScienceDetectingLLMGenerated2023}, an increasing amount of works tackle this issue \citep{abassyLLMDetectAIveToolFineGrained2024,marchitanQwenItDetect2025}, and preliminary research seems to indicate that a growing amount of text on the Internet is AI-generated, whether it be on social media \citep{sunAreWeAIGenerated2024,weiUnderstandingImpactAI2024} or even in Wikipedia edits \citep{brooksRiseAIGeneratedContent2024}. Such an increasing use of LLMs to publish content online could result in bias propagation among people who are exposed to that content. Similarly, the use of synthetically generated data for the training of AI systems \citep{longLLMsDrivenSyntheticData2024} also poses a risk of bias propagation, which could have a ripple effect across various domains and applications. Finally, users of LLMs themselves may also be directly exposed to MG and thus be affected by the related bias.

While previous research largely discussed the issues surrounding the propagation and amplification of gender biases by LLMs in specific or constrained contexts (e.g., instructing an LLM to continue a gender-ambiguous sentence, or asking it to translate a sentence from a gender-neutral language to a gendered one), a crucial gap exists in the literature when it comes to the biases conveyed by LLMs' responses to generic instructions. Moreover, to date, no studies have been conducted on the use of MG by LLMs in spite of their widespread deployment in various open text generation applications.

We believe that understanding how LLMs propagate inherent biases through text generation can provide valuable insights into their role in reinforcing and amplifying gender-related cognitive biases, and may give guidance as to how best to tackle such issues.


This study focuses on only one language with MG (French), but our methodology can be extended to any other gender-marked language. We first investigate the prominence of MG occurrences in human-written instruction/output pairs. We then use a filtered set of generic LLM instructions to gather the outputs of a total of 6 LLMs (3 proprietary models: GPT-4o mini \citep{openaiGPT4oMiniAdvancing2024}, Claude 3 Haiku \citep{anthropicClaude3Model2024} and Gemini 1.5 Flash \citep{geminiteamGemini15Unlocking2024}; 3 open-source models: Llama 3 8B \citep{touvronLLaMAOpenEfficient2023}, Ministral 8B \citep{mistralaiMinistralMinistraux2024} and Mistral Small 3 \citep{mistralaiMistralSmall32025}) and analyze their use of MG.
The code is available at \repourl.

\section{Related Work}

MG and the associated male bias have been extensively studied in the field of psycholinguistics for a variety of gender-marked languages \citep{silveiraGenericMasculineWords1980,stahlbergNameYourFavorite2001,safinaEffectsGrammaticalGender2024}, including French \citep{gygaxMasculineFormIts2012,richyDemelerEffetsStereotypes2021}. For example, a study conducted by \citet{gygaxGenericallyIntendedSpecifically2008} in English, French and German had participants read a sentence A with a role noun (e.g., "social workers"; MG in both French and German). Then, when presented with a sentence B with a noun referring to the members of the noun group in A (e.g., "women"), participants had to judge as quickly as possible if it was a coherent continuation of sentence A. Results showed that in English, gender representations aligned with stereotypes: female-stereotyped roles (e.g., "beauticians") led to female-biased interpretations, while male-stereotyped roles (e.g., "politicians") led to male-biased interpretations. Neutral roles showed no bias. In French and German, grammatical gender overrode stereotypes: masculine plural forms led to predominantly male-biased interpretations, even for female-stereotyped roles.

Similarly, a survey conducted by \citet{harrisinteractiveLecritureInclusivePopulation2017} regarding the use of inclusive or gender-neutral language in French highlighted the impact of such language on mental representations. It was found that respondents, when asked to name a celebrity using a formulation that was either inclusive (i.e., using both masculine and feminine forms of a role noun) or gender-neutral (i.e., using a non-gender-specific role noun or an equivalent) were more likely to name a woman as opposed to a man, while MG formulations led to more male-centric answers. Similar findings were reported by \citet{stahlbergNameYourFavorite2001} for German using MG role nouns.

Recently published studies have resorted to more precise methods such as EEG to track how language-related gender information is processed by the brain to evaluate the effects of MG. \citet{glimGenericMasculineRole2024} showed that even explicitly disambiguated
MG nouns used to refer to women led to higher cognitive load from participants for the task of noun phrase reference resolution, further indicating that the alleged genericness of MG lacks empirical backing.

When it comes to natural language processing (NLP), gender bias is by far the most studied type of bias \citep{ducelRechercheBiaisDans2024}. Numerous researchers have drawn attention to gender bias in word embeddings \citep{bolukbasiManComputerProgrammer2016}, in machine translation systems \citep{savoldiGenderBiasMachine2021,wisniewskiBiaisGenreDans2021} or in text classification tasks \citep{sobhaniFairerNLPModels2024}.

More recently, LLMs too have been shown to exhibit gender biases and convey stereotypes when generating context-restricted textual content (see \citet{kotekGenderBiasStereotypes2023} for pronoun disambiguation; \citet{dollEvaluatingGenderBias2024} for pronoun prediction; \citet{youBinaryGenderLabels2024} for neutral name prediction), including in languages other than English \citep{zhaoGenderBiasLarge2024,ducelEvaluationAutomatiqueBiais2024}. Nonetheless, in spite of the propensity for investigating gender bias in NLP and the attention given to LLMs with regard to this issue, no studies discussing MG and to what extent LLMs are prone to propagating MG-related gender bias have so far been conducted, a gap this work aims to bridge. We present our detailed methodology in the next section.


\section{Methodology}
\label{sec:meth}

Our methodology is divided into two main tasks. First, we create a database of French human nouns (HN), which will be used both to detect occurrences of MG and evaluate the ratio of HN to MG uses in LLM instructions and outputs. Second, we retrieve and filter different datasets of human-written instruction/output pairs as well as AI-generated instructions to remove entries exhibiting specific uses of the masculine gender. We send a filtered and narrowed set of generic instructions to a wide range of LLMs, and retrieve their responses. Leveraging these responses, we perform quantitative and qualitative analyses of the use of MG by LLMs.

\subsection{Database of French Human Nouns}
\label{sec:db}

Since MG strictly refer to human beings, the first step was to use a database of French HNs. To our knowledge, the only existing database is that of the NHUMA project \citep{stosicBaseDonneesNHUMA2013}\footnote{\url{https://nomsdhumains.weebly.com/}}, whose goal is to provide an extensive linguistic description of HN in French. However, this database has a limited number of entries (1,107), many of which are not MG. We thus extended it taking the following steps.

We first leveraged several existing public French lexical databases, namely Demonette \citep{namerDemonette2DerivationalDatabase2023}, TLFi\footnote{\url{http://atilf.atilf.fr/}} and Wiktionary\footnote{\url{https://fr.wiktionary.org/}}. While Demonette provides an easily accessible list of masculine-feminine noun pairs\footnote{\url{https://demonette.fr/demonext/vues/gender_equivalents_table.php}}, no such option is offered for TLFi. As a result, we built a custom scraper using the Playwright Node.js library\footnote{\url{https://playwright.dev/}} to retrieve TLFi entries that could refer to human beings. We restricted the search to entries having one of their definitions starting with gender-neutral or gender-specific nouns or pronouns commonly used in dictionary entries related to human beings (see list in Appendix \ref{apx:defnouns}). We also performed recursive search to get nouns whose definition started with one of the previously scraped nouns. For Wiktionary, we used Wiktextract \citep{ylonen-2022-wiktextract}\footnote{\url{https://kaikki.org/}} and applied the same filtering rule with all but one noun ("individu")\footnote{This noun was removed from the filtering list as we had a certain number of false positives, as it can be used as a synonym of "species" and refer to living beings other than humans.}. Following the preliminary work of \citet{lernerINCLUREDatasetToolkit2024}, we also used Wikidata to retrieve masculine-feminine HN pairs.

We noticed that the Demonette database includes pairs of words that do not necessarily refer to human beings (animal nouns, mythology-related nouns), or typos. Furthermore, the recursive search performed on TLFi led to a certain number of false positives. As a potential solution to these false positives, we built a HN classifier to detect whether a noun is commonly used to refer to a human being.

\subsubsection{HScorer: a French Human Noun Classifier}
\label{sec:hscore}

To build our classifier, we created golden HN and non-HN datasets. The golden HN dataset was built from previously scraped entries from Wikidata and Wiktionary, as well as entries from the NHUMA database. For the golden non-HN dataset, we leveraged the WordNet database \citep{princetonuniversityWordNet2010} and its collection of universal synsets to filter out human-related nouns and retrieve non-HN with synsets \verb`artifact.n.01`, \verb`object.n.01` and \verb`living_thing.n.01`\footnote{\url{https://github.com/opinionscience/InstructionFr/tree/main/wikipedia}}. We retrieved a total of 14,942 HN and 17,912 non-HN.

This dataset of HN and non-HN (32,854 entries in total) was then used to train three types of models: a logistic regression (LR) model, a XGBoost model \citep{chenXGBoostScalableTree2016} and a Transformer model \citep{vaswaniAttentionAllYou2017}. We did this to take advantage of the strengths of each model type. In particular, we wanted to avoid adding false positives to our final database: only words classified as HNs by the three models (full agreement) would be part of the final HN database. The LR and XGBoost model types were chosen based on their best performance compared to a set of classical machine learning model types. 
For these models, we calculated four different scores to be used as features (see Appendix \ref{apx:hscore} for more details). For the Transformer model, we used CamemBERT \citep{martinCamemBERTTastyFrench2020} and forwarded tokenized $\vec{W}$ as input.

We trained all three models on our golden HN and non-HN datasets with a 80/20 training-validation split using an NVIDIA RTX 4070 Super GPU. Hyperparameter optimization (HPO) search was performed on all models using Weight \& Biases \citep{wandb}. Chosen hyperparameters can be found in Appendix \ref{apx:hscorehp}. An accuracy of 0.914 was achieved for the LR model; 0.937 for the XGBoost model; 0.927 for the Transformer model.

The three models were used to classify words from the Demonette dataset (8,048 entries) as well as recursive search results from TLFi (13,961 entries). Demonette counts exclude demonyms (21,340 entries). After classification, we retrieved 2,312 entries with 100\% humanness agreement from Demonette, and 1,428 from TLFi recursive search results. Our final human noun database contains 16,652 unique nouns across all six initial datasets (Wiktionary, Wikidata, NHUMA, Demonette, TLFi and TLFi Recursive).

\subsection{Analyzing MG Use in LLM Instructions and Outputs}
\label{sec:mguseinstrout}

Our main objective is to comprehensively evaluate the extent to which LLMs are prone to using MG in their answers to generic (non-specific) instructions. To this goal, we leveraged the French HN database introduced in Section \ref{sec:db} to build a MG subset (5,140 entries) comprised of male-only HNs to be used in our analyses. This subset was created by removing epicene (i.e., words whose masculine and feminine forms are identical) and feminine words from the original HN database.

For the purposes of the analysis, we resorted to four different datasets. Two of them are human-written instruction/output datasets, while the two others are AI-generated/translated instruction datasets. Human-generated instruction/output datasets are included to gauge the extent to which French human speakers are prone to resorting to MG in written texts, and thus have a baseline when comparing with LLM outputs (see Section \ref{sec:measuring}). We separated human-written instructions from AI-translated French instructions in our analysis as we wanted to analyze the writings by native French speakers, and because numerous studies have shown that AI translation from and to languages with different gender systems could lead to biases, notably by favoring the use of the masculine gender \citep{zaranisWatchingWatchersExposing2024,vanmassenhoveGenderBiasMachine2024}.

The human-written instruction/output datasets include "oracle", a set of question-answer pairs from Wikipedia users\footnote{\url{https://github.com/opinionscience/InstructionFr/tree/main/wikipedia}} \footnote{\url{https://fr.wikipedia.org/wiki/Wikipédia:Oracle}} (4,613 pairs), and "oasst2" (Open Assistant Conversations Dataset Release 2)\footnote{\url{https://huggingface.co/datasets/OpenAssistant/oasst2}}, filtered to only get French-tagged entries (1,773 chats). Both datasets only contain human-generated conversations.

In addition, we use two other instruction AI-generated/translated datasets: "french\_hh\_rlhf"\footnote{\url{https://huggingface.co/datasets/AIffl/french_hh_rlhf}} and "French-Alpaca-dataset-Instruct-55K"\footnote{\url{https://huggingface.co/datasets/Anthropic/hh-rlhf}}. The "french\_hh\_rlhf" dataset (henceforth, "hh\_rlhf") is a direct translation of Anthropic's hh-rlhf dataset\footnote{\url{https://huggingface.co/datasets/jpacifico/French-Alpaca-dataset-Instruct-55K}}, and contains 161,000 instruction/output pairs. The "French-Alpaca-dataset-Instruct-55K" dataset (henceforth, "alpaca") is comprised of 55,184 French instruction/output pairs generated by OpenAI's GPT-3.5. For both datasets, we retrieved only the corresponding instructions. For the hh\_rlhf dataset specifically, only positive-rated ("chosen") instructions were selected.
\subsubsection{Instruction and Output Filtering}
\label{sec:filtering}

To conduct a comprehensive and precise analysis of how MG nouns are used both in human-written instructions/outputs and in LLMs' responses to generic instructions, we designed filtering rules to remove from the datasets entries exhibiting specific uses of the masculine gender, that is contexts where a masculine word does indeed refer to a male individual. This was done to avoid biasing the MG use analysis results.

First, we removed instructions and outputs containing names of people or personalities. We used spaCy v3.8 \citep{montaniSpaCyIndustrialstrengthNatural2024} and the French model \verb`fr_core_news_lg` and the integrated NER component to detect words labelled with "PER". As some first name occurrences failed to be detected as "PER", we also used the publicly available dataset of given names on Open Data Paris\footnote{\url{https://opendata.paris.fr/explore/dataset/liste_des_prenoms/information/?disjunctive.annee&disjunctive.prenoms}} and checked if names with the "MISC" label were in that list.

Similarly, leveraging the French spaCy Transformer model \verb`fr_dep_news_trf` for state-of-the-art performance, we performed POS tagging and dependency parsing for entries in our two datasets and removed instructions containing the interrogative pronoun "qui" ("who"), strictly used to refer to people, as this could bias answers towards referring to a specific person. For the same reason, instructions including a singular possessive determiner (e.g., "mon", 'my') or a definite determiner (e.g., "ce", 'this') followed by a HN were excluded. Finally, and only for the "oracle" dataset, we left out parts of instructions containing "oracle" or "pythie", jargon used by the French Wikipedia community to refer to people answering user inquiries.

At this point, preliminary experiments revealed that many nouns considered MG were in fact contextually ambiguous nouns not always referring to human beings. For instance, in French, "facteur" can refer to a mailman, but can also mean "factor, cause". Similarly, "navigateur" can refer either to a (male) sailor or to an Internet browser. Including non-human-related occurrences to our analysis would obviously invalidate our approach. To remedy this problem, we added two more steps:

\begin{enumerate}
\item \textbf{Ambiguous Noun List Filtering}. We specifically filtered a certain number of nouns found to be ambiguous during our preliminary tests\footnote{The full list of filtered nouns is made available on this paper's GitHub repository: \url{https://github.com/spidersouris/llm_masc_gen} (license: MIT)}. We also scraped Wiktionary's list of 1,750 most common French words\footnote{\url{https://fr.wiktionary.org/wiki/Wiktionnaire:Liste_de_1750_mots_français_les_plus_courants}} to a standard one-word-per-line format and manually removed nouns referring to human beings to add to our filtering list. Note that only nouns with a primary non-human-related meaning were filtered; we did not filter nouns which are commonly used to either refer to human or non-human entities (such as the two previous examples, "facteur" and "navigateur"), as we leave that for the LLM post-processing step (see below). Moreover, as we had previously used data from Wiktionary to fill our MG dictionary, we removed all nouns from Wiktionary whose human-related definition index was greater than 2\footnote{According to the style guide of Wiktionary (\url{https://en.wiktionary.org/wiki/Wiktionary:Style_guide\#Definitions}), the most common word definitions are placed first. By retrieving only the top 2 entries, we are making sure we are only using nouns commonly used to refer to human beings.}. This helped reduce the number of nouns which were not primarily and/or inherently considered or used as HNs.
\item \textbf{LLM HN Classification}. As a post-processing step, we leverage GPT-4o mini\footnote{\url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}} and in-context learning \citep{brownLanguageModelsAre2020} to validate nouns considered HNs. GPT-4o mini was chosen for its great performance\slash cost ratio. The words and the context in which they appear are forwarded to the LLM. We use Pydantic \citep{colvinPydantic2025} to constrain GPT-4o mini's output to JSON format. We set max tokens to 500, temperature to 0. See Appendix \ref{apx:llm-prompting} for prompting details.
\end{enumerate}

We evaluate GPT-4o mini's HN classification. We extract 1,000 instructions from the hh\_rlhf and French-Alpaca datasets (see Section \ref{sec:mguseinstrout}) and apply our MG use analysis pipeline. We narrow down the analysis results to 500 to only retrieve contexts where at least one HN has been found. Those results are then sent to GPT-4o mini for validation. Two annotators classified the nouns in the 500 contexts as HN or non-HN. Cohen's kappa \citep{cohenCoefficientAgreementNominal1960}, noted $\kappa$, was used to calculate inter-annotator agreement, where $\kappa = 0.944$. We built a reference annotation dataset and compared it with GPT-4o mini's output, where $\kappa = 0.855$.%

\subsection{LLM Instructing and Answer Retrieval}
\label{sec:analysis}

\begin{figure*}[]
	\centering 
	\includegraphics[width=1\textwidth]{methodologyul.pdf}
	\caption{Overview of our methodology. We filter human and AI-written instructions to remove specific contexts that could lead to masculine specific uses. We send these instructions to LLMs and retrieve their responses. LLM and human-written answers are analyzed for human-related MG (red) and inclusive or neutral (blue) terms. GPT-4o mini is used for human noun validation, and we use the final analysis results to calculate a M Score for each text.} 
	\label{fig_meth}%
\end{figure*}

To gauge the use of MG by LLMs, we query a set of LLMs of different sizes and types (both proprietary and local). We use a total of 6 different models. This includes 3 proprietary models (GPT-4o mini, Claude 3 Haiku and Gemini 1.5 Flash), and 3 local models (Llama 3 8B, Ministral 8B and Mistral Small 3) using OpenRouter. Figure \ref{fig_meth} illustrates our methodology.

We retrieved instructions from our four filtered instruction/output datasets. In addition to previous filtering steps, we also removed instructions containing nouns from our MG dataset to avoid biasing the LLM towards generating MG.\footnote{Since LLMs are autoregressive models, if a MG is used in an instruction, the LLM receiving that instruction has very high chance of making it a part of its answer.}

We collected 42,896 unique instructions in total. The number of instructions was narrowed down to 10,000 to reduce computing and API call costs. The narrowed dataset was created by proportionally weighting instructions to the original number of entries (see Appendix \ref{apx:datasetcount} for count details).

We sent these 10,000 instructions to all LLMs and retrieved their responses. For every LLM (proprietary and local), temperature was set to 1 to reflect the real usage from LLM users, as most deployed LLMs use a similar temperature; the number of maximum generated tokens was set to 1,500; and a canonical system prompt was defined ("You are a helpful French assistant.").

The aforementioned filtering steps were applied to the 10,000 LLM responses to only keep generic responses, that is responses that do not refer to specific individuals. Similarly, all HNs found in the filtered responses were validated using GPT-4o mini. Table in Appendix \ref{apx:llmcount} shows the number of responses after filtering.

\subsection{Measuring MG Bias}
\label{sec:measuring}

We created an analysis pipeline to automatically evaluate MG bias in human-written instructions and in human/LLM-generated responses after HN validation by GPT-4o mini. In a first step, we focus on responses and calculate the percentage of biased responses based on the presence of MG, both across responses with HNs as well as all responses. Then, in a second step, we calculate the ratio of MG terms to HNs in both human-written instructions and human-written/LLM-generated outputs. This ratio is noted "M Score". For each text, we divide the number of MG terms by the number of HNs found in the text. A score is computed only if the number of HNs is greater than zero. Then, for each dataset/model output, we compute an overall M Score (total number of MG nouns divided by the total number of HNs) as well as a mean M Score (sum of M Scores divided by the total count of M Scores). The overall score gives a dataset-wide measure of MG bias, while the mean score represents the average bias per text.

In addition to MG use, we also aim to see if LLMs' answers contain gender-fair language terms. Indeed, in response to MG, several writing techniques commonly referred to as "écriture inclusive" ('inclusive writing') were developed in French and other languages to either promote the visibility of women or reduce the prominent use of MG terms \citep{viennotNonMasculinNe2014}. Consequently, we defined several language marker lists to capture such occurrences (see Appendix \ref{apx:markers}).

Another element that we want to analyze is the type of HNs used as MG. French HNs have been extensively studied in the literature, and many works have focused on describing their specificities and types \citep{schnedeckerNomsGenerauxDenotant2018,mihatschNomsDhumainsCategorie2015}. As the NHUMA dataset (used to create our HN dictionary in Section \ref{sec:db}) has human-annotated HN types, we used these annotations as a basis (789 annotated nouns with labels such as "profession", "doer", "relationship" or "status"). To complete those, we used a spaCy model trained by \citet{papasseudiClassificationNomsDHumains2023} on NHUMA data for automatic HN type labelling (see Table \ref{tab:classesbarbara} for details) and applied it to nouns in our MG dictionary missing human annotation (i.e., non-NHUMA entries). Entries not detected as HNs by the model were left unannotated. In total, our HN dataset contains 2,893 annotated entries (i.e., +2,104 after applying the model).

\section{Results}
\label{sec:res}

\begin{figure}[]
	\centering 
	\includegraphics[width=0.5\textwidth]{mguseratesorted.pdf}
	\caption{Percentage of MG use rate by model} 
	\label{fig_mguserate}%
\end{figure}

\begin{figure*}[]
	\centering 
	\includegraphics[width=0.9\textwidth]{mscorellmtwocols.pdf}
	\caption{Overall (\textcolor{blue}{\textbf{blue}}) and mean (\textcolor{red}{\textbf{red}}) M Score results for human-written and LLM responses by dataset/model} 
	\label{fig_mscore}%
\end{figure*}

Figure \ref{fig_mguserate} shows the percentage of MG in generic instruction outputs across models and datasets. We find that LLMs use MG in 39.5\% of all their responses on average. In particular, we find that gpt4o\_mini is generally the most MG-biased model, with 42.03\% of its responses overall exhibiting at least one occurrence of MG. Conversely, llama is the least biased model overall with 36.61\% of its total responses containing MG. Bias rate for human-written instructions (oracle and oasst2) is generally on par with that of LLMs, with the exception of oasst2, which achieves an exceptionally low MG bias rate overall compared to other models (32.26\%). When considering responses with HNs only, the average percentage of biased responses across LLMs amounts to 73.1\%. ministral has the highest bais rate with 75.81\% of its responses with HNs containing MG, while claude-3-haiku achieves the lowest bias rate (69.13\%).

M Score results are displayed in Figure \ref{fig_mscore}. Human-written responses datasets oasst2\_assistant and oracle\_assistant show the highest M Score (0.644 mean and 0.612 mean, respectively). When comparing LLMs only, M Score is generally correlated with the bias rate: ministral has the highest mean M Score (0.603), followed by gemini (0.591) and gpt4o\_mini (0.585). mistral-small and llama both have the lowest overall score (0.542 and 0.545 respectively). The lowest mean scores are achieved by mistral-small, claude-3-haiku and llama (0.557, 0.570 and 0.577 respectively).

Figure \ref{fig_hnclasses} reveals that the top 5 MG human noun classes found across responses are "profession", "doer", "speciality", "relationship" and "status". More precisely, nouns with the "profession" and "doer" classes are the most used (freq. between 120 and 44 for "profession"; between 57 and 24 for "doer"). Among LLMs, gemini, claude-3-haiku and gpt4o\_mini (all proprietary models) are the ones that are the most using nouns from the "profession" and "doer" MG human noun classes. We find that respectively 120, 119 and 114 unique human nouns with class "profession" were used in the responses of gemini, claude-3-haiku and gpt4o\_mini (57, 55 and 51 for the "doer" class).

As seen in Figure \ref{fig_langmarkers}, we find that LLMs use neutral words such as "personne" or "individu" in 10.7\% of their responses on average, with gpt4o-mini having the highest rate of responses with neutral words (12.8\%), and claude-3-haiku the lowest (9.4\%). For other gender-related language markers, their use is very sporadic, and even sometimes non-existent. The only model which shows a relatively notable use of markers other than neutral words is llama, with 0.7\% of responses with feminine endings (the highest), 0.1\% with inclusive greetings (same score as gpt4o-mini) and 0.1\% with inclusive pairs (same score as mistral-small). Across LLMs, no model responses exhibit the use of neutral neopronouns such as "iel" or "celleux". 

\section{Discussion}

Surprisingly, the highest M Scores are to be found in human-written responses. It should however be noted that this score is calculated based on the number of HNs and MG found in the responses, and that there is a large difference in the number of responses analyzed (2,359 human-written responses on average vs. 4,888 LLM responses on average; more than the double). Looking at the bias rate in models and datasets, however, we find that LLMs are more prone to using MG in their responses.

Generally, llama is the most gender-fair model out of all tested models, considering its bias rate and its use of inclusive language markers. Given the results, and comparing with other models, we find it likely that extra care has been taken during the training of this model to promote language fairness. We give a few examples of gender-fair language outputs in Appendix \ref{apx:gfl}. While these results definitely show a step in the right direction, efforts should be further intensified, as the percentage of responses with gender-fair language remains extremely low. Local models in general appear to be slightly less biased compared to proprietary models, with claude-3-haiku being the least biased proprietary model.

Our results for MG human noun classes use unsurprisingly show that the "profession" class is the one whose nouns are the most used as MG. This is consistent with the methodology of psycholinguistics works focusing on MG as they prevalently use this type of noun in their experiments. Still, not all MG nouns have had their classes annotated for our analysis, so our findings need to be completed. %

That no models appear to use neutral neopronouns is not that surprising given that those pronouns are rather novel and are still used by a small (but growing) section of the population. As a result, there may not be many occurrences in the training data. Nonetheless, these neopronouns play a key role as they challenge the binary male/female gender dichotomy, and some people may not feel represented when simply reading masculine/feminine pair forms such as "ils et elles" instead of "iels". It is thus important for LLMs to integrate such neopronouns to their responses, both to promote gender diversity and to reflect new language usage trends.

Overall, our results indicate that LLMs largely exhibit MG bias when generating responses to generic instructions. While previous research has clearly demonstrated that LLMs display gender bias in specific or constrained contexts, our results provide evidence that LLMs' responses are inherently gender-biased in normal use contexts. That such bias can be found in everyday, instruction-based interactions with LLMs is obviously concerning, as it participates in further reinforcing other existing gender bias, increases male mental representations and defies efforts to promote more inclusion and equality. These results show that fairness in language should be attentively considered when training LLMs in heavily gender-marked languages. Linguistics gender bias may be reduced by filtering or rewriting texts that exhibit MG bias \citep{vanmassenhoveNeuTralRewriterRuleBased2021,velosoRewritingApproachGender2023,doyenNeutralisationAutomatiqueGenre2024,lernerINCLUREDatasetToolkit2024}, or using data augmentation to include texts featuring more diverse gender forms to the training data \citep{zmigrodCounterfactualDataAugmentation2019}. We leave these considerations for future work.

\section{Conclusion}

Our work expands on prior research by analyzing gender bias in everyday, instruction-based interactions with LLMs. We created a French HN database using HScorer and found that $\approx$39.5\% of responses in average exhibit MG bias ($\approx$73.1\% across responses with HNs), with LLMs generally avoiding gender-fair language. Our methodology is adaptable to other languages and LLMs. We hope this work encourages further research on MG bias beyond French and contributes to promoting gender-fair language in LLM-generated texts.

\section{Limitations}

Our work has several limitations. First, the French HN database that we created is not exhaustive and may have missing nouns. Similarly, even though we took care in not adding non-HNs to our database, a very small number of words not used to refer to human beings may be present in the data. Second, even though we took several steps to validate human nouns, both by pre-filtering our datasets and using an LLM for automatic verification (which we evaluated), many human nouns are polysemic, and some nouns may have incorrectly been detected as human nouns, or incorrectly left undetected. While we experimentally tried multiple human noun validation runs and did not see much change in the results, errors in validation might still have slightly impacted the results. Third, detection of masculine specific uses remains a challenging task. Even though we designed several filtering steps to remove instructions and responses which may contain specific uses of the masculine so as to not bias the results, a small portion might still have been included to the data. Finally, our analysis only focuses on a small set of local and proprietary LLMs. More models, and especially local models with higher parameter counts, should be analyzed to have a better overview of MG bias in LLMs.

\section{Ethics Statement}

The LLM instruction/output datasets were not filtered for harmful or malicious content. Similarly, LLM responses to generic instructions were not checked or filtered for such content. In addition, a very small portion of the HN database we created contains slurs and pejorative or discriminative terms used to refer to human beings. We deliberately did not filter these nouns, as they may be used in non-prejudicial contexts or with a non-harmful meaning. Finally, results of this work might be used in a counteractive way to increase gender bias in LLMs, for instance by removing occurrences of inclusive language markers from the training data or by increasing the number of MG occurrences in responses to generic instructions.



\bibliography{phd}

\clearpage

\appendix
\renewcommand\thexnumi{A.5.\arabic{xnumi}}

\section{HScorer Scoring Functions}
\label{apx:hscore}

This section details the scoring functions used as features $\vec{x}$ of LR and XGBoost models, along with the FastText vector for word embedding representation. We calculate four different scores to be used: WordNet Hypernym Score ($H$), WordNet Definition Score ($D$), FastText Score ($F$) and Suffix Score ($S$). Let $W$ be a word to be classified. The final feature vector for word $W$ is the concatenation:

$$\vec{v}(W) = [H(W) \oplus D(W) \oplus F(W) \oplus S(W) \oplus v_W]$$

Let $H(W) = (h_s, n_s)$ where:

$$h_s = \frac{\sum_{p \in P} \sum_{y \in p} 1_h(y)}{|P|}$$

$$n_s = \frac{\sum_{p \in P} \sum_{y \in p} 1_n(y)}{|P|}$$

Where $P$ is the set of hypernym paths for $W$, $1_h(y)$ is the indicator function for human hypernyms, and $1_n(y)$ is the indicator function for non-human hypernyms.

Let $I_h$ be a set of human indicator words and $I_n$ a set of non-human indicator words (see complete list in Appendix). Those indicator words are manually defined English words related to human ("someone", "person", "who") or non-human ("object", "plant", "chemical") entities, and are used to search in WordNet definitions. Since WordNet definitions are universally in English, we used English words. See our GitHub repo for the full list. Let $D(W) = (h_d, n_d)$ where:

$$h_d = \frac{\sum_{s \in S} \sum_{i \in I_h} f(i, \text{d}(s))}{|S|}$$

$$n_d = \frac{\sum_{s \in S} \sum_{i \in I_n} f(i, \text{d}(s))}{|S|}$$

Where $S$ is the set of synsets for $W$ and $f(i,d)$ counts occurrences of indicator $i$ in definition $d$.

We define a set of human prototypes $P_h$ and non-human prototypes $P_n$ (see complete list in Appendix). Those prototype words are manually defined French words prototypically related to human ("personne" (\emph{person}), "homme" (\emph{man}), "femme" (\emph{woman})) or non-human ("objet" (\emph{object}), "chose" (\emph{item}), "machine" (\emph{machine})) entities, and are used to calculate semantic similarity. The contents of the prototype sets are stricter than those of indicator sets. The complete list of prototypes words is available on GitHub. Let $F(W) = (h_f, n_f)$ where:

$$h_f = \frac{\sum_{p \in P_h} \cos(v_W, v_p)}{|P_h|}$$

$$n_f = \frac{\sum_{p \in P_n} \cos(v_W, v_p)}{|P_n|}$$

Where $v_W$ is the 300-dimension FastText \citep{bojanowski2016enriching} vector for word $W$ and $\cos(v_a,v_b)$ is the cosine similarity between vectors $a$ and $b$.

Finally, $S(W) = 1$ if $W$ ends with one of the suffixes $\hat{s} \in \hat{S}$ (see our GitHub repo for the complete list), else $S(W) = 0$.

\section{HScorer Hyperparameters}
\label{apx:hscorehp}

\subsection*{Logistic Regression (LR)}
\begin{align*}
\text{penalty} &= \text{l1} \\
\text{solver} &= \text{saga} \\
\text{C} &= 100
\end{align*}

\subsection*{XGBoost}
\begin{align*}
\text{booster} &= \text{gbtree} \\
\text{learning\_rate} &= 0.22394632872649503 \\
\text{max\_depth} &= 10 \\
\text{min\_child\_weight} &= 78 \\
\text{subsample} &= 1 \\
\text{colsample\_bytree} &= 1 \\
\text{n\_estimators} &= 912 \\
\text{gamma} &= 0 \\
\text{reg\_alpha} &= 0 \\
\text{reg\_lambda} &= 0 \\
\text{objective} &= \text{binary:logistic} \\
\text{early\_stopping\_rounds} &= 20 \\
\text{random\_state} &= 42 \\
\text{tree\_method} &= \text{gpu\_hist} \\
\text{n\_jobs} &= 24
\end{align*}

\subsection*{Transformer}
\begin{align*}
\text{train\_batch\_size} &= 16 \\
\text{eval\_batch\_size} &= 16 \\
\text{eval\_strategy} &= \text{epoch} \\
\text{save\_strategy} &= \text{epoch} \\
\text{num\_train\_epochs} &= 5 \\
\text{weight\_decay} &= 0.061748150962771656 \\
\text{learning\_rate} &= 8.497821083760116 \times 10^{-6}
\end{align*}

\section{Narrowed Instruction Dataset Count Details}
\label{apx:datasetcount}

\begin{table}[H]
\centering
\caption{Narrowed instruction dataset counts by original dataset}
\begin{tabular}{lll}
\toprule
Dataset name & Original count & Narrowed count \\
\midrule
alpaca & 29,179 & 6,803 \\
hh\_rlhf & 10,806 & 2,520 \\
oracle & 2,600 & 605 \\
oasst2 & 311 & 72 \\
\bottomrule
\end{tabular}
\end{table}

\section{Number of Non-Specific LLM Responses}
\label{apx:llmcount}

\begin{table}[H]
\centering
\caption{Number of LLM responses without specific masculine markers by model}
\label{tab:mgllmresponses}
\begin{tabular}{lr}
\toprule
Model &
Count \\
\midrule
claude-3-haiku & 7,221 \\
gemini & 5,224 \\
gpt4o-mini & 4,873 \\
ministral & 4,660 \\
llama & 3,920 \\
mistral-small & 3,432 \\
\bottomrule
\end{tabular}
\end{table}

\section{MG Human Noun Classes}
\label{apx:defnouns}

\begin{table}[H]
\centering
\caption{Nouns and pronouns used to perform definition search}
\label{tab:defnouns}
\begin{tabular}{lll}
\toprule
Noun & English Translation & Websites \\
\midrule
personne & person & TLFi, Wiktionary \\
individu & individual & TLFi \\
quelqu'un & someone & TLFi, Wiktionary \\
homme & man & TLFi, Wiktionary \\
femme & woman & TLFi, Wiktionary \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Human noun classes and counts used by \citet{papasseudiClassificationNomsDHumains2023} for spaCy model training}
\label{tab:classesbarbara}
\begin{tabular}{ccc}
\toprule
Class &
Count & Mapped to \\
\midrule
NH-Mét & 697 & profession \\
NH-Fonc & 402 & profession \\
NH-Spé & 385 & speciality \\
NH-Titre & 111 & title \\
NH-Grade & 16 & title \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Human noun classes and counts across entire human noun database (including non-MG)}
\label{tab:classes}
\begin{tabular}{lr}
\toprule
Class &
Count \\
\midrule
profession & 2,893 \\
demonym & 1,290 \\
doer & 388 \\
speciality & 269 \\
attribute & 179 \\
relationship & 44 \\
status & 24 \\
title & 18 \\
patient & 14 \\
other & 7 \\
recipient & 5 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}[]
	\centering 
	\includegraphics[width=1.1\textwidth]{hnclasses.pdf}
	\caption{Frequency of unique MG nouns in responses based on their associated human noun class} 
	\label{fig_hnclasses}%
\end{figure*}

\clearpage

\section{Language Markers}
\label{apx:markers}

The table below shows the language markers used for the analysis. For information, \verb|fem_ending| refers to occurrences where the feminine ending of a HN is separated from the original, masculine form to increase its visibility. This is done either by adding a special separator (e.g., "auteur·ice", "auteur(ice)") or by spelling the feminine ending with capitals (e.g., "auteurICE").

\begin{table}[H]
\centering
\parbox{12.5cm}{\caption{Examples of inclusive markers by type}}
\label{tab:defnouns}
\begin{tabular}{lll}
\toprule
Example & English Translation & Type \\
\midrule
mesdames et messieurs & Ladies and gentlemen & incl\_greetings \\
tous et toutes & everyone & incl\_greetings \\
un ou une & a [m] or a [f] & incl\_pairs \\
il ou elle & he or she & incl\_pairs \\
iel & they (sg.) & neutral\_prons \\
auteur·ice; auteur(ice); auteurICE & author & fem\_ending \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}
	\centering 
	\includegraphics[width=1.1\textwidth]{languagemarkers.pdf}
	\caption{Percentage of responses with inclusive language markers across models} 
	\label{fig_langmarkers}%
\end{figure*}

\clearpage

\parbox{16.5cm}{\section{GPT-4o mini Human Noun Validation Prompting Details}\label{apx:llm-prompting}}

\begin{lstlisting}[language=xml, caption={System Prompt}, label=lst:system-prompt]
You are an assistant that validates human noun classifications in French texts.
\end{lstlisting}

\begin{lstlisting}[language=xml, caption={User Prompt}, label=lst:user-prompt]
Given a text and nouns, for each noun, determine if it is a human noun in context.
Some nouns may appear multiple times in the text. In such cases, they are distinguished by ID ('noun_1', 'noun_2'...), following the order in which they appear.
Do not assume that all occurrences of the same noun are either human or non-human; instead, assess each occurrence individually based on its unique context.
Only respond in this format, where human_noun is the noun being considered.
{{
  "human_noun": 0,
  "human_noun_2": 1
}}

## Examples
Text: Les facteurs d'employabilité des facteurs, chargés de distribuer le courrier, vont évoluer.
Nouns: facteurs, facteurs_2
Output: {{ "facteurs": 0, "facteurs_2": 1 }}

Text: Le président a annoncé aux citoyens une série de mesures pour renforcer l'économie du pays.
Nouns: président, citoyens, mesures
Output: {{ "président": 1, "citoyens": 1, "mesures": 0 }}

Text: Il croit aux esprits et aux fantômes depuis qu'il est enfant.
Nouns: esprits, fantômes, enfant
Output: {{ "esprits": 0, "fantômes": 0, "enfant": 1 }}

Text: {text}
Nouns: {human_nouns}
Output:
\end{lstlisting}

\clearpage

\parbox{16.5cm}{\section{Examples of Gender-Fair LLM Outputs}\label{apx:gfl}}

\begin{table}[H]
\centering
\parbox{16.5cm}{\caption{Examples of gender-fair LLM outputs by model and marker type}}
\label{tab:llmgf}
\begin{tabular}{ccp{10cm}}
\toprule
Model & Marker Type & Output \\
\midrule
mistral-small & incl\_pairs & Vous soulevez un point intéressant. L'expression "c'est une égalité parfaite" est souvent utilisée dans le langage courant pour renforcer l idée de parité ou d'équilibre absolu entre deux choses. Cependant, du point de vue mathématique, une égalité est une relation binaire qui est, par définition, soit vraie soit fausse. En mathématiques, on ne parle pas d'une égalité "imparfaite" ou "parfaite" car l'égalité exprimée comme \(2 + 2 = 4\) est tout simplement vraie ou \(2 + 2 = 5\) est tout simplement fausse. Dans le langage courant, cette expression peut être vue comme un abus de langage ou une façon hyperbolique de souligner un équilibre ou une symétrie. Par exemple, si quelqu'un dit "c'est une égalité parfaite entre nos deux équipes," \textbf{il ou elle} veut dire que les deux équipes sont très équilibrées en termes de force et de capacité. Pour gérer ces situations, vous pouvez soulager ce sentiment en disant que "l'expression 'de parfaite équité' ou 'd'équilibre parfait' pourrait mieux exprimer l'idée d'une égalité absolue." \\
gemini & fem\_ending & Les médias sociaux, bien que présentant des avantages en termes de connexion et d'information, engendrent également des effets négatifs. La comparaison constante avec des versions idéalisées de la vie des autres \textbf{utilisateur·ices} peut entraîner une baisse de l'estime de soi et de l'anxiété. Une exposition excessive à des contenus négatifs, polarisants ou manipulatoires peut affecter le bien-être mental et contribuer à la propagation de fausses informations. Enfin, la dépendance aux notifications et la distraction constante peuvent entraver la concentration et la vie sociale réelle.  \\
llama & incl\_greetings & \textbf{Mesdames et messieurs}, Aujourd'hui, je voudrais vous parler d'un sujet qui concerne tous les habitants de notre planète : la sauvegarde des espèces menacées. C'est un problème immense et urgent qui nécessite notre attention et notre action. Mais avant de vous présenter les raisons pour lesquelles la sauvegarde des espèces menacées est une question essentielle, je voudrais poser une question simple : qu'adviendrait-il si nous perdions définitivement certaines espèces qui vivent sur terre? [...] \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
