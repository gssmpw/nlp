\section{Related Work}
\label{sec:related-work}

% Explained what is shapelets. 
% anomaly detection: 

\textbf{Shapelets}____ are originally proposed as time series sub-sequences that are considered as discriminating patterns for classification of temporal sequences.  The basic idea to discover shapelets is to assess all possible segments from time-series data based on a merit function that measures the predictive power of a given sub-sequence for some class labels. In ____ at first a large set of shapelet candidates are generated and their similarity to time series segments across all the training samples is computed using the brute-force algorithm. Then a decision tree algorithm is applied to recursively split training samples into different subsets by selecting a shapelet candidate to maximize the information gain for classification.
 %i.e. trying to find a shapelet such that based on its distances to time series data it is easier to split the time series data into two" purer" subset. By early abandon of distance calculation and entropy based pruning techniques the computational time for discoverying shapelets are approximately quadratic on the length of time series, i.e. $\mathcal{O}(10^{\log(\frac{I^2 m^2}{2})-1 })$, where $m$ is the length of time series and $I$ is the number of time series. This requires still a large amount of time. One of the reason it is so computational expensive is due to the necessary of one-to-one point computation between candidate shapelets and the sample time series. Therefore, other techniques trying to compress this real-valued information into discrete representation emerges, such as piecewise aggregate approximation (PAA) ____ and its extension to symbolic aggregate approximation ____. Although it has been proven that the distance between the approximated sequences lower-bounds the distance between the true represented time-series as in  ____ ____, the choice of the number of approximating elements and the granularity of each approximating element to achieve the tightest possible lower bounds are  highly data dependent. Apart from this, one better approximation method, i.e. locally percetually important point (PIP), is proposed in ____. It iteratively locates an important point that has a maximum perpendicular distance to a line connecting two adjacent previously detected points. In this way, a long time series can be downsampled into a short sequence depending on a required query size for time series comparison. One drawback of this approach is that an outlier in the time-series due to sensor noises will also be considered as a PIP, which is highly undesirable.  

 
%\textbf{Shapelets learning for time series classification}
%Instead of searching for shapelets exhaustively, different machine learning approaches are proposed to learn a shapelet, such as learning a shapelet by neural network ____.It jointly learn the optimal shapelets and the optimal linear hyper-plane for shapelet transformed distances that minimize a classification objective, such as cross-entropy function.  It can learn more generalizing shapelets that is not exactly represent in the dataset and also requires less training time. This approach is a supervised learning and it requires labeling data. 
%Besides of this, the goal of the proposed methods in ____ is to improve classification accuracy. Therefore, the learned shapelets may not necessary to be as similar as the input time serie segments as possible, if the network relies more on capturing the interaction among different shapelets for classification. 
\/*
\textbf{Shapelets learning by clustering}
Instead of finding the optimal shapelets based on labeled data, the learning of shapelets can be considered as a process of dimension reduction or data compression with each shapelet representing a possible dimension or coordinate of time series. 
% som:For this a self-organizing map can be applied ____. A map consists of two kind of nodes, i.e. input nodes with each representing a feature dimension of the input data and output nodes with each carrying weights for each input feature value. For each input data, a best matching unit in the map is selected. Centered at the best matching unit or units, a radius, which is schrinking as training time increases, is used to defined to include neighborhood units around this most matching unit or units. Their weights will then be updated based on its similarity to input data points. The benefits of self-organizing map (SOM) is that the result is visually interpretable. A continuous transition between different cluster centers can be observed.  However, comprehensive studies comparing self-organizing map to K-means clustering show that for high dimensional data SOM  performs significantly worse in terms of data points misclassified ____. A probable reason for it is that the  different density distributions in different dimensions of input data can not be locally preserved by lower-dimensionality of SOM.    
One of the best performing algorithm is K-Shape ____, which can learn shapelets that are shift and scale-invariant.
%K-means as a traditional method for clustering tends to take an average over the time series segments that are similar to each other.  As a result, the shape of the learned segments is not so sharp as shown in ____. Hence, a better approach has been proposed to learn shapelets shift and scale invariant ____. It is referred to as K-shape.  
It is based on K-Means but uses normalized cross-correlation as a metric to measure time-series similarity that is shift-invariant and solves the maximization of Rayleigh Quotient for cluster centroid update. 
%As a result, the updated centers are the eigen-vector along the direction of the covariance matrix of the input data that are most varying. However, the way to assign a sample to a cluster in k-shape is similar to k-means by choosing the cluster centroid that has the smallest distance to the considered data sample. Therefore, k-Shape, similarly as k-means can get stuck in a local minimum even on perfect data sets and it tends to cluster data when there is no cluster. It is poor at discovering internal data structure. 
Recently, the artificial neural network is emerging due to its high representation power and its ability to learn complex cluster separation boundaries.
%Other two neural network approaches: 
Inspired by ____ to use a neural network to learn shapelets, ____ proposes to learn shapelets in an unsupervised way. Instead of using labeled data, ____ enforces that similar input data should have similar pseudo-class labels by spectral analysis. It jointly optimizes candidate shapelets, class boundaries, and pseudo labels. %Motivated by the advantages of using spectral analysis to preserve local structures and in order to circumvent the drawback of k-means, i.d. unable to recover local structure, a novel unsupervised learning method with deep embedding and spectral analysis to preserve data internal structure is proposed in this paper. 
*/
\textbf{Unsupervised feature selection} entails shapelet learning, because a shapelet can also be considered as a time series feature. The challenge of feature selection without class labels is to discover uncorrelated and discriminative features. Different algorithms have been introduced. One of the state-of-art algorithms is using dictionary learning to discover shapelet in a generative way ____. It considers a shapelet as an atom of dictionary and requires sliding the original time series into sub-sequences of the same length as shapelet, i.e., a dictionary atom. By trying to reconstruct slid sub-sequences, the algorithm jointly optimizes the shapelet dictionary and its corresponding sparse encoding for slid sub-sequences. This can result in losing global information of the original time-series and falling into the pitfalls of time series sub-sequence clustering ____. Besides focusing on the shape feature of time series, each sample point in a time series can also be referred to as a potential feature similar to using pixels as input features for image clustering. ____ presents a $l_{2,1}$-norm regularized discriminative feature selection  algorithm for unsupervised learning (UDFS) . 
%While the local manifold structure is preserved, a discriminative analysis is incorporated to jointly optimize feature selection and the linear separable hyper-plane using these features. And ____ complicates feature learning by considering dataset in spatio-temporal domains, such as videos. It proposes a new descriptor named spatio-temporal hierarchical matching pursuit, i.e., ST-HMP.
%It uses K-Means algorithm for sparse vector quantization, i.e., K-SVD, to learn code-book in an unsupervised manner. And from the learned code-books, the spatio-temporal hierarchical matching pursuit, i.e., ST-HMP, calculates sparse code sequences and then pools them in a spatio-temporal pyramid manner to achieve robustness against noises that can cause spatial and temporal variation. 
%To further increase the predicted power of smaller set of selected features, a discriminative and uncorrelated feature selection with constrained spectral analysis in unsupervised learning (DUCFS) is proposed ____. At first it represents given training data and their pairwise constraint with an intrinsic graph for must link data and a penalty graph for cannot link data. Then follows a constrained spectral analysis to reduce the dimension of data considering manifold structure. Then features are selected by utilizing regularized regression under uncorrelated constrain. However, the selected features are not interpretable and difficult to understand.    

The presented algorithm in the scope of this paper differs from the above mentioned methods, in which we employ t-stochastic neighborhood embedding to perform a non-linear mapping between original data and shapelets and also attains the interpretability of the selected features.
 % paper structure