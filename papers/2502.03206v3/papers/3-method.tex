
\section{\our}
\subsection{A General Command-Space for Humanoid Locomotion}\label{sec:command-space}
% The versatile control of humanoid robots can be decoupled into the control of upper and lower body. 
% In this work, we are more focused on the diversity of lower body movements. 
% The control of the both arms is achieved through teleoperation.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/CommandVisualize_V4.pdf}
%     \label{fig:CommandGait}
%     \vspace{-24pt}
%     \caption{\small \textbf{Visualization of parts of commands}. The side view (left) highlights the linear velocity, foot swing height, and body pitch commands. The top view (lower right) illustrates the angular velocity and waist roll commands. The body height command is shown in the comparison (upper right). Illustrated with Unitree H1 robot.}
% \end{figure}
We define the command space of the humanoid whole-body controller $\mathcal{C}=\mathcal{K}\times\mathcal{B}$ by two sets of commands, the task commands $\mathcal{K}$ and the behavior commands $\mathcal{B}$. The task commands determine a goal for the robot to reach, typically for movement, while the behavior commands construct the specific behavior pattern of the humanoid robots. In this work, we specify the task command as the target velocity $v_{t} \in \mathbb{R}^3$ (including the longitudinal and horizontal linear velocities $v_{t,x}$, $v_{t,y}$ and the angular velocity $\omega_t$) at each time step $t$. As for the behavior command, we define the behavior command $b_t$ as a vector:
\begin{equation}
\begin{aligned}
{\mathopen{\bBigg@{1.5}{[}}}          
         \underbrace{f_t,
         l_t}_{\text{foot}},
         \underbrace{h_t,
         p_t,
         w_t}_{\text{posture}},
         \underbrace{\psi_t,
         \phi_{t,1},
         \phi_{t,2},
         \phi_{t,\text{stance}}}_\text{gait}
     {\mathclose{\bBigg@{1.5}]}}~,
\end{aligned}
\end{equation}
where
$f_t \in \mathbb R$ is the gait frequency and $l_t \in \mathbb{R}$ is the maximum foot swing height, both of which can be explained as foot behaviors.
Besides, $h_t \in \mathbb{R}$ represents the body height, $p_t \in \mathbb{R}$ is the body pitch angle, and $w_t \in \mathbb{R}$ is the waist yaw rotation. These commands can be regarded as controlling the posture behavior.
% $h_t \in \left[1.4, 1.8\right]$ represents the body height, $p_t \in \left[-0.1, 0.8\right]$ is the body pitch angle, $w_t \in \left[-1.0, -1.0\right]$ is waist (yaw) rotation, and $s_t  \in \left[0.1, 0.4\right]$ is the maximum foot swing height.
% represents the humanoid robotâ€™s squatting and bending movements

Beyond the commands above, we further introduce distinct gaits, such as walking, standing, jumping, and hopping. To do so, we refer to legged gait control~\cite{2021BipedalPeriodicReward, margolis2022walktheseways} and define $\phi_i \in [0, 1)$, $i = 1, 2$ as two 
% independent was removed because of phi_2 = phi_1 + psi in sometime
time-varying periodic phase variables to represent the humanoid gaits, on behalf of two legs (feet).
These two phase variables can either be set as constants, or be computed by the phase offset $\psi$ and the gait frequency $f_t \in \mathbb{R}$ at each time: 
\begin{equation}\label{eq:phase}
\begin{aligned}
& \phi_{t+1, 1} = \left ( \phi_{t,1} + f_t \times \text{d}t \right)~, \\
& \phi_{t+1, 2} = \left ( \phi_{t+1, 1} + \psi \right)~,
\end{aligned}
\end{equation}
where $\text{d}t$ is a discrete time step. When following the computation of Eq. \ref{eq:phase}, each $\phi_i$ loops in a range of $[0,1)$, resulting in repeated phase cycles. 
$\phi_{\text{stance}} \in [0, 1]$ is the duty cycle, which divides the gait cycle into two stages: stance (i.e., foot in contact with the ground) when $\phi_i < \phi_{\text{stance}}$, and swinging (i.e., foot in the air) otherwise. $f$ is the stepping frequency, determining the wall time of each gait cycle.


\noindent\textbf{Humanoid gait control.} 
\label{Humanoid gait control}
We consider four distinct standard gaits in this project, \textit{i.e.}, \textit{walking}, \textit{jumping}, \textit{standing}, \textit{hopping}\footnote{We note that \textit{running} can be further derived from the \textit{walking} gaits via high-velocity and small duty cycle commands, which promotes the prolonged flight of both feet.}
By constructing the behavior commands above, we can adjust the phase offset $\psi$, the duty cycle $\phi_{\text{stance}}$, and the phase variable $\phi_i$ for each leg to control the humanoid robots in versatile gaits.
In this work, we only consider standard gaits, so we set the phase offset $\psi = 0.5$ for \textit{walking} gaits \cite{2021BipedalPeriodicReward}, since the phase difference between the left and right foot is half a cycle;
Regarding \textit{jumping} gaits, the phase of the left and right foot is the same, thus, we set $\psi$ to 0. 
As for the \textit{standing} and the \textit{hopping} gaits, a certain foot of the robot is always in two states of contact or non-contact with the ground, which motivates a constant $\phi_i$ (resulting in constant contact probability of either 0 or 1, and $\psi$ is not working). In particular, for the standing gait, we set $\phi_i=0.25$ for both feet; and for the hopping gait, $\phi_i=0.75$ for the flying leg, and $\phi_i$ of the other leg steps with frequency $f$.
% Due to the rolling change of phase variables $\phi_i$ leading to a continuous transformation of the contact state between the foot and the ground, these modes are rather difficult to generate. 
% To resolve this challenge, we randomly sample the $\phi_i$ as a constant value for learning the standing and hopping gaits. 
% For the standing gait, the $\phi_i$ of both feet is set to 0. The $\phi_i$ of the flying leg is set to 0.75 and the $\phi_i$ of another leg steps with frequency $f$ for hopping gait. 
The $\phi_{\text{stance}}$ determines the time ratio of stance and swinging during a gait cycle, and a smaller $\phi_{\text{stance}}$ will promote longer leg flight time. 
To represent a smooth switch between stance and swinging, 
we introduce the expected contact probability function $C(\phi_{t,i})$ for leg $i\in\{1,2\}$ at each time step $t$ as:
\begin{align}
% \begin{aligned}
\begin{split}
C(\phi_{t,i}) &= \Phi(\bar{\phi}_{t,i} / \sigma) [1 - \Phi((\bar{\phi}_{t,i} - 0.5)/\sigma)] \\
& \quad + \Phi((\bar{\phi}_{t,i}- 1)/\sigma) [1 - \Phi((\bar{\phi}_{t,i} - 1.5)/\sigma)]~,
\end{split}
\\
\bar{\phi_i} &= \left\{\begin{aligned}
&0.5 \times \frac{\phi_i}{\phi_\text{stance}}, 
&\phi_i < \phi_\text{stance} \\ 
& 0.5 + 0.5 \times \frac{\phi_i - \phi_\text{stance}}{1 - \phi_\text{stance}},   &\phi_i \ge \phi_{\text{stance}}
\end{aligned}\right.~,\label{eq:bar_phi}
% \end{aligned}
\end{align}
%OLD VERSION
% \begin{align}
% % \begin{aligned}
% \begin{split}
% C^t_i(\phi_i) &= \Phi_i(\bar{\phi_i}) [1 - \Phi_i(\bar{\phi_i} - 0.5)] \\
% &+ \Phi_i(\bar{\phi_i}- 1) [1 - \Phi_i(\bar{\phi_i} - 1.5)]~,
% \end{split}
% \\
% \bar{\phi_i} &= \left\{\begin{aligned}
% &0.5 \times \frac{\phi_i}{\phi_{stance}}, 
% &\phi_i < \phi_{stance} \\ 
% & 0.5 + 0.5 \times \frac{\phi_i - \phi_{stance}}{1 - \phi_{stance}},   &\phi_i > \phi_{stance}
% \end{aligned}\right.~,\label{eq:bar_phi}
% % \end{aligned}
% \end{align}
where $\bar{\phi_i} \in [ 0, 1 ]$ is a homogenized phase variable that maps the $\phi_i$ of the stance and swinging phases to intervals $[0, 0.5]$ and $[0.5, 1]$ according to $\phi_{\text{stance}}$, as computed in \eq{eq:bar_phi}. $\Phi(\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution $N(0, 1)$. The standard deviation $\sigma$ allows for the relaxation of switching points ($\bar{\phi_i} = 0, 0.5$) to switching interval ($\bar{\phi_i} \in [-3 \sigma, 3 \sigma], [0.5 -3 \sigma, 0.5 + 3 \sigma]$)~(see \fig{fig:contact_curve} for a detailed explanation). 
Intuitively, $C(\phi_{t,i})$ is the probability of leg $i$ coming into contact with the ground.
As one may notice, when $\bar{\phi}_{t,i} \in [0, 0.5]$, the first term of $C(\phi_{t,i})$ is dominant; otherwise, the second term becomes dominant.
In this work, we set a constant $\phi_{\text{stance}}=0.5$ for all supported gaits in all time steps, which means half-time stance/swinging during one cycle.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/contact-curve.pdf}
    \caption{\small The expected contact probability function $C(\phi_{t,i})$ in the loose and normal formulation. The larger $C(\phi_{t,i})$, the higher the expectation of contact with the ground. The CDF of the normal distribution is introduced into the normal contact probability function to relax the constraint of the foot contact at the switching boundary, resulting in a smooth transition between the swing and the stance phase.}
    \label{fig:contact_curve}
    \vspace{-20pt}
\end{figure}

We highlight that \our trained \textit{one single policy} for the standing, walking, and jumping gaits, and an independent policy for the hopping gait.


\subsection{Detailed Observation} 
\label{sec:OBS}
In our asymmetric actor-critic framework, the observation for the critic network $o^{V}_t$ obtains all information related to the environment,
including proprioceptive observations $o_t^{\text{pro}}$, privileged observations $o_t^{\text{pri}}$, terrain observations $o_t^{\text{ter}}$, commands $c_t$, and an indicator signal $I(t)$. Regarding the actor network, its available observation $o_t^{\pi}$ only contains history of proprioceptive observations within last $k$ steps $o_t^{\text{his}} = (o_{t-k+1}^{\text{pro}}, \dots, o_t^{\text{pro}})$, commands $c_t$, and the indicator signal $I(t)$.
The proprioceptive $o_t^{\text{pro}} \in \mathbb{R}^{63}$ consists of angular velocity and gravity projection in the robot's base frame, joint position, joint velocity, and previous policy output $a_{t-1}$. 
The privileged observations $o_t^{\text{pri}} \in \mathbb{R}^{24}$ contain the linear velocity, the base height error, foot clearance, friction coefficient of the ground, feet contact forces, and collision detection of the link (trunk, hip, thigh, shank, shoulder, and arm). The terrain observations $o_t^{\text{ter}} \in \mathbb{R}^{221}$ are samplings of terrain elevation points around the robot.

\noindent\textbf{Commands.}
The commands $c_t=[v_t, \tilde{b}_t]$ includes the task command (\textit{i.e.}, target velocity $v_t$ in this work) and the extended behavior command $\tilde{b}_t \in \mathbb{R}^{9}$, 
% where we extend the behavior command $b_t$ defined above with two additional terms $[\sin\left(2\pi\phi_1^t\right), \sin\left(2\pi\phi_2^t\right)]$ representing the contact of both feet. 
where we extend the behavior command $b_t$ defined above through replacing the phase variables $\phi_i,~ i= 1, 2$ with two additional clock functions $[Cl_L(t), Cl_R(t)]=[\sin\left(2\pi\bar\phi_{t,1}\right), \sin\left(2\pi\bar\phi_{t,2}\right)]$ representing the contact of both feet, where $\bar \phi_{t,i},~ i=1,2$ are the homogenized phase variables defined in \eq{eq:bar_phi}.
Note that the sine function $\sin\left(2\pi\bar\phi_{t,i}\right), i= 1, 2$ is a gait cycle contact indicator function, designed for a smoother transition between swinging and stance phases. An illustrative explanation of the phase variables and the clock function is shown in \fig{fig:GaitPhiPsiClock}.  
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{imgs/phipsi_V5.png}
    \vspace{-6pt}
    \caption{\small \textbf{Phase variables and clock functions under different gaits.} \textbf{Left}: The \textcolor{myPurple}{purple} ring represents the phase variable $\phi_1$ for the left foot, while the \textcolor{myGreen}{green} ring represents the phase variable $\phi_2$ for the right foot. $\psi$ is the phase offset from $\phi_1$ to $\phi_2$. The dividing phase between stance (marked in \textcolor{myBlue}{blue}) and swing (marked in \textcolor{myYellow}{yellow}) is the duty cycle $\phi_{\text{stance}} (0.5)$. \textbf{Right}: The \textcolor{myPurple}{purple} line depicts the clock function $Cl_L(t)$ for the left foot over a cycle, while the \textcolor{myGreen}{green} line represents the clock function $Cl_R (t)$ for the right foot over a cycle. 
    }
    \label{fig:GaitPhiPsiClock}
    \vspace{-10pt}
\end{figure}

\noindent\textbf{External upper-body control.}
We want to build a general humanoid whole-body controller that also supports external upper-body control (e.g., teleoperation). Thereafter, we introduce a binary indicator function $I(t)$ to identify whether an external upper-body controller is involved.
If there is no external upper-body control signal involved, the upper-body joints are controlled by our developed whole-body controller by default, which swings the arms naturally.

\subsection{Reward Design for Policy Learning}
Our humanoid whole-body controller is obtained through an asymmetric actor-critic training paradigm via reinforcement learning (RL). To learn a policy with general and diverse behaviors, we design a set of reward functions, which mainly consist of three parts: task rewards, behavior rewards, and regularization rewards. The details of the rewards are concluded in \tb{tab:rewards}.

\input{tables/rewards}

The \textit{task} rewards are meant to track any task command $k$. In this work, it is the target velocity $v$, including the linear and angular velocities. 
% The \textit{regularization} rewards are items for regularizing the behaviors during training.
The \textit{regularization} rewards take into account the performance of physical hardware and impose constraints on the smoothness and safety of the locomotion.
These are commonly used in previous works~\cite{rudin2022learning}.
In this work, since we want to build a general whole-body controller to support versatile locomotion behaviors for humanoid robots, we introduce a set of \textit{behavior} rewards to encourage the robots to track any behavioral commands $b$, shown below.

For most behavior commands, including body height $h$, body pitch $p$, and waist rotation $w$, we simply formulated the rewards with mean squared error (MSE):
\begin{equation}
\begin{aligned}
r_t^{\text{cmd}} = \left \| e_t^{\text{target}} - e_t^{\text{cmd}} \right \|^2~.
\end{aligned}
\end{equation}
Beyond these simple tracking rewards, we further introduce periodic contact-swing rewards $r_t^{\text{contact}}$~\citep{2021BipedalPeriodicReward, margolis2022walktheseways} and the foot trajectory rewards $r_t^{\text{traj}}$ to help generate complicated gaits.

The periodic contact-swing reward $r_t^{\text{contact}}$ is designed for precise adjustments between swinging and stance in different gaits, according to $\phi_i$. 
Since humanoid gaits can be expressed as different combinations of contact sequences, like foot contact forces and velocities, we define the periodic contact-swing rewards $r^{\text{contact}}_t$ over them to generate desired contact patterns.
Based on $C(\phi_{t,i})$ defined as \eq{eq:bar_phi}, we then construct the periodic contact-swing rewards $r_t^{\text{contact}}$ to encourage humanoid robots to learn specific contact modes and generate various humanoid gaits:
\begin{equation}
\begin{aligned}
r_t^{\text{contact}} = & -\sum_{i=1}^{2} [1-C(\phi_{t,i})]\left[1-\exp\left(\left \|  f^{\text{foot}, i}_t \right \|^2 / \sigma_{cf} \right)\right] \\
& -\sum_{i=1}^{2} C(\phi_{t,i})\left[1-\exp\left(\| v^{\text{foot}, i}_{t, xy} \|^2 / \sigma_{cv} \right)\right]~,
\end{aligned}
\end{equation}
where $f_t^{\text{foot}, i}$ denotes the foot contact force and $v^{\text{foot}, i}_{t, xy}$ is the foot velocity. $\sigma_{cf}$ and $\sigma_{cv}$ are hyperparameters, fine-tuned according to the range of previous work \citep{margolis2022walktheseways} (We set the value as $\sigma_{cf}=50$, $\sigma_{cf}=5$). Note that during the stance phase, this reward function penalizes the foot velocities and ignores the foot contact force; on the other hand, during the swing phase, it penalizes the foot contact force and ignores the foot velocity.

Except for the contact in gait control, we also require the foot to smoothly reach the highest point and fall down, ensuring a precise and controllable swing. We introduce the foot trajectory reward $r_t^{\text{swing}}$ to achieve this:
\begin{equation}\label{eq:foot_traj}
\begin{aligned}
r_t^{\text{swing}} = & \sum_{i=1}^2 [1-C(\phi_{t,i})]\left \| l_t^{\text{target}, i} - l_t^{\text{foot},i} \right \|^2~.
\end{aligned}
\end{equation}
Note that in \eq{eq:foot_traj}, $l_t^{\text{foot},i}$ denotes the actual swing height of foot $i$, $C(\phi_{t,i})$ is the expected contact probability function. 
$l_t^{\text{target}, i}$ is the target swing height, derived from a desired foot trajectory, as discussed below.

A desired foot trajectory should typically require the fulfillment of three key criteria: 
1) zero foot velocity and acceleration during the stance phase; 2) zero foot velocity and acceleration at the end of the swing phase; and 3) continuity of both foot velocity and acceleration during the transition between the two phases. This is beneficial for enhancing motion stability and reducing energy consumption. 
In this work, we follow the experience in robot kinetics and quadruped robots~\citep{2016RasHermiteSplines, LocManMPC2021RAL}, and incorporate the quintic polynomial trajectory to compute the target swing height $l_t^{\text{target}, i}$ at each control step: % The Bezier and polynomial curves are commonly employed in foot trajectory planners for quadruped robots.
\begin{equation}
\begin{aligned}
l_t^{\text{target},i} = \left\{\begin{aligned} 
&l_t \sum_{k=0}^{5} a_k \left(0.25 - \left| \bar{\phi}_{t,i} -0.75 \right| \right)^k   , &\bar{\phi}_{t,i} >0.5\\ 
& 0, & \bar{\phi}_{t, i} < 0.5 \end{aligned}\right.~.
\end{aligned}
\label{eq:ltarget}
\end{equation}
Here $l_t$ is the foot swing height command, and the polynomial coefficient $a_k$ is determined based on the homogenized phase variable $\bar{\phi_i}$, as well as the boundary conditions of swing position, velocity, and acceleration. A detailed explanation of the calculation process is provided in the \ap{ap:Foot Target}. Note that \eq{eq:ltarget} only defines the target trajectory in the $z$-axis. On natural terrains, the robot's precise foothold planning is not required. As for swing trajectories in the $x$-axis and the $y$-axis, which determines the stride, they can be computed based on the gait frequency $f$ and the velocities $v,\omega$~\citep{gehring2013control,linearmpc2018cheetah}. 
% \yufei{The complete foot trajectory of a robot includes trajectories in the x, y, and z directions. Equation (10) only represents the trajectory in the z direction, and x, y determines the robot's stride. However, in this paper, we introduce gait frequency and velocity, which jointly determine the length of the trajectory in the x and y directions. Therefore, there is no need to set a reward. Do we need the derivation formulas for x, y, and stride length, as well as the reason for not displaying the reward for constructing the x and y trajectories?}

\subsection{Mirror Function and Symmetry Loss}
Natural and symmetrical motion behavior is gradually mastered by humans through acquired learning, due to its inherent elegance and efficiency in minimizing energy expenditure. Humanoid robots, with highly biomimetic mechanisms, also have symmetrical structural features.
However, without prior knowledge, the symmetrical morphology information is difficult to be explored by the policy, especially for policies that generate diverse behaviors. This makes the initial exploration much more difficult, making the policy easily fall into local optima and leading to unnatural movements.
To leverage the advantage of this morphological symmetry and inspired by \cite{symmetry2018tog}, we proposed the mirror function $\mathcal{F} \left( \cdot \right)$ for a humanoid robot to encourage the policy to generate symmetric and natural motion. Under such a symmetrical structure, ideally, the policy output should satisfy:
\begin{equation}
\begin{aligned}
\pi (o_t^{\pi}) = \mathcal{F}_a (\pi(\mathcal{F}_o (o_t^{\pi}))) ~.
\end{aligned}
\end{equation}
Intuitively, the mirror function produces a mirror output symmetric to the X-Z plane.
Here $\mathcal{F}_a$ and $\mathcal{F}_o$ are called \textit{action mirror function} and \textit{observation mirror function}, respectively, which map actions and observations to their mirrored version. Derived from these symmetric functions, we define a symmetry loss function $\mathcal{L}_\text{sym}$. The policy learning objective for controlling robots with symmetrical structures can be written as:
\begin{equation}\label{eq:sym-obj}
% \begin{aligned} %OLD VERSION
% \mathcal{L}(\pi)_\text{sym} = \mathbb{E}_{\tau \thicksim {\pi}} [\sum_{t=0}^{\infty}\Vert \pi \left({o_t^{\pi}} \right) - \mathcal{F}_{a} \left( \pi \left( \mathcal{F}_{o}  \left( {o_t^{\pi}}\right) \right) \right) \Vert^2]~,
% \end{aligned}
\begin{aligned}
\mathcal{L}_\text{sym} = \sum_t\left\| \pi \left({o^{\pi}_t} \right) - \mathcal{F}_{a} \left( \pi \left( \mathcal{F}_{o}  \left( {o^{\pi}_t}\right) \right) \right) \right\|^2~,
\end{aligned}
\end{equation}
The $\mathcal{L}_\text{sym}$ is independent of the RL objective, making it easy to extend to different RL algorithms. 
It is worth noting that the symmetric loss function is in fact encouraging symmetric actions on symmetric states (and commands), and it can be utilized for behaviors from symmetric ones (like walking and jumping) to asymmetric ones, such as hopping gaits, where hopping with the left foot is symmetric to hopping with the right one.

\noindent\textbf{Overall training objective.}
\our adopt an asymmetric actor-critic framework~\citep{nahrendra2023dreamwaq}, taking PPO~\citep{schulman2017proximal} as the RL algorithm to train the whole-body policy. Therefore, the total training objective can be written as:
\begin{equation}\label{eq:total-obj}
\mathcal{L} = \mathcal{L}_\text{AAC} + \beta \mathcal{L}_\text{sym}, \end{equation}
where $\beta$ is a weight coefficient to balance between minimizing the RL objective and symmetry gait (we simply set $\beta$ = 0.5 in our practice). 
We implemented a critic network, an actor network, along with the privileged encoder, all as Multi-Layer Perceptrons (MLPs). The actor network, combined with the encoder, can be directly deployed onto the real robot at a control frequency of 50 Hz. The sampled trajectories have a maximum length of 1000 timesteps, and the termination conditions include trunk collision with the ground or other links, as well as large body inclinations.

% \subsection{Policy Architectures}
% With the observation setting mentioned in subsection \ref{sec:OBS}, the critic network takes $o^{\text{critic}}_t$ to predict the expected discounted returns, while the actor network first takes $o^{\text{his}}_t$ to estimate the key privileges information (linear velocity, robot's body height and robot's feet swing height) and then output action $a_t$.
% \begin{equation}
% \begin{aligned}
%     &z^{\text{pri}}_{t} = \mathcal{E} (o^{\text{his}}_t) \\
%     &a_t = \mathcal{A}(z^{\text{pri}}_t, o^{\text{pro}}_t, c_t, I(t))
% \end{aligned}
% \end{equation}
% All components of the network mentioned above are implemented as Multi-Layer Perceptrons (MLPs) within 3 layers, which can be directly deployed on robot hardware by exporting the model checkpoint into Just-In-Time (JIT) format.

% \our supports to train the whole-body-control policy in either a teacher-student mode and an asymmetric training mode.
% The teacher policy is composed of a terrain encoder, a privileged encoder, a low-level network, and a critic, which are all constructed by Multi-layer Perceptrons (MLPs). The terrain encoder and the privileged encoder encode terrain observations $i_t^{e}$ and privileged observations $s_t^{p}$ into a concatenated latent representation $z_t$.
% The low-level network accepted $z_t$, proprioceptive perception $o_t^p$, task commands $v_t$ and behavior commands $\tilde{b}_t$ and predict the action $a_t$. The critic provide the expected value based on $i_t^{e}$, $s_t^{p}$, $o_t^{p}$, $b_t$.
% In the first stage, \our trains the teacher with $L(\theta)$ as defined in \eq{eq:sym-obj}.

% In the second stage, the student policy $\pi_\text{student}$ is trained to imitate the action of $\pi_\text{teacher}$ only through proprioceptive observations. The $\pi_\text{student}$ inherits teacher's low-level network, and adopts a three-layer Long Short-Term Memory (LSTM) as a historical encoder, which is trained to predict the latent representation $z_t$. Since the teacher policy can be regarded as expert demonstration with symmetry, we did not add the additional symmetry objective during the student training. More details of networks can be found at \tb{}.


\subsection{External Upper-Body Intervention Training}
\label{sec:intervention}
So far we learned a whole-body controller, which controls the upper and lower body jointly.
Nevertheless, the goal of this work is not a controller specifically designed for locomotion tasks, but to build a unified and general humanoid controller that can serve as a basic support for loco-manipulation tasks. In other words, our controller should also support flexible and precise control of the upper body (arm and hands). Different from some previous works~\citep{he2024omnih2o,he2024hover} that augment the command space with upper body commands (\textit{e.g.}, arm joint positions), we consider decoupling the upper body control as external control intervention by teleoperation signals~\citep{cheng2024tv,lu2024pmp} or retargeted motion joints~\citep{cheng2024expressive,2024exbody2}, while not affecting the lower-body gaits, due to their high control precision.
Our solution is sampling alternative actions to replace the upper-body actions produced by the whole-body policy during training, making the policy robust to any intervention.

\noindent\textbf{Switching between whole-body control and intervention.}
Denote $I(t)$ a binary indicator function for whether the external control signal intervenes at each time step $t$, we assign a small probability of $p$ ($p=0.005$ in this work) to reverse $I(t)$.
This leads the expected length of a continuous sequence without changing the upper-body control mode to be $\sum_{n=1}^{\infty}np(1-p)^n=\frac{1-p}{p}$,
ensuring infrequently switching between two control modes and most of the trajectories are either long sequence of whole-body controlling or intervention, preventing rapid switches.
% By this way, the expectation of a continuous sequence length with or without changing upper-body states is $(1-p)/p$, taking into consideration of three situations: long sequence of whole-body controlling, long sequence of intervenes and the switching between them.

% two sources: 1) the filtered motion capture dataset AMASS~\citep{he2024omnih2o}, which is retargeted from the SMPL model to the morphology of our robot;
% ~\citep{smpl, expressive_humanoid}; NOT FOR THE NEWEST VERSION
% During training, we mix the two sources in a ratio of 0.7 for the noises to 0.3 for the motion dataset.
\noindent\textbf{Intervention sampling.}
The intervened actions of the humanoid upper body are sampled from uniform noises, which introduce the potential for collisions with the body, simulating erroneous operations during external intervention. 
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{imgs/Noise_V2.pdf}
\vspace{-8pt}
\caption{\small \textbf{Intervention noise curriculum.} We illustrate sampled noise by visualizing the hand positions relative to the visualized robot hand joints. \textbf{Top 1-3}: Noise samples of three curriculum stages with noise levels ranging from small to large. These noises are only relative to the robot hand joints as visualized in the figures. \textbf{Bottom 4}: Front and top views of the noise samples from the final noise curriculum.}
\label{Fig: Noise}
\vspace{-10pt}
\end{figure}

\noindent\textbf{Noise intervention interpolation.}
To prevent meaningless jitters caused by noise intervention sampling, the intervention action $a_{\text{noise}}^{\text{targ}}$ is randomly sampled in the action space every $t_\text{interval} = 90$ time steps. 
During the first two third time steps in the interval, linear interpolation is applied to smoothly transition the intervention joint positions from the initial pose $a_{\text{noise}}^{\text{init}}$ to the target pose $a_{\text{noise}}^{\text{targ}}$, while the target intervention action is maintained for the remaining time steps.
\begin{equation}
    \begin{aligned}
        &a^{\text{interv}}_{\text{noise},t} = (1-r)a_{\text{noise}}^{\text{init}} + r a_{\text{noise}}^{\text{targ}} ~, \\
        &r = \min \left(1, \frac 3 2\frac{t-t_0}{t_{\text{interval}}} \right) ~.
    \end{aligned}
\end{equation}
In this equation, $r$ is the ratio for the linear interpolation and $t_0$ is the sampled time.
% Old Version
% And the intervention action going to be executed is 
% \begin{equation}
%     a^{\text{interv}}_{\text{noise}, t} = \text{clip}(a_t^{\text{policy}}+\epsilon, q_t - r, q_t + r)~,
% \end{equation}
% where $a_t^{\text{policy}}$ is the output of the whole-body control policy, $\epsilon \sim U(-1,1)$ is the noise, $q_t$ is the joint position of upper body at time step $t$, $r$ is the maximum action rate. 

\noindent\textbf{Reward mask.}
When the intervention is involved, we mask the regularization rewards of the upper body during training, in order to eliminate the potential conflict of the policy output that tries to take over the upper body.

\noindent\textbf{Noise curriculum.}
The replaced intervention action $a_t^{\text{interv}}$ is gradually transited from the policy action $a_t$ to the sampled noise $a^{\text{interv}}_{\text{noise}, t}$:
\begin{equation}
    a_t^{\text{interv}} = \alpha a^{\text{interv}}_{\text{noise}, t} + (1-\alpha) a_t~,
    \label{eq:noisecurr}
\end{equation}
where the smoothing factor $\alpha$ increases per the progression of the intervention curriculum. In detail, $\alpha$ increases by 0.01 when both the linear and angular velocity tracking rewards exceed predefined thresholds. Conversely, if either of the velocity rewards fails to reach two-thirds of these thresholds, $\alpha$ is decreased by 0.01. The noise curriculum is illustrated in \fig{Fig: Noise}.

\subsection{Curriculum Learning}
Directly learning a diverse policy from manual rewards presents significant challenges due to the simultaneous optimization and exploration of multiple objectives. We thereby propose a curriculum learning approach to improve training efficiency. In particular, we split two distinct parallel robot training groups: an ``agile group'', tasked with learning high-speed, agile locomotion, and an ``intervention group'', focused on developing a control policy for managing external upper-body interventions.
% \input{tables/curriculum_initial_range}
At the beginning of training, each group of robots randomly samples one specific gait from four humanoid gaits, \textit{i.e.}, standing, walking, jumping, hopping. The remaining behavioral commands $(f_t, l_t, h_t, p_t, w_t)$ and the task commands $v_t$ are uniformly sampled from the specified ranges, which can be further referred to in \tb{tab:commands}. 
% \wentao{If there exists a better and simpler way to say the initial range than the table? }
Following \cite{wjzamp}, we employ a terrain curriculum for both groups, which consists of continuous rough terrain. Once the robot successfully masters the most challenging terrain, we keep that terrain and initiate an intervention noise curriculum and a speed curriculum simultaneously.
On the one hand, the speed curriculum only works for the agile group, meant to learn high agility, which gradually increases the speed commands $v_t$ following a grid adaptive curriculum strategy~\citep{margolis2022rapid}.
On the other hand, the intervention noise curriculum as described in \se{sec:intervention} works for the intervention group, focused on working with arbitrary upper-body intervention signals.




