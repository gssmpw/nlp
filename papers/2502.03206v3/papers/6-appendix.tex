\newpage
\appendices

\section{Extended Background}
\label{ap:bk}
% \subsection{Humanoid Whole-Body Control as Reinforcement Learning}
% To support various high-level functionalities and allow the humanoid robot to perform complicated tasks, we will need a whole-body controller that serves as a basic component for controlling the robots.
% Formally, we can define the humanoid whole-body control tasks as a Partial Observable Markov Decision Process (POMDP) defined by tuple $\mathcal M = \{\mathcal{S}, \mathcal{O}, \mathcal{C}, \mathcal{A}, T, r, \gamma\}$, where $\mathcal{S}, \mathcal{O}, \mathcal{C}, \mathcal{A}$ are the state, observation, command and action spaces respectively. $T(\cdot|s_t, a_t)$ is the transition density when action $a_t$ received at state $s_t$. Reward functions $r$ is defined typically as the negative distances $D(\cdot)$ or the positive similarity $S(\cdot)$ between the current robot state and the desired robot state $s_t^{c_t}$ which is decided by the actual command $c_t$:
% \begin{equation}
%     \centering
%     \begin{aligned}
%     &r_{\text{negative}}(s_t, a_t, c_t) = - D(s_t, 
%     s_t^{c_t}) \\
%     &r_{\text{positive}}(s_t, a_t, c_t) = S(s_t, s_t^{c_t})
%     \end{aligned}
%     ~.
% \end{equation}
% And $\gamma$ is the discount factor. 

% \minghuan{The two subsections need to be rephrased as I just copied from another doc.}

\subsection{Proximal Policy Optimization}
Proximal policy optimization (PPO)~\citep{schulman2017proximal} is one of the popular algorithms that solve reinforcement learning (RL) problems. 
The goal of RL is to find the optimal policy $\pi^*: \mathcal{O} \times \mathcal{C} \to \mathcal{A}$ for command tracking that maximizes the expected discounted return:
\begin{equation}
    \pi ^{*} = \mathop{\arg \max}_{\pi} \mathbb E_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r(o_t, a_t, c_t)\right]
\end{equation}
The basic idea behind PPO is to maximize a surrogate objective that constrains the size of the policy update. In particular, PPO optimizes the following objective:
\begin{equation}
    \mathcal L_{\text{policy}} = \bbE_{\pi}[\min(rA, \text{clip}(r,1-\epsilon,1+\epsilon)A)],
\end{equation}
where $r=\frac{\pi(a|o,c)}{\pi_{old}(a|o,c)}$ defines the probability ratio of the current policy and the old policy at the last optimization step, $A$ is the advantage function, which is calculated by learning the value function:
\begin{equation}
\begin{aligned}
    &\mathcal L_{\text{value}} = \bbE_\pi\left[\|V_\pi(o,c) - V^{\text{targ}}(o,c)\|^2\right]~, \\
    &A(o,a,c) = \sum_{t} \gamma^t r(o_t,a_t,c_t) - V(o,c) |_{o_0=o,a_0=a,c_0=c}~,
\end{aligned}
\end{equation}
where $V^{\text{targ}}$ is the target value function, defined as the expected return on the state $o,c$:
\begin{equation}
    V^{\text{targ}}(o,c) = \bbE_{\pi}\left [\sum_{t} \gamma^t r(o_t,a_t,c_t) |o_0=o,c_0=c\right]
\end{equation}


% In our work, we utilize a reinforcement learning algorithm, PPO, and an online imitation learning algorithm, DAgger, to learn the control policies under our framework. In this section, we briefly introduce their basics.
% \subsection{Online Imitation Learning and Teacher-Student Training}
% In general, imitation learning (IL) \cite{ross2011reduction,liu2020energy} studies the task of learning from expert demonstrations. In this work, instead of learning from offline expert data, we refer to an online IL method, dataset aggregation (DAgger)~\cite{ross2011reduction} such that we request an online expert to provide the demonstrated action. Formally, the goal is to train a student policy $\hat{\pi}$ minimizing the action distance between the expert policy $\pi_E$ under its encountered states:
% \begin{equation}
% \hat{\pi} = \arg\min_{\pi\in\Pi}\mathbb E_{s\sim d_{\pi}}[\ell(s,\pi)].
% \label{eq:imitation}
% \end{equation}
% Here, $\ell$ is the mean square error in practice.

\subsection{Asymmetric Training}
The asymmetric training introduces a separate encoder to estimate the key privileged information $s^{\text{key}}$ from $k$-step history proprioceptive observations $h^{k}$, which is trained by an estimation loss $\mathcal{L}_{\text{est}}$: 
\begin{equation}
    \mathcal{L}_{\text{est}} = \bbE_\pi\left[\|\mathcal E_{\pi}(h^k) - s^{\text{key}}\|^2\right]
\label{eq:asym_detail}
\end{equation}

\section{Implementations Details}
% \subsection{Details of Network Architecture}

\subsection{Unitree H1 DOF}
The Unitree H1, as demonstrated in \fig{fig:H1-dof}, has 19 DoFs in total, including two 3-DOF shoulder joints,
two elbow joints, one waist joint, two 3-DOF hip joints, two
knee joints, and two ankle joints. 
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/H1Dof.png}
    \caption{\small DOF demonstration of Unitree H1.}
    \label{fig:H1-dof}
\end{figure}

\subsection{Commands Space of The Hopping Gait}
\label{ap:Hopping}
Hopping, characterized by a single foot consistently maintaining ground contact while the other remains in the air, represents an extremely unstable gait that requires coordinated whole-body motor control to maintain balance while tracking task commands. Among the behavior commands, all terms significantly challenge the delicate balance of the robot, except for body height, which poses minimal disruption. 
Therefore, the command space of hopping gait is designed as $\{v_x, v_y, \omega, h\}$, whose remaining behavior command terms for other gaits turn into regular terms.
The command ranges and default value for gait hopping are illustrated in \tb{tab:hopcommands}.
\begin{table}[t]
    \centering
    \caption{\small Ranges and default values of commands for gait hopping.} 
    \begin{tabular}{@{}c|ccc@{}} \toprule
        Group                              &  Term       & Default         &  Range \\ \midrule
        \multirow{3}{*}{Movement} & linear velocity $v_x$    & 0               & $[-0.6, 0.6]$ \\
                                  & linear velocity $v_y$    & 0               & $[-0.6, 0.6]$ \\
                                  & angular velocity $\omega$ & 0               & $[-0.6, 0.6]$ \\ \midrule
        Posture & body height $h$         & 0               & $[-0.3, 0] $  \\
        \bottomrule

    \end{tabular}
    \label{tab:hopcommands}
\end{table}

\subsection{Foot Trajectory Target}
\label{ap:Foot Target}
There are various methods for robot foot trajectory planning, including Bezier trajectory, polynomial trajectory, and so on. Due to the smoothness provided by polynomial trajectory, they are widely used in the swing trajectory planning of quadruped robots~\citep{LocManMPC2021RAL}. Based on this, a polynomial foot trajectory planner integrated with homogeneous variables $\bar{\phi_i}$ is designed in this paper.
% In the z direction, a quintic polynomial trajectory is used for planning. The specific expression of the quintic polynomial is as follows:
% \yufei{The normal quintic curve is a function of time t, but in this work, in order to fit the instruction space, we replaced time t with a uniform phase variable. Do we need to explain this, or do we just write down the equation}
% \begin{equation}\label{eq:foot_traj}
% \begin{aligned}
% l_z(t) = a_5\bar{\phi_i}^5+a_4\bar{\phi_i}^4+a_3\bar{\phi_i}^3+a_2\bar{\phi_i}^2+a_1\bar{\phi_i}+a_0,
% \end{aligned}
% \end{equation}
% where $a_i, i=0, 1, 2, 3, 4, 5$ is the coefficient of polynomial, and $\bar{\phi_i}$ is homogeneous variables. 
In the $z$-axis, the swing trajectory is divided into two segments: from the starting position $p_{s,z}$ to the highest point $l_t$, and from the $l_t$ to the end position $p_{e, z}$. In this study, a piecewise quintic polynomial is used for the foot trajectory planning. For the $p_{s, z}$ and $p_{e, z}$, it is desirable for the foot to make contact with the ground as smoothly as possible. Therefore, both velocity and acceleration are set to zero at these boundary points. The boundary conditions for the piecewise quintic polynomial trajectory are summarized in the \tb{tab:boundary_condition}. 
The coefficients of a polynomial can be calculated:

% \begin{eqnarray}
\begin{equation}
\resizebox{\columnwidth}{!}{
$
\begin{aligned}
\begin{split}
&\left[
\begin{array}{c}
   p_{s, z} \\
   l_t \\
   0 \\
   0 \\
   0 \\
   0
\end{array}
\right]
= 
&\left[
\begin{array}{cccccc}
    ({\bar{\phi}_i^{0.5}})^5 & ({\bar{\phi}_i^{0.5}})^4 & ({\bar{\phi}_i^{0.5}})^3 & ({\bar{\phi}_i^{0.5}})^2 & \bar{\phi}_i^{0.5} & 1 \\
    ({\bar{\phi}_i^{0.75}})^5 & ({\bar{\phi}_i^{0.75}})^4 & ({\bar{\phi}_i^{0.75}})^3 & ({\bar{\phi}_i^{0.75}})^2 & \bar{\phi}_i^{0.75} & 1 \\
    5({\bar{\phi}_i^{0.5}})^4 & 4({\bar{\phi}_i^{0.5}})^3 & 3({\bar{\phi}_i^{0.5}})^2 & 2{\bar{\phi}_i^{0.5}} & 1 & 0 \\
    5({\bar{\phi}_i^{0.75}})^4 & 4({\bar{\phi}_i^{0.75}})^3 & 3({\bar{\phi}_i^{0.75}})^2 & 2{\bar{\phi}_i^{0.75}} & 1 & 0 \\
    20({\bar{\phi}_i^{0.5}})^3 & 12({\bar{\phi}_i^{0.5}})^2 & 6{\bar{\phi}_i^{0.5}} & 2 &0 & 0 \\
    20({\bar{\phi}_i^{0.75}})^3 & 12({\bar{\phi}_i^{0.75}})^2 & 6{\bar{\phi}_i^{0.75}} & 2 &0 & 0
\end{array}
\right] 
\left[
\begin{array}{c}
    a_{5}^1 \\ 
    a_{4}^1 \\
    a_{3}^1 \\
    a_{2}^1 \\
    a_{1}^1 \\
    a_{0}^1
\end{array}
\right]~,
\end{split}
\\
\begin{split}
&\left[
\begin{array}{c}
   l_t \\
   p_{e, z} \\
   0 \\
   0 \\
   0 \\
   0
\end{array}
\right]
= 
&\left[
\begin{array}{cccccc}
    ({\bar{\phi}_i^{0.75}})^5 & ({\bar{\phi}_i^{0.75}})^4 & ({\bar{\phi}_i^{0.75}})^3 & ({\bar{\phi}_i^{0.75}})^2 & \bar{\phi}_i^{0.75} & 1 \\
    ({\bar{\phi}_i^{1.0}})^5 & ({\bar{\phi}_i^{1.0}})^4 & ({\bar{\phi}_i^{1.0}})^3 & ({\bar{\phi}_i^{1.0}})^2 & \bar{\phi}_i^{1.0} & 1 \\
    5({\bar{\phi}_i^{0.75}})^4 & 4({\bar{\phi}_i^{0.75}})^3 & 3({\bar{\phi}_i^{0.75}})^2 & 2{\bar{\phi}_i^{0.75}} & 1 & 0 \\
    5({\bar{\phi}_i^{1.0}})^4 & 4({\bar{\phi}_i^{1.0}})^3 & 3({\bar{\phi}_i^{1.0}})^2 & 2{\bar{\phi}_i^{1.0}} & 1 & 0 \\
    20({\bar{\phi}_i^{0.75}})^3 & 12({\bar{\phi}_i^{0.75}})^2 & 6{\bar{\phi}_i^{0.75}} & 2 &0 & 0 \\
    20({\bar{\phi}_i^{1.0}})^3 & 12({\bar{\phi}_i^{1.0}})^2 & 6{\bar{\phi}_i^{1.0}} & 2 &0 & 0
\end{array}
\right] 
\left[
\begin{array}{c}
    a_{5}^2 \\ 
    a_{4}^2 \\
    a_{3}^2 \\
    a_{2}^2 \\
    a_{1}^2 \\
    a_{0}^2
\end{array}
\right]~.
\end{split}
\label{eq:polynomial coefficients}
\end{aligned}
$
}
\end{equation}
% \end{eqnarray}

where $p_{s,z}$ is the $z$-coordinate of the start position, $p_{e,z}$ is the $z$-coordinate of the start position and $l_t$ is swing highest position. The piecewise quintic polynomial trajectory $l_t^{\text{target},i}$ is formulated as:
\begin{equation}
\resizebox{\columnwidth}{!}{
$
\begin{aligned}
l_t^{\text{target},i} = \left\{\begin{aligned}
&\frac{6(l_t-p_{s,z})}{(\bar{\phi}_i^{0.75}-\bar{\phi}_i^{0.5})^5}(\bar{\phi_i}-0.5)^5 + \frac{15(p_{s,z} - l_t)}{(\bar{\phi}_i^{0.75}-\bar{\phi}_i^{0.5})^4}(\bar{\phi_i}-0.5)^4 + \frac{10(p_{s,z}-l_t)}{(\bar{\phi}_i^{0.75}-\bar{\phi}_i^{0.5})^3}(\bar{\phi_i}-0.5)^3 + p_{s,z},
&0.5 < \phi_i < 0.75 \\ 
&\frac{6(p_{e, z} - l_t)}{(\bar{\phi}_i^{1.0}-\bar{\phi}_i^{0.75})^5}(1-\bar{\phi_i})^5 + \frac{15(l_t - p_{e, z})}{(\bar{\phi}_i^{1.0}-\bar{\phi}_i^{0.75})^4}(1-\bar{\phi_i})^4 + \frac{10(l_t - p_{e,z})}{(\bar{\phi}_i^{1,0} -\bar{\phi}_i^{0.75})^3}(1-\bar{\phi_i})^3 + l_t ,   & 0.75 < \phi_i < 1.0
\end{aligned}\right.~,
\end{aligned}
$
}
\end{equation}



\input{tables/boundary_condition}

\subsection{Details of Intervention Baseline}
In experiment section \ref{sec:InterventionExp}, we compare \our with a baseline policy that is trained with intervention actions sampled from the AAMAS motion dataset.

\noindent\textbf{Motion intervention interpolation.}
Since the frequency of the motion data is different from the control frequency, we interpolate the intervened actions from motion capture datasets to match the control frequency. Formally, at time step $t$, the intervention action is a linear interpolation of the closest two frames from the dataset:
\begin{equation}
    a^{\text{interv}}_{t, \text{dataset}} = (1-\gamma) a_k^{\text{traj}_j} + \gamma a_{k+1}^{\text{traj}_j}
\end{equation}
where 
$$
    \gamma = \frac{f^{\text{traj}_j}\cdot t-T_k^{\text{traj}_j}}{T_{k+1}^{\text{traj}_j}-T_{k}^{\text{traj}_j}}
$$
is the interpolation coefficient,
$T_k^{\text{traj}_j}$ is the original time stamp of the $k$-th frame of $j$-th trajectory in the dataset and $f^{\text{traj}_j}$ is the frequency of the $j$-th trajectory.
The training process keeps the same curricula as described in \eq{eq:noisecurr} by replacing $a^{\text{interv}}_{\text{noise}}$ with $a^{\text{interv}}_{\text{dataset}}$.
% The update time step interval $t_{\text{interval}}$ is $2$, due to the high frequency of frames in the trajectory.

\subsection{Details of Network Architecture}
We deployed an asymmetric training framework. 
\our actor network consists of three key components: a historical state encoder, a state estimator, and a low-level network. The historical state encoder takes in five frames of historical proprioceptive observations $o_t^{\text{his}}$ and outputs an encoded historical vector $z_t$. The state estimator leverages this encoded vector to implicitly estimate linear velocity $\hat{v_t}$, foot clearance $\hat{l_t}$, and body height $\hat{h_t}$ that are often challenging to measure accurately with onboard sensors. Finally, the low-level network processes the $z_t$, the estimated states $\hat{v_t}$, $\hat{l_t}$, $\hat{h_t}$ ,current proprioceptive observations $o_t^{pro}$, the commands $c_t$ and binary indicator $I(t)$, ultimately generating the joint actions $a_t$. A more detailed description of network architecture is shown in the Tab. \ref{tab:network architecture}.

\begin{table}[t]
\setlength{\abovecaptionskip}{0.cm}
\setlength{\belowcaptionskip}{-0.cm}
    \centering
    \caption{\small \textbf{Network architectures.}} 
    \label{tab:network architecture}
\begin{tabular}{llll}
\hline
Module                   & Inputs                                                                                                  & Hidden Layers       & Outputs                           \\ \hline
Historical State Encoder & $o_t^{\text{his}}$                                                                                             & {[}256, 128{]}      & $z_t$                             \\
State Estimator          & $z_t$                                                                                                   & {[}64, 32{]}        & $\hat{v_t}, \hat{l_t}, \hat{h_t}$ \\
Low-Level Network        & \makecell[l]{$z_t, \hat{v_t}, \hat{l_t}, \hat{h_t}$, \\ $o_t^{\text{pro}}, c_t, I(t)$} & {[}256, 128, 64{]}  & $a_t$                             \\
Critic                  & $o_t^{\text{pro}},o_t^{\text{pri}},o_t^{\text{ter}}$                                                                         & {[}512, 256, 128{]} & $V_t$                             \\ \hline
\end{tabular}
\end{table}

\subsection{Policy Learning Time}
The overall policy learning time was 16 hours of wall-clock time, using a single NVIDIA RTX 4090 GPU.

\section{Extended Experiment}
\subsection{Extensive Analysis of Commands Combination}
\label{ap:heatmaps}
We draw heatmaps and line charts to illustrate the tracking accuracy when combining two different commands across their ranges under different gaits, shown in \fig{fig:Heatmaps}.
% \noindent\textbf{Walking.} As shown in the \fig{fig:heatmapwalk}, 
% \emph{linear velocity x}, \emph{angular velocity yaw}, \emph{body height} and \emph{waist yaw} are orthogonal under gait walking, meaning that when each command is sampled within its respective range, it does not significantly affect the tracking performance of the other commands.
% When \emph{linear velocity x} exceeds $1.5m/s$, the orthogonality between \emph{linear velocity x} command and others has decreased. 
% The main reason is that the dynamic stability of the robot decreases during high-speed motion, and the robot usually sacrifices the tracking accuracy of commands to maintain the body's stability.
% The \emph{gait frequency} demonstrates discrete orthogonality. At a \emph{gait frequency} of 1.5 or 2, the robot achieves optimal tracking performance for various commands. However, under high-frequency gait conditions, the tracking accuracy for commands declines.
% The \emph{linear velocity y}, \emph{foot swing height}, and \emph{body pitch} are only orthogonal to the other commands within a narrow range. As the range of commands increases, they begin to significantly affect the every term’s tracking performance. Specifically, the design of the H1 robot's structure results in poor performance in tasks involving lateral movement. Additionally, the foot swing height compromises the robot's dynamic stability, further reducing the tracking accuracy of its commands. Adjusting the robot's pitch causes the center of gravity to shift forward, exacerbating instability and posing an even greater challenge to its balance.

% \noindent\textbf{Jumping.} The command orthogonality in the jumping is similar to that in the walking, but the overall orthogonal range is smaller.
% This is primarily due to the increased challenge of the jumping motion, especially in high-speed movement modes, in which the tracking errors for various commands significantly increase. 
% In each gait cycle, the robot must leap a significant distance forward to match the current speed. To continuously perform this high-difficulty jumping action, the robot must adjust to an optimal posture at the start of each gait cycle. Both legs generate substantial torque to propel the body forward. Upon landing, the robot must quickly adjust its posture to maintain stability and continuously repeat these actions. As a result, during movement, the robot can only follow other commands within a relatively limited range.

% \noindent\textbf{Hopping.}
% In the hopping gait, the various commands exhibit little to no clear orthogonal relationship, as shown in Fig \ref{fig:heatmaphop}. In this gait, the robot is only able to effectively track commands related to \emph{linear velocity x}, \emph{linear velocity y}, \emph{angular velocity yaw}, and \emph{body height}. However, for other commands, the robot is completely unable to track. This indicates that hopping motions introduce more instability, and the robot's control system must focus more on maintaining balance, making it difficult to simultaneously handle complex, multi-dimensional commands. As for adjustments to body height, it can be understood that a lower body height improves dynamic stability. Therefore, adjusting the body height plays a positive role in enhancing the robot's hopping performance.

% \noindent\textbf{Standing.} In the standing state, we tested the tracking errors of commands related to posture. The results showed that the tracking errors were similar to those observed during the walking with zero velocity. The \emph{waist yaw} is orthogonal to the other two commands. As the range of commands increases, the orthogonality between \emph{body height} and \emph{body pitch} decreases. Specifically, the H1 robot has only one degree of freedom at the waist, meaning that it can only adjust its posture through the hip pitch joint. This limitation restricts the utilization of the hip pitch joint in other tasks. When the body height decreases by 0.3 m, the range of motion of the hip pitch joint becomes almost zero, making it difficult for the robot to achieve precise tracking of body pitch.

\input{tables/standjump-single-commands-track}
\subsection{Commands Tracking with Interventions}
\label{ap:SingleCommandsTracking-REMAIN}
We further show the single command tracking evaluation results for the standing gait and the jumping gait, in \tb{tab:Intervention Tracking Error stand jump}.
On these gaits, \our also achieves the best tracking performance under almost all test cases, except the body pitch and waist yaw tracking with no intervention. In contrast, the policy trained with AMASS data is still limited to handling actions within the scope of that data, and the policy trained without intervention fails with any external upper-body control. We thus can conclude that intervention training applies to a variety of gaits. 

In particular, under the jumping gait, the intervention tasks had a significant impact on the robot tracking performance. This is mainly because jumping gait is more challenging for humanoid robots, which rely heavily on arm swings to complete the motion task. Therefore, when the arm movement is restricted, the robot's performance is notably compromised.
Under the standing gait, \our shows significantly lower posture-related tracking errors compared to the walking and jumping gaits. 

Since hopping is a highly unstable gait, which is rarely used for loco-manipulation tasks and is implemented with an independent policy, we did not involve intervention training for the hopping gait.


\begin{figure*}[htbp]
    \centering
\begin{subfigure}[htbp]{\textwidth}
    \includegraphics[width=\linewidth]{imgs/Walking_V4.pdf}
    \subcaption{Walking.}
    \label{fig:heatmapwalk}
\end{subfigure}
\end{figure*}
\begin{figure*}[htbp]\ContinuedFloat
\begin{subfigure}[htbp]{\textwidth}
    \includegraphics[width=\linewidth]{imgs/Jumping_V4.pdf}
    \subcaption{Jumping.}
    \label{fig:heatmapjump}
\end{subfigure}
\end{figure*}
\begin{figure*}[htbp]\ContinuedFloat
\begin{subfigure}[htbp]{0.49\textwidth}
    \includegraphics[width=\linewidth]{imgs/Standing_V4.pdf}
    \subcaption{Standing.}
    \label{fig:heatmapstand}
\end{subfigure}
\begin{subfigure}[htbp]{0.49\textwidth}
    \includegraphics[width=\linewidth]{imgs/Hopping_V4.pdf}
    \subcaption{Hopping.}
    \label{fig:heatmaphopping}
\end{subfigure}
\caption{\small \textbf{Tracking-error heat maps of command combination under different gaits}. Each column represents one of the following command parameters: \emph{linear velocity x}, \emph{linear velocity y}, \emph{angular velocity yaw}, \emph{gait frequency}, \emph{foot swing height}, \emph{body height}, \emph{body pitch}, and \emph{waist roll}. The standing includes the three series commands for the \emph{body height}, \emph{body pitch}, and \emph{waist yaw}.
For the off-diagonal sub-figures, the range for each command is indicated along the vertical axis (left) and horizontal axis (bottom). The corresponding error values are indicated by ticks on the right-side color bar. 
The colder %darker
the color of the pixel, the larger the tracking error the commands faces, and the color bars in different rows have different ranges of error.}
\label{fig:Heatmaps}
\end{figure*}

\subsection{Comparison with Other Whole-body Controllers}
We compare \our with two \textit{open-sourced SOTA learning-based humanoid whole-body controllers}, HOVER~\citep{he2024hover} and Exbody~\citep{cheng2024expressive} in simulation, shown in \tb{tab:baseline}. Nevertheless, the training and control modes of these controllers rely heavily on motion datasets. 
For example, the command space of ExBody includes target expression goal (upper body) and root movement goal (lower body), which are sampled from trajectories.
% Its training relies heavily on the motion dataset, with command combinations sampled directly from the data. 
Although HOVER features a multi-mode command space, it requires high consistency between different command terms due to the motion tracking task setting.
% ExBody~\citep{cheng2024expressive} also requires motion reference, whose command spaces include body keypoints and joint positions (upper body), and root movement (lower body) which can be decoupled to our posture and movement commands.
To compare them to ours, we keep the upper body of humanoids remaining at the default joint positions as the required reference, and compute the tracking error as in \tb{tab:Single commands}.
Note that we evaluate the performance of \our under the \textit{walking} gait, as the baselines do not support gait switch without reference motion, and \our does not require upper-body reference motion and controls the whole-body joints.
The comparison experiment forces HOVER and ExBody policies to perform tasks beyond their intended design, resulting in poorer performance than demonstrated in their respective papers.

\begin{table}[htbp]
    \centering
    \caption{Single command tracking error comparison with learning based baselines.}
    % under the walking gait, settings are the same as in Table III.}
    % \vspace{-5pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|cccc|ccc} \toprule
      Methods & $E_{v_x}^{\text{low}}$ & $E_{v_x}^{\text{high}}$ & $E_{v_y}$ & $E_{\omega}$ & $E_h$ & $E_p$ & $E_w$ \\ \midrule
      HOVER~\citep{he2024hover}   &  0.559 & 1.324 & 0.328 & 0.436 & 0.270 & 0.127 & 0.082\\
      ExBody~\citep{cheng2024expressive}   &  0.109 & 0.242 & 0.114 & 0.587 & 0.145 & 0.122 & 0.097\\
      \textsc{HugWBC} (Ours) & \textbf{0.030} & \textbf{0.216} & \textbf{0.085} & \textbf{0.054} & \textbf{0.064} & \textbf{0.038} & \textbf{0.075} \\ \bottomrule
    \end{tabular}
    }
    \label{tab:baseline}
\end{table}