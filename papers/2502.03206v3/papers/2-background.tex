\section{Background}
\subsection{Humanoid Whole-Body Control}
To support various high-level functionalities and allow the humanoid robot to perform complicated tasks, a basic whole-body controller is essential.
Formally, given a set of continuous commands $\mathcal{C}$ and observations $\mathcal{O}$, our objective is to develop a control function that maps these inputs to appropriate control signals.
Model-based approaches represent one solution paradigm, typically decomposing the control function into planning and tracking modules~\citep{nolinearMPC2023, 2023troBiConMP}. The planning module generates optimal trajectories and contact sequences based on $\mathcal{O}$ and $\mathcal{C}$, while the tracking module translates these into control laws, specifying joint positions, velocities, and torques. However, these methods face computational challenges due to the complex dynamics of humanoid robots and the discrete nature of whole-body contact points. 
Learning-based methods offer an alternative approach by directly learning a policy function $a=\pi(o,c)$ that maps observations $\mathcal{O}$ and commands $\mathcal{C}$ to joint-level actions~\citep{2021BipedalPeriodicReward}. These actions typically represent offsets to target joint positions across three categories: upper-body, lower-body (legs), and hands. The final target position combines the nominal position with these learned offsets, which is then tracked using a proportional derivative (PD) controller with fixed gains.
% \subsection{}
% We formulate the locomotion problem as a Markov decision problem (MDP) in the discrete-time domain. At each time step t, the humanoid robot observes the state $x_t$ of the environment, and the policy $\pi_\theta$ generates a distribution of actions based on the state of the environment. The robot then executes the action $a_t$ sampled from the action distribution, and the environment transitions to the next state $x_{t+1}$ based on the state transition probability function $P \left( x_{t+1} | x_t, a_t \right)$ and returns a reward $r_{t}$. The goal of reinforcement learning is to find an optimal parameter to maximize the expected discounted return of future trajectories:
% \begin{equation}
%  J_{\theta}\left(\pi \right)  = \mathbb{E}_{\pi} \left[ {\sum\limits_{t = 0}^\infty  {{\gamma ^t}{r_t}} } \right],
% \end{equation}
% where $\gamma\in\left[0, 1\right)$ is discount factor. In order to enable a single policy to generate diverse behaviors, we designed a parameterized behavioral space $b_t$ and commands $c_t$ that simultaneously generates action distributions based on the state of the environment and the commands.

\subsection{Command Tracking as Reinforcement Learning}
To achieve a generalized and powerful whole-body control behavior for humanoid robots, we learn a policy function by constructing a command-tracking problem. In detail, we want the learned policy $\pi$ to control the robots to match the provided commands $c$. To this end, we use reinforcement learning (RL), where we define the reward functions $r$ typically by distances $d$ or similarities $s$  of the command $c$ and the observed robot state $s_c$ corresponding to that command:
\begin{equation}
\begin{aligned}
r(o, a, c) = -d(c, s_c) \text{ or } r(o, a, c) = s(c, s_c)~.
\end{aligned}
\end{equation}
Under the formulation of RL, the policy is trained to maximize the rewards, corresponding to matching these commands.

\subsection{Simulation Training and Real-World Transfer}
Many recent works, especially those of legged robots, take advantage of RL training a robust robot-control policy with a large set of parallel environments in simulation and directly deploying into the real world~\citep{cheng2024extreme,liu2024visual,he2024hover, sci2024dtc}.
Due to the partial observability of the real robot, whose onboard sensors can only provide limited and noisy observations, it is difficult to learn a deployable policy from them directly. 
Therefore, researchers have developed a set of sim-to-real techniques to resolve the challenge. Among them, one of the most commonly used techniques is asymmetric training~\citep{asyframework,nahrendra2023dreamwaq}, which is proposed as a one-stage solution for sim-to-real training.
% Among them, two of the most commonly used techniques are teacher-student training~\citep{lee2020learning} and asymmetric training~\citep{asyframework,ji2022concurrent}. Details of the algorithms are listed in \ap{ap:bk}.

% For the teacher-student training paradigm, a teacher policy with privileged states is first trained via RL (e.g., Proximal Policy Optimization (PPO)~\citep{schulman2017proximal}) in a highly efficient way, since the privileged states are low-dimensional and include clear task-related information. Then, a student policy is trained via online imitation learning (e.g., DAgger~\citep{ross2011reduction}) to mimic the actions of the teacher policy, which only observes the same partial and noisy observations as on real robots. 
% To mitigate the information loss during the distillation phase of the student policy, 
In this paper, we adopt an asymmetric actor-critic (AAC) framework proposed for quadruped locomotion~\citep{sci2023defrmable}.
In this framework, the critic network has access to all privileged information, while the actor network only receives data available from onboard sensors, with a separate encoder to estimate the key privileged information (\textit{e.g.}, the linear velocity, robot's body height, and robot's feet swing height). The training paradigm incorporates the RL objective (including a value loss $\mathcal{L}^{\text{value}}$ and a policy loss $\mathcal{L}^{\text{policy}}$) with an estimation loss \citep{nahrendra2023dreamwaq, liu2024skill, sci2023defrmable} $\mathcal{L}^{\text{est}}$ to train the encoder:
\begin{equation}
\begin{aligned}
    \mathcal L^{\text{AAC}} = \mathcal L^{\text{value}} + \lambda^{\text{policy}} \mathcal L^{\text{policy}} + \lambda^{\text{est}} \mathcal{L}^{\text{est}}
\end{aligned}
\end{equation}
In this work, we take AAC as our default training framework, but the proposed techniques are not limited to it.