We evaluate the performance of \system by answering the following research questions:

\begin{itemize}[leftmargin=*]
\item \textbf{RQ 1: Identification Accuracy.} Can \system accurately identify sensitive functions within a program?

\item \textbf{RQ 2: Transformation Consistency.} Can \system ensure that sensitive functions are correctly transformed, achieving the same functionality as the original code?

\item \textbf{RQ 3: Resource Consumption.} What is the resource consumption in code transformations?

%\item \textbf{RQ 4: Application.} How does \system perform in TEEs?

\end{itemize}
\subsection{Experiment Preparation}

\begin{description}[leftmargin = 0pt]
\item [Experiment Environment.]
We ran \system on a server equipped with an AMD EPYC 7763 processor and an NVIDIA RTX 6000 Ada graphics card.
We chose four LLMs: GPT-4o~\cite{openai2024gpt4o}, Qwen2.5-coder:32b~\cite{abs240912186}, Deepseek-v3~\cite{bi2024deepseek}, and \tool{LLama3.1:8b}~\cite{abs230709288} in our experiments, subsequently referred to as GPT-4o, Qwen2.5, Deepseek, and LLama3.1.
Qwen2.5 and LLama3.1 are deployed locally on our server.
For TEE platforms, we selected Intel SGX (Intel i7-9700 processor) for processor-based TEE experiments and AMD SEV (AMD EPYC 7763 processor) for VM-based TEE experiments.

\item [Dataset Construction.]
To the best of our knowledge, no existing dataset of functions to be protected under TEEs is available.
Thus, we manually constructed a benchmark for the task addressed in our paper.
To obtain repositories that potentially include sensitive operations, we conducted keyword searches using \emph{password}, \emph{credential}, \emph{seal}, \emph{serialization}, and \emph{cryptography} on \tool{GitHub}.
To ensure the quality of collected projects, we only kept projects with more than 50 stars, a commonly used threshold to exclude toy projects~\cite{han2023credential, feng2022automated}.
Finally, we obtained 38 Java repositories and 30 Python repositories.
By parsing these projects, we obtained 7,214 and 3,770 Java and Python functions, respectively.
We retained the leaf functions and conducted a manual review with three students who possess expertise in cryptography or software engineering.
Only those functions that were consistently identified as sensitive by all three reviewers were retained.
In total, we marked 232 sensitive functions for Java and 153 sensitive functions for Python.
Additionally, to construct our dataset, we randomly added 241 Java and 166 Python normal functions from the remaining code (i.e., normal functions).
\system utilizes LLMss to generate test inputs for sensitive functions.
We leverage \tool{JaCoCo} for Java and \tool{Pytest-cov} for Python to measure line coverage of these test inputs.
The Java test inputs achieve 89.3\% line coverage, while the Python test inputs achieve 94.6\%.
These coverage rates indicate that the test inputs generated by \system cover the vast majority of the code.

\end{description}

\subsection{RQ1: Identification Accuracy}
In this experiment, we applied \system to the dataset to evaluate its effectiveness in sensitive function identification.
As presented in Table~\ref{tab:eva:detection}, in general, LLMs exhibit high accuracy in identification, with an overall F1-score exceeding 86\%.
GPT-4o demonstrates the best performance in both Java and Python, achieving the highest precision and F1 scores.
Deepseek follows closely behind; Qwen2.5 and LLama3.1 show lower performance in comparison.

For these failure samples, we conducted a manual examination to analyze their characteristics.
Functions that contain multiple shift operations or array manipulations may be incorrectly identified as sensitive operations because shifting is a commonly used operation in cryptography for security handling.
Additionally, if a function involves operations related to a dictionary with key-value pairs, the term \dquote{key} may be improperly identified as referring to cryptographic actions.

\input{table/evaluation/detection}

\subsection{RQ2: Transformation Consistency}
This experiment employs 232 sensitive functions written in Java and 153 in Python.
For these samples, \system transforms them into Rust code.
We validated the consistency after transformation, analyzed the reasons for failures, and compared \system with other prompting methods.

\subsubsection{Metric}
Before launching the experiment, we established several metric measures.

\begin{itemize}[leftmargin=*]
    \item \textbf{\#Original:} Refers to the number of samples marked as sensitive, specifically those that require transformation.

    \item \textbf{\#Direct:} Refers to the number of samples that directly achieve executability and pass the consistency validation after an \emph{initial transformation}, with no need for activating \emph{iterative refinement}.

    \item \textbf{\#Succeed:} Refers to the number of samples that become executable and pass consistency validation following \emph{iterative refinement}.
    Note that {\#Succeed:} includes {\#Direct:}.

    \item \textbf{Avg. Iter.:} Refers to the average number of iterations the code successfully completes the transformation while achieving both TEE-executable and consistent functionality with the original code. 
    The {\#Direct:} samples do not included in the statistics.

\end{itemize}

\subsubsection{Initial Transformation}
Initially, referring to the first step in Section~\ref{subsub:initial_transformation}, we applied several prompts and examples to transform the code into Rust.
The \textit{\#Direct} row in Table~\ref{tab:eva:consistent} shows the result.
The table shows that, solely with the provided prompts and examples, the LLM agent is unable to successfully transform the majority of the samples, achieving a success rate of less than 21\%.
%This indicates that the remaining functions require further iterative refinement.
We manually analyzed these \emph{Direct} samples and summarized their characteristics.
Typically, they contain only a few simple statements, including direct processing of strings (e.g., hashing) and returning the result without any branching.
However, a high failure rate indicates that only providing prompts and examples cannot meet our transformation requirements, as other samples may contain complex operations, such as standard library imports, initialization of cryptographic algorithms, and pertinent error handling.

\input{table/evaluation/consistency}

\subsubsection{Iterative Refinement}
For remaining samples that did not achieve successful transformation, \system carried out \emph{iterative refinement} with the \tool{ReAct} strategy to modify the code.
The \textit{Succeed} row in Table~\ref{tab:eva:consistent} illustrates the results that achieve successful transformation.
Meanwhile, the \textit{Ave. Iter.} row represents the average number of iterations required to attain success.
Figure~\ref{code:eva:key_case} shows an example of successful transformation, which presents code for private and public key generation.
The code defines a method that generates an RSA key pair with a specified number of bits.
It use a random number as the seed to create the key pair and return.
The transformed code used Rust's library implement the same function, and it also incorporated additional exception handling.

\input{code/case_key_gen}

Overall, the success rate of GPT-4o is the highest, regardless of the programming language employed.
When processing Java transformations, GPT-4o achieved a 92.2\% success rate across 214 samples.
Qwen2.5 achieved an 82.3\% success rate on 191 samples, while Deepseek reached 87.9\% success on 204 samples.
LLama3.1 lagged behind, transforming only 4 samples with a 1.7\% success rate.
Python transformations require more iterations and yield lower overall success rates.
GPT-4o transformed 127 samples with an 83.6\% success rate.
Qwen2.5 managed a 66.1\% success rate on 101 samples, while Deepseek achieved 76.5\% success on 117 samples.
LLama3.1 again lagged significantly, transforming only 4 samples with a 2.6\% success rate.
The lowest success rate of LLama3.1 can be attributed to its smaller model size (8 billion parameters) in our evaluation, which has limited reasoning capacity to accurately leverage actions during the iteration process.
From this perspective, our approach requires the model to have strong reasoning capabilities.
Since LLama3.1 no longer has sufficient capability, subsequent experiments will exclude this model.




Part of Python's lower success rate compared to Java can be attributed to its dynamic typing characteristic.
As an interpreted language, Python's absence of explicit type declarations limits the information available to the LLM.
Conversely, the target language, Rust, being statically typed, requires more type information for transformation.
For verification, we added type annotations for the arguments and return values of failed samples to transform again.
Table~\ref{tab:eva:add_type} illustrates their results.
Among these samples, GPT-4o successfully transformed an additional 4, Qwen2.5 transformed 12, and Deepseek transformed 10.
This resulted in success rates of 85.6\%, 73.9\%, and 83.1\%, respectively.
Given that GPT-4o already achieved the highest success rate, its improvement is not substantial.
The success rate of Qwen2.5 has improved most significantly.
Deepseek had nearly matched the performance of GPT-4o.
It is evident that for interpreted languages, the success rate can be enhanced through the manual addition of type annotations.

\input{table/evaluation/add_type}

\subsubsection{Failure Analysis}
For the code that failed to transform, we performed a manual examination of its implementation details and code structure, identifying the following issues:
\begin{description}[leftmargin = 0pt]
    \item [Missing flag variable.] There is a global static flag whose specific value the agent cannot determine, resulting in continuous substitution with incorrect variables.

    \item [Shift operations.]
    Code containing numerous shift operations, even if successfully transformed for execution, may terminate due to illegal shift operation errors.

    \item [Sophisticated cryptography.]
    If a code employs sophisticated and multiple algorithms, the agent may fail to accurately transform the code. 
    For instance, a Java code simultaneously utilizes \tool{Curve25519} public keys and \tool{XSalsa20} as a stream cipher for encryption, ultimately employing the \tool{Poly1305} algorithm for authentication.
    \system inaccurately transforms the code due to the complexity of the cryptographic operations, preventing successful consistency validation before the iteration threshold was reached.
    
    \item [Functionality change.]
    Among the samples that did not pass consistency validation, there are instances of \dquote{security upgrade} modifications.
    For example, the original code employed \emph{SHA-1} for hash computation, while the transformed code replaced it with \emph{SHA-256}, thereby enhancing security.
    Although this results in inconsistencies, from a security standpoint, \emph{SHA-256} offers greater reliability than \emph{SHA-1}, 
        given that \emph{SHA-1} has been demonstrated to be more vulnerable to collision attacks~\cite{Merrill17limits, wang2005finding}.
    Subjectively, we consider this to be a beneficial modification as it offers a more secure implementation of functionality.
    Unfortunately, the agents did not always exhibit such reliability.
    In the absence of sufficient semantic information, 
        particularly with Python code, 
        the agent may compromise the security of the transformed code while also producing inconsistent outputs.
    For example, Figure~\ref{code:eva:hashing} is the original and transformed Python code, which calculates the hash value using the hashing and salting method (recursive hash).
    Although the agent substituted \emph{SHA-1} with \emph{SHA-256}, the omission of the salt model still led to a functional inconsistency.
\end{description}

We also calculated the relationship between the success rate and the code size, as shown in Figure~\ref{fig:eva:success}.
The results indicate that the code size of the function has little impact on the transformation success rate. 
Among our experiments, the samples with a code size between 120-160 Lines are fewer, hence they show a relatively higher success rate.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/eva/success_rate_by_line.pdf}
    \caption{The success rate under different function sizes.}
    \label{fig:eva:success}
\end{figure}

\input{code/inconsistent}

\subsubsection{Comparison and Analysis}
\system employs multiple methods to assist the agent in accomplishing the transformation tasks.
This sub-experiment still employs LLMs for transformation but uses different methods for comparison:
\begin{itemize}[leftmargin = *]
    \item \textbf{Zero-Shot:} This method directly prompted the LLM to transform sensitive functions into Rust code without providing any additional information.
    
    \item \textbf{One-Shot:} This method prompted the LLM to transform the code using three transformed examples. 
    However, unlike the \emph{initial transformation}, it did not require the LLM to analyze the code first.

    \item \textbf{Compiler Check:} Other processes are consistent with \system, except that there is no consistency validation during the iterative refinement.
    It only utilized compiler checks as feedback for the LLM agent.

\end{itemize}

\input{table/evaluation/cmp}

Table~\ref{tab:eva:cmp} presents the comparison results.
According to the table, the success rate for the \textit{Zero-Shot} is the lowest because of the lowest information provided.
When additional transformation examples were introduced (\textit{One-Shot}), the success rate improved.
Despite this enhancement, a significant number of samples (over 80\%) still remained unsuccessful.
Following the introduction of the \textit{Compiler Check}, the success rate exhibited a substantial increase.
\textit{Compiler Check} resulted in the majority of the transformed code being executable; however, it did not ensure their consistency with the original code.
In comparison, since consistency validation was introduced, \system achieved the highest success rate, ensuring both executable and consistent functionality of transformed code.

\subsubsection{Stability}
Utilizing LLMs as agents introduces an element of randomness, which can lead to different transformation outcomes for the same code.
To assess the stability of \system, we conducted multiple rounds of repeated experiments.
We conducted four more rounds of repeated experiments, and the results are presented in Table~\ref{tab:eva:rep}.
For these 37 samples, GPT-4o, Qwen2.5, and Deepseek successfully transformed 35, 30, and 33 samples, respectively, in the original experiment.
The table indicates that, in most cases, the success rate remained consistent.
However, the results also exhibited minor fluctuations.
In \textit{Exp1}, GPT-4o successfully transformed one additional sample, whereas Qwen2.5 in \textit{Exp3} failed to transform one previously successful sample.
The one more successful transformation is a data encryption process.
The failure instance arose from a functionality change, where the source code used \emph{SHA1}, while the transformed result applied \emph{SHA256}.
In general, despite the presence of some fluctuations, \system demonstrates stability in its transformation outcomes.

\input{table/evaluation/reproducibility}

\subsection{RQ3: Resource Consumption}
The bottleneck of our \system is the response consumption of LLMs.
We recorded their average response time during our experiments, with the results presented in Table~\ref{tab:time_consumption}.
Here, \textit{Identify Sensitive Function} records the average response time taken to identify sensitive operations in code.
\textit{Thought \& Action} records the average response time taken to create a response in \emph{iterative refinement} with the \tool{ReAct} strategy.
Since each \emph{iterative refinement} carries out multiple thoughts and actions, this process is the most time-consuming in \system.
Figure~\ref{fig:eva:dis} illustrates the distribution of the number of iterations within this process.
From the figure, the majority of successful transformations require more than three iterations.
% which means it requires at least three responses, i.e., 24.9s, 54.6s, and 21.9s on average.

\input{table/evaluation/time}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/eva/distribution.pdf}
    \caption{Iteration count distribution for iterative refinement across models and languages.}
    \label{fig:eva:dis}
\end{figure}

% \subsection{Case Study}
% In this section, we utilize two cases in the GPT-4o experiment to illustrate how \system protects a program using TEEs.
% \system identified their sensitive operations and transformed them into Rust code passing compiler checks and consistency validation.
% Besides, we also developed them into both SGX and SEV TEE and utilized test inputs to evaluate the changes in operational overhead.
% For SGX, we utilized the \tool{Rust-SGX}~\cite{rustsgx} tool to load our transformed code into the TEE.
% For AMD SEV, we launched an Ubuntu 20.04 virtual machine through SEV to execute transformed code.
% Table~\ref{tab:time_run} presents the recorded execution times for both SGX and SEV, along with the changes compared to the original code.

% \input{table/evaluation/time_run}

% \subsubsection{Encoding Data}
% Figure~\ref{code:eva:encode_case} illustrates the original and transformed code of a function implementing data encoding (serialization).
% This function does not involve any third-party libraries and encodes the input data according to its rules.
% For operational overhead, due to the simplicity of the functionality, the original code requires only 0.03s to complete its execution.
% SGX and SEV require 0.14s and 0.07s, respectively.
% SGX experiences higher time consumption due to environment switches (TEE and normal world) compared to VM-based SEV, which has latency similar to standard network communications.
% However, initiating SEV demands significant resources as it involves launching a complete operating system in device memory.
% Furthermore, it does not actively release these resources after the operation has concluded.

% \input{code/case_encode}

% \subsubsection{Create Key}
% Figure~\ref{code:eva:key_case} presents code for private and public key generation.
% The code defines a method that generates an RSA key pair with a specified number of bits.
% For operational overhead, the original code requires 0.22s to create keys.
% SGX requires 18.31s to complete its execution, representing a significant increase in time.
% This is due to the Rust code's reliance on the standard library, which frequently switches back to the normal world for library calls and subsequently returns to the TEE.
% This results in frequent context switching.
% A more effective solution to eliminate this problem is to implement the same library within the TEE; however, this topic is outside the scope of this paper.
% \input{code/case_key_gen}

\subsection{Threats to Validity}
One potential threat to validity arises from the fact that the input tests utilized in consistency verification are inadequate to comprehensively cover the original function. 
In order to address this concern and to enhance the accuracy of consistency verification, we employed line coverage tools (i.e., JaCoCo and Pytest-cov) to assess the coverage achieved by the input tests generated by the LLM. 
The coverage information motivates the LLM to optimize coverage of input tests to the greatest extent possible.
Another potential threat to validity pertains to the labeling process. Specifically, during the identification of functions that contain sensitive operations, there exists a risk of human error in judgment.
We engaged three authors with expertise in software engineering and security to collaboratively assess the labeling, thereby enhancing the accuracy of our dataset.
Another threat to validity is the reasoning capabilities of LLMs. 
Our experiments have demonstrated that a smaller parameter model, like Llama 3.1:8B, is unable to successfully perform the transformation tasks that we require.
Therefore, we employ models with higher reasoning capabilities, such as DeepSeek and GPT-4o.