\subsection{Application Migration to TEE}
% JVM
\tool{Lejacon}~\cite{miao2023lejacon} developed a solution for running Java applications on SGX using a customized JVM approach. This isolates Java applications within a secure environment, protecting their code and data confidentiality.
While it successfully runs Java in a TEE, the method increases the Trusted Computing Base (TCB) size, which expands the attack surface and potential security risks.

% Protect ML
\tool{TEESlice}~\cite{zhang2024no} enhances the security of on-device machine learning by accurately isolating privacy-sensitive weights in models. It uses a partition-before-training strategy, dividing models into backbone and private slices. \tool{TEESlice} employs a dynamic pruning algorithm to optimize slice size, ensuring they fit within TEEs without losing accuracy.

% MyTEE IOT TEE
\tool{MyTEE}~\cite{han2023mytee} provides TEE functionality for embedded devices lacking full TrustZone support. It uses stage-2 page tables to isolate the TEE from the untrusted OS and implements a DMA filter to prevent malicious memory access. Secure I/O is achieved by delegating peripheral requests to the untrusted OS, with essential components protected by paging.

% ACAI
For safeguarding confidential environments, \tool{ACAI}~\cite{Supraja294554} utilizes Arm's Confidential Computing Architecture (CCA) to securely integrate accelerators like GPUs and FPGAs. This method extends CCA's security features to these devices, ensuring virtual machine-level isolation and protection against both software and physical threats.

% Additionally, it addresses challenges such as performance overhead and integration complexity. % Sensitive function and data-in-use is a small fraction of an application.
% \tool{TEESlice}~\cite{zhang2024no} addresses deep neural network partitioning challenges by accurately isolating privacy-sensitive weights into private slices. This ensures that only the necessary parts of the model are executed within the TEE, optimizing security and computational efficiency.

% TEEpart
To address the issue of an excessively large Trusted Computing Base in TEE application adaptation, \citet{lind2017glamdring} proposes a partitioning method that extracts security-sensitive components and places them within a secure enclave. This ensures that the enclave's code does not compromise data integrity or confidentiality.

In contrast, we are migrating only a small portion of critical code into the TEE in the form of native code. This approach allows \system to maintain functionality while avoiding the introduction of additional runtime overhead.

\subsection{Code Transformation}
% Static Analysis
\citet{hong2023improving} proposes enhancing C-to-Rust translation through the automatic replacement of unsafe features with safe alternatives. 
While Rust's ownership type system prevents memory and thread bugs, current translators only perform syntactic conversions, requiring developers to handle manual refactoring. 
Their approach focuses on replacing unsafe features such as the lock API and output parameters.

However, their approach is limited to C transformations.
%
\tool{CodeStylist}~\cite{ting2023codestylist} utilizes neural methods for code style transfer. The system builds upon existing code language models pre-trained on extensive open-source codebases. 
\tool{CodeStylist} fine-tunes these models through a multi-task training approach to handle diverse style transformations.

However, it requires a substantial amount of pre-training data.
% To CUDA
\tool{BabelTower}~\cite{wen2022babeltower} presents a learning-based framework that automates the transformation of sequential C code into parallel CUDA code, effectively addressing GPU programming complexities. 
The tool employs an extensive dataset of compute-intensive functions, utilizing back-translation combined with a discriminative reranker to handle unpaired corpora and semantic transformations.

% Human-machine
\tool{hmCodeTrans}~\cite{liu2024hmcodetrans} provides a method for interactive human-machine collaboration in code translation. The method introduces two collaboration patterns—prefix-based and segment-based—enabling software engineers' edits to guide the model for improved retranslation. Additionally, response time is reduced through an attention cache module that prevents redundant prefix inference and a suffix splicing module that minimizes unnecessary suffix inference.

We utilize a LLM to assist in code transformation, simultaneously incorporating input test checks and compiler feedback to ensure the functional consistency of the transformation. This process does not require prior data training.