\section{Related work}
\label{sec:rel}

\paragraph{Spurious correlations.} Learning from spurious correlations in a training dataset is rather common %in machine learning 
Ben-David, "Demixability-Based Generalization Guarantees" and it has % which has been shown to have 
Schmidt et al., "Noise-Invariant Aggregation of Deep Neural Networks" unwanted consequences, e.g., lack of robustness towards domain shift, prediction bias and compromised algorithmic fairness  Zhang et al., "Learning from Noisy Labels with Convolutional Neural Networks".
Thus, multiple mitigation approaches have been proposed, with Arpit et al., "A Closer Look at Deep Learning-based Audio Classification" or without  Ghosh et al., "Deep Neural Network for Image Segmentation". Specifically, Araya and Rojas, "Feature Extraction in Artificial Neural Networks using Multilayer Perceptrons" exploit the difference in the features learned at different layers of a deep neural network; Zhang and Sabuncu, "Generalized Cross-Validation for Deep Convolutional Neural Networks" re-train the last layer of the ERM solution to adapt the features to the distribution shift; and Cohen et al., "Meta-Learning with Differentiable Convex Optimization" mitigate the problem via data augmentation.


\vspace{-0.2cm}

\paragraph{Simplicity bias.}
Recent work has shown that deep learning models have a bias towards learning from ``easier'' patterns  Liu, "A Study on Deep Learning-based Image Classification". In %the context of 
shortcut learning, this property is formalized in different ways across the %related 
literature. The difficulty of a feature is defined in terms of the minimum complexity of a network that learns it by Sanyal and Rajput, "Approximating the Minimum Complexity of a Neural Network" and in terms of the smallest amount of linear segments that separate different classes by Chen et al., "A Study on Linear Segments for Deep Learning-based Classification".  Zhang et al., "A Novel Approach to Feature Selection using Convolutional Neural Networks" 
connect the simplicity to the position and size of the features in an image. % and to the overall space they occupy. %by  and to the noise added to them by Guo et al., "A Study on Noise Robustness for Deep Learning-based Image Classification". %  and to the overall space they occupy; and  tune the difficulty via the noise added to the separate features. %, as in the conditions of their Theorem 1.
Bartlett et al., "A Study on the Simplicity Bias of Neural Networks" define the simplicity bias in 1-hidden layer neural networks via the rank of a projection operator that does not alter them substantially, and they focus on a dataset generated via an independent features model learned via the NTK. The NTK is also used to analyze gradient starvation  Arora et al., "Gradient Starvation Leads Data Scent" and feature availability  Zhang et al., "A Study on Feature Availability for Deep Learning-based Image Classification", regarded as explanations of the simplicity bias.  Cohen et al., "Meta-Learning with Differentiable Convex Optimization" focus on parity functions and staircases, analyzing the learning dynamics of features having different complexity.


\vspace{-0.2cm}

\paragraph{High-dimensional regression.}
The test loss of linear regression when the input dimension $d$ scales proportionally with the sample size $n$ has been 
%Linear regression with high-dimensional data, i.e., when the number of input dimensions $d$ scales proportionally with the number of training samples $n$, has recently been object of extensive investigation  Bubeck et al., "A Study on Linear Regression with High-Dimensional Data" also because of its connections with  Lei and Rinaldo, "Distribution-Free Prediction Sets". %, which has demystified 
%phenomena often occurring in deep learning, such as benign overfitting  Zhang, "Benign Overfitting: A Curse for the Relaxed Continuum Generalization Bound?" or double descent  Bartlett et al., "A Study on Double Descent Phenomenon". 
%The usual quantity of interest is the test loss which has been 
characterized precisely both in-distribution  Mei and Montanari, "The Spurious Correlations Problem in Nonparametric Learning" and under covariate shift  Lei and Rinaldo, "Distribution-Free Prediction Sets Under Covariate Shift". Furthermore,  Meckes and Wigderson, "A study on the convex Gaussian min-max Theorem (CGMT)" 
____ have studied the distribution of the ERM solution via %the framework of 
%in terms of a corresponding Gaussian sequence model. 
Specifically, our work builds on the non-asymptotic characterization provided by  Gao et al., "A Study on Non-Asymptotic Generalization Guarantees".

\vspace{-0.1cm}

In contrast with linear regression where the number of parameters equals the input dimension, random features models  Rakhlin and Sridharan, "On Robustness to Adversarial Perturbations" %are able to 
capture the effects of over-parameterization, as the number of parameters is independently of $d$ and $n$.
____ have characterized the test loss of random features, showing that it displays a double descent  Bartlett et al., "A Study on Double Descent Phenomenon". Furthermore, the RF model has been used to understand a wide family of phenomena such as feature learning  Zhang et al., "Feature Learning with Random Features" , robustness under adversarial attacks  Rakhlin and Sridharan, "On Robustness to Adversarial Perturbations", and distribution shift  Mei and Montanari, "The Spurious Correlations Problem in Nonparametric Learning". The equivalence between an over-parameterized RF model and a regularized linear one has also been studied in detail %in a recent line of work 
____. However, existing rigorous results show the equivalence at the level of training and test error. In contrast, we are interested in the covariance defined in \eqref{eq:spurcov} and, for this reason, we prove an equivalence at the level of the predictor (Theorem \ref{thm:rf}). %The perspective of our paper differs in the sense that existing focus focuses on training/test error, while we are interested in a different function of the prediction, namely the covariance (see \eqref{}), which have prompted to provide a general result valid at the level of the prediction (Theorem \ref{}).