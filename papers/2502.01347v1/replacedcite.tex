\section{Related work}
\label{sec:rel}


\paragraph{Spurious correlations.} Learning from spurious correlations in a training dataset is rather common %in machine learning 
____ and it has % which has been shown to have 
unwanted consequences, e.g., lack of robustness towards domain shift, prediction bias and compromised algorithmic fairness ____.
Thus, multiple mitigation approaches have been proposed, with ____ or without ____ available annotations. Specifically, ____ exploit the difference in the features learned at different layers of a deep neural network; ____ re-train the last layer of the ERM solution to adapt the features to the distribution shift; and ____ mitigate the problem via data augmentation.






\vspace{-0.2cm}

\paragraph{Simplicity bias.}
Recent work has shown that deep learning models have a bias towards learning from ``easier'' patterns ____. In %the context of 
shortcut learning, this property is formalized in different ways across the %related 
literature. The difficulty of a feature is defined in terms of the minimum complexity of a network that learns it by ____ and in terms of the smallest amount of linear segments that separate different classes by ____. ____
connect the simplicity to the position and size of the features in an image. % and to the overall space they occupy. %by  and to the noise added to them by ____. %  and to the overall space they occupy; and  tune the difficulty via the noise added to the separate features. %, as in the conditions of their Theorem 1.
____ define the simplicity bias in 1-hidden layer neural networks via the rank of a projection operator that does not alter them substantially, and they focus on a dataset generated via an independent features model learned via the NTK. The NTK is also used to analyze gradient starvation ____ and feature availability ____, regarded as explanations of the simplicity bias. ____ focus on parity functions and staircases, analyzing the learning dynamics of features having different complexity.


\vspace{-0.2cm}

\paragraph{High-dimensional regression.}
The test loss of linear regression when the input dimension $d$ scales proportionally with the sample size $n$ has been 
%Linear regression with high-dimensional data, i.e., when the number of input dimensions $d$ scales proportionally with the number of training samples $n$, has recently been object of extensive investigation ____, also because of its connections with %. %, which has demystified 
%phenomena often occurring in deep learning, such as benign overfitting ____. % or double descent ____. 
%The usual quantity of interest is the test loss which has been 
characterized precisely both in-distribution  ____ and under covariate shift ____. Furthermore, 
____ have studied the distribution of the ERM solution via %the framework of 
the convex Gaussian min-max Theorem %(CGMT)
____. % in terms of a corresponding Gaussian sequence model. 
Specifically, our work builds on the non-asymptotic characterization provided by ____. 

\vspace{-0.1cm}

In contrast with linear regression where the number of parameters equals the input dimension, random features models ____ %are able to 
capture the effects of over-parameterization, as the number of parameters is independently of $d$ and $n$.
____ have characterized the test loss of random features, showing that it displays a double descent ____. Furthermore, the RF model has been used to understand a wide family of phenomena such as feature learning ____, robustness under adversarial attacks ____, and distribution shift ____. The equivalence between an over-parameterized RF model and a regularized linear one has also been studied in detail %in a recent line of work 
____. However, existing rigorous results show the equivalence at the level of training and test error. In contrast, we are interested in the covariance defined in \eqref{eq:spurcov} and, for this reason, we prove an equivalence at the level of the predictor (Theorem \ref{thm:rf}). %The perspective of our paper differs in the sense that existing focus focuses on training/test error, while we are interested in a different function of the prediction, namely the covariance (see \eqref{}), which have prompted to provide a general result valid at the level of the prediction (Theorem \ref{}).



%\vspace{-0.1cm}