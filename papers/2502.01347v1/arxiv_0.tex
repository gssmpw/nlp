\documentclass[11pt]{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{wrapfig}


\usepackage{setspace}
\usepackage{cprotect}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[noend]{algorithmic}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{makeidx}
\usepackage{enumerate}
\usepackage{graphicx,float,psfrag,epsfig}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{bigints}
\usepackage{mathtools}
\usepackage[mathscr]{euscript}


\newcommand{\simone}[1]{#1}
\input{commands}

\numberwithin{equation}{section}

\newtheoremstyle{myexample} % name
    {\topsep}                    % Space above
    {\topsep}                    % Space below
    {\rm }                   % Body font
    {}                           % Indent amount
    {\bf }                   % Theorem head font
    {.}                          % Punctuation after theorem head
    {.5em}                       % Space after theorem head
    {}  % Theorem head spec (can be left empty, meaning normal)

\newtheoremstyle{myremark} % name
    {\topsep}                    % Space above
    {\topsep}                    % Space below
    {\rm}                        % Body font
    {}                           % Indent amount
    {\bf}                        % Theorem head font
    {.}                          % Punctuation after theorem head
    {.5em}                       % Space after theorem head
    {}  % Theorem head spec (can be left empty, meaning normal)


\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}[claim]{Lemma}
\newtheorem{fact}[claim]{Fact}
\newtheorem{assumption}{Assumption}
\newtheorem{conjecture}[claim]{Conjecture}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[claim]{Proposition}
\newtheorem{corollary}[claim]{Corollary}
\newtheorem{definition}[claim]{Definition}


\theoremstyle{myremark}
\newtheorem{remark}{Remark}[section]
\theoremstyle{myremark}
\newtheorem{open}{Open Problem}[section]
\theoremstyle{myexample}
\newtheorem{example}[remark]{Example}




\author{Simone Bombari\thanks{Institute of Science and Technology Austria (ISTA). Emails: \texttt{\{simone.bombari, marco.mondelli\}@ist.ac.at}.}\;,
\;\;Marco Mondelli\footnotemark[1]}






\title{Spurious Correlations in High Dimensional Regression: \\
The Roles of Regularization, Simplicity Bias and Over-Parameterization}



\begin{document}

\newtheorem*{theoremcentering}{Theorem \ref{thm:maincentering}}
\newtheorem*{theoremcentered}{Theorem \ref{thm:centered}}
\newtheorem*{corhammer}{Corollary \ref{cor:hammer}}
\newtheorem*{cormem}{Corollary \ref{cor:memcap}}
\newtheorem*{theoremoptim}{Theorem \ref{thm:optimization}}

\maketitle



\begin{abstract}

Learning models have been shown to rely on spurious correlations between non-predictive features and the associated labels in the training data, with negative implications on robustness, bias and fairness.
In this work, we provide a statistical characterization of this phenomenon for high-dimensional regression, when the data contains a predictive \emph{core} feature $x$ and a \emph{spurious} feature $y$. Specifically, we quantify the amount of spurious correlations $\mathcal C$ learned via linear regression, in terms of the data covariance and the strength $\lambda$ of the ridge regularization. As a consequence, we first capture the simplicity of $y$ through the spectrum of its covariance, and its correlation with $x$ through the Schur complement of the full data covariance. Next, we prove a trade-off between $\mathcal C$ and the in-distribution test loss $\mathcal L$, by showing that the value of $\lambda$ that minimizes $\mathcal L$ lies in an interval where $\mathcal C$ is increasing. Finally, we investigate the effects of over-parameterization via the random features model, by showing its equivalence to regularized linear regression.
Our theoretical results are supported by numerical experiments on Gaussian, Color-MNIST, and CIFAR-10 datasets.
\end{abstract}


\section{Introduction}

Machine learning systems have been shown to learn from patterns that are statistically correlated with the intended task, despite not being causally predictive \cite{geirhos2020, xiao2021noise}. As a concrete example, a blue background in a picture might be positively correlated with the presence of a boat in the foreground, and while not being a predictive feature per se, a trained deep learning model could use this information to bias its prediction. % given the statistical nature of the learning algorithms. geirhos2018imagenettrained,
In the literature, this statistical (but non causal) connection is referred to as a \emph{spurious correlation} between a % spurious
feature and the learning task. A recent and extensive line of research has investigated the extent to which deep learning models manifest this behavior \cite{geirhos2018imagenettrained, xiao2021noise} and has proposed different mitigation approaches \cite{sagawa2020a, liu2021just}, % , chang21augmentation
given its implications to robustness, bias, and fairness \cite{zliobaite15, zhou21combating}. % gheiros2018
% This phenomenon, also referred as \emph{shortcut learning}, is often attributed to the relative ``simplicity'' of spurious features \cite{geirhos2020, shah2020pitfalls, Hermann2020whatshapes}, and at the implicit bias over-parameterized models have towards learning simpler patterns \cite{Belkin19, rahaman2019spectral, kalimeris2019sgd}. Hence, deep learning models happen to neglect the information from the \emph{core features} that unequivocally determine the right prediction (the boat in the foreground in the previous example) since the easier background (just the color blue) offers an easier shortcut to over-fitting optimization algorithms. 
% Previous work % looked at this intuitively reasonable argument %, this argument lacks a mathematical foundation. Previous work already attempted in 
% formalizes this \emph{simplicity bias} and its interplay with over-parameterization \cite{sagawa2020b}, relying on notions of simplicity related to boolean functions \cite{qiu2024complexity}, tied to the learning model \cite{morwani2023simplicity}, or based on 1-dimensional features \cite{shah2020pitfalls}, and their pair-wise relation \cite{pezeshki2021gradient}.
The phenomenon, also referred to as shortcut learning, is often attributed to the relative ``simplicity'' of spurious features \cite{geirhos2020, shah2020pitfalls, Hermann2020whatshapes} and to the implicit bias of over-parameterized models toward learning simpler patterns \cite{Belkin19, rahaman2019spectral, kalimeris2019sgd}. Consequently, the %deep learning models may neglect 
\emph{core features} that are % actually
informative about the task %essential for correct predictions
(e.g., the boat in the foreground) may be neglected, as \emph{spurious features} (e.g., the blue background) provide an easier shortcut to % towards
minimize the loss function. %for an optimization algorithm that interpolating optimization algorithms.

%\vspace{-0.1cm}

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/cmnistcifar10croppedtruck.pdf}
  \end{center}
  % \vspace{-0.1cm}
  \caption{\emph{Left two panels:} pictorial representation of the core (spurious) feature $x$ ($y$) and an independent core feature $\tilde x$, taken from an image of a boat and a truck in the CIFAR-10 dataset. \emph{Right two panels:} examples from a binary Color-MNIST dataset, where the labels correspond to the number shapes, and the zeros (ones) are colored in blue (red) with probability $(1 + \alpha)/2$.}
  \label{fig:intro}
  % \vspace{-0.1cm}
\end{wrapfigure}


Prior work has attempted to formalize the \emph{simplicity bias} % and its interplay with over-parameterization \cite{sagawa2020b}, 
relying %. Different notions so far rely 
on boolean functions \cite{qiu2024complexity}, model-specific biases \cite{morwani2023simplicity}, one-dimensional features \cite{shah2020pitfalls} and their pairwise interactions \cite{pezeshki2021gradient}. However, when considering high-dimensional natural data (e.g., the boat and its background in Figure \ref{fig:intro}), it remains unclear, based on these notions, what exactly makes the features easy or difficult to learn, and to what extent a trained model relies on spurious correlations.
%to describe the simplicity of their features  based on these notions. Thus, %  on what makes an input feature look easy to be learned?"
Furthermore, while \cite{sagawa2020b} show that over-parameterization can exacerbate spurious correlations when re-weighting the objective on minority groups (e.g., boats with a green background), its effect on models trained via empirical risk minimization (ERM) is less understood. This is a critical point %Better understanding this setting is crucial 
when additional group membership annotations are too expensive to obtain, and ERM is a key part of training %procedure
\cite{liu2021just, ahmed2021systematic}.

%\vspace{-0.1cm}

%In fact, in this setting, the statistical mechanisms defining the learning of spurious correlations still lack a formal characterization, leaving a gap between its rigorous and intuitive understanding.

% \simone{something is missing.} In particular, these notions of sensitivity become difficult to interpret in the setting of high dimensional data, as in the running example on image recognition, and the considerations on the effect of over-parameterization are \simone{limited to... Thus, at the current stage, a characterization (and formal intuition) on how high-dimensional models learn and rely on spurious correlations is still lacking, and with it also a practical intuition on the underlying statistical mechanisms responsible for this phenomenon in deep learning.}
% \simone{Understanding spurious corr of ERM makes sense . Some algorithms like DRO or reweighing require knowledge on the presence of the spurious feature or minority group. }

Our work tackles these issues: we provide a rigorous characterization of the statistical mechanisms behind learning spurious correlations in high-dimensional data, focusing on %regression and 
the solution obtained via ERM. % data %thus addressing the gap between their rigorous and intuitive understading; 
%and, to do so, we focus on popular and mathematically tractable high-dimensional . 
Formally, we model the input sample $z$ as composed by two distinct features, \emph{i.e.}, $z = [x, y]$, where $x \in \R^d$ is the core feature and $y \in \R^d$ the spurious one. The first panel of Figure \ref{fig:intro} provides an illustration with a boat in the foreground ($x$) and its blue background ($y$). Then, we quantify spurious correlations via the covariance $\mathcal C$ (see \eqref{eq:spurcov}) between the label $g$ (``boat'') and the model output given $\tilde z = [\tilde x, y]$ as input. Here, $\tilde x$ (a truck in the foreground) is a new core feature independent of everything else, see the second panel of Figure \ref{fig:intro}. Now, if $\mathcal C$ is positive, it means that the model is biased towards $g$ only because of $y$, since $\tilde x$ is independent from $x$ and $g$.

More precisely, we provide a sharp, non-asymptotic characterization of $\mathcal C$ for linear regression (Theorem \ref{thm:C}). Armed with such a characterization, we then: 






%\vspace{-0.1cm}

$\bullet$ Interpret $\mathcal C$ via upper bounds on its magnitude (Proposition \ref{prop:boundsC}). This highlights the role of the regularization strength and of the data covariance via \emph{(i)} its Schur complement with respect to the covariance of the core feature $x$, and \emph{(ii)} the covariance of the spurious feature $y$. Specifically, %In particular, in Section \ref{sec:regsimp}, 
we link the smallest eigenvalue of the Schur complement to the strength of the correlation between $y$ and $x$, and the largest eigenvalue of the spurious covariance %of the spurious feature 
to the simplicity of $y$.

%\vspace{-0.1cm}

$\bullet$ Prove a trade-off between $\mathcal C$ and the test loss (Proposition \ref{prop:incr}), which implies that spurious correlations can be beneficial to performance when learning in-distribution. Specifically, we show that the optimal regularization minimizing the test loss lies in an interval where $\mathcal C$ is positive and monotonically increasing.

%\vspace{-0.15cm}

$\bullet$ Investigate the role of over-parameterization via a \emph{random features} (RF) model. % \cite{rahimi2007random}. %, \emph{i.e.} a 2-layer neural network with the hidden weights fixed at initialization. In Theorem \ref{thm:rf}, 
Specifically, we show that the RF model is equivalent to %the solution of %regularized 
linear regression %(as characterized before) 
with an effective regularization %$\tilde \lambda$ 
that depends on the over-parameterization %and the activation function 
(Theorem \ref{thm:rf}). This %in turn
allows to leverage the earlier analysis on regularized linear regression to quantify spurious correlations in over-parameterized, non-linear models. %number of parameters of the model (see \eqref{eq:tildelambda}), which allows to connect the previous results on linear regression to the behavior of over-parameterized models.


%\vspace{-0.15cm}

Throughout the paper, the theoretical %Our %theoretical 
results %and discussions 
are supported %and motivated 
by numerical experiments on %synthetically generated 
Gaussian data, Color-MNIST, and CIFAR-10, %see Figure \ref{fig:intro}), 
which validates % confirms the predictive power of 
our analysis even in settings not strictly following the modeling choices.


%\vspace{-0.4cm}


\section{Related work}\label{sec:rel}


\paragraph{Spurious correlations.} Learning from spurious correlations in a training dataset is rather common %in machine learning 
\cite{geirhos2018imagenettrained, arjovsky2020invariant, geirhos2020, sagawa2020a, xiao2021noise, singla2022salient} and it has % which has been shown to have 
unwanted consequences, e.g., lack of robustness towards domain shift, prediction bias and compromised algorithmic fairness \cite{zliobaite15, geirhos2018imagenettrained, zhou21combating, veitch2021counterfactual, seonguk22bias}.
Thus, multiple mitigation approaches have been proposed, with \cite{sagawa2020a, zhang2021coping} or without \cite{liu2021just, ahmed2021systematic} available annotations. Specifically, \cite{tiwari2023overcoming} exploit the difference in the features learned at different layers of a deep neural network; \cite{izmailov2022on, kirichenko2023last} re-train the last layer of the ERM solution to adapt the features to the distribution shift; and \cite{chang21augmentation, plumb2022finding} mitigate the problem via data augmentation.






\vspace{-0.2cm}

\paragraph{Simplicity bias.}
Recent work has shown that deep learning models have a bias towards learning from ``easier'' patterns \cite{Belkin19, rahaman2019spectral, kalimeris2019sgd}. In %the context of 
shortcut learning, this property is formalized in different ways across the %related 
literature. The difficulty of a feature is defined in terms of the minimum complexity of a network that learns it by \cite{Hermann2020whatshapes} and in terms of the smallest amount of linear segments that separate different classes by \cite{shah2020pitfalls}. \cite{moayeri2022hard}
connect the simplicity to the position and size of the features in an image. % and to the overall space they occupy. %by  and to the noise added to them by \cite{sagawa2020b}. %  and to the overall space they occupy; and  tune the difficulty via the noise added to the separate features. %, as in the conditions of their Theorem 1.
\cite{morwani2023simplicity} define the simplicity bias in 1-hidden layer neural networks via the rank of a projection operator that does not alter them substantially, and they focus on a dataset generated via an independent features model learned via the NTK. The NTK is also used to analyze gradient starvation \cite{pezeshki2021gradient} and feature availability \cite{hermann2024on}, regarded as explanations of the simplicity bias. \cite{qiu2024complexity} focus on parity functions and staircases, analyzing the learning dynamics of features having different complexity.


\vspace{-0.2cm}

\paragraph{High-dimensional regression.}
The test loss of linear regression when the input dimension $d$ scales proportionally with the sample size $n$ has been 
%Linear regression with high-dimensional data, i.e., when the number of input dimensions $d$ scales proportionally with the number of training samples $n$, has recently been object of extensive investigation \cite{misiakiewicz2024six}, also because of its connections with %. %, which has demystified 
%phenomena often occurring in deep learning, such as benign overfitting \cite{bartlett2020benign}. % or double descent \cite{Belkin19}. 
%The usual quantity of interest is the test loss which has been 
characterized precisely both in-distribution  \cite{hastie2019surprises, cheng2024dimension} and under covariate shift \cite{yang2023precise, mallinar2024minimumnorm,song2024generalization}. Furthermore, 
\cite{montanari2019generalization,chang2021provable,han2023distribution} have studied the distribution of the ERM solution via %the framework of 
the convex Gaussian min-max Theorem %(CGMT)
\cite{thrampoulidis2015regularized}. % in terms of a corresponding Gaussian sequence model. 
Specifically, our work builds on the non-asymptotic characterization provided by \cite{han2023distribution}. 

\vspace{-0.1cm}

In contrast with linear regression where the number of parameters equals the input dimension, random features models \cite{rahimi2007random} %are able to 
capture the effects of over-parameterization, as the number of parameters is independently of $d$ and $n$.
\cite{mei2022generalization} have characterized the test loss of random features, showing that it displays a double descent \cite{Belkin19}. Furthermore, the RF model has been used to understand a wide family of phenomena such as feature learning \cite{ba2022highdimensional, damian2022neural, moniri2023theory}, robustness under adversarial attacks \cite{dohmatob2022non, bombari2023universal, hassani2024curse}, and distribution shift \cite{tripuraneni2021overparameterization, lee2023demystifying}. The equivalence between an over-parameterized RF model and a regularized linear one has also been studied in detail %in a recent line of work 
\cite{goldt2022gaussian, goldt2020modeling, hu23universality, montanari2022universality}. However, existing rigorous results show the equivalence at the level of training and test error. In contrast, we are interested in the covariance defined in \eqref{eq:spurcov} and, for this reason, we prove an equivalence at the level of the predictor (Theorem \ref{thm:rf}). %The perspective of our paper differs in the sense that existing focus focuses on training/test error, while we are interested in a different function of the prediction, namely the covariance (see \eqref{}), which have prompted to provide a general result valid at the level of the prediction (Theorem \ref{}).



%\vspace{-0.1cm}


\section{Preliminaries}\label{sec:preliminaries}


\paragraph{Notation.} Given a vector $v$, we denote by $\norm{v}_2$ its Euclidean norm. Given a matrix $A$, we denote by $\tr(A)$ and $\opnorm{A}$ its trace and operator (spectral) norm. Given a symmetric matrix $A$, we denote by $\evmin{A}$ ($\evmax{A}$) its smallest (largest) eigenvalue.
All complexity notations $\Omega(\cdot)$, $\mathcal{O}(\cdot)$, $\omega(\cdot)$, $o(\cdot)$ and $\Theta(\cdot)$ are understood for large data size $n$, input dimension $d$, and number of parameters $p$. We indicate with $C,c>0$ numerical constants, independent of $n$, $d$, $p$, whose value may change from line to line.

\vspace{-0.2cm}


\paragraph{Setting.} We consider %the standard 
supervised learning %setting 
with $n$ training samples $\{ (z_1, g_1), \ldots, (z_n, g_n) \}$ and labels defined by a (not necessarily deterministic) function of the inputs $g_i = f^*(z_i)$, where $z_i \in \R^{2d}$ denotes the $i$-th training input and $g_i \in \R$ the corresponding %$i$-th training 
%(scalar) 
label. Input samples are composed by two distinct parts (or \emph{features}), \emph{i.e.}, $z_i^\top = [x_i^\top, y_i^\top]$, with $x_i, y_i \in \R^d$, and they are sampled i.i.d.\ from the distribution $\mathcal P_{XY}$. We further denote with $\mathcal P_X$ ($\mathcal P_Y$) the marginal distribution of the $x_i$-s ($y_i$-s). The features $x$ and $y$ have the same dimension $d$ to ease the %discussion and the
presentation. %, but our results can be generalized to features of different dimension.

%For the purposes of our work, w
We focus on the setting where the labels $g_i$ depend only on $x_i$, \emph{i.e.}, $g_i = f^*(z_i) = f^*_x(x_i)$ for some (not necessarily deterministic) function $f^*_x$. Hence, $y_i$ is independent from $g_i$, after conditioning on $x_i$. We highlight that the independence between $y_i$ and $g_i$ is conditional to $x_i$, as the covariance between $y_i$ and $x_i$ is in general non-zero. %trivial.
We refer to $y_i$ as the \emph{spurious feature} of the $i$-th sample, and to $x_i$ as its \emph{core feature}. As an example, %one could think of 
$x_i$ may represent the main object in an image and %of 
$y_i$ %as 
the (not necessarily independent) background, see %the CIFAR-10 boat example in
Figure \ref{fig:intro}.

In this setup, %supervised learning setting, 
the training data is used to learn $f^*(z)$ through a parametric model $f(\theta, z)$ via regularized empirical risk minimization (ERM). Specifically, we perform the following optimization in parameter space:
\begin{equation}\label{eq:hattheta}
    \hat \theta = \arg \min_\theta \left( \frac{1}{n} \sum_{i = 1}^n \ell \left( f(\theta, z_i), g_i \right) + \lambda \norm{\theta}_2^2 \right),
\end{equation}
for some regularization term $\lambda \geq 0$, where $\ell$ is a loss function\footnote{In general, existence and uniqueness of $\hat \theta$ depend on the choice of the model $f(\theta, z)$, the loss function $\ell$ and the regularization term $\lambda$. For the purposes of our work, we will precisely define $\hat \theta$ for linear regression (Section \ref{sec:lr}) and for random features (Section \ref{sec:rf}).}. We define the (in-distribution) test loss associated to the model $f(\hat \theta, \cdot)$ as
\begin{equation}\label{eq:testloss}
    \mathcal L(\hat \theta) = \E_{z \sim \mathcal P_{XY}, \, g = f^*(z)} \left[ \ell \left( f(\hat \theta, z), g \right) \right].
\end{equation}


\paragraph{Spurious correlations.} %In many applications, minimizing the test loss $\mathcal L$ is not the sole learning objective, as other properties of the predictor are undesirable, such as the presence of \emph{spurious correlations} between the spurious feature $y$ and the output when evaluating the model $f(\hat \theta, \cdot)$ on new samples taken from a (not necessarily identical) test distribution. In this work, w
We express the extent to which a model $f(\hat \theta, \cdot)$ learns spurious correlations between the spurious feature $y$ and the label $g$ as
\begin{equation}\label{eq:spurcov}
    \mathcal C(\hat \theta) = \Cov \left( f\left(\hat \theta,  [\tilde x^\top, y^\top]^\top \right), g \right),
\end{equation}
where the covariance is computed on the probability space of %the random variables 
$[x^\top, y^\top]^\top \sim \mathcal P_{XY}$, $g = f^*_x(x)$ and of the independent core feature $\tilde x \sim \mathcal P_X$. In words, $\mathcal C(\hat \theta)$ expresses how the output of the model $f(\hat \theta, \cdot)$ evaluated on an out-of-distribution sample $[\tilde x^\top, y^\top]^\top$ (where the two features are sampled independently from the marginal distributions $\mathcal P_X$ and $\mathcal P_Y$) correlates to the label associated to the in-distribution sample $g = f^*(z) = f^*_x(x)$. We highlight that, if the model $f(\hat \theta, \cdot)$ \emph{does not rely} on the spurious feature $y$, then $\mathcal C(\hat \theta) = 0$ as $x$ and $\tilde x$ are independent. 

We note that \eqref{eq:spurcov} formalizes to the regression setting the definition in the survey \cite{ye2024spurious} and the fairness metric in \cite{zliobaite15} (when interpreting $y$ as the protected variable). Furthermore, 
in the context of classification, it can connected to the worst group accuracy \cite{sagawa2020a, sagawa2020b}. In fact, %, since it captures the detrimental effect learning spurious correlations would have on samples where $f^*(\tilde x)$ is different from $f^*(x)$. %  in the classification setting.
%
%Nevertheless, in this work, we focus on quantifying the amount of spurious correlations, and thus on the quantity in \eqref{eq:spurcov}, whichin the regression setting, . This 
%
our definition \eqref{eq:spurcov} is also related to the \emph{out-of-distribution} test loss: %, see the discussion %: intuitively, if $f(\hat \theta, \cdot)$ relies on $y$ to predict the label associated to (the independent) $\tilde x$, the excess variance will increase the loss. This point is further discussed 
% at the end of Section \ref{sec:regsimp}. %, and we elaborate more formally, and numerically, in Appendix \ref{app:ood}.} 
% \marco{add a pointer to the later discussion with the plot (deferred to Appendix). If time allows, add there the bound on the out-of-distribution test loss. This can be a paragraph in Section 4.}
assuming $\E [ f(\hat \theta, [\tilde x^\top, y])^2 ] = \E [ f^*_x(\tilde x)^2 ] = 1$ % \E_x \left[ f^*_x(x)^2 | y = \bar y \right] = 1$
and $\E [ f(\hat \theta, [\tilde x^\top, y]) ] = \E [ f^*_x(\tilde x) ] = 0$ for simplicity and considering a quadratic loss, we have
\begin{equation}\label{eq:oodlossbody}
    \E_{\tilde x, y} \left[ \left( f(\hat \theta, [\tilde x^\top, y]) - f^*_x(\tilde x) \right)^2 \right] \geq 2 - 2 \sqrt{1 - \mathcal C(\hat \theta)^2}.
\end{equation}
This implies that an increase in $\mathcal C(\hat \theta)$ hurts the performance of the model when %on data where 
core and spurious features are sampled independently (and, thus, the model is tested out-of-distribution). The proof of \eqref{eq:oodlossbody} is in Appendix \ref{app:ood}.



\section{Precise analysis for linear regression}\label{sec:lr}

To study $\mathcal C(\cdot)$ as defined in \eqref{eq:spurcov}, we focus on a %the analytically tractable setting of the 
high-dimensional \emph{linear regression} model, \emph{i.e.},
\begin{equation}\label{eq:lrmodel}
    f_{\textup{LR}}(\theta, z) = z^\top \theta,
\end{equation}
where $\theta \in \R^{2d}$. The data also follows a linear model, \emph{i.e.}, 
\begin{equation}\label{eq:datamodel}
g_i = z_i^\top \theta^* + \epsilon_i = x_i^\top \theta^*_x + \epsilon_i,    
\end{equation}
where $\theta^* \in \R^{2d}$, $\theta^*_x \in \R^{d}$, and $\epsilon_i$ is label noise.
The second equality in \eqref{eq:datamodel} implies that $\theta^* = [{\theta^*_x}^\top, \mathbf{0}_d^\top]^\top$, where $\theta^*_x, \mathbf{0}_d \in \R^d$ and each entry of $\mathbf{0}_d$ is 0. We set $\norm{\theta^*}_2 = \norm{\theta^*_x}_2 = 1$ and let the $\epsilon_i$-s be i.i.d.\ (and independent from the $z_i$-s), mean-0, sub-Gaussian, with variance $\sigma^2 > 0$.

We introduce the shorthands $Z = [z_1^\top, \dots, z_n^\top]^\top \in \R^{n \times 2d}$, $G = [g_1, \dots, g_n]^\top \in \R^{n}$, and $\mathcal E = [\epsilon_1, \dots, \epsilon_n]^\top \in \R^{n}$ to indicate the data matrix, the labels, and the noise vector respectively. Then, using a quadratic loss, \eqref{eq:hattheta} reads
\begin{equation}\label{eq:hatthetalr}
    \hat \theta_{\textup{LR}}(\lambda) = \arg \min_\theta \left( \frac{1}{n} \norm{Z \theta - G}_2^2 + \lambda \norm{\theta}_2^2 \right),
\end{equation}
which admits the unique solution
\begin{equation}\label{eq:hatthetalambda}
    \hat \theta_{\textup{LR}}(\lambda) = \left( Z^\top Z + n \lambda I \right)^{-1} Z^\top G,
\end{equation}
for $\lambda > 0$ and, if $Z^\top Z$ is invertible, also for $\lambda = 0$.

\begin{assumption}[Data distribution]\label{ass:data}
    %The input samples 
    $\{z_i\}_{i=1}^n$ are $n$ i.i.d.\ samples from the multivariate, mean-0, Gaussian distribution $\mathcal P_{XY}$, such that its covariance $\Sigma := \E \left[zz^\top \right] \in \R^{2d \times 2d}$ is invertible, with $\evmax{\Sigma} = \bigO{1}$, $\evmin{\Sigma} = \Omega(1)$, and $\tr(\Sigma) = 2d$.
    % \item $z \sim P_Z$ respects the Lipschitz concentration property.
\end{assumption}
% \begin{assumption}[Data distribution]\label{ass:data}
%     The input samples $\{z_i\}_{i=1}^n$ are $n$ i.i.d.\ samples from the multivariate, mean-0, Gaussian distribution $\mathcal P_{XY}$, such that
%     \begin{enumerate}
%     \item its covariance $\Sigma \in \R^{2d \times 2d}$ is invertible, with $\evmax{\Sigma} = \bigO{1}$, $\evmin{\Sigma} = \Omega(1)$, and $\tr(\Sigma) = 2d$;
%     \item for $z \sim P_Z$, the random variable $\Sigma^{-1/2} z$ has independent, mean-0, unit variance, sub-Gaussian entries;
%     % \item $z \sim P_Z$ respects the Lipschitz concentration property.
%     \end{enumerate}
% \end{assumption}
This requirement could be relaxed to having sub-Gaussian data. We focus on the Gaussian case for simplicity, deferring the discussion on the generalization to Appendix \ref{app:datageneral}.
%While our discussion considers multivariate Gaussian data, it is possible to consider a more general setting (such as sub-Gaussian data, see Appendix \ref{app:notation}). However, for the ease of presentation and to avoid the additional notation required from this case, we defer the discussion  






\paragraph{Warm-up: no regularization ($\lambda=0$).} Our first result concerns the amount of spurious correlations learned in the un-regularized setting. % $\mathcal C(\hat \theta_{\textup{LR}}(\lambda = 0))$.

\begin{proposition}\label{prop:lambda0}
Let $\lambda = 0$ and $Z^\top Z \in \R^{2d \times 2d}$ be invertible\footnote{Under Assumption \ref{ass:data}, this holds with probability 1 for $n \geq 2d$.}. Let $\mathcal C(\hat \theta_{\textup{LR}}(0))$ be the amount of spurious correlations learned by the model $f_{\textup{LR}}(\hat \theta_{\textup{LR}}(0) )$. % as defined in \eqref{eq:spurcov}. %, where \hat \theta_{\textup{LR}}(0) = \left( Z^\top Z \right)^{-1} Z^\top G
%Then, \eqref{eq:hatthetalr} admits the unique solution
%\begin{equation}\label{eq:hatthetalr0}
%   \hat \theta_{\textup{LR}}(0) = \left( Z^\top Z \right)^{-1} Z^\top G .
%\end{equation}
 Then, we have that 
 \begin{equation}\label{eq:exp0}
 \E_{\mathcal E}[ \mathcal C(\hat \theta_{\textup{LR}}(0)) ] = 0.    
 \end{equation}
Furthermore, if Assumption \ref{ass:data} holds and $n = \omega(d)$, %we have that 
 \begin{equation}\label{eq:exp0c}
| \mathcal C(\hat \theta_{\textup{LR}}(0)) | = \mathcal O (\log d / \sqrt d),
\end{equation}
with probability at least $1 - 2 \exp ( -c \log^2 d )$ over $Z$ and $\mathcal E$, where $c$ is an absolute constant.
\end{proposition}


In words, %this result shows that in the non-regularized setting the model 
$f_{\textup{LR}} (\hat \theta_{\textup{LR}}(0))$ \emph{does not learn} any spurious correlation between the spurious feature $y$ and the label $g$. This is also clear from Figure \ref{fig:lambda}, where we report in red the value of $\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$, % for both synthetic Gaussian data and the binary Color-MNIST (see Figure \ref{fig:intro}), and 
which approaches 0 as $\lambda$ becomes small.

The idea of the argument is to write explicitly the solution 
\begin{equation}
\hat \theta_{\textup{LR}}(0) =  \left( Z^\top Z \right)^{-1} Z^\top G = \theta^* + \left( Z^\top Z \right)^{-1} Z^\top \mathcal E, 
\end{equation}
where in the second step we separate the ground truth $\theta^*$ (which does not capture any dependence on $y$) from a term only depending on the label noise, which is mean-0 and independent from $y$. This directly gives \eqref{eq:exp0}. Then, the %non-asymptotic
bound  in \eqref{eq:exp0c} is obtained via standard concentration results on $\evmin{Z^\top Z}$. The details are in %deferred to 
Appendix \ref{app:lr}. %\marco{2 lines about the proof} %  and \simone{something else?}







\paragraph{General case with regularization ($\lambda > 0$).} 

Setting a regularizer $\lambda > 0$ often reduces the test loss, see the black curve in Figure \ref{fig:lambda}. However, it also leads to non-trivial spurious correlations, and our main result provides a non-asymptotic characterization of this phenomenon. %$\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$ in the proportional regime $n=\Theta(d)$.

\begin{theorem}\label{thm:C}
    Let Assumption \ref{ass:data} hold, %and let
    $n = \Theta(d)$ and %. Let $\hat \theta_{\textup{LR}}(\lambda)$ be defined as in \eqref{eq:hatthetalambda}, and let 
    $\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$ be the amount of spurious correlations learned by the model $f_{\textup{LR}} (\hat \theta_{\textup{LR}}(\lambda) )$ for $\lambda > 0$. % as defined in \eqref{eq:spurcov}.
    %Then, for any $\lambda > 0$, d
    Denote by $P_y \in \R^{2d \times 2d}$ the projector on the last $d$ elements of the canonical basis in $\R^{2d}$, and set %introducing the shorthand
    \begin{equation}\label{eq:CSigmatau}
        \mathcal C^\Sigma(\lambda) := {\theta^*}^\top \Sigma \left( \Sigma + \tau(\lambda) I \right)^{-1} P_y \Sigma \theta^*, 
    \end{equation}
    where $\tau:=\tau(\lambda)$ is implicitly defined as the unique positive solution of
\begin{equation}\label{eq:tau}
    1 - \frac{\lambda}{\tau} = \frac{1}{n} \tr \left( \left( \Sigma + \tau I \right)^{-1} \Sigma\right).
\end{equation}
    Then, 
%   defined according to \eqref{eq:tau}, we have that, 
for every $t \in (0, 1/2)$,
    \begin{equation*}% \label{eq:concC}
    \P_{Z, \mathcal E} \left( \left| \mathcal C(\hat \theta_{\textup{LR}}(\lambda)) - \mathcal C^\Sigma(\lambda) \right| \geq t \right)  \leq C d \exp \left( -d t^4 / C \right),
    \end{equation*}
    where $C$ is an absolute constant. 
\end{theorem}



\begin{wrapfigure}{r}{0.6\textwidth}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/synthetic-mnist-lambda-ns.pdf}
  \end{center}
  \vspace{-0.2cm}
  \caption{Test loss $\mathcal L(\hat \theta_{\textup{LR}}(\lambda))$ (black) and spurious correlations $\mathcal C(\hat \theta_{\textup{LR}}(\lambda)$ (red) as a function of the regularization term $\lambda$ for two values of the number of samples $n$. \emph{Left:} synthetic Gaussian dataset, with $d = 400$ (additional details in Appendix \ref{app:experiments}); \emph{right:} binary Color-MNIST dataset with correlation $\sqrt{1 - \alpha^2} = 0.25$ between color and digit (see Figure \ref{fig:intro}).}
  \vspace{-0.1cm}
  \label{fig:lambda}
\end{wrapfigure}


In words, Theorem \ref{thm:C} %provides a non-asymptotic concentration bound for $\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$, as it 
guarantees that $| \mathcal C(\hat \theta_{\textup{LR}}(\lambda)) - \mathcal C^\Sigma(\lambda) | = o(1)$ with high probability (e.g., setting $t = d^{-1/5}$). Thus, for large $d, n$, we can estimate $\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$ via the deterministic quantity $\mathcal C^\Sigma(\lambda)$, % defined in \eqref{eq:CSigmatau}, 
which depends on the true parameter $\theta^*$, the covariance of the data $\Sigma$, and the regularization $\lambda$ via the parameter $\tau(\lambda)$ introduced in \eqref{eq:tau}. 

%In this setting, $\hat \theta_{\textup{LR}}(\lambda)$ in \eqref{eq:hatthetalr} always admits the unique solution
%\begin{equation}\label{eq:hatthetalambda}
%    \hat \theta_{\textup{LR}}(\lambda) = \left( Z^\top Z + n \lambda I \right)^{-1} Z^\top G.
%\end{equation}
%Differently from the case $\lambda = 0$, % it is in general not true that $C\left( \hat \theta_{\textup{LR}}(\lambda) \right) = 0$. In fact,
%the computation of $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$ is less trivial, 
Note that, since $\hat \theta_{\textup{LR}}(\lambda)$ is given by \eqref{eq:hatthetalambda}, when $\lambda>0$ it cannot be %\hat \theta_{\textup{LR}}(\lambda)$ cannot be easily 
decomposed as $\theta^* + \left( Z^\top Z \right)^{-1} Z^\top \mathcal E$ (as in the proof of Proposition \ref{prop:lambda0} for $\lambda=0$). Thus, %the %precise 
%analysis of 
%to analyze $\mathcal C\left (\hat \theta_{\textup{LR}}(\lambda)\right)$, 
we % requires a more sophisticated approach. In this work, we 
rely on the non-asymptotic characterization of $\hat \theta_{\textup{LR}}(\lambda)$ recently provided by \cite{han2023distribution}. In particular, their analysis focuses on the proportional regime $n = \Theta(d)$, and it allows to provide concentration bounds on a certain family of low-dimensional functions of $\hat \theta_{\textup{LR}}(\lambda)$, which includes $\mathcal C$ as defined in \eqref{eq:spurcov}. 
The details are in %deferred to %proof of Theorem \ref{thm:C} is an application of Theorem 2.3 in \cite{han2023distribution}, and is deferred to 
Appendix \ref{app:lr}.


%For the discussion, it is key to introduce the following
%\begin{definition}
%Let $\tau$ be the unique positive solution of the following equation
%\begin{equation}\label{eq:tau}
%    1 - \frac{\lambda}{\tau} = \frac{1}{n} \tr \left( \left( \Sigma + \tau I \right)^{-1} \Sigma\right).
%\end{equation}
%\end{definition}
%The uniqueness of a positive solution $\tau$ of the equation above is proven in \cite{han2023distribution}. 

%With a slight abuse of notation, this allows to define the function $\tau(\lambda) : \R^+ \to \R^+$, proven to be increasing in \cite{han2023distribution}. Furthermore, notice that the RHS of \eqref{eq:tau} is smaller or equal to $2d / n$; thus, if we consider $2d < n$, we also get $\tau \leq \lambda \left( 1 - 2d / n\right)^{-1}$, which implies $\tau(\lambda) \to 0$ as $\lambda \to 0$. Also, since the RHS is always positive, \eqref{eq:tau} also tells that $\tau(\lambda) \to \infty$ as $\lambda \to \infty$. % Then, we are ready to state the following







% \begin{proposition}\label{prop:Cwithblocks}
%     Let $\mathcal C^\Sigma(\lambda)$ be defined in \eqref{eq:CSigmatau}, and let $S_x^{\Sigma + \tau(\lambda) I}$ be the Schur complement of $\Sigma + \tau(\lambda) I$ with respect to the top-left $d \times d$ block. Then, we have that
%     \begin{equation}\label{eq:statement1bound}
%     \begin{aligned}
%         &\mathcal C^\Sigma(\lambda) = \\
%         &\tau(\lambda) \, {\theta^*_x}^\top  \left( \Sigma_{xx} + \tau(\lambda) I \right)^{-1} \Sigma_{xy} \left( S_x^{\Sigma + \tau(\lambda) I} \right)^{-1} \Sigma_{yx} \theta^*_x.
%         % &= \tau \, \left( {\theta^*_x}^\top  \left( I - \left( \Sigma_{xx} + \tau I \right) ^{-1} \Sigma_{xy} \left(\Sigma_{yy} + \tau I\right)^{-1} \Sigma_{yx}\right)^{-1} {\theta^*_x}  - 1 \right) \\
%         % &= \tau \, \left( {\theta^*_x}^\top \left( S_y^{\Sigma + \tau I} \right)^{-1} \Sigma_{xx} {\theta^*_x} - 1 \right)
%     \end{aligned}
%     \end{equation}
% \end{proposition}
% In words, Proposition \ref{prop:boundsC} expresses $\mathcal C^\Sigma(\lambda)$ in terms of $\lambda$, the blocks defined in \eqref{eq:Sigmablocks}, and the Schur complement $S^{\Sigma + \tau(\lambda) I}_x$ via \eqref{eq:statement1bound}. 


\section{Roles of regularization and simplicity bias}\label{sec:regsimp}

We now interpret %the quantity 
$\mathcal C^\Sigma(\lambda)$, which characterizes the
spurious correlations via Theorem \ref{thm:C}, in terms of the data covariance $\Sigma$ and the regularization $\lambda$. To do so, 
%regarded as a proxy for the spurious correlation $\mathcal C$  introduced in \eqref{eq:CSigmatau}, with the purpose of interpreting the effects that the structure of $\Sigma$ defined in \eqref{eq:Sigmablocks} and $\lambda$ have on its magnitude.
%While statistically informative, Theorem \ref{thm:C} is not easily interpretable in terms of how the structure of $\Sigma$ and the value of $\lambda$ affect the magnitude of $\mathcal C$. To address this, % we study the behavior of $\mathcal C^\Sigma(\lambda)$ performing additional algebraic manipulations. To do so,
%it is helpful to 
we express $\Sigma$ as %introduce the following notation
\begin{equation}\label{eq:Sigmablocks}
\Sigma = \left(\begin{array}{@{}c|c@{}}
  \Sigma_{xx}
  & \Sigma_{xy} \\
\hline
  \Sigma_{yx} &
  \Sigma_{yy}
\end{array}\right),
\end{equation}
where the block $\Sigma_{xx} = \E_{x \sim \mathcal P_{X}} \left[ xx^\top \right]\in \R^{d \times d}$ ($\Sigma_{yy} = \E_{y \sim \mathcal P_{Y}} \left[ yy^\top \right]\in \R^{d \times d}$) denotes the covariance of the core (spurious) feature sampled from its marginal distribution. The off-diagonal blocks are $\Sigma_{xy} = \Sigma_{yx}^\top = \E_{[x^\top, y^\top]^\top \sim \mathcal P_{XY}} \left[ xy^\top \right] \in \R^{d \times d}$. Let us denote by 
\begin{equation}\label{eq:schur}
 S_x^\Sigma := \Sigma_{yy} - \Sigma_{yx} \Sigma_{xx}^{-1} \Sigma_{xy}   
\end{equation}
the Schur complement of $\Sigma$ with respect to the top-left $d \times d$ block $\Sigma_{xx}$.
% It is a useful tool in numerical analysis and linear algebra \simone{CITECITE}
In our setting, $S_x^\Sigma$ offers a helpful statistical interpretation. In fact, for multivariate Gaussian data, it corresponds to the conditional covariance of $y$ given $x$, \emph{i.e.},
\begin{equation}\label{eq:schurcondcovariance}
% \begin{aligned}
    S_x^\Sigma = \Cov \left( y | x = \bar x \right) = \E_{y | x = \bar x} \left[ \left(y - \E_{y | x = \bar x} [y] \right) \left(y - \E_{y | x = \bar x} [y] \right)^\top \right].
% \end{aligned}
\end{equation}
Therefore, the spectrum of $S_x^\Sigma$ describes the degree of dependence between $y$ and $x$: on the one hand, if its eigenvalues are small, the feature $y$ is close to be determined by the knowledge of the feature $x$ (\emph{i.e.}, $y$ is highly correlated with $x$); on the other hand, if its eigenvalues are large, the two features tend to be independent. % In particular, in our discussion, we will exploit $\evmin{S_x^\Sigma}$ to quantify ``degree of correlation'' between $y$ and $x$
%We finally remark that $S_x^\Sigma$ is p.s.d., and in particular we have $\evmin{S_x^\Sigma} \geq \evmin{\Sigma}$. We defer the proof of this statement, as well as the one of \eqref{eq:schurcondcovariance}, to Appendix \ref{app:schur}.

As an intuitive example, let us interpret classification for the binary Color-MNIST dataset (see Figure \ref{fig:intro}) as a 2-dimensional problem: the core feature is $x=+1$ ($-1$) if the digit on the image is $1$ ($0$); and the spurious feature is $y=+1$ ($-1$) if the color is red (blue). If the digits and the colors have a correlation of $\alpha$, then $\Sigma_{xx} = \Sigma_{yy} = 1$ and $\Sigma_{xy} = \Sigma_{yx} = \alpha$. Thus, %the (one-dimensional) Schur complement is equal to 
$S_x^\Sigma = 1 - \alpha^2$, which becomes 0 if $|\alpha| = 1$ (full correlation), and it is maximized in the setting of independence between colors and parity ($\alpha = 0$).

At this point, leveraging the decomposition of $\Sigma$ in \eqref{eq:Sigmablocks} and the Schur complement in \eqref{eq:schur}, we provide the following bounds on $\mathcal C^\Sigma(\lambda)$, which are proved in Appendix \ref{app:lr}. %the  
% We formally tackle this problem via the following
% Then, exploiting the definition of $S^\Sigma_x$ and the notation in \eqref{eq:Sigmablocks} we have the following
% We have

\begin{proposition}\label{prop:boundsC}
    Let $\mathcal C^\Sigma(\lambda)$ and $S_x^{\Sigma}$ be defined in \eqref{eq:CSigmatau} and \eqref{eq:schur}, respectively. Then, %let $S_x^{\Sigma}$ be the Schur complement of $\Sigma$ with respect to the top-left $d \times d$ block. Then, we have that
    \begin{equation}\label{eq:boundsC}
    % \begin{aligned}
        \left| \mathcal C^\Sigma(\lambda) \right| \leq \min \left(\opnorm{\Sigma_{yx}}, \frac{\evmax{\Sigma}^2}{\tau(\lambda)}, \tau(\lambda) \sqrt{\Var(g) - \sigma^2} \frac{\evmax{\Sigma_{yy}} - \evmin{S_x^\Sigma}}{\evmin{S_x^\Sigma} \sqrt{\evmin{\Sigma_{xx}}}} \right).
    % \end{aligned}
    \end{equation}
\end{proposition}

% In words, Proposition \ref{prop:boundsC} expresses $\mathcal C^\Sigma(\lambda)$ in terms of $\lambda$, the blocks defined in \eqref{eq:Sigmablocks}, and the Schur complement $S^{\Sigma + \tau(\lambda) I}_x$ via \eqref{eq:statement1bound}. 

We discuss the three upper bounds in \eqref{eq:boundsC} below.
%The three upper-bounds offer the following intuitions:

\emph{(i)}: $|\mathcal C^\Sigma(\lambda)| \leq \opnorm{\Sigma_{yx}}$. The off-diagonal blocks $\Sigma_{yx} = \E[y x^\top]$ and $\Sigma_{xy} = \Sigma_{yx}^\top$ describe the correlation between $y$ and $x$. In the limit case $\opnorm{\Sigma_{yx}} = 0$, we have that $x$ and $y$ are uncorrelated and, therefore, $\mathcal C^\Sigma(\lambda) = 0$, as there is no spurious correlation that the model can learn.

\emph{(ii)}: $|\mathcal C^\Sigma(\lambda)| \leq \evmax{\Sigma}^2 / \tau(\lambda)$. From \eqref{eq:tau}, one %readily 
obtains that $\tau (\lambda) \to \infty$ as $\lambda \to \infty$. Thus, %Besides the scaling factor $\evmax{\Sigma}^2$, 
the bound implies that $\mathcal C^\Sigma(\lambda)$ approaches 0 as $\lambda$ grows large. This captures the intuition that, when the regularization %term
$\lambda$ is large, the minimization in \eqref{eq:hatthetalr} is biased towards solutions with small norm and, therefore, the output of the model is small, which drives to $0$ % which drive to $0$ the %such that $\|\hat \theta_{\textup{LR}}(\lambda)\|_2$ is sufficiently small to make the output of the model (and therefore the 
the spurious correlations as defined in \eqref{eq:spurcov}. The behavior is confirmed by %the numerical experiments of 
Figure \ref{fig:lambda}:  %where % in both datasets
$|\mathcal C(\hat \theta_{\textup{LR}}(\lambda))|$ is decreasing for large values of $\lambda$ and it eventually vanishes; at the same time, large values of $\lambda$ make the output of the model small, which in turn increases the test loss $\mathcal L(\hat \theta_{\textup{LR}}(\lambda))$. %, as displayed in black in Figure \ref{fig:lambda}.






\emph{(iii)}: The third bound in \eqref{eq:boundsC} can be rewritten as
\begin{equation}\label{eq:rewrite}
    \frac{|\mathcal C^\Sigma(\lambda)| \sqrt{\evmin{\Sigma_{xx}}} }{\sqrt{\Var(g) - \sigma^2}} \leq \tau(\lambda)  \frac{\evmax{\Sigma_{yy}} - \evmin{S_x^\Sigma}}{\evmin{S_x^\Sigma}},
\end{equation}
where we have isolated on the LHS the terms depending on the covariance of the core feature $x$ ($\sqrt{\evmin{\Sigma_{xx}}}$) and on the scaling of the labels ($\sqrt{\Var(g) - \sigma^2}$). We now discuss the dependence of the RHS of \eqref{eq:rewrite} w.r.t.\ \emph{(a)} $\tau(\lambda)$, \emph{(b)} $\evmin{S_x^\Sigma}$, and \emph{(c)} $\evmax{\Sigma_{yy}}$. As for \emph{(a)},
we note that $\mathcal C^\Sigma(\lambda)$ approaches 0 for small values of $\lambda$. In fact, 
the RHS of \eqref{eq:tau} is smaller or equal to $2d / n$; thus, if we consider $2d < n$, we also get $\tau \leq \lambda \left( 1 - 2d / n\right)^{-1}$, which implies $\tau(\lambda) \to 0$ as $\lambda \to 0$. This is in agreement with Proposition \ref{prop:lambda0}, which handles the case without regularization, and also with the numerical experiments of Figure \ref{fig:lambda}. As for \emph{(b)}, we note that the bound is decreasing with $\evmin{S_x^\Sigma}$. This is in agreement with the earlier discussion on how the spectrum of the Schur complement $S_x^\Sigma$ measures the degree of independence between the spurious feature $y$ and the core feature $x$. Finally, as for \emph{(c)}, we note that the bound is increasing with $\evmax{\Sigma_{yy}}$, which is connected below to the \emph{simplicity} of the spurious feature $y$. The increasing (decreasing) trend of $\mathcal C^\Sigma(\lambda)$ w.r.t.\ $\evmax{\Sigma_{yy}}$ ($\evmin{S_x^\Sigma}$) is clearly displayed in Figure \ref{fig:simplicitysynthetic} for Gaussian data. 

%In Figure \ref{fig:simplicitysynthetic}, we numerically investigate the dependence discussed in \emph{(2)} and \emph{(3)} for synthetically generated Gaussian datasets and a fixed value of $\lambda$, and recover the .



\begin{wrapfigure}{r}{0.6\textwidth}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/simplicity_and_correlation_synthetic_new.pdf}
  \end{center}
  \vspace{-0.1cm}
  \caption{Test loss $\mathcal L(\hat \theta_{\textup{LR}/\textup{RF}}(\lambda))$ (black) and spurious correlations $\mathcal C(\hat \theta_{\textup{LR}/\textup{RF}}(\lambda))$ (red) as a function of $\evmax{\Sigma_{yy}}$ (left) and $\evmin{S_x^\Sigma}$ (right) on a synthetic Gaussian dataset, for both linear regression and random features,  with $\lambda = 1$ (additional details in Appendix \ref{app:experiments}).}
  \label{fig:simplicitysynthetic}
  \vspace{-0.1cm}
\end{wrapfigure}


The connection between $\evmax{\Sigma_{yy}}$ and the \emph{simplicity bias} of ERM can be illustrated via our initial image recognition example. The (spurious) background feature is intuitively an easy pattern to learn from the model: %it is simple and consistent across training samples. The simplicity and consistency are due to the fact that 
the pixels corresponding to the spurious feature behave consistently %among themselves and 
across the training data. % points. 
This in turn 
skews the spectrum of $\Sigma_{yy}$, which has few dominant directions with eigenvalues much larger than the others. 
%decreases the number of directions of that effectively contain any signal from $y$
%,  and, therefore, the difficulty to learn it. 
Note that this interpretation is similar to the model-dependent definition of simplicity in \cite{morwani2023simplicity}. %, but is fundamentally different from considering $\tr(\Sigma_{yy})$ instead, which in turn concentrates to $\norm{y}_2^2$, quantity previously associated to the simplicity of the feature $y$ \cite{moayeri2022hard}.}
An %simple 
empirical verification %of this interpretation 
is provided in Figure \ref{fig:simplicitycifar}, where we consider the CIFAR-10 dataset, restricted to the ``boat'' and ``truck'' classes. %, mapped to the labels 1 and -1 respectively
Before training a regression model, we whiten up to some level the background feature (as defined in Figure \ref{fig:intro}) to make it harder to learn, see the right side of Figure \ref{fig:simplicitycifar}. Then, for different levels of whitening, we report $\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$ as a function of $\evmax{\Sigma_{yy}}$. We normalize $\evmax{\Sigma_{yy}}$ by the trace  $\tr(\Sigma_{yy})$ to exclude the size of the pattern from our experiment\footnote{If $y$ has 0-mean, then $\E\|y\|_2^2= \tr(\Sigma_{yy})$, i.e., the trace captures the size of the pattern.}. The red curve shows an increasing trend analogous to that displayed in Figure \ref{fig:simplicitysynthetic} for Gaussian data: small values of $\evmax{\Sigma_{yy}}$ %$/\tr(\Sigma_{yy})$ 
correspond to significant whitening and, hence, to small spurious correlations, as predicted by Proposition \ref{prop:boundsC}. % This  the highest level of whitening small, analogous to that obtained for Gaussian data in Figure \ref{fig:simplicitysynthetic}, which is well explained by our analysis. %done previously, as well as we estimate the covariance $\Sigma_{yy}$, and in particular its largest eigenvalue. We report the dependence between these two variables in the left panel of Figure \ref{fig:simplicitycifar}, where the small values of $\evmax{\Sigma_{yy}}$ correspond to the highest level of whitening before training, and we recover the same trend for the Gaussian setting shown .

% In fact, the largest eigenvalue of $\Sigma_{yy}$ encodes how much of the full spurious feature size $\| y \|_2^2$ (which concentrates to $\tr(\Sigma_{yy})$) distributes on an individual low dimensional space. Following the image recognition example, it is common to consider the backgrounds as ``easy'' features the the algorithm can easily use to predict the right label. This simplicity is intuitively due to the fact that this pattern has only one color (blue or green) that repeats in all the pixels of the background. Thus, denoting with $v_{\textup{color}}$ the vector containing $+1$ in all the background entries of the blue channel, and $-1$ in all the ones of the green channel, we expect to have $v_{\textup{color}}^\top  \Sigma_{yy} v_{\textup{color}} \propto d \| v_{\textup{color}} \|_2^2$, where in this case $d$ represents the number of pixels in the background. 
% The dependence of $\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$ on $\evmax{\Sigma_{yy}}$ ($\evmin{S_x^\Sigma}$) is numerically represented in the left (right) panel of Figure \ref{fig:simplicitysynthetic} on a synthetic dataset, which displays the expected increasing (decreasing) trend.




\paragraph{Trade-off between $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$ and $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$.}
Figure \ref{fig:lambda} shows that there is an interval of values for the regularization ($\lambda \sim 10^{-1}$) where the test loss $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$ is decreasing in $\lambda$, while the spurious correlations $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$ are %positive and
increasing. This evidence suggests a natural trade-off between these two quantities, mediated by $\lambda$.
% Both the results on the Gaussian synthetic dataset and color-MNIST suggest that the test loss can benefit from a strictly positive value of the regularization $\lambda$, while it tends to increase for larger than optimal values of it. % (up to some critical value, where afterwards increasing further the value of $\lambda$ increases the test loss).
% On the other hand, the experiments also show that $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$ is positive and strictly increasing in $\lambda$ at first, and then tends to 0 at larger values of $\lambda$ (as also predicted by the upper-bounds in Proposition \ref{prop:boundsC}).
% On the other hand, Proposition \ref{prop:boundsC} does not predict this initial increasing trend of $\mathcal C$, as in particular it does not even provide any insight on the sign of $\mathcal C$. Therefore, in this paragraph, 
To theoretically capture such trade-off, % in this paragraph, we provide a result on the positivity of $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$, and of its increasing trend in $\lambda$ in an interval containing the value of the regularization $\lambda^*$ that minimizes $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$, demonstrating a trade-off between the minimization of these two quantities.
% In order to state our results, 
we first provide a non-asymptotic concentration bound for $\mathcal L(\hat \theta_{\textup{LR}}(\lambda))$. % a non-asymptotic concentration bound on $\mathcal L^\Sigma(\lambda)$ via the following
% we will consider the additional hypothesis that the core features are isotropic, \emph{i.e.} $\Sigma_{xx} = I$.
% \marco{since the lemma below is just a re-statement of Han and Xu, maybe it's not worth stating it separately. We can just incorporate the result in the proof that needs it}
\begin{proposition}\label{lemma:L}
    Let Assumption \ref{ass:data} hold, $n = \Theta(d)$ and %. Let $\hat \theta_{\textup{LR}}(\lambda)$ be defined as in \eqref{eq:hatthetalambda}, and let
    $\mathcal L(\hat \theta_{\textup{LR}}(\lambda))$ be the in-distribution test loss of the model $f_{\textup{LR}}(\hat \theta_{\textup{LR}}(\lambda) )$ for $\lambda>0$. %as defined in \eqref{eq:lrmodel}. Then, for any $\lambda > 0$, introducing the shorthand
Set
    \begin{equation}\label{eq:LSigmatau}
        \mathcal L^\Sigma(\lambda) := \frac{\sigma^2 + \tau(\lambda)^2 \norm{ \left( \Sigma + \tau (\lambda)I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2}{1 - \frac{ \tr \left( \left( \Sigma + \tau (\lambda)I\right)^{-2} \Sigma^2 \right)}{n}},
    \end{equation}
    where $\tau(\lambda)$ is defined via $\eqref{eq:tau}$. Then, for every $t \in (0, 1/2)$,
    \begin{equation*}
    \P_{Z, \mathcal E} \left( \left| \mathcal L(\hat \theta_{\textup{LR}}(\lambda)) - \mathcal L^\Sigma(\lambda) \right| \geq t \right)  \leq C d \exp \left( -d t^4 / C \right),
    \end{equation*}
    where $C$ is an absolute constant.
\end{proposition}
In words, %Similarly to Theorem \ref{thm:C}, Lemma
Proposition \ref{lemma:L} %provides a non-asymptotic concentration bound for $\mathcal L(\hat \theta_{\textup{LR}}(\lambda))$, as we have 
guarantees that $| \mathcal L(\hat \theta_{\textup{LR}}(\lambda)) - \mathcal L^\Sigma(\lambda) | = o(1)$ with high probability. Its proof is an adaptation of Theorem 3.1 in \cite{han2023distribution}, and the details are in Appendix \ref{app:lr}.








Armed with the non-asymptotic bounds of Theorem \ref{thm:C} and Proposition \ref{lemma:L}, we characterize the trade-off between $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$ and $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$ by studying the monotonicity of $\mathcal L^\Sigma(\lambda)$ and $\mathcal C^\Sigma(\lambda)$. %  via the following
% \begin{proposition}\label{prop:incr}
%     Let $\mathcal C^\Sigma(\tau(\lambda))$ and $\mathcal L^\Sigma(\tau(\lambda))$ be defined as in \eqref{eq:CSigmatau} and \eqref{eq:LSigmatau}, respectively, where the notation $\tau(\lambda)$ highlights that $\tau$ depends on $\lambda$ via \eqref{eq:tau}. Assume that $\tr \Sigma = 2d$ and $\tr \Sigma_{xx} = d$.
%     Then, as long as
%     \begin{equation}
%     \frac{2d}{n} \leq \frac{\lambda_{\min}(\Sigma)}{4 } \min \left(1,  \frac{2\lambda_{\max}(\Sigma) / \sigma^2}{\left(\lambda_{\max}(\Sigma) / \lambda_{\min}(\Sigma)  + 1\right)^2} \right),
%     \end{equation}
%     $L^\Sigma(\tau(\lambda))$ is monotonically increasing for $\lambda\ge \bar\lambda$. 
% %    we have $\tau^* \leq \sqrt{\evmin{S^\Sigma_{x}}}$, where $\tau^* := \argmin_{\tau} \mathcal L^\Sigma(\lambda)$.
% Furthermore, if $\Sigma_{xx} = I$, then $\mathcal C^\Sigma(\tau(\lambda))$ is non-negative and monotonically increasing in %$\tau$ (and therefore in 
% $\lambda$ for $\lambda\le \tilde\lambda$, where $\tilde\lambda\ge \bar\lambda$. % all $\tau \leq \sqrt{\evmin{S^\Sigma_{x}}}$. 
% \end{proposition}
\begin{proposition}\label{prop:incr}
    Let $\mathcal C^\Sigma(\lambda)$ and $\mathcal L^\Sigma(\lambda)$ be defined as in \eqref{eq:CSigmatau} and \eqref{eq:LSigmatau}. % respectively. %, where the notation $\tau(\lambda)$ highlights that $\tau$ depends on $\lambda$ via \eqref{eq:tau}. % Assume that $\tr \Sigma = 2d$ and $\tr \Sigma_{xx} = d$.
    Then, if $2d < n$, we have that $\mathcal L^\Sigma (\lambda)$ is monotonically decreasing in a right neighborhood of $\lambda = 0$, and there exists $\lambda_{\mathcal L}>0$ such that $\mathcal L^\Sigma(\lambda)$ is monotonically increasing for $\lambda \ge \lambda_{\mathcal L}$. 
    Furthermore, if $\Sigma_{xx} = I$, then $\mathcal C^\Sigma(\lambda)$ is non-negative and there exists $\lambda_{\mathcal C}$ such that $\mathcal C^\Sigma(\lambda)$ is monotonically increasing for $\lambda \leq \lambda_{\mathcal C}$.
    Finally, as long as
    \begin{equation}\label{eq:shaperatiobound}
    \frac{2d}{n} \leq \frac{\lambda_{\min}(\Sigma)}{4 } \min \left(1,  \frac{2\lambda_{\max}(\Sigma) / \sigma^2}{\left(\lambda_{\max}(\Sigma) / \lambda_{\min}(\Sigma)  + 1\right)^2} \right),
    \end{equation}
    we have that $\lambda_{\mathcal C} \ge \lambda_{\mathcal L}$.
\end{proposition}


In words, Proposition \ref{prop:incr} shows that $\mathcal C^\Sigma(\lambda)$ grows with $\lambda$ at least until the regularization equals a value $\lambda_{\mathcal C}$. For example, in Figure \ref{fig:lambda}, $\lambda_{\mathcal C} \sim 1$ for a Gaussian data and $\lambda_{\mathcal C} \sim 10$ for Color-MNIST. Furthermore, in this interval, $\mathcal L^\Sigma(\lambda)$ is initially decreasing and then increasing as $\lambda \ge \lambda_{\mathcal L}$. These trends in turn imply that the optimal value $\lambda^*_{\mathcal L}$ that minimizes the test loss is s.t.\ $\lambda^*_{\mathcal L} \in (0, \lambda_{\mathcal C}]$ -- an interval where the spurious correlations are strictly positive and increasing. %, formalizing the trade-off between $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$ and $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$. 

The proof of Proposition \ref{prop:incr} (whose details are in Appendix \ref{app:lr}) relies on the monotonicity of $\tau(\lambda)$ in $\lambda$, and the last statement follows from showing that $\tau(\lambda_{\mathcal C}) \geq \evmin{S^\Sigma_{x}} \geq \evmin{\Sigma} \geq \tau(\lambda_{\mathcal L})$. The upper bound on $2d / n$ in \eqref{eq:shaperatiobound} is required to prove that $\evmin{\Sigma} \geq \tau(\lambda_{\mathcal L})$ and, due to Assumption \ref{ass:data}, it is implied by taking $n = \omega(d)$. We note that the latter scaling holds in standard %supervised learning
datasets, e.g., MNIST ($n = 6 \cdot 10^4$, $2d \approx 2 \cdot 10^3$ when considering the 3 color channels) and CIFAR-10 ($n = 5 \cdot 10^4$, $2d \approx 3 \cdot 10^3$).


\begin{wrapfigure}{r}{0.6\textwidth}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/simplicity_cifar_new.pdf}
  \end{center}
  \vspace{-0.1cm}
  \caption{Test loss $\mathcal L(\hat \theta_{\textup{LR}/\textup{RF}}(\lambda))$ (black) and spurious correlations $\mathcal C(\hat \theta_{\textup{LR}/\textup{RF}}(\lambda)$ (red) as a function of $\evmax{\Sigma_{yy}} / \tr(\Sigma_{yy})$ on a CIFAR-10 dataset for different levels of whitening (details on the whitening process in Appendix \ref{app:experiments}). We restrict to the classes ``boat'' and ``truck'' ($n = 10000$) and consider both linear regression and random features, with $\lambda = 1$.}
  \label{fig:simplicitycifar}
  \vspace{-0.1cm}
\end{wrapfigure}

We conclude the section by noting that learning spurious correlations can be beneficial to minimize the (in-distribution) test loss. In fact, the spurious features in $y$ are effectively correlated with the labels, due to their correlation with the core feature $x$, and hence they can be helpful at prediction time. This phenomenon is numerically supported by Figures \ref{fig:simplicitysynthetic} and \ref{fig:simplicitycifar}, where for a fixed value of $\lambda$, easier spurious features (or higher correlations) generate both higher values of $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$ and lower values of $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$. In words, while a blue background cannot strictly predict the label ``boat'', it is a useful feature in prediction as long as the boats in the test data tend to have a blue background. 

The same conclusion does not hold for the out-of-distribution test loss, where the features $x$ and $y$ are sampled independently: the lower bound in \eqref{eq:oodlossbody} increases with  $\mathcal C$. Figure \ref{fig:out} in Appendix \ref{app:ood} provides an additional  numerical validation of this statement.

% On the other hand, $\mathcal L^\Sigma(\tau(\lambda))$ can start increasing with $\lambda$ at earlier values, \emph{i.e.} for $\lambda \ge \lambda_{\mathcal L}$ with $\lambda_{\mathcal L} \le \lambda_{\mathcal C}$. As a consequence, the minimum of the loss is achieved for a regularization at which the amount of spurious correlations is still increasing, suggesting a trade-off between minimizing the two quantities $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$ and $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$. This phenomenon is pictorially represented in Figure \ref{fig:lambda}. \simone{add one more figure and mention intervals}

% in this paragraph, we provide a result on the positivity of $\mathcal C (\hat \theta_{\textup{LR}}(\lambda))$, and of its increasing trend in $\lambda$ in an interval containing the value of the regularization $\lambda^*$ that minimizes $\mathcal L (\hat \theta_{\textup{LR}}(\lambda))$, demonstrating a trade-off between the minimization of these two quantities.


% \simone{In general learning the correlations help to lower the test loss!}

% \begin{remark}
%     \simone{A result like the one above, cannot be proven in general for any p.s.d. $\Sigma_{xx}$. In particular, it can be shown that $\mathcal C^\Sigma(\lambda)$ is in general not positive. Consider for example the matrix... and see Figure...}
% \end{remark}



% \begin{figure*}[t]
%   \begin{center}
%     \includegraphics[width=0.98\textwidth]{figures/synthetic-mnist-rf_new.pdf}
%   \end{center}
%   \vspace{-0.5cm}
%   \caption{Test loss $\mathcal L(\hat \theta_{\textup{RF}}(\lambda))$ (black) and spurious correlations $\mathcal C(\hat \theta_{\textup{RF}}(\lambda)$ (red) for a RF model with ReLU and $\phi_1 = h_1 + 0.1 h_2$ activation as a function of the regularization term $\lambda$ and number of parameters $p$. \emph{First and second panel:} synthetic Gaussian dataset, with $d = 400$ and $n = 4000$ (details on the synthetic data generation are deferred to Appendix \ref{app:experiments}); \emph{right:} binary Color-MNIST dataset with correlation $\alpha \simeq 0.86$ between color and digit, and $n = 2000$. \simone{Put details and double check.}}
%   \vspace{-0.5cm}
%   \label{fig:rf}
% \end{figure*}





% \begin{figure}[t]
%   \begin{center}
%     \includegraphics[width=0.48\textwidth]{figures/synthetic_rf_tanh.pdf}
%   \end{center}
%   \vspace{-0.5cm}
%   \caption{Test loss $\mathcal L(\hat \theta_{\textup{RF}}(\lambda))$ (black) and spurious correlations $\mathcal C(\hat \theta_{\textup{RF}}(\lambda)$ (red) for a RF model with $\tanh$ and $\phi_1 = h_1 + 0.1 h_3$ activation as a function of the regularization term $\lambda$ and number of parameters $p$, trained on a synthetic Gaussian dataset (the details of the synthetic data generation are deferred to Appendix \ref{app:experiments}).}
%   \label{fig:rf}
%   \vspace{-0.5cm}
% \end{figure}




\section{Role of over-parameterization}\label{sec:rf}



\begin{wrapfigure}{r}{0.6\textwidth}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/overparam.pdf}
  \end{center}
  \vspace{-0.1cm}
  \caption{Test loss $\mathcal L(\hat \theta_{\textup{NN}/\textup{RF}}(\lambda))$ (black) and spurious correlations $\mathcal C(\hat \theta_{\textup{NN}/\textup{RF}}(\lambda)$ (red) as a function of $\lambda$. \emph{Left:} 2-layer fully connected ReLU network, trained on the binary color(C)-MNIST and CIFAR-10 (boats and trucks). \emph{Right:} RF model with $\tanh$ and $\phi_1 = h_1 + 0.1 \, h_3$ activation. Implementation details %of the implementations 
  are in Appendix \ref{app:experiments}.}
  \label{fig:nn}
  \vspace{-0.1cm}
\end{wrapfigure}


Our analysis has so far focused on %the setting of 
linear regression, highlighting the role of data covariance and regularization. However, moving to complex predictive models, such as neural networks, may lead to differences in the degree to which spurious correlations are learned. As an example, in the left panel of Figure \ref{fig:nn}, we %run a simple experiment 
train an over-parameterized two-layer neural network on the binary Color-MNIST and CIFAR-10 datasets, % (see Appendix \ref{app:experiments} for details), 
for different values of the regularizer $\lambda$.
%While the increasing trend between %spurious correlations
% $\mathcal C(\cdot)$ and $\lambda$ predicted by Proposition \ref{prop:incr} is still evident,
While for high values of $\lambda$ the results are qualitatively similar to the ones in Figure \ref{fig:lambda}, a striking difference %with respect to the previous analysis 
is that spurious correlations remain significant even when there is little to no regularization (i.e., $\lambda \approx 0$), in sharp contrast with Proposition \ref{prop:lambda0}. We also note that the phenomenon is in line with previous empirical work \cite{sagawa2020a}. %, the phenomenon is 

%Furthermore \simone{maybe something goes here...}
% In particular, Proposition \ref{prop:lambda0} remarks how setting $\lambda = 0$ in linear regression stops the model to learn any spurious correlation. Beyond the linear regression model, it is not straightforward to generalize this result. In particular, in \simone{Figure} we numerically investigate the relation between $\mathcal C$ and $\lambda$ in the setting of a 2-layer neural network, trained both on synthetic and the Color-MNIST data. While the increasing trend between $\mathcal C$ and $\lambda$ predicted by Proposition \ref{prop:incr} is still apparent, a major difference with respect to Figure \ref{fig:lambda} is that $\mathcal C$ is significantly positive for $\lambda = 0$.
%This experimental evidence highlights the role that using over-parameterized models have on the learning, problem that was object of investigation also in \simone{CITE}, and that is hard to tackle via the linear regression setting of Section \ref{sec:lr}, as the number of parameters in the model is by definition tied to the input dimension $2d$. 

%In this section, w
We bridge the gap between linear regression %(where the number of parameters is by definition tied to the input dimension $2d$) 
and over-parameterized models %(where the number of parameters may exceed input dimension and number of samples). To do so, 
by focusing on \emph{random features}: %. In formulas, %$ which takes the form
\begin{equation}\label{eq:rf}
    f_{\textup{RF}}(z, \theta) = \phi(V z)^\top \theta,
\end{equation}
where $V$ is a $p \times 2d$ matrix s.t.\ $V_{i,j} \distas{}_{\rm i.i.d.}\mathcal{N}(0, 1 / (2d))$, and $\phi$ is an activation %function
applied component-wise. The number of parameters of this model is $p$, as $V$ is a fixed random matrix and $\theta\in \mathbb R^p$ contains trainable parameters. The scaling of input data ($\tr(\Sigma) = 2d$) and the variance of the entries of $V$ guarantee that the pre-activations of the model (\emph{i.e.}, the entries of the vector $Vz \in \R^p$) are of constant order. %The RF model can be regarded as a two-layer neural network where the hidden-layer is fixed at initialization, and it offers a well-established setup to investigate over-parameterization as its number of parameters $p$ scales independently of $d$ and $n$. %Then, w

We consider the ERM in \eqref{eq:hattheta} with a quadratic loss
\begin{equation}\label{eq:argminrf}
    \hat \theta_{\textup{RF}}(\lambda) =\arg \min_\theta \left( \frac{1}{n} \norm{\Phi \theta - G}_2^2 + \lambda \norm{\theta}_2^2 \right),
\end{equation}
where we set $\Phi := [\phi(V z_1), \ldots, \phi(V z_n)]^\top \in \R^{n \times p}$. When $\lambda = 0$, if $\Phi \Phi^\top$ is invertible, the minimization above does not necessarily have a unique solution. In that case, we set $\hat \theta_{\textup{RF}}(0)$ to be the solution obtained via gradient descent with 0 initialization, which corresponds to the min-norm interpolator (see equation (33) in \cite{bartlett2021deep}). Then\footnote{$\Phi \Phi^\top$ is proved to be invertible with high probability in Lemma \ref{lemma:conckernel}.}, we can write, for $\lambda \geq 0$,
\begin{equation}\label{eq:hatthetarf}
    \hat \theta_{\textup{RF}}(\lambda) = \Phi^\top \left( \Phi \Phi^\top + n \lambda I \right)^{-1} G.
\end{equation}
% where the invertibility of $\Phi \Phi^\top$ is proven with high probability in Lemma \ref{lemma:conckernel}. 

% We will consider
\begin{assumption}[Activation function]\label{ass:activation}
    The activation $\phi: \R \to \R$ is a non-linear, odd, Lipschitz function, such that its first Hermite coefficient $\mu_1 \neq 0$.
\end{assumption}
This choice is motivated by theoretical convenience and is similar to the one considered in \cite{hu23universality}. We believe that our result can be extended to a more general setting, as the ones in \cite{mei2022generalization, mmm2022}, with a more involved analysis. We refer to \cite{booleananalysis} for background on Hermite coefficients. % of real functions.


\begin{assumption}[Over-parameterization]\label{ass:overparam}
We let $p$ grow s.t.\ $p = \omega \left(n \log^4 n \right)$ and $\log p = \Theta( \log n)$.
\end{assumption}
This requires the width of the model (and, hence, its number of parameters) to grow faster (by at least a poly-log factor) than the number of training samples.

Finally, %for the main result of this section to hold, 
our requirements on the data are less restrictive than those coming from Assumption \ref{ass:data}. 

\begin{assumption}[Data distribution, less restrictive]\label{ass:data2}
    %The input samples 
    $\{z_i\}_{i=1}^n$ are $n$ i.i.d.\ samples from a mean-0, Lipschitz concentrated distribution $\mathcal P_{XY}$, with covariance $\Sigma$ s.t.\ $\tr(\Sigma) = 2d$. Furthermore, %we have that 
    the labels $g_i$ are i.i.d. sub-Gaussian random variables.
\end{assumption}

Note %We note that % 
%we require less restrictive hypothesis on the data distribution than the ones in . In particular, 
that the labels $g_i$ are not required to follow a linear model $g_i = z_i^\top \theta^* + \epsilon_i$. % and we will simply consider
The Lipschitz concentration property (see Appendix \ref{app:notation} for details) corresponds to data having well-behaved tails, it includes the distributions considered in Assumption \ref{ass:data}, as well as the uniform distribution on the sphere or the hypercube \cite{vershynin2018high}, and it is a common requirement in the related literature \cite{tightbounds, bubeck2021a, bombari2022memorization}. 




% Then, we have the following
\begin{theorem}\label{thm:rf}
    Let Assumptions \ref{ass:activation}, \ref{ass:overparam}, and \ref{ass:data2} hold, $n = \Theta(d)$, and $z \in \R^{2d}$ be sampled from a distribution satisfying Assumption \ref{ass:data2}, not necessarily with the same covariance as $\mathcal P_{XY}$, independent from everything else. Let $f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda))$ be the RF model defined in \eqref{eq:rf} with $\hat \theta_{\textup{RF}}(\lambda)$ given by \eqref{eq:hatthetarf}, and $f_{\textup{LR}}(z, \hat \theta_{\textup{LR}}(\tilde \lambda))$ be the linear regression model defined in \eqref{eq:lrmodel} with $\hat \theta_{\textup{LR}}(\tilde \lambda)$ given by \eqref{eq:hatthetalambda}.
    Then, for $\lambda \geq 0$, %we have that
    \begin{equation}\label{eq:equivalence}
        \left| f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda)) - f_{\textup{LR}}(z, \hat \theta_{\textup{LR}}(\tilde \lambda))\right| = \bigO{\frac{d^{1/4} \log d}{p^{1/4}} + \frac{\log^{3/2} d}{d^{1/8}}} = o(1),
    \end{equation}
    with probability at least $1 - C \sqrt d \log^2 d / \sqrt p - C \log^3 d / d^{1/4}$, where the \emph{effective regularization} $\tilde \lambda$ is given by
    \begin{equation}\label{eq:tildelambda}
        \tilde \lambda = \frac{2{\tilde \mu}^2 d}{\mu_1^2 n} + \frac{2d}{\mu_1^2 p} \lambda,
    \end{equation}
    and ${\tilde \mu}^2 =  \sum_{k \geq 2} \mu_k^2$, with $\mu_k$ denoting the $k$-th Hermite coefficient of $\phi$.
\end{theorem}

In words, Theorem \ref{thm:rf} shows that the over-parameterized RF model, when evaluated on a new test sample (not necessarily from the same distribution as the input data), is asymptotically equivalent to linear regression with regularization $\tilde \lambda$ given by \eqref{eq:tildelambda}. In particular, even in the ridgeless case ($\lambda = 0$), the RF model is equivalent to linear regression with strictly positive regularization. Thus, we expect the presence of spurious correlations, just like in Figure \ref{fig:nn}, since $\mathcal C (\hat \theta_{\textup{RF}}(0))$ approaches $\mathcal C^\Sigma(\tilde \lambda)$ with $\tilde\lambda>0$ . Notably, the effective regularization $\tilde \lambda$ depends on the activation $\phi$ via its Hermite coefficients, and it increases with the ratio $\tilde \mu^2 / \mu_1^2$.

We point out some differences between Theorem \ref{thm:rf} and earlier work on the equivalence of random features with regularized linear regression \cite{goldt2020modeling,goldt2022gaussian,hu23universality,montanari2022universality}. First, as mentioned in Section \ref{sec:rel}, existing results prove equivalence of training and test loss, which does not imply either the equivalence of the covariance in \eqref{eq:spurcov} nor the point-wise guarantee of \eqref{eq:equivalence}. Second, existing results focus on the regime where $p$ and $n$ are proportional, while Assumption \ref{ass:overparam} requires $p = \omega \left(n \log^4 n \right)$. This reflects on the third difference which is in the proof technique: existing results use Lindeberg method, while our strategy (summarized below) relies on concentration tools. % and our approach relying on concentration as summarized below.

\paragraph{Proof sketch:} % According to \eqref{eq:hatthetarf} $f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda)) = \phi(Vz)^\top \Phi^\top \left( \Phi \Phi^\top + n \lambda I \right)^{-1} G$. Then,
The proof builds on 3 core steps.

\emph{Step 1:} We show that, with high probability,
$$\opnorm{\frac{\Phi \Phi^\top}{p} - \mu_1^2 \frac{ZZ^\top}{2d} - {\tilde \mu}^2 I} % = \bigO{\sqrt{\frac{d}{p}} + \frac{\log^3 d}{d}}
= o \left(\frac{\evmin{\Phi \Phi^\top}}{p}\right)
,$$
which is a consequence of the concentration of $\Phi \Phi^\top$ to its expectation with respect to $V$, % which is
then expressed in terms of the Hermite coefficients of $\phi$ (Lemmas \ref{lemma:long} and \ref{lemma:conckernel}).

\emph{Step 2:} In Lemma \ref{lemma:Eopnormsmall} we upper bound the term $\|\E_{z} [ \tilde \phi( V z) \tilde  \phi( V z)^\top ] \|_{\textup{op}}$ (where we set $\tilde \phi(\cdot) := \phi(\cdot) - \mu_1 (\cdot)$), which is then used to show that
$$\left| \phi(Vz)^\top \hat \theta_{\textup{RF}} - \mu_1 z^\top V^\top \hat \theta_{\textup{RF}} \right| = o(1),$$
with high probability, due to Markov inequality. This means that the non-linear component of $\phi(Vz)$ has a negligible effect on the output (see \eqref{eq:forproofsketch} in Lemma \ref{lemma:3terms}).

\emph{Step 3:} Using a similar intuition, we use matrix Bernstein inequality (Lemma \ref{lemma:matbernstein}) to show that $\|(\Phi - \mu_1 Z V^\top) V\|_{\textup{op}}$ is small with high probability, so that
$$\left| \mu_1 z^\top V^\top \left(\Phi^\top - \mu_1 V Z^\top\right)  \left( \Phi \Phi^\top + n \lambda I \right)^{-1} G \right| = o(1),$$
i.e., the non-linear component of $\Phi^\top$ in \eqref{eq:hatthetarf} is also negligible (see \eqref{eq:forproofsketch2}). 

Finally, by combining \emph{Step 2} and \emph{Step 3} with standard concentration arguments, we conclude that 
$$\left| \phi(Vz)^\top \hat \theta_{\textup{RF}} - \mu_1^2 p \frac{z^\top Z^\top}{2d} (\Phi \Phi^\top + n \lambda I)^{-1} G \right| = o(1),$$
and the thesis follows from \emph{Step 1} and Woodbury matrix identity for the inverse. \qed %  The full argument in Appendix \ref{app:rf}.

The right panel of Figure \ref{fig:nn} presents the test loss $\mathcal L (\hat \theta_{\textup{RF}}(\lambda))$ (in black) and the spurious correlations $\mathcal C (\hat \theta_{\textup{RF}}(\lambda))$ (in red) for two activation functions: $\tanh$ and $\phi_1 = h_1 + 0.1 h_3$, where $h_1$ and $h_3$ denote the first and third Hermite polynomials, respectively. Notice that this gives ${\tilde \mu}^2 / \mu_1^2 \sim 0.1$ for $\tanh$, ${\tilde \mu}^2 / \mu_1^2 \sim 0.01$ for $\phi_1$, and we take $d = 400$ and $n = 2000$. 
As expected, $\mathcal C (\hat \theta_{\textup{RF}}(0)) > 0$ for the $\tanh$ activation function, since $\tilde \lambda \sim 0.05$ (which matches the corresponding value in Figure \ref{fig:lambda}). On the other hand, $\mathcal C (\hat \theta_{\textup{RF}}(0)) \sim 0$ for the activation $\phi_1$, since $\tilde \lambda \sim 0.005$. As $\lambda$ grows, $\mathcal C (\hat \theta_{\textup{RF}}(\lambda))$ goes to 0 faster for the $\tanh$ activation function (which has higher $\tilde \lambda$), as predicted by the second upper bound in Proposition \ref{prop:boundsC}.
These results match the qualitative behavior of the 2-layer neural network trained on the Color-MNIST dataset (left panel).



\section{Conclusions}


Our work provides a rigorous study of spurious correlations in high-dimensional regression models, with the purpose of connecting the statistical foundation of this phenomenon with its practical intuition. Specifically, we translate the problem into characterizing the deterministic object $\mathcal C^\Sigma(\lambda)$, which allows us to quantitatively capture the roles of ridge regularization, data covariance and over-parameterization.

An interesting future direction is to employ our principled approach to design algorithms that go beyond ERM to mitigate spurious correlations. In that regard, let us mention the usage of multiple models
with different ridge regularization or early stopping (connected to ridge penalties, see \cite{raskutti2014early}), in order to generate additional supervision. Here, having a quantitative control on which features (core or spurious) are learned by which model would 
allow to optimally use the extra labels (e.g., for up-weighting minority groups, as in \cite{liu2021just}). 



\section*{Acknowledgements}

Marco Mondelli is funded by the European Union (ERC, INF$^2$, project number 101161364). Views and opinions expressed are however
those of the author(s) only and do not necessarily reflect those of the European Union or the
European Research Council Executive Agency. Neither the European Union nor the granting
authority can be held responsible for them.
% The authors were also supported by the 2019 Lopez-Loreta prize, and 
Simone Bombari is supported by a Google PhD fellowship. The authors would like to thank GuanWen Qiu for helpful discussions.



{
\small

\bibliographystyle{plain}
\bibliography{bibliography.bib}

}

\newpage



\appendix





\section{Additional notation}\label{app:notation}

We define a sub-Gaussian random variable according to Proposition 2.5.2 in \cite{vershynin2018high}, and $\subGnorm{X} := \inf \{ t>0 \; : \; \E \left[ \exp(X^2/t^2) \right] \leq 2 \}$. If $X \in \R^n$ is a random vector, then $\subGnorm{X} := \sup_{\norm{u}_2=1} \subGnorm{u^\top X}$. When we state that a random variable or vector $X$ is sub-Gaussian, % (or sub-exponential), 
we implicitly mean $\subGnorm{X} = \bigO{1}$, \emph{i.e.} its sub-Gaussian norm does not increase with the scalings of the problem.

We say that $X$ respects the Lipschitz concentration property if, for all 1-Lipschitz continuous functions $\varphi$, we have $\subGnorm{\varphi(X) - \E \left[ \varphi(X) \right]} = \bigO{1}$. Notice that then, if $X$ is Lipschitz concentrated, then $X - \E[X]$ is sub-Gaussian.

Given two symmetric matrices $A, B$, we use the notation $A \succeq B$ if $A - B$ is p.s.d. Notice that if $A \succeq B \succ 0$, then we also have $B^{-1} \succeq A^{-1}$. We denote with $\norm{A}_F$ the Frobenius norm of $A$, and with $\ker(A)$ its kernel space. If $A$ is a square matrix, we use the notation $\diag(A)$ to denote a matrix identical to $A$ on the diagonal, and 0 everywhere else. We let $A \circ B$ denote the Hadamard (component-wise) product between matrices, and $A^{\circ k}$ denote $A \circ A \circ ... \circ A$, where $A$ appears $k$ times.


\section{Proofs for linear regression}\label{app:lr}


\paragraph{Proof of Proposition \ref{prop:lambda0}.}

Note that 
\begin{equation}\label{eq:hatthetalr0}
   \hat \theta_{\textup{LR}}(0) = \left( Z^\top Z \right)^{-1} Z^\top G .
\end{equation}
Since we have $g_i = z_i^\top \theta^* + \epsilon_i$, \eqref{eq:hatthetalr0} reads
\begin{equation}\label{eq:easydecomp}
    \hat \theta_{\textup{LR}}(0) = \left( Z^\top Z \right)^{-1} Z^\top \left( Z \theta^* + \epsilon \right) = \theta^* + \left( Z^\top Z \right)^{-1} Z^\top \mathcal E.
\end{equation} 
Then, we can plug this result in the definition of $\mathcal C(\hat \theta)$ in \eqref{eq:spurcov} to obtain
\begin{equation}\label{eq:Clr0}
\begin{aligned}
    \E_{\mathcal E} \left[ \mathcal C(\hat \theta_{\textup{LR}}(0)) \right] &= \E_{\mathcal E} \left[ \Cov_{[x^\top, y^\top]^\top \sim P_{XY}, \, g = f^*_x(x), \, \tilde x \sim P_X} \left( f_{\textup{LR}} \left(\hat \theta_{\textup{LR}}(0),  [\tilde x^\top, y^\top]^\top \right), g \right) \right] \\
    &= \E_{\mathcal E} \left[ \Cov_{[x^\top, y^\top]^\top \sim P_{XY}, \, \tilde x \sim P_X} \left( [\tilde x^\top, y^\top] \hat \theta_{\textup{LR}}(0) , x^\top \theta^*_x \right) \right] \\
    &= \E_{\mathcal E} \left[ \Cov_{[x, y] \sim P_{XY}, \, \tilde x \sim P_X} \left( \tilde x^\top \theta^*_x + [\tilde x^\top, y^\top] \left( Z^\top Z \right)^{-1} Z^\top \mathcal E , x^\top \theta^*_x  \right) \right] \\
    &= \Cov_{[x, y] \sim P_{XY}, \, \tilde x \sim P_X} \left( \tilde x^\top \theta^*_x, x^\top \theta^*_x  \right) \\
    &= 0,
\end{aligned}
\end{equation}
where in the second line we used that $\mathcal E$ is independent from everything else, in fourth line we used $\E \left[ \mathcal E \right] = 0$, and that $\mathcal E$ is independent from all the other random variables, and the last step holds since $\tilde x$ is independent from $x$.

For the second part of the statement we have that
\begin{equation}\label{eq:Clr0new}
\begin{aligned}
    \mathcal C(\hat \theta_{\textup{LR}}(0)) &= \Cov_{[x^\top, y^\top]^\top \sim P_{XY}, \, \tilde x \sim P_X} \left( [\tilde x^\top, y^\top] \left( Z^\top Z \right)^{-1} Z^\top \mathcal E , x^\top \theta^*_x  \right)  \\
    &= \Cov_{[x^\top, y^\top]^\top \sim P_{XY}, \, \tilde x \sim P_X} \left(  \mathcal E^\top  Z \left( Z^\top Z \right)^{-1}  P_y [x^\top, y^\top]^\top, [x^\top, y^\top] \theta^* \right) \\
    &=  \mathcal E^\top  Z \left( Z^\top Z \right)^{-1} P_y \Sigma \theta^*,
\end{aligned}
\end{equation}
where in the second line we introduced $P_y \in \R^{2d \times 2d}$, defined as the projector on the last $d$ elements of the canonical basis in $\R^{2d}$. Then, since $\mathcal E$ is a sub-Gaussian vector (the entries are mean-0, i.i.d. sub-Gaussian) independent from everything else, we have that, with probability at least $1 - 2 \exp \left( -c_1 \log^2 d \right)$,
\begin{equation}\label{eq:nonasympt0}
\begin{aligned}
    \left| \mathcal C(\hat \theta_{\textup{LR}}(0)) \right| &\leq \log d \norm{Z \left( Z^\top Z \right)^{-1} P_y \Sigma \theta^*}_2 \\
    &\leq \log d \opnorm{Z \left( Z^\top Z \right)^{-1}} \opnorm{P_y} \opnorm{\Sigma} \norm{\theta^*}_2 \\
    &\leq \frac{\log d \opnorm{\Sigma}}{\sqrt{\evmin{Z^\top Z}}},
\end{aligned}
\end{equation}
where we used $\opnorm{P_y} = 1$ and $\norm{\theta^*}_2 = 1$. Since $Z$ is a $n \times 2d$ matrix with independent rows having second moment $\Sigma$, by Theorem 5.39 in \cite{vershrandmat} (see Remark 5.40), we have that
\begin{equation}
    \opnorm{\frac{Z^\top Z}{n} - \Sigma} = \bigO{\sqrt{\frac{d}{n}}} = o(1),
\end{equation}
with probability at least $1 - 2 \exp \left( -c_2 d \right)$. Hence, with this probability, by Weyl's inequality, we also have
\begin{equation}
    \evmin{Z^\top Z} \geq n \evmin{\Sigma} - \opnorm{Z^\top Z - n \Sigma} = \Theta(n),
\end{equation}
where the last step holds because of Assumption \ref{ass:data}. Thus, we have that \eqref{eq:nonasympt0} reads
\begin{equation}
    \left| \mathcal C(\hat \theta_{\textup{LR}}(0)) \right| \leq \frac{\log d \opnorm{\Sigma}}{\sqrt{\evmin{Z^\top Z}}} = \bigO{\frac{\log d}{\sqrt n}},
\end{equation}
with probability at least $1 - 2 \exp \left( -c_3 \log^2 d \right)$ over $Z$ and $\mathcal E$, which gives the desired result.
\qed












\paragraph{Proof of Theorem \ref{thm:C}.} As in \cite{han2023distribution}, we define the Gaussian sequence model $\hat \theta^{\rho} \in \R^{2d}$ as 
\begin{equation}\label{eq:Gaussseq}
    \hat \theta^{\rho} = \left( \Sigma + \tau (\lambda) I \right)^{-1} \Sigma^{1/2} \left( \Sigma^{1/2} \theta^* + \frac{\gamma \rho}{\sqrt {2d}}\right),
\end{equation}
where $\rho$ is a standard Gaussian vector in $\R^{2d}$. In the equation above, $\gamma > 0$ is implicitly defined via
\begin{equation}
% \begin{aligned}
    \frac{n \gamma^2}{2d} = \sigma^2 + \E_\rho \left[ \norm{\Sigma^{1/2} \left( \hat \theta^{\rho} - \theta^*  \right)}_2^2 \right] % \\
    % &= \sigma^2 + \norm{\Sigma^{1/2} \left( \left( \Sigma + \tau I\right)^{-1} \Sigma - I \right) \theta^* }_2^2 + \frac{\gamma^2}{2d} \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right),
% \end{aligned}
\end{equation}
% which reads
% \begin{equation}\label{eq:gamma}
%     \gamma^2 = \frac{d}{n} \frac{\sigma^2 + \tau^2 \norm{ \left( \Sigma + \tau I_d\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2}{1 - \frac{ \tr \left( \left( \Sigma + \tau I_d\right)^{-2} \Sigma^2 \right)}{n}},
% \end{equation}
% where $\tau$ is defined via \eqref{eq:tau}.
% As we are now considering $\lambda > 0$, Proposition 2.1 in \cite{han2023distribution} guarantees that the fixed point equations above admit a unique solution $(\gamma, \tau)$. Furthermore, we have that $\tau$ is a strictly increasing function of $\lambda$.
On the other hand, following a similar argument as the one in \eqref{eq:Clr0}, we have that, for every $\theta \in \R^{2d}$,
\begin{equation}\label{eq:foralltheta}
\begin{aligned}
    \mathcal C( \theta) &= \Cov_{[x^\top, y^\top]^\top \sim P_{XY}, \, \tilde x \sim P_X} \left( [\tilde x^\top, y^\top] \theta , x^\top \theta^*_x  \right) \\
    &= \theta^\top \E_{[x^\top, y^\top]^\top \sim P_{XY}, \, \tilde x \sim P_X} \left[ [\tilde x^\top, y^\top]^\top x^\top \right] \theta^*_x \\
    &= \theta^\top \E_{[x^\top, y^\top]^\top \sim P_{XY}} \left[ [\mathbf 0^\top, y^\top]^\top [x^\top, \mathbf 0^\top]^\top \right] \theta^* \\
    &= \theta^\top P_y \E_{[x^\top, y^\top]^\top \sim P_{XY}} \left[ [x^\top, y^\top]^\top [x^\top, y^\top]^\top \right] \theta^* \\
    &= \theta^\top P_y \Sigma \theta^*,
\end{aligned}
\end{equation}
where the third line holds since $\tilde x$ has 0 mean and is independent with $x$ and $y$, and by definition of $\theta^*_x$, and the fourth line holds because $P_y [x^\top, y^\top]^\top = [\mathbf 0^\top, y^\top]^\top$ and because the last $d$ entries of $\theta^*$ are 0 (\emph{i.e.}, $P_y \theta^* = 0$). Thus, since we have that $\norm{P_y \Sigma \theta^*}_2 \leq \opnorm{P_y} \opnorm{\Sigma} \norm{\theta^*}_2 \leq \opnorm{\Sigma}$ because of Assumption \ref{ass:data}, we have that $\mathcal C(\cdot) : \R^{2d} \to \R$ is a $\opnorm{\Sigma}$-Lipschitz function.

Now, % due to Assumption \ref{ass:theta^*}, 
since $\mathcal P_{XY}$ is multivariate Gaussian, Theorem 2.3 of \cite{han2023distribution} gives that, for any 1-Lipschitz function $\varphi: \R^{2d} \to \R$, and any $t \in (0, 1/2)$,
\begin{equation}\label{eq:thmhan}
    \P_{Z, G} \left( \left| \varphi(\hat \theta_{\textup{LR}}(\lambda)) - \E_\rho \left[ \varphi \left( \hat \theta^{\rho} \right) \right]  \right| \geq t \right)  \leq C_1 d \exp \left( -d t^4 / C_1 \right),
\end{equation}
where $C_1$ is a constant depending on $\evmin{\Sigma}$, $\opnorm{\Sigma}$, $\sigma^2$, and $n/d = \Theta(1)$. Since $\mathcal C(\cdot)$ is linear, notice that we have
\begin{equation}\label{eq:ErhoC}
    \E_\rho \left[ \mathcal C \left( \hat \theta^{\rho}  \right) \right] =  \mathcal C \left( \E_\rho \left[ \hat \theta^{\rho}  \right] \right) = \mathcal C \left( \left( \Sigma + \tau(\lambda) I \right)^{-1} \Sigma \theta^* \right) = {\theta^*}^\top \Sigma \left( \Sigma + \tau(\lambda) I \right)^{-1} P_y \Sigma \theta^* = \mathcal C^\Sigma(\lambda),
\end{equation}
where we used \eqref{eq:foralltheta} in the third step, and the definition of $\mathcal C^\Sigma(\lambda)$  in \eqref{eq:CSigmatau} in the last one. Thus, setting $\varphi(\cdot)$ to be $\mathcal C(\cdot) / \opnorm{\Sigma}$, and plugging \eqref{eq:ErhoC} in \eqref{eq:thmhan} we obtain
\begin{equation}
    \P_{Z, G} \left( \frac{\left| \mathcal C(\hat \theta_{\textup{LR}}(\lambda)) - \mathcal C^\Sigma(\lambda) \right|}{\opnorm{\Sigma}} \geq t \right)  \leq C_1 d \exp \left( -d t^4 / C_1 \right),
\end{equation}
which gives the thesis after absorbing the constant $\opnorm{\Sigma}$ in $t$, and noticing that the bound is still true for $t \in (0, 1/2)$ since $\opnorm{\Sigma} \geq \tr (\Sigma) / 2d = 1$ by Assumption \ref{ass:data}.
\qed









\begin{proposition}\label{prop:Cwithblocks}
    Let $\mathcal C^\Sigma(\lambda)$ be defined in \eqref{eq:CSigmatau}, and let $S_x^{\Sigma + \tau(\lambda) I}$ be the Schur complement of $\Sigma + \tau(\lambda) I$ with respect to the top-left $d \times d$ block. Then, we have that
    \begin{equation}\label{eq:statement1bound}
    \begin{aligned}
        \mathcal C^\Sigma(\lambda) = \tau(\lambda) \, {\theta^*_x}^\top  \left( \Sigma_{xx} + \tau(\lambda) I \right)^{-1} \Sigma_{xy} \left( S_x^{\Sigma + \tau(\lambda) I} \right)^{-1} \Sigma_{yx} \theta^*_x.
        % &= \tau \, \left( {\theta^*_x}^\top  \left( I - \left( \Sigma_{xx} + \tau I \right) ^{-1} \Sigma_{xy} \left(\Sigma_{yy} + \tau I\right)^{-1} \Sigma_{yx}\right)^{-1} {\theta^*_x}  - 1 \right) \\
        % &= \tau \, \left( {\theta^*_x}^\top \left( S_y^{\Sigma + \tau I} \right)^{-1} \Sigma_{xx} {\theta^*_x} - 1 \right)
    \end{aligned}
    \end{equation}
\end{proposition}
% In words, Proposition \ref{prop:boundsC} expresses $\mathcal C^\Sigma(\lambda)$ in terms of $\lambda$, the blocks defined in \eqref{eq:Sigmablocks}, and the Schur complement $S^{\Sigma + \tau(\lambda) I}_x$ via \eqref{eq:statement1bound}. 
\begin{proof}
% \paragraph{Proof of Proposition \ref{prop:Cwithblocks}.}
During the proof, to ease the notation, we will often leave implicit the dependence of $\tau$ on $\lambda$. Then, we can write
\begin{equation}\label{eq:beforeblocking}
\begin{aligned}
    \mathcal C^\Sigma(\lambda) &= {\theta^*}^\top \Sigma \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^* \\
    &= {\theta^*}^\top (\Sigma + \tau I - \tau I) \left( \Sigma + \tau I \right)^{-1} P_y \Sigma  \theta^* \\
    &= - \tau {\theta^*}^\top \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^* + {\theta^*}^\top P_y \Sigma \theta^* \\
    &= - \tau {\theta^*}^\top \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^*,
\end{aligned}
\end{equation}
where the last step holds since $P_y \theta^* = 0$. This expression can be further manipulated using the notation introduced in \eqref{eq:Sigmablocks}. We also introduce the following notation
\begin{equation}\label{eq:Sigmablocksinverse}
  \left( \Sigma + \tau I\right)^{-1} = \left(\begin{array}{@{}c|c@{}}
  \left[\left( \Sigma + \tau I\right)^{-1}\right]_{xx}
  & \left[\left( \Sigma + \tau I\right)^{-1}\right]_{xy} \\
\hline
  \left[\left( \Sigma + \tau I\right)^{-1}\right]_{yx} &
  \left[\left( \Sigma + \tau I\right)^{-1}\right]_{yy},
\end{array}\right)
\end{equation}
where we divided $\left( \Sigma + \tau I\right)^{-1}$ in four $d \times d$ blocks. Notice that, the expression in \eqref{eq:beforeblocking} only depends on $\left[\left( \Sigma + \tau I\right)^{-1}\right]_{xy}$, \emph{i.e.},
\begin{equation}\label{eq:stat1int}
\begin{aligned}
    \mathcal C^\Sigma(\lambda) %&= - \tau {\theta^*}^\top \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^* \\
    &= - \tau {\theta^*}^\top P_x \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^* %\\
%    &= - \tau {\theta^*}^\top \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^* 
= 
- \tau {\theta^*_x}^\top \left[\left( \Sigma + \tau I\right)^{-1}\right]_{xy} \Sigma_{yx} \theta_x^*,
\end{aligned}
\end{equation}
where we denoted the projector on the first $d$ elements of the canonical basis of $\R^{2d}$ as $P_x \in \R^{2d \times 2d}$. Exploiting the Schur complement $S_x^{\Sigma + \tau I}$, it holds that
\begin{equation}
\begin{aligned}
    \left[\left( \Sigma + \tau I\right)^{-1}\right]_{xy} = - \left( \Sigma_{xx} + \tau I \right)^{-1} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1}, %\\
    % &= - \left( \Sigma_{xx} + \tau I \right)^{-1} \Sigma_{xy} \left( \Sigma_y + \tau I - \Sigma_{yx} \left( \Sigma_x + \tau I \right)^{-1} \Sigma_{xy} \right)^{-1},
\end{aligned}
\end{equation}
which combined with \eqref{eq:stat1int} %ultimately yields to
%\begin{equation}\label{eq:statement1bound}
%\begin{aligned}
%    {\theta^*}^\top \Sigma \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^* % &= \tau \,{\theta^*_x}^\top  \left( \Sigma_x + \tau I \right)^{-1} \Sigma_{xy} \left( \Sigma_y + \tau I - \Sigma_{yx} \left( \Sigma_x + \tau I \right)^{-1} \Sigma_{xy} \right)^{-1}   \Sigma_{yx} \theta^*_x \\
%    = \tau \, {\theta^*_x}^\top  \left( \Sigma_{xx} + \tau I \right)^{-1} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \theta^*_x,
%\end{aligned}
%\end{equation}
%thus 
proves \eqref{eq:statement1bound}. %ing the first part of the statement.
%\qed    
\end{proof}



\paragraph{Proof of Proposition \ref{prop:boundsC}.}
During the proof, to ease the notation, we will often leave implicit the dependence of $\tau$ on $\lambda$. Then, according to \eqref{eq:CSigmatau}, we have that
\begin{equation}\label{eq:easybound1}
    \left| \mathcal C^\Sigma(\lambda) \right| = \left| {\theta^*}^\top \Sigma \left( \Sigma + \tau I \right)^{-1} P_y \Sigma P_x \theta^* \right| \leq \norm{\theta^*}_2^2 \opnorm{ \Sigma \left( \Sigma + \tau I \right)^{-1}} \opnorm{P_y \Sigma P_x} \leq \opnorm{\Sigma_{yx}},
\end{equation}
and
\begin{equation}\label{eq:easybound2}
    \left| \mathcal C^\Sigma(\lambda) \right| = \left| {\theta^*}^\top \Sigma \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^* \right| \leq \norm{\theta^*}_2^2 \opnorm{\Sigma}^2 \frac{1}{\evmin{\Sigma} + \tau} \leq \frac{\evmax{\Sigma}^2}{\tau}.
\end{equation}

Then, using \eqref{eq:statement1bound}, we get
% Notice that, up to a first order in $\tau$, we have
% \begin{equation}
% \begin{aligned}
%     \mathcal C^\Sigma(\lambda) &= \tau \, {\theta^*_x}^\top  \left(\Sigma_{xx} + \tau I \right)^{-1} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \theta^*_x \\
%     & = \tau \, {\theta^*_x}^\top  \left(\Sigma_{xx} + \tau I \right)^{-1/2} \left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \left(\Sigma_{xx} + \tau I \right)^{-1/2} \left(\Sigma_{xx} + \tau I \right)^{1/2} \theta^*_x \\
%     &\leq \tau \norm{\theta^*}_2^2 \opnorm{\left(\Sigma_{xx} + \tau I \right)^{-1/2} \left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \left(\Sigma_{xx} + \tau I \right)^{-1/2} \left(\Sigma_{xx} + \tau I \right)^{1/2}} \\
%     &= \tau \opnorm{\left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \left(\Sigma_{xx} + \tau I \right)^{-1/2}} \\
%     &\leq \tau \frac{\opnorm{\left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy}}^2}{\evmin{S_x^{\Sigma + \tau I}}} \\
%     &= \tau \frac{\evmax{\Sigma_{yx} \left(\Sigma_{xx} + \tau I\right)^{-1} \Sigma_{xy}}}{\evmin{\Sigma_{yy} + \tau I - \Sigma_{yx} \left(\Sigma_{xx} + \tau I\right)^{-1} \Sigma_{xy}}} \\
%     &\leq \tau \frac{\evmax{\Sigma_{yx} \Sigma_{xx} ^{-1} \Sigma_{xy}}}{\evmin{\Sigma_{yy} + \tau I - \Sigma_{yx} \Sigma_{xx} ^{-1} \Sigma_{xy}}} \\
%     &= \tau \frac{\evmax{\Sigma_{yy}} - \evmin{S_x^\Sigma}}{\evmin{S_x^\Sigma} + \tau},
% \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
    \mathcal C^\Sigma(\lambda) &= \tau \, {\theta^*_x}^\top  \left(\Sigma_{xx} + \tau I \right)^{-1} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \theta^*_x \\
    & = \tau \, {\theta^*_x}^\top  \left(\Sigma_{xx} + \tau I \right)^{-1/2} \left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \left(\Sigma_{xx} + \tau I \right)^{-1/2} \left(\Sigma_{xx} + \tau I \right)^{1/2} \theta^*_x \\
    &\leq \tau \norm{\left(\Sigma_{xx} + \tau I \right)^{-1/2} \theta_x^*}_2 \norm{\left(\Sigma_{xx} + \tau I \right)^{1/2} \theta^*_x}_2 \\
    & \qquad \opnorm{\left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \left(\Sigma_{xx} + \tau I \right)^{-1/2}} \\
    &\leq \tau \, \frac{1}{\sqrt{\evmin{\Sigma_{xx}} + \tau}} \, \sqrt{{\theta^*_x}^\top \Sigma_{xx} \theta^*_x + \tau} \\
    & \qquad \opnorm{\left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \left(\Sigma_{xx} + \tau I \right)^{-1/2}} \\
    &\leq \tau \frac{\sqrt{\E_{x \sim P_X} \left[ \left( x^\top \theta^*_x \right)^2 \right] + \tau}}{\sqrt{\evmin{\Sigma_{xx}} + \tau}}  \frac{\opnorm{\left(\Sigma_{xx} + \tau I \right)^{-1/2} \Sigma_{xy}}^2}{\evmin{S_x^{\Sigma + \tau I}}} \\
    &= \tau \frac{\sqrt{\E_{g = x^\top \theta^*_x + \epsilon} \left[ g ^2 \right] - \sigma^2 + \tau}}{\sqrt{\evmin{\Sigma_{xx}} + \tau}}  \frac{\evmax{\Sigma_{yx} \left(\Sigma_{xx} + \tau I\right)^{-1} \Sigma_{xy}}}{\evmin{\Sigma_{yy} + \tau I - \Sigma_{yx} \left(\Sigma_{xx} + \tau I\right)^{-1} \Sigma_{xy}}} \\
    &\leq \tau \frac{\sqrt{\E_{g} \left[ g ^2 \right] - \sigma^2 + \tau}}{\sqrt{\evmin{\Sigma_{xx}} + \tau}} \frac{\evmax{\Sigma_{yx} \Sigma_{xx} ^{-1} \Sigma_{xy}}}{\evmin{\Sigma_{yy} + \tau I - \Sigma_{yx} \Sigma_{xx} ^{-1} \Sigma_{xy}}} \\
    &= \tau \frac{\sqrt{\E_{g} \left[ g ^2 \right] - \sigma^2 + \tau}}{\sqrt{\evmin{\Sigma_{xx}} + \tau}} \frac{\evmax{\Sigma_{yy}} - \evmin{S_x^\Sigma}}{\evmin{S_x^\Sigma} + \tau} \\
    &\leq \tau \sqrt{\Var(g) - \sigma^2} \frac{\evmax{\Sigma_{yy}} - \evmin{S_x^\Sigma}}{\evmin{S_x^\Sigma} \sqrt{\evmin{\Sigma_{xx}}}},
\end{aligned}
\end{equation}
where in the fifth line we denoted with $\mathcal P_X$ the marginal distribution of the core feature $x$, and $\E_g \left[ \cdot \right]$ from the sixth line on denotes an expectation with respect to $g$ distributed as the labels of the model.
The last step simplifies the expression with respect to $\tau$, and it holds since $\Var(g) - \sigma^2 = {\theta^*_x}^\top \Sigma_{xx} \theta^*_x \geq \evmin{\Sigma_{xx}}$. 
This, together with \eqref{eq:easybound1} and \eqref{eq:easybound2} gives the desired result.
\qed





\paragraph{Proof of Proposition \ref{lemma:L}.}
During the proof, to ease the notation, we will often leave implicit the dependence of $\tau$ on $\lambda$. Then, as in \cite{han2023distribution} and in the proof of Theorem \ref{thm:C}, we define the Gaussian sequence model $\hat \theta^{\rho} \in \R^{2d}$ as \eqref{eq:Gaussseq}
where $\rho$ is a standard Gaussian vector in $\R^{2d}$ and %. In the equation above, 
$\gamma > 0$ is implicitly defined via
\begin{equation}
    \frac{n \gamma^2}{2d} = \sigma^2 + \E_\rho \left[ \norm{\Sigma^{1/2} \left( \hat \theta^{\rho} - \theta^*  \right)}_2^2 \right] = \sigma^2 + \norm{\Sigma^{1/2} \left( \left( \Sigma + \tau I\right)^{-1} \Sigma - I \right) \theta^* }_2^2 + \frac{\gamma^2}{2d} \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right),
\end{equation}
which also reads
\begin{equation}\label{eq:gamma}
    \frac{n \gamma^2}{2d} = \frac{\sigma^2 + \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2}{1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n}}.
\end{equation}
% On the other hand, for every $\theta \in \R^{2d}$, according to \eqref{eq:testloss} and \eqref{eq:lrmodel} with quadratic loss, we have that
% \begin{equation}
% \begin{aligned}
%     \mathcal L\left( \theta \right) &= \E_{z \sim \mathcal P_{XY}, \, g = z^\top \theta^* + \epsilon} \left[ \left( z^\top \theta - g \right)^2 \right] \\
%     &= \E_{z \sim \mathcal P_{XY}} \left[ \left( z^\top \left( \theta - \theta^* \right) \right)^2 \right] + \E \left[ \epsilon^2 \right] \\
%     &= \norm{\Sigma^{1/2} \left( \theta - \theta^*\right)}_2^2 + \sigma^2, \\
% \end{aligned}
% \end{equation}
% where the second step holds since $z$ is mean-0 and independent from $\epsilon$.
Then, due to % Assumption \ref{ass:theta^*},
Theorem 3.1 of \cite{han2023distribution} on the prediction risk, since $\mathcal P_{XY}$ is a multivariate Gaussian due to Assumption \ref{ass:data}, we have that, %for any 1-Lipschitz function $\varphi: \R^{2d} \to \R$, and 
for any $t \in (0, 1/2)$,
\begin{equation}\label{eq:thmhan2}
    \P_{Z, G} \left( \left| \mathcal L(\hat \theta_{\textup{LR}}(\lambda)) - \mathcal L^\Sigma(\lambda) \right| \geq t \right)  \leq C d \exp \left( -d t^4 / C \right),
\end{equation}
where $C$ is a positive constant depending on $\evmin{\Sigma}$, $\opnorm{\Sigma}$, $\sigma^2$, and $n/d = \Theta(1)$.
\qed




\paragraph{Proof of Proposition \ref{prop:incr}}

As $\tau(\lambda)$ is an increasing function of $\lambda$, all the statements on the monotonicity of $\mathcal L^\Sigma(\lambda)$ and $\mathcal C^\Sigma(\lambda)$ can be proved by showing monotonicity w.r.t.\ $\tau$ (whose dependence w.r.t.\ $\lambda$ is left implicit throughout the argument). In particular, we have
\begin{equation}\label{eq:4terms}
\begin{aligned}
    \frac{\diff \mathcal L^\Sigma(\lambda)}{\diff \tau} &= \frac{\frac{\diff}{\diff \tau} \left( \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2 \right) \left( 1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} \right)}{\left( 1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} \right)^2} \\
    & \qquad - \frac{\left( \sigma^2 + \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2 \right) \frac{\diff}{\diff \tau} \left(  1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} \right)}{\left( 1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} \right)^2}.
\end{aligned}
\end{equation}
To study the sign of the above expression, it suffices to focus on the numerators, as the denominator is always positive. 

Note that the RHS of \eqref{eq:tau} is smaller or equal to $2d / n$; thus, as $2d < n$, we also get $\tau \leq \lambda \left( 1 - 2d / n\right)^{-1}$, which implies $\tau(\lambda) \to 0$ as $\lambda \to 0$.
Hence, to show that $\mathcal L^\Sigma(\lambda)$ is monotonically decreasing in a right neighborhood of $\lambda=0$, it suffices to show that \eqref{eq:4terms} evaluated in $\tau = 0$ is strictly negative. For $\tau = 0$, the first factor in the numerator of the first term in \eqref{eq:4terms} is 0, as the following chain of equalities holds:
\begin{equation}\label{eq:term11newbef}
\begin{aligned}
    \frac{\diff}{\diff \tau} \left( \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2 \right) &= \frac{\diff}{\diff \tau} \left(  \norm{ \left( \Sigma / \tau + I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2 \right) \\
    &= {\theta^*}^\top \frac{\diff}{\diff \tau} \left( \Sigma / \tau + I\right)^{-2} \Sigma \theta^* \\
    &= - {\theta^*}^\top \left( \Sigma / \tau + I\right)^{-2} \left( \frac{\diff}{\diff \tau}  \left( \Sigma / \tau + I\right)^{2} \right) \left( \Sigma / \tau + I\right)^{-2} \Sigma \theta^* \\
    &= - {\theta^*}^\top \left( \Sigma / \tau + I\right)^{-2} \left( -\frac{2 \Sigma}{\tau^2}  \left( \Sigma / \tau + I\right) \right) \left( \Sigma / \tau + I\right)^{-2} \Sigma \theta^* \\
    &= 2 \tau {\theta^*}^\top \left( \Sigma + \tau I\right)^{-3} \Sigma ^2 \theta^*.
\end{aligned}
\end{equation}
Furthermore, the second term gives
\begin{equation}
    - \sigma^2 \frac{\diff}{\diff \tau} \left(  1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} \right) =  \frac{\sigma^2}{n} \frac{\diff}{\diff \tau} \left( \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)^2} \right) = - \frac{2 \sigma^2}{n} \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)^3} < 0,
\end{equation}
where $\lambda_k$ denotes the $k$-th eigenvalue of $\Sigma$. This gives the first claim.


To show that there exists $\lambda_{\mathcal L}>0$ such that $\mathcal L^\Sigma(\lambda)$ is monotonically increasing for $\lambda \ge \lambda_{\mathcal L}$ we will show that % there exists a value $\tau_{\mathcal L}$ such that
the derivative of $\mathcal L^\Sigma(\lambda)$ with respect to $\tau$ is positive for all $\tau \geq \tau_{\mathcal L} := \tau(\lambda_{\mathcal L})$.
%Thus, since $\tau(\lambda)$ is a monotonically increasing function (see the discussion after \eqref{eq:tau}), we have that there exists $\lambda_{\mathcal L}$ such that $L^\Sigma(\tau(\lambda))$ is monotonically increasing for $\lambda \ge \lambda_{\mathcal L}$. 
For simplicity, in the rest of the argument we use the notation %$\lambda_k$ to denote the $k$-th eigenvalue of $\Sigma$, and 
$\lambda_{\max}$ and $\lambda_{\min}$ to indicate the largest and smallest eigenvalues of $\Sigma$, respectively. Instead, the notation $\evmin{\cdot}$ still represents the smallest eigenvalue of its argument.
For the first factor of the first term of \eqref{eq:4terms}, continuing from \eqref{eq:term11newbef}, we have
\begin{equation}\label{eq:term11new}
\begin{aligned}
    \frac{\diff}{\diff \tau} \left( \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2 \right) 
    &\geq 2 \frac{1}{\evmax{\Sigma / \tau + I}^3} \evmin{ \Sigma / \tau}^2 = \frac{2 \lambda_{\min}^2}{ \tau^2 \left(\lambda_{\max} / \tau + 1 \right)^3}.
\end{aligned}
\end{equation}
For the second factor of the first term of \eqref{eq:4terms}, we have
\begin{equation}\label{eq:term12new}
    1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} = 1 - \frac{1}{n} \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)^2} \geq 1 - \frac{2d \lambda_{\max}^2}{n \tau^2}.
\end{equation}
For the first factor of the second term of \eqref{eq:4terms}, we have
\begin{equation}\label{eq:term21new}
    \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2  \leq \tau^2 \frac{\lambda_{\max}}{\left( \lambda_{\min} + \tau \right)^2}
\end{equation}
For the second factor of the second term of \eqref{eq:4terms}, we have
\begin{equation}\label{eq:term22new}
    \frac{\diff}{\diff \tau} \left(  1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} \right) = - \frac{1}{n} \frac{\diff}{\diff \tau} \left( \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)^2} \right) = \frac{2}{n} \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)^3} \leq \frac{4d \lambda_{\max}^2}{n \tau^3}.
\end{equation}


Thus, putting together \eqref{eq:4terms}, \eqref{eq:term11new}, \eqref{eq:term12new}, \eqref{eq:term21new}, and \eqref{eq:term22new}, the monotonicity of $\mathcal L^\Sigma(\lambda)$ is implied by %thesis in \eqref{eq:thesispropositionincreasing} is 
\begin{equation}\label{eq:thesis2propnew}
    \frac{2 \lambda_{\min}^2}{ \tau^2 \left(\lambda_{\max} / \tau + 1 \right)^3} \left(1 - \frac{2d \lambda_{\max}^2}{n \tau^2} \right) \stackrel{?}{\geq} \left( \sigma^2 +\tau^2 \frac{\lambda_{\max}}{\left( \lambda_{\min} + \tau \right)^2} \right) \frac{4d \lambda_{\max}^2}{n \tau^3}.
\end{equation}
Now, we have that the above inequality holds for sufficiently large $\tau$: the LHS is $\Theta(1/\tau^2)$ (considering fixed the other quantities), while the RHS is $\Theta(1/\tau^3)$; and the desired statement is therefore proved.



% To show that $\mathcal C^\Sigma(\lambda)$ is non-negative and there exists $\lambda_{\mathcal C}$ such that $\mathcal C^\Sigma(\lambda)$ is monotonically increasing for $\lambda \leq \lambda_{\mathcal C}$, 
Next, we set $\tau_{\mathcal C}:= \tau(\lambda_{\mathcal C}) = \sqrt{\evmin{S^\Sigma_{x}}}$ and show that $\mathcal C^\Sigma(\lambda)$ is monotonically increasing for $\tau \in [0, \tau_{\mathcal C}]$. Plugging $\Sigma_{xx} = I$ in \eqref{eq:statement1bound} we get
% To prove the monotonicity of $\mathcal C^\Sigma(\lambda)$, we plug $\Sigma_{xx} = I$ into the result of Proposition \ref{prop:boundsC}:
\begin{equation}\label{eq:simplid}
     \mathcal C^\Sigma(\lambda) = \tau \, {\theta^*_x}^\top  \left( \Sigma_x + \tau I \right)^{-1} \Sigma_{xy} \left( S_x^{\Sigma + \tau I} \right)^{-1} \Sigma_{yx} \theta^*_x = \frac{\tau}{1 + \tau} \, {\theta^*_x}^\top \Sigma_{xy} \left( \Sigma_{yy} + \tau I - \frac{\Sigma_{yx}\Sigma_{xy}}{1 + \tau} \right)^{-1} \Sigma_{yx} \theta^*_x.
\end{equation}
By the product rule, and introducing the shorthand $A(\tau) =  \Sigma_{yy} + \tau I - \frac{\Sigma_{xy}^\top \Sigma_{xy}}{1 + \tau}$, we have
\begin{equation}
\begin{aligned}
    \frac{\diff \mathcal \mathcal C^\Sigma(\lambda)}{\diff \tau} &= \left( \frac{\diff}{\diff \tau} \left( \frac{\tau}{1+ \tau} \right) \right) \left(  {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x \right) + \left( \frac{\tau}{1+ \tau} \right) \left( \frac{\diff}{\diff \tau}  \left( {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x \right) \right) \\
    &= \frac{1}{(1 + \tau)^2} \, {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x + \left( \frac{\tau}{1+ \tau} \right) \left( {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \left( - \frac{\diff}{\diff \tau} A(\tau) \right) A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x \right) \\
    &= \frac{1}{(1 + \tau)^2} {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x - \frac{\tau}{1+ \tau}  \left(  {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \left( I +  \frac{\Sigma_{xy}^\top \Sigma_{xy}}{(1 + \tau)^2} \right) A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x \right) \\
    &= \frac{1}{(1 + \tau)} {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \left( \frac{A(\tau)}{1+\tau}  - \tau \left( I +  \frac{\Sigma_{xy}^\top \Sigma_{xy}}{(1 + \tau)^2} \right) \right) A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x \\
    &= \frac{1}{(1 + \tau)} {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \left( \frac{\Sigma_{yy} + \tau I}{1+\tau} - \frac{\Sigma_{xy}^\top \Sigma_{xy}}{(1 + \tau)^2} - \tau I - \tau \frac{\Sigma_{xy}^\top \Sigma_{xy}}{(1 + \tau)^2} \right) A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x \\
    &= \frac{1}{(1 + \tau)^2} {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \left( \Sigma_{yy} - \Sigma_{xy}^\top \Sigma_{xy} - \tau^2 I \right) A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x \\
    &= \frac{1}{(1 + \tau)^2} {\theta^*_x}^\top \Sigma_{xy} A(\tau)^{-1} \left( S^\Sigma_{x} - \tau^2 I \right) A(\tau)^{-1} \Sigma_{xy}^\top \theta^*_x,
\end{aligned}
\end{equation}
where in the second line we used the identity $\frac{\diff}{\diff \tau}  \left( A(\tau)^{-1} \right) = A(\tau)^{-1} \left( - \frac{\diff}{\diff \tau} A(\tau) \right) A(\tau)^{-1}$. Then, if $\tau \leq \sqrt{\evmin{S^\Sigma_{x}}} = \tau_{\mathcal C}$, we have that $\left( S^\Sigma_{x} - \tau^2 I \right)$ is p.s.d., which in turn implies $\frac{\diff \mathcal \mathcal C^\Sigma(\lambda)}{\diff \tau} \geq 0$, thus giving the desired claim. The non-negativity of $\mathcal C^\Sigma(\lambda)$ readily follows from \eqref{eq:simplid}.


For the last statement, setting $\tau_{\mathcal L} = \evmin{\Sigma}$ we show that $\mathcal L^\Sigma(\lambda)$ is monotonically increasing for all $\tau \in [\tau_{\mathcal L}, + \infty)$ as long as the additional bound on $2d/n$ %\eqref{eq:shaperatiobound}
holds.
As $\evmin{S^\Sigma_{x}} \leq \evmin{\Sigma_{yy}} \leq \tr \left( \Sigma_{yy} \right) / d = \tr \left( \Sigma - \Sigma_{xx} \right) / d = 1$, we also have 
\begin{equation}\label{eq:thesispropositionincreasing}
    \tau_{\mathcal C} = \sqrt{\evmin{S^\Sigma_{x}}} \geq \evmin{S^\Sigma_{x}} \geq \evmin{\Sigma} = \tau_{\mathcal L},
\end{equation}
where the second inequality follows from Lemma \ref{lemma:schurevmin}. Thus, from the monotonicity of %$\mathcal L^\Sigma(\lambda)$ for $\tau\in [\evmin{\Sigma}, + \infty)$ and the monotonicity of 
$\tau(\lambda)$ in $\lambda$, the final result readily follows.

It remains to prove the monotonicity of $\mathcal L^\Sigma(\lambda)$ in $[\evmin{\Sigma}, + \infty)$. To do so, we again study the sign of \eqref{eq:4terms}.


For the first factor of the first term of \eqref{eq:4terms}, we have
\begin{equation}\label{eq:term11}
\begin{aligned}
    \frac{\diff}{\diff \tau} \left( \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2 \right) &= 2 {\theta^*}^\top \left( \Sigma / \tau + I\right)^{-3} \left( \Sigma / \tau \right)^2 \theta^* \\
    &= 2 {\theta^*}^\top \Sigma^{1/2} \left( \Sigma + \tau I\right)^{-1} \left( \Sigma + \tau I \right)^{-1} \tau \Sigma \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2} \theta^* \\
    &\geq \frac{2}{\tau} \evmin{\Sigma \left( \Sigma + \tau \right)^{-1}} \tau^2 \norm{\left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2} \theta^*}_2^2 \\
    &= \frac{2}{\tau} \frac{\lambda_{\min}}{\lambda_{\min} + \tau} \tau^2 \norm{\left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2} \theta^*}_2^2.
\end{aligned}
\end{equation}
For the second factor of the first term of \eqref{eq:4terms}, we have
\begin{equation}\label{eq:term12}
    1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} = 1 - \frac{1}{n} \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)^2} \geq 1 - \frac{2d}{n}.  %  \frac{2d \lambda_{\max}}{n \tau^2}.
\end{equation}

For the second factor of the second term of \eqref{eq:4terms}, we have
\begin{equation}\label{eq:term22}
    \frac{\diff}{\diff \tau} \left(  1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n} \right) = \frac{2}{n} \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)^3} \leq \frac{2}{n} \frac{1}{\tau \left(\lambda_{\min} + \tau\right)} \sum_{k = 1}^{2d} \frac{\lambda_k^2}{\left(\lambda_k + \tau \right)} \leq \frac{4d}{n \tau \left( \lambda_{\min} + \tau \right)}.
\end{equation}


Thus, putting together \eqref{eq:4terms}, \eqref{eq:term11}, \eqref{eq:term12}, and \eqref{eq:term22}, the monotonicity of $\mathcal L^\Sigma(\lambda)$ is implied by
\begin{equation}\label{eq:thesis2prop}
    \frac{2}{\tau} \frac{\lambda_{\min}}{\lambda_{\min} + \tau} \tau^2 \norm{\left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2} \theta^*}_2^2 \left( 1 - \frac{2d}{n} \right) \stackrel{?}{\geq} \left( \sigma^2 + \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2 \right) \frac{4d}{n \tau \left( \lambda_{\min} + \tau \right)}
\end{equation}

Since we assumed that $2d / n \leq \lambda_{\min}/4 \leq 1 / 4$, we have
\begin{equation}\label{eq:firstboundused}
    \frac{2}{\tau} \frac{\lambda_{\min}}{\lambda_{\min} + \tau}  \left( 1 - \frac{2d}{n} \right) -  \frac{4d}{n \tau \left( \lambda_{\min} + \tau \right)} = \frac{2}{\tau} \frac{\lambda_{\min}}{\lambda_{\min} + \tau}  \left( 1 - \frac{2d}{n} - \frac{2d}{n \lambda_{\min}}\right) \geq \frac{1}{ \tau} \frac{\lambda_{\min}}{\lambda_{\min} + \tau},
\end{equation}
and
\begin{equation}\label{eq:tausquarenorm}
\begin{aligned}
    \tau^2 \norm{\left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2} \theta^*}_2^2 &\geq \evmin{\tau^2 \Sigma \left( \Sigma + \tau I\right)^{-2}} \\
    &= \min_k \frac{\tau^2 \lambda_k}{\left(\lambda_k + \tau\right)^2} \\
    &= \min_k \frac{\lambda_k}{\left(\lambda_k / \tau + 1 \right)^2} \\
    &\geq \min_k \frac{\lambda_k}{\left(\lambda_k / \lambda_{\min} + 1 \right)^2} \\
    &= \lambda_{\min} \min_k \frac{\lambda_k / \lambda_{\min}}{\left(\lambda_k / \lambda_{\min}  + 1\right)^2} \\
    &= \frac{\lambda_{\max}}{\left(\lambda_{\max} / \lambda_{\min}  + 1\right)^2},
\end{aligned}
\end{equation}
where in the fourth line we used that $\tau \geq \lambda_{\min}$, % (see the discussion prior to \eqref{eq:thesispropositionincreasing}), 
and in the last step we used that $f(x) := x / (x + 1)^2$ is decreasing for $x \geq 1$.

Thus, using \eqref{eq:firstboundused} and \eqref{eq:tausquarenorm} gives that \eqref{eq:thesis2prop} is implied by
\begin{equation}
    \frac{\lambda_{\max}}{\left(\lambda_{\max} / \lambda_{\min}  + 1\right)^2} \frac{1}{ \tau} \frac{\lambda_{\min}}{\lambda_{\min} + \tau} \stackrel{?}{\geq} \sigma^2 \frac{4d}{n \tau \left( \lambda_{\min} + \tau \right)},
\end{equation}
which holds since we assumed
\begin{equation}
    \frac{2d}{n} \leq \frac{1}{2 \sigma^2} \frac{\lambda_{\max} \lambda_{\min}}{\left(\lambda_{\max} / \lambda_{\min}  + 1\right)^2}.
\end{equation}
%This concludes the proof.
\qed



\subsection{\texorpdfstring{Proofs on $S^\Sigma_x$}{Proofs on S Sigma x}}\label{app:schur}
% \subsection{Proofs on $S^\Sigma_x$}

For completeness, in this section we prove two known results about $S^\Sigma_x$. %, and in particular

\begin{lemma}\label{lemma:schurcovariance}
    Let $z = \left[x^\top, y^\top\right]^\top \sim \mathcal P_{XY}$ be distributed according to a mean-0, multivariate Gaussian distribution with covariance $\Sigma$, such that $\Sigma$ is invertible. Then, the Schur complement $S^\Sigma_x$ of $\Sigma$ with respect to the top left block $\Sigma_{xx}$ (see \eqref{eq:Sigmablocks}) corresponds to the conditional covariance of $y$ given $x$, \emph{i.e.},
\begin{equation}
    S_x^\Sigma = \Cov \left( y | x = \bar x \right) = \E_{y | x = \bar x} \left[ \left(y - \E_{y | x = \bar x} [y] \right) \left(y - \E_{y | x = \bar x} [y] \right)^\top \right].
\end{equation}
\end{lemma}
\begin{proof}
    Consider the expression $z^\top \Sigma^{-1} z$. According to the notation in \eqref{eq:Sigmablocks} and in \eqref{eq:Sigmablocksinverse}, we have
    \begin{equation}
        z^\top \Sigma^{-1} z = x^\top \left[ \Sigma^{-1} \right]_{xx} x + y^\top \left[ \Sigma^{-1} \right]_{yy} y + x^\top \left[ \Sigma^{-1} \right]_{xy} y + y^\top \left[ \Sigma^{-1} \right]_{yx} x.
    \end{equation}
    Then, the formulas for the inverse of a block matrix give
    \begin{equation}\label{eq:lemmaschur1}
    \begin{aligned}
         &z^\top \Sigma^{-1} z \\
         &= x^\top \left( \Sigma_{xx}^{-1} + \Sigma_{xx}^{-1} \Sigma_{xy} {S^\Sigma_x}^{-1} \Sigma_{yx} \Sigma_{xx}^{-1}  \right) x +  y^\top {S^\Sigma_x}^{-1} y \, + \\
         & \qquad + x^\top \left( - \Sigma_{xx}^{-1} \Sigma_{xy} {S^\Sigma_x}^{-1} \right) y +  y^\top \left( - {S^\Sigma_x}^{-1} \Sigma_{yx} \Sigma_{xx}^{-1} \right) x \\
         &= x^\top  \Sigma_{xx}^{-1} x +  \left(y - \Sigma_{yx} \Sigma_{xx}^{-1} x \right)^\top {S^\Sigma_x}^{-1} \left(y - \Sigma_{yx} \Sigma_{xx}^{-1} x \right).
    \end{aligned}
    \end{equation}
    Then, denoting with $p(x, y)$ and $p(x)$ the probability density functions of $z = \left[x^\top, y^\top\right]^\top$ and $x$ respectively, we get that the probability density function of $y$ conditioned on $x$ takes the form
    \begin{equation}
    \begin{aligned}
        p(y | x) &= \frac{p(x, y)}{p(x)} \\
        &= \frac{\sqrt{\left( 2 \pi \right)^{d} \det \left( \Sigma_{xx} \right) }}{\sqrt{\left( 2 \pi \right)^{2d} \det \left( \Sigma \right) }} \frac{\exp\left( - \left[x^\top, y^\top\right] \Sigma^{-1}\left[x^\top, y^\top\right]^\top / 2\right)}{\exp\left( - x^\top \Sigma_{xx}^{-1} x / 2\right)} \\
        &= \frac{1}{\sqrt{\left( 2 \pi \right)^{d} \det \left( S^\Sigma_x \right) }} \exp\left( - \left[x^\top, y^\top\right] \Sigma^{-1}\left[x^\top, y^\top\right]^\top / 2  + x^\top \Sigma_{xx}^{-1} x / 2\right) \\
        &= \frac{\exp\left( - \left(y - \Sigma_{yx} \Sigma_{xx}^{-1} x \right)^\top {S^\Sigma_x}^{-1} \left(y - \Sigma_{yx} \Sigma_{xx}^{-1} x \right) / 2 \right)}{\sqrt{\left( 2 \pi \right)^{d} \det \left( S^\Sigma_x \right) }},
    \end{aligned}
    \end{equation}
    where we used Schur formula for the determinants in the third line, and \eqref{eq:lemmaschur1} in the last step. Thus, we have that $p(y|x)$ describes the density of a multivariate Gaussian random variable, with covariance $S^\Sigma_x$.
\end{proof}


\begin{lemma}\label{lemma:schurevmin}
    Let $\Sigma \in \R^{2d \times 2d}$ be a p.s.d., invertible matrix. Then, the Schur complement $S^\Sigma_x \in \R^{d \times d}$ of $\Sigma$ with respect to the top left block $\Sigma_{xx}$ (see \eqref{eq:Sigmablocks}) is such that
    \begin{equation}
        \evmin{S^\Sigma_x} \geq \evmin{\Sigma}.
    \end{equation}
\end{lemma}
\begin{proof}
    Let $\Gamma \in \R^{2d \times d}$ be the rank-$d$ matrix defined as
    \begin{equation}
        \Gamma = \left(\begin{array}{@{}c}
            \Sigma_{xx}^{1/2}  \\
            \hline
            \Sigma_{yx} \Sigma_{xx}^{-1/2}
        \end{array}\right),
    \end{equation}
    and $S \in \R^{2d \times 2d}$ as the matrix containing $S^\Sigma_x$ in its bottom-right $d \times d$ block, and 0 everywhere else. Then, we have that
    \begin{equation}
        \Sigma = S + \Gamma \Gamma^\top,
    \end{equation}
    where both $S$ and $\Gamma \Gamma^\top$ are rank-$d$ p.s.d. matrices.
    
    Denoting by $\lambda_k(S)$ the $k$-th largest eigenvalue of $S$, by the Courant–Fischer–Weyl min-max principle, we can write
    \begin{equation}
        \lambda_k(S) = \max_{W, \, \dim \left( W \right) = k} \min_{u \in W, \, \norm{u}_2 = 1} \left(u^\top S u\right),
    \end{equation}
    where with $W$ we denote a generic $k$-dimensional subspace of $\R^{2d}$. Thus, the desired result follows from
    \begin{equation}
    \begin{aligned}
        \evmin{\Sigma} &= \evmin{S + \Gamma \Gamma^\top} \\
        &= \min_{\norm{u}_2 = 1} u^\top \left( S + \Gamma \Gamma^\top \right) u \\
        &\leq \min_{u \in \ker\left( \Gamma \Gamma^\top \right), \, \norm{u}_2 = 1} u^\top \left( S + \Gamma \Gamma^\top \right) u \\
        &=\min_{u \in \ker\left( \Gamma \Gamma^\top \right), \, \norm{u}_2 = 1} u^\top S u \\
        &\leq \max_{W, \, \dim \left( W \right) = d} \min_{u \in W, \, \norm{u}_2 = 1} u^\top S u \\
        &=\lambda_d(S) \\
        &=\evmin{S^\Sigma_x},
    \end{aligned}
    \end{equation}
    where the last step holds since the $d$ smallest eigenvalues of $S$ are equal to $0$, and the $d$ largest correspond to the ones of $S^\Sigma_x$.
\end{proof}






\subsection{Remarks on Assumption \ref{ass:data}}\label{app:datageneral}

Our results on linear regression rely on Assumption \ref{ass:data}, and in particular on the training samples to be normally distributed. This assumption is made for technical convenience, as the concentration results in Theorem \ref{thm:C} and Proposition \ref{lemma:L} still hold under the following milder requirement.


\begin{assumption}[Data distribution]\label{ass:datageneral}
    The input samples $\{z_i\}_{i=1}^n$ are $n$ i.i.d.\ samples from a mean-0, sub-Gaussian distribution $\mathcal P_{XY}$, such that
    \begin{enumerate}
    \item its covariance $\Sigma \in \R^{2d \times 2d}$ is invertible, with $\evmax{\Sigma} = \bigO{1}$, $\evmin{\Sigma} = \Omega(1)$, and $\tr(\Sigma) = 2d$;
    \item for $z \sim P_Z$, the random variable $\Sigma^{-1/2} z$ has independent, mean-0, unit variance, sub-Gaussian entries.
    \end{enumerate}
\end{assumption}

This assumption resembles the requirements A-B in Section 2.2 in \cite{han2023distribution}, where we also included the scaling of the trace. To formally state the equivalent of Theorem \ref{thm:C} and Proposition \ref{lemma:L}, one also has to enforce the following technical condition on the true parameter $\theta^*$.

\begin{assumption}\label{ass:theta^*}
    Let $\delta = 1 / 72$, then we assume that
    \begin{equation}
        \theta^* \textup{ s.t. } \norm{\Sigma^{1/2} \tau (\Sigma + \tau I)^{-1} \theta^*}_\infty \leq C d^{\delta - 1/2}.
    \end{equation}    
\end{assumption}
In Proposition 10.3 in \cite{han2023distribution}, it is shown that this condition excludes a negligible fraction ($C e^{-n^{2\delta}/C}$) of the $\theta^*$ on the unit ball. Since we set $\delta = 1 / 72$, following the same arguments of the proofs of Theorem \ref{thm:C} and Proposition \ref{lemma:L}, we have that Theorems 2.4 and 3.1 in \cite{han2023distribution} imply the results below.

\begin{theorem}\label{thm:Cdatageneral}
    Let Assumptions \ref{ass:data2} and \ref{ass:theta^*} hold, and let $n = \Theta(d)$. Let $\hat \theta_{\textup{LR}}(\lambda)$ be defined as in \eqref{eq:hatthetalambda}, and let $\mathcal C(\hat \theta_{\textup{LR}}(\lambda))$ be the amount of spurious correlations learned by the model $f_{\textup{LR}} \left(\hat \theta_{\textup{LR}}(\lambda) \right)$ as defined in \eqref{eq:spurcov}. Then, for any $\lambda > 0$, % denoting with $P_y \in \R^{2d \times 2d}$ the projector on the last $d$ elements of the canonical basis in $\R^{2d}$, and introducing the shorthand
    % \begin{equation}\label{eq:CSigmataugeneral}
    %     \mathcal C^\Sigma(\lambda) = {\theta^*}^\top \Sigma \left( \Sigma + \tau I \right)^{-1} P_y \Sigma \theta^*,
    % \end{equation}
    % where $\tau$ is defined according to $\eqref{eq:tau}$,
    we have that, for every $t \in (0, 1/2)$, % \marco{bad notationun} \simone{send $\tau$ in $\lambda$ all around the paper!}
    \begin{equation}
    \P_{Z, G} \left( \left| \mathcal C(\hat \theta_{\textup{LR}}(\lambda)) - \mathcal C^\Sigma(\lambda) \right| \geq t \right)  \leq C t^{-13} d^{-1/8},
    \end{equation}
    where $\mathcal C^\Sigma(\lambda)$ is defined in \eqref{eq:CSigmatau}, and $C$ is a an absolute constant. 
\end{theorem}
\begin{proposition}\label{lemma:Lgeneral}
    Let Assumptions \ref{ass:data2} and \ref{ass:theta^*} hold, and let $n = \Theta(d)$. Let $\hat \theta_{\textup{LR}}(\lambda)$ be defined as in \eqref{eq:hatthetalambda}, and let $\mathcal L(\hat \theta_{\textup{LR}}(\lambda))$ be the in-distribution test loss of the model $f_{\textup{LR}}(\hat \theta_{\textup{LR}}(\lambda) )$ as defined in \eqref{eq:lrmodel}. Then, , for any $\lambda > 0$,
    % introducing the shorthand
    % \begin{equation}
    %     \mathcal L^\Sigma(\lambda) := \frac{\sigma^2 + \tau^2 \norm{ \left( \Sigma + \tau I\right)^{-1} \Sigma^{1/2}  \theta^* }_2^2}{1 - \frac{ \tr \left( \left( \Sigma + \tau I\right)^{-2} \Sigma^2 \right)}{n}},
    % \end{equation}
    % where $\tau$ is defined according to $\eqref{eq:tau}$,
    we have that, for every $t \in (0, 1/2)$,
    \begin{equation}
    \P_{Z, G} \left( \left| \mathcal L(\hat \theta_{\textup{LR}}(\lambda)) -\mathcal L^\Sigma(\lambda) \right| \geq t \right)  \leq C t^{-c} d^{-1 / 6.5},
    \end{equation}
    where $\mathcal L^\Sigma(\lambda)$ is defined in \eqref{eq:LSigmatau}, and $C$ and $c$ are positive absolute constants.
\end{proposition}






\section{Proofs for Random Features}\label{app:rf}


\begin{lemma}\label{lemma:facts}
    We have that
    \begin{equation}
        \opnorm{V} = \bigO{\sqrt{\frac{p}{d}}},
    \end{equation}
        \begin{equation}
        \opnorm{Z} = \bigO{\sqrt{d}},
    \end{equation}
    with probability at least $1 - 2 \exp \left( -cd \right)$ over $V$ and $Z$, where $c$ is an absolute constant. Furthermore, for every $i \in [n]$, we have  
    \begin{equation}
        \subGnorm{\norm{z_i}_2 - \sqrt{2d}} = \bigO{1}.
    \end{equation}
\end{lemma}
\begin{proof}
    $V$ has independent, mean-0, unit variance, sub-Gaussian entries. Then, the first statement is a direct consequences of Theorem 4.4.5 of \cite{vershynin2018high} and of the scaling $d = o(p)$.

    By Assumption \ref{ass:data2}, we have that $Z$ has i.i.d. mean-0, Lipschitz concentrated rows. This property also implies that the rows are i.i.d.\ sub-Gaussian. Thus, by Remark 5.40 in \cite{vershrandmat}, we have that
    \begin{equation}
        \opnorm{Z^\top Z - n \Sigma} = \bigO{n \frac{d}{n}} = \bigO{d},
    \end{equation}
    with probability at least $1 - 2 \exp \left( -c_1 d \right)$. Then, conditioning on this high probability event, by Weyl's inequality, we have
    \begin{equation}\label{eq:evminXeq1}
        \opnorm{Z^\top Z} \leq \opnorm{n \Sigma} + \opnorm{Z^\top Z - n \Sigma} = \bigO{d},
    \end{equation}
    where the last step follows from the argument used to prove $\opnorm{\Sigma} = \bigO{1}$ in Lemma C.1 in \cite{bombari2024privacy}.

    % Assumption \ref{ass:data} allows us to write $z_i$ as
    % \begin{equation}
    %     z_i = \Sigma^{1/2} \xi,
    % \end{equation}
    % where $\xi \in \R^{2d}$ has independent, mean-0, unit variance, sub-Gaussian entries. Then, by Theorem 6.3.2 in \cite{vershynin2018high} we have that
    % \begin{equation}
    %     \subGnorm{\norm{z_i}_2 - \norm{\Sigma^{1/2}}_F} = \subGnorm{\norm{\Sigma^{1/2} \xi}_2 - \sqrt{\tr(\Sigma)}} = \bigO{1},
    % \end{equation}
    % where in the last step we used that $\opnorm{\Sigma^{1/2}} = \bigO{1}$, which follows again from Assumption \ref{ass:data}. Then, since for the same assumption we also have $\tr(\Sigma) = 2d$, the thesis readily follows.
    For the last statement, we have 
    \begin{equation}
        2d = \tr(\Sigma) = \tr\left(\E\left[ zz^\top \right]\right) = \E\left[ \tr \left( zz^\top \right) \right] = \E\left[ \tr \left( z^\top z \right) \right] = \E\left[ \norm{z}_2^2 \right],
    \end{equation}
    where we used the cyclic property of the trace. Furthermore, we have
    \begin{equation}\label{eq:concnormz}
        \subGnorm{\norm{z}_2 - \E\left[ \norm{z}_2 \right]} = \bigO{1},
    \end{equation}
    since $z$ is Lipschitz concentrated. Then,
    \begin{equation}
        0 \leq 2d - \E\left[\norm{z}_2\right]^2 = \E \left[ \left(\norm{z}_2 - \E\left[ \norm{z}_2 \right]\right)^2 \right] \leq C_1,
    \end{equation}
    for some absolute constant $C_1$. Thus, as $\sqrt{1 - x} \geq 1 - x$ for $x \in [0, 1]$, we obtain
    \begin{equation}
        1 - \frac{C_1}{2d} \leq \sqrt{1 - \frac{C_1}{2d}} \leq \frac{\E\left[\norm{z}_2\right]}{\sqrt{2d}} \leq 1.
    \end{equation}
    Plugging this last result in \eqref{eq:concnormz} %, and since $\subGnorm{C_1 / \sqrt d} = \bigO{1/\sqrt d}$, Triangle inequality 
    gives the desired claim.
\end{proof}








\begin{lemma}\label{lemma:long}
%Denoting with ${\tilde \mu}^2 =  \sum_{k \geq 3} \mu_k^2$, w
We have that, denoting with ${\tilde \mu}^2 =  \sum_{k \geq 2} \mu_k^2$, with $\mu_k$ denoting the $k$-th Hermite coefficient of $\phi$,
\begin{equation}
    \opnorm{\E_{V} \left[ \Phi \Phi^\top \right] -  p \left( \mu_1^2 \frac{ZZ^\top}{2d} + {\tilde \mu}^2 I \right)} = \bigO{\frac{p \log^3 d}{\sqrt d}},
\end{equation}
with probability at least $1 - 2 \exp \left( -c \log^2 d \right)$ over $Z$, where $c$ is an absolute constant.
\end{lemma}
\begin{proof}
For all $i \in [n]$, we define the functions $\phi^{(i)}: \R \to \R$ as $\phi^{(i)}(\cdot) = \phi(\norm{z_i}_2 \cdot / \sqrt{2d})$. Note that $\phi^{(i)}$ is odd, since $\phi$ is odd by Assumption \ref{ass:activation}. Thus, denoting with $\mu_k^{(i)}$ the $k$-th Hermite coefficient of $\phi^{(i)}$, for every $i \in [n]$, we have that $\mu_k^{(i)} = 0$ for all even $k$. This implies that, by denoting with $v$ a random vector distributed as the rows of $V$, \emph{i.e.}, $\sqrt{2d} \, v$ is a standard Gaussian vector, we have
\begin{equation}
\begin{aligned}
    \left[ \E_{V} \left[ \Phi \Phi^\top \right] \right]_{ij} &= p \, \E_{v} \left[ \phi(z_i^\top v) \phi(z_j^\top v) \right] \\
    &= p \, \E_{v} \left[ \phi^{(i)} \left( \frac{z_i^\top}{\norm{z_i}_2} \sqrt{2d} v \right) \phi^{(j)} \left( \frac{z_j^\top}{\norm{z_j}_2} \sqrt{2d} v \right)\right] \\
    &= p \sum_{k = 0}^{+ \infty} \mu_k^{(i)} \mu_k^{(j)} \left( \frac{z_i^\top z_j }{\norm{z_i}_2\norm{z_j}_2}\right)^k \\
    &= p \mu_1^{(i)} \mu_1^{(j)} \frac{z_i^\top z_j }{\norm{z_i}_2\norm{z_j}_2} + p \sum_{k \geq 3} \mu_k^{(i)} \mu_k^{(j)} \left( \frac{z_i^\top z_j }{\norm{z_i}_2\norm{z_j}_2}\right)^k.
\end{aligned}
\end{equation}
Then, denoting with $D_k \in \R^{n \times n}$ the diagonal matrix containing $\mu_k^{(i)} / \norm{z_i}_2^k$ in its $i$-th entry, we can write
\begin{equation}\label{eq:EwithDk}
    \E_{V} \left[ \Phi \Phi^\top \right] = p D_1 ZZ^\top D_1 + p \sum_{k \geq 3} D_k \left( ZZ^\top\right)^{\circ k} D_k.
\end{equation}


Notice that, due to the last statement in Lemma \ref{lemma:facts}, we have that, jointly for all $i \in [n]$,
\begin{equation}\label{eq:allnormsconcentrate}
    \left| \frac{\norm{z_i}_2}{\sqrt{2d}} - 1 \right| = \bigO{\frac{\log d}{\sqrt d}},
\end{equation}
with probability at least $1 - 2 \exp (-c_1 \log^2 d)$. Then, conditioning on such high probability event and denoting with $\rho$ a standard Gaussian random variable, for all $i \in [n]$ we have
\begin{equation}\label{eq:mu1conc}
\begin{aligned}
    \left| \mu_1^{(i)} - \mu_1 \right| &= \left| \E_\rho \left[ \rho \phi^{(i)} (\rho) \right] - \E_\rho \left[ \rho \phi(\rho) \right] \right| \\
    &= \left| \E_\rho \left[ \rho \left( \phi \left( \frac{\norm{z_i}_2}{\sqrt{2d}} \rho  \right) -  \phi(\rho) \right) \right] \right|\\
    &= \left| \E_\rho \left[ \rho \left( \phi \left( \left( \frac{\norm{z_i}_2}{\sqrt{2d}} - 1 \right) \rho + \rho \right) - \phi(\rho) \right) \right]  \right| \\
    &\leq \E_\rho \left[ \left| \rho \right| \left| \phi \left( \left( \frac{\norm{z_i}_2}{\sqrt{2d}} - 1 \right) \rho + \rho \right) - \phi(\rho) \right| \right] \\
    &\leq L \E_\rho \left[ \left| \rho \right| \left| \frac{\norm{z_i}_2}{\sqrt{2d}} - 1 \right| \left| \rho \right| \right] \\
    &= L \left| \frac{\norm{z_i}_2}{\sqrt{2d}} - 1 \right| \E_\rho \left[  \rho^2 \right] \\
    &= \bigO{\frac{\log d}{\sqrt d}},
\end{aligned}
\end{equation}
where we used Jensen's inequality in the fourth line, the $L$-Lipschitzness of $\phi$ in the fifth line, and \eqref{eq:allnormsconcentrate} in the last step. With a similar approach, denoting with $\norm{\cdot}_{L^2}$ the $L^2$ norm with respect to the Gaussian measure, we have that for all $i \in [n]$
\begin{equation}\label{eq:L2conc}
\begin{aligned}
    \left| \norm{\phi^{(i)}}_{L^2} - \norm{\phi}_{L^2} \right| &\leq \norm{\phi^{(i)} - \phi}_{L^2} \\
    &= \E_\rho \left[ \left(\phi^{(i)}(\rho) - \phi(\rho)\right)^2 \right]^{1/2} \\
    &= \E_\rho \left[ \left( \phi \left( \left( \frac{\norm{z_i}_2}{\sqrt{2d}} - 1 \right) \rho + \rho \right) - \phi(\rho)\right)^2 \right]^{1/2} \\
    &\leq L \left| \frac{\norm{z_i}_2}{\sqrt{2d}} - 1 \right| \E_\rho \left[  \rho^2 \right]^{1/2} \\
    &= \bigO{\frac{\log d}{\sqrt d}},
\end{aligned}
\end{equation}
which directly implies that, for all $i \in [n]$, $\norm{\phi^{(i)}}_{L^2} = \sum_{k \geq 0} \left(\mu_k^{(i)}\right)^2 = \Theta(1)$, and that
\begin{equation}\label{eq:mu3conc}
    \left| \sum_{k \geq 3}  \left(\mu_k^{(i)}\right)^2 -  \sum_{k \geq 3} \mu_k^2 \right| \leq \left| \norm{\phi^{(i)}}^2_{L^2} - \norm{\phi}^2_{L^2} \right| + \left| \left(\mu^{(i)}_1\right)^2  - \mu_1^2 \right| = \bigO{\frac{\log d}{\sqrt d}}.
\end{equation}



Thus, we are ready to estimate the operator norm of the off-diagonal part of the second term on the RHS of \eqref{eq:EwithDk}, specifically
\begin{equation}\label{eq:offdiag}
    \begin{aligned}
        &\opnorm{\sum_{k \geq 3} D_k \left( ZZ^\top\right)^{\circ k} D_k - \diag \left( \sum_{k \geq 3} D_k \left( ZZ^\top\right)^{\circ k} D_k \right)} \\
        &\qquad \leq \sum_{k \geq 3} \norm{ D_k \left( ZZ^\top\right)^{\circ k} D_k - \diag \left( D_k \left( ZZ^\top\right)^{\circ k} D_k \right)}_F \\
        &\qquad \leq \sum_{k \geq 3} \max_{i \neq j} \left( \frac{ \left| z_i^\top z_j \right| }{\norm{z_i}_2\norm{z_j}_2}\right)^k \left( \sum_{i \in [n], j \in [n]} \left(\mu_k^{(i)} \mu_k^{(j)}\right)^2 \right)^{1/2} \\
        &\qquad \leq  \max_{i \neq j} \left( \frac{ \left| z_i^\top z_j \right| }{\norm{z_i}_2\norm{z_j}_2}\right)^3  \sum_{i = 0}^n \sum_{k \geq 3} \left(\mu_k^{(i)}\right)^2 \\
        &\qquad= \bigO{ \frac{1}{d^{3/2}} \log^3 d \, n} = \bigO{ \frac{\log^3 d}{\sqrt d}},
    \end{aligned}
\end{equation}
where in the first step we replaced the operator norm with the Frobenius norm, and used triangle inequality; in the fifth line we used that $\norm{z_i}_2 = \Theta(\sqrt d)$ for all $i \in [n]$ (true because of \eqref{eq:allnormsconcentrate}), and that jointly for all $i \neq j$ we have $\left| z_i^\top z_j \right| / \norm{z_j}_2  = \bigO{\log d}$ with probability at least $1 - 2 \exp (-c_2 \log^2 d)$ since the $z_i$-s are independent sub-Gaussian vectors (since they are mean-0 and Lipschitz concentrated). The diagonal part of the second term on the RHS of \eqref{eq:EwithDk} respects
\begin{equation}\label{eq:diag}
    \opnorm{\diag \left( \sum_{k \geq 3} D_k \left( ZZ^\top\right)^{\circ k} D_k \right) - {\tilde \mu}^2 I} = \max_{i\in [n]} \left| \sum_{k \geq 3}  \left(\mu_k^{(i)}\right)^2 -  \sum_{k \geq 3} \mu_k^2 \right| = \bigO{\frac{\log d}{\sqrt d}}, 
\end{equation}
because of \eqref{eq:mu3conc}. Lastly, notice that,
\begin{equation}\label{eq:linear1}
\begin{aligned}
    \opnorm{D_1 ZZ^\top D_1 - \mu_1^2 \frac{ZZ^\top}{2d}} &= \sup_{\norm{u}_2 = 1} \left| u^\top D_1 ZZ^\top D_1 u - \mu_1^2 u^\top \frac{ZZ^\top}{2d} u \right| \\
    &= \sup_{\norm{u}_2 = 1} \left| \norm{Z^\top D_1 u}_2^2 - \mu_1^2 \norm{\frac{Z^\top}{\sqrt{2d}} u}_2^2 \right| \\
    &\leq  \sup_{\norm{u}_2 = 1}  \left(  \norm{Z^\top D_1 u}_2 + \mu_1 \norm{\frac{Z^\top}{\sqrt{2d}} u}_2 \right) \sup_{\norm{u}_2 = 1}  \left(  \norm{Z^\top D_1 u - \mu_1 \frac{Z^\top}{\sqrt{2d}} u}_2 \right) \\
    &\leq \left( \opnorm{Z^\top D_1} + \mu_1 \opnorm{\frac{Z^\top}{\sqrt{2d}}} \right) \opnorm{Z^\top D_1 - \mu_1 \frac{Z^\top}{\sqrt{2d}}} \\
    &\leq \left( \opnorm{Z} \opnorm{D_1} + \mu_1 \frac{\opnorm{Z}}{\sqrt{2d}} \right) \opnorm{Z} \opnorm{D_1 - \frac{\mu_1}{\sqrt{2d}}}.
\end{aligned}
\end{equation}
By Lemma \ref{lemma:facts}, we have that $\opnorm{Z} = \bigO{\sqrt d}$ with probability at least $1 - 2 \exp \left( -c_2 d \right)$, and since $\norm{z_i}_2 = \Theta(\sqrt d)$ and $\mu_1^{(i)} = \bigO{1}$ for all $i \in [n]$ (true because of \eqref{eq:allnormsconcentrate} and \eqref{eq:mu1conc} respectively), we have that $\opnorm{D_1} = \bigO{1 / \sqrt d}$. Furthermore, we have
\begin{equation}
\begin{aligned}
     \opnorm{D_1 - \frac{\mu_1}{\sqrt{2d}}} &= \max_i \left| \frac{\mu_1^{(i)}}{\norm{z_i}_2} - \frac{\mu_1}{\sqrt{2d}} \right| \\
     &\leq \max_i \frac{1}{\norm{z_i}_2} \left( \left| \mu_1^{(i)} - \mu_1 \right| + \mu_1 \left| 1 - \frac{\norm{z_i}_2}{\sqrt{2d}} \right|\right) \\
     &= \bigO{\frac{\log d}{d}},
\end{aligned}
\end{equation}
where the last step is a consequence of \eqref{eq:allnormsconcentrate} and \eqref{eq:mu1conc}. Then, we have that \eqref{eq:linear1} reads
\begin{equation}\label{eq:linear2}
    \opnorm{D_1 ZZ^\top D_1 - \mu_1^2 \frac{ZZ^\top}{2d}} = \bigO{\frac{\log d}{\sqrt{d}}}.
\end{equation}
A
standard application of the triangle inequality to \eqref{eq:offdiag}, \eqref{eq:diag} and \eqref{eq:linear2} gives
\begin{equation}
    \opnorm{\left(D_1 ZZ^\top D_1 + \sum_{k \geq 3} D_k \left( ZZ^\top\right)^{\circ k} D_k \right) - \left( \mu_1^2 \frac{ZZ^\top}{2d} + {\tilde \mu}^2 I \right)} = \bigO{\frac{\log^3 d}{\sqrt d}},
\end{equation}
with probability at least $1 - 2 \exp \left( -c_3 \log^2 d \right)$ over $Z$ (where we used $\mu_2 = 0$ since $\phi$ is odd), which readily gives the thesis when plugged in \eqref{eq:EwithDk}.
\end{proof}






\begin{lemma}\label{lemma:conckernel}
    We have that
    \begin{equation}
        \opnorm{\Phi} = \bigO{\sqrt p},
    \end{equation}
    \begin{equation}
        \opnorm{\Phi \Phi^\top - \E_{V} \left[ \Phi \Phi^\top \right]} = \bigO{\sqrt {pd}},
    \end{equation}
    \begin{equation}
        \evmin{\Phi \Phi^\top} = \Omega \left( p \right),
    \end{equation}
    with probability at least $1 - 2 \exp \left( -c \log^2 d \right)$ over $Z$ and $V$, where $c$ is an absolute constant.
\end{lemma}
\begin{proof}
    $\Phi^\top$ is a matrix with i.i.d. rows in the probability space of $V$. In particular, its $i$-th row takes the form
    \begin{equation}
        \left[ \Phi^\top \right]_{i:} = \phi \left( Z V_{i:} \right) = \phi \left( Z V_{i:} \right) - \E_{V} \left[ \phi \left( Z V_{i:} \right) \right],
    \end{equation}
    where the last step holds since the (Gaussian) distribution of $V_{i:}$ is symmetric and $\phi$ is an odd function by Assumption \ref{ass:activation}. Then, since $\sqrt{2d} \, V_{i:}$ is a standard Gaussian (and hence Lipschitz concentrated) random vector, and $\phi$ is a Lipschitz continuous function, we have that
    \begin{equation}
        \subGnorm{\left[ \Phi^\top \right]_{i:}} = \bigO{\frac{\opnorm{Z}}{\sqrt d}} = \bigO{1},
    \end{equation}
    where the $\subGnorm{\cdot}$ is meant on the probability space of $V$, and the second step holds with probability at least $1 - 2 \exp \left( -c_1 d \right)$ over $Z$ due to Lemma \ref{lemma:facts}. Conditioning on this high probability event, $\Phi^\top$ is a  $p \times n$ matrix whose rows are i.i.d. mean-0 sub-Gaussian random vectors in $\R^n$. Then, by Lemma B.7 in \cite{bombari2022memorization}, we have
    \begin{equation}
        \opnorm{\Phi^\top} = \bigO{\sqrt n + \sqrt p} = \bigO{\sqrt p},
    \end{equation}
    with probability at least $1 - 2 \exp( - c_2 n)$ over $V$, where the second step holds because $n = o(p)$.

    For the second part of the proof, we again follow the argument in Lemma B.7 in \cite{bombari2022memorization}, which in turn exploits the discussion in Remark 5.40 in \cite{vershrandmat}, and conclude that
    \begin{equation}
        \opnorm{\Phi \Phi^\top - \E_{V} \left[ \Phi \Phi^\top \right]} = \bigO{p \sqrt{\frac{n}{p}}} =  \bigO{\sqrt {pn}} = \bigO{\sqrt {pd}},
    \end{equation}
    with probability at least $1 - 2 \exp( - c_3 n)$ over $Z$ and $V$.

    For the last statement, Lemma \ref{lemma:long} and Weyl's inequality imply that, with probability at least $1 - 2 \exp \left( -c_2 \log^2 d \right)$ over $Z$ we have
    \begin{equation}
    \begin{split}        
        \evmin{\Phi \Phi^\top} &\geq p \, \evmin{ \mu_1^2 \frac{ZZ^\top}{2d} + {\tilde \mu}^2 I } - \opnorm{\E_{V} \left[ \Phi \Phi^\top \right] -  p \left( \mu_1^2 \frac{ZZ^\top}{2d} + {\tilde \mu}^2 I \right)} \\
        & \qquad -\opnorm{\Phi \Phi^\top - \E_{V} \left[ \Phi \Phi^\top \right]} \\ &\geq p {\tilde \mu}^2 -\bigO{\frac{p \log^3 d}{\sqrt d}} -\bigO{\sqrt{pd}}= \Omega(p),
    \end{split}
    \end{equation}
    where the last step is true since $\tilde \mu \neq 0$, as $\phi$ is non-linear by Assumption \ref{ass:activation}.
\end{proof}









\begin{lemma}\label{lemma:n'}
    Let $\tilde \phi: \R \to \R$ be defined as $\tilde \phi(\cdot) := \phi(\cdot) - \mu_1 (\cdot)$, and set
    \begin{equation}
        n' = \min \left( \left \lfloor \frac{p}{\log^4 p} \right \rfloor, \left \lfloor \frac{d^{3/2}}{\log^3 d} \right \rfloor \right).
    \end{equation}
    Let $\{ \hat z_i \}_{i = 1}^{n'}$ be $n'$ i.i.d.\ random variables sampled from a distribution respecting Assumption \ref{ass:data2}, not necessarily with the same covariance as $\mathcal P_{XY}$, and independent from $V$. Then, if $\tilde \Phi_{n'} \in \R^{n' \times p}$ is defined as the matrix containing $\tilde \phi( V \hat z_i)$ in its $i$-th row, we have that
    \begin{equation}
        \opnorm{\tilde \Phi_{n'}} = \bigO{\sqrt p},
    \end{equation}
    with probability at least $1 - 2 \exp \left( -c \log^2 d \right)$ over $\{ \hat z_i \}_{i = 1}^{n'}$ and $V$, where $c$ is an absolute constant.
\end{lemma}
\begin{proof}
    The proof follows the same strategy as Lemma C.8 in \cite{bombari2024privacy}, with the only difference that they work under their Assumption 1.2, \emph{i.e.} that the data is normalized as $\norm{\hat z_i}_2 = \sqrt{2d}$ (in our notation). This difference, however, does not affect the result. We can in fact condition on the high probability event that all $\hat z_i$ are such that $\norm{\hat z_i}_2 = \Theta(\sqrt d)$, which holds with probability at least $1 - 2 \exp \left( -c d \right)$ by Lemma \ref{lemma:facts}, and proceed in the same way (as their Equation (C.78) now holds) until their Equation (C.81), which requires their Lemma C.7, \emph{i.e.} that
    \begin{equation}
        \opnorm{\E_V \left[ \tilde \Phi_{n'} {\tilde \Phi_{n'}}^\top  \right]} = \bigO{p}.
    \end{equation}
    This holds also in our case, as it can be proven following the argument in \eqref{eq:offdiag} and \eqref{eq:diag}, where now the $n$ in the last line of \eqref{eq:offdiag} has to be replaced with $n'$, making the RHS there being $\bigO{1}$, as $n' = \bigO{\frac{d^{3/2}}{\log^3 d}}$ by definition. Lastly, the normalization of the data is used one more time in their Equation (C.91), but it is not critical to obtain the result, as $\norm{\hat z_i}_2 = \Theta(\sqrt d)$ is sufficient. We remark that Assumption 1.2 in \cite{bombari2024privacy} also requires the covariance of the distribution to be well-conditioned, which however is not required for the purposes of the above mentioned lemmas.
\end{proof}






\begin{lemma}\label{lemma:Eopnormsmall}
Let $\tilde \phi: \R \to \R$ be defined as $\tilde \phi(\cdot) := \phi(\cdot) - \mu_1 (\cdot)$, and let $z\in \R^{2d}$ be sampled from a distribution respecting Assumption \ref{ass:data2}, not necessarily with the same covariance as $\mathcal P_{XY}$, and independent from $V$. Then we have that
\begin{equation}
    \opnorm{\E_{z} \left[ \tilde \phi( V z) \tilde  \phi( V z)^\top \right]} = \bigO{\log^4 d + \frac{p \log^3 d}{d^{3/2}}},
\end{equation}
with probability at least $1 - 2 p^2 \exp \left( -c \log^2 d \right)$ over $V$, where $c$ is an absolute constant.
\end{lemma}
\begin{proof}
The proof follows a similar path as the one in Lemma C.15 in \cite{bombari2024privacy}. In particular, set
\begin{equation}
    n' = \min \left( \left \lfloor \frac{p}{\log^4 p} \right \rfloor, \left \lfloor \frac{d^{3/2}}{\log^3 d} \right \rfloor \right) , \qquad N = p^2 n',
\end{equation} and let $\tilde \Phi_N \in \R^{N \times p}$ be a matrix containing $\tilde \phi(V \hat z_i)$ in its $i$-th row, where every $\{ \hat z_i \}_{i = 1}^N$ is sampled independently from the same distribution of $z$. 
Thus, $\tilde \Phi_N$ can be seen as the vertical stacking of $p^2$ matrices with size $n' \times p$. All these matrices respect the hypotheses of Lemma \ref{lemma:n'}, and hence have their operator norm bounded by $\bigO{\sqrt p}$ with probability at least $1 - 2 \exp \left( -c_1 \log^2 d \right)$. Thus, performing a union bound over these $p^2$ matrices, we get
\begin{equation}\label{eq:PhiNopnormstack}
    \opnorm{\tilde \Phi_N^\top \tilde \Phi_N} = \bigO{p^2 \, p} = \bigO{\frac{Np}{n'}} = \bigO{N \log^4 p + \frac{N p \log^3 d}{d^{3/2}} },
\end{equation}
with probability at least $1 - 2 p^2 \exp \left( -c_1 \log^2 d \right)$ over $V$ and $\{ \hat z_i \}_{i = 1}^N$.

Via the same argument used for the last statement of Lemma \ref{lemma:facts}, denoting with $v_k \in \R^{2d}$ the $k$-th row of $V$, we have that $\norm{v_k}_2 = \bigO{1}$ uniformly for every $k$ with probability at least $1 - 2 p \exp(-c_2 d)$. Conditioning on such event, we have that each entry of $\tilde \phi(V \hat z_1)$ is sub-Gaussian (with uniformly bounded sub-Gaussian norm), since $\hat z_1$ is sub-Gaussian (as it is mean-0 and Lipschitz concentrated) and $\tilde \phi$ is a Lipschitz function. Thus, we have that each entry of $\E_{\hat z_1} \left[ \tilde \phi(V \hat z_1) \right]$ is $\bigO{1}$ (see Proposition 2.5.2 in \cite{vershynin2018high}), and therefore that $\subGnorm{\E_{z_1} \left[ \tilde \phi(V \hat z_1) \right]} = \bigO{\norm{\E_{z_1} \left[ \tilde \phi(V \hat z_1) \right]}_2} = \bigO{\sqrt p}$. Then, conditioning on the high probability event $\opnorm{V} = \bigO{\sqrt{p/d}}$ given by Lemma \ref{lemma:facts}, we have
\begin{equation}\label{eq:notsolikely}
    \subGnorm{\tilde \phi(V \hat z_1)} \leq \subGnorm{\tilde \phi(V \hat z_1) - \E_{z_1} \left[ \tilde \phi(V \hat z_1) \right]} +\subGnorm{\E_{z_1} \left[ \tilde \phi(V \hat z_1) \right]} = \bigO{\sqrt{\frac{p}{d}} + \sqrt p } = \bigO{\sqrt p},
\end{equation}
where the second step holds because $\hat z_1$ is Lipschitz concentrated and $\tilde \phi$ is Lipschitz. Since the rows of $\tilde \Phi_N$ are identically distributed, this also holds jointly for all other $\hat z_i$-s, for $i \in [N]$. Then, $\tilde \Phi_N / \sqrt p$ is a matrix with independent sub-Gaussian rows, and by Theorem 5.39 in \cite{vershrandmat} (see their Remark 5.40 and Equation (5.25)), we have that
\begin{equation}\label{eq:PhiNconc}
    \frac{1}{p}  \opnorm{\frac{\tilde \Phi_N^\top \tilde \Phi_N}{N} - \E_{z} \left[  \tilde \phi( V z) \tilde  \phi( V z)^\top \right]} = \bigO{\sqrt{\frac{p}{N}}},
\end{equation}
with probability at least $1 - 2 \exp \left( -c_3 p \right)$ over $\{ \hat z_i \}_{i = 1}^N$.
Then, we have
\begin{equation}
    \begin{aligned}
        \opnorm{\E_{z} \left[ \tilde \phi( V z) \tilde  \phi( V z)^\top  \right]} &\leq \opnorm{\frac{\tilde \Phi_N^\top \tilde \Phi_N}{N} - \E_{z} \left[ \tilde \phi( V z) \tilde  \phi( V z)^\top  \right]} + \frac{\opnorm{\tilde \Phi_N^\top \tilde \Phi_N}}{N} \\
        &= \bigO{p \sqrt{\frac{p}{N}}} + \bigO{\log^4 p + \frac{p \log^3 d}{d^{3/2}}} \\
        &= \bigO{\sqrt p \, \sqrt{ \frac{\log^4 p}{p}} + \sqrt p \, \sqrt{\frac{\log^3 d}{d^{3/2}}}} + \bigO{\log^4 p + \frac{p \log^3 d}{d^{3/2}}} \\
        &= \bigO{\log^4 p + \frac{p \log^3 d}{d^{3/2}}},
    \end{aligned}
\end{equation}
where the first step follows from the triangle inequality, the second step is a consequence of \eqref{eq:PhiNconc} and \eqref{eq:PhiNopnormstack}, and the third step follows from the definition of $N$.

Taking the intersection between the high probability events in \eqref{eq:PhiNopnormstack}, \eqref{eq:notsolikely} and \eqref{eq:PhiNconc}, the previous equation then holds with probability at least $1 - 2 p^2 \exp \left( -c_4 \log^2 d \right)$ over $V$ and $\{ \hat z_i \}_{i = 1}^N$. Also note that its LHS does not depend on $\{ \hat z_i \}_{i = 1}^N$, which were introduced as auxiliary random variables. Thus, the high probability bound holds restricted to the probability space of $V$, and the desired result follows.
\end{proof}





\begin{lemma}\label{lemma:matbernstein}
    Let $\tilde \phi: \R \to \R$ be defined as $\tilde \phi(\cdot) := \phi(\cdot) - \mu_1 (\cdot)$ and $\tilde \Phi \in \R^{n \times p}$ as the matrix containing $\tilde \phi( V z_i)$ in its $i$-th row. Then, we have
    \begin{equation}
        \opnorm{\tilde \Phi V} = \bigO{\sqrt p \log d + \frac{p \log d}{d}},
    \end{equation}
    with probability at least $1 - 2 \exp \left( -c \log^2 d \right)$ over $Z$ and $V$, where $c$ is an absolute constant.
\end{lemma}
\begin{proof}
%Let $\tilde \phi: \R \to \R$ be defined as $\tilde \phi(\cdot) := \phi(\cdot) - \mu_1 (\cdot)$. 
Note that $\tilde \phi$ is Lipschitz (since $\phi$ is Lipschitz by Assumption \ref{ass:activation}). During all the proof, we condition on the event $\opnorm{Z} = \bigO{\sqrt d}$ and $\norm{z_i}_2 = \Theta \left(\sqrt d \right)$ for all $i \in [n]$, which holds with probability at least $1 - 2 \exp \left( -c_1 d \right)$ by Lemma \ref{lemma:facts}. During the proof we also use the shorthand $v \in \R^{2d}$ to denote a random vector such that $\sqrt{2d} \, v$ is a standard Gaussian vector, \emph{i.e.}, it has the same distribution as the rows of $V$. This implies
\begin{equation}
    \mathbb E_v \left[ \norm{\tilde \phi \left( Z v \right)}_2 \right] = \bigO{\sqrt n}, \qquad \subGnorm{\norm{\tilde \phi \left( Z v \right)}_2 - \mathbb E_v \left[ \norm{\tilde \phi \left( Z v \right)}_2 \right]} = \bigO{1},
\end{equation}
and
\begin{equation}
    \mathbb E_v \left[ \norm{v}_2 \right] = \bigO{1}, \qquad \subGnorm{\norm{v}_2 - \mathbb E_v \left[\norm{v}_2 \right]} = \bigO{\frac{1}{\sqrt d}},
\end{equation}
where both sub-Gaussian norms are meant on the probability space of $v$, and where the very first equation follows from the discussion in Lemma C.3 in \cite{bombari2022memorization}. 
Then, there exists an absolute constant $C_1$ such that we jointly have
\begin{equation}
    \norm{\tilde \phi \left( Z v \right)}_2 \leq C_1 \sqrt d, \qquad \norm{v}_2 \leq C_1
\end{equation}
with probability at least $1 - 2 \exp \left( -c_2 d \right)$ over $v$.

Let $E_k$ be the indicator defined on the high probability event above with respect to the random variable $v_k := V_{:k}$ (the $k$-th row of $V$), \emph{i.e.}
\begin{equation}\label{eq:ek}
    E_{k} := \mathbf 1 \left( \norm{v_k}_2 \leq C_1 \textup{ and } \; \norm{\tilde \phi \left( Z v_k \right)}_2 \leq C_1 \sqrt d \right),
\end{equation}
and we define $E \in \R^{p \times p}$ as the diagonal matrix containing $E_k$ in its $k$-th entry. Notice that we have $\opnorm{I - E} = 0$ with probability at least $1 - 2 p \exp \left( -c_2 d \right)$, and $\E_V \left[ \opnorm{I - E }\right] \leq 2p \exp \left( -c_2 d \right)$.

Thus, we have
\begin{equation}\label{eq:polyvsexp}
\begin{aligned}
    \opnorm{\E_V \left[ \tilde \Phi \left( I - E \right) V \right]} &\leq \E_V \left[ \opnorm{\tilde \Phi} \opnorm{1 - E} \opnorm{V} \right] \\
    &\leq \E_V \left[ \opnorm{\tilde \Phi}^2  \opnorm{V}^2 \right]^{1 / 2}  \E_V \left[ \opnorm{I - E}^2 \right]^{1/2} \\
    &\leq \E_V \left[ \opnorm{\tilde \Phi}^4 \right]^{1/4} \E_V \left[ \opnorm{V}^4 \right]^{1 / 4} \left( 2p \exp \left( -c_2 d \right) \right) ^{1/2} \\
    &\leq \E_V \left[ \norm{\tilde \Phi}^4_F \right]^{1/4} \E_V \left[ \norm{V}^4_F \right]^{1 / 4} \left( 2p \exp \left( -c_2 d \right) \right) ^{1/2} \\
    & = o(1),
\end{aligned}
\end{equation}
where the last step holds because of our initial conditioning on $Z$: the first two terms are the sum of finite powers of sub-Gaussian random variables (the entries of $\tilde \Phi$ and $V$), and thus (see Proposition 2.5.2 in \cite{vershynin2018high}) the first two factors in the third line of the previous equation will be $\bigO{p^\alpha}$ for some finite $\alpha$, which gives the last line due to Assumption \ref{ass:overparam}.


As in Lemma \ref{lemma:long}, we introduce the notation (for all $i \in [n]$) $\tilde \phi^{(i)}: \R \to \R$ such that $\tilde \phi^{(i)}(\cdot) = \tilde \phi(\norm{z_i}_2 \cdot / \sqrt{2d})$. Thus, denoting with $v \in \R^{2d}$ a random vector such that $\sqrt{2d} v$ is standard Gaussian (\emph{i.e.}, distributed as the rows of $V$), we can write
\begin{equation}
    \left[ \E_V \left[ \tilde \Phi V \right] \right]_{ij} = p \left[ \E_v \left[ \tilde \phi (Zv) v^\top \right] \right]_{ij} = \frac{p}{\sqrt{2d}} \E_v \left[ \tilde\phi^{(i)} \left( \frac{z_i^\top}{\norm{z_i}_2} \sqrt{2d} v \right)  \left( e_j^\top \left( \sqrt{2d} v\right) \right) \right] = \frac{p}{\sqrt{2d}} \frac{ {\tilde \mu}_1^{(i)} z_i^\top e_j}{\norm{z_i}_2},
\end{equation}
where ${\tilde \mu}_1^{(i)}$ is the first Hermite coefficient of $\tilde \phi^{(i)}$. Then, denoting with $\tilde D \in \R^{n \times n}$ the diagonal matrix containing ${\tilde \mu}_1^{(i)} / \norm{z_i}_2$ in its $i$-th entry, we can write
\begin{equation}
    \opnorm{\E_V \left[ \tilde \Phi V \right] } = \frac{p}{\sqrt{2d}} \opnorm{\tilde D Z} \leq \frac{p}{\sqrt{2d}} \opnorm{\tilde D} \opnorm{Z}.
\end{equation}
Then, since we conditioned on $\norm{z_i}_2 = \Theta \left(\sqrt d \right)$ for all $i \in [n]$, following the same argument as in \eqref{eq:mu1conc}, and since the first Hermite coefficient of $\tilde \phi$ is 0 by definition, we have 
\begin{equation}
    \opnorm{\E_V \left[ \tilde \Phi V \right] } = \bigO{\frac{p}{\sqrt{d}} \, \frac{\log d}{d} \, \sqrt d} = \bigO{\frac{p \log d}{d}}, 
\end{equation}
with probability at least $1 - 2 \exp \left( -c_3 \log^2 d\right)$ over $Z$. A standard application of the triangle inequality to this last equation and \eqref{eq:polyvsexp} then gives
\begin{equation}\label{eq:forlater}
    \opnorm{\E_V \left[ \tilde \Phi E V \right] } \leq \opnorm{\E_V \left[ \tilde \Phi V \right] } + \opnorm{\E_V \left[ \tilde \Phi \left( I - E \right) V \right]} = \bigO{\frac{p \log d}{d}}, 
\end{equation}
with probability at least $1 - 2 \exp \left( -c_4 \log^2 d\right)$ over $Z$.

Let's now look at
\begin{equation}\label{eq:sumforB}
    \tilde \Phi E V  - \E_V \left[ \tilde \Phi E V \right] = \sum_{k = 1}^p \tilde \phi (Z v_k) E_k v_k^\top - \E_{v_k} \left[ \tilde \phi (Z v_k) E_k v_k^\top \right] =: \sum_{k = 1}^p W_k,
\end{equation}
where we defined the shorthand $W_k = \tilde \phi (Z v_k) E_k v_k^\top - \E_{v_k} \left[ \tilde \phi (Z v_k) E_k v_k^\top \right]$. \eqref{eq:sumforB} is the sum of $p$ i.i.d.\ mean-0 random matrices $W_k$ (in the probability space of $V$), such that
\begin{equation}
\begin{aligned}
    \sup_{v_k} \opnorm{\tilde \phi (Z v_k) E_k v_k^\top - \E_{v_k} \left[ \tilde \phi (Z v_k) E_k v_k^\top \right]} &\leq 2 \sup_{v_k} \opnorm{\tilde \phi (Z v_k) E_k v_k^\top} \\
    &= 2 \sup_{v_k} \norm{\tilde \phi (Z v_k)}_2 \norm{v_k}_2 E_k \\
    &\leq 2C_1^2 \sqrt d,
\end{aligned}
\end{equation}
because of \eqref{eq:ek}. Then, by matrix Bernstein's inequality for rectangular matrices (see Exercise 5.4.15 in \cite{vershynin2018high}), we have that
\begin{equation}\label{eq:bernstein}
    \P_V \left( \opnorm{\tilde \Phi E V  - \E_V \left[ \tilde \Phi E V \right]} \geq t  \right) \leq \left( n + d \right) \exp \left( - \frac{t^2 / 2}{\sigma^2 +2C_1^2 \sqrt d t / 3 } \right),
\end{equation}
where $\sigma^2$ is defined as
\begin{equation}\label{eq:sigmamax}
    \sigma^2 = p \max\left( \opnorm{\E_{v_k} \left[ W_k W_k^\top \right]}, \opnorm{\E_{v_k} \left[ W_k^\top W_k \right]} \right).
\end{equation}
For every matrix $A$, we have $\E \left[ \left(A - \E [A] \right) \left(A - \E [A] \right)^\top \right] = \E \left[ AA^\top \right] - \E [A] \E[A]^\top \preceq \E \left[ AA^\top \right]$. Thus,
\begin{equation}
\begin{aligned}
     \opnorm{\E_{v_k} \left[ W_k W_k^\top \right]} &\leq \opnorm{\E_{v_k} \left[ \tilde \phi (Z v_k) E_k v_k^\top v_k E_k \tilde \phi (Z v_k)^\top \right]} \\
     &\leq \opnorm{\E_{v_k} \left[ \tilde \phi (Z v_k) \tilde \phi (Z v_k)^\top \right]} \sup_{v_k} \left( E_k \norm{v_k}_2^2 \right) \\
     &\leq C_1^2 \opnorm{\E_{v_k} \left[ \tilde \phi (Z v_k) \tilde \phi (Z v_k)^\top \right]} \\
     &= \bigO{1},
\end{aligned}
\end{equation}
where the last step is a direct consequence of Lemma \ref{lemma:long}, applied to $\tilde \Phi$ instead of to $\Phi$, and holds with probability at least $1 - 2 \exp \left( -c_5 \log^2 d\right)$ over $Z$. For the other argument in the $\max$ in \eqref{eq:sigmamax} we similarly have
\begin{equation}
\begin{aligned}
     \opnorm{\E \left[ W_k^\top W_k \right]} &\leq \opnorm{\E_{v_k} \left[ v_k E_k \tilde \phi (Z v_k)^\top \tilde \phi (Z v_k) E_k v_k^\top  \right]} \\
     &\leq \opnorm{\E_{v_k} \left[ v_k v_k^\top \right]} \sup_{v_k} \left( E_k \norm{\tilde \phi (Z v_k)}_2^2 \right) \\
     &\leq \frac{1}{d} \, C_1^2 d  \\
     &= \bigO{1}.
\end{aligned}
\end{equation}
Then, plugging these last two equations in \eqref{eq:bernstein} we get
\begin{equation}\label{eq:cnew}
\begin{aligned}
    \P_V \left( \opnorm{\tilde \Phi E V  - \E_V \left[ \tilde \Phi E V \right]} \geq \sqrt p \log d  \right) &\leq \left( n + d \right) \exp \left( - \frac{p \log^2 d / 2}{C_2 p + 2C_1^2 \sqrt d \sqrt p \log d / 3 } \right) \\
    &\leq 2 \exp \left( -c_6 \log^2 d \right),
\end{aligned}
\end{equation}
where we used Assumption \ref{ass:overparam}. Then, applying a triangle inequality and using \eqref{eq:forlater} and  \eqref{eq:cnew}, we get
\begin{equation}
    \opnorm{\tilde \Phi E V} = \bigO{\sqrt p \log d + \frac{p \log d}{d}},
\end{equation}
with probability at least $1 - 2 \exp \left( -c_7 \log^2 d \right)$ over $Z, V$.
Then, since $E = I$ with probability at least $1 - 2 p \exp \left( -c_3 d \right)$, using  Assumption \ref{ass:overparam} we get the desired result.

\end{proof}









\begin{lemma}\label{lemma:3terms}
    Let $z \in \R^{2d}$ be sampled from a distribution respecting Assumption \ref{ass:data2}, not necessarily with the same covariance as $\mathcal P_{XY}$, independent from everything else, and let $f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda))$ be the RF model defined in \eqref{eq:rf} with $\hat \theta_{\textup{RF}}(\lambda)$ defined in \eqref{eq:hatthetarf}, with $\lambda \geq 0$. Then, we have that 
    \begin{equation}
        \left| f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda)) - \mu_1^2 p \frac{z^\top Z^\top}{2d}  \left( \Phi \Phi^\top + n \lambda I \right)^{-1} G \right| = \bigO{\frac{d^{1/4} \log d}{p^{1/4}} + \frac{\log^{3/2} d}{d^{1/8}}} = o(1),
    \end{equation}
    with probability $1 - C \sqrt d \log^2 d / \sqrt p - C \log^3 d / d^{1/4}$ over $Z$, $G$, $V$ and $z$, where $C$ is an absolute constant
\end{lemma}
\begin{proof}
    Let $\tilde \phi: \R \to \R$ be defined as $\tilde \phi(\cdot) := \phi(\cdot) - \mu_1 (\cdot)$, and let $\tilde \Phi \in \R^{n \times p}$ be defined as the matrix containing $\tilde \phi \left( V z_i \right)$ in its $i$-th row. Then, introducing the shorthand $\hat G = \left( \Phi \Phi^\top + n \lambda I \right)^{-1} G$, we can write
    \begin{equation}\label{eq:3terms}
    \begin{aligned}
        f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda)) &= \left( \mu_1 V z + \tilde \phi \left( V z \right) \right)^\top \left( \mu_1 V Z^\top + \tilde \Phi^\top \right) \hat G \\
        &= \mu_1^2 p \frac{z^\top Z^\top}{2d}  \hat G + \mu_1^2 z^\top \left( V^\top V - \frac{p}{2d} I \right) Z^\top \hat G + \mu_1 z^\top V^\top \tilde \Phi^\top \hat G + \tilde \phi \left( V z \right)^\top \Phi \hat G.
    \end{aligned}
    \end{equation}
    Notice that since every entry of $G$ is sub-Gaussian and independent by Assumption \ref{ass:data2}, Theorem 3.1.1 in \cite{vershynin2018high} readily gives $\norm{G}_2 = \bigO{\sqrt n} = \bigO{\sqrt d}$ with probability at least $1 - 2 \exp (-c_1 d)$ over $G$. Then, conditioning on the high probability event described by Lemma \ref{lemma:conckernel}, we get
    \begin{equation}\label{eq:hatG}
        \norm{\hat G}_2 \leq \left(\evmin{\Phi \Phi^\top + n \lambda I}\right)^{-1} \norm{G}_2 \leq \left(\evmin{\Phi \Phi^\top}\right)^{-1} \norm{G}_2 = \bigO{\frac{\sqrt{d}}{p}},
    \end{equation}
    with probability at least $1 - 2 \exp \left( -c_2 \log^2 d \right)$ over $V$, $Z$ and $G$. We will condition on this high probability event until the end of the proof. Let's then investigate the last 3 terms on the RHS of \eqref{eq:3terms} separately:
    \begin{itemize}
        \item[(i)] A direct application of Theorem 5.39 of \cite{vershrandmat} (see their Equation 5.23) gives
        \begin{equation}
            \opnorm{ \frac{2d}{p} V^\top V - I} = \bigO{\sqrt{\frac{d}{p}}},
        \end{equation}
        with probability at least  $1 - 2 \exp \left( -c_3 d \right)$ over $V$. Then, since $z$ is sub-Gaussian and independent from everything else, with probability $1 - 2 \exp \left( -c_4 \log^2 d \right)$ over itself we have
        \begin{equation}
        \begin{aligned}
            \left|  \mu_1^2 z^\top \left( V^\top V - \frac{p}{2d} I \right) Z^\top \hat G \right| &\leq \log d \norm{\left( V^\top V - \frac{p}{2d} I \right) Z^\top \hat G}_2 \\
            &\leq \log d \opnorm{V^\top V - \frac{p}{2d} I} \opnorm{Z} \norm{\hat G}_2 \\
            &=\bigO{\log d \, \sqrt{\frac{p}{d}} \, \sqrt d \, \frac{\sqrt{d}}{p}} = \bigO{\frac{\sqrt d \log d}{\sqrt p}},
        \end{aligned}
        \end{equation}
        where the third step holds with probability at least $1 - 2 \exp \left( -c_5  d \right)$ over $Z$ due to Lemma \ref{lemma:facts}.
        \item[(ii)] As before, since $z$ is sub-Gaussian and independent from everything else, with probability $1 - 2 \exp \left( -c_4 \log^2 d \right)$ we have
        \begin{equation}\label{eq:forproofsketch2}
        \begin{aligned}
            \left|\mu_1 z^\top V^\top \tilde \Phi^\top \hat G  \right| &\leq \log d \opnorm{V^\top \tilde \Phi^\top} \norm{\hat G}_2 \\
            &= \bigO{\log d \left( \sqrt p \log d + \frac{p \log d}{d} \right) \frac{\sqrt d}{p}} = \bigO{\log^2 d 
            \left( \sqrt{\frac{d}{p}} + \frac{1}{\sqrt d}\right)},
        \end{aligned}
        \end{equation}
        where the second step holds because of Lemma \ref{lemma:matbernstein}, and holds with probability at least $1 - 2 \exp \left( -c_6 \log^2 d \right)$ over $Z$ and $V$.
        \item[(iii)] For the last term of the RHS of \eqref{eq:3terms}, its second moment in the probability space of $z$ reads
        \begin{equation}
        \begin{aligned}
            \E_z \left[ \hat G^\top \Phi^\top \tilde \phi \left( V z \right) \tilde \phi \left( V z \right)^\top \Phi \hat G \right] &\leq \opnorm{\E_z \left[ \tilde \phi \left( V z \right) \tilde \phi \left( V z \right)^\top \right]}  \opnorm{\Phi}^2 \norm{\hat G}_2^2 \\
            &= \bigO{\left(\log^4 d + \frac{p \log^3 d}{d^{3/2}} \right) \sqrt d \frac{\sqrt d}{p}} = \bigO{\frac{d \log^4 d}{p} + \frac{\log^3 d}{\sqrt d}},
        \end{aligned}
        \end{equation}
        where the second step follows from Lemmas \ref{lemma:Eopnormsmall} and \ref{lemma:conckernel}, and holds with probability at least $1 - 2 \exp \left( -c_7 \log^2 d \right)$ over $Z$ and $V$. Then, by Markov inequality, we have that there exists a constant $C_1$ such that 
        \begin{equation}
            \left( \tilde \phi \left( V z \right)^\top \Phi \hat G \right)^2 < C_1 \left(\frac{d \log^4 d}{p} + \frac{\log^3 d}{\sqrt d}\right) t,
        \end{equation}
        with probability at least $1 - 1 / t$ over $z$. Setting
        \begin{equation}
            t = \min\left(\frac{\sqrt p}{\sqrt d \log^2 d}, d^{1/4} \right) = \omega(1),
        \end{equation}
        since $p =\omega \left(d \log^4 d \right)$ by Assumption \ref{ass:overparam}, we have
        \begin{equation}\label{eq:forproofsketch}
            \left| \tilde \phi \left( V z \right)^\top \Phi \hat G \right| = \bigO{\frac{d^{1/4} \log d}{p^{1/4}} + \frac{\log^{3/2} d}{d^{1/8}}},
        \end{equation}
        with probability at least $1 - \sqrt d \log^2 d / \sqrt p - \log^3 d / d^{1/4}$.
    \end{itemize}
    Then, plugging (i), (ii) and (iii) in \eqref{eq:3terms} gives
    \begin{equation}
        \left| f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda)) - \mu_1^2 p \frac{z^\top Z^\top}{2d}  \hat G \right| = \bigO{\frac{d^{1/4} \log d}{p^{1/4}} + \frac{\log^{3/2} d}{d^{1/8}}},
    \end{equation}
    with probability at least $1 - c_8 \frac{\sqrt d \log^2 d}{\sqrt p} - c_8 \frac{\log^3 d}{d^{1/4}}$, which gives the desired result.
\end{proof}



\paragraph{Proof of Theorem \ref{thm:rf}}
Let $E \in \R^{n \times n}$ be the matrix defined as
\begin{equation}
    E = \Phi \Phi^\top - p \left( \mu_1^2 \frac{ZZ^\top}{2d} + {\tilde \mu}^2 I \right).
\end{equation}
Note that
\begin{equation}\label{eq:opnormE}
    \opnorm{E} \leq \opnorm{\Phi \Phi^\top - \E_{V} \left[ \Phi \Phi^\top \right]} + \opnorm{\E_{V} \left[ \Phi \Phi^\top \right] - p \left( \mu_1^2 \frac{ZZ^\top}{2d} + {\tilde \mu}^2 I \right)} = \bigO{p \left( \sqrt{\frac{d}{p}} + \frac{\log^3 d}{\sqrt d}\right)},
\end{equation}
with probability at least $1 - 2 \exp \left( -c_1 \log^2 d \right)$ over $Z, V$ due to Lemmas \ref{lemma:long} and \ref{lemma:conckernel}. By the Woodbury matrix identity (or Hua's identity), we have
\begin{equation}
\begin{aligned}
    \left( \Phi \Phi^\top + n \lambda I \right)^{-1} &= \left( p \left( \mu_1^2 \frac{ZZ^\top}{2d} + {\tilde \mu}^2 I \right) + E + n \lambda I \right)^{-1} \\
    &= \left(  \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I + E \right)^{-1} \\
    &= \left(  \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I \right)^{-1} \\
    & \qquad - \left(  \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I \right)^{-1} E  \left( \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I + E  \right)^{-1},
\end{aligned}
\end{equation}
which gives
\begin{equation}\label{eq:usingEop}
\begin{aligned}
     &\left| \mu_1^2 p \frac{z^\top Z^\top}{2d} \left( \Phi \Phi^\top + n \lambda I \right)^{-1} G - \mu_1^2 p \frac{z^\top Z^\top}{2d}\left(  \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I \right)^{-1} G \right| \\
     &\qquad \leq \left|  \mu_1^2 p \frac{z^\top Z^\top}{2d} \left(  \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I \right)^{-1} E  \left( \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I + E  \right)^{-1} G \right| \\
     &\qquad \leq \log d \norm{\frac{p}{2d} Z^\top \left(  \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I \right)^{-1} E  \left( \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I + E  \right)^{-1} G }_2 \\
     &\qquad \leq \log d \, \frac{p}{2d} \opnorm{Z} \frac{1}{{\tilde \mu}^2 p} \opnorm{E} \frac{1}{\evmin{\Phi \Phi^\top}} \norm{G}_2 \\
     &\qquad = \bigO{\log d \, \frac{p}{d} \, \sqrt d \, \frac{1}{p} \, p \left( \sqrt{\frac{d}{p}} + \frac{\log^3 d}{\sqrt d} \right) \, \frac{1}{p} \, \sqrt{d}} \\
     &\qquad = \bigO{\log d \sqrt{\frac{d}{p}} + \frac{\log^4 d}{\sqrt d}}.
\end{aligned}
\end{equation}
Here, the second step holds with probability at least $1 - 2 \exp \left( -c_2 \log^2 d \right)$ since $z$ is sub-Gaussian and independent from everything else;  the fourth step is a consequence of Lemma \ref{lemma:facts}, \eqref{eq:opnormE}, Lemma \ref{lemma:conckernel}, and $\norm{G}_2 = \bigO{\sqrt d}$ (see the argument prior to \eqref{eq:hatG}), and as a whole holds with probability $1 - 2 \exp \left( -c_3 \log^2 d \right)$ over $Z$, $G$, and $V$.

Note that the second term in the LHS of \eqref{eq:usingEop} can be written as
\begin{equation}\label{eq:equaltoLR}
\begin{aligned}
    \mu_1^2 p \frac{z^\top Z^\top}{2d}\left(  \mu_1^2  p \frac{ZZ^\top}{2d} + \left( {\tilde \mu}^2 p  + n \lambda \right) I \right)^{-1} G &= z^\top Z^\top \left( Z Z^\top + n \left( \frac{2{\tilde \mu}^2 d}{\mu_1^2 n} +  \frac{2d}{\mu_1^2 p} \lambda \right) I \right)^{-1} G \\
    &= z^\top \left( Z^\top Z + n \left( \frac{2{\tilde \mu}^2 d}{\mu_1^2 n} +  \frac{2d}{\mu_1^2 p} \lambda \right) I \right)^{-1} Z^\top G \\
    &= f_{\textup{LR}}(z, \hat \theta_{\textup{LR}}(\tilde \lambda)),
\end{aligned}
\end{equation}
where the second line is due to the classical identity $A^\top \left(AA^\top +\kappa I \right)^{-1} = \left(A^\top A +\kappa I \right)^{-1} A^\top$, and the third line uses the definition in \eqref{eq:lrmodel}, with $\hat \theta_{\textup{LR}}(\tilde \lambda)$ defined in \eqref{eq:hatthetalambda} and
\begin{equation}
    \tilde \lambda = \frac{2{\tilde \mu}^2 d}{\mu_1^2 n} + \frac{2d}{\mu_1^2 p} \lambda.
\end{equation}
Furthermore, the first term of the LHS of \eqref{eq:usingEop} satisfies
\begin{equation}\label{eq:equaltoRF}
    \left|  \mu_1^2 p \frac{z^\top Z^\top}{2d}  \left( \Phi \Phi^\top + n \lambda I \right)^{-1} G - f_{\textup{RF}}(z, \hat \theta_{\textup{RF}}(\lambda))  \right| = \bigO{\frac{d^{1/4} \log d}{p^{1/4}} + \frac{\log^{3/2} d}{d^{1/8}}},
    \end{equation}
    with probability $1 - C_1 \sqrt d \log^2 d / \sqrt p - C_1 \log^3 d / d^{1/4}$ over $Z$, $G$, $V$ and $z$, due to Lemma \ref{lemma:3terms}. Thus, an application of the triangle inequality together with \eqref{eq:usingEop}, \eqref{eq:equaltoLR} and \eqref{eq:equaltoRF} gives the desired result.
\qed



\section{\texorpdfstring{Proof of \eqref{eq:oodlossbody}: connection with out-of-distribution loss}{Proof of (4): connection with out-of-distribution loss}\label{app:ood}}

% \section{Proof of \eqref{eq:oodlossbody}: connection with out-of-distribution loss}\label{app:ood}

Let $\tilde x$ and $[x^\top, y^\top]^\top$ be sampled independently from $\mathcal P_X$ and $\mathcal P_{XY}$ respectively. For simplicity, assume that $\E \left[ f(\hat \theta, [\tilde x^\top, y])^2 \right] = \E \left[ f^*_x(\tilde x)^2 \right] = 1$ % \E_x \left[ f^*_x(x)^2 | y = \bar y \right] = 1$
and $\E \left[ f(\hat \theta, [\tilde x^\top, y]) \right] = \E \left[ f^*_x(\tilde x) \right] = 0$. Thus, for the quadratic loss, we readily get
\begin{equation}\label{eq:oodloss}
\begin{aligned}
    \E_{\tilde x, y} \left[ \left( f(\hat \theta, [\tilde x^\top, y]) - f^*_x(\tilde x) \right)^2 \right] &= 2 - 2 \E_{\tilde x, y} \left[ f(\hat \theta, [\tilde x^\top, y]) f^*_x(\tilde x) \right] \\
    &= 2 - 2 \Cov \left( f(\hat \theta, [\tilde x^\top, y]) , f^*_x(\tilde x) \right).
\end{aligned}
\end{equation}
Denoting with $S$ the covariance matrix of the three random variables $f(\hat \theta, [\tilde x^\top, y])$, $f^*_x(\tilde x)$, and $f^*_x(x)$, we have
\begin{equation}\label{eq:Sblocks}
S = \left(\begin{array}{@{}c c c@{}}
  1 & \rho & \mathcal C \\
  \rho & 1 & 0 \\
  \mathcal C & 0 & 1
\end{array}\right),
\end{equation}
where we introduced % the shorthands
$\mathcal C = \Cov \left( f(\hat \theta, [\tilde x^\top, y]) , f^*_x(x) \right)$ and $\rho = \Cov \left( f(\hat \theta, [\tilde x^\top, y]) , f^*_x(\tilde x) \right)$. Since $S$ is p.s.d., its determinant has to be non-negative, hence
\begin{equation}
    1 - \rho^2 - \mathcal C^2 \geq 0,
\end{equation}
which, when plugged in \eqref{eq:oodloss}, gives \eqref{eq:oodlossbody}. 
%\begin{equation}
%    \E_{\tilde x, y} \left[ \left( f(\hat \theta, [\tilde x^\top, y]) - f^*_x(\tilde x) \right)^2 \right] \geq 2 - 2 \sqrt{1 - \mathcal C^2},
%\end{equation}
%which provides a lower bound on the out-of-distribution test loss in terms of the amount of learned spurious correlations $\mathcal C$.

This bound suggests the close connection between $\mathcal C$ and the out-of-distribution test loss. % $\mathcal L_{\textup{out}}$ in our setting. 
In Figure \ref{fig:out}, we repeat the same experiments of Figures \ref{fig:simplicitysynthetic}-\ref{fig:simplicitycifar} and report in black the out-of-distribution test loss. The plots clearly show that $\mathcal C$ and the out-of-distribution test loss follow a similar trend, for both linear regression and random features.


\begin{figure}
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/simplicity_and_correlation_synthetic_out.pdf}
        % \caption*{(a) Synthetic Data}  % \caption* makes it unnumbered
        % \label{fig:sub1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/simplicity_cifar_new_out.pdf}
        % \caption*{(b) CIFAR Data}
        % \label{fig:sub2}
    \end{minipage}
    \caption{Out-of-distribution test loss $\mathcal L(\hat \theta_{\textup{LR}/\textup{RF}}(\lambda))$ (black) and spurious correlations $\mathcal C(\hat \theta_{\textup{LR}/\textup{RF}}(\lambda)$ (red) as a function of $\evmax{\Sigma_{yy}}$ (first panel) and $\evmin{S_x^\Sigma}$ (second panel) on a Gaussian synthetic dataset, and for the CIFAR-10 experiment (third panel). We consider the same set-up as Figures \ref{fig:simplicitysynthetic} and \ref{fig:simplicitycifar}, for Gaussian and CIFAR-10 data, respectively.}
    \label{fig:out}
\end{figure}


\section{Experimental details}\label{app:experiments}

All the plots in the figures report the average over 10 independent trials, with a shaded area describing a confidence interval of 1 standard deviation. For the Gaussian and Color-MNIST datasets, every iteration involves re-generating (or re-coloring) the data, while for the CIFAR-10 dataset the randomness comes from the model and the training algorithm.

\paragraph{Synthetic Gaussian data generation.} This follows the same model across all the numerical experiments presented in the paper. In particular, we fix $d = 400$ and set $\Sigma_{xx} = I$. $\Sigma_{yy}$ is a diagonal matrix, such that its first entry equals $\evmax{\Sigma_{yy}}$ and all the other entries equal $\left(d - \evmax{\Sigma_{yy}}\right) / (d - 1)$. In this way, $\tr(\Sigma) = 2d$. Then, we set the off-diagonal blocks $\Sigma_{xy}$ and $\Sigma_{yx}$ to the same diagonal matrix, so that
\begin{equation}
    \Sigma_{xy} = \Sigma_{yx} = (\Sigma_{yy} - \beta I)^{1/2},
\end{equation}
which implies that the Schur complement $S^\Sigma_{x} = \beta I$ and, therefore, $\evmin{S^\Sigma_x} = \beta$. To conclude, we set the ground truth $\theta^*_x = e_1$, \emph{i.e.}, the first element of the canonical basis in $\R^d$. This design choice is motivated by our interest in capturing the role of $\evmax{\Sigma_{yy}}$ and to have an easy control on the  Schur complement $S^\Sigma_x$ (which is therefore chosen to be proportional to the identity).

Unless differently stated in the figure, $n=2000$, $\evmax{\Sigma_{yy}} = 2$, $\beta = 0.5$, and $\lambda = 1$. Furthermore, to generate the labels, we add an independent noise with variance $\sigma^2 = 0.25$, and we subtract this quantity from the test loss, so that the optimal predictor $\theta^*$ has a test loss equal to 0.

When we use an RF model, on every dataset, unless differently stated in the figure, we use $\tanh$ as activation function, with $p = 20000$ neurons.


\paragraph{Binary color MNIST.} This dataset is graphically shown in Figure \ref{fig:intro}. To generate it, we take a subset of the MNIST training dataset ($n = 1000$ samples as default, unless differently specified) made only of zeros and ones. Then, for every training image, we color the white portion in blue (red) with probability $(1 + \alpha) / 2$ if the digit is a zero (one), and red (blue) otherwise. For the test set, we proceed in the same way, but setting $\alpha = 0$, to make the core feature (the digit) effectively independent from the spurious one (the color). For all the experiments, we set $\beta = 1 - \alpha^2 = 0.25$.

\paragraph{CIFAR-10.} For the experiments on CIFAR-10, we implicitly suppose that the middle $22 \times 22$ square contains the core, predictive feature $x$. Thus, we sum to all the channels of the outer region white noise with increasing variance, and we later clamp the pixels to ensure their value is between 0 and 1. Increasing the variance of the noise, this progressively makes the outer portion being dominated by random noise, thus reducing its value of $\evmax{\Sigma_{yy}} / \tr(\Sigma_{yy})$ when estimating the covariance on the perturbed training set. At test time, we take the images from the CIFAR-10 test set, and we add the same level of noise. To compute $\mathcal C$, we create an out-of-distribution dataset where the core features are randomly permuted across different backgrounds. We always consider the subset of boats and trucks, which contains $n=10000$ images.

\paragraph{2-layer neural network.} In the experiments shown in Figure \ref{fig:nn}, we consider a 2-layer neural network trained with gradient descent and quadratic loss on the Color-MNIST and CIFAR-10 datasets. For both datasets, we train for 1000 epochs, with learning rate 0.003, and batch size 1000.




\end{document}