\section{Related Work}
\begin{figure*}[h]
    \centering
    \includegraphics[width = 1\linewidth]{UnitCoder_V3.png}
    \caption{\textbf{The UnitCoder pipeline.} The pipeline consists of three main stages: (1) Data Preparation - filter package-centric data from raw code corpus and fine-tune a unit test generator to produce corresponding tests; (2) Fix and Refine Flow - execute function-test pairs in sandbox, iteratively fix failed cases via bug-fix agent, and refine successful code through refine agent; (3) Post-Train - construct prefix-completion pairs for post-training.}
    \label{fig:overall_framework}
\end{figure*}



\paragraph{Code LLMs}

% Code LLM developments contain two lines of work, large-scale pre-training, and prompt-based code instruct-tuning.
% Early work for code pre-training, exemplified by CodeX \cite{Chen2021EvaluatingLL}, CodeGen \cite{nijkamp2022codegen}, StarCoder~\cite{li2023starcoder} and CodeLlama \cite{Rozire2023CodeLO} introduces Code-based pre-trained models which developed from previous trends of BERT \cite{Devlin2019BERTPO} to CodeBERT pre-training \cite{Feng2020CodeBERTAP}.
% Later, open-sourcing series such as Qwen~\cite{Bai2023QwenTR,Yang2024Qwen2TR} and Deepseek~\cite{guo2024deepseekcoder} built code models, Qwen2.5 Coder~\cite{deepseekai2024deepseekv2strongeconomicalefficient, bi2024deepseek}, and Deepseek-Coder-V2 accordingly based on their open-sourced base models by collecting and filtering large-scale code data that exceeds 5 trillion tokens. 
% Further, using LLMs to help filter code data is a more costly but effective method of preparing code data.
% WaveCoder \cite{Yu2023WaveCoderWA} introduces GPT-4 as a discriminator to obtain high-quality data.
% Prompt-based code instruct-tuning methods exemplified by WizardCoder~\cite{luo2023wizardcoder} focus on improving code instructions with LLMs~\cite{Jiang2024ASO,Zan2023CanPL,Zhu2022ASO}. 
Code LLM developments have progressed along two main directions: large-scale pre-training and specialized instruct-tuning.
Early works of code pre-training models include pioneering works like CodeX \cite{Chen2021EvaluatingLL}, CodeGen \cite{nijkamp2022codegen}, StarCoder~\cite{li2023starcoder} and CodeLlama \cite{Rozire2023CodeLO}.
% built upon the foundation laid by BERT \cite{Devlin2019BERTPO} and CodeBERT \cite{Feng2020CodeBERTAP}.
Open-sourcing series such as Qwen~\cite{Bai2023QwenTR,Yang2024Qwen2TR} and Deepseek~\cite{deepseekai2024deepseekv2strongeconomicalefficient} build specialized code models as well, examplified by Qwen-Coder and Deepseek-Coder\cite{Hui2024Qwen25CoderTR, guo2024deepseekcoder}. 
% based on their open-sourced base models by collecting and filtering large-scale code data with trillions of tokens.

Furthermore, leveraging language models for code data filtering provides a valuable quality supervision signal in data preparation. For example, WaveCoder~\cite{Yu2023WaveCoderWA} employs GPT-4 as a discriminator, while similar work such as Arctic-SnowCoder~\cite{Arctic-SnowCoder} explores the potential of BERT-based models for code data filtering.

In parallel, 
% prompt-based code instruct-tuning approaches,
works represented by WizardCoder~\cite{luo2023wizardcoder} focus on enhancing instruction diversity through improved instruction engineering with powerful LLMs~\cite{Jiang2024ASO,Zan2023CanPL,Zhu2022ASO}.  
Additionally, research exemplified by AgentCoder~\cite{Huang2023AgentCoderMC} investigates prompt-based approaches that integrate test cases and multi-agent collaboration to improve coding performance~\cite{Huang2023AgentCoderMC, chen2022codet, islam2024mapcoder}.


% While impressive, these methods primarily focus on improving model performance on specific benchmarks through prompt engineering, with limited exploration of scalable, high-quality code synthesis. Our method offers a more practical solution by leveraging existing pre-training corpora and mainstream open-source models for large-scale code synthesis.

\paragraph{LLM-Driven Unit Test Generation}

Software engineering is a major topic in coding and programming, with LLM-based unit test generation emerging as a promising direction. Large language models have demonstrated remarkable capabilities in this area, as exemplified by TestPilot~\cite{empirical-unittest}, which introduces a comprehensive framework for automated test generation using LLMs. 
Several other works further validate LLMs' effectiveness in generating high-quality test cases~\cite{chatunitest, chattester}.
% including ChatUniTest~\cite{} and ChatTester~\cite{},
Building upon these foundations, subsequent works continue to explore LLM-based unit test generation. Several works focus on improving metrics like coverage and accuracy~\cite{Achiam2023GPT4TR, codeaware_prompt, pizzorno2024coverup}. 
% Meanwhile, works have demonstrated the versatility of LLM-based testing across various domains, from industrial-scale test automation to specific tasks like security vulnerability detection~\cite{Auto-Test-Meta, Fang2024LLMAC-Oneday, siddiq2023generate_pray}. 
These works collectively demonstrate the potential of LLMs in advancing automated unit test generation practices.