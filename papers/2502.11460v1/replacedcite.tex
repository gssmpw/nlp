\section{Related Work}
\begin{figure*}[h]
    \centering
    \includegraphics[width = 1\linewidth]{UnitCoder_V3.png}
    \caption{\textbf{The UnitCoder pipeline.} The pipeline consists of three main stages: (1) Data Preparation - filter package-centric data from raw code corpus and fine-tune a unit test generator to produce corresponding tests; (2) Fix and Refine Flow - execute function-test pairs in sandbox, iteratively fix failed cases via bug-fix agent, and refine successful code through refine agent; (3) Post-Train - construct prefix-completion pairs for post-training.}
    \label{fig:overall_framework}
\end{figure*}



\paragraph{Code LLMs}

% Code LLM developments contain two lines of work, large-scale pre-training, and prompt-based code instruct-tuning.
% Early work for code pre-training, exemplified by CodeX ____, CodeGen ____, StarCoder____ and CodeLlama ____ introduces Code-based pre-trained models which developed from previous trends of BERT ____ to CodeBERT pre-training ____.
% Later, open-sourcing series such as Qwen____ and Deepseek____ built code models, Qwen2.5 Coder____, and Deepseek-Coder-V2 accordingly based on their open-sourced base models by collecting and filtering large-scale code data that exceeds 5 trillion tokens. 
% Further, using LLMs to help filter code data is a more costly but effective method of preparing code data.
% WaveCoder ____ introduces GPT-4 as a discriminator to obtain high-quality data.
% Prompt-based code instruct-tuning methods exemplified by WizardCoder____ focus on improving code instructions with LLMs____. 
Code LLM developments have progressed along two main directions: large-scale pre-training and specialized instruct-tuning.
Early works of code pre-training models include pioneering works like CodeX ____, CodeGen ____, StarCoder____ and CodeLlama ____.
% built upon the foundation laid by BERT ____ and CodeBERT ____.
Open-sourcing series such as Qwen____ and Deepseek____ build specialized code models as well, examplified by Qwen-Coder and Deepseek-Coder____. 
% based on their open-sourced base models by collecting and filtering large-scale code data with trillions of tokens.

Furthermore, leveraging language models for code data filtering provides a valuable quality supervision signal in data preparation. For example, WaveCoder____ employs GPT-4 as a discriminator, while similar work such as Arctic-SnowCoder____ explores the potential of BERT-based models for code data filtering.

In parallel, 
% prompt-based code instruct-tuning approaches,
works represented by WizardCoder____ focus on enhancing instruction diversity through improved instruction engineering with powerful LLMs____.  
Additionally, research exemplified by AgentCoder____ investigates prompt-based approaches that integrate test cases and multi-agent collaboration to improve coding performance____.


% While impressive, these methods primarily focus on improving model performance on specific benchmarks through prompt engineering, with limited exploration of scalable, high-quality code synthesis. Our method offers a more practical solution by leveraging existing pre-training corpora and mainstream open-source models for large-scale code synthesis.

\paragraph{LLM-Driven Unit Test Generation}

Software engineering is a major topic in coding and programming, with LLM-based unit test generation emerging as a promising direction. Large language models have demonstrated remarkable capabilities in this area, as exemplified by TestPilot____, which introduces a comprehensive framework for automated test generation using LLMs. 
Several other works further validate LLMs' effectiveness in generating high-quality test cases____.
% including ChatUniTest____ and ChatTester____,
Building upon these foundations, subsequent works continue to explore LLM-based unit test generation. Several works focus on improving metrics like coverage and accuracy____. 
% Meanwhile, works have demonstrated the versatility of LLM-based testing across various domains, from industrial-scale test automation to specific tasks like security vulnerability detection____. 
These works collectively demonstrate the potential of LLMs in advancing automated unit test generation practices.