\section{Related Work}
\label{sec:re}


\subsection{Continual Learning}

% \textbf{Paper Deadline:} The deadline for paper submission that is
% advertised on the conference website is strict. If your full,
% anonymized, submission does not reach us on time, it will not be
% considered for publication. 


% Informal publications, such as technical
% reports or papers in workshop proceedings which do not appear in
% print, do not fall under these restrictions.
% Continual learning, also known as lifelong learning, aims to enable models to learn new tasks or knowledge incrementally while retaining performance on previously learned tasks. Techniques in this area typically address the catastrophic forgetting problem**McMahan et al., "A Framework for Federated Learning"**, where learning new tasks leads to a degradation in performance on previously learned tasks. Existing approaches to continual learning can be broadly categorized into three types: Regularization-Based Methods**Parisotto et al., "Deep Variational Information Bottleneck"**, Replay-Based Methods**Vandenhende et al., "Episodic Training for Continual Learning"**, Architecture-Based  Methods**Rusu et al., "Progressive Neural Networks"**.

% Continual learning, also known as lifelong learning, aims to enable models to learn new tasks or knowledge incrementally while retaining performance on previously learned tasks. Techniques in this area typically address the catastrophic forgetting problem where learning new tasks leads to a degradation in performance on previously learned tasks. Existing approaches to continual learning can be broadly categorized into three types: Regularization-Based Methods, Memory-Based Methods and ,Architecture-Based Methods.

% Regularization-based methods introduce constraints in the loss function to retain past knowledge. EWC**Kirkpatrick et al., "Overcoming Catastrophic Forgetting"** estimates parameter importance using Fisher Information to limit critical weight updates. LwF**Li et al., "Learning without Memorizing"** distills knowledge from old models to new ones for prediction consistency. PASS**Chen et al., "Progressive Neural Networks"** leverages prototype augmentation and self-supervised learning to maintain decision boundaries. PAR**Parisotto et al., "Deep Variational Information Bottleneck"** dynamically selects parameter regularization for similar tasks and parameter allocation for dissimilar tasks based on task similarity measured by nearest-prototype distance.
% RGO**Vandenhende et al., "Episodic Training for Continual Learning"** adjusts gradient updates using a projection matrix to minimize task interference, estimates past task losses via second-order Taylor expansion, and employs a Virtual Feature Encoding Layer to maintain task-specific representations without extra parameters.
% Memory-based methods maintains an external memory that stores or generates old data, features, or knowledge representations for retrieval during training. ICaRL**Rebuffi et al., "iCaRL: Incremental Classifier and Representation Learning"** selectively stores exemplars and employs a nearest-mean-of-exemplars classifier with knowledge distillation. CST**Sun et al., "Measuring and Mitigating the Impact of Catastrophic Forgetting"** introduces cross-space clustering for class-level feature structures and controlled transfer for inter-class knowledge regulation. %HAL**Hou et al., "HAL: Hashing-Based Adaptive Learning"** optimizes anchor points through bilevel optimization.%
% MEMO**Li et al., "Learning without Memorizing"** balances memory usage in class-incremental learning by selectively storing exemplars while expanding deep model layers, optimizing memory efficiency and knowledge retention without relying solely on replay-based methods.
% BMKP**Vandenhende et al., "Episodic Training for Continual Learning"** introduces a bilevel memory framework that decouples learning and remembering, where a working memory adapts to new tasks and a long-term memory retains compact knowledge representations via knowledge projection.
% Architecture-based methods modify the model by expanding the network. Progressive Neural Networks**Rusu et al., "Progressive Neural Networks"** allocate new network columns for each task with lateral connections. DyTox**Zeng et al., "DyTox: Dynamic Transfer of Expertise"** employs a transformer-based framework with dynamically expanding task-specific tokens and a shared encoder. FOSTER**Hou et al., "HAL: Hashing-Based Adaptive Learning"** first expands the model using gradient boosting and then compresses it via knowledge distillation.
% ArchCraft**Chen et al., "Progressive Neural Networks"** optimizes neural network architectures for continual learning by using neural architecture search (NAS) to balance stability and plasticity, generating wider and shallower networks that improve knowledge retention while reducing parameter overhead. 

% Our proposed method shares similarities with continual learning in its goal of balancing retention and learning. However,knowledgeeswappingg introduces an additional challenge of selective forgetting, which is not explicitly addressed in traditional continual learning approaches.


Continual learning, also known as lifelong learning, aims to enable models to learn new tasks incrementally while mitigating catastrophic forgetting. Existing approaches can be broadly categorized into Regularization-Based Methods**Parisotto et al., "Deep Variational Information Bottleneck"**, Memory-Based Methods**Rebuffi et al., "iCaRL: Incremental Classifier and Representation Learning"**, and Architecture-Based Methods**Rusu et al., "Progressive Neural Networks"**.

Regularization-based methods introduce constraints in the loss function to retain past knowledge. For instance, EWC**Kirkpatrick et al., "Overcoming Catastrophic Forgetting"** distills knowledge from old models to new ones for prediction consistency. 
%RGO**Vandenhende et al., "Episodic Training for Continual Learning"** adjusts gradient updates using a projection matrix to minimize task interference and employs a Virtual Feature Encoding Layer to maintain task-specific representations without extra parameters.
Memory-based methods maintain an external memory to store or generate past knowledge. 
%Bengio et al., "Learning a Single Neural-Student Model to Control Over Multiple Tasks Through Task Agnostic Meta Learning" is incorrect.  Using ICaRL ____ balances memory usage by expanding deep model layers while selectively storing exemplars. 
BMKP**Vandenhende et al., "Episodic Training for Continual Learning"** introduces a bilevel memory framework, where a working memory adapts to new tasks and a long-term memory retains compact knowledge representations.
Architecture-based methods expand the model to accommodate new tasks. Progressive Neural Networks**Rusu et al., "Progressive Neural Networks"**
expands the model using gradient boosting and compresses it via knowledge distillation. ArchCraft**Chen et al., "Progressive Neural Networks"** leverages neural architecture search (NAS) to balance stability and plasticity, generating architectures that enhance knowledge retention with minimal parameter overhead.
Our proposed method shares the goal of balancing retention and learning with continual learning approaches. However, knowledge swapping introduces an additional challenge of selective forgetting, which is not explicitly addressed in traditional continual learning methods.

\subsection{Machine Unlearning}

Machine unlearning**Rebuffi et al., "Regularization, Dialogue Systems"** focuses on efficiently removing specific data or knowledge from a trained model without requiring full retraining, which is crucial for data privacy compliance. 
% As this field is still in its early stages, we present a chronological overview of representative methods that have shaped the development of this area. 
One of the earliest approaches is fine-tuning, which exploits catastrophic forgetting by retraining the model on a retention dataset, though it may leave residual traces of forgotten data. This method forms the foundation for subsequent unlearning techniques. Building on this, Influence Functions**Haghifam et al., "Influence Function and Loss Gradient"** emerged, whiestimatetes the influence of individual data points on model parameters, providing a more precise and computationally efficient method for data removal without retraining the entire model. Later, more sophisticated methods are introduced. NegGrad+**Rebuffi et al., "Regularization, Dialogue Systems"** balances maximizing the loss on the forget set and minimizing the loss on the retention dataset, achieving a more controlled trade-off. SCRUB**Haghifam et al., "Influence Function and Loss Gradient"** refines this idea by using contrastive learning, where a student model mimics a teacher on retained data while contrasting its behavior on $D_{f}$. Other methods, such as influence functions**Papernot et al., "Practical Black-Box Attacks against Machine Learning"**, estimate the impact of data points on model parameters, offering precise yet computationally lightweight data removal. Techniques such as L1-Sparse**Haghifam et al., "Influence Function and Loss Gradient"** utilize $L_1$-regularization to zero out parameters associated with $D_{f}$, while relabeling methods, such as Saliency Unlearning**Papernot et al., "Practical Black-Box Attacks against Machine Learning"**, disrupt the model's memory of $D{f}$ by altering its labels and focusing on critical parameters.

% Unlike traditional unlearning methods, which focus on data removal at the individual level, our proposed framework introduces a new task that enables category-level forgetting. This framework integrates learning, retention, and forgetting into a unified system, allowing for more flexible and controlled knowledge management.

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.