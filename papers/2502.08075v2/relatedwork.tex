\section{Related Work}
\label{sec:re}


\subsection{Continual Learning}

% \textbf{Paper Deadline:} The deadline for paper submission that is
% advertised on the conference website is strict. If your full,
% anonymized, submission does not reach us on time, it will not be
% considered for publication. 


% Informal publications, such as technical
% reports or papers in workshop proceedings which do not appear in
% print, do not fall under these restrictions.
% Continual learning, also known as lifelong learning, aims to enable models to learn new tasks or knowledge incrementally while retaining performance on previously learned tasks. Techniques in this area typically address the catastrophic forgetting problem\cite{french1999catastrophic}, where learning new tasks leads to a degradation in performance on previously learned tasks. Existing approaches to continual learning can be broadly categorized into three types:Regularization-Based Methods\cite{kirkpatrick2017overcoming,liu2018rotate,dhar2019learning}, Replay-Based Methods\cite{caccia2020online,lopez2017gradient,hou2019learning}, Architecture-Based  Methods\cite{mallya2018piggyback,aljundi2017expert,hu2019overcoming}.

% Continual learning, also known as lifelong learning, aims to enable models to learn new tasks or knowledge incrementally while retaining performance on previously learned tasks. Techniques in this area typically address the catastrophic forgetting problem where learning new tasks leads to a degradation in performance on previously learned tasks. Existing approaches to continual learning can be broadly categorized into three types: Regularization-Based Methods, Memory-Based Methods and ,Architecture-Based Methods.

% Regularization-based methods introduce constraints in the loss function to retain past knowledge. EWC~\cite{kirkpatrick2017overcoming} estimates parameter importance using Fisher Information to limit critical weight updates. LwF~\cite{li2017learning} distills knowledge from old models to new ones for prediction consistency. PASS~\cite{zhu2021prototype} leverages prototype augmentation and self-supervised learning to maintain decision boundaries. PAR\cite{} dynamically selects parameter regularization for similar tasks and parameter allocation for dissimilar tasks based on task similarity measured by nearest-prototype distance.
% RGO\cite{liu2022continual} adjusts gradient updates using a projection matrix to minimize task interference, estimates past task losses via second-order Taylor expansion, and employs a Virtual Feature Encoding Layer to maintain task-specific representations without extra parameters.
% Memory-based methods maintains an external memory that stores or generates old data, features, or knowledge representations for retrieval during training. ICaRL~\cite{rebuffi2017icarl} selectively stores exemplars and employs a nearest-mean-of-exemplars classifier with knowledge distillation. CST~\cite{ashok2022class} introduces cross-space clustering for class-level feature structures and controlled transfer for inter-class knowledge regulation. %HAL~\cite{chaudhry2021using} optimizes anchor points through bilevel optimization.%
% MEMO\cite{zhou2022model} balances memory usage in class-incremental learning by selectively storing exemplars while expanding deep model layers, optimizing memory efficiency and knowledge retention without relying solely on replay-based methods.
% BMKP\cite{sun2023decoupling} introduces a bilevel memory framework that decouples learning and remembering, where a working memory adapts to new tasks and a long-term memory retains compact knowledge representations via knowledge projection.
% Architecture-based methods modify the model by expanding the network. Progressive Neural Networks~\cite{rusu2016progressive} allocate new network columns for each task with lateral connections. DyTox~\cite{douillard2022dytox} employs a transformer-based framework with dynamically expanding task-specific tokens and a shared encoder. FOSTER~\cite{wang2022foster} first expands the model using gradient boosting and then compresses it via knowledge distillation.
% ArchCraft\cite{lu2024revisiting} optimizes neural network architectures for continual learning by using neural architecture search (NAS) to balance stability and plasticity, generating wider and shallower networks that improve knowledge retention while reducing parameter overhead. 

% Our proposed method shares similarities with continual learning in its goal of balancing retention and learning. However,knowledgeeswappingg introduces an additional challenge of selective forgetting, which is not explicitly addressed in traditional continual learning approaches.


Continual learning, also known as lifelong learning, aims to enable models to learn new tasks incrementally while mitigating catastrophic forgetting. Existing approaches can be broadly categorized into Regularization-Based Methods~\cite{kirkpatrick2017overcoming,li2017learning,zhu2021prototype, liu2022continual, wang2023task}, Memory-Based Methods~\cite{rebuffi2017icarl,ashok2022class,chaudhry2021using,zhou2022model,sun2023decoupling}, and Architecture-Based Methods~\cite{rusu2016progressive,douillard2022dytox,wang2022foster,lu2024revisiting}.

Regularization-based methods introduce constraints in the loss function to retain past knowledge. For instance, EWC~\cite{kirkpatrick2017overcoming} distills knowledge from old models to new ones for prediction consistency. 
%RGO~\cite{liu2022continual} adjusts gradient updates using a projection matrix to minimize task interference and employs a Virtual Feature Encoding Layer to maintain task-specific representations without extra parameters.
Memory-based methods maintain an external memory to store or generate past knowledge. 
%ICaRL ~\cite{zhou2022model} balances memory usage by expanding deep model layers while selectively storing exemplars. 
BMKP~\cite{sun2023decoupling} introduces a bilevel memory framework, where a working memory adapts to new tasks and a long-term memory retains compact knowledge representations.
Architecture-based methods expand the model to accommodate new tasks. Progressive Neural Networks\cite{rusu2016progressive}
expands the model using gradient boosting and compresses it via knowledge distillation. ArchCraft~\cite{lu2024revisiting} leverages neural architecture search (NAS) to balance stability and plasticity, generating architectures that enhance knowledge retention with minimal parameter overhead.
Our proposed method shares the goal of balancing retention and learning with continual learning approaches. However, knowledge swapping introduces an additional challenge of selective forgetting, which is not explicitly addressed in traditional continual learning methods.

\subsection{Machine Unlearning}

Machine unlearning~\cite{koh2017understanding, kurmanji2024towards, liu2024model} focuses on efficiently removing specific data or knowledge from a trained model without requiring full retraining, which is crucial for data privacy compliance. 
% As this field is still in its early stages, we present a chronological overview of representative methods that have shaped the development of this area. 
One of the earliest approaches is fine-tuning, which exploits catastrophic forgetting by retraining the model on a retention dataset, though it may leave residual traces of forgotten data. This method forms the foundation for subsequent unlearning techniques. Building on this, Influence Functions~\cite{koh2017understanding} emerged, whiestimatetes the influence of individual data points on model parameters, providing a more precise and computationally efficient method for data removal without retraining the entire model. Later, more sophisticated methods are introduced. NegGrad+~\cite{kurmanji2024towards} balances the loss on the forgotten set and the retention dataset, offering a more controlled trade-off in the unlearning process. 
% Following this, SCRUB~\cite{kurmanji2024towards} further advances the field by incorporating contrastive learning, where a student model mimics the behavior of a teacher model on retained data while contrasting its output on the forgotten data, allowing for a more targeted approach to unlearning. 
To further refine the removal of specific knowledge, L1-Sparse~\cite{liu2024model} introduces the use of L1-regularization to zero out parameters associated with forgotten data, effectively eliminating their influence on the model. Additionally, Relabeling techniques, such as Saliency Unlearning~\cite{feldman2020does} disrupts the modelâ€™s memory of forgotten data by altering its labels, focusing on modifying key parameters that store the data's influence. Unlike conventional unlearning methods, which primarily focus on individual data points, our proposed framework introduces a novel approach for category-level forgetting. By integrating the processes of learning, retention, and forgetting into a unified system, this framework offers more flexibility and control over knowledge management, marking a significant advancement in this area of research.
\begin{figure*}[!htb]
    \centering
    \subfigure[\small Learning Before Forgetting ($L \rightarrow F$)]{
        \includegraphics[width=0.85\linewidth, trim=0mm 0mm 0mm 0mm, clip]{assets/fltf-combine-l2nrom_0126.pdf}
    } \\
    \vspace{-3mm}
    \subfigure[\small Learning After Forgetting ($F \rightarrow L$)]{
        \includegraphics[width=0.85\linewidth]{assets/fftl-combine-l2nrom_0126.pdf}
    }
    \vspace{-4mm}
    \caption{\small \textbf{$\mathcal{L}_2$ norm for each parameter under $L\rightarrow F$ and $F\rightarrow L$.} The superscript $W$ denotes the weight norm value at the current stage. The figure illustrates that (a) during the \textit{Learning Before Forgetting} phase, changes in parameter norms are predominantly concentrated in layers responsible for high-level semantic representations. Conversely, (b) in the \textit{Learning After Forgetting} phase, parameter norm changes primarily occur in layers associated with low-level feature representations.
    % Each group of LoRA parameters corresponds to a Transformer block, with indices 0-23 representing encoder LoRA parameters and indices 24-39 representing decoder LoRA parameters. In Figure (a), the L2 norm of each group of LoRA parameters is computed at the initial time, after learning, and after forgetting. Then, the absolute difference between the L2 norm after forgetting and the L2 norm after learning is calculated to represent the parameter changes induced by the forgetting process, which is shown by the green line. In Figure (b), the L2 norm of each group of LoRA parameters is computed at the initial time, after forgetting, and after learning. Then, the absolute difference between the L2 norm after forgetting and the L2 norm after learning is calculated to represent the parameter changes induced by the forgetting process, which is shown by the red line.
    }
    \vspace{-5mm}
    \label{fig:l2norm-seg}
\end{figure*}
% Machine unlearning focuses on efficiently removing specific data or knowledge from a trained model without retraining it from scratch, making it highly relevant for data privacy compliance (e.g., GDPR). Methods in this area can be broadly categorized as follows:
% Fine-tuning exploits catastrophic forgetting by training the model on the retention dataset $D_{r}$, though it may leave residual traces of the forgotten set $D_{f}$. NegGrad+\cite{kurmanji2024towards} balances maximizing the loss on the forget set and minimizing the loss on the retention dataset, achieving a more controlled trade-off. SCRUB\cite{kurmanji2024towards} refines this idea by using contrastive learning, where a student model mimics a teacher on retained data while contrasting its behavior on $D_{f}$. Other methods, such as influence functions\cite{koh2017understanding}, estimate the impact of data points on model parameters, offering precise yet computationally lightweight data removal. Techniques such as L1-Sparse\cite{liu2024model} utilize $L_1$-regularization to zero out parameters associated with $D_{f}$, while relabeling methods, such as Saliency Unlearning\cite{feldman2020does}, disrupt the model's memory of $D{f}$ by altering its labels and focusing on critical parameters.

% Unlike traditional unlearning methods, which focus on data removal at the individual level, our proposed framework introduces a new task that enables category-level forgetting. This framework integrates learning, retention, and forgetting into a unified system, allowing for more flexible and controlled knowledge management.

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.