\subsection{RQ1: How do heavy LLM users integrate LLMs into their everyday decision-making practices?}
Our first research question explored how heavy users incorporate LLMs into their decision-making processes. Through screening forms and interviews, we discovered diverse applications ranging from quick validations to complex relationship advice. The most common use cases included validation for social appropriateness and context understanding, self-regulation in purchase decisions, and interpersonal guidance.

\subsubsection{Case 1: Social Appropriateness and Context Understanding}
Participants used LLMs to navigate social situations where they felt uncertain about appropriate behavior. This pattern spanned from immediate communication decisions to long-term social planning. Participants primarily sought to align their actions with social norms, especially when they lacked confidence in their social judgment.

Six participants (P2-P7) used LLMs to validate their responses during time-sensitive situations like answering professors' questions or contributing to meetings. Rather than using search engines, they quickly shared context, for instance by giving the class material itself, and proposed their responses with LLMs to assess appropriateness. As P6 explained, they wanted to ensure responses "made sense in the provided context." This validation either boosted their confidence to respond or helped them avoid potentially awkward or disruptive contributions. Participants particularly valued avoiding responses that might interrupt the meeting flow with contextually inappropriate or "wrong" comments.

Participants also sought LLM guidance for context-appropriate attire. P2, new to academia, consulted ChatGPT about conference dress codes, specifically seeking validation for her preference of wearing jeans. P3 used LLMs to help choose appropriate attire for meeting a former partner. In both scenarios, participants used LLMs to ensure their choices aligned with social expectations.

P3, who self-identified as having lower social awareness, regularly used LLMs to navigate everyday social situations. When selecting gifts, he found value in ChatGPT's specific follow-up questions about the recipient, such as "whether this person usually wears gold or silver." Similarly, when deciding between wine or champagne for a party, he appreciated how LLMs considered multiple factors including the event's purpose, intended mood, and the social implications of each beverage choice. He noted that these interactions helped him recognize the complex factors involved in social decisions.

\subsubsection{Case 2: Self-Regulation in Purchase Decisions}
Two participants (P2, P7) employed LLMs as a tool for self-regulation during impulse purchases, though with divergent expectations and outcomes. P2 used LLM feedback to curb impulsive buying, stating, "I knew buying this item was impulsive and irrational, and just needed to hear it out from another being." This led to her avoiding the purchase. P7, however, leveraged LLMs to justify his impulse purchases, noting that "LLMs such as ChatGPT have a tendency of saying what the audience wants to hear." He deliberately chose LLMs over friends for purchase validation, knowing they would help rationalize his impulse buying decisions.

\subsubsection{Case 3: Interpersonal Guidance through LLMs}
Five participants (P2, P3, P4, P6, P7) sought LLM assistance for interpersonal challenges, though their approaches and expectations varied. While P6 used LLMs primarily as an emotional outlet during a conflict, others engaged in deeper social problem-solving. P2 and P3 used LLMs for self-reflection, asking questions like "Am I being too sensitive?". P3 described a particularly striking experience where ChatGPT analyzed his past conflict with an advisor. While the LLM validated some of his concerns about the situation, it simultaneously challenged him by pointing out how his confrontational response to the professor conflicted with traditional conservative ethical principles in Korean academic contexts - a dual perspective that he found profoundly unexpected. P4 developed a particularly significant relationship with ChatGPT, using it as a confidant for relationship advice. She valued its ability to help her understand her boyfriend's perspective, interpret text messages, and craft responses. P4 specifically appreciated the balance between neutral advice and emotional support, noting that the LLM could maintain objectivity while remaining sympathetic to her situation. Participants preferred LLMs over human advisors for these personal matters, citing the absence of social pressure and judgment. They noted that LLMs offered comprehensive social guidance while considering individual context, without the self-consciousness associated with seeking human advice.

\subsection{RQ2: What underlying needs do heavy LLM users seek to fulfill through LLM assistance?}
Our analysis implied three fundamental needs driving users' LLM engagement: boosting decision-making confidence, validating choices, and improving cognitive efficiency by delegating challenging tasks.

\subsubsection{Gaining Self-Confidence through Validation}
Participants often consulted LLMs not to make decisions, but to gain confidence in choices they were already inclined to make. P2 and P4 sought confirmation for preliminary decisions across various situations, including outfit choices, housing options, and course schedules. Rather than seeking help with the initial decision-making process, they would present their analysis of options to the LLM, seeking reassurance to overcome their uncertainty. This pattern of seeking reassurance extended to impulse purchases, where P2 and P7 approached LLMs with specific expectations. As both noted, "I just wanted to hear those words from someone"---revealing how users specifically sought out either permission or restraint from LLMs to support their predetermined decisions.

\subsubsection{Finding the "Right" Answer}
Participants utilized LLMs to consider as many options and diverse sources of information to reach optimal decisions, and to check if their thoughts were the "right" and "optimal" answer. For instance, participants would validate their ideas before meetings or use LLMs to organize options for trip planning and gift buying. They believed LLMs would consider comprehensive contextual information to provide the "right" answer. This aligned with participants' views of ideal decision-making - to consider as many options as possible (P1, P4, P5, P6). Participants leveraged LLMs to strengthen this approach by having the models organize different options, though some (P5) expressed concern that their decision-making might be confined to LLM-suggested options, potentially missing other alternatives.

\subsubsection{Optimizing Cognitive Resources through Task Delegation}
Participants completely delegated tasks they saw as not worth their mental effort to LLMs, seeking to free themselves from such responsibilities while preserving their cognitive energy for more valuable activities. The definition of "lower priority" varied among users in notably subjective ways. P6, a graduate student, chose to delegate course assignments to LLMs while engaging deeply with research work, despite both being essential academic responsibilities. P7 outsourced travel planning but enthusiastically handled laptop selection personally, citing a personal interest in electronics despite both tasks being similar to consumer decisions. P3, another graduate student, delegated lab-related social interactions while actively engaging in research ideation, showing how even within the same professional context, participants made highly personal choices about which tasks to delegate. In these cases, participants accepted and implemented LLM suggestions without seeking additional information or engaging in further analysis.

P6, a PhD student with a demanding schedule who described himself as "lacking energy," noted that he had developed a task-ranking system to identify which activities could be fully delegated to LLMs, requiring minimal personal mental effort. He prioritized delegating administrative duties and course assignments to LLMs in order to preserve his cognitive resources for higher-priority research work. This strategy emphasized maintaining cognitive efficiency for important tasks, even if it meant accepting lower-quality outcomes for less critical activities. Similarly, P3 applied this approach to social decisions, seeking satisfactory results while minimizing mental effort. As P6 explained, this deliberate trade-off between cognitive efficiency and output quality was specifically designed for lower-priority tasks.

\subsection{RQ3: How do heavy LLM users conceptualize and evaluate their relationship with LLMs?}
We examined users' mental models of LLMs and how these perceptions influenced their decision-making delegation patterns. Our analysis explored both their current usage patterns and their anticipated future reliance on LLMs for decision support.

\subsubsection{Mental Models of LLMs}
\paragraph{LLMs as Rational and Consistent Beings}
Participants predominantly conceptualized LLMs as consistent and decisive entities. P2 exemplified this by delegating complex decisions, such as course selection and housing choices, to LLMs. Unlike cases where participants used LLMs to compare options, P2 delegated entire decisions to LLMs when she struggled with decisiveness or questioned the optimality of her preferences. She valued ChatGPT's "perseverance" and rational nature, contrasting it with humans who might give inconsistent advice based on timing or previous interactions. Similarly, P4, who integrated LLMs into various aspects of her life, appreciated their dual capability of providing both rational guidance and perspective-aligned advice. She particularly valued that ChatGPT could offer both objective analysis and personally tailored guidance.

\paragraph{LLMs as Average Decision-Makers}
A contrasting mental model emerged where participants (P3, P7) viewed LLMs as "average and general decision-makers." P3, who felt he had below-average social understanding, relied on LLMs for social decisions, believing their training on vast data would provide at least average human-level responses. P7 valued this "averageness" for its guaranteed minimal competence with minimal time investment. However, this perceived averageness became a limitation for tasks requiring creativity and depth, such as research ideation or CV writing (P3, P5, P6, P7). Participants described LLM outputs as "exemplary," "general," and "uniform," leading them to seek alternative approaches for creative tasks. Some turned to colleagues (P3), others avoided LLM influence (P6), while P7 developed a strategy of asking LLMs to pose questions before starting tasks, optimizing their capabilities within recognized limitations.

\subsubsection{Self-Reflection on LLM Usage}
Participants demonstrated awareness of their extensive LLM use, often viewing it critically upon reflection during interviews. Their concerns spanned both societal and personal dimensions. On a personal level, some participants (P5, P6) worried about a potential decline in their creative and problem-solving abilities. P2 described consciously setting limits on which suggestions to follow. She noted that while ChatGPT provided the decisions, she ultimately bore the responsibility, expressing that it felt like "her boundaries were being invaded" and emphasizing that "I should be responsible." P3 expressed particular concern about the potential erosion of human agency, especially for ethical issues: "To maintain our humanity and human nature, we need to constantly verify and struggle with our own humanness. But because I find it tiresome and troublesome, I end up letting machines do it for me. If this happens on a real social scale, it's no joke." 

Despite these concerns, when asked about potentially reducing their LLM usage, participants indicated plans to maintain or expand their use, including diversifying usage across different domains. As P6 explained, while being worried about diminishing problem-solving skills, he also noted, "The problem-solving skills of LLMs will develop at a much faster speed than my own, so it would be better to try to figure out how to use LLMs better, which is another form of problem-solving." P7 also expressed a more optimistic view, stating, "Unconditionally accepting everything the LLM says isn't good, and it won't help us get better at making decisions. That's why I try to find different ways to use LLMs, like asking them to ask me questions that make me think more deeply. I think this kind of approach shows how we can use LLMs better in the future."