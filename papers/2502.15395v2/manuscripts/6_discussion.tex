%% New Disucssion 
Our study reveals how heavy users integrate LLMs into their daily tasks through distinct patterns. Rather than simple tool usage, participants demonstrated sophisticated cognitive offloading strategies that transformed their decision-making processes. In our study, we observed participants delegating social and interpersonal reasoning to LLMs, suggesting ways users might leverage AI collaboration to support their social cognition processes.

Participants' mental models of LLMs directly influenced their cognitive strategies---those viewing LLMs as rational entities engaged in cognitive complementarity by leveraging LLM capabilities where they perceived personal limitations, while those viewing LLMs as average decision-makers used cognitive benchmarking, establishing baseline standards while reserving higher-order tasks for themselves.
% While delegating a broad range of decisions raised potential concerns about over-reliance and diminished critical thinking, our findings also highlight a nuanced form of human-AI collaboration where users and LLMs develop complementary relationships. Participants showed diverse usage strategies, treating LLMs as an emerging problem-solving tool and developing sophisticated prompting techniques. Most notably, participants frequently sought LLM guidance on social appropriateness and interpersonal situations. Although some users expressed concerns about potential skill degradation and a sense of unease, LLM consultations often led to a more thorough consideration of social factors and an enhanced understanding of different perspectives.

This raises questions for future research on redefining how we conceptualize and measure over-reliance on LLMs. Current metrics typically assess over-reliance through simplified quantitative measures in controlled settings, primarily focusing on users' acceptance rates of LLM outputs ~\cite{bo2024rely, kim2024rely}. However, our findings reveal more complex patterns of engagement. Participants did not blindly adopt LLM outputs, even in cases where they eventually accepted them. Instead, participants demonstrated thoughtful delegation strategies, using LLMs to validate existing decisions, automate routine tasks, or navigate unfamiliar situations. The critical concern was not users' acceptance of LLM outputs, but rather instances where users adopted LLM reasoning without exploring alternative perspectives. Future research should expand the definition of over-reliance beyond simple acceptance rates to examine how users critically engage with alternative lines of reasoning.

Another key direction for future research involves capturing diverse user contexts. Our participants valued the ability of LLMs to extract necessary contextual information when not initially provided. They appreciated that they could receive meaningful responses without extensively explaining background information, even for context-heavy topics like relationship advice. Future research should explore ways to incorporate multi-modal inputs beyond text-based interactions, allowing users to convey context through various channels. Additionally, LLMs' ability to elicit implicit user intentions without explicit prompting is crucial, as demonstrated by recent advances in reasoning-focused LLM architectures that can proactively identify and address underlying user needs.

The development of active usage patterns with LLMs appeared more prominent among younger users who had less experience managing tasks without these systems. Participants with extensive pre-LLM experience maintained clearer boundaries and showed greater awareness of system limitations. In contrast, users with less experience with LLMs demonstrated fewer reservations, viewing LLM interaction itself as a skill and actively developing their prompting strategies. Conducting design studies focused on younger generations, to better understand and support these emerging interaction patterns represents a crucial direction for future research.