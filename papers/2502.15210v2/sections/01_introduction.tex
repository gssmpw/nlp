\vspace{-2pt}
\section{Introduction}
\vspace{-2pt}

\input{imgs/fig1}

Vision language models (\modelss{}) have progressed to the point of having impressive performance on a wide array of tasks \cite{achiam2023gpt,laurenccon2024matters,  reid2024gemini, abdin2024phi, wang2024enhancing,llama3}
%jiang2024mantis, liu2024improved, liu2024llavanext, abdin2024phi, wang2024enhancing, chen2024expanding, llama3}. 
These tasks range from summarization, visual question answering, image captioning, common sense reasoning question answering \cite{kembhavi2016diagram, johnson2017clevr, zellers2019recognition, lu2023mathvista, chen2024we, liu2025mmbench} 
%to common sense reasoning \cite{johnson2017clevr,zellers2019recognition}.  
Due to the complexity of some of these tasks, performance evaluation requires understanding and interpreting model outputs and matching them with human annotations. 
However, human evaluation is expensive, time-consuming, and not scalable. It can also be inaccurate, as human annotations often have high variance~\citep{liu2019user, knox2024models} due to variations in the perception of quality among annotators~\citep{feng2024sample}. 
%substantial variance in annotator judgments
In practice, assessments of \modelss{} on these tasks often involve the use of more powerful language models, e.g., GPT4o, that serve as evaluators for interpreting predictions or outcomes \cite{manas2024improving, liu2024visual, liu2025mmbench}.


Using \modelss{} as judges relies on the critical ability to compare data instances, i.e., reliably assessing their similarity. 
For example, in numerous tasks, \model{} evaluators should determine the degree of relevance between one or multiple text pairs or image-text pairs, e.g., between model outputs and natural examples~\citep{liu2025mmbench}. 
Nevertheless, all data comparisons heavily depend on the \model{} instruction following capability and understanding of the context provided in the prompt, and reliable assessment of similarities. 

Our work explores the extent to which state-of-the-art \modelss{} can act as effective ``similarity kernels'' by analyzing their outputs when tasked with comparing controlled samples.
We do so by measuring the following key properties on control samples: 
alignment between predicted similarity scores and ground truth relevance of the pairs of data points; invariance to the data order, i.e., symmetry ($sim(a, b)=sim(b,a)$); smoothness/nuance of the score distribution; and finally controllability in terms of how sensitive or invariant they can be made to properties in the data via instructions in the prompt.
For instance, as seen in Figure \ref{fig:fig1}, commonly used \model{} judges, i.e., \gptFouroEleven{} and \geminiPro{}, may not be symmetric in some instances and may not even follow the prompt properly.

To assess these desiderata, we introduce \mmscore{}, a framework consisting of a suite of metrics and data-pair generation techniques for evaluating the ability of \modelss{} to be reliable similarity estimators, without depending on expensive and high variance expert validation.
% We formulate these properties in our proposed framework, \mmscore{}, that given a dataset, can evaluate the ability of models to compare the data points of the dataset 
% with no additional expert annotations while controlling for different features and variations in data points. 
More specifically, we formalize the metrics \nmi, \relaxsym, \textbf{Smoothness}, and \textbf{Controllability} 
% as metrics that \mmscore measures for a model that acts as a similarity kernel on a given dataset. 
% Based on the original dataset, our framework
and propose transformation-based techniques for creating datasets of synthetic paired images, where each pair is different in one or multiple features. 
This allows us to control the type and degree of dissimilarity between data points inputted to the models and hence, directly examining biases, i.e., which features the \model{} struggles or succeeds at detecting their variation.

% \textcolor{red}{Spandana:No need to give all subset details of \mmscore\  here}

% We first instantiate \mmscore with data points from \imagenet, \coco, and \wu{} and create \mmscorein, \mmscorecoco, \mmscorewuimgimg, and \mmscorewuimgtext{} and create multiple variations, e.g., color jittering, between data points. Our instance of \mmscore{} overall consists of \mmscorecomparesize{} data pairs  consisting of image-image and image-text pairs, in which one or multiple controlled features differ between pairs of images. We release both our framework \mmscore{} and our instances of it \ref{here}. 

Finally, we leverage multiple instances of \mmscore{} to conduct a large-scale investigation covering numerous proprietary and open-source \modelss{}. 
Our analysis demonstrate that no model is superior in all four proposed metrics and their superiority highly depends on the task or data at hand. 
Furthermore, we show that although \mmscore{} focuses on the simple task of synthetic data pair comparison at a low cost compared to large-scale benchmarks, it is highly predictive of \modelss{}' performance on well-known benchmarks with other diverse tasks \cite{yue2024mmmu, lu2023mathvista, chen2024we, guan2024hallusionbench, liu2024ocrbench, kembhavi2016diagram}. 
In other words, measuring metrics in \mmscore{}, e.g., \nmi or \relaxsym,  is a cost-effective alternative to extensive and resource-intensive benchmarks for ranking models or performing cross-validation during training.

%is a cheap surrogate to expensive and large benchmarks for ranking models or performing cross-validation during training.


Our contributions are as follows:
\vspace{-4mm}
\begin{itemize}
    \item We propose \mmscore{}, a framework for evaluating \modelss{} as similarity kernels,  which does not require additional expert annotations and is cheap to instantiate.
    \item We further create and release four instantiations of \mmscore{}, \mmscorein, \mmscorecoco, \mmscorewuimgimg, and \mmscorewuimgtext, which consist of \mmscorecomparesize{} data pairs for comparisons.
    \item We carry out a broad benchmarking of various closed- and open-source \modelss{} on the different configurations within \mmscorein, \mmscorecoco, \mmscorewuimgimg, and \mmscorewuimgtext{} to show how models differ and give insight into to what extent they can be trusted to act as auto evaluators on image-image and image-text data pairs,
    \item Lastly, we report the correlations of \mmscore{} with popular benchmarks and show the formulated properties in \mmscore, have predictive power of benchmark performance, and can act as a low-cost surrogate during training or validation of \modelss{}.
\end{itemize}