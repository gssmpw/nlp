\begin{abstract}
As large vision language models (\modelss{}) are increasingly used as automated evaluators, understanding their ability to effectively compare data pairs as instructed in the prompt becomes essential.
To address this, we present \mmscore{}, a low-cost framework that systematically evaluates \modelss{} as customizable similarity tools across various modalities and scenarios. 
Through \mmscore, we introduce four metrics that represent key desiderata of similarity scores: alignment with human annotations, consistency for data pairs irrespective of their order, smoothness of similarity distributions, and controllability through prompting.
Our analysis demonstrates that no model, whether closed- or open-source, is superior on all metrics; the optimal choice depends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp judge), highlighting risks of widespread adoption of \modelss{} as evaluators without thorough assessment. For instance, the majority of \modelss{} struggle with maintaining symmetric similarity scores regardless of order.
Additionally, our results show that the  performance of \modelss{} on the metrics in \mmscore{} closely correlates with popular benchmarks, showcasing its predictive power in ranking models.

\end{abstract}

% Did a minor change in the caption of figure 1 so things would fit more nicely as before. That's it from me, no more changes from my side :)
% Thanks so much! I think the change definitely added a lot of value! Great addition! Thanks!

%  I think this looks much better now! Yup, this is on point :)
% Great! Now the only issue is that the abstract got more than before and it went over 8 pages, smh. I'll see if I can do vspace stuff in the meanwhile
% I see, let's cut it shorter
%  do we need to mention MMMU in your opinion? If not now it's good
% I think we're god now!
% AMAZING!
%  I think this is perfect! Should we submit? Let's goo!!
% Thank you so much Joao! Super suuuper helpful and amazing chatting here as well, lol
%Lol yeah this is a first for me
% hahaa, I'm going to submit and sleep now! :) 
%awesome, have a nice weekend, and great job!! Fingers crossed now for decent reviewers
% Thank you you too! Have a great rest of the day and weekend and fingers crossed!
% a model's performance varies depending on why it is used, highlighting the limitations of widespread
% the optimal choice depends on which aspect of similarity is most important to the user. 
% which ever you think makes more sense, haha
% yours is great!
%  got help from gpt, haha
% Lol yeah I'd say let's go with that :)
% ok, how does this look:
% which aspect of similarity is a bit vague
% , but no big deal
% maybe which metric of similarity? To kind of 
% I was thinking on something alluding to the behaviour of the auto evaluator
% a harsh one vs a sharp one for instance
% but yeah this is very minor haha
% lol, hmm 

% a model's choice to define an auto evaluator should depend on its desired behaviour (e.g., a smooth vs. a sharp judge), varies depending on why it is used, highlighting the limitations of widespread
% I think your version sounds much muuch nicer and to the point!
% let me try to use it then