\input{imgs/mmscore-examples}
\vspace{-2mm}
\section{\mmscore{}}
\subsection{Dataset Creation}
The \mmscore{} framework takes in a source dataset and creates augmented versions of the data to obtain data pairs to probe the evaluation skills of a model. 
In our instances, we use \coco{} \cite{lin2014microsoft}, \imagenet{} \cite{deng2009imagenet} and \wu{} \cite{kamath2023s} datasets as the source for the original data points. We utilize \coco{} and \imagenet{} as image-only datasets and \wu{} as an image-text dataset. We select 500 random images from each of \coco{} and \imagenet{} and all the image-text pairs from both subsets provided by the \wu{} dataset to be used in our instantiation of \mmscore. Full details of our released datasets are given in Table \ref{tab:mmscore-info}.

% Considering that the \mmscore{} framework aims to measure how well \modelss{} can detect different features and score them between data points, 
To isolate the effect of different data characteristics on model performance,
\mmscore{} creates pairs of image-image and image-text data that are identical except for one or a few controlled features. The generated data consists of points from the original dataset paired with their transformed version.
% these image-image pairs by using all three source datasets and construct the image-text pairs using the two subsets of \wu{}. 
For \coco{} and \imagenet, we create a different control sample for each one of the transformations in $\{$color jitter, rotation, gaussian blur, perspective shift, elastic transformation$\}$, which defines the characteristic that differs between images. For the data from \wu{}, we construct the data pairs by either only using the `spatial position' transform, or `spatial position' transform in addition to one of the previous five characteristics to additionally assess coupling effects. 
However, since transforms are not well-defined for texts, only `spatial position' transform is applied for the image-text pairs. Note that the image-image pairs from \wu{} are the most challenging since they all have at least the `spatial position' transform, which is a well-known blind-spot of \modelss{} as shown by previous literature \cite{kamath2023s, wang2024picture}. As a result, we end up creating five image-image sub-datasets for each of \coco{} and \imagenet, six subsets for each of the two subsets of \wu{}, using each of the transformations, and one image-text sub-dataset for each of the subsets of \wu{}. The details of the transforms applied to each category are shown in Figure \ref{fig:mmscore-examples}.

Next, for each original image, we construct three types of pairs: an identical, a transformed, and an irrelevant pair. In all three versions of these pairs, the first data point is the original (non-transformed) image. For the `identical' pair, the second data point is another version of the image with $95\%$ of its original size for the image-image pair and the correct caption for the image-text pair. The second data point in the `transformed' pair is the original image (caption) with the transformation applied to it for the image-image (image-text) pair. Finally, the `irrelevant' pair's second data point is a transformed version of a random image (caption) from the rest of the dataset. 

Equipped with the constructed control samples, \mmscore{} prompts the \model{} to score the similarity of each data pair based on a set of criteria. 
The criteria consists of the conditions indicating whether the model under examination should be `sensitive' or `invariant' to the transformations applied for that specific sub-dataset. These two settings (sensitive or invariant) measure how well each model can recognize the differences between the data pair and follow the prompt's criteria. If a model can successfully capture a specific feature, it will have no problem being variant or invariant to it; however, if it cannot detect it or has a bias towards a feature, it will favor being sensitive or invariant to that feature over its opposite. On a scale of 1 to 10, we consider the ground-truth score of the `identical' and `irrelevant' pair 10 and 1, respectively, in both `sensitive' and `invariant' settings. However, for the `transformed' pair, we consider the score 10 in the `invariant' version, and `8' in the `sensitive' version of the prompt. 
To make sure the performance gap between models is not merely a consequence of biased prompt wording, \mmscore{} comes with five template prompts with different lengths and wordings but with the same semantic meaning, that are randomly selected for each data pair, to make sure the prompting does not affect the model's performance. These prompt templates are reported in Appendix \ref{sec:prompt-templates}.

Ultimately, we end up with 4 different datasets created by \mmscore: \mmscorecoco{}, \mmscorein{}, \mmscorewuimgimg{}, and \mmscorewuimgtext{}. \mmscorecoco{} and \mmscorein{} compare and score image-pairs and have 5 splits (Color Jitter (CJ), Rotation (R), Gaussian Blur (GB), Perspective Shift (PS), and Elastic Transformation (ET). \mmscorewuimgimg{} consists of 2 subsets, each with 6 splits; one split with only the Spatial Position transform (SP), and the rest with SP combined with one of the previous five transformations (CJ, R, GB, PS, and ET). \mmscorewuimgtext{} consists of only the SP split for each of the two subsets in the \wu{} dataset. Details of each split in Appendix \ref{sec:mmscore-info}.

\subsection{Metrics}
To measure the reliability of \modelss{} in scoring data pairs, we define four metrics that we measure across datasets and models: \nmi, \relaxsym, Smoothness (\smoothness), and Controllability (\control). 

% We follow this notation to formulate the metrics: we denote the \model{} being evaluated as $\mathcal{M}$ and the prompt as $P_{C}$ where $C \in \left\{\texttt{sens}, \texttt{inv} \right\}$, as sensitive or invariant. Finally, given a dataset $\mathcal{D}_N = \{(d_1, d_2), (d_3, d_4), \dots, (d_{2N-1}, d_{2N}))\}$, we denote the similarity score of a data pair $(d_i, d_j) \in \mathcal{D}_N$ returned by an \model{} ($\mathcal{M}$) and for a given prompt ($C$) as:
% % $$s_{\mathcal{M}}^{C}(d_i, d_j) \vcentcolon= \mathcal{M}(C, d_i, d_j),$$
% $$s_{\mathcal{M}}^{C}(d_i, d_j) \vcentcolon= \mathcal{M}(C, d_i, d_j),$$

We adopt the following notation to formulate the metrics: we denote the \model{} being evaluated as $\mathcal{M}$ and the condition, which determines if the prompt instructs the model to be sensitive or invariant to a visual feature, as $C \in \left\{\texttt{sens}, \texttt{inv} \right\}$. Finally, given a dataset $\mathcal{D}_N = \{(d_1, d_2), (d_3, d_4), \dots, (d_{2N-1}, d_{2N}))\}$, we denote the similarity score of a data pair $(d_i, d_j) \in \mathcal{D}_N$ returned by an \model{} ($\mathcal{M}$) for a given condition ($C$) as:
% $$s_{\mathcal{M}}^{C}(d_i, d_j) \vcentcolon= \mathcal{M}(C, d_i, d_j),$$
$$s_{\mathcal{M}}^{C}(d_i, d_j) \vcentcolon= \mathcal{M}(C, d_i, d_j),$$
where $(d_i, d_j)$ could be an image-image or image-text pair. Note that we instruct the model to generate the output in a structured format to make sure the predicted score is parsable from the model output. If $s_{\mathcal{M}}^{C}(d_i, d_j)$ is valid, it would fall in the set
% $\mathcal{V} = [1, 2, 3, \dots, 10\}$. 
$\mathcal{V} = [1, 10]$. 
However, models often do not consistently follow the details of the prompt and may produce scores not in $\mathcal{V}$ or outputs that do not satisfy the output format, in which case we set $s_{\mathcal{M}}^{C}(d_i, d_j) = -1$. 

Finally, to evaluate a model $\mathcal{M}$ on $\mathcal{D}_N$ given condition $C$, we create and annotate the set of all its outputs as:
$$S_{\mathcal{M}}^{C}(\mathcal{D}_N) = \left\{ s_{\mathcal{M}}^{C}(d_i, d_j) \,\middle|\, (d_i, d_j) \in \mathcal{D}_N \cup \text{rev}(\mathcal{D}_N) \right\},
$$
where $\text{rev}(\mathcal{D}_N) = \{(d_2, d_1), (d_4, d_3), \dots, (d_{2N}, d_{2N-1}))\}$ are the data pairs in reverse order.



\subsubsection{\nmi{}}
We consider the normalized mutual information (\nmi{}) between the predicted scores and the ground-truth ones as the main metric of \mmscore{}. Instead of accuracy or squared error metric, we consider \nmi{} since we do not explicitly prompt the \model{} with examples of the correct scores and hence, cannot expect it to predict them directly. 
\nmi{} is better suited for \mmscore{} as it focuses on whether the \model{}'s scores are predictive of the ground-truth ones without penalizing outputs that do not exactly match them. 
The better a model can reproduce the variance in the ground-truth score, the better it is able to recognize that characteristic. Hence we write,
% \vspace{-1mm}
\[
\nmi(\mathcal{M}, C, \mathcal{D}_N) = \text{NMI}(S_{\mathcal{M}}^{C}(\mathcal{D}_N), GT_C(\mathcal{D}_N)),
\]

where $\text{NMI}(.,.)$ is the normalized mutual information and $GT_C(.)$ is the ground truth of the input dataset considering the condition of $C$.



% \subsubsection{\softsymmetrytitle (\normsym)}
\vspace{-1mm}
\subsubsection{\relaxsym{}}

When leveraging \modelss{} as similarity kernels or auto evaluators, a fundamental characteristic one would expect is their symmetry as a kernel.  Surprisingly, however, we found that most models do not satisfy exact symmetry, i.e., the equality of $sim(a, b)$ and $sim(b, a)$. We thus introduce \relaxsym, which tolerates a difference of $\varepsilon$ between the scores that should be equal. More specifically, to analyze the symmetry of \modelss{} on a dataset $\mathcal{D}_N$, we compute the \relaxsym{} of $\mathcal(M)$ on $\mathcal{D_N}$:

% $$\text{\normsym}(\mathcal{M}, \mathcal{D}_N) = 1 - \frac{1}{N}\sum_{(d_i, d_j) \in \mathcal{D}_N}\text{Diff}(\mathcal{M}, d_i, d_j),$$
\resizebox{\linewidth}{!}{$
\text{\relaxsym}(\mathcal{M}, \mathcal{D}_N) = \frac{1}{N}\sum_{(d_i, d_j) \in \mathcal{D}_N}\text{SoftEq}_\varepsilon(\mathcal{M}, d_i, d_j),
$}

where $\text{SoftEq}_\varepsilon(\mathcal{M}, d_i, d_j)$ is defined as:
$$
% \text{Diff}(\mathcal{M}, d_i, d_j)=
\text{SoftEq}_\varepsilon(\mathcal{M}, d_i, d_j)=
$$
\resizebox{\linewidth}{!}{$
% Diff(\mathcal{M}, d_i, d_j) &=
\begin{cases} 
    % \frac{\lvert s_{\mathcal{M}}^{P_.}(d_i, d_j) - s_{\mathcal{M}}^{P_.}(d_j, d_i)\rvert}{9}, & s_{\mathcal{M}}^{P_.}(d_i, d_j), s_{\mathcal{M}}^{P_.}(d_j, d_i) \in \mathcal{V}, \\
    % 1, & \text{otherwise}.
    \mathbbm{1}(\lvert s_{\mathcal{M}}^{C}(d_i, d_j)-s_{\mathcal{M}}^{C}(d_j, d_i) \le \varepsilon \rvert), & s_{\mathcal{M}}^{C}(d_i, d_j), s_{\mathcal{M}}^{C}(d_j, d_i) \in \mathcal{V}, \\
    0, & \text{otherwise}.
\end{cases}
$}


In the continuation of this paper, we set $\varepsilon = 1$ and provide ablation studies in Figure \ref{fig:diff-relax-sym-eps} in the Appendix.

\vspace{-2mm}
\subsubsection{\textbf{Smoothness}}

We aim to measure how smooth kernels induced by \modelss{} are. For instance, a non-smooth kernel would be such that pairs are either exactly the same or completely different, while a smoother kernel is more nuanced. We measure for smoothness via the diversity of the predicted scores. Given $S^{C}_{\mathcal{M}}$, smoothness (\smoothness) is computed as:
% \begin{align}
$$\smoothness(\mathcal{M}, \mathcal{D}_N, C) = Ent(\left\{s \,\middle|\, s \in S_{\mathcal{M}}^{C}(\mathcal{D}_N)  \,\text{and}\, s \in \mathcal{V}\right\}),$$
% \end{align}

where $Ent(.)$ is the entropy of a set relative to its support, i.e., the set of candidate inputs.

\subsubsection{\textbf{Controllability}}

To measure how responsive a model is to the given prompt, we define a metric based on the difference of its \nmi{} in the sensitive and invariant settings. The more controllable a model is, the less discrepancy is observed between the \texttt{sens} and \texttt{invar} settings. Hence, when measuring the controllability on $\mathcal{D}_N$ for a model $\mathcal{M}$ is defined as
\vspace{-5mm}

$$ \control({\mathcal{M}, \mathcal{D}_N}) = $$
\resizebox{\linewidth}{!}{$
\frac{|\nmi(\mathcal{M}, \texttt{sens}, \mathcal{D}_N) - \nmi(\mathcal{M}, \texttt{inv}, \mathcal{D}_N)|}{\sqrt{(\nmi(\mathcal{M}, \texttt{sens}, \mathcal{D}_N) \times \nmi(\mathcal{M}, \texttt{inv}, \mathcal{D}_N))}}$}.

% $$ C_{\mathcal{M}, f} = 1 - \frac{|\mathcal{M}_{sens}(f) - \mathcal{M}_{invar}(f)|}{max(\mathcal{M}_{sens}(f), \mathcal{M}_{invar}(f))}$$





