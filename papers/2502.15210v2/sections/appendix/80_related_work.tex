\section{Related Work}

% [what are some different benchmarks]

% [how are people evaluating vlms]

% [encoders are also being used as judges (text-image and image-image)]

% [what benchmarks are using vlms as judges]

% [literature on looking into invariance in LMMs]


% [downsides of using clip and other models (do not get negation), look at it as bag of words, spatially flawed (whatsup), looking into propperties that are not captures in benchmarks]
Using language models as automatic evaluators has become a somewhat common practice with popular approaches such as \textsc{GPTScore} and G-eval~\citep{fu2023gptscore,liu2023geval} being used to rank responses in the NLP domain. 
Due to that, there has been a significant amount of recent work that has investigated the capabilities and limitations of using LLMs as judges~\citep{thakur2024judging, chiang2023can, murugadoss2024evaluating, shankar2024validates}. 
\citet{chiang2023can} have shown that LLM evaluations are consistent and reproducible, making them suitable alternatives for human evaluation, they argue that these models inherent biases should prevent them using independently rather than \textit{alongside} human experts. 
Furthermore, \citet{zheng2023judging} reveal that large \modelss{{}, e.g., GPT-4 Turbo, align well with human judgments and \citet{thakur2024judging} further states that simpler models may still outperform GPT-4 Turbo in ranking tasks due to superior alignment metrics. Also, recent work assessed how humans can help LLMs evaluate better by testing different instruction types or designing tools that result in more balanced evaluations~\citep{murugadoss2024evaluating, shankar2024validates}.

It is worth noting that known limitations of LLMs such as their lack of invariance to the order of examples given in a prompt, which is a well studied issue of natural language models~\citep{fang2024rethinking}, 
% and also observed in the multimodal case by the lack of symmetry we reported,
may render auto evaluation unreliable. Similarly, \citet{berglund2023reversal} show failure cases where models trained on unidirectional relationships do not infer the reverse, indicating systemic limitations even in state-of-the-art LLMs such as \textsc{GPT-4} (as seen in Figure~\ref{fig:fig1} and in Appendix \ref{sec:error-analysis} for \modelss{}). Our main goal is
to investigate the reliability of automated evaluation in the multimodal context, by probing the models to compare data pairs. 
%thus to assess to what extent auto-evaluation can be done reliably, focusing in the multimodal case, by probing models on their ability to compare.

Namely, the evaluations we carry out focus on testing in multiple different ways how good \modelss{} are when it comes to comparing data instances, such as whether \modelss{} prompted to compare are symmetric or smooth for instance, and to what extent they can be controlled, i.e., instructed to pay attention to or ignore certain features of the inputs. While the literature is more sparse regarding testing \modelss{} in this setting, recent work has tested for something along those lines. \citet{chen2024mllm} for instance propose a benchmark for evaluating \modelss{} in multiple different scenarios, including checking whether pairwise comparisons of responses to a query correlated with human judgments. They concluded that although correlations are relatively high on comparison tasks, biases and inconsistencies affect performance on pair scoring and batch ranking. Similarly, \citet{awal2024vismin} introduced a synthetic dataset containing paired images that differ only along one feature (e.g., the color of an object). We seek to add to this branch of the literature by introducing a framework where controlled experiments can be carried out to anticipate the performance of models when being used as judges, and various different characteristics of automatic judges can be identified (e.g., how smooth they are).

Unlike the case of generative \modelss{} discussed above, discriminative visual language models such as CLIP~\citep{radford2021learning} are covered by a significant amount of recent work, and several failure modes are well reported, mostly deriving from the fact this class of \modelss{} tends to behave as bag-of-words models, focusing on nouns and ignoring relationships and semantics in their input data~\citep{yuksekgonul2023when}. For instance, CLIP was observed to struggle with spatial reasoning~\citep{kamath2023s} and ignore negation~\citep{alhamoud2025vision}. On the other hand, fine-tuning CLIP to reason about pairwise differences \citet{sam2024finetuning} showed that discriminative \modelss{} can improve on how well they manage to reason about pairwise differences if training is tailored for enabling so, highlighting the benefits that being able to measure these skills may inform training and improve models as a consequence. \citet{ouali2024discriminative} showed that fine-tuning generative \modelss{} to turn them into discriminative models results in improved image-retrieval from text, which aligns with results we reported in Section~\ref{sec:ecoders_vs_decoders} showing a gap between open-sources \modelss{} and CLIP-style encoders.
