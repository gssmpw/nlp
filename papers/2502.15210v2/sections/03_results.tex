\input{imgs/all-metrics-best-models}




\section{Evaluation Results}
\subsection{Experimental Setting}
% [We use a set of opensource and closed source models.]
We choose a comprehensive set of open- and closed-source vision-language models and evaluate them using the instantiations of \mmscore. We evaluated the following openly available models: 
\vspace{-2mm}
\begin{itemize}
    \item 
    \chameleon{} \cite{lu2024chameleon}, 
\vspace{-2mm}
    \item \llavaonevision{} \cite{li2024llava}, 
\vspace{-2mm}
    \item \pixtral{} \cite{agrawal2024pixtral},
\vspace{-2mm}
    \item \phiThreeFive{} \cite{abdin2024phi},
\vspace{-2mm}
    \item four versions of InternVL2 \cite{wang2024enhancing}: \internvlTwoEightB{}, \internvlTwoFourB{}, \internvlTwoTwoB{}, \internvlTwoOneB{}, 
\vspace{-2mm}
    \item four versions of InternVL2.5 \cite{chen2024expanding}: \internvlTwoFiveEightB{}, \internvlTwoFiveFourB{}, \internvlTwoFiveTwoB{}, \internvlTwoFiveOneB{}, 
\vspace{-2mm}
    \item two versions of Qwen2-VL \cite{wang2024qwen2}: \qwenTwoVLTwoB{}, \qwenTwoVLSevenB{}, %and
\vspace{-2mm}
    \item three versions of Molmo \cite{deitke2024molmo}: \molmoEOneB{}, \molmoOSevenB{}, \molmoDSevenB{}.
\end{itemize}

We also considered commercial grade models and benchmarked 3 versions of GPT-4o \cite{achiam2023gpt}(\gptFouroFive{}, \gptFouroEight{}, \gptFouroEleven{}), \gptFouroMini{}, and two versions of Gemini-1.5 \cite{reid2024gemini} (\geminiFlash{}, \geminiPro{}). 
Note that we consider multiple versions of the same architecture, as opposed to using the newest/largest version, to understand better how model capacity affects each of the metrics. We provide an extended analysis of different model versions in Appendix \ref{sec:model-versions}. 

We run all open-source models on a single NVIDIA H100 GPU and use API calls for closed-source models either from \openrouter\footnote{\url{https://openrouter.ai/}} or OpenAI \footnote{\url{https://platform.openai.com/}}.

Also note that, since \mmscore{} aims to evaluate \modelss{} as similarity kernels on image-only or text-image pairs, we do not evaluate text-only reasoning models such as OpenAI-o1 or DeepSeek-R1 \cite{guo2025deepseek}. Further, we do not evaluate \texttt{Llama3.2-11B} \cite{llama3} as its official implementation on HuggingFace\footnote{\url{https://huggingface.co/}} does not support Flash Attention \cite{dao2022flashattention} and inference was prohibitively slow. We further tried \texttt{Llama3.2-11B}, and higher capacity models e.g., \texttt{Qwen2-VL-72B} and \texttt{Llama3.2-90B}, using API calls to \openrouter; however, they tended to under-perform drastically compared to lower capacity models (e.g. \qwenTwoVLTwoB{}), raising concerns about potential issues on \openrouter's end for these models. As a result, we excluded them from our final results.

% [For models that do not support multiple images, we concatentate.]

\input{tabs/mmscore-all-metrics-all-models}
\input{imgs/mi-closed-vs-open}
\input{imgs/mmscore_nmi-vs-bms}
\subsection{Results}
\vspace{-1mm}
We analyze and plot the results of the best models in Figure \ref{fig:mi-best-models} and provide an aggregated version of the metrics over all four datasets in Table \ref{tab:model_performance}. We aggregate different splits/datasets by taking the average of them to give each sub-dataset equal importance in the final number. The full set of benchmarking results of all models for \mmscore{} on all datasets and metrics are reported in Appendix \ref{sec:full-results}.

\vspace{-2mm}
\subsubsection{General Observations}
As illustrated in Figure \ref{fig:mi-best-models}, we observe no model, whether closed- or open-source, is the best performer across all four metrics. Moreover, we further observe that for each metric, no model is the `best' similarity kernel across the four different datasets either. This shows how features of the dataset and also the metrics a user might want to optimize play a crucial role in which \model{} to choose as the best similarity kernel/judge. For instance, among open-source models, although \internvlTwoFiveEightB{} outperforms the rest in \nmi, it is less controllable and smooth than \qwenTwoVLSevenB{} or \llavaonevision{}.
% in case symmetry is an important factor when comparing image-text data pairs, among closed-source models using Gemini models are preferred over GPT4o.

When considering \mmscore's main metric, \nmi, we notice that the performance of models is generally better on image-image pairs rather than image-text pairs. Furthermore, we observe that although open-source \modelss{} are roughly comparable to closed-source ones on \mmscorewuimgtext, the gap between the two groups is larger in the image-image pairs. However, \internvlTwoFiveEightB{} is a strong competitor to closed-source models considering all four metrics and could potentially be used as a substitute to closed-source models as a similarity kernel.

% Interestingly, we further observe a pattern regarding \gptFouroEleven{}, a common default judge used in the literature, and its lower cost version, \gptFouroMini{}; they both suffer from low \relaxsymone{} when comparing image-text pairs, and the cheaper model's \control{} and \smoothness{} is higher or comparable to that of the expensive one across datasets. This emphasizes the importance of \mmscore{} in analyzing the capabilities of models as similarity kernels to be better used as judges. We analyze and plot these results further in Appendix \ref{sec:full-results} and show errors the best \modelss{} make in scoring and reasoning in Appendix \ref{sec:error-analysis} \red{pointer to failure cases in appendix - show how sota models fail in symmetry and the evaluation tasks }.
Interestingly, we further observe a pattern regarding \gptFouroEleven{}, a common default judge used in the literature, and its lower cost version, \gptFouroMini{}; they both suffer from low \relaxsymone{} when comparing image-text pairs, and the cheaper model's \control{} and \smoothness{} is higher or comparable to that of the expensive one across datasets. This emphasizes the importance of \mmscore{} in analyzing the capabilities of models as similarity kernels to be better used as judges. We analyze and plot these results further in Appendix \ref{sec:full-results} and further have qualitative examples of the errors the best \modelss{} make in these tasks in Appendix \ref{sec:error-analysis}.~\looseness-1
% Lastly, we observe that smoothness is important when using a \model{} as a similarity kernel, the strongest models in \nmi{} are not necessarily the best ones. While 

% Given these observations, we emphasize the importance of identifying the task at hand at choosing a judge-model accordingly.

% We show finer grain comparisons across models in Figure \ref{fig:best-models-mmscore-smoothness-sym} in Appendix \ref{sec:full-results}.


% \subsubsection{Closed-source vs Open-source Models}
% We aggregate all models into ``open'' and ``closed'' models to observe the overall pattern of them in Figure \ref{fig:mi-close-vs-open}. Supporting recent literature \{REF\}, it is observed that the gap between open and closed source models is vanishing on image-text pairs and the closed- and open-source models perform comparable on \mmscorewuimgtext. However, on the image-pair comparisons, we observe a larger gap and wider-spread performance distribution for the open-source models, showcasing the superiority of closed- over open-source models. Figure \ref{fig:mi-close-vs-open}

\vspace{-2mm}
\subsubsection{Encoders vs \modelss{}}    
\label{sec:ecoders_vs_decoders}

For the image-image task, we explore how image encoders compare to \modelss{} on our metrics. To this end, three DINOv2 versions (\dinoBase, \dinoSmall, and \dinoLarge) and the LAION- and OpenAI- CLIP-trained ViTs (base and large) are chosen to encode images. Since feature controllability on image-encoders is limited to the image augmentation transformation (CJ, R, PS, GB, ET), we only compare image-encoders to \modelss{} on \mmscorecoco{} and \mmscorein.

To generate the similarity score of a given image-pair with an image-encoder, we compute the cosine similarity of the representation of each image and scale the scores between 1-10, and round them to the nearest integer. To generate the criteria-sensitive similarity score, we create the representations of the image-pair by simply using the representations output by the encoder for each image. On the other hand, when generating the criteria-invariant score, where the criteria is a specific transformation ($T$), we generate the representation of each image as the average of the representations of the encoder for $k$ versions of the image where random amounts of $T$ are applied to the image. In our experiments, we set $k=5$.

We report results in Figure \ref{fig:mi-encoder-vs-lmms}. We see encoders do better than open-source \modelss{} most of the time and are comparable to closed-source models (besides CJ). This shows although significantly smaller, encoders can be at least as good as \modelss{}, enabling similarity scoring at a much lower cost. Also, encoder-generated scores are trivially symmetric as well since the underlying cosine similarity is symmetric. However, they lack in controllability as they are limited to image-only comparisons and can only consider criteria that can be applied to the image using augmentations, i.e., spatial position transform cannot be applied to images for encoders.

\vspace{-2mm}
\subsection{Correlation with Benchmarks}

To showcase the effectiveness of our introduced metrics with \mmscore{} in predicting model performance, we compute the Spearman correlation with other popular benchmarks used in the literature. By showing correlations of our metrics with these benchmarks, we show that although the \mmscore{} framework introduces simple and cheap methods focused on evaluating similarity kernels induced by prompted \modelss{}, these metrics are predictive of an \model{}'s performance on other benchmarks.

We collect all the model performances from the \textsc{OpenVLM Leaderboard}\cite{duan2024vlmevalkit} and filter out the models we evaluate, resulting in all 23  (including different versions/capacities of closed- and open-source) models. Next, by filtering out the benchmarks that have evaluation scores for all 23 models on OpenVLM, we end up with AI2D \cite{kembhavi2016diagram}, HallusionBench \cite{guan2024hallusionbench}, MMBench \cite{liu2025mmbench}, MMStar \cite{chen2024we}, MMMU \cite{yue2024mmmu}, MathVista \cite{lu2023mathvista}, MM-Vet \cite{yu2023mm}, OCRBench \cite{liu2024ocrbench}.


Before computing the correlations, each metric is first aggregated for each model across all the configurations created by \mmscore. Specifically, we aggregate all features within each dataset (e.g., CJ, SP, etc.) and further across all datasets (e.g., \coco, \wu) to have a single number per metric for each model. As seen in Table \ref{tab:benchmark_comparison}, all metrics in \mmscore{} have a high positive correlation with benchmarks. Note that since \nmi{} has the highest significant correlation, we choose it to be the main metric of \mmscore. However, measuring any of these metrics incurs a low cost as it does not require expert-generated or costly annotations, and since they have high correlations, they can serve as a low-cost surrogate of a model's performance during training or validation. We further show scatter plots that highlight correlations in Figure~\ref{fig:mmscore-vs-bms-nmi}, and more comprehensively in Figure~\ref{fig:sym-vs-bm} in the Appendix \ref{sec:full-results}.

\input{tabs/mmscore-bm-corr-filtered}


% \section{Discussion}