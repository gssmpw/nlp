\begin{figure*}[ht]
    \centering
    
    % \includegraphics[width=0.95\linewidth,trim={.1cm .2cm .2cm .2cm},clip]{imgs/MMScore-NMI-coco-in100-wu_img_img-wu_img_text-mi-plt.pdf}
    % \caption{Closed- and open-source models perform comparable on image-text tasks. First three image-image, last image-text. }
    % \label{fig:mi-close-vs-open}
    
    % \vspace{1ex}
    
    \includegraphics[width=0.92\linewidth,trim={.1cm .2cm .2cm .2cm},clip]{imgs/Encoders-vs.-LMMs-colorjitter-elastic-gaussianblur-perspective-rotate-mi-plt.pdf}
    
    \caption{A simple vision encoder outperforms open-sourced \modelss{} and has on par performance with closed sourced models which are much more expensive, for image-image tasks (results combine \mmscorecoco{} and \mmscorein), and similar pattern is observed across different transformations.}
    \label{fig:mi-encoder-vs-lmms}
\end{figure*}