\documentclass{article}


\usepackage{arxiv}
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.


\usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage{placeins}  % 在导言区加入
\usepackage{afterpage}  % 在导言区加入

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{booktabs}
\usepackage{hyperref}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{tcolorbox}
\usepackage{float}


\usepackage{multirow}      % 用于多行合并
\usepackage{booktabs}      % 用于美化表格横线 (\toprule, \midrule, \bottomrule)
\usepackage{adjustbox}     % 如果需要在表格过宽时进行缩放
\usepackage{caption}       % 美化表格/图的标题
\usepackage{geometry}      % 若需要可自行调整页面边距
\usepackage{amssymb}
% \usepackage{minipage}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{textcomp} % or fontenc, depending on your setup

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xcolor}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{framed}
\usepackage{pifont}
\definecolor{shadecolor}{rgb}{0.92,0.92,0.92}

\PassOptionsToPackage{table}{xcolor} 
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }


\title{Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text
Modifications}


\author{
 Yiming Zeng \\
  University of Connecticut\\
  \texttt{yiming.zeng@uconn.edu} \\
  %% examples of more authors
   \And
 Wanhao Yu \\
  University of North Carolina at Charlotte\\
  \texttt{wyu6@charlotte.edu} \\
  \And
 Zexin Li \\
  University of California, Riverside\\
  \texttt{zli536@ucr.edu} \\
\And
 Tao Ren \\
  University of Pittsburgh\\
  \texttt{tar118@pitt.edu} \\
\And
 Yu Ma \\
  Carnegie Mellon University \\
  \texttt{yuma13926@gmail.com} \\
\And
 Jinghan Cao \\
  San Francisco State University \\
  \texttt{jcao3@alumni.sfsu.edu} \\
\And
 Xiyan Chen \\
  University of Pittsburgh \\
  \texttt{xic130@pitt.edu} \\
\And
 Tingting Yu \\
  University of Connecticut \\
  \texttt{tingting.yu@uconn.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) have transformed natural language processing, yet they still struggle with direct text editing tasks that demand precise, context-aware modifications. While models like ChatGPT excel in text generation and analysis, their editing abilities often fall short, addressing only superficial issues rather than deeper structural or logical inconsistencies. In this work, we introduce a dual approach to enhance LLMs editing performance. First, we present InstrEditBench, a high-quality benchmark dataset comprising over 20,000 structured editing tasks spanning Wiki articles, LaTeX documents, code, and database Domain-specific Languages (DSL). InstrEditBench is generated using an innovative automated workflow that accurately identifies and evaluates targeted edits, ensuring that modifications adhere strictly to specified instructions without altering unrelated content. Second, we propose FineEdit, a specialized model trained on this curated benchmark. Experimental results demonstrate that FineEdit achieves significant improvements around {10\%}  compared with Gemini on direct editing tasks, convincingly validating its
effectiveness.
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
Large Language Models (LLMs) have revolutionized natural language processing, unlocking capabilities once thought unattainable. ChatGPT, for example, shows exceptional skills in text generation and logical reasoning~\cite{openai2023chatgpt}. Despite these impressive advancements, LLMs still face significant challenges in the underperformance of text editing tasks.~\cite{castillo2022chat} has mentioned that the use of ChatGPT for editing has obvious limitations, which might not accurately follow the editing task instructions and understand the author's intent, leading to changes that are not appropriate to the context of the text. Meanwhile, it effectively addresses surface-level issues, such as spelling and formatting, but cannot resolve complex challenges, such as editing in long text context or strict following task instructions.

% \begin{figure}[!tbp]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figure/figure1_Edit.pdf}
%     \caption{Direct Editing Task. As illustrated, the ChatGPT processes the edit request (e.g., changing a label reference), modifies the original context, and generates the updated content.}
%     \vspace{-3mm}
%     \label{fig:edit-example}
% \end{figure}    


To address these limitations, researchers have developed methods to enhance the editing capabilities of LLMs, particularly under task-specific scenarios, e.g., editing in code, LaTeX, etc. However, LLMs’ general editing capabilities in task-specific settings often fall short~\cite{yao-etal-2023-editing, ma-etal-2024-robustness}. 
They tend to generate incorrect outputs and stray from the given editing instructions. This issue arises primarily because these models overly emphasize task-specific constraints and are susceptible to hallucinating extraneous information. 

In contrast, we notice that if narrowing the model's focus to just two factors: the precise location of the edit and the specific content to be changed, the edit task itself could be better accomplished. Per this intuition, we propose a dual approach consisting of a benchmark for editing tasks and a model named FineEdit. For the benchmark, we design an automated workflow that focuses on accurately identifying and evaluating structured text edits. This workflow identifies precise differences and ensures correct edits through quality control. By reducing noise and focusing on meaningful modifications, this process produces a benchmark that is both practical for training and robust for evaluation. It directly addresses limitations in existing methods and aligns better with the practical demands of real-world editing tasks.

Furthermore, we use part of the curated benchmark to train the model, focusing on direct editing tasks. FineEdit achieves an over 10\% improvement over Gemini 1.5 Flash and Gemini 2.0 Flash~\cite{google2024gemini}, and up to 30\% over Llama-3.2-3B~\cite{meta2024llama3_2} on diverse editing benchmarks, while outperforming Mistral-7B-OpenOrca~\cite{lian2023mistralorca1, mukherjee2023orca, longpre2023flan} over 40\% in direct editing tasks.

The main contributions of this work include:
\begin{itemize}
    \item \textbf{A high-quality benchmark dataset (InstrEditBench)}\footnote{We will release all datasets and the code to promote reproducibility on acceptance.}: We created a curated dataset with 20,000+ structured editing tasks across Wiki articles, LaTeX documents, Code, and Database DSL, providing a unified evaluation standard for structured text editing research.
    
    \item \textbf{An innovative automated dataset generation workflow}: We developed a comprehensive workflow that ensures the benchmark's quality by accurately identifying line numbers and applying rigorous criteria to filter meaningful and relevant edits.
    
    \item \textbf{The FineEdit model}: We introduce a specialized model designed for structured direct text editing, demonstrating superior performance across benchmarks compared with existing models.
\end{itemize}

\section{Background}
\subsection{Problem Formulation}
Each data point consists of an original structured text, \(T_{\text{orig}}\), and an editing instruction, \(I_{\text{edit}}\). The objective is to generate an edited text, \(T_{\text{edit}}\), that incorporates the modifications specified by \(I_{\text{edit}}\). Formally, this process is defined as
\begin{equation}
    T_{\text{edit}} = f\Bigl(T_{\text{orig}}, I_{\text{edit}}; \theta\Bigr)
\end{equation}
where \(\theta\) represents learned parameters and \(f\) denotes a function instantiated by a LLM that maps the original text \(T_{\text{orig}}\) and editing instruction \(I_{\text{edit}}\) to the edited text \(T_{\text{edit}}\).

The parameters \(\theta\) are learned from a dataset consisting of triples \(\{(T_{\text{orig}}^{(i)}, I_{\text{edit}}^{(i)}, T_{\text{edit}}^{(i)})\}_{i=1}^{N}\) during training, where the objective is to minimize the discrepancy between the generated output and the ground truth edited text.

Internally, \(f\) concatenates \(T_{\text{orig}}\) and \(I_{\text{edit}}\) into a single prompt and generates \(T_{\text{edit}}\) token by token in an autoregressive manner. Specifically, if 
\(T_{\text{edit}} = (y_1, y_2, \dots, y_t)\),
the probability of the edited text is factorized as
\begin{equation}
\begin{split}
        p(T_{\text{edit}} \mid T_{\text{orig}}, I_{\text{edit}}) =& \prod_{i=1}^{t} p\Bigl(y_i \mid T_{\text{orig}}, I_{\text{edit}}, \\
        & y_1, y_2, \dots, y_{i-1}\Bigr)
\end{split}
\end{equation}

For finetuning on the editing task, the prompt tokens (i.e., the original text and the editing instruction) are masked out in the loss function to ensure that the model focuses only on predicting the correct edited tokens. At inference time, the model processes the prompt and subsequently generates \(T_{\text{edit}}\).

The parameters \(\theta\) are fine-tuned on labeled examples \((T_{\text{orig}}, I_{\text{edit}}, T_{\text{edit}})\) by minimizing the negative log-likelihood of the target tokens with the loss:
\begin{equation}
    \mathcal{L}(\theta) = - \sum_{t=1}^{|T_{\text{edit}}|} \log P_{\theta}(y_t \mid T_{\text{orig}}, I_{\text{edit}}, y_{1:t-1})
\end{equation}
over all training samples in the dataset

\subsection{LLM Editing Tasks}

LLMs are increasingly recognized as versatile tools for automating and enhancing editing tasks across diverse domains. Previous studies have explored LLMs for editing tasks in areas such as natural language (e.g., wiki articles) and code. For instance, CoEdIT~\cite{raheja2023coedit} employs task-specific instruction tuning to achieve precise modifications, while other works fine-tune models like T5~\cite{raffel2020exploring} on pairs of original and edited texts~\cite{faltings2021leveraging, reid2022learning, mallinson2022edit5, du2022grit1, du2022grit2, kim2022towards}. However, many of these approaches rely on specialized techniques or focus narrowly on specific tasks, such as grammar correction~\cite{mallinson2022edit5, fang2023hierarchical}, text simplification~\cite{stajner2022simple}, paraphrase generation~\cite{chowdhury2022enhanced}, or style transfer~\cite{reif2022style}, which limits their generalizability across a broader range of editing scenarios. In the realm of code editing, Fan et al.~\cite{fan2024codechange} examined LLMs for code change tasks and identified weaknesses in generating accurate reviews and commit messages. While these studies offer valuable insights, they often fall short in providing unified benchmarks and robust solutions to address the full spectrum of editing challenges. Our work addresses these gaps by introducing a comprehensive, cross-scenario editing tasks benchmark that covers Wiki, code, DSL, and LaTeX.

\subsection{LLM Benchmarking}
LLM benchmarking is a crucial aspect of evaluating the diverse capabilities of LLM. Researchers have developed numerous benchmarks spanning multiple domains, including code~\cite{chen2021evaluating,austin2021program,jimenez2024swebench,yang2024swebenchmultimodal}, commonsense reasoning~\cite{bisk2020piqa,sap2019socialiqa,zellers2019hellaswag,sakaguchi2021winogrande}, reading comprehension~\cite{rajpurkar2018know,choi2018quac,clark2019boolq}, and language understanding~\cite{wang2018glue,wang2019superglue,xu2020clue}. However, only a few works benchmark the editing performance of LLMs. For example, GEM~\cite{xu2024benchmarking} introduces metrics for subjective tasks without gold standards, while CriticBench~\cite{lin2024criticbench} assesses iterative output refinement. Additionally, \cite{cassano2023can} explores that fine-tuning with curated training data significantly improves code editing performance. Some automated evaluation is also involved in LLM benchmarking. For instance, G-Eval~\cite{liu2023geval} is an automated evaluation framework that leverages large language models to assess text quality and model performance in generative tasks. Built on a Chain-of-Thought (CoT) prompting strategy \cite{wei2022chain}, G-Eval guides the model to articulate intermediate reasoning steps before reaching its final evaluation, leading to outputs that align closely with human judgments~\cite{liu2023geval}. However, these efforts focus on short-context, isolated tasks and do not systematically evaluate an LLM’s ability to locate and modify content within long contexts. Our work addresses this gap by introducing a comprehensive benchmark covering Wiki, code, DSL, and LaTeX, emphasizing long-context editing.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figure/Figure2.pdf}
    \caption{Workflow of Generating High-quality FineEdit benchmark. The content difference is highlighted in red. }
    \label{fig:example-pdf}
\end{figure*}

\section{Method}

\subsection{Instruction categories}

We leverage four data sources to cover a wide range of representative text application scenarios: Wiki, Code, DSL, and LaTeX. The details of each categories are described as follows:

\begin{itemize}
    \item \textbf{Wiki}: Data is extracted from the WikiText language modeling dataset~\cite{merity2016pointer}, which contains over 100 million tokens from a dedicated subset of Wikipedia's Good articles~\cite{wikipedia_good_articles} and Wikipedia's Featured articles~\cite{wikipedia_featured_articles}. Specifically, sections from these articles are extracted and then contiguous segments are randomly selected to provide data points with various lengths.
    \item \textbf{Code}: Code samples are extracted from the CodeSearchNet corpus \cite{husain2019codesearchnet}, which contains about two million pairs of comments and code from GitHub projects. To make the edit task more challenging, each code sample in our benchmark is made up of several instead of one code segment because one single code segment is too short (about 10 lines).
    \item \textbf{DSL}: Database Domain Specific Language (DSL) is also considered in our benchmark. It consists of queries and schema definitions from multiple public repositories~\cite{hive,b-mc2_2023_sql-create-context,cassandra,chinookDatabase}.
    \item \textbf{LaTeX}: LaTeX data is extracted from the Latex2Poster dataset~\cite{latex2poster} that offers the LaTeX source code document of research papers along with metadata. Specifically, each data point in our benchmark consists of multiple subsections from each extracted document data.
\end{itemize}

\subsection{Instruction Generation}

Zero-shot instruction generation is efficient but often lacks diversity. To address this limitation, we build on the work of \cite{wang2022self,taori2023stanford} by leveraging ChatGPT-4o mini combined with in-context learning (ICL)~\cite{dong2024survey}. Our approach is designed to generate specific edit requests tailored to the structural characteristics of different data categories, as process \ding{192} in Figure~\ref{fig:example-pdf}. For Wiki, which primarily consists of clear structural text elements like headings and subheadings, we apply a zero-shot prompting strategy. In contrast, for more complex domains such as LaTeX, code, and DSL, we adopt ICL to improve the diversity and nuance of generated instructions. This category-specific strategy not only enriches the instruction sets but also enhances their ability to capture domain-specific editing challenges without compromising on precision and efficiency. We will describe prompt details in Appendix~\ref{sec:dataset_generation_prompts}.

% \zexin{this paragraph maybe wrong! we focus on edit itself, but not respect task-specific constraint.}
% By designing the ICL prompt, we cover different edit task scenarios in the prompts to help LLM improve the variety and accuracy of the generated editing instructions. For instance, the LaTeX editing process must respect a specific formatting standard. Accordingly, we concentrate on modifications relevant to LaTeX special formats—such as adjustments to environments, command modifications, citation corrections, and the handling of graphical elements, etc. This is more challenging than simply editing text without additional requirements. For Code, edited content should present proper format, including indentation, whitespace, variable, and function naming. Meanwhile, for database DSL, edited content should be legitimate query clauses (e.g., \texttt{SELECT}, \texttt{FROM}, \texttt{WHERE}, \texttt{JOIN}, etc.), consistent keyword casing, structured clause ordering with line breaks and indentation, and etc. \zexin{We will describe prompt details in Appendix~\ref{?}.}

\subsection{Instruction filtering}

After obtaining the edit instructions for each content, we apply them to the original text to produce an edited version as process \ding{193} in Figure~\ref{fig:example-pdf}. However, ensuring the quality of the edited content remains challenging. Although LLM generally follows the edit instructions, errors may occur---for example, targeting incorrect line numbers or misinterpreting the intended semantics~\cite{wang2025understandingcharacteristicscodegeneration, cassano2024editevaluatingabilitylarge}. To address this problem and improve data quality, we propose {DiffEval Pipeline}, which integrates G-Eval~\cite{liu2023geval} and Git-Diff as an automatic filter to improve data quality. 

% \zexin{this sounds others work, consider putting it into a background and only briefly describe in one sentence here.}
% G-Eval~\cite{liu2023geval} is an automated evaluation framework that leverages large language models to assess text quality and model performance in generative tasks. Built on a Chain-of-Thought (CoT) prompting strategy \cite{wei2022chain}, G-Eval guides the model to articulate intermediate reasoning steps before reaching its final evaluation, leading to outputs that align closely with human judgments~\cite{liu2023geval}.

Besides adopting G-Eval for automated assessment \citep{liu2023geval}, the DiffEval Pipeline also relies on \texttt{git} \citep{gitdiff}, a widely used version control system, to detect and classify textual modifications. Specifically, the command \texttt{git diff} specifies differences between the original and modified texts as process \ding{194} in Figure~\ref{fig:example-pdf}, categorizing changes into four types:

\begin{itemize}
\item \textbf{Replacements}: an original segment is transformed into a new form, indicated as \texttt{[original\_text -> modified\_text]}. This captures cases where an existing text portion is substituted with different content, which may alter meaning or style.
\item \textbf{Deletions}: a segment is removed entirely, shown as \texttt{[-original\_text-]}. Such removals can simplify the text or eliminate irrelevant or erroneous sections.
\item \textbf{Insertions}: new content is added, denoted as \texttt{[+modified\_text+]}. Insertions enrich the text with extra details, clarifications, or elaborations.
\item \textbf{Unchanged Text}: labeled as \texttt{equal: unchanged\_text}. This indicates portions that remain identical between the original and modified versions, providing a reference for what the model has chosen to retain.
\end{itemize}

By categorizing changes into these four types, the DiffEval Pipeline offers a structured view of how text is altered, enabling more precise evaluations when paired with G-Eval.

We make a concrete instance using data in the LaTeX category in Table~\ref{tab:edit-intentions}. If the edit request is to ``Remove the duplicate \texttt{\textbackslash{}begin\{abstract\}} at the beginning of the abstract environment," the diff output might display on Line~1:
\begin{verbatim}
\begin{abstract}[-\begin{abstract}-]
\end{verbatim}
This indicates that the duplicate has been successfully removed. 

Finally, process \ding{195} in Figure~\ref{fig:example-pdf} demonstrates that DiffEval carefully reviews the aggregated data (marked with red arrows) alongside the edit request to fully grasp the context, structure, and nuances of the text. It identifies discrepancies between the intended edits and the actual modifications, verifying whether the changes faithfully implement the edit instructions. By using the \texttt{git diff} output instead of the complete edited content, DiffEval can precisely locate modifications using supplementary information such as line numbers and structured differences. Moreover, \texttt{git diff} minimizes unnecessary noise and reduces computational overhead by significantly lowering the token count compared with the full edited content. Once all required data is gathered, the G-Eval analysis process evaluates the collected information to further enhance the dataset quality.

Specifically, the analysis process begins by parsing the structure of \texttt{git diff} outputs, categorizing changes as replacements, deletions, insertions, or unchanged segments. Next, it evaluates the semantic meaning of both the original content and the modifications to ensure that the changes are accurate and complete. This involves a thorough review of the original text, the edit request, and the resulting edits, applying predefined categorization rules, and assessing overall coherence.

Based on this analysis process, the DiffEval is able to assign a coherence score, \texttt{G-Score}, to the edited content, reflecting the semantic integrity and logical consistency of the modifications. This score is used to filter out output that does not meet the desired quality threshold $\alpha$.



\subsection{Data Statistics}

Our curated benchmark comprises 28,050 items spanning a diverse array of structured data types, including 8,366 LaTeX contexts, 7,712 code segments, 8,025 WikiText entries, and 3,947 database language samples, thereby reflecting both the generality and scale of real-world structured data. Table~\ref{tab:edit-intentions} shows the example across four categories. For each item, it has the following attributes: 

\begin{itemize}
    \item \texttt{Id:} a unique identifier for each entry.
    \item \texttt{Original content:} the content directly extracted from the data source. 
    \item \texttt{Edit request:} The editing instruction generated
through zero-shot or few-shot prompting based on the original content
    \item \texttt{Edited Content:} the output after applying edit request to the original content.
    \item \texttt{Difference:} the changed part between Edit content and original content. 
    \item \texttt{G-score:} evaluates the quality of the edited content based on its strict adherence to the edit request content. 
\end{itemize}


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{ % 自动调整表格宽度
\begin{tabular}{@{}p{2.5cm}p{5.5cm}p{5.5cm}p{5.5cm}p{6.0cm}p{1.0cm}@{}}
\toprule
\textbf{Data Category} & \textbf{Orignal Content} & \textbf{Edit Request} & \textbf{Edited Content} & \textbf{Difference} & \textbf{G-score}\\ \midrule
\textbf{WikiText} & ...As with previous <unk> Chronicles games, Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit... & Replace ``\textbackslash{}<unk>\textbackslash{}'' with ``Valkyria'' where it appears in the text.
 & ...As with previous Valkyria Chronicles games, Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit... & Line 2 differs: Differences: ...As with previous \textcolor{red}{[<un -> Val]k[> -> yria]} Chronicles games, Valkyria Chronicles III
is a tactical role @-@ playing game
where players take control of a mili-
tary unit... & 9 \\ \midrule 
\textbf{LaTex} & 
\textbackslash{}begin\{abstract\}\textbackslash{}n\textbackslash{}begin\{abstract\}\textbackslash {}n  \%\textbackslash{}mika\{\}, \textbackslash{}guandao\{\}, \textbackslash{}leo\{\}\textbackslash{}n  \textbackslash{}vspace\{-0.2cm\}\textbackslash{}n  Neural radiance fields (NeRF) rely on volume rendering to...
 & Remove the duplicate \textbackslash{}begin\{abstract\} at the beginning of the abstract environment. & 
\textbackslash{}begin\{abstract\}\textbackslash{}n  \%\textbackslash{}mika\{\}, \textbackslash{}guandao\{\},\textbackslash{}leo\{\}\textbackslash{}n  \textbackslash{}vspace\{-0.2cm\}\textbackslash{}n  Neural radiance fields (NeRF) rely on volume rendering to... & Line 1 differs: Differences: \textbackslash{}begin\{abstract\}\textcolor{red}{[- \textbackslash{}begin\{abstract\}-]} & 9
 \\ \midrule
\textbf{Code} & {...def yield\_nanopub(assertions, annotations, line\_num):\textbackslash{}n """Yield nanopub object""" if not assertions:...}
& Change the function definition from:

def yield\_nanopub(assertions, annotations, line\_num)

to include type annotations as:

def yield\_nanopub(assertions: list, annotations: dict, line\_num: int) -> dict
&...def yield\_nanopub(assertions: list, annotations: dict, line\_num: int) -> dict:
 """Yield nanopub object"""
if not assertions:... & Line 1 differs:
Differences: def yield\_nanopub({assertions\textcolor{red}{[+: list+]}, annotations\textcolor{red}{[+: dict+]}, line\_num\textcolor{red}{[+: int+]})\textcolor{red}{[+ -> dict+]}:} & 10
 \\ \midrule

\textbf{Database DSL} & 
...CREATE TABLE DB\_PRIVS\textbackslash{}n
(\textbackslash{}n
DB\_GRANT\_ID NUMBER NOT NULL,\textbackslash{}n
CREATE\_TIME NUMBER (10) NOT NULL,\textbackslash{}n
DB\_ID NUMBER NULL,\textbackslash{}n
)...
 & Rename the column \texttt{"CREATE\_TIME"} in the \texttt{DB\_PRIVS} table to \texttt{"CREATION\_TIMESTAMP"} & ...CREATE TABLE DB\_PRIVS\textbackslash{}n
(\textbackslash{}n
DB\_GRANT\_ID NUMBER NOT NULL,\textbackslash{}n
CREATION\_TIMESTAMP NUMBER (10) NOT NULL,\textbackslash{}n
DB\_ID NUMBER NULL,\textbackslash{}n
)... & Line 4 differs: Differences: CREATE\textcolor{red}{[E \texttt{->}ION]}\_TIME\textcolor{red}{[+STAMP+]} NUMBER (10) NOT NULL,
 & 9 \\ \bottomrule
\end{tabular}
}
\caption{Data examples of different data categories with all attributes (content, edit request, edited content, difference, and G-score).}
\label{tab:edit-intentions}
\end{table*}


\section{Evaluation}

\subsection{Experimental Setup}
In this section, we detail the experimental setups, including dataset splits, model variants, baselines, evaluation metrics, and implementation specifics.

\noindent \textbf{Dataset and Model Variants.} We evaluate FineEdit on our proposed InstrEditBench using a 90/10 train-test split. Additionally, we introduce three versions of FineEdit—FineEdit-L, FineEdit-XL, and FineEdit-Pro—fine-tuned from LLaMA-3.2-1B, LLaMA-3.2-3B, and Qwen2.5-3B-Instruct base models, respectively, to cover a wide spectrum of architectures and parameter scales.

\noindent \textbf{Baselines.} Our baselines include Gemini 1.5 Flash, Gemini 2.0 Flash, LLaMA-3.2-1B, LLaMA-3.2-3B, Qwen2.5-3B-Instruct, and Mistral-7B, spanning diverse architectures and sizes. We evaluate both zero-shot and few-shot prompting on the Gemini models, while open-source models are assessed using zero-shot prompting.

\noindent \textbf{Metrics.} Following established approaches~\cite{nakamachi2020text,shen2017style}, we use BLEU and ROUGE‑L metrics to assess the vocabulary and structural consistency between the edited and reference texts.

\noindent \textbf{Implementation details.} For existing models, we strictly adhere to configurations from their original papers. To manage fixed maximum token lengths \(L\), if the combined \(T_{\text{orig}}\) and \(I_{\text{edit}}\) exceed \(L\), we partition \(T_{\text{orig}}\) into chunks of size \(\leq L\), process each chunk independently with the same edit instruction, and concatenate the outputs to form the complete edited text. We fine-tune models using Low-Rank Adaptation (LoRA) \cite{hu2021lora} with \(r=8\), \(\alpha=32\), and a dropout rate of 0.05, employing the AdamW optimizer with a learning rate of \(2 \times 10^{-5}\), training for 2 epochs, an effective batch size of 1, and 4 gradient accumulation steps. During generation, we set the temperature to 0.7 and use top-p sampling with \(p=0.9\), merging outputs from all chunks to produce the final edited text. Additional hyperparameter configurations and training details are provided in Appendix~\ref{sec:appx_implementation_detail}.

\subsection{Performance of Existing Models}

We evaluated FineEdit against several state-of-the-art baselines on the InstrEditBench dataset across four data categories as presented in Table~\ref{tab:llm_comparison}.

\noindent \textbf{Comparison with Zero-shot Performance.} Among all baselines, Gemini 1.5 Flash achieved the highest overall scores, while Mistral-7B-OpenOrca recorded the lowest BLEU and ROUGE-L values. Although model size is often a crucial factor, Gemini 2.0 Flash did not surpass Gemini 1.5 Flash in overall effectiveness. For instance, despite having more parameters than LLaMA-3.2-1B, Mistral-7B-OpenOrca underperformed in both metrics, highlighting the significance of model architecture and training methods. Moreover, while Gemini 2.0 Flash shows superior semantic understanding in the Wiki category—achieving a BLEU score of 0.9133 and a ROUGE-L score of 0.9429—its overall performance remains below that of its counterpart.

FineEdit, and in particular its FineEdit-Pro variant, further outperforms all zero-shot baselines. FineEdit-Pro achieves an overall BLEU score of 0.9245, representing improvements of approximately 11.6\%, 57.7\%, and 184.7\% over Gemini 1.5 Flash (0.8285), LLaMA-3.2-3B (0.5862), and Mistral-7B-OpenOrca (0.3246), respectively. These gains are consistently observed across individual data categories—for example, FineEdit-Pro attains BLEU scores of 0.9521 and 0.9538 in the DSL and Code domains, respectively. These results underscore the effectiveness of FineEdit’s targeted fine-tuning strategy, which focuses on precise editing of location and content to preserve both structural and semantic integrity.

\noindent \textbf{Comparison with Few-shot Performance.} We further evaluated few-shot learning on the Gemini models. Although few-shot performance notably improved in some categories—for example, in the LaTeX domain, where Gemini 2.0 Flash exhibited a 20\% higher BLEU score than in the zero-shot setting—the overall few-shot results still lag behind FineEdit. In certain cases, such as the SQL category, few-shot learning made little difference, with BLEU and ROUGE-L scores of only 0.1600 and 0.1814, respectively. These findings reinforce the value of our curated benchmark in driving improvements in editing tasks.

\begin{minipage}{0.45\textwidth}
\begin{shaded}
    \noindent \textbf{Key Findings:} FindEdit demonstrates robust overall effectiveness across Wiki, Code, DSL, and LaTeX categories. These results not only position FineEdit as a competitive method for structured editing tasks but also provide valuable insights into how targeted training strategies can elevate model performance in diverse application scenarios.
\end{shaded}
\end{minipage}


\begin{table*}[!tbp]
    \centering
    \small
    \renewcommand{\arraystretch}{1.4}
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lcc|cc|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} &
        \multirow{2}{*}{\textbf{Model}} &
        \multirow{2}{*}{\textbf{Open-Source}} &
        \multicolumn{2}{c|}{\textbf{LaTeX}} &
        \multicolumn{2}{c|}{\textbf{DSL}} &
        \multicolumn{2}{c|}{\textbf{Wiki}} &
        \multicolumn{2}{c|}{\textbf{Code}} &
        \multicolumn{2}{c}{\textbf{Overall}} \\
        \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(l){12-13}
        & & & \textbf{BLEU} & \textbf{ROUGE-L}
              & \textbf{BLEU} & \textbf{ROUGE-L}
              & \textbf{BLEU} & \textbf{ROUGE-L}
              & \textbf{BLEU} & \textbf{ROUGE-L}
              & \textbf{BLEU} & \textbf{ROUGE-L} \\
        \midrule
        \multirow{6}{*}{\textbf{Zero-shot}}
            & Gemini 1.5 Flash  & \ding{55} & 0.8665 & 0.9150 & 0.8297 & 0.8555 & 0.7626 & 0.8361 & 0.8551 & 0.9073 & 0.8285 & 0.8819 \\
             
            & Gemini 2.0 Flash  & \ding{55} & 0.7413& 0.7951 & 0.4706 & 0.4964 & 0.9133 & 0.9429 & 0.1339 & 0.2737 & 0.5853 & 0.6519 \\
             
            & Llama-3.2-1B & {\checkmark} & 0.5088 & 0.6108 & 0.5564 & 0.6596 & 0.4413 & 0.5766 & 0.4742 & 0.6072 & 0.4867 & 0.6069 \\
             
            & Llama-3.2-3B & {\checkmark} & 0.5969 & 0.6925 & 0.5747 & 0.6821 & 0.5061 & 0.6384 & 0.6638 & 0.7727 & 0.5862 & 0.6976 \\
             
            & Qwen2.5-3B-Instruct & {\checkmark} & 0.5467 & 0.6712 & 0.4107 & 0.4991 & 0.4170 & 0.5699 & 0.3967 & 0.5390 & 0.4492 & 0.5816 \\
             
            & Mistral-7B-OpenOrca & {\checkmark} & 0.3782 & 0.5770 & 0.0361 & 0.1638 & 0.3608 & 0.5840 & 0.3763 & 0.6447 & 0.3246 & 0.5395 \\
        \midrule
        \multirow{2}{*}{\textbf{Few-shot}}
            & Gemini 1.5 Flash$_{(2-shot)}$   & \ding{55} & 0.8742 & 0.9324 & 0.0908 & 0.1190 & 0.8657 & 0.9139  & 0.7412 & 0.8302 & 0.7249 & 0.7845 \\
            & Gemini 2.0 Flash$_{(2-shot)}$   & \ding{55} & 0.9464 & 0.9723 & 0.1600 & 0.1814 & \textbf{0.9380} & \textbf{0.9665} & 0.8327 & 0.8698 & 0.8011 & 0.8302 \\
        \midrule

        \multirow{3}{*}{\textbf{FineEdit}}
            & FineEdit-L  & {\checkmark} & 0.9311 & 0.9697 & 0.9334 & 0.9615 & 0.8077 & 0.9036 & 0.9296 & 0.9725 & 0.8957 & 0.9504 \\
            & FineEdit-XL  & {\checkmark} & 0.8867 & 0.9502 & 0.9241 & 0.9552 & 0.8120 & 0.9056 & 0.9295 & 0.9720 & 0.8824 & 0.9441 \\
            & FineEdit-Pro & {\checkmark} & \textbf{0.9539} & \textbf{0.9821} &\textbf{ 0.9521} &\textbf{ 0.9710 } &  0.8521 & 0.9185 & \textbf{ 0.9538} & \textbf{0.9836} & \textbf{0.9245} & \textbf{0.9628} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Comparison of LLMs on BLEU and ROUGE-L  for LaTeX, DSL, Wiki, Code. Overall data displays average performance among all data categories. The best results are highlighted in bold.}
    \label{tab:llm_comparison}
\end{table*}


\subsection{FineEdit: Supervised Finetuning}

Our FineEdit model is offered in three variants: FineEdit-L, FineEdit-XL, and FineEdit-Pro. Under zero-shot conditions, FineEdit-L consistently outperforms all baseline models in BLEU and ROUGE-L scores for LaTeX, DSL, Wiki, and Code tasks. For example, compared to Gemini 1.5 Flash, FineEdit-L improves overall BLEU scores by roughly 8\%, with even larger gains observed in specific categories. Notably, FineEdit-XL performs similarly to FineEdit-L, suggesting that increasing the parameter count from 1B to 3B using LLaMA does not yield a significant performance boost.

By leveraging the superior instruction-following capabilities of Qwen2.5-3B-Instruct, our final variant, FineEdit-Pro, further elevates performance. FineEdit-Pro achieves an overall BLEU score of 0.9245, which represents improvements of approximately 11.6\% over Gemini 1.5 Flash, and gains of around 14.7\% and 11.7\% in the DSL and Wiki tasks, respectively. These consistent improvements across multiple data categories underscore the effectiveness of our supervised fine-tuning strategy and highlight the importance of a strong instruction-tuned base model over merely increasing model size.

We also compared our models with Gemini's few-shot prompting approach in real-world scenarios. Although in-context learning (ICL) boosts Gemini’s performance in some cases—such as a 8\% higher BLEU score in Wiki dataset for Gemini 2.0 Flash—the overall results still lag behind FineEdit-Pro. This evidence confirms that our tailored supervised fine-tuning approach yields a more robust and generalizable solution for structured editing tasks.

\begin{minipage}{0.45\textwidth}
\begin{shaded}
    \noindent \textbf{Key Findings:} FineEdit's supervised fine-tuning markedly enhances performance. FineEdit-L surpasses zero-shot baselines and FineEdit-XL offers comparable gains, while FineEdit-Pro (built on Qwen2.5-3B-Instruct) achieves the highest scores. This highlights that robust instruction tuning is more effective than merely scaling model size.
\end{shaded}
\end{minipage}


\subsection{Qualitative Study}

To qualitatively assess the performance of FindEdit, we conduct several studies as shown in Figure~\ref{fig:qualitative_study}. This figure illustrates eight examples of how FineEdit-Pro and Gemini respond to diverse editing requests. In several cases, FineEdit-Pro accurately applies changes—such as adding new columns in DSL or adjusting environment commands—while Gemini often restates the instruction without implementing the intended modifications.

Specifically, both Gemini 1.5 Flash and 2.0 Flash perform well on LaTeX and Wiki tasks, yet they struggle with DSL and Code tasks. For example, as shown in Figure~\ref{fig:qualitative_study}, FineEdit-Pro correctly identifies the target table and appends a new column named \texttt{created\_at} with the data type \texttt{DEFAULT CURRENT\_TIMESTAMP}. In contrast, Gemini misinterprets the instruction, merely repeating the edit request rather than applying the intended change. These observations highlight the qualitative strengths of our proposed FineEdit approach.

Nonetheless, FineEdit is not without shortcomings. In the LaTeX example depicted in Figure~\ref{fig:qualitative_study}, Gemini accurately locates the \texttt{\/subsection\{Strengths\}} and updates it as specified. However, although FineEdit-Pro also identifies and modifies the correct location, it generates the correct response twice, which deviates from the direct editing requirement. This discrepancy suggests that FineEdit-Pro, though generally more reliable, can overapply modifications in specific cases.

Overall, these results illustrate FineEdit-Pro’s capacity to handle more complex edits, particularly for DSL and Code, while Gemini often fails to implement them. Nevertheless, occasional issues like duplicate outputs highlight the need for refinement, ensuring FineEdit-Pro consistently adheres to direct editing requirements without introducing redundant content. On the other hand, Gemini occasionally performs better in simpler tasks, such as LaTeX updates.

\begin{minipage}{0.45\textwidth}
\begin{shaded}
    \noindent \textbf{Key Findings:} FineEdit-Pro demonstrates superior handling of DSL and Code edits compared to Gemini, though minor issues such as duplicate outputs in LaTeX tasks remain. Overall, FineEdit's qualitative performance confirms its robust ability to interpret and execute complex editing instructions.
\end{shaded}
\end{minipage}

\begin{figure*}[!tbp]
    \centering
    \includegraphics[width=0.88\textwidth]{figure/figure3_v3.pdf}
    \caption{Comparison between Gemini and FindEdit Pro response.}
    \label{fig:qualitative_study}
\end{figure*}

\subsection{Human Evaluation}

To assess whether \texttt{DiffEval} enhances overall dataset quality, we conducted a human evaluation. Given that our dataset includes Code and DSL categories—areas closely tied to computer programming—we have three evaluators, each holding at least a Bachelor's degree in a Computer Science-related field. We established the following guidelines to ensure rigorous assessment:
(1) {Precise Observation:} Confirm that the updated content exactly corresponds to the segment specified by the edit request. (2) {No Unintended Modifications:} Verify that no other sections have been altered; any unexpected changes result in failure. (3) {Three-Round Procedure:} Two evaluators independently review each item, with a third evaluator resolving any discrepancies.

We examined 100 items per category and found that data processed through our DiffEval pipeline exhibited noticeably enhanced accuracy, as shown in Table~\ref{tab:human_eval}. The Wiki and Code datasets, in particular, demonstrated the most reliable outcomes, with edited content precisely matching the requested modifications. Notably, the DSL dataset experienced the greatest improvement, with quality increasing by over 24\% compared to data that did not meet DiffEval’s standards.

\begin{table}[!tbp]
\centering
\renewcommand\arraystretch{0.95}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lcccc}
\hline
       & Wiki & LaTeX & DSL & Code \\
\hline
G-score $\geq$ 9  & 97\% & 93\% & 90\% & 97\% \\
G-score $<$ 9   & 87\% & 89\% & 66\% & 83\% \\
\hline
\end{tabular}
}
\caption{Sample performance based on the G-Score}
\label{tab:human_eval}
\end{table}

\begin{minipage}{0.45\textwidth}
\begin{shaded}
    \noindent \textbf{Key Findings:} The DiffEval pipeline significantly improves dataset quality, with Wiki and Code categories achieving high precision, and the DSL category showing over 24\% enhancement in quality.
\end{shaded}
\end{minipage}


\section{Conclusion}
This work addresses the critical gap in LLMs' ability to perform precise and targeted text modifications. We introduce {InstrEditBench}, a high-quality benchmark with 20,000+ structured editing tasks across Wiki articles, LaTeX documents, code, and database DSLs, enabling rigorous evaluation of direct editing capabilities. To further advance LLMs’ editing proficiency, we propose {FineEdit}, a specialized model trained on this benchmark. Extensive evaluations demonstrate that FineEdit outperforms state-of-the-art models, including GPT-4o, Gemini 2.0, and LLaMa-3.2, with up to 10\% improvement compared to Gemini in direct editing task performance.

\clearpage

\section{Limitations}

\noindent \textbf{Limited Deployment Scope.} Due to cost and hardware constraints, our evaluations were limited to large proprietary LLMs (e.g., Gemini), rather than large open-source models.

\noindent \textbf{Controlled Context Evaluation.} Our benchmark focuses on controlled evaluation context, where it does not yet encompass long-context chain-of-thought scenarios, as smaller LLMs are confined by limited context windows, even though such techniques could be effective in proprietary models
%\bibliographystyle{plain}%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.
\input{main.bbl}  % 直接使用 bbl


%%% Comment out this section when you \bibliography{references} is enabled.


\appendix

\section{Dataset Generation Prompts}
\label{sec:dataset_generation_prompts}

We use the following prompts for dataset generation on each domain.
\begin{tcolorbox}
\small
\texttt{
user\_prompt = r'''Task: Gener
ate one precise editing request for the given LaTeX code, focusing exclusively on one detailed LaTeX-specific aspect.\\
\ \ \ \ \ 1. Analyze LaTeX Components: Examine the LaTeX code thoroughly, identifying elements such as commands, environments, packages, mathematical expressions, figures, tables, references, labels, and syntax structures.\\
\ \ \ \ \ 2. Target a Single LaTeX Issue: The editing request must address only one specific LaTeX-related issue such as commands, environments, packages, mathematical expressions, figures, tables, references, labels, and syntax structures.\\
\ \ \ \ \ 3. Clearly define the exact edit needed. The action should be definitive and unambiguous, avoiding any form of suggestion, optional language, or choices. Do not include reasons for the edit or any additional information beyond the request.\\
\ \ \ \ \ 4. Do not include reasons for the edit or any additional information beyond the edit request. The request should be a direct instruction.\\
\ \ \ \ \ The request examples are:\\
\ \ \ \ \ [Example 1]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Replace the \textbackslash begin\{equation\} ... \textbackslash end\{equation\} environment with a \textbackslash [ ...\textbackslash ] display math environment to present the equation.\\
\ \ \ \ \ </Edit Request>\\
\ \ \ \ \ [Example 2]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Remove the \textbackslash centering command inside the figure environment and insert \textbackslash centering immediately after \textbackslash begin\{figure\}.\\
\ \ \ \ \ </Edit Request>\\
\ \ \ \ \ [Example 3]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Change the citation command \textbackslash cite\{einstein\} to \textbackslash parencite\{einstein\} to display the citation in parentheses.\\
\ \ \ \ \ </Edit Request>\\
\ \ \ \ \ [Example 4]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Change the column specification in the tabular environment from \{l l l\} to \{l c r\} to adjust the alignment of the data columns.\\
\ \ \ \ \ </Edit Request>\\
\ \ \ \ \ [Example 5]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Replace the placeholder ??? in the reference text with \textbackslash ref\{sec:relwork\} to properly reference the “Related Work” section.\\
\ \ \ \ \ </Edit Request>\\
\ \ \ \ \ [Example 6]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Rename the macro \textbackslash vect to \textbackslash vecbold in both its definition and throughout the document.\\
\ \ \ \ \ </Edit Request>\\
}
\end{tcolorbox}
\begin{tcolorbox}
\small
\texttt{
\ \ \ \ \ [Example 7]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Add the optional width argument to \textbackslash includegraphics\{example-image\} as \textbackslash includegraphics[width=0.5\textbackslash textwidth] \\
\ \ \ \ \ \{example-image\} to scale the image.\\
\ \ \ \ \ </Edit Request>\\
\ \ \ \ \ [Example 8]\\
\ \ \ \ \ <Edit Request>\\
\ \ \ \ \ Remove the \textbackslash usepackage\{epsfig\} line and replace it with \textbackslash usepackage\{graphicx\} to handle graphics\\
\ \ \ \ \ </Edit Request>\\
\\
\ \ \ \ \ I will give you the content and then the editing request.\\ 
\ \ \ \ \ Please Edit the content based on the editing request. \\
\ \ \ \ \ While Editing, don't add other words like\\
\ \ \ \ \ modified or something. Just Edit directly. \\
\\
\ \ \ \ \ Content: \{original\_context\} \\
\ \ \ \ \ Editing Request: \{edit\_request\} \\
\ \ \ \ \ Please return the complete content after editing. \\
\ \ \ \ \ Don't skip the empty line and keep the original\\
\ \ \ \ \ apart from the editing part.
}
\end{tcolorbox}

We use the following prompt for doing G-eval.
% \FloatBarrier  % 防止浮动元素跨越 section
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figure/figure2_v6.pdf}
    \vspace{-3mm}
    \label{fig:additional-edit-example}
\end{figure}

\section{Additional Implementation Details}
\label{sec:appx_implementation_detail}


\noindent \textbf{Chunking long context:}
Many large language models impose a fixed maximum token length \( L \) on their input (and sometimes output) sequences. Consequently, if the combination of \( T_{\text{orig}} \) and \( I_{\text{edit}} \) exceeds this limit, we divide the \( T_{\text{orig}} \) into smaller chunks of size \( \leq L \). Each chunk is then processed independently—paired with the same edit request and later concatenated to form the complete edited text. This approach ensures that every chunk fits within the model’s token budget, preventing overflow and reducing memory usage while preserving the overall structured editing behavior. 

\noindent \textbf{Fine-Tuning Strategy:}  
We use Low-Rank Adaptation (LoRA) \cite{hu2021lora} to efficiently adapt these models to our task, significantly reducing the number of trainable parameters while preserving their expressive power. In all LoRA configurations, We set the rank $r=8$ and scaling $\alpha=32$, and use a dropout probability of 0.05. For both Llama-based and Qwen-based models, we apply  LoRA to the attention's projection layers through trainable low-rank matrices. We used the AdamW optimizer with a learning rate of $2 \times 10^{-5}$, training for 2 epochs, and set the effective batch size of 1 with gradient accumulation steps of 4 due to device limits. This strategy not only reduces computational overhead but also enables rapid convergence on our structured editing tasks. Preliminary experiments guided the choice of hyperparameters across all three model variants. 

\noindent \textbf{Decoding and Inference:}  
During generation, we set the temperature to 0.7 and used top-p sampling with a probability of 0.9 to balance diversity and coherence. Greedy decoding is applied by default if without sampling setting. The final edited text is obtained by merging the edited outputs from all chunks.

% \section{Additional Data Example}

% We provide several additional qualitative results as displayed in Figure~\ref{fig:additional-edit-example}.

% \begin{figure}[!tbp]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figure/Additional Figure.pdf}
%     \caption{FileEdit and Original LLMs differ in editing tasks, FileEdit can reduce LLM hallucinations and follow task instructions to complete tasks.}
%     \vspace{-3mm}
%     \label{fig:additional-edit-example}
% \end{figure} 





\end{document}
