\section{Related Work}
\subsection{Proof assistants}\label{ssec:proofassist}

Over time, many softwares have been created to algorithmically check the correctness of mathematical proofs written in formalized mathematics, or a machine-readable language. Most proof assistants are based on type theory, which groups mathematical objects into types like integers and functions. 

In most proof assistants, proving a theorem means transforming a beginning state to an end state using tactics, or preexisting theorems. Each state could also be considered a theorem of its own. For example, the state ``\texttt{(a b c : $\mathbb{R}$) (a * b) * c = a * (b * c)}'' is the associative property of multiplication.

An early example of a proof assistant is HOL, which allows theorems to be proven in higher-order logic. However, it is fairly difficult to learn. Currently, the most popular proof assistants are Lean **Koellenmeier et al., "Formalizing Basic Real Analysis"**__**, Agda **Hage et al., "Agda's new development environment"**__**, Coq **Bertot et al., "The Coq Proof Assistant"**__**, and Isabelle **Nipkow et al., "Isabelle/HOL"**__. Among them, Lean has gained the most popularity in recent years. Lean allows the user to introduce hypotheses or subcases, and a large amount of work can be offloaded to these tactics, making automated theorem proving much easier. In late 2023 Terrence Tao, Timothy Gowers, Ben Green, and Freddie Manners proved the polynomial Freiman-Ruzsa (PFR) conjecture using Lean **Tao et al., "Polynomial Freiman-Ruzsa Conjecture"**__. In 2024 DeepMind unveiled the Lean-based AlphaProof ____, which was close to getting a gold medal in IMO 2024, together with AlphaGeometry.   

\subsection{Automated Theorem Provers}\label{ssec:atp}
Currently, there are many models that attempt to perform automated theorem proving using methods like premise selection, which selects several potential tactics, and chooses one to apply. Recently, language models like GPT-f **Brown et al., "Language Models Play D&D"**__ leveraging the power of generative models have also become viable for ATP. 

The HOList __ environment represents a significant step in applying machine learning to automated theorem proving in the primitive HOL formal logic language, as it was one of the earliest neural network based approaches. HOList integrates neural networks and reinforcement learning to guide the proof search process. Bansal et al. __ have also created a comprehensive dataset derived from formal proofs in the HOL Light system, capturing proof states, tactics, and other relevant information. 

Paliwal et al. (2019) **Paliwal et al., "Graph Neural Networks for Higher-Order Logic"**__ investigates the use of graph neural networks (GNNs) to improve HOL theorem proving. The authors propose several graphical representations of higher order logic, including abstract syntax trees (ASTs), which are a tree representation of formal logic expressions. Each logical expression, including function applications, variable bindings, and operators, is encoded as a subtree within a larger AST. Nodes in an AST are variables, variable operators like functions, function definitions, and constant values. The study demonstrates that GNNs can effectively guide the proof search process by learning from these graph representations, leading to significant performance improvements in theorem proving tasks.

A very recent study employing large language models and Lean is ReProver ____, which performs premise generation by first embedding the theorem state, and then choosing the premises with embeddings closest to the theorem's. ReProver's training data is obtained via LeanDojo ____, a program that enables interaction with and extracts specific theorem data from the Lean formal logic environment. Other methods like PACT ____ put more emphasis on the training format, trying methods like predicting masked parts of a proof. 

\subsection{LLMs As Automated Theorem Provers}\label{ssec:llmatp}
In recent years we have witnessed the improvements of LLMs in solving generic math problems. However, the generic purpose LLMs have disappointing performances on generating proofs in formal languages such as Lean. This is because generating mathematical proofs in a formal language is a distinct challenge, differing significantly from proving theorems in natural language. Consider the example from Lean's official tutorial (Mathematics in Lean), which proves ``\texttt{a * 1 = a}'', where \texttt{a} belongs to a group \texttt{G}.

In a typical mathematical book, one can probably prove the above theorem directly using the group's property. For example, here is a proof given by GPT-4o:

\vspace{0.1in}

\begin{quote}

``To prove the statement $a \cdot 1 = a$ where a belongs to a group, we rely on the group axiom of Identity element: There exists an element $1 \in G$ such that for every $a \in G$, $a \cdot 1 = 1 \cdot a = a$.

\vspace{0.1in}

\noindent By this axiom of the group, the identity element 1 satisfies $a \cdot 1 = a$ for all $a \in G$. Therefore, this equality holds directly from the definition of the identity element in a group.''
\end{quote}

\vspace{0.1in}

In contrast, below is the proof from \emph{Mathematics In Lean}, the official tutorial of Lean's Mathlib.

\vspace{0.1in}

\begin{quote}
    \hspace{-0.3in}
        \fbox{\begin{minipage}{3.25in}
        \small
            \texttt{theorem mul\_one (a : G) : a * 1 = a := by} \\
            \texttt{rw [$\leftarrow$ mul\_left\_inv a]} \\
            \texttt{rw [$\leftarrow$ mul\_assoc]} \\
            \texttt{rw [mul\_inv\_cancel]} \\
            \texttt{rw [one\_mul]}
        \end{minipage}}
\end{quote}

\vspace{0.1in}

We can see the proof in Lean is much more complex. Mathlib contains a theorem saying ``\texttt{1 * a = a}'', we must convert the above theorem into that. However, four steps are needed because group multiplication is noncommutative. The first step rewrites the ``\texttt{1}'' into ``\texttt{(a$^{-1}$ * a)}'' using the definition of a multiplicative inverse. The second step removes the parentheses using the associative property of multiplication. The third step uses the multiplicative inverse definition again, turning the state into ``\texttt{1 * a = a}''. The final step proves the theorem using the fact that multiplication between the identity element of a group and any other element of it keeps it the same. 

From the above example we can see that, although LLMs have been trained on huge datasets and could finish some relatively easy proofs in natural languages, such training is not very helpful for generating proofs in Lean.