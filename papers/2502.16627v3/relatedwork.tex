\section{Literature Review}
The expanding field of artificial intelligence (AI) has increasingly emphasized sustainability, particularly in optimizing transformer models for time series classification and forecasting \cite{Kaur2024LeveragingAI}. Initially developed for natural language processing (NLP), transformers have demonstrated remarkable proficiency in handling sequential data, leading to their adaptation for time series tasks. This literature review consolidates key findings from foundational studies, exploring optimization strategies such as pruning and quantization while evaluating their impact on model accuracy, computational efficiency, and energy consumption.

\subsection{Transformers in Time Series Forecasting and Classification}

Transformers utilize self-attention mechanisms to capture long-range dependencies in sequential data, making them particularly effective for time series forecasting and classification~\cite{Wen2022,vaswani2017attention}. Traditional models, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, struggle with long-range dependencies due to their sequential nature, often suffering from vanishing gradient issues. In contrast, transformers process all time steps simultaneously, enhancing their ability to learn complex temporal patterns~\cite{Zhou2021}.

A significant challenge in applying transformers to time series data is their computational complexity. The self-attention mechanism scales quadratically with sequence length, making it computationally expensive for long time series. Various optimization strategies have been developed to address this limitation. The FEDformer model integrates frequency-enhanced decomposition techniques to reduce computational demands while maintaining high forecasting accuracy~\cite{Tian2022}. Similarly, the Informer architecture adopts a probabilistic attention approach to reduce computational burden by focusing on the most relevant portions of the input sequence~\cite{Zhou2021}. These advancements illustrate the ongoing efforts to improve transformer efficiency in time series applications.

Domain-specific preprocessing has further enhanced transformer performance. Wavelet transforms have been employed to enable multi-resolution representations of time series data, improving their ability to capture both local and global patterns~\cite{Tao2023}. In addition, ensemble learning techniques have been explored to enhance forecasting accuracy~\cite{Li2024}.

Empirical evaluations have reinforced transformers' effectiveness in time series forecasting and classification. A study by Lara-Benítez et al.~\cite{LaraBenitez2021} analyzed transformer performance across 12 datasets, providing crucial insights into their advantages and limitations across various domains. This comprehensive evaluation not only highlighted the strong performance of transformers in various settings but also revealed specific scenarios where they perform exceptionally well, as well as cases where their effectiveness might be limited.

\subsection{Model Compression Strategies}

In recent years, model compression has gained attention in the domain of deep learning as it directly addresses challenges related to the large size and computational demands of neural networks. Two prominent strategies employed in model compression are pruning and quantization. 

Pruning is a well-established method for reducing model size by removing parameters deemed less significant. This technique has been applied to transformers to improve efficiency while preserving accuracy. Pruning-guided feature distillation has been introduced to create lightweight transformer architectures that maintain predictive performance while reducing computational costs~\cite{Kim2024}. Additionally, global structural pruning has demonstrated significant reductions in latency and computational requirements~\cite{Liu2023}. Cheong~\cite{Cheong2019} further highlights the role of pruning in compressing transformers to enhance inference speed and energy efficiency.

Quantization reduces the precision of model weights and activations, making it a popular method for decreasing memory usage and enhancing inference speed. Quantization-aware training has demonstrated effectiveness in reducing memory footprints while maintaining accuracy~\cite{Zhu2023}. Techniques such as mixed-precision quantization~\cite{Xu2021}, post-training quantization~\cite{Liu2023}, and quantized feature distillation have been effective in reducing resource consumption.


\subsection{Optimizing Transformer Inference}

Beyond pruning and quantization, structural modifications have been explored to enhance transformer efficiency. Gated Transformer Networks (GTNs) improve feature extraction by capturing both channel-wise and step-wise correlations in multivariate time series data~\cite{Liu2021}. Sparse binary transformers have also demonstrated their effectiveness in reducing parameter redundancy while preserving model performance~\cite{Gorbett2023}. Hybrid methodologies, such as Autoformer, leverage auto-correlation mechanisms to enhance long-term forecasting accuracy~\cite{Wu2021}.

Further research into inference optimization has underscored the significance of architectural bottlenecks, hardware constraints, and algorithmic refinements. A full-stack co-design approach that integrates software and hardware optimizations has achieved up to an 88.7× speedup in inference without compromising accuracy~\cite{kim2023stackoptimizationtransformerinference}. Similarly, comprehensive surveys of transformer inference optimization strategies highlight the effectiveness of pruning, quantization, knowledge distillation, and hardware acceleration in reducing latency and energy consumption while maintaining predictive performance~\cite{chitty2023survey}.
GPU-accelerated optimal inferential control framework was proposed using ensemble Kalman smoothing to efficiently handle high-dimensional spatio-temporal CNNs \cite{vaziri2024optimalinferentialcontrolconvolutional}.

A study utilized the NIOT framework, specifically designed for modern CPUs, which integrates architecture-aware optimizations such as memory tiling, thread allocation, and cache-friendly fusion strategies. These improvements have led to latency reductions of up to 29\% for BERT \cite{devlin2019bert} and 43\% for vision transformers, significantly surpassing traditional inference techniques~\cite{zhang2023niot}.



Energy efficiency remains a critical concern in transformer-based models, particularly in applications requiring continuous inference. The integration of optimized data preprocessing techniques has shown significant potential in improving both computational efficiency and predictive accuracy. ~\cite{10.1016/j.ijinfomgt.2020.102282} emphasize that the structural representation of weather-related features substantially impacts forecasting performance. Their findings highlight the necessity of refining preprocessing pipelines to enhance energy efficiency in transformer-based applications.

The sustainability of transformer models has been analyzed within the broader framework of green computing. A study introduced the concept of green algorithms, providing a quantitative framework for assessing the carbon footprint of computational tasks~\cite{Lannelongue2021}. This metric is instrumental in evaluating the environmental impact of transformer-based architectures in time series classification, reinforcing the importance of computational efficiency in sustainable AI practices.

Several studies have examined the trade-offs between performance and energy efficiency in transformer inference. Some studies present empirical evaluations illustrating how software-level optimizations can significantly lower energy consumption without sacrificing predictive accuracy \cite{Bannour2021,dice2021optimizing}. These findings underscore the necessity for targeted optimization strategies, particularly for CPU-based inference, where resource constraints are a fundamental challenge.

In addition to these studies, a research introduced the Greenup, Powerup, and Speedup (GPS-UP) metrics to evaluate energy efficiency in software optimizations ~\cite{abdulsalam2015using}. By categorizing computational trade-offs into multiple distinct scenarios, their study provides a structured framework for analyzing the relationship between software modifications and energy consumption. Unlike conventional energy-delay metrics, GPS-UP facilitates a more nuanced evaluation of how performance improvements interact with power efficiency, contributing to the development of sustainable yet high-performance transformer models.