@article{10.1016/j.ijinfomgt.2020.102282,
  author = {Nguyen, H. D. and Tran, K. P. and Thomassey, S. and Hamad, M. M.},
  title = {Forecasting and anomaly detection approaches using lstm and lstm autoencoder techniques with the applications in supply chain management},
  journal = {International Journal of Information Management},
  year = {2021},
  volume = {57},
  pages = {102282},
  doi = {10.1016/j.ijinfomgt.2020.102282}
}

@inproceedings{Bannour2021,
  author    = {Bannour, Nesrine and Ghannay, Sahar and Névéol, Aurélie and Ligozat, Anne-Laure},
  title     = {Evaluating the Carbon Footprint of NLP Methods: A Survey and Analysis of Existing Tools},
  booktitle = {Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing},
  pages     = {11--21},
  year      = {2021}
}

@misc{Cheong2019,
  author    = {Cheong, Robin},
  title     = {Transformers . zip: Compressing Transformers with Pruning and Quantization},
  year      = {2019}
}

@article{Gorbett2023,
  author    = {Gorbett, M. and Shirazi, H. and Ray, I.},
  title     = {Sparse Binary Transformers for Multivariate Time Series Modeling},
  journal   = {Proceedings of ACM Conference},
  pages     = {544-556},
  year      = {2023},
  doi       = {10.1145/3580305.3599508}
}

@article{Kaur2024LeveragingAI,
title={Leveraging Artificial Intelligence for Enhanced Sustainable Energy Management},
author={Swapandeep Kaur and Raman Kumar and Kanwardeep Singh and Yinglai Huang},
journal={Journal of Sustainability for Energy},
year={2024},
page={1-20},
doi={https://doi.org/10.56578/jse030101}
}

@article{Kim2024,
  author    = {Kim, D.},
  title     = {Pruning-Guided Feature Distillation for an Efficient Transformer-Based Pose Estimation Model},
  journal   = {IET Computer Vision},
  volume    = {18},
  number    = {6},
  pages     = {745-758},
  year      = {2024},
  doi       = {10.1049/cvi2.12277}
}

@article{Lannelongue2021,
  author    = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},
  title     = {Green Algorithms: Quantifying the Carbon Footprint of Computation},
  journal   = {Advanced Science},
  volume    = {8},
  number    = {12},
  pages     = {2100707},
  year      = {2021},
  doi       = {10.1002/advs.202100707}
}

@article{LaraBenitez2021,
  author    = {Lara-Benítez, P. and Gallego-Ledesma, L. and Carranza-García, M. and Luna-Romera, J.},
  title     = {Evaluation of the Transformer Architecture for Univariate Time Series Forecasting},
  journal   = {Springer},
  pages     = {106--115},
  year      = {2021},
  doi       = {10.1007/978-3-030-85713-4-11}
}

@article{Li2024,
  author    = {Li, D.},
  title     = {TST-Refrac: A Novel Transformer Variant for Prediction of Production of Re-Fractured Oil Wells},
  journal   = {Journal of Physics: Conference Series},
  volume    = {2901},
  number    = {1},
  pages     = {012030},
  year      = {2024},
  doi       = {10.1088/1742-6596/2901/1/012030}
}

@article{Liu2021,
  author    = {Liu, M.},
  title     = {Gated Transformer Networks for Multivariate Time Series Classification},
  journal   = {arXiv preprint},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2103.14438},
  url       = {https://doi.org/10.48550/arxiv.2103.14438}
}

@article{Liu2023,
  author    = {Liu, S.},
  title     = {LLM-FP4: 4-Bit Floating-Point Quantized Transformers},
  journal   = {EMNLP},
  year      = {2023},
  doi       = {10.18653/v1/2023.emnlp-main.39}
}

@article{Tao2023,
  author    = {Tao, G.},
  title     = {Time Series Forecasting Based on the Wavelet Transform},
  journal   = {SPIE},
  year      = {2023},
  doi       = {10.1117/12.2684645}
}

@article{Tian2022,
  author    = {Tian, Z. and Ma, Z. and Wen, Q. and Wang, X. and Sun, L. and Jin, R.},
  title     = {FEDformer: Frequency Enhanced Decomposed Transformer for Long-Term Series Forecasting},
  journal   = {arXiv preprint},
  year      = {2022},
  archivePrefix = {arXiv},
  eprint    = {2201.12740}
}

@article{Wen2022,
  author    = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  title     = {Transformers in Time Series: A Survey},
  journal   = {arXiv preprint},
  year      = {2022},
  archivePrefix = {arXiv},
  eprint    = {2202.07125}
}

@article{Wu2021,
  author    = {Wu, H. and Xu, J. and Wang, J. and Long, M.},
  title     = {Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},
  journal   = {arXiv preprint},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2106.13008}
}

@article{Xu2021,
  author    = {Xu, J. and Hu, S. and Yu, J. and Liu, X. and Meng, H.},
  title     = {Mixed Precision Quantization of Transformer Language Models for Speech Recognition},
  journal   = {Proceedings of ICASSP},
  year      = {2021},
  doi       = {10.1109/icassp39728.2021.9414076}
}

@article{Zhou2021,
  author    = {Zhou, H. et al.},
  title     = {Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2021},
  doi       = {10.1609/aaai.v35i12.17325}
}

@article{Zhu2023,
  author    = {Zhu, K. et al.},
  title     = {Quantized Feature Distillation for Network Quantization},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2023},
  doi       = {10.1609/aaai.v37i9.26354}
}

@inproceedings{abdulsalam2015using,
  title={Using the Greenup, Powerup, and Speedup metrics to evaluate software energy efficiency},
  author={Abdulsalam, Sarah and Zong, Ziliang and Gu, Qijun and Qiu, Meikang},
  booktitle={2015 Sixth International Green and Sustainable Computing Conference (IGSC)},
  pages={1--8},
  year={2015},
  organization={IEEE}
}

@article{chitty2023survey,
  title={A survey of techniques for optimizing transformer inference},
  author={Chitty-Venkata, Krishna Teja and Mittal, Sparsh and Emani, Murali and Vishwanath, Venkatram and Somani, Arun K},
  journal={Journal of Systems Architecture},
  pages={102990},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{dice2021optimizing,
  title={Optimizing inference performance of transformers on CPUs},
  author={Dice, Dave and Kogan, Alex},
  journal={arXiv preprint arXiv:2102.06621},
  year={2021}
}

@misc{kim2023stackoptimizationtransformerinference,
      title={Full Stack Optimization of Transformer Inference: a Survey}, 
      author={Sehoon Kim and Coleman Hooper and Thanakul Wattanawong and Minwoo Kang and Ruohan Yan and Hasan Genc and Grace Dinh and Qijing Huang and Kurt Keutzer and Michael W. Mahoney and Yakun Sophia Shao and Amir Gholami},
      year={2023},
      eprint={2302.14017},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.14017}, 
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{vaziri2024optimalinferentialcontrolconvolutional,
      title={Optimal Inferential Control of Convolutional Neural Networks}, 
      author={Ali Vaziri and Huazhen Fang},
      year={2024},
      eprint={2410.09663},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2410.09663}, 
}

@article{zhang2023niot,
  title={NIOT: A Novel Inference Optimization of Transformers on Modern CPUs},
  author={Zhang, Zining and Chen, Yao and He, Bingsheng and Zhang, Zhenjie},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={6},
  pages={1982--1995},
  year={2023},
  publisher={IEEE}
}

