@article{sweere2022deep,
  title={Deep learning-based super-resolution and de-noising for XMM-newton images},
  author={Sweere, Sam F and Valtchanov, Ivan and Lieu, Maggie and Vojtekova, Antonia and Verdugo, Eva and Santos-Lleo, Maria and Pacaud, Florian and Briassouli, Alexia and C{\'a}mpora P{\'e}rez, Daniel},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={517},
  number={3},
  pages={4054--4069},
  year={2022},
  publisher={Oxford University Press}
}

@article{vojtekova2021learning,
  title={Learning to denoise astronomical images with U-nets},
  author={Vojtekova, Antonia and Lieu, Maggie and Valtchanov, Ivan and Altieri, Bruno and Old, Lyndsay and Chen, Qifeng and Hroch, Filip},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={503},
  number={3},
  pages={3204--3215},
  year={2021},
  publisher={Oxford University Press}
}
@article{ChittyVenkata2023,
  author    = {Chitty-Venkata, Krishna Teja and Mittal, Sparsh and Emani, Murali Krishna and Vishwanath, Venkatram and Somani, Arun},
  title     = {A Survey of Techniques for Optimizing Transformer Inference},
  journal   = {J. Syst. Archit.},
  volume    = {144},
  pages     = {102990},
  year      = {2023},
  doi       = {10.1016/j.sysarc.2023.102990}
}

@misc{Cheong2019,
  author    = {Cheong, Robin},
  title     = {Transformers . zip: Compressing Transformers with Pruning and Quantization},
  year      = {2019}
}

@article{Wen2022,
  author    = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  title     = {Transformers in Time Series: A Survey},
  journal   = {arXiv preprint},
  year      = {2022},
  archivePrefix = {arXiv},
  eprint    = {2202.07125}
}

@article{DiceKogan2021,
  author    = {Dice, Dave and Kogan, Alex},
  title     = {Optimizing Inference Performance of Transformers on CPUs},
  journal   = {arXiv preprint},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2102.06621}
}

@inproceedings{Bannour2021,
  author    = {Bannour, Nesrine and Ghannay, Sahar and Névéol, Aurélie and Ligozat, Anne-Laure},
  title     = {Evaluating the Carbon Footprint of NLP Methods: A Survey and Analysis of Existing Tools},
  booktitle = {Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing},
  pages     = {11--21},
  year      = {2021}
}

@article{Lannelongue2021,
  author    = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},
  title     = {Green Algorithms: Quantifying the Carbon Footprint of Computation},
  journal   = {Advanced Science},
  volume    = {8},
  number    = {12},
  pages     = {2100707},
  year      = {2021},
  doi       = {10.1002/advs.202100707}
}

@article{Micucci2017,
  author    = {Micucci, Daniela and Mobilio, Marco and Napoletano, Paolo},
  title     = {UniMiB SHAR: A Dataset for Human Activity Recognition Using Acceleration Data from Smartphones},
  journal   = {Applied Sciences},
  volume    = {7},
  number    = {10},
  pages     = {1101},
  year      = {2017},
  doi       = {10.3390/app7101101}
}

@phdthesis{Arcidiacono2022,
  author    = {Arcidiacono, Andrea},
  title     = {Efficient Transformer Attentions in Time Series Forecasting},
  school    = {Politecnico di Torino},
  year      = {2022}
}

@article{Alharthi2024,
  author    = {Alharthi, M.},
  title     = {Enhanced Linear and Vision Transformer-Based Architectures for Time Series Forecasting},
  year      = {2024},
  doi       = {10.20944/preprints202404.1024.v1}
}

@article{Arthur2024,
  author    = {Arthur, C.},
  title     = {Autocyclic: Deep Learning Optimizer for Time Series Data Prediction},
  journal   = {IEEE Access},
  volume    = {12},
  pages     = {14014-14026},
  year      = {2024},
  doi       = {10.1109/access.2024.3356553}
}

@article{Gorbett2023,
  author    = {Gorbett, M. and Shirazi, H. and Ray, I.},
  title     = {Sparse Binary Transformers for Multivariate Time Series Modeling},
  journal   = {Proceedings of ACM Conference},
  pages     = {544-556},
  year      = {2023},
  doi       = {10.1145/3580305.3599508}
}

@article{Kim2024,
  author    = {Kim, D.},
  title     = {Pruning-Guided Feature Distillation for an Efficient Transformer-Based Pose Estimation Model},
  journal   = {IET Computer Vision},
  volume    = {18},
  number    = {6},
  pages     = {745-758},
  year      = {2024},
  doi       = {10.1049/cvi2.12277}
}

@article{LaraBenitez2021,
  author    = {Lara-Benítez, P. and Gallego-Ledesma, L. and Carranza-García, M. and Luna-Romera, J.},
  title     = {Evaluation of the Transformer Architecture for Univariate Time Series Forecasting},
  journal   = {Springer},
  pages     = {106--115},
  year      = {2021},
  doi       = {10.1007/978-3-030-85713-4-11}
}

@article{Li2024,
  author    = {Li, D.},
  title     = {TST-Refrac: A Novel Transformer Variant for Prediction of Production of Re-Fractured Oil Wells},
  journal   = {Journal of Physics: Conference Series},
  volume    = {2901},
  number    = {1},
  pages     = {012030},
  year      = {2024},
  doi       = {10.1088/1742-6596/2901/1/012030}
}

@article{Tao2023,
  author    = {Tao, G.},
  title     = {Time Series Forecasting Based on the Wavelet Transform},
  journal   = {SPIE},
  year      = {2023},
  doi       = {10.1117/12.2684645}
}

@article{Tian2022,
  author    = {Tian, Z. and Ma, Z. and Wen, Q. and Wang, X. and Sun, L. and Jin, R.},
  title     = {FEDformer: Frequency Enhanced Decomposed Transformer for Long-Term Series Forecasting},
  journal   = {arXiv preprint},
  year      = {2022},
  archivePrefix = {arXiv},
  eprint    = {2201.12740}
}

@article{Wu2021,
  author    = {Wu, H. and Xu, J. and Wang, J. and Long, M.},
  title     = {Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},
  journal   = {arXiv preprint},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2106.13008}
}

@article{Xu2021,
  author    = {Xu, J. and Hu, S. and Yu, J. and Liu, X. and Meng, H.},
  title     = {Mixed Precision Quantization of Transformer Language Models for Speech Recognition},
  journal   = {Proceedings of ICASSP},
  year      = {2021},
  doi       = {10.1109/icassp39728.2021.9414076}
}

@article{Yang2021,
  author    = {Yang, H. and Yin, H. and Shen, M. and Molchanov, P. and Li, H. and Kautz, J.},
  title     = {NVIT: Vision Transformer Compression and Parameter Redistribution},
  journal   = {arXiv preprint},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2110.04869}
}

@article{Yildiz2022,
  author    = {Yıldız, A. and Koç, E. and Koç, A.},
  title     = {Multivariate Time Series Imputation with Transformers},
  journal   = {IEEE Signal Processing Letters},
  volume    = {29},
  pages     = {2517-2521},
  year      = {2022},
  doi       = {10.1109/lsp.2022.3224880}
}
@article{Zhou2021,
  author    = {Zhou, H. et al.},
  title     = {Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2021},
  doi       = {10.1609/aaai.v35i12.17325}
}
@article{Liu2023,
  author    = {Liu, S.},
  title     = {LLM-FP4: 4-Bit Floating-Point Quantized Transformers},
  journal   = {EMNLP},
  year      = {2023},
  doi       = {10.18653/v1/2023.emnlp-main.39}
}
@article{Zhu2023,
  author    = {Zhu, K. et al.},
  title     = {Quantized Feature Distillation for Network Quantization},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2023},
  doi       = {10.1609/aaai.v37i9.26354}
}
@article{Liu2021,
  author    = {Liu, M.},
  title     = {Gated Transformer Networks for Multivariate Time Series Classification},
  journal   = {arXiv preprint},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2103.14438},
  url       = {https://doi.org/10.48550/arxiv.2103.14438}
}
@misc{dau2019,
      title={The UCR Time Series Archive}, 
      author={Hoang Anh Dau and Anthony Bagnall and Kaveh Kamgar and Chin-Chia Michael Yeh and Yan Zhu and Shaghayegh Gharghabi and Chotirat Ann Ratanamahatana and Eamonn Keogh},
      year={2019},
      eprint={1810.07758},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1810.07758}, 
}

@inproceedings{Gao2014,
  title = {PLAID: a public dataset of high-resolution electrical appliance measurements for load identification research: demo abstract},
  author = {Gao, Jingkun and Giri, Suman and Kara, Emre Can and Berges, Mario},
  booktitle = {Proceedings of the 1st ACM Conference on Embedded Systems for Energy-Efficient Buildings},
  pages = {198--199},
  year = {2014},
  organization = {ACM}
}
@misc{kim2023stackoptimizationtransformerinference,
      title={Full Stack Optimization of Transformer Inference: a Survey}, 
      author={Sehoon Kim and Coleman Hooper and Thanakul Wattanawong and Minwoo Kang and Ruohan Yan and Hasan Genc and Grace Dinh and Qijing Huang and Kurt Keutzer and Michael W. Mahoney and Yakun Sophia Shao and Amir Gholami},
      year={2023},
      eprint={2302.14017},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.14017}, 
}

@article{dice2021optimizing,
  title={Optimizing inference performance of transformers on CPUs},
  author={Dice, Dave and Kogan, Alex},
  journal={arXiv preprint arXiv:2102.06621},
  year={2021}
}
@article{chitty2023survey,
  title={A survey of techniques for optimizing transformer inference},
  author={Chitty-Venkata, Krishna Teja and Mittal, Sparsh and Emani, Murali and Vishwanath, Venkatram and Somani, Arun K},
  journal={Journal of Systems Architecture},
  pages={102990},
  year={2023},
  publisher={Elsevier}
}

@article{zhang2023niot,
  title={NIOT: A Novel Inference Optimization of Transformers on Modern CPUs},
  author={Zhang, Zining and Chen, Yao and He, Bingsheng and Zhang, Zhenjie},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={6},
  pages={1982--1995},
  year={2023},
  publisher={IEEE}
}

@inproceedings{abdulsalam2015using,
  title={Using the Greenup, Powerup, and Speedup metrics to evaluate software energy efficiency},
  author={Abdulsalam, Sarah and Zong, Ziliang and Gu, Qijun and Qiu, Meikang},
  booktitle={2015 Sixth International Green and Sustainable Computing Conference (IGSC)},
  pages={1--8},
  year={2015},
  organization={IEEE}
}
@article{10.37798/2023724508,
  author = {Fuhrmann, F. and Maly, A. and Blass, M. and Waikat, J. and Belavić, F. and Graf, F.},
  title = {3d acoustic heat-maps for transformer monitoring applications},
  journal = {Journal of Energy - Energija},
  year = {2024},
  volume = {72},
  issue = {4},
  pages = {15-18},
  doi = {10.37798/2023724508}
}
@article{10.21203/rs.3.rs-4118482/v1,
  author = {Tripathi, S. M. and Upadhyay, H. and Soni, J.},
  title = {A quantum lstm based approach to cyber threat detection in virtual environment},
  year = {2024},
  doi = {10.21203/rs.3.rs-4118482/v1}
}
@misc{zeng2022transformerseffectivetimeseries,
      title={Are Transformers Effective for Time Series Forecasting?}, 
      author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},
      year={2022},
      eprint={2205.13504},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2205.13504}, 
}
@article{Kaur2024LeveragingAI,
title={Leveraging Artificial Intelligence for Enhanced Sustainable Energy Management},
author={Swapandeep Kaur and Raman Kumar and Kanwardeep Singh and Yinglai Huang},
journal={Journal of Sustainability for Energy},
year={2024},
page={1-20},
doi={https://doi.org/10.56578/jse030101}
}
@article{10.1016/j.ijinfomgt.2020.102282,
  author = {Nguyen, H. D. and Tran, K. P. and Thomassey, S. and Hamad, M. M.},
  title = {Forecasting and anomaly detection approaches using lstm and lstm autoencoder techniques with the applications in supply chain management},
  journal = {International Journal of Information Management},
  year = {2021},
  volume = {57},
  pages = {102282},
  doi = {10.1016/j.ijinfomgt.2020.102282}
}
@article{10.18653/v1/p19-1355,
  author = {Strubell, E. and Ganesh, A. and McCallum, A.},
  title = {Energy and policy considerations for deep learning in nlp},
  journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year = {2019},
  doi = {10.18653/v1/p19-1355}
}

@article{khaniki2024vision,
  title={Vision transformer with feature calibration and selective cross-attention for brain tumor classification},
  author={Khaniki, Mohammad Ali Labbaf and Mirzaeibonehkhater, Marzieh and Manthouri, Mohammad and Hasani, Elham},
  journal={Iran Journal of Computer Science},
  pages={1--13},
  year={2024},
  publisher={Springer}
}
@inproceedings{cordonnier2021differentiable,
  title={Differentiable patch selection for image recognition},
  author={Cordonnier, Jean-Baptiste and Mahendran, Aravindh and Dosovitskiy, Alexey and Weissenborn, Dirk and Uszkoreit, Jakob and Unterthiner, Thomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2351--2360},
  year={2021}
}
@misc{zeraatkar2025visirvisiontransformersingle,
      title={ViSIR: Vision Transformer Single Image Reconstruction Method for Earth System Models}, 
      author={Ehsan Zeraatkar and Salah Faroughi and Jelena Tešić},
      year={2025},
      eprint={2502.06741},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.06741}, 
}

@misc{vaziri2024optimalinferentialcontrolconvolutional,
      title={Optimal Inferential Control of Convolutional Neural Networks}, 
      author={Ali Vaziri and Huazhen Fang},
      year={2024},
      eprint={2410.09663},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2410.09663}, 
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}
@article{10.1007/978-3-030-01237-3_12,
  author = {Zhang, T. and Ye, S. and Zhang, K. and Tang, J. and Wen, W. and Fardad, M. and Wang, Y.},
  title = {A systematic dnn weight pruning framework using alternating direction method of multipliers},
  journal = {Lecture Notes in Computer Science},
  year = {2018},
  pages = {191-207},
  doi = {10.1007/978-3-030-01237-3_12}
}
@article{10.3390/electronics9071059,
  author = {Guo, W. and Yantır, H. E. and Fouda, M. E. and Eltawil, A. M. and Saláma, K.},
  title = {Towards efficient neuromorphic hardware: unsupervised adaptive neuron pruning},
  journal = {Electronics},
  year = {2020},
  volume = {9},
  issue = {7},
  pages = {1059},
  doi = {10.3390/electronics9071059}
}