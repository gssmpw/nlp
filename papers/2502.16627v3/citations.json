[
  {
    "index": 0,
    "papers": [
      {
        "key": "Kaur2024LeveragingAI",
        "author": "Swapandeep Kaur and Raman Kumar and Kanwardeep Singh and Yinglai Huang",
        "title": "Leveraging Artificial Intelligence for Enhanced Sustainable Energy Management"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "Wen2022",
        "author": "Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang",
        "title": "Transformers in Time Series: A Survey"
      },
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Zhou2021",
        "author": "Zhou, H. et al.",
        "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Tian2022",
        "author": "Tian, Z. and Ma, Z. and Wen, Q. and Wang, X. and Sun, L. and Jin, R.",
        "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-Term Series Forecasting"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Zhou2021",
        "author": "Zhou, H. et al.",
        "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Tao2023",
        "author": "Tao, G.",
        "title": "Time Series Forecasting Based on the Wavelet Transform"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "Li2024",
        "author": "Li, D.",
        "title": "TST-Refrac: A Novel Transformer Variant for Prediction of Production of Re-Fractured Oil Wells"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "LaraBenitez2021",
        "author": "Lara-Ben\u00edtez, P. and Gallego-Ledesma, L. and Carranza-Garc\u00eda, M. and Luna-Romera, J.",
        "title": "Evaluation of the Transformer Architecture for Univariate Time Series Forecasting"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Kim2024",
        "author": "Kim, D.",
        "title": "Pruning-Guided Feature Distillation for an Efficient Transformer-Based Pose Estimation Model"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Liu2023",
        "author": "Liu, S.",
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Cheong2019",
        "author": "Cheong, Robin",
        "title": "Transformers . zip: Compressing Transformers with Pruning and Quantization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "Zhu2023",
        "author": "Zhu, K. et al.",
        "title": "Quantized Feature Distillation for Network Quantization"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Xu2021",
        "author": "Xu, J. and Hu, S. and Yu, J. and Liu, X. and Meng, H.",
        "title": "Mixed Precision Quantization of Transformer Language Models for Speech Recognition"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "Liu2023",
        "author": "Liu, S.",
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "Liu2021",
        "author": "Liu, M.",
        "title": "Gated Transformer Networks for Multivariate Time Series Classification"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "Gorbett2023",
        "author": "Gorbett, M. and Shirazi, H. and Ray, I.",
        "title": "Sparse Binary Transformers for Multivariate Time Series Modeling"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "Wu2021",
        "author": "Wu, H. and Xu, J. and Wang, J. and Long, M.",
        "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "kim2023stackoptimizationtransformerinference",
        "author": "Sehoon Kim and Coleman Hooper and Thanakul Wattanawong and Minwoo Kang and Ruohan Yan and Hasan Genc and Grace Dinh and Qijing Huang and Kurt Keutzer and Michael W. Mahoney and Yakun Sophia Shao and Amir Gholami",
        "title": "Full Stack Optimization of Transformer Inference: a Survey"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "chitty2023survey",
        "author": "Chitty-Venkata, Krishna Teja and Mittal, Sparsh and Emani, Murali and Vishwanath, Venkatram and Somani, Arun K",
        "title": "A survey of techniques for optimizing transformer inference"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "vaziri2024optimalinferentialcontrolconvolutional",
        "author": "Ali Vaziri and Huazhen Fang",
        "title": "Optimal Inferential Control of Convolutional Neural Networks"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "devlin2019bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhang2023niot",
        "author": "Zhang, Zining and Chen, Yao and He, Bingsheng and Zhang, Zhenjie",
        "title": "NIOT: A Novel Inference Optimization of Transformers on Modern CPUs"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "10.1016/j.ijinfomgt.2020.102282",
        "author": "Nguyen, H. D. and Tran, K. P. and Thomassey, S. and Hamad, M. M.",
        "title": "Forecasting and anomaly detection approaches using lstm and lstm autoencoder techniques with the applications in supply chain management"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "Lannelongue2021",
        "author": "Lannelongue, Lo\u00efc and Grealey, Jason and Inouye, Michael",
        "title": "Green Algorithms: Quantifying the Carbon Footprint of Computation"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "Bannour2021",
        "author": "Bannour, Nesrine and Ghannay, Sahar and N\u00e9v\u00e9ol, Aur\u00e9lie and Ligozat, Anne-Laure",
        "title": "Evaluating the Carbon Footprint of NLP Methods: A Survey and Analysis of Existing Tools"
      },
      {
        "key": "dice2021optimizing",
        "author": "Dice, Dave and Kogan, Alex",
        "title": "Optimizing inference performance of transformers on CPUs"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "abdulsalam2015using",
        "author": "Abdulsalam, Sarah and Zong, Ziliang and Gu, Qijun and Qiu, Meikang",
        "title": "Using the Greenup, Powerup, and Speedup metrics to evaluate software energy efficiency"
      }
    ]
  }
]