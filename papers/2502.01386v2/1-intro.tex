\section{Introduction}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.47\textwidth]{figs/intro-topic-rag-attack.pdf}
  \caption{An concise overview of topic-oriented adversarial opinion manipulation attacks on RAG systems.}
\end{figure}

Retrieval-Augmented Generation (RAG) systems, built on large language models (LLMs), have advanced significantly and been widely applied in tasks like question answering and content generation \cite{gao2023retrieval,zhao2024retrieval}. These systems integrate retrieval mechanisms with generative models, accessing diverse sources, e.g., Wikipedia, Reddit, and news articles, to deliver updated knowledge. While research has focused on the development of RAG frameworks \cite{asai2023self,shi2025know,xu2024search}, the majority of studies emphasize performance and generalization. Consequently, security aspects have received comparatively little attention.


A defining feature of RAG systems is the vast scale of their referenced corpora or knowledge bases. However, these documents might originate from sources beyond service providers' control and may resist complete content purification. This creates new security risks for RAG-supported LLMs: adversaries can inject meticulously crafted malicious content \cite{carlini2024poisoning} into retrieval collections, ensuring its prioritized retrieval to influence LLM outputs. The inherent vulnerabilities of RAG models, particularly their persistent impact on public opinion and information dissemination \cite{chen2025flipedragblackboxopinionmanipulation}, have gradually drawn significant attention from the AI safety research community \cite{zhong2023poisoning,xue2024badrag,zou2024poisonedrag}. Investigating these vulnerabilities is critical for advancing RAG security and ensuring system robustness and credibility.


Early adversarial attacks on RAG systems primarily focused on jailbreak techniques and query-specific perturbations \cite{chen2025flipedragblackboxopinionmanipulation,zhong2023poisoning}. These methods tamper with knowledge bases to manipulate retrieval rankings, enabling LLMs to provide answers defined by attackers for specific queries. Recent research, such as PoisonedRAG \cite{zou2024poisonedrag}, illustrates how poisoned texts can lead to the generation of misinformation, revealing critical vulnerabilities in applications like cybersecurity and healthcare. However, these attacks often lack practicality as they concentrate on isolated factual queries. Such queries can often be countered through mitigation strategies such as reranking \cite{de2024rag}, which tend to overlook thematic or topic-level manipulation. Additionally, existing studies largely overlook controversial topics, where limited user understanding may increase susceptibility to opinion manipulation.


This paper addresses this critical gap by concentrating on the nuanced manipulation of opinions through adversarial attacks targeted at topics within RAG models, which presents a novel and urgent challenge given the increasing complexity and reliance on these systems. 

In line with settings from previous data poisoning findings\cite{zou2024poisonedrag,carlini2024poisoning}, we consider a scenario where the attacker could inject a few carefully crafted poisoned texts into the knowledge base. For example, if the knowledge base includes texts sourced from Wikipedia, the adversary might inject poisoned content by maliciously editing Wikipedia entries.
Besides, we focus on more practical and challenging black-box scenario, where no model information is disclosed, except that the attackers can query the target RAG and obtain responses, which contains the corresponding candidate referential documents \cite{chen2025flipedragblackboxopinionmanipulation}. 

In this paper, we propose the Topic-FlipRAG, which is a two-stage multi-granularity attack method designed to manipulating the stance polarity of RAGs with camouflaged modification of target documents. Specifically, in the first stage, we perform a stealthy adversarial modifications on target documents. This is achieved by incorporating the extensive internal general semantic knowledge in LLMs with its analysis and reasoning ability.
In the second stage, inspired by adversarial semantic collision \cite{song2020adversarial}, we utilize gradients from an open-sourced neural ranking model (NRM) to generate topic-specific adversarial triggers.


Our experiments show Topic-FlipRAG achieves 0.5 average stance variation (ASV) across four domains, significantly outperforming other baselines. It demonstrates such adversarial attacks can effectively manipulate RAG models to produce outputs aligned with specific topical stances, thus influencing how information is presented and perceived. Based on that, we also conduct the user experiments, which reveal a significant impact on usersâ€™ polarities toward controversial topics, with polarity shifts exceeding 16\% after interacting with poisoned RAG systems. Futhermore, we explore several potential mitigation strategy, including perplexity-based detection, random masking, and reranking. Our results show these mitigations are inadequate against adversarial opinion manipulation attack, especially for Topic-FlipRAG, underscoring the necessity for novel mitigation strategies.

Our major contributions are as follows:

(1) We explore a novel and practical security scenario, \textit{Topic-oriented RAG Opinion Manipulation}, which presents a broader and deeper threat to real-world users by enabling opinion manipulation across multiple topic-related queries.

(2) We propose Topic-FlipRAG, a knowledge-guided and multi-granularity black-box attack method tailored for topic-level opinion manipulation in RAG systems.

(3) Through extensive experiments, we demonstrate that Topic-FlipRAG achieves significantly better attack performance across all metrics compared to baseline methods. Furthermore, we evaluate its practical impact through user experiments, highlighting its potential to manipulate user opinions.

(4) We analyze the efficacy of several defense mechanisms and demonstrate their insufficiency in countering Topic-FlipRAG. This underscores the urgent need for robust and adaptive defense strategies against such attacks.

