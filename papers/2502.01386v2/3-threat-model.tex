\section{Threat Model}
\label{sec:threat_model}
Given a set of topic-queries $\mathbf{Q} = \{q_1, q_2, \ldots, q_{|\mathbf{Q}|}\}$, a RAG corpus $D$, and a desired target polarity $S_t$ (e.g., \textit{Pro} or \textit{Con}), The adversary's objective is to steer the RAG system's overall stance across the entire set of queries toward stance $S_t$. Let $\text{doc}_{\text{tar}}$ be the target document the attacker aims to promote. The black-box retrieval model $\text{RM}$ assigns a relevance score $R(q, d)$ to each document $d \in D$ and returns the top-$k$ documents as $\text{RM}_k(q)=\{d_1, d_2, \ldots, d_k\}$. The LLM then generates an answer $(q, \text{RM}_k(q))$, and the extracted opinion from this response is $S_o = \text{S}(\text{LLM}(q, RM_k(q)))$.


\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/Overview_RAG_Topic_Attack.pdf}
  \caption{An overview of our proposed Topic-Orientated Adversarial Opinion Manipulation Attack method for RAG systems.}
  \label{overview}
\end{figure*}


\subsection{\textbf{Objective of the Adversary}}
The adversary seeks to modify the target document $doc_{tar}$ through a series of subtle perturbations $p_{adv}$, while preserving its original semantics and fluency, thereby transforming the corpus $D$ into $D' = D(\text{doc}; \text{doc}_{\text{tar}} \oplus p_{\text{adv}})$. By increasing the retrieval model’s relevance score $R(q, \text{doc}_{\text{tar}} \oplus p_{\text{adv}})$ for each query $q \in \mathbf{Q}$, the modified document becomes more likely to appear among the top-$k$ results $\text{RM}_k(q)$.

As a result, when the LLM generates answers conditioned on $(q, \text{RM}_k(q))$, the adversarially augmented document will influence the opinion $S(\text{LLM}(q, \text{RM}_k(q)))$ towards the target polarity $S_t$. Crucially, the objective extends beyond a single query: the adversary aims to ensure that, on average, the opinions produced by the LLM across the entire query set $\mathbf{Q}$ align with $S_t$. In other words, the attacker seeks:
\begin{align*}
    \max_{p_{\text{adv}}} \frac{1}{|\mathbf{Q}|}\sum_{q \in \mathbf{Q}} I\bigl(S(\text{LLM}(q, \text{RM}_k(q))) = S_t\bigr) \hspace{5mm}
\end{align*}


where $I(\cdot)$ is an indicator function that returns $1$ if the generated stance matches $S_t$ and $0$ otherwise. By optimizing for this criterion, the adversary ensures that the aggregated stance across all topic queries shifts significantly toward the desired polarity. 




\subsection{\textbf{Capabilities of the Adversary}}
In the black-box setting, the adversary is restricted to modifying a limited number of documents within the corpus $D$. Obtaining detailed information about the specific retrieval model employed by the target RAG system is typically impractical in real-world scenarios. The adversary has no access to the internal parameters or architecture of either the retrieval model or the large language model (LLM), and cannot alter the LLM’s prompt templates.