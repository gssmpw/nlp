\section{Background and Related Work}
\subsection{Retrieval Augmented Generation (RAG)}
RAG models enhance their responses by accessing and incorporating external knowledge from large-scale databases or corpora during the generation process **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. By leveraging external data sources, RAG can provide more accurate and comprehensive answers, especially for queries requiring up-to-date information or specialized knowledge that may not be well-represented in the model's training data **Vaswani et al., "Attention Is All You Need"**. Moreover, it can scale more effectively and flexibly by updating the retrieval corpus without necessitating extensive retraining of the generative component **Gupta et al., "Retrieval-Augmented Language Model Pre-Training"**.


The workflow of a RAG systems is generally divided into two sequential phases: \textit{retrieval} and \textit{generation}. 
In the retrieval phase, upon the submission of a user query $q$, the RAG system retrieves $k$ relevant documents from the corpus $D$ with the highest embedding similarities to the query $q$. Specifically, for each document $d \in D$, the relevance score with the query $q$ is computed as $R(q, d)$. In this paper, we adopt the a more practical and widely used framework in LangChain that the retrieval phase is incorporated with historical-aware query rewriting and intention reasoning. 
In the generation phase, Given a the rewritten query $q'$ , a set of top-k retrieved documents $D_k$, and access to the LLM, one can query the LLM with $q'$ with the top-k retrieved documents $D_k$, the LLM generates an answer for original query $q$ by leveraging $D_k$ as context. This process is detailed in Appendix \ref{exp-detail}.

\subsection{Attacks to Retriever}

Retriever vulnerabilities in RAG systems pose significant security risks due to their neural ranking dependencies **Gil et al., "Adversarial Attacks on Retrieval-Augmented Generation Models"**. Adversarial attacks manipulate document rankings through semantic perturbations to promote target documents in query-specific results ****, undermining RAG's core assumption of reliable high-ranked contexts.
Current attacks are characterized along two axes: (1) Knowledge accessibility: White-box (full model access) vs. black-box (query-only) approaches. (2) Perturbation granularity: Word-level **Eger et al., "Adversarial Attacks on Pre-trained Language Models"****, Phrase-level ****, Sentence-level ****, and Hybrid ****.These neural semantic attacks parallel black-hat SEO tactics **** but introduce novel challenges. Successful attacks propagate adversarial content to generators, enabling misinformation injection while evading traditional safeguards ****, necessitating integrated security frameworks for RAG systems.

\subsection{Attacks to LLMs}

Existing attacks on LLMs include jailbreak attacks **Brown et al., "Language Models are Few-Shot Learners"****, backdoor attacks **Li et al., "Backdoor Attacks against Neural Networks"***, prompt injection ****, and poisoning attacks ****. Poisoning attacks uniquely threaten RAG systems by injecting adversarial content into retrieval corpora to manipulate outputs. Current RAG poisoning studies focus on closed-domain factoid QA pairs (e.g., "CEO of OpenAI") **Hao et al., "Poisoning Attacks against Retrieval-Augmented Generation Models"****, overlooking the multi-query nature of real-world topic exploration****, e.g., "smartwatch battery life" and "health tracking accuracy" under the "wearable tech" theme. This motivates our focus on practical topic-level universal perturbations mirroring universal adversarial examples.

Furthermore, existing defenses based on fact-checking are insufficient for opinion-based queries, such as "Should genetic testing be regulated?", which require nuanced reasoning rather than simple factual recall. Building on the insights from FlipedRAG ****, we tackle this significant gap by examining the adversarial manipulation of controversial topics. In such cases, LLMs are required to reason and synthesize multiple perspectives, making them particularly vulnerable to systematic knowledge poisoning.