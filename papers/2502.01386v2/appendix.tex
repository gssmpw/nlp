\appendix

\clearpage
\section{Appendix}






\subsection{Full experiment details }\label{exp-detail}
(1)Black-box RAG.  We represent the black-box RAG process as \( \text{RAG}_{\text{black}} \). The RAG framework is Conversational RAG from LangChain. Conversational RAG adds logic for incorporating historical messages and intention reasoning, thus supporting back-and-forth conversation with users and serving as a widely used RAG framework. Based on a retriever and a large language model (LLM), Conversational RAG mainly consists of a history-awaring retriever to identify users' intention and a question-answering chain to produce output, as shown in Figure\ref{conversation rag}. The LLMs adopted are the open-source models Meta-Llama-3.1-8B-Instruct (Llama3.1), Qwen-2.5-7B-Instruct(Qwen2.5). They perform well across various tasks among all open-source LLMs. The prompt connecting the retriever and the LLM in \( \text{RAG}_{\text{black}} \) adopts the basic RAG prompt from LangChain:
\begin{tcolorbox}
Use the following pieces of retrieved context to answer the question. Keep the answer concise.\\
Context: \{context\}.\\
Question: \{question\}.
\end{tcolorbox}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.47\textwidth]{figs/Conversational_RAG.png}
  \caption{The framework of Conversational RAG.}
  \label{conversation rag}
\end{figure}

(2) Target retriever model in RAG-system. People usually use dense retrieval models as the retriever in RAG. Therefore, we select three representative dense retrieval models, Contriever, DPR and ANCE, as the target retrieval model \cite{gao2021unsupervised, karpukhin-etal-2020-dense, xiong2020approximate}. By convention, we use dot product between the embedding vectors of questions(queries) and candidate documents as their similarity score \(R\) in the ranking. 

(3) Manipulation target. For a controversial topic-queries-set \( Q \), documents \( doc_{tar} \) with the target opinion \( S_t \) are manipulated by modifing  $doc_{tar}$ through a series of subtle alterations $p_{adv}$, . This manipulation aims to place these perturbed documents as prominently as possible in the top \( k \) rankings of the RAG retriever \( \mathrm{RM}_k(q) \), where \( k \) denotes the number of documents obtained from the retrieval results by the LLM in \( \text{RAG}_{\text{black}} \) . In this paper, \( k \) is set to 3.
\label{opinion-classfication}

(4) Opinion classification. We use Qwen2.5-Instruct-72B as the opinion classifier. Qwen2.5-Instruct-72B, due to its large parameter size, is capable of accurately identifying and classifying opinions within text.

(5) Experimental parameters. In the knowledge-guided attack process, we set the maximum editing distance parameter $\epsilon$ to 0.2, the semantic similarity threshold $\lambda$ to 0.85, and the number of iterations $N$  to 5.For adversarial trigger generation, we configured the experimental parameters as follows: the number of beams was set to 3, the top-$k$ value to 10, the batch size to 32, the temperature value to 1.0, the learning rate to 0.005, and the sequence length to 10.

(6) Poisoned Target.For the PROCON dataset, to investigate the manipulation performance under more challenging conditions, we performed relevance ranking for the documents with respect to each topic-query set $Q$ and the target stance $S_t$ . From the ranked list, we selected the last five documents (i.e., those with the lowest relevance) as the target poisoned documents.
For the MS MARCO dataset, we utilized the top-1000 relevance-ranked passage list for each topic-query set. From this list, we selected the passage with the lowest average rank as the target passage. This approach ensures that the evaluation focuses on passages that are least relevant to the target queries, thus providing a more rigorous benchmark for the proposed method.

(7) Experimental environment. All our experiments are conducted in Python 3.8 environment and run on a NVIDIA DGX H100 GPU. 



\subsection{Details of baselines}\label{baselines-details}
\textbf{PoisonedRAG}:  
Zou et al. \cite{zou2024poisonedrag} propose PoisonedRAG which injects a few poisoned texts into the RAG database such that the LLM generates manipulated answers. They consider both black-box and white-box settings. In the black-box setting, they attempt to include the question itself as part of the poisoned text to make it semantically similar to the target question.  For our task, we adapted this strategy to the topic-queries setting by randomly selecting one query from the topic-queries set \( Q \) as the trigger and inserting it at the start of the document. The manipulated documents were then evaluated for their impact on retrieval rankings and subsequent comparisons.

\textbf{PAT}:  
As a representative adversarial retrieval attack strategy, PAT employs a pairwise generation paradigm. Given a target query, the target candidate item, and the top-ranked candidate item (referred to as the anchor, which guides adversarial text generation), the method utilizes gradient-based optimization of a pairwise loss function calculated between the target and the anchor. The strategy incorporates additional constraints for fluency and next-sentence prediction to ensure coherence. Using beam search, the final adversarial trigger, denoted as \( T_{\text{pat}} \), is generated iteratively in an auto-regressive manner. In our task, we adapted PAT for topic-queries by generating \( T_{\text{pat}} \) for the target document considering the whole topic-queries set and evaluating its effectiveness under black-box constraints.

\textbf{Collision}:  
It employs a gradient-based optimization approach to generate adversarial paragraphs, referred to as collisions, which are semantically similar to the target query. Using gradient descent, continuous representations of collisions are optimized and subsequently discretized into tokens via beam search. ASC provides three variants: aggressive, aggressive-regularized, and natural. For evaluation, we focused on the most and least aggressive variants to examine their effectiveness in generating adversarial texts. Using parameters and settings derived from the original Birch model implementation, collisions were generated with a fixed length of 10 tokens. In the context of topic-queries, ASC was applied to create collisions for the entire topic-queries set, and its transfer attack performance was evaluated on the black-box RAG retriever.



\subsection{Ablation Study Supplement}
Due to space limitations, only a subset of ablation study results is presented in the main text. Figure~\ref{sentiment-know-1} illustrates the impact of the Polarity Control module in Topic-FlipRAG on the sentiment of the documents when the target polarity is set to \texttt{PRO}. Figures~\ref{baseline-sentiment-0} and \ref{baseline-sentiment-1} depict the influence of various baseline methods on document sentiment.

\begin{figure}[]
  \centering
  \includegraphics[width=0.45\textwidth]{figs/1-ablation-sentiment.pdf}
  \caption{Impact of Polarity Control on documents sentiment (target polarity:PRO) . }
  \label{sentiment-know-1}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=0.45\textwidth]{figs/ablation-stance.pdf}
  \caption{Impact of Polarity Control on document stance.}
  \label{ablation-stance}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=0.45\textwidth]{figs/baseline-sentiment-0.pdf}
  \caption{Impact of Polarity Control on document stance(target polarity:CON).}
  \label{baseline-sentiment-0}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=0.45\textwidth]{figs/baseline-sentiment-1.pdf}
  \caption{Impact of Polarity Control on document stance(target polarity:PRO).}
  \label{baseline-sentiment-1}
\end{figure}


Additionally, Figure~\ref{ablation-stance} demonstrates the effect of the Polarity Control module on the actual stance polarity of the documents.The distinction between stance polarity and sentiment is critical. Stance polarity is determined by utilizing the opinion classification approach described in Section~\ref{opinion-classfication}, which evaluates whether the document supports or opposes the corresponding topic. In contrast, sentiment is directly inferred using a sentiment classification model, specifically \textit{cardiffnlp/twitter-roberta-base-sentiment-latest}.



\footnote{\url{https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest}}




\begin{table*}[htbp]
  \caption{Topic-Oriented RAG attack results on RPOCON dataset based on ANCE.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccccccc}
    \toprule
    \multirow{2}[4]{*}{\textbf{Method}} & \multirow{2}[4]{*}{\textbf{Target Stance}} & \multicolumn{5}{c}{\textbf{ANCE+qwen-2.5-8B-instruct}} &       & \multicolumn{5}{c}{\textbf{ANCE+llama3.1-8B-instruct}} &  \\
\cmidrule(r){3-7}\cmidrule(r){9-14}          &       & \textbf{H\&E} & \textbf{G\&P} & \textbf{EDU} & \textbf{S\&C} & \textbf{avg. ASV} & \textbf{$\Delta$ ASV} & \textbf{H\&E} & \textbf{G\&P} & \textbf{EDU} & \textbf{S\&C} & \textbf{avg. ASV} & \textbf{$\Delta$ ASV} \\
    \midrule
    \multirow{2}[2]{*}{Clean} & CON   & 0.21  & 0.24  & 0.18  & 0.18  & 0.21  & --    & 0.24  & 0.24  & 0.24  & 0.25  & 0.24  & -- \\
          & PRO   & 0.19  & 0.24  & 0.17  & 0.20  & 0.21  & --    & 0.21  & 0.27  & 0.23  & 0.24  & 0.24  & -- \\
    \midrule
    \multirow{2}[2]{*}{ASC} & CON   & 0.30  & 0.30  & 0.25  & 0.23  & 0.28  & 0.06  & 0.35  & 0.35  & 0.26  & 0.28  & 0.32  & 0.08  \\
          & PRO   & 0.24  & 0.35  & 0.25  & 0.27  & 0.29  & 0.06  & 0.27  & 0.40  & 0.27  & 0.29  & 0.31  & 0.07  \\
    \midrule
    \multirow{2}[2]{*}{PAT} & CON   & 0.23  & 0.27  & 0.24  & 0.25  & 0.25  & 0.04  & 0.32  & 0.28  & 0.27  & 0.27  & 0.29  & 0.05  \\
          & PRO   & 0.25  & 0.23  & 0.27  & 0.29  & 0.25  & 0.05  & 0.29  & 0.29  & 0.30  & 0.29  & 0.30  & 0.05  \\
    \midrule
    \multirow{2}[2]{*}{PoisonedRAG} & CON   & 0.26  & 0.34  & 0.36  & 0.44  & 0.35  & 0.13  & 0.32  & 0.43  & 0.36  & 0.40  & 0.36  & 0.09  \\
          & PRO   & 0.31  & 0.37  & 0.30  & 0.28  & 0.33  & 0.12  & 0.34  & 0.31  & 0.30  & 0.31  & 0.33  & 0.09  \\
    \midrule
    \multirow{2}[2]{*}{Topic-FlipRAG} & CON   & \textbf{0.39}  & \textbf{0.44}  & \textbf{0.40}  & \textbf{0.42}  & \textbf{0.42}  & \textbf{0.21}  & \textbf{0.40}  & \textbf{0.50}  & \textbf{0.41}  & \textbf{0.50}  & \textbf{0.46}  & \textbf{0.23}  \\
          & PRO   & \textbf{0.43}  & \textbf{0.41}  & \textbf{0.31}  & \textbf{0.39}  & \textbf{0.39}  & \textbf{0.18}  & \textbf{0.40}  & \textbf{0.52}  & \textbf{0.37}  & \textbf{0.56}  & \textbf{0.47}  & \textbf{0.23}  \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:rag-ance}%
\end{table*}%

\vspace*{2\baselineskip}

\begin{table*}[htbp]
\caption{Topic-Oriented RAG attack results on RPOCON dataset based on DPR.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccccccc}
    \toprule
    \multirow{2}[4]{*}{\textbf{Method}} & \multirow{2}[4]{*}{\textbf{Target Stance}} & \multicolumn{5}{c}{\textbf{DPR+qwen-2.5-8B-instruct}} &       & \multicolumn{5}{c}{\textbf{DPR+llama3.1-8B-instruct}} &  \\
\cmidrule(r){3-7}\cmidrule(r){9-14}          &       & \textbf{H\&E} & \textbf{G\&P} & \textbf{EDU} & \textbf{S\&C} & \textbf{avg. ASV} & \textbf{$\Delta$ ASV} & \textbf{H\&E} & \textbf{G\&P} & \textbf{EDU} & \textbf{S\&C} & \textbf{avg. ASV} & \textbf{$\Delta$ ASV} \\
    \midrule
    \multirow{2}[2]{*}{Clean} & CON   & 0.22  & 0.26  & 0.27  & 0.25  & 0.25  & --    & 0.25  & 0.24  & 0.21  & 0.23  & 0.23  & -- \\
          & PRO   & 0.23  & 0.26  & 0.24  & 0.25  & 0.24  & --    & 0.24  & 0.27  & 0.22  & 0.22  & 0.24  & -- \\
    \midrule
    \multirow{2}[2]{*}{Collision} & CON   & 0.25  & 0.30  & 0.28  & 0.21  & 0.27  & 0.02  & 0.33  & 0.34  & 0.33  & 0.31  & 0.33  & 0.10  \\
          & PRO   & 0.27  & 0.34  & 0.31  & 0.29  & 0.31  & 0.07  & 0.28  & 0.39  & 0.39  & 0.39  & 0.35  & 0.12  \\
    \midrule
    \multirow{2}[2]{*}{PAT} & CON   & 0.22  & 0.23  & 0.34  & 0.26  & 0.26  & 0.01  & 0.32  & 0.26  & 0.35  & 0.19  & 0.28  & 0.05  \\
          & PRO   & 0.23  & 0.23  & 0.23  & 0.23  & 0.25  & 0.01  & 0.21  & 0.35  & 0.35  & 0.27  & 0.29  & 0.04  \\
    \midrule
    \multirow{2}[2]{*}{PoisonedRAG} & CON   & 0.32  & 0.40  & 0.28  & 0.48  & 0.37  & 0.12  & 0.39  & 0.39  & 0.36  & 0.58  & 0.43  & 0.20  \\
          & PRO   & 0.29  & 0.36  & 0.31  & 0.31  & 0.32  & 0.08  & 0.30  & 0.42  & 0.36  & 0.42  & 0.37  & 0.14  \\
    \midrule
    \multirow{2}[2]{*}{Topic-FlipRAG} & CON   & \textbf{0.45}  & \textbf{0.42}  & \textbf{0.40}  & \textbf{0.49}  & \textbf{0.44}  & \textbf{0.19}  & \textbf{0.45}  & \textbf{0.48}  & \textbf{0.41}  & \textbf{0.48}  & \textbf{0.46}  & \textbf{0.23}  \\
          & PRO   & \textbf{0.41}  & \textbf{0.55}  & \textbf{0.40}  & \textbf{0.39}  & \textbf{0.46}  & \textbf{0.21}  & \textbf{0.37}  & \textbf{0.52}  & \textbf{0.42}  & \textbf{0.42}  & \textbf{0.44}  & \textbf{0.21}  \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:rag-dpr}%
\end{table*}%




\subsection{RAG opinion manipulation result based on other retrievers}




To further explore RQ2: \textit{To what extent does Topic-FlipRAG affect the answers generated by the target RAG systems?}, we conducted RAG-based opinion manipulation experiments on the PROCON dataset using ANCE and DPR as retrievers. The results obtained for each baseline are consistent with the overall trends observed in Table~\ref{tab:rag-procon}. Detailed experimental outcomes are provided in Table~\ref{tab:rag-ance} and Table~\ref{tab:rag-dpr}.



