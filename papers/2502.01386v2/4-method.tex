


\section{Methodology}

\subsection{Overview}

In this study, we introduce \textbf{Topic-FlipRAG}, a two-stage, multi-granularity, topic-oriented attack pipeline designed to manipulate the stance polarity of retrieval-augmented generation (RAG) systems without altering the original content of the target document (Figure~\ref{overview}). Our proposed topic-oriented adversarial opinion manipulation attack for black-box RAG systems is also outlined in pseudo Algorithm~\ref{alg:topic-rag}. The pipeline proceeds as follows:


\begin{algorithm}[!t]
\caption{Topic-FlipRAG(black-box) }
\label{alg:topic-rag}
\KwIn{
  Query list $Q$, 
  target document $doc_\text{tar}$, 
  target opinion $S_t$, 
  iteration count $I$, 
  keynode size $K$, 
  augmentation factor $t$, 
  sample size $N$, 
  trigger length $T$, 
  beam size $B$, 
  top-$k$ size $K_b$, 
  learning rate $\eta$ 
}
\KwOut{Adversarial document $doc_\text{adv}$ (trigger + $doc_\text{know}$)}

\SetKwProg{Phase}{Phase}{}{}

\Phase{1: Knowledge-Guide-Attack}{
    \textbf{INIT:} $doc_\text{selected} \gets \{\}$ \\
    \For{$n = 1 \dots N$}{
        \For{$i = 1 \dots I$}{
            keynodes $\gets \mathrm{ExtractKeynodes}(Q, K)$ \\
            analysis $\gets \mathrm{AnalyzeKeynodes}(keynodes, doc_\text{tar})$ \\
            adjustment $\gets \mathrm{Adjust}(S_t, \text{analysis}, doc_\text{tar}, t)$ \\
            doc$_m \gets \mathrm{Edit}(S_t, \text{adjustment}, doc_\text{tar})$ \\
            filter\_list $\gets \mathrm{FilterDocument}(doc_m, doc_\text{tar})$ \\
            doc$_\text{selected}.\mathrm{extend}(\text{filter\_list})$
        }
        $t' \gets \mathrm{Rewarding Function}(t, \text{filter\_list}, doc_\text{tar}, Q)$ \\
        $t \gets t'$ \\
    }
    doc$_\text{know} \gets \mathrm{FindBestDoc}(doc_\text{selected})$ \\
    \Return{doc$_\text{know}$}
}

\Phase{2: Adversarial Gradient-Based Trigger Generation}{
    \textbf{INIT:} $\mathbf{Z} \leftarrow [\mathbf{z}_1, \ldots, \mathbf{z}_T], \quad , \text{where } \mathbf{Z}_t \in \mathbb{R}^{|\mathcal{V}|} \sim \mathcal{N}(0, 1)$,\\
    $\quad \mathbf{z}_t \leftarrow w + \epsilon \; \text{for} \; w \in \mathbf{z}_t \; \text{if} \; w \in \mathcal{V}_{\text{sim}}$\\
    Top-n similar words based on Q and top-ranking passages$\mathcal{V}_{\text{sim}}$

    \While{\text{not converged} \textbf{ and } \text{iteration} < $I$}{
     
        $combined\_vec \gets \mathrm{concat}(Z, doc_\text{know})$ \\
        $\nabla_Z \gets \mathrm{ComputeGradient}(combined\_vec, Q)$ \\
        Update $Z$: \\
        $Z \gets Z - \eta \cdot \nabla_Z$ \\
        Perform Beam Search for the best trigger $Z$: \\
    }

    
    $Trigger \gets \mathrm{BeamSearch}(Z, doc_\text{know}, T, B , K_b)$ \\

    \textbf{OUTPUT:} $doc_\text{adv} \gets \mathrm{concat}(Trigger, doc_\text{know})$ \\
    \Return{$doc_\text{adv}$}
}
\end{algorithm}


\noindent\textbf{Stage One: Knowledge-Guided Attack.}
In the initial phase, we leverage the extensive internal knowledge and reasoning capabilities of large language models to perform adversarial edits on the target document at a general semantic level. We identify and extract essential topic-related information nodes from the queries, then guide the LLM to integrate these nodes into \( doc_{\text{tar}} \) through a multi-granular editing strategy across three semantic dimensions. Simultaneously, rigorous polarity control is applied to preserve the document’s original polarity, ensuring precise stance manipulation. A dynamic reward function refines each modification step, embedding critical elements and relevant information while maintaining minimal disruption to the document’s original semantics. This process establishes a firm basis for subsequent adversarial opinion manipulation.

\noindent\textbf{Stage Two: Adversarial Trigger Generation.}
In the second phase, we enhance the similarity of the adversarially edited document \(doc_{\text{know}}\) from Stage One. Leveraging an open-sourced Neural Ranking Model (NRM), we generate a concise adversarial trigger specifically crafted to maximize the alignment between \( doc_{\text{know}} \) and the topic queries. This trigger is then fused with \( doc_{\text{know}} \) and injected as a poisoned document into the RAG system’s database, significantly increasing its likelihood of appearing in retrieval results. Consequently, the system’s generated output shifts toward the desired stance polarity, thereby fulfilling the goal of topic-oriented adversarial manipulation.
    

\subsection{Knowledge-Guided Attack}
\label{sec:knowledge_guide_attack}

In this section, we introduce a three-phase framework, termed the \textit{knowledge-guided attack} (abbreviated as know-attack), designed to systematically modify a target document \( doc_{\text{tar}} \) to enhance its relevance to a given query set. The framework aims to achieve minimal modification of the document while effectively integrating topic-specific nodes, thereby influencing the final outputs of RAG systems. By operating across multiple levels of textual modification, this approach ensures semantic coherence, adheres to minimal editing principles, and maintains a predefined stance, facilitating precise and controlled manipulation of the target document.



\noindent\textbf{Problem Statement.}  
Given a target document $\text{doc}_{\text{tar}}$ and a query set $\mathbf{Q}=\{q_1, q_2, \ldots, q_{|\mathbf{Q}|}\}$, we seek a modified document $\text{doc}_{\text{know}} = \text{doc}_{\text{tar}} \oplus \mathcal{P}$ that increases its overall relevance to all queries while adhering to a target stance $S_t$.

\begin{equation*}
\max_{\mathcal{P}} \frac{1}{|\mathbf{Q}|}\sum_{q \in \mathbf{Q}} Sim(q,\text{doc}_{\text{know}})
\end{equation*} 

subject to:
\begin{equation*}
\|\mathcal{P}\| \leq \epsilon,\quad Sim(\text{doc}_{\text{know}}, \text{doc}_{\text{tar}}) \geq \lambda,\quad S(\text{doc}_{\text{know}}) = S_t.
\end{equation*}

Here, $\|\mathcal{P}\|$ constrains the degree of textual alteration to ensure minimal editing, and $Sim(\cdot)$ ensures sufficient semantic similarity between the original and modified documents. By incorporating these constraints, the resulting $\text{doc}_{\text{know}}$ not only aligns with the chosen stance $S_t$ and achieves higher relevance for all queries, but also maintains the document’s intrinsic coherence and readability, increasing its likelihood of influencing the RAG system’s outputs.

\subsubsection {\textbf{Phase 1: Key Node Extraction and Selection}}
Given a topic and its corresponding list of queries $\mathbf{Q} = \{q_1, q_2, \ldots, q_{|\mathbf{Q}|}\}$, we first leverage a large language model (e.g.GPT-4o-mini) to identify a set of $K$ key information nodes that are most salient for maximizing relevance with respect to the entire query list. These $K$ nodes form a candidate node list $\mathcal{L}$ encapsulating crucial topical aspects.




Next, we analyze $\text{doc}_{\text{tar}}$ to determine which nodes from $\mathcal{L}$ are underemphasized or entirely absent. The LLM generates a \emph{doc-specific node list} with providing explicit reasoning for each missing or underemphasized node. This doc-specific list guides subsequent adversarial editing, ensuring that modifications focus on incorporating these critical nodes without introducing extraneous or tangential content.




\subsubsection{\textbf{Phase 2: Multi-Granular Adversarial Editing and Polarity Control}}
Inspired by \cite{liu2024multi} and adversarial strategies against black-box neural ranking systems \cite{liu2022order,wu2023prada}, we employ a multi-granular editing approach leveraging the advanced language understanding of LLMs. The editing operates across three levels: (1) \textbf{lexical substitutions}, where individual words are replaced with synonyms or node-related terms while preserving original meaning; (2) \textbf{sentential rewrites}, which adjust sentence structures and introduce minimal relevant information to incorporate node details without altering semantics; and (3) \textbf{phrase insertions}, adding brief, contextually appropriate sentences containing node information at strategic points to maintain narrative coherence and content integrity.


During this adversarial editing process, we employ a module called \textbf{Polarity Control} to enforce a target stance \( S_t \) constraint, guiding the document’s stance. For instance, if the target polarity is set to CON (opposing a viewpoint), the modifications introduced by the model consistently adopt negative or critical perspectives. This polarity control ensures that the integrated nodes and textual changes align with the desired stance, ultimately steering the RAG output towards the intended viewpoint.



\subsubsection{\textbf{Iteration with Rewarding Function and Final Output Selection}}

To adhere to the principle of minimal editing while ensuring effective document modification, we introduce an iterative framework guided by an \emph{augmentation factor} $t$. This factor dynamically controls the extent and frequency of adversarial modifications during each editing iteration. A larger $t$ promotes more extensive and aggressive edits, while a smaller $t$ leads to a more conservative editing strategy. To mitigate fluctuations caused by the stochasticity of LLMs, we perform $I$ repeated sampling runs for each value of $t$. This ensures consistent performance by averaging the outcomes across multiple candidate modifications. After each editing iteration, $t$ is adaptively adjusted based on feedback from a filtering mechanism to maintain a balance between relevance and minimal editing.

In each iteration $n$ ($n=1,\dots,N$), for a given $t_n$, we generate $I$ candidate modified documents $\{\text{doc}_{m}^{(n,i)}\}_{i=1}^I$. Each candidate document is evaluated against two critical metrics:


\textbf{Edit Distance(Edit Ratio)}: $d_{\mathrm{edit}}\bigl(\text{doc}_{m}, \text{doc}_{tar}\bigr)$ measures the extent of textual modification compared to the original document.

\textbf{Semantic Similarity}: $d_{\mathrm{sem}}\bigl(\text{doc}_{m}, \text{doc}_{tar}\bigr)$ evaluates how well the modified document preserves the original semantics.


To ensure minimal disruption, we specify a strict edit ratio threshold $\epsilon$ and a semantic similarity threshold $\theta$. A filtering indicator determines the validity of each candidate document:


{\small
\[
    \mathrm{Filter}\bigl(\text{doc}_{m}\bigr) 
    \;=\; 
    \begin{cases}
        1, & \begin{matrix}
        \text{if } (d_{\mathrm{edit}}\bigl(\text{doc}_{m}, \text{doc}_{tar}\bigr)\,\le\,\epsilon ) \\
        \;\;\wedge \;(d_{\mathrm{sem}}\bigl(\text{doc}_{m}, \text{doc}_{tar}\bigr)\,\le\,\theta),
        \end{matrix} \\
        0, & \text{otherwise}.
    \end{cases}
\]
}

The augmentation factor $t$ is then dynamically updated based on the feedback from the filtering layer. To balance the trade-off between under-editing and over-editing, we introduce a partial edit threshold $\rho = 0.75\,\epsilon$ and employ the following update rule:
{\small
\[
  t_{n+1} \;=\;
  \begin{cases}
    t_n - \delta, 
      & \text{if } \max\limits_{i} \mathrm{Filter}\bigl(\text{doc}_{m}^{(n,i)}\bigr) = 0, \\[6pt]
    t_n + \delta, 
      & \text{if } \exists\,
      \begin{matrix}
          ( i \text{ s.t. } \mathrm{Filter}\bigl(\text{doc}_{m}^{(n,i)}\bigr) = 1)\\ 
          \;\wedge\;
          (d_{\mathrm{edit}}\!\bigl(\text{doc}_{m}^{(n,i)}, \text{doc}_{tar}\bigr) < \rho), \end{matrix}
          \\
        t_n, 
      & \text{otherwise}.
  \end{cases}
\]}
Here, $\delta>0$ controls the aggressiveness of the adjustments. If no candidate satisfies the filtering constraints, $t$ is reduced to prevent excessive modifications. Conversely, if valid candidates exist but fail to meet the partial edit threshold $\rho$, $t$ is increased to encourage bolder edits. Otherwise, $t$ remains unchanged for stability.

After $N$ iterations, this process yields a set of $M$ valid candidate documents that satisfy the edit ratio and semantic similarity constraints. From this set, we apply a final selection phase to identify the optimal output. For each retained candidate $\text{doc}_m$, we use a neural ranking model (NRM) to estimate its relevance for each query $q_i \in \mathbf{Q}$, denoted as $R\bigl(q_i, \text{doc}_m\bigr)$. The average relevance score for each candidate is computed as:
\[
\bar{R}\bigl(\text{doc}_m\bigr) \;=\; 
\frac{1}{|\mathbf{Q}|}\,\sum_{i=1}^{|\mathbf{Q}|} R\bigl(q_i, \text{doc}_m\bigr).
\]

The final modified document $\text{doc}_{\text{know}}$ is selected by maximizing the average relevance:
\[
\text{doc}_{\text{know}} 
\;=\;
\arg\max_{\text{doc}_m} \; \bar{R}\bigl(\text{doc}_m\bigr).
\]

The iterative process, directed by the rewarding function and filtering mechanisms, ensures that the final output $\text{doc}_{\text{know}}$ incorporates topic-specific nodes with minimal yet effective modifications. By maintaining semantic coherence and adhering to predefined constraints, it enhances the relevance of $\text{doc}_{\text{know}}$ to the query set while aligning its stance with $S_t$.

\subsection{Adversarial Gradient-Based Trigger Generation}
\label{sec:adv_trigger_generation}

Building on the know-attack introduced in Section~\ref{sec:knowledge_guide_attack}, which integrates general semantic-level node information into the target document \(doc_{tar}\), we aim to further generate adversarial triggers targeting the retrieval model in the RAG system. Specifically, we append a short adversarial trigger \(T\) to the previously constructed \(\text{doc}_{\text{know}}\), resulting in the final adversarially augmented document \(\text{doc}_{\text{adv}}\). Inspired by adversarial semantic collision \cite{song2020adversarial}, we derive \(T\) through a gradient-based optimization process using a public neural ranking model\footnote{\url{https://huggingface.co/nboost/pt-bert-base-uncased-msmarco}}. By integrating \(\text{doc}_{\text{know}}\) with \(T\), we align the document more closely with the query set \(\mathbf{Q}\), increasing its prominence in the retrieval results. Once surfaced among the top-K documents, the strategically designed content within \(\text{doc}_{\text{adv}}\) can direct the LLM’s final output toward \(S_t\).


\subsubsection{Problem Formulation}

Our broader goal, as outlined previously in Section~3.2, is to influence the overall stance that the RAG system adopts when answering all queries in $\mathbf{Q}$. While our ultimate objective is to ensure that the LLM’s output consistently aligns with the target polarity $S_t$ across the entire query set, directly optimizing this stance-level objective remains challenging due to the lack of gradient signals from the LLM.

To address this challenge, we first concentrate on an intermediate relevance-based objective. Let $RM(q_i, \text{doc})$ represent the neural ranking model’s relevance score for a query-document pair $(q_i, \text{doc})$. Given a modified document $\text{doc}_{\text{adv}} = [\text{doc}_{\text{know}}, T]$, our interim goal is to maximize the average relevance score in all queries: \[ \max_{T} \frac{1}{|\mathbf{Q}|} \sum_{i=1}^{|\mathbf{Q}|} RM\bigl(q_i, [\text{doc}_{\text{know}}, T]\bigr). \].

By enhancing the document’s relevance to each $q \in \mathbf{Q}$, we increase the likelihood that $\text{doc}_{\text{adv}}$ appears among the top-$k$ retrieved documents for all queries. Achieving this intermediate objective paves the way for influencing the stance of the RAG system’s responses at a broader, topic-queries scale. 


\subsubsection{Gradient-Based Optimization}
We begin by leveraging gradients from a neural ranking model over the entire topic-query set, initially refining a soft trigger representation. Next, we employ beam search to transform the refined soft trigger into an optimal discrete token sequence. By iterating between gradient-based adjustments and beam search selection, the approach converges on a trigger that maximizes the document’s relevance across all topic queries. The following steps detail the core components of this optimization procedure.

\textbf{Gradient Optimization:} During each optimization step, we compute and combine gradients for all queries in \(\mathbf{Q}\). Additionally, a \emph{word enhancement argument} is introduced, which incorporates query-specific keywords and lexical distributions from the model’s vocabulary. This guided selection of candidate tokens captures semantic overlaps among multiple queries and enhances overall trigger relevance.

\textbf{Beam Search:} In the beam search phase, we evaluate candidate triggers—along with doc-specific augmentation—to select those that maximize combined similarity scores across the entire query set. This ensures the chosen trigger reliably improves the target passage’s ranking impact when integrated with \(\text{doc}_{\text{know}}\).

\textbf{Doc-Specific Augmentation:} At each iteration of gradient calculation and beam search, the trigger’s intermediate soft representation is evaluated only when concatenated with \(\text{doc}_{\text{know}}\) and considered jointly with all queries. Although this increases computational overhead, it ensures tight alignment with the target document’s context, yielding more pronounced improvements in ranking performance.



\subsubsection{Final Output: $\text{doc}_{\text{adv}}$ and RAG Opinion Influence}
After convergence, the adversarial trigger $T$ is appended to $\text{doc}_{\text{know}}$ to form the final adversarial document:
\[
\text{doc}_{\text{adv}} = [\text{doc}_{\text{know}}, T].
\]
This $\text{doc}_{\text{adv}}$ is then injected into the RAG system's corpus $D'$, significantly increasing its likelihood of being retrieved among the top-$k$ documents for each query $q \in Q$. Consequently, $\text{doc}_{\text{adv}}$ frequently appears in retrieval results, leading the LLM to generate outputs that align with the desired stance $S_t$. This process effectively manipulates the overall stance of the RAG system by ensuring that the augmented document is both prominently retrieved and influences the generated opinions towards $S_t$.


