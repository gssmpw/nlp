
\section{Experiments}

\subsection{Datasets}

\textbf{MSMARCO}.
We utilized the MS MARCO Passage Ranking dataset as the data source to evaluate the ability of our method to improve document rankings under more challenging topic-query tasks. Specifically, we assessed whether our method could significantly enhance the ranking of documents by the retrieval model within a RAG system.

To construct topic-lists for evaluation, we applied a K-means clustering algorithm to group similar queries, forming topics that each contained a series of related queries. To further evaluate the performance of our method under extreme topic-query scenarios, we applied an intra-topic similarity filtering process. Only topics with queries exhibiting high semantic diversity and containing a sufficient number of queries were retained.

This process resulted in 29 topics, with each topic containing an average of 22.28 queries. The average similarity score within each topic was approximately 0.5, indicating sufficient diversity among queries to ensure a rigorous evaluation. This curated dataset enabled us to test the robustness of our method in handling highly diverse and challenging topic-query tasks within a RAG system.

\textbf{PROCON}.
To conduct our experiments, we utilized controversial topic data scraped from the PROCON.ORG website. This dataset includes over 80 topics covering various domains, such as society, health, government, and education. Each topic is discussed from two stance labels \{\textit{PRO (support), CON (oppose)}\}, with passages arguing from these perspectives.

To simulate real-world user interactions with a RAG system, we instructed a large language model (GPT-4o) to act as a user and generate 40 potential sub-queries for each topic. These sub-queries were designed to reflect the diverse questions and concerns users might raise when exploring a specific controversial topic. 

After generating the sub-queries, we applied a similarity filtering process to ensure diversity by retaining only those with a similarity score below approximately 0.85. The filtering step effectively removed redundant queries while preserving a wide range of perspectives. As a result, the final set of topic-queries achieved an average similarity score of approximately 0.7, ensuring that the queries were sufficiently diverse yet semantically relevant. This process provided a robust and balanced sub-queries set for evaluation.


\subsection{Experiment Details}
The specific setting details for the Topic-queries RAG manipulation experiment are as follows:

(1) Black-box RAG. We represent the black-box RAG process as \( \text{RAG}_{\text{black}} \). The RAG framework is Conversational RAG from LangChain. The LLMs adopted in RAG are the open-source models Meta-Llama-3.1-8B-Instruct (Llama3.1), Qwen-2.5-7B-Instruct(Qwen2.5). The system prompt and additional detailed descriptions are provided in Appendix~\ref{exp-detail}.

(2) Retrieval Model Specification. We benchmark three dominant dense retrievers—Contriever \cite{gao2021unsupervised}, DPR \cite{karpukhin-etal-2020-dense}, and ANCE \cite{xiong2020approximate}.By convention, we use dot product between the embedding vectors of questions(queries) and candidate documents as their similarity score \(R\) in the ranking. 


\label{opinion-classfication}
(3) Opinion classification. We use Qwen2.5-Instruct-72B as the opinion classifier. Qwen2.5-Instruct-72B, due to its large parameter size, is capable of accurately identifying and classifying opinions within text.

(4) Experimental parameters. In knowledge-guided attack process, we set the maximum editing distance $\epsilon$ to 0.2, the semantic similarity threshold $\lambda$ to 0.85, and the number of iterations $N$ to 5. For adversarial trigger generation, we used a beam size of 3, top-$k$ of 10, a batch size of 32, a temperature of 1.0, a learning rate of 0.005, and a sequence length of 10. In RAG\textsubscript{black}, $k$ (the number of retrieved documents) is set to 3, with the LLM temperature also fixed at 1.0 to mirror real-world conditions.

(5) Poisoned Target. For the PROCON dataset, to investigate the manipulation performance under more challenging conditions, we performed relevance ranking for the documents with respect to each topic-query set $Q$ and the target stance $S_t$ . From the ranked list, we selected the last five documents (i.e., those with the lowest relevance) as the target poisoned documents.
For the MS MARCO dataset, we utilized the top-1000 relevance-ranked passage list for each topic-query set. From this list, we selected the passage with the lowest average rank as the target passage. This approach ensures that the evaluation focuses on passages that are least relevant to the target queries, thus providing a more rigorous benchmark for the proposed method.

(6) Experimental environment. All our experiments are conducted in Python 3.8 environment and run on a NVIDIA DGX H100 GPU. 

\subsection{Research Questions}

We propose four research questions to evaluate the effectiveness of our method in the topic-queries task, focusing on black-box NRM attacks and opinion manipulation to RAGs.

\textbf{RQ1}: Can Topic-FlipRAG significantly enhance the rankings of target documents in the RAG retriever for topic-queries?

\textbf{RQ2}: To what extent does Topic-FlipRAG affect the answers generated by the target RAG systems?

\textbf{RQ3}: Does topic-oriented opinion manipulation significantly impact users' perceptions of controversial topics?

\textbf{RQ4}: How robust does Topic-FlipRAG against exisiting mitigation mechanism?

\subsection{Baseline Settings}
To assess the effectiveness of our proposed method, we compare it against adversarial attack baselines designed for black-box, topic-oriented RAG scenarios, ensuring minimal modifications to the original documents. We exclude BadRAG\cite{xue2024badrag}, a backdoor RAG attack limited to white-box scenarios, and topic-IR-attack\cite{liu2023topic}, as its incomplete implementation prevents reliable replication.
For the selected baseline methods, we adapted them to meet the requirements of our task while preserving their core components. A brief overview of the baseline methods is provided below, with detailed descriptions available in Appendix~\ref{baselines-details}.

\textbf{PoisonedRAG.}
Zou et al.\cite{zou2024poisonedrag} propose an approach adaptable to both black-box and white-box settings. For our task, we employ its black-box strategy by inserting a randomly chosen query from the topic-queries set \( Q \) at the beginning of each document.

\textbf{PAT.}
This gradient-based adversarial retrieval attack uses a pairwise loss function to generate triggers that meet fluency and coherence constraints. We adapt PAT to produce triggers \( T_{\text{pat}} \) for target documents within the topic-queries set, evaluating their effectiveness under black-box conditions.


\textbf{Collision.}
This method generates adversarial paragraphs (collisions) via gradient-based optimization to produce content semantically aligned with the target query. In a topic-queries context, we create collisions for the entire topic-queries set and examine their transfer performance on black-box RAG retrievers.

These baseline methods provide benchmarks for comparing the efficacy of our approach in a fully black-box, topic-oriented RAG attack scenario.

\subsection{Evaluation Metrics}

For \textbf{RQ1}, we focus on ranking manipulation. We measure the average proportion of target opinions in top-3 rankings before and after manipulation (\(\text{Top3}_{\text{ori}}, \text{Top3}_{\text{att}}\)) and define top3-v as their difference. We also employ the Ranking Attack Success Rate (RASR), reflecting how often target documents are successfully boosted, and Boost Rank (BRank), denoting the average rank improvement for all target documents. Lastly, we report the proportion of target documents in the Top-50 and Top-500 positions to indicate how effectively they are pushed toward higher rankings.

\textbf{top3-v.} Computed by subtracting \(\text{Top3}_{\text{ori}}\) from \(\text{Top3}_{\text{att}}\), top3-v ranges from -1 to 1. A positive value signifies a successful increase of the target opinion in top-3 results, while a negative value indicates a detrimental effect.

\textbf{Ranking Attack Success Rate (RASR).} RASR captures how frequently target documents are successfully boosted in each query’s ranking. Higher values indicate greater attack effectiveness.

\textbf{Boost Rank (BRank).} BRank is the average rank improvement for all target documents under each query. A target document contributes negatively if its rank is unintentionally lowered.

\textbf{Top-50, Top-500.} These metrics represent the percentage of target documents that move into specific ranking thresholds in the MS MARCO Dataset after manipulation. Higher percentages imply more effective promotion of target documents. 


For \textbf{RQ2}, we employ Average Stance Variation (ASV) to assess how significantly our opinion manipulation influences the LLM’s responses in a black-box RAG. To address the natural variability of controversial topics and the inherent instability of large language models, we also propose Real Adjusted ASV (\(\Delta\)-ASV).

\textbf{Average Stance Variation (ASV).}
ASV is defined as the absolute difference between the manipulated opinion score and the original opinion score assigned to an LLM response (0 = opposing, 1 = neutral, 2 = supporting). A higher ASV signifies a more pronounced shift in polarity and hence greater manipulation effectiveness.

\textbf{Real Adjusted ASV ($\Delta$-ASV)}. To account for the inherent variability of controversial topics and the instability of large language models, we measure the baseline ASV in a clean state, denoted as ASV\textsubscript{clean} (calculated without adversarial manipulation). $\Delta$-ASV is then obtained by subtracting ASV\textsubscript{clean} from the manipulated ASV, i.e., \( \text{$\Delta$-ASV} = \text{ASV} - \text{ASV\textsubscript{clean}} \). This adjustment ensures that $\Delta$-ASV reflects the true impact of adversarial manipulation by eliminating the influence of natural stance variation. It reflects the extent to which the polarity of the RAG-system outputs is affected by the manipulation.  A positive $\Delta$-ASV indicates a significant shift in the opinion polarity due to manipulation, with larger values representing greater manipulation effectiveness.
