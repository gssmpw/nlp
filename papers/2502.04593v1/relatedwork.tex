\section{Related Work}
\label{sec:related}

\parhead{State-Space Models.} State-space models (SSMs) have emerged as a popular framework for modeling time-dependent data across various domains~\citep{gu2023mamba, rezaei2022direct,rezaei2021real,auger2021guide,rangapuram2018deep}. Recent advancements include the Mamba architecture~\citep{gu2023mamba}, which employs a selective state space mechanism defined by 
\begin{align*}
    \mathbf{h}_t &= \text{SSM}(\bx_t, \mathbf{h}_{t-1}), \quad \by_t = \text{Linear}(\mathbf{h}_t)
\end{align*}
where $\mathbf{h}_t$ represents the hidden state, $\bx_t$ is the input, and $\by_t$ is the output at time $t$. In contrast, the $\alpha$-Alternator employs a dynamic state transition, see Algorithm \ref{alg:Sampling}, where $\mathbf{z}_t$ serves an analogous role to Mamba's $\mathbf{h}_t$ but with explicit control over state transitions through $\alpha_t$. While Mamba has demonstrated success in applications from speech recognition~\citep{zhang2024mamba} to protein folding~\citep{xu2024protein}, its architecture requires high-dimensional hidden states $\mathbf{h}_t \in \mathbb{R}^d$ that have the same dimensionality as the data. The $\alpha$-Alternator addresses this limitation by operating in a lower-dimensional latent space $\mathbf{z}_t \in \mathbb{R}^{d_z}$ where $d_z \ll d$, while incorporating the adaptive weighting mechanism to balance between observation influence and state persistence. The lower dimensional state of the $\alpha$-Alternator ($d_z \ll d$) yields reduced computational complexity while the adaptive weighting mechanism is particularly beneficial for stochastic processes like neural recordings where noise characteristics vary significantly over time.

\parhead{Alternators.} The Alternator framework~\citep{rezaei2024alternators} represents a significant departure from traditional SSMs by introducing a dual-network architecture that alternates between producing observations and low-dimensional latent variables over time. The parameters of these two networks are learned by minimizing a cross entropy criterion over the resulting trajectories~\citep{rezaei2024alternators}. This approach has demonstrated superior performance compared to established methods such as Neural ODEs~\citep{chen2018neural}, dynamical VAEs such as VRNNs~\citep{gregor2014deep}, and diffusion models~\citep{dutordoir2022neural, lin2023diffusion} across various sequence modeling tasks. However, the Alternator uses a fixed weighting parameter $\alpha$ when defining the mean of the latent states, which is limiting. The $\alpha$-Alternator extends this framework by letting $\alpha$ vary across time steps using the Vendi Score to automatically adjust its reliance on observations versus latent history. The $\alpha$-Alternator maintains the computational efficiency of the original Alternator while providing greater robustness to temporal variations in sequence noise. Furthermore, the $\alpha$-Alternator's masking strategy during training strengthens its ability to handle missing or corrupted data, a common challenge in real-world applications such as neural decoding and time-series forecasting.