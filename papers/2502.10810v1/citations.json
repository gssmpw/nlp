[
  {
    "index": 0,
    "papers": [
      {
        "key": "yang2024ldre",
        "author": "Yang, Zhenyu and Xue, Dizhan and Qian, Shengsheng and Dong, Weiming and Xu, Changsheng",
        "title": "LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval"
      },
      {
        "key": "yang2024semantic",
        "author": "Yang, Zhenyu and Qian, Shengsheng and Xue, Dizhan and Wu, Jiahong and Yang, Fan and Dong, Weiming and Xu, Changsheng",
        "title": "Semantic Editing Increment Benefits Zero-Shot Composed Image Retrieval"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yang2023dawn",
        "author": "Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan",
        "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "reid2024gemini",
        "author": "Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "maaz2023video",
        "author": "Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz",
        "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "cheng2024videollama",
        "author": "Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others",
        "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lin2024vila",
        "author": "Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song",
        "title": "Vila: On pre-training for visual language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "qian2024streaming",
        "author": "Qian, Rui and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Ding, Shuangrui and Lin, Dahua and Wang, Jiaqi",
        "title": "Streaming long video understanding with large language models"
      },
      {
        "key": "chen2024videollm",
        "author": "Chen, Joya and Lv, Zhaoyang and Wu, Shiwei and Lin, Kevin Qinghong and Song, Chenan and Gao, Difei and Liu, Jia-Wei and Gao, Ziteng and Mao, Dongxing and Shou, Mike Zheng",
        "title": "VideoLLM-online: Online Video Large Language Model for Streaming Video"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jang2017tgif",
        "author": "Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee",
        "title": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xu2017video",
        "author": "Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting",
        "title": "Video question answering via gradually refined attention over appearance and motion"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2024mvbench",
        "author": "Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others",
        "title": "Mvbench: A comprehensive multi-modal video understanding benchmark"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2023llama",
        "author": "Li, Yanwei and Wang, Chengyao and Jia, Jiaya",
        "title": "Llama-vid: An image is worth 2 tokens in large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "huang2020movienet",
        "author": "Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua",
        "title": "Movienet: A holistic dataset for movie understanding"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "su2020moviechats",
        "author": "Su, Hui and Shen, Xiaoyu and Xiao, Zhou and Zhang, Zheng and Chang, Ernie and Zhang, Cheng and Niu, Cheng and Zhou, Jie",
        "title": "Moviechats: Chat like humans in a closed domain"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yu2019activitynet",
        "author": "Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng",
        "title": "Activitynet-qa: A dataset for understanding complex web videos via question answering"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "caba2015activitynet",
        "author": "Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan",
        "title": "Activitynet: A large-scale video benchmark for human activity understanding"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wu2021towards",
        "author": "Wu, Chao-Yuan and Krahenbuhl, Philipp",
        "title": "Towards long-form video understanding"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2023llama",
        "author": "Li, Yanwei and Wang, Chengyao and Jia, Jiaya",
        "title": "Llama-vid: An image is worth 2 tokens in large language models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "huang2020movienet",
        "author": "Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua",
        "title": "Movienet: A holistic dataset for movie understanding"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "su2020moviechats",
        "author": "Su, Hui and Shen, Xiaoyu and Xiao, Zhou and Zhang, Zheng and Chang, Ernie and Zhang, Cheng and Niu, Cheng and Zhou, Jie",
        "title": "Moviechats: Chat like humans in a closed domain"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "mangalam2024egoschema",
        "author": "Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra and others",
        "title": "Egoschema: A diagnostic benchmark for very long-form video language understanding"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "grauman2022ego4d",
        "author": "Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others",
        "title": "Ego4d: Around the world in 3,000 hours of egocentric video"
      }
    ]
  }
]