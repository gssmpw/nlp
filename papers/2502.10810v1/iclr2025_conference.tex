\PassOptionsToPackage{dvipsnames}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
%
\usepackage{subfigure}
\usepackage{bbding} 
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{capt-of}
\usepackage[subfigure]{tocloft}

\usepackage{graphicx}
\usepackage{wrapfig}

% \title{Formatting Instructions for ICLR 2025 \\ Conference Submissions}

\title{SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Zhenyu Yang\textsuperscript{1,2,3}\thanks{Work done during an internship at Kuaishou Technology.}, Yuhang Hu\textsuperscript{4}, Zemin Du\textsuperscript{5}, Dizhan Xue\textsuperscript{1,2}, Shengsheng Qian\textsuperscript{1,2}\thanks{Corresponding author.}, Jiahong Wu\textsuperscript{3},\\ \textbf{Fan Yang\textsuperscript{3}, Weiming Dong\textsuperscript{1,2}, Changsheng Xu\textsuperscript{1,2,6}} \\
\textsuperscript{1}Institute of Automation, Chinese Academy of Sciences,
\textsuperscript{2}University of Chinese Academy of Sciences,\\
\textsuperscript{3}Kuaishou Technology,
\textsuperscript{4}Zhengzhou University,
\textsuperscript{5}ShanghaiTech University,
\textsuperscript{6}Peng Cheng Laboratory\\
\texttt{yangzhenyu2022@ia.ac.cn}\\
\texttt{shengsheng.qian@nlpr.ia.ac.cn}\\
% \thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. 
%
Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. 
%
To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. 
%
We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains.
%, specifically designed to assess the capability to reason through time.
%
Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding.
%
We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks.
%
We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs.
%
Our benchmark and model can be accessed at \url{https://yzy-bupt.github.io/SVBench}.
\end{abstract}

\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}
\section{Introduction}
% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.

% Video understanding has become one of the most intensively researched areas in computer vision and artificial intelligence due to the proliferation of multimedia content and the rapid evolution of video capture and distribution technologies. Video understanding not only covers the traditional image recognition and semantic segmentation, but also involves the time dimension of information extraction, such as action recognition, event detection and scene understanding.In order to advance the research of video understanding technology, academia and industry have jointly created a number of benchmarks that have contributed to the advancement and improvement of algorithms in different aspects.
%

In recent years, the rapid advancements in Large Language Models (LLMs) \cite{touvron2023llama, achiam2023gpt} and visual processors \cite{radford2021learning, dosovitskiy2020image} have significantly enhanced the performance of Large Vision-Language Models (LVLMs) \cite{zhu2023minigpt, ataallah2024minigpt4, maaz2023video}. 
%
These powerful models have been instrumental in pushing the boundaries of artificial intelligence, showcasing exceptional progress in domains such as visual reasoning and dialogue, particularly in the field of video understanding. 
%

Furthermore, there is a growing interest in applying these advancements to the emerging field of streaming video understanding \cite{qian2024streaming, chen2024videollm}. 
%
In conventional video understanding tasks, models are given the entire video and can leverage past, current, and future content to comprehend a specific video segment.
%
In real-world scenarios, such as live streaming and security monitoring, video content is continually streamed, necessitating that dialogues should update concurrently with the temporal flow of the video without knowing the future information.
%
Recently, streaming videos have become increasingly popular with diverse sources, including online video platforms \cite{zellers2022merlot}, live streaming services \cite{gao2023livechat}, and wearable camera footage \cite{grauman2022ego4d}. 
%
Therefore, researchers have begun to study LVLMs with the ability to interpret and interact with streaming content \cite{qian2024streaming, chen2024videollm}. 
%
%For instance, VideoStreaming \cite{qian2024streaming} is designed to analyze and understand video content in real-time through streaming encoding and memory selection. 
%
%Besides, VideoLLM-online \cite{chen2024videollm} possesses the capability to respond instantaneously and sustain temporal reasoning throughout the duration of the video content. 
%
Given these research advancements, it becomes imperative to establish a comprehensive evaluation benchmark specifically tailored to assess the progress in streaming video understanding achieved by LVLMs. 
%This benchmark will play a crucial role in tracking future developments and guiding research efforts in this dynamic area of study.

% Propelled by a diverse array of high-quality instruction fine-tuning data sourced from various video platforms—such as online video sites, live streaming platforms, and wearable camera footage—LVLMs are set to make significant strides in the area of streaming video understanding. 
% %
% These models are developed to comprehend content synchronously as videos play, equipped with real-time response capabilities and the ability to reason through time. 
% %
% Therefore, it is essential to establish a comprehensive evaluation benchmark designed to effectively track and measure the advancements in streaming video understanding facilitated by LVLMs. 
%This benchmark should not only assess the technical performance of these models but also their applicability and efficiency in real-world scenarios. This will help ensure that LVLMs continue to evolve in a direction that maximizes their utility and impact across various applications in the ever-evolving landscape of video technology.
%


% Contrastingly, effective human-computer interaction in daily scenarios necessitates mastering core skills such as engaging in multi-turn dialogues and comprehending extensive contextual histories to foster coherent and contextually relevant conversations. Although present benchmarks do address these skills, they are constrained by the use of solely multiple static images \cite{feng2022mmdialog} or brief video snippets \cite{zhao2018multi}, and casual conversational formats that do not adequately test the capabilities required for Language and Vision Language Models (LVLMs). To truly evaluate and enhance these competencies, more rigorous and holistic benchmarks are essential.

% Motivated by this gap, we set forth to tackle \textbf{Challenge 1}: Developing a temporally coherent and contextually interconnected multi-turn question-answering dataset focused on extended video content.

Existing video benchmarks primarily focus on single-turn text input and cannot measure the ability to conduct temporally sequential reasoning, which falls short of capturing the complexity of streaming video understanding. 
%
For instance, several question-answering benchmarks \cite{yu2019activitynet,zhang2023movqa,xiao2021next} typically comprise disjointed QA pairs tied to individual video clips, ignoring the continuous and dynamic nature of video streams.
%
However, in real-world scenarios, users usually ask multiple questions during the entire duration of a video stream, with potential relevance among questions and video clips.
%Furthermore, current benchmarks are unable to measure the ability to sustain temporal reasoning and to provide immediate responses throughout the entire duration of video streams.
%
Such complex human-computer interaction necessitates mastering fundamental skills, including engaging in multi-turn dialogues and comprehending extensive contextual histories to foster coherent and contextually relevant conversations.
%
Although some current benchmarks partially assess essential interaction abilities, they are limited by relying on either multiple static images \cite{feng2022mmdialog} or short clips \cite{zhao2018multi} to simulate multi-turn dialogues, rather than using long streaming videos. 
%
%TikTalk \cite{lin2023tiktalk} has overcome these constraints by facilitating chitchat over full-length videos. 
%
%However, the format of casual conversations is not suitable for evaluating the capabilities. 
%
Therefore, appropriate benchmarks are required to evaluate the ability of streaming video understanding.
%
To tackle these problems, we propose the first benchmark for streaming video understanding, named \textbf{S}treaming \textbf{V}ideo understanding
\textbf{Bench}mark (\textbf{SVBench}), which aims at comprehensively
evaluating the temporal multi-turn dialogue capabilities of LVLMs
for streaming videos. 
%Seamlessly connecting a complete video narrative through a series of coherent question-answering sequences is far from a trivial task. 
%
%In real-world scenarios, such as interactions via head-mounted devices or real-time video streaming, video content is continually streamed, necessitating that questions and answers should update concurrently with the temporal flow of the video. However, existing question-answering benchmarks typically isolate local and global information and disregard the continuous and dynamic nature of video streams. 
%
%For example, MovieChat \cite{song2024moviechat} assesses the video understanding capabilities of LVLMs by employing two distinct modes: a global mode for evaluating the capacity to comprehend the overall video content and a breakpoint mode to evaluate the capacity to capture specific moments. 



\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{Figure/overview.png} % Reduce the figure size so that it is slightly narrower than the column.
\vspace{-0.6cm}
\caption{\textbf{Illustration of temporal multi-turn dialogues.} A temporal dialogue path represents a conversation within a video progressing over time. Our SVBench evaluates the capabilities of LVLMs in long-context streaming video understanding by constructing temporal dialogue paths to assess 9 critical skills.}
\vspace{-0.2cm}
\label{fig:teaser}
\end{figure*}

First, we introduce a novel and challenging task named temporal multi-turn dialogue for streaming videos. We define a QA chain to represent a series of consecutive multi-turn dialogues over a video segment. Subsequently, we define \textit{Temporal Linkages} between successive QA chains for video segments, which can be established based on the common people, events, objects, etc. LVLMs should understand the current video segment, dialogue, as well as historical video segments and dialogues to answer the current question. For example, as shown in Figure \ref{fig:teaser}, to answer the question \textit{``Did any other racers fall?"}, an LVLM should recall the previously called racer in historical video segments and dialogues. Our proposed task aims to comprehensively evaluate the capability to leverage historical content and conduct multi-turn dialogue throughout a real-time streaming video.

Second, we construct a large-scale dataset with temporal multi-turn question-answering chains for the proposed streaming video understanding task. We compare our dataset with existing video datasets in Table \ref{tab:fashioniq}. 
%
Our dataset comprises 1,353 diverse videos from 6 streaming platforms, each undergoing thorough filtering and meticulous selection. Coupled with streaming videos are annotations comprising 49,979 QA pairs, where, on average, each video contains 36.94 pairs, which is the highest number among known video datasets. Moreover, we build temporal dialogue paths that occur in sync with a video progressing over time, as shown in Figure \ref{fig:teaser}, designed specifically to assess the capability to reason effectively through time.

Third, we conduct extensive evaluations, in terms of dialogue and streaming performance, of various prevalent LVLMs on our SVBench.
%
Our results provide the first overview insight into the streaming video understanding capability of existing LVLMs. 
%
Surprisingly, these state-of-the-art LVLMs are far from satisfactory, in terms of streaming video understanding.
%
These results motivate us to develop a stronger LVLM, namely \textbf{StreamingChat},
% , by jointly utilizing training data in our dataset and a wide spectrum of existing video datasets. 
which significantly improves the overall dialogue evaluation score by 9.41\% and the dialogue evaluation score by 3.30\% on our SVBench, compared to the top-performing open-source LVLMs, while achieving comparable performance on conventional image and video benchmarks. Our benchmark and model will be publicly available, in order to catalyze the progress in streaming video understanding.

%We construct these QA chains for video dialogue to gauge the ability of LVLMs to sustain multi-turn dialogues and comprehend extensive video-aligned contextual histories, thus promoting coherent and contextually relevant conversations.

% To deal with these challenges, we introduce \textbf{SVBench}, a novel benchmark with temporal multi-turn question-answering chains for streaming video understanding, which encompasses a video understanding dataset, a data construction pipeline, as well as a set of evaluation methods designed to thoroughly assess current LVLMs. 
% %
% For \textbf{Challenge 1}, we design QA chains to represent a series of consecutive multi-turn dialogues over a video segment. We construct these QA chains for video dialogue to gauge the ability of LVLMs to sustain multi-turn dialogues and comprehend extensive video-aligned contextual histories, thus promoting coherent and contextually relevant conversations.
% %
% For \textbf{Challenge 2}, we propose a methodology for constructing \textit{Temporal Linkages} between successive QA chains in our dataset. This facilitates temporal dialogue paths that occur in sync with a video
% progressing over time, as shown in Figure \ref{fig:teaser}, designed specifically to assess the capability to reason effectively through time.
% This design enables the creation of temporal dialogue paths that unfold in sync with the video as it plays over time, as illustrated in Figure \ref{fig:teaser}. This approach is specifically intended to evaluate the ability to reason effectively across temporal dimensions.

%reasoning-through-time  


% In summary, our contributions are summarized as follows:
% \begin{itemize}
% \item 
% %
% We propose \textbf{SVBench}, a novel benchmark with temporal multi-turn question-answering chains,
% %for streaming video understanding
% which encompasses a video understanding dataset, a data construction pipeline, as well as a set of evaluation methods designed to thoroughly assess the ability of long-context streaming video understanding of current LVLMs.
% %

% \item
% We construct a video dataset comprising 1,353 diverse videos from 6 streaming platforms, each undergoing thorough filtering and meticulous selection. 
% %
% Coupled with these are annotations comprising 49,979 QA pairs, where, on average, each video contains 36.94 pairs, the highest number compared to any other known dataset.

% %On average, each video features 36.94 pairs, which is the highest number of question-answer pairs per video compared to any other known dataset.
% %each video contains around 7 times more pairs than the currently largest known dataset.
% %


% %
% \item
% % We propose a semi-automated video annotation pipeline for streaming video, which includes a multi-stage LVLM-assisted process for generating: (1) multi-turn QA chains to assess the model's multi-turn video question-and-answer capability; (2) QA score checking; (3) establishing links between QA chains to assess the model's ability to reason over time, as well as facilitating multi-turn manual annotations.
% We design a semi-automated video annotation pipeline for temporal multi-turn dialogues in streaming videos, involving a multi-stage LLM-assisted generation process interspersed with iterative rounds of manual annotation.
% %
% % Our method involves a multi-staged language model-assisted generation process combined with iterative rounds of manual annotation.
% %: (1) Generating multi-turn questions and answers to create QA chains, to assess the video contextual dialogue capability.
% %(2) Generating evaluation scores to check the quality of the QA.
% %(3) Generating connections between QA chains, to evaluate the ability to reason through time. This process is interspersed with multiple rounds of manual annotation.

% \item 
% % We release the first streaming video understanding benchmark and conduct extensive dialogue and streaming evaluation to evaluate the comparable performance of long-context and streaming understanding capability.
% %
% We release the first benchmark for streaming video understanding and conduct extensive experiments including dialogue and streaming performance evaluations of LVLMs in handling long-context and streaming video comprehension.
% % Extensive experiments conducted on the benchmark datasets demonstrate ...
% \end{itemize}

\begin{table*}[t]
% \renewcommand\arraystretch{0.8}
\centering
\scriptsize
    %
\caption{The comparison of different datasets. \textbf{Avg. Q/V}: the average number of QA pairs per video. \textbf{Open-Domain}: whether the video sources are diverse. \textbf{Long}: whether the average video length is greater than 2 minutes. \textbf{Dialogue}: whether there are contextual connections between QA pairs. \textbf{Streaming}: whether the QA pairs can be tested in sync with the video over time.}
% Best and second-best scores are highlighted in bold and underlined, respectively.}
% \vspace{-1mm}

\setlength{\tabcolsep}{5pt}
\begin{tabular}{@{}cccccccc@{}}
\toprule
\textbf{Dataset}                                              & \textbf{\#QAs}           & \textbf{Avg. Q/V}       & \textbf{Long}                           & \textbf{Open-Domain}                         & \textbf{Dialogue}                            & \textbf{Streaming}                           & \textbf{Annotation}            \\ \midrule
EgoSchema   \cite{mangalam2024egoschema}          & 5,063           & 1.00           & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Auto\&Manual          \\
ActivityNet-QA   \cite{yu2019activitynet}         & 800             & 1.00           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Manual                \\
MVBench \cite{li2024mvbench}                      & 4,000           & 1.10           & \color{red}{\XSolidBrush}           & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Auto                  \\
How2QA \cite{li2020hero}                          & 44,007          & 2.00           & \color{red}{\XSolidBrush}           & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Manual                \\
Perception Test   \cite{patraucean2024perception} & 44,000          & 3.79           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Auto\&Manual          \\
Social-IQ   \cite{zadeh2019social}                & 7,500           & 6.00           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Auto\&Manual          \\
MSVD-QA \cite{xu2017video}                        & 13,157          & 6.68           & \color{red}{\XSolidBrush}           & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Auto                  \\
TVQA \cite{lei2018tvqa}                           & 152,545         & 7.00           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Manual                \\
NExT-QA \cite{xiao2021next}                       & 52,044          & 9.57           & \color{red}{\XSolidBrush}           & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Manual                \\
MovieChat   \cite{song2024moviechat}              & 13,000          & 13.00          & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Manual                \\
LVBench \cite{wang2024lvbench}                    & 1,549           & 15.04          & \color{ForestGreen}{\CheckmarkBold} & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Manual                \\
TGIF-QA \cite{jang2017tgif}                       & 165,165         & 17.25          & \color{red}{\XSolidBrush}           & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Auto\&Manual          \\
MSRVTT-QA \cite{xu2017video}                      & 72,821          & 24.35          & \color{red}{\XSolidBrush}           & \color{ForestGreen}{\CheckmarkBold} & \color{red}{\XSolidBrush}           & \color{red}{\XSolidBrush}           & Auto                  \\ \midrule
\textbf{SVBench (Ours)}                           & \textbf{49,979} & \textbf{36.94} & \color{ForestGreen}{\CheckmarkBold} & \color{ForestGreen}{\CheckmarkBold} & \color{ForestGreen}{\CheckmarkBold} & \color{ForestGreen}{\CheckmarkBold} & \textbf{Auto\&Manual} \\ \bottomrule
\end{tabular}
\label{tab:fashioniq}
\vspace{-0.2cm}
\end{table*}

\section{Related Work}
% \subsection{Streaming Video Models}
\subsection{Large Vision-Language Models for Video}
%Based on the significant achievements of Large Language Models (LLMs), recent studies have increasingly shifted towards exploring and developing Large Vision-Language Models (LVLMs). These research endeavors aim to enhance multimodal comprehension and generation capabilities, fully leveraging the abilities of LLMs to tackle new tasks, particularly those involving video. Currently,  LVLMs that process video tasks can be broadly categorized into two types based on the nature of their inputs: one type utilizes text and static images as input, such as the recently popular GPT-4(Vision) and GPT-4(Omni); the other type employs video modalities that go beyond static images as input, aiming to further unearth the potential of LLMs in video task processing to improve performance like PLLaVA and Gemini. However, these LVLMs are not yet fully competent in handling video tasks, as they still lack a profound understanding of video content. Therefore, we propose a challenging video benchmark—streamingbench—to objectively and thoroughly assess their capabilities.
Recent advancements in Large Language Models (LLMs) have paved the way for a significant research focus on Large Vision-Language Models (LVLMs) aimed at improving multimodal understanding, such as multimedia retrieval~\cite{yang2024ldre,yang2024semantic}, particularly in video content.
%
Currently, in addition to the popular closed-source LVLMs such as GPT-4V \cite{yang2023dawn}, GPT-4o \cite{achiam2023gpt}, and Gemini 1.5 Pro \cite{reid2024gemini}, an increasing number of open-source LVLMs, including Video-ChatGPT \cite{maaz2023video}, VideoLLaMA2 \cite{cheng2024videollama}, and VILA \cite{lin2024vila}, also have demonstrated the impressive capability in video understanding tasks. 
%
Advanced human-computer interaction in everyday life requires the ability to engage in multi-turn dialogues and understand extensive contextual histories to maintain coherent and contextually appropriate conversations \cite{qian2024streaming, chen2024videollm}.
%
However, these LVLMs are still not fully adept at handling the intricacies of streaming videos and do not completely grasp the complexities of real-world contexts.
%
% Therefore, we propose a challenging video benchmark—SVbench—to objectively and thoroughly assess their capabilities.
%
% 
To rigorously evaluate the capabilities of these models, we propose SVBench to measure the performance of LVLMs in video-related tasks that imitate the complexity of real-world interactions.

\subsection{Video Understanding Benchmarks}
In recent years, the exponential growth of video data has elevated video understanding to a crucial area within computer vision.
%
To rigorously assess the performance of LVLMs, researchers have introduced a range of standardized benchmarks. 
%
These benchmarks provide not only comparative evaluation criteria for models but also catalyze advancements in video understanding models. 
%
The recent benchmarks, such as TGIF-QA \cite{jang2017tgif}, MSVD-QA \cite{xu2017video}, and MVBench \cite{li2024mvbench}, primarily comprise relatively brief videos capturing single events, thereby overlooking the temporal dependencies inherent in longer videos. 
%
To address the intricacies of long video comprehension, benchmarks using longer videos like movies have been developed. 
%
For instance, LLaMA-Vid \cite{li2023llama}, based on MovieNet \cite{huang2020movienet}, has developed a movie QA dataset to identify character relationships.
%
Similarly, MovieChat \cite{su2020moviechats} employs a diverse set of videos and avoids specific character names or plot details within its questions.
%
However, these benchmarks often fall short in addressing the intricate challenges posed by streaming videos, which encompass extended temporal contexts and dynamic scene variations. 
%
Therefore, we establish SVBench, a novel and comprehensive benchmark that aims to bridge this gap by offering an elaborate evaluation framework for long-context streaming video understanding.
% However, these benchmarks operate offline and rely on entire videos or single images for evaluation. 
%
%In contrast, our newly proposed SVBench is designed to assess the ability to continuously receive and process video frames, as it concurrently updates visual content and evaluates the capacity to handle a dynamic dialogue in real time.
% In contrast, our proposed SVBench is designed to assess the ability to continuously receive and process video frames, as it concurrently updates visual content and evaluates the capacity to handle a dynamic dialogue in real-time.
% In contrast, an online assistant should be equipped to continuously receive and process video frames, as it concurrently updates the visual content and facilitates a dynamic dialogue in real-time.
% MovieChat takes on a different approach, avoiding specific plot details in its questions, thereby requiring minimal dependency understanding. Benchmarks like EgoSchema focus on first-person footage with an emphasis on daily activities, but tend to involve short sequences with few scene changes. In contrast, our new benchmark, StreamingBench, offers numerous benefits for evaluating video understanding models, thanks to its complex scenarios, wide range of question-answer pairs, focus on temporal and causal reasoning, and dynamic scene simulation.
%
% Furthermore, their annotations are automatically generated, lacking human verification. ActivityNet-QA \cite{yu2019activitynet}, derived from the ActivityNet \cite{caba2015activitynet} dataset, introduces longer motion, spatial, and temporal QAs, thereby increasing the complexity of video understanding tasks. Nevertheless, it remains largely focused on specific activities within short videos, failing to address the detection and inference required for complex scenes in extended videos. 
%To address the intricacies of long video comprehension, researchers have begun utilizing long videos, particularly films, to construct benchmarks. For instance, the LVU \cite{wu2021towards} dataset proposes tasks such as predicting the release year of a movie and identifying character relationships. Similarly, LLaMA-Vid \cite{li2023llama}, based on MovieNet \cite{huang2020movienet}, has developed a movie QA dataset. Despite employing long videos, many questions pertain to general knowledge about films, allowing answers to be obtained without intensive video analysis. In contrast, MovieChat \cite{su2020moviechats} employs a diverse set of videos and intentionally avoids specific character names or plot details within its questions. However, the dependency relationships between question-answer pairs are relatively minimal, enabling models to respond based on isolated information. Beyond cinematic content, other long video understanding benchmarks have emerged, such as EgoSchema \cite{mangalam2024egoschema}, which utilizes first-person footage from Ego4D \cite{grauman2022ego4d} to present video reasoning tasks. Nonetheless, EgoSchema mainly concentrates on shorter segments of daily activities, featuring minimal scene transitions. In this paper, we propose StreamingBench, a novel benchmark dataset designed to evaluate the performance of video understanding models in dynamic, continuous scenes. When compared to preceding benchmarks, StreamingBench offers several notable advantages: (1) complexity of scenarios (2) extensive question-answer pairs (3) assessment of temporal continuity and causal reasoning, and (4) simulation of dynamic scenes.



\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{Figure/framework.png} % Reduce the figure size so that it is slightly narrower than the column.
\vspace{-0.4cm}
\caption{\textbf{Overview of the proposed SVBench framework:} (1) Filtering raw videos from diverse streaming sources; (2) Detecting scenes and splitting videos accordingly; (3) Constructing QA chains for dialogues within videos; (4) Performing manual annotation and quality assessment; (5) Identifying temporal linkages between QA chains; (6) Connecting QA chains to facilitate temporal reasoning; (7) Building temporal dialogue paths for evaluating LVLMs.}
\vspace{-0.2cm}
\label{fig2}
\end{figure*}
% "Overview of the proposed SVBench framework: (1) Filtering raw videos from diverse streaming sources; (2) Detecting scenes and splitting videos accordingly; (3) Constructing question-and-answer (QA) chains for dialogues within videos; (4) Performing manual annotation and quality assessment; (5) Identifying temporal linkages between events; (6) Connecting QA chains to facilitate temporal reasoning; (7) Building temporal dialogue paths for evaluating Language-Vision-Language Models (LVLMs)."


\section{Dataset}
We develop a comprehensive data collection pipeline to construct a high-quality streaming video dataset, tailored for annotating temporal multi-turn dialogues. 
%
We split the SVBench dataset into a training set and an evaluation set, ensuring that videos and their corresponding QA pairs appear in only one split. This results in 42,605 QA pairs for training and 7,374 QA pairs for evaluation, with 1,153 videos and 200 videos in each set, respectively.
% This pipeline includes sophisticated filtering, scene detection, and video splitting to guarantee video quality and suitability for in-depth temporal analysis.
%
% We have developed a detailed data collection pipeline to create a high-quality streaming video dataset, specifically designed for the annotation of complex, temporally-structured multi-turn dialogues. This pipeline includes advanced filtering techniques, accurate scene detection algorithms, and precise video segmentation to ensure that each video clip is of high quality and appropriately suited for in-depth temporal analysis.
% In this study, we outline a comprehensive data collection pipeline designed to construct a high-quality streaming video dataset tailored for advanced computer vision and multimedia analysis tasks. The proposed pipeline is systematically categorized into several key stages, ensuring the dataset's relevance, diversity, and overall quality.

\subsection{Data Filtering and Scene Splitting}

We source 12,989 raw video data from a variety of publicly available datasets, including YT-Temporal-1B \cite{zellers2022merlot}, YouCook2 \cite{zhou2018towards}, ActivityNet \cite{caba2015activitynet}, MovieChat \cite{song2024moviechat}, Panda-70M \cite{chen2024panda}, and Ego4D \cite{grauman2022ego4d}. 
%
These datasets offer a wide spectrum of video content, ensuring a rich diversity that is essential for streaming video understanding. 
%
We then filter high-quality videos of adequate lengths, high aesthetic scores, and appropriate optical flow scores.
%
% To ensure the efficacy and relevance of the data, an initial filtering step is conducted based on video duration. 
%
% Specifically, videos with a duration of less than one minute or greater than eight minutes were excluded. This duration-based filtering ensures that the remaining videos possess sufficient informational richness for subsequent analyses.
% Aesthetic Evaluation aesthetic quality
% Optical Performance Assessment Optical Flow Score
% To maintain data quality and relevance, we implemented an initial screening based on video length, excluding any videos shorter than one minute. This criterion is established to guarantee that the selected videos provide ample informational depth for our subsequent analysis.
%\subsection{Scene Detection and Video Splitting}
Subsequently, we employ PySceneDetect\footnote{https://github.com/Breakthrough/PySceneDetect} to identify and enumerate scenes within filtered videos. These results are crucial in determining whether the video content exhibits adequate variation and complexity. Further filtering is conducted to retain only those videos containing 5 to 15 scenes, thereby excluding content that is either excessively monotonous or overly intricate. Moreover, only videos with an appropriate average scene duration are chosen, ensuring fluidity and rhythm. Finally, 1,353 videos are selected. 
%
Ultimately, we split each video into clips $V=\{s_i \ |\  0 \leq i < |V| \}$ based on timestamps, merging clips shorter than 2 seconds with their adjacent ones. Notably, to prevent disjointedness and ensure continuity between scenes, we add an extra 0.5 seconds to both the beginning and end of each clip, resulting in a one-second overlap between consecutive clips. More details are included in Appendix \ref{apd:data}.
%
% are chosen, ensuring a smooth flow and rhythm that are crucial for effective streaming data analysis.
% Moreover, the average duration of scenes in each video is calculated. Videos where the average scene duration ranged from 5 to 30 seconds were chosen to ensure a smooth flow and rhythm, 

% \subsection{Aesthetic Evaluation}

% To further ensure dataset quality, an aesthetic evaluation model is applied to assess the visual appeal of each video. This model generated an aesthetic score based on a series of visual features. Videos with an aesthetic score greater than four (on a scale from one to five) were retained. This selection criterion ensures that the dataset not only contains content-rich videos but also those with high visual appeal.

% \subsection{Optical Performance Assessment}

% Further evaluation of video quality is conducted using an optical scoring model, which assesses parameters such as brightness, contrast, and light distribution. The optical scoring model employs a range of image processing techniques to generate scores. Videos with optical scores outside the range of 0.5 to 100 were eliminated, thereby removing those with obvious optical deficiencies or subpar quality. This step further refined the dataset by eliminating videos of poor optical quality, enhancing the overall dataset standard.






We propose a semi-automated annotation pipeline for streaming videos, as shown in Figure \ref{fig2}, including a multi-stage LLM-assisted generation process with several rounds of manual annotation.
%
See Appendix \ref{apx:prom} for detailed prompts.
% 
The annotation takes about 3 months and involves over 30 professional annotators.
%
% We design a semi-automated video annotation pipeline for streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over a video segment and constructing temporal linkages between successive QA chains.
%, specifically designed to assess the capability to reason through time.
\subsubsection{Constructing QA Chains for Video Dialogues}
% In order to evaluate a model's ability to engage in multi-turn dialogues and to understand the capacity of long contextual histories to facilitate coherent and contextually appropriate conversations, we propose the construction of a multi-turn question-answering dataset on extended videos. Initially, we define a series of N-turn question-and-answer interactions on video segments as a single question-and-answer chain in our paper.
We propose creating a multi-turn question-answering dataset on videos to evaluate the ability of LVLMs to conduct multi-turn dialogues and comprehend video-related contextual information. This ability is essential for the coherence and relevance of conversations in various contexts. Initially, we define a series of multi-turn question-and-answer interactions on video clips as QA chains.
% Q and A represent the jth question and answer on the i-th video clip
For every video clip $s_i \in V$, we build a QA chain $C_i = \{ \langle Q_j^i, A_j^i \rangle \ |\  0 \leq j < |C_i| \}$, where $Q_j^i$ and $A_j^i$ represent the $j$-th question and answer generated on the $i$-th video clip. To achieve this, we harness the video understanding capabilities of existing LVLMs (e.g. GPT-4o):
\begin{equation}
\mathcal{C} = \{ C_i = \text{LVLM}(p_v,s_i) \ | \ 0\leq i<M \},
\end{equation}
where $p_v$ represents the prompt that generates 5 to 6 consecutive rounds of questions and answers, and $M=|V|$ is the number of clips segmented from the video. 
%
However, due to the limitations of the video understanding and text generation capabilities of current LVLMs, we have to employ human annotators to manually augment, delete, and modify the QA pairs so that the QA in the chain is connected and aligned with the video. 
%
% 例子
For instance, specific persons or objects mentioned in questions should be modified to utilize third-person pronouns (e.g. he/she/it/they).
%

% we conceptualize an 'N-turn question-and-answer thread'—a structured sequence of interactive exchanges set against video segments—as the foundational element for this multi-faceted dataset detailed within our paper.


\subsubsection{Implementing QA Quality Evaluation}
%In the endeavor to ensure high-quality QA chains for streaming video content within the SVBench framework, our methodology incorporates a multi-faceted evaluation mechanism leveraging advanced large language models. 
Due to the inconsistent quality of manually annotated QA chains, we devise a comprehensive evaluation mechanism to guarantee their high quality. 
% The evaluation of these QA chains is conducted with GPT-4, which assesses them across 7 critical dimensions: Accuracy, Completeness, Relevance, Fluency, Context Understanding, Logicality, and Temporal Understanding. Each dimension is scored on a scale of 0 to 100, providing a comprehensive measure of the QA chain quality.
We utilize GPT-4 to assess QA chain quality across 7 dimensions—accuracy, completeness, relevance, fluency, contextual comprehension, logical consistency, and temporal understanding, scoring each from 0 to 100. 
%
%
In addition to the 7 dimensions, the QA chain quality also has an overall score from 0 to 100, with a 90-point minimum for high standards. 
%
Any QA chain failing to meet this threshold must undergo manual revision again. This iterative process ensures the production of highly reliable and insightful QA chains, contributing to the utility of our SVBench.
% In addition to the 7 dimensions, there is an overall score providing a holistic evaluation of the QA chain quality, which is an aggregated metric that synthesizes the scores from all dimensions. This score, which is on a scale from 0 to 100 as well, requires achieving a threshold of 90 to ensure high standard compliance. Any QA chain failing to meet this threshold should undergo manual revision again. This iterative process guarantees the production of highly reliable and insightful QA chains that contribute to the fidelity and utility of the SVBench.
% A composite Overall Score, with a high-quality threshold set at 90, aggregates these metrics. QA chains that fall short are subject to revisitation, thereby ensuring the integrity and value of SVBench outputs. 
% In addition to the seven dimensions used for individual assessments, the QA chain's quality is also evaluated using an overall score that gives a comprehensive evaluation across all dimensions. This score ranges from 0 to 100. To meet the high standard set by SVBench, a QA chain must achieve a score of at least 90. If a QA chain does not reach this threshold, it should be subjected to further manual review. By applying this iterative review process, we can ensure that only QA chains that are highly reliable and offer valuable insights are included in SVBench.


\subsubsection{Identifying Temporal Linkages}
%
Given that adjacent QA chains are derived from sequential video clips, they inherently contain overlapping entities such as objects, scenes, and events, as well as inherent temporal linkages.
%
To effectively establish these temporal linkages, we initially employ LLMs (e.g. GPT-4) to search for and identify potential linkages between adjacent QA chains.
%
For the junction of the $i$-th and the $(i+1)$-th QA chains, we construct a set of relations  $R_i (0\leq i<M-1)$ as follows:
\begin{equation}
R_i = \{ r_j |\  0 \leq j < |R_i| \} = \text{LLM}(p_l,C_i,C_{i+1}),
\end{equation}
% stimulates the generation of candidate connections and contextual categories between two QA sequences.
where $p_l$ is the prompt that stimulates the generation of candidate relations and contextual categories between two given QA chains. Here, an individual relation $r_j$ is represented as a quintuple, structured as:
\begin{equation}
r_j = \langle Q_x^i, A_x^i, Q_y^{i+1}, A_y^{i+1}, Rc \rangle,
\end{equation}
which suggests that a relation exists between the $x$-th QA pair within the QA chain of the $i$-th clip and the $y$-th QA pair within the subsequent QA chain, characterized by the relationship category $Rc$.
% To this end, we define a series of relationship categories, including actions, people, objects, events, environments, quantities, etc.
Accordingly, we have delineated a range of relationship types to classify these relations, the outer ring as shown in Figure \ref{fig:vis:b}.
%
% These candidate connections are usually similar QA pairs. action, person, object, event, environment, and quantity.
\subsubsection{Linking QA Chains for Temporal Reasoning}
To evaluate the ability to reason through time, we need to establish temporal linkages between successive QA chains, which facilitates multi-turn QA interactions between the chains throughout the duration of the video content.
%so that multiple rounds of QA can be performed between chains as the video plays.
% In order to assess our model's capacity for temporal reasoning, it's necessary to construct temporal connections between successive QA chains in our dataset. This will facilitate multi-round question-answering interactions between the chains in synchronicity with the video playback.
%
Identifying potential connections between two consecutive QA chains allows us to create a coherent narrative thread by adjusting the QA pairs in the following chain. This process helps to maintain a smooth multi-turn QA interaction across the entire video.
%
The modified relation $\widetilde{r}_j$ is as follows:
%
\begin{equation}
\widetilde{r}_j = \langle Q_x^i, A_x^i, \widetilde{Q}_y^{i+1}, \widetilde{A}_y^{i+1}, \widetilde{Rc} \rangle,
\end{equation}
%
where $\widetilde{Rc}$, $\widetilde{A}_y^{i+1}$ and $\widetilde{Q}_y^{i+1}$ respectively represent the modified relationship category, the question and answer in the subsequent chain after modification.
%
During the modification phase, the following criteria are met: (1) Contextual coherence within each QA pair of a single chain (e.g., $\langle Q_{y-1}^{i+1}, A_{y-1}^{i+1} \rangle \rightarrow \langle \widetilde{Q}_y^{i+1}, \widetilde{A}_y^{i+1} \rangle \rightarrow \langle Q_{y+1}^{i+1}, A_{y+1}^{i+1} \rangle$) has to be preserved. 
% (2) that logical connections are also generated between the previous and next question and answer chains, and cross-clip reasoning is performed, such as $\langle Q_{x}^{i}, A_{x}^{i} \rangle \stackrel{Rc}{\longrightarrow} \langle \widetilde{Q}_y^{i+1}, \widetilde{A}_y^{i+1} \rangle$. 
(2) Logical links should be forged both within individual chains and between different chains, to facilitate cross-clip reasoning, as illustrated by $\langle Q_{x}^{i}, A_{x}^{i} \rangle \stackrel{\widetilde{Rc}}{\longrightarrow} \langle \widetilde{Q}_y^{i+1}, \widetilde{A}_y^{i+1} \rangle$. 
(3) Through analytical reasoning and harnessing inter-clip information, repetitive, similar, and simple QAs should be changed into more in-depth and complex QAs.
% Through analytical reasoning and leveraging inter-clip information, repetitive, overly similar, and simplistic QAs should be transformed into more comprehensive and intricate questions and answers. 
% By conducting an analytical approach to reasoning and harnessing inter-clip information, repetitive, analogous, and simplistic queries are elevated to probe more profound lines of inquiry.
% Through reasoning analysis and harnessing inter-clip information, we transform repetitive, similar and simplistic queries into more in-depth and complex lines of inquiry.
%z
Given the difficulty of simultaneously meeting these stringent criteria, utilizing LLMs to establish these linkages proves to be impractical. Therefore, manual annotation is required, with the specific modification guideline outlined in Appendix \ref{apx:mod}.
%
% Given that these stringent criteria are challenging to adhere to simultaneously, using Language Learning Models (LLMs) to create these links is impractical. This necessitates manual annotation, with specific methods detailed in the Modification Guideline. 
%
\begin{figure}[t]
 %\centering

 \subfigure[Video Distribution]{% Distribution of video categories.
 \begin{minipage}{0.5\linewidth}{
  \label{fig:vis:a} %% label for first subfigure
  \centerline{\includegraphics[width=0.8\linewidth]{Figure/ring1.png}}}
  %(a) t-SNE on original message features.
 % \hspace{1in}
 \end{minipage}
 }
 \hspace{0.5mm}
 %\hfill
 \subfigure[QA Distribution]{% Distribution of question categories.
 \begin{minipage}{0.5\linewidth}{
  \centering
  \label{fig:vis:b} %% label for first subfigure
  \centerline{\includegraphics[width=0.8\linewidth]{Figure/ring2.png}}}
  \end{minipage}}
  % \hfill
  \vspace{-0.3cm}
   \caption{Distributions of videos and QA categories.}
   \vspace{-0.5cm}
 \label{fig:visualization} %% label for entire figure
\end{figure}





\section{Statistical Analysis}

%In this section, we present a comprehensive statistical analysis of our dataset, which encompasses a diverse collection of 1,353 videos. The videos exhibit considerable variability in duration, with an average length of approximately 2.5 minutes. Furthermore, the distribution of video lengths spans a wide range as well, from the shortest video at 1 minute to the longest video approaching 8 minutes, which ensures substantial disparity in content and complexity across the dataset.

%In this section, we present a comprehensive statistical analysis of our dataset, which encompasses a diverse collection of 1,353 videos. Table \ref{tab:fashioniq} demonstrates that our dataset exhibits advantages not present in other datasets. Compared to other datasets, our dataset is the sole one that attains the five criteria with the highest average number of QA pairs per video and an average length of approximately 2.5 minutes. Furthermore, the annotation that we take is conducted through a combination of automated and manual methods, thereby ensuring both the quantity and quality of the QA pairs.
% We present a comprehensive statistical analysis of our dataset, which includes a diverse collection of 1,353 videos from 6 distinct video sources. Table \ref{tab:fashioniq} demonstrates that our dataset exhibits advantages not present in other datasets. Notably, it features an average video length of over 2 minutes, and it draws from a total of 6 distinct video sources. Furthermore, our dataset is unique in that it includes contextual links between QA pairs. 
%
We present a comprehensive statistical analysis of our dataset, which is the first dataset annotated with temporal multi-turn dialogues for streaming videos, to the best of our knowledge. Our dataset comprises 1,353 videos from 6 distinct sources and stands out with an average of 36.94 semi-automatically annotated QA pairs per video, which is the highest among existing datasets. Notably, it also consists of long videos with an average length exceeding 2 minutes. Moreover, each multi-turn dialogue in our dataset contains an average of 4.29 QA pairs and every video contains an average of 8.61 multi-turn dialogues. The details and comparisons are illustrated in Table \ref{tab:fashioniq}.
% the annotation that we take is conducted through a combination of automated and manual methods, thereby ensuring both the quantity and quality of the QA pairs.
% "In this section, we conduct an extensive statistical analysis of our dataset, which comprises a diverse set of 1,353 videos. As shown in Table \ref{tab:fashioniq}, our dataset offers several advantages over existing collections. Additionally, our dataset is unique in that it includes contextual links between the question and answer pairs. The annotations for our dataset were gathered using a hybrid approach that combines automated processes with human verification, ensuring a balance of high volume and high fidelity in our question and answer pairs."
%The videos exhibit considerable variability in duration, with an average length of approximately 2.5 minutes. Furthermore, the distribution of video lengths spans a wide range as well, from the shortest video at 1 minute to the longest video approaching 8 minutes, which ensures substantial disparity in content and complexity across the dataset.

%\subsubsection{Genre and Classification Diversity}
%SVBench demonstrates extraordinary versatility in its classification schema, significantly enhancing its robustness in terms of diversity and representativeness. The dataset is meticulously categorized into twelve distinct genres based on the YouTube-1B classification system. These categories encompass Lifestyle \& Wellness, Science, Arts \& Entertainment, Adventure \& Animals, Home Life \& DIY, Sports, Business \& Careers, News, Education, Hobbies, Automotive \& Gadgets, and Food \& Cooking.

\textbf{Video Categories.}
% Following YT-Temporal-1B, the videos in our dataset fall into 12 distinct categories, as shown in Figure \ref{fig:vis:a}, which reflects the diversity and comprehensiveness of video categories within our dataset.
%
Our dataset contains videos organized into 12 primary categories and 36 subcategories, as depicted in Figure \ref{fig:vis:a}, which illustrates the 
% wide range and inclusiveness of video types present in our collection.
the diversity and inclusiveness of video types within our dataset.

%SVBench demonstrates extraordinary versatility in its video category schema, significantly enhancing its robustness in terms of diversity and representativeness. Our dataset is meticulously categorized into twelve distinct genres based on the YouTube-1B classification system. These categories encompass Lifestyle \& Wellness, Science, Arts \& Entertainment, Adventure \& Animals, Home Life \& DIY, Sports, Business \& Careers, News, Education, Hobbies, Automotive \& Gadgets, and Food \& Cooking.
% \subsubsection{QA Pairs Distribution}
% SVBench comprises an extensive collection of 49613 QA pairs, which forms an essential dataset for evaluating the performance of QA systems. These pairs are uniformly distributed across 12 categorized videos, with each video containing an average of 9 scenes. Within each scene, there is an average of 4 QA pairs, resulting in approximately 37 QA pairs per video. 


%\subsubsection{Question Categories and Distribution}
%In the construction of SVBench, the distribution of question categories has been meticulously balanced to ensure a comprehensive and nuanced evaluation. The categories of questions are meticulously designed to encompass a comprehensive range of cognitive and analytical tasks. These categories include:
\textbf{Question Categories}.
\label{sec:qcat}
To facilitate a more comprehensive evaluation of the capabilities of LVLMs, we classify the questions into 9 distinct categories as shown in Figure \ref{fig:vis:b}. Each category corresponds to the assessment for one specific skill of LVLMs. The criteria for these categories are as follows:
(1) \textit{Intention Inference (II)}: Discerning the underlying intention behind actions of characters.
(2) \textit{Potentiality Assessment (PA)}: Evaluating the feasibility of an action under certain conditions.
(3) \textit{Counterfactual Reasoning (CR)}: Analyzing outcomes by hypothesizing alternative scenarios.
(4) \textit{Spatio-Temporal Speculation (STS)}: Understanding the spatial and temporal relationships within the video.
(5) \textit{Relationship Inference (RI)}: Identifying and interpreting the relationships between entities.
(6) \textit{Character State and Transition (CST)}: Tracking the emotional states and transitions of characters under analysis within specific contexts.
(7) \textit{Comparison and Trend Analysis (CTA)}: Comparing different entities and analyzing emerging trends.
(8) \textit{Common Sense Inference (CSI)}: Applying general world knowledge to provide a logical framework.
(9) \textit{Event-Centric Analysis (ECA)}: Focusing on in-depth examination of significant events.
Details of the above categories are provided in Appendix \ref{apx:qcat}.
% \subsection{Question Length Analysis}
% Further analysis reveals that question lengths vary, with an average of 12 words per question. The distribution of question lengths is relatively normal, with a standard deviation of 5 words, indicating moderate variance in question complexity and verbosity. The shortest questions consist of 5 words, while the longest extend to 30 words, reflecting the diverse nature of inquiries present in the dataset.

% \subsection{Answer Length Analysis}
% Answer lengths exhibit similar variability. On average, answers contain 20 words, demonstrating a broader range of complexity and detail conveyed in responses. The distribution of answer lengths is normally distributed with a standard deviation of 8 words. The concise answers consist of 5 words, typically for straightforward factual questions, whereas the most detailed answers extend up to 50 words, often corresponding to descriptive or inferential inquiries.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{Figure/model_frame.png}
 \vspace{-0.4cm}
	\caption{Architecture of the proposed StreamingChat model.}
	\label{fig:model}
\vspace{-0.4cm}
\end{figure}

% \begin{figure}[t]
%  %\centering

%  \subfigure[Video Distribution]{% Distribution of video categories.
%  \begin{minipage}{0.5\linewidth}{
%   \label{fig:vis:a} %% label for first subfigure
%   \centerline{\includegraphics[width=0.8\linewidth]{Figure/ring1.png}}}
%   %(a) t-SNE on original message features.
%  % \hspace{1in}
%  \end{minipage}
%  }
%  \hspace{0.5mm}
%  %\hfill
%  \subfigure[QA Distribution]{% Distribution of question categories.
%  \begin{minipage}{0.5\linewidth}{
%   \centering
%   \label{fig:vis:b} %% label for first subfigure
%   \centerline{\includegraphics[width=0.8\linewidth]{Figure/ring2.png}}}
%   \end{minipage}}
%   % \hfill
%   \vspace{-0.3cm}
%    \caption{Distributions of videos and QA categories.}
%    \vspace{-0.5cm}
%  \label{fig:visualization} %% label for entire figure
% \end{figure}



\section{StreamingChat}
\textbf{Model Architecture.} Built upon InternVL2 \cite{chen2024far}, we develop a streaming LVLM baseline named StreamingChat. It comprises a vision encoder (InternViT \cite{chen2023internvl}), an MLP projector, and an LLM (InternLM2 \cite{cai2024internlm2}), as illustrated in Figure \ref{fig:model}.
%
For the vision encoder, we employ the InternViT model (pre-trained on a combination of image captioning and OCR-specific datasets)  to extract video frame embeddings at 1 FPS. To enhance the efficiency of learning streaming video understanding capabilities, we fine-tune the model using a static resolution strategy, which allows the model to handle several minutes of video and context within a 32k context window. The extracted frame embeddings are then fed into an MLP projector to generate frame tokens, following the approach used in LLaVA-1.5 \cite{liu2024improved}. These frame tokens are interleaved with language tokens and input into the LLM, InternLM2. Additionally, we incorporate LoRA \cite{hu2021lora} in every linear layer of the LLM to facilitate efficient tuning.

\textbf{Supervised Fine-Tuning Data.} We utilize the training data from our dataset for supervised fine-tuning to enhance the performance of StreamingChat. Each data entry represents a temporal dialogue path, which is converted into conversation data. The data format follows a multi-turn, multi-image structure. Specifically, the video multi-turn dialogue format involves sequentially inputting segments of the video and engaging in several rounds of dialogue for each segment until the entire video has been processed: \textit{\textless video\textgreater Segment 1 \textless /video\textgreater \textless question\_1\textgreater \textless answer\_1\textgreater ... \textless question\_N\textgreater \textless answer\_N\textgreater \textless video\textgreater Segment 2 \textless /video\textgreater ...} 
%
Through this approach, we sequentially input video segments and engage in multiple rounds of dialogue for each segment until the entire video has been processed. Due to the context window limitation, we split temporal dialogue paths exceeding 100 frames into multiple segments for training.


\section{Experiments}
\subsection{Experimental Setup}
To effectively evaluate the performance of current LVLMs in streaming video understanding, we meticulously select a diverse range of state-of-the-art models, encompassing both open-source and closed-source LVLMs. 
%, such as GPT-4V \cite{yang2023dawn}, GPT-4o \cite{achiam2023gpt}, Gemini 1.5 Pro \cite{reid2024gemini}, ShareGPT4Video \cite{chen2024sharegpt4video}, Video-LLaVA \cite{lin2023video}, Video-LLaMA2 \cite{cheng2024videollama}, Video-ChatGPT \cite{maaz2023video}, InternLM-XComposer2.5 \cite{zhang2024internlm}, MiniCPM-V-2.6 \cite{yao2024minicpm}, InternVL2 \cite{chen2024far}, TimeChat \cite{ren2024timechat}, VILA \cite{lin2024vila}, and MovieChat \cite{song2024moviechat}. 
%
We design two distinct experimental setups within the SVBench evaluation set to rigorously assess the capabilities of these LVLMs. 
% These setups are meticulously crafted to probe the models' proficiency in handling the complex and nuanced requirements of dialogue mode and streaming mode.

\textbf{Dialogue Evaluation.} 
% In this setup, each LVLM is provided with contextual information that includes all prior interactions up to the current timestamp of the QA chain within the video.
% %
% For each QA chain, the model receive the sequence of preceding QA pairs and is then prompted to answer each subsequent question in chronological order. 
In this setup, we evaluate the capabilities of LVLMs in understanding long-context scenarios within temporal multi-turn dialogues.
% 
Each LVLM is provided with a contextual history that includes all preceding QA pairs up to the current timestamp in the QA chain.
% 
Once a dialogue sequence concludes chronologically, the model transitions to the next video clip and addresses its associated QA chain. This process continues until the entire video is played and every question has been answered.
% Once a dialogue sequence concludes in chronological order, the model transitions to the next video clip and addresses its respective QA chain until the video is fully played and every question is answered.
% 
This evaluation allows us to assess the ability of LVLMs to maintain continuity over multiple turns and to respond while considering the accumulated context.
% 
The aim is to simulate real-world scenarios where users pose a series of related questions while watching a video, requiring the model to track and integrate information over time.

% In this setup, we evaluate the capabilities of LVLMs in understanding long-context scenarios within temporal multi-turn dialogues. 
% %
% Each LVLM is fed a contextual history, comprising all preceding interactions up to the current timestamp of the QA chain. 
% %
% Once a dialogue sequence concludes in chronological order, the model transitions to the next video clip and addresses its respective QA chain until the video is played and every question is answered.
% %
% This evaluation allows us to assess the ability to maintain continuity over multiple turns and to respond accurately with consideration to the accumulated context.
% %
% It aims to simulate real-world scenarios where users might pose a series of related questions as they watch a video, requiring the model to track and integrate information over time.
%
% This method allows us to gauge the ability to maintain context across multiple turns and accurately respond based on the accumulated information.  
%
% The setup is designed to simulate real-world scenarios where users may ask a series of related questions as they watch a video, requiring the model to track and integrate information over time.
\textbf{Streaming Evaluation.} Building upon the dialogue evaluation, this setup focuses on assessing the ability of LVLMs to perform temporal reasoning by introducing probabilistic transitions between related QA chains.
% 
Similar to the above setup, models are presented with the context up to the current timestamp.
% 
However, when encountering questions that have temporal linkages to subsequent QA chains, there is an 80\% probability that the model will jump to the corresponding related question in the following chain.
% 
This evaluation aims to challenge the understanding of temporal dependencies and its capability to reason about the sequence of events across different but related video segments.

% Building upon the dialogue evaluation, this setup focuses on the ability to perform temporal reasoning by introducing probabilistic transitions between related QA chains.
% %
% Similar to the dialogue evaluation mode setup, the models are presented with the context up to the current timestamp.
% %
% However, when encountering questions with temporal linkages to subsequent QA chains, there is an 80\% chance that the model will jump to the corresponding related question in the following chain.
% %
% This evaluation is intended to challenge the understanding of temporal dependencies and the capability to reason about the sequence of events across different but related video segments.
% This methodology is specifically designed to test the models' grasp of temporal connections and their ability to logically deduce the order of events across different yet related segments of the video.
% \begin{itemize}
%     \item \textbf{Dialogue Mode} In this setup, each LLM is provided with contextual information up to the current timestamp of the QA chain within the video.  For each QA chain, the model received the sequence of preceding QA pairs and is then prompted to answer each subsequent question in chronological order.  This method allowed us to gauge the model's ability to maintain context across multiple turns and accurately respond based on the accumulated information.  The setup is designed to simulate real-world scenarios where users might ask a series of related questions as they watch a video, requiring the model to track and integrate information over time.
%     \item \textbf{Streaming Mode} In the second setup, we focused on the models' ability to perform temporal reasoning by introducing probabilistic jumps between related QA chains.  Similar to the multi-turn setup, the models were provided with the context up to the current timestamp.  However, when encountering questions that had a temporal link to subsequent QA chains, the evaluation included a mechanism to probabilistically transition to the corresponding related question in the next chain.  This approach is intended to challenge the models' understanding of temporal dependencies and their capability to reason about the sequence of events across different but related segments of the video.
% \end{itemize}

% \subsubsection{Data Formatting for Input}
% To format the data for each model, each scene is allocated a maximum number of frames based on the model's capacity. ShareGPT4Video and Video-LLaMA2 were each capped at 16 frames per scene. In contrast, Video-LLaVA is limited to 8 frames, Video-ChatGPT to 100 frames, LLaVA-NeXT to 32 frames, and VILA to 6 frames. Whenever the number of frames extracted from a scene is insufficient, additional frames from preceding scenes were used to meet the model's input requirements. For models lacking built-in memory capabilities, the entire history of previous question-answer pairs is concatenated with the current input to provide full contextual understanding. This approach ensured that all models had consistent and comparable input data for open-domain QA tasks across differing video inputs.

% For Models without Memory Abilities:

% \begin{equation*}
% \begin{aligned}
%     &prompt = ```\\
%     &Frames: [frame_1, frame_2, ..., frame_n]\\
%     &QA History:\\
%     &  - Question 1: <question>  Answer 1: <answer>\\
%     &  - Question 2: <question>  Answer 2: <answer>\\
%     &Current Question: <new question>\\
%     &```
% \end{aligned}
% \end{equation*}

% For Models with Memory Abilities:

% \begin{equation*}
% \begin{aligned}
%     &prompt = ```\\
%     &Frames: [frame_1, frame_2, ..., frame_n]\\
%     &Current Question: <new question>\\
%     &```
% \end{aligned}
% \end{equation*}

\begin{table*}[t]
\renewcommand\arraystretch{0.6}
\footnotesize
\centering
\caption{Evaluation results of various models on SVBench in dialogue and streaming evaluation.} % Performance of Various Models on Different QA Tasks} Evaluation results of various models on SVBench across nine long-context streaming video understanding skills.
\label{tab:evaluation}
\setlength\tabcolsep{3pt}

\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{6}{c}{\textbf{Dialogue Evaluation}}                                                    & \multicolumn{6}{c}{\textbf{Streaming Evaluation}}                                                   \\
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
                                & \textbf{SA}    & \textbf{CC}    & \textbf{LC}    & \textbf{TU}    & \textbf{IC}    & \textbf{OS}    & \textbf{SA}    & \textbf{CC}    & \textbf{LC}    & \textbf{TU}    & \textbf{IC}    & \textbf{OS}    \\ \midrule
\multicolumn{13}{c}{\textbf{Open-source LVLMs}}                                                                                                                                                                                             \\ \midrule
MovieChat                       & 20.46          & 20.05          & 27.76          & 21.81          & 22.21          & 21.89          & 17.99          & 16.42          & 20.37          & 15.77          & 19.08          & 17.43          \\
Video-ChatGPT                   & 31.86          & 32.58          & 40.28          & 35.32          & 36.26          & 33.80          & 27.98          & 29.54          & 33.81          & 27.95          & 31.00          & 28.88          \\
Video-LLaVA                     & 35.62          & 36.52          & 42.93          & 38.63          & 38.84          & 37.34          & 32.22          & 32.83          & 36.35          & 32.46          & 34.54          & 32.79          \\
ShareGPT4Video                  & 39.01          & 40.42          & 47.89          & 41.42          & 43.18          & 40.70          & 34.65          & 36.70          & 41.07          & 35.76          & 37.22          & 35.79          \\
VideoLLaMA2                     & 39.13          & 40.33          & 47.60          & 42.36          & 41.80          & 40.60          & 35.68          & 36.40          & 42.23          & 34.65          & 36.70          & 35.84          \\
TimeChat                        & 36.19          & 37.06          & 44.72          & 40.42          & 37.12          & 37.22          & 35.72          & 37.88          & 42.65          & 36.23          & 36.34          & 36.32          \\
InternVL2                       & 45.91          & 46.30          & 52.67          & 49.81          & 46.25          & 46.13          & 43.55          & 44.10          & 48.91          & 40.95          & 44.17          & 42.71          \\
VILA                            & 46.83          & 48.41          & 54.92          & 48.30          & 50.12          & 48.51          & 46.19          & 47.95          & 51.60          & 44.84          & 48.56          & 46.26          \\
InternLM-XComposer2.5          & 51.57          & 53.93          & 59.69          & 51.57          & \underline{56.28} & 52.31          & 52.22          & 53.39          & 58.14          & 48.05          & \underline{54.79} & 51.46          \\
MiniCPM-V 2.6                   & \underline{53.50} & \underline{55.42} & \underline{60.88} & \underline{55.03} & 55.78          & \underline{54.30} & \underline{53.33} & \underline{54.30} & \underline{58.97} & \underline{49.64} & 54.71          & \underline{52.19} \\
StreamingChat                   & \textbf{59.48} & \textbf{61.31} & \textbf{66.05} & \textbf{58.61} & \textbf{61.09}          & \textbf{59.41} & \textbf{55.10} & \textbf{56.66} & \textbf{60.72} & \textbf{51.78} & \textbf{55.87}          & \textbf{53.90} \\ \midrule
\multicolumn{13}{c}{\textbf{Closed-source LVLMs}}                                                                                                                                                                                           \\ \midrule
Gemini 1.5 Pro                  & 54.89          & 56.05          & 61.45          & 53.08          & 56.06          & 54.29          & 49.06          & 50.05          & 54.62          & 45.73          & 49.84          & 48.02          \\
GPT-4V                          & 65.56          & 68.02          & 71.78          & 63.80          & 68.01          & 65.19          & 58.82          & 59.55          & 64.29          & 54.08          & 60.61          & 57.35          \\
GPT-4o                          & \textbf{65.73} & \textbf{68.10} & \textbf{71.95} & \textbf{66.54} & \textbf{68.40} & \textbf{66.29} & \textbf{59.52} & \textbf{60.42} & \textbf{65.45} & \textbf{55.10} & \textbf{61.36} & \textbf{58.17} \\ \bottomrule
\end{tabular}

\vspace{-3mm}
\end{table*}



\subsection{Evaluation Metrics}

\textbf{Basic Metrics.} 
Evaluating performance in streaming video understanding requires sophisticated metrics to capture various dimensions. Below, we outline the commonly-used evaluation metrics.
% Evaluating the performance of LVLMs in video question-answering tasks necessitates a nuanced set of metrics that capture various dimensions of model efficacy. Below, we describe the key evaluation metrics that are commonly employed:
\begin{itemize}[leftmargin=6mm]
    \item \textbf{METEOR} \cite{banerjee2005meteor} evaluates the precision, recall, and alignment of words and phrases between the references and the ground truth by considering synonymy and stemming.
    % \item \textbf{BLEU-4} Evaluates the fluency and relevance of the generated responses by comparing them with human-annotated references.
    % \item \textbf{ROUGE-L} Measures the overlap of n-grams between the model-generated answers and the ground truth, assessing both precision and recall.
    % \item \textbf{CIDEr} Evaluates the consensus-based similarity of the generated answers to the ground-truth answers.
    \item \textbf{GPT4-Score} assessed by GPT-4, evaluates the accuracy of generated answers solely based on the semantic similarity between a single answer and the ground truth.
    % assesses the accuracy of generated answers solely based on the semantic similarity between a single answer and the ground truth.
\end{itemize}
 
% \subsubsection{Dialogue Mode Evaluation by GPT4}
% The effectiveness of a dialogue mode system hinges on the model's ability to maintain contextual consistency across multiple rounds of interaction. To evaluate such systems comprehensively, it is imperative to consider several key dimensions, each contributing to a holistic assessment of the model’s overall performance.
% \begin{itemize}
%     % \item \textbf{Accuracy} is the cornerstone of any evaluation, determining whether the response correctly and precisely addresses the posed question. An ideal response is not only factually correct but also aligns perfectly with the expected ground truth. Any deviations or errors, however minor, necessitate a corresponding reduction in the accuracy score.
%     \item \textbf{Completeness} assesses the extent to which a response covers all necessary aspects of the question. A complete response should provide exhaustive information, ensuring no critical detail is omitted. Responses that are partial or superficial in their coverage of relevant details should be penalized accordingly.
%     \item \textbf{Relevance} measures how closely the response aligns with the question's intent. The model must demonstrate the ability to discern and focus on pertinent information while excluding irrelevant content. Any deviation from the topic or inclusion of unnecessary details diminishes the relevance score.
%     \item \textbf{Fluency} pertains to the linguistic quality of the response. This includes both grammatical accuracy and the natural flow of language, ensuring that the answer is not only correct but also clearly and effectively communicated. Responses that exhibit a high degree of fluency, free from significant grammatical or logical flaws, are scored more favorably.
%     \item \textbf{Contextual Understanding} evaluates the model’s capability to accurately interpret and respond to the context provided by the video across multiple turns of questioning. A response that demonstrates deep contextual awareness ensures alignment with the video's narrative, thereby enhancing the overall coherence of the interaction.
%     \item \textbf{Logicality} examines the internal consistency and coherence of the response. A logically sound response follows a clear reasoning path, maintaining consistency with previous answers. This is especially crucial in multi-turn interactions, where the continuity of reasoning is essential for maintaining coherence across exchanges.
%     \item \textbf{Temporal Understanding} assesses the model's ability to accurately reflect the sequence and causality of events depicted in the video. Correctly acknowledging the timeline and causal relationships between events is essential, particularly for questions involving processes or sequences of actions.
%     \item \textbf{Overall Score} aggregates these individual dimensions into a single, comprehensive evaluation of the model's performance. This score ranges from poor to excellent, offering a succinct yet comprehensive reflection of the model's proficiency in dialogue mode.
% \end{itemize}
 
\textbf{Dialogue Evaluation Framework.} 
To evaluate the capabilities in both dialogue and streaming evaluations, it is crucial to adopt a multidimensional framework that assesses the quality of multi-turn dialogues.
%, rather than just using semantic accuracy for single QAs. 
We propose an LLM-based evaluation framework encompassing several key aspects that contribute to the holistic assessment of LVLMs. See Appendix \ref{apx:prom} for detailed prompts.
% To effectively assess the quality of multi-turn dialogue systems, rather than just using simple semantic accuracy for single question-answers (QAs), it's essential to establish a sophisticated evaluation framework. This framework should consider multiple dimensions that together offer a comprehensive evaluation of language models, particularly focusing on their performance in both conversational dialogues and streaming content.
% We should use a multi-faceted evaluation framework to assess abilities in both dialogue and streaming evaluation modes for engaging with LVLMs (Language and Vision in Lifelike Models) in multi-turn dialogues, extending our focus past basic semantic accuracy, which is often used in single-question assessments. Our suggested evaluation framework includes various important factors aimed at delivering a comprehensive evaluation of the performance of LVLMs.
% In order to conduct a comprehensive evaluation of capabilities in both dialogue and streaming assessment modes, it is imperative to employ a multidimensional evaluation framework that goes beyond simple semantic accuracy. We propose an in-depth evaluation framework comprising multiple essential aspects that together provide a complete appraisal of the Long-Video Language Models (LVLMs).
\begin{itemize}[leftmargin=6mm]
    \item \textbf{Semantic Accuracy (SA)} evaluates the accuracy of the generated answers based on a holistic understanding. It considers not only the direct overlap with ground-truth answers but also the context, coherence, and overall relevance of the response to the question posed.
    \item \textbf{Contextual Coherence (CC)} examines the ability to maintain relevance and context across sequential questions and answers, ensuring continuity and alignment with the evolving discourse.
    \item \textbf{Logical Consistency (LC)} evaluates the logical progression and consistency of answers, ensuring that answers do not contradict each other or previous information.
    \item \textbf{Temporal Understanding (TU)} assesses the model's proficiency in comprehending and reasoning about temporal events and sequences depicted in the video content. 
    \item \textbf{Informational Completeness (IC)} measures the comprehensiveness to gauge whether the model captures and conveys all relevant elements from the video to provide a thorough answer.
    \item \textbf{Overall Score (OS)} is derived by aggregating the scores from each aforementioned criterion.
\end{itemize}

% \begin{table*}[t]
% \renewcommand\arraystretch{0.8}
% \small
% \centering
% \caption{Performance of Various Models on Different QA Tasks}
% \label{tab:performance}
% \begin{adjustbox}{width=\textwidth}
% \begin{tabular}{l@{\extracolsep{\fill}}*{16}{c}@{}}
% \toprule
% \textbf{Model} & \makebox[1.8cm]{\textbf{SingleQA}} & \multicolumn{9}{c}{\textbf{chainQA}} & \multicolumn{6}{c}{\textbf{ConQA}} \\
% \cmidrule(lr){2-2} \cmidrule(lr){3-11} \cmidrule(lr){12-17}
%  & \textbf{Acc} & \textbf{Acc} & \textbf{Com} & \textbf{Rel} & \textbf{Flu} & \textbf{Con} & \textbf{Und} & \textbf{Log} & \textbf{TU} & \textbf{OS} & \textbf{SA} & \textbf{CC} & \textbf{LC} & \textbf{TR} & \textbf{IC} & \textbf{OE} \\
% \midrule
% gpt4o                          & 60.70 & 65.73 & 68.40 & 80.66 & 95.41 & 68.10 & 71.95 & 66.54 & 71.95 & 70.02 & 75.52 & 77.00 & 73.46 & 68.18 & 70.08 \\
% gpt4v                          & 60.11 & 65.56 & 68.00 & 80.65 & 95.58 & 68.02 & 71.78 & 63.80 & 70.78 & 69.20 & 74.44 & 75.64 & 72.10 & 67.34 & 69.10 \\
% interlm-xcomposer-2.5          & 49.69 & 51.57 & 56.27 & 66.77 & 90.27 & 53.93 & 59.69 & 51.57 & 59.69 & 61.44 & 66.74 & 68.40 & 64.06 & 60.82 & 62.00 \\
% minicpm-v-2.6                  & 48.57 & 53.30 & 55.78 & 70.37 & 91.88 & 55.42 & 60.88 & 55.03 & 60.88 & 62.74 & 67.88 & 69.38 & 66.18 & 60.86 & 62.88 \\
% vila                           & 45.41 & 49.18 & 51.04 & 64.16 & 89.75 & 48.41 & 54.91 & 48.30 & 54.91 & 54.34 & 59.94 & 60.70 & 59.78 & 53.96 & 55.74 \\
% internvl2                      & 42.25 & 45.91 & 46.34 & 61.64 & 88.67 & 46.30 & 52.66 & 49.81 & 52.66 & 51.24 & 55.12 & 57.54 & 54.60 & 49.08 & 51.46 \\
% video\_llava                   & 40.03 & 35.62 & 41.35 & 50.73 & 78.85 & 36.52 & 42.93 & 38.63 & 42.93 & 37.90 & 41.04 & 42.76 & 43.28 & 38.38 & 39.50 \\
% sharegpt4video                 & 40.00 & 45.47 & 43.12 & 56.27 & 86.67 & 40.42 & 41.42 & 41.42 & 40.76 & 45.88 & 47.89 & 48.32 & 47.68 & 41.36 & 43.12 \\
% video\_llama2                  & 38.88 & 39.41 & 41.80 & 56.09 & 86.09 & 40.33 & 47.60 & 42.36 & 41.98 & 45.50 & 49.68 & 46.20 & 40.78 & 43.18 \\
% video\_chatgpt                 & 34.62 & 38.16 & 36.26 & 46.57 & 80.78 & 32.58 & 40.28 & 35.32 & 32.92 & 36.92 & 39.78 & 37.26 & 34.44 & 34.80 \\
% timechat                       & 34.42 & 36.19 & 37.12 & 55.13 & 87.31 & 37.06 & 44.72 & 40.42 & 42.02 & 47.35 & 50.18 & 48.30 & 40.38 & 43.76 \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \vspace{-1mm}
% \end{table*}


% Ablation studies of various models on the SVBench streaming video Open QA benchmark reveal insightful trends and performance disparities across both open-source and closed-source large video-language models (LVLMs). As indicated in Table \ref{tab:performance}, models were evaluated using two metrics: METEOR, which measures the fluency and recall in responses, and SA (Semantic Alignment), capturing the accuracy of the semantic content.

% For models with no former question-answer pairs (Sin.), the highest METEOR score among open-source models is achieved by VILA (28.44), closely followed by GPT-4V (28.66) among closed-source models. When former QAs are provided (Mul.), substantial improvements are observed across all models, with GPT-4o (36.84) and GPT-4V (36.47) leading the scores. A similar trend is evident for the SA metric, where improvements range from 5\% to over 20\%, with GPT-4o demonstrating the highest SA in the presence of former QAs (60.70).

%\subsection{Results and Analysis}

\begin{table*}[t]

\renewcommand\arraystretch{0.6}
\footnotesize
\centering
\caption{Evaluation results on SVBench across 9 long-context streaming video understanding skills.}
\label{tab:performance}
\setlength\tabcolsep{3pt}
\begin{tabular}{@{}cccccccccc@{}}
\toprule
\textbf{Model}         & \textbf{II}    & \textbf{PA}    & \textbf{RI}    & \textbf{CR}    & \textbf{CST}   & \textbf{CSI}   & \textbf{CTA}   & \textbf{ECA}   & \textbf{STS}   \\ \midrule
\multicolumn{10}{c}{\textbf{Open-source LVLMs}}                                                                                                                                 \\ \midrule
MovieChat \cite{song2024moviechat}             & 21.91          & 21.32          & 18.82          & 17.78          & 24.58          & 19.23          & 16.77          & 16.59          & 15.41          \\
Video-ChatGPT  \cite{maaz2023video}         & 35.85          & 40.12          & 31.18          & 29.37          & 39.38          & 29.95          & 25.77          & 26.97          & 22.79          \\
TimeChat \cite{ren2024timechat}              & 33.26          & 43.62          & 29.86          & 30.58          & 39.97          & 28.49          & 27.08          & 28.10          & 24.22          \\
VideoLLaMA2 \cite{cheng2024videollama}           & 36.76          & 47.38          & 30.56          & 33.08          & 39.85          & 37.09          & 29.67          & 31.75          & 26.03          \\
ShareGPT4Video \cite{chen2024sharegpt4video}        & 39.92          & 47.54          & 33.61          & 33.56          & 43.69          & 35.97          & 28.98          & 31.25          & 26.43          \\
Video-LLaVA  \cite{lin2023video}           & 38.99          & 46.42          & 35.86          & 34.27          & 46.09          & 35.65          & 31.21          & 31.20          & 26.67          \\
InternVL2 \cite{chen2024far}             & 37.10          & 51.19          & 38.25          & 36.79          & 36.02          & 41.66          & 32.22          & 33.24          & 29.07          \\
VILA \cite{lin2024vila}                  & 42.13          & 55.48          & \underline{40.60} & 39.48          & 45.93          & 43.29          & 35.87          & 34.22          & 31.21          \\
InternLM-XComposer2.5 \cite{zhang2024internlm} & \underline{47.64} & \underline{57.71} & 40.49          & \underline{41.72} & 50.00          & 47.20          & 36.99          & \underline{39.53} & \underline{33.90} \\
MiniCPM-V 2.6 \cite{yao2024minicpm}         & 43.25          & 50.45          & 39.93          & 39.96          & \underline{51.61} & \underline{47.40} & \underline{37.31} & 38.67          & 33.43          \\
StreamingChat (Ours) & \textbf{53.94} & \textbf{71.96} & \textbf{50.22} & \textbf{50.49} & \textbf{59.26} & \textbf{53.46} & \textbf{44.60} & \textbf{47.68} & \textbf{37.99}          \\ \midrule
\multicolumn{10}{c}{\textbf{Closed-source LVLMs}}                                                                                                                               \\ \midrule
Gemini 1.5 Pro \cite{reid2024gemini}        & 41.29          & 43.32          & 42.24          & 39.10          & 47.20          & 50.80          & 39.19          & 34.78          & 35.98          \\
GPT-4V \cite{yang2023dawn}                & 56.84          & \textbf{61.57} & 49.10          & \textbf{51.31} & \textbf{58.57} & 55.76          & 47.47          & 47.44          & 42.97          \\
GPT-4o  \cite{achiam2023gpt}                & \textbf{57.95} & 59.47          & \textbf{52.29} & 49.97          & 56.63          & \textbf{58.65} & \textbf{49.02} & \textbf{47.47} & \textbf{44.58} \\ \bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table*}



\subsection{Overall Performance}
The dialogue and streaming evaluation results on SVBench, outlined in Table \ref{tab:evaluation}, provide a comprehensive comparison among various LVLMs. Notably, closed-source models such as GPT-4o and GPT-4V attain significantly higher scores across all metrics, with GPT-4o achieving an Overall Score (OS) of 66.29 in dialogue evaluation and 58.17 in streaming evaluation. 
% This superior performance underscores the advanced capabilities and fine-tuning potential typical of proprietary models. 
% Among the open-source models, MiniCPM-V 2.6 and InternLM-XComposer2.5 emerge as the top performers, with OS of 54.30 and 52.31 respectively in dialogue evaluation, and 52.19 and 51.46 in streaming evaluation.
% %
Among the open-source models, StreamingChat and MiniCPM-V 2.6 stand out as top performers, achieving OS scores of 59.41 and 54.30 respectively in dialogue evaluation, and 53.90 and 52.19 in streaming evaluation. Notably, StreamingChat demonstrates a significant improvement over the original InternVL2, with a 28.79\% increase in dialogue evaluation and a 26.20\% increase in streaming evaluation. This underscores the effectiveness of SVBench training data for streaming video understanding tasks.

Additionally, we conduct performance comparisons before and after fine-tuning on 6 image and video understanding benchmarks, as illustrated in Figure \ref{fig:benchmark}. The results indicate that the fine-tuned StreamingChat shows substantial improvements over the original InternVL2 on SVBench. While there are slight decreases in performance on the image benchmark MMBench \cite{liu2023mmbench} and the video benchmark MMBench-Video \cite{fang2024mmbench}, there are modest gains on the video benchmarks VideoMME \cite{fu2024video} and MVBench \cite{li2024mvbench}. These findings suggest that StreamingChat enhances streaming video understanding capabilities without compromising fundamental image and video comprehension skills.
% A noteworthy observation is the performance disparity between the dialogue and streaming modes across models. 
% Obviously, streaming evaluation scores are consistently lower than their dialogue evaluation scores.

It is evident that scores in streaming evaluation are consistently lower compared to those in dialogue evaluation. This discrepancy can be attributed to the inherent complexity of streaming evaluation, which demands seamless comprehension and processing of dynamically evolving video content. Unlike dialogue evaluation, which deals with relatively static and contextually stable inputs, streaming evaluation necessitates understanding and integration of extended temporal contexts and dynamic scenes, posing significant challenges for current models.

% Further scrutiny reveals specific shortcomings, particularly in the domains of Temporal Understanding (TU) and Informational Completeness (IC). These domains are critical in streaming evaluation as they involve intricate temporal reasoning and the extraction of substantial amounts of relevant information from video streams. For instance, while InternVL2 and VILA show competent performance, their scores in TU and IC indicate room for improvement in handling temporally complex scenes and comprehensively capturing the essence of streaming content.

% Overall, despite advancements, the gap in performance between the two evaluation modes highlights the need for continued research and development to enhance the streaming capabilities of LVLMs. Addressing these challenges comprehensively would entail innovations in temporal reasoning, context retention, and the ability to synthesize dispersed pieces of information into coherent responses. Such advancements are vital for the evolution of robust, real-time multimedia interpretative models.

% The evaluation results on SVBench, encompassing both dialogue and streaming modes across a variety of models, present a comprehensive view of model performance in Open QA tasks. The open-source LVLMs exhibit a layered performance hierarchy, with models such as MiniCPM-V 2.6 and InternLM-XComposer2.5 demonstrating leading results within this category. In the dialogue evaluation mode, MiniCPM-V 2.6 achieves the highest scores, showcasing superior abilities with scores of 53.50 in SA, 55.42 in CC, and 60.88 in LC. However, these models still trail behind closed-source LVLMs, where GPT-4V and GPT-4o establish the benchmark with exceptionally high scores across all metrics, such as 65.73 in SA and 68.40 in IC for the latter.

% Conversely, the streaming evaluation reveals the intrinsic challenge of maintaining consistent high performance across dynamic and continuous data flows. The scores generally decline when transitioning from dialogue to streaming evaluation, underscoring the complexity of real-time processing and understanding. For instance, although MiniCPM-V 2.6 leads the open-source models in streaming mode, its scores (53.33 in SA, 54.30 in CC) still fall considerably short compared to GPT-4o’s 59.52 in SA and 60.42 in CC. The inherent difficulty in streaming evaluation lies in real-time context management and adaptability, where models must interpret and respond to continually evolving inputs without sacrificing accuracy — a gap that remains evident from the results.

% The presented results in Table \ref{tab:performance} provide a comprehensive evaluation of various models on the SVBench benchmark, focusing on both dialogue and streaming modes. Among open-source LVLMs, MiniCPM-V 2.6 demonstrates superior performance, achieving the highest scores across most metrics in both evaluation modes. Its dominance is exemplified by scores of 53.50 and 55.42 in dialogue metrics SA and CC respectively, and consistent high-performance scores like 58.97 in streaming LC and 54.71 in IC, showcasing its robustness across diverse QA tasks. Comparatively, closed-source LVLMs such as GPT-4V and GPT-4o set a higher benchmark, with GPT-4o leading marginally in almost all categories, underscoring the potential advantages of proprietary datasets and architectures. In particular, GPT-4o excels with a noteworthy 71.95 in dialogue LC and 65.45 in streaming LC, highlighting its nuanced understanding and interaction capabilities with streaming video data. Despite the outperformance of closed-source models, open-source LVLMs like InternLM-XComposer2.5 and VILA exhibit commendable performances, indicating significant progress and potential in the open-source research trajectory. These results collectively reflect the advancements and persistent gaps in the development of LVLMs tailored for the complex task of streaming video open QA, indicating areas for further research and optimization.

% \subsubsection{Category-wise Performance Analysis}
% Performance is further analyzed across different video categories, such as sports, movies, instructional videos, and news segments. It is observed that:
%  Sports Videos: ShareGPT4Video and Video-LLaVA consistently outperformed others, likely due to their robust temporal reasoning and ability to handle fast-paced event sequences.
%  Movies: Video-LLaMA2 excelled in narrative comprehension, leveraging its frame extraction strategy to capture intricate plot details.
%  Instructional Videos: LLaVA-NeXT and VILA showcased strong performance, demonstrating high precision in step-by-step instructional scenarios.
%  News Segments: Video-ChatGPT’s extensive frame extraction proved beneficial, offering high accuracy in summarizing and contextualizing news events.

% \subsubsection{Length-wise Performance Analysis}
% The performance metrics were also correlated with video length. For shorter videos, models like VILA and LLaVA-NeXT, with fewer frame extractions, performed efficiently without significant performance degradation. However, as video length increased:
% \begin{itemize}
%     \item \textbf{Short Videos (1-2 minutes)}: Video-LLaVA and ShareGPT4Video maintained high performance across most metrics.
%     \item \textbf{Medium-Length Videos (2-6 minutes)}: Models with more granular frame extraction, such as Video-ChatGPT, showed a performance dip, suggesting a trade-off between frame count and model responsiveness.
%     \item \textbf{Long Videos (6-8 minutes)}: Models with optimized frame extraction strategies, like Video-LLaMA2, demonstrated a balanced performance, effectively capturing extended narrative contexts without overwhelming latency.
% \end{itemize}

\subsection{Performance Analysis of Video Understanding Skills}
The analysis of model performance across 9 long-context streaming video understanding skills (see Section \ref{sec:qcat}) in SVBench reveals significant disparities, underscoring the inherent challenges posed by different types of skills. According to Table \ref{tab:performance}, skills such as Intention Inference (II) and Potentiality Assessment (PA) generally exhibit higher performance across most models, particularly in closed-source LVLMs such as GPT-4V and GPT-4o. This suggests that both open-source and closed-source models are relatively adept at deciphering character intentions and predicting potential future actions.
% , likely due to the more explicit nature of cues in the videos and the models' capability in handling temporally linear predictions.
Conversely, more complex skills like Counterfactual Reasoning (CR) and Spatio-Temporal Speculation (STS) manifest comparatively lower accuracy. 
% For example, open-source models such as InternLM-XComposer2.5, despite being competitive in other categories, obtain only 41.72 in CR and 33.90 in STS. 
This trend is echoed in closed-source models, where even the high-performing GPT-4V and GPT-4o models show relative declines in performance, though still maintaining superior scores compared to other models. This drop can be attributed to the cognitive demands of these tasks, which involve abstract reasoning, intricate scenario construction, and temporal-spatial awareness—areas that current models struggle to emulate effectively. RI and ECA also pose significant challenges (see Appendix \ref{apx:prom}). Both involve multi-entity and context-sensitive interactions, leading to performance variability. 
%
Notably, StreamingChat outperforms other open-source LVLMs across all 9 skills, and even surpasses closed-source models like GPT in PA, CST, and ECA.
%
Nearly all models scored below 60 on 9 long-context streaming video understanding skills, suggesting that SVBench poses a challenging task.
% Despite these challenges, categories requiring general world knowledge, such as Common Sense Inference (CSI), fare better, reflecting the models' training on diverse, real-world datasets. It's evident that while existing video-language models exhibit considerable capability, their performance is mitigated by the intricacy of the cognitive demands of certain question categories, highlighting critical areas for future enhancement in model architecture and training paradigms.

% \subsubsection{Qualitative Analysis}
% A qualitative examination of the model outputs revealed several critical insights. ShareGPT4Video’s outputs were consistently fluent and contextually relevant, a reflection of its balanced frame extraction and model architecture. Video-LLaVA’s responses, while accurate, occasionally exhibited verbosity.
% Video-LLaMA2’s descriptive outputs were rich in detail but sometimes lacked conciseness. LLaVA-NeXT’s structured answers were effective in procedural contexts, but its performance varied with video length and complexity. Video-ChatGPT’s extensive frame extraction allowed for nuanced inferential reasoning, albeit at the cost of occasional latency issues. VILA, while performant in short videos, faced challenges in maintaining coherence in extended dialogue interactions.


\begin{figure}[t]
\centering
\begin{minipage}{0.49\textwidth}
%\begin{table}[t]
%\renewcommand\arraystretch{0.6}
\small
\centering
\renewcommand\arraystretch{0.6}
\setlength\tabcolsep{2pt}
\captionof{table}{Ablation study on single-instance (Sin.) and multi-turn (Mul.) QA evaluation.}
\label{tab:ablation}

\begin{tabular}{@{}ccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{METEOR}} & \multicolumn{2}{c}{\textbf{GPT4-Score}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                                & \textbf{Sin.}    & \textbf{Mul.}    & \textbf{Sin.}  & \textbf{Mul.}  \\ \midrule
\multicolumn{5}{c}{\textbf{Open-source LVLMs}}                                                          \\ \midrule
MovieChat                       & 19.20            & 23.77            & 17.18          & 21.81          \\
Video-ChatGPT                   & 25.93            & 30.44            & 26.83          & 34.62          \\
Video-LLaVA                     & 26.29            & 31.51            & 30.65          & 40.03          \\
ShareGPT4Video                  & 26.31            & 31.38            & 30.41          & 39.68          \\
VideoLLaMA2                     & 24.52            & 30.87            & 29.67          & 38.88          \\
TimeChat                        & 25.50            & 27.47            & 26.64          & 34.42          \\
InternVL2                       & 26.85            & 32.74            & 32.26          & 42.25          \\
VILA                            & \underline{28.44}            & 33.57            & 34.63          & 45.41          \\
InternLM-XComposer2.5          & 23.32            & 31.02            & \underline{37.11}          & \underline{49.69}          \\
MiniCPM-V 2.6                   & 27.36            & \underline{33.90}            & 36.73          & 48.57          \\
StreamingChat                   & \textbf{32.94}            & \textbf{35.58}            & \textbf{43.85}          & \textbf{57.94}          \\ \midrule
\multicolumn{5}{c}{\textbf{Closed-source LVLMs}}                                                        \\ \midrule
Gemini 1.5 Pro                  & 26.63            & 32.91            & 36.85          & 48.83          \\
GPT-4V                          & 28.66            & 36.47            & 44.94          & 60.11          \\
GPT-4o                          & \textbf{28.81}            & \textbf{36.84}            & \textbf{45.37}          & \textbf{60.70}          \\ \bottomrule
\end{tabular}

%\vspace{-0.3cm}
%\end{table}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Figure/model_performance_radar_chart_fresh.png}
 \vspace{-0.4cm}
	\caption{Performance comparisons on 6 image and video understanding benchmarks: Before fine-tuning (InternVL2) and after fine-tuning (StreamingChat).}
	\label{fig:benchmark}
\end{minipage}
\vspace{-0.5cm}
\end{figure}

\subsection{Ablation Study}
% In order to evaluate the performance and robustness of various models across different settings on the SVBench benchmark, 
Ablation studies are conducted to assess the effectiveness of dialogue evaluation (multi-turn QA denoted as ``Mul.") versus traditional evaluation (single-instance QA denoted as ``Sin.") in our dataset. As shown in Table~\ref{tab:ablation}, models generally exhibit improved metrics on both METEOR and GPT4-Score when additional contextual information from previous QAs is incorporated. 
%For instance, Video-LLaVA achieved a METEOR score increase from 26.29 to 31.51 and an SA score uplift from 30.65 to 40.03 under the Mul. condition as compared to the Sin. condition. 
This trend is consistent across both open-source and closed-source LVLMs.
% 
These results indicate that dialogue evaluation can significantly enhance the performance of models when applied to streaming video understanding.  Unlike the traditional single-instance QA evaluation, the dialogue evaluation leverages the rich contextual information accumulated from previous interactions, thereby providing a more comprehensive evaluation framework. 
% This improvement is pivotal for advancing state-of-the-art LVLMs and facilitating more coherent and contextually aware responses in streaming video understanding.
%, with GPT-4o, a closed-source model, improving from 28.81 to 36.84 in METEOR and from 45.37 to 60.70 in SA.
% The effectiveness of dialogue evaluation suggests that streaming video understanding intrinsically benefits from a narrative-driven approach, where context accumulates and informs subsequent interactions. This mode of testing aligns more closely with real-world applications where dialogue continuity is fundamental, thereby enhancing response relevance and coherence. 
% The findings advocate for a paradigm shift towards dialogue mode evaluation in future research and deployments, ensuring that the intrinsic value of context and continuity is harnessed to its fullest potential, thereby pushing the boundaries of current model capabilities in Intelligent Video Analysis systems.
% These findings highlight several critical insights. Firstly, the substantial enhancement in model performance under the streaming mode (Mul.) underscores the importance of sequential contextual understanding, which is a significant advantage inherent to streaming video Open QA tasks. Secondly, the notable gains in both open-source and closed-source models suggest a probable universal applicability of this advantage, emphasizing the potential for broad improvements in Intelligent Video Analysis systems.
Despite the overall observed improvements in performance enabled by dialogue evaluation, there are notable instances where models exhibit relatively low scores even in this enhanced setting.  
%
These instances provide valuable insights into the limitations and areas for improvement in existing frameworks for streaming video understanding.
% However, the ablation studies also elucidate certain shortcomings of existing models in traditional mode. The substantial performance drop in the absence of prior context points to a dependency on continuity and history for accurate comprehension, indicating areas where models need to be fortified for better isolated understanding. Moreover, the disparity between open-source and closed-source models' performance reveals a gap, particularly seen in the elevated metrics of closed-source models like GPT-4V, which may imply that proprietary enhancements and training data could be contributing to their superior results. These observations not only pave the way for directed future research focusing on augmenting isolated QA capabilities but also reinforce the pivotal role of context in enhancing model efficacy.

% Ablation studies are conducted to assess the impact of incorporating prior question-answer pairs (denoted as "Mul.") versus not incorporating them (denoted as "Sin.") on the performance of various open-source and closed-source LVLMs. As shown in Table \ref{tab:ablation}, across all models, the integration of former QAs consistently led to a significant enhancement in both METEOR and SA metrics.  This trend is observable in both open-source models such as MovieChat, Video-ChatGPT, and VILA, and in closed-source models such as Gemini 1.5 Pro and GPT-4V.  For instance, VILA exhibited an improvement from 28.44 to 33.57 in METEOR and from 34.63 to 45.41 in SA upon the provision of prior QAs.  Similar performance boosts were noted in closed-source models;  GPT-4V, for instance, demonstrated an increase in METEOR from 28.66 to 36.47 and in SA from 44.94 to 60.11.  These findings underscore the pivotal role of context continuity, simulated by the presence of previous QAs, in enhancing model comprehension and response quality in complex, dynamic streaming video environments.

\section{Conclusion}
This paper presents \textbf{SVBench}, a novel benchmark for the assessment of long-context streaming video understanding. SVBench comprises a diverse collection of 1,353 streaming videos from 6 streaming platforms and 49,979 meticulously annotated question-answer pairs for temporal multi-turn dialogues.
%, presenting a robust evaluation framework for assessing LVLMs on real-world streaming video understanding tasks. 
Our experiments reveal that while state-of-the-art LVLMs have made strides in single-instance video QA, their performance on streaming videos falls short of human-level accuracy. Motivated by this, we develop a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. By providing a challenging benchmark, we hope to stimulate the development of advanced models capable of tackling the complexities of streaming video understanding.

% \textbf{Limitations.} While SVBench essentially addresses the key aspects of long-context streaming video understanding, there is room for improvement to achieve greater comprehensiveness. For instance, it could accommodate tasks involving super-long videos (hour-long), or more specific tasks such as live commentary and assisted driving. Our work is an ongoing endeavor, where new tasks and video types will continually be integrated into the benchmark.



\section{Acknowledgment}
We would like to present our appreciation to the anonymous reviewers and ACs  for their constructive suggestions. 
This work is supported by the National Key Research and Development Program of China (No.2023YFC3310700), the Beijing Natural Science Foundation (JQ23018), and the National Natural Science Foundation of China (No.62036012, 62276257).

% In this comprehensive evaluation of state-of-the-art models for open-domain video question answering, our benchmark, \textbf{SVBench}, underscores the nuanced interplay between frame extraction strategies and multi-turn dialogue capabilities. ShareGPT4Video emerged as a consistent performer across various metrics, particularly excelling in maintaining conversational coherence and generating contextually relevant responses. Video-LLaVA demonstrated notable efficiency in CIDEr scores, reflecting its adeptness in aligning generated answers with human-annotated references. However, the variability in performance across video categories and question types revealed the strengths and limitations of each model. For instance, Video-ChatGPT's extensive frame extraction enabled superior inferential reasoning but occasionally hampered responsiveness. Conversely, models like VILA and LLaVA-NeXT, with their optimized frame extraction, balanced accuracy and efficiency, especially in shorter video contexts. Our findings illuminate the critical importance of tailoring frame extraction and memory strategies to the specific demands of video QA tasks, paving the way for future advancements in this dynamic research area.


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.
\clearpage
\renewcommand{\contentsname}{Appendix}
\tableofcontents
\appendix
%

%
%
%\begin{appendices}
%\renewcommand{\thesection}{\Alph{section}}
%\mtcaddchapter 

%
\addtocontents{toc}{\protect\setcounter{tocdepth}{3}}


\section{Details of Data Collection}
\label{apd:data}
\subsection{Data Filtering}

Initially, we source raw video data from a variety of publicly available datasets, including YT-Temporal-1B, YouCook2, ActivityNet, MovieChat, Panda-70M, and Ego4D. 
%
These datasets offer a wide spectrum of video content, ensuring a rich diversity that is essential for streaming video understanding. 
%
In our initial filtering stage, we enforce a minimum duration threshold, excluding videos shorter than 1 minute to ensure sufficient temporal information depth. 
%
Subsequently, we employ aesthetic assessments to retain videos with an aesthetic score of 4 or above, thereby ensuring video clarity and visual quality. Furthermore, we filter videos with optical flow scores within the range of 0.5 to 100, capturing both visual complexity and motion coherence.
%

\subsection{Aesthetic Assessment}

To further ensure video clarity and visual quality, we apply Open-Sora to assign a score for the aesthetic appeal of videos. Videos with an aesthetic score of 4 or above are retained. This criterion ensures that videos within our dataset are not only content-rich videos but also with high aesthetic appeal.

\subsection{Optical Performance Assessment}

Furthermore, we apply Open-Sora for assigning optical flow scores to each video within our dataset as well, which reflects both the visual complexity and the motion coherence of a video. We filter videos with optical flow scores within the range of 0.5 to 100 to ensure the quality of videos within our dataset.

% To ensure the efficacy and relevance of the data, an initial filtering step is conducted based on video duration. 
%
% Specifically, videos with a duration of less than one minute or greater than eight minutes were excluded. This duration-based filtering ensures that the remaining videos possess sufficient informational richness for subsequent analyses.
% Aesthetic Evaluation aesthetic quality
% Optical Performance Assessment Optical Flow Score
% To maintain data quality and relevance, we implemented an initial screening based on video length, excluding any videos shorter than one minute. This criterion is established to guarantee that the selected videos provide ample informational depth for our subsequent analysis.

\subsection{Scene Detection and Video Splitting}

After the filtering and the assessment above, we employ advanced scene detection algorithms to identify and enumerate scenes within each video. These results are crucial in determining whether the video content exhibits adequate variation and complexity. Further filtering is conducted to retain only those videos containing 5 to 15 scenes, thereby excluding content that is either excessively monotonous or overly intricate. Moreover, we calculate the average duration of scenes for each video. Only videos with an average scene duration between 5 and 30 seconds are chosen, ensuring fluidity and rhythm that are essential for effective streaming data analysis.
%
Ultimately, we split each video into clips based on timestamps, merging any clips shorter than 2 seconds with their adjacent ones. Notably, to prevent disjointedness and ensure continuity between scenes, we add an extra 0.5 seconds to both the beginning and end of each clip, resulting in a one-second overlap between consecutive clips.

\begin{wrapfigure}[8]{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.48\textwidth]{Figure/length_distribution_plot.png}
    \caption{Distribution of video lengths.}
    \label{fig:distribution}
\end{wrapfigure}

\subsection{Distribution of Video Lengths}
Figure~\ref{fig:distribution} shows the distribution of video lengths. The results indicate that 95.05\% of the videos in the dataset are longer than 1 minute, primarily ranging from 60 to 240 seconds.

\section{Question Categories}
\label{apx:qcat}

All the questions within our dataset fall into nine categories as follows, with each category corresponding to the assessment for one specific skill of LVLMs.
\subsection{Intention Inference} %When two questions (Q1 and Q2) present similar actions, Q2 can be modified to inquire about the purpose or intent of the action.
%
%This adaptation prompts an understanding of the underlying motivation behind an action, rather than simply observing the action itself.

A category aimed at delving into the underlying intent behind an action. It assesses whether LVLMs truly comprehend the latent motivation behind an action, rather than simply observing the action itself.

\subsection{Potentiality Assessment} %Questions about the feasibility of an action under certain conditions, or its potential outcomes if the context is changed, evaluate the likelihood of an action's success and the conditions required for its realization.

A category that involves inferring the feasibility of an action under certain conditions or its potential outcomes if the context is changed. It evaluates whether LVLMs understand the nature of an action and the conditions required for realizing an action.

\subsection{Counterfactual Reasoning} %Questions that explore alternate possibilities if different actions were taken or if actions occurred under different circumstances. 
%
%Counterfactuals are crucial for understanding causal relationships and the impact of specific variables on outcomes.

A category that entails hypothetical reasoning within a scenario opposite to the existing facts. It assesses the capacity of LVLMs for causal understanding and recognizing the significance of variables.

\subsection{Spatio-Temporal Speculation} %This involves placing actions or events in a particular context of time and space and examining their implications, differences, or changes. The approach is to analyze how time and space variables are influencing the unfolding of actions/events.

A category concerned with placing events or actions in particular temporal and spatial contexts to infer their implications, differences, or changes. It assesses the capacity of LVLMs to comprehend the influence of temporal and spatial factors on entities.

\subsection{Relationship Inference} %This relates to determining the links or associations between different entities (people or objects). The focus is on understanding the nature of relationships on the basis of observed interactions.

A category focusing on exploring the links or associations between entities (such as persons or objects). It evaluates the capacity of LVLMs to understand the nature of relationships on the basis of observed interactions.

\subsection{Character State and Transition} %This involves dissecting the emotional states and transitions of characters under analysis within specific contexts or time-frames. It further extends to examining the potential impacts of such states on character actions or events.

A category that involves dissecting emotional states and transitions of characters under analysis within specific contexts. It gauges the capacity of LVLMs to accurately perceive emotional states and the potential impact of contextual changes.

\subsection{Comparison and Trend Analysis} %This involves juxtaposing different categories or entities (objects, actions, or events) with the intent to identify comparative differences, similarities, or changes in trends. Further complexity can be added by probing into the temporal spacing between set events or actions.

A category dedicated to the comparison and trends across different entities (objects, actions, or events). It assesses the capacity of LVLMs to accurately discern similarities and differences of different entities and analyse the changes in trends.

\subsection{Common Sense Inference} %This relates to using general knowledge or accepted facts to deduce or infer impacts or realities about the object, character, or event under examination. Applying universal knowledge helps in providing a logical framework for inference-making.

A category that employs common sense or established facts to provide a logical framework for contextual inference. It evaluates the capacity of LVLMs to understand the essences of entities and utilize common sense for accurate inference.

\subsection{Event-Centric Analysis} %This involves an in-depth exploration of a significant event, from understanding its triggers to its effects on the overall narrative trajectory. The event-centric approach can also include categorizing the types of events and predicting potential future occurrences.

A category devoted to in-depth examination of significant events, including investigation of triggering factors and their effects on the overall narrative trajectory. It evaluates the capacity of LVLMs to comprehend the nature and future trajectories of the events.

\begin{table*}[t]

% \renewcommand\arraystretch{0.8}
% \small
\centering
\caption{Results of Different Primary Evaluation Metrics on Various LVLMs}
\label{tab:performance2}

\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Model}         & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE-L} & \textbf{CIDEr} & \textbf{GPT4-Score} \\ \midrule
\multicolumn{6}{c}{\textbf{Open-source LVLMs}}                                                                       \\ \midrule
MovieChat              & 1.87            & 23.77           & 17.74            & 14.68          & 21.81               \\
Video-ChatGPT          & 4.69            & 30.44           & 26.17            & 51.60          & 34.62               \\
Video-LLaVA            & 5.38            & 31.51           & 26.61            & 62.84          & 40.03               \\
ShareGPT4Video         & 4.51            & 31.38           & 26.64            & 52.60          & 39.68               \\
VideoLLaMA2            & 4.43            & 30.87           & 24.40            & 49.07          & 38.88               \\
TimeChat               & 4.56            & 27.47           & 25.63            & 64.52          & 34.42               \\
InternVL2              & 5.96            & 32.74           & 27.32            & 67.57          & 42.25               \\
VILA                   & \underline{6.37}   & 33.57           & \underline{29.30}   & \underline{75.31} & 45.41               \\
InternLM-XComposer-2.5 & 4.36            & 31.02           & 22.90            & 39.18          & \underline{49.69}      \\
MiniCPM-V 2.6          & 5.94            & \underline{33.90}  & 27.95            & 73.97          & 48.57               \\
StreamingChat          & \textbf{8.30}            & \textbf{37.58}  & \textbf{34.92}            & \textbf{91.11}          & \textbf{57.88}               \\ \midrule
\multicolumn{6}{c}{\textbf{Closed-source LVLMs}}                                                                     \\ \midrule
Gemini 1.5 Pro         & 5.78            & 32.91           & 27.03            & 78.51          & 48.83               \\
GPT-4V                 & 6.67            & 36.47           & 29.58            & 82.45          & 60.11               \\
GPT-4o                 & \textbf{6.93}   & \textbf{36.84}  & \textbf{29.77}   & \textbf{83.69} & \textbf{60.70}      \\ \bottomrule
\end{tabular}

\end{table*}

\section{Modification Guideline}
\label{apx:mod}
In the modification phase, adherence to specific criteria is essential. To this end, we propose a comprehensive modification guideline. 

Due to our categorization of QA pairs into six types of relationships, the modification guideline we propose is also divided into six major modules, including Action, Person, Object, Event, Environment, and Quantity. Each module contains more detailed methods of modification to assist annotators in revising questions to better evaluate diverse skills of LVLMs.
\subsection{Action}
If adjacent chains contain QA pairs related to action, QA pairs within the subsequent chain can be modified according to the methods mentioned in the Action module. The specific method of modification is determined by whether the actions involved in the QA pairs within the adjacent chains are the same.

If the actions are the same (Q1 is similar to Q2 and A1 is similar to A2), the following modification methods can be adopted:
\begin{itemize}
\item
\textbf{Modify the question based on the purpose of the action.} Change Q2 to ``What is the purpose or intent of this action?"
\item
\textbf{Modify the question based on the likelihood of the action occurring.} Change Q2 to ``Is this action possible under the given conditions? If so, what conditions need to be met?"
\item
\textbf{Modify the question based on the purpose of the action and spatio-temporal inference.} Change Q2 to ``What is the purpose or intent of this action at a specific time and place?"
\item
\textbf{Modify the question based on counterfactuals.} Change Q2 to ``What different outcomes or impacts might result from taking a different action?"
\item
\textbf{Modify the question based on counterfactuals and spatio-temporal inference.} Change Q2 to ``What different outcomes or impacts might result if this action were performed at another time and place?"
\item
\textbf{Modify the question based on the sequence of actions.} Change Q2 to ``What is the action before/after this one?" or further modify Q2 to ``How does the action before this one affect the execution of the current action?" to deepen the inquiry.
\item
\textbf{Modify the question based on the comparison of multiple actions.} Change Q2 to ``Which action at a specific time and place is more complex/simple compared to a specific action at a previous time and place?" or further modify Q2 to ``How long is the interval between these two actions in time?" to deepen the inquiry.
\end{itemize}
If the actions are different (Q1 is not similar to Q2 and A1 is not similar to A2), the following modification methods can be adopted:
\begin{itemize}
\item 
\textbf{Modify the question based on sequence.} Change Q2 to ``What action is taken next?"
\end{itemize}
\subsection{Quantity}
If adjacent chains contain QA pairs related to quantity, QA pairs within the subsequent chain can be modified according to the methods mentioned in the quantity module, as follows:
\begin{itemize}
\item
\textbf{Modify the question based on the comparison of two quantities.} Change Q2 to ``Comparing the quantities of two categories/types of things, which is more/less?" For example, if Q1 is ``How many people are involved on the field?" and Q2 is ``How many red balls are on the field?" change Q2 to "Which is greater in number on the field, the balls or the people?"
\item
\textbf{Modify the question based on the comparison of two quantities and the trend of change.} Change Q2 to ``What is the trend of change in the quantity of these things under a certain context/time?" or further modify Q2 to ``Does this change have periodicity or regularity, and if so, what is the nature of this regularity?" to deepen the inquiry.
\end{itemize}
\subsection{Person}
If adjacent chains contain QA pairs related to people, QA pairs within the subsequent chain can be modified according to the methods mentioned in the person module, and the specific method of modification is determined by whether the people involved in the QA pairs within the adjacent chains are the same.

If the people involved are different (P2 is different from P1), the following modification methods can be adopted:
\begin{itemize}
\item
\textbf{Modify the question based on the social relationship between two people.} Change Q2 to ``From the existing video, what is the relationship between P2 and P1?" or further modify Q2 to ``Does their relationship change in different plots of the video?" to deepen the inquiry.
\item
\textbf{Modify the question based on the relative positional relationship between two people.} Change Q2 to ``In the most recent scene where P1 and P2 appear together, what is the positional relationship between P2 and P1?" or further modify Q2 to ``How does the distance and direction between them change in a certain context/time?" to deepen the inquiry.
\item
\textbf{Modify the question based on the interaction between two people.} Change Q2 to ``What is the interaction between characters in a certain context/time?"
\end{itemize}
If the people involved are the same (P2 is the same as P1), the following modification methods can be adopted:
\begin{itemize}
\item
\textbf{Modify the question based on the emotional state of the person.} Change Q2 to ``Based on a specific context/time in the video, how does this person's emotional state change?" or further modify Q2 to ``How does the change in emotional state affect behavior?" to deepen the inquiry.
\item
\textbf{Modify the question based on the person's identity background.} Change Q2 to ``What is the background of this person?"
\end{itemize}
\subsection{Object}
If adjacent chains contain QA pairs related to the same object, QA pairs within the subsequent chain can be modified according to the methods mentioned in the object module, as follows:
\begin{itemize}
\item
\textbf{Modify the question based on inference.} Change Q2 to ``How does the existence or characteristics of this object affect other objects or events?" or further modify Q2 to ``If the object no longer exists or its characteristics change, how would the event differ?" to deepen the inquiry.
\item
\textbf{Modify the question based on counterfactuals.} Change Q2 to ``If certain attributes or characteristics of this object were different, what different outcomes or impacts might there be?"
\item
\textbf{Modify the question based on outcomes.} Change Q2 to ``What is the contribution of this object to a certain event or phenomenon?"
\item
\textbf{Modify the question based on counterfactuals and spatio-temporal inference.} Change Q2 to ``If this object were not present at a certain time and place for a certain event or phenomenon, what different impact or outcome might there be?"
\item
\textbf{Modify the question based on impact and spatio-temporal inference.} Change Q2 to ``What is the contribution of this object to a certain event or phenomenon at a specific time and place?"
\item
\textbf{Modify the question based on counterfactuals and general knowledge.} Change Q2 to ``If certain attributes or characteristics of this object were different, how might it affect our common sense or known facts?"
\item
\textbf{Modify the question based on impact and general knowledge.} Change Q2 to ``How does the existence or characteristics of this object affect our common sense or known facts?"
\item
\textbf{Modify the question based on state evolution.} Change Q2 to "How does the state of this object change in a certain context/time in the video?"
\item 
\textbf{Modify the question based on the trajectory of action.} Change Q2 to ``What is the trajectory of this object's action in a certain context/time in the video?"
\end{itemize}
\subsection{Event}
If adjacent chains contain QA pairs related to the same event, QA pairs within the subsequent chain can be modified according to the methods mentioned in the event module, as follows:
\begin{itemize}
\item
\textbf{Modify the question based on sequence.} Change Q2 to ``What happens next?"
\item
\textbf{Modify the question based on the relationship of events.} Change Q2 to ``What event caused this event?" or further modify Q2 to ``What kind of chain reaction did the occurrence of this event cause?" to deepen the inquiry.
\item
\textbf{Modify the question based on event classification.} Change Q2 to ``To which category of events does this event belong (e.g., natural disasters, social events, etc.)?"
\item
\textbf{Modify the question based on the impact of events.} Change Q2 to ``What impact will this event have?"
\item
\textbf{Modify the question based on event prediction.} Change Q2 to ``Based on the current situation, what events might occur in the future?" or further modify Q2 to ``Based on the current situation, how likely is it that certain events will occur in the future, and what is the basis for this?" to deepen the inquiry.
\end{itemize}

\subsection{Environment}
If adjacent chains contain QA pairs related to the same environment, QA pairs within the subsequent chain can be modified according to the methods mentioned in the environment module, as follows:
\begin{itemize}
\item
\textbf{Modify the question based on environmental changes and trends.} Change Q2 to ``Compared to before, what changes are there in the current environment?" or further modify Q2 to ``How will this environment change afterward?" to deepen the inquiry.
\item
\textbf{Modify the question based on counterfactuals.} Change Q2 to ``Without this environment, what impact would there be?"
\end{itemize}


\section{Primary Evaluation Metrics}
As shown in the Table \ref{tab:performance2}, we employ four primary evaluation metrics to assess the answers generated by various LVLMs mentioned and compare the evaluation results with those evaluated by GPT-4. It can be observed that the results from these four primary evaluation metrics are broadly in agreement with the evaluation made by GPT-4, which suggests that our SVBench is capable of accurately and effectively distinguishing distinct levels of capabilities possessed by various LVLMs.
%
Just like the use of sentence similarity \cite{di2024grounded} in QAEgo4D as an important evaluation metric for computing distances in semantic space, we will subsequently use semantic similarity as an evaluation criterion to assess the consistency between the model's output answers and the ground truth.

\section{Case Study}
\label{apx:case}
\subsection{Benchmark Case} We employ qualitative comparisons in the necessity of contextual content and the responses within a QA chain between our SVBench and answers generated by both GPT-4V and GPT-4o, with details in both the Figure \ref{fig_case_single} and the Figure \ref{fig_case_chain}. Furthermore, we conduct additional comparisons in streaming question-answering and nine skills assessment between our SVBench and answers generated by GPT-4o, as shown in Figure \ref{fig_case_streaming} and Figure \ref{fig_case_9_skills}. These comparisons above present that our SVBench is a challenge to the existing LVLMs.

\subsection{Model Case} As demonstrated by Figure \ref{fig_case_10_models}, the current best-performing open-source model, MiniCPM-V 2.6, is unable to analyze the questioner's intent based on contextual information and provide the most appropriate answer. MiniCPM-V 2.6 relies on the most accurate information from the current question and video segments to answer as accurately as possible (and fails to respond when information is insufficient), but it overlooks the coherence and contextual information between questions.

\section{Additional Experiments}
\label{apx:add}


\subsection{Comparison of LLM-based Evaluations with Human Evaluation}
To ensure consistency with LLM assessments, we incorporate human involvement in the generation process and human evaluation. We add human evaluations of various LVLMs results (see Figure \ref{fig:comp}). To validate the consistency between human scores and those from open-source and closed-source models, we employ human evaluation (10 people annotating 200 videos over a week), open-source model evaluation (InternVL2-Llama3-76B), closed-source evaluation model (GPT-4), and traditional evaluation metrics (METEOR). We then plot a score comparison on Multi-turn QA Evaluation. The results indicate: (1) The score variations between Human and GPT-4 are generally consistent across different models, though human scores are more discriminative, suggesting that GPT-4 scores are reasonably reliable; (2) InternVL2-Llama3-76B shows excessive leniency, with more than half of the models scoring above 60 using the same prompt as GPT-4, while METEOR scores are too low, lacking discrimination.
%
\begin{wrapfigure}[11]{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.48\textwidth]{Figure/scores_comparison.png}
    \vspace{-0.5cm}
    \caption{Score comparison on multi-turn QA evaluation (Mul.).}
    \label{fig:comp}
\end{wrapfigure}

%
Regarding the circular dependency issue, we mitigate the reliance on LLMs by incorporating two stages of manual annotations during the construction of annotations. Additionally, the inclusion of open-source and human evaluations enriches the evaluation results, further reducing the dependency on GPT-4 for evaluation.

\begin{figure}[t]
\centering
\begin{minipage}{0.49\textwidth}
%\begin{table}[t]
%\renewcommand\arraystretch{0.6}
% \small
\centering
\includegraphics[width=\linewidth]{Figure/inference_time_vs_video_length.png}
 \vspace{-0.4cm}
	\caption{Correlation between inference speed with video length.}
	\label{fig:infer_time}

%\vspace{-0.3cm}
%\end{table}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Figure/resource_consumption_vs_video_length.png}
 \vspace{-0.4cm}
	\caption{Correlation between resource consumption with video length.}
	\label{fig:res_con}
\end{minipage}
\vspace{-0.5cm}
\end{figure}

\subsection{Correlation Between Inference Speed and Consumption with Video Length}
Figure~\ref{fig:infer_time} and Figure~\ref{fig:res_con} show the inference speed and resource consumption of multiple LVLMs in relation to video length. The inference time is generally positively correlated with the video length. For resource consumption, some models show a plateau as the input length increases, due to the saturation of input frames and the memory reaching its preset limit.

\subsection{Additional Evaluation Results}
We include a comparison with human performance, as shown in \ref{tab:new_evaluation}, which can provide valuable insights into the gap between current models and human capabilities in long-context streaming video understanding. The results show that human performance significantly outperforms various open-source and closed-source models across all metrics in both evaluation settings (Dialogue Evaluation and Streaming Evaluation). Additionally, humans excel in Temporal Understanding (TU) but perform relatively weaker in Informational Completeness (IC). We also included LVLMs such as Flash-VStream-7B~\cite{zhang2024flash}, Qwen2-VL-7B~\cite{Qwen2VL}, and LLaVA-NeXT-Video-7B-DPO~\cite{zhang2024llavanext-video}. We will also add the evaluation results for the models (Oryx~\cite{liu2024oryx}, Long-LLaVA~\cite{wang2024longllavascalingmultimodalllms}, LongVILA~\cite{longvila}) to Table \ref{tab:new_evaluation} as soon as possible.

\begin{table*}[t]
\renewcommand\arraystretch{0.6}
\footnotesize
\centering
\caption{Evaluation results of various models on SVBench in dialogue and streaming evaluation.} % Performance of Various Models on Different QA Tasks} Evaluation results of various models on SVBench across nine long-context streaming video understanding skills.
\label{tab:new_evaluation}
\setlength\tabcolsep{3pt}

\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{6}{c}{\textbf{Dialogue Evaluation}}                                                    & \multicolumn{6}{c}{\textbf{Streaming Evaluation}}                                                   \\
\cmidrule(lr){2-7} \cmidrule(lr){8-13}
                                & \textbf{SA}    & \textbf{CC}    & \textbf{LC}    & \textbf{TU}    & \textbf{IC}    & \textbf{OS}    & \textbf{SA}    & \textbf{CC}    & \textbf{LC}    & \textbf{TU}    & \textbf{IC}    & \textbf{OS}    \\ \midrule
{Human} & \textbf{88.43} &	\textbf{87.47} &	\textbf{89.58} &	\textbf{88.29} &	\textbf{76.15} &	\textbf{83.93} &	\textbf{83.57} &	\textbf{80.71} &	\textbf{87.86} &	\textbf{86.43} &	\textbf{75.71} &	\textbf{80.24}                 
\\ \midrule
\multicolumn{13}{c}{\textbf{Open-source LVLMs}}                                                                                                                                                                                             \\ \midrule
MovieChat                       & 20.46          & 20.05          & 27.76          & 21.81          & 22.21          & 21.89          & 17.99          & 16.42          & 20.37          & 15.77          & 19.08          & 17.43          \\
Video-ChatGPT                   & 31.86          & 32.58          & 40.28          & 35.32          & 36.26          & 33.80          & 27.98          & 29.54          & 33.81          & 27.95          & 31.00          & 28.88          \\
Video-LLaVA                     & 35.62          & 36.52          & 42.93          & 38.63          & 38.84          & 37.34          & 32.22          & 32.83          & 36.35          & 32.46          & 34.54          & 32.79          \\
ShareGPT4Video                  & 39.01          & 40.42          & 47.89          & 41.42          & 43.18          & 40.70          & 34.65          & 36.70          & 41.07          & 35.76          & 37.22          & 35.79          \\
VideoLLaMA2                     & 39.13          & 40.33          & 47.60          & 42.36          & 41.80          & 40.60          & 35.68          & 36.40          & 42.23          & 34.65          & 36.70          & 35.84          \\
{Flash-VStream}                     & 38.23 &	41.56 &	51.51 &	45.99 &	43.32 &	42.69 &	36.78 &	37.50 &	40.48 &	32.46 &	35.23 &	35.92           \\
TimeChat                        & 36.19          & 37.06          & 44.72          & 40.42          & 37.12          & 37.22          & 35.72          & 37.88          & 42.65          & 36.23          & 36.34          & 36.32          \\
InternVL2                       & 45.91          & 46.30          & 52.67          & 49.81          & 46.25          & 46.13          & 43.55          & 44.10          & 48.91          & 40.95          & 44.17          & 42.71          \\
{LLaVA-NeXT-Video}                       & 43.56 &	48.33 &	51.79 &	48.54 &	47.40 &	45.48 &	40.79 &	48.84 &	54.61 &	43.03 &	48.32 &	45.36 \\
VILA                            & 46.83          & 48.41          & 54.92          & 48.30          & 50.12          & 48.51          & 46.19          & 47.95          & 51.60          & 44.84          & 48.56          & 46.26          \\
{Qwen2-VL}                            & 48.52 &	50.65 &	54.75 &	43.04 &	48.59 &	48.65 &	48.94 &	52.46 &	53.24 &	48.32 &	50.70 &	49.48           \\
InternLM-XComposer2.5          & 51.57          & 53.93          & 59.69          & 51.57          & \underline{56.28} & 52.31          & 52.22          & 53.39          & 58.14          & 48.05          & \underline{54.79} & 51.46          \\
MiniCPM-V 2.6                   & \underline{53.50} & \underline{55.42} & \underline{60.88} & \underline{55.03} & 55.78          & \underline{54.30} & \underline{53.33} & \underline{54.30} & \underline{58.97} & \underline{49.64} & 54.71          & \underline{52.19} \\
StreamingChat                   & \textbf{59.48} & \textbf{61.31} & \textbf{66.05} & \textbf{58.61} & \textbf{61.09}          & \textbf{59.41} & \textbf{55.10} & \textbf{56.66} & \textbf{60.72} & \textbf{51.78} & \textbf{55.87}          & \textbf{53.90} \\ \midrule
\multicolumn{13}{c}{\textbf{Closed-source LVLMs}}                                                                                                                                                                                           \\ \midrule
Gemini 1.5 Pro                  & 54.89          & 56.05          & 61.45          & 53.08          & 56.06          & 54.29          & 49.06          & 50.05          & 54.62          & 45.73          & 49.84          & 48.02          \\
GPT-4V                          & 65.56          & 68.02          & 71.78          & 63.80          & 68.01          & 65.19          & 58.82          & 59.55          & 64.29          & 54.08          & 60.61          & 57.35          \\
GPT-4o                          & \textbf{65.73} & \textbf{68.10} & \textbf{71.95} & \textbf{66.54} & \textbf{68.40} & \textbf{66.29} & \textbf{59.52} & \textbf{60.42} & \textbf{65.45} & \textbf{55.10} & \textbf{61.36} & \textbf{58.17} \\ \bottomrule
\end{tabular}

\vspace{-3mm}
\end{table*}

\begin{wrapfigure}[16]{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.48\textwidth]{Figure/gpt4_scores_stylized.png}
    \vspace{-0.5cm}
    \caption{Impact of model sizes on performance of InternVL2.}
    \label{fig:size}
\end{wrapfigure}

\subsection{Impact of Model Size on Performance}
To ensure a fair comparison, we select base models with 7B or 8B parameters. Our benchmark is an ongoing project, and we plan to update it with different sizes of mainstream Video-LLMs, maintaining a real-time updated leaderboard. Additionally, we conduct further experiments comparing the performance of models with different LLM sizes, specifically InternVL2-2B, 4B, 8B, 26B, 40B, and 76B, on SVBench, as shown in Figure~\ref{fig:size}. 
%
The results indicate that model size significantly impacts performance, with larger models demonstrating better accuracy. 
%
Interestingly, the performance of InternVL2 with 76B parameters is actually lower compared to the 40B parameter version. Both models use the same vision part, InternViT-6B, but the LLM has been changed from Yi-based to Llama3-based. This suggests that Llama3 may have weaker visual language processing capabilities, as we also observed a similar decrease in performance on MVBench.


\section{Prompts}
\label{apx:prom}
Below are the prompts used for generation of QA chains within our dataset and evaluation of answers generated by all the LVLMs mentioned. 

\begin{table*}[!htb]
\centering
\begin{tcolorbox}[%`colback`=gray!10,
        colframe=black,
        width=\textwidth,
        arc=2mm, auto outer arc,
        title={Prompt for generating QA chains}]		
Task:

Please construct a chain of 5-6 consecutive open-ended QA pairs based on a series of video frames arranged in temporal order. The chain should follow the rule that, except for the first question, the content of each subsequent question must continue from the answer to the previous question. Vague questions like "What is he doing?" are permissible. If the correct answer to the previous question is not provided, the current question cannot be answered. The chain must not leapfrog to content beyond the answer to the previous question.

Generated example:

    \{\{
        "questions": [
        
            "Did any other competitors fall?",
            
            "How did he fall?",
            
            "What about the other fallen competitor?",
            
            "What does he look like?",
            
            ...
            
        ],
        
        "answers": [
        
            "Another rider dressed in red, white, and blue also fell.",
            
            "He tried to avoid the first person who fell but failed.",
            
            "The other fallen competitor ran off the track and sat down by the side of the track.",
            "He looks seriously injured.",
            
            ...
            
        ]
    \}\}

Required output:

- Generate the QA chain following the format of the provided example, and output the chain in JSON format. Do not produce any additional content. The QA chain should contain two key-value pairs: "questions" and "answers". "Questions" is a list of all the questions generated in the QA chain; "answers" is a list of all the answers generated in the QA chain.

- Ensure that the questions in the QA chain are open-ended and can explore multiple different aspects of the video content without focusing too much on the details. Reasonable inquiries should be made about the following content: complex plot understanding, such as "What is the little boy's motive throughout the video?"; inference of implied information, such as "How did the little boy and little girl in the video meet?"; analysis of emotional coherence, such as "At which moments in the video does the boy feel the happiest?"; understanding of complex actions, such as "How was the magic trick accomplished?"; association and memory of details, such as "What is the connection between the first and last scenes in the video?"; multi-level plot analysis, such as "What is the turning point of the plot?".

- Ensure that the questions and answers in the QA chain are strictly based on the video content itself, constructed only from the direct information in the video, avoiding any unnecessary speculation or over-association.

- Ensure that the questions in the QA chain are clear and precise, directly corresponding to specific information or events in the video, and can be answered by watching the video content without the need for a video description or inference, avoiding questions that require assumptions or reasoning.

- Ensure that the text of the questions and answers in the QA chain does not include specific time descriptions such as "which second".

- Ensure there are no references to the source of information in the QA, avoiding expressions like "from the image", "sequence of pictures", "which frame", or "which photo"; you should understand the input as a video and describe it using video footage.

- Ensure that all questions and answers are output in Chinese.

Video frame input:

\{video frames\}

\end{tcolorbox}
\end{table*}

\begin{table*}[!htb]
\scriptsize
\centering
\begin{tcolorbox}[%`colback`=gray!10,
        colframe=black,
        width=\textwidth,
        arc=2mm, auto outer arc,
        title={Prompt for generating relationships between consecutive QA chains}]		
Task:

You will be given two QA chains, each consisting of 5-6 consecutive open-ended QA pairs, generated for two different but consecutive segments of the same video. Your task is to identify the related QA pairs between the two QA chains.
        
Generated example:
 
    \{\{
"questionsBefore": [

"What kind of gear are the racers wearing?",

"What kind of gear are the racers wearing?",

"What are they preparing to do?",

...

],

"answersBefore": [

"The racers are wearing motocross attire and helmets.",

"The racers are wearing motocross attire and helmets.",

"They are getting ready to start the race.",

...

],

"questionsAfter": [

"What kind of outfits are the racers wearing?",

"What kind of outfits are the racers wearing?",

"What were the racers doing at the start of the race?",

...

],

"answersAfter": [

"The racers are wearing racing suits of various colors, most equipped with safety helmets.",

"The racers are wearing racing suits of various colors, most equipped with safety helmets.",

"The racers were rushing down the hillside, ready to jump.",

...

],

"relationship":[

"Person",

"Object",

"Action",

...

]
    \}\}

    Required output:

     - Note that when you are looking for related QA pairs, you should search from the following six aspects: the relationship between actions, that is, when two QA pairs involve related actions; the relationship between quantities, that is, when two QA pairs involve the number of related people or objects; the relationship between persons, that is, when two QA pairs involve related persons; the relationship between objects, that is, when two QA pairs involve related objects; the relationship between events, that is, when two QA pairs involve the same or related events and activities; the relationship between environments, that is, when two QA pairs involve the same scene or changes in the scene.
    
    - Generate results following the format of the provided example and output all results in JSON format without producing any additional content. The output should include five key-value pairs, where "questionsBefore" is a list of questions from the first QA chain within related QA pairs; "answersBefore" is a list of answers from the first QA chain within related QA pairs; "questionsAfter" is a list of questions from the second QA chain within related QA pairs;"answersAfter" is a list of answers from the second QA chain within related QA pairs; "relationship" is the list of relationships corresponding to the related QA within the two QA chains.
    
    - Note that the number of related QA pairs in the "questionsBefore" and "questionsAfter" lists should be consistent, and the number of relationships in the "relationship" list should match the number of related QA pairs in the two QA chains.
    
    - Note that for the two QA chains, you should output at least six related QA pairs, and each related QA pair should conform to one of the six types of relationships mentioned above.

The "relation" list should only include the following relationships:

1. "Action", when there is a relationship between actions in related QA pairs.

2. "Quantity", when there is a relationship between quantities in related QA pairs.

3. "Person", when there is a relationship between people in related QA pairs.

4. "Object", when there is a relationship between objects in related QA pairs.

5. "Event", when there is a relationship between occurrences in related QA pairs.

6. "Environment", when there is a relationship between environments in related QA pairs.

The first QA chain:
\{chain1\}

The second QA chain:
\{chain2\}
\end{tcolorbox}
\end{table*}

\begin{table*}[!htb]
\centering
\begin{tcolorbox}[%`colback`=gray!10,
        colframe=black,
        width=\textwidth,
        arc=2mm, auto outer arc,
        title={Prompt for generating answers of questions within our dataset by both GPT-4V and GPT-4o }]		
Task: 

You are a video comprehension expert, and you need to answer the questions posed in sequence based on the provided video image sequence. The generated answers should be concise, clear, with an emphasis on the key points, and summarized in one sentence.

                Generated example:
                \{\{
                    "They are smiling and looking at the camera."
                \}\}

                Required output:
                
                - Ensure that the content in the answer is closely related to the topic, avoiding unnecessary expansion and redundancy to provide concise, direct, and relevant information.
                
                - Summarize the answer clearly in one sentence, ensuring conciseness and emphasis on the key points.
                
                - Ensure that the answer precisely targets the posed question, providing comprehensive and direct information. When answering, clearly articulate your viewpoint and ensure all content is closely related to meet the requirements of the posed question.
                
                - Answers should be given following the provided examples, only output the answer, and do not output any text irrelevant to the answer.
                
                - Do not provide information sources in the answer, avoid expressions like "from the image," "picture sequence," "frame number," or "picture number." You should understand the input as a video and describe it using video footage.

                Video frame input:
                \{video frames\}

                Posed questions:
                \{question\}

\end{tcolorbox}
\end{table*}

\begin{table*}[!htb]
\begin{tcolorbox}[%`colback`=gray!10,
        colframe=black,
        width=\textwidth,
        arc=2mm, auto outer arc,
        title={Prompt for GPT4-Score}]		
Task Description:

    You are an expert judge evaluating the accuracy of answers to question about scenes in a streaming video. For each scene, there is a specific question and its ground truth answer. Several models have provided responses to these questions. Your task is to evaluate the accuracy of each response on a scale from 0 to 10, where:

    - 10: The response is completely accurate and matches the ground truth in all relevant details, providing any necessary context.
    
    - 8-9: The response is mostly accurate but may miss minor details or context.
    
    - 6-7: The response is somewhat accurate but lacks significant details or context.
    
    - 4-5: The response provides some relevant information but misses key aspects of the ground truth.
    
    - 2-3: The response has little relevance or severely misconstrues the ground truth.
    
    - 0-1: The response is completely inaccurate or off-topic.

    Additional Requirements and Considerations for the Evaluator:
    
    1. Thoroughly Understand the Question: Ensure that you fully grasp the context and nuances of the question before evaluating the response.
    
    2. Accurate Comparison: Compare the model's response against the ground truth answer with a high degree of precision. Pay attention to the correctness, completeness, and relevance of the information provided.
    
    3. Objective Scoring: Assign a score on a scale from 1 to 10, focusing solely on the accuracy of the response. Do not consider style, grammar, or additional information that is unrelated to accuracy.
    
    4. Detailed Explanation: Provide a clear and concise explanation for the score you assign. This explanation should justify your scoring by pointing out specific accurate or inaccurate details in the model's response.
    
    5. Consistency: Apply the same criteria uniformly across all evaluations to ensure fairness and consistency in scoring.
    
    6. Be Neutral and Unbiased: Do not let any prior knowledge, assumptions, or personal opinions affect your judgment. Only use the provided ground truth and the response when making your decision.

    For the following QA, please evaluate the model's performance according to the criteria mentioned above and provide a detailed justification for each score.

    Questions:
    
    \{question\}
    
    Ground Truth:
    
    \{ground\_truth\}

    Model Responses:
    
    \{model\_response\}

    Please provide your evaluation score and detailed comment below:

    Accuracy:
    
    Score: 
    
    Comments:


\end{tcolorbox}
\end{table*}

\begin{table*}[!htb]
\small
\centering
\begin{tcolorbox}[%`colback`=gray!10,
        colframe=black,
        width=\textwidth,
        arc=2mm, auto outer arc,
        title={Prompt for dialogue evaluation}]		
Task Description:

    You are an evaluation expert for video multi-turn dialogue evaluation. Each video contains different timestamps that are followed by a series of QAs. Your task is to evaluate the quality of model responses to a series of open-ended questions at the same timestamp within a streaming video. You will assess the responses based on several specified dimensions:

    1. Semantic Accuracy: evaluates the accuracy of the generated answers based on a holistic understanding. It considers not only the direct overlap with ground-truth answers but also the context, coherence, and overall relevance of the response to the question posed.
    
    Scoring Guidelines:       
        - 10 points: Completely accurate, directly reflecting the video with no apparent errors.
        - 7-9 points: Mostly accurate, with a few minor detail errors.
        - 4-6 points: Several errors, but generally conveys most of the content.
        - 1-3 points: Only a small part of the content is accurate or mostly incorrect.
        - 0 points: Completely inaccurate, unrelated to the video.

    2. Contextual Coherence: examines the ability of LVLMs to maintain relevance and context across sequential questions and answers, ensuring continuity and alignment with the evolving discourse.
    
    Scoring Guidelines:        
        - 10 points: Highly coherent, natural transition between scenes.
        - 7-9 points: Mostly coherent, with minor issues in transition points.
        - 4-6 points: Some coherence, but loose or partially disjointed transitions.
        - 1-3 points: Poor coherence, most transition points unnatural.
        - 0 points: Completely incoherent, response appears to be unrelated or independent content.

    3. Logical Consistency: evaluates the logical progression and consistency of answers, ensuring that answers do not contradict each other or previous information.
    
    Scoring Guidelines:
        - 10 points: Logically consistent, no sense of incongruity.
        - 7-9 points: Mostly consistent, with few minor inconsistencies.
        - 4-6 points: Several logical issues, but the response is somewhat understandable.
        - 1-3 points: Logically chaotic, difficult to understand or largely unreasonable.
        - 0 points: Completely illogical, contradicts video content.

    4. Temporal Understanding: assesses the model’s proficiency in comprehending and reasoning about temporal events and sequences depicted in the video content.
    
    Scoring Guidelines:        
        - 10 points: The response accurately reflects the timeline and causal relationships of events.
        - 7-9 points: The response largely reflects the correct timeline with minor errors or omissions.
        - 4-6 points: The response reflects the event sequence partially but has significant time-related errors or key omissions.
        - 1-3 points: The response has little correct temporal understanding, with many time errors.
        - 0 points: The response entirely fails to reflect the correct time sequence or events process.
        - -1 points: The question do not involve temporal understanding

    5. Informational Completeness: measures the comprehensiveness to gauge whether the model captures and conveys all relevant elements from the video to provide a thorough answer.
    
    Scoring Guidelines:        
        - 10 points: Fully comprehensive, covering all necessary details.
        - 7-9 points: Mostly comprehensive, with some missing details.
        - 4-6 points: Partially informative, but incomplete.
        - 1-3 points: Largely incomplete, containing only a few details.
        - 0 points: Contains no useful information.

    Overall Score: is derived by aggregating the scores from each aforementioned criterion, ranked as follows:
    
    - Scores 1-2: Irrelevant, factually incorrect, or harmful content.
    - Scores 3-4: Low quality, with no major errors but not meeting requirements.
    - Scores 5-6: Moderate quality, meets basic requirements but performs poorly in some aspects.
    - Scores 7-8: High quality, performs well in most dimensions.
    - Scores 9-10: Excellent performance, fully addressing the questions and all criteria, significantly exceeding the reference answers.

    Additional Requirements and Considerations for the Evaluator:
    1. Unbiased Evaluation: Ensure an unbiased assessment by focusing purely on the content and quality of the responses compared to the ground truth.
    2. Consistency: Maintain consistency in scoring across different responses by adhering strictly to the detailed scoring breakdown provided.
    3. Detail and Justification: Provide detailed feedback for each criterion, explaining why a particular score was assigned to help identify strengths and weaknesses in the responses.
    4. Thoroughness: Avoid rushing through the evaluation. Ensure each response is carefully reviewed and scored based on all aspects of the criteria.

    For the following QA chain, please evaluate the model's performance according to the criteria mentioned above and provide a detailed justification for each score.

    Questions:
    \{question\}
    
    Ground Truth:
    \{ground\_truth\}

    Model Responses:
    \{model\_response\}

    Please provide your evaluation scores and detailed comments for each criterion below:

    Semantic Accuracy:
    
    Score: 
    
    Comments:


\end{tcolorbox}
\end{table*}

\begin{table*}[!htb]
\small
\centering
\begin{tcolorbox}[%`colback`=gray!10,
        colframe=black,
        width=\textwidth,
        arc=2mm, auto outer arc,
        title={Prompt for streaming evaluation}]		
Task Description:

    You are an evaluation expert for streaming video evaluation.  Each video contains different timestamps that are followed by a series of QAs. Your task is to evaluate the quality of model responses to a series of open-ended questions at different timestamps within a streaming video. You will assess the responses based on several specified dimensions:

    1. Semantic Accuracy: evaluates the accuracy of the generated answers based on a holistic understanding. It considers not only the direct overlap with ground-truth answers but also the context, coherence, and overall relevance of the response to the question posed.
    
    Scoring Guidelines:       
        - 10 points: Completely accurate, directly reflecting the video with no apparent errors.
        - 7-9 points: Mostly accurate, with a few minor detail errors.
        - 4-6 points: Several errors, but generally conveys most of the content.
        - 1-3 points: Only a small part of the content is accurate or mostly incorrect.
        - 0 points: Completely inaccurate, unrelated to the video.

    2. Contextual Coherence: examines the ability of LVLMs to maintain relevance and context across sequential questions and answers, ensuring continuity and alignment with the evolving discourse.
    
    Scoring Guidelines:        
        - 10 points: Highly coherent, natural transition between scenes.
        - 7-9 points: Mostly coherent, with minor issues in transition points.
        - 4-6 points: Some coherence, but loose or partially disjointed transitions.
        - 1-3 points: Poor coherence, most transition points unnatural.
        - 0 points: Completely incoherent, response appears to be unrelated or independent content.

    3. Logical Consistency: evaluates the logical progression and consistency of answers, ensuring that answers do not contradict each other or previous information.
    
    Scoring Guidelines:
        - 10 points: Logically consistent, no sense of incongruity.
        - 7-9 points: Mostly consistent, with few minor inconsistencies.
        - 4-6 points: Several logical issues, but the response is somewhat understandable.
        - 1-3 points: Logically chaotic, difficult to understand or largely unreasonable.
        - 0 points: Completely illogical, contradicts video content.

    4. Temporal Understanding: assesses the model’s proficiency in comprehending and reasoning about temporal events and sequences depicted in the video content.
    
    Scoring Guidelines:        
        - 10 points: The response accurately reflects the timeline and causal relationships of events.
        - 7-9 points: The response largely reflects the correct timeline with minor errors or omissions.
        - 4-6 points: The response reflects the event sequence partially but has significant time-related errors or key omissions.
        - 1-3 points: The response has little correct temporal understanding, with many time errors.
        - 0 points: The response entirely fails to reflect the correct time sequence or events process.
        - -1 points: The question do not involve temporal understanding

    5. Informational Completeness: measures the comprehensiveness to gauge whether the model captures and conveys all relevant elements from the video to provide a thorough answer.
    
    Scoring Guidelines:        
        - 10 points: Fully comprehensive, covering all necessary details.
        - 7-9 points: Mostly comprehensive, with some missing details.
        - 4-6 points: Partially informative, but incomplete.
        - 1-3 points: Largely incomplete, containing only a few details.
        - 0 points: Contains no useful information.

    Overall Score: is derived by aggregating the scores from each aforementioned criterion, ranked as follows:
    
    - Scores 1-2: Irrelevant, factually incorrect, or harmful content.
    - Scores 3-4: Low quality, with no major errors but not meeting requirements.
    - Scores 5-6: Moderate quality, meets basic requirements but performs poorly in some aspects.
    - Scores 7-8: High quality, performs well in most dimensions.
    - Scores 9-10: Excellent performance, fully addressing the questions and all criteria, significantly exceeding the reference answers.

    Additional Requirements and Considerations for the Evaluator:    
    1. Unbiased Evaluation: Ensure an unbiased assessment by focusing purely on the content and quality of the responses compared to the ground truth.
    2. Consistency: Maintain consistency in scoring across different responses by adhering strictly to the detailed scoring breakdown provided.
    3. Detail and Justification: Provide detailed feedback for each criterion, explaining why a particular score was assigned to help identify strengths and weaknesses in the responses.
    4. Thoroughness: Avoid rushing through the evaluation. Ensure each response is carefully reviewed and scored based on all aspects of the criteria.

    For the following streaming QAs, please evaluate the model's performance according to the criteria mentioned above and provide a detailed justification for each score.

    Questions: 
    \{question\}
    
    Ground Truth:
    \{ground\_truth\}

    Model Responses:
    \{model\_response\}

    Please provide your evaluation scores and detailed comments for each criterion below:

    Semantic Accuracy:
    
    Score: 
    
    Comments:


\end{tcolorbox}
\end{table*}

\begin{figure*}[htb]
\centering
\includegraphics[width=0.95\textwidth]{Figure/case_single.png} % Reduce the figure size so that it is slightly narrower than the column.
\caption{\textbf{Case study between a single QA pair with contextual content and the same QA pair without contextual content:} The red text highlights inaccuracies in the answer generated by both GPT-4v and GPT-4o. }
\label{fig_case_single}
\end{figure*}

\begin{figure*}[htb]
\centering
\includegraphics[width=0.95\textwidth]{Figure/case_chain.png} % Reduce the figure size so that it is slightly narrower than the column.
\caption{\textbf{Case study of a QA chain:} The red text highlights inaccuracies in answers generated by both GPT-4v and GPT-4o. }
\label{fig_case_chain}
\end{figure*}

\begin{figure*}[htb]
\centering
\includegraphics[width=0.95\textwidth]{Figure/streaming.png} % Reduce the figure size so that it is slightly narrower than the column.
\caption{\textbf{Case study of Streaming Question-Answering:} The red text highlights inaccuracies in answers generated by GPT-4o. }
\label{fig_case_streaming}
\end{figure*}

\begin{figure*}[htb]
\centering
\includegraphics[width=0.95\textwidth]{Figure/9skills.png} % Reduce the figure size so that it is slightly narrower than the column.
\caption{\textbf{Case study of 9 skills assessment:} The red text highlights inaccuracies in answers generated by GPT-4o. }
\label{fig_case_9_skills}
\end{figure*}

\begin{figure*}[htb]
\centering
\includegraphics[width=0.95\textwidth]{Figure/10modelcase.png} % Reduce the figure size so that it is slightly narrower than the column.
\caption{\textbf{Case study: Comparison of StreamingChat and the best-performing open-source LVLM on SVBench.} The red text highlights inaccuracies in answers generated by MiniCPM-V 2.6. }
\label{fig_case_10_models}
\end{figure*}

\end{document}
