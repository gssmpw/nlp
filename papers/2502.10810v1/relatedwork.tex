\section{Related Work}
% \subsection{Streaming Video Models}
\subsection{Large Vision-Language Models for Video}
%Based on the significant achievements of Large Language Models (LLMs), recent studies have increasingly shifted towards exploring and developing Large Vision-Language Models (LVLMs). These research endeavors aim to enhance multimodal comprehension and generation capabilities, fully leveraging the abilities of LLMs to tackle new tasks, particularly those involving video. Currently,  LVLMs that process video tasks can be broadly categorized into two types based on the nature of their inputs: one type utilizes text and static images as input, such as the recently popular GPT-4(Vision) and GPT-4(Omni); the other type employs video modalities that go beyond static images as input, aiming to further unearth the potential of LLMs in video task processing to improve performance like PLLaVA and Gemini. However, these LVLMs are not yet fully competent in handling video tasks, as they still lack a profound understanding of video content. Therefore, we propose a challenging video benchmark—streamingbench—to objectively and thoroughly assess their capabilities.
Recent advancements in Large Language Models (LLMs) have paved the way for a significant research focus on Large Vision-Language Models (LVLMs) aimed at improving multimodal understanding, such as multimedia retrieval~\cite{yang2024ldre,yang2024semantic}, particularly in video content.
%
Currently, in addition to the popular closed-source LVLMs such as GPT-4V \cite{yang2023dawn}, GPT-4o \cite{achiam2023gpt}, and Gemini 1.5 Pro \cite{reid2024gemini}, an increasing number of open-source LVLMs, including Video-ChatGPT \cite{maaz2023video}, VideoLLaMA2 \cite{cheng2024videollama}, and VILA \cite{lin2024vila}, also have demonstrated the impressive capability in video understanding tasks. 
%
Advanced human-computer interaction in everyday life requires the ability to engage in multi-turn dialogues and understand extensive contextual histories to maintain coherent and contextually appropriate conversations \cite{qian2024streaming, chen2024videollm}.
%
However, these LVLMs are still not fully adept at handling the intricacies of streaming videos and do not completely grasp the complexities of real-world contexts.
%
% Therefore, we propose a challenging video benchmark—SVbench—to objectively and thoroughly assess their capabilities.
%
% 
To rigorously evaluate the capabilities of these models, we propose SVBench to measure the performance of LVLMs in video-related tasks that imitate the complexity of real-world interactions.

\subsection{Video Understanding Benchmarks}
In recent years, the exponential growth of video data has elevated video understanding to a crucial area within computer vision.
%
To rigorously assess the performance of LVLMs, researchers have introduced a range of standardized benchmarks. 
%
These benchmarks provide not only comparative evaluation criteria for models but also catalyze advancements in video understanding models. 
%
The recent benchmarks, such as TGIF-QA \cite{jang2017tgif}, MSVD-QA \cite{xu2017video}, and MVBench \cite{li2024mvbench}, primarily comprise relatively brief videos capturing single events, thereby overlooking the temporal dependencies inherent in longer videos. 
%
To address the intricacies of long video comprehension, benchmarks using longer videos like movies have been developed. 
%
For instance, LLaMA-Vid \cite{li2023llama}, based on MovieNet \cite{huang2020movienet}, has developed a movie QA dataset to identify character relationships.
%
Similarly, MovieChat \cite{su2020moviechats} employs a diverse set of videos and avoids specific character names or plot details within its questions.
%
However, these benchmarks often fall short in addressing the intricate challenges posed by streaming videos, which encompass extended temporal contexts and dynamic scene variations. 
%
Therefore, we establish SVBench, a novel and comprehensive benchmark that aims to bridge this gap by offering an elaborate evaluation framework for long-context streaming video understanding.
% However, these benchmarks operate offline and rely on entire videos or single images for evaluation. 
%
%In contrast, our newly proposed SVBench is designed to assess the ability to continuously receive and process video frames, as it concurrently updates visual content and evaluates the capacity to handle a dynamic dialogue in real time.
% In contrast, our proposed SVBench is designed to assess the ability to continuously receive and process video frames, as it concurrently updates visual content and evaluates the capacity to handle a dynamic dialogue in real-time.
% In contrast, an online assistant should be equipped to continuously receive and process video frames, as it concurrently updates the visual content and facilitates a dynamic dialogue in real-time.
% MovieChat takes on a different approach, avoiding specific plot details in its questions, thereby requiring minimal dependency understanding. Benchmarks like EgoSchema focus on first-person footage with an emphasis on daily activities, but tend to involve short sequences with few scene changes. In contrast, our new benchmark, StreamingBench, offers numerous benefits for evaluating video understanding models, thanks to its complex scenarios, wide range of question-answer pairs, focus on temporal and causal reasoning, and dynamic scene simulation.
%
% Furthermore, their annotations are automatically generated, lacking human verification. ActivityNet-QA \cite{yu2019activitynet}, derived from the ActivityNet \cite{caba2015activitynet} dataset, introduces longer motion, spatial, and temporal QAs, thereby increasing the complexity of video understanding tasks. Nevertheless, it remains largely focused on specific activities within short videos, failing to address the detection and inference required for complex scenes in extended videos. 
%To address the intricacies of long video comprehension, researchers have begun utilizing long videos, particularly films, to construct benchmarks. For instance, the LVU \cite{wu2021towards} dataset proposes tasks such as predicting the release year of a movie and identifying character relationships. Similarly, LLaMA-Vid \cite{li2023llama}, based on MovieNet \cite{huang2020movienet}, has developed a movie QA dataset. Despite employing long videos, many questions pertain to general knowledge about films, allowing answers to be obtained without intensive video analysis. In contrast, MovieChat \cite{su2020moviechats} employs a diverse set of videos and intentionally avoids specific character names or plot details within its questions. However, the dependency relationships between question-answer pairs are relatively minimal, enabling models to respond based on isolated information. Beyond cinematic content, other long video understanding benchmarks have emerged, such as EgoSchema \cite{mangalam2024egoschema}, which utilizes first-person footage from Ego4D \cite{grauman2022ego4d} to present video reasoning tasks. Nonetheless, EgoSchema mainly concentrates on shorter segments of daily activities, featuring minimal scene transitions. In this paper, we propose StreamingBench, a novel benchmark dataset designed to evaluate the performance of video understanding models in dynamic, continuous scenes. When compared to preceding benchmarks, StreamingBench offers several notable advantages: (1) complexity of scenarios (2) extensive question-answer pairs (3) assessment of temporal continuity and causal reasoning, and (4) simulation of dynamic scenes.



\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{Figure/framework.png} % Reduce the figure size so that it is slightly narrower than the column.
\vspace{-0.4cm}
\caption{\textbf{Overview of the proposed SVBench framework:} (1) Filtering raw videos from diverse streaming sources; (2) Detecting scenes and splitting videos accordingly; (3) Constructing QA chains for dialogues within videos; (4) Performing manual annotation and quality assessment; (5) Identifying temporal linkages between QA chains; (6) Connecting QA chains to facilitate temporal reasoning; (7) Building temporal dialogue paths for evaluating LVLMs.}
\vspace{-0.2cm}
\label{fig2}
\end{figure*}
% "Overview of the proposed SVBench framework: (1) Filtering raw videos from diverse streaming sources; (2) Detecting scenes and splitting videos accordingly; (3) Constructing question-and-answer (QA) chains for dialogues within videos; (4) Performing manual annotation and quality assessment; (5) Identifying temporal linkages between events; (6) Connecting QA chains to facilitate temporal reasoning; (7) Building temporal dialogue paths for evaluating Language-Vision-Language Models (LVLMs)."