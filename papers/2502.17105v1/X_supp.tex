\onecolumn
\section{Additional results on scatter plots} 
\label{sec:additional-results-on-scatter-plots} 
Additional results to \cref{sec:score-emsembling} are presented in \cref{fig:additional-score-ensemble-viz}. 


\begin{figure*}[!ph]
    \centering
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-progan-shuffle28.pdf}
        \subcaption{ProGAN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-stylegan-shuffle28.pdf}
        \subcaption{StyleGAN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-stylegan2-shuffle28.pdf}
        \subcaption{StyleGAN2}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-biggan-shuffle28.pdf}
        \subcaption{BigGAN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-cyclegan-shuffle28.pdf}
        \subcaption{CycleGAN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-stargan-shuffle28.pdf}
        \subcaption{StarGAN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-gaugan-shuffle28.pdf}
        \subcaption{GauGAN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-deepfake-shuffle28.pdf}
        \subcaption{DeepFake}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-dalle-shuffle28.pdf}
        \subcaption{DALL-E}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-glide_100_10-shuffle28.pdf}
        \subcaption{GLIDE\_100\_10}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-glide_100_27-shuffle28.pdf}
        \subcaption{GLIDE\_100\_27}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-glide_50_27-shuffle28.pdf}
        \subcaption{GLIDE\_50\_27}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-guided-shuffle28.pdf}
        \subcaption{ADM}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-ldm_100-shuffle28.pdf}
        \subcaption{LDM\_100}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-ldm_200-shuffle28.pdf}
        \subcaption{LDM\_200}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-ldm_200_cfg-shuffle28.pdf}
        \subcaption{LDM\_200\_cfg}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-sitd-shuffle28.pdf}
        \subcaption{SITD}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-san-shuffle28.pdf}
        \subcaption{SAN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-crn-shuffle28.pdf}
        \subcaption{CRN}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-imle-shuffle28.pdf}
        \subcaption{IMLE}
    \end{subfigure}
     \caption{Scatter plots of per-sample scores. X-axis is UnivFD logits, and Y-axis is the logit from PatchShuffle with patch size 28. The decision boundary of UnivFD (\textcolor{red}{red}) and SFLD (\textcolor{green}{green}) are shown.}
     \vspace{-1em}
     \label{fig:additional-score-ensemble-viz}
     \vspace{-1em}
\end{figure*}
\twocolumn

\section{Datasets}
\label{supp:datasets}

\subsection{Train dataset}
To establish a baseline for comparison, we adopt the most common setting for training the detection model, namely the train set from ForenSynths\cite{wang2020cnn}. 
The train set consists of real images and ProGAN\cite{karras2018progressive}-generated images. It involves 20 different object class categories, each containing 18K real images from the different LSUN\cite{yu2015lsun} datasets and 18K synthetic images generated by ProGAN.

\subsection{Test dataset}
We evaluate the performance of SFLD on (1) conventional benchmarks, (2) TwinSynths which we proposed, (3) low-level vision and perceptual loss benchmarks. In this section, we provide a detailed description of the configurations for the conventional benchmarks and low-level vision and perceptual loss benchmarks.

\textbf{Conventional benchmark} 
This is from ForenSynths\cite{wang2020cnn} and Ojha \etal\cite{ojha2023towards}, including 16 different subsets of generated images, synthesized by seven GAN-based generative models, eight diffusion-based generative models and one deepfake model. 
The subset of GAN-based fake images are from ForenSynths\cite{wang2020cnn}, including ProGAN\cite{karras2018progressive}, StyleGAN\cite{karras2019style}, StyleGAN2\cite{Karras2019stylegan2}, BigGAN\cite{brock2018biggan}, CycleGAN\cite{CycleGAN}, StarGAN\cite{choi2018stargan}, and GauGAN\cite{GauGAN}.
The subset of diffusion-based fake images are from Ojha \etal\cite{ojha2023towards}, including DALL-E\cite{dayma2021dall}, three different variants of Glide\cite{nichol2021glide}, ADM(guided-diffusion)\cite{dhariwal2021diffusion}, and three different variants of LDM\cite{rombach2022high}.
Deepfake set is from FaceForensices++\cite{Deepfake} which is included in ForenSynths\cite{wang2020cnn}.
The real images corresponding to the fake images described above were directly taken from the same datasets. Those are sampled from LSUN\cite{yu2015lsun}, ImageNet\cite{russakovsky2015imagenet}, CycleGAN\cite{CycleGAN}, CelebA\cite{CelebA}, COCO\cite{coco}, and FaceForensics++\cite{Deepfake}.


\textbf{Low-level vision and perceptual loss benchmarks} 
Low-level vision benchmark consists of SITD\cite{chen2018SITD} and SAN\cite{dai2019SAN}.
These are image processing models that approximate long exposures in low light conditions from short exposures in raw camera input or process super-resolution on low-resolution images.
Perceptual benchmark consists of CRN\cite{chen2017CRN} and IMLE\cite{li2019IMLE}.
These models color the semantic segmentation map into a realistic image while directly optimizing a perceptual loss.
These benchmarks are from ForenSynths\cite{wang2020cnn}.



\section{Qualitative analysis on TwinSynths dataset}
We show the GradCAM visualization of UnivFD\cite{ojha2023towards} and Patch-shuffle 28×28 using the TwinSynths dataset in \cref{fig:cam-viz-twinsynth}.
Similar to \cref{sec:qualitative-analysis}, UnivFD is shown to focus on the class-dependent salient region, whereas our method focuses on the entire image region. 
Moreover, we observed that for TwinSynths dataset, UnivFD does respond identically to real/fake images which indicate its inability to capture subtle fake image fingerprints, whereas our method shows the response to such a difference. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/benchmark_vis/cam-twinsynth-univfd.pdf}
        \caption{UnivFD\cite{ojha2023towards} examples}
    \end{subfigure}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/benchmark_vis/cam-twinsynth-shuffle28.pdf}
        \caption{PatchShuffle(patch size 28) examples}
    \end{subfigure}
    \caption{Class activation maps (CAM) for UnivFD\cite{ojha2023towards} and the patch-shuffled detector (ours) in TwinSynths dataset. 
    Each row shows examples from TwinSynths-real, TwinSynths-GAN, TwinSynths-DM sets. 
    GradCAM\cite{selvaraju2020grad,jacobgilpytorchcam} was used to obtain the heatmaps. }
    \label{fig:cam-viz-twinsynth}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/suppl_C.pdf}
    \caption{Illustration of the test input processing strategy. In typical methods, a test image is center-cropped before being passed to the detector. Our patch shuffling strategy allows us to select patches from the entire image region, effectively increasing its receptive field.}
    \label{fig:selecting_patches}
    \vspace{-10pt}
\end{figure}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
        \input{tables/suppl-fullimage}
    }
    \caption{mAP results of the various sizes of test images, comparing two different patch selecting methods. \emph{Center} denotes that the images have been center-cropped to 224×224, while \emph{full image} means that random patches from the full image have been combined to reconstruct a 224×224 image.}
    \label{tab:suppl-fullimage}
\end{table}

\section{Effect of selecting patches from the whole image}
\label{sec:Appendix-fullimagesampling}
\cref{fig:selecting_patches} illustrates the concept of patch extraction of SFLD mentioned in \cref{subsec:SFLD}.
Unlike many alternative detection methodologies, SFLD extracts patches from any position within the input image at the test time.
This approach enhances the detector's receptive field and improves performance for images that have higher resolution than 224×224.
In \cref{tab:suppl-fullimage}, we compare results on benchmarks that have high-resolution images.
We consider different SFLD ensemble options and the location of the selected patch.
The main benchmark consists mostly of 256×256 images, which have little margin with a 224×224 center crop.
Meanwhile, the CRN and IMLE benchmarks have 512×256 images, and the SITD benchmark includes images much larger up to 2,848×4,256 or 4,032×6,030.

We observed that the discrepancy between the two methodologies was minimal when the test image was small.
However, as the image size increased, the performance of the method that solely focused on the center of an image became increasingly constrained.



\section{Image degradation examples}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/rebut/robustness_degradation.png}
    \caption{Examples of two image degradation}
    \label{fig:robustness_examples}
\end{figure}

\cref{fig:robustness_examples} shows examples of image gradations.
According to our definition of high- and low-level features, we can consider that the gaussian blur attacks both high- and low-level features in the image, and the JPEG compression attacks on low-level features in the image.


\section{Robustness against image degradation}
\label{sec:supple_robustness}

Since image degradation was not considered during training, it may be useful to examine the changes in output distribution (as shown in \cref{fig:robustness_distribution_jpeg} in supplementary material) to analyze the model's operational tendencies in detail.
\cref{fig:robustness_distribution_jpeg} reveals distinctions between the high-level feature model (UnivFD \cref{fig:robustness_univfd}), low-level feature model (NPR \cref{fig:robustness_npr}), and integrated model.
The distributions of SFLD and UnivFD remain distinguishable, despite a slight decline in discrimination performance.
However, NPR aligns real and generated images into the same distribution.
This behavior arises from the operational mechanism of each model.
NPR primarily focuses on low-level features, resulting in a catastrophic failure to maintain robustness against JPEG compression.
UnivFD demonstrates relative robustness due to its emphasis on high-level features through CLIP visual encoders; however, there is a slight performance penalty because the visual encoder does not completely disregard low-level features.
In contrast, SFLD exhibits robustness against JPEG compression by integrating both high- and low-level features through ensemble/fusion, allowing each to compensate for the information lost in the other.


\section{Effect of patch sizes}

\label{sec:supple_patchsize}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebuttal-patchsize/patch_size_gan.pdf}
        \subcaption{GAN-based generators.}
        \label{fig:rebuttal-patchsize-gan}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebuttal-patchsize/patch_size_diff.pdf}
        \subcaption{Diffusion-based generators.}
        \label{fig:rebuttal-patchsize-diff}
    \end{subfigure}
    \caption{Results of the ensemble models of UnivFD and the patch-shuffled model with each patch size. For 224, it is the same as UnivFD.}
    \label{fig:rebuttal-patchsize}
\end{figure}
To supplement \cref{fig:patch_size} in the main text, we checked the AP for each generator, rather than the average AP on the conventional benchmark.
\cref{fig:rebuttal-patchsize} illustrates that SFLD consistently maintains high performance as long as the patch size is not smaller than the patch size of the image encoder backbone. 
This is because when the shuffling patch size $s_N$ is smaller than the ViT's patch size, the input tokens are affected by patch-shuffling to get an unnatural image patch, resulting in the encoder not properly embedding the visual feature.


\section{Ablation on the pre-trained image encoder}
\label{sec:suppl_pretrained}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebuttal-encoders/clip.pdf}
        \subcaption{CLIP-ViT}
        \label{fig:rebuttal-encoder-clip}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebuttal-encoders/dino.pdf}
        \subcaption{DINOv2-ViT}
        \label{fig:rebuttal-encoder-dino}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebuttal-encoders/openclip.pdf}
        \subcaption{OpenCLIP-ViT}
        \label{fig:rebuttal-encoder-openclip}
    \end{subfigure}
    \caption{Class-wise detection results for StyleGAN-\{\emph{bedroom, car, cat}\} class categories reported in AP. \emph{bedroom} class is a novel class that is not in the training set.}
    \label{fig:rebuttal-encoders}
\end{figure}


The pre-trained image encoder is employed to learn the features of the ``real'' class.
According to \cite{ojha2023towards}, directly fine-tuning the encoder makes the detector overfit to a specific generator used in training. This results in low generalization to unseen generators.
Therefore, we utilized the frozen CLIP:ViT-L/14 model following UnivFD.

\cref{tab:rebuttal-encoder} show that our patch shuffling and ensembling strategy improves the performance regardless of the pre-trained backbone.
All models are trained only with real and generated images from ProGAN and tested on the various unseen generated images in conventional benchmark. For ImageNet-ViT, we used ViT-B/16 model, following UnivFD paper \cite{ojha2023towards}. Since its encoders have patch size of 16, we utilized 16 and 32 for patch sizes instead of 28 and 56.
Moreover, note that simply employing different pre-training datasets or strategies -- ImageNet, DINOv2, OpenCLIP -- does not address the content bias problem. (see \cref{fig:rebuttal-encoders}) 



\begin{figure*}[!h]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebut/robustness_SFLD_all_final.pdf}
        \subcaption{The changes of SFLD output distribution}
        \label{fig:robustness_sfld}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebut/robustness_univfd_all_final.pdf}
        \subcaption{The changes of UnivFD output distribution}
        \label{fig:robustness_univfd}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/rebut/robustness_NPR_all_final.pdf}
        \subcaption{The changes of NPR output distribution}
        \label{fig:robustness_npr}
    \end{subfigure}
    \caption{The changes of model output distribution against JPEG compression}
    \label{fig:robustness_distribution_jpeg}
\end{figure*}

\begin{table*}[h]
    \centering
    \resizebox{\textwidth}{!}{
        \input{tables/rebuttal_vary_encoder_total}
    }
    \caption{Detection accuracy and AP on a conventional benchmark of the proposed patch shuffling and ensembling (SFLD) strategy across various pre-trained encoders. For the ImageNet encoder, ViT-B/16 is used. For the other encoders, ViT-L/14 is used.}
    \label{tab:rebuttal-encoder}

\end{table*}


\section{In-the-wild applications of SFLD}
\label{sec:suppl_inthewild}

We applied our SFLD to in-the-wild AI-generated image detection, especially to a deepfake detection benchmark.
We have already demonstrated performance on a FaceForensics++\cite{roessler2019faceforensicspp} subset, which is a deepfake detection benchmark created using face manipulation software \cite{Deepfakes}. Here, we have added \cref{tab:rebuttal-deepfake} with experiments using Generated Faces in the Wild\cite{borjiGFW} datasets. SFLD shows state-of-the-art performance in detecting real-world deepfakes. 


\section{Pseudocode of SFLD}
\label{sec:pseudocode}
See \cref{alg:pseudocode}. 

% Let $\phi(\cdot)$ bet the visual encoder (in our case, CLIP ViT-B/14)

\begin{algorithm}[h]
    \begin{lstlisting}[language=python]
    """
    Args: 
        image: A test image instance
        n_views: Number of views for random patch shuffle averaging. Defaults to 10. 
        visual_encoder: A CLIP-pretrained ViT-L/14 visual encoder.
    Returns: 
        output: a real/fake score normalized to [0,1] range.
    """

    # prediction from 224x224 unshuffled view
    feature = visual_encoder(image)
    output_224 = classifier_univfd(feature)

    # prediction from 56x56 random shuffled views
    output_56 = []
    for _ in range(n_views): 
        image_shuffled = patch_shuffle(image, size=56)
        feature = visual_encoder(image_shuffled)
        output = classifier_56(feature)
        output_56.append(output)
    output_56 = mean(output_56)

    # prediction from 28x28 random shuffled views
    output_28 = []
    for _ in range(n_views): 
        image_shuffled = patch_shuffle(image, size=28)
        feature = visual_encoder(image_shuffled)
        output = classifier_28(feature)
        output_28.append(output)
    output_28 = mean(output_28)

    # ensemble the logit scores
    output = mean([output_224, output_56, output_28])
    output = output.sigmoid()
    \end{lstlisting}
    \caption{PyTorch-style pseudocode of SFLD}
    \label{alg:pseudocode}
\end{algorithm}

\begin{table}[t]
    \centering
    \resizebox{0.55\linewidth}{!}{
        \input{tables/rebuttal_UADFV}
    }
    \caption{Performance on the in-the-wild deepfake detection benchmark.}
    \label{tab:rebuttal-deepfake}
\end{table}
%------------ related works

\input{tex/2_relatedwork}

