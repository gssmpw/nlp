\section{Method}
\label{sec:method}


\begin{figure}[t]
     \centering
     \includegraphics[width=1.0\linewidth]{figs/method.pdf}
     \caption{Architecture of the proposed fake image detector (SFLD). $z_{s_i}$ refers to the logit score generated from an input image processed via $s_i\text{×}s_i$ patch size. $\Sigma$ indicates weighted sum.}
     \label{fig:method-architecture}
\end{figure}

\subsection{Patch Shuffling Fake Detection} 
\label{subsec:SFLD}
\textbf{Backbone.}
We utilize the visual encoder of CLIP ViT-L/14\cite{dosovitskiy2021an, CLIP} to leverage the pre-trained feature space. This choice is based on Ojha \etal\cite{ojha2023towards}, which showed that it outperforms other models such as CLIP:ResNet-50, ImageNet:ResNet-50, and ImageNet:ViT-B/16 in distinguishing real from fake images. The results indicated that both the architecture and the pre-training data are crucial. 
Based on this insight, we chose the ViT model for our backbone. 
As shown in \cref{fig:method-architecture}, we extract CLIP features and train a fully connected layer to classify real and fake images.


\textbf{PatchShuffle.}
To effectively integrate both semantic and textural features, PatchShuffle disrupts the global structure of an image while preserving local features.
In the PatchShuffle process, the input images are divided into non-overlapping patches of size $s \times s$ and then randomly shuffled. This operation produces a new shuffled image $x_s$.

For a given $s$, the logit score of the shuffled image is, 
\begin{equation}
    z_{s} = \psi(f(x_s)) \,,
\end{equation}
where $f( \cdot )$ represents a pre-trained CLIP encoder and $\psi(\cdot)$ is a single fully connected layer appended to $f$.

There are classifiers for each patch size of shuffled images to leverage local structure information hierarchically within the image.
We selected patch sizes of 28, 56, and 224 for the proposed SFLD.
As shown in \cref{fig:method-architecture}, $s_0$ is 224, $s_1$ is 56 and $s_2$ is 28.
These configurations are studied in detail in \cref{subsec:patchsetting}.
For each patch size $s_j$, the classifier $\psi_{s_j}$ is trained independently.
Notably, UnivFD takes a center-cropped 224×224 image as input to the CLIP encoder.
Therefore, when using a patch size of 224 in PatchShuffle, it effectively corresponds to the same setting as UnivFD\cite{ojha2023towards}.

We employ binary cross-entropy loss for each classifier:
\small\begin{equation}
    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log \sigma(z_{s_j}) + (1 - y_i) \log \left(1 - \sigma(z_{s_j}) \right) \right] 
\end{equation}\normalsize
where $N$ is the number of data and $y_i \in \{0, 1\}$ is the label whether an input $x_i$ is real ($y_i = 0$) or fake ($y_i = 1$).

\textbf{SFLD.} SFLD combines multiple classifiers trained on shuffled images with different patch sizes.
By varying the patch size, SFLD incorporates models that focus on various levels of structural features, ranging from fine-grained local details to more global patterns.

During testing, $N_{views}=10$ shuffled views are generated for each patch size. The logits from these views are averaged and processed by the corresponding classifier. The final probability $P_{SFLD}(y|x)$ is computed by averaging the logits across patch sizes and applying the sigmoid function:
\small\begin{equation}
    P_{\text{SFLD}}(y|x) = \sigma\left(\frac{1}{k} \sum_{j=1}^{k} \psi_{s_j}\left(\frac{1}{N_{\text{views}}} \sum_{i=1}^{N_{\text{views}}} f(x_{s_j}^i)\right)\right) \,,
\end{equation}\normalsize
where $k$ is the number of patch sizes used in the ensemble (e.g., $k=3$ in our configuration).

Binary classification is done using a threshold of 0.5 on $P_{SFLD}$. Although the fusion method is simple and not tuned for each test class, its simplicity enables strong generalization across diverse fake image sources. By combining classifiers trained on different patch sizes, SFLD achieves a robust and general detection performance. \cref{alg:pseudocode} shows the full workflow of SFLD, especially the fusion of multiple classifiers during inference.

\subsection{TwinSynths} 
In \cref{sec:intro}, we pointed out three shortcomings in the previous benchmarks: low image quality, lack of content preservation, and limited class diversity.
This issue must be addressed to allow a comprehensive comparison of detectors.
Therefore, we propose a novel dataset creation methodology and \emph{TwinSynths} benchmark, consisting of GAN- and diffusion-based generated images that are  paired with visually-identical real counterparts.
To create a practical benchmark for evaluating generated image detectors, it is essential to ensure the generation of high-quality images that preserve the original content.
To achieve this, the image generation process should ideally sample a distribution that closely resembles a real distribution.
From this perspective, the image generation or sampling process can be interpreted as effectively fitting the generator to a single real image.
Through this approach, we construct image pairs that preserve the content of the images while reflecting the architectural traits of the generative models.
Additionally, this methodology allows for the expansion of target classes in the benchmark by generating paired images for any real image.
\cref{fig:benchmark_visual_new} are some examples of TwinSynths.
We can see that the content of the paired real image is faithfully reproduced and the quality of the generated image is guaranteed.

\textbf{TwinSynths-GAN benchmark.}
The GAN-based subsets in the previous benchmark have disparate training configurations, especially the class of training images, resulting in a discrepancy between the generated and the real images.
In order to generate a high quality image that preserves the content of the paired real image while leveraging the training methodology of GANs, we trained the generator from scratch using a single real image.
The MSE loss was provided to the generator to generate an image that is identical to the original image.
For reproduction, the latent vector for the generator input is maintained at a fixed value.
We created 8,000 generated images from 80 selected ImageNet\cite{russakovsky2015imagenet} classes, which is much larger than previous benchmarks.
We selected 40 classes following the \emph{ProGAN} subset in ForenSynths\cite{wang2020cnn}, while the other 40 classes were chosen arbitrarily. 
We utilized DCGAN \cite{radford2015unsupervised} architecture. 

\textbf{TwinSynths-DM benchmark.} 
In comparison to GAN-based subsets, diffusion-based subsets in conventional benchmarks were generated with off-the-shelf pretrained models, having much severer content discrepancy between real and generated images.
In order to generate a high quality image that preserves the content of paired real image while leveraging the inference process of diffusion models, we used DDIM inversion\cite{songdenoising} to generate image that is similar to the real image.
We apply a DDIM forward process to the real image to make it noisy and perform text-conditioned DDIM denoising process using the prompt template \texttt{`a photo of  \{class name\}'}. 
For the prompts, we used the class names from ImageNet.
This process makes TwinSynths-DM preserve the similarity with the paired real images. 
We used the same image classes used to create TwinSynths-GAN. 
We utilized the pretrained decoder and scheduler of \cite{songdenoising}.
