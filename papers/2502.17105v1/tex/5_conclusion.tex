\section{Discussion}
\subsection{Detailed Comparison with UnivFD}

\begin{table}[t]
    \centering
    \resizebox{0.8\linewidth}{!}{
        \input{tables/real-fake-accuracy}
    }
    \caption{Classification accuracy on real and fake sets on ForenSynths \cite{wang2020cnn} and diffusion sets in Ojha \etal \cite{ojha2023towards}.}
    \label{tab:real-fake-accuracy}
\end{table}

This section presents a comprehensive comparison of SFLD against UnivFD.
\cref{tab:real-fake-accuracy} shows the classification accuracy of the prediction results of real and fake images on each generator in the conventional benchmark.
It is evident that SFLD exhibits superior performance in predicting generated images.
Notably, UnivFD is unable to predict fake images in some generated subsets, whereas SFLD demonstrates its strength in both real and generated images.
This result supports that SFLD can capture both low-level feature artifacts and high-level feature artifacts, making the detector better generalize on novel generators.


\subsection{Score Ensembling}
\label{sec:score-emsembling}
\textbf{Scatter plots.} 
Ensembling of the detection scores of the original image and patch-shuffled images is supported by \cref{fig:score-ensemble-viz}. In all cases, ensembling the two detectors with patch sizes 224 and 28 as an average of the two logit scores consistently improved binary separation and thus resulted in superior performance with the default threshold (as evidenced by \cref{tab:main-results-map,tab:main-results-acc}). This proves that the two detection methods work as complementary functions. 

\textbf{A closer look into failure cases.} 
\cref{fig:scatterplot-quadrants} visualizes some exact failure cases with StyleGAN-generated images (\cref{fig:scatterplot-stylegan}).
\cref{fig:scatterplot-quadrant2} shows a case where UnivFD fails and PatchShuffle succeeds. These images seem to cause UnivFD to fail because the high-level feature is well generated (high global structure fidelity). In contrast, PatchShuffle, which focuses on local structure, succeeds in detection. %Therefore, o
Our method with score ensembling was able to capture these examples illustrated as the green line in \cref{fig:score-ensemble-viz}.
On the other hand, \cref{fig:scatterplot-quadrant4} shows a case where PatchShuffle fails and UnivFD succeeds. These generated images have well-generated local structures like textures but have defects in global structures such as ears, eyes, and faces. However, there are very few examples corresponding to this. This analysis indicates that using both local and global information is necessary for detecting generated images.


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-stylegan-shuffle28.pdf}
        \subcaption{StyleGAN}
        \label{fig:scatterplot-stylegan}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-biggan-shuffle28.pdf}
        \subcaption{BigGAN}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-dalle-shuffle28.pdf}
        \subcaption{DALL-E}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/scatterplots/scatterplot-glide_50_27-shuffle28.pdf}
        \subcaption{GLIDE\_50\_27}
    \end{subfigure}
     \caption{Scatter plots of per-sample scores. X-axis is the UnivFD logits, and Y-axis is the logit from PatchShuffle with patch size 28. The decision boundary of UnivFD (\textcolor{red}{red}) and SFLD (\textcolor{green}{green}) are shown. See \cref{sec:additional-results-on-scatter-plots} for extended results.}
     \label{fig:score-ensemble-viz}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q2/001927.png}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q2/002665.png}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q2/011398.png}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q2/011634.png}
        \end{subfigure}
        \subcaption{Fake image examples on the second quadrant of \cref{fig:scatterplot-stylegan}.}
        \label{fig:scatterplot-quadrant2}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q4/073114.png}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q4/080002.png}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q4/096131.png}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figs/scatterplot-quadrants/q4/097426.png}
        \end{subfigure}
        \subcaption{Fake image examples on the fourth quadrant of \cref{fig:scatterplot-stylegan}.}
        \label{fig:scatterplot-quadrant4}
    \end{subfigure}
    \caption{A closer look into the failure cases from the StyleGAN-generated test images.}
    \label{fig:scatterplot-quadrants}
\end{figure}



\subsection{Robustness Against Image Degradation}
\label{sec:discussion_robustness}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/attack_robustness/gan_gaussian.pdf}
        \subcaption{\makecell{Gaussian blur, \\GANs from \cite{wang2020cnn}.}}
        \label{fig:gan_gaussian}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/attack_robustness/diff_gaussain.pdf}
        \subcaption{\makecell{Gaussian blur, \\DMs from \cite{ojha2023towards}.}}
        \label{fig:diff_gaussian}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/attack_robustness/gan_jpeg.pdf}
        \subcaption{\makecell{JPEG quality, \\GANs from \cite{wang2020cnn}.}}
        \label{fig:gan_jpeg}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/attack_robustness/diff_jpeg.pdf}
        \subcaption{\makecell{JPEG quality, \\DMs from \cite{ojha2023towards}.}}
        \label{fig:diff_jpeg}
    \end{subfigure}
    \caption{Robustness against simulated image degradation. Methods include Gaussian blur and JPEG compression.}
    \label{fig:robustness}
\end{figure}

Applying a Gaussian blur and JPEG compression to an image is a common degradation that can naturally occur. 
\cref{fig:robustness} illustrates the impact of each attack on two subsets of generated images.
The diffusion-subset and GAN-subset are subsets of diffusion and GAN generators, respectively, drawn from the conventional benchmark.
Gaussian indicates the addition of a Gaussian blur with a standard deviation of $\sigma$.
JPEG indicates the application of JPEG compression with a specified compression quality.
Note that JPEG compression with quality 100 does not result in the same image, as JPEG compression reduces color information and rounds coefficients, thereby losing some information.

If the model is vulnerable to image degradation, we can infer that it is influenced by the features targeted by the degradation.
Specifically, Gaussian blur affects both high- and low-level features in the image, while JPEG compression primarily targets low-level features (see \cref{fig:robustness_examples}).
\cref{fig:gan_gaussian,fig:diff_gaussian} demonstrates that SFLD always shows the best performance against Gaussian blur, since it integrates both high- and low-level features through ensemble/fusion, enabling each to compensate for the information lost in the other.
\cref{fig:gan_jpeg,fig:diff_jpeg} illustrates that SFLD restores robustness against JPEG compression, supporting the fundamental principle behind our model.
Additionally, UnivFD, which focuses on capturing high-level feature artifacts is also robust against JPEG compression.
However, NPR, which focuses on capturing low-level feature artifacts, is vulnerable to both Gaussian blur and JPEG compression even at JPEG compression quality 100.



\subsection{Qualitative Analysis}
\label{sec:qualitative-analysis}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/cam-univfd.pdf}
        \caption{UnivFD\cite{ojha2023towards} examples}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figs/cam-patchshuffle28.pdf}
        \caption{Patch-shuffle 28×28 examples}
        \label{fig:cam-viz-ps28x28}
    \end{subfigure}
    \caption{Class activation maps (CAM) for UnivFD\cite{ojha2023towards} and the patch-shuffled detector (ours). GradCAM\cite{selvaraju2020grad,jacobgilpytorchcam} was used to obtain the heatmaps. The ground truth real/fake labels and class labels are displayed on top of each image. Note that for \cref{fig:cam-viz-ps28x28}, the heat map is split into patches then reverse-shuffled back to the corresponding spatial location of the input image. }
    \label{fig:cam-viz}
\end{figure}

\textbf{GradCAM visualization.} 
See \cref{fig:cam-viz} for image attribution heat maps generated using GradCAM\cite{selvaraju2020grad,jacobgilpytorchcam}.
The examples are from the ProGAN test set.
In addition, the heat maps are averaged across ten predictions to reduce the randomness from the patch permutation. 
The CAM of UnivFD focuses on the class-dependent salient region, whereas the patch-shuffled detector focuses on the entire image region.

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/embeddings-univfd.pdf}
    \subcaption{Embeddings from UnivFD\cite{ojha2023towards,CLIP}}
  \end{subfigure}
  \begin{subfigure}[t]{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/embeddings-patchshuffle28.pdf}
    \subcaption{PatchShuffle(28) embeddings averaged across $N_{\text{views}}=10$ shuffles.}
  \end{subfigure}
  \caption{UMAP visualization\cite{SMG2020} of feature embeddings. Left and right plots show the same projected embeddings colored by real/fake labels (left) and object category labels (right). Our method destroys the class information from the embeddings, thereby improving the generalization by reducing the content bias.}
  \label{fig:embedding-visualization}
\end{figure}

\textbf{Feature visualization.}
Because taking an average of the logits generated via a linear layer is equivalent to taking an average of the feature embeddings, we can understand the SFLD embeddings by taking the average of the embeddings over multiple shuffles. \cref{fig:embedding-visualization} visualizes the feature embeddings by projecting onto a 2D plane using UMAP\cite{SMG2020}. We used the ProGAN test set to extract the embeddings. 

Because UnivFD learns the features directly from the CLIP visual encoder, the embeddings form class-dependent clusters. This creates class-dependent decision boundary, which may introduce unintended content bias to the real-fake detector. In contrast, because PatchShuffle destroys class-related information from the image, the corresponding embeddings show more dispersion within each class. 


\subsection{Effect of PatchShuffle Hyperparameters}

\textbf{Improving feature extraction with PatchShuffle.}
We suggest additional details to get better CLIP features from the shuffled images.
To improve stability against the randomness introduced by PatchShuffle, we use the averaged logits of $N_{views}=10$ randomly shuffled patch combinations for each input image during testing.

Moreover, in our problem setup, training images are fixed at 256×256 size, while test images can vary in size.
Resizing test images is avoided, as image degradation due to resizing (e.g., JPEG compression or blur) has been shown to impact the detection of AI-generated images negatively \cite{wang2020cnn}.
Instead, recent detectors\cite{tan2024rethinking, ojha2023towards} prefer cropping over resizing.
Our backbone model without PatchShuffle also extracts CLIP features from 224×224 center-cropped images without resizing. 
However, we can extract information not only from the center of the image but from the entire image by taking advantage of the proposed PatchShuffle, which allows non-consecutive patchwise combinations.
We divide the entire test image into non-overlapping patches of the given patch size and combine these patches into 224×224 images. This approach enables the detector to analyze information from the entire image, rather than being constrained to a single central region. 
See \cref{sec:Appendix-fullimagesampling} for more details.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/patch_size.pdf}
        \subcaption{Sweep over patch size}
        \label{fig:patch_size}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/view.pdf}
        \subcaption{Sweep over $N_{\text{views}}$}
        \label{fig:view}
    \end{subfigure}
    \caption{Best patch sizes were found at 28×28 and 56×56. $N_{\text{views}}=10$ showed the best balance between performance and inference cost.}
\end{figure}

\label{subsec:patchsetting}
\textbf{Patch size.}
The optimal patch size should be sufficiently small to disrupt the underlying image structure while preserving some high-level feature artifacts. The results for the performance difference according to patch sizes on a conventional benchmark are presented in \cref{fig:patch_size}. Each patch size model in x-axis refers to the ensemble between the corresponding PatchShuffle model and UnivFD(patch size 224). It can be observed that an too small patch size and an excessively large patch size do not assist the model in capturing useful high-level and low-level feature artifacts. Therefore, the majority of experiments in this paper utilized patch sizes 28x28 and 56x56 according to this result.


\textbf{Number of shuffled views.} 
To ensure the stability of the random patch shuffle, SFLD generates multiple versions of shuffled image from a single test image and employs the average of them as the score. As illustrated in \cref{fig:view}, mAP enhances with higher $N_{\text{views}}$. However, due to the tradeoff with inference time, we chose $N_{\text{views}}=10$, and all results presented in this paper were obtained with this setting. The results in \cref{fig:view} are from the PatchShuffle model with a patch size of 28, without an ensemble with UnivFD. The inference time was measured using RTX 4090 GPU.


\section{Conclusions}
\label{sec:conclusion}
In this paper, we introduced SFLD, a novel method for detecting AI-generated images that effectively combines global semantic structures and textural structures to improve detection performance. 
By leveraging random patch shuffling and an ensemble of classifiers trained on patches of varying sizes, our approach effectively addresses the shortcomings of existing methods, such as their content bias and susceptibility to image perturbations.
Also, We proposed a new quality-ensuring benchmark, TwinSynths. It is the first to consider a scenario of infinitely real-like fake images, providing a valuable resource for future research in this area. 
We demonstrated that SFLD outperforms SOTA methods in generalization to various generators, even in challenging scenarios simulated with TwinSynths.


\noindent
\textbf{Acknowledgements} This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2021-II212068, Artificial Intelligence Innovation Hub).