\section{Experiments}
\label{sec:experiments}

\begin{table*}[ht]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \resizebox{\textwidth}{!}{
        \input{tables/main-result-map}
    }
    \caption{Generalization performance on the conventional benchmark reported in AP. SFLD (224+28) indicates the ensemble of the classifier with patch sizes 224 and 28. And SFLD indicates the ensemble of the three classifiers with patch sizes 224, 56, and 28.}
    \label{tab:main-results-map}
\end{table*}

\begin{table*}[ht]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \resizebox{\textwidth}{!}{
        \input{tables/main-result-acc}
    }
    \caption{Generalization performance on the conventional benchmark reported in accuracy.}
    \label{tab:main-results-acc}
\end{table*}

\subsection{Settings}
\label{subsec:settings}


\textbf{Datasets}. 
Following the conventions of AI-generated image detection, all detectors were trained using the ForenSynths train set\cite{wang2020cnn}. This train set consists of real images used to train ProGAN\cite{karras2018progressive} and ProGAN-generated images. We evaluate the performance of SFLD on several benchmarks, including conventional benchmarks, TwinSynths, and low-level vision/perceptual loss benchmarks. For more detailed descriptions of the datasets and configurations used, please refer to \cref{supp:datasets}.

\textbf{Baseline methods}. 
We compare the performance of the proposed SFLD with existing AI-generated image detection methods. It includes CNNSpot\cite{wang2020cnn}, FreDect\cite{Frank}, GramNet\cite{liu2020global}, Fusing\cite{ju2022fusing}, LNP\cite{liu2022detecting}, LGrad\cite{Tan2023CVPR}, UnivFD\cite{ojha2023towards}, and NPR\cite{tan2024rethinking}. 
We conducted evaluations on the detection methods with our test dataset. 
The evaluation is done by the official models\cite{wang2020cnn, ojha2023towards}, re-implemented models\cite{Frank, liu2020global, ju2022fusing, liu2022detecting, Tan2023CVPR} by Zhong \etal\cite{rptc-AIGCDetection}, or trained model with the official codes using 20-classes train set \cite{tan2024rethinking}.

\textbf{Evaluation metrics}. 
We assess the performances of the detection models by average precision score (AP) and classification accuracy (Acc.), following previous works\cite{wang2020cnn, ojha2023towards, tan2024rethinking}. The AP metric is not dependent on the threshold value, whereas the Acc. is calculated with a fixed threshold of 0.5 across all generation models. 


\subsection{Results on Conventional Benchmark}
\cref{tab:main-results-map,tab:main-results-acc} shows the detection performance on conventional benchmarks in AP and Acc.
All baselines are trained on only the ProGAN train dataset consisting of 20 classes.
Higher performance is colored darker.
SFLD demonstrates robust and generalized performance across various generators in the benchmark. 
Note that SFLD achieves above 90.0 AP on every unseen generator. 
SFLD has an average of 98.43 AP, outperforming the best-performing baseline, UnivFD, by up to 2.14 in average. 
While for some tasks NPR has shown outperforming AP values in some generators, it has shown relatively low performance on some generators.
In this regard, we found that NPR is sensitive to some image degradation or different post-processing methods in different generative models, which limits its practicality.
Refer to \cref{sec:discussion_robustness} for further comparison of robustness on image degradation.

SFLD also exhibits state-of-the-art performance in classification accuracy. It performs particularly well on challenging datasets like DeepFake and ADM. On DeepFake, it improves accuracy from 74.6\% to 84.2\% (+9.6), and on ADM, from 79.5\% to 86.0\% (+6.5). These gains highlight its superior generalization in difficult scenarios.


\subsection{Analysis on TwinSynths} 
\label{sec:experiments_bench_ours}

\begin{table}[t]
    \centering
    \resizebox{0.8\linewidth}{!}{
        \input{tables/bench_ours}
    }
    \caption{Performance comparisons on TwinSynths. Values indicate AP score. \emph{DM} refers to diffusion model.}
    \label{tab:bench_ours}
\end{table}

\cref{tab:bench_ours} illustrates the detection performance on TwinSynths in AP.
The results demonstrate that SFLD is effective in TwinSynths while some detectors have shown a significant drop in performance.
Note that the TwinSynths focused on three key aspects: image quality, content preservation, and class diversity.
This suggests that the high performance on conventional benchmarks may not guarantee the detector's performance in real-world scenarios.

The results of TwinSynths allow an indirect analysis of the factors that the detectors focus on.
For convenience, we now define high-level features and low-level features.
high-level features are semantic information and their artifacts originate from distribution disparity between real images and generated images.
low-level features are texture information and their artifacts stem from the generator traces and image quality of generated images.
The TwinSynths-GAN preserves the content of the real image with minimal alteration, as the images are generated from a single real image.
This results in UnivFD, which captures high-level feature artifacts on the entire image, resulting in poor performance on the TwinSynths-GAN subset.
In contrast, NPR, which captures high-frequency artifacts in neighboring pixels, demonstrates better performance than UnivFD on the TwinSynths-GAN subset.
On the other hand, the generated images in TwinSynths-DM contain low-level discriminative features introduced by the DDIM decoder, which incorporates additional fully connected layers and post-processing steps following the upsampling blocks.
We can see that NPR exhibits lower performance, whereas UnivFD demonstrates higher performance.
Nevertheless, SFLD demonstrates superior and robust performance on both benchmarks, indicating its ability to capture both low-level feature artifacts and high-level feature artifacts.
Notably, no existing detector has ever exhibited such a high level of performance on both benchmarks.


\subsection{Low-level Vision and Perceptual Benchmark}
\label{sec:experiments_lowlevel}

\begin{table}[t]
    \centering
    \resizebox{0.8\linewidth}{!}{
        \input{tables/low-level-vision-tasks}
    }
    \caption{Low-level vision and perceptual benchmarks. Values indicate AP scores.}
    \label{tab:low-level-vision-tasks}
\end{table}

\cref{tab:low-level-vision-tasks} shows the detection performance on different benchmarks from ForenSynths\cite{wang2020cnn}.
Low-level vision models, including SITD and SAN, preserve high-level features of real images.
Perceptual models (CRN and IMLE) color semantically segmented images to match real images, preserving semantic information.
Notably, while NPR was able to detect some super-resolution images from SAN, it failed to perform well in other image-to-image translation tasks. This indicates that detectors specialized in identifying low-level feature artifacts from ProGAN struggle to generalize to images generated from different vision tasks. 
Conversely, a detector that focuses on high-level feature artifacts demonstrates strong performance on these benchmarks.
SFLD integrates semantic and structural information from different patch sizes to show superior performance on low-level vision and perceptual benchmarks.
