\section{Empirical Experiments}
\label{sec:experiments}
\subsection{Experimental Setup}
\label{subsec:experimental_setup}

\noindent\textbf{Benchmark Datasets.} In the absence of real-world HASNs, we propose four strategies for generating such graphs based on the three widely-used datasets (i.e., Cora, CiteSeer, and PubMed). Table \ref{dataset statistics} in Appendix \ref{sec: Basic Statistics of Network Datasets} shows some basic statistics of the three datasets for our experiments.

Initially, we generate a specific number of new AI nodes, such as $n\%$ 
(default set to 10\%) \footnote{The sensitivity to different AI ratios is investigated in Appendix \ref{Sensitivity to Different AI Ratios}.} of the total number of nodes in a real-world network dataset. Subsequently, we employ proposed generation strategies for inserting AI nodes into these networks, thereby constructing HASNs. Notably, after creating the HASNs using the proposed generation strategies, we anticipate the evolution of the social network over time, potentially resulting in the formation of new connections between individuals. Due to space limitations, we outline the four proposed generation strategies along with their specific assumptions \cite{nomiAI2024}\cite{zhang2024better}\cite{skjuve2021my}\cite{loveys2019reducing} and provide detailed descriptions of the evolution method for HASNs in Appendix \ref{sec: The Proposed Four Generation Strategies} and \ref{sec: The Evolution of HASNs}, respectively. Additionally, the validation of generation strategies using real-world HASNs is presented in Appendix \ref{Validation of Generation Strategies}.

\begingroup
\renewcommand{\arraystretch}{1}
\begin{table}[t]
\centering
\caption{The Q, HQ, HMR, and ADM results on the Cora dataset transformed into HASNs using the generation strategy (1), obtained by different AI node scoring methods used in CUSA, compared to no removal and all removal of AIs.}
\LARGE
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{AI Scoring} & \textbf{Q (↑)} & \textbf{HQ (↑)} & \textbf{HMR (↑)} & \textbf{ADM (↑)} \\
\midrule
No removal (N) & 0.7761 & 0.6966 & 42.25\% & 4.24 \\
All removal (A) & 0.8164 & 0.8164 & \underline{0}\% & \underline{0} \\
\midrule
EC & 0.8159 & 0.8092 & 20.34\% & 25.22 \\
BC & 0.8135 & 0.8051 & 27.96\% & 27.32 \\
CC & 0.8156 & 0.8040 & \textbf{58.80}\% & \textbf{43.62} \\
EC+BC & 0.8151 & 0.8069 & 25.63\% & 25.04 \\
EC+CC & 0.8135 & 0.8036 & 31.26\% & 22.58 \\
BC+CC & 0.8113 & 0.8020 & 38.24\% & 27.63 \\
EC+BC+CC & \textbf{0.8190} & \textbf{0.8174} & \underline{14.09}\% & \underline{20.41} \\
\bottomrule
\end{tabular}%
}
\label{AI node scoring methods}
\end{table}
\endgroup




\noindent\textbf{Comparative Methods.} We compare CUSA with four baselines: two traditional non-deep-learning community detection methods (Spectral and Louvain) and two GNN-based community detection methods (GCC and LSEnet).
\begin{enumerate}
    \renewcommand{\labelenumi}{(\theenumi)}
    \item Spectral \cite{amini2013pseudo} is a clustering algorithm based on the normalized Laplacian matrix and the regularized adjacency matrix to minimize ratio cut.
    \item Louvain \cite{blondel2008fast} detects communities by iteratively moving nodes to optimize modularity, maximizing links within communities and minimizing links between them.
    \item GCC \cite{fettal2022efficient} is a graph convolutional clustering method designed for efficient and scalable community detection.
    \item LSEnet \cite{sun2024lsenet} leverages structural embeddings for community detection, integrating node features via manifold-valued graph convolution in hyperbolic space.
    
\end{enumerate}
Each baseline is equipped with three AI node-handling strategies, namely, \underline{n}o removal of AIs (N), \underline{a}ll removal of AIs (A), and removal of certain AIs by GA\underline{D} (D). (\textit{cf.} Figure \ref{illustrative} (a)-(c))
\vspace{1em}

\noindent\textbf{Evaluation Metrics.} For the problem of \problem\ (\textit{cf.} Section \ref{subsec:objective_function}), we employ widely used modularity Q \cite{newman2004finding} and the proposed human-centric modularity HQ as our evaluation metrics to assess the effectiveness of CUSA. The Q accesses clustering quality based on cluster closeness, while HQ does so with minimal AI involvement. Higher Q and HQ (ranging from -0.5 to 1) values indicate better quality of the detected communities. In addition to clustering quality, we aim to understand the impact of remaining AI on community structures after applying CUSA. We propose two metrics: Human Migration Ratio (HMR) and Average AI-driven Migration (ADM). $\text{HMR(\%)} = \frac{\text{\# human node migration}}{\text{\# total human nodes}}$ measures the proportion of humans moving from one community to another, used to evaluate the overall extent of community changes. $\text{ADM} = \frac{\text{\# human node migration}}{\text{\# remaining AI nodes}}$ measures the average number of humans migrating due to remaining AI nodes, used to assess the average influence of AI nodes involved in the community. Higher HMR and ADM values indicate that the preserved AIs have greater capabilities to influence human communities. The next section presents the results, while additional experiments on CUSA scalability, HASNs without evolution, random networks, and real-world HASNs are included in Appendix \ref{sec: Supplementary Experimental Results} for further context.


\begin{table}[t]
\centering
\caption{The HQ results on different datasets each transformed into HASNs using the generation strategy (1), obtained by CUSA in comparison to that of four clustering baselines each equipped with no removal of AIs (N), all removal of AIs (A), and removal of AIs by GAD (D).}
\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{\textbf{Method}} & \textbf{Cora} & \textbf{CiteSeer} & \textbf{PubMed} \\ 
\hline
\multirow{3}{*}{Spectral \cite{amini2013pseudo}} 
 & N & 0.2907 & 0.2563 & 0.0245\\ 
 & A & 0.3721 & 0.2952 & 0.1394 \\ 
 & D & 0.3308 & 0.2642 & 0.0553 \\ 
 \hline
\multirow{3}{*}{Louvain \cite{blondel2008fast}} 
 & N & 0.6966 & 0.8047 & 0.6613 \\ 
 & A & 0.8164 & 0.8962 & 0.7690 \\  
 & D & 0.7702 & 0.8438 & 0.7245 \\ 
 \hline
\multirow{3}{*}{GCC \cite{fettal2022efficient}} 
 & N & 0.6724 & 0.6353 & 0.6218 \\ 
 & A & 0.7275 & 0.7110 & 0.6672 \\ 
 & D & 0.6910 & 0.6762 & 0.6483 \\ 
 \hline
\multirow{3}{*}{LSEnet \cite{sun2024lsenet}} 
 & N & 0.6628 & 0.6289 & 0.6182 \\ 
 & A & 0.7202 & 0.7089 & 0.6490 \\ 
 & D & 0.6984 & 0.6720 & 0.6378 \\ 
 \hline
\textbf{CUSA} (ours) & - & \textbf{0.8174} & \textbf{0.8984} & \textbf{0.7716} \\ 
\bottomrule
\end{tabular}%
}
\label{The HQ results on different datasets}
\end{table}

\begingroup
\renewcommand{\arraystretch}{1}
\begin{table*}[t]
\centering
\caption{The Q and HQ results on the Cora dataset transformed into HASNs using different generation strategies, obtained by CUSA in comparison to that of four clustering baselines each equipped with no removal of AIs (N), all removal of AIs (A), and removal of AIs by GAD (D).}
\label{The Q and HQ results on the Cora dataset}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{\makebox[0.15\textwidth]{\textbf{Method}}} & \multirow{2}{*}{} & \multicolumn{2}{c}{\textbf{\textit{k\%} random insertion}} & \multicolumn{2}{c}{\textbf{Inverse degree insertion}} & \multicolumn{2}{c}{\textbf{Inner and outer AI mix}} & \multicolumn{2}{c}{\textbf{AI with dual personality}} \\
\cline{3-10}
& & \textbf{Q (↑)} & \textbf{HQ (↑)} & \textbf{Q (↑)} & \textbf{HQ (↑)} & \textbf{Q (↑)} & \textbf{HQ (↑)} & \textbf{Q (↑)} & \textbf{HQ (↑)} \\
\hline
\multirow{3}{*}{\makebox[0.15\textwidth]{Spectral \cite{amini2013pseudo}}} 
& N & 0.3087 & 0.2907 & 0.3878 & 0.3733 & 0.3013 & 0.2833 & 0.3206 & 0.3028 \\
& A & 0.3721 & 0.3721 & 0.3958 & 0.3958 & 0.3553 & 0.3552 & 0.3594 & 0.3594 \\
& D & 0.3378 & 0.3308 & 0.3902 & 0.3818 & 0.3148 & 0.3066 & 0.3334 & 0.3240 \\
\hline
\multirow{3}{*}{\makebox[0.15\textwidth]{Louvain \cite{blondel2008fast}}} 
& N & 0.7761  & 0.6866 & 0.8123 & 0.7561 & 0.8265 & 0.7994 & 0.8094 & 0.7688 \\
& A & 0.8164 & 0.8164 & 0.8235 & 0.8235 & 0.8214 & 0.8214 & 0.8202 & 0.8202 \\
& D & 0.7990 & 0.7702 & 0.8182 & 0.7895 & 0.8238 & 0.8070 & 0.8152 & 0.7938 \\
\hline
\multirow{3}{*}{\makebox[0.15\textwidth]{GCC \cite{fettal2022efficient}}} 
& N & 0.6612 & 0.6243 & 0.7032 &0.6795 & 0.7186 & 0.6824 & 0.7218 & 0.6920 \\
& A & 0.7275 & 0.7275 & 0.7462 & 0.7462 & 0.7523 & 0.7523 & 0.7508 & 0.7508 \\
& D & 0.6846 & 0.6731 & 0.7315 & 0.7168 & 0.7420 & 0.7325 & 0.7436 & 0.7296 \\
\hline
\multirow{3}{*}{\makebox[0.15\textwidth]{LSEnet \cite{sun2024lsenet}}} 
& N & 0.6973 & 0.6342 & 0.7284 & 0.6857 & 0.7514 & 0.7269 & 0.7213 & 0.7029 \\
& A & 0.7202 & 0.7202 & 0.7423  & 0.7423 & 0.7558  & 0.7558 & 0.7412 & 0.7412 \\
& D & 0.7023 & 0.6876 & 0.7370 & 0.7152 & 0.7520 & 0.7325 & 0.7364 & 0.7283 \\
\hline
\multirow{1}{*}{\makebox[0.15\textwidth]{\textbf{CUSA} (ours)}} 
& - & \textbf{0.8190} & \textbf{0.8174} & \textbf{0.8251} & \textbf{0.8245} & \textbf{0.8310} & \textbf{0.8224} & \textbf{0.8214} & \textbf{0.8211} \\
\bottomrule
\end{tabular}%
}
\end{table*}
\endgroup

\subsection{Experimental Results}
\subsubsection{Comparison of Different AI Node Scoring Methods Used in CUSA} In the first set of experiments, we assess the performance of different AI scoring methods used in CUSA compared to standard Louvain clustering \cite{blondel2008fast} without AI removal and with all AI removal, when clustering on the Cora dataset transformed into HASNs using the generation strategy (1). We test the three different scoring methods, eigenvector centrality (EC), betweenness centrality (BC), and clustering coefficient (CC), as well as their combination scores (denoted by "+" for the linear combination of normalized scores). The corresponding results are presented in Table \ref{AI node scoring methods} \footnote{The HMR and ADM results on different datasets are examined in Appendix \ref{HMR and ADM Results on Different Datasets}.}. Inspection of Table \ref{AI node scoring methods} reveals three noteworthy points. First, all seven AI scoring methods (below the third line) used in CUSA outperform the standard Louvain clustering with no AI removal, as shown by higher Q and HQ scores, demonstrating their better ability to capture structural nuances in HASNs. Second, the EC+BC+CC stands out in performance, achieving a 5.3\% and 17.6\% relative gain in Q and HQ, respectively, compared to clustering with no AI removal. Moreover, the HMR and ADM of the baseline method with all AI removal are 0\% and 0, respectively, whereas our CUSA achieves 14.09\% and 20.41. These suggest that EC, BC, and CC capture different aspects of information about AI nodes. Third, CC excels in HMR and ADM, indicating that CC may serve as a bridge for potential communities. In the following experiments, we adopt EC+BC+CC which achieves the best Q and HQ scores, as our AI scoring in CUSA (i.e., line 3 of Algorithm \ref{alg:CUSA}) for comparison with the baselines \footnote{Due to the randomness in HASNs generated by our strategies, each experiment was run 100 times, and the results were averaged for reliability.}.

\subsubsection{Comparison of CUSA with Baselines on Different Datasets} In the second set of experiments, we compare CUSA against four baselines: two traditional non-deep learning methods, Spectral \cite{amini2013pseudo} and Louvain \cite{blondel2008fast}, and two GNN-based methods, GCC \cite{fettal2022efficient} and LSEnet \cite{sun2024lsenet}. Evaluations are conducted on three datasets (Cora, CiteSeer, and PubMed), each transformed into HASNs using the proposed generation strategy (1). Notably, we equip each baseline with three AI node-handling strategies: \underline{n}o removal of AIs (N), \underline{a}ll removal of AIs (A), and removal of certain AIs by GA\underline{D} techniques (D) (\textit{cf.} Figure \ref{illustrative}. (a)-(c)). As to the GAD method, we adopt the recently proposed DCI method \cite{wang2021decoupling}, which leverages self-supervised learning to decouple node representation learning from classification, to detect anomalous AI nodes in an HASN. Two important observations can be drawn from Table \ref{The HQ results on different datasets} which presents the human-centric HQ scores (modified from standard modularity Q in Section \ref{subsec:objective_function}) of the resulting clusters.
First, across the three datasets, each baseline achieves the highest HQ when all AI nodes are removed, followed by GAD-based removal, while the worst occurs with no AI removal. Second, CUSA significantly outperforms baselines across three datasets by adaptively balancing AI removal and community integrity, leading to better clustering results. In contrast, baselines struggle to handle the interweaving of humans and AIs.

\subsubsection{Comparison of CUSA with Baselines on a Dataset using Different Generation Strategies} To further validate the superiority and feasibility of CUSA, we evaluate its performance on the Cora dataset, transformed into four HASNs using the proposed generation strategies: $k\%$ random insertion, inverse degree insertion, inner and outer AI mix, and AI with dual personality (\textit{cf.} Appendix \ref{sec: The Proposed HASN Scenarios}). Each AI-augmented Cora graph evolves through link prediction based on Jaccard similarity \cite{arrar2024comprehensive}\cite{daud2020applications}. We compare CUSA against four baselines, each equipped with three AI node-handling strategies: \underline{n}o removal of AIs (N), \underline{a}ll removal of AIs (A), and removal of certain AIs by GA\underline{D} (D). We present the two evaluation metrics standard Q and human-centric HQ (\textit{cf.} Section \ref{subsec:objective_function}). The corresponding results are shown in Table \ref{The Q and HQ results on the Cora dataset}, from which we can draw three important observations. First, across four HASN scenarios, each baseline obtains the highest Q and HQ when all AI nodes are removed, followed by GAD-based removal, with the worst performance when no AI nodes are removed. Second, CUSA outperforms all baselines, achieving an average relative gain of 21.8\% in Q and HQ (e.g., inverse degree insertion with full AI removal). These indeed demonstrate the efficacy of AI-aware clustering techniques for dealing with \problem\ problem since they identify and preserve AI nodes that can potentially reshape new communities and enhance human closeness. Third, as we can see, the elaborate generation strategies of the latter three result in clusters with significantly better performance compared to random insertion ($k\%$ random insertion). This also suggests that industries generating AIs can use these strategies to enhance human closeness and discover potential communities for such emerging social networks (HASNs) in the future.






















