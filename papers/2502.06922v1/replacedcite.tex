\section{Related Work}
\label{sec:prev}
To the best of our knowledge, we are the first to present a multimodal text and audio framework with synthetic audio data from TTS systems for cognitive state tasks.
However, regarding experiments with audio signal, there has been previous work on multimodal (text and audio) and unimodal (audio only) models for corpora in emotion, belief, deception, and sentiment. 

\paragraph{Multimodal} There has been some work in fusing text and audio features for cognitive state tasks, specifically in emotion and belief. In emotion, 
____ present a novel architecture containing a refined attention mechanism, a novel perception unit aligning the emotion frame to the global audio context, and a new convolution procedure to effectively fuse audio and text features.
____ fine-tune BERT ____ and fuse with ASR features derived from speech, achieving state-of-the-art results on the multimodal emotion corpus IEMOCAP ____, which we also test our SAD framework on.
In the multimodal belief prediction task, ____ were the first to show that fusing text with audio features helps, achieving state-of-the-art results on the CB-Prosody corpus ____. 

Regarding deception, there has been previous work on acoustic and lexical approaches.  Testing on the CXD corpus ____, ____ show that a multimodal architecture boosts performance compared to a unimodal text-only approach.

\paragraph{Audio Only} There has also been work focusing on the audio-only modality for cognitive state tasks, but considerably less than multimodal. For the deception detection task, ____ focus on training classical machine learning methods with acoustic and prosodic features. Regarding emotion detection, ____ were the first to fine-tune a pre-trained speech model for emotion detection, specifically Wav2Vec2.0 ____.