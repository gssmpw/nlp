\section{Related work}
\textbf{Efficient and Heterogeneity-aware Distributed Training.} 
There have been several works to improve (communication) efficiency of LLM training____
where they show that the communication overhead can be significantly reduced by minimizing the synchronization for gradients in DP.
Moreover, there are several heterogeneity-aware scheduling methods____ proposing efficient DP and PP arrangement of the nodes to minimize the communication overhead. 
Yet, pipelining is always done in a sequential execution of all layers____.
To the best of our knowledge, no prior work has studied the opportunity of optimizing for partial pipeline usage.




\textbf{Skip Connections and Early Exit.} Models employing skip connections have been known to exhibit robustness to random layer omission and perturbation ____. Works such as ____ demonstrated how larger models can be trained with less resources, by skipping certain layers during training. LayerDrop____ demonstrated that models trained partially are more robust to layer omission during inference. Based on this work, Layerskip____ proposed a novel training approach and loss function, which enabled them to perform early exiting during inference - running only part of the model to generate tokens and using the whole model only to verify their probability.