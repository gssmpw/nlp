
\section{System Setting}
In this section, we explain the distributed training environment and the terminology used throughout the paper.

\textbf{System setup.} There are $\mathcal{N}$ distributed nodes for training an LLM model of \(\mathcal{L}\) layers, which is divided in pipeline stages $\mathcal{S}:=(S_0, S_1,\ldots,S_s)$. 
Each stage \(S_i\) holds an (equal)\footnote{Not necessary for our solution, but for simplicity and clarity we focus on the homogeneous stage/node setting.} number of consecutive  layers \(L_j...L_{j+\delta}\) and there are no overlapping layers across stages. 


We assume each node has the same memory capacity that allows them to operate the same number of microbatches. We assume each node can communicate with any other and the communication cost between nodes is modelled with $(\mathcal{B},\Lambda)$ matrices where communication between nodes \(N_i\) and \(N_j\) has a cost associated to it modelled by the latency \(\lambda_{i,j} \in \Lambda\) and bandwidth \(\beta_{i,j} \in \mathcal{B}\). Thus for a message of size $|msg|$, its communication takes \(\lambda_{i,j} + \frac{|msg|}{\beta_{i,j}}\) (mili)seconds. While communication may not be symmetric, since each link is used twice, once during forward and once during backward, we model latencies and bandwidth as the average of the two directions (e.g., \(\lambda'_{i,j} = \frac{\lambda_{i,j} + \lambda_{j,i}}{2}\)), as in \cite{dtfm}. 


\textbf{Distributed Training.} Each node is mapped to a single LLM stage. 
To train the LLM with data and pipeline parallelism, a batch is split into multiple microbatches, which perform forward and backward passes through each stage. 
Nodes sharing the same stage communicate the gradient updates in DP, and nodes in different stages communicate activations in PP.

We consider synchronous updates in pipelining, i.e., the weight update of an iteration is done after all the corresponding microbatches are processed. 
However, unlike common pipelining where each microbatch passes through all stages in the sequential order, we propose partial and reordered pipelining which is explained below.



\textbf{Partial and Reordered Pipeline.} 
The prior work pinpointed that transformer-based architectures are robust to layer skipping, i.e., not executing a given layer ~\cite{vitrob,LayerDrop}. We investigate if stage skipping is also advantageous in pipeline parallelism. We term such an idea partial pipeline parallelism. In the full pipeline scenario, microbatches traverse through the stages sequentially, e.g. $\mathcal{S}:=(S_0, S_1,S_2,S_3,S_4,S_5)$. In our case microbatches can traverse through different sequence of stages, due to skipping a given stage ($\mathcal{S}:=(S_0,S_1,S_4,S_6)$) or swapping the order of two stages ($\mathcal{S}:=(S_0,S_1,S_3,S_2)$). The key research questions thus are which stages to skip such that negligible performance loss occurs to the LLM, while minimising the training time, by choosing faster and shorter paths through the system.



