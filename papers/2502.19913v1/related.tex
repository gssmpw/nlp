
\section{Related work}
\textbf{Efficient and Heterogeneity-aware Distributed Training.} 
There have been several works to improve (communication) efficiency of LLM training~\cite{diloco,demo,opendiloco}
where they show that the communication overhead can be significantly reduced by minimizing the synchronization for gradients in DP.
Moreover, there are several heterogeneity-aware scheduling methods~\cite{dtfm,swarm,hetpipe,metis,flashflex} proposing efficient DP and PP arrangement of the nodes to minimize the communication overhead. 
Yet, pipelining is always done in a sequential execution of all layers~\cite{gpipe,zb,pipedream,hetpipe}.
To the best of our knowledge, no prior work has studied the opportunity of optimizing for partial pipeline usage.




\textbf{Skip Connections and Early Exit.} Models employing skip connections have been known to exhibit robustness to random layer omission and perturbation \cite{skipconnresnet,vitrob}. Works such as \cite{stochasticdepth} demonstrated how larger models can be trained with less resources, by skipping certain layers during training. LayerDrop~\cite{LayerDrop} demonstrated that models trained partially are more robust to layer omission during inference. Based on this work, Layerskip~\cite{LayerSkip} proposed a novel training approach and loss function, which enabled them to perform early exiting during inference - running only part of the model to generate tokens and using the whole model only to verify their probability. 













