\section{Introduction}
Deep transformer-based architectures~\cite{transformer} have recently enabled unprecedented performance on complex language and cognitive tasks~\cite{gpt}. 
These leaps can be explained by the ever growing corpora of available data and by the increasing size of (Large) Language Models (LLMs)~\cite{llama,gpt3,palm,BERT,megatron}. As a consequence, models are now too large to fit and be efficiently trained on a single GPU. 



Distributed training techniques, such as Pipeline Parallelism (PP) and Data Parallelism (DP), become indispensable to efficiently  train large models on distributed nodes.\footnote{Here, nodes are also referred as devices or GPUs.} In the former the model is split in stages, containing non-overlapping sections of the model, across a set of nodes, which communicate sequentially between each other to run the whole model, thus forming a pipeline. In the latter, multiple pipelines train the model independently on different data batches, communicating between each other to synchronize the model weights after an update. Training with the standard synchronous algorithms and renting private clusters to train models can easily cost more than tens of thousands of dollars~\cite{dtfm}, even for smaller models. Some prior work has proposed training on smaller clusters over a heterogeneous network (different communication latency and bandwidth between nodes), however in such a setting the communication between the GPUs is still  one of the main limiting factors~\cite{dtfm}.


Recent work has aimed to improve cost effectiveness of LLM training via less frequent synchronizations within DP~\cite{diloco,demo,opendiloco} and heterogeneity-aware arrangement of the GPUs/clusters ~\cite{dtfm,swarm,hetpipe,metis,flashflex}.
The former, Diloco~\cite{diloco} and DeMo~\cite{demo}, show that the synchronization for gradients can be reduced by orders of magnitude. The latter, heterogeneity-aware scheduling methods~\cite{dtfm,swarm,hetpipe,metis,flashflex}, present efficient arrangement of the GPUs to minimize the communication overhead in heterogeneous network settings. 
Yet, pipelining is done strictly following a sequential execution of layers from beginning to the end for all microbatches~\cite{gpipe,zb,dtfm,hetpipe}.

 \begin{figure*}[t]
    \centering
\includegraphics[width=2\columnwidth]{figs/main_fig.png}
      \caption{An example of   
      partial pipeline parallelism scheduling where each colored (solid or dashed) path represents a different microbatch. Each node in stage 0 sends out 2 microbatches, the first in solid, the second in dashed. Green backgrounds show the forward pass, while light orange - the backwards pass. For better visualisation, we omit the loss and deembedding computations. We place arrows to show the prioritisation of the microbatches from forward to backward pass within the same node. An example of a collision can be seen on node 7 during the forward pass, which subsequently delays the execution of the solid blue microbatch because of the dashed yellow microbatch.}   \label{fig:highlevelourprotocol}
\end{figure*}

The works of \cite{vitrob,LayerDrop,LayerSkip} have demonstrated transformer architectures' robustness against layer skipping and even layer reordering during training and inference. We leverage this fact to propose a novel optimisation to traditional training - \sys, which is the first partial pipeline framework that skips (and re-orders) pipeline stages. \sys improves the training throughput measured in time per iteration while preserving convergence of the model on distributed nodes and it is also suitable for communication heterogeneous settings. Moreover, the partial training via stage skipping in \sys also improves the inference with layer/stage skips, which is beneficial for not only the early-exit inference methods, but also fault-tolerant ones.


To minimize the end-to-end training latency via stage skipping and reordering, \sys is composed of two modules: allocating nodes to pipeline stages, and a path scheduler for microbatches.
For a given (heterogeneous) network of nodes and pipeline stages of an LLM model, \sys first allocates nodes to the stages where nodes in the same stage communicate in DP manner and nodes in different stages communicate in PP. Then, differently from standard pipelining where each microbatch passes through all stages sequentially along the same path, \sys 
schedules partial paths for each microbatch that skip some stages and run others out of order. 
As illustrated in Figure~\ref{fig:highlevelourprotocol}, each microbatch skips \(k\%\) of the model where \(k\) is a user-defined parameter.

The key challenge is how to select the path such that the number of microbatch collisions is minimised and the model convergence is not affected negatively.



Our contributions can be summarised as follows: 
\begin{itemize}
    \item We propose a novel and effective partial and reordered pipelining framework for distributed LLM training to reduce the computation and communication overhead.
    \item We design a pipeline execution scheduler optimising the throughput for heterogenous network of nodes by utilising skipping and swapping stages and reducing collisions (overlapping microbatches executions). 
    \item We evaluate our scheduler and present up to \(60\%\) reduction in training time when training with \sys compared to training with a standard full-model framework in a heterogeneous network. Also, we show that there is no convergence degradation.
    \item We show that the models trained with \sys also provide significant resistance to layer omission during inference, e.g., with a perplexity drop off of only 7\% when executing half the model.
\end{itemize}
