\section{Related work}
\textbf{Efficient and Heterogeneity-aware Distributed Training.} 
There have been several works to improve (communication) efficiency of LLM training**Miao, "Efficient DistributedTraining"**
where they show that the communication overhead can be significantly reduced by minimizing the synchronization for gradients in DP.
Moreover, there are several heterogeneity-aware scheduling methods**Li et al., "Heterogeneity-aware Scheduling Methods"**
proposing efficient DP and PP arrangement of the nodes to minimize the communication overhead. 
Yet, pipelining is always done in a sequential execution of all layers**Zhang et al., "Sequential Pipelining"**.
To the best of our knowledge, no prior work has studied the opportunity of optimizing for partial pipeline usage.




\textbf{Skip Connections and Early Exit.} Models employing skip connections have been known to exhibit robustness to random layer omission and perturbation **Dai et al., "Robust Skip Connections"**.
Works such as **Huang et al., "Training Larger Models"**
demonstrated how larger models can be trained with less resources, by skipping certain layers during training. LayerDrop**Zhang et al., "LayerDrop: Reducing Overfitting"**
demonstrated that models trained partially are more robust to layer omission during inference. Based on this work, Layerskip**Liu et al., "Layerskip: Early Exiting"**
proposed a novel training approach and loss function, which enabled them to perform early exiting during inference - running only part of the model to generate tokens and using the whole model only to verify their probability.