
\section{Experimental Results of \sys}



We demonstrate that \sys provides significant improvement in the training throughput, while preserving convergence even when a model is trained partially. We evaluate several LLaMa-like models, ranging from 500M to 8B in a geo-distributed setting and we use RedPajamas~\cite{redpajama} and TOPv2 datasets~\cite{topv2}. We observe that using \sys, the models converge at the same rate but with a significantly higher throughput, meaning that training converges much faster in wall-clock time.

We present our experiments in two categories: throughput and convergence analysis.
Throughput experiments investigate the speed up of our partial pipeline scheduler \sys wrt. the baseline SOTA schedulers on a LLaMa-1.5B model. 
In convergence experiments, we analyse the convergence of training from scratch of LLaMa-500M model and parameter efficient finetuning of LLaMa-8B model with three skipping ratios: 0\%, 25\% and 33\%.

Code is available at \url{https://github.com/gensyn-ai/skippipe}, which utilises DecCom,\footnote{\url{https://github.com/NikolayBlagoev/DecCom-Python}} a modular stack framework for decentralised communication.


\subsection{Throughput}
We evaluate throughput improvement of our algorithm by measuring the end to end time pipeline training of an iteration.
We test the throughput of a LLaMa-1.5B model training (see Appendix \ref{appendix:mc} for architecture details) with 3 different skipping ratios (0\%, 25\% and 33\%) and different number of nodes. 
We utilise H100s to simulate the nodes for our measurement where we host several homogeneous nodes within the single GPU.
For simulating geo-distributed nodes, we utilise the bandwidth and latency values given in DT-FM~\cite{dtfm}.

In Figure~\ref{fig:throughput}, we present the experimental results for two skip percentages ($k:=$25\% and 33\%) and 4 different schedulers. 
Also, we use varying number of samples per microbatch - of 1, 2 and 4, and make use of gradient accumulation.
We compare our scheduler, \sys, with (i) DT-FM: 0\% skip training using DT-FM scheduler, (ii) DT-FM-skip: $k$\% skip training using DT-FM scheduler with additional constraints (see Appendix \ref{qrps}), (iii) \sys (no \tc{2}): $k$\% skip training using our scheduler \sys where the collision constraint \tc{2} is ignored.
The time per iteration values are averaged over 50 different (randomly sampled) bandwidth and latency values.
Since we optimise the pipelining schedule for a given node/stage allocation, we measure the pipelining time and omit the data parallelism part where weight aggregation happens because the aggregation time is the same for a fixed node/stage allocation regardless of the microbatch paths. 
Finally, we perform one warm-up iteration where nodes discover each other. 

In Figure~\ref{fig:throughput_first}, we have the results for 25\% skip case. We tested 4 stages with 18 nodes where the nodes are distributed to the stages according to Equation~\ref{eqn:node_distribution}: $(6,4,4,4)$, except the 0\% skipping case used in DT-FM baseline.
To keep the node/stage sizes the same, for the DT-FM baseline, we use 16 nodes where nodes are equally distributed $(4,4,4,4)$.
To (over)compensate the baseline case using less nodes, we project their performance by proportionally reducing the end-to-end latency. 
Specifically, we multiply the latency of baseline by $\frac{16}{18}$, and these compensated latency results are represented by DT-FM$^*$.
Note that considering the communication of those additional nodes being ignored, this is an upper bound of their performance. 
As seen~\ref{fig:throughput_first}, \sys achieves \(40-50\%\) improvement compared to the baseline in the 8K and 16K tokens case.


In Figure~\ref{fig:throughput_second}, we have the results for 33\% skip case where we tested 6 stages with 20 nodes.
Similarly to the above case, number of nodes per stage is $(5,3,3,3,3,3)$, except the baseline, which is compensated by multiplying the corresponding latency values with $\frac{18}{20}$. 
We observe a consistent speedup of \(50\%\) compared to DT-FM$^*$, and even a \(55\%\) speed up in the \(16K\) tokens per microbatch. Additionally, removal of collisions provides a speedup of \(10\%\). 



\begin{figure}[tp!]
 \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/25_skip.pdf}
        \caption{Heterogeneous setting with 18 nodes and 25\% skip rate.}
        \label{fig:throughput_first}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/33_skip.pdf}
        \caption{Heterogeneous setting with 20 nodes and 33\% skip rate.}
        \label{fig:throughput_second}
    \end{subfigure}
    \hfill
    \caption{Time per iteration with different strategies. We analyse four schedulers with two skip percentages (25\% and 33\%) and three token numbers (4K, 8K and 16K). \sys is compared with: DT-FM$^*$ representing the compensated results for the baseline with no skips, DT-FM-skip uses node arrangement of DT-FM and skips $k\%$ with additional constraints (see Appendix \ref{qrps}), \sys (no \tc{2}) is our scheduler without \tc{2}. }\label{fig:throughput}
\end{figure}




\subsection{Convergence}
\label{sec:convg}
With convergence experiments, we show that our scheduler \sys does not degrade the convergence of the training compared to no skipping case. We verify this by training from scratch a LLaMa-500M on the RedPajamas data \cite{redpajama} and finetuning LLaMa-8B model~\cite{llama8b} with LoRA~\cite{lora} on the TOPv2 dataset~\cite{topv2} with three different skip rates - 0\% (baseline), \(25\%\), and \(33\%\) skips.


In Figure~\ref{fig:convg_ft}, we report the validation loss every 50th iteration by running the entire model convergence (regardless of the training schedule). Our experiments show that \sys achieves similar convergence to the baseline for both training (see Figure~\ref{fig:conv_first}) and finetuning (see Figure~\ref{fig:conv_second}), despite training with a fraction of the model each time. Also, since \sys has a much higher throughput, convergence in terms of wall-clock time is significantly faster.

\begin{figure}[tp!]
 \begin{subfigure}{0.235\textwidth}    \includegraphics[width=\textwidth]{figs/convergence.pdf}
        \caption{Training LLaMa-500M model with RedPajamas.}
        \label{fig:conv_first}
     \end{subfigure}
\hfill
 \begin{subfigure}{0.235\textwidth}
 \includegraphics[width=\textwidth]{figs/convergence_ft.pdf}
        \caption{Finetuning LLaMa-8B model with TOPv2.}
        \label{fig:conv_second}
    \end{subfigure}
    \caption{Convergence of validation loss with \(33\%\) skip rate, \(25\%\) skip rate, and \(0\%\) skips (full model).}
    \label{fig:convg_ft} 
\end{figure}



\section{Inference Benefits of \sys Training}
\par Training with partial pipelines results in models with inference robustness - they are resistant to a certain degree of layer/stage removal in inference, without sacrificing performance. We demonstrate this property in two settings: early exit where we employ self-speculative decoding to perform inference on the middle layer and fault tolerant inference where we test the inference pipeline with failing nodes.

\subsection{Early Exit}

\begin{table}[b]
\caption{Results for LLaMa-500M with various inference strategies. We report the ratio of accepted tokens at the middle layer and the relative speed up achieved with early exit.}
\label{table:resultsoptimal}
\begin{center}
\begin{threeparttable}
\begin{tabular}[H]{p{2.0cm} p{2.6cm} p{2.0cm}}
% {|m{5em}|m{2cm}|m{3cm}|} 
% \caption{Which cost dominates in training}
  % \hline
  \toprule
   & Token Acceptance  & Speedup    \\ 
  % \hline
  \midrule
   LayerSkip\tnote{a} & \(77.4\%\) & \textbf{1.76} \\ 
  %\hline
  \sys & \(62.8\%\) & 1.41 \\ 
  \bottomrule
\end{tabular}
{\small
\begin{tablenotes}
\item [a] Reported results in~\cite{LayerSkip} for LLaMa-1.5B.
\end{tablenotes}
}
\end{threeparttable}
\end{center}
\end{table}


\begin{table*}[ht]
  \caption{Perplexity (lower is better) on Arxiv dataset across 1000 evaluation samples for various inference and training skip rates. The inference (training) skip rate shows the percentage of stages being skipped during inference (training).}
  \label{table:ape}
  \begin{center}
  \begin{threeparttable}
    \begin{tabular}[H]{p{3cm}  p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm}}
      \toprule
   Inference skip rate   & \multicolumn{3}{c}{0\%} & \multicolumn{3}{c}{25\%} & \multicolumn{3}{c}{33\%}   & \multicolumn{3}{c}{50\%}\\
   %\hline
    Training skip rate & 0\% & 25\% & 33\%    & 0\% & 25\% & 33\% & 0\% & 25\% & 33\% & 0\% & 25\% & 33\% \\  
    %\hline
    \midrule
\textbf{Arxiv} $\downarrow$ &    8.59 & \textbf{8.32} & 8.96 &    29.58 & \textbf{8.5} & 9.8\tnote{a} &     33.35 & 10.57\tnote{a} & \textbf{9.0} &     81 & 9.9  & \textbf{9.57}\\

\bottomrule
    \end{tabular}
    \begin{tablenotes}
\item [a] Partial stage skips where number of stages is not divisible by the desired skip ratio.
\end{tablenotes}
\end{threeparttable}
  \end{center}
\end{table*}

\par Using our LLaMa-500M model trained with \(33\%\) skip rate, we employ self-speculative decoding strategies as in~\cite{LayerSkip}. To this end, during inference, we generate a number of draft tokens \(T_1, T_2, ...\) by stopping at the middle stage. These are then verified in a single pass by the remainder of the model with the middle stage's logits of the last draft token fed into the remaining stages. All tokens up to the first one that doesn't match between the draft tokens and the verified ones are kept. The first mismatched token is added to the prompt and generation continues from there. We compare the performance of our strategy against LayerSkip~\cite{LayerSkip}. Results are presented in Table~\ref{table:resultsoptimal}. 
We achieve 1.41 speedup for LLaMa-500M model without manipulating the loss function, whereas LayerSkip achieves 1.76 speedup for LLaMa-1.5B model by applying early exit loss function. 




\subsection{Fault Tolerant Inference}

By training with \sys, the models exhibit robust inference results even if some stages are failed (except the first one). We demonstrate this by evaluating the perplexity of the trained Llama-500M models (in Section \ref{sec:convg}) given different inference stage skip rates on the Arxiv \cite{arxiv} dataset. For each skip rate a corresponding number of stages is dropped at random per sample. For the cases where the number of stages is not divisible by the desired skip ratio, we allow partial stage skips (executing a subset of the layers on the stage - the first half). 

As seen in Table \ref{table:ape}, our partial pipelining provides robustness against arbitrary stage removal during inference time. 
Overall, we observe relatively low perplexity values for the chosen dataset, as the models primarily trained on the Arxiv subset of the RedPajamas dataset.
Nonetheless, perplexities of the models trained with \sys stays lower than 10, whereas the baseline increases to 81 when half model is executed. 
Interestingly, we observe that when we perform partial stage skips, performance degrades more significantly. This suggests that layers exhibit a degree of co-learning. Thus it is possible to provide even stronger robustness by allowing for stages to be executed partially during training. 






