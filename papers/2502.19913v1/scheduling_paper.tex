%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}

%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
 %\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{color}
\usepackage{xspace}
%\usepackage{algcompatible}
\usepackage{algorithm,algorithmic}
\usepackage{threeparttable}
\usepackage{breakurl}

\def\UrlBreaks{\do\/\do-}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newif\ifcomm
\commtrue
%\commfalse


\newcommand{\lc}[1]{{\color{magenta} \ifcomm \texttt{} Lydia: #1 \fi}}
\newcommand{\NB}[1]{{\color{blue} \ifcomm \texttt{} Nick: #1 \fi}}
\newcommand{\onote}[1]{{\color{teal} \ifcomm \texttt{} Ozzy: #1 \fi}}

\newcommand{\sys}{\texttt{SkipPipe}\xspace}
\newcommand{\cc}[1]{\texttt{CC#1}\xspace}
\newcommand{\tc}[1]{\texttt{TC#1}\xspace}
\newcommand{\hr}[1]{\textit{HR#1}\xspace}
%\newcommand{\type0}{\texttt{CC1}\xspace}
%\newcommand{\typeI}{\texttt{CC2}\xspace}
%\newcommand{\typeII}{\texttt{TC1}\xspace}
%\newcommand{\typeIII}{\texttt{TC2}\xspace}


\usepackage{soul}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\sys:  Partial Pipeline Parallelism}

 
\begin{document}

\twocolumn[
\icmltitle{\sys: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks}



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nikolay Blagoev}{wt}
\icmlauthor{Lydia Yiyu Chen}{un}
\icmlauthor{O\u{g}uzhan Ersoy}{ga}
\end{icmlauthorlist}

\icmlaffiliation{wt}{Worker Thread}
\icmlaffiliation{un}{University of Neuchatel \& TU Delft}
\icmlaffiliation{ga}{Gensyn}


\icmlcorrespondingauthor{Nikolay Blagoev}{workerthreadllc@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{LLM, Distributed Training, Scheduling}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
 
Data and pipeline parallelism are ubiquitous for training of Large Language Models (LLM) on distributed nodes. Driven by the need for cost-effective training, recent work explores efficient communication arrangement for end to end training. Motivated by LLM's resistance to layer skipping and layer reordering, in this paper, we explore stage (several consecutive layers) skipping in pipeline training, and challenge the conventional practice of sequential pipeline execution. We derive convergence and throughput constraints (guidelines) for pipelining with skipping and swapping pipeline stages.
Based on these constraints, we propose \sys, the first partial pipeline framework to reduce the end-to-end training time for LLMs while preserving the convergence. The core of \sys is a path scheduling algorithm that optimizes the paths for individual microbatches and reduces 
idle time (due to microbatch collisions) on the distributed nodes, complying with the given stage skipping ratio.  We extensively evaluate \sys on LLaMa models from 500M to 8B parameters on up to 20 nodes. Our results show that \sys reduces training iteration time by up to 55\% compared to full pipeline.
 Our partial pipeline training also improves resistance to layer omission during inference, experiencing a drop in perplexity of only 7\% when running only half the model. Our code is available at \url{https://github.com/gensyn-ai/skippipe}.

\end{abstract}


\input{introduction}
\input{problem}
\input{method}
\input{evaluation}
\input{related}

\section{Conclusion}
Training state-of-the-art LLMs requires a significant number of GPUs and enormous training data.
There have been several work driven by the need for cost-effective training where they explored communication and computation improvements over DP and PP methods. 
Yet, existing PP methods are limited to the sequential execution of the layers.
In this paper we introduced a novel approach to pipeline parallelism, \sys, which makes use of stage skips and swaps to increase throughput by up to \(55\%\) in heterogeneous network settings. Our partial training also produces models resistant to layer removals during inference, which makes them suitable for early exit and fault tolerant inference. Our LLaMa-500M model trained with \sys experiences a drop in perplexity of only 7\% when running half the model.

Finally, while this paper focuses on the homogeneous nodes/ heterogeneous network, in future work, we plan to extend our solution to the full heterogeneous setting where nodes can have different memory and computational capacities.

\section*{Acknowledgment}
This work has been supported by Gensyn. 




\bibliography{scheduling_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Model Configurations}
\label{appendix:mc}
\par We perform all our experiments with LLaMa-based \cite{llama} model architectures with the Sentence Piece Tokenizer \cite{SP}. The different models and their parameters are shown in Table \ref{table:models}. 

\begin{table}[h]
\caption{Model parameters.}
\label{table:models}
\begin{center}
\begin{tabular}[H]{p{6cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}}

  \toprule
  Model & Dim  & Heads & Layers & Context     \\ 
  
  \midrule
  LLaMa 50M & 288 & 6 & 12 & 256 \\
  LLaMa 500M & 1024 & 16 & 24 & 1024 \\
  LLaMa 1.5B~\cite{LayerSkip} & 2048  & 16 & 24 & 4096  \\
  LLaMa-2 7B~\cite{llama} & 4096  & 32 & 32 & 4096  \\
  LLaMa-3 8B~\cite{llama8b} & 4096  & 32 & 32 & 4096  \\
  
  \bottomrule
  

  

\end{tabular}
\end{center}
\end{table}


\section{Test Configurations}
\par Configurations of the throughput tests are presented in Table \ref{table:tests}. Stage sizes for the \(33\%\) case are (5,3,3,3,3,3) (6 stages, 5 of size 3 and 1 of size 5), and for the \(25\%\) case - (6,4,4,4) (4 stages, 3 of size 4, 1 of size 6).
\par For the convergence test, 4 samples per microbatch were used, with a total batch size of 737K tokens. Learning rate was set to \(3\times10^{-4}\) and gradient norms were clipped to \(1.0\).
\subsection{DT-FM-skip path selection}
\label{qrps}
\par Here we explain how the DT-FM-skip is determined. We choose paths that satisfy constraints \cc{1} in an optimised arrangement of nodes in stages. DT-FM-skip serves as a skip baseline which is mainly optimised for the initial node arrangement, but not necessarily for the partial microbatch paths.
\par In order to keep comparison fair, we chose to satisfy constraints \tc{1}, as otherwise delays will be introduced on nodes whose memory is exceeded, as it will need to wait for a backwards pass to come through, before it can continue with this forward pass. Due to this, and our experiment setups, we also inadvertently would satisfy constraints \cc{3}. Thus the algorithm for determining the paths for this baseline is identical to that of the non-collision aware one, except that the computation time of each node and communication time between nodes is set to \(1\). Thus the algorithm does not optimise for fastest paths or \tc{2} constraints.
\begin{table}
\caption{Test settings.}
\label{table:tests}
\begin{center}
\begin{threeparttable}[b]
\begin{tabular}[H]{p{1cm}p{4.2cm}p{1.4cm}p{1.2cm}p{1.6cm}}
% {|m{5em}|m{2cm}|m{3cm}|} 
  % \hline
  \toprule
  Skip & Path finding  & World size & Samples per MB & Batch size  (tokens)   \\ 
  % \hline
  \midrule
  0\% & DTFM \cite{dtfm}  & $18\rightarrow 20$\tnote{a} & 1 & 184K  \\
  \hline
  0\% & DTFM \cite{dtfm}  & $18\rightarrow 20$\tnote{a} & 2 & 368K  \\
  \hline
  0\% & DTFM \cite{dtfm}  & $18\rightarrow 20$\tnote{a} & 4 & 737K  \\
  \hline
  33\% & DT-FM-skip   & 20 & 1 & 184K  \\
  \hline
  
  33\% & DT-FM-skip  & 20 & 2 & 368K  \\
  \hline
  33\% & DT-FM-skip   & 20 & 4 & 737K  \\
  \hline
  33\% & non-collision aware  & 20 & 1 & 184K  \\
  \hline
  
  33\% & non-collision aware  & 20 & 2 & 368K  \\
  \hline
  33\% & non-collision aware  & 20 & 4 & 737K  \\
  \hline
  33\% & collision aware  & 20 & 1 & 184K  \\
  \hline
  
  33\% & collision aware  & 20 & 2 & 368K  \\
  \hline
  33\% & collision aware  & 20 & 4 & 737K  \\
  \hline
  0\% & DTFM \cite{dtfm}  & $16\rightarrow 18$\tnote{b} & 1 & 147K  \\
  \hline
  0\% & DTFM \cite{dtfm}  & $16\rightarrow 18$\tnote{b} & 2 & 294K  \\
  \hline
  0\% & DTFM \cite{dtfm}  & $16\rightarrow 18$\tnote{b} & 4 & 589K  \\
  \hline
  25\% & DT-FM-skip  & 18 & 1 & 147K  \\
  \hline
  
  25\% & DT-FM-skip   & 18 & 2 & 294K  \\
  \hline
  25\% & DT-FM-skip  & 18 & 4 & 589K  \\
  \hline
  25\% & non-collision aware  & 18 & 1 & 147K  \\
  \hline
  
  25\% & non-collision aware  & 18 & 2 & 294K  \\
  \hline
  25\% & non-collision aware  & 18 & 4 & 589K  \\
  \hline
  25\% & collision aware  & 18 & 1 & 147K  \\
  \hline
  
  25\% & collision aware  & 18 & 2 & 294K  \\
  \hline
  25\% & collision aware  & 18 & 4 & 589K  \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
\item [a] In 33\% skip experiment, we use 6 stages with $(5,3,3,3,3,3)$ nodes. DT-FM 0\% skip does not use extra nodes in the first stage (as all stages are used equally). To (over)compensate them using less nodes (while keeping the stage sizes the same), we project their performance by linearly reducing the latency accordingly. In other words, if an iteration of DT-FM case takes 20sn with 18 nodes, we assume it would take 18sn with 20 nodes. Considering the communication of those additional nodes being ignored, this is upper bound of their performance.
\item [b] Same with above except in 25\% skip experiment, we use 4 stages with $(6,4,4,4)$ nodes. Therefore, 16 nodes are projected to 18 nodes.
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Detailed Path Selection Algorithm}\label{sec:appx_algorithm}
In Algorithm~\ref{alg:csf} we present the steps of our path selection function.

\begin{algorithm}
\caption{Path Selection Function.}
\label{alg:csf}
\begin{algorithmic}[1]
{\small
\REQUIRE{ \(\mathcal{S}\), \(k\%\), $G$ - initial node/stage arrangement}
\ENSURE{\(\mathcal{P}\)} 
\STATE \(\mathcal{O} \leftarrow \emptyset\)
\STATE $ T_{constraints} \leftarrow \emptyset$
\STATE Assign $S_0$ to the first stage of $ T_{paths}$
\STATE $ T_{paths} \leftarrow$ find paths via A*(\(G, T_{constraints}\)) 
\STATE Order $ T_{paths}$ by their time to complete in ascending order
\STATE \(T_{cost} \leftarrow \) time for slowest agent to complete route
\STATE Insert \(T\) into \(Open\)

\WHILE{$|\mathcal{O}|<32$ }
\STATE \(T \leftarrow \) best solution from \(Open\)
\STATE Check for \(S_i\) in \(T\) which has more than \(|\mathcal{S}|k\%\) agents going through other than \(S_0\)

\IF{conflict}
\STATE \(\mathcal{K} \leftarrow\) number of agents going through \(S_i\)
\STATE \(Solution \leftarrow\) new node
\STATE $ Solution_{constraints} \leftarrow T_{constraints}$
\FOR{each \(\mathcal{D}_m \in S_i\)}
\FOR{each of the \(\mathcal{K} - |\mathcal{P}|k\%\) fastest paths \(p \in \mathcal{P}\) going through \(S_i\)} 
\STATE $ Solution_{constraints} \leftarrow Solution_{constraints} + (p, -\inf, \inf, \mathcal{D}_m)$ 
\ENDFOR
\ENDFOR
\STATE $ Solution_{paths} \leftarrow$ find paths via A*(\(G, Solution_{constraints}\))
\STATE Order $ Solution_{paths}$ by their time to complete in ascending order
\STATE \(Solution_{cost} \leftarrow \) time for slowest agent to complete route
\STATE Insert \(Solution\) into \(Open\)
\ELSE
\STATE \(\mathcal{O} \leftarrow \mathcal{O} \cup T\)
\ENDIF
\ENDWHILE
\WHILE{\(\mathcal{O}\) is not empty}
\STATE \(T \leftarrow \) best solution from \(\mathcal{O}\)
\STATE Check for conflicts \tc{1} or \tc{2} in \(T\)
\IF{conflict of type \tc{1}}
\STATE \(D_k\) would be the node, whose \(m\) is exceeded as per \tc{1}
\STATE \(\mathcal{K}\) the paths that go through \(D_m\)
\STATE \(Solution \leftarrow\) new node
\FOR{each of the \(\mathcal{K} - m\) fastest paths \(p \in \mathcal{P}\) going through \(D_k\)} 
\STATE $ Solution_{constraints} \leftarrow Solution_{constraints} + (p, -\inf, \inf, \mathcal{D}_k)$ 
\ENDFOR

\STATE $ Solution_{paths} \leftarrow$ find paths via A*(\(G, Solution_{constraints}\))
\STATE Order $ Solution_{paths}$ by their time to complete in ascending order
\STATE \(Solution_{cost} \leftarrow \) time for slowest agent to complete route
\STATE Insert \(Solution\) into \(\mathcal{O}\)
\ELSIF{conflict of type \tc{2}}

\STATE Two paths, \(p_i\) and \(p_j\) collide on \(D_k\). Each of them is at the node during the intervals \(t_{s,i},t_{e,i}\) and \(t_{s,j},t_{e,j}\), respectively
\STATE \(Solution \leftarrow\) new node
\IF{\( E2E(p_i) >  E2E(p_j)\) or \(|E2E(p_j) - E2E(p_j)| < \delta \)}
\STATE \(Solution_{constraints} \leftarrow T_{constraints} + (p_j,t_{s,i},t_{e,i}, D_k)\)
\ENDIF
\IF{\( E2E(p_i) <  E2E(p_j)\) or \(|E2E(p_j) - E2E(p_j)| < \delta \)}
\STATE \(Solution_{constraints} \leftarrow T_{constraints} + (p_i,t_{s,j},t_{e,j}, D_k)\)
\ENDIF
\STATE $ Solution_{paths} \leftarrow$ find paths via A*(\(G, Solution_{constraints}\))
\STATE Order $ Solution_{paths}$ by their time to complete in ascending order
\STATE \(Solution_{cost} \leftarrow \) time for slowest agent to complete route
\STATE Insert \(Solution\) into \(\mathcal{O}\)
\ELSE
\STATE \textbf{Return} $\mathcal{P} \leftarrow T_{paths}$
\ENDIF
\ENDWHILE
\STATE \textbf{Return} $\emptyset$
}
\end{algorithmic}
\end{algorithm}




\section{Possible Extensions of Our Algorithm}
\subsection{Path Coarsening}
\par Here we also present an alternative path finding method based on path coarsening that finds solutions faster, but they may be sub-optimal. The reason for the sub-optimality is that it may increase idle time on devices. However, in a strictly homogeneous device memory setting, it can ignore \tc{2} constraints. Thus, it is best suited for large systems of nodes with equal memory capabilities, where an exact solution may be too costly to compute and due to the homogeneity of the system, most quality solutions will have similar throughput.

\par Here we make use of \textbf{path coarsening} - grouping multiple paths into one meta-agent. Meta-agents traverse a node sequentially, without interruption, and take the total amount of execution time of all the microbatches in the meta-agent. Meta-agent thus become 2-dimensional objects, rather than the point-agents we were considering prior. The downside is that in heterogeneous environments, meta-agents might become more stretched out or mode condensed as they traverse the system. Consider three nodes arranged as A-B-C, taking time to process a microbatch of respectively 1, 2, and 1 seconds. communication between them is 1 second per microbatch. Initially, a meta-agent of 2 microbatches, would have a size of 2 seconds at node A. At node B, due to its delay of processing, the agent will be resized to size of 4, even though the subsequent node would have a gap of 1 second where it would be idle between the two microbatches. However, with meta-agents with multiple paths this level of detail is lost in favour of faster solutions. The best benefit of path coarsening is in a fully homogeneous node setting - equal processing time and equal memory for each. In such a setting we can create meta-agents with number of microbatches in them equal to the memory of the nodes. When finding the solution, all meta-agents will have mutually exclusive paths, thus no collisions need to be considered. Proving the optimality of such a solution is beyond the scope of the paper.
\par In fact our solution has already made use of a degree of coarsening, as we optimise only the first forward pass in an iteration. It is possible to find an even better solution across where no path is reused by microbatches, however, due to the difficulty of finding such a solution even for a small world and small number of agents, we have not performed further analysis.

\subsection{Multiple Swaps}
It is possible to increase the number of swaps by introducing some linear penalty for paths that have swaps more than the desired amount, as a higher number of swaps  hampers convergence, but may increase throughput. It is also possible to define an additional constraint that sets a maximum number of swaps across all paths, which would be delegated to CBS to resolve like constraint \cc{3}, e.g. at most \(|\mathcal{P}|\) swaps across all paths. This would however greatly increase the time to find a quality solution. 
\section{Further Experimental Results}
\par Here we reaffirm our findings from Section \ref{sec:31} in the context of Large Language Models used in practice. While training billion parameter models is too expensive, here we focus on the inference case to confirm some of our previous findings. To such an end, we conduct an empirical performance study on skipping layers during inference on training a LLaMa-7B model~\cite{llama}, on the WikiPedia dataset \cite{wikidump}. We consider four layer skipping strategies: (i) 0\% skipping running the entire model end to end, (ii) 25\% random skipping, (iii) 50\% of random skipping, and (iv) 0\% skipping and swapping the order of two chunks of size 4. We also repeat these four strategies by fixing the first four layer (they never get skipped or swapped). We summarize their loss  in figure Fig. \ref{fig:inferencellmfixed4}. Additionally, we demonstrate in the same setting the effect on inference of skipping any arbitrary stage in the LLaMa-7B model \cite{llama} during inference in \ref{fig:layers-skip}.


\begin{figure}[htp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.485\columnwidth]{figs/mdl_comp.pdf} &
		\includegraphics[width=0.485\columnwidth]{figs/mdl_comp_fixed_4.pdf} \\
		
		(a) Fully random skipping. & (b) Fixed first four layers.  
	\end{tabular}
	
	
		\caption{The validation loss of LLaMa-7B under \% of random skipping in pipeline training.}
		\label{fig:inferencellmfixed4}
	\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/perf_degr.pdf}
    \caption{Validation loss when a given layer is skipped.}
    \label{fig:layers-skip}
\end{figure}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
