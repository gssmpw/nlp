\section{Approach}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/combined_plots_legend_moved.pdf}
    \vspace{-2em} % Adjust this value as needed
    \caption{\textbf{(a) Cross Entropy Loss} on test set during pretraining. The models with varying sizes show a stable decrease in loss. \\
    \textbf{(b) No-Bias Normalization Layer} stabilizes the average value of feed-forward weights in transformer layers. This pattern is consistent across all the transformer blocks.}
     \vspace{-1em}
    \label{fig:no-bais-norm}
\end{figure}
 \begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/overview_method.pdf}
    \vspace{-2em} % Adjust this value as needed
    \caption{\textbf{Overview of of Omni-DNA architecture.} In  \textbf{pretraining}, \method are trained on DNA only with next-token prediction. \textbf{Multi-task finetuning} enables the model to perform diverse tasks including classification, function prediction, and DNA-to-image.}
    \label{fig:overview_method}
    % https://drive.google.com/file/d/19XSZ3zSJSxsz19a4aeOn_NHspRRKliyU/view?usp=share_link
  \vspace{-1em}
\end{figure*}

In this section, we introduce \method with various model sizes for genomic tasks. The pretraining process is present in Section~\ref{sec:pretraining}, followed by cross-modal multi-task finetuning in Section~\ref{sec:omni-finetuning}. The framework is illustrated in Figure~\ref{fig:overview_method}.

% In this section, we introduce \method, a novel approach to create AR models solving genomic tasks  using a generative paradigm. This is achieved through a two step process. In the first step, transformer-based models are pretrained on unlabeled DNA $\mathcal{D}$ with NTP objective. Followed by Genomic Supervised FineTuning (G-SFT), enabling cross-modality mapping from DNA to tokens of various types. This approach enables multitask learning with a single model and effectively handles more expressive tasks, as demonstrated in Figure XXX.
 


\subsection{Pretraining}
 
\label{sec:pretraining}
\paragraph{Pretraining Dynamics:} we pretrain a series of auto-regressive transformer-based models using unlabeled DNA data, with log-uniformly spaced sizes ranging from 20 Million to 1 Billion. The pretraining procedure are conducted with minimum loss spike as shown in \cref{fig:no-bais-norm}(a) across models with various sizes. These model are trained on 300 billion nucleotides. The full hyperparameters used by our model and a comparison with prior genomic models are shown in \cref{app:pretrain-config}, below highlights key configurations.

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/weight_plot.pdf}
%     \caption{\textbf{No-Bias Normalization Layer Stabilizes the Average value of Feed-Forward Weights in Pretraining}. Each model is trained with 300 Billion Nucleotides. This pattern is consistent across all the transformer blocks.}
%     \label{fig:no-bais-norm}
% \end{figure}
 
% \paragraph{Model Architecture}
\paragraph{Model Architecture:} We build on Open Language Model (OLMo)~\citep{groeneveld2024olmo} for DNA Sequence Modeling. Compared to the vanilla transformer~\citep{vaswani2017attention},  several improvements introduced by LLaMA family~\citep{touvron2023llama} and OLMo family are adopted to our model. 
% \begin{enumerate}[itemsep=1mm,leftmargin=4.5mm]
    \textbf{(i) LayerNorm} The Non-parametric layer norm~\citep{ba2016layer} without Bias are applied to our 116M and 1B model. RMSNorm~\citep{zhang2019root} are applied to the remaining fours models. We find while both types of LayerNorm result in a stable pretraining process, the Non-parametric layer norm tends to maintain more stable average weights on the feed forward layer as shown in \cref{fig:no-bais-norm}, which in turns help to achieve higher accuracy when performing fullsize finetuning on the downstream tasks (\cref{sec:fullfinetune}).
    \textbf{(ii) Relative Positional Embeddings} 
    Although previous work~\cite{zhou2023dnabert} indicates that relative positional embedding methods such as ALiBi~\cite{press2021train} may offer improved extrapolation capabilities, the context length of the pretrained model can still be easily extended during finetuning. We observed slower convergence when using ALiBi in our pretraining experiments (see \cref{sec:ablation}).
    \textbf{(iii) BPE Tokenizer} We adopt Byte-Pair Encoding (BPE)~\cite{sennrich2015neural} with an initial vocabulary of 4096, following~\citet{zhou2023dnabert}. During the vocabulary expansion, newly added tokens are always recognized as standalone tokens rather than retraining the tokenizer.
    
    % Byte-Pair Encoding (BPE)~\cite{sennrich2015neural} with an initial vocabulary size of 4096 are used, following the prior work~\cite{zhou2023dnabert}. In particular, we allow the addition of new tokens to the BPE tokenizer, and a high priority given to the explicitly added token. And no retraining of tokenizer is needed. In this manner, we can ensure the newly added tokens always stand out as a independent vocabulary to facilitate the integration of downstream tasks.
% \end{enumerate}

\paragraph{Pretraining Dataset Deduplication}

Duplicate data can slow model convergence, making deduplication standard in LLM pretraining~\cite{rae2021scaling, groeneveld2024olmo}. Because genomic data is highly duplicated, we removed exact duplicates from NCBI's  multi-species genome dataset~\cite{schoch2020ncbi}, reducing it to 30 billion nucleotides. Despite this reduction, multiple epochs can still be used in training.

% Duplication in pretraining datasets has been shown to slow convergence during model training~\cite{rae2021scaling}. As a result, deduplication has become a standard step in preparing data for large language models (LLMs)~\cite{groeneveld2024olmo}. For genomic data, duplication is prevalent. At the individual level, human genomes are approximately 99.9\% identical. Across mammals, the genomic similarity is also significant due to their shared evolutionary ancestry. To address this, we performed deduplication on a multi-species genome dataset obtained from NCBI~\cite{schoch2020ncbi} by removing exact duplicate chunks as details in \cref{app:deduplication}. This procedure reduced the dataset size from 170 billion to 30 billion nucleotides. While this reduced the raw data, pretraining can still leverage the dataset by iterating over it for multiple epochs, ensuring that repeated data instances are separated to mitigate the effects of redundancy.


\begin{table*}[t!]
\centering
\footnotesize % Reduce text size for the table
\caption{Examples of Genomic Task Unification, including inputs, labels, instructions, and responses.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcc|cc@{}}
\toprule
\textbf{Task Type} & \multicolumn{2}{c|}{\textbf{Original}} & \multicolumn{2}{c}{\textbf{Unified Format}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & \textbf{Input} $x^{(i)}$& \textbf{Target} $y^{(i)}$ & \textbf{Instruction} $\tilde{x}^{(i)}$ & \textbf{Response} $\tilde{y}^{(i)}$ \\ \midrule
\small Promoter CLS\fontsize{5}{5}\selectfont~\yrcite{dalla2024nucleotide}& AT...T & 0/1 & AT...T\textcolor{cyan!50!blue}{is this a promoter?} & Yes/No \\
\small  Enhancer Types CLS \fontsize{5}{5}\selectfont~\yrcite{dalla2024nucleotide}&  AT...A  &0/1/2  & AT...A\textcolor{cyan!50!blue}{what is enhancer type?} & Type 0/1/2 \\
% \small  GV CLS~\fontsize{5}{5}\selectfont\yrcite{li2024gv}& Ref$\|$Alt & 0/1 & Ref$\|$Alt\textcolor{cyan!50!blue}{is this pathogenic?}& Yes/No \\
\small  DNA2Image & AT...G & \includegraphics[width=0.4cm]{figures/img_1.jpg} & AT...G\textcolor{cyan!50!blue}{map to image} & $[0,2,...,512]$ (discretized tokens) \\
\small  DNA2Text & AT...T & \small {It encodes H-box helicase protein} & AT...T\textcolor{cyan!50!blue}{what is the function?} & It encodes H-box helicase protein \\
\bottomrule
\end{tabular}
}
\label{tab:unification_tasks}
 \vspace{-1em}
\end{table*}




\subsection{Cross-modal Multi-task Finetuning}
\label{sec:omni-finetuning}


\begin{algorithm}[t!] \small
\caption{\textbf{Cross-modal Multi-task Finetuning}}
%: build Multitasking Genomic Foundation Model}
\begin{algorithmic}[1]
\REQUIRE $K$ datasets $\{\mathcal{D}_k\}_{k=1}^K$, 
         pretrained model $f_\theta$, 
         vocabulary $V_\text{pretrained}$, repeating factor $\alpha$, Noise Level $\beta$

% \STATE  $\text{Discretize}\bigl(y^{(i)},\alpha \bigr)$ if $y^{(i)} \notin \mathbf{R}$  \COMMENT{Response Discretization for Continuous Variable}
\STATE $\mathcal{D}_\text{uni} \gets \bigcup_{k=1}^K \mathcal{D}_k$
\COMMENT{Task Unification}
\STATE Extract task-specific multi-modal tokens $V_k$ from $\mathcal{D}_k$
%\STATE $V_\text{task} \gets \bigcup_{k=1}^K V_k$
%\COMMENT{Unique vocab from all tasks}
\STATE $V_\text{expand} \gets V_\text{pretrained} \cup \bigcup_{k=1}^K V_k$
\COMMENT{Vocab. expansion}

\REPEAT
    \STATE $\{(x^{(i)}, y^{(i)})\}_{i=1}^B \sim \mathcal{D}_\text{uni}$ \COMMENT{Batch size $B$}
    
    \FOR{$i$ from 1 to $B$}
        \STATE $y^{(i)} \gets \text{ReplicateKeyToken}\bigl(y^{(i)},\alpha \bigr)$ 
            \COMMENT{Replicate Class Labels in target for Classification Tasks}
        \STATE $\text{loss}^{(i)} \gets -\sum_{t=1}^{|y^{(i)}|} 
            \log p_\theta\Bigl(y^{(i)}_t \,\Big\vert\, y^{(i)}_{1:t-1},x^{(i)} ,\beta\Bigr)$
            \COMMENT{Add noise using NEFTune with $\beta$ during loss computing}
    \ENDFOR
    \STATE $\text{Loss}_\text{batch} \gets \frac{1}{B}\sum_{i=1}^{B} \text{loss}^{(i)}$
    \STATE $\theta \gets \text{optimizer}\bigl(\theta, \text{Loss}_\text{batch}\bigr)$ 
        %\COMMENT{Update model parameters}
\UNTIL{\emph{stopping criterion} or \emph{max iterations}}

\label{algorithm:1}
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}
% \caption{\textbf{Multi-task Finetuning}: build Multitasking Genomic Foundation Model}
% \begin{algorithmic}[1]
% \REQUIRE $K$ tokenized genomic datasets $\{\mathcal{D}_k\}_{k=1}^K$, 
%          pretrained autoregressive model $f_\theta$, 
%          pretrained model parameters $\theta$, 
%          vocabulary $V_\text{pretrained}$, 
%          optimizer $\text{opt}(\cdot)$
% \REQUIRE Hyperparameter: Repeating Factor $\alpha \in \mathbb{Z}$ 
% \STATE $\mathcal{D}_\text{uni} \gets \bigcup_{k=1}^K \mathcal{D}_k$
% \COMMENT{Task Unification}
% \STATE $V_\text{task} \gets \bigcup_{k=1}^K V(\mathcal{D}_k)$
% \COMMENT{Unique vocab from all tasks}
% \STATE $V_\text{expand} \gets V_\text{pretrained} \cup V_\text{task}$
% \COMMENT{Vocab. expansion}

% \REPEAT
%     \STATE $\{(X^{(i)}, Y^{(i)})\}_{i=1}^B \sim \mathcal{D}_\text{uni}$ \COMMENT{Batch size $B$, seq. length $L$}
    
%     \FOR{$i$ from 1 to $B$}
%         \STATE $Y^{(i)} \gets \text{ReplicateKeyToken}\bigl(Y^{(i)},\alpha \bigr)$ 
%             \COMMENT{Replicate Class Labels in target for Classification Tasks}

%         \STATE $\text{loss}^{(i)} \gets -\sum_{t=1}^{|Y^{(i)}|} 
%             \log p_\theta\Bigl(Y^{(i)}_t \,\Big\vert\, Y^{(i)}_{1:t-1},X^{(i)}\Bigr)$
%     \ENDFOR
%     \STATE $\text{loss}_\text{batch} \gets \frac{1}{B}\sum_{i=1}^{B} \text{loss}^{(i)}$
%     \STATE $\theta \gets \text{opt}\bigl(\theta, \text{loss}_\text{batch}\bigr)$ 
%         \COMMENT{Update model parameters}
% \UNTIL{\emph{stopping criterion} or \emph{max iterations}}

% \label{algorithm:1}
% \end{algorithmic}
% \end{algorithm}

% Cross-Modality Multitasking Finetuning is designed to \textbf{(1)} extend the pretrained AR models to comprehend and generate outputs in various modalities beyond DNA tokens; and \textbf{(2)} solve multiple downstream tasks effectively through a single finetuning process. 

Here we extend pretrained models to handle multiple modalities beyond DNA tokens and enable learning of diverse downstream tasks through a single finetuning process. We first introduce the whole process of finetuning and then describe the detailed modules.  


\paragraph{Whole Process} As illustrated in \cref{algorithm:1}, we need to finetune $K$ task-specific datasets with different token sets and modalities. We first merge $K$ task-specific datasets into a unified dataset $\mathcal{D}_\text{uni}$, and the tokenizer vocabulary are expanded to include unique new tokens from each task. During each training iteration, a batch of $(x^{(i)}, y^{(i)})\}_{i=1}^B$ is first sampled from $\mathcal{D}_\text{uni}$, $y^{(i)}$ is replicated by a factor $\alpha$ to emphasize class labels, and the loss is computed. The parameters $\theta$ are then updated via gradient-based optimization. By unifying multiple tasks under a single loop, the algorithm encourages the model improves the average performance across diverse genomic tasks. %The detailed modules are described in the following.


\paragraph{Multi-task Dataset Unification}  
We integrate \( K \) genomic tasks with labeled datasets \(\{\mathcal{D}_k\}_{k=1}^K\) of various formats into a unified dataset $\mathcal{D}_{\text{uni}} = \{\tilde{x}^{(i)},\tilde{y}^{(i)}\}$. Specifically, each sample from task $k$ is modified by appending a task-specific prompt \(\textcolor{cyan!50!blue}{\text{Prompt}_k}\) to its input \(x^{(i)}\):
$
\tilde{x}^{(i)} = \big(x^{(i)}, \textcolor{cyan!50!blue}{\text{Prompt}_k}\big).
$
The corresponding response \(\tilde{y}^{(i)}\) is treated as tokens and the typical transformation is shown in \cref{tab:unification_tasks}.




% transformed via a task-specific token mapping \(M\):  
% $
% \tilde{y}^{(i)} = M\big(y^{(i)}\big).
% $
% Examples of common tasks are shown in \cref{tab:unification_tasks}.

\paragraph{Vocabulary Expansion} As the multi-task dataset unification introduces new tokens (i.e., prompt and response tokens), it is necessary to expand the vocabulary during SFT stage. This is a key difference between genomic language model and normal language models. A direct result of vocabulary expansion is distribution shift. Based on~\cite{hewitt2021initializing}, the distribution shift after adding a new set of vocabulary  $V_{z}$ to pretrained model $\theta$ results in a new model $\theta'$, let $\scriptstyle Z = \sum\limits_{m \in V_{o}} \exp(h_{t-1}^\top e_m)$. Then the distribution shift are:
\begin{equation} \small
    p_{\theta'}(x_t \mid x_{1:t-1}) = p_\theta(x_t \mid x_{1:t-1}) \cdot \frac{1}{1 + \frac{\sum\limits_{n \in V_{z}} \exp(h_{t-1}^\top e_n)}{Z}}.
    \label{eq:5}
\end{equation}
 In other words, all the words probability are reduced. To alleviate it, we should minimize the size of added tokens. 
% One is vocabulary expansion. While this is optional for LLM, vocabulary expansion are necessary to allow the pretrained DNA model to perform meaningful downstream tasks. 

\paragraph{NEFTune \& Key Token Replication}  
To mitigate catastrophic forgetting~\cite{zheng2024towards} caused by distribution shifts from vocabulary expansion, we employ two simple yet effective techniques: \textit{NEFTune}~\citep{press2021train} and \textit{Key Token Replication}. NEFTune introduces random noise to embedding vectors during finetuning, reducing overfitting. Key Token Replication, applied to classification tasks, replicates label tokens in \( y^{(i)} \) by a factor of \( \alpha \). This ensures stronger signal propagation, facilitating the model’s transition from DNA sequence generation to classification by reinforcing the association between input sequences and their corresponding labels.



\begin{table*}[ht!]
\caption{\textbf{Comparison of Omni-DNA with existing GFMs on 18 NT downstream tasks.} The family of Omni-DNA obtains the best average result.
In splice tasks, Omni-DNA performs lower than NT models but still surpasses other models.}
\label{tab:pretraining_comparison}
\resizebox{\textwidth}{!}{
%\small
\begin{tabular}{@{}lllcccccc@{}}
\toprule
\tiny{\textbf{Pretraining}} & \tiny\textbf{Pretraining} & \textbf{Model} & \textbf{Histone} & \textbf{Enhancer} & \textbf{Promoter} & \textbf{Splice} & \textbf{Average} \\
\tiny\textbf{Objective} & \tiny\textbf{Data} & & \tiny (AVG. MCC in 10 Tasks)$\uparrow$ & \tiny (AVG. MCC in 2 Tasks)$\uparrow$ & \tiny (AVG. F1 in 3 Tasks)$\uparrow$ & \tiny (AVG. F1 in 3 Tasks)$\uparrow$ & \tiny (Across 18 Tasks)$\uparrow$ \\
\midrule
\multirow{5}{*}{\small MLM} & \multirow{2}{*}{\small\shortstack{Human}} 
                     & \small CADUCEUS-PH (1.9M)    & 0.635 & 0.4925 & 0.964 & 0.942 & 0.715 \\
                     &                                & \small CADUCEUS-PS (1.9M)   & 0.585  & 0.454 & 0.964 & 0.912 & 0.689 \\
\cmidrule{2-8}
                     & \multirow{6}{*}{\small\shortstack{Multi-\\Species}} 
                     & \small DNABERT2 (120M)       & 0.551 & 0.470 & 0.966 & 0.959 & 0.679 \\
                     &                                & \small NT (50M)      & 0.493 & 0.465 & 0.953 & 0.980 & 0.648 \\
                     &                                & \small NT (100M)  & 0.496 & 0.475 & 0.957 & 0.983 & 0.661 \\
                     &                                & \small NT (250M)          & 0.536 & 0.472 & 0.968 & 0.983 & 0.675 \\
                     &                                & \small NT (500M)          & 0.572 & 0.486 & 0.972 & \underline{0.983}   & 0.698 \\
                     &                                & \small NT (2.5B)     & 0.584 & 0.527 & \underline{0.971} & \textbf{0.986}  & 0.709 \\
\midrule
\multirow{8}{*}{\small NTP} & \multirow{1}{*}{\small\shortstack{Human}} 
                     & \small HyenaDNA (1.6M)     & 0.610 & 0.452 & 0.954 &0.954   & 0.707 \\
\cmidrule{2-8}
                     & \multirow{6}{*}{\small\shortstack{Multi-\\Species\\(\textbf{Ours})}} 
                     & \small Omni-DNA (20M)     & 0.538 & 0.484 & 0.957 & 0.897 & 0.662 \\
                     &                                & \small Omni-DNA (60M)      &0.559 & 0.499 & 0.966 & 0.938 & 0.690 \\
                     &                                & \small Omni-DNA (116M)  & \underline{0.675} & \underline{0.545} & 0.970 & 0.960   & \underline{0.755} \\
                     &                                & \small Omni-DNA (300M)    & 0.659 & 0.486 & 0.969    & 0.956& 0.741 \\
                     &                                & \small Omni-DNA (700M)       & 0.675 & 0.519 & 0.968  & 0.960 & 0.754 \\
                     &                                & \small Omni-DNA (1B)        & \textbf{0.694} & \textbf{0.536} & \textbf{0.973}  & 0.958  & \textbf{0.767} \\
\bottomrule
\end{tabular}
}
\vspace{-1em}
\end{table*}

%\paragraph{Map DNA to High Dimensional Continuous Variables} 
\paragraph{Response Discretization} In many genomic tasks such as genomic assay prediction~\cite{avsec2021effective} and structure prediction~\cite{abramson2024accurate}, the response is a high-dimensional continuous variable $\mathbf{y} \in \mathbb{R}^{d_1 \times d_2\cdots \times d_k}$. To adopt Omni-DNA, we need to discretize the response first. 
%map the DNA to other continuous variable with \textit{response discretization}. 

% The mapping from DNA sequence $\mathbf{x} \in \mathbb{N}^T_4$ to high dimensional continuous variable $\mathbf{Y} \in \mathbb{R}^{d_1 \times d_2\cdots \times d_k}$ are underlying many genomic sequence modeling task such as genomic assay prediction~\cite{avsec2021effective} and structure prediction~\cite{abramson2024accurate}. 



Here we consider a three-dimensional continuous variable $\mathbf{y} \in R^{d_1\times d_2\times d_3}$. \textit{Response discretization} is achieved by training a VQ-VAE~\cite{van2017neural}, which learns a latent embedding space of dimension $[K, D]$, where $K$ represents the number of quantized embeddings, and $D$ denotes the dimensionality of each latent embedding vector $\mathbf{e}_i \in \mathbb{R}^D$. The VQ-VAE compresses the input $\mathbf{y}^{(i)}$ to a sequence of discrete latent variables $\mathbf{\tilde{y}}^{(i)}=\{e_0,e_1,...,e_L\} \in \mathbb{N}^{L}_K$. Here, $L = d_1' \times d_2'$ corresponds to the spatial dimensions of the encoded variable $\mathbf{\tilde{y}}^{(i)}$, where $d_1'=d_1/r$ and $d_2'=d_2/r$. The compression ratio $r$ quantifies the reduction in dimensionality achieved during encoding. The decoder then reconstructs the input $\mathbf{y}$ from discrete latent sequences $\mathbf{\tilde{y}}$.  By employing response discretization, any DNA-to-continuous-variable task can be converted into a DNA-to-discrete-token task. %This transformation enables the application of \cref{algorithm:1} to effectively solve the task.
