\begin{table*}[ht!]
%\small
\centering
\caption{\textbf{Performance of \method across 8 tasks in the genomic benchmark.} \method (116M) achieves the highest average. $\pm$ indicates the difference between the max and min value in 10 fold cross-validation.}
%\caption{Comparison of performance metrics across various tasks and models. The number of parameters is shown in parentheses, below the model names, with standard errors included for each cell where available. Missing values are denoted with "--".}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Task Name} & \textbf{CNN} & \textbf{HYENADNA} & \textbf{CADUCEUS -PH} & \textbf{DNABERT2} & \textbf{Omni-DNA} \\
 & \small\textbf{(264K)} & \small\textbf{(436K)} & \small\textbf{(470K)} & \small\textbf{(117M)} & \small\textbf{(116M)} \\ \midrule
\textbf{MOUSE ENHANCERS}          & $0.715 \pm 0.087$  & $0.780 \pm 0.025$  & $0.754 \pm 0.074$  & $\underline{0.792} \pm 0.031 $  & $\textbf{0.799} \pm 0.004$  \\
\textbf{CODING VS. INTERGENOMIC}  & $0.892 \pm 0.008$  & $0.904 \pm 0.005$  & $0.915 \pm 0.003$  & $\textbf{0.949} \pm 0.002 $    & $\underline{0.942} \pm 0.010$  \\
\textbf{HUMAN VS. WORM}           & $0.942 \pm 0.002$  & $0.964 \pm 0.002$  & $0.973 \pm 0.001$  & $0.975\pm 0.002 $ & $\textbf{0.976} \pm 0.001$ \\
\textbf{HUMAN ENHANCERS COHN}     & $0.702 \pm 0.021$  & $0.729 \pm 0.014$  & $\textbf{0.747} \pm 0.004$  & $0.714 \pm 0.025 $      & $\underline{0.738} \pm 0.002$ \\
\textbf{HUMAN ENHANCER ENSEMBL}   & $0.744 \pm 0.122$  & $0.849 \pm 0.006$  & $\underline{0.893} \pm 0.008$  & $0.891 \pm 0.051 $      & $\textbf{0.919} \pm 0.021$ \\
\textbf{HUMAN REGULATORY}         & $0.872 \pm 0.005$  & $0.869 \pm 0.012$  & $\underline{0.872} \pm 0.011$  & $0.852 \pm  0.024$     & $\textbf{0.895} \pm 0.012$ \\
\textbf{HUMAN OCR ENSEMBL}        & $0.698 \pm 0.013$  & $0.783 \pm 0.007$  & $\textbf{0.828} \pm 0.006$  & $0.789 \pm 0.012$         & $\underline{0.791} \pm 0.001$ \\
\textbf{HUMAN NONTATA PROMOTERS}  & $0.861 \pm 0.009$  & $0.944 \pm 0.002$  & $0.946 \pm 0.007$  & $0.912 \pm 0.013$ & $\textbf{0.968} \pm 0.013$ \\ 
\midrule
\textbf{Average}  & $0.803$  & $0.853$  & $0.866$  & $0.859$ & $\textbf{0.879}$ \\ 
\bottomrule
\end{tabular}
}
\label{tab:gb_reuslts}
\vspace{-1em}
\end{table*}

\section{Results on Conventional Genomics Tasks}
\label{sec:fullfinetune}
\paragraph{Setup} To evaluate the quality of \method and establish a baseline for multi-tasking, we first follow the conventional evaluation process in single task mode. We compare \method with DNABERT-2~\cite{zhou2023dnabert}, NT-Transformer~\cite{dalla2024nucleotide}, HyenaDNA~\cite{nguyen2024hyenadna}, and Caduceus models~\cite{schiff2024caduceus}, across two widely used benchmarks: NT-Downstream Tasks ~\cite{dalla2024nucleotide} and  Genomic Benchmark ~\cite{grevsova2023genomic}. Evaluation is performed using a conventional approach, where a MLP head is attached to the pretrained models, followed by full-size finetuning. The detailed set up of the evaluation is in \cref{app:finetune_ch}.


\subsection{Nucleotide Transformer Downstream Tasks}
\label{sec:nt_classification}

NT Downstream Tasks include 18 distinct binary or three-class classification tasks. These tasks can be categorized into four categories: epigenetic marker~\cite{pokholok2005genome},  promoter~\citep{oubounyt2019deepromoter}, enhancer and splice site prediction. Following the prior evaluation setup~\cite{schiff2024caduceus}, Matthews Correlation Coefficient (MCC) is used for histone marker and enhancer, while F1-score is used for splice and promoter classification. We finetune each model with 10-fold cross-validation \cite{schiff2024caduceus}, with maximum epochs set to 20.

\Cref{tab:pretraining_comparison} shows the average performance of 16 models across different tasks types and the average performance across 18 tasks. Omni-DNA (1B) and Omni-DNA (116M) achieve the best average performance of 0.767 and the second of 0.755. Both models are trained with non-parametric norm. We provide the task-specific performance of 16 models in \Cref{tab:pretraining_comparison_part1}, Omni-DNA achieves superior performance on 13 out of 18 benchmark tasks, surpassing competing methods. Task-specific results in \Cref{tab:pretraining_comparison_part1} show that Omni-DNA outperforms competing methods on 13 out of 18 benchmarks. For the remaining five tasks, Omni-DNA (1B) ranks second and third in Promoter:ALL and Promoter:NonTATA. In the three splice site classification tasks, Omni-DNA models perform lower than NT models but still surpass other models.

% \paragraph{Task-specific performance} We provide the task-specific performance of 16 models in \Cref{tab:pretraining_comparison_part1}, Omni-DNA achieved superior performance on 13 out of 18 benchmark tasks, surpassing competing methods. A key divergence emerged when analyzing scalability: models trained with masked language modeling (MLM) plateaued in performance as their size increased, while those trained with next token prediction (NTP) exhibited sustained improvements with larger architectures. This contrast suggests that MLM’s focus on reconstructing masked tokens may prioritize memorizing local patterns (e.g., conserved motifs or common substitutions), causing diminishing returns at scale. In contrast, NTP’s autoregressive objective—predicting sequential dependencies across entire sequences—likely enables larger models to capture global genomic context (e.g. long-range regulatory logic or evolutionary constraints). While architectural differences may contribute, the divergence underscores how training objectives shape knowledge storage of GFMs.

\subsection{Genomic Benchmark Results}
Genomic Benchmark (GB)~\cite{grevsova2023genomic} contains eight DNA regulatory element classification tasks, similar to NT downstream tasks, with additional tasks on species classification and gene regulatory element classification on mouse. The number of sequences from seven tasks in GB was 10 times larger compared to NT downstream tasks. The details on the finetuning setting are included in \cref{app:gb_ft}. Each model was finetuned for a max of 10 epochs. We compare the performance of smaller size model from each type of the model in \Cref{tab:gb_reuslts}. 

Omni-DNA (116M) achieved the highest average score, ranking first in five out of eight tasks and second in the remaining three. Compared to DNABERT-2, which has a similar model size, Omni-DNA (116M) outperformed DNABERT-2 in seven out of eight tasks.

