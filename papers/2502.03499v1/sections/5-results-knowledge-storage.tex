% \section{Detailed Limitations} 
% \label{app:limitation}
% Our work primarily focuses on transformer-based architectures, leaving the exploration of alternative approaches, such as state-space models like Mamba \cite{gu2023mamba} and Hyena \cite{nguyen2024hyenadna}, and recurrent neural network architectures like xLSTM \cite{schmidinger2024bio} for future research. Additionally, while our model demonstrates the feasibility of mapping DNA to multi-dimensional outputs, such as structure or genomic assay signals, achieving high-fidelity cross-modality translation remains an open challenge. Future improvements in tokenization strategies \cite{li2024vqdna, pagnoni2024byte, qiao2024model}, including more robust vector quantization (VQ-VAE) techniques, could potentially improve the model’s ability to handle complex modalities with greater precision. Moreover, the DNA pretraining corpus, while extensive at 300B nucleotides, may not fully capture the diversity of regulatory elements across all species and tissue types \cite{andrews2023mammalian}. Finally, while we have demonstrated strong performance on regulatory element classification and selected tasks, our evaluation does not encompass important genomic challenges like long-range interaction prediction \cite{cheng2025dnalongbench} or variant effect prediction \cite{li2024gv}.

\section{Ablation Study}
\label{sec:ablation}
\subsection{Ablation on Positional Embedding Methods}

We utilized \method(116M) to test the effect of two positional embedding methods: ALiBi~\citep{press2021train} and RoPE~\citep{su2024roformer}. We perform pretraining on 300 billion nucleotides with keeping the remaining hyperparameters unchanged. 
The training and test losses during pretraining are shown in \Cref{fig:embedding}. 
RoPE achieves a lower test loss and converges faster, indicating that it better captures the contextual and relational properties of DNA compared to ALiBi.

% For the 116M model, we perform pretraining on 300 billion nucleotides with two variants of \method, keeping all hyperparameters identical except for the positional embedding: one using ALiBi~\citep{press2021train} and the other using Rotary Positional Embedding (RoPE)~\citep{su2024roformer}. The training and test losses during pretraining are shown in \Cref{fig:embedding}. RoPE achieves lower test loss and converges faster. In contrast, ALiBi’s slower convergence suggests that RoPE’s structure is better suited to the contextual and relational requirements of DNA sequences.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/ablation_emb.pdf}
    \vspace{-2em} % Adjust this value as needed
    \caption{\textbf{Losses for \method(116M) pretrained with ALiBi and RoPE,} indicating RoPE’s faster convergence and lower losses.} 
    %ALiBi embedding leads to slow convergence as indicated by the Test Loss Line.}
    \label{fig:embedding}
\end{figure}

\subsection{Larger Models Mitigate Distribution Shifts} %from Vocabulary Expansion}
\Cref{eq:5} shows that introducing new vocabulary leads to a distribution shift of existing tokens. While synergistic effects in multi-task settings can mitigate this shift \cite{son2024multi}, single-task scenarios experience performance degradation when new tokens are added compared to using a classification head. \Cref{fig:vocab_expansion} shows how performance degradation varies with model size in the promoter TATA classification task. 
Notably, increasing the model size alleviates this issue, and \method(1B) maintains performance comparable to those, prior to vocabulary expansion.

% \Cref{eq:5} indicates that introducing new vocabulary leads to a distribution shift of existing tokens. In a multi-task setting, synergy effects may mitigate this shift. However, in single-task scenarios, adding new tokens leads to performance degradation compared to using a classification head, due to the distribution shift. \Cref{fig:vocab_expansion} shows how performance degradation varies with model size in the promoter TATA classification task. Notably, increasing the model size alleviates this issue, and \method(1B) maintains performance levels comparable to those before vocabulary expansion.
\begin{figure}[ht!]
    \centering
    % \includegraphics[width=0.6\textwidth]{figures/ablation_model_size_performance_degration.pdf}
    \includegraphics[width=\columnwidth]{figures/ablation_model_size_performance_degration.pdf}
    \vspace{-1em} % Adjust this value as needed
    \caption{\textbf{Impact of model size on performance degradation due to vocabulary expansion.} Larger models exhibit better resilience against distribution shifts.}
    \label{fig:vocab_expansion}
\end{figure}

\subsection{Impact of Token Replication Factor}
\Cref{fig:repearting_factor} illustrates the effect of the token replication factor $\alpha$ on the promoter TATA classification task for \method sized 116M and
1B. Without token replication ($\alpha=1$), classification performance is poor. We find that setting $\alpha$ within the range [8, 11] is effective across various task types.
% \Cref{fig:repearting_factor} shows the influence of token replication factor $\alpha$ in the promoter TATA classification task for Omni-DNA with size 1B and 116M. Generally, not using Token Replication technique (corresponding to $\alpha =1$) result in poor performance in sequence classification. We found the range $\alpha \in [8,11]$ is applicable across various types of the tasks.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/ablation_repeating_factor.pdf}
    \vspace{-2em} % Adjust this value as needed
    \caption{\textbf{Impact of the token replication factor $\alpha$} on the promoter TATA classification task.}
    \label{fig:repearting_factor}
\end{figure}



