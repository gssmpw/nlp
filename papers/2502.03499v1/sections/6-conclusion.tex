\section{Conclusion}
We have demonstrated that pretraining an auto-regressive transformer on DNA sequences, followed by cross-modal multi-task finetuning, is an effective strategy for constructing a unified genomic foundation model.
This model can handle diverse genomic tasks across multiple modalities, including discrete labels, textual descriptions, and other multi-dimensional outputs.
%, capable of mapping DNA to discrete labels, text, and multi-dimensional outputs. 
Specifically, we demonstrate that:
(i) The auto-regressive model can match or outperform the bidirectional transformer on genomic sequence modeling. (ii) The model pretrained solely on genomic sequences can generalize to unseen tokens through finetuning, achieving the SOTA performance. Our proposed \method not only handles  multiple classification tasks within a single model, but also tackles more complex challenges, such as converting DNA into coherent, contextually relevant natural language, and mapping DNA to multi-dimensional representations, as showcased in DNA2Funtion and DNA2image.

%such as mapping DNA to natural language with coherent and contextually relevant text generation, or translating DNA sequences into multi-dimensional representations, as demonstrated in the DNA-to-image task.

We hope that this paper with available codes for pretraining and finetuning, will serve as a starting point for researchers to explore more complex genomic tasks and develop more comprehensive genomics foundation models.

%, along with the software we provide for pretraining and finetuning genomic sequences, will help researchers explore beyond single-modality and single-task genomic foundation models. We also aim to encourage further work toward building more comprehensive foundation models for genomics.

% \textbf{Limitations.} Our work focuses on transformer-based architectures, leaving the exploration of alternative approaches, such as Mamba and xLSTM for future research. See \cref{app:limitation} for detailed discussion on the limitations.
\paragraph{Limitations} Our work primarily focuses on transformer-based architectures, leaving the exploration of alternative approaches, such as state-space models like Mamba \cite{gu2023mamba} and Hyena \cite{nguyen2024hyenadna}, and recurrent neural network architectures like xLSTM \cite{schmidinger2024bio} for future research. Additionally, while our model demonstrates the feasibility of mapping DNA to multi-dimensional outputs, such as structure or genomic assay signals, achieving high-fidelity cross-modality translation remains an open challenge. Future improvements in tokenization strategies \cite{li2024vqdna, pagnoni2024byte, qiao2024model}, including more robust vector quantization (VQ-VAE) techniques, could potentially improve the modelâ€™s ability to handle complex modalities with greater precision. Moreover, the DNA pretraining corpus, while extensive at 300B nucleotides, may not fully capture the diversity of regulatory elements across all species and tissue types \cite{andrews2023mammalian}. Finally, while we have demonstrated strong performance on regulatory element classification and selected tasks, our evaluation does not encompass important genomic challenges like long-range interaction prediction \cite{cheng2025dnalongbench} or variant effect prediction \cite{li2024gv}.
