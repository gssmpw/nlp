
\section{Preliminaries and Notations}

% \subsection{Downstream Tasks in Genomic Sequence Modelling}

\subsection{Genomic Sequence Modeling} 
DNA is a polymer made of up four types of nucleotides: Adenine (\textit{A}), Thymine (\textit{T}), Guanine (\textit{G}), and Cytosine (\textit{C}). Let $ \mathbb{N}_4 = \{A, T, G, C\}$. A DNA sequence of length $T$, denoted as $\mathbf{x} = {(\mathbf{x}_1, \mathbf{x}_2,...,\mathbf{x}_T)} \in \mathbb{N}^T_4$, follows a natural distribution $\mathbf{x} \sim p_\theta(\mathbf{x})$. We use $p_{\hat{\theta}}(\mathbf{x})$ to represent an estimate to the true distribution. The dataset of unlabeled genomic sequences is given in the form of \( \{\mathbf{x}^{(i)}\}_{i=1}^N\).

Genomic sequence modeling aims to learn a function $f$ that maps input sequences to biological annotations using a labeled dataset \(\mathcal{D} = \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N\). The type of $y^{(i)}$ varies depending on the types of tasks: $y^{(i)}$ is a class label in \textbf{DNA Sequence Classification}~\cite{grevsova2023genomic,dalla2024nucleotide}. Or a real value vector in \textbf{Genomic Assay Prediction} tasks~\cite{avsec2021effective,linder2025predicting}. Current genomic sequence models typically follow a two-stage strategy. In the pretraining phase, we learn the data distribution $p_{\hat{\theta}}(x)$ on a unlabeled dataset from unlabeled data using losses such as masked language modeling (MLM)
$p(\mathbf{x}) = \prod_{t \in \mathcal{M}} p_\theta(\mathbf{x}_t \mid \mathbf{x}_{1:t-1}, \mathbf{x}_{t+1:T})$ 
or next token prediction (NTP) $p(\mathbf{x}) = \prod_{t=1}^{T}p_\theta(\mathbf{x}_t | \mathbf{x}_{1:t-1})$. 

%To adapt into downstream tasks, we update the posterior distribution of $\theta_{\text{task}}$ given a labeled dataset $\mathcal{D}$ and the pretrained $\hat{\theta}$.



% \subsection{Genomic Sequence Modeling}
% \textbf{What}\textit{ is the target}? Similar to earlier work of language modeling~\citep{wang2018glue,wang2019superglue}, the goal of genomic sequence modeling is to learn a function $f:\mathcal{D} \rightarrow \mathcal{Y} $, that maps input sequences to their corresponding biological annotations. Given a labelled dataset \(\mathcal{D} = \{\mathbf{x}^{(n)}, \mathbf{y}^{(n)}\}_{n=1}^N\), with the annotation $\mathbf{y}^{(i)}$. Current genomic tasks consist of three primary classes based on biological annotation type: \textit{i.)} \textbf{DNA Sequence Classification} includes tasks like regulatory region identification~\cite{grevsova2023genomic} and histone marker prediction~\citep{dalla2024nucleotide}. In this task, 
% $f$ directly map a given sequence $\mathbf{x}^{(i)}$ to a class label $y^{(i)}$. \textit{ii.)} \textbf{Genomic Assay Prediction}: this includes works from ~\cite{avsec2021effective,linder2025predicting}. $\mathbf{y}^{(n)}$ is a real value vector. Typically this task has longer sequence length $T$ ranging from 10K to 100K. \textit{iii.)} \textbf{Genetic Variant Prediction} standout as a separate tasks, taking the reference sequence, mutated sequence and metadata as input, to predict class labels (e.g., pathogenic vs. benign variants). Such predictions are critical for clinical applications, aiding in diagnosis and personalized treatment strategies. Also exsit other more diverse tasks. For example, there are general interests  in learning a direct function mapping from DNA to the function of it. 

% \textbf{How} \textit{is sequence modeling performed}? A function $f$ solving the above proposed question, can be directly learned end-to-end with a labeled dataset $\mathcal{D}$ using supervised methods like CNNs ~\cite{} and more recently ``pretrain - finetune'' paradigm, which is becoming the dominant paradigm for GFMs. It involves two stages in which we first learn the data distribution $p_{\hat{\theta}}$ on a unlabeled dataset (pretraining), and then subsequently learn the posterior distribution $p(f|\mathcal{D},\hat{\theta})$ (finetuning). This approach has driven the state-of-the-art performance in many models (e.g. HyenaDNA, DNABERT-2, Nucleotide transformer) across various genomic tasks. For instance, Genomic Pre-trained Network (GPN)~\citep{benegas2023dna} leveraged this approach and achieved state-of-the-art (SoTA) performance for genetic variant prediction. The parameters $\theta$ are typically obtained through different pretraining strategies, common approaches include masked language modeling (MLM)  and next token prediction (NTP), formalized in \cref{eq:1} and  \cref{eq:2}, respectively. Here, $\mathcal{M}$ denotes the set of indices for masked tokens, and the model learns to reconstruct the original sequence or predict subsequent tokens based on the unmasked context.

% \begin{equation} \label{eq:1}
% p^{\text{MLM}}(\mathbf{x}) = \prod_{i \in \mathcal{M}} p_\theta(\mathbf{x}_i \mid \mathbf{x}_{1:i-1}, \mathbf{x}_{i+1:T})
% \end{equation}
% \begin{equation} \label{eq:2}
%     p^{AR}(\mathbf{x}) = \prod_{i=1}^{T}p_\theta(\mathbf{x}_i | \mathbf{x}_{1:i-1}).
% \end{equation}

% \paragraph{\textit{Limitations} of Existing Models} Existing genomic foundation models vary significantly in architecture and tokenization strategies, but fundamental questions remain unresolved. First, despite the widespread adoption of pretraining objectives like masked language modeling (MLM) and next token prediction (NTP), there is no consensus on which objective is more effective for genomic sequence modeling. Due to the prohibitive computational cost of pretraining, there is a lack of systematic comparisons to answer this question. While many believe the bidirectional training enables the model to better learn a complete context and genomic element interaction, recent work by \citet{allen2023physics} argues that MLM may inadvertently disrupt long-range sequence dependencies, limiting knowledge retention.  Secondly, existing approaches typically require training separate task-specific models, meaning $f$ needs to be trained multiple times given K tasks $\mathcal{D}$. This results in 1) the additional cost of storing \( \mathcal{O}(K) \) copies of model weights, incurring significant I/O latency, memory costs, and context-switching penalties 2) prevents models from leveraging shared biological patterns (e.g., conserved regulatory motifs or chromatin accessibility signatures) across tasks. This isolation limits generalization and ignores cross-task dependencies formalized in the joint distribution \( p(f \mid \mathcal{D}_1, \dots, \mathcal{D}_K, \theta) \), where \( \mathcal{D}_k \) represents data for task \( k \).

\subsection{Supervised Finetuning}

Supervised Finetuning (SFT) plays a key role in enhancing the instruction-following~\citep{mishra2021reframing,sanh2021multitask,wei2022chain} and reasoning capabilities~\citep{lambert2024t}. For a pretrained auto-regressive model with a fixed vocabulary \( V_x \) and a labeled dataset  $\mathcal{D}$, SFT maximizes the likelihood:
$
\hat{\theta} = \arg\max_{\theta} \sum_{i=1}^N \log p_\theta\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right).
$
%where \( \mathbf{x}^{(i)} \) and \( \mathbf{y}^{(i)} \) are input-output pairs. 
This process retains the model’s pretrained knowledge while aligning its outputs with task-specific objectives, typically using \textbf{cross-entropy loss} on the target tokens:
\begin{equation} \label{eq:3} \small
    p_\theta(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}) = \sum_{i=1}^N \sum_{t=1}^{T'} \log p_\theta(\mathbf{y}_t^{(i)} | \mathbf{y}^{(i)}_{1:t-1},\mathbf{x}^{(i)}).
\end{equation}
% \begin{equation} \label{eq:3} \small
%     p^{AR}(\mathbf{y}) = \prod_{t=1}^{T'}p_\theta(\mathbf{y}_t | \mathbf{y}_{1:t-1},\mathbf{x}).
% \end{equation}
Notably, $\mathbf{y}$ could include new set of vocabulary $V_{z}$ that do not overlap with $V_{o}$. Therefore, each term in \cref{eq:3} is computed through \Cref{eq:4}.
\begin{equation} \label{eq:4} 
\small
    p_\theta(\mathbf{y}_t \mid \mathbf{y}_{1:t-1},\mathbf{x}) = \frac{\exp(h_{t-1}^\top e_{\mathbf{y}_t})}{\sum\limits_{m \in V_{o}} \exp(h_{t-1}^\top e_m) + \sum\limits_{n \in V_{z}} \exp(h_{t-1}^\top e_n)},
\end{equation} 
where $h_{t-1}$ is the neural representation of the prefix sequence $(\mathbf{y}_{1:t-1},\mathbf{x})$, $e_m$ is the embedding of vocab $m$.

As a result, during the finetuning stage, the embeddings for each new vocabulary token \( n \in V_z \) must be initialized, and the original token probabilities are shifted due to the expanded output space, as detailed in~\citet{hewitt2021initializing}.

% A critical challenge during SFT lies in preserving the model’s pretrained knowledge while adapting it to downstream tasks, necessitating careful balancing of task-specific learning with mitigation of catastrophic forgetting~\cite{zheng2024towards}. 

% SFT offers distinct advantages over traditional approaches like classifier head augmentation. First, it enables unified adaptation to multiple downstream tasks through a single finetuning process. Second, it supports complex output generation—such as mapping DNA sequences to structured multimodal outputs —rather than simple class labels. While SFT has been extensively studied in language models, recent work demonstrates its effectiveness in cross-modal settings, including biological sequences ~\cite{jiang2024neurolm}. In this paper, we extend SFT to pretrained autoregressive genomic language models, enabling flexible adaptation to diverse sequence-to-function prediction tasks while preserving foundational knowledge of genomic grammar and patterns.
