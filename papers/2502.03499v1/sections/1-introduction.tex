\section{Introduction}
\input{figures/multitask_example}


% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/radar_plot.pdf}
%     \vspace{-2em} % Adjust this value as needed
%     \caption{\textbf{Accuracy comparison of Omni-DNA@MultiTask against single-task models and baseline methods across 10 NT tasks.} Omni-DNA@MultiTask achieves significant improvements on four tasks, surpassing both Omni-DNA@SingleTask and other baseline approaches.}
%     \label{fig:multi-tasking}
% \end{figure}

The volume of genomic data has been increasing at an exponential rate over the past decades~\cite{lathe2008genomic}, making it infeasible to annotate these sequences manually. 
Genomic Foundation Models (GFMs)~\cite{nguyen2024hyenadna,zhou2023dnabert,dalla2024nucleotide,schiff2024caduceus}, a type of Deep Neural Networks (DNNs) for genomic sequence modeling, has emerged as essential tools to automate the annotation process. GFMs has been used for associating mutation in the genome with diseases~\cite{benegas2023dna,cheng2023accurate}, revealing regulatory effects of genomic sequences~\cite{avsec2021effective,linder2025predicting}, and genomic elements annotation~\cite{de2024segmentnt}. Although these GFMs achieve great performance on genomic tasks~\citep{zhou2015predicting,grevsova2023genomic}, they are still far away from generalist models, which can handle multiple tasks simultaneously. This contrasts with Large Language Models (LLMs)~\citep{radford2019language,team2023gemini,touvron2023llama}, which have seen remarkable success at solving general tasks from question answering to theorem proofs~\cite{xin2024deepseek}. The success of LLMs is pretraining transformer-based auto-regressive models~\cite{vaswani2017attention} on internet-scale data, followed by post-training to increase instruction-following abilities~\cite{lambert2024t}. Inspired by this, we ask: \textit{is it possible to develop a generalist genomic foundation model for addressing diverse genomic tasks simultaneously?}

Existing GFMs vary significantly in architecture and tokenization strategies, but they all follow a ``pretrain + task-specific finetune'' paradigm: first pretraining on unlabeled DNA sequences, and task-specific Multi-Layer Perceptrons (MLPs) are attached for finetuning. This paradigm limits the generalization of the model in two ways. \textbf{(i)} The pretrained model needs to be separately finetuned $K$ times given $K$ tasks. This results in the additional cost of storing $ \mathcal{O}(K) $ copies of model weights, incurring significant I/O latency, memory costs, and context-switching penalties. This also prevents models from leveraging shared biological patterns (e.g., conserved regulatory motifs or chromatin accessibility signatures~\cite{van2012motif, yang2023pioneer}) across finetuning tasks. \textbf{(ii)} The output space of existing models is rigidly constrained to DNA sequences or predefined genomic labels, limiting their ability to generate diverse, semantically meaningful outputs. As a result, current GFMs struggle for multi-modal downstream tasks such as DNA2Text and DNA2Image.

% This isolation limits generalization and ignores cross-task dependencies formalized in the joint distribution \( p(f \mid \mathcal{D}_1, \dots, \mathcal{D}_K, \theta) \), where \( \mathcal{D}_k \) represents data for task \( k \). Another limitation is the expressitivity of the model arising from this is the uni-modality. Existing finetuning paradigm only allow the mapping from DNA to discrete numerical labels. This greately restrict the type of tasks GFMs can be applied to, almost restricting it to classification tasks. Despite some preliminary work which add limited tokens in to the pretraining stage (GPTDNA), the performance of these model are low. And the mapping are also restrcited to the pre-defined labels. 

To overcome these limitations, we introduce \textsf{\method}, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. This is achieved through carefully ablated pretraining of transformer-based auto-regressive models on DNA sequences and cross-modal multi-task finetuning. 

The \textit{pretraining} stage explores previously overlooked key configurations and their impact on training dynamics. This includes a comparison of non-parametric LayerNorm~\cite{ba2016layer} vs. RMSNorm~\cite{zhang2019root}, RoPE~\cite{su2024roformer} vs. ALiBi~\cite{press2021train} positional embeddings, and other configurations, ultimately leading \method to achieve state-of-the-art performance on 18 out of 26 tasks on the Nucleotide Transformer~\cite{dalla2024nucleotide} and GB benchmarks~\cite{grevsova2023genomic}. When evaluated with a conventional paradigm, \method outperforms existing bidirectional models.

\textit{Cross-modal multi-task finetuning} expands the vocabulary in the model's tokenizer by dynamically modifying the embedding matrix. To address the distribution shift of existing tokens due to vocabulary expansion~\cite{hewitt2021initializing}, we propose \textit{Important Key Repetition} and adopt NEFTune~\cite{jain2023neftune}. Additionally, we implement a task unification strategy, aligning multiple genomic tasks under a unified format. The combination of these techniques enables a single model to solve multiple acetylation and methylation tasks at once, surpassing models trained on each task individually, as demonstrated in \cref{fig:multi-tasking}. Furthermore, beyond conventional genomic tasks, \method explores direct mapping from DNA sequences to functional descriptions and images, unlocking cross-modal capabilities.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7 \columnwidth]{figures/radar_plot.pdf}
     \vspace{-1em} % Adjust this value as needed
    \caption{\textbf{Accuracy comparison of Omni-DNA@mult. against Omni-DNA@sgl. and baselines across 10 NT tasks.} Omni-DNA@mult. achieves the highest average accuracy.}%, and significantly outperforms on 4 tasks.}
    \label{fig:multi-tasking}
    \vspace{-1em}
\end{figure}


\textbf{In summary, our contributions are three-fold}: \\
\noindent $(1)$  We revisit the auto-regressive transformers for GFMs, indicating the potential of next token prediction paradigm.  \\
\noindent$(2)$  We present \method, a unified cross-modal and multi-task GFM ranging from 20M to 1B parameters, achieving SOTA performance on multiple conventional benchmarks.\\ 
\noindent$(3)$ We evaluate \method on novel cross-modal and multi-task genomic tasks, such as DNA2Func and DNA2Image.
%A streamlined toolkit is finally offered. 




 % In contrast, foundation models for genomic (DNA) sequences are still far away from generalists or even domain experts. The dominant approach for building Genomic Foundation Models (GFMs) follows a ``pretrain–finetune'' paradigm: a transformer first is pretrained on unlabeled DNA data, then extra modules are attached on top of the hidden states to fine-tune for a specific downstream task.
% Although these GFMs outperform models trained from scratch on certain biological tasks, such as promoter or species classification~\cite{}, they remain limited to performing only one task per finetuning. For a set of $K$ tasks, the full model needs to be retrained $K$ times. Beyond the inefficiency during fine-tuning, it also fails to leverage the potential synergy among multiple related downstream tasks.

% The emerging ``pretrain–prompt'' paradigm is also been explored in GFMs. For example, DNAGPT~\citep{} designed various tokens (e.g., DNA sequence tokens, property tokens, number tokens) and reformulated the format of different tasks into sequences for pretraining via next-token prediction. Similarly, HyenaDNA~\citep{} experimented soft prompting and few-shot prompting. 
% However, due to different factors, they have not yet achieved great performance compared to full-model fine-tuning. 

% In contrast, foundation models for genomic (DNA) sequences are still far away from generalists or even domain experts. The dominant approach for building Genomic Foundation Models (GFMs) uses transfer learning, by first pretraining Deep Neural Networks (DNNs) on unlabeled DNA, and then attaching an MLP on top of the hidden states~\citep{} to produce outputs for downstream tasks. The DNNs and MLP  need to be finetuned for each of the new tasks. Despite achieving higher accuracy than models trained from scratch in select biological tasks such as promoter classification or species classification~\cite{}, existing GFMs can only perform one task per finetuning. For a set of $K$ tasks, $K$ times the size of single model weight needs to be retrained. 

% The lack of understanding of the following two questions has been the major obstacle, slowing down the development of more capable GFMs: \textbf{(i.)} If pretraining on a large amount of DNA sequences can help the model develop a general knowledge of the syntax or semantics of genomes? \textbf{(ii.)} If such knowledge does exist, how we can extract these knowledge effectively beyond adding a classification head or embedding extraction? Are there more effective ways to communicate with pretrained GFMs, akin to prompting for LLMs?  

% In this work, instead of focusing on building a single model, we aim to provide a perspective on where the field is standing by exploring the knowledge storage and extraction of transformer-based Auto Regressive (AR) models pretrained on genomic sequences. Our work started with pretraining a series of AR models with different sizes ranging from 20 Million to 1 Billion, and test their ability through several basic tasks. These tasks are designed to directly show the model's understanding towards genomic sequences. We then 
% move to existing benchmark datasets, and explore multitasking supervised finetuning. A general task definition framework is designed such that all the tasks can be cast into a consistent format. 
% \textcolor{red}{more detailed description of the results, SOTA output of how many tasks}





