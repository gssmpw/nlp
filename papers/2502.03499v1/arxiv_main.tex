% PLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{enumitem}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
% \usepackage{tabularx}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, calc, backgrounds, positioning, shadows, fit}

\usepackage{xcolor} % For defining colors
\usepackage{xspace}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithmic}
\definecolor{OliveGreen}{RGB}{107,142,35} % Olive green color
\renewcommand{\algorithmiccomment}[1]{\hfill {\color{OliveGreen!75} \(\triangleright\) #1}}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
\usepackage[toc]{appendix} 
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{rotating} % For rotating tables
\usepackage{booktabs} % For better table formatting
\usepackage{pifont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Colors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{olmodna}{RGB}{128,64,128}
\definecolor{groundtruth}{RGB}{53,122,235}
\definecolor{nttransformer}{RGB}{72,155,125}
\definecolor{nttransformer2}{RGB}{127,127,127}
\definecolor{darkgreen}{RGB}{0, 100, 0} % Define a dark green color
\definecolor{lightgrey}{RGB}{211, 211, 211}
\definecolor{lightred}{RGB}{255, 182, 193}
\definecolor{lightgreenRGB}{RGB}{144, 238, 144}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\method}{Omni-DNA\xspace}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning}

\begin{document}



\twocolumn[
% \icmltitle{Omni-DNA: Scaling Autoregressive Transformers for\\ 
% Cross-modal and Multi-task Genomic Foundation Models}

\icmltitle{Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zehui Li}{zzz,yyy}
\icmlauthor{Vallijah Subasri}{vec,UHN}
\icmlauthor{Yifei Shen}{zzz}
\icmlauthor{Dongsheng Li}{zzz}
\icmlauthor{Yiren Zhao}{yyy}
\icmlauthor{Guy-Bart Stan}{yyy}
\icmlauthor{Caihua Shan}{zzz}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Imperial College London}
\icmlaffiliation{zzz}{Microsoft Research}
\icmlaffiliation{vec}{Vector Institute}
\icmlaffiliation{UHN}{University Health Network}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Zehui Li}{zl6222@ic.ac.uk}
\icmlcorrespondingauthor{Caihua Shan}{caihua.shan@microsoft.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}



Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow.
Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce \textsf{\method}, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters.  Our approach consists of two stages: (i) \textbf{pretraining} on DNA sequences with next token prediction objective, and (ii) \textbf{expanding} the multi-modal task-specific tokens and \textbf{finetuning} for multiple downstream tasks simultaneously. When evaluated on the Nucleotide Transformer and GB benchmarks, \method achieves state-of-the-art performance on 18 out of 26 tasks. Through multi-task finetuning, \method addresses 10 acetylation and methylation tasks at once, surpassing models trained on each task individually. Finally, we design two complex genomic tasks, \textit{DNA2Function} and \textit{Needle-in-DNA}, which map DNA sequences to textual functional descriptions and images, respectively, indicating Omni-DNA’s cross-modal capabilities to broaden the scope of genomic applications. All the models are available through \url{https://huggingface.co/collections/zehui127}.

%This leads to context-switching overhead, particularly as model sizes grow. 

%simultaneously solves 10 acetylation tasks outperforming models trained exclusively on individual tasks. 
%Finally, we construct two datasets—\textit{DNA2Function} and \textit{Needle-in-DNA}—allowing \textsf{\method} to emerge with cross-modality abilities, mapping DNA sequences to grammatical text and legitimate images.

% The state-of-the-art genomic foundation models (GFMs) are trained with mask prediction paradigms. Compared with the next-token-prediction, mask prediction requires 



% Instead of focusing on a specific part of building a model, this work aim to provide a perspective on where the field is standing by exploring the knowledge storage and extraction of models pretrained on genomic sequences, and various factors such as model sizes, trained objective. And also the synergy effect between different downstream tasks when training a multi-task model. As a side product, we produce a series of transformer-based Autoregressive models, termed OLMoDNA, achieving the state of the art performance on NT-downstream, GB benchmark, and other more challening tasks. These moodels is capable of sovling multiple tasks with one finetuning.  Meantime, a task definition framework is proposed, All the existing tasks can be easily casted to the format, but it also enable the framing of more challening task such as the direct mapping from DNA sequence to the function description of the DNA sequuence. To facilitate the follow-up work, we have made the pretraining and inference code, also sft code and pretrained models available. 
\end{abstract}

\input{sections/1-introduction}
\input{sections/2-preliminary}
\input{sections/2.5-setup}
\input{sections/3-results-tasks}
\input{sections/4-results-knowledge-extraction}
\input{sections/5-results-knowledge-storage}
\input{sections/6-conclusion}
% \newpage
% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field
% of Machine Learning for genomics. There are many potential societal
% consequences of our work, none which we feel must be
% specifically highlighted here.


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\clearpage
\onecolumn

\input{appendix/2-pretrain-model-parameter}
\input{appendix/1-append-nt-results}
\input{appendix/3-deduplication}
\input{appendix/4-nt_dowstream_hyperparamter}
\input{appendix/5-needle-in-dna}
\input{appendix/4.5-dna2func}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
