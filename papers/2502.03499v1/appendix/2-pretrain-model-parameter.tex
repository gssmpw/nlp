\section{Pretraining Model Configurations}\label{app:pretrain-config}
The family of Omni-DNA has six models, and their parameters and architectures are shown in Table~\ref{tab:hyperparameters} and~\cref{tab:omni_dna_dnabert_nttransformer}.

\begin{table*}[ht!]
\centering
\caption{Architecture for 6 Omni-DNA models.}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Parameters} & \textbf{Layers} & \textbf{Heads}  & \(\mathbf{d_{\text{model}}}\) & \textbf{LayerNorm} \\ \midrule
20M                & 8              & 8                          & 256             & RMS          \\
60M                & 8              & 8                         & 512             & RMS          \\
116M               & 12             & 12                        & 768             & No Bias      \\
300M               & 16             & 16                        & 1024            & RMS          \\ 
700M               & 16             & 16                        & 1536            & RMS          \\ 
1B                 & 16             & 16                        & 2048            & No Bias      \\ 
\bottomrule
\end{tabular}
\label{tab:hyperparameters}
}
\end{table*}


\begin{table}[ht]
\centering
\caption{Comparison of pretraining setup between Omni-DNA, DNABERT2, and NT-transformer.}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
 & \textbf{Omni-DNA (six sizes)} & \textbf{DNABERT2 (116M)} & \textbf{NT-transformer (5 sizes)} \\
\midrule
Layer norm type         & non-parametric/RMSNorm  & parametric        & parametric \\
Positional embeddings   & RoPE            & ALiBi           & RoPE \\
Attention variant       & full            & full            & full \\
Biases                  & none            & in both LN and Attention         & in both LN and Attention \\
Block type              & sequential      & sequential     & sequential \\
Activation             & SwiGLU          & GEGLU         & SwiGLU \\
Sequence length (In Tokens)         & 250            & 128           & 1000 \\
Batch size (Across GPUs)  & 384            & 4096           & 512 \\
Total Steps     & $800000$& $150000$& $16,000$ \\
Warmup steps            & 5000            &  -           & 16,000 \\
Peak LR                 & 3.0E-04         & 5.0E-04        & 1.0E-04 \\
Minimum LR              & 3.0E-05         & 0        & 5.0E-05 \\
Weight decay            & 0.1             & 1e-5            & 0.1 \\
Beta1                   & 0.9             & 0.9            & 0.9 \\
Beta2                   & 0.95            & 0.98           & 0.999 \\
Epsilon                 & 1.0E-05         & 1.0E-05        & 1.0E-08 \\
LR schedule             & linear          & linear         & linear \\
Gradient clipping       & global 1.0      & -    &- \\
Training Precision  & Mxied (bf16 for most operations)        & -           & - \\
Training Framework  & Pytorch Built-in FSDP        &  Pytorch HuggingFace trainer         & - \\
Training Duration (min)   & 8 GPUs* 1 days        &  8GPUs * 14 days        & 128 GPUs*1days \\
Training Duration (max)  & 8  GPUs* 7 days       &  8GPUs * 14 days        &  128 GPUs*28days \\
GPU Types  & A100 Nvidia GPU 40GB       &  Nvidia RTX 2080Ti GPUs.         &  A100 Nvidia GPU \\
\bottomrule
\end{tabular}
}
\label{tab:omni_dna_dnabert_nttransformer}
\end{table}
%\newpage