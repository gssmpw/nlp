\section{Detailed Deduplication Process}
\label{app:deduplication}
\subsection{Handling Non-\{A, T, G, C\} Characters}
The raw genome data from the reference genome includes additional characters beyond \{A, T, G, C\}, which represent ambiguities or gaps. These characters are defined as follows:
\begin{itemize}
    \item \textbf{N}: Represents any nucleotide (A, T, G, or C).
    \item \textbf{R}: Represents purines (A or G).
    \item \textbf{Y}: Represents pyrimidines (C or T).
    \item Other characters (e.g., \textbf{W}, \textbf{S}, \textbf{K}, \textbf{M}, etc.) represent specific nucleotide subsets or unknown bases.
\end{itemize}
These characters were removed during preprocessing to retain only the core nucleotide sequences (\{A, T, G, C\}), ensuring consistency and facilitating the deduplication process.

\subsection{Chunk-Based Deduplication}
The deduplication procedure was performed as follows:
\begin{enumerate}
    \item \textbf{Chunking}: The genome was divided into non-overlapping chunks of 1024 base pairs.
    \item \textbf{Exact Matching}: Identical sequences across the dataset were identified and removed.
    \item \textbf{Efficiency}: This step utilized hashing techniques and optimized string comparison algorithms to handle the large dataset efficiently.
\end{enumerate}

\subsection{Results}
The raw genome dataset initially contained approximately 170 billion nucleotides. After applying the deduplication process, the dataset was reduced to 30 billion nucleotides, representing unique sequences across multiple species.

\subsection{Rationale for Multiple Epochs}
Although deduplication significantly reduced the size of the dataset, the smaller dataset was iterated over multiple epochs during pretraining. This approach ensured that repeated sequences were temporally separated during the training process. By maximizing the temporal distance between occurrences of the same sequence, the risk of overfitting was mitigated, leading to better generalization performance.