\section{Needles in a DNA}
\label{app:needle}

For random DNA sequence generation, we use the Expectation-Maximization (EM) algorithm implemented in the Python package \texttt{hmmlearn} to train a Hidden Markov Model (HMM) with two states, learning both the transition and emission probabilities. We then insert one of the functional motifs from \{TATAAA, CAAT, GGGCGG, TTAGGG\} into the randomly generated sequences. Finally, we filter the sequences, preserving only those that contain exactly one instance of a motif. We generate 12,000 sequences for each motif type.

Next, we process the MNIST images through a trained Vector Quantized Variational Autoencoder (VQ-VAE) to obtain discretized token representations corresponding to each motif type. The pretraining and architecture details of the VQ-VAE are described in the following section.


\subsection{VQ-VAE}

To learn discrete representations of image features, we employ a Vector Quantized Variational Autoencoder (VQ-VAE), which consists of an encoder, a vector quantization layer, and a decoder.

\subsubsection{Encoder}
The encoder transforms input images into a latent space representation. It is implemented as a convolutional neural network (CNN) with residual connections. The architecture consists of:

\begin{itemize}
    \item An initial $4 \times 4$ convolution with stride 2 and padding 1, reducing the spatial dimensions while increasing feature depth.
    \item A second $4 \times 4$ convolution with stride 2, further downsampling the input.
    \item A $3 \times 3$ convolution maintaining the number of hidden channels.
    \item A residual stack of multiple layers with $3 \times 3$ and $1 \times 1$ convolutions, enhancing feature extraction.
\end{itemize}

The output of the encoder is then passed through a $1 \times 1$ convolution to adjust the feature dimensionality before quantization.

\subsubsection{Vector Quantization}
The latent representations are discretized using vector quantization, which maps continuous encodings to the nearest prototype in a learned codebook. We use two different approaches: 

\begin{itemize}
    \item The standard VQ-VAE, where the codebook is learned via backpropagation.
    \item The exponential moving average (EMA) variant, which updates the codebook using an EMA of past embeddings to stabilize training.
\end{itemize}

Given a latent representation $z_e$, the closest embedding vector $z_q$ is selected based on the Euclidean distance. The loss function includes a commitment loss term, weighted by a hyperparameter $\beta$, ensuring that encoded representations stay close to their assigned embedding vectors.

\subsubsection{Decoder}
The decoder reconstructs the input from the discretized representation. It mirrors the encoder with transposed convolutions for upsampling:

\begin{itemize}
    \item A $3 \times 3$ convolution to process the quantized representation.
    \item A residual stack for improved reconstruction.
    \item Two transposed convolutions ($4 \times 4$, stride 2) to upsample back to the original image size.
\end{itemize}

\subsubsection{Training and Hyperparameters}
We train the VQ-VAE using the Adam optimizer with a learning rate of $5 \times 10^{-4}$. The following hyperparameters are used:

\begin{itemize}
    \item Number of hidden channels: 128
    \item Number of residual layers: 2
    \item Number of residual hidden units: 32
    \item Codebook size: 6
    \item Embedding dimension: 32
    \item Commitment cost ($\beta$): 0.25
    \item Decay factor (for EMA variant): 0.99
\end{itemize}

The reconstruction loss is measured using mean squared error (MSE), and training is monitored with additional metrics such as perplexity and codebook usage. The trained VQ-VAE is used to convert MNIST images into discrete tokens, which are then linked to DNA motifs for downstream analysis.

 \begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{appendix/concatenated_image.png}
    % \vspace{-2em} % Adjust this value as needed
    \caption{\textbf{Images Generated by Omni-DNA.} These digits are novel in the sense that they do not exist in the training set.}
    \label{fig:mnist_sample}
% https://drive.google.com/file/d/1wDsMfefBgGdnAlqFbvc8slthBj4nuMys/view?usp=sharing
\end{figure*}




% \begin{algorithm}
% \caption{\textbf{OmniGenome-FT: Genomic Finetuning to build Multitasking Genomic Foundation Model}}
% \begin{algorithmic}
% \REQUIRE $K$ tokenized genomic datasets $\{\mathcal{D}_k\}_{k=1}^K$, pretrained model $f_\theta$,vocabulary $V_\text{pretrained}$, pretrained model parameters $\theta$, loss function $\text{loss}(\cdot)$, optimizer $\text{opt}(\cdot)$
% \REQUIRE Hyperparameter: importance weight $\alpha \in \mathbb{R}^+$
% \STATE Initialize $\theta$ from a pretrained model.
% \STATE $\mathcal{D}_\text{uni} \gets \bigcup_{k=1}^K \mathcal{D}_k$ \COMMENT{Task Unification}
% \STATE $V_\text{task} \gets V(\mathcal{D}_k)$ \COMMENT{Unique vocab from all tasks}
% \STATE $V_\text{expand} \gets V_\text{pretrained} \cup V_\text{task}$ \COMMENT{Expand vocab set}
% \REPEAT
%     \STATE Sample minibatch $(X^{(i)}, Y^{(i)}) \sim \{\mathcal{D}_\text{uni}\}$ \COMMENT{Batch size $B$, seq. length $L$}
%     \STATE $\hat{Y}^{(i)} \gets f_\theta(X^{(i)})$ \COMMENT{Make predictions}
%     \STATE $w_{t} \gets \alpha$ if token $t$ is important, else $1$ \COMMENT{Set token weights}
%     \STATE $\text{loss}_{\alpha} \gets -\frac{1}{|X^{(i)}|} \sum_{t \in X^{(i)}} w_t \cdot \log P(Y_{t}^{(i)} | \hat{Y}_{t}^{(i)})$ \COMMENT{Compute weighted loss}
%     \STATE $\theta \gets \text{opt}(\theta, \text{loss}_{\alpha})$ \COMMENT{Update model parameters}
% \UNTIL{Stopping criteria met or max iterations.}
% \label{algorithm:1}
% \end{algorithmic}
% \end{algorithm}


% \subsection{Examples generated by three models}
% \begin{algorithm}
% \caption{\textbf{OmniGenome-FT (Replicate Important Tokens in \(\mathbf{y}\))} 
%          \\ \small Genomic Finetuning with Cross-Entropy Objective}
% \begin{algorithmic}[1]
% \REQUIRE $K$ tokenized genomic datasets $\{\mathcal{D}_k\}_{k=1}^K$, 
%          pretrained autoregressive model $f_\theta$, 
%          pretrained model parameters $\theta$, 
%          vocabulary $V_\text{pretrained}$, 
%          optimizer $\text{opt}(\cdot)$
% \REQUIRE Hyperparameter: Repeating Factor $\alpha \in \mathbb{Z}$ 
% \STATE $\mathcal{D}_\text{uni} \gets \bigcup_{k=1}^K \mathcal{D}_k$
% \COMMENT{Task Unification}
% \STATE $V_\text{task} \gets \bigcup_{k=1}^K V(\mathcal{D}_k)$
% \COMMENT{Unique vocab from all tasks}
% \STATE $V_\text{expand} \gets V_\text{pretrained} \cup V_\text{task}$
% \COMMENT{Unique vocab from all tasks}
% % \STATE \textbf{function} ReplicateImportantTokens($Y$):
% % \STATE \quad \textbf{for each} token $t$ in $Y$:
% % \STATE \quad \quad \textbf{if} $t$ is \emph{important}:
% % \STATE \quad \quad \quad \textbf{replicate} $t$ in $Y$ by factor $\alpha>1$ 
%         % \COMMENT{E.g., insert $t$ $(r-1)$ more times}
% % \STATE \quad \textbf{return} Modified $Y$

% \REPEAT
%     \STATE $\{(X^{(i)}, Y^{(i)})\}_{i=1}^B \sim \mathcal{D}_\text{uni}$ \COMMENT{Batch size $B$, seq. length $L$}
    
%     \FOR{$i$ from 1 to $B$}
%         \STATE $Y^{(i)} \gets \text{ReplicateImportantTokens}\bigl(Y^{(i)},\alpha \bigr)$ 
%             \COMMENT{Replicate important tokens in target for Classification Tasks}

%         \STATE $\text{loss}^{(i)} \gets -\sum_{t=1}^{|Y^{(i)}|} 
%             \log p_\theta\Bigl(Y^{(i)}_t \,\Big\vert\, Y^{(i)}_{1:t-1},X^{(i)}\Bigr)$
%     \ENDFOR
%     \STATE $\text{loss}_\text{batch} \gets \frac{1}{B}\sum_{i=1}^{B} \text{loss}^{(i)}$
%     \STATE $\theta \gets \text{opt}\bigl(\theta, \text{loss}_\text{batch}\bigr)$ 
%         \COMMENT{Update model parameters}
% \UNTIL{\emph{stopping criterion} or \emph{max iterations}}

% \label{algorithm:omnigenome-ft}
% \end{algorithmic}
% \end{algorithm}