\section{Finetuning with Classificatoin Head}
\label{app:finetune_ch}
\label{app:nt_downstream_ft}
\label{app:gb_ft}
\paragraph{Basic Setup} 
We closely follow the setup from Caduceus~\cite{schiff2024caduceus} for evaluating Omni-DNA. The following models are compared: \method, DNABERT-2~\cite{zhou2023dnabert}, NT-Transformer~\cite{dalla2024nucleotide}, HyenaDNA~\cite{nguyen2024hyenadna}, and the Caduceus models~\cite{schiff2024caduceus}.  

For NT Downstream tasks, we use a maximum fine-tuning epoch of 20, while for the Genomic Benchmark (GB) tasks, we use a maximum of 10 epochs. Both NT Downstream and GB tasks include a training set and a test set. For hyperparameter search, 10\% of the training set is reserved as a validation set.  

\paragraph{Hardware \& Framework} 
All fine-tuning is conducted on a single NVIDIA A100 40GB GPU. We utilize the Hugging Face \texttt{Trainer} API for full-size fine-tuning of the pretrained checkpoints. Models are loaded using the \texttt{AutoModelForSequenceClassification} class, which automatically adds a linear layer on top for sequence classification.  

Our experimental results align with those reported in Caduceus~\cite{schiff2024caduceus}. For consistency, we report statistics from~\cite{schiff2024caduceus} for two Caduceus models, DNABERT-2, NT-Transformer 500M, and HyenaDNA in NT Downstream tasks, as well as CNN, HyenaDNA, and CADUCEUS-PH in GB tasks. For NT2.5B, we use the results from its original paper~\cite{dalla2024nucleotide}.

We replicate experiments for the following models: NT50M, NT100M, NT250M, along with six \method models on 18 NT Downstream tasks. Additionally, we fine-tune DNABERT-2 and \method 116M on the eight Genomic Benchmark tasks.  

\paragraph{Hyperparameters} 
A simple grid search is performed for each model using hyperparameters provided in \cref{tab:finetuning_hyperparameters}.  
\begin{table}[ht]
\centering
\caption{Finetuning hyperparameters.}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Peak learning rate & $[3 \times 10^{-4}, 1 \times 10^{-5}, 5 \times 10^{-6} ]$ \\
Per device train batch size & [8,16,32] \\
Max gradient norm & 1.0 \\
Weight decay & 0.1 \\
Adam $\beta_1$ & 0.9 \\
Adam $\beta_2$ & 0.999 \\
Adam $\epsilon$ & $1 \times 10^{-8}$ \\
Optimizer & ADAMW \\
\bottomrule
\end{tabular}
\label{tab:finetuning_hyperparameters}
}
\end{table}

\paragraph{Additional Notes}
We observe a performance gap in HyenaDNA between our replication results, which align with those of~\cite{schiff2024caduceus}, and the results reported in the original paper~\cite{nguyen2024hyenadna}, both on NT downstream and GB tasks. Our replication follows our own setup, yielding results consistent with~\cite{schiff2024caduceus}. Upon reviewing the code within the container image provided by~\cite{nguyen2024hyenadna}, we identified additional techniques such as Exponential Moving Average (EMA) and data augmentation that were employed, which may account for the discrepancy. In this work, we use HyenaDNA results obtained without these techniques as the baseline.