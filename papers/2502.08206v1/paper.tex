\documentclass[a4paper, USenglish, numberwithinsect, cleveref]{lipics-v2021}

\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository
\nolinenumbers %uncomment to disable line numbering

\usepackage{algorithm}  % float wrapper for algorithms
\usepackage{algorithmic}  % algorithm typesetting environment
\bibliographystyle{plainurl}% the mandatory bibstyle

\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}
\newcommand{\celine}[1]{\todo[color=green!20]{{\bf C:} #1}}
\newcommand{\eric}[1]{\todo[color=black!20]{{\bf E:} #1}}
\newcommand{\abdelkrim}[1]{\todo[color=orange!20]{{\bf A:} #1}}
\newcommand{\matt}[1]{\todo[color=blue!20]{{\bf M:} #1}}

%% notation
%\usepackage{bbm}  % \mathbbm{1}
\usepackage{bbold}

\newcommand\bE{\mathbb{E}}
\newcommand\bP{\mathbb{P}}
\newcommand\bR{\mathbb{R}}
\newcommand\bN{\mathbb{N}}
\newcommand\bZ{\mathbb{Z}}

\newcommand\cP{\mathcal{P}}
\newcommand\cX{\mathcal{X}}

\newcommand\re{\mathrm{e}}
\newcommand\rNN{\mathrm{NN}}

\newcommand\un{{\mathfrak{n}}}
\newcommand\um{{\mathfrak{m}}}

\newcommand\one[1]{\mathbb{1}\mathopen{}\left[#1\right]\mathclose{}}
\newcommand\param{w}

\DeclareMathOperator{\cov}{Cov}

% acronyms
\usepackage[acronym]{glossaries}
\glsdisablehyper
\newacronym{CS}{CS}{central server}
\newacronym{FIFO}{FIFO}{first-in-first-out}
\newacronym{FL}{FL}{federated learning}
\newacronym{iid}{iid}{independent and identically distributed}
\newacronym{SGA}{SGA}{stochastic gradient ascent}
\newacronym{SGD}{SGD}{stochastic gradient descent}

\title{Optimizing Asynchronous Federated Learning: A~Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency}

\titlerunning{Optimizing Asynchronous Federated Learning}

\author{Abdelkrim Alahyane}{%
	EMINES-UM6P, Ben Guerir, Morocco
	\and LAAS--CNRS, Université de Toulouse, CNRS, Toulouse, France
}%
{abdelkrim.alahyane@emines.um6p.ma}%
{}%{https://orcid.org/0000-0002-1825-0097}%
{}

\author{C\'eline Comte}{%
	LAAS--CNRS, Université de Toulouse, CNRS, Toulouse, France
}{celine.comte@cnrs.fr}
{https://orcid.org/0009-0005-9413-7124}
{}

\author{Matthieu Jonckheere}{%
	LAAS--CNRS, Université de Toulouse, CNRS, Toulouse, France
}{matthieu.jonckheere@laas.fr}
{https://orcid.org/0000-0003-3576-5866}
{}

\author{Eric Moulines}{%
	\'Ecole Polytechnique, Palaiseau, France
}{eric.moulines@polytechnique.edu}
{https://orcid.org/0000-0002-2058-0693}
{}

\authorrunning{A.\ Alahyane, C.\ Comte, M.\ Jonckheere, and E.\ Moulines}

\Copyright{Abdelkrim Alahyane, C\'eline Comte, Matthieu Jonckheere, and Eric Moulines}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Mathematics of computing~Stochastic processes}

\keywords{Federated learning, relative delay, throughput, Little's law} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

\funding{The research presented here was supported in part by ANR EPLER, Projet E2CC, and the ``Data Science \& Processus Industriels'' chair funded by École polytechnique, the Mohammed VI Polytechnique University, and Fondation de l’X. This research was also facilitated by the support of LabEx CIMI via the SOLACE project-team and the ``Stochastic control and learning for complex networks'' thematic semester.}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering





\begin{document}

\maketitle

\begin{abstract}
	Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like \texttt{FedAsync} and \texttt{GeneralizedFedAsync} address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little's law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date. Second, we observe that naively optimizing this metric leads us to slow down the system drastically by overemphazing staleness at the detriment of throughput. This motivates us to introduce an alternative metric that also takes system speed into account, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show that these optimizations enhance accuracy by 10\% to 30\%.
\end{abstract}





\section{Introduction}

\Gls{FL} is a  distributed learning strategy that enables model training across multiple clients under the supervision of a \gls{CS}, without requiring data sharing \cite{konevcny2015federated, mcmahan2017communication}. In synchronous \gls{FL}, \gls{SGD} is executed in rounds; during each round, the \gls{CS} sends the current model parameters to a subset of clients, waits until all of them return a new stochastic gradient estimate, and then updates the model parameters before sending them to another batch of clients. Unfortunately, the performance of this synchronous approach is often hindered by the variability of computational speeds among clients, leading to the straggler effect \cite{chen2021towards}.

Asynchronous algorithms such as \texttt{FedAsync} \cite{xie2019asynchronous, chen2020asynchronous, xu2021asynchronous} aim to tackle these challenges. This framework allows clients and the \gls{CS} to communicate asynchronously; in particular, the \gls{CS} may update the model parameters while clients are estimating gradients (possibly based on outdated model parameters). Asynchronicity is implemented by allowing tasks (i.e., requests for gradient estimates) to be queued at the clients \cite{koloskova2022sharper}.
Intuitively, asynchronous \gls{FL} has the potential to speed up the system by circumventing the straggler effect, possibly at the cost of errors due to the staleness of the model parameters used to estimate gradients.

Consequently, a significant research effort has been devoted to analyzing the performance of asynchronous \gls{FL} in both homogeneous and heterogeneous data settings \cite{chen2020asynchronous, chen2021towards, xu2021asynchronous,koloskova2022sharper}. However, as we will review in \Cref{sec:related-work}, almost all existing analyses overlook the intricate dynamics of queues and the variability in processing capabilities across clients. In contrast, \cite{leconte2024queuing} showed recently that the performance of asynchronous \gls{FL} critically depends on these via the staleness of the model parameters used by the clients to estimate gradients. Staleness is captured by the \emph{relative delay}, defined as the (stochastic) number of times the \gls{CS} updates the model parameters while a task is held at a client (either queued or being processed). While existing analyses assume this relative delay is bounded irrespective of the system parameters, the results of \cite{leconte2024queuing} imply that relative delay depends heavily on these parameters and may grow arbitrarily large even in realistic scenarios.

As we will see in \Cref{sec:contributions},
we provide actionable means
of optimizing performance in asynchronous \gls{FL}.
First, building on the results of \cite{leconte2024queuing},
we derive an explicit expression
for the mean relative delay and its gradient
by leveraging the framework of Jackson networks~\cite{jackson1957networks},
which in turn allows us to design a gradient-descent algorithm
that optimizes performance.
In a second time, we observe that
minimizing the average norm of the gradient per update,
as it is done traditionally in the literature,
may actually be counterproductive:
since it completely ignores the throughput of model updates,
this leads us to slow the system down to the pace of the slowest client, thus underutilizing all other clients.
Although we focus on an extension of \texttt{AsyncSGD} called \texttt{Generalized AsyncSGD}, we believe our results are relevant to other asynchronous \gls{FL} algorithms.



\subsection{Related work} \label{sec:related-work}

\gls{FL} has shifted decisively from synchronous \cite{wang2020tackling, qu2021feddq, makarenko2022adaptive, mao2022communication, tyurin2022dasha} to asynchronous models, driven by challenges in scaling, resource allocation, and coordination. Key studies highlight the limitations of synchronous \gls{FL}, such as the susceptibility to delays from stragglers and difficulties in managing communication rounds as the number of agents increases \cite{xie2019asynchronous}. This has accelerated the adoption of asynchronous approaches, such as \texttt{FedAsync} and its variants \cite{xie2019asynchronous, chen2020asynchronous, xu2021asynchronous}, which incorporate memory-based updates, adaptive learning rate adjustments, and strategies to reduce staleness and handle varying computational speeds.

Nevertheless, limitations persist, especially in dealing with heterogeneous data environments \cite{koloskova2022sharper}. Innovations such as \texttt{FedBuff} \cite{Nguyen2021FederatedLW}, with \gls{CS}-side buffering, and \texttt{AsyncSGD} \cite{koloskova2022sharper}, with its delay guarantees, address some issues, but existing performance guarantees often rely on simplified assumptions about queue dynamics and processing delays, affecting \gls{FL} system performance and efficiency.

Recent work like \texttt{AsGrad} \cite{islamov2023asgrad}, which extends asynchronous \gls{FL} analysis under bounded-gradient assumptions, underscores the benefits of deeper insights into these dynamics. The typical finite-delay assumption reveals a major gap in the literature: existing models fail to fully capture the unpredictable and heterogeneous nature of client speeds and network conditions, even if these are the fundamental motivation for introducing asynchronous \gls{FL} in the first place. Despite progress in the understanding of asynchronous FL, a comprehensive analysis of the underlying stochastic queuing dynamics remains urgently needed.
A preliminary attempt is made in~\cite{leconte2024queuing}, which introduces \texttt{Generalized AsyncSGD}—an algorithm that utilizes non-uniform client selection to address queuing dynamics and heterogeneous server speeds. However, their analysis falls short in providing explicit performance bounds, instead relying on scaling regimes to approximate the system behavior\celine{One could argue that you were still able to optimize the bound~$G$. Maybe explain why it's not sufficient to analyze scaling regimes and/or why it was "hard" to optimize~$G$?}. A detailed analysis is central to the optimization of \gls{FL} systems, not only in terms of traditional network performance metrics, but also for achieving effective, efficient, and fair learning outcomes with diverse and distributed clients.
Our results highlight in particular that routing strategies should be adapted based on the specific bottlenecks imposed by applications, such as the number of computations rounds versus actual training time.



\subsection{Contributions} \label{sec:contributions}

Our results stem from key theoretical results allowing the explicit characterization of the impact of queue dynamics on the performance of asynchronous \gls{FL}. They can be summarized as follows:
\begin{description}
	\item[Derive tractable bounds on loss gradients]
	Using the framework of queuing theory,
	we derive exact and tractable formulas for the mean relative delay and its gradient.
	In general, these formulas can be estimated in time $O(n^2 m^2)$,
	where $n$ is the number of clients
	and $m$ the concurrency level,
	or they can be estimated through Monte Carlo simulations.
	Further simplifications allow us to compute them in time $O(n)$ for simple routing strategies.
	\item[Optimize performance]
	Leveraging this result,
	we design an algorithm
	that optimizes the performance of \texttt{Generalized AsyncSGD}
	by minimizing the bound of~\cite{leconte2024queuing}.
	Our result also allows us to gauje the bound sensitivity to crucial system parameters,
	such as the ratio of the concurrency level to the number of clients,
	and the clients' service speeds.
	These insights are relevant to other asynchronous \gls{FL} algorithms, and they underscore that routing strategies should be adapted based on application-specific bottlenecks.
	\item[Account for clock-time performance]
	We observe both analytically and numerically
	that minimizing the average norm-square of the gradient per update
	actually leads us to slow down the system considerably
	by underutilizing all clients but (the slowest) one.
	Roughly speaking, since this metric ignores the throughput of model updates,
	it is optimized by minimizing staleness,
	which is achieved by 
	giving priority to the slowest client.
	This motivates us to introduce an alternative metric that explicitly accounts for throughput, and for which we derive a tractable upper-bound that can again be optimized efficiently.
\end{description}
Our findings provide both qualitative insights into the impact of queuing dynamics and efficient numerical methods to optimize performance.
Our experiments on real-world datasets show that tuning the routing strategies and/or the concurrency level can improve accuracy by 10\% to 30\%.



\subsection{Notations}
$\bZ, \bN, \bN_{> 0}, \bR, \bR_{\ge 0}, \bR_{> 0}$ denote the sets of integers, non-negative integers, positive integers, real numbers, non-negative real numbers, and positive real numbers.
Let $| \cdot |$ denote the $\ell_1$-norm and $\one{\,\cdot\,}$ the indicator function.
For each $n, m \in \bN_{> 0}$,
let $\cX_{n, m} = \{x \in \bN^n: |x| = m\}$
denote the set of $n$-dimensional natural-number-valued vectors with $\ell_1$-norm~$m$.
For every $n \in \bN_{> 0}$,
let $\cP_n = \{p \in \bR^n: 0 < p_i < 1 \text{ for } i \in \{1, 2, \ldots, n\} \text{ and } |p| = 1\}$.

\glsresetall





\section{Model and prior results}



\subsection{Asynchronous federated learning} \label{sec:fl}

The goal in \gls{FL} is to optimize the average performance of a model across multiple clients under the supervision of a \gls{CS}: $\min_{\param \in \bR^d} f(w)$,
where $f(w) = \frac{1}{n} \sum_{i=1}^n f_i(\param)$, and
\begin{align*}
f_i(\param) &= \mathbb{E}_{(x, y) \sim \mathcal{D}_i}[\ell_{i}(\mathrm{NN}(x, \param), y)],
\quad i \in \{1, 2, \ldots, n\}.
\end{align*}
Here, $\param$ denotes the parameters of a deep neural network, $d$ the number of parameters (including weights and biases), $\rNN(x, \param)$ the prediction function of the neural network, $n$ the number of clients, $\ell_{i}$ the local loss function of client~$i$, and $\mathcal{D}_i$ the data distribution at client~$i$. Each client~$i$ approximates the gradient $\nabla_w f_i(w)$ of its local loss function using a stochastic gradient denoted by~$g_{i}(w)$. The computation of such a stochastic gradient by a client is called a \emph{task}. 

\texttt{Generalized AsyncSGD} \cite{leconte2024queuing}
is shown in Algorithms~\ref{alg:CS} (\gls{CS})
and~\ref{alg:client} (client~$i$).
A task assigned to a busy client is queued according to the \gls{FIFO} policy.
As we will see, performance
depends critically on two parameters:
$p$, the routing probability vector of tasks to clients;
and $m$, the concurrency~\cite{koloskova2022sharper},
defined as the number of tasks that are concurrently dispatched
to the clients (either queued or being processed).

Let us focus on \Cref{alg:CS} (\gls{CS} perspective).
The model parameter is initialized to a random vector $\param_0$,
and the system state is initialized to a random vector
$\xi = (\xi_1, \xi_2, \ldots, \xi_n) \in \cX_{n, m}$,
such that $\xi_i$ is the number of tasks initially dispatched to client~$i$,
for each $i \in \{1, 2, \ldots, n\}$.
After this initialization, each iteration~$t \in \{0, 1, \ldots, T\}$
of the \texttt{for} loop (Line~7) proceeds as follows:
whenever a client~$C_t$
completes a task and reports a gradient estimate $g_{C_t}(w_{I_t})$
(Line~8; $I_t$ will be defined shortly),
the \gls{CS} immediately updates the model parameter
(Line~9)
and sends it to the next client $A_{t+1}$,
where $\bP(A_{t+1} = i) = p_i$
for each $i \in \{1, 2, \ldots, n\}$
(Lines~10 and~11).
Recall that a client can be chosen
even if it is processing a task.

\begin{algorithm}[b]
	\caption{\texttt{Generalized AsyncSGD (CS)}}
	\label{alg:CS}
	\begin{algorithmic}[1]
		\STATE {\bf Input:} Numbers $T$, $n$, and $m$ of rounds, clients, and tasks; routing $p$; learning rate $\eta$
		\STATE Initialize parameters $\param_0$ randomly
		\STATE Initialize state vector $\xi \in \cX_{n, m}$ randomly
		\FOR{$i = 1, 2, \ldots, n$}
		\STATE Send $\xi_i$ times model parameter $\param_0$ to client $i$
		\ENDFOR
		\FOR{$t = 0, \dots, T$ \label{line:for}}
		\STATE \gls{CS} receives stochastic gradient $g_{C_t}(\param_{I_t})$ from a client $C_t$ \label{line:CS1}
		\STATE Update $\param_{t+1} \leftarrow \param_t - \frac{\eta}{n p_{C_t}} g_{C_t}(\param_{I_t})$ \label{line:CS2}
		\STATE Sample a new client $A_{t+1}$ according to $p$ \label{line:CS3}
		\STATE Send model parameter $\param_{t+1}$ to client $A_{t+1}$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
	\caption{\texttt{Generalized~AsyncSGD~(Client~$i$)}}
	\label{alg:client}
	\begin{algorithmic}[1]
		\STATE {\bf Input:} Queue of received parameters, local dataset
		\IF{Queue is not empty}
		\STATE Take the received parameter $\param$ from the queue using a FIFO policy
		\STATE Compute the gradient estimate $g_{i}(\param)$
		\STATE Send the gradient to the \gls{CS}
		\STATE Repeat
		\ENDIF
	\end{algorithmic}
\end{algorithm}

Time indices are such that,
for each $t \in \{1, 2, \ldots, T\}$,
$A_t$ and $C_t$ correspond to
the same (model-parameter update) round, 
where a round is defined as the time between the assignment of a task to a client and the next task completion (leading to a model update). In \Cref{sec:optimize-time,sec:num}, we will see that throughput, defined as the inverse of the (clock-time) duration of a typical round, has a crucial impact on performance.
For each $t \in \{0, 1, \ldots, T\}$, we let $X_t = (X_{1, t}, X_{2, t}, \ldots, X_{n, t}) \in \cX_{n, m-1}$ denote the state at the end of round~$t$, so that, for each $i \in \{1, 2, \ldots, n\}$, we have $X_{i, 0} = \xi_i - \one{C_0 = i}$ and, for each $t \in \{1, 2, \ldots, T\}$,
\begin{align} \label{eq:X-rec}
X_{i, t} &= X_{i, t-1} + \one{A_t = i} - \one{C_t = i}.
\end{align}
The processing times of successive tasks at client~$i$ are independent, exponentially distributed with rate $\mu_i > 0$, for each $i \in \{1, 2, \ldots, n\}$.
In particular, for each $t \in \{1, 2, \ldots, T\}$, we have
$\bP[C_t = i | X_{t-1}, A_t] \propto \mu_i$
for each $i \in \{1, 2, \ldots, n\}$
such that $X_{i, t-1} + \one{A_t = i} > 0$.

Critically, the gradient estimate $g_{C_t}(w_{I_t})$
returned at the end of an iteration~$t$
is based on the model parameters~$w_{I_t}$
known to the \gls{CS} at the beginning of round~$I_t = \sum_{s = 0}^t s \one{s + D_{A_s, s} = t}$
when the task was assigned to client~$C_t$,
where $D_{i, t}$ is called the \emph{relative delay}
and is defined as the number of rounds that get completed
during the sojourn of a task at a client:
\begin{align}
\label{eq:Dit}
D_{i, t} &= \one{A_t = i} R_{i, t},
~\text{where} \\
\label{eq:Rit}
R_{i, t}
&= \min\left\{
r \in \bN:
\sum_{s = t}^{t+r} \one{C_s = i}
= X_{i, t-1} + \one{A_t = i}
\right\}.
\end{align}

Consistently with the literature on decentralized learning, we make the following assumptions: uniform lower bound~\(f^*\) for the objective function~\(f\), \(L\)-Lipschitz continuity of the gradients~\(\nabla f_i\) to ensure smoothness, upper bound~\(\sigma^2\) on the variance of the stochastic gradients, and upper bound~\(M^2\) on the gradient dissimilarity across clients (see Assumptions~A\ref{A1}--A\ref{A4} in Section~\ref{assumptions} of the supplementary materials).



\subsection{Queuing dynamics} \label{sec:queuing}

The next result is a variant of \cite[Proposition~2]{leconte2024queuing}
and will be instrumental throughout the paper.
It relates the queuing dynamics to the routing and service-rate vectors $p = (p_1, p_2, \ldots, p_n)$ and $\mu = (\mu_1, \mu_2, \ldots, \mu_n)$.
Recall that the state space is $\cX_{n, m-1}$ (and not $\cX_{n, m}$)
because we consider model-parameter-update times.
Buzen's algorithm was introduced in \cite{buzen:1973}.

\begin{proposition} \label{prop:jackson} 
	In the framework of \Cref{sec:fl},
	the sequence $(X_t, t \in \bN)$
	defines an irreducible positive recurrent Markov chain
	with stationary distribution
	\begin{align} \label{eq:pi}
	\pi_{n, m-1}(x)
	&= \frac1{Z_{n, m-1}} \prod_{i = 1}^n \left( \frac{p_i}{\mu_i} \right)^{x_i},
	~ x \in \cX_{n, m-1},
	\end{align}
	where the normalizing constant~$Z_{n, m-1}$
	can be computed by applying Buzen's recursive algorithm:
	\begin{itemize}
		\item $Z_{\un, 0} = 1$
		for each $\un \in \{1, 2, \ldots, n\}$,
		\item $Z_{1, \um} = (\frac{p_1}{\mu_1})^\um$
		for each $\um \in \{0, 1, 2, \ldots, m\}$
		\item $Z_{\un, \um} = Z_{\un-1, \um} + \frac{p_\um}{\mu_\um} Z_{\un, \um - 1}$
		for $\un \in \{2, 3, \ldots, n\}$
		and $\um \in \{1, 2, \ldots, m\}$.
	\end{itemize}
\end{proposition}
\begin{proof}
	See Section~\ref{app:jackson} in th supplementary material.
\end{proof}

In the rest of the paper,
we will assume that the system starts in steady state.
This assumption is reasonable
when the total number~$T$ of updates is sufficiently large,
as the distribution of~$X_t$
converges exponentially fast with~$t$
towards the stationary distribution
regardless of the initial distribution (see Theorem~13.4.14 in \cite{Brémaud:2020}).
Concretely, we will assume that
$X \triangleq X_0$ follows the stationary distribution~$\pi_{n, m-1}$,
and we will often drop the time index,
so that for instance $D_i$ will be a random variable
distributed like $D_{i, t}$ for any $t \in \bN_{> 0}$.





\section{Optimize model updates} \label{sec:optimize-updates}

Consistently with the literature on asynchronous \gls{FL}, in this section, we assume the \gls{CS} has a fixed budget~$T$ of model parameter updates, and we try to make the best of these updates. This is particularly relevant in contexts, such as cellular networks with costly data plans or satellite internet services, where data transmission costs are high.



\subsection{Bound, delay, and gradient descent} \label{sec:optimize-updates-delay}

\cite[Theorem~1]{leconte2024queuing} gave the following upper bound on the ergodic mean of the norm-square of the gradient of~$f$: there exists $\eta_{\text{max}} > 0$ dependent on~$p$ so that, for any $\eta \in (0, \eta_{\text{max}})$,
\begin{align}
\nonumber
&\frac{1}{T + 1}\sum_{t=0}^{T} \mathbb{E}[\|\nabla f(w_t)\|^2] \leq 8G, \text{ where} \\
\label{eq:G}
&\begin{aligned}
G ={}
\frac{A}{\eta(T + 1)}
+ \frac{\eta L B}{n^2} \sum_{i=1}^{n} \frac{1}{p_i}
+ \frac{\eta^2 L^2 B m}{n^2} \sum_{i=1}^{n} \frac{\bE[D_i]}{p_i^2}
\text{.}
\end{aligned}
\end{align}
The constants $A$, $B$, and $L$ depend on the learning problem under consideration (see~\cite{leconte2024queuing}) but not on the routing vector\footnote{Starting from \cite[Equation~(2)]{leconte2024queuing}, we can apply Assumption~A\ref{A1} to write $A = \bE[f(w_0)] - f^*$, which does not depend on~$p$.}~$p$, and they are estimated heuristically.
If needed, the dependency of~$G$ on the routing vector~$p$ will be made explicit by writing $G(p)$.
We will use~$G$ as a proxy to minimize the ergodic norm-squared of the gradient of~$f$.

The first term in~$G$ follows a classical pattern, indicating how initialization influences convergence.
The next two terms depend strongly on the routing and service-rate vectors $p$ and~$\mu$, both explicitly and implicitly via the mean relative delays $\bE[D_i]$.
In principle, the mean relative delays $\bE[D_i]$ are complex functions of the system dynamics, as a task's relative delay may depend on an unbounded number of rounds in the future.

\Cref{theo:little}, our first main contribution, bypasses this difficulty by expressing the expected relative delays as functions of the mean numbers of stationary tasks, which in turn allows us to derive closed-form expressions for the mean relative delays and their gradient.

\begin{theorem} \label{theo:little}
	In the framework of \Cref{sec:fl}, we have
	\begin{align}
	\label{eq:D}
	\bE[D_i]
	&= \bE[X_i],
	\quad i \in \{1, 2, \ldots, n\},
	\\
	\label{eq:gradD}
	\frac{\partial \bE[D_i]}{\partial p_j}
	&= \frac1{p_j} \cov[X_i, X_j],
	\quad i, j \in \{1, 2, \ldots, n\},
	\end{align}
	where for each $i, j \in \{1, 2, \ldots, n\}$,
	\begin{align}
	\label{eq:Xi}
	\bE[X_i]
	&= \sum_{k = 1}^{m-1}
	\left( \frac{p_i}{\mu_i} \right)^k
	\frac{Z_{n, m-1-k}}{Z_{n, m-1}}. \\
	\label{eq:XiXj}
	\bE[X_i X_j]
	&=
	\hspace{-.05cm}
	\sum_{\substack{k, \ell = 1 \\ k + \ell \le m - 1}}^{m-1}
	\hspace{-.05cm}
	\left( \frac{p_i}{\mu_i} \right)^{k}
	\left( \frac{p_j}{\mu_j} \right)^{\ell}
	\frac{Z_{n, m-1-k-\ell}}{Z_{n, m-1}},
	\end{align}
	and the constants $Z_{n, \um}$ for $\um \in \{0, 1, \ldots, m - 1\}$
	are computed as in \Cref{prop:jackson}.
\end{theorem}

\begin{proof}
	The proof of \Cref{theo:little} is in Section~\ref{app:little} of the supplementary material.
	Here we briefly give the intuition behind~\eqref{eq:D},
	which is analogous to Little's law.
	If each task at client~$i$ pays \$1
	each time a task gets completed at another client,
	there are two ways of collecting payments:
	either we receive an upfront payment of $\$ R_i$
	when a task is assigned to client~$i$ (yielding $\bE[D_i]$),
	or we earn $\$ X_i$
	each time a task gets completed at another client (yielding $\bE[X_i]$).
	Equations~\eqref{eq:gradD}--\eqref{eq:XiXj}
	follow by direct computations,
	after observing that the distribution~\eqref{eq:pi}
	is an exponential family.
\end{proof}

In the remainder, we will apply \eqref{eq:D}--\eqref{eq:gradD} to compute $\bE[D_i]$ and $\nabla_p \bE[D_i]$. However, these equations can also be used to estimate these quantities through Monte Carlo simulations.
Section~\ref{app:G} of the supplementary material
gives a gradient-descent algorithm to optimize the routing vector~$p$ in view of minimizing~$G$, which will be applied in \Cref{sec:num} to run extensive numerical results.



\subsection{Discussion} \label{sec:optimize-updates-discussion}

\Cref{theo:little} allows us to gain insight into
the impact on~$G$ of the system parameters.

\paragraph*{How to minimize~$G$?}

One can verify that the second term in~$G$ is minimized by applying
the uniform routing~$p^{\text{uniform}}$,
given by $p^{\text{uniform}}_i = \frac1n$
for each $i \in \{1, 2, \ldots, n\}$.
\Cref{fig:G-third-term} shows that minimizing the third term in~$G$,
involving the mean relative delays,
is more challenging: even in a toy 2-client example,
the third term is non-monotonic
and is minimized by assigning almost all tasks to the slowest client.

To gain more insight into this third term,
observe that Equation~\eqref{eq:D} from \Cref{theo:little}
yields the following simple relation between the mean relative delays:
\begin{align} \label{eq:tot}
\sum_{i = 1}^n \bE[D_i]
= \sum_{i = 1}^n \bE[X_i]
= m - 1.
\end{align}
This relation has several consequences, in particular:
(i) the sum of the mean relative delays depends only on the numbers~$n$ and~$m$ of clients and tasks, while the vectors~$p$ and~$\mu$ impact only how the relative delay is distributed across clients;
(ii) decreasing the relative delay at a client necessarily comes at the cost of an increased relative delay at another client;
and (iii) since both the routing probabilities $p_i$ and relative delays $\bE[D_i]$ have constant sums over the clients~$i$, minimizing the third term in~$G$ requires finding a routing vector~$p$ such that a client~$i$ with a high relative delay $\bE[D_i]$ also has a relatively large routing probability~$p_i$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.6\textwidth]{third_term.png}
	\caption{Third term of the bound~$G$ given in~\eqref{eq:G} vs.\ the routing probability to the slowest client, in a toy example with $n = 2$ clients and $m = 20$ tasks, for various speed vectors~$\mu=\left(\mu_s, \mu_f \right)$.}
	\label{fig:G-third-term}
\end{figure}

\paragraph*{Dependency on the number~$m$ of tasks}

Another consequence of \Cref{theo:little}
is that the bound~$G$
is an increasing function of the number~$m$ of tasks
(by combining~\eqref{eq:D}
with the observation that $\bE[X_i]$ is a non-decreasing function of~$m$ \cite[Lemma~2]{suri1985monotonicity}).
In particular,
keeping all other system parameters fixed,
the performance is optimized when only~$m = 1$ task circulates in the network!
In this case, \eqref{eq:tot} implies
that the third term in~$G$ is equal to zero.
This is intuitive because, with a single task circulating in the network, the staleness issue is trivially eliminated, and the system works like a synchronous \gls{FL} system in which the \gls{CS} samples a single client at each round. This observation motivates the alternative metric we introduce in \Cref{sec:optimize-time}.

\paragraph*{Simple routing strategies}

Our result allows us to simplify the bound~$G$
for two noteworthy routing vectors.
First, under uniform routing $p^{\text{uniform}}$,
\eqref{eq:tot} yields
\begin{align*}
G(p^{\text{uniform}})
&= \frac{A}{\eta (T + 1)}
+ \eta L B
+ \eta^2 L^2 B m (m-1).
\end{align*}
Another strategy that we can analyze explicitly
is the \emph{balanced} routing vector~$p^{\propto \mu}$, such that
$p^{\propto \mu}_i = \mu_i / \sum_j \mu_j$,
i.e., the probability of routing a task to a client
is proportional to this client's service speed.
This routing strategy is popular in queuing theory
because it ``balances load''
across clients,
in the sense that $\bE[D_i] = \bE[X_i] = (m-1) / n$
for each $i \in \{1, 2, \ldots, n\}$
(by \eqref{eq:pi}, \eqref{eq:D}, and \eqref{eq:tot}).
% Letting $|\mu| = \sum_{i = 1}^n \mu_i$, we obtain
It follows that
\begin{align*}
G(p^{\propto \mu})
&= \frac{A}{\eta (T + 1)}
+ \frac{\eta L B |\mu|}{n^2} \sum_{i = 1}^n \frac1{\mu_i}
+ \frac{\eta^2 L^2 B m (m-1) |\mu|^2}{n^3} \sum_{i = 1}^n \frac1{\mu_i^2}.
\end{align*}





\section{Optimize physical time} \label{sec:optimize-time}

In this section, we aim to achieve optimal performance in terms of physical time, that is, accounting for the duration of model-parameter update rounds.
In \Cref{sec:optimize-time-delay}, we introduce a new performance metric that takes this duration into account, and we derive an upper-bound~$H$, which serves as the counterpart to~$G$.
\Cref{sec:optimize-time-discussion} highlights the key differences compared to \Cref{sec:optimize-updates}.

\subsection{Bound, delay, and gradient descent} \label{sec:optimize-time-delay}

\Cref{theo:bound-time} below provides an upper bound on the ergodic mean of the squared norm of the gradient of~$f$ multiplied by the duration of the corresponding round.
This can be interpreted as minimizing the average per-round cost, where the cost of a round is the integral of the squared norm of the gradient of~$f$ over its duration.
For each $t \in \{0, 1, \dots, T\}$, $\tau_t$ denotes the duration of round~$t$ in the model of \Cref{sec:fl}.

\begin{theorem} \label{theo:bound-time}
	In the framework of \Cref{sec:fl}, there exist $\tilde{A} > 0$ and a random variable $\eta_{\text{max}} > 0$ so that, for any $\eta \in (0 , \eta_{\text{max}})$,
	\begin{align} \label{boundH}
	&\bE \left[ \frac{1}{T+1}\sum_{t=0}^{T} \tau_{t} \|\nabla f(w_t)\|^2 \right] \leq 8H,
	~\text{where} \\
	\label{eq:H}
	&H = 
	\frac{\tilde{A}}{\eta \lambda}
	+ \frac{\eta L B}{\lambda n^2} \sum_{i=1}^{n} \frac{1}{p_i}
	+ \frac{\eta^2 L^2 B m}{\lambda n^2} \sum_{i=1}^{n} \frac{\mathbb{E}[\xi_i]}{p_i^2}  \text{.}
	\end{align}
	% \tilde{A} = \tilde{A}(w_0, p), \lambda = \lambda(p)
	Here, $B$ and $L$ are as defined in~\eqref{eq:G},
	the random vector $\xi \in \cX_{n, m}$ follows the distribution $\pi_{n, m}$ as defined in~\eqref{eq:pi}, and $\lambda$ is the throughput of the Jackson network, that is, the number of rounds per unit of physical time, given as follows, with $Z_{n, m}$ and $Z_{n, m-1}$ as defined in \Cref{prop:jackson}:
	\begin{align}
	\label{eq:throughput}
	\lambda = \sum_{i=1}^n \mu_i \mathbb{P}(\xi_i > 0) = \frac{Z_{n,m-1}}{Z_{n,m}}.
	\end{align}
\end{theorem}
\begin{proof}
	See Section~\ref{app:time} of the supplementary material.
\end{proof}

The bound~$H$ in \Cref{theo:bound-time} can be seen as the throughput-aware counterpart of the bound~$G$ in \Cref{sec:optimize-updates}. The main differences are as follows. First, $A$ is replaced with $\tilde{A}$, which is \emph{a priori} dependent on~$p$. We can verify that $\tilde{A}$ can be upper-bounded by a constant independent of~$p$ when the function~$f$ is bounded, and we conjecture this is true in general; in the numerical results, we will treat it as a constant, like~$A$. Second, the number~$T$ of updates in the first term of~$G$, independent of~$p$, is replaced with the throughput~$\lambda$ that appears in all terms of~$H$. Lastly, the expectation~$\bE[X_i] (= \bE[D_i])$ in~$G$, with $X \sim \pi_{n, m-1}$, is replaced with $\bE[\xi_i]$ in~$H$, with $\xi \sim \pi_{n, m}$. The intuition is that the relative delay (counting the number of rounds) in~$G$ is replaced with the absolute delay (counting the physical time) in~$H$; hence, the counterpart of \Cref{theo:little} yields the state at an arbitrary time, which has state space~$\cX_{n, m}$.



\subsection{Discussion} \label{sec:optimize-time-discussion}

Part of the discussion in~\Cref{sec:optimize-updates-discussion}
can be adapted to~$H$ with minor modification.
In particular, the counterpart of~\eqref{eq:tot} is that
$\sum_{i = 1}^n \bE[\xi_i] = m$.
This equality has consequences in terms of optimization
and can be used to simplify the expression of~$H$ under simple routing strategies
(though the throughput may not simplify).

One fundamental difference between~$G$ and~$H$
is that $H$ is in general \emph{not} a non-decreasing function
of the number~$m$ of tasks.
Figure~\ref{fig:optimal_m}\celine{Zoomer l'axe des y pour n'afficher que les valeurs au-dessous de 400 ?}\celine{Il faudrait déplacer la légende pour qu'elle ne soit pas sur la courbe.} shows the bound $H(p^{\text{uniform}})$ as a function of the number~$m$ of tasks, under different step sizes~$\eta$, and it reveals the existence of an optimal number~$m^* > 1$ of tasks that minimizes~$H$. Our intuition is that, when $m<m^*$, the throughput is insufficient and clients are underutilized; conversely, when $m>m^*$, the throughput increases but so does staleness, which ultimately hinders convergence,
as already observed with~$G$.
As a side remark, observe that the optimal number~$m^*$ of tasks decreases (albeit slowly) with~$\eta$. Our intuition is that, with a higher~$\eta$, the impact of stale gradients becomes more significant, as each gradient carries more weight in the model-parameter updates.\celine{Ce serait bien de comprendre pourquoi il y a des irrégularités dans la figure 2.}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\textwidth]{H_m.png} 
	\caption{Bound~$H(p^{\text{uniform}})$ as a function of the number~$m$ of tasks for different values of the step size~$\eta$. The system consists of 50 clients, with speeds given by $\mu_i=\textrm{e}^{i/100}$ for each $i \in \{1,\ldots,n\}$.}
	\label{fig:optimal_m}
\end{figure}




\section{Numerical results} \label{sec:num}

In this section, we numerically evaluate the impact of the optimizations proposed in \Cref{sec:optimize-updates,sec:optimize-time} on the accuracy and loss of \texttt{Generalized AsyncSGC} for several image-classification tasks.
We consider mainly two data-distribution scenarios:
\begin{description}
	\item[Homogeneous]
	Data is distributed independently and identically across clients, and each client receives an equal number of data points from each class.
	\item[Heterogeneous]
	Datasets are heterogeneous across clients, both in terms of distribution and volume. For each class~\( k \), we sample a vector \( p_k \sim \text{Dir}_n(0.5) \), where \( p_{k,j} \) is the proportion of class-\( k \) instances allocated to client~\( j \) and \( \text{Dir}_n(\beta) \) the Dirichlet distribution with dimension \( n \) and concentration parameter \( \beta > 0 \).
\end{description}

Given a system as described in \Cref{sec:fl}, the $G$-optimized (resp.\ $H$-optimized) routing vector $p^*_G$ (resp.\ $p^*_H$) is computed by minimizing $G$ (resp.\ $H$) using the Adam gradient-descent algorithm with standard hyperparameter values from the literature, initialized with the uniform routing vector~$p^{\text{uniform}}$. Since~$G$ and~$H$ are non-convex, the obtained routing vector may be a local minimum. The gradients of~$G$ and~$H$ are computed in closed form, using the results of \Cref{sec:optimize-updates,sec:optimize-time}, as detailed in Sections~\ref{app:G} and~\ref{app:H} of the supplementary material.

Given a routing vector~$p$, we simulate the system of \Cref{sec:fl} and evaluate \texttt{Generalized AsyncSGD} on image classification tasks using the Fashion-MNIST \cite{deng2012mnist}, KMNIST \cite{clanuwat2018deep}, CIFAR-10, and CIFAR-100 \cite{Krizhevsky2009LearningML} datasets. We employ the standard multi-class cross-entropy loss and evaluate performance on an unseen validation dataset. Additional details on the experiments are provided in Section~\ref{exp:dl} of the supplementary material.



\subsection{Optimize model updates} \label{num:optimize-updates}

We first focus on the performance goal of \Cref{sec:optimize-updates}, namely, to minimize the average norm-square of the gradient of~$f$ per update round.
We consider the system of \Cref{sec:fl}, with $n = 20$ clients handling $m = 100$ tasks. For each $i \in \{1,\ldots,20\}$, the service speed at client~$i$ is $\mu_i=\textrm{e}^{i/100}$, so that the fastest client is 20\% faster than the slowest. The learning rate is $\eta=0.01$, and $L=1$. As discussed in \Cref{sec:optimize-updates-discussion}, this high-concurrency scenario is particularly challenging when it comes to minimize gradient staleness.

\paragraph*{Optimizing~$G$}

The optimized vector~$p^*_G$ obtained by minimizing~$G$ selects the slowest client over 40\% of the time, while the routing probabilities to faster clients are all of the same magnitude but decrease as the client speed increases. This result is consistent with \cite[Section~5]{leconte2024queuing} but counterintuitive \textit{a priori}. Such a skewed routing vector is obtained because it significantly reduces the third term of~$G$ (see \Cref{sec:optimize-updates-discussion}). Intuitively, $p^*_G$ synchronizes the system to the pace of the slowest client, while routing tasks to faster clients in inverse proportion to their speeds, to reduce errors caused by stale gradients.

Additional numerical results (not shown here) reveal that the skewness of the optimized routing vector~$p^*_G$ towards the slowest client is accentuated when the concurrency~$m$ is large, as in the scenario we consider here. More specifically, the vector~$p^*_G$ is closer to the uniform vector~$p^{\text{uniform}}$ when the concurrency level~$m$ is small relative to the number~$n$ of clients, while the routing probability to the slowest client rises significantly as $m$ increases.
This is in line with the discussion of \Cref{sec:optimize-updates-discussion}, where we observed that the relative weight of third term of~$G$ tends to increase with~$m$.

\paragraph*{Performance on datasets}

\begin{figure}[htb]
	\begin{center}
		\centerline{\includegraphics[width=\textwidth]{Results.png}}
		\caption{Performance on the validation set at the \gls{CS} in the scenario of \Cref{num:optimize-updates}, with $n = 20$ clients and $m = 100$ tasks, for homogeneous and heterogeneous data. Solid lines show metrics averaged over independent simulations, and shaded areas represent the standard deviation. For Fashion-MNIST, we simulated the learning process over 3,000 rounds, repeating the simulations 10 times. The accuracy and loss on an unseen validation set were recorded every 5 rounds. For CIFAR-10 AND CIFAR-100, we applied standard normalization and data augmentation techniques, running the simulation over 30,000 rounds, repeated three times. The accuracy and loss on the unseen validation set were logged every 50 rounds.}
		\label{fig:sim_results}
	\end{center}
\end{figure}

\Cref{fig:sim_results} shows that the optimized routing strategy~$p^*_G$ improves performance over the uniform (\( p^{\text{uniform}} \)) and balanced (\( p^{\propto \mu} \)) routing strategies
across all our experiments and throughout the learning process.
In particular, while assigning a higher routing probability to the slowest client may seem to skew the model toward the data distribution associated with that client, our simulations indicate that the optimized routing strategy consistently achieves superior and more reliable performance. The standard deviation is also markedly higher under $p^{\text{uniform}}$ and $p^{\propto \mu}$, suggesting $p^*_G$ exhibits greater robustness.
Section~\ref{app:num-additional} of the supplementary material shows that the same conclusions hold in even-more-heterogeneous data-distribution scenarios, where each client's dataset only contains a subset of the image labels (possibly disjoint from other clients).

Anticipating over \Cref{num:optimize-time}, we note that the relative performance of the optimized routing~$p^*_G$ versus the uniform and balanced strategies would be reversed if in \Cref{fig:sim_results} we used physical time on the x-axis: as observed in \Cref{sec:optimize-updates-discussion,sec:optimize-time-discussion}, since~$G$ disregards throughput, it reduces gradient staleness at the cost of a lower throughput.
Concretely, in \Cref{fig:sim_results}, the average (physical) time to complete the 3,000 update rounds was 7 to 8 times larger under $p^*_G$ compared to $p^{\text{uniform}}$ and $p^{\propto \mu}$. We believe such an increase in computating time may be prohibitive even when physical time is not the primary concern.



\subsection{Optimize physical time} \label{num:optimize-time}

We analyze a network of $n = 30$ clients with concurency level $m = 30$. Clients are organized into three clusters, each containing 10 clients. The slowest cluster has an average service time of 100 time units, the medium cluster of 10 time units, and the fastest cluster of 1 time unit. This low-concurrency high-speed-heterogeneity scenario is particularly challenging when it comes to optimizing throughput. We set the parameters as follows: $\tilde{A} = 15$, $L = 1$, $\sigma = 3$, $M = 10$, and $\eta = 0.01$. More details appear in Section~\ref{exp:dl} of the supplementary material. 

\paragraph*{Optimizing~$H$}

The H-optimized routing probabilities are (per client) $p^*_\text{H, \text{slow}}=0.0068$, $p^*_\text{H, \text{medium}}=0.0449$, and $p^*_\text{H, \text{fast}}=0.0487$. Contrary to \Cref{num:optimize-updates}, faster clients receive a larger fraction of tasks, but $p^*_{H, \text{medium}}$ and $p^*_{H, \text{fast}}$ are still of the same order, so that $p^*_H$ seems to a tradeoff between minimizing staleness (like $p^*_G$) and maximizing throughput (like $p^{\propto \mu}$--a common heuristic to maximize throughput). Concretely, within the physical time frame of 3,000 units plotted in \Cref{fig:sim_time_results}, $p^{\propto \mu}$ completes 17,000 update rounds, which sets it apart from $p^*_H$ (3,200 rounds), $p^{\text{uniform}}$ (690 rounds), and $p^*_G$ (145 rounds).

\paragraph*{Performance on datasets}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\textwidth]{results_time_kmnist.png} 
	\caption{Performance with respect to physical time on the validation set at the \gls{CS} in the scenario of \Cref{num:optimize-time}, with $n = 30$ clients and $m = 30$ tasks, for homogeneous and heterogeneous KMNIST datasets. We simulated the process over 3,000 physical time units, recording accuracy and loss on a validation set every 5 rounds. Each experiment was repeated independently 10 times.}
	\label{fig:sim_time_results}
\end{figure}

Figure~\ref{fig:sim_time_results} compares the accuracy and loss of \texttt{Generalized AsyncSGD} under these four routing strategies.
First focusing on the average performance,
we observe that in the homogeneous scenario
$p^{\propto \mu}$ performs better than $p^*_H$,
which in turn outperforms $p^*_G$ and $p^{\text{uniform}}$,
while in the heterogeneous scenario
$p^*_H$ outperforms all strategies.
Furthermore, the balanced strategy $p^{\propto \mu}$
exhibits sharp spikes in the loss trajectory in all scenarios,
and a particularly large standard deviation in the heterogeneous scenario.
This constrasts with the low and stable standard deviation
exhibited by both $p^*_G$ and $p^*_H$.
All in all, these numerical results confirm
that when physical time is a performance criterion,
the $H$-optimized strategy provides a suitable trade-off between
minimizing staleness (overweighted by $p^*_G$)
and maximizing throughput (overweighted by $p^{\propto \mu}$), even though the bound $H$ serves only as a proxy for actual performance.
Additional plots are provided in \Cref{app:num-additional}
for other image-classification tasks.





\section{Conclusion}

We provide novel insight into the impact of queuing dynamics on the performance of asynchronous \gls{FL}. This allows us not only to derive methods for optimizing performance, but also to identify and overcome fundamental limitations of existing performance objectives. Our numerical results on real datasets show that such queuing effects can have very significant impact on performance, and that the optimizations we propose can increase accuracy by 10\% to 30\%. We believe that these insights are relevant for \texttt{Generalized AsyncSGC}, our focus 
here, but also to other algorithms like \texttt{FedBuff}. 
Specifically, the asynchronous aggregation in \texttt{FedBuff} introduces additional control dimensions, which could also benefit from our stochastic modeling insights.





\bibliography{paper}





\appendix





\section{Assumptions} \label{assumptions}
Our analysis is grounded in the assumptions presented in \Cref{sec:fl}, which align with those established in \cite{leconte2024queuing}. These assumptions are detailed as follows:
\begin{enumerate}[$\text{A}$1]
	\item \label{A1} \textbf{Lower Boundedness:} The objective function $f$ is bounded from below by some real number $f^*$, meaning $f(w) \geq f^*$ for all $w \in \mathbb{R}^d$.
	
	\item \label{A2} \textbf{Gradient Smoothness:} Each client's local function $f_i$ has an $L$-Lipschitz continuous gradient, where $L > 0$. Mathematically, for any vectors $w, \mu \in \mathbb{R}^d$:
	\[
	\|\nabla f_i(w) - \nabla f_i(\mu)\| \leq L \|w - \mu\|.
	\]
	
	\item \label{A3} \textbf{Stochastic Gradient Properties:} For each client $i$, the stochastic gradient $g_i(w)$ is an unbiased estimator of the gradient $\nabla f_i(w)$ with bounded variance $\sigma^2 > 0$. That is, for all $w \in \mathbb{R}^d$:
	\[
	\mathbb{E}[g_i(w)-\nabla f_i(w)] = 0 \text{ and }
	\mathbb{E}[\|g_i(w) - \nabla f_i(w)\|^2] \leq \sigma^2.
	\]
	
	\item \label{A4} \textbf{Bounded Client Heterogeneity:} There exist constant $M > 0$, such that for all $w \in \mathbb{R}^d$:
	\[
	\|\nabla f(w) - \nabla f_i(w)\|^2 \leq M^2.
	\]
	
	\item \label{A5} \textbf{Stationary Regime:} The dynamics of the closed Jackson network are assumed to be in their stationary regime. Specifically, the random $ n $-dimensional vector $ \xi(t) $, where the $ i $-th component $ \xi_i(t) $ represents the number of tasks at client $ i $ at time $ t $, follows its stationary distribution $ \pi_{n,m} $.
\end{enumerate}




\section{Proof of \Cref{prop:jackson}} \label{app:jackson}

The result follows by observing that
$(X_t, t \in \bN)$ tracks the state at departure times
of a Jackson network~\cite{jackson1957networks,serfozo:1999}
with $n$ clients (usually called \textit{servers})
and $m$ tasks (often called \textit{customers} or \textit{jobs}),
with service rate vector~$\mu$
and relative arrival rate vector~$p$.
In particular, \eqref{eq:pi} follows from
Chapter~1 (Definition~1.8 and Theorem~1.12)
and Section~4.8 (Definition~4.34 and Example~4.38)
in \cite{serfozo:1999}.

\section{Proof of \Cref{theo:little}} \label{app:little}

Consider the asynchronous \gls{FL} framework of \Cref{sec:fl}.
After proving preliminary results in
\Cref{lem:Rit} and \Cref{coro:Dit-bijection},
we prove \eqref{eq:D} in \Cref{app:D}
and \eqref{eq:gradD} in \Cref{app:gradD}.

\begin{lemma} \label{lem:Rit}
	For each $i \in \{1, 2, \ldots, n\}$, for each $t \in \bN_{> 0}$ we have
	\begin{align*}
	R_{i, t+1}
	&= \begin{cases}
	\max(0, R_{i, t} - 1)
	&\text{if $A_{t+1} \neq i$,} \\
	\min\{r \ge R_{i, t}: C_{t + 1 + r} = i\}
	&\text{if $A_{t+1} = i$.}
	\end{cases}
	\end{align*}
\end{lemma}

\begin{proof}
	Let $i \in \{1, 2, \ldots, n\}$ and $t \in \bN_{> 0}$.
	
	The following preliminary results stem
	from the definition~\eqref{eq:Rit} of $R_{i, t}$
	and the observation that
	the sequence $r \in \bN \mapsto \sum_{s = t}^{t+r} \one{C_s = i}$
	is nondecreasing with increments 0 or 1
	and takes value $\one{C_t = i} \in \{0, 1\}$ at $r = 0$
	(equal to $0$ if $X_{i, t-1} + \one{A_t = i} = 0$ by definition of $C_t$):
	\begin{itemize}
		\item We can replace the equal sign with a larger-than-or-equal sign
		in the definition of $R_{i, t}$:
		\begin{align} \label{eq:Rit-ge}
		R_{i, t} = \min\left\{ r \in \bN: \sum_{s = t}^{t + r} \one{C_s = i} \ge X_{i, t-1} + \one{A_t = i} \right\}.
		\end{align}
		\item We have
		\begin{align} \label{eq:Rit-eq}
		\sum_{s = t}^{t + R_{i, t}} \one{C_s = i} = X_{i, t-1} + \one{A_t = i}.
		\end{align}
		\item Lastly, we can verify that $A_t = i$ implies $C_{t + R_{i, t}} = i$.
	\end{itemize}
	
	Now focusing on $R_{i, t+1}$, we have successively:
	\begin{align*}
	R_{i, t+1}
	&\overset{\text{(a)}}{=} \min\left\{
	r \in \bN:
	\sum_{s = t+1}^{t+1+r} \one{C_s = i}
	\ge X_{i, t} + \one{A_{t+1} = i}
	\right\}, \\
	&\overset{\text{(b)}}{=} \min\left\{
	r \in \bN:
	\sum_{s = t}^{t+1+r} \one{C_s = i}
	\ge X_{i, t-1} + \one{A_t = i} + \one{A_{t+1} = i}
	\right\}, \\
	&\overset{\text{(c)}}{=} \min\left\{
	r \ge \max(0, R_{i, t} - 1):
	\sum_{s = t}^{t+1+r} \one{C_s = i}
	\ge X_{i, t-1} + \one{A_t = i} + \one{A_{t+1} = i}
	\right\}, \\
	&\overset{\text{(d)}}{=} \min\left\{
	r \ge \max(0, R_{i, t} - 1):
	\sum_{s = t + R_{i, t} + 1}^{t + 1 + r} \one{C_s = i}
	\ge \one{A_{t+1} = i},
	\right\}
	\end{align*}
	where (a) and (c) follow from~\eqref{eq:Rit-ge},
	(b) from~\eqref{eq:X-rec},
	and (d) by injecting~\eqref{eq:Rit-eq}.
	Consistently with the result accounced in the lemma,
	we conclude by making a case disjunction:
	\begin{itemize}
		\item If $A_{t+1} \neq i$: The right-hand side
		of the inequality in (d) is $0$,
		hence the inequality is already satisfied
		by choosing the smallest authorized value for~$R_{i, t}$.
		\item If $A_{t+1} = i$: The right-hand side
		of the inequality in (d) is~$1$.
		Since the sum $\sum_{s = t + R_{i, t} + 1}^{t + 1 + r} \one{C_s = i}$
		is~$0$ (empty) when $r = R_{i, t} - 1$,
		remains $0$ as long as $C_{t + 1 + r} \neq i$,
		and increases to~$1$ whenever $C_{t+r+1} = i$,
		the result follows.\qedhere
	\end{itemize}
\end{proof}

\begin{corollary} \label{coro:Dit-bijection}
	For each $i \in \{1, 2, \ldots, n\}$,
	the function $t \in \bN_{> 0} \mapsto t + D_{i, t}$
	defines a bijective increasing mapping
	from the set $\{t \in \bN_{> 0}: A_t = i\}$
	onto the set $\{t \in \bN_{> 0}: C_t = i\}$.
\end{corollary}

\begin{proof}
	Let $i \in \{1, 2, \ldots, n\}$.
	By definition of $D_{i, t}$,
	we have $D_{i, t} = R_{i, t}$ for each $t \in \bN_{> 0}$ so that $A_t = i$,
	hence it suffices to prove the result for $R_{i, t}$ instead of $D_{i, t}$.
	
	For each $t \in \bN_{> 0}$,
	$t + R_{i, t}$ is the round at the end of which
	client~$i$ finishes processing
	the last task that has arrived at this client
	by the beginning of round~$t$.
	\Cref{lem:Rit} implies that,
	for each $t \in \bN_{> 0}$, we have
	\begin{align*}
	t + 1 + R_{i, t+1}
	&= \begin{cases}
	t + \max(1, R_{i, t})
	&\text{if $A_{t+1} \neq i$}, \\
	\min\{s > t + R_{i, t}: C_s = i\}
	&\text{if $A_{t+1} = i$.}
	\end{cases}
	\end{align*}
	It follows directly that the function $t \in \bN_{> 0} \mapsto t + R_{i, t}$
	is nondecreasing
	and defines an injection
	from $\{t \in \bN_{> 0}: A_t = i\}$
	onto $\{t \in \bN_{> 0}: C_t = i\}$.
	Furthermore,
	to prove this injection is also a surjection,
	it suffices to verify that
	$A_{t+1} \neq i$ implies
	either $t + 1 + R_{i, t+1} = t + R_{i, t}$
	or $C_{t + 1 + R_{i, t+1}} \neq i$.
	If $A_{t+1} \neq i$,
	the only way that $t + 1 + R_{i, t+1} > t + R_{i, t}$
	is when $R_{i, t} = 0$,
	so that the maximum is attained at~$1$
	and $t + 1 + R_{i, t+1} = t + 1$.
	Using~\eqref{eq:Rit}, we can verify that
	$R_{i, t} = 0$ implies $X_{i, t} = 0$
	which, combined with $A_{t+1} \neq i$, in turn implies
	$C_{t+1} (= C_{t + 1 + R_{i, t+1}}) \neq i$ by definition of $C_{t+1}$.
\end{proof}



\subsection{Proof of Equation~\eqref{eq:D}} \label{app:D}

Let $i \in \{1, 2, \ldots n\}$.
Our goal in this section is to prove that
$\bE[D_i] = \bE[X_i]$.
In a nutshell, we will prove that
\begin{align*}
\bE[D_i]
&\overset{(a)}{=} \lim_{T \to +\infty} \frac1T \sum_{t = 1}^T D_{i, t}
\overset{(b)}{=} \lim_{T \to +\infty} \frac1T \sum_{t = 1}^T X_{i, t}
\overset{(c)}{=} \bE[X_i],
\end{align*}
where all equal signs hold almost surely.
Equation~(a) will be shown in~\Cref{lem:limit}
using an ergodicity argument.
Equation~(b) will be proved using a squeeze argument
formulated in \Cref{lem:squeeze,lem:limRit}.
Lastly, Equation~(c) follows from the classical ergodic theorem
for irreducible positive-recurrent Markov chains.
These arguments are combined at the end of the section to prove~\eqref{eq:D}.

\begin{lemma} \label{lem:limit}
	For each $i \in \{1, 2, \ldots, n\}$, we have
	\begin{align*}
	\lim_{T \to +\infty} \frac1T \sum_{t = 1}^T D_{i, t} = \bE[D_i]
	\quad \text{almost surely}.
	\end{align*}
\end{lemma}

\begin{proof}[Proof of \Cref{lem:limit}]
	Equations~\eqref{eq:Dit} and~\eqref{eq:Rit}
	show that, for each $i \in \{1, 2, \ldots, n\}$,
	there exists a deterministic function
	$g_i: (\bN \times \{1, 2, \ldots, n\} \times \{1, 2, \ldots, n\})^\bN \to \bN$
	such that we can write
	$D_{i, t} = g_i((X_{t+s}, A_{t+s}, C_{t+s}), s \in \bN)$
	for each $t \in \bN$.
	Since the sequence $((X_t, A_t, C_t), t \in \bN)$ is ergodic,
	the conclusion follows from
	\cite[Remark~16.1.11]{bremaud2020}.
\end{proof}

\begin{lemma} \label{lem:squeeze}
	Let $i \in \{1, 2, \ldots, n\}$.
	If $X_{i, 0} = 0$, then for each $T \in \{1, 2, 3, \ldots\}$, we have
	\begin{align} \label{eq:bounds}
	\sum_{t = 1}^T X_{i, t}
	&\le \sum_{t = 1}^T D_{i, t}
	\le \sum_{t = 1}^{T + R_{i, T}} X_{i, t}.
	\end{align}
\end{lemma}

\begin{proof}[Proof of \Cref{lem:squeeze}]
	Let $i \in \{1, 2, \ldots, n\}$
	and $T \in \bN_{> 0}$,
	and assume that $X_{i, 0} = 0$.
	If the set $\{t \in \{1, 2, \ldots, T\}: A_t = i\}$ is empty,
	then all three sums are zero, and the inequalities are trivially satisfied. Therefore, in the remainder, we focus on sample paths for which this set is nonempty.
	
	First observe that, for each $t \in \bN$, we have
	\begin{align} \label{eq:X-unfolded}
	X_{i, s} = \sum_{t = 1}^{+\infty} \one{A_t = i, t \le s < t + D_{i, t}}.
	\end{align}
	Indeed, we have successively:
	\begin{align*}
	X_{i, s}
	&\overset{\textrm{(a)}}{=}
	\sum_{t = 1}^s \one{A_t = i}
	- \sum_{t = 1}^s \one{C_t = i}
	\overset{\textrm{(b)}}{=}
	\sum_{t = 1}^s \one{A_t = i}
	- \sum_{t = 1}^s \one{A_t = i, t + D_{i, t} \le s},
	\end{align*}
	where (a) follows by unfolding~\eqref{eq:X-rec}
	and using our assumption that $X_{i, 0} = 0$,
	and (b) follows by observing that,
	by~\Cref{coro:Dit-bijection},
	the function $t \mapsto t + D_{i, t}$
	defines a bijective (increasing) mapping
	from the set $\{t \in \bN_{> 0}: A_t = i\}$
	onto the set $\{t \in \bN_{> 0}: C_t = i\}$.
	Equation~\eqref{eq:X-unfolded} then follows by rearranging the terms.
	
	Now, we can also use the following trick
	to rewrite $\sum_{t = 1}^T D_{i, t}$
	in a manner that is similar to~\eqref{eq:X-unfolded}:
	\begin{align}
	\nonumber
	\sum_{t = 1}^T D_{i, t}
	&= \sum_{t = 1}^T
	\one{A_t = i} D_{i, t}
	\nonumber
	= \sum_{t = 1}^T
	\one{A_t = i}
	\sum_{s = 1}^{+\infty}
	\one{t \le s < t + D_{i, t}}, \\
	\label{eq:sumD}
	&= \sum_{s = 1}^{+\infty}
	\sum_{t = 1}^T
	\one{A_t = i, t \le s < t + D_{i, t}}.
	\end{align}
	The lower bound in~\eqref{eq:bounds} follows by
	capping the outer sum at $s \in \{1, 2, \ldots, T\}$ in~\eqref{eq:sumD}
	and injecting \eqref{eq:X-unfolded}.
	The upper-bound in~\eqref{eq:bounds} follows in a similar spirit:
	\begin{align*}
	\sum_{t = 1}^T D_{i, t}
	&\overset{\text{(a)}}{=}
	\sum_{s = 1}^{T + R_{i, T}} \sum_{t = 1}^T
	\one{A_t = i, t \le s < t + D_{i, t}}
	\overset{\text{(b)}}{\le}
	\sum_{s = 1}^{T + R_{i, T}} X_{i, s},
	\end{align*}
	where (a) is equivalent to~\eqref{eq:sumD},
	after recalling that tasks leave clients
	in the same order as they arrive,
	and (b) follows by expanding the inner sum to $t \in \bN_{> 0}$.
\end{proof}

\begin{lemma} \label{lem:limRit}
	For each $i \in \{1, 2, \ldots, n\}$,
	we have almost surely that
	$R_{i, T} = o(T)$ as $T \to +\infty$.
\end{lemma}

\begin{proof}[Proof of \Cref{lem:limRit}]
	Let $i \in \{1, 2, \ldots, n\}$.
	For each $t \in \bN$,
	we have $0 \le R_{i, t} \le E_{i, t}$,
	where $E_{i, t} = \min\{r \in \bN: X_{i, t + r} = 0\}$
	is the number of rounds that complete,
	starting from round~$t$,
	before client~$i$ first becomes empty.
	Since the Markov chain $(X_t, t \in \bN)$ is positive recurrent and has a finite state space,
	$\bE[E_{i, t}]$ has a finite limit
	(as the expectation, under the stationary distribution of this Markov chain,
	of the expected hitting time of the set of states where client~$i$ is empty).
	It follows that $\lim_{t \to +\infty} \bE[E_{i, t}] / t = 0$,
	which implies that $\lim_{t \to +\infty} E_{i, t} / t = 0$ almost surely,
	and therefore that $\lim_{t \to +\infty} R_{i, t} / t = 0$ almost surely.
\end{proof}

\begin{proof}[Proof of Equation~\eqref{eq:D}]
	Let $i \in \{1, 2, \ldots, n\}$.
	Let us assume for now that
	the initial distribution of the Markov chain $(X_t, t \in \bN)$
	is such that $X_{i, 0} = 0$ (with probability~$1$).
	
	
	Let us first prove that the upper and lower bound in \Cref{lem:squeeze}
	converge almost surely to the same limit,
	and that this limit is~$\bE[X_i]$, that is:
	\begin{align} \label{eq:squeeze}
	\lim_{T \to +\infty}
	\frac{1}{T} \sum_{t=1}^{T} X_{i, t} 
	&\overset{(a)}{=} \bE[X_i]
	\overset{(b)}{=} \lim_{T \to +\infty}
	\frac1T \sum_{t = 1}^{T + R_{i, T}} X_{i, t}.
	\end{align}
	Since $(X_t, t \in \bN)$ is an irreducible positive-recurrent Markov chain,
	(a) follows directly from
	the ergodic theorem \cite[Theorem 3.3.2]{bremaud2020}.
	The argument for~(b) is in a similar spirit,
	with the extra-complication that the upper bound of summation ($T + R_{i, T}$)
	is different from the denominator ($T$).
	We start by rewriting the upper bound as follows:
	\begin{align*}
	\frac1T \sum_{t = 1}^{T + R_{i, T}} X_{i, t}
	&= \left( 1 + \frac{R_{i, T}}T \right)
	\frac1{T + R_{i, T}}
	\sum_{t = 1}^{T + R_{i, T}} X_{i, t}.
	\end{align*}
	Equation~(b) then follows by combining two arguments:
	(i) \Cref{lem:limRit} implies that
	$\lim_{T \to +\infty} 1 + \frac{R_{i, T}}T = 1$ almost surely;
	(ii) since $\lim_{T \to +\infty} T + R_{i, T} = +\infty$,
	the ergodic theorem for irreducible positive-recurrent Markov chains again implies that
	\begin{align*}
	\lim_{T \to +\infty}
	\frac1{T + R_{i, T}}
	\sum_{t = 1}^{T + R_{i, T}} X_{i, t}
	= \bE[X_i],
	\quad \text{almost surely.}
	\end{align*}
	
	Now, combining~\eqref{eq:squeeze} with \Cref{lem:squeeze} and the squeeze theorem
	allows us to conclude that
	\begin{align*}
	\lim_{T \to +\infty} \frac1T \sum_{t = 1}^T D_{i, t} = \bE[X_i],
	\quad \text{almost surely}.
	\end{align*}
	Equation~\eqref{eq:D} then follows from \Cref{lem:limit}.
	
	To prove that~\eqref{eq:D} also holds
	without the assumption that $X_{i, 0} = 0$ with probability~$1$,
	it suffices to recall that
	the expectations $\bE[D_i]$ and $\bE[X_i]$
	do not depend on the initial distribution.
\end{proof}



\subsection{Proof of Equation~\eqref{eq:gradD}} \label{app:gradD}

Let $i, j \in \{1, 2, \ldots, n\}$.
Our goal is to prove~\eqref{eq:gradD},
which by~\eqref{eq:D} is equivalent to
\begin{align*}
\frac{\partial \bE[X_i]}{\partial (\log p_j)}
&= \cov[X_i, X_j].
\end{align*}
Recall that the vector $X$ follows the stationary distribution~\eqref{eq:pi},
which we can rewrite as
\begin{align} \label{eq:pi-log}
\log \bP(X = x)
&= \log \pi_{n, m-1}(x)
= \sum_{i = 1}^n x_i \left( \log p_i - \log \mu_i \right) - \log Z_{n, m-1},
\quad x \in \cX_{n, m-1},
\end{align}
where $Z_{n, m-1}$ follows by normalization:
\begin{align} \label{eq:C-log}
\log Z_{n, m-1}
&= \log \left(
\sum_{x \in \cX_{n, m-1}}
\exp \left( \sum_{i = 1}^n x_i \left( \log p_i - \log \mu_i \right) \right)
\right).
\end{align}

Let us first prove the following intermediary result:
\begin{align} \label{eq:grad-log}
\frac{\partial \log Z_{n, m-1}}{\partial (\log p_j)}
&= \mathbb{E}[X_j],
&
\frac{\partial \log \pi_{n, m-1}(x)}{\partial (\log p_j)}
&= x_j - \mathbb{E}[X_j],
\quad x \in \cX_{n, m-1}.
\end{align}
The first part of~\eqref{eq:grad-log} follows by
taking the partial derivative of~\eqref{eq:C-log}
and rearranging the terms to retrieve the definition of~$\pi_{n, m-1}$:
\begin{align*}
\frac{\partial \log(Z_{n, m-1})}{\partial (\log p_j)}
&= \frac1{Z_{n, m-1}} \frac{\partial Z_{n, m-1}}{\partial (\log p_j)}
= \frac1{Z_{n, m-1}}
\sum_{x \in \cX_{n, m-1}}
x_j
\exp \left( \sum_{i = 1}^n x_i (\log p_i - \log \mu_i) \right), \\
&= \sum_{x \in \cX_{n, m-1}}
x_j \exp \left( \sum_{i = 1}^n x_i (\log p_i - \log \mu_i) - \log Z_{n, m-1} \right), \\
&= \sum_{x \in \cX_{n, m-1}} x_j \pi_{n, m-1}(x)
= \mathbb{E}[X_j].
\end{align*}
Now, the second part of~\eqref{eq:grad-log} follows
by taking the partial derivative of~\eqref{eq:pi-log}
and injecting the previous result:
\begin{align*}
\frac{\partial \log \pi_{n, m-1}(x)}{\partial (\log p_j)}
&= x_j - \frac{\partial \log Z_{n, m-1}}{\partial (\log p_j)}
= x_j - \bE[X_j],
\quad x \in \cX_{n, m-1}.
\end{align*}

To conclude, it suffices to inject the second part of~\eqref{eq:grad-log}
into the definition of expectation:
\begin{align*}
\frac{\partial \mathbb{E}[X_i]}{\partial (\log p_j)}
&= \sum_{x \in \cX_{n, m-1}}
x_i \frac{\partial \pi_{n, m-1}(x)}{\partial (\log p_j)}
= \sum_{x \in \cX_{n, m-1}}
\pi_{n, m-1}(x) x_i \frac{\partial \log \pi_{n, m-1}(x)}{\partial (\log p_j)}, \\
&= \sum_{x \in \cX_{n, m-1}}
\pi_{n, m-1}(x) x_i (x_j - \mathbb{E}[X_j])
= \cov[X_i, X_j].
\end{align*}



\subsection{Proof of Equations~\eqref{eq:Xi} and~\eqref{eq:XiXj}} \label{app:buzen}


Equation~\eqref{eq:Xi} was proved in \cite[Equation~(8)]{buzen:1973}.
Buzen's algorithm \cite{buzen:1973} as described in the proposition
was introduced and shown to yield the correct result
in \cite[Paragraph ``Computation of $G(N)$'']{buzen:1973}.
(Note that, in \cite{buzen:1973}, $N$ corresponds to our~$m$ and $M$ to our~$n$.)
All that remains is to prove~\eqref{eq:XiXj},
which we do in a similar way to the proof of \eqref{eq:Xi} in \cite{buzen:1973}.
To simplify notation, we can focus without loss of generality on the pair $(i, j) = (1, 2)$.
We have successively:
\begin{align*}
\mathbb{E}[X_1 X_2]
&= \sum_{x \in \cX_{n, m-1}} x_1 x_2 \pi_{n, m-1}(x)
= \sum_{x \in \cX_{n, m-1}} \sum_{k = 1}^{x_1} \sum_{\ell = 1}^{x_2} \pi_{n, m-1}(x), \\
&= \sum_{\substack{k, \ell = 1 \\ k + \ell \le m-1}}^{m-1}
\sum_{\substack{x \in \cX_{n, m-1} \\ x_1 \ge k, x_2 \ge \ell}}
\pi_{n, m-1}(x)
= \frac1{Z_{n, m-1}}
\sum_{\substack{k, \ell = 1 \\ k + \ell \le m-1}}^{m-1}
\sum_{\substack{x \in \cX_{n, m-1} \\ x_1 \ge k, x_2 \ge \ell}}
\prod_{i = 1}^n \left( \frac{p_i}{\mu_i} \right)^{x_i}, \\
&\overset{(*)}= \frac1{Z_{n, m-1}}
\sum_{\substack{k, \ell = 1 \\ k + \ell \le m-1}}^{m-1}
\left( \frac{p_1}{\mu_1} \right)^{k}
\left( \frac{p_2}{\mu_2} \right)^{\ell}
\sum_{y \in \cX_{n, m - 1 - k - \ell}}
\prod_{i = 1}^n \left( \frac{p_i}{\mu_i} \right)^{y_i}, \\
&= \frac1{Z_{n, m-1}}
\sum_{\substack{k, \ell = 1 \\ k + \ell \le m-1}}^{m-1}
\left( \frac{p_1}{\mu_1} \right)^{k}
\left( \frac{p_2}{\mu_2} \right)^{\ell}
Z_{n, m-1-k-\ell},
\end{align*}
where ($*$) follows by making the change of variable
$y = x - k e_1 - \ell e_2$,
where $e_i$ is the $n$-dimensional vector
with one in component $i$ and zero elsewhere.





\section{Proof of \Cref{theo:bound-time}} \label{app:time}

To prove \Cref{theo:bound-time}, we build on the virtual iterates framework introduced in \cite{koloskova2022sharper} and further refined  in~\cite{leconte2024queuing}. Our proof follows the same reasoning as the proof of \cite[Theorem 1]{leconte2024queuing}, with necessary modifications to incorporate the duration of each round. The analysis relies on the assumptions outlined in Section~\ref{assumptions}.

For $k \in \mathbb{N}_{>0}$, define the filtration $\mathcal{F}_k$ as:
\begin{align*}
\mathcal{F}_k = \sigma(w_l, A_m, 0 \leq l \leq k,1 \leq  m \leq k-1),
\end{align*}
which represents the history of updates and client selections up to the end of iteration $k-1$. The virtual iterates $\mu_k$ are then defined recursively as:
\begin{align*}
\begin{cases}
\mu_0 = \param_0, \\
\mu_1 = \mu_0 - \eta
\sum_{i = 1}^n \frac{\xi_i}{n p_i} g_i(\param_0),
\\
\mu_{k+1} = \mu_k - \frac{\eta}{n p_{A_{k}}} g_{A_{k}}(\param_{k}), & k \geq 1.
\end{cases}
\end{align*}

The difference between $\mu_k$ and $\param_k$ captures the computation tasks dispatched by the \gls{CS}, for which the resulting gradients have not yet been received at the end of round $k-1$ (i.e., just after receiving the gradient from client $C_{k-1}$ and before dispatching a new task to client $A_k$).

Now, consider the system described in \Cref{sec:fl}, where $\tau_t$ denotes the duration of round $t$, and $\tilde{\tau}_t$ represents the time at which the round $t$ concludes, for $t \in \{0, 1, \ldots, T\}$. $\tilde{\tau}_t$ is given by:
\begin{align*}
\tilde{\tau}_t = \sum_{k=0}^t \tau_k \text{,} \quad t \in \{0, 1, \ldots, T\} \text{.}
\end{align*}
The system operates over a (random) total training time of $\tilde{\tau}_T$.

We begin by presenting the following lemma:

\begin{lemma} \label{lem:bound1}
	In the framework of \Cref{sec:fl}, assuming that $\eta \leq \frac{n^2}{8L \sum_{i=1}^n \frac{1}{p_i}}$, the following inequality holds:
	\begin{align*}
	\frac{1}{T+1} \sum_{k=0}^{T} \tau_k \|\nabla f(\param_k)\|^2 \leq
	&\begin{aligned}[t]
	&\frac{4}{\eta (T+1)} \sum_{k=0}^{T} \tau_k \left( f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right] \right) \\
	&+ \frac{2 L^2}{T+1} \sum_{k=0}^{T} \tau_k \|\mu_k - \param_k\|^2 +
	\frac{4 \eta L B}{n^2 (T+1)} \sum_{i=1}^n \frac{1}{p_i} \sum_{k=0}^{T} \tau_k,
	\end{aligned}
	\end{align*}
	where $B = 2M^2 + \sigma^2$.
\end{lemma}

\begin{proof}[Proof of \Cref{lem:bound1}]
	We begin by revisiting an intermediate result from the proof of Lemma~6 in \cite{leconte2024queuing}, which is verified under the same assumptions as \Cref{lem:bound1}. This result, restated below, holds for each $k \in \{0,1, \ldots, T \}$:
	\begin{align} \label{ineq:grad}
	\begin{aligned}
		\|\nabla f(\param_k)\|^2
		\leq
		&\frac{4}{\eta} \left(f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right] \right) + 2 L^2 \|\mu_k - \param_k\|^2 + 8\eta L \sum_{i=1}^n \frac{M^2}{n^2 p_i} \\
		&+ 4 \eta L \sigma^2 \sum_{i=1}^n \frac{1}{n^2 p_i}.
	\end{aligned}
	\end{align}
	To complete the proof, we multiply \eqref{ineq:grad} by $\frac{\tau_k}{T+1}$ and sum the resulting inequality over all $k \in \{0,1,\ldots,T\}$.
\end{proof}

\begin{lemma} \label{lem:bound2}
	In the framework of \Cref{sec:fl} and under the condition $\eta \leq \eta_{\text{max}}$, where
	\begin{align*}
	\eta_{\text{max}} = \min \left( \frac{n^2}{8L \sum_{i=1}^n \frac{1}{p_i}},\min_{k \in \{0,\ldots,T\}} \frac{n}{4L}\sqrt{\frac{\tau_k}{m \sum_{i=1}^{n} \frac{S_{i,k}}{p_i^2}}} \right),
	\end{align*}
	the following inequality holds:
	\begin{align*}
	\frac{1}{8(T+1)}\sum_{k=0}^{T} \bE \left[ \tau_k \|\nabla f(\param_k)\|^2  \right]
	\leq
	\begin{aligned}[t]
		&\frac{1}{\eta (T+1)} \sum_{k=0}^{T} \bE \left[ \tau_k \delta_k \right]
		+ \frac{\eta^2 L^2 B m}{n^2} \sum_{i=1}^{n} \frac{1}{p_i^2} \bE \left[ \frac{1}{T+1}\sum_{k=0}^{T} S_{i,k} \right] \\
		&+ \frac{\eta L B}{n^2} \bE \left[ \frac{\tilde{\tau}_T}{T+1}\right] \sum_{i=1}^n \frac{1}{p_i}.
	\end{aligned}
	\end{align*}
	Here, $\delta_  k = f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right]$ , and $S_{i,k}$ represents the sojourn time (measured in physical time) of the task assigned to client $i$ at round~$k$, if such a task exists; otherwise, it is set to $0$. To clarify, $S_{A_k,k}$ corresponds to the sojourn time of the task sent to client $A_k$ at round $k$, while for all clients $i \neq A_k$, we have $S_{i,k} = 0$.
\end{lemma}

\begin{proof}[Proof of \Cref{lem:bound2}]
	To prove \Cref{lem:bound2}, we begin by deriving an upper bound for the second term on the right-hand side of the inequality stated in \Cref{lem:bound1}.
	
	Let $\mathcal{Q}_k$ represent the multiset of tuples consisting of client indices whose gradient computation tasks are still in progress (either currently being computed or queued) at the beginning of round~$k$ (i.e., immediately after receiving the gradient from client $C_{k-1}$ and prior to assigning a new task to client $A_k$), along with the corresponding round during which these gradients were dispatched to the clients.
	
	The multiset $\mathcal{Q}_k$ can be defined recursively as follows:\celine{Simplify the definition of $\mathcal{Q}_{k+1}$?}\abdelkrim{done}\celine{J'ai échangé $A_k$ et $C_k$, sinon je crois que ça ne marche pas}
	\begin{align*}
	\mathcal{Q}_1 = \left( \bigcup_{i = 1}^n \{(i, 0)\}^{\xi_i - \one{C_0 = i}} \right),
	% \mathcal{Q}_1 = \{(i, 0) \mid i \in S_0, i \neq C_0\},
	\text{ and for all } k \ge 1, \text{ }
	% \mathcal{Q}_{k+1} = \left( \mathcal{Q}_k \setminus \{(C_k, I_k)\} \right) \cup \{(A_{k}, k)\}
	\mathcal{Q}_{k+1} = \left( \mathcal{Q}_k  \cup \{(A_{k}, k)\}  \right) \setminus \{(C_k, I_k)\}.
	\end{align*}
	Recall that $\xi$ is the initial state vector, such that $\xi_i$ is the number of tasks initially assigned to client~$i$, for each $i \in \{1, 2, \ldots, n\}$, and $|\xi| = m$.
	
	For each $k \in \{1, \ldots, T\}$, we have \cite[Lemma 9]{leconte2024queuing}:\celine{Is it correct that the equality is due to the lemma, and the inequality is a direct consequene of the equality due to some known norm-related inequality?}\abdelkrim{yes}
	\begin{align*}
	\|\mu_k - \param_k\|^2 &= \left\| - \eta \sum_{(i,j) \in \mathcal{Q}_k} \frac{1}{n p_i} g_i(\param_j) \right\|^2
	\leq \eta^2 |\mathcal{Q}_k| \sum_{(i,j) \in \mathcal{Q}_k} \frac{1}{n^2 p_i^2} \|g_i(\param_j)\|^2,
	\end{align*}
	where $|\mathcal{Q}_k|$ denotes the cardinality of the multiset $\mathcal{Q}_k$, and for each $k \in \mathbb{N}_{>0}$, $|\mathcal{Q}_k| = m-1$.
	
	Now, taking into account the weighted summation over all $T+1$ rounds, we obtain:
	\begin{align*}
	\frac{1}{T+1} \sum_{k=0}^{T} \tau_k \|\mu_k - \param_k\|^2
	&\leq \frac{\eta^2 m}{T+1} \sum_{k=1}^{T} \tau_k \sum_{(i,j) \in \mathcal{Q}_k} \frac{1}{n^2 p_i^2} \|g_i(\param_j)\|^2, \\
	&= \frac{\eta^2 m}{T+1} \sum_{k=1}^{T} \tau_k \sum_{i=1}^{n} \frac{1}{n^2 p_i^2} \sum_{j=0}^{k-1} \|g_i(\param_j)\|^2 \one{(i,j) \in \mathcal{Q}_k},  \\
	&= \frac{\eta^2 m}{T+1} \sum_{i=1}^{n} \frac{1}{n^2 p_i^2} \sum_{j=0}^{T-1} \|g_i(\param_j)\|^2 \sum_{k=j+1}^{T} \tau_k \one{(i,j) \in \mathcal{Q}_k} .
	\end{align*}
	The equalities are derived by rearranging the terms. Finally, we observe that for all $j \in \{0,1,\ldots,T-1\}$ and $i \in \{1,2,\ldots,n\}$:
	\begin{align*}
	\sum_{k=j+1}^{T} \tau_k \one{(i,j) \in \mathcal{Q}_k} = \min \left(S_{i,j}-\tau_j \one{A_j=i},\tilde{\tau}_T-\tilde{\tau}_{j}\right) \leq S_{i,j}-\tau_j \one{A_j=i} \leq S_{i,j},
	\end{align*}
	where $S_{i,j}$ represents the sojourn time of the task assigned to client $i$ at the beginning of round $j$, provided that client $i$ was the one to receive the task ($A_j=i$), otherwise, $S_{i,j}$ is defined as $0$.
	By taking the expectation, we obtain:
	\begin{align}
	\nonumber
	\bE \left[ \frac{1}{T+1} \sum_{k=0}^{T} \tau_k \|\mu_k - \param_k\|^2 \right]
	&\leq \frac{\eta^2 m}{T+1} \sum_{i=1}^{n} \frac{1}{n^2 p_i^2} \sum_{j=0}^{T-1} \bE \left[ \|g_i(\param_j)\|^2 S_{i,j} \right], \\
	\label{ineq:giSij}
	&\leq \frac{\eta^2 m}{T+1} \sum_{i=1}^{n} \frac{1}{n^2 p_i^2} \sum_{j=0}^{T} \bE \left[ \|g_i(\param_j)\|^2 S_{i,j} \right].
	\end{align}
	
	Next, we examine the term $\bE \left[ \|g_i(\param_j)\|^2 S_{i,j} \right]$ for all $j \in \{0,1,\ldots,T\}$ and $i \in \{1,2,\ldots,n\}$. Using the law of total expectation, we have:\celine{I'm not sure why you need the conditioning on $w_j$. To be able to apply A3?}
	\begin{align}
	\label{eq:giSij}
	\bE \left[ \|g_i(\param_j)\|^2 S_{i,j} \right] = \bE  \left( \bE \left[ \|g_i(\param_j)\|^2 S_{i,j} | \param_j \right]  \right)
	=  \bE  \left( \bE \left[ \|g_i(\param_j)\|^2 | \param_j \right] \bE \left[ S_{i,j} | \param_j \right]  \right).
	\end{align}
	
	To bound $\bE \left[ \|g_i(\param_j)\|^2 | \param_j \right]$, we utilize the model assumptions. Expanding and simplifying, we get:\celine{Maybe add justification for the third inequality? (I think you already told me why it was true but I forgot.}
	\begin{align*}
	\bE \left[ \|g_i(\param_j)\|^2 | \param_j \right] &=  \bE \left[ \|g_i(\param_j)-\nabla f_i(\param_j)+\nabla f_i(\param_j)\|^2 | \param_j \right], \\
	&\leq   2 \bE \left[ \|g_i(\param_j)-\nabla f_i(\param_j)\|^2 | \param_j \right] + 2 \bE \left[ \|\nabla f_i(\param_j)\|^2 | \param_j \right], \\
	&\leq 2 \sigma^2 + 2 \bE \left[ \|\nabla f_i(\param_j)\|^2 | \param_j \right] \quad \text{(using Assumption~A\ref{A3})}, \\
	& \leq 2 \sigma^2 + 4 \bE \left[ \|\nabla f_i(\param_j) - \nabla f(\param_j)\|^2 | \param_j \right] + 4 \bE \left[ \|\nabla f(\param_j)\|^2 | \param_j \right], \\
	&\leq 2 \sigma^2 + 4 M^2 + 4 \bE \left[ \|\nabla f(\param_j)\|^2 | \param_j \right] \quad \text{(using Assumption~A\ref{A4})}.
	\end{align*}
	
	Substituting this bound into \Cref{eq:giSij} and defining $B = \sigma^2 + 2 M^2$, we find:
	\begin{align*}
	\bE \left[ \|g_i(\param_j)\|^2 S_{i,j} \right] \leq 2 \bE \left[ \left( 2 \|\nabla f(\param_j)\|^2 + B \right) S_{i,j} \right].
	\end{align*}
	
	Plugging this result into inequality \eqref{ineq:giSij}, we get:
	\begin{align} \label{ineq:inFlight}
	\bE \left[ \frac{1}{T+1} \sum_{k=0}^{T} \tau_k \|\mu_k - \param_k\|^2 \right]
	&\leq \frac{2 \eta^2 m}{T+1} \sum_{i=1}^{n} \frac{1}{n^2 p_i^2} \sum_{j=0}^{T} \bE \left[ \left( 2 \|\nabla f(\param_j)\|^2 + B \right) S_{i,j} \right].
	\end{align}
	Finally, using the definition of $\tilde{\tau}_T$, we have:
	\begin{align}\label{eq:tau_T}
	\frac{1}{T+1}\sum_{k=0}^{T} \tau_k = \frac{\tilde{\tau}_T}{T+1}.
	\end{align}
	By taking the expectation of the inequality established in \Cref{lem:bound1} and incorporating inequality~\eqref{ineq:inFlight} along with equation~\eqref{eq:tau_T}, we obtain the following:
	\begin{align*}
	\frac{1}{T+1}\sum_{k=0}^{T} \bE \left[ \tau_k \|\nabla f(\param_k)\|^2 \right]
	\leq
	&\frac{4}{\eta (T+1)} \sum_{k=0}^{T} \bE \left[ \tau_k \delta_k \right] \\
	& + \frac{4 \eta^2 L^2 m}{n^2} \sum_{i=1}^{n} \frac{1}{p_i^2}  \frac{1}{T+1}\sum_{k=0}^{T}  \bE \left[ \left(2 \|\nabla f(\param_k)\|^2 + B \right) S_{i,k} \right] \\
	& + \frac{4 \eta L B}{n^2} \sum_{i=1}^n \bE \left[ \frac{\tilde{\tau}_T}{T+1}\right] \frac{1}{p_i}.
	\end{align*}
	Therefore, we conclude:
	\begin{align*}
	&\frac{1}{T+1}\sum_{k=0}^{T} \bE \left[ \tau_k \left( \frac{1}{4} - 2 \frac{\eta^2 L^2 m}{\tau_k} \sum_{i=1}^{n} \frac{S_{i,k}}{n^2 p_i^2} \right)\|\nabla f(\param_k)\|^2  \right] \\
	&\leq
	\frac{1}{\eta (T+1)} \sum_{k=0}^{T} \bE \left[ \tau_k \delta_k \right]
	+ \frac{\eta^2 L^2 B m}{n^2} \sum_{i=1}^{n} \frac{1}{p_i^2} \bE \left[ \frac{1}{T+1}\sum_{k=0}^{T} S_{i,k} \right]
	+ \frac{\eta L B}{n^2} \bE \left[ \frac{\tilde{\tau}_T}{T+1}\right] \sum_{i=1}^n \frac{1}{p_i}.
	\end{align*}
	The result follows by taking:
	\begin{align*}
	\eta \le \min_{k \in \{0,1,\ldots,T\}} \frac{n}{4L}\sqrt{\frac{\tau_k}{m \sum_{i=1}^{n} \frac{S_{i,k}}{p_i^2}}}.
	\end{align*}
\end{proof}

The last lemma we need to introduce before proving \Cref{theo:bound-time} is as follows:
\begin{lemma} \label{lem:bound3}
	In the framework of \Cref{sec:fl}, we have:
	\begin{align}
	\label{exp:lambda}
	\bE \left[\frac{\tilde{\tau}_T}{T+1}\right] &= \frac{1}{\lambda}, \\
	\label{exp:s_time}
	\bE \left[\frac{1}{T+1} \sum_{k=0}^{T} S_{i,k}\right] &= \frac{\bE[\xi_i]}{\lambda},
	\quad i \in \{1, 2, \ldots, n\}, \\
	\label{exp:init_term}
		\mathbb{E}\left( \tau_k \left(f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right]\right) \right)
		&= \frac{1}{\lambda} \sum_{x \in \mathcal{X}_{n, m}} \pi_{n,m}(x) \mathbb{E}\left(f(\mu_k) - f(\mu_{k+1}) | Y_k =x \right),
	\end{align}
	where the last equality holds for each $k \in \{0, 1, \ldots, T\}$.
	Here, $Y_k = \left(Y_{1,k}, Y_{2,k}, \ldots, Y_{n,k}\right)$ denotes the state of the network during round~$k$,
	so that $Y_{i, k} = X_{i, k} + \one{A_k = i}$.
	$\lambda$ is the throughput of the Jackson network, that is, the number of rounds per unit of physical time, given as follows, with $Z_{n, m}$ and $Z_{n, m-1}$ as defined in \Cref{prop:jackson}:
	\begin{align} \label{eq:throughput2}
	\lambda = \sum_{i=1}^n \mu_i \mathbb{P}(\xi_i > 0) = \frac{Z_{n,m-1}}{Z_{n,m}}.
	\end{align}
	The random vector $\xi \in \cX_{n, m}$ follows the distribution $\pi_{n, m}$ given in~\eqref{eq:pi}.
\end{lemma}
Observe that \Cref{exp:s_time} serves as the continuous-time counterpart to \Cref{eq:D}. Similarly, analogous expressions for \Cref{eq:gradD}, \eqref{eq:Xi}, and \eqref{eq:XiXj} can be readily derived for $\xi$ by following the same reasoning as in \Cref{theo:little}.

\begin{proof}[Proof of \Cref{lem:bound3}]
	We prove each equation individually.
	
	\vspace{.2cm}
	
	\noindent \emph{Proof of \Cref{eq:throughput2}.}
	We start by proving \Cref{eq:throughput2}. The throughput $\lambda$ is defined as the average number of rounds per unit time. Mathematically, for a given physical time window $s \in \mathbb{R}_{>0}$, the throughput can be expressed as:
	\begin{align*}
	\lambda &= \mathbb{E} \left[\frac{1}{s} \sum_{k \ge 0} \one{\tilde{\tau}_k \leq s} \right]
	= \mathbb{E} \left[ \frac{1}{s} \int_0^s \sum_{i=1}^n \mu_i \one{\xi_i(t)>0} dt \right], \\
	& = \sum_{i=1}^n \mu_i \bP(\xi_i>0)
	= \sum_{i=1}^n \mu_i \frac{p_i}{\mu_i} \frac{Z_{n,m-1}}{Z_{n,m}}
	= \frac{Z_{n,m-1}}{Z_{n,m}}.
	\end{align*}
	
	The first equality follows from the definition of throughput. The second equality uses the stochastic intensity formula \cite[Section 1.8.3 "Stochastic Intensity Integration Formula"]{baccelli2002}, considering the point process $\{\tilde{\tau}_k\}_{k \in \mathbb{N}}$ with its stochastic intensity $\sum_{i=1}^n \mu_i \one{\xi_i(t) > 0}$. The third equality arises by interchanging the expectation and the integral under stationarity assumptions. The fourth equality replaces $\bP(\xi_i > 0)$ according to \cite[Equation~(6)]{buzen:1973}. This concludes the proof.
	
	\vspace{.2cm}
	
	\noindent \emph{Proof of \Cref{exp:lambda}.}
	By the definition of $\tilde{\tau}_T$, we have:
	\begin{align*}
	\mathbb{E}[\tilde{\tau}_T] = \sum_{k=0}^{T} \mathbb{E}[\tau_k].
	\end{align*}
	
	Recall that $Y_t = \left( Y_{1,t}, Y_{2,t}, \ldots, Y_{n,t} \right)$, where $t \in \mathbb{N}$, represents the state of the network at round~$t$, so that $Y_{i, t} = X_{i, t} + \one{A_t = i}$. The sequence $\{Y_t\}_{t \in \mathbb{N}}$ can be recursively defined as follows: 
	\begin{itemize}
		\item $Y_0 = \xi$ represents the initial state of the network during the first round of training.
		\item For each $t \in \mathbb{N}_{>0}$ and $i \in \{1,\ldots,n\}$, $Y_{i, t} = Y_{i, t-1} + \one{A_t = i} - \one{C_{t-1} = i}$.
	\end{itemize}
	We know that the sequence $\{\tau_k | Y_k\}_{k \in \{0, \ldots, T\}}$ consists of independent random variables, such that $\tau_k | Y_k$ is exponentially distributed with a parameter $\sum_{i=1}^n \mu_i \one{Y_{i,k} \geq 1}$, for each $k \in \{0, 1, \ldots, T\}$. Conditioning on $Y_k$, we can write:
	\begin{align*}
	\mathbb{E}[\tilde{\tau}_T] &= \sum_{k=0}^{T} \sum_{x \in \mathcal{X}_{n, m}} \mathbb{E}[\tau_k | Y_k=x] \mathbb{P}(Y_k=x)
	= \sum_{k=0}^{T} \sum_{x \in \mathcal{X}_{n, m}} \frac{1}{\sum_{i=1}^n \mu_i \one{x_i \geq 1}} \mathbb{P}(Y_k=x).
	\end{align*}
	
	The sequence $\{Y_k\}_{k \in \mathbb{N}}$ forms an ergodic, discrete, homogeneous Markov chain. Its stationary distribution can be derived by observing that it corresponds to the jump chain of the ergodic, time-continuous Markov chain $\{\xi_t\}_{t \geq 0}$. Using Equation~(13.56) of Theorem~13.4.5 from \cite{Brémaud:2020}, the stationary distribution of $\{Y_k\}_{k \in \mathbb{N}}$ is given by:
	\begin{align} \label{st:jump}
	\mathbb{P}(Y_k=x) = \frac{1}{V_{n,m}} \sum_{i=1}^n \mu_i \one{x_i \geq 1} \prod_{j=1}^n \left( \frac{p_j}{\mu_j} \right)^{x_j},
	\quad x \in \mathcal{X}_{n, m},
	\end{align}
	where $V_{n,m}$ is a normalizing constant.
	
	Under the stationarity assumption (A\ref{A5}) and substituting \eqref{st:jump} into the previously derived expression for $\mathbb{E}[\tilde{\tau}_T]$, we obtain:
	\begin{align} \label{eq:E-tau_T}
	\bE[\tilde{\tau}_T] &= \sum_{k=0}^{T} \sum_{x \in \cX_{n, m}} \frac{1}{V_{n,m}} \prod_{j = 1}^n \left( \frac{p_j}{\mu_j} \right)^{x_j} 
	\overset{(*)}{=} (T+1) \frac{Z_{n,m}}{V_{n,m}} .
	\end{align}
	
	Next, we demonstrate that $V_{n,m} = Z_{n,m-1}$. By definition, we have:
	\begin{align*} 
	V_{n,m} &= \sum_{x \in \cX_{n, m}} \sum_{\substack{i=1\\ x_i \geq 1 }}^n \mu_i \prod_{j = 1}^n \left( \frac{p_j}{\mu_j} \right)^{x_j}, \\
	& \overset{(a)}{=} \sum_{i=1}^n \mu_i \sum_{\substack{x \in \cX_{n, m}\\ x_i \geq 1 }} \prod_{j = 1}^n \left( \frac{p_j}{\mu_j} \right)^{x_j}
	\overset{(b)}{=} \sum_{i=1}^n p_i \sum_{\substack{x \in \cX_{n, m}\\ x_i > 0 }} \prod_{j = 1}^n \left( \frac{p_j}{\mu_j} \right)^{(x-e_i)_j},
	\end{align*}
	where, for each $i \in \{1,\ldots,n\}$, $e_i$ denotes the $n$-dimensional vector with one in component $i$ and zero elsewhere.
	(a) is obtained by rearranging the order of summation, while (b) follows from factoring out $\frac{p_i}{\mu_i}$. Applying the variable substitution $y = x - e_i$, the expression simplifies as:
	\begin{align*}
	V_{n,m} = \underbrace{\sum_{i=1}^n p_i}_{=1} \underbrace{\sum_{y \in \mathcal{X}_{n, m-1}} \prod_{j=1}^n \left( \frac{p_j}{\mu_j} \right)^{y_j}}_{=Z_{n,m-1}} = Z_{n,m-1}.
	\end{align*}
	
	Thus, incorporating this result into equality ($*$) of \eqref{eq:E-tau_T}, and using \eqref{eq:throughput2}, we conclude that:
	\[
	\mathbb{E}\left[\frac{\tilde{\tau}_T}{T+1}\right] = \frac{Z_{n,m}}{Z_{n,m-1}} = \frac{1}{\lambda}.
	\]
	
	\vspace{.2cm}
	
	\noindent \emph{Proof of \Cref{exp:s_time}.} To establish \Cref{exp:s_time}, consider $i \in \{1, 2, \ldots, n\}$ and $k \in \mathbb{N}$. By conditioning on the state of $A_k$, we have:  
	\begin{align*}
	\mathbb{E} \left[S_{i,k}\right] = \sum_{j=1}^n p_j \mathbb{E} \left[S_{i,k} \mid A_k=j\right] 
	= \mathbb{E} \left[S_{i,k} \mid A_k=i\right] p_i.
	\end{align*}
	Here, $S_{i,k} \mid A_k=i$ represents the sojourn time of the task assigned to client~$i$ at round~$k$.  
	
	Assuming stationarity (A\ref{A5}), and invoking Little's Law \cite{Little2011ORF}, we deduce that for each $k \in \mathbb{N}$:  
	\begin{align*}
	\mathbb{E} \left[\xi_i \right] = p_i \lambda \mathbb{E} \left[S_{i,k} \mid A_k=i\right],
	\end{align*}
	where $p_i \lambda$ is the arrival rate to client~$i$ in the closed Jackson network.
	
	Thus, we obtain:  
	\begin{align*}
	\mathbb{E} \left[\frac{1}{T+1} \sum_{k=0}^{T} S_{i,k}\right] &= \frac{\mathbb{E}[\xi_i]}{\lambda}, \quad \forall i \in \{1, 2, \ldots, n\},
	\end{align*}
	which completes the proof.
	
	\vspace{.2cm}
	
	\noindent \emph{Proof of \Cref{exp:init_term}.}
	For each $k \in \{0, 1, \ldots, T\}$, conditioning on $Y_k$, we can write:
	\begin{align*}
	&\mathbb{E}\{ \tau_k \left(f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right]\right) \} \\
	&= \sum_{x \in \mathcal{X}_{n, m}} \mathbb{E} \{ \tau_k \left(f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right]\right) | Y_k=x \} \mathbb{P}(Y_k=x), \\
	&\overset{(c)}{=} \sum_{x \in \mathcal{X}_{n, m}} \left(\sum_{\substack{i=1\\ x_i \geq 1 }}^n \mu_i \right)^{-1} \mathbb{E} \{ f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right] | Y_k=x \} \frac{1}{V_{n,m}} \sum_{\substack{i=1\\ x_i \geq 1 }}^n \mu_i \prod_{j=1}^n \left( \frac{p_j}{\mu_j} \right)^{x_j}, \\ 
	&\overset{(d)}{=} \frac{1}{V_{n,m}} \sum_{x \in \mathcal{X}_{n, m}} \prod_{j=1}^n \left( \frac{p_j}{\mu_j} \right)^{x_j} \mathbb{E}\{f(\mu_k) - f(\mu_{k+1}) | Y_k=x \}.
	\end{align*}
	Step (c) follows from the fact that for all $x \in \mathcal{X}_{n, m}$, the holding times $\{\tau_k | Y_k=x\}_{k \in \mathbb{N}}$ are exponentially distributed with parameter $\sum_{\substack{i=1, x_i \geq 1 }}^n \mu_i$ and are independent of everything else, while step (d) is obtained by simplifying the expression. 
	
	Dividing and multiplying by $Z_{n,m}$, we get:
	\begin{align*}
	&\mathbb{E}\{ \tau_k \left(f(\mu_k) - \mathbb{E}\left[ f(\mu_{k+1}) \mid \mathcal{F}_k \right]\right) \} \\
	&= \frac{Z_{n,m}}{V_{n,m}} \sum_{x \in \mathcal{X}_{n, m}} \frac{1}{Z_{n,m}} \prod_{j=1}^n \left( \frac{p_j}{\mu_j} \right)^{x_j} \mathbb{E}\{f(\mu_k) - f(\mu_{k+1}) | Y_k=x \},  \\ 
	&= \frac{1}{\lambda} \sum_{x \in \mathcal{X}_{n, m}} \pi_{n,m}(x) \mathbb{E}\left(f(\mu_k) - f(\mu_{k+1}) | Y_k =x \right).
	\end{align*}
	This concludes the proof.
\end{proof}

\begin{proof}[Proof of \Cref{theo:bound-time}]
	Taking the inequality from \Cref{lem:bound2} and applying the results of \Cref{lem:bound3}, we obtain:
	\begin{align*} 
	&\bE \left[ \frac{1}{8(T+1)}\sum_{t=0}^{T} \tau_{t} \|\nabla f(w_t)\|^2 \right] \leq \frac{\tilde{A}}{\eta \lambda}
	+ \frac{\eta L B}{\lambda n^2} \sum_{i=1}^{n} \frac{1}{p_i}
	+ \frac{\eta^2 L^2 B m}{\lambda n^2} \sum_{i=1}^{n} \frac{\mathbb{E}[\xi_i]}{p_i^2}  \text{,}
	\end{align*}
	where: $ \tilde{A} = \frac{1}{T+1}\sum_{k=0}^T \sum_{x \in \mathcal{X}_{n, m}} \pi_{n,m}(x) \mathbb{E}\left(f(\mu_k) - f(\mu_{k+1}) | Y_k =x \right)$.
\end{proof}




\section{Compute $G$ and $\nabla_p G$} \label{app:G}

\begin{proposition} \label{prop:bound}
	In the framework of \Cref{sec:fl}, the expression of the upper bound $G(p)$ in terms of routing probabilities is as follows:
	\begin{align}
	G(p)
	&= \frac{A}{\eta(T + 1)}
	+ \frac{\eta L B}{n^2} \sum_{i=1}^{n} \frac{1}{p_i}
	+ \frac{\eta^2 L^2 B m}{n^2} \sum_{i=1}^{n} \sum_{k = 1}^{m-1}
	\frac{p_i^{k-2}}{\mu_i^k} 
	\frac{Z_{n, m-1-k}}{Z_{n, m-1}},
	\end{align}
	where the normalizing constants $Z_{n, \um}$ for $\um \in \{0, 1, \ldots, m - 1\}$ can be computed explicity
	with $O(nm)$ time and memory complexity,
	as shown in \Cref{prop:jackson}.
\end{proposition}	

To identify the best routing strategy, we aim to minimize this upper bound using a gradient descent algorithm. This requires computing the gradient of the function $G(p)$ with respect to the routing probability vector, as detailed in the following proposition, which is a consequence of \Cref{theo:little}.

\begin{proposition} \label{prop:grad}
	For each $j \in \{1,\ldots,n\}$, we have
	\begin{align*}
	\frac{\partial G}{\partial p_j} =
	\frac{\eta L B}{n^2 p_j}
	\left(
	-\frac{1}{p_j}
	+ \eta m L \left(
	\sum_{i=1}^n \frac{\cov[X_i, X_j]}{p_i^2}
	- \frac{2 \bE[X_j]}{p_j^2}
	\right)
	\right),
	\end{align*}
	
	where $\bE[X_j]$ and $\cov[X_i, X_j]$ can be computed explicity
	with $O(nm)$ time and memory complexity by applying Buzen's algorithm, 
	as shown in \Cref{prop:jackson}.
\end{proposition}

To ensure that the parameters \( p = (p_i, i \in \{1, 2, \ldots, n\}) \) satisfy probability constraints, we introduce auxiliary parameters \( \Theta = (\theta_i, i \in \{1, 2, \ldots, n\}) \) and apply the softmax function:
\begin{align*}
p_j = \frac{\re^{\theta_j}}{\sum_{i=1}^n \re^{\theta_i}}, \quad j \in \{1, \ldots, n\}.
\end{align*}
This method guarantees that the resulting parameters \( p \) are valid probabilities.

The gradient of the upper bound with respect to \( \Theta \) is given by:
\begin{align*}
\frac{\partial G}{\partial \theta_j}
= \left\langle \nabla_p G , \frac{\partial p}{\partial \theta_j} \right\rangle, \quad 
\text{where: } \frac{\partial p}{\partial \theta_j} = p_j (e_j - p), \text{for each \( j \in \{1, \ldots, n\} \).}
\end{align*}
Here, \( e_j \) denotes the $n$-dimensional vector with one in component $j$ and zero elsewhere, and \( \langle \cdot , \cdot \rangle \) represents the dot product in \( \mathbb{R}^n \).

\section{Compute $H$ and $\nabla_p H$} \label{app:H}

\begin{proposition} \label{prop:boundH}
	In the framework of \Cref{sec:fl}, the expression of the upper bound $H(p)$ in terms of the routing probabilities is given by:
	\begin{align}
	H
	&= \frac{Z_{n,m}}{Z_{n,m-1}} \left( \frac{\tilde{A} }{\eta}
	+ \frac{\eta L B}{ n^2} \sum_{i=1}^{n} \frac{1}{p_i} \right)
	+ \frac{\eta^2 L^2 B m}{n^2} \sum_{i=1}^{n} \sum_{k = 1}^{m}
	\frac{p_i^{k-2}}{\mu_i^k} 
	\frac{Z_{n, m-k}}{Z_{n, m-1}},
	\end{align}
	where the normalizing constants $Z_{n, \um}$ for $\um \in \{0, 1, \ldots, m\}$ can be computed explicitly
	with $O(nm)$ time and memory complexity,
	as shown in \Cref{prop:jackson}.
\end{proposition}	

To find the optimal routing strategy, we aim to minimize this upper bound using a gradient descent algorithm. This requires the gradient of $H(p)$ with respect to the routing probabilities, given in the following proposition.

\begin{proposition} \label{prop:gradH}
	For each $j \in \{1,\ldots,n\}$, the gradient $\nabla_p H$ with respect to the routing probabilities $p_j$ is:
	\begin{align*}
	\frac{\partial H}{\partial p_j} =
	\frac{\tilde{A} \bE[\xi_j-X_j]}{\eta \lambda p_j}+
	\frac{\eta L B}{n^2 p_j \lambda}
	\begin{aligned}[t]
		\Bigg(
			&-\frac{1}{p_j}+
			\bE[\xi_j-X_j] \sum_{i=1}^n \frac{1}{p_i} \\
			&+ \eta m L \left(
			- \frac{2 \bE[\xi_j]}{p_j^2} 
			+ \sum_{i=1}^n \frac{\bE[\xi_i \xi_j]}{p_i^2}
			- \bE[X_j] \sum_{i=1}^n \frac{\bE[\xi_i]}{p_i^2}
			\right)
		\Bigg),
	\end{aligned}
	\end{align*}
	where the random vectors $\xi \in \cX_{n, m}$ and $X \in \cX_{n, m-1}$ are distributed according to $\pi_{n, m}$ and $\pi_{n, m-1}$, respectively, as defined in~\eqref{eq:pi}. Furthermore, for all $(i, j) \in \{1, \ldots, n\}^2$, the quantities $\bE[X_j]$, $\bE[\xi_j]$, and $\bE[\xi_i \xi_j]$ can be efficiently computed with $O(nm)$ time and memory complexity using Buzen's algorithm, as described in \Cref{prop:jackson}.
\end{proposition}





\section{Experiments details} \label{exp:dl}

\subsection{Neural networks architectures}
The Fashion-MNIST and KMNIST datasets each contain 70,000 grayscale images, with 60,000 designated for training and 10,000 reserved for testing. These images, sized at 28×28 pixels, are evenly distributed across 10 classes.
On the other hand, the CIFAR-10 and CIFAR-100 datasets feature 60,000 color images (RGB) with a resolution of 32×32 pixels. Of these, 50,000 are used for training, while 10,000 are allocated for testing. CIFAR-10 is organized into 10 classes, while CIFAR-100 expands to 100 classes.
Notably, all these datasets are class-balanced, ensuring each class has an equal number of images.

For the Fashion-MNIST and KMNIST datasets, we employ a convolutional neural network (CNN) with the following structure:
\begin{itemize}
	\item Two convolutional layers with 7x7 filters, each followed by a ReLU activation function. The first convolutional layer has 20 channels, while the second has 40 channels.
	\item A 2x2 max pooling layer.
	\item A final fully connected layer with 10 neurons, concluded by a softmax activation function.
\end{itemize}
For the CIFAR-10 and CIFAR-100 datasets, we employ a convolutional neural network (CNN) with the following architecture:
\begin{itemize}
	\item \textbf{Three sequential convolutional blocks}, each consisting of two 3$\times$3 convolutional layers. Each layer is followed by ReLU activation and Group Normalization. The configuration of these blocks is as follows:
	\begin{itemize}
		\item The first block contains convolutional layers with 32 channels.
		\item The second block contains convolutional layers with 64 channels.
		\item The third block contains convolutional layers with 128 channels.
	\end{itemize}
	Additionally, each block includes 2$\times$2 max pooling and a dropout layer with a probability of 0.25.
	
	\item \textbf{A classification block} comprising:
	\begin{itemize}
		\item A flattening layer.
		\item A fully connected layer with 128 neurons.
		\item A dropout layer with a probability of 0.25.
		\item A final fully connected layer with the number of neurons corresponding to the number of classes (10 for CIFAR-10 and 100 for CIFAR-100), followed by a softmax activation function.
	\end{itemize}
\end{itemize}

In all experiments, the stochastic gradient for each task is computed using a batch size of 512 data points. The numerical implementation is carried out in PyTorch, and the experiments are performed on an NVIDIA Tesla P100 GPU.



\subsection{Closed Jackson network simulation}

For each $t \in \{0, \ldots, T\}$, recall that $Y_t$ denote the $n$-dimensional random variable representing the queue lengths during round~$t$, and $\tau_t$ represent the duration (in physical time) of round~$t$. The Jackson network simulation proceeds by iterating through the following steps for each round~$t$:
\begin{itemize}
	\item Sample $\tau_t$ from an exponential distribution with rate parameter $\sum_{j=1}^n \mu_j \mathbf{1}\{Y_{j,t} > 0\}$, where $\mu_j$ represents the processing rate of client $j$ and $Y_{j,t}$ denotes the queue length of client $j$ at round~$t$.
	
	\item Select a client $k$ to complete a task at the end of round~$t$ from the set of non-empty queues. The probability of selecting client $k$ is proportional to the processing speeds of the clients, given by the distribution:
	\[
	\left( \frac{\mu_i \one{Y_{i,t} > 0}}{\sum_{j=1}^n \mu_j \one{Y_{j,t} > 0}}, i \in \{1, \ldots, n\} \right).
	\]
	
	\item Reassign the task to another client $l$ based on the routing distribution $(p_i, i \in \{1, \ldots, n\})$. The queue lengths are then updated as follows:
	\[
	Y_{t+1} = Y_t - e_k + e_l,
	\]
	where $e_k$ and $e_l$ are unit vectors indicating that the task is removed from client $k$ and added to client $l$, respectively. This concludes round~$t$ and initiates the next round.
\end{itemize}

\subsection{Additional experiments} \label{app:num-additional}

To evaluate the robustness of the optimized routing strategy~$p^*_G$ with respect to $G$ across challenging heterogeneous environments, we conducted additional experiments using the Fashion-MNIST and CIFAR-100 datasets. The first scenario considered is the same as the one presented in \Cref{num:optimize-updates}, but with a different data partitioning across clients.

\paragraph*{Scenario of \cref{num:optimize-updates} with highly-heterogeneous data distribution}

\Cref{fig:sim_3classes} shows the accuracy and loss trajectories over 3,000 rounds of the optimized, uniform, and balanced routing strategies, for the Fashion-MNIST dataset.
Clients were each limited to 3 unique image labels out of 10, assigned sequentially and cyclically. Specifically, the first client accessed labels 0, 1, and 2; the second client, labels 3, 4, and 5; and so on, wrapping around to 0 after label 9. This ensured a non-\gls{iid} data distribution, as each client could only access a limited subset of labels. All clients were allocated an equal number of images. We ran 10 independent simulations using different random seeds, initializing the central neural networks with identical weights for each routing strategy to eliminate the impact of initialization on convergence. Performance metrics were recorded every 5 rounds on a test dataset.\celine{C'est bizarre parce que dans cette figure, la loss du routage optimisé est plus élevée au début. Une intuition pourquoi ? De même, l'accuracy est légèrement plus basse au début. Du coup c'est délicat quand on dit que "the optimized routing strategy achieves the highest accuracy throughptu the training process.}

\begin{figure*}[hbtp]
	\centering
	\includegraphics[width=.9\textwidth]{results_3_labels.png} 
	\caption{Performance on the validation set at the \gls{CS} in the scenario of \Cref{num:optimize-updates} under highly-heterogeneous data splits using the Fashion-MNIST dataset. Solid lines show metrics averaged over independent simulations, and shaded areas represent the standard deviation.}
	\label{fig:sim_3classes}
\end{figure*}

\begin{figure*}[hbtp]
	\centering
	\includegraphics[width=.9\textwidth]{results_5_labels.png}
	\caption{Performance on the validation set at the \gls{CS} in the scenario of \Cref{num:optimize-updates} under highly-heterogeneous data splits using the CIFAR-100 dataset. Solid lines show metrics averaged over independent simulations, and shaded areas represent the standard deviation.}
	\label{fig:sim_5classes}
\end{figure*}

\Cref{fig:sim_5classes} compares the accuracy and loss trajectories over 30,000 rounds under the three routing strategies.
We applied a similar partitioning approach, restricting each client to 5 unique labels out of the 100 available, assigned sequentially. All clients received an equal number of images. Three independent simulations were conducted with varying random seeds, yet all maintained identical initial weights for the central neural networks across the different routing strategies.
Performance metrics were evaluated every 50 rounds on a test dataset.

Our results show that the optimized routing strategy ultimately achieves the highest accuracy, with a notably faster performance gain compared to the other methods. In contrast, the uniform and balanced routing strategies exhibit higher volatility across independent simulations, and frequent spikes in loss.

\paragraph*{Scenario with disjoint datasets}

To further validate these findings, we examined another challenging heterogeneous setting involving a system with 10 clients and 100 tasks. In this scenario, each image label from the 10 labels in the Fashion-MNIST dataset is assigned exclusively to a single client, ensuring that each client has a completely distinct data distribution. Additionally, the service speed of each client~$i$ is defined as $\mu_i = \textrm{e}^{i/50}$, resulting in the fastest client being approximately 20\% faster than the slowest. The learning rate is set to $\eta = 0.005$, and the smoothness constant is $L = 1$.

In \Cref{fig:sim_1class}, we compare the evolution of accuracy and loss under the optimized routing strategy against the uniform and balanced routing strategies in this more complex environment. Similar to the previous experiment, training spans 3,000 rounds, with metrics recorded on a test dataset at intervals of 5 rounds.

\begin{figure*}[hbtp]
	\centering
	\includegraphics[width=.9\textwidth]{results_10_clients.png} 
	\caption{Performance on the validation set at the \gls{CS} with 10 clients for heterogeneous data splits across 100 tasks. Solid lines show metrics averaged over independent simulations, and shaded areas represent the standard deviation.}
	\label{fig:sim_1class}
\end{figure*}

Our results further validate that the optimized routing strategy~$p^*_G$ consistently surpasses the alternatives $p^{\text{uniform}}$ and $p^{\propto \mu}$, achieving superior accuracy with a consistently faster performance gain. Furthermore, it demonstrates greater robustness and stability in this highly heterogeneous environment, showing a smaller standard deviation in both loss and accuracy across the 10 independent simulations, along with fewer loss spikes compared to the uniform and balanced strategies. Additionally, although the optimized routing often selects the slowest client more frequently, the learning process does not develop a bias toward optimizing the local loss of this client. Instead, it preserves balanced global performance, effectively avoiding overfitting to the image labels associated with any specific client.

\paragraph*{Scenario of \cref{num:optimize-time} with CIFAR-10}

To further evaluate the effectiveness of H-optimized routing in terms of physical time performance, we conduct experiments on the CIFAR-10 dataset and compare it against uniform routing across three different configurations. The first configuration considers homogeneous datasets across clients, the second involves heterogeneous datasets distributed according to a Dirichlet distribution (as described in \cref{num:optimize-time}), and the third represents a highly heterogeneous setting where each client has access to only three out of the ten image classes. We use the same network setup as in \cref{num:optimize-time}. The results, presented in \cref{fig:sim_time}, consistently demonstrate the superiority of H-optimized routing across all configurations, showcasing its ability to effectively balance update frequency while minimizing gradient staleness.

\begin{figure*}[ht]
	\begin{center}
		\centerline{\includegraphics[width=\textwidth]{cifar10_time.png}}
		\caption{Performance over physical time on the validation set in the scenario of \Cref{num:optimize-time}, with $n = 30$ clients and $m = 30$ tasks, evaluated under homogeneous, heterogeneous, and highly heterogeneous data distributions. Standard normalization and data augmentation techniques are applied. The simulation runs for 30,000 time units, recording accuracy and loss every 10 rounds for $p^{\text{uniform}}$ and every 50 rounds for $p^*_H$. Each experiment is repeated independently five times.Solid lines show metrics averaged over independent simulations, and shaded areas represent the standard deviation.}
		\label{fig:sim_time}
	\end{center}
\end{figure*}





\end{document}
