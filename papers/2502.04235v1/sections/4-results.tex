\vspace{-0.5em}
\section{Experiments}
\vspace{-0.5em}
Having established our MAGACorpus generation framework, we now evaluate its effectiveness through a series of experiments designed to answer two key research questions:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{RQ1:} How effective is MAGACorpus as a synthetic expansion to original dataset?
    \item \textbf{RQ2:} How does MAGACorpus aid model scaling under data-constrained scenarios?
\end{itemize}

\subsection{Setup}
\vspace{-0.5em}
\paragraph{Datasets}
To ensure reproducibility, we build MAGACorpus based on SmolLM-Corpus\footnote{\url{https://github.com/huggingface/smollm/tree/main/text/pretraining}}~\citep{benallal2024smollmcorpus}, expanding fineweb-edu-dedup source from 195B tokens to 770B tokens.


\paragraph{Models and Hyperparams}
The architecture of pretraining model follows llama3 \citep{dubey2024llama}.
Experiments across various sizes (134M/377M/1.7B/7B/13B) are running with WSD lr scheduler~\citep{hu2024minicpm} where 0.1\% warmup steps, 75\% stable and final 25\% decay phase.
Detailed model specifications are provided in \autoref{sec:appd_training} and \autoref{tab:model_hyperparams}.
Then, we conduct two groups of experiments: 
(1) to validate MAGACorpus quality in Section~\ref{sec:main_exp}, 
and (2) to demonstrate how MAGA reformulation aids scaling under data-constrained scenarios in Section~\ref{sec:scaling_exp}.
\vspace{-0.5em}
\paragraph{Evaluation}
In Section~\ref{sec:main_exp}, we follow popular practice of \textsc{LightEval}~\citep{lighteval}, evaluating on a comprehensive suite of open benchmarks aligned with Fineweb/SmolLM/Cosmopedia settings\footnote{\url{https://github.com/huggingface/cosmopedia/blob/main/evaluation}}, 
include ARC-E+C~\citep{clark2018think}, HellaSwag~\citep{zellers2019hellaswag}, Winogrande~\citep{sakaguchi2021winogrande}, MMLU~\citep{hendrycks2020measuring}, GSM8K~\citep{cobbe2021gsm8k}, etc.
While model performance is influenced by multiple factors, we list some recently SOTA models as reference, all the models are evaluated in the same environment except Llama-3.2-1B\footnote{Our access request is rejected by repo authors, so we use scores reported by SmolLM.}.
Then in Section~\ref{sec:scaling_exp} and Section~\ref{sec:ablations} we will report the scores and training dynanmics provided by our in-house evaluation kit, where all benchmarks are grouped into knowledge, reasoning and math categories.
\vspace{-0.5em}
\subsection{Effectiveness of MAGACorpus}
\label{sec:main_exp}
\vspace{-0.5em}
\paragraph{Data Recipe}
Models of 134M/377M/1.7B sizes are trained from scratch for up to 1000 billion tokens.
Our baseline is trained on SmolLM-Corpus~\citep{benallal2024smollmcorpus} dataset,
and the other two experiments use MAGACorpus as incremental/replacement data.
In contrast to SmolLM's recipe, we use unique token number from each source as the mixing ratio shown in \autoref{table:subsource_weight}.
This ensures that different sources have consistent repetition epochs during training.

\begin{table*}[h]
  \vspace{-1.5em}
  \centering
  \caption{MAGACorpus experiments data source weight (\%).
  }
  \begin{adjustbox}{max width=0.85\textwidth}
    \setlength{\tabcolsep}{1mm}{
  \begin{tabular}{lccccc}
  \toprule
  experiments & fineweb-edu-dedup & cosmopedia-v2 & python-edu & open-web-math & maga-corpus \\
  \midrule
  Baseline & 80.89 & 11.65 & 1.66 & 5.80 & - \\
  MAGA-Only & - & 11.65 & 1.66 & 5.80 & \textbf{80.89} \\
  MAGA-Mix & \textbf{16.29} & 11.65 & 1.66 & 5.80 & \textbf{64.59} \\
  \bottomrule
  \end{tabular}
  }
  \end{adjustbox}

  \label{table:subsource_weight}
  \vspace{-1em}
\end{table*}

\paragraph{Performance training on MAGACorpus.}
As shown in~\autoref{tab:main_exp}, the MAGA-Mix group shows consistent improvements across different model sizes,
with larger performance gains as model size increases, +0.26/+0.95/+2.15 for 134M/377M/1.7B models respectively.
Notably, MAGA-Mix achieved substantial gains in TriviaQA(+2.03/+6.99/+15.47) and GSM8K(+0.15/+0.22/+6.06).
Additional insights and explanations regarding metric changes are provided in \autoref{sec:appd_experiments}.
These results strongly demonstrate the effectiveness and quality of MAGACorpus.

\begin{table*}[h]
    \center
    \vspace{-1em}
    \setlength{\fboxsep}{1pt}
    \caption{Benchmark {\ours} with SOTA small LMs. Models of similar size are grouped.
    All results are obtained through \textsc{LightEval}~\citep{lighteval}.
    Best results in each group are highlighted in \textbf{bold}, the second in \underline{underline}, and in \colorbox{green!15}{green} for that MAGA-Mix wins under fair comparison.
    }
    \vspace{-0.5em}
    \renewcommand{\arraystretch}{1.15}
    \begin{adjustbox}{max width=\textwidth}

    \setlength{\tabcolsep}{1mm}{
    \setlength{\fboxsep}{1.5pt}
    \begin{tabular}{@{\extracolsep{\fill}}lcccccccccccccccc}
    \toprule

      Model & \#Params. & \#Tokens & ARC(C+E) & Wino. & Hella. & MMLU & MMLU-PRO & CSQA & OpenBookQA & PIQA & TriviaQA & GSM8K & Avg. \\
      \midrule
      SmolLM2-135M & 135M & 2T & \textbf{44.12} & 51.07 & \textbf{42.03} & \textbf{31.27} & 11.06 & \underline{33.82} & 35 & \textbf{68.23} & \underline{1.91} & \textbf{1.52} & \textbf{32.00} \\
      SmolLM-135M & 135M & 600B & 42.47 & 51.54 & 41.08 & 29.93 & \underline{11.4} & 32.51 & 33.2 & \underline{68.17} & 1.08 & 0.99 & 31.24 \\
      Baseline & 134M & 600B & 41.71 & \textbf{52.41} & 40.69 & 30.03 & 11.37 & \textbf{34.32} & \underline{35.4} & 67.85 & 0.02 & 1.29 & 31.51 \\
      MAGA-Mix & 134M & 600B & \colorbox{green!15}{\underline{43.01}} & \underline{51.7} & \colorbox{green!15}{\underline{41.25}} & \colorbox{green!15}{\underline{30.1}} & \colorbox{green!15}{\textbf{11.76}} & 32.68 & \colorbox{green!15}{\textbf{36.4}} & 67.3 & \colorbox{green!15}{\textbf{2.05}} & \colorbox{green!15}{\underline{1.44}} & \colorbox{green!15}{\underline{31.77}} \\
      \midrule
      Qwen2.5-0.5B & 360M & 18T & 45.16 & \textbf{53.99} & 51.16 & 33.51 & \textbf{11.97} & 31.61 & 37.6 & 69.97 & 3.96 & \textbf{32.9} & \underline{37.18} \\
      SmolLM2-360M & 360M & 4T & \textbf{53.4} & 52.33 & \textbf{54.58} & \textbf{35.29} & 11.17 & \textbf{37.92} & 37.6 & 71.76 & \textbf{16.73} & \underline{2.96} & \textbf{37.37} \\
      SmolLM-360M & 360M & 600B & \underline{49.99} & \underline{52.96} & \underline{51.67} & 33.84 & \underline{11.42} & 34.81 & 37.6 & \underline{71.87} & 2.27 & 1.97 & 34.84 \\
      Baseline & 377M & 600B & 48.57 & 52.64 & 51.02 & 33.63 & 11.25 & 36.77 & \textbf{39} & 71 & 0.29 & 1.52 & 34.57 \\
      MAGA-Mix & 377M & 600B & \colorbox{green!15}{49.39} & 52.64 & \colorbox{green!15}{51.34} & \colorbox{green!15}{\underline{34.09}} & \colorbox{green!15}{11.35} & \colorbox{green!15}{\underline{37.1}} & \underline{38} & \colorbox{green!15}{\textbf{72.31}} & \colorbox{green!15}{\underline{7.28}} & \colorbox{green!15}{1.74} & \colorbox{green!15}{35.52} \\
      \midrule
      Qwen2.5-1.5B & 1.3B & 18T & 58.36 & \underline{58.64} & \underline{66.39} & 40.23 & 13.85 & 34.4 & 39.6 & 75.95 & 20.51 & \textbf{60.8} & \underline{46.87} \\
      SmolLM2-1.7B & 1.7B & 11T & \textbf{60.42} & \textbf{59.59} & \textbf{68.73} & \textbf{41.4} & \textbf{19.61} & \textbf{43.65} & 42.6 & \textbf{77.53} & \textbf{36.68} & \underline{29.04} & \textbf{47.93} \\
      Llama-3.2-1B & 1.2B & 9T & 49.2 & 57.8 & 61.2 & 36.63 & 11.7 & 41.2 & 38.4 & 74.8 & \underline{28.1} & 7.2 & 40.62 \\
      OLMo-1B-0724 & 1B & 3.05T & 44.71 & 56.04 & 64.38 & 32.3 & 11.8 & 33.09 & 38 & 75.24 & 13.82 & 2.43 & 37.18 \\
      SmolLM-1.7B & 1.7B & 1T & 59.95 & 54.7 & 62.83 & 39.35 & 10.92 & 38 & 42.6 & 75.9 & 13.14 & 4.62 & 40.20 \\
      Baseline & 1.7B & 1T & 59.63 & 57.38 & 65.19 & 39.4 & 12.11 & \underline{42.59} & \textbf{45.6} & 76.88 & 4.95 & 7.81 & 41.15 \\
      MAGA-Mix & 1.7B & 1T & \colorbox{green!15}{\underline{60.36}} & \colorbox{green!15}{57.46} & \colorbox{green!15}{65.52} & \colorbox{green!15}{\underline{40.79}} & \colorbox{green!15}{\underline{14.1}} & 41.11 & \underline{42.8} & \colorbox{green!15}{77.53} & \colorbox{green!15}{20.42} & \colorbox{green!15}{13.87} & \colorbox{green!15}{43.4} \\
    \bottomrule
    \end{tabular}
    }
    \end{adjustbox}
    \label{tab:main_exp}

\end{table*}
\vspace{-1em}

\subsection{Scaling Results}
\label{sec:scaling_exp}
\vspace{-0.5em}

\paragraph{Scaling Strategies}
In this section, we train models from 377M to 13B without the learning rate decay phase,
the hyperparams except data scheduler are algined with Section~\ref{sec:main_exp}.

\begin{table*}[h]
  \vspace{-1em}
  \centering
  \caption{Scaling experiments data recipe, values represent \#unique\_tokens × \#epochs.}
  \begin{adjustbox}{max width=\textwidth}
    \setlength{\tabcolsep}{1mm}{
  \begin{tabular}{ccccccl}
  \toprule
  \multirow{2}*{Repetition} &\multirow{2}*{Experiments} & Training  & fineweb-edu & maga & fineweb & \multicolumn{1}{c}{\multirow{2}*{Design Rationale}} \\
  & & Budget & dedup & corpus & random & \\
  \midrule
  \multirow{3}*{EntireSet}  & Baseline         & 500B & 50 × 10 & - & -  & \\
                            & Full-Fineweb-Edu & 500B & 195 × 2.56 & - & - & What if we could collect more unique data.\\
                            & MAGA Expansion   & 500B & 50 × 2 & 200 × 2 & - & Add maga to reduce the repetition num. \\
  \midrule
  \multirow{3}*{Subset } & Baseline       & 700B & 50 × 1.4 & - & 450 × 1.4 & \\
                         & Upsample-EDU   & 700B & 50 × 5 & - & 450 × 1 & Upsample to get 200B more budget.\\
                         & MAGA Expansion & 700B & 50 × 1 & 200 × 1 & 450 × 1 & Add maga to achieve the same target.  \\
  \bottomrule
  \end{tabular}
  }
  \end{adjustbox}

  \label{table:scaling_weight}
\end{table*}
% \vspace{-0.5em}

We simulate two common scenarios under data-constrained conditions:
\begin{itemize}[leftmargin=1.5em]
  \vspace{-0.5em}
  \item Scenario 1: Expanding a 50B high-quality dataset to a 500B training budget.
  % \vspace{-0.3em}
  \item Scenario 2: Expanding a 500B mixed-quality dataset to a 700B training budget.
\end{itemize}
\vspace{-0.5em}
The experiment design for different strategies is presented in \autoref{table:scaling_weight},
which involves three datasets:
(1) a 50B-token random sample from fineweb-edu-dedup,
(2) a corresponding filtered subset from MAGACorpus,
and (3) a 450B-token corpus obtained from Fineweb~\citep{penedo2024finewebdatasets} after global min-hash deduplication.


\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/scaling-exp.pdf}
  \vspace{-2em}
  \caption{The first and last two figures illustrate the training dynamics of EntireSet and SubSet data recipes, respectively.
  Benchmark details are provided in~\autoref{sec:appd_experiments}.}
  \label{fig:scaling_exp}
\end{figure}

\paragraph{Scaling Results}
As shown in~\autoref{fig:scaling_exp}, the EntireSet experiments, increasing unique token count through Full-Fineweb-Edu shows marginal improvements,
while MAGA expansion demonstrates consistent gains.
Similarly, the Subset experiments, while both upsampling and MAGA expansion improve upon the baseline,
their scaling properties differ significantly.
The performance advantage of upsampling remains relatively constant across model sizes (+0.89/+1.53/+1.23/+1.41),
whereas MAGA expansion demonstrates superior scaling characteristics, its performance gains amplify with increasing model scale (+1.46/+2.67/+3.59/+3.73).
This widening gap with model size suggests that synthetic data expansion is a more promising approach for scaling language models effectively.


\paragraph{Validation Losses}
Although MAGACorpus demonstrates superior benchmark performance, we observe increasing validation losses compared to baseline models.
While higher validation losses might seem concerning at first glance, it's important to note that validation loss may not fully reflect model performance,
as token-level perplexity is inherently biased by the frequency distribution of the validation set, and in-domain validation metrics may not necessarily correlate with out-of-domain generalization capabilities.
This observation, combined with recent studies linking loss degradation to model collapse phenomena~\citep{dohmatob2024talecollapse,dohmatob2024strongcollapse,zhu2024synthesize}, calls for a more nuanced analysis, which we provide in Section~\ref{sec:disscuss_model_collapse}.




