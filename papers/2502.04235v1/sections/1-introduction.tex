\section{Introduction}
\label{sec:intoduction}
The success of Large Language Models(LLMs) can be largely attributed to the scale of model parameters and training data~\citep{kaplan2020scaling,hoffmann2022empirical}. 
However, recent studies indicate that we are approaching a critical point where high-quality natural language data is becoming increasingly scarce~\citep{villalobos2022will}.
This ``data exhaustion'' phenomenon poses a fundamental challenge to the continued scaling of language models, as model size growth begins to outpace the availability of diverse, high-quality training data.

Recent efforts to enhance LLMs under data-constrained scenarios have explored several promising directions. 
Researchers have made significant progress in optimizing data curation workflows to reduce filtration losses while maintaining quality~\citep{su2024nemotron, penedo2024finewebdatasets}. 
Another straight-forward approach is controlled data repetition, which shows that moderate repetition (up to 4×) yields negligible loss changes compared to unique data~\citep{muennighoff2023scaling}. 
Perhaps most intriguingly, researchers have begun leveraging LLMs themselves to synthesize high-quality training data ~\citep{abdin2024phi, su2024nemotron}, creating a bootstrapping effect for model improvement.

Among these approaches, data synthesis presents a particularly promising direction, as it offers the potential to generate unlimited training data while maintaining control over quality and distribution.
However, existing methods heavily rely on large-scale models for synthetic data generation. 
Nemotron-CC~\citep{su2024nemotron} employs a 12B dense model, 
while Phi series~\citep{abdin2024phi} depends on GPT-4-level capabilities.
Additionally, Phi introduces a sophisticated seed curation system to control data diversity,
an approach later adopted by Cosmopedia~\citep{benallal2024smollmcorpus}. 
The reliance on large-scale models and careful curation creates both computational bottlenecks 
and scalability challenges for pretraining data synthesis. 

\begin{figure*}[t]
    \vspace{-4em}
    \centering
    \includegraphics[width=1\columnwidth]{figures/method-overview.pdf}
    \vspace{-2em}
    \caption{Overview of {\ours} framework. Our method expands the original corpus through a two-stage synthesis process. Each document is reformulated to 5 new documents, achieving 3.9× token number expansion while maintaining diversity through massive (genre, audience) pairs.}
    \label{fig:method-overview}
\end{figure*}

In this work, we propose a more efficient approach {\ours}. As illustrated in~\autoref{fig:method-overview}, our approach leverages a 3.3B MoE model that adaptively expands datasets using only raw documents as input, keeping it lightweight and scalable.
Our main contributions are:
\begin{itemize}[leftmargin=1.5em]
\item We build a 770 billion tokens MAGACorpus based on existing high-quality text collections, demonstrating superior performance across various sizes (134M/377M/1.7B/7B/13B parameters) compared to original corpus.
\item We perform representative evaluation of data budget scaling strategies, revealing that MAGACorpus yields consistent improvements compared to data repetition and upsamling, demonstrating the necessity of synthetic data in next-generation language model training.
\item We analyze synthetic data collapse from two key perspectives, characterizing prompt engineering's mitigating effects while revealing limitations of validation loss as a collapse detection metric, providing insights for future synthetic data optimization.
\end{itemize}
