\section{Conclusion}

In this work, we present MAGA, a novel approach for expanding pre-training corpus through model-assisted generation. 
Through extensive experiments across different model scales (134M to 13B parameters),
we demonstrate that MAGA effectively addresses data scarcity challenges in typical scenario.
Our empirical results show consistent improvements across various benchmarks, highlighting the robustness and scalability of our approach.

The effectiveness of MAGA highlights the potential of synthetic data in modern language model training. 
While current experiments focus on moderate-scale scenarios, our findings suggest promising directions for addressing larger-scale training challenges. 
The framework's flexibility in balancing data quality and quantity offers new possibilities for resource-efficient training strategies. 
As the field continues to explore solutions for data-constrained scenarios, MAGA provides a practical approach that complements existing data curation methods.

\subsubsection*{Acknowledgements}
We are grateful to Chao He, Zhixin Yao, Yue Chen and Runyu Shi for their help with prompt templates and case studies,
and to Seed-Foundation team for providing the stable training/inference platform, 
which enabled us to build the synthetic pipeline and corpus within a reasonable timeframe.
The icons shown in Figure 1 are designed by Freepik.
