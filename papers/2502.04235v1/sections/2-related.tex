\section{Related Work}
\paragraph{Data Curation}
While web-crawled data contains hundreds of trillions of tokens, stringent quality filters typically remove the majority of this content.
Popular datasets like C4, Gopher, Dolma, RefinedWeb~\citep{raffel2020exploring,rae2021gopher,penedo2023refinedweb,soldaini2024dolma} use nonlearned heuristics method.
And recently FineWeb-Edu~\citep{penedo2024finewebdatasets}, DCLM~\citep{li2024datacomplm}, FineFineWeb~\citep{zhang2024finefineweb} focus on aggressive model-based and retrieval-based filtering. 
Such heavy filtering results in removal of 90\% of tokens, some researchers turn their attention to balance accuracy and data quantity~\citep{su2024nemotron}.
However, this does not alter the fact that the total amount of high-quality data remains limited.

\paragraph{Repetition Training}
Studies on subset repetition training have revealed that model divergence tends to occur earlier as model parameters increase~\citep{hernandez2022scaling}.
For scenarios training on entire datasets repeated, limiting to 4 epochs or fewer results in minimal efficiency degradation~\citep{muennighoff2023scaling,taylor2022galactica}.
Furthermore, \citep{xue2024repeat} shows that some regularization techniques (e.g., dropout) and leveraging MoE architecture can help efficient LLM development on a broader scale.
Overall, this topic remains understudied across different model architectures, data distributions, and repetition ratios.

\paragraph{Synthetic Pretrain}
Current synthetic data generation methods for language model pretraining can be primarily categorized into two approaches: seed based synthesis and raw text based rephrasing. 
The seed based method, exemplified by Phi-4~\citep{abdin2024phi} and Cosmopedia~\citep{benallal2024smollmcorpus}, employs predefined seed systems and task templates to precisely control the type and structure of generated content. 
The rephrasing method, represented by WRAP~\citep{maini2024rephrasing} and Nemotron-CC~\citep{su2024nemotron}, generates data by rephrasing web content into QA pairs and wiki-style texts, 
demonstrating significant effectiveness in processing noisy web text, though its benefits may be limited when applied to high-quality source data~\citep{pieler2024rephrasing}.
Additionally,~\citet{ge2024scalingpersonas} introduce an innovative text generation method based on billion personas, offering new insights into enhancing the diversity of synthetic data.

While existing approaches have made significant progress, they face key limitations:
seed-based methods require complex initialization systems, limiting investigation of their scaling properties,
while rephrasing-based approaches struggle to effectively augment high-quality corpus at scale.
To address these limitations, our framework {\ours} bridges this gap by leveraging and extending the inherent diversity of existing corpus.
Specifically, it adaptively generates multiple \textbf{Genre} and \textbf{Audience} seeds for each document of SmolLM-Corpus~\citep{benallal2024smollmcorpus}, enabling 3.9x token expansion while maintaining diversity and quality.