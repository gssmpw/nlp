\section{Introduction}
\label{sec:intro}

Materials synthesis underpins advances in energy storage, catalysis, electronics, and biomedical devices~\cite{olivetti2020data}. Despite its importance, synthesis processes remain largely empirical, relying on trial-and-error approaches guided by expert intuition~\cite{merchant2023scaling}. This inefficiency highlights the need for systematic, data-driven approaches to predict synthesis workflows and optimize experimental design~\cite{huang2023application}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{image/intro/intro_v03.png}
    \caption{An overview of our contributions, featuring the Open Materials Guide Dataset for large-scale synthesis recipes and AlchemyBench for scalable, expert-level evaluation.}
    \label{fig:intro}
\end{figure}

\begin{figure*}[ht!]
    \begin{subfigure}{.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{image/data/periodic-ours-small.png}
    \caption{The periodic table of logarithmic frequency of elements.}
    \label{fig:data-element-distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{image/data/top10process_twotone_margin.png}
    \caption{The distribution of synthesis techniques.}
    \label{fig:periodic-inorganic}
    \end{subfigure}

    \caption{The periodic table (left) demonstrates that \oursdatashort~covers diverse elements used in target materials, with darker colors indicating higher usage frequencies. A pie chart (right) illustrates the diversity of synthesis methods, highlighting the contributions of prior studies (white) and our dataset (white + green).}
    \label{fig:periodic-all}
\end{figure*}

Recent progress in machine learning and large language models has opened new avenues for extracting and generating synthesis procedures from unstructured scientific literature~\cite{song2023matsci,dunn2020benchmarking}. However, practical adoption is hampered by several challenges. Existing datasets are often small, domain-specific, and noisy, limiting model generalizability. Moreover, the absence of comprehensive benchmarks makes it difficult to assess the performance of synthesis prediction methods, while expert evaluations remain too costly and time-consuming for large-scale use.

% To address these challenges, we introduce \oursdatalong~(\oursdatashort), the largest expert-verified dataset for materials synthesis, featuring 17K high-quality recipes extracted from scientific literature. We also develop \oursbench, the first end-to-end benchmark for evaluating ML models on synthesis prediction, covering.


% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{image/intro/process_v02.png}
%     \caption{Illustration of \oursdatalong~construction workflow.}
%     \label{fig:intro_data_collection}
% \end{figure}



To address these challenges, we introduce \oursdatalong~(\oursdatashort), a dataset comprising 17K high-quality, expert-verified synthesis recipes curated from open-access literature. This dataset is the foundation for our benchmark, \oursbench, which evaluates synthesis prediction across multiple facets—from inferring raw materials and recommending appropriate synthesis equipment to generating detailed procedural steps and forecasting suitable characterization techniques.

Additionally, we investigate an LLM-as-a-Judge framework to automate the evaluation process. Our systematic comparisons reveal a strong statistical agreement between LLM-based assessments and expert judgments, underscoring the potential of LLMs to serve as scalable, automated evaluators.

Our work makes the following key contributions:
\begin{itemize}
    \item \oursdatalong~(\oursdatashort), the most significant materials synthesis dataset, comprises 17K high-quality recipes from open-access literature. We demonstrated that various models can improve their performance with the proposed data-driven Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval} experiments. From the improvement, we validate the applicability of our data.

    \item \oursbench, the first end-to-end benchmark for ML-driven synthesis prediction utilizing LLM-as-a-Judge, a scalable framework for evaluating synthesis predictions, demonstrating strong alignment with expert assessments. This framework enables automated benchmarking of synthesis prediction models, significantly reducing the reliance on costly and time-intensive expert evaluations while maintaining high evaluation reliability.
    
    \item \textbf{Extensive experimental insights} into model performance, identifying key challenges, potential capabilities, and future directions to utilize LLM for fully-automated materials synthesis.
    
\end{itemize}
To enhance reproducibility and accessibility, we release the dataset and code as an open-source resource for the research community\footnote{\url{https://github.com/HeegyuKim/AlchemyBench}}.





    % \item {\textbf{Open Source}: We will publish all framework data and code, enhancing reproducibility and providing a robust foundation for research groups to develop bespoke synthesis prediction agents, accelerating innovation in materials science.} \hist{이건 따로 정의하기 애매한 contribution 인 듯. 데이터랑 benchmark 가 contribution 인 순간부터, closed 일 수 없다고 해야하나?}




%\section{Introduction}
% \label{sec:intro}

% Materials synthesis is a cornerstone of scientific progress, driving innovations across various domains such as energy storage, catalysis, electronics, and biomedical devices~\cite{olivetti2020data}. Despite notable advancements, the synthesis process remains empirical, relying on trial-and-error approaches guided by expert intuition~\cite{merchant2023scaling}. This reliance on heuristic methods often impedes novel materials' rapid discovery and optimization. Consequently, a systematic understanding and predictive capability in materials synthesis are critical for accelerating research and development~\cite{huang2023application}.

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=\linewidth]{image/intro/intro_v01_simple.png}
%     \caption{An illustrated example of materials synthesis pipeline in \oursbench.}
%     \label{fig:intro}
% \end{figure}

% % In recent years, progress in natural language processing (NLP) has demonstrated promising avenues for extracting actionable insights from unstructured scientific literature. By leveraging large-scale text data, NLP-based methods can help automate the identification of synthesis parameters, uncover hidden correlations, and generate testable hypotheses for materials discovery~\cite{song2023matsci,dunn2020benchmarking}. However, the application of NLP in materials science remains limited by data quality and domain diversity.
% % Existing datasets and benchmarks are 1) often small in scale. 2) are prone to noise 3) are narrowly focused target materials~\cite{sun2025critical}. These limitations undermine the generalizability of machine learning models developed for synthesis prediction.

% In recent years, Machine Learning (ML) and Large Language Models (LLMs) have emerged as promising tools for leveraging the vast amount of unstructured scientific literature available in materials science. These methods can automate the extraction of synthesis parameters, uncover hidden correlations, and generate testable hypotheses~\cite{song2023matsci,dunn2020benchmarking}. However, several challenges hinder the practical application in this domain: \textbf{1) Data Limitations:} Existing datasets are prone to narrowly focused on specific synthesis procedures~(\eg, solid-state, sol-gel, and solution-based) and prone to noisy~\cite{sun2025critical}, limiting the generalizability of ML models.
% \textbf{2) Lack of Standardized Benchmarks:} The absence of comprehensive benchmarks for evaluating end-to-end synthesis prediction models impedes progress and prevents meaningful comparisons across approaches.
% \textbf{3) Unclear Model Capabilities:} The current level of performance achievable by ML or LLM models in predicting synthesis workflows remains underexplored.
% \textbf{4) Evaluation Challenges:} Determining whether LLMs can reliably approximate human-level evaluations for synthesis predictions is an open question.

% To address these challenges, we propose \oursdatalong~(\oursdatashort), a dataset of high-quality 17K material synthesis recipes curated from a large corpus of scientific literature using automated methods. 
% Based on this dataset, we designed \oursbench~to benchmark models on tasks that mirror real-world materials synthesis workflows:

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{image/intro/process_v02.png}
%     \caption{An illustrated workflow of \oursdatalong~construction.}
%     \label{fig:intro_data_collection}
% \end{figure}

% \begin{enumerate}
%     \item \textbf{Predicting raw materials:} Given a target material, models are challenged to generate precursor, solvent, and catalyst identities and predict their respective quantities or ratios.
%     \item \textbf{Predicting synthesis equipment:} Models must determine the apparatus for a given synthesis based on the target material and precursor information.
%     \item \textbf{Predicting synthesis procedures:} Given the target material, raw materials with proportions, and equipment, the model should generate the procedural steps.
%     \item \textbf{Predicting characterization methods and outcomes:} Models must infer the appropriate characterization techniques and their possible outcomes based on complete synthesis details.
% \end{enumerate}




% Figures~\ref{fig:intro} and~\ref{fig:intro_data_collection} illustrate the overall workflow of our proposed dataset construction and benchmark tasks. By capturing the end-to-end pipeline of materials synthesis, our benchmark provides a holistic framework for evaluating NLP models that can generalize across diverse synthesis methods and materials.

% Moreover, we explore the potential of LLMs to act as evaluators for synthesis predictions. While human experts provide the gold standard for evaluation, their involvement is time-intensive and costly. We investigate whether LLMs can reliably approximate human-level judgment across multiple evaluation dimensions. This dual focus—on both model performance and evaluation reliability—aims to advance the integration of data-driven approaches into practical materials science.

% In summary, this work makes the following contributions:
% \begin{itemize}
%     \item We present \oursdatalong~(\oursdatashort), a large-scale dataset of 17K material synthesis recipes curated from the scientific literature~(including the latest literature until 2024).
%     \item We introduce \oursbench, a comprehensive benchmark that evaluates models on end-to-end tasks spanning precursor prediction to characterization outcomes.
%     \item We assess the capabilities of state-of-the-art LLMs in predicting synthesis workflows and examine their reliability as evaluators compared to human experts.
%     \item We provide experimental results highlighting key insights into model performance and propose future directions for improving data-driven approaches in materials science.
% \end{itemize}



% By capturing the end-to-end pipeline of materials synthesis, this benchmark lays the groundwork for evaluating and developing NLP models that can effectively generalize across diverse synthesis methods and materials. In particular, our emphasis on real-world workflows provides a holistic perspective that moves beyond isolated prediction tasks to encompass all stages of the synthesis process.

% Ultimately, we aim to facilitate the convergence of data-driven approaches and practical materials science. We strive to spur methodological advancements in NLP and deeper collaborations between the materials science and machine learning communities by offering a rigorous evaluation framework. Through this benchmark, we envision a future where machine-learning models expedite the discovery of novel materials and provide actionable guidance for their synthesis and characterization. 