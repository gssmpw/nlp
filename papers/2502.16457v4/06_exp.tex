\section{Experiments}
\label{sec:experiment}

% This section evaluates the performance of five LLMs on the proposed benchmark using our LLM-as-a-Judge framework based on GPT-4o Aug. We analyze their performance across multiple metrics, including mean and maximum scores, score distributions, and the impact of retrieval-augmented generation (RAG).

This section evaluates five LLMs on our benchmark using the LLM-as-a-Judge framework based on GPT-4o-Aug, analyzing performance across multiple metrics and the impact of retrieval-augmented generation (RAG).

\subsection{Experiment Setup}
\label{subsec:experiment_setup}

To comprehensively evaluate the models, we conducted experiments with the following setup:

\paragraph*{Base LLMs}
We evaluated four LLMs, including reasoning-based models (o3-mini) and general-purpose models (GPT-4o variants). The knowledge cutoff of these models is October 2023, minimizing potential data contamination in our test sets, which contain 2024 and beyond\footnote{The knowledge cutoff of OpenAI's models is described in \href{https://platform.openai.com/docs/models}{this documentation}.}. We prompt the LLM with a fixed one-shot example from our train set to predict all components ($\mathbf{P_X}$) well\footnote{Details about LLM prompt and hyperparameters are described in Appendix~\ref{sec:appendix_experiments}.}. Moreover, we varied the reasoning\_effort of o3-mini to ensure the effectiveness of reasoning effort in materials synthesis.



\paragraph*{Evaluation Framework}
Each model generated synthesis recipes for both the \testhi~and \testsi. Recipes were evaluated using our LLM-as-a-Judge method based on GPT-4o-Aug. The evaluation criteria focused on material appropriateness, procedural feasibility, and overall recipe quality.
% \paragraph*{Retrieval-Augmented Generation (RAG)}
% To assess the impact of similar recipes in prediction, we implemented a retrieval-augmented generation~\cite{lewis2020retrieval}~(RAG) pipeline using OpenAI Embeddings~\cite{OpenAIEmbeedings2022}, which model is text-embedding-3-large with 3,072 dimensions. For each input \(X\), we retrieved the top-\(K\) most similar contributions from the training set based on cosine similarity in embedding space and their recipes. These retrieved recipes were included as references in the prompts provided to the LLMs. We evaluated \(K = \{0, 1, 5, 10, 25\}\) to study how increasing contextual information affects recipe generation quality.

\paragraph*{Retrieval-Augmented Generation (RAG)}
To evaluate the impact of retrieval on recipe generation, we implemented a RAG pipeline using OpenAI's text-embedding-3-large model~\cite{OpenAIEmbeedings2022}. For each input \(X\), we retrieved the top-\(K\) most similar recipes from the train set based on cosine similarity and included them as references in LLM prompts. We evaluated \(K = \{0, 1, 5, 10, 25\}\) to assess the effect of contextual information.


This experimental setup ensures a thorough evaluation of both baseline performance and improvements achieved through retrieval augmentation. Due to computational constraints, RAG experiments were conducted on three representative models (GPT-4o-Nov, GPT-4o-mini, and o3-mini) using only the \testhi.

\subsection{Insights from Experimental Results}
\label{subsec:insights}

\input{table/exp/base_results}

The experimental results provide valuable insights into the challenges and opportunities in materials synthesis prediction, structured around the following research questions:


\paragraph*{RQ1: Is High-Impact Set More Challenging than Standard-Impact Set?}
The results confirm that \testhi~is indeed more challenging than \testsi. Across models without GPT-4o-Nov, average scores on the \testhi~were generally lower than those on the \testsi. For example, o3-mini (high) achieved an average score of \(3.759 \pm 0.407\) on \testhi~compared to \(3.885 \pm 0.377\) on \testsi~(Table~5). This discrepancy highlights the increased complexity of High-Impact synthesis workflows, which often involve novel materials or cutting-edge techniques requiring greater reasoning and contextual understanding.

% \paragraph*{RQ2: Are Reasoning-Based Models Effective in Materials Science?}
% The reasoning-based models, particularly o3-mini (high), demonstrated superior performance compared to general-purpose models like GPT-4o-Nov. On both High-Impact and Standard-Impact datasets, o3-mini (high) consistently achieved higher mean scores and broader score distributions, indicating its ability to generate high-quality recipes in complex scenarios. For instance, o3-mini (high) achieved a maximum score of \(4.71\) on High-Impact, matching GPT-4o-Nov but with a higher average score (\(3.759\) vs. \(3.709\)).

% These results align with trends observed in domains such as mathematics and programming, where reasoning-based models excel due to their structured problem-solving capabilities. However, the performance gap between o3-mini (high) and other reasoning-based variants (e.g., o3-mini (medium)) suggests that high reasoning capabilities are essential for tackling the nuanced challenges of materials science.

% Materials science relies heavily on trial-and-error experimentation, making reasoning-based approaches particularly relevant for complex synthesis tasks. To explore whether increased reasoning effort improves the quality of generated synthesis recipes, we evaluated the o3-mini model under medium and high reasoning efforts. As shown in Table~\ref{tab:base_results}, o3-mini (high) achieved the highest mean scores across both \textit{High-Impact} (\(3.759 \)) and \textit{Standard-Impact} (\(3.885\)) sets, outperforming both its medium-intensity counterpart and general-purpose models like GPT-4o-Nov. Notably, o3-mini (high) also achieved the maximum score of \(4.80\) on the \testsi, demonstrating its ability to generate high-quality recipes even in less challenging scenarios.

% These findings suggest that increased reasoning effort enhances a model's ability to capture procedural nuances and dependencies critical for materials synthesis. For instance, o3-mini (high) exhibited superior coherence and feasibility in step-by-step instructions compared to GPT-4o-Nov, which achieved a lower mean score (\(3.709\)) on the \testhi~despite matching o3-mini (high)'s maximum score (\(4.71\)). This trend highlights the importance of structured problem-solving capabilities in addressing the nuanced challenges of materials science while emphasizing the need for domain-specific optimization to leverage reasoning-based models fully.


\paragraph*{RQ2: Does Increasing Reasoning Effort Improve Recipe Quality in Materials Science?}  
Materials science relies heavily on trial-and-error experimentation, making reasoning-based approaches particularly relevant for complex synthesis tasks. To explore this, we evaluated the o3-mini model using low, medium, and high reasoning efforts. As shown in Table~\ref{tab:base_results}, o3-mini (high) achieved the highest mean scores across both \textit{High-Impact} (\(3.759\)) and \textit{Standard-Impact} (\(3.885\)) sets, outperforming both its low and medium reasoning efforts and general-purpose models like GPT-4o-Nov. o3-mini (high) exhibited superior performane in step-by-step instructions compared to GPT-4o-Nov, which achieved a lower mean score (\(3.709\)) on the \testhi~despite matching o3-mini (high)'s maximum score (\(4.71\)). These findings demonstrate the importance of structured problem-solving capabilities in materials synthesis tasks.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{image/exp/rag_high_impact.png}
    \caption{Impact of the Retrieval Augmented Generation (RAG) in \testhi.}
    \label{fig:rag_high_impact}
\end{figure}
\paragraph*{RQ3: Does Recipes of Similar Contributions Improve Prediction Performance?}
RAG with similar recipes significantly enhances model performance through domain-relevant examples. As shown in Figure~\ref{fig:rag_high_impact}, increasing \(K\) improves scores across all models, with different patterns between reasoning-based and general-purpose models.
For o3-mini (high), performance diminish after \(K=5\), reaching a maximum score of \(4.0\). In contrast, GPT-4o-Nov shows continuous improvement up to \(K=25\), achieving \(3.98\). The close performance between GPT-4o-Nov and o3-mini (high) suggests RAG can effectively bridge the gap between general-purpose and reasoning-based models.
These results highlight two key points: (1) RAG benefits general-purpose models that rely on external context, and (2) retrieval effectiveness depends on model architecture and training data quality. Our findings suggest that \(K=5\) provides the best trade-off between context enrichment and cognitive load, as additional references beyond this point yield minimal improvements.


% The experimental results confirm that RAG with similar recipes from our train set significantly enhances model performance by grounding predictions in domain-relevant examples retrieved from the training set. As shown in Figure~\ref{fig:rag_high_impact}, increasing \(K\) (the number of retrieved references) improves overall scores for all tested models, with notable differences in how reasoning-based and general-purpose models benefit from retrieval.

% For o3-mini (high), performance diminish beyond \(K=5\), achieving a maximum score of \(4.0\) at \(K=5\). This suggests that while retrieval provides valuable context, these models already possess strong reasoning capabilities to generate high-quality recipes independently.

% In contrast, \textbf{GPT-4o-Nov}, a general-purpose model, exhibits consistent improvement even beyond \(K=5\), ultimately achieving a competitive overall score of \(3.98\) at \(K=25\). This result highlights that retrieval mechanisms can partially compensate for the lack of inherent reasoning capabilities in general-purpose models by providing explicit contextual information. The marginal difference between GPT-4o-Nov (\(3.98\)) and o3-mini (high) (\(4.0\)) suggests that retrieval can narrow the performance gap between reasoning-based and general-purpose models in materials science tasks.

% These findings underscore two key insights:
% 1) Retrieval augmentation is a foundational component for improving recipe generation quality, particularly for general-purpose models that rely heavily on external context.
% 2) The retrieval effectiveness depends on the model architecture and the quality of the training set. The observed gains validate the utility of our curated dataset as a reliable source of domain-specific knowledge.

% However, diminishing returns beyond \(K=5\) for most models suggest an optimal range for reference incorporation, balancing contextual relevance with cognitive load on the model.