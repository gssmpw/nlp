\section{Related Work}

\paragraph*{Materials Synthesis Datasets}

Existing materials synthesis datasets, such as those focusing on solid-state~\cite{kononova2019text} and solution-based~\cite{wang2022dataset} methods, have provided valuable resources for machine learning applications. However, these datasets often suffer from issues of incompleteness and low quality, with many synthesis procedures lacking critical parameters necessary for reproducibility or predictive modeling. For instance, only 28\% of solid-state synthesis paragraphs yield complete reactions, and over 90\% of recipes are missing key parameters. These limitations hinder their utility in guiding novel synthesis workflows.

\paragraph*{LLM-based Generation for Materials Science}

Large language models (LLMs) have shown promise in accelerating materials discovery by automating hypothesis generation~\cite{kumbhar2025hypothesisgenerationmaterialsdiscovery}, property prediction~\cite{chiang2024llamplargelanguagemodel}, and evaluation~\cite{mishra2024llamat}. However, their effectiveness is constrained by the lack of high-quality domain-specific datasets and the need for retrieving or fine-tuning to handle complex synthesis workflows. Our work addresses these gaps and introduces a large-scale dataset and benchmark tailored to the real-world synthesis workflow, enabling rigorous evaluation of LLM capabilities in materials science.


% \section{Related Work}

% \subsection{Materials Synthesis Datasets}

% Several datasets have been developed to support machine learning applications in materials synthesis. Text-mined datasets focus on extracting synthesis procedures from scientific literature using natural language processing (NLP) techniques. For instance, a dataset of 19,488 solid-state synthesis recipes~\cite{kononova2019text} was generated from over 53,000 paragraphs, including details like precursors, operations, and conditions. Similarly, solution-based synthesis datasets~\cite{wang2022dataset} include 35,675 procedures with detailed attributes like precursor quantities and reaction formulas. However, these datasets face significant limitations in quality and completeness. A critical review~\cite{sun2025critical} highlighted issues with the ``4 Vs.'' of data science—volume, variety, veracity, and velocity—arguing that many recipes lack essential reproducibility or predictive modeling information. For example, only 28\% of extracted solid-state synthesis paragraphs yielded complete reactions, and our validation revealed that more than 90\% of these synthesis procedures have missing parameters. These challenges limit the utility of such datasets for guiding novel materials synthesis.


% \subsection{LLM-based Generation for Materials Science}
% Recent advancements in large language models (LLMs) have showcased their potential to accelerate materials discovery, simulation, and evaluation by automating hypothesis generation~\cite{kumbhar2025hypothesisgenerationmaterialsdiscovery}, predicting material properties~\cite{chiang2024llamplargelanguagemodel}, and even serving as evaluators~\cite{mishra2024llamat}. These systems integrate domain-specific knowledge with computational workflows, offering promising tools for designing novel materials or extracting structured insights from the literature. However, their effectiveness remains constrained by challenges such as limited domain-specific datasets, the need for fine-tuning to handle complex synthesis workflows, and insufficient interpretability in critical decision-making processes. These limitations underscore the need for standardized benchmarks and high-quality datasets to rigorously evaluate LLM capabilities in materials science. Our work addresses these gaps by introducing a comprehensive dataset and benchmark tailored to real-world synthesis workflows while also exploring the reliability of LLMs as evaluators against human experts.


% \subsection{LLM-Based Evaluation in Materials Science}
% LLMs have shown significant potential in materials synthesis research, outperforming traditional machine learning models in tasks such as precursor selection and synthesizability prediction~\cite{lei2024materials,kim2024large}. Tools like LLMatDesign~\cite{jia2024llmatdesign} autonomously generate and evaluate hypotheses for material design, while benchmarks like ALDbench~\cite{yanguas2024benchmarking} assess LLMs' ability to address domain-specific questions. However, their application in evaluating synthesis quality remains underexplored, with limited systematic comparisons between LLM-based evaluations and human expert judgments in materials science. This highlights the need for further research to establish LLMs as reliable evaluators aligned with expert assessments.

% Large Language Models (LLMs) have recently gained attention for their potential to evaluate and predict materials synthesis pathways. Studies demonstrate that fine-tuned LLMs can perform comparable or superior to traditional machine learning models in tasks like precursor selection and synthesizability prediction~\cite{lei2024materials,kim2024large}. For example, LLMatDesign~\cite{jia2024llmatdesign} uses LLMs to design materials by iteratively generating hypotheses and evaluating outcomes autonomously. Benchmarks like ALDbench~\cite{yanguas2024benchmarking} assess LLMs' ability to answer domain-specific questions about atomic layer deposition processes. 

% Despite these advancements, using LLMs for evaluating materials synthesis quality remains underexplored. \citeauthor{luo2024large} have investigated human-LLM agreement in other scientific domains, showing promising correlations between LLM predictions and expert evaluations. However, in materials science specifically, systematic experiments comparing LLM-based evaluations with human experts are limited. This gap underscores the need for further research into leveraging LLMs as reliable evaluators in this domain while ensuring alignment with expert judgment.
