\section{AlchemyBench}
\label{sec:benchmark}

We present \oursbench, a comprehensive benchmark for evaluating materials synthesis prediction models. This framework addresses key challenges in synthesis recipe evaluation through structured tasks, expert-aligned metrics, and scalable assessment strategies.

\subsection{Motivation}
\label{subsec:motivation}

Evaluating synthesis predictions presents several fundamental challenges:

\begin{itemize}
    \item \textbf{Lack of Benchmarks:} No standardized evaluation framework exists, making it challenging to compare synthesis models systematically. Prior datasets lack critical synthesis parameters and structured ground truth labels, making meaningful comparisons difficult.
    \item \textbf{Limitations of Traditional Metrics:} Traditional metrics, such as BLEU~\cite{papineni2002bleu} and ROUGE~\cite{lin2004rouge} prioritize lexical overlap but fail to capture the procedural correctness of synthesis recipes. \citeauthor{na2023artificial} et al. introduced the Jaccard score to measure set overlap in synthesis procedures, yet it lacks sensitivity to sequential dependencies critical in procedural texts. BERTScore~\cite{zhang2019bertscore} improves contextual similarity measurement but struggles with domain-specific dependencies unique to materials synthesis. Moreover, these metrics do not account for experimental feasibility, limiting their applicability in real-world synthesis.
    \item \textbf{High Cost of Human Evaluation:} Expert-based assessments require significant time and resources, averaging 23 minutes per prediction ($\sigma=7.57$) in our experiment. This cost makes large-scale benchmarking impractical, requiring an automated evaluation system.
    \item \textbf{Scalability Requirements:} Large-scale benchmarking necessitates an automated yet reliable evaluation system, which LLMs can provide~\cite{gu2025surveyllmasajudge}. However, prior attempts to use LLMs for evaluation lacked systematic validation against human expert assessments in materials science, raising concerns about reliability.
\end{itemize}

\subsection{Task Definition}
\label{subsec:task_definition}

\oursbench~simulates real-world synthesis workflows, where models must predict the following components given input $\mathbf{X}$ (target material, synthesis method, application domain):

\begin{itemize}
    \item $\mathbf{P_M}$: Raw materials (e.g., reagents, solvents) with quantities.
    \item $\mathbf{P_E}$: Required equipment (e.g., furnace, autoclave).
    \item $\mathbf{P_P}$: Synthesis procedures (e.g., reaction steps, temperatures).
    \item $\mathbf{P_C}$: Characterization methods and expected outcomes.
\end{itemize}

\input{table/benchmark/criteria}
Predictions $\mathbf{P_X} = \{\mathbf{P_M,P_E,P_P,P_C}\}$ are evaluated against ground truth $\mathbf{Y_X} = \{\mathbf{Y_M,Y_E,Y_P,Y_C}\}$ using the LLM-as-a-Judge framework. Unlike prior benchmarks that rely on lexical similarity, \oursbench\ assesses procedural correctness and experimental feasibility. The evaluation criteria are described in Table~\ref{tab:judgment_criteria}.

The scoring function is computed as:

\[
\text{Score}(P_X, Y_X) = \frac{\sum_{i=1}^{N_C} C_i}{N_C}
\]

where $C_i$ represents the score for criterion $i$, and $N_C$ is the total number of evaluation criteria. These criteria were developed in collaboration with domain experts to ensure alignment with real-world synthesis evaluation.

\subsection{Dataset Splits and Distribution}
\label{subsec:dataset_splits}

We divided \oursdatashort~to three splits to ensure robust evaluation:

\begin{itemize}
    \item \textbf{Training Set:} 16,026 articles published before 2024.
    \item \textbf{Test - Standard Impact:} 1,472 articles (2024 and beyond) from journals with Impact Factor (IF) $<$ 10.
    \item \textbf{Test - High Impact:} 169 articles (2024 and beyond) from journals with IF $\geq$ 10.
\end{itemize}

The \textbf{temporal split} ensures that models are evaluated on \textit{unseen future research}, mitigating data contamination. Additionally, stratification by \textbf{journal impact} allows assessment of a model’s ability to process high-impact findings, often introducing novel and complex synthesis techniques. This split design evaluates both \textit{generalizability} and the ability to meet the rigorous standards of top-tier journals\footnote{Table~\ref{tab:high_impact_journals} describes the detailed list of high-impact journals utilized for our test-set split.}.



% \section{AlchemyBench}
% \label{sec:benchmark}

% This section introduces our benchmark for evaluating models on materials synthesis prediction. We outline the motivation for using LLMs as judges, task definitions, dataset splits, and evaluation metrics, culminating in a framework reflecting real-world materials science challenges.

% \subsection{Motivation: Why LLM-as-a-Judge?}
% \label{subsec:motivation}
% The evaluation of general materials synthesis recipes poses unique challenges that necessitate scalable and efficient solutions:
% \begin{itemize}
%     \item \textbf{Lack of Existing Benchmarks:} There is no established benchmark for evaluating general materials synthesis recipes nor a standardized framework to assess their quality systematically.
%     \item \textbf{Limitations of Traditional Metrics:} Commonly used metrics such as BLEU~\cite{papineni2002bleu} and ROUGE~\cite{lin2004rouge} focus on surface-level lexical similarity but fail to capture the semantic requirements of synthesis recipes. \citeauthor{na2023artificial} et al. utilized the Jaccard score to evaluate the set overlap of synthesis procedures but lacks sensitivity to sequential dependencies critical in procedural texts. Additionally, BERTScore~\cite{zhang2019bertscore} leverages contextual embeddings from pretrained language models to measure semantic similarity beyond token matching, offering a more context-aware evaluation. However, despite its advantages, BERTScore often underperforms in the Materials Synthesis Recipe domain, where the intricate sequential dependencies and specialized domain semantics are not fully captured.
%     \item \textbf{Cost and Inefficiency of Human Evaluation:} Employing domain experts for recipe evaluation is prohibitively expensive and time-consuming, with an average evaluation time of 23 minutes per recipe ($\sigma=7.57$) in our experiment.
%     \item \textbf{Need for Scalable Solutions:} A scalable and high-performance evaluation tool is essential to support high-throughput benchmarking. LLMs offer a promising alternative by providing consistent and interpretable evaluations at scale~\cite{gu2025surveyllmasajudge}.
% \end{itemize}

% By addressing these challenges, LLM-as-a-Judge enables efficient benchmarking while maintaining alignment with expert evaluations.


% \subsection{Task Definition}
% \label{subsec:task_definition}
% The proposed benchmark simulates end-to-end workflows in materials synthesis. Given an input $\mathbf{X}$ — comprising the target material, synthesis method, and application domain — the model must predict the following components:
% \begin{itemize}
%     \item $\mathbf{P_M}$: Raw materials (e.g., reagents, solvents) and their quantities.
%     \item $\mathbf{P_E}$: Required equipment (e.g., furnace, autoclave).
%     \item $\mathbf{P_P}$: Synthesis procedures (e.g., reaction steps, times, temperatures).
%     \item $\mathbf{P_C}$: Characterization methods and expected outcomes.
% \end{itemize}

% The predictions $\mathbf{P_X}=\{\mathbf{P_M,P_E,P_P,P_C}\}$ are compared against ground-truth recipes $\mathbf{Y_X}=\{\mathbf{Y_M}, \mathbf{Y_E}, \mathbf{Y_P}, \mathbf{Y_C}\}$ using the LLM-as-a-Judge framework. The evaluation criteria include appropriateness, completeness, feasibility, and similarity to ground truth. This task setup was designed in collaboration with domain experts and mirrors real-world scenarios where researchers infer complete workflows from minimal initial information.

% The LLM-as-a-Judge's overall score $\text{Score}(P_X,Y_X)$ is calculated as follows:
% \[
% \text{Score}(P_X, P_Y) = \frac{\sum_{i=1}^{N_C} C_i}{N_C}
% \]
% where $C_i$ represents the score for criterion $i$, and $N_C$ is the total number of criteria. We will introduce the detailed prompt and criteria in Section~\ref{sec:reliability}.

% \subsection{Dataset Splits and Distribution}
% Our dataset comprises 17,667 articles curated from diverse materials science literature. To ensure robust evaluation, we divide the dataset into three splits based on publication year and journal Impact Factor (IF):
% \begin{itemize}
%     \item \textbf{Training Set:} 16,026 articles published before 2024.
%     \item \textbf{Test - Standard Impact:} 1,472 articles published in 2024 or later with IF $<$ 10.
%     \item \textbf{Test - High Impact:} 169 articles published in 2024 or later with IF $\geq$ 10.
% \end{itemize}

% The temporal split ensures test data represents unseen future research trends, mitigating data contamination. Further stratification by IF allows targeted assessment of model performance on high-impact research, often involving novel and complex synthesis techniques. This split design evaluates both generalizability and the ability to meet the rigorous standards of top-tier journals.


