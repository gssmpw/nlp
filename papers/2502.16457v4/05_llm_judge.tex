% \section{LLM as a Judge}
% \label{sec:reliability}

% Ensuring the reliability of the LLM-as-a-Judge framework is critical for establishing trustworthy benchmarking. This section evaluates the alignment between LLM-based judgments and human expert evaluations, providing a detailed analysis of inter-rater agreement and the statistical metrics used to validate the framework.

% \subsection{Evaluation Metrics}
% \label{subsec:evaluation_metrics}
% \input{table/benchmark/criteria}

% Evaluating synthesis recipes requires systematical judgment across multiple dimensions, including material appropriateness, procedural feasibility, etc. We employ three complementary statistical metrics to ensure rigorous assessment: Intraclass Correlation Coefficient (ICC), Pearson Correlation Coefficient, and Spearman’s Rank Correlation.

% % The \textbf{ICC}~\cite{shrout1979intraclass} is used to measure inter-rater reliability, capturing the consistency of scores across evaluators. The Intra-Class Correlation Coefficient (ICC) measures data consistency within a group and is commonly used for reliability analysis and agreement evaluation. ICC(3,k) assesses absolute agreement in a repeated-measures setting where the same raters or measurements are used. It averages \( k \) measurements for a more stable reliability estimate.  
% This metric is particularly relevant for subjective judgment tasks, where expert agreement is a gold standard for evaluation reliability. The \textbf{Pearson Correlation Coefficient} quantifies the linear alignment between human and LLM scores, providing insight into how closely LLM judgments approximate expert evaluations on a continuous scale. 

% \textbf{ICC}~\cite{shrout1979intraclass} measures inter-rater reliability by assessing score consistency within a group, with ICC(3,k) specifically evaluating absolute agreement in repeated measures through averaging \( k \) evaluations. This makes it particularly valuable for subjective judgment tasks relying on expert consensus. In contrast, the \textbf{Pearson Correlation Coefficient} quantifies the linear relationship between human and LLM scores, indicating how closely LLM assessments align with expert evaluations.


% By combining these metrics, we ensure a comprehensive evaluation that captures both consistency and alignment across multiple dimensions. This approach addresses limitations in traditional metrics such as BLEU and ROUGE-L, which focus on surface-level text similarity and fail to account for the contextual appropriateness or procedural coherence required in synthesis recipes.

% \subsection{Human Expert Evaluation Setup}
% \label{subsec:humaneval_setup}

% To establish a reliable ground truth for comparison, we enlisted eight domain experts from three organizations (2/3/3) with extensive experience in materials synthesis. Each expert independently evaluated model-generated recipes based on seven criteria outlined in Table~\ref{tab:judgment_criteria}, scoring each criterion on a 1–5 scale. On average, each evaluation required 23 minutes (\(\sigma=7.57\)) per recipe. To ensure high-quality assessments, we collected expert confidence scores and highlighted the agreement of one organization of three experts with the highest confidence levels on average (denoted as `High').

% The dataset used for evaluation consisted of ten representative papers from \oursdatashort~covering diverse synthesis workflows. A professor in the field of materials science selected these papers based on the expertise of evaluators. Prediction recipes were generated by two models (GPT-4o-mini and o1-mini), resulting in 20 predictions evaluated by human experts and LLM judges.

% \subsection{Inter-Expert Agreement Analysis}
% \label{subsec:inter_rater_analysis}
% \input{table/experts/inter-raters-agreement}

% As shown in Table~\ref{tab:inter-raters}, high ICC values were observed for ``Procedure Feasibility" (0.70) and ``Overall Score" (0.75), indicating substantial agreement among experts on these critical criteria. Lower ICC values for ``Procedure Similarity`` (0.34) reflect inherent subjectivity in assessing how closely predicted procedures match ground truth recipes. However, the overall agreement supports the reliability of expert judgments as a gold standard for evaluation.

% \subsection{LLM-Expert Agreement Analysis}
% \label{subsec:llm_expert_analysis}
% \input{table/experts/llm-experts-agreement}

% To evaluate the alignment between LLM-based judgments and human expert evaluations, we analyzed the performance of four state-of-the-art LLMs using Pearson correlation coefficients. 
% We set the temperature to zero for GPT-4o and GPT-4o-mini\footnote{o1-mini and o3-mini do not allow to set temperature.}.

% Table~\ref{tab:llm-expert} summarizes these results. Notably, GPT-4o Aug achieved a Pearson correlation of 0.80 (\(p=0.00\)), demonstrating statistically a strong positive alignment with expert scores. These results surpass traditional metrics like BLEU (\(-0.16\), \(p=0.50\)) and ROUGE-L (0.06, \(p=0.80\)), which fail to capture expert-level aspects of recipe quality.

% The demonstrated alignment between GPT-4o Aug and the high-confidence group validates its use as a scalable proxy for expert judgment in large-scale benchmarking tasks where manual evaluation is impractical.

% \subsection{Comparison with Traditional Metrics}
% \label{subsec:traditional_metrics}

% Traditional metrics such as BLEU, ROUGE-L, and BERTScore were also evaluated as baselines but failed to align with human evaluations meaningfully (Table~\ref{tab:llm-expert}). These metrics rely on surface-level textual similarity and do not account for contextual appropriateness or procedural coherence—key aspects of synthesis recipe quality. In contrast, LLM-based evaluations explicitly incorporate these dimensions through structured scoring criteria (Table~\ref{tab:judgment_criteria}), making them better suited for this task.

% \subsection{Summary and Implications}
% \label{subsec:summary_implications}

% The results presented in this section demonstrate the reliability of both human expert evaluations and LLM-based judgments within our benchmarking framework. High inter-expert agreement establishes the validity of human evaluations as a gold standard. At the same time, significant alignment between GPT-4o Aug and expert scores supports its integration into our benchmark as a scalable alternative to manual annotation.

% Our framework achieves robust benchmarking without compromising scientific rigor by leveraging advanced statistical metrics and addressing limitations in traditional evaluation methods. Future work could further enhance alignment by incorporating pairwise comparison techniques or fine-tuning LLMs using reinforcement learning from human feedback.


\section{LLM as a Judge}
\label{sec:reliability}

A reliable evaluation framework is essential for benchmarking synthesis prediction models. This section examines the alignment between LLM-based and human expert judgments, evaluating inter-rater agreement and assessing the effectiveness of LLMs as automated evaluators.

\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}
We employ two metrics for evaluating the reliability of metrics: BLEU, ROUGE-L, BERTScore, and our LLM-as-a-Judge approach. \textbf{Pearson Correlation Coefficient} measures how closely LLM scores align with expert ratings on a continuous scale, capturing linear relationships. Finally, the \textbf{Spearman’s Rank Correlation} assesses rank-order consistency, beneficial when the relative ranking of recipes is more informative than absolute scores. 

\subsection{Human Expert Evaluation Setup}
\label{subsec:humaneval_setup}

Before evaluating whether the reliability of~\oursbench~assessment aligns with expert evaluations, we enlisted eight materials science researchers from three institutions to establish reliable ground truth. Each evaluator had prior experience in experimental synthesis and was selected based on their publication record and domain expertise. Experts independently assessed model-generated recipes using seven criteria (Table~\ref{tab:judgment_criteria}) on a 1–5 scale. To ensure high-quality assessments, we collected expert confidence scores and highlighted the agreement of one organization of three experts with the highest confidence levels on average (denoted as `High'). ICC(3,k) ensures the annotators' consensus's reliability. 

The dataset for evaluation included ten representative synthesis workflows selected by a senior materials scientist to ensure diversity. Prediction recipes were generated using two models (GPT-4o-mini and o1-mini), resulting in 20 unique predictions evaluated by human experts and LLM judges. Appendix~\ref{subsec:expert-protocol} and~\ref{sec:appendix_experiments} describe the experts annotation details and hyperparameters.

\subsection{Inter-Expert Agreement Analysis}
\label{subsec:inter_rater_analysis}
\input{table/experts/inter-raters-agreement}

The comparison between the High Confidence group and the All group in Table \ref{tab:inter-raters} highlights key differences in inter-rater reliability. The All group achieves higher ICC values for ``Material Appropriateness" (0.80) and ``Characterization Appropriateness" (0.78) compared to the High group (0.61 and 0.45, respectively), indicating better consensus among the broader panel for these criteria. However, the High group shows significantly stronger agreement on ``Procedure Feasibility" (ICC = 0.70) than the All group, which exhibits a negative ICC value (-0.58), suggesting inconsistencies in feasibility evaluations within the larger group. Both groups display similar reliability for ``Equipment Appropriateness" (ICC = 0.63). Overall, while larger panels may enhance agreement on straightforward criteria, smaller high-confidence subgroups provide more consistent evaluations for complex aspects like procedural feasibility.
% As shown in Table~\ref{tab:inter-raters}, ``Procedure Feasibility" (ICC = 0.70) and ``Overall Score" (ICC = 0.75) exhibit substantial agreement, confirming the reliability of expert assessments. However, the lower ICC for ``Procedure Similarity" (0.34) suggests inherent subjectivity in stepwise synthesis evaluation, reflecting variations in how experts interpret procedural similarity.

\subsection{LLM-Expert Agreement Analysis}
\label{subsec:llm_expert_analysis}
\input{table/experts/llm-experts-agreement}

In Table~\ref{tab:llm-expert}, the traditional metrics (BLEU, ROUGE-L, and BERTScore) exhibit low or even negative correlations with the domain expert consensus regardless of the evaluator group, whereas the LLM-based scores consistently yield higher and statistically significant correlations. The values obtained for GPT-4o-mini, GPT-4o-Aug, and o3-mini (high) are notably higher in the high confidence subgroup—0.61, 0.80, and 0.62 respectively—compared to 0.45, 0.61, and 0.47 for the full panel, suggesting that evaluations from the more confident experts are more tightly aligned with these models. In contrast, GPT-4o-Nov shows a higher correlation with all eight experts (0.75) than with the high confidence subset (0.63), indicating that its performance remains robust even when considering a broader range of expert opinions. Overall, the comparison underscores the influence of expert group composition on evaluation outcomes and highlights the superior alignment of advanced LLM evaluators with expert assessments over traditional similarity metrics\footnote{Spearman correlation scores are described in Appendix~\ref{subsec:llm-expert-agreement-details}.}.

Our experiment confirms that LLM-generated scores correlate significantly better with expert assessments, supporting their use as scalable synthesis evaluators.

% We analyzed the correlation between LLM and human expert scores across four state-of-the-art models, GPT-4o-mini, GPT-4o-Aug (2024-08-06), GPT-4o-Nov(2024-11-20), and o3-mini, using Pearson correlation. Table~\ref{tab:llm-expert} shows that GPT-4o-Aug achieved the highest agreement with expert ratings ($r=0.80, p=0.00$), demonstrating its effectiveness as an evaluator.

% Several factors likely contribute to this performance. First, GPT-4o-Aug incorporates advanced contextual reasoning capabilities, allowing it to assess synthesis coherence better. Second, its larger pretraining dataset, including scientific texts, may improve domain-specific evaluation. In contrast, smaller models like GPT-4o-mini exhibit weaker alignment with expert judgments, suggesting that model scale and pretraining corpus diversity significantly impact evaluation accuracy.

% \subsection{Comparison with Traditional Metrics}
% \label{subsec:traditional_metrics}

% While widely used in NLP, BLEU, ROUGE-L, and BERTScore are inadequate for evaluating synthesis workflows. These metrics prioritize lexical similarity, failing to capture underlying procedural dependencies and material constraints.

% In contrast, LLM-based evaluation incorporates structured reasoning, enabling a more holistic assessment of synthesis recipes. Table~\ref{tab:llm-expert} confirms that LLM-generated scores correlate significantly better with expert assessments, supporting their use as scalable synthesis evaluators.

\subsection{Summary and Implications}
\label{subsec:summary_implications}

Our findings demonstrate that LLM-based evaluation provides a scalable and effective alternative to traditional synthesis assessment methods. GPT-4o-Aug exhibits strong agreement with expert ratings, outperforming traditional NLP metrics. 

However, challenges remain, as LLMs can be sensitive to ambiguous phrasing and domain-specific biases, affecting evaluation consistency. Future work should explore hybrid approaches integrating expert feedback with LLM scoring. Reinforcement learning from human feedback~\cite{ouyang2022training} (RLHF) and domain-specific fine-tuning~\cite{anisuzzaman2025fine} may improve alignment with expert reasoning.

Future work should investigate methods for mitigating biases and inconsistencies to enhance reliability, such as integrating expert validation into LLM-based evaluation pipelines. This study highlights the potential of LLMs as automated evaluators, paving the way for AI-driven, context-aware benchmarking frameworks in materials science.

% Our findings demonstrate that LLM-based evaluation provides a scalable and effective alternative to traditional synthesis assessment methods. GPT-4o-Aug exhibits strong agreement with expert ratings, outperforming traditional NLP metrics. 

% However, challenges remain. LLMs can be sensitive to ambiguous phrasing and domain-specific biases, which may affect evaluation consistency. Future work should explore hybrid approaches integrating expert feedback with LLM scoring. Reinforcement learning from human feedback (RLHF) and domain-specific fine-tuning may further improve alignment with expert reasoning.

% This study highlights the potential of LLMs as automated evaluators, paving the way for AI-driven, context-aware benchmarking frameworks in materials science.






% \subsection{Human Expert Evaluation Setup}
% \label{subsec:humaneval-setup}
% \input{table/benchmark/criteria}


% To validate the performance of LLMs as evaluators, we conducted a systematic comparison with domain experts. Below, we outline the evaluation setup:

% \paragraph*{Domain Experts}
% We enlisted six materials science experts, all holding at least a master's degree and extensive materials synthesis experience. Each expert independently evaluated predictions generated by LLMs using a 1–5 scale across seven criteria, including material appropriateness, procedural coherence, and overall recipe quality (refer to Table~\ref{tab:judgment_criteria}). On average, each evaluation required 23 minutes (\(\sigma=7.57\)) per recipe. Appendix~\ref{sec:appendix_annotation} provides detailed evaluation guidelines and training materials.

% To ensure high-quality evaluations, we requested expert confidence scores for each assessment. Three experts with lower confidence scores were excluded from further analysis, leaving three highly confident evaluators whose consensus was used as the ground truth for comparison.

% \paragraph*{Dataset}
% We selected ten papers from \oursdatashort~that were representative of diverse materials synthesis workflows. Two LLMs (GPT-4o-mini and o1-mini) were tasked with generating recipes (\(\mathbf{P_M}\), \(\mathbf{P_E}\), \(\mathbf{P_P}\), and \(\mathbf{P_C}\)) for each paper based on input \textbf{X}. This resulted in a total of 20 predictions (10 per model). The expert panel evaluated these predictions against ground-truth recipes from the original papers.

% \paragraph*{Judge LLMs}
% We evaluated five state-of-the-art LLMs as judges:
% \begin{itemize}
%     \item \textbf{GPT-4o Aug and Nov}: The August and November 2024 release of GPT-4o with enhanced general-purpose capabilities.
%     \item \textbf{GPT-4o-mini}: A lightweight version of GPT-4o optimized for efficiency.
%     \item \textbf{o1-mini and o3-mini}: Reasoning-based flagship models outperforming in math, code, and academic tasks.
% \end{itemize}

% The primary objective was to determine whether these LLMs could reliably approximate human expert evaluations when scoring recipes generated by other models. We set reasoning\_effort to high for o3-mini, set temperature to zero for GPT-4o and GPT-4o-mini\footnote{o1-mini and o3-mini do not allow to set temperature.}.

% \paragraph*{Evaluation Metrics}
% We employed the following statistical metrics to measure the alignment between human expert scores and LLM-based judgments:
% \begin{itemize}
%     \item \textbf{Intraclass Correlation Coefficient (ICC)}: Used to assess consistency among evaluators across all criteria.
%     \item \textbf{Pearson Correlation Coefficient}: Measures linear agreement between human and LLM scores.
%     \item \textbf{Spearman’s Rank Correlation}: Evaluates rank-order agreement to account for non-linear relationships.
% \end{itemize}

% High agreement across these metrics would indicate that the LLM Judge can be a reliable proxy for expert evaluations, enabling scalable benchmarking without compromising scientific rigor.


% \subsection{Inter-Experts Agreement Analysis}
% \input{table/experts/inter-raters-agreement}

% To quantify consensus among human evaluators, we computed ICC using a two-way mixed-effects model (ICC3k) for absolute agreement. The inter-expert agreement analysis (Table~\ref{tab:inter-raters}) shows that domain experts exhibit substantial consensus across key evaluation criteria. Specifically, high ICC3k values for \textbf{Procedure Feasibility} (0.70) and \textbf{Overall Score} (0.75) confirm that experts consistently assess the practical executability and overall quality of synthesis recipes. While lower ICC values for \textbf{Procedure Similarity} (0.34) suggest some subjectivity in interpreting "closeness" to ground truth, the overall agreement supports the reliability of expert judgments as a gold standard for evaluation.  


% \subsection{LLM-Expert Agreement Analysis}

% \input{table/experts/llm-experts-agreement}

% The LLM-Expert agreement analysis (Table~\ref{tab:llm-expert}) highlights the viability of using GPT-4o Aug as an automated evaluator. GPT-4o Aug achieves a Pearson correlation of 0.52 with human scores, indicating a strong alignment with expert consensus. 
% % Notably, GPT-4o Nov improves this alignment further, achieving a state-of-the-art Pearson correlation of 0.52, demonstrating its ability to approximate expert-level evaluations effectively. 
% These results validate utilizing LLMs as cost-effective and scalable alternatives to complete human annotation, particularly for large-scale benchmarking tasks where manual evaluation is impractical.  

% \subsection{Summary and Implications}  
% \label{subsec:summary_implications}  

% The results presented in this section demonstrate the reliability of both human expert evaluations and the LLM Judge within our benchmarking framework. Robust inter-expert agreement and high LLM-expert alignment establish a strong foundation for our evaluation framework. The demonstrated reliability of GPT-4o as a judge supports its integration into our benchmark while maintaining scientific rigor. By leveraging LLM-based evaluation, we achieve significant efficiency gains without compromising the validity or quality of recipe assessments.
