
% \begin{figure*}[ht!]
% \begin{subfigure}{.55\textwidth}
% \centering
% \includegraphics[width=\linewidth]{image/data/element_freq_heatmap.png}
% \caption{Periodic table distribution of target materials}
% \label{fig:data-element-distribution}
% \end{subfigure}
% \begin{subfigure}{.43\textwidth}
% \centering
% \includegraphics[width=\linewidth]{image/data/top10process.png}
% \caption{Diversity of captured synthesis methods}
% \label{fig:data-process-distribution}
% \end{subfigure}
% \caption{Dataset composition reflecting broad coverage of materials systems and synthesis techniques}
% \label{fig:main-data-distribution}
% \end{figure*}

% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{image/data/top10process_twotone.png}
% \caption{Diversity of categorized synthesis methods of \oursdatashort, covering 10+ methods for materials synthesis. Prior studies of white colors cover the solution-based and solid-state methods.}
% \label{fig:data-process-distribution}
% \end{figure}

\section{Data Collection and Preparation}
\label{sec:data_collection }

\subsection{Motivation}
\label{subsec:motivation}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{image/data/example_icon.png}
\caption{An example of extracted recipe from~\citeauthor{zhao2020synthesis} demonstrates structured annotation of materials, equipment, procedures, and characterization methods.}
\label{fig:data-example}
\end{figure*}

% Prior large-scale datasets~\cite{kononova2019text,wang2022dataset} to extracting synthesis procedures from materials science literature have faced three critical limitations.
% \begin{enumerate}
%     \item Common errors include missing reagent concentrations, incorrect reaction temperatures, and misordered procedural steps, making existing datasets unreliable for machine-learning-driven synthesis prediction~\cite{sun2025critical}. Relying on keyword rules and Word2Vec-based~\cite{mikolov2013efficient} extraction produced excessively noisy outputs, rendering over \textbf{92\%} of \citeauthor{kononova2019text} and \textbf{98\%} of \citeauthor{wang2022dataset} lack necessary synthesis parameters such as heating temperature, hours, and mixing media, indicating unusable for downstream applications in our analysis. 
%     \item Narrowly focuses on limited synthesis routes—primarily solid-state, solution-based, and sol-gel methods—while real-world materials innovation utilizes dozens of specialized techniques~\cite{xu2023small}. 
%     \item Copyright restrictions prohibit the extraction and redistribution of textual synthesis procedures from commercial journals, creating legal barriers to dataset sharing~\cite{authorsalliance2024tdm}.
% \end{enumerate}

% To address these challenges, we introduce three innovations: 1) LLM-driven parsing to achieve substantial accuracy in recipe extraction, 2) systematic collection of 10+ synthesis methods spanning solution-based, vapor deposition, hybrid material systems, etc, and 3) exclusive use of open-access publications to enable legal dataset distribution. This approach overcomes the breadth, quality, and accessibility constraints of prior efforts. \hist{coverage, accuracy, accessibility가 앞에서도 언급되면 좋을 것 같네요}

Previous large-scale datasets for extracting synthesis procedures from materials science literature have faced several critical challenges~\cite{kononova2019text,wang2022dataset}. The most significant limitation involves common extraction errors—such as missing reagent concentrations, incorrect reaction temperatures, and misordered procedural steps—which have rendered many outputs unreliable for downstream synthesis prediction~\cite{sun2025critical}. We analyzed existing datasets and revealed that over 92\% of records in \citeauthor{kononova2019text} and 98\% in \citeauthor{wang2022dataset} lacked essential synthesis parameters (e.g., heating temperature, duration, mixing media). Additionally, these datasets are narrowly focused on a few synthesis techniques (such as solid-state and solution-based). At the same time, real-world materials innovation employs a broader range of specialized techniques~\cite{xu2023small}. Finally, copyright restrictions from commercial journals have limited the legal redistribution of textual synthesis procedures~\cite{authorsalliance2024tdm}.

To overcome these limitations, we propose~\oursdatashort~with three innovations: an LLM-driven parsing approach that improves extraction accuracy, a systematic collection covering more than ten distinct synthesis techniques (including vapor deposition, hydrothermal, and hybrid material systems), and the exclusive use of open-access publications to enable legal distribution of the dataset.


\label{subsec:quality_verification}


% \subsection{Collection and Extraction}
% \label{subsec:collection_extraction}

% Our pipeline begins with retrieving 28,685 open-access articles from the 400K searched articles using Semantic Scholar API~\cite{semanticscholar2023} using \hist{using - using 으로 이어짐} 50 domain-specific search terms (e.g., ``sol-gel synthesis", ``thin film deposition") suggested by domain experts \hist{domain experts 가 누군데? 생각이 살짝 드네요 (앞에 없었음)}. After converting PDFs to structured Markdown via \texttt{PyMuPDFLLM}~\cite{pymupdf4llm2024}, we employ GPT-4o~\cite{achiam2023gpt} for multi-stage annotation:

% \begin{itemize}
% \item \textbf{Document categorization:} classifies articles by the presence of synthesis protocols, target materials, synthesis methods, and applications. \hist{categorization 의 목적이 뭔지가 좀 불분명한 것 같습니다. 이런 것들이 없으면 filter out 하기 위함인건가요?}
% \item \textbf{Recipe segmentation:} extracts five critical components from confirmed articles:
% \begin{enumerate}
% \item \textbf{X}: Target material, synthesis method, application summary
% \item \(\mathbf{Y_M}\): Raw materials with quantitative details
% \item \(\mathbf{Y_E}\): Equipment specifications
% \item \(\mathbf{Y_P}\): Step-by-step procedures
% \item \(\mathbf{Y_C}\): Characterization methods/results
% \end{enumerate}
% \end{itemize}

% This process yielded \oursdatashort~of 17,667 high-quality recipes (\(\sim\) 62\% yield) covering 10 synthesis methods. Figure~\ref{fig:main-data-distribution} demonstrates our dataset's comprehensive coverage of elements and processes, and Figure~\ref{fig:data-example} demonstrates the actual synthesis recipe data extracted. Detailed LLM prompts and search keywords are described in Appendix~\ref{sec:appendix_dataset}.

\subsection{Dataset Construction}
Our pipeline begins by retrieving 28,685 open-access articles from a pool of 400K search results using the Semantic Scholar API with 60 domain-specific search terms (e.g., ``solid state sintering process", ``metal organic CVD") recommended by domain experts. We convert PDFs to structured Markdown using \texttt{PyMuPDFLLM}~\cite{pymupdf4llm2024} and then employ GPT-4o in a multi-stage annotation process. First, articles are categorized based on whether they contain synthesis protocols, target materials, synthesis techniques, and applications. For articles confirmed to include synthesis procedures, the text is segmented into five key components, as illustrated in Figure~\ref{fig:data-example}:

\begin{itemize}
    \item \textbf{X}: A summary of the target material, synthesis method, and application.
    \item \(\mathbf{Y_M}\): Raw materials, including quantitative details.
    \item \(\mathbf{Y_E}\): Equipment specifications.
    \item \(\mathbf{Y_P}\): Step-by-step procedural instructions.
    \item \(\mathbf{Y_C}\): Characterization methods and results.
\end{itemize}

This systematic extraction yielded a dataset of 17,667 high-quality recipes (approximately a 62\% yield) covering 10 diverse synthesis methods. Figure~\ref{fig:periodic-all} demonstrates our dataset's broad coverage of materials systems and synthesis techniques. Detailed LLM prompts and search keywords are provided in Appendix~\ref{sec:appendix_dataset}.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{image/data/top10process.png}
%     \caption{Diversity of categorized synthesis methods.}
%     \label{fig:data-process-distribution}
% \end{figure}


% To ensure the accuracy of our automatically extracted recipes, we assembled a panel of eight domain experts with extensive experience in materials synthesis from three institutions—two in materials modeling, three in thin film transistors, and three in transparent electrodes\footnote{Details about domain experts are described in Appendix~\ref{sec:appendix_annotation}}. Then, we manually verified and sampled ten recipes by eight domain experts using the following metrics:

\subsection{Quality Verification}
To ensure the accuracy of our automatically extracted recipes, we assembled a panel of eight domain experts from three institutions 
\footnote{Appendix~\ref{sec:appendix_annotation} describes the details about domain experts}. The experts manually reviewed a representative sample of ten recipes, evaluating them based on the following criteria:

% \begin{itemize}
%     \item \textbf{Completeness:} Did the extracted text capture the full scope of the reported recipe (\textbf{X}, \(\mathbf{Y_M}\), \(\mathbf{Y_E}\), \(\mathbf{Y_P}\), and \(\mathbf{Y_C}\))?
%     \item \textbf{Correctness:} Were critical details like temperature values or reagent amounts accurately extracted?
%     \item \textbf{Coherence:} Did the extracted recipe retain coherence and avoid contradictory information?
% \end{itemize}


\begin{itemize}
    \item \textbf{Completeness:} Capturing the full scope of the reported recipe (\textbf{X}, \(\mathbf{Y_M}\), \(\mathbf{Y_E}\), \(\mathbf{Y_P}\), and \(\mathbf{Y_C}\)).
    \item \textbf{Correctness:} Extracting critical details such as temperature values and reagent amounts accurately.
    \item \textbf{Coherence:} Retaining a logical, consistent narrative without contradictions or abrupt transitions.
\end{itemize}




Table~\ref{tab:human_verification} presents our expert evaluation results using a five-point Likert scale (1 = poor, 5 = excellent). To measure expert agreement, we computed the \textbf{Intraclass Correlation Coefficient (ICC)}~\cite{shrout1979intraclass}, utilizing a two-way mixed-effects model (ICC (3,k)) that quantifies agreement among evaluators, ensuring reliability in subjective scoring.
The extracted data exhibited high mean scores, but inter-rater reliability varied across criteria, particularly for articles with well-structured experimental sections.

\input{table/data/human_verification}
% Completeness received moderate-to-good agreement among evaluators (ICC = 0.695), confirming that essential synthesis details were generally well captured. However, correctness (ICC = 0.258) and coherence (ICC = 0.429) exhibited lower inter-rater reliability, suggesting inconsistencies in how evaluators interpreted minor details, such as variations in equipment naming and missing characterization information.
Completeness showed moderate agreement (ICC = 0.695), while correctness (ICC = 0.258) and coherence (ICC = 0.429) had lower agreement due to variations in naming conventions and missing characterization details.
Although the completeness score (4.2/5.0) was slightly lower than those for correctness (4.7/5.0) and coherence (4.8/5.0), correctness and coherence exhibited lower inter-rater reliability (ICC = 0.258 and 0.429, respectively), suggesting inconsistencies in how evaluators interpreted minor details. 
Variability in scores for correctness and coherence arose from differences in how evaluators weighted minor inconsistencies, such as variations in equipment naming or missing characterization information. Some considered these negligible, while others applied stricter criteria, underscoring the need for refined annotation guidelines.

While manual verification confirms the effectiveness of our extraction process, it cannot fully ensure consistent performance across the diverse range of synthesis procedures. In the following section (Section~\ref{sec:benchmark}), we present a structured evaluation framework for tasks such as raw materials and equipment inference, procedure generation, and characterization outcome forecasting.







% As shown in Table~\ref{tab:human_verification}, our expert evaluation using a five-point Likert scale (1 = poor, 5 = excellent) demonstrated strong performance across all criteria. The recipes achieved high scores for completeness (4.2/5.0), correctness (4.7/5.0), and coherence (4.8/5.0), particularly in articles with well-structured experimental sections. 

% \hist{설문지의 scale 이 앞에서 언급이 안 돼서, 4.2, 4.7, 4.8이 낮은건지 높은건지 모르겠어요}

% Completeness was assessed with relatively high agreement, indicating that key synthesis details were well captured. While its score was slightly lower than correctness and coherence due to occasional omissions of finer experimental details, such as precise molar ratios or characterization settings, the extracted recipes still provide a strong foundation for understanding synthesis procedures. In contrast, despite receiving high scores, correctness and coherence showed more significant variability among evaluators. 
% Variability in scores for correctness and coherence arose from differences in how evaluators weighted minor inconsistencies, such as variations in equipment naming or missing characterization information.
% % \hist{Variability in scores for correctness and coherence arose from differences in how evaluators weighted minor inconsistencies, such as variations in equipment naming or missing characterization information. 이 의미가 맞을까요? differences in how minor inconsistencies were weighted contributed to this variation이 읽기 좀 어려워서;;} 
% Some evaluators viewed these as negligible, while others applied stricter criteria, highlighting the need for more precise annotation guidelines. Overall, these results suggest that the extraction process is already highly effective and can be further refined to enhance the level of detail captured.  
% However, manual verification alone cannot ensure consistent performance across diverse synthesis procedures.  
% We introduce a benchmarking framework that quantitatively assesses model performance on end-to-end synthesis prediction to systematically evaluate how well models can predict and extract synthesis protocols.  
% Table~\ref{tab:human_verification} indicates that the majority of the extracted data is highly accurate, particularly for articles with well-structured experimental sections. Completeness received high agreement among evaluators, confirming that essential synthesis details were generally well captured. Although the completeness score was slightly lower than those for correctness and coherence—owing to occasional omissions of finer details such as precise molar ratios or specific characterization settings—the overall extraction quality provides a robust foundation for understanding synthesis procedures. Variability in scores for correctness and coherence arose from differences in how evaluators weighted minor inconsistencies, such as variations in equipment naming or missing characterization information. Some considered these negligible, while others applied stricter criteria, underscoring the need for refined annotation guidelines.

% The next step is to establish a structured evaluation framework with the dataset validated.  
% In the following section (Section~\ref{sec:benchmark}), we present a benchmark designed to assess NLP models on materials synthesis prediction, covering key tasks such as precursor and equipment inference, procedure generation, and characterization outcome forecasting.  
% \hist{이 문단이 굳이 이 섹션에 있어야 할까요? (Quality Verification 이고, benchmarking 부터는 말 그대로 AlchemyBench 에 어울리는 내용이 아닐까 싶어서요)}
% \hist{이 문단으로 다음 섹션을 시작하는건 어떨지... While manual verification confirms the effectiveness of our extraction process, it cannot fully ensure consistent performance across the diverse range of synthesis procedures. Therefore, we introduce a benchmarking framework that quantitatively assesses model performance on end-to-end synthesis prediction tasks. In the following section (Section~\ref{sec:benchmark}), we present a structured evaluation framework for tasks such as precursor and equipment inference, procedure generation, and characterization outcome forecasting.}

