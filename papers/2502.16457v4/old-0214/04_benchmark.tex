\section{AlchemyBench}
\label{sec:benchmark}

This section introduces our benchmark for evaluating models on materials synthesis prediction. We outline the motivation for using LLMs as judges, task definitions, dataset splits, and evaluation metrics, culminating in a framework reflecting real-world materials science challenges.

\subsection{Motivation: Why LLM-as-a-Judge?}
\label{subsec:motivation}
The evaluation of general materials synthesis recipes poses unique challenges that necessitate scalable and efficient solutions:
\begin{itemize}
    \item \textbf{Lack of Existing Benchmarks:} There is no established benchmark for evaluating general materials synthesis recipes nor a standardized framework to assess their quality systematically.
    \item \textbf{Limitations of Traditional Metrics:} Commonly used metrics such as BLEU~\cite{papineni2002bleu} and ROUGE~\cite{lin2004rouge} focus on surface-level lexical similarity but fail to capture the semantic requirements of synthesis recipes. \citeauthor{na2023artificial} et al. utilized the Jaccard score to evaluate the set overlap of synthesis procedures but lacks sensitivity to sequential dependencies critical in procedural texts. Additionally, BERTScore~\cite{zhang2019bertscore} leverages contextual embeddings from pretrained language models to measure semantic similarity beyond token matching, offering a more context-aware evaluation. However, despite its advantages, BERTScore often underperforms in the Materials Synthesis Recipe domain, where the intricate sequential dependencies and specialized domain semantics are not fully captured.
    \item \textbf{Cost and Inefficiency of Human Evaluation:} Employing domain experts for recipe evaluation is prohibitively expensive and time-consuming, with an average evaluation time of 23 minutes per recipe ($\sigma=7.57$) in our experiment.
    \item \textbf{Need for Scalable Solutions:} A scalable and high-performance evaluation tool is essential to support high-throughput benchmarking. LLMs offer a promising alternative by providing consistent and interpretable evaluations at scale~\cite{gu2025surveyllmasajudge}.
\end{itemize}

By addressing these challenges, LLM-as-a-Judge enables efficient benchmarking while maintaining alignment with expert evaluations.


\subsection{Task Definition}
\label{subsec:task_definition}
The proposed benchmark simulates end-to-end workflows in materials synthesis. Given an input $\mathbf{X}$ — comprising the target material, synthesis method, and application domain — the model must predict the following components:
\begin{itemize}
    \item $\mathbf{P_M}$: Raw materials (e.g., reagents, solvents) and their quantities.
    \item $\mathbf{P_E}$: Required equipment (e.g., furnace, autoclave).
    \item $\mathbf{P_P}$: Synthesis procedures (e.g., reaction steps, times, temperatures).
    \item $\mathbf{P_C}$: Characterization methods and expected outcomes.
\end{itemize}

The predictions $\mathbf{P_X}=\{\mathbf{P_M,P_E,P_P,P_C}\}$ are compared against ground-truth recipes $\mathbf{Y_X}=\{\mathbf{Y_M}, \mathbf{Y_E}, \mathbf{Y_P}, \mathbf{Y_C}\}$ using the LLM-as-a-Judge framework. The evaluation criteria include appropriateness, completeness, feasibility, and similarity to ground truth. This task setup was designed in collaboration with domain experts and mirrors real-world scenarios where researchers infer complete workflows from minimal initial information.

The LLM-as-a-Judge's overall score $\text{Score}(P_X,Y_X)$ is calculated as follows:
\[
\text{Score}(P_X, P_Y) = \frac{\sum_{i=1}^{N_C} C_i}{N_C}
\]
where $C_i$ represents the score for criterion $i$, and $N_C$ is the total number of criteria. We will introduce the detailed prompt and criteria in Section~\ref{sec:reliability}.

\subsection{Dataset Splits and Distribution}
Our dataset comprises 17,667 articles curated from diverse materials science literature. To ensure robust evaluation, we divide the dataset into three splits based on publication year and journal Impact Factor (IF):
\begin{itemize}
    \item \textbf{Training Set:} 16,026 articles published before 2024.
    \item \textbf{Test - Standard Impact:} 1,472 articles published in 2024 or later with IF $<$ 10.
    \item \textbf{Test - High Impact:} 169 articles published in 2024 or later with IF $\geq$ 10.
\end{itemize}

The temporal split ensures test data represents unseen future research trends, mitigating data contamination. Further stratification by IF allows targeted assessment of model performance on high-impact research, often involving novel and complex synthesis techniques. This split design evaluates both generalizability and the ability to meet the rigorous standards of top-tier journals.


