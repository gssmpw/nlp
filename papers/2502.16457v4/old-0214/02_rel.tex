\section{Related Work}

\subsection{Materials Synthesis Datasets}

Several datasets have been developed to support machine learning applications in materials synthesis. Text-mined datasets focus on extracting synthesis procedures from scientific literature using natural language processing (NLP) techniques. For instance, a dataset of 19,488 solid-state synthesis recipes~\cite{kononova2019text} was generated from over 53,000 paragraphs, including details like precursors, operations, and conditions. Similarly, solution-based synthesis datasets~\cite{wang2022dataset} include 35,675 procedures with detailed attributes like precursor quantities and reaction formulas. However, these datasets face significant limitations in quality and completeness. A critical review~\cite{sun2025critical} highlighted issues with the ``4 Vs.'' of data science—volume, variety, veracity, and velocity—arguing that many recipes lack essential reproducibility or predictive modeling information. For example, only 28\% of extracted solid-state synthesis paragraphs yielded complete reactions, and one in four compositions had unspecified stoichiometries. These challenges limit the utility of such datasets for guiding novel materials synthesis.


\subsection{LLMs for Materials Science}

Recent advancements in large language models (LLMs) have showcased their potential to accelerate materials discovery, simulation, and evaluation by automating hypothesis generation~\cite{kumbhar2025hypothesisgenerationmaterialsdiscovery}, predicting material properties~\cite{chiang2024llamplargelanguagemodel}, and even serving as evaluators~\cite{mishra2024llamat}. These systems integrate domain-specific knowledge with computational workflows, offering promising tools for designing novel materials or extracting structured insights from the literature. However, their effectiveness remains constrained by challenges such as limited domain-specific datasets, the need for fine-tuning to handle complex synthesis workflows, and insufficient interpretability in critical decision-making processes. These limitations underscore the need for standardized benchmarks and high-quality datasets to evaluate LLM capabilities in materials science rigorously. Our work addresses these gaps by introducing a comprehensive dataset and benchmark tailored to real-world synthesis workflows while also exploring the reliability of LLMs as evaluators against human experts.


\subsection{LLM-Based Evaluation in Materials Science}

Large Language Models (LLMs) have recently gained attention for their potential to evaluate and predict materials synthesis pathways. Studies demonstrate that fine-tuned LLMs can perform comparable or superior to traditional machine learning models in tasks like precursor selection and synthesizability prediction~\cite{lei2024materials,kim2024large}. For example, LLMatDesign~\cite{jia2024llmatdesign} uses LLMs to design materials by iteratively generating hypotheses and evaluating outcomes autonomously. Benchmarks like ALDbench~\cite{yanguas2024benchmarking} assess LLMs' ability to answer domain-specific questions about atomic layer deposition processes. 

Despite these advancements, using LLMs for evaluating materials synthesis quality remains underexplored. \citeauthor{luo2024large} have investigated human-LLM agreement in other scientific domains, showing promising correlations between LLM predictions and expert evaluations. However, in materials science specifically, systematic experiments comparing LLM-based evaluations with human experts are limited. This gap underscores the need for further research into leveraging LLMs as reliable evaluators in this domain while ensuring alignment with expert judgment.
