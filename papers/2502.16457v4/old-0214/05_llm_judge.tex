\section{LLM as a Judge}
\label{sec:reliability}

Ensuring the reliability of the LLM-as-a-Judge framework is critical for establishing trustworthy benchmarking. This section evaluates the alignment between LLM-based judgments and human expert evaluations, providing a detailed analysis of inter-rater agreement and the statistical metrics used to validate the framework.

\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}
\input{table/benchmark/criteria}

Evaluating synthesis recipes requires systematical judgment across multiple dimensions, including material appropriateness, procedural feasibility, etc. We employ three complementary statistical metrics to ensure rigorous assessment: Intraclass Correlation Coefficient (ICC), Pearson Correlation Coefficient, and Spearman’s Rank Correlation.

The \textbf{ICC} is used to measure inter-rater reliability, capturing the consistency of scores across evaluators. This metric is particularly relevant for subjective judgment tasks, where expert agreement is a gold standard for evaluation reliability. The \textbf{Pearson Correlation Coefficient} quantifies the linear alignment between human and LLM scores, providing insight into how closely LLM judgments approximate expert evaluations on a continuous scale. 
% Finally, the \textbf{Spearman’s Rank Correlation} evaluates rank-order agreement, making it suitable for scenarios where relative ranking of recipe quality is more important than absolute scores.

By combining these metrics, we ensure a comprehensive evaluation that captures both consistency and alignment across multiple dimensions. This approach addresses limitations in traditional metrics such as BLEU and ROUGE-L, which focus on surface-level text similarity and fail to account for the contextual appropriateness or procedural coherence required in synthesis recipes.

\subsection{Human Expert Evaluation Setup}
\label{subsec:humaneval_setup}

To establish a reliable ground truth for comparison, we enlisted eight domain experts from three organizations (2/3/3) with extensive experience in materials synthesis. Each expert independently evaluated model-generated recipes based on seven criteria outlined in Table~\ref{tab:judgment_criteria}, scoring each criterion on a 1–5 scale. On average, each evaluation required 23 minutes (\(\sigma=7.57\)) per recipe. To ensure high-quality assessments, we collected expert confidence scores and highlighted the agreement of one organization of three experts with the highest confidence levels on average (denoted as `High').

The dataset used for evaluation consisted of ten representative papers from \oursdatashort~covering diverse synthesis workflows. A professor in the field of materials science selected these papers based on the expertise of evaluators. Prediction recipes were generated by two models (GPT-4o-mini and o1-mini), resulting in 20 predictions evaluated by human experts and LLM judges.

\subsection{Inter-Expert Agreement Analysis}
\label{subsec:inter_rater_analysis}
\input{table/experts/inter-raters-agreement}

We computed the Intraclass Correlation Coefficient (ICC) using a two-way mixed-effects model (ICC3k) to quantify consensus among human evaluators. As shown in Table~\ref{tab:inter-raters}, high ICC values were observed for ``Procedure Feasibility" (0.70) and ``Overall Score" (0.75), indicating substantial agreement among experts on these critical criteria. Lower ICC values for ``Procedure Similarity`` (0.34) reflect inherent subjectivity in assessing how closely predicted procedures match ground truth recipes. However, the overall agreement supports the reliability of expert judgments as a gold standard for evaluation.

\subsection{LLM-Expert Agreement Analysis}
\label{subsec:llm_expert_analysis}
\input{table/experts/llm-experts-agreement}

To evaluate the alignment between LLM-based judgments and human expert evaluations, we analyzed the performance of four state-of-the-art LLMs using Pearson correlation coefficients. 
We set the temperature to zero for GPT-4o and GPT-4o-mini\footnote{o1-mini and o3-mini do not allow to set temperature.}.

Table~\ref{tab:llm-expert} summarizes these results. Notably, GPT-4o Aug achieved a Pearson correlation of 0.80 (\(p=0.00\)), demonstrating statistically a strong positive alignment with expert scores. These results surpass traditional metrics like BLEU (\(-0.16\), \(p=0.50\)) and ROUGE-L (0.06, \(p=0.80\)), which fail to capture expert-level aspects of recipe quality.

The demonstrated alignment between GPT-4o Aug and the high-confidence group validates its use as a scalable proxy for expert judgment in large-scale benchmarking tasks where manual evaluation is impractical.

\subsection{Comparison with Traditional Metrics}
\label{subsec:traditional_metrics}

Traditional metrics such as BLEU, ROUGE-L, and BERTScore were also evaluated as baselines but failed to align with human evaluations meaningfully (Table~\ref{tab:llm-expert}). These metrics rely on surface-level textual similarity and do not account for contextual appropriateness or procedural coherence—key aspects of synthesis recipe quality. In contrast, LLM-based evaluations explicitly incorporate these dimensions through structured scoring criteria (Table~\ref{tab:judgment_criteria}), making them better suited for this task.

\subsection{Summary and Implications}
\label{subsec:summary_implications}

The results presented in this section demonstrate the reliability of both human expert evaluations and LLM-based judgments within our benchmarking framework. High inter-expert agreement establishes the validity of human evaluations as a gold standard. At the same time, significant alignment between GPT-4o Aug and expert scores supports its integration into our benchmark as a scalable alternative to manual annotation.

Our framework achieves robust benchmarking without compromising scientific rigor by leveraging advanced statistical metrics and addressing limitations in traditional evaluation methods. Future work could further enhance alignment by incorporating pairwise comparison techniques or fine-tuning LLMs using reinforcement learning from human feedback.

% \subsection{Human Expert Evaluation Setup}
% \label{subsec:humaneval-setup}
% \input{table/benchmark/criteria}


% To validate the performance of LLMs as evaluators, we conducted a systematic comparison with domain experts. Below, we outline the evaluation setup:

% \paragraph*{Domain Experts}
% We enlisted six materials science experts, all holding at least a master's degree and extensive materials synthesis experience. Each expert independently evaluated predictions generated by LLMs using a 1–5 scale across seven criteria, including material appropriateness, procedural coherence, and overall recipe quality (refer to Table~\ref{tab:judgment_criteria}). On average, each evaluation required 23 minutes (\(\sigma=7.57\)) per recipe. Appendix~\ref{sec:appendix_annotation} provides detailed evaluation guidelines and training materials.

% To ensure high-quality evaluations, we requested expert confidence scores for each assessment. Three experts with lower confidence scores were excluded from further analysis, leaving three highly confident evaluators whose consensus was used as the ground truth for comparison.

% \paragraph*{Dataset}
% We selected ten papers from \oursdatashort~that were representative of diverse materials synthesis workflows. Two LLMs (GPT-4o-mini and o1-mini) were tasked with generating recipes (\(\mathbf{P_M}\), \(\mathbf{P_E}\), \(\mathbf{P_P}\), and \(\mathbf{P_C}\)) for each paper based on input \textbf{X}. This resulted in a total of 20 predictions (10 per model). The expert panel evaluated these predictions against ground-truth recipes from the original papers.

% \paragraph*{Judge LLMs}
% We evaluated five state-of-the-art LLMs as judges:
% \begin{itemize}
%     \item \textbf{GPT-4o Aug and Nov}: The August and November 2024 release of GPT-4o with enhanced general-purpose capabilities.
%     \item \textbf{GPT-4o-mini}: A lightweight version of GPT-4o optimized for efficiency.
%     \item \textbf{o1-mini and o3-mini}: Reasoning-based flagship models outperforming in math, code, and academic tasks.
% \end{itemize}

% The primary objective was to determine whether these LLMs could reliably approximate human expert evaluations when scoring recipes generated by other models. We set reasoning\_effort to high for o3-mini, set temperature to zero for GPT-4o and GPT-4o-mini\footnote{o1-mini and o3-mini do not allow to set temperature.}.

% \paragraph*{Evaluation Metrics}
% We employed the following statistical metrics to measure the alignment between human expert scores and LLM-based judgments:
% \begin{itemize}
%     \item \textbf{Intraclass Correlation Coefficient (ICC)}: Used to assess consistency among evaluators across all criteria.
%     \item \textbf{Pearson Correlation Coefficient}: Measures linear agreement between human and LLM scores.
%     \item \textbf{Spearman’s Rank Correlation}: Evaluates rank-order agreement to account for non-linear relationships.
% \end{itemize}

% High agreement across these metrics would indicate that the LLM Judge can be a reliable proxy for expert evaluations, enabling scalable benchmarking without compromising scientific rigor.


% \subsection{Inter-Experts Agreement Analysis}
% \input{table/experts/inter-raters-agreement}

% To quantify consensus among human evaluators, we computed ICC using a two-way mixed-effects model (ICC3k) for absolute agreement. The inter-expert agreement analysis (Table~\ref{tab:inter-raters}) shows that domain experts exhibit substantial consensus across key evaluation criteria. Specifically, high ICC3k values for \textbf{Procedure Feasibility} (0.70) and \textbf{Overall Score} (0.75) confirm that experts consistently assess the practical executability and overall quality of synthesis recipes. While lower ICC values for \textbf{Procedure Similarity} (0.34) suggest some subjectivity in interpreting "closeness" to ground truth, the overall agreement supports the reliability of expert judgments as a gold standard for evaluation.  


% \subsection{LLM-Expert Agreement Analysis}

% \input{table/experts/llm-experts-agreement}

% The LLM-Expert agreement analysis (Table~\ref{tab:llm-expert}) highlights the viability of using GPT-4o Aug as an automated evaluator. GPT-4o Aug achieves a Pearson correlation of 0.52 with human scores, indicating a strong alignment with expert consensus. 
% % Notably, GPT-4o Nov improves this alignment further, achieving a state-of-the-art Pearson correlation of 0.52, demonstrating its ability to approximate expert-level evaluations effectively. 
% These results validate utilizing LLMs as cost-effective and scalable alternatives to complete human annotation, particularly for large-scale benchmarking tasks where manual evaluation is impractical.  

% \subsection{Summary and Implications}  
% \label{subsec:summary_implications}  

% The results presented in this section demonstrate the reliability of both human expert evaluations and the LLM Judge within our benchmarking framework. Robust inter-expert agreement and high LLM-expert alignment establish a strong foundation for our evaluation framework. The demonstrated reliability of GPT-4o as a judge supports its integration into our benchmark while maintaining scientific rigor. By leveraging LLM-based evaluation, we achieve significant efficiency gains without compromising the validity or quality of recipe assessments.
