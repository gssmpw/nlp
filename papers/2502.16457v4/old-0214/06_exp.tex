\section{Experiments}
\label{sec:experiment}

This section evaluates the performance of five LLMs on the proposed benchmark using our LLM-as-a-Judge framework based on GPT-4o Aug. We analyze their performance across multiple metrics, including mean and maximum scores, score distributions, and the impact of retrieval-augmented generation (RAG).

\subsection{Experiment Setup}
\label{subsec:experiment_setup}

To comprehensively evaluate the models, we conducted experiments with the following setup:

\paragraph*{Base LLMs}
We evaluated four LLMs, including reasoning-based models (o3-mini) and general-purpose models (GPT-4o variants). The knowledge cutoff of these models is October 2023, minimizing potential data contamination in our test sets\footnote{The knowledge cutoff of OpenAI's models is described in \url{https://platform.openai.com/docs/models}.}. Moreover, we varied the reasoning\_effort of o3-mini to ensure the effectiveness of reasoning length in materials synthesis.

\paragraph*{Evaluation Framework}
Each model generated synthesis recipes for both the \testhi~and \testsi. Recipes were evaluated using our LLM-as-a-Judge method based on GPT-4o Aug. The evaluation criteria focused on material appropriateness, procedural feasibility, and overall recipe quality.
\paragraph*{Retrieval-Augmented Generation (RAG)}
To assess the impact of retrieval-augmented generation~\cite{lewis2020retrieval}, we implemented a RAG pipeline using OpenAI Embeddings~\cite{OpenAIEmbeedings2022}, which model is text-embedding-3-large with 3072 dimensions. For each input \(X\), we retrieved the top-\(K\) most similar contributions from the training set based on cosine similarity in embedding space and their recipes. These retrieved recipes were included as references in the prompts provided to the LLMs. We evaluated \(K = \{0, 1, 5, 10, 25\}\) to study how increasing contextual information affects recipe generation quality.

This experimental setup ensures a thorough evaluation of both baseline performance and improvements achieved through retrieval augmentation. Due to computational constraints, RAG experiments were conducted on three representative models (GPT-4o Nov, GPT-4o-mini, and o3-mini) using only the \testhi.

\subsection{Insights from Experimental Results}
\label{subsec:insights}


The experimental results provide valuable insights into the challenges and opportunities in materials synthesis prediction, structured around the following research questions:


\input{table/exp/base_results}
\paragraph*{RQ1: Is High-Impact Set More Challenging than Standard-Impact Set?}
The results confirm that \testhi~is indeed more challenging than \testsi. Across all models, average scores on the \testhi~were generally lower than those on the \testsi. For example, o3-mini-high achieved an average score of \(3.759 \pm 0.407\) on \testhi~compared to \(3.885 \pm 0.377\) on \testsi~(Table~5). This discrepancy highlights the increased complexity of High-Impact synthesis workflows, which often involve novel materials or cutting-edge techniques requiring greater reasoning and contextual understanding.

\paragraph*{RQ2: Are Reasoning-Based Models Effective in Materials Science?}
The reasoning-based models, particularly o3-mini-high, demonstrated superior performance compared to general-purpose models like GPT-4o Nov. On both High-Impact and Standard-Impact datasets, o3-mini-high consistently achieved higher mean scores and broader score distributions, indicating its ability to generate high-quality recipes in complex scenarios. For instance, o3-mini-high achieved a maximum score of \(4.71\) on High-Impact, matching GPT-4o Nov but with a higher average score (\(3.759\) vs. \(3.709\)).

These results align with trends observed in domains such as mathematics and programming, where reasoning-based models excel due to their structured problem-solving capabilities. However, the performance gap between o3-mini-high and other reasoning-based variants (e.g., o3-mini-medium) suggests that high reasoning capabilities are essential for tackling the nuanced challenges of materials science.

\paragraph*{RQ3: Does RAG Improve Performance?}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{image/exp/rag_high_impact.png}
    \caption{Impact of the Retrieval Augmented Generation (RAG) in \testhi.}
    \label{fig:rag_high_impact}
\end{figure}
The experimental results confirm that RAG significantly enhances model performance by grounding predictions in domain-relevant examples retrieved from the training set. As shown in Figure~\ref{fig:rag_high_impact}, increasing \(K\) (the number of retrieved references) improves overall scores for all tested models, with notable differences in how reasoning-based and general-purpose models benefit from retrieval.

For o3-mini (high), performance diminish beyond \(K=5\), achieving a maximum score of \(4.0\) at \(K=5\). This suggests that while retrieval provides valuable context, these models already possess strong reasoning capabilities to generate high-quality recipes independently.

In contrast, \textbf{GPT-4o Nov}, a general-purpose model, exhibits consistent improvement even beyond \(K=5\), ultimately achieving a competitive overall score of \(3.98\) at \(K=25\). This result highlights that retrieval mechanisms can partially compensate for the lack of inherent reasoning capabilities in general-purpose models by providing explicit contextual information. The marginal difference between GPT-4o Nov (\(3.98\)) and o3-mini-high (\(4.0\)) suggests that retrieval can narrow the performance gap between reasoning-based and general-purpose models in materials science tasks.

These findings underscore two key insights:
1) Retrieval augmentation is a foundational component for improving recipe generation quality, particularly for general-purpose models that rely heavily on external context.
2) The retrieval effectiveness depends on the model architecture and the quality of the training set. The observed gains validate the utility of our curated dataset as a reliable source of domain-specific knowledge.

However, diminishing returns beyond \(K=5\) for most models suggest an optimal range for reference incorporation, balancing contextual relevance with cognitive load on the model.