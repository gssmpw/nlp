\section{LLM as a Judge}
\label{sec:reliability}

Ensuring the reliability of the LLM-as-a-Judge framework is critical for establishing trustworthy benchmarking. This section evaluates the alignment between LLM-based judgments and human expert evaluations, analyzing inter-rater agreement across multiple dimensions.

\subsection{Human Expert Evaluation Setup}
\label{subsec:humaneval-setup}

To validate the performance of LLMs as evaluators, we conducted a systematic comparison with domain experts. Below, we outline the evaluation setup:

\paragraph*{Domain Experts}
We enlisted six materials science experts, all holding at least a master's degree and extensive materials synthesis experience. Each expert independently evaluated predictions generated by LLMs using a 1–5 scale across seven criteria, including material appropriateness, procedural coherence, and overall recipe quality (refer to Table~\ref{tab:judgment_criteria}). On average, each evaluation required 23 minutes (\(\sigma=7.57\)) per recipe. Appendix~\ref{sec:appendix_annotation} provides detailed evaluation guidelines and training materials.

To ensure high-quality evaluations, we requested expert confidence scores for each assessment. Three experts with lower confidence scores were excluded from further analysis, leaving three highly confident evaluators whose consensus was used as the ground truth for comparison.

\paragraph*{Dataset}
We selected ten papers from \oursdatashort~that were representative of diverse materials synthesis workflows. Two LLMs (GPT-4o-mini and o1-mini) were tasked with generating recipes (\(\mathbf{P_M}\), \(\mathbf{P_E}\), \(\mathbf{P_P}\), and \(\mathbf{P_C}\)) for each paper based on input \textbf{X}. This resulted in a total of 20 predictions (10 per model). The expert panel evaluated these predictions against ground-truth recipes from the original papers.

\paragraph*{Judge LLMs}
We evaluated four state-of-the-art LLMs as judges:
\begin{itemize}
    \item \textbf{GPT-4o Aug and Nov}: The August and November 2024 release of GPT-4o with enhanced general-purpose capabilities.
    \item \textbf{GPT-4o-mini}: A lightweight version of GPT-4o optimized for efficiency.
    \item \textbf{o1-mini and o3-mini}: Reasoning-based flagship models outperforming in math, code, and academic tasks. We set \texttt{reasoning\_effort = high} for o3-mini.
\end{itemize}

The primary objective was to determine whether these LLMs could reliably approximate human expert evaluations when scoring recipes generated by other models.

\paragraph*{Evaluation Metrics}
We employed the following statistical metrics to measure the alignment between human expert scores and LLM-based judgments:
\begin{itemize}
    \item \textbf{Intraclass Correlation Coefficient (ICC)}: Used to assess consistency among evaluators across all criteria.
    \item \textbf{Pearson Correlation Coefficient}: Measures linear agreement between human and LLM scores.
    \item \textbf{Spearman’s Rank Correlation}: Evaluates rank-order agreement to account for non-linear relationships.
\end{itemize}

High agreement across these metrics would indicate that the LLM Judge can be a reliable proxy for expert evaluations, enabling scalable benchmarking without compromising scientific rigor.


\subsection{Inter-Experts Agreement Analysis}
\input{table/experts/inter-raters-agreement}

To quantify consensus among human evaluators, we computed ICC using a two-way mixed-effects model (ICC3k) for absolute agreement. The inter-expert agreement analysis (Table~\ref{tab:inter-raters}) shows that domain experts exhibit substantial consensus across key evaluation criteria. Specifically, high ICC3k values for \textbf{Procedure Feasibility} (0.70) and \textbf{Overall Score} (0.75) confirm that experts consistently assess the practical executability and overall quality of synthesis recipes. While lower ICC values for \textbf{Procedure Similarity} (0.34) suggest some subjectivity in interpreting "closeness" to ground truth, the overall agreement supports the reliability of expert judgments as a gold standard for evaluation.  


\subsection{LLM-Expert Agreement Analysis}

\input{table/experts/llm-experts-agreement}

The LLM-Expert agreement analysis (Table~\ref{tab:llm-expert}) highlights the viability of using GPT-4o Aug as an automated evaluator. GPT-4o Aug achieves a Pearson correlation of 0.52 with human scores, indicating a strong alignment with expert consensus. 
% Notably, GPT-4o Nov improves this alignment further, achieving a state-of-the-art Pearson correlation of 0.52, demonstrating its ability to approximate expert-level evaluations effectively. 
These results validate utilizing LLMs as cost-effective and scalable alternatives to complete human annotation, particularly for large-scale benchmarking tasks where manual evaluation is impractical.  

\subsection{Summary and Implications}  
\label{subsec:summary_implications}  

The results presented in this section demonstrate the reliability of both human expert evaluations and the LLM Judge within our benchmarking framework. Robust inter-expert agreement and high LLM-expert alignment establish a strong foundation for our evaluation framework. The demonstrated reliability of GPT-4o as a judge supports its integration into our benchmark while maintaining scientific rigor. By leveraging LLM-based evaluation, we achieve significant efficiency gains without compromising the validity or quality of recipe assessments.
