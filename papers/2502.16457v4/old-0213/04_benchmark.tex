\section{Benchmark Definition and Tasks}
\label{sec:benchmark}

This section introduces our benchmark for evaluating models on materials synthesis prediction. We outline the task definitions, dataset splits, and evaluation metrics, culminating in a framework reflecting real-world materials science challenges.

\subsection{Task Definition}
The benchmark is designed to simulate end-to-end materials synthesis workflows. Given only the input \textbf{X}—which includes the target material, synthesis method, and application domain—the model must predict all subsequent components:  
\begin{itemize}
    \item \(\mathbf{P_M}\): Raw materials (e.g., reagents, solvents) and their quantities.
    \item \(\mathbf{P_E}\): Required equipment (e.g., furnace, autoclave).
    \item \(\mathbf{P_P}\): Synthesis procedures (e.g., reaction steps, times, temperatures).
    \item \(\mathbf{P_C}\): Characterization methods and expected outcomes.
\end{itemize}

The predicted outputs are compared against ground-truth recipes (\(\mathbf{Y_M}\), \(\mathbf{Y_E}\), \(\mathbf{Y_P}\), and \(\mathbf{Y_C}\)). Evaluations are conducted by either domain experts or an LLM Judge, as detailed in Section~\ref{subsec:metric}. This setup mirrors real-world scenarios where researchers must infer complete synthesis workflows from minimal initial information.

\subsection{Dataset Splits and Distribution}
Our dataset comprises 17,667 articles curated from diverse materials science literature. To ensure robust evaluation, we divide the dataset into three splits based on publication year and journal impact factor (IF):
\begin{itemize}
    \item \textbf{Training Set:} 16,026 articles published before 2024.
    \item \textbf{Test - Standard Impact:} 1,472 articles published in 2024 or later with IF $<$ 10.
    \item \textbf{Test - High Impact:} 169 articles published in 2024 or later with IF $\geq$ 10.
\end{itemize}

The temporal split ensures test data represents unseen future research trends, mitigating data contamination. Further stratification by IF allows targeted assessment of model performance on high-impact research, often involving novel and complex synthesis techniques. This split design evaluates both generalizability and the ability to meet the rigorous standards of top-tier journals.


\subsection{Evaluation Metrics: LLM-as-a-Judge}
\label{subsec:metric}

Evaluating synthesis predictions requires nuanced judgment across multiple dimensions. While human experts provide the gold standard for evaluation, their involvement is time-intensive and costly. To address this, we employ a large language model (LLM) as an efficient alternative evaluator.

\input{table/benchmark/criteria}
\paragraph*{Evaluation Criteria}
Table~\ref{tab:judgment_criteria} outlines the seven criteria for evaluating synthesis recipes.
Each criterion is scored on a 1–5 scale. The overall score is computed as an average across all criteria to reflect recipe quality comprehensively.

\paragraph*{LLM Evaluation Methodology}
We prompted LLMs to generate Chain-of-Thought (CoT) reasoning~\cite{wei2022chain}, followed by structured outputs in JSON format for scoring each criterion. This approach ensures transparency and interpretability in LLM judgments. Detailed prompts and examples are provided in Appendix~\ref{subsec:appendix_llm_judge}.

The reliability of LLM-based evaluations was validated by comparing them against human expert scores. A Pearson correlation coefficient of 0.52 between human and LLM evaluations demonstrates significant alignment, supporting using LLMs as scalable proxies for expert judgment. Further analysis of LLM-human agreement is discussed in Section~\ref{sec:reliability}.

\subsection{Summary and Outlook}
Our benchmark bridges the gap between controlled academic experiments on small datasets and real-world materials synthesis scenarios encountered in laboratories. By encompassing tasks that span precursor selection to characterization prediction, this framework provides a holistic evaluation platform for NLP models. Including high-impact test data ensures relevance to cutting-edge research, while leveraging LLMs as evaluators enable scalable benchmarking without sacrificing rigor.

Future iterations of this benchmark could further incorporate multimodal data (e.g., figures, tables) to enhance its applicability to practical materials science challenges.