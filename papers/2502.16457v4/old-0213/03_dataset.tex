\section{Data Collection and Preparation}
\label{sec:data_collection}

This section details our data collection pipeline, from document retrieval to extracting key synthesis information. We focus on how a large corpus of materials science literature is curated, classified, and annotated with recipes essential for downstream benchmarking tasks.

\subsection{Literature Retrieval and Filtering}
We began by compiling a list of 50 keywords that broadly represent research in materials science, ranging from specific compounds (e.g., ``perovskite,'' ``metal--organic framework'') to broader application domains (e.g., ``battery electrode,'' ``catalysis material''). These keywords were used to query the Semantic Scholar API~\cite{semanticscholar2023}. We restricted the collection of articles that include publicly available PDF files to comply with legal and ethical guidelines for text mining. This process yielded a dataset of 28,685 PDF documents spanning various journals and conferences. 

\subsection{PDF Conversion and Document Categorization}


\begin{figure*}[!ht]
    \begin{subfigure}{.55\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/data/element_freq_heatmap.png}
        \caption{The distribution of elements in the target material shown in the periodic table}
        \label{fig:data-element-distribution}
    \end{subfigure}
    \begin{subfigure}{.43\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/data/top10process.png}
        \caption{Distribution of categorized manufacturing methods}
        \label{fig:data-process-distribution}
    \end{subfigure}
    \caption{Data Distribution}
    \label{fig:main-data-distribution}
\end{figure*}


After retrieving the PDFs, we converted each file to Markdown format using the \texttt{PyMuPDFLLM}~\cite{pymupdf4llm2024}. 
The resulting Markdown files preserve paragraph structures and metadata while remaining flexible for downstream tasks (e.g., classification, named entity recognition).
Using the processed Markdown files, we employed GPT-4o~\cite{achiam2023gpt} for an automated categorization step, labeling each article according to:

\begin{itemize}
    \item \textbf{Presence of a synthesis recipe:} Whether the text explicitly provides a detailed experimental procedure.
    \item \textbf{Primary material(s):} The main material name, chemical formula, and class studied (e.g., inorganic oxides, polymers).
    \item \textbf{Process or synthesis route:} The dominant experimental methods (e.g., hydrothermal, solution-based, vapor deposition).
    \item \textbf{Application domain:} The intended use-case (e.g., battery, catalysis, biomedical).
\end{itemize}

Based on these annotations, we identified 17,667 articles (\(\sim62\%\) of the corpus) containing explicit synthesis ``recipes'', making them highly relevant for tasks involving the prediction or generation of experimental protocols.

\subsection{Recipe Extraction}
\label{sec:recipe_extraction}

\subsubsection{Key Information Segmentation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{image/data/example.png}
    \caption{An actual example of extracted recipe.}
    \label{fig:data-example}
\end{figure*}

For each of the 17,667 articles confirmed to include a synthesis recipe, we extracted five critical components, denoted as \textbf{X}, \(\mathbf{Y_M}\), \(\mathbf{Y_E}\), \(\mathbf{Y_P}\), and \(\mathbf{Y_C}\). These components capture the end-to-end workflow of materials synthesis:

\begin{enumerate}
    \item \textbf{X: Target Material, Synthesis Method, and Application Domain}
    
    A concise summary describing:
    \begin{itemize}
        \item \textit{What is being synthesized?} (e.g., Ni-rich layered oxides)
        \item \textit{Which process is used?} (e.g., solvothermal, sol-gel)
        \item \textit{What is the main application?} (e.g., electrode for lithium-ion batteries)
    \end{itemize}
    
    \item \(\mathbf{Y_M}\): \textbf{Raw Materials}
    
    A list of raw materials (e.g., reagents, solvents) and any quantitative details (e.g., molar ratios, concentrations, masses).
    
    \item \(\mathbf{Y_E}\): \textbf{Synthesis Equipment}
    
    A catalog of apparatus (e.g., furnace, autoclave, spin coater) essential for the synthesis steps.
    
    \item \(\mathbf{Y_P}\): \textbf{Synthesis Procedure}
    
    A step-by-step protocol detailing how the reagents are combined and processed (e.g., reaction times, temperatures, pH, stirring speeds).
    
    \item \(\mathbf{Y_C}\): \textbf{Characterization Methods and Results}
    
    Techniques (e.g., X-ray diffraction, electron microscopy) and outcomes (e.g., crystal structure, particle size, electrochemical performance) used to evaluate the synthesized material.
\end{enumerate}

\subsubsection{LLM-Driven Parsing and Annotation}
The extraction of these elements from each Markdown file was again performed via GPT-4o. We minimized human labor in large-scale annotation by carefully prompting the model to identify and extract relevant text blocks.
Detailed prompts are described in Appendix~\ref{sec:appendix_annotation}.

\subsection{Data Verification and Quality Assessment}
To ensure the accuracy of our automatically extracted recipes, we manually verified sampled recipes by eight domain experts, at least a master's student majoring in materials science. These experts evaluated:

\begin{itemize}
    \item \textbf{Completeness:} Did the extracted text capture the full scope of the reported recipe (\textbf{X}, \(\mathbf{Y_M}\), \(\mathbf{Y_E}\), \(\mathbf{Y_P}\), and \(\mathbf{Y_C}\))?
    \item \textbf{Correctness:} Were critical details like temperature values or reagent amounts accurately extracted?
    \item \textbf{Coherence:} Did the extracted recipe retain coherence and avoid contradictory information?
\end{itemize}

\input{table/data/human_verification}
Preliminary reviews indicate that the \emph{majority} of extracted data is highly accurate, especially for articles with well-structured experimental sections. 

Completeness was assessed with relatively high agreement, indicating that key synthesis details were well captured. While its score was slightly lower than correctness and coherence due to occasional omissions of finer experimental details, such as precise molar ratios or characterization settings, the extracted recipes still provide a strong foundation for understanding synthesis procedures. In contrast, correctness and coherence, despite receiving high scores, showed greater variability among evaluators. Differences in how minor inconsistencies—such as variations in equipment names or missing characterization details—were weighted contributed to this variation. Some evaluators viewed these as negligible, while others applied stricter criteria, highlighting the need for clearer annotation guidelines. Overall, these results suggest that the extraction process is already highly effective and can be further refined to enhance the level of detail captured.

% {\color{red}Comprehensive metrics on precision and recall will be added once the manual verification is complete}% 
% Insert final statistics later.

With the dataset prepared and validated, we now focus on how these recipes are used to define a comprehensive benchmark. In the following section (Section~\ref{sec:benchmark}), we introduce the benchmark that evaluates NLP models on end-to-end materials synthesis prediction, ranging from precursor and equipment inference to procedure generation and characterization outcome forecasting.

\subsection{comparison with Existing Datasets}
A direct comparison with prior dataset is necessary to highlight the advantages of the Open Materials Guide (OMG) dataset.