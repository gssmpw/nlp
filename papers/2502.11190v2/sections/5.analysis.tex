\section{Further Analysis}
\label{sec:analysis}
\begin{figure}[!t] 
    \includegraphics[width=\columnwidth]{images/robustness.pdf} 
    \vspace{-4.5ex}
    \caption{Robustness Evaluation compares the KFR of three methods under precision changes (float16 â†’ bfloat16) and jailbreak attacks.} \vspace{-3ex} 
    \label{fig:robustess} 
\end{figure}
\subsection{Robustness Evaluation}
Building on previous work \citep{zhang2024doesllmtrulyunlearn, lu2024eraserjailbreakingdefenselarge}, which demonstrates that parameter precision and jailbreak attacks affect unlearning, we analyze the robustness of unlearned models under these conditions on KnowUnDo. 
The results are presented in Figure~\ref{fig:robustess}, and we can summarize two key findings.
\paragraph{ReLearn Prevents Knowledge Leakage under Precision Variation.}
As seen from Figure~\ref{fig:robustess}, we observe that reducing the precision of the parameter from float16 to bfloat16 causes a significant decrease in KFR performance, 9.7\% for GA and 18.2\% for NPO.
This suggests that GA and NPO are sensitive to parameter precision and rely on fine-grained adjustments during LoRA fine-tuning.
The sentence completion examples in Appendix Table~\ref{tab:robustness_case} demonstrate that while GA and NPO exhibit unreadable outputs in most cases, indicating over-forgetting, they also reveal some instances of knowledge leakage.
In contrast, ReLearn shows a slight performance improvement of 1.4\% under reduced precision while consistently maintaining a coherent output.
\paragraph{ReLearn Effectively Resists Jailbreaks.}
By using the AIM jailbreak attack \citep{NEURIPS2023_fd661313}, a prompt engineering method that forces compromised model responses (with templates in Appendix~\ref{appendix:AIM}), we observe KFR performance degradation of 5.0\% for GA and 9.1\% for NPO.
In particular, ReLearn achieves a performance improvement of 6.9\%. 
This difference indicates that GA and NPO weaken the base model's inherent jailbreak resistance, while ReLearn maintains and even enhances this defensive capability. 
As seen from the examples shown in Table~\ref{tab:robustness_case}, when attacked, ReLearn effectively prevents jailbreak attacks targeting forgotten knowledge, while GA and NPO tend to leak private information (sometimes incomplete) or generate unreadable responses.

\subsection{The Mechanism of Unlearning}
In this section, we analyze how GA and NPO disrupt the model's linguistic ability and explore how ReLearn reconstructs it.
We analyze from three perspectives: Knowledge Distribution, Knowledge Memory, and Knowledge Circuits.

\subsubsection{Knowledge Distribution}
GA and NPO both rely on reverse optimization to suppress the probabilities of the target token, leading to \textbf{\textit{a disruptive ``probability seesaw effect''}}. 
To explore the knowledge distribution of different unlearning models, we calculate the top-5 candidate tokens in their outputs, as shown in Figure~\ref{fig:prob} and Figure~\ref{fig:gemma_top5} in the Appendix. 
As observed, in models with a \textbf{multi-peaked probability distribution} (e.g., Llama2 Vanilla in Figure~\ref{fig:prob}), the ``seesaw'' effect exhibits two sequent steps: 
(1) \emph{Initial Target Token Suppression:} By suppressing the initially top-1 token and guiding the model towards other high-probability tokens, this potentially leads to sensitive responses (as illustrated in Figure~\ref{fig:prob}, where the top-2 token in the Vanilla model becomes the top-1 token in the NPO model).
(2) \emph{Subsequent Top Token Suppression:} This involves the continued suppression of high-probability tokens, resulting in probability redistribution across random tokens (as observed on Llama2 GA in Figure~\ref{fig:prob}).  
In contrast, for models with a \textbf{unimodal probability distribution} (e.g., Gemma in Figure~\ref{fig:gemma_top5}), reverse optimization merely suppresses the single high-probability peak of the target token, resulting in a more uniform probability distribution across random tokens after unlearning. 

The disrupted probability distributions resemble \emph{cognitive conflict} \citep{xu-etal-2024-earth}, which arises from the conflict between the intrinsic knowledge of a model and external inputs or training objectives.  
\textbf{Reverse optimization directly drives the decoding space toward randomness, leading to a significant cognitive mismatch between the pre-unlearning and post-unlearning states, limiting question understanding and coherent generation.}  
In contrast, ReLearn does not aim for a complete disruption of the knowledge distribution.  
By learning to generate relevant yet non-sensitive answers, ReLearn guides the model toward a new cognitive pattern.

\begin{figure}[!htbp]
\includegraphics[width=\linewidth]{images/top5_llama2.pdf}
% \vspace{-3ex}
  \caption{The top-5 candidate tokens distribution of different unlearning approaches on KnowUnDo.}
  % \vspace{-2ex}
  \label{fig:prob}
\end{figure}
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{images/knowledge_memory.pdf}
  \vspace{-4ex}
  \caption{Knowledge Memory. Vanilla model generates ``5000 Sierra Rd Bogota Colomb''; GA/NPO produce repetitive ``at''; ReLearn generates a contextually relevant but non-sensitive response.}
  \vspace{-2ex}
  \label{fig:mem}
\end{figure}
\subsubsection{Knowledge Memory}
Inspired by recent research \citep{geva-etal-2022-transformer, geva-etal-2023-dissecting, ghandeharioun2024s, menta2025analyzingmemorizationlargelanguage} that the early layers process context, the deeper layers memorize, and the last few layers handle the prediction of the next token, our analysis focuses on the final token position's outputs across all decoding layers\citep{belrose2023elicitinglatentpredictionstransformers}.

Figure~\ref{fig:mem} demonstrates the difference between these methods.
When queried with ``Carlos Rivera's mailing address is...'', the vanilla model directly activates both general concepts like ``address'' and ``location'', as well as the answer terms such as ``Colomb''. 
In contrast, ReLearn preserves semantic understanding without directly recalling the answer. 
In its middle and later layers, it recalls related concepts like ``located'' and ``address'', along with query terms such as ``Carlos''.
In comparison, reverse optimization methods like NPO activate ``address'' before the 20th layer but fail to trigger related knowledge afterward, instead repeating ``at'' beyond the 20th layer.

Moreover, the Forward-KL, which represents the KL Divergence between the current and final layers, shows a gradual shift for the vanilla and ReLearn models, but a severe shift for GA/NPO.
This severe change hinders the effective use of semantic information for knowledge retrieval and refinement, impeding the appropriate generation of responses.

In summary, \textbf{reverse optimization significantly impairs knowledge memory by overemphasizing next-token prediction and disrupting the ability of gradual information adjustment}, which is similar to memory loss in Alzheimer's disease \citep{Jahn2013memoryloss}. 
In contrast, ReLearn maintains robust knowledge memory across layers, preserving linguistic capabilities, and enabling fluent, relevant responses through positive optimization.

\subsubsection{Knowledge Circuits}
We employ the LLMTT tool \citep{tufanov2024lm} to visualize \textit{knowledge circuits} and investigate how different unlearning methods affect model focus.
LLMTT identifies the salient connections (``circuits'') within the LLM inference process by varying the threshold, where higher thresholds indicate stronger connections.
As shown in Figure~\ref{fig:circuits} in the Appendix, with a threshold of 0.06, the vanilla, GA, and NPO models exhibit similar circuit patterns. 
However, ReLearn notably reduces circuits associated with sensitive entities, indicating a weakened focus on sensitive information.
When the threshold increases to 0.08, the circuits of vanilla model and ReLearn model become empty, while GA and NPO strengthen partial circuits, particularly those specific question patterns (e.g., ``How does...background...?'').
This observation suggests that \textbf{GA and NPO over-forget specific question patterns}, while ReLearn achieves generalized unlearning by weakening entity associations.
% resulting in issues similar to generalization problems caused by spurious correlations \citep{bayat2024pitfallsmemorizationmemorizationhurts}.
