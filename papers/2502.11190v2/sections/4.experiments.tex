\section{Experiments}
\label{section:experiments}
\input{tables/kud-llama2-7b}
\subsection{Datasets}
We evaluate our method on two benchmark datasets:
(1) TOFU \citep{maini2024tofutaskfictitiousunlearning}, a synthetic dataset comprising 4,000 QA pairs from 200 fictitious authors (20 pairs per author). 
(2) KnowUnDo \citep{tian2024forgetnotpracticalknowledge}, generated by GPT-4 to simulate real-world scenarios with QA pairs on sensitive content.
We use the forget10 subset for TOFU and the privacy subset for KnowUnDo.
TOFU evaluates performance on the training set, while KnowUnDo evaluates generalization on a separate validation set. 
Notably, ReLearn trains only on augmented variants, so the reported results inherently offer an evaluation of unlearning generalization.

\subsection{Baselines and Metrics}
\label{sec:baselines_metrics}
To evaluate the forgetting performance of ReLearn, we compare it against three gradient-based baselines from prior LLM unlearning methods, focusing on their forgetting loss:
(1) \textbf{Gradient Ascent (GA)} \citep{ga}, which employs gradient ascent on the knowledge to be forgotten;
(2) \textbf{Negative Preference Optimization (NPO)} \citep{npo}, which leverages preference optimization only for the knowledge to be forgotten; 
and (3) \textbf{Saliency-Based Unlearning with a Large Learning Rate (SURE)} \citep{zhang2024doesllmtrulyunlearn}, which dynamically identifies and updates the most relevant parameters for forgetting in each training step.
We exclude representation-based unlearning methods due to their difficulty in balancing forgetting and retention \citep{shi2024musemachineunlearningsixway}.
For retention loss, we employ \textbf{Gradient Descent on Retain Set (GDR)} and \textbf{KL Divergence Minimization on Retain Set (KLR)} to improve knowledge preservation.
Detailed formulas are provided in the Appendix~\ref{section:baselines}.

As described in \S\ref{rethinkunlearneval}, our evaluation uses \textbf{KFR} and \textbf{KRR} to measure knowledge unlearning and retention; and \textbf{LS} to evaluate response quality. 
The constants $c_1$ in Eq~\eqref{eq:kfr} and $c_2$ in Eq~\eqref{eq:krr} are set to 0.3 for these metrics. 
All scores are averaged across the samples. 
To assess fluency (Flu.) and relevance (Rel.), 
we employ \textbf{GPT Score} \citep{sottana-etal-2023-evaluation}, generated by GPT-4o, ranging from 1 to 5.
The prompt templates are shown in the appendix~\ref{appedix:gpt4o}.

Detailed design principles for all metrics are provided in Appendix~\ref{section:metrics}.

\subsection{Settings}
We utilize Deepseek-V3 \citep{deepseekai2024deepseekv3technicalreport} for data augmentation and fine-tune the Llama-2â€“7b-chat \citep{touvron2023llama2openfoundation} and gemma-2-2b-it \citep{gemmateam2024gemma2improvingopen} models using LoRA \citep{hu2021loralowrankadaptationlarge}. 
For KnowUnDo, \emph{it takes nearly 1,149,855 input tokens, 310,353 output tokens, and 240 minutes for data synthesis and training.} 
All analysis experiments in this paper employ the regularized GA and NPO variants, i.e., GA$_{GDR}$+SURE as GA and NPO$_{GDR}$+SURE as NPO.
Additional implementation details are provided in the Appendix~\ref{appendix:implementation}.

\input{tables/tofu-llama2-7b}

\subsection{Results}
\paragraph{Main Results.}
We report the unlearning performance of Llama-2-7b-chat on KnowUnDo in Table~\ref{tab:knowundo} and TOFU in Table~\ref{tab:tofu}; additional results for gemma-2-2b-it can be found in Table~\ref{tab:gemma2-2b} in the Appendix. 
Across these datasets, ReLearn achieves a competitive KFR of 0.88 on KnowUnDo and 0.81 on TOFU while maintaining high KRR (0.74 on KnowUnDo and 0.98 on TOFU). 
In contrast, the best baseline, NPO$_{GDR}$, obtains KFR values of 0.99 on KnowUnDo and 1.00 on TOFU but much lower KRR (0.45 and 0.54, respectively). 
Notably, GA and NPO severely degrade the LS compared to the vanilla model (0.15$\sim$0.16 $\to$ $\leq$0.1 on KnowUnDo; 0.10$\sim$0.11$ \to$ $\leq$0.03 on TOFU) and exhibit extremely low Fluency (Flu.$\approx$1) and Relevance (Rel.$\approx$1).
In contrast, ReLearn preserves good LS (0.13$\sim$0.17 on KnowUnDo and 0.08$\sim$0.10 on TOFU) while maintaining Fluency and Relevance comparable to the vanilla model.

These results show that ReLearn effectively balances forgetting and retention while preserving linguistic quality. 
In contrast, GA and NPO achieve extremely high KFR but suffer from poor retention performance.
This trend persists in different datasets and models.
Detailed cases are provided in Table~\ref{tab:case},  
and supplementary studies in Appendix~\ref{appendix:supplemetary_studies} further demonstrate the balanced performance and adaptability of ReLearn.

\paragraph{Human Evaluation \& General Task Test.}
To further verify the unlearning performance and linguistic quality, we implement human evaluation to assess responses on Forgetting (Forget.), Relevance (Rel.), and Fluency (Flu.) using a discrete rating scale of 1 to 5, as elaborated in Appendix~\ref{Human_eval}. 
The model names are anonymized and the scores are averaged among three volunteers. 
As shown in Table~\ref{tab:combined_results}, ReLearn achieves a score of 4.30 for ``Forgetting'', effectively forgetting sensitive knowledge, while other models obtain low relevance and fluency scores, as they often produce repetitive and meaningless responses. 
Moreover, ReLearn performs best on two generic tasks (MMLU and GSM8K).
\input{tables/human-eval}
