\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{images/flow.pdf}
    % \vspace{-2ex}
    \caption{Illustration of ReLearn: High-quality data synthesis for effective unlearning.}
    \vspace{-3ex}
    \label{flow}
\end{figure*}

\section{Methodology}
We elaborate \textbf{ReLearn} in this section, which is illustrated in Figure~\ref{flow}. 
ReLearn achieves effective unlearning through data augmentation and fine-tuning. 
This strategy replaces sensitive content with new, non-sensitive knowledge, guided by two key principles: 
(1) ensuring the successful forgetting of key content, and (2) generating relevant and coherent responses. 
% Notably, ReLearn trains only on augmented question variants, excluding originals from the forget set, which reflects the  generalization capabilities.
\paragraph{Unlearning Data Synthesis.}
The first step of ReLearn is to synthesize non-sensitive training data. 
This is achieved by augmenting the forget set $D_f$ with diverse variations, ensuring comprehensive coverage of the knowledge to be forgotten. 
Data synthesis is entirely performed by an LLM using specific prompts, with details provided in Appendix~\ref{sec:prompt}.
This process involves two key steps:

\textit{Question Augmentation:}
For each question-answer pair $(q, a) \in D_f$, we synthesize four types of question variations:
(1) \underline{Simple Variant}: Prevent overfitting to specific phrasings by varying the question language (e.g., ``What is'' $\rightarrow$ ``Can you tell me'').
(2) \underline{Contextual Variant}: Ensuring forgetting across contexts by adding situational context (e.g., ``in a ... setting'').
(3) \underline{Noise Variant}: Enhance robustness to noisy inputs.
(4) \underline{Logical Variant}: Adapting to different knowledge forms by altering the logic of the questions (e.g., ``What is your email?'' $\rightarrow$ ``What are the different parts of your email address?'').
The augmented questions \(\tilde{q}\), along with their corresponding original answers \(a\), form the set \(\tilde{D}_f^{Q} = \{(\tilde{q}, a)\}\).

\textit{Answer Augmentation:} 
For each $(\tilde{q}, a) \in \tilde{D}_f^{Q}$, we synthesize new pairs $(\tilde{q}, \tilde{a})$ with relevant, deliberately vague answers ($\tilde{a}$).  
Critically, $\tilde{a}$ must be: 
(1) \underline{Unlearned}, containing no original sensitive content; 
(2) \underline{Relevant}, aligning with the question context; 
and (3) \underline{No-risk}, avoiding introducing new sensitive content. 
All such pairs form the augmented forget QA set $\tilde{D}_f^{QA} = \{(\tilde{q}, \tilde{a})\}$.
This ensures that the model can respond appropriately without retaining the original sensitive details.

Detailed examples of augmented QA pairs are provided in Appendix~\ref{appendix:Augumented Cases}.
\paragraph{Content Verification.}
Synthesized data may introduce new privacy risk. 
To ensure the safety of the augmented data, we employ a Content Verification process for the answers in $\tilde{D}_f^{QA}$.  
This process utilizes LLMs to conduct Chain-of-Thought \citep{wei2023chainofthoughtpromptingelicitsreasoning} analysis on each augmented answer, evaluating it against predefined safety criteria.
Detailed prompts for the verification are provided in Appendix~\ref{appendix:Verification}.
If verification fails, indicating a potential risk in the augmented data, the process returns to the step of ``\emph{Answer Augmentation}''.

\paragraph{Data Diversification.}
(1) \textit{Sentence Completion:} 
To prevent QA format overfitting, we augment data with sentence completion pairs ($\tilde{D}_f^{SC}$), split from each answer in $\tilde{D}_f^{QA}$. 
For example, splitting ``Isabella Marquez can be reached through conventional electronic communication channels.'' into the text ``Isabella Marquez can be reached through'' and the label ``conventional electronic communication channels.''.
Then, we obtain $\tilde{D}_f = \tilde{D}_f^{QA}\cup\tilde{D}_f^{SC}$.
(2) \textit{Generic Dataset:} 
To prevent catastrophic forgetting, we incorporate generic data. 
We randomly sample questions from WikiQA \citep{yang-etal-2015-wikiqa} and Chatbot Instruction \citep{kim2022prosocialdialog} to form a generic dataset ($\tilde{D}_g$). 
For TOFU \citep{maini2024tofutaskfictitiousunlearning} and KnowUnDo \citep{tian2024forgetnotpracticalknowledge},  $\tilde{D}_g$ is mixed with the augmented forget set ($\tilde{D}_f$) in the ratio of 1:1 . 

\paragraph{Unlearning via Learning.}
We formulate the unlearning objective using three datasets: 
the augmented forget set $\tilde{D_f}$, the retain set $D_r$, and the generic dataset $D_g$.
For datasets $\tilde{D_f} \cup D_g$ and $D_r$, we employ cross-entropy loss:
\begin{equation}
  L_{GDF} = \mathbb{E}_{(x,y) \sim \tilde{D_f} \cup D_g}[-\log P_{\theta}(y|x)]
\end{equation}
\begin{equation}
  L_{GDR} = \mathbb{E}_{(x,y) \sim D_r}[-\log P_{\theta}(y|x)]
\end{equation}
To preserve knowledge in the retain set, we minimize Kullback-Leibler Divergence (KL) between vanilla model and current model:
\begin{equation}
  L_{KLR} = \mathbb{E}_{x \sim D_r}[D_{KL}(P_{\theta}(\cdot|x) || P_{\theta_0}(\cdot|x))]
\end{equation}
where $P_{\theta_0}$ denotes the vanilla model distribution.

Finally, the overall loss of ReLearn is:
\begin{equation}
  L_{ReLearn} = L_{GDF} + L_{GDR} + L_{KLR}
\end{equation}