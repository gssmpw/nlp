\section{Preliminary}  
\subsection{Problem Definition}
We define LLM unlearning as follows:
given a vanilla model \( M \) trained on a dataset \( D \) that consists of a forget set \( D_f \) and a retain set \( D_r \).
For all \((x_f, y_f) \in D_f\) and \((x_r, y_r) \in D_r\), the unlearning goal is to transform \( M \) into an unlearned model \( M_{\text{unl}} \), with the following goals:

\textbf{Forgets} the content in \( D_f \), i.e., \( M_{\text{unl}}(x_f) \neq y_f \).

\textbf{Retains} the content in \( D_r \), i.e., \( M_{\text{unl}}(x_r) = y_r \).

\textbf{Preserves} its performance on generic tasks and linguistic coherence.

Ideally, \( M_{\text{unl}} \) should behave identically to a model \( M_{\text{ret}} \) (the retrained model) trained only on \( D \setminus D_f \) (the dataset \( D \) excluding the data \( D_f \)).
However, due to the high computational cost of retraining LLMs from scratch, the focus shifts to \textbf{Approximate Unlearning} \citep{eldan2023whosharrypotterapproximate}, where \( M_{\text{unl}} \) approximates the behavior of \( M_{\text{ret}} \) without strict equality.

\subsection{Rethinking Unlearning}
\label{rethinkunlearneval}
Existing unlearning methods, such as GA and NPO, rely on reverse optimization, which often leads to unpredictable outputs.
Furthermore, traditional evaluation metrics for unlearning, such as ROUGE-L Recall and Perplexity (PPL), exhibit significant limitations.
ROUGE-L treats all tokens equally, making it sensitive to output length and superficial wording changes, as evidenced by the NPO example in Figure~\ref{fig:comparison}.
Similarly, PPL, which measures average token probabilities, can be misleadingly low even for poor-quality outputs, as evidenced by the repetitive sequences generated by GA in Figure~\ref{fig:comparison}. 
These shortcomings reveal that current metrics fall short of capturing the overall performance of unlearned models, especially in terms of relevance and fluency.

\begin{figure}[!t]
  \includegraphics[width=\columnwidth]{images/comparison.pdf}
  \vspace{-4ex}
  \caption{Limitations of Existing Metrics: \textbf{ROUGE-L} is susceptible to output length due to treating all tokens equally. \textbf{PPL}'s average token probability can mask quality issues with partial high probability tokens.}
  \vspace{-1ex}
  \label{fig:comparison}
\end{figure}

In practice, effective unlearning should result in a model that behaves as if it were never exposed to the knowledge to be forgotten.
As illustrated in Figure~\ref{fig:comparison}, when queried about forgotten knowledge (e.g., ``How can fans contact Priya Gupta?''), a well-unlearned model should produce relevant but privacy-free responses (e.g., ``Fans can reach out through conventional electronic communication channels.''), rather than nonsensical outputs (e.g., ``at at.'') or sensitive responses (e.g., ``priya.gupta@delhimail.in'').

In conclusion, a robust response after unlearning should satisfy three critical criteria: (a)  \textbf{Forgetting}, (b)  \textbf{Relevance}, and (c)  \textbf{Fluency}.

\subsection{Unlearning Evaluation Metrics}
To address the limitations of existing unlearning metrics, we propose a comprehensive evaluation framework comprising three novel metrics:
Knowledge Forgetting Ratio (KFR), Knowledge Retention Ratio (KRR), and Linguistic Score (LS).

\textbf{KFR and KRR} measure the extent of knowledge forgetting and retention, respectively. 
These metrics are computed using the Entity Coverage Score (ECS) and the Entailment Score (ES), as detailed in the Appendix~\ref{section:metrics}. 
ECS assesses the presence of critical entities in the model's outputs, and ES measures whether the output implies the target knowledge using Natural Language Inference (NLI) \citep{10.1145/3605943}. 
KFR and KRR are formulated as follows:
\vspace{-1.5ex}
\begin{align}
&\text{KFR} = \frac{1}{D} \sum_{i=1}^{D} \mathbb{I}\Big((E_i < c_1) \lor \notag \\
&\quad\big(M_{\text{NLI}}(T^i_{\text{gen}}, T^i_{\text{ref}}) = \text{contradiction}\big)\Big)
\label{eq:kfr}
\end{align}
\vspace{-4ex}
\begin{align}
&\text{KRR} = \frac{1}{D} \sum_{i=1}^{D} \mathbb{I}\Big((E_i > c_2) \land \notag \\ 
&\quad\big(M_{\text{NLI}}(T^i_{\text{ref}}, T^i_{\text{gen}}) \neq \text{contradiction}\big) \Big)
\label{eq:krr}
\end{align}
where, for each instance in the evaluation dataset \(D\), KFR assesses forgetting either when the ECS (\(E_i\)) is below a threshold \(c_1\), or when NLI model \(M_{\text{NLI}}\) detects a contradiction between generated text \(T^i_{\text{gen}}\) and reference text \(T^i_{\text{ref}}\). 
Conversely, KRR evaluates retention when \(E_i > c_2\) and no contradiction is detected between \(T^i_{\text{ref}}\) and \(T^i_{\text{gen}}\).

\textbf{LS} evaluates the linguistic quality of the unlearned model, inspired by cognitive linguistic research on Alzheimer's patients \citep{fraser2016linguistic, heitz-etal-2024-influence}. 
This metric captures linguistic degradation patterns, such as reduced vocabulary diversity, simplified syntax, and diminished lexical richness. 
LS is computed as the harmonic mean of three complementary measures: 
PPL as a baseline, along with Brunet's Index (BI) \citep{brunet1978vocabulaire} and Honore's Statistic (HS) \citep{honore1979simple}, which offer more nuanced cognitive assessments, including vocabulary diversity and lexical richness.
The formulation is as follows: 
%\vspace{-4ex}
\begin{align}
\text{LS} = \  \mathbb{HM} \big ( & \sigma(-\log(\text{PPL})), \notag \\
& \sigma(-\log(\text{BI})), \sigma(\log(\text{HS})) \big )
\end{align}
where \(\sigma\) is the sigmoid function and $\mathbb{HM}$ is the harmonic mean. 
BI and HS are calculated as follows:
\vspace{-2ex}
\begin{equation}
\text{BI} = \frac{1}{D} \sum_{i=1}^{D}N_i^{V_i^{-0.165}}
\end{equation}
\vspace{-1ex}
\begin{equation}
\text{HS} = \frac{1}{D} \sum_{i=1}^{D} \frac{100 \log N_i}{1 - V_1^i/V_i}
\end{equation}
where, for each instance in the evaluation dataset \(D\), \(N_i\) is the word count, \(V_1^i\) is the number of words appearing only once, and \(V_i\) is the total vocabulary size of the text.
Lower BI values indicate greater vocabulary diversity, while higher HS values signify increased lexical richness.
These metrics were selected for their demonstrated sensitivity to linguistic deterioration.

Finally, we employ GPT-4o \citep{openai2024gpt4ocard} to assess \textbf{Fluency} of the output, validating the rationality of our proposed Linguistic Score; 
and to evaluate \textbf{Relevance}, measuring the model's ability to generate contextually appropriate responses while avoiding hallucinations or collapses.