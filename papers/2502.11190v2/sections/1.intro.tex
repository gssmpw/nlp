\section{Introduction}
\label{section:intro}
The widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information \citep{carlini2021extractingtrainingdatalarge,Chen_2024,Lucchi_2024}, poses significant ethical and legal challenges.
Recent developments, such as the New York Times lawsuit against OpenAI \citep{npr2025nytopenai} over unauthorized data usage, have further highlighted these challenges.
To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). 
Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative.

\begin{figure}[!t]
\includegraphics[width=\columnwidth]{images/intro.pdf}
\vspace{-3ex}
\caption{The Probability Seesaw Effect: Reverse optimization methods (GA/NPO) indiscriminately suppress target token probabilities, while ReLearn reconstructs knowledge space via positive optimization.}
\vspace{-3ex}
\label{fig:intro}
\end{figure}

However, existing unlearning methods, such as Gradient Ascent (GA) \citep{ga} and Negative Preference Optimization (NPO) \citep{npo}, raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients \citep{fraser2016linguistic}. 
As illustrated in Figure~\ref{fig:intro}, the core issue with GA and NPO stems from the ``probability seesaw effect'' caused by reverse optimization. 
This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways:
(1) vocabulary collapse (reduced fluency)
and (2) contextual incoherence (diminished relevance). 
Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance. 

To address these issues, we introduce \textbf{ReLearn}, a novel unlearning pipeline that leverages data augmentation and positive optimization.
ReLearn overwrites sensitive information with new authorized knowledge by training the model on augmented data.
This preserves the model's linguistic ability while forgetting target knowledge, akin to human memory updating \citep{Lee2017}. 
Additionally, we introduce a comprehensive evaluation framework comprising three metrics: Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS).
These metrics respectively evaluate knowledge forgetting, retention, and linguistic quality, providing a more holistic evaluation of unlearning performance. 

Our experiments demonstrate that reverse optimization methods (GA and NPO) struggle to balance knowledge forgetting and retention, often producing repetitive and incoherent text. 
Furthermore, they are unstable under varying parameter precision and jailbreak attacks.
In contrast, ReLearn effectively balances forgetting and retention while ensuring robustness against precision variations and jailbreak attacks. 
The ReLearn model retains a general understanding of forgotten questions, enabling it to generate relevant, fluent, and privacy-preserving responses.
Finally, we provide a mechanistic analysis, revealing how reverse optimization methods disrupt the modelâ€™s ability to generate coherent outputs, while ReLearn preserves this capability. 

In summary, our main contributions are:
\begin{itemize}
    \vspace{-1.3ex}
    \item \textbf{Paradigm Innovation}: We introduce ReLearn, a novel unlearning paradigm based on positive optimization.
    \vspace{-2ex}
    \item \textbf{Evaluative Framework}: 
    We propose a comprehensive set of unlearning evaluation metrics to address the limitations in current ROUGE-based and PPL-based metrics. 
    \vspace{-2ex}
    \item \textbf{Mechanistic Insights}: Our analysis reveals the disruptive impact of reverse optimization and highlights the plasticity of ReLearn. 
\end{itemize}