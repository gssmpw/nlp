\section{Related Work}
\paragraph{Unlearning Methods for LLMs.}
LLM unlearning has recently gained significant attention.
Gradient Ascent \citep{ga} maximizes loss for forgetting, while Negative Preference Optimization \citep{npo} draws on Direct Preference Optimization \citep{DPO}.
Various unlearning methods have been proposed \citep{NEURIPS2022_b125999b,eldan2023whosharrypotterapproximate,yu-etal-2023-unlearning,chen2023unlearnwantforgetefficient,pawelczyk2024incontextunlearninglanguagemodels, gandikota2024erasingconceptualknowledgelanguage,liu-etal-2024-towards-safer,seyitoÄŸlu2024extractingunlearnedinformationllms,ding2024unifiedparameterefficientunlearningllms,baluta2024unlearninginvsoutofdistribution, zhuang2024uoeunlearningexpertmixtureofexperts, wei2025underestimatedprivacyrisksminority}.
Another strategy, ``locate-then-unlearn,'' includes Memflex \citep{tian2024forgetnotpracticalknowledge} and SURE \citep{zhang2024doesllmtrulyunlearn}. 
Several data-based methods have also been introduced \citep{jang2022knowledgeunlearningmitigatingprivacy,ma2024unveilingentitylevelunlearninglarge, liu2024learningrefusemitigatingprivacy,gu2024meowmemorysupervisedllm, sinha2024unstarunlearningselftaughtantisample,mekala-etal-2025-alternate}. 
Furthermore, some papers have highlighted the limitations of current machine unlearning \citep{10488864, zhou2024limitationsprospectsmachineunlearning, thaker2024positionllmunlearningbenchmarks, cooper2024machineunlearningdoesntthink, barez2025openproblemsmachineunlearning}.
\paragraph{Unlearning Evaluation for LLMs.}
Most studies \citep{maini2024tofutaskfictitiousunlearning, tian2024forgetnotpracticalknowledge} utilize ROUGE and PPL for evaluating unlearning.
Building upon these metrics, 
\citet{joshi-etal-2024-towards} measure unlearning via benchmark data transformation;
WMDP \citep{pmlr-v235-li24bc} further probes all layers to verify unlearning;
MUSE \citep{shi2024musemachineunlearningsixway} extends evaluation by using Member Inference Attack \citep{kim2024detectingtrainingdatalarge};
RWKU \citep{jin2024rwku} introduces a concept-level unlearning benchmark with adversarial attacks.
Similarly, Unstar \citep{sinha2024unstarunlearningselftaughtantisample} uses GPT scores, and \citet{ma2024benchmarkingvisionlanguagemodel} introduces a vision unlearning benchmark.