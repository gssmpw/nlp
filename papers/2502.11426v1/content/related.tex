\section{Related Work}
\label{sec::related}
In this section, we review the field of off-road autonomy and current evaluation practices to motivate Verti-Bench. 


\subsection{Off-Road Autonomy}
Starting from the DARPA LAGR program~\cite{jackel2006darpa}, roboticists have started developing autonomous systems to operate in off-road enviornments. Compared to indoor or on-road operations, off-road environments pose significant challenges during the entire sense-plan-act loop: Robot state estimation systems, such as visual inertial odometry~\cite{forster2016manifold} and simultaneous localization and mapping~\cite{stachniss2016simultaneous}, are easily affected by the unstructured visual features from the off-road environments as well as noisy inertial signals caused by the extensive vehicle vibrations; Perception is more than a geometric problem, i.e., free vs. obstacle spaces, as in indoor or on-road settings, and requires the consideration of semantics, e.g., gravel vs. grass. Both terrain geometry and semantics need to be represented~\cite{miki2022elevation, ewen2022these, dashora2022hybrid, sikand2022visual} for downstream planning and control tasks; Off-road planners and controllers need to reason beyond collision avoidance and consider factors such as vehicle stability~\cite{pokhrel2024cahsor, bae2021curriculum}, wheel slippage~\cite{siva2019robot, sharma2023ramp, siva2022nauts}, and terrain traversability~\cite{castro2023does, cai2024evora, seo2023learning, fan2021step}, oftentimes in $\mathbb{SE}(3)$ instead of $\mathbb{SE}(2)$~\cite{datar2024toward} due to the uneven off-road terrain surfaces~\cite{lee2023learning, datar2024toward}. 
The recent increase in interest in off-road autonomy~\cite{xiao2022motion} necessitates standard benchmarks to objectively and quantitatively evaluate research progress from the entire community, which is the motivation behind Verti-Bench. 

\subsection{Evaluating Off-Road Perception}
Off-road perception research still dominates the majority of the body of off-road autonomy work~\cite{xiao2022motion}. Fortunately, evaluating off-road perception can be mostly carried out on pre-collected, static datasets in a vehicle-agnostic fashion. 
For both evaluation and training of perception systems in a data-driven manner, a variety of off-road perception datasets are available for research in semantics segmentation~\cite{wigness2019rugd, jiang2021rellis, mortimer2024goose}, freespace detection~\cite{min2022orfd}, place recognition~\cite{knights2023wild}, traversability estimation~\cite{sharma2022cat}, and map reconstruction~\cite{liu2024botanicgarden}. Given a new off-road perception system taking the recorded sensor data and robot actions as inputs, its outputs can be simply compared against the ground truth labels provided by those datasets. A performance metric, e.g., segmentation accuracy, detection rate, recognition count, traversability correctness, and reconstruction precision, can then be computed to quantify how well the new perception system compares against others. Unfortunately, off-road mobility evaluation cannot be conducted on such pre-collected, static datasets. Notice that while off-road dynamics datasets~\cite{triest2022tartandrive} can be used to learn off-road navigation models, they cannot be used to evaluate off-road mobility.  


\subsection{Evaluating Off-Road Mobility}
To the best of our knowledge, no standard off-road mobility benchmarks currently exist in the literature. 
% in which those variations can be mitigated by a standard dataset
Unlike off-road perception evaluation, mobility evaluation requires the vehicle-terrain interactions to be unfolded, since a different action will lead the robot into a different state absent from the dataset. Such a distribution shift necessitates mobility evaluation to be based on the unfolded, not pre-collected, vehicle trajectories.

To unfold vehicle-terrain interactions for objective off-road mobility evaluation, the vehicle platforms and terrain properties are required to be standardized, which, unfortunately, vary significantly across different research groups: Robots of different sizes, weights, actuation mechanisms (e.g., wheeled, tracked, steered, and differential-driven) are used to compare new mobility systems with existing ones. The latter is usually customized to fit for a different robot, potentially causing misinterpretation of implementation details; Different simulators, e.g., Gazebo~\cite{rana2024towards}, IsaacGym~\cite{yu2024adaptive}, Unreal~\cite{young2020unreal}, and Unity~\cite{so2022sim}, are leveraged to train and evaluate off-road mobility systems. To improve simulation speed, most of those simulators do not focus on physics fidelity, which is crucial for off-road mobility on extremely rugged and deformable surfaces; In the real world, small-scale indoor testbeds have been set up with foam, rocks, plywood, and pipes to emulate vertically challenging terrain in the wild~\cite{xu2023efficient, datar2024toward, xiao2018review, xiao2015locomotive}; Enclosed outdoor off-road tracks have been constructed to evaluate aggressive autonomous driving~\cite{goldfain2019autorally, xiao2021learning, atreya2022high, karnan2022vi, pan2020imitation}; A few research groups have access to large-scale off-road testing facilities with full-size vehicles~\cite{triest2022tartandrive, han2023model, jiang2021rellis}. 

With such diverse setups, an objective evaluation and comparison across different off-road mobility systems become infeasible. Verti-Bench, based on a high-fidelity multi-physics dynamics simulator, is hence motivated to fill such a gap of missing standard off-road mobility benchmarks. Notice that Verti-Bench is not meant to replace existing evaluation setups, but to complement them with a new standardized option to facilitate fair comparison across off-road mobility systems. 


% mirsky2021conflict, karnan2022socially, xiao2022learning, francis2023principles, park2023learning, nguyen2023toward, xiao2020appld, wang2021appli, wang2021apple, xu2021applr, xiao2022appl, xu2021machine, xiao2021toward, xiao2021agile, wang2021agile, liu2021lifelong, xu2023benchmarking, karnan2022voila


