\section{Related Work}
\paragraph{Self-supervised learning for molecules and crystals}

GNNs have significantly impacted material property predictions **Wang et al., "Reducing Labeling Cost through Contrastive Learning"** by providing an alternative way for complex quantum chemical computations. However, the costly labeling process requiring expensive quantum chemical calculations is the bottleneck in GNN application. To address this challenge, various self-supervised learning (SSL) methods have been proposed. **Koker et al., "Contrastive Learning for Periodic Crystals"** reduced the need for expensive quantum chemical labeling by applying contrastive learning to unlabeled 10 million molecules using molecular GNNs. **Magar et al., "Barlow Twins for Self-Supervised Learning"** emphasized the importance of augmentation techniques when applying contrastive learning to encoding periodic crystals as graphs with CGCNNs as encoders. **Koker et al., "Contrastive Learning for Periodic Crystals"**, **Magar et al., "Barlow Twins for Self-Supervised Learning"** improved the performance of CGCNNs by applying Barlow twins ____, a self-supervised learning method that does not require negative pairs.

\paragraph{Multimodal learning for molecules}
In the molecular domain, performing multimodal contrastive learning between graphs and text has been demonstrated to improve representation ability by encoding knowledge from an unstructured domain-specific corpus. **Zeng et al., "Graph-Text Contrastive Learning for Molecules"** proposed to encode both SMILES ____, representation and text corpus of molecules simultaneously using a BERT ____ model as backbone. To train the proposed model, they extracted a corpus from the semantic scholar ____, a database of over 136M published scientific literature, to construct 10k molecule-text pairs. **Su et al., "Contrastive Learning for Graph-Text Molecules"** proposed to apply the contrastive learning paradigm with distinguishing graph and text branches. To overcome limited training data, authors utilized pre-trained models in the biochemical molecular domain for both GNN, GraphCL ____, and text encoder, SciBERT ____. **Liu et al., "Pretraining for Multimodal Learning"** pre-trained their model on a larger training set of 280k pairs, and both previous studies enabled zero-shot tasks and showed improved performance compared to unimodal models on downstream tasks.

\paragraph{Multimodal learning for crystals}
Multimodal learning in crystalline materials has not been explored extensively. **Das et al., "CrysMMNet: Multimodal Learning for Crystals"** proposed CrysMMNet, which simultaneously integrates graph inputs and text inputs describing the structure of materials generated by a Robocrystallographer ____. However, this approach focuses on creating joint representations and using them only to predict material properties and does not address issues related to zero-shot tasks and downstream tasks using natural language.