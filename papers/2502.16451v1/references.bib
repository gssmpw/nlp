
@misc{park2023CLaMPcodebase,
  author = {Park, Yang Jeong and Li, Ju},
  title = {CLaMP codebase},
  year = {2024},
  publisher = {Figshare},
  journal = {Figshare repository},
  howpublished = {\url{https://doi.org/10.6084/m9.figshare.25205201}},
}

@misc{park2023CLaMPcodebase,
  author = {Park, Yang Jeong and Kim, HyunGi and Jo, Jeonghee and Yoon, Sungroh},
  title = {CLaMP codebase},
  year = {2024},
  publisher = {Figshare},
  journal = {Figshare repository},
  howpublished = {\url{https://doi.org/10.6084/m9.figshare.25205201}},
}

@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}
@InProceedings{shrivastava2023clip-lite,
  title = 	 {CLIP-Lite: Information Efficient Visual Representation Learning with Language Supervision},
  author =       {Shrivastava, Aman and Selvaraju, Ramprasaath R. and Naik, Nikhil and Ordonez, Vicente},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {8433--8447},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/shrivastava23a/shrivastava23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/shrivastava23a.html},
  abstract = 	 {We propose CLIP-Lite, an information efficient method for visual representation learning by feature alignment with textual annotations. Compared to the previously proposed CLIP model, CLIP-Lite requires only one negative image-text sample pair for every positive image-text sample during the optimization of its contrastive learning objective. We accomplish this by taking advantage of an information efficient lower-bound to maximize the mutual information between the two input modalities. This allows CLIP-Lite to be trained with significantly reduced amounts of data and batch sizes while obtaining better performance than CLIP at the same scale. We evaluate CLIP-Lite by pretraining on the COCO-Captions dataset and testing transfer learning to other datasets. CLIP-Lite obtains a +14.0$%$ mAP absolute gain in performance on Pascal VOC classification, and a +22.1$%$ top-1 accuracy gain on ImageNet, while being comparable or superior to other, more complex, text-supervised models. CLIP-Lite is also superior to CLIP on image and text retrieval, zero-shot classification, and visual grounding. Finally, we show that CLIP-Lite can leverage language semantics to encourage bias-free visual representations that can be used in downstream tasks. Implementation: https://github.com/4m4n5/CLIP-Lite}
}
@InProceedings{schutt2021painn,
  title = 	 {Equivariant message passing for the prediction of tensorial properties and molecular spectra},
  author =       {Sch{\"u}tt, Kristof and Unke, Oliver and Gastegger, Michael},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9377--9388},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/schutt21a/schutt21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/schutt21a.html},
  abstract = 	 {Message passing neural networks have become a method of choice for learning on graphs, in particular the prediction of chemical properties and the acceleration of molecular dynamics studies. While they readily scale to large training data sets, previous approaches have proven to be less data efficient than kernel methods. We identify limitations of invariant representations as a major reason and extend the message passing formulation to rotationally equivariant representations. On this basis, we propose the polarizable atom interaction neural network (PaiNN) and improve on common molecule benchmarks over previous networks, while reducing model size and inference time. We leverage the equivariant atomwise representations obtained by PaiNN for the prediction of tensorial properties. Finally, we apply this to the simulation of molecular spectra, achieving speedups of 4-5 orders of magnitude compared to the electronic structure reference.}
}
@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
}
@article{choudhary2020JARVIS,
  title={The joint automated repository for various integrated simulations (JARVIS) for data-driven materials design},
  author={Choudhary, Kamal and Garrity, Kevin F and Reid, Andrew CE and DeCost, Brian and Biacchi, Adam J and Hight Walker, Angela R and Trautt, Zachary and Hattrick-Simpers, Jason and Kusne, A Gilad and Centrone, Andrea and others},
  journal={npj computational materials},
  volume={6},
  number={1},
  pages={173},
  year={2020},
  publisher={Nature Publishing Group UK London}
}
@article{jain2013MaterialsProject,
  title={The materials project: A materials genome approach to accelerating materials innovation, APL Mater},
  author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and others},
  journal={APL Materials},
  year={2013}
}
@article{park2023materialsnarratives,
  title={1.5 million materials narratives generated by chatbots},
  author={Park, Yang Jeong and Jerng, Sung Eun and Park, Jin-Sung and Kwon, Choah and Hsu, Chia-Wei and Ren, Zhichu and Yoon, Sungroh and Li, Ju},
  journal={arXiv preprint arXiv:2308.13687},
  year={2023}
}
@article{gupta2022matscibert,
  title={MatSciBERT: A materials domain language model for text mining and information extraction},
  author={Gupta, Tanishq and Zaki, Mohd and Krishnan, NM Anoop and Mausam},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={102},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{venugopal2021looking,
  title={Looking through glass: Knowledge discovery from materials science literature using natural language processing},
  author={Venugopal, Vineeth and Sahoo, Sourav and Zaki, Mohd and Agarwal, Manish and Gosvami, Nitya Nand and Krishnan, NM Anoop},
  journal={Patterns},
  volume={2},
  number={7},
  year={2021},
  publisher={Elsevier}
}
@article{you2020GraphCL,
  title={Graph contrastive learning with augmentations},
  author={You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5812--5823},
  year={2020}
}
@article{koker2022CrystalCLR,
  title={Graph Contrastive Learning for Materials},
  author={Koker, Teddy and Quigley, Keegan and Spaeth, Will and Frey, Nathan C and Li, Lin},
  journal={arXiv preprint arXiv:2211.13408},
  year={2022}
}
@article{xie2018CGCNN,
  title={Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties},
  author={Xie, Tian and Grossman, Jeffrey C},
  journal={Physical review letters},
  volume={120},
  number={14},
  pages={145301},
  year={2018},
  publisher={APS}
}
@article{choudhary2021ALIGNN,
  title={Atomistic line graph neural network for improved materials property predictions},
  author={Choudhary, Kamal and DeCost, Brian},
  journal={npj Computational Materials},
  volume={7},
  number={1},
  pages={185},
  year={2021},
  publisher={Nature Publishing Group UK London}
}
@InProceedings{radford2021CLIP,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}
@article{devlin2018BERT,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{li2021DeCLIP,
  title={Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm},
  author={Li, Yangguang and Liang, Feng and Zhao, Lichen and Cui, Yufeng and Ouyang, Wanli and Shao, Jing and Yu, Fengwei and Yan, Junjie},
  journal={arXiv preprint arXiv:2110.05208},
  year={2021}
}
@article{su2022MoMu,
  title={A molecular multimodal foundation model associating molecule graphs with natural language},
  author={Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Yujie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2209.05481},
  year={2022}
}
@article{magar2022CrystalTwins,
  title={Crystal twins: self-supervised learning for crystalline material property prediction},
  author={Magar, Rishikesh and Wang, Yuyang and Barati Farimani, Amir},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={231},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{wang2022MolCLR,
  title={Molecular contrastive learning of representations via graph neural networks},
  author={Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={279--287},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{zeng2022ImageMol,
  title={Accurate prediction of molecular properties and drug targets using a self-supervised image representation learning framework},
  author={Zeng, Xiangxiang and Xiang, Hongxin and Yu, Linhui and Wang, Jianmin and Li, Kenli and Nussinov, Ruth and Cheng, Feixiong},
  journal={Nature Machine Intelligence},
  volume={4},
  number={11},
  pages={1004--1016},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{fang2022GEM,
  title={Geometry-enhanced molecular representation learning for property prediction},
  author={Fang, Xiaomin and Liu, Lihang and Lei, Jieqiong and He, Donglong and Zhang, Shanzhuo and Zhou, Jingbo and Wang, Fan and Wu, Hua and Wang, Haifeng},
  journal={Nature Machine Intelligence},
  volume={4},
  number={2},
  pages={127--134},
  year={2022},
  publisher={Nature Publishing Group}
}
@article{walker2021SOFC-slot,
  title={The impact of domain-specific pre-training on named entity recognition tasks in materials science},
  author={Walker, Nicholas and Trewartha, Amalie and Huo, Haoyan and Lee, Sanghoon and Cruse, Kevin and Dagdelen, John and Dunn, Alexander and Persson, Kristin and Ceder, Gerbrand and Jain, Anubhav},
  journal={Available at SSRN 3950755},
  year={2021}
}
@article{xie2021CDVAE,
  title={Crystal diffusion variational autoencoder for periodic material generation},
  author={Xie, Tian and Fu, Xiang and Ganea, Octavian-Eugen and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2110.06197},
  year={2021}
}
@article{lyngby2022CDVAE2Ddiscovery,
  title={Data-driven discovery of 2D materials by deep generative models},
  author={Lyngby, Peder and Thygesen, Kristian Sommer},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={232},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{hoffmann2019Voxel-VAE,
  title={Data-driven approach to encoding and decoding 3-d crystal structures},
  author={Hoffmann, Jordan and Maestrati, Louis and Sawada, Yoshihide and Tang, Jian and Sellier, Jean Michel and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1909.00949},
  year={2019}
}
@article{noh2019inverseMatter,
  title={Inverse design of solid-state materials via a continuous representation},
  author={Noh, Juhwan and Kim, Jaehoon and Stein, Helge S and Sanchez-Lengeling, Benjamin and Gregoire, John M and Aspuru-Guzik, Alan and Jung, Yousung},
  journal={Matter},
  volume={1},
  number={5},
  pages={1370--1384},
  year={2019},
  publisher={Elsevier}
}
@article{court2020Cond-DFC-VAE,
  title={3-D inorganic crystal structure generation and property prediction via representation learning},
  author={Court, Callum J and Yildirim, Batuhan and Jain, Apoorv and Cole, Jacqueline M},
  journal={Journal of Chemical Information and Modeling},
  volume={60},
  number={10},
  pages={4518--4535},
  year={2020},
  publisher={ACS Publications}
}
@article{pakornchote2024dpcdvae,
  title={Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling},
  author={Pakornchote, Teerachote and Choomphon-Anomakhun, Natthaphon and Arrerut, Sorrjit and Atthapak, Chayanon and Khamkaeo, Sakarn and Chotibut, Thiparat and Bovornratanaraks, Thiti},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={1275},
  year={2024},
  publisher={Nature Publishing Group UK London}
}
@article{gong2023GNNforcrystal,
  title={Examining graph neural networks for crystal structures: limitations and opportunities for capturing periodicity},
  author={Gong, Sheng and Yan, Keqiang and Xie, Tian and Shao-Horn, Yang and Gomez-Bombarelli, Rafael and Ji, Shuiwang and Grossman, Jeffrey C},
  journal={Science Advances},
  volume={9},
  number={45},
  pages={eadi3245},
  year={2023},
  publisher={American Association for the Advancement of Science}
}
@article{liu2023MoleculeSTM,
  title={Multi-modal molecule structure--text model for text-based retrieval and editing},
  author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Animashree},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1447--1457},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{kaundinya2022predictionALIGNN,
  title={Prediction of the electron density of states for crystalline compounds with Atomistic Line Graph Neural Networks (ALIGNN)},
  author={Kaundinya, Prathik R and Choudhary, Kamal and Kalidindi, Surya R},
  journal={arXiv preprint arXiv:2201.08348},
  year={2022}
}
@inproceedings{wang2019deep,
  title={Deep graph library: Towards efficient and scalable deep learning on graphs},
  author={Wang, Minjie Yu},
  booktitle={ICLR workshop on representation learning on graphs and manifolds},
  year={2019}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{satorras2021EGNN,
  title={E (n) equivariant graph neural networks},
  author={Satorras, V{\i}́ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={9323--9332},
  year={2021},
  organization={PMLR}
}
@InProceedings{gilmer2017MPNN,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}
@article{xu2018GIN,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}
@article{unke2019physnet,
  title={PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges},
  author={Unke, Oliver T and Meuwly, Markus},
  journal={Journal of chemical theory and computation},
  volume={15},
  number={6},
  pages={3678--3693},
  year={2019},
  publisher={ACS Publications}
}
@article{schutt2018schnet,
  title={Schnet--a deep learning architecture for molecules and materials},
  author={Sch{\"u}tt, Kristof T and Sauceda, Huziel E and Kindermans, P-J and Tkatchenko, Alexandre and M{\"u}ller, K-R},
  journal={The Journal of Chemical Physics},
  volume={148},
  number={24},
  pages={241722},
  year={2018},
  publisher={AIP Publishing LLC}
}
@article{kim2017VirtualScreening,
  title={Virtual screening of inorganic materials synthesis parameters with deep learning},
  author={Kim, Edward and Huang, Kevin and Jegelka, Stefanie and Olivetti, Elsa},
  journal={npj Computational Materials},
  volume={3},
  number={1},
  pages={53},
  year={2017},
  publisher={Nature Publishing Group UK London}
}
@article{sorkun2020AI-VirtualScreening,
  title={An artificial intelligence-aided virtual screening recipe for two-dimensional materials discovery},
  author={Sorkun, Murat Cihan and Astruc, S{\'e}verin and Koelman, JM Vianney A and Er, S{\"u}leyman},
  journal={npj Computational Materials},
  volume={6},
  number={1},
  pages={106},
  year={2020},
  publisher={Nature Publishing Group UK London}
}
@article{lookman2019ALinMaterials,
  title={Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design},
  author={Lookman, Turab and Balachandran, Prasanna V and Xue, Dezhen and Yuan, Ruihao},
  journal={npj Computational Materials},
  volume={5},
  number={1},
  pages={21},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@article{chen2020GenerativeInverseDesignUsingAL,
  title={Generative deep neural networks for inverse materials design using backpropagation and active learning},
  author={Chen, Chun-Teh and Gu, Grace X},
  journal={Advanced Science},
  volume={7},
  number={5},
  pages={1902607},
  year={2020},
  publisher={Wiley Online Library}
}
@article{belsky2002ICSD,
  title={New developments in the Inorganic Crystal Structure Database (ICSD): accessibility in support of materials research and design},
  author={Belsky, Alec and Hellenbrandt, Mariette and Karen, Vicky Lynn and Luksch, Peter},
  journal={Acta Crystallographica Section B: Structural Science},
  volume={58},
  number={3},
  pages={364--369},
  year={2002},
  publisher={International Union of Crystallography}
}
@article{irwin2005zinc,
  title={ZINC- a free database of commercially available compounds for virtual screening},
  author={Irwin, John J and Shoichet, Brian K},
  journal={Journal of chemical information and modeling},
  volume={45},
  number={1},
  pages={177--182},
  year={2005},
  publisher={ACS Publications}
}
@InProceedings{das2023crysmmnet,
  title = 	 {{CrysMMNet}: Multimodal Representation for Crystal Property Prediction},
  author =       {Das, Kishalay and Goyal, Pawan and Lee, Seung-Cheol and Bhattacharjee, Satadeep and Ganguly, Niloy},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {507--517},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/das23a/das23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/das23a.html},
  abstract = 	 {Machine Learning models have emerged as a powerful tool for fast and accurate prediction of different crystalline properties. Exiting state-of-the-art models rely on a single modality of crystal data i.e crystal graph structure, where they construct multi-graph by establishing edges between nearby atoms in 3D space and apply GNN to learn materials representation. Thereby, they encode local chemical semantics around the atoms successfully but fail to capture important global periodic structural information like space group number, crystal symmetry, rotational information etc, which influence different crystal properties.  In this work, we leverage textual descriptions of materials to model global structural information into graph structure and learn a more robust and enriched representation of crystalline materials. To this effect, we first curate a textual dataset for crystalline material databases containing descriptions of each material. Further, we propose CrysMMNet, a simple multi-modal framework, which fuses both structural and textual representation together to generate a joint multimodal representation of crystalline materials. We conduct extensive experiments on two benchmark datasets across ten different properties to show that CrysMMNet outperforms existing state-of-the-art baseline methods with a good margin. We also observe that fusing the textual representation with crystal graph structure provides consistent improvement for all the SOTA GNN models compared to their own vanilla versions. We have shared the textual dataset, that we have curated for both the benchmark material databases, with the community for future use..}
}
@article{huang2023MatInformer,
  title={Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction},
  author={Huang, Hongshuo and Magar, Rishikesh and Xu, Changwen and Farimani, Amir Bariti},
  journal={arXiv preprint arXiv:2308.16259},
  year={2023}
}
@InProceedings{zbontar2021barlowtwins,
  title = 	 {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author =       {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12310--12320},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zbontar21a.html},
  abstract = 	 {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.}
}
@article{lo2019s2orc,
  title={S2ORC: The semantic scholar open research corpus},
  author={Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Dan S},
  journal={arXiv preprint arXiv:1911.02782},
  year={2019}
}
@article{zeng2022KV-PLM,
  title={A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals},
  author={Zeng, Zheni and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={862},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{weininger1988smiles,
  title={SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
  author={Weininger, David},
  journal={Journal of chemical information and computer sciences},
  volume={28},
  number={1},
  pages={31--36},
  year={1988},
  publisher={ACS Publications}
}
@article{weininger1989smiles,
  title={SMILES. 2. Algorithm for generation of unique SMILES notation},
  author={Weininger, David and Weininger, Arthur and Weininger, Joseph L},
  journal={Journal of chemical information and computer sciences},
  volume={29},
  number={2},
  pages={97--101},
  year={1989},
  publisher={ACS Publications}
}
@article{ganose2019robocrystallographer,
  title={Robocrystallographer: automated crystal structure text descriptions and analysis},
  author={Ganose, Alex M and Jain, Anubhav},
  journal={MRS Communications},
  volume={9},
  number={3},
  pages={874--881},
  year={2019},
  publisher={Cambridge University Press}
}
@article{gasteiger2020DimeNet,
  title={Directional message passing for molecular graphs},
  author={Gasteiger, Johannes and Gro{\ss}, Janek and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2003.03123},
  year={2020}
}
@article{gasteiger2020DimeNet++,
  title={Fast and uncertainty-aware directional message passing for non-equilibrium molecules},
  author={Gasteiger, Johannes and Giri, Shankari and Margraf, Johannes T and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2011.14115},
  year={2020}
}
@article{agrawal2016FourthParadigmPerspective,
  title={Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science},
  author={Agrawal, Ankit and Choudhary, Alok},
  journal={Apl Materials},
  volume={4},
  number={5},
  year={2016},
  publisher={AIP Publishing}
}
@article{butler2018MLforMolecularandMaterialsScience,
  title={Machine learning for molecular and materials science},
  author={Butler, Keith T and Davies, Daniel W and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
  journal={Nature},
  volume={559},
  number={7715},
  pages={547--555},
  year={2018},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{mihindukulasooriya2023text2kgbench,
  title={Text2kgbench: A benchmark for ontology-driven knowledge graph generation from text},
  author={Mihindukulasooriya, Nandana and Tiwari, Sanju and Enguix, Carlos F and Lata, Kusum},
  booktitle={International Semantic Web Conference},
  pages={247--265},
  year={2023},
  organization={Springer}
}
@article{kim2023DDelect_design,
  title={Data-driven electrolyte design for lithium metal anodes},
  author={Kim, Sang Cheol and Oyakhire, Solomon T and Athanitis, Constantine and Wang, Jingyang and Zhang, Zewen and Zhang, Wenbo and Boyle, David T and Kim, Mun Sek and Yu, Zhiao and Gao, Xin and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={10},
  pages={e2214357120},
  year={2023},
  publisher={National Acad Sciences}
}
@article{yuan2018ALPiezoelectricBaTiO3,
  title={Accelerated discovery of large electrostrains in BaTiO3-based piezoelectrics using active learning},
  author={Yuan, Ruihao and Liu, Zhen and Balachandran, Prasanna V and Xue, Deqing and Zhou, Yumei and Ding, Xiangdong and Sun, Jun and Xue, Dezhen and Lookman, Turab},
  journal={Advanced materials},
  volume={30},
  number={7},
  pages={1702884},
  year={2018},
  publisher={Wiley Online Library}
}
@article{white2023FutureOfChemistryLanguage,
  title={The future of chemistry is language},
  author={White, Andrew D},
  journal={Nature Reviews Chemistry},
  pages={1--2},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{berrios2023LENS,
  title={Towards language models that can see: Computer vision through the lens of natural language},
  author={Berrios, William and Mittal, Gautam and Thrush, Tristan and Kiela, Douwe and Singh, Amanpreet},
  journal={arXiv preprint arXiv:2306.16410},
  year={2023}
}
@article{chen2023TSMixer,
  title={Tsmixer: An all-mlp architecture for time series forecasting},
  author={Chen, Si-An and Li, Chun-Liang and Yoder, Nate and Arik, Sercan O and Pfister, Tomas},
  journal={arXiv preprint arXiv:2303.06053},
  year={2023}
}
@inproceedings{kolesnikov2019SSLVision,
  title={Revisiting self-supervised visual representation learning},
  author={Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
  editor={Davis, Larry and Torr, Philip and Zhu, Song-Chun},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1920--1929},
  year={2019}
}
@article{radford2019GPT2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{rombach2022StableDiffusion,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  editor={Chellappa, Rama and Matas, Jiri and Quan, Long and Shah, Mubarak},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}
@article{liu2023llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}
@article{li2023llava-med,
  title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
  author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2306.00890},
  year={2023}
}
@InProceedings{ramesh2021Dalle,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}
@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}


@misc {park2024GPTNarrativesHF,
	author       = { {Yang Jeong Park} },
	title        = { {{GPT-Narratives-for-Materials (Revision d56c661). Hugging Face.}} },
	year         = 2024,
	url          = { https://huggingface.co/datasets/yjeong/GPT-Narratives-for-Materials },
	doi          = { https://doi.org/10.57967/hf/2770 },
	publisher    = { Hugging Face }
}