@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
}

@InProceedings{das2023crysmmnet,
  title = 	 {{CrysMMNet}: Multimodal Representation for Crystal Property Prediction},
  author =       {Das, Kishalay and Goyal, Pawan and Lee, Seung-Cheol and Bhattacharjee, Satadeep and Ganguly, Niloy},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {507--517},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/das23a/das23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/das23a.html},
  abstract = 	 {Machine Learning models have emerged as a powerful tool for fast and accurate prediction of different crystalline properties. Exiting state-of-the-art models rely on a single modality of crystal data i.e crystal graph structure, where they construct multi-graph by establishing edges between nearby atoms in 3D space and apply GNN to learn materials representation. Thereby, they encode local chemical semantics around the atoms successfully but fail to capture important global periodic structural information like space group number, crystal symmetry, rotational information etc, which influence different crystal properties.  In this work, we leverage textual descriptions of materials to model global structural information into graph structure and learn a more robust and enriched representation of crystalline materials. To this effect, we first curate a textual dataset for crystalline material databases containing descriptions of each material. Further, we propose CrysMMNet, a simple multi-modal framework, which fuses both structural and textual representation together to generate a joint multimodal representation of crystalline materials. We conduct extensive experiments on two benchmark datasets across ten different properties to show that CrysMMNet outperforms existing state-of-the-art baseline methods with a good margin. We also observe that fusing the textual representation with crystal graph structure provides consistent improvement for all the SOTA GNN models compared to their own vanilla versions. We have shared the textual dataset, that we have curated for both the benchmark material databases, with the community for future use..}
}

@article{devlin2018BERT,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{ganose2019robocrystallographer,
  title={Robocrystallographer: automated crystal structure text descriptions and analysis},
  author={Ganose, Alex M and Jain, Anubhav},
  journal={MRS Communications},
  volume={9},
  number={3},
  pages={874--881},
  year={2019},
  publisher={Cambridge University Press}
}

@InProceedings{gilmer2017MPNN,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}

@article{koker2022CrystalCLR,
  title={Graph Contrastive Learning for Materials},
  author={Koker, Teddy and Quigley, Keegan and Spaeth, Will and Frey, Nathan C and Li, Lin},
  journal={arXiv preprint arXiv:2211.13408},
  year={2022}
}

@article{liu2023MoleculeSTM,
  title={Multi-modal molecule structure--text model for text-based retrieval and editing},
  author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Animashree},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1447--1457},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{lo2019s2orc,
  title={S2ORC: The semantic scholar open research corpus},
  author={Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Dan S},
  journal={arXiv preprint arXiv:1911.02782},
  year={2019}
}

@article{magar2022CrystalTwins,
  title={Crystal twins: self-supervised learning for crystalline material property prediction},
  author={Magar, Rishikesh and Wang, Yuyang and Barati Farimani, Amir},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={231},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{su2022MoMu,
  title={A molecular multimodal foundation model associating molecule graphs with natural language},
  author={Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Yujie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2209.05481},
  year={2022}
}

@article{wang2022MolCLR,
  title={Molecular contrastive learning of representations via graph neural networks},
  author={Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={279--287},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{weininger1988smiles,
  title={SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
  author={Weininger, David},
  journal={Journal of chemical information and computer sciences},
  volume={28},
  number={1},
  pages={31--36},
  year={1988},
  publisher={ACS Publications}
}

@article{weininger1989smiles,
  title={SMILES. 2. Algorithm for generation of unique SMILES notation},
  author={Weininger, David and Weininger, Arthur and Weininger, Joseph L},
  journal={Journal of chemical information and computer sciences},
  volume={29},
  number={2},
  pages={97--101},
  year={1989},
  publisher={ACS Publications}
}

@article{xie2018CGCNN,
  title={Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties},
  author={Xie, Tian and Grossman, Jeffrey C},
  journal={Physical review letters},
  volume={120},
  number={14},
  pages={145301},
  year={2018},
  publisher={APS}
}

@article{you2020GraphCL,
  title={Graph contrastive learning with augmentations},
  author={You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5812--5823},
  year={2020}
}

@InProceedings{zbontar2021barlowtwins,
  title = 	 {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author =       {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12310--12320},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zbontar21a.html},
  abstract = 	 {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlowâ€™s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.}
}

@article{zeng2022KV-PLM,
  title={A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals},
  author={Zeng, Zheni and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={862},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

