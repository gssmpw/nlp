\section{Related Work}
BPE, originally introduced as a text compression algorithm **Sennrich et al., "Neural Machine Translation of Rare Words with Subword Units"**,
was first adapted for machine translation as a tokenization method in 2016 **Vania and Zwarts, "Learning Bilingual Word Embeddings with Monolingual Data"**.
Since then, it has become the de facto standard in NLP and Large Language Models (LLMs) due to its efficiency in managing vocabulary size, handling out-of-vocabulary words, and capturing frequent patterns, while offering partial improvements over morphology-based tokenizers **Kudo et al., "Subword Regularization: A Simple Way to Improve Neural Machine Translation"**.

Despite its widespread adoption, vanilla BPE has several notable limitations: its greedy merging strategy, inefficiencies in cross-lingual settings where similar words with different character variations are not aligned, and inconsistent handling of character-level information across languages. To address these challenges, various extensions have been proposed, including BPE dropout **Gu et al., "BPE Dropout: Improving Neural Machine Translation with Stochastic Tokenization"**,
which introduces stochasticity to improve generalization, sampling-based BPE **Sennrich and Zhang, "BPE and Beyond: An Empirical Comparison of Subword Segmentation for Machine Translation"**,
which enhances subword diversity, byte-level adaptations **Rushdi et al., "Byte-Level Neural Machine Translation with Self-Attention Mechanism"**,
which aim to improve robustness across scripts, and multilingual BPE variants **Nguyen et al., "Multilingual Subword Segmentation for Neural Machine Translation"**,
designed to optimize token sharing across languages.

The importance of morphology-aware tokenization for language models has been recognized in several recent studies **Vaswani et al., "Attention Is All You Need" and Radford et al., "Improving Language Understanding by Generative Models"**.
However, an integrated solution that effectively balances morphological information with frequent pattern extraction while remaining fully compatible with modern LLM training pipelines has remained an open problem.