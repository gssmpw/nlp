\section{Related Work}
BPE, originally introduced as a text compression algorithm \citep{shibata1999byte}, was first adapted for machine translation as a tokenization method in 2016 \citep{sennrich-etal-2016-neural}. Since then, it has become the de facto standard in NLP and Large Language Models (LLMs) due to its efficiency in managing vocabulary size, handling out-of-vocabulary words, and capturing frequent patterns, while offering partial improvements over morphology-based tokenizers \citep{sennrich-etal-2016-neural}.  

Despite its widespread adoption, vanilla BPE has several notable limitations: its greedy merging strategy, inefficiencies in cross-lingual settings where similar words with different character variations are not aligned, and inconsistent handling of character-level information across languages. To address these challenges, various extensions have been proposed, including BPE dropout \citep{provilkov-etal-2020-bpe}, which introduces stochasticity to improve generalization, sampling-based BPE \citep{asgari2019probabilistic, asgari2020subword}, which enhances subword diversity, byte-level adaptations \citep{wang2020neural}, which aim to improve robustness across scripts, and multilingual BPE variants \citep{liang-etal-2023-xlm}, designed to optimize token sharing across languages.  

The importance of morphology-aware tokenization for language models has been recognized in several recent studies \citep{park-etal-2021-morphology, jabbar2023morphpiece, marco-fraser-2024-subword, weller-di-marco-fraser-2024-analyzing}. However, an integrated solution that effectively balances morphological information with frequent pattern extraction while remaining fully compatible with modern LLM training pipelines has remained an open problem.