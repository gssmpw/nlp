\section{Related Work}
BPE, originally introduced as a text compression algorithm ____, was first adapted for machine translation as a tokenization method in 2016 ____. Since then, it has become the de facto standard in NLP and Large Language Models (LLMs) due to its efficiency in managing vocabulary size, handling out-of-vocabulary words, and capturing frequent patterns, while offering partial improvements over morphology-based tokenizers ____.  

Despite its widespread adoption, vanilla BPE has several notable limitations: its greedy merging strategy, inefficiencies in cross-lingual settings where similar words with different character variations are not aligned, and inconsistent handling of character-level information across languages. To address these challenges, various extensions have been proposed, including BPE dropout ____, which introduces stochasticity to improve generalization, sampling-based BPE ____, which enhances subword diversity, byte-level adaptations ____, which aim to improve robustness across scripts, and multilingual BPE variants ____, designed to optimize token sharing across languages.  

The importance of morphology-aware tokenization for language models has been recognized in several recent studies ____. However, an integrated solution that effectively balances morphological information with frequent pattern extraction while remaining fully compatible with modern LLM training pipelines has remained an open problem.