
%\begin{table}
%\centering
%\caption{LLM Training Parameters for Evaluating $MorphBPE$ vs. BPE Across Languages with Diverse Morphologies: Hungarian, Arabic, Russian, and English.}
%\label{tab:llm_params}
%\resizebox{0.8\width}{!}{%
%\begin{tabular}{lccc}
%\toprule
% Language &         \# of Tokens (Model: 300M) &  \# of Tokens (Model: 1B) &  Token Vocab. Size \\
%\midrule
%Hungarian &                 6B &            20B &                  24K \\
%  Russian &                6B &            20B &                  64K \\
%  English &                6B &            20B &                  96K \\
%   Arabic &         6B &            20B &                  96K \\
%\bottomrule
%\end{tabular}%
%}
%\end{table}


%\begin{table*}[ht!]
%  \centering
%    \label{tab:metrics}
%    \caption{Intrinsic valuation metrics proposed for the evaluation of tokenizers and their definitions.}
%  \resizebox{0.75\width}{!}{\begin{tabular}{lll}
%        \toprule
%        \textbf{Metric} & \textbf{Symbol} & \textbf{Definition} \\
%        \midrule
%        Fertility & \(\phi\) & \( \phi = \frac{\text{Number of tokenizer-generated tokens}}{\text{Number of whitespace-based tokens}} \) \\
%        Morphology Edit Distance Score & \(\mu\) & Pairwise alignment score measuring tokenization consistency with morphological segmentation \\
%        Perplexity & \(\pi\) & \( \pi = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i) \right) \), where \( P(w_i) \) is the model's probability of word \( w_i \) \\
%        \bottomrule
%    \end{tabular}}
%\end{table*}


% \begin{table*}[ht!]
%   \centering
%     \label{tab:metrics}
%     \caption{Intrinsic valuation metrics proposed for the evaluation of tokenizers and their definitions.}
%   \resizebox{0.75\width}{!}{\begin{tabular}{lll}
%         \toprule
%         \textbf{Metric} & \textbf{Symbol} & \textbf{Definition} \\
%         \midrule
%         Fertility & \(\phi\) & \( \phi = \frac{\text{Number of tokenizer-generated tokens}}{\text{Number of whitespace-based tokens}} \) \\
%         Morphology Edit Distance Score & \(\mu\) & Pairwise alignment score measuring tokenization consistency with morphological segmentation \\
%         Perplexity & \(\pi\) & \( \pi = \exp \left( -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i) \right) \), where \( P(w_i) \) is the model's probability of word \( w_i \) \\
%         \bottomrule
%     \end{tabular}}
% \end{table*}

% \begin{figure*}[ht!]
%     \centering
%     \includegraphics[width=0.95\textwidth]{diagrams/ce_loss_log.pdf}
%     \caption{Comparison of training cross-entropy loss between BPE and $MorphBPE$ across four languages. Results are shown for both the small (300M) and large (1B) models.}
%     \label{fig:cross_entropy_loss}
% \end{figure*}


    % \item \textbf{Fertility rate aligns with morphological complexity}: For English, which has relatively simple morphology, both BPE and $MorphBPE$ show **higher fertility rates** compared to **Hungarian and Russian**. However, the fertility gap between $MorphBPE$ and BPE is **larger in languages with complex morphology**, indicating that $MorphBPE$ generates **more interpretable** tokenization with better morpheme segmentation.

%\item \textbf{BPE over-segments morphologically rich languages}: As seen in **Figure~\ref{fig:morph_fertility}**, BPE produces **significantly higher morphological distance** in **Hungarian and Arabic**, reinforcing the intuition that it fails to respect meaningful morpheme boundaries. In contrast, $MorphBPE$ mitigates this issue by aligning better with the underlying linguistic structure.

%\item \textbf{Computational implications}: The fertility rate differences suggest that BPE might lead to inefficient tokenization, increasing sequence lengths unnecessarily. This can **negatively impact training and inference efficiency** in LLMs, especially for morphologically rich languages.