\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{50}{1em}}}%

\newcolumntype{P}[2]{%
  >{\begin{turn}{#1}\begin{minipage}{#2}\small\raggedright\hspace{0pt}}l%
  <{\end{minipage}\end{turn}}%
}

\definecolor{open_models_below_4B}{RGB}{185, 235, 255}
\definecolor{open_models_7B_12B}{RGB}{255, 219, 187}
\definecolor{closed_models}{RGB}{240, 240, 240}

\begin{table*}[t!]
\centering
\caption{\textbf{Main Results on General Document Benchmarks.} We compare \ourmodel{} (ours) with state-of-the-art (SOTA) open and closed-source instructed models, and with base models that we trained using the process described in Section~\ref{sec:trainin-datasets-stages}. \ourmodel{} models outperform all Base VLM models trained in the same data regime. Our models also perform competitively across document benchmarks even compared with SOTA models, in which the data regime is more targeted and optimized. Color coding for comparison: \colorbox{closed_models!70}{closed-source models}, \colorbox{open_models_below_4B!50}{open-source models below 7B parameters}, \colorbox{open_models_7B_12B!50}{open-source models between 7-12B parameters}.}



\resizebox{0.96\textwidth}{!}{%
\begin{tabular}{lcccccccccc}

\textbf{Model} &
\rot{\shortstack{\textbf{DocVQA} \\ \textcolor{gray}{\tiny{\textbf{VAL}}}}} & 
 \rot{\shortstack{\textbf{InfoVQA} \\ \textcolor{gray}{\tiny{VAL}}}} &
 \rot{\shortstack{\textbf{DeepForm} \\ \textcolor{gray}{\tiny{TEST}}}} &
 \rot{\shortstack{\textbf{KLC} \\ \textcolor{gray}{\tiny{TEST}}}} &
 \rot{\shortstack{\textbf{WTQ} \\ \textcolor{gray}{\tiny{TEST}}}} &
 \rot{\shortstack{\textbf{TabFact} \\ \textcolor{gray}{\tiny{TEST}}}} &
 \rot{\shortstack{\textbf{ChartQA} \\ \textcolor{gray}{\tiny{TEST}}}} &
 \rot{\shortstack{\textbf{TextVQA} \\ \textcolor{gray}{\tiny{VAL}}}} &
 \rot{\shortstack{\textbf{TableVQA} \\ \textcolor{gray}{\tiny{TEST}}}} &
 \rot{\textbf{Avg. Score}}  
 \\ 

\toprule
\multicolumn{11}{c}{\textbf{Closed-Source VLMs}}\\
\multicolumn{11}{c}{(\small{\textit{Opaque Training Data}})}
\vspace{3px}
\\
\rowcolor{closed_models!70}
Claude-3.5 Sonnet & 88.48 & 59.05 & 31.41 & 24.82 & 47.13 & 53.48 & 51.84 & \textbf{71.42} & \textbf{81.27} & \cellcolor{closed_models}56.54 \\

\rowcolor{closed_models!70}
GeminiPro-1.5& 91.23 & \textbf{73.94} & 32.16 & 24.07 & \textbf{50.29} & 71.22 & 34.68 & 68.16 & 80.43 & \cellcolor{closed_models}58.46 \\
\rowcolor{closed_models!70}
GPT-4o 20240806 & \textbf{92.80} & 66.37 & \textbf{38.39} & \textbf{29.92} & 46.63 & \textbf{81.10} & \textbf{85.70} & 70.46 & 72.87 & \cellcolor{closed_models}\textbf{64.91} \\

\hline

\multicolumn{11}{c}{
\textbf{Open-Source Instruct VLMs}}\\
\multicolumn{11}{c}{(\small{\textit{Semi-Opaque Training Data}})}
\vspace{3px}
\\


\rowcolor{open_models_below_4B!50}
Janus-\textbf{1.3B}~\citep{wu2024janusdecouplingvisualencoding}  & 
30.15 & 17.09 & 0.62 & 15.06 & 9.30 & 51.34 & 57.20 & 51.97 & 18.67 & \cellcolor{open_models_below_4B}27.93 \\

\rowcolor{open_models_below_4B!50}
Qwen2-VL-\textbf{2B}~\citep{wang2024qwen2vlenhancingvisionlanguagemodels} & 
\textbf{89.16} & \textbf{64.11} & {32.38} & 25.18 & \textbf{38.20} & {57.21} & {73.40} & 79.90 & {43.07} & 
\cellcolor{open_models_below_4B}\textbf{55.84} \\


\rowcolor{open_models_below_4B!50}
InternVL-2.5-\textbf{2B}~\citep{chen2024internvl}  & 
87.70 & 61.85 & 13.14 & 16.58 & 36.33 & \textbf{57.26} & 74.96 & 76.85 & 42.20 & \cellcolor{open_models_below_4B}51.87 \\


\rowcolor{open_models_below_4B!50}
DeepSeek-VL2-Tiny-\textbf{3.4B}~\citep{wu2024deepseekvl2mixtureofexpertsvisionlanguagemodels}  & 
88.57 & 63.88 & 25.11 & 19.04 & 35.07 & 52.15 & 80.92 & \textbf{80.48} & 56.30 & \cellcolor{open_models_below_4B}55.72 \\


\rowcolor{open_models_below_4B!50}
Phi3.5-Vision-\textbf{4B}~\citep{abdin2024phi3technicalreporthighly} &
{86.00} & {56.20} & {10.47} & {7.49} & {17.18} & {30.43} & \textbf{82.16} & {73.12} & \textbf{70.70} & 
\cellcolor{open_models_below_4B}{48.19} \\
\hline





\rowcolor{open_models_7B_12B!50}
Qwen2-VL-\textbf{7B}~\citep{wang2024qwen2vlenhancingvisionlanguagemodels}  & 
\textbf{93.83} & \textbf{76.12} & 34.55 & 23.37 & \textbf{52.52} & 74.68 & \textbf{83.16} & \textbf{84.48} & \textbf{53.97} & \cellcolor{open_models_7B_12B}\textbf{64.08} \\

\rowcolor{open_models_7B_12B!50}
LLaVA-NeXT-\textbf{7B}~\citep{xu2024llavauhd}  & 
{63.51} & {30.90} & {1.30} & {5.35} & {20.06} & {52.83} & {52.12} & {65.10} & {32.87} & 
\cellcolor{open_models_7B_12B}{36.00} \\



\rowcolor{open_models_7B_12B!50}
DocOwl1.5-\textbf{8B}~\citep{hu2024mplugdocowl15unifiedstructure} & 
{80.73} & {49.94} & \textbf{68.84} & \textbf{37.99} & {38.87} & \textbf{79.67} & {68.56} & {68.91} & {52.60} & 
\cellcolor{open_models_7B_12B}{60.68} \\

\rowcolor{open_models_7B_12B!50}
InternVL-2.5-\textbf{8B}~\citep{chen2024internvl}  & 
91.98 & 75.36 & 34.55 & 22.31 & 50.33 & 74.75 & 82.84 & 79.00 & 52.10 & \cellcolor{open_models_7B_12B}{62.58} \\

\rowcolor{open_models_7B_12B!50}
Ovis-1.6-Gemma2-\textbf{9B}~\citep{ovis}  & 
88.84 & 73.97 & 45.16 & 23.91 & 50.72 & 76.66 & 81.40 & 77.73 & 48.33 & \cellcolor{open_models_7B_12B}62.96 \\

\rowcolor{open_models_7B_12B!50}
Llama3.2-\textbf{11B}~\citep{llama3}  & 
82.71 & 36.62 & 1.78 & 3.47 & 23.03 & 58.33 & 23.80 & 54.28 & 22.40 & \cellcolor{open_models_7B_12B}34.04 \\



\rowcolor{open_models_7B_12B!50}
Pixtral-\textbf{12B}~\citep{agrawal2024pixtral12b}  & 
87.67 & 49.45 & 27.37 & 24.07 & 45.18 & 73.53 & 71.80 & 76.09 & 67.13 & \cellcolor{open_models_7B_12B}58.03 \\


\hline
\multicolumn{11}{c}{\textbf{Document Understanding Instructed Models}}\\


\multicolumn{11}{c}{(\small{\textit{Instruction Tuned on BigDocs-7.5M + DocDownStream~\citep{bigdocs, hu2024mplugdocowl15unifiedstructure}}})}\\


\rowcolor{open_models_below_4B!50}
Qwen2-VL-\textbf{2B} (base+)~\citep{qwen2025qwen25technicalreport} & 
57.23 & 31.88 & 49.31 & 34.39 & {31.61} & {64.75} & {68.60} & \textbf{61.01} & {47.53} & \cellcolor{open_models_below_4B}{49.59} \\



\rowcolor{open_models_below_4B!50}
\textbf{\ourmodel{}}-Llama-3.2-\textbf{1B} (ours)& 
72.42 & 38.16 & 60.47 & 33.71 & 28.66 & 71.31 & 65.44 & 48.81 & 50.29 & \cellcolor{open_models_below_4B}52.14 \\

\rowcolor{open_models_below_4B!50}
\textbf{\ourmodel{}}-Llama-3.2-\textbf{3B} (ours) & 
\textbf{79.63} & \textbf{44.53} & \textbf{63.49} & \textbf{35.25} & \textbf{38.59} & \textbf{78.51} & \textbf{71.88} & 57.38 & \textbf{60.10} & \cellcolor{open_models_below_4B}\textbf{58.81} \\

\hline
\rowcolor{open_models_7B_12B!50}
DocOwl1.5-\textbf{8B} (base+)~\citep{hu2024mplugdocowl15unifiedstructure} &
{78.70} & {47.62} & {64.39} & {36.93} & {35.69} & {72.65} & 65.80 & {67.30} & {49.03} & \cellcolor{open_models_7B_12B}{57.56} \\

\rowcolor{open_models_7B_12B!50}
Llama3.2-\textbf{11B} (base+)~\citep{llama3}  & 
78.99 & 44.27 & \textbf{67.05} & \textbf{37.22} & 40.18 & 78.04 & 71.40 & \textbf{68.46} & 56.73 & \cellcolor{open_models_7B_12B}60.26 \\



\rowcolor{open_models_7B_12B!50}
\textbf{\ourmodel{}}-Llama-3.1-\textbf{8B} (ours)  & 
\textbf{81.18} & \textbf{53.75} & 63.25 & 35.50 & \textbf{45.31} & \textbf{83.04} & \textbf{75.00} & 64.60 & \textbf{64.33} & \cellcolor{open_models_7B_12B}\textbf{62.88} \\






\bottomrule
\end{tabular}%
}

\label{tab:bigdocs}
\vspace{-10px}
\end{table*}























































