
\section{Introduction}
\label{section:introduction}


Vision-Language Models (VLMs) have gained significant traction in recent years as a powerful framework for multimodal document understanding tasks that involve interpreting both the visual and textual contents of scanned documents~\citep{donut, pix2struct, liu2023improvedllava, liu2024llavanext, hu2024mplugdocowl15unifiedstructure, wang2023docllmlayoutawaregenerativelanguage, rodriguez2024starvector}. Such tasks are common in real-world commercial applications, including invoice parsing~\citep{park2019cord}, form reading~\citep{funsd}, and document question answering~\citep{docvqa}. 
VLM architectures typically consist of three components:
\textit{(i)} a vision encoder to process raw images, \textit{(ii)} a Large Language Model (LLM) pre-trained on text, and \textit{(iii)} a connector module that maps the visual features from the vision encoder into the LLM's semantic space.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth%
    ]{figures/radar.pdf}
    \caption{\textbf{Performance of Different VLM Connectors.} The proposed \textit{\textbf{\alignmodule{}}} connector outperforms other methods across benchmarks using the same training configuration. Radial distance is proportion of maximal score, truncated at $0.7$ (black dot).}
    \label{fig:radar-plot-connectors}
\end{figure}

A central challenge in this pipeline is to effectively map the continuous feature embeddings of the vision encoder into the latent space of the LLM while preserving the semantic properties of visual concepts. Existing approaches can be broadly categorized into \emph{deep fusion} and \emph{shallow fusion} methods. \emph{Deep fusion} methods, such as NVLM~\citep{dai2024nvlm}, Flamingo~\citep{flamingo}, CogVLM~\citep{cogvlm:wang2023}, and LLama 3.2-Vision~\citep{llama3}, integrate visual and textual features by introducing additional cross-attention and feed-forward layers at each layer of the LLM. While effective at enhancing cross-modal interaction, these methods substantially increase the parameter count of the VLM compared to the base LLM, resulting in high computational overhead and reduced efficiency. 

In contrast, \emph{shallow fusion} methods project visual features from the vision encoder into the LLM input embedding space using either multilayer perceptrons (MLPs)~\citep{liu2023llava, liu2024llavanext} or attention-based mechanisms such as the Perceiver Resampler~\citep{blip2, idefics2, flamingo}, before concatenating them with the textual prompt's input embeddings. This approach is more parameter-efficient and computationally lighter than \emph{deep fusion} methods, but it lacks a mechanism to ensure the projected embeddings remain within the region spanned by the LLM's text embeddings -- i.e. regions the LLM was pretrained to understand.
As a result, unconstrained visual features
can produce out-of-distribution (OOD) and noisy inputs, leading to misalignment between modalities and often degrading overall performance. Recent methods like Ovis~\citep{ovis} attempt to alleviate these issues by introducing separate visual embeddings 
indexed from the vision encoder outputs and combined together 
to construct the visual inputs to the LLM. However, this approach significantly increases parameter count due to the massive embedding matrix and requires extensive training to learn a new embedding space without guaranteeing alignment with the LLM's input latent space.


To address these limitations, this paper introduces \textbf{\ourmodel{}}, a novel framework that sidesteps direct projection of visual features into the LLM embedding space. Instead, our proposed connector, \alignmodule, maps visual features into probability distributions over the LLM's \emph{existing} pretrained vocabulary embeddings, which are 
then combined into a weighted representation of the text embeddings.
By constraining each visual feature as a convex combination of the LLM text embeddings, our approach leverages the linguistic priors already encoded in the LLM's text space. This ensures that the resulting visual features lie within the convex hull of the LLM's embedding space, reducing the risk of noisy or out-of-distribution inputs and improving alignment between modalities.
Our experimental results show that this approach improves performance on various document understanding tasks, outperforming prior connector methods by effectively fusing visual and linguistic content. We summarize our main contributions as follows:
\vspace{-0.3cm}
\begin{itemize} 
\setlength{\itemsep}{0pt}
    \item We propose a novel connector, \textbf{\alignmodule}, to bridge the representation gap between vision and text modalities.
    \item We introduce a family of Vision-Language Models, \textbf{\ourmodel{}}, that achieves state-of-the-art performance on multimodal document understanding tasks by leveraging \alignmodule.
    \item We conduct extensive experiments %
    demonstrating the robustness and effectiveness of \textbf{\alignmodule} across different model sizes ranging from 1B to 8B parameters.
    \vspace{-0.3cm}
\end{itemize}
Our code and models will be public upon acceptance. %
