\definecolor{align_blue}{RGB}{207,228,255}
\definecolor{align_pink}{RGB}{245,196,245}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Model_Diagram.pdf}
    \caption{\textbf{\ourmodel{} Model Architecture.} The vision encoder extracts image features, which are processed to produce probabilities over the LLM embeddings. A weighted average combines these probabilities with embeddings to generate vision input vectors. Text inputs are tokenized, and the corresponding embeddings are selected from the embedding matrix, which is then used as input to the LLM. We display the vision layers in \colorbox{align_blue}{blue}, and the text layers in \colorbox{align_pink}{purple}.}
    \label{fig:model-architecture}
\end{figure*}

\section{Methodology}

\subsection{Model Architecture}\label{sec:architecture}
The overall model architecture, shown in Figure~\ref{fig:model-architecture}, consists of three main components:
\paragraph{(1) Vision Encoder.}\label{sec:vision-encoder}
To handle high-resolution images of different aspect ratios, we divide each input image into multiple tiles according to one of the predefined aspect ratios (e.g., $1{:}1,\,1{:}2,\,\dots,\,9{:}1$) chosen via a coverage ratio~\citep{ovis, chen2024fargpt4vclosinggap}. Due to limited computational resources, we set the maximum number of tiles to 9. Each tile is further partitioned into $14\times 14$ patches, projected into vectors, and processed by a SigLip-400M vision encoder~\citep{zhai2023sigmoidlosslanguageimage} to extract contextual visual features.

Each tile $t \in \{1,\cdots,T\}$ is divided into $N_t$ patches
\begin{equation}
\mathbf{P}_t = \{\mathbf{p}_{t,1}, \cdots, \mathbf{p}_{t,N_t}\},\nonumber
\end{equation}
where $\mathbf{p}_{t,i}$ is the $i$-th patch of tile $t$. The vision encoder maps these patches to a set of visual feature vectors
\begin{gather}
\mathbf{F}_t = \mathrm{VisionEncoder}(\mathbf{P}_t) \nonumber \\
\mathbf{F}_t = \{\mathbf{f}_{t,1}, \cdots, \mathbf{f}_{t,N_t}\},
\quad
\mathbf{f}_{t,i} \in \mathbb{R}^d.\nonumber
\end{gather}
Finally, we concatenate the feature sets across all tiles into a single output
\begin{equation}
\fvisraw = \mathrm{concat}\Bigl(\mathbf{F}_1, \mathbf{F}_2, \cdots, \mathbf{F}_T\Bigr).\nonumber
\end{equation}





\paragraph{(2) \alignmodule{} Module.}
This module aligns the visual features with the LLM. A linear layer $\mathbf{W}_1 \in \mathbb{R}^{D \times d}$ first projects the visual features $\fvisraw \in \mathbb{R}^{T \cdot N_t \times d}$ to the LLM's token embedding space: one $\mathbb{R}^D$ vector per token. A second linear layer $\mathbf{W}_2 \in \mathbb{R}^{V \times D}$ (initialized from the LLM's language-model head) followed by a softmax, produces a probability simplex $\mathbf{P}_\text{vocab}$
over the LLM's vocabulary ($V$ tokens)
\begin{align}
&\mathbf{P}_\text{vocab} = \label{eq:vocab_projection}\\ 
&\qquad\softmax( \layernorm(\mathbf{W}_2 \layernorm(\mathbf{W}_1 \fvisraw)))  \nonumber
\end{align}
We then use the LLM text embeddings $\mathbf{E}_\text{text} \in \mathbb{R}^{V \times D}$ to compute a weighted sum
\begin{equation}
\falign = \mathbf{P}_\text{vocab}^\top \mathbf{E}_\text{text}.
\label{eq:feature_alignment}
\end{equation}

Finally, we concatenate $\falign$ with the tokenized text embeddings to form the LLM input
\begin{equation}
\mathbf{H}_\text{input}
= \mathrm{concat}\bigl(\falign, \mathbf{E}_\text{text}(\mathbf{x})\bigr),\nonumber \label{eq:3}
\end{equation}
where $\mathbf{E}_\text{text}(\mathbf{x})$ is obtained by tokenizing the input text $\mathbf{x} = (x_1,\cdots,x_M)$ and selecting the corresponding embeddings from $\mathbf{E}_\text{text}$ such that
\begin{align}
\mathbf{E}_\text{text}(\mathbf{x}) 
& = \bigl[\mathbf{E}_\text{text}(x_1), \cdots, \mathbf{E}_\text{text}(x_M)\bigr].\label{eq:ftext}
\end{align}

\paragraph{(3) Large Language Model.}
We feed the concatenated vision and text vectors, $\mathbf{H}_\text{input}$, into the LLM, which then generates output text auto-regressively. To demonstrate the effectiveness of our alignment technique, we experiment with the Llama 3.1 model family ~\citep{llama3}. These models offer state-of-the-art performance and permissive licenses, making them suitable for commercial applications. In particular, we utilize Llama 3.2-1B, Llama 3.2-3B, and Llama 3.1-8B.



\subsection{Motivation and relation with existing methods}
By construction, each $\mathbb{R}^D$ representation in $\falign$ is constrained to the %
convex hull of the points $\mathbb{E}_{\text{text}}$, thus concentrating the visual features in the part of latent space that the LLM can effectively interpret. 
Moreover, we argue that our initialization of $\mathbf{W}_2$ to the language model head is an inductive bias toward \emph{recycling} some of the semantics of these text tokens into visual tokens. 
This contrasts with past methods that have been proposed to adapt the vision encoder outputs $\fvisraw \in \mathbb{R}^{T \cdot N_t \times d}$ to an $\fvis \in \mathbb{R}^{T \cdot N_t \times D}$ to be fed to the LLM.
Here, we consider two examples in more detail, highlighting these contrasts.

\emph{(1) MLP Connector} \cite{liu2023llava} applies a linear projection with parameters $\mathbf{W}_{\text{MLP}} \in \mathbb{R}^{D \times d}$ and $\mathbf{b}_{\text{MLP}} \in \mathbb{R}^D$, followed by an activation function $\sigma$ (e.g., ReLU)
\[
\fmlp = \sigma(\mathbf{W}_{\text{MLP}} \mathbf{F} + \mathbf{b}_{\text{MLP}}).
\]
These parameters are all learned from scratch, with no particular bias aligning them to text embeddings.

\emph{(2) Visual Embedding Table} \cite{ovis} introduces an entire new set of visual embeddings $\mathbf{E}_{\text{VET}} \in \mathbb{R}^{K \times D}$ which, together with the weights $\mathbf{W}_{\text{VET}} \in \mathbb{R}^{K\times d}$, specifies
\begin{equation}
\fvet = \softmax( \mathbf{W}_{\text{VET}} \fvisraw )^\top \mathbf{E}_{\text{VET}} . \nonumber
\end{equation}
When $D < d$, our $\mathbf{W}_2 \mathbf{W}_1$ amounts to a low-rank version of $\mathbf{W}_{\text{VET}}$.
There is thus much more to learn to obtain $\fvet$, and there is again no explicit pressure to align it with the text embeddings.












\subsection{Training Datasets \& Stages}\label{sec:trainin-datasets-stages}
\input{tables/bigdocs_docdownstream_results}

We train our model in three stages:
\paragraph{Stage 1.} This stage focuses on training the \alignmodule{}  Module to map visual features to the LLM's text embeddings effectively. We use the CC-12M dataset~\cite{cc12m}, a large-scale web dataset commonly used for VLM pretraining~\cite{liu2023llava}, which contains 12M image-text pairs. However, due to broken or unavailable links, we retrieved 8.1M pairs. This dataset facilitates the alignment of visual features with the text embedding space of the LLM. During this stage, we train the full model, as this approach improves performance and stabilizes the training of the \alignmodule{} Module.

\paragraph{Stage 2.} The goal is to enhance the model's document understanding capabilities, such as OCR, document structure comprehension, in-depth reasoning, and instruction-following. We leverage the BigDocs-7.5M dataset~\cite{bigdocs}, a curated collection of license-permissive datasets designed for multimodal document understanding. This dataset aligns with the Accountability, Responsibility, and Transparency (ART) principles \cite{bommasani2023foundationmodeltransparencyindex, caitlin2021}, ensuring compliance for commercial applications.
As in Stage 1, we train the full model during this stage.

\paragraph{Stage 3.} To enhance the model's instruction-tuning capabilities, particularly for downstream tasks like question answering, we further train it on the DocDownstream~\cite{bigdocs, hu2024mplugdocowl15unifiedstructure} instruction tuning dataset. In this stage, the vision encoder is frozen, focusing training exclusively on the LLM and \alignmodule{} module.
