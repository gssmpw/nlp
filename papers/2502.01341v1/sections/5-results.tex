
\vspace{-0.5em}
\section{Results}
\subsection{Main Results}
\label{sec:main_results}
Table \ref{tab:bigdocs} presents the performance of \ourmodel{} compared to state-of-the-art (SOTA) open- and closed-source instructed models, as well as baseline Base VLMs fine-tuned in the same instruction-tuning setup. The results demonstrate that \ourmodel{} consistently outperforms all Base VLMs within the same size category and achieves competitive performance against SOTA Instruct VLMs despite being trained on a more limited data regime. Below, we provide a detailed analysis.
\vspace{-0.3em}
\paragraph{\ourmodel{} vs. Base VLMs.} Our \ourmodel{} models, based on Llama 3.2-1B and Llama 3.2-3B, significantly outperform the corresponding Base VLM, Qwen2-VL-2B, by up to \emph{9.22\%}. Notably, \ourmodel{}-Llama-3.2-3B surpasses DocOwl1.5-8B, which has 4B more parameters, demonstrating the effectiveness of \alignmodule{} in enhancing multimodal capabilities compared to traditional \emph{shallow fusion} methods (e.g., MLPs). Furthermore, our 8B model achieves a \emph{2.62\%} improvement over Llama3.2-11B despite sharing the same Base LLM, Llama3.1-8B. Since all models in this comparison were trained on the same instruction-tuning setup, this experiment provides a controlled evaluation, isolating the impact of architectural differences rather than dataset biases. Consequently, these results suggest that \ourmodel{} outperforms VLMs with shallow fusion techniques and surpasses parameter-heavy \emph{deep fusion} VLMs, such as Llama3.2-11B, while maintaining a more efficient architecture.
\vspace{-0.3em}
\paragraph{\ourmodel{} vs. Instruct VLMs.} 
Even as open-source Instruct models are trained on significantly larger, often undisclosed instruction-tuning datasets, \ourmodel{} achieves superior performance. 
For instance, \ourmodel{}-Llama-3.2-3B (\emph{58.81\%}) outperforms all instructed VLMs in its size category, surpassing its closest competitor, Qwen2-VL-2B (\emph{55.84\%}), by \emph{2.97\%}. Additionally, our 8B model outperforms significantly larger models such as Llama 3.2-11B and PixTral-12B by substantial margins. It also surpasses InternVL-2.5-8B and performs competitively with Qwen2-VL-7B, though a direct comparison may not be entirely fair since Qwen2-VL-7B was trained on an undisclosed instruction-tuning dataset. Finally, \ourmodel{} also exhibits comparable performance to closed-source models like GeminiPro-1.5 and GPT4o. 

Overall, these results validate the effectiveness of \alignmodule{} and establish \ourmodel{} as a state-of-the-art model for multimodal document understanding.



\subsection{Impact of Connector Designs on VLM Performance}

To assess the effectiveness of our \alignmodule{} module, we compare it against three different and widely used \emph{shallow fusion} VLM connectors: MLP, Perceiver Resampler, and Ovis. The results in Table \ref{tab:connectors-ablations} show that \alignmodule{} consistently outperforms all alternatives, demonstrating its superiority both in aligning visual and textual modalities and in multimodal document understanding. MLP and Perceiver Resampler achieve the lowest performance, 53.06\% and 50.68\%, respectively, due to their direct feature projection, which lacks an explicit mechanism to align visual features with the LLM's text space, leading to misalignment. Ovis introduces a separate visual embedding table, but this additional complexity does not significantly improve alignment, yielding only 54.72\% accuracy.
In contrast, \alignmodule{} ensures that visual features remain within the convex hull of the LLM's text latent space, leveraging the linguistic priors of the LLM to enhance alignment and mitigate noisy embeddings. This design leads to the highest performance (58.81\%), establishing \alignmodule{} as the most effective connector for integrating vision and language in multimodal document understanding.
We provide some example outputs of the Llama-3.2-3B models with different connector designs in Appendix~\ref{app:case}.




\subsection{Probability Distribution over Text Tokens Analysis}
To better understand the behavior of \alignmodule{}, we examine the probability distribution, \(\mathbf{P}_{\text{vocab}}\) in Eq \eqref{eq:vocab_projection}, over the LLM’s text vocabulary generated from visual features. Specifically, we process 100 document images through the vision encoder and \alignmodule{}, then average the resulting probability distributions across all image patches. The final distribution is shown in Figure ~\ref{fig:prob-distribution-tokens}. 
As illustrated, the distribution is \emph{dense} (rather than sparse), with the highest probability assigned to a single token being \emph{0.0118}. This can be explained by the vision feature space being continuous and of much higher cardinality than the discrete text space.
Indeed, while the LLM has 128K distinct vocabulary tokens, an image patch (e.g., 14×14 pixels) contains continuous, high-dimensional information that cannot be effectively mapped to a single or a few discrete tokens.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/prob_dist_image.png}
    \caption{\textbf{Probability distribution over the LLM text tokens}, showing dense probabilities and higher values for tokens associated with white space in document images.}
    \label{fig:prob-distribution-tokens}
\end{figure}


Furthermore, we observe that tokens on the left side of the distribution in Figure ~\ref{fig:prob-distribution-tokens} have higher probabilities than the rest. Upon investigation, we found that these tokens correspond to patches that are predominantly white -- a common feature in document images. Further analysis of the associated text tokens reveals that they predominantly consist of punctuation marks, as illustrated further in Appendix~\ref{app:vision_to_text}. This suggests that the model repurposes punctuation marks to represent whitespaces.
This may be attributed to the fact that both punctuation and whitespaces act as structural cues and separators.
Other possibilities include whitespaces being rarely directly-required to perform a task, and LLMs may pay less specific attention to common tokens such as punctuation.



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/vcr_results.pdf}
    \caption{
        Comparison of Llama-3.2-3b-\alignmodule{} and Llama-3.2-3B-MLP 
        on the Easy and Hard VCR tasks. 
    }
    \label{fig:vcr}
\end{figure}









\begin{figure*}[h!]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/vcr_example1.png}
        \vspace{5pt}
        \begin{tabular}{@{}l@{\hspace{6pt}}p{0.8\linewidth}}

            \footnotesize \textbf{GT:} & \footnotesize \textit{(appears in written history in)} \\
            \footnotesize
            \footnotesize \textbf{MLP:} & \footnotesize \textit{(\textcolor{red}{census} in written history in)} \xmark \\
            \footnotesize
            \footnotesize \textbf{\alignmodule{} } & \footnotesize \textit{(appears in written history in)} \cmark \\
        \end{tabular}
        \caption{Positive Example 1}
        \label{posvcr1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/vcr_example2.png}
        \vspace{5pt}
        \begin{tabular}
        {@{}l@{\hspace{6pt}}p{0.8\linewidth}}
            \footnotesize \textbf{GT:} & 
            \footnotesize \textit{(the system used for assigning)} \\
            \footnotesize \textbf{MLP:} &
            \footnotesize \textit{(the system used for \textcolor{red}{accounting})} \xmark \\
            \footnotesize \textbf{\alignmodule{} } & 
            \footnotesize \textit{(the system used for assigning)} \cmark \\
        \end{tabular}
        \caption{Positive Example 2}
        \label{posvcr2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/vcr_example3.png}
        \vspace{5pt}
        \begin{tabular}{@{}l@{\hspace{6pt}}p{0.8\linewidth}}
            \footnotesize \textbf{GT:} & \footnotesize \textit{(mines situated near Llanengan on)} \\
            \footnotesize \textbf{MLP:} & \footnotesize \textit{(mines situated near Llanengan on)} \cmark \\
            \footnotesize \textbf{\alignmodule{}} & \footnotesize \textit{(mines situated near \textcolor{red}{Llanongan} on)} \xmark \\
        \end{tabular}
        \caption{Negative Example 1}
        \label{negvcr1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/vcr_example4.png}
        
        
        \vspace{5pt}
        \begin{tabular}{@{}l@{\hspace{6pt}}p{0.6\linewidth}}
            \footnotesize \textbf{GT:} & \footnotesize \textit{(Gorden County is home to)} \\
            \footnotesize \textbf{MLP:} & \footnotesize \textit{(Gorden County is home to)} \cmark \\
            \footnotesize \textbf{\alignmodule{} } & \footnotesize \textit{(\textcolor{red}{Garden} County is home to)} \xmark \\
        \end{tabular}
        \caption{Negative Example 2}
        \label{negvcr2}

    \end{subfigure}
    
    \caption{\textbf{Case Study for Pixel-Level Tasks.} We provide examples of our proposed \textbf{\alignmodule{}} connector compared with a the Multi-Layer
Perceptron (MLP) connector. The \textbf{\alignmodule{}} connector tends to better map visual elements to common words. GT is the ground truth.
} 
\label{fig:vcr_case_studies}
\end{figure*}



\subsection{Pixel-Level Tasks Analysis}
To rigorously evaluate the ability of vision-language models to integrate fine-grained visual and textual pixel-level cues, we test our model on the VCR benchmark~\citep{zhang2024vcr}, which requires the model to recover partially occluded texts with pixel-level hints from the revealed parts of the text. This task challenges VLM's alignment of text and image in extreme situations. Current state-of-the-art models like GPT-4V \cite{openai2023gpt4}, Claude 3.5 Sonnet \cite{anthropic2024claude}, and Llama-3.2 \cite{dubey2024llama} significantly underperform humans on \emph{hard} VCR task due to their inability to process subtle pixel-level cues in occluded text regions. These models frequently discard critical visual tokens during image tokenization on semantic priors, overlooking %
the interplay between partial character strokes and contextual visual scenes. To evaluate performance on VCR, we modify our Stage 3 SFT dataset composition by replacing the exclusive use of DocDownstream with a 5:1 blended ratio of DocDownstream and VCR training data. This adjustment enables direct evaluation of our architecture \alignmodule{}’s ability to leverage pixel-level character cues.


From the experimental outcomes, it is evident that \ourmodel{} consistently outperforms the MLP Connector Model across both easy and hard settings of the pixel-level VCR task (see Figure~\ref{fig:vcr}), with improvements ranging from 10.18\% on the hard setting to 14.41\% on the easy setting. 

We provide a case study on VCR in Figure~\ref{fig:vcr_case_studies}, featuring four representative examples. In Figure \ref{posvcr1},  it is evident that the MLP connector model fails to capture semantic consistency as effectively as \ourmodel{}. 
The phrase ``The commune first \textit{census in written history in}'' (where the words in italics are generated by the model while the rest are in the image) is not as semantically coherent as the phrase generated by \alignmodule{} ``The commune first \emph{appears in written history in}''.

 Beyond the issue of semantic fluency, in Figure~\ref{posvcr2} we also observe that \ourmodel{} successfully identifies the uncovered portion of the letter ``g'' in ``accounting'' and uses it as a pixel-level hint to infer the correct word. In contrast, the MLP model fails to effectively attend to this crucial detail.

Figures \ref{negvcr1} and \ref{negvcr2} show examples where \ourmodel{} fails on the VCR task. 
These carefully picked instances show that our method mistakes names of landmarks with common words when the two are very similar. As seen in the examples, \ourmodel{} mistakes ``Llanengan" for ``Llanongan" and ``Gorden" for ``Garden''. In both instances, the pairs differ by one character, indicating perhaps that \ourmodel{} tends to align vision representations to more common tokens in the vocabulary. One approach that would potentially mitigate such errors would be to train \ourmodel{} with more contextually-relevant data.










\subsection{Robustness to Noise Analysis}
To evaluate the robustness of our \textbf{\alignmodule{}} connector to noisy visual features, we conduct an experiment where random Gaussian noise is added to the visual features produced by the vision encoder before passing them into the connector. Specifically, given the visual features \( \mathbf{F} \in \mathbb{R}^{N \times d} \) output by the vision encoder (where \( N \) is the number of feature vectors and \( d \) is their dimensionality), we perturbed them as
\[
\widetilde{\mathbf{F}} = \mathbf{F} + \mathbf{N}, \quad \mathbf{N} \sim \mathcal{N}(0, \sigma = 3). 
\]
\begin{table}[htb]
\centering
\caption{\textbf{Robustness to Noise.} Comparison of Avg. Scores with and without Gaussian noise (\( \sigma = 3 \)), including performance drop (\( \Delta \)).}
\label{tab:noise}
\renewcommand{\arraystretch}{1.2} %
\setlength{\tabcolsep}{6pt} %
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{lccc}
\textbf{Model}      & \textbf{Without Noise} & \textbf{With Noise} & \textbf{Drop (\( \Delta \))} \\ \hline
Llama-3.2-3B-MLP    & 53.06                  & 27.52               & \( \downarrow 25.54 \)                     \\ 
Llama-3.2-3B-\alignmodule{} (ours) & \textbf{58.81}                  & \textbf{57.14}               & \( \downarrow \textbf{1.67} \)                      \\ \hline
\end{tabular}%
}
\end{table}

As shown in Table~\ref{tab:noise}, our \textbf{\alignmodule{}} connector demonstrates high robustness to noise, with only a \emph{1.67\%} average drop in performance. In contrast, the widely adopted MLP connector suffers a significant performance degradation of \emph{25.54\%}, highlighting its vulnerability to noisy inputs. These empirical results support our hypothesis that leveraging the knowledge encoded in the LLM's text embeddings and constraining the visual features within the convex hull of the text latent space act as a regularization mechanism, reducing the model's sensitivity to noisy visual features.







