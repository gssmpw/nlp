\vspace{-0.5em}
\section{Experimental Setup}
\input{tables/connectors_ablations}
\vspace{-0.4em}

\paragraph{Setup.} We conduct all experiments using 8 nodes of H100 GPUs, totaling 64 GPUs. For model training, we leverage the MS-Swift framework~\citep{zhao2024swiftascalablelightweightinfrastructure} for its flexibility. Additionally, we utilize the DeepSpeed framework~\citep{aminabadi2022deepspeedinferenceenablingefficient}, specifically the ZeRO-3 configuration, to optimize efficient parallel training across multiple nodes. Detailed hyperparameters are outlined in Appendix~\ref{app:hyperparameters}.

\paragraph{Baselines.} 
Our work focuses on architectural innovations, so we ensure that all baselines are trained on the same datasets.
To enable fair comparisons, we evaluate our models against a set of \textbf{Base VLMs} fine-tuned on the same instruction-tuning tasks (Stages 2 and 3) as our models, using the BigDocs-7.5M and BigDocs-DocDownstream datasets. This approach ensures consistent training data, avoiding biases introduced by the \textbf{Instruct} versions of VLMs, which are often trained on undisclosed instruction-tuning datasets. 
Due to the scarcity of recently released publicly available Base VLMs, we primarily compare our model against the following Base VLMs of varying sizes: Qwen2-VL-2B~\citep{wang2024qwen2vlenhancingvisionlanguagemodels}, DocOwl1.5-8B~\citep{hu2024mplugdocowl15unifiedstructure}, and LLama 3.2-11B~\citep{llama3}.

For additional context, we also include results from the Instruct versions of recent VLMs of different sizes: Phi3.5-Vision-4B~\citep{abdin2024phi3technicalreporthighly}, Qwen2-VL-2B and 7B~\citep{wang2024qwen2vlenhancingvisionlanguagemodels}, LLaVA-NeXT-7B~\citep{liu2024llavanext}, InternVL2.5-2B and 8B~\citep{chen2024internvl}, Janus-1.3B~\citep{wu2024janusdecouplingvisualencoding}, DeepSeek-VL2-Tiny~\citep{wu2024deepseekvl2mixtureofexpertsvisionlanguagemodels}, Ovis1.6-Gemma-9B~\citep{ovis}, Llama3.2-11B~\citep{llama3}, DocOwl1.5-8B~\citep{hu2024mplugdocowl15unifiedstructure}, and Pixtral-12B~\citep{agrawal2024pixtral12b}.

\paragraph{Evaluation Benchmarks.} We evaluate our models on a diverse range of document understanding benchmarks that assess the model's capabilities in OCR, chart reasoning, table processing, or form comprehension. In particular, we employ the VLMEvalKit~\citep{duan2024vlmevalkit} framework and report the results on the following popular benchmarks:  DocVQA~\citep{docvqa}, InfoVQA~\citep{mathew2021infographicvqa}, DeepForm~\citep{svetlichnaya2020deepform}, KLC~\citep{stanislawek2021kleister}, WTQ~\citep{pasupat2015compositional}, TabFact~\citep{Chen2020TabFact}, ChartQA~\citep{masry2022chartqa}, TextVQA~\citep{singh2019towards}, %
and TableVQA~\citep{kim2024tablevqa}.
