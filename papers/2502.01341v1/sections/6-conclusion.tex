\section{Conclusion}
We introduce \alignmodule, a novel connector designed to align vision and language latent spaces in vision-language models (VLMs), specifically enhancing multimodal document understanding. By improving cross-modal alignment and minimizing noisy embeddings, our models, \ourmodel, which leverage \alignmodule, achieve state-of-the-art performance across diverse document understanding tasks. This includes outperforming base VLMs trained on the same datasets and open-source instruct models trained on undisclosed data. Extensive experiments and ablations validate the robustness and effectiveness of \alignmodule \ compared to existing connector designs, establishing it as a significant contribution to vision-language modeling. Future work will explore training on more diverse instruction-tuning datasets to generalize beyond document understanding to broader domains.
