\section{Related Work}

\subsection{Vision-Language Models}

Over the past few years, Vision-Language Models (VLMs) have achieved remarkable progress, largely due to advances in Large Language Models (LLMs). Initially demonstrating breakthroughs in text understanding and generation~\citep{gpt3, t5, gpt4, llama3, qwen2025qwen25technicalreport, geminiteam2024geminifamilyhighlycapable}, LLMs are now increasingly used to effectively interpret visual inputs~\citep{liu2023llava, llavaonevision, wang2024qwen2vlenhancingvisionlanguagemodels, chen2024internvl, dai2024nvlm, drouin2024workarenacapablewebagents, rodriguez2022ocrvqgan}. This progress has enabled real-world applications across diverse domains, particularly in multimodal document understanding for tasks like form reading~\citep{svetlichnaya2020deepform}, document question answering~\citep{docvqa}, and chart question answering~\citep{masry2022chartqa}. VLMs commonly adopt a three-component architecture: a pretrained vision encoder~\citep{zhai2023sigmoidlosslanguageimage, radford2021learningtransferablevisualmodels}, a LLM, and a connector module. A key challenge for VLMs is effectively aligning visual features with the LLM's semantic space to enable accurate and meaningful multimodal interpretation.



\subsection{Vision-Language Alignment for Multimodal Models}

Existing vision-language alignment approaches can be classified into \emph{deep fusion} and \emph{shallow fusion}. Deep fusion methods integrate visual and textual features by modifying the LLM's architecture, adding cross-attention and feed-forward layers. For example, Flamingo~\citep{flamingo} employs the Perceiver Resampler, which uses fixed latent embeddings to attend to vision features and fuses them into the LLM via gated cross-attention layers. Similarly, NVLM~\citep{dai2024nvlm} adopts cross-gated attention while replacing the Perceiver Resampler with a simpler MLP. CogVLM~\citep{cogvlm:wang2023} extends this approach by incorporating new feed-forward (FFN) and QKV layers for the vision modality within every layer of the LLM. While these methods improve cross-modal alignment, they significantly increase parameter counts and computational overhead, making them less efficient.

On the other hand, shallow fusion methods are more computationally efficient, mapping visual features into the LLM's embedding space without altering its architecture. These methods can be categorized into three main types: \emph{(1) MLP-based mapping}, such as LLaVA~\citep{liu2023llava} and PaliGemma~\citep{beyer2024paligemmaversatile3bvlm}, which use multilayer perceptrons (MLP) to project visual features but often produce misaligned or noisy features due to a lack of constraints~\citep{rodriguez2024starvector}; \emph{(2) cross-attention mechanisms}, BLIP-2~\citep{blip2} uses Q-Former, which utilizes a fixed set of latent embeddings to cross-attend to visual features, but that may still produce noisy or OOD visual features; and \emph{(3) visual embeddings}, such as those introduced by Ovis~\citep{ovis}, which use embeddings indexed by the vision encoder's outputs to produce the visual inputs. While this regularizes feature mapping, it adds substantial parameter overhead and creates a new vision embedding space, risking misalignment with the LLM's text embedding space. Encoder-free VLMs, like Fuyu-8B~\footnote{https://www.adept.ai/blog/fuyu-8b} and EVE~\citep{eve:diao2024unveiling}, eliminate dedicated vision encoders but show degraded performance ~\citep{beyer2024paligemmaversatile3bvlm}.

In contrast, \ourmodel{} maps visual features from the vision encoder into probability distributions over the LLM's text embeddings, using them to compute a convex combination. By leveraging the linguistic priors encoded in the LLM's vocabulary, \ourmodel{} ensures that visual features remain within the convex hull of the text embeddings, mitigating noisy or out-of-distribution inputs and enhancing alignment, particularly for tasks that require joint modalities representation like multimodal document understanding.



  

















