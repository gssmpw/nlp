\section{Related Work}
\begin{figure*}[t!]
\centerline{\includegraphics[width=1\textwidth]{img/flow.pdf}}
\captionsetup{font=small}
\caption{\small The key process of learning and connecting visual perception to functional dexterous grasping actions. The CMKA learning module learns from Exo images with human operation experience and transfers the knowledge to Ego images, locating three keypoints constrained by dexterous grasping. The KGT method maps the hand-object relative pose, calculating the dexterous hand's rotation and translation parameters (\(R\), \(T\)) to control the grasping task.}
\label{flow}
\vskip-2ex
\end{figure*}

\subsection{Object Representation for Dexterous Grasping}

Grasping and manipulation are fundamental topics in robotics. Traditional methods~\cite{srivastava2014combined, tyree20226, wen2024foundationpose} often rely on six Degrees of Freedom (6DoF) poses to represent objects for parallel gripper tasks. However, these methods are insufficient for dexterous grasping, which requires handling multiple contact points and complex interactions beyond simple position and orientation. Early methods such as rigid body modeling~\cite{rosales2012synthesis, el2015computing} and template matching~\cite{gabellieri2020grasp, kokic2020learning} are task-specific and lack generalization, limiting their applicability to diverse tasks. Recent studies have focused on object structure-based grasp affordance representations, such as ContactDB~\cite{brahmbhatt2019contactdb}, which annotates object-finger contact relationships;
the method in~\cite{zhu2023toward}, which maps contact points to finger regions and intent codes; 
and F2F~\cite{yang2024task}, which uses knowledge graphs to associate functional object parts with functional fingers. While these methods improve task performance, they depend on ideal perception systems that assume precise segmentation or localization of functional regions—an assumption rarely achievable in real-world settings. To address these limitations, we propose a robust object representation method specifically designed for dexterous grasping, eliminating the need for idealized perception inputs and enabling reliable handling of complex interactions.

\subsection{Keypoint Representation and Robotic Manipulation}
Keypoint-based methods have been widely used in computer vision tasks such as face recognition~\cite{mayo20093d, berretti20113d}, human pose estimation~\cite{belagiannis2017recurrent}, and tracking~\cite{chan2017robust}, where keypoints typically serve as low-level features or part-level object descriptors. In robotics, keypoints provide compact representations of the environment and objects. 
For example, KETO~\cite{qin2020keto} defines three types of keypoints—grasp points, functional points, and operation points—to describe tasks, whereas SKP~\cite{luo2022skp} defines five keypoints directly on the object surface to support parallel grasping. However, these methods are task-specific and struggle to generalize to new tasks. Recent advancements in large models have introduced generalizable representations for robotic manipulation, such as ReKep~\cite{huang2024rekep}, which employs  LVMs~\cite{oquab2023dinov2, kirillov2023segment} to extract candidate keypoints and vision-language models to filter task-relevant keypoints for direct operational guidance. 
However, ReKep~\cite{huang2024rekep} focuses on simple parallel gripper tasks and requires additional reasoning steps, making it unsuitable for dexterous manipulation. 
Inspired by human hand interactions~\cite{hang2024dexfuncgrasp}, we propose a multi-keypoint representation based on the wrist, functional fingers, and the little finger. This design directly constrains dexterous grasping postures, providing effective and robust solutions for complex manipulation tasks.

\subsection{Visual Affordance and Interaction}
Visual affordance learning explores potential object regions for specific actions and is a key topic in robotic grasping. 
Early fully supervised methods~\cite{nguyen2017object,yang2023grounding} relied on large-scale annotated datasets, which were both expensive and time-consuming to create. To reduce annotation costs, recent research has shifted toward weakly supervised methods, leveraging keypoints~\cite{sawatzky2017weakly,sawatzky2017adaptive} or image-level labels~\cite{luo2022learning,li2023locate,nagarajan2019grounded}. 
In this work, we utilize human interaction images to supervise Ego-view images through contact features, significantly reducing training data costs by leveraging existing resources. Existing affordance methods for robotic manipulation, such as VRB~\cite{bahl2023affordances}, learn contact points and trajectories from human operation videos, whereas Robo-ABC~\cite{ju2024robo} generates hand-object contact datasets to enable zero-shot generalization. Similarly, GAT~\cite{li2024learning} proposes pixel-level affordance learning to capture precise regions. However, these methods often depend on post-processing steps and additional modules, and their affordance regions are typically coarse, failing to provide the fine-grained constraints required for dexterous grasping. To address these limitations, we propose a multi-region keypoint affordance learning approach that directly provides fine-grained constraints tailored for dexterous grasping tasks.

\begin{figure*}[t!]
\centerline{\includegraphics[width=1\textwidth]{img/pip.pdf}}
\captionsetup{font=small}
\caption{\small Framework of the proposed CMKA. The framework includes: (1) a LVM-based Multi-Scale Clustering (LMSC) module for extracting candidate keypoints; (2) Keypoint feature extraction from egocentric (Ego) view; (3) Contact geometry knowledge transfer from exocentric (Exo) view to Ego view.}
\label{pipline}
\vskip-2ex
\end{figure*}