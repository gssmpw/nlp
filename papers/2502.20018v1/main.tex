%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                             
\overrideIEEEmargins                     
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{graphics} 
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{pifont}

\usepackage{colortbl}

\usepackage[table,xcdraw]{xcolor}
\usepackage{amssymb}
\newcommand{\YKL}[1]{\textcolor{red}{#1}}
\newcommand{\YF}[1]{\textcolor{orange}{#1}}
\definecolor{rblue}{rgb}{0,0.5,1}
\definecolor{awesome}{rgb}{1.0, 0.13, 0.32}
\definecolor{hollywoodcerise}{rgb}{0.96, 0.0, 0.63}
\definecolor{lasallegreen}{rgb}{0.03, 0.47, 0.19}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}

\definecolor{mygray}{gray}{.9}
\definecolor{mygreen}{RGB}{93,174,86}

\makeatletter
\newcommand{\thickhline}{%
	\noalign {\ifnum 0=`}\fi \hrule height 1pt
	\futurelet \reserved@a \@xhline
}

\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{colorlinks=true,linkcolor={red},citecolor={hanpurple},urlcolor={magenta}}
\makeatletter
\let\NAT@parse\undefined
\makeatother
%\usepackage[colorlinks=true,linkcolor=red,citecolor=blue]{hyperref}
%\usepackage[colorlinks,linkcolor=blue,anchorcolor=blue,urlcolor=magenta,citecolor=awesome]{hyperref}
\usepackage{xcolor}
\definecolor{rblue}{rgb}{0,0.5,1}
\definecolor{hollywoodcerise}{rgb}{0.96, 0.0, 0.63}
\definecolor{lasallegreen}{rgb}{0.03, 0.47, 0.19}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{colorlinks,linkcolor={red},citecolor={hanpurple},urlcolor={magenta}} 

\title{\LARGE \bf
Multi-Keypoint Affordance Representation for Functional\\Dexterous Grasping
}

%\author{Anonymous Submission
\author{Fan Yang$^{1,2}$, Dongsheng Luo$^{1}$, Wenrui Chen$^{1,2,*}$, Jiacheng Lin$^{3}$, Junjie Cai$^{1}$,\\Kailun Yang$^{1,2}$, Zhiyong Li$^{1,2,3}$, and Yaonan Wang$^{1,2}$% <-this % stops a space
%\thanks{This work was partially supported by National Key RD Program of China under Grant 2022YFB4701400, the National Natural Science Foundation of China under Grant 62273137, 62473139, U21A20518, and U23A20341, Hunan Province RD project under Grant 2022SK218.}
\thanks{$^{1}$The authors are with the School of Robotics, Hunan University, China.}%
\thanks{$^{2}$The authors are also with the National Engineering Research Center of Robot Visual Perception and Control Technology, Hunan University, China.}
\thanks{$^{3}$The authors are with the College of Computer Science and Electronic Engineering, Hunan University, Changsha, China.}
\thanks{$^{*}$Corresponding authors: Wenrui Chen.}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
Functional dexterous grasping requires precise hand-object interaction, going beyond simple gripping. Existing affordance-based methods primarily predict coarse interaction regions and cannot directly constrain the grasping posture, leading to a disconnection between visual perception and manipulation. To address this issue, we propose a multi-keypoint affordance representation for functional dexterous grasping, which directly encodes task-driven grasp configurations by localizing functional contact points. Our method introduces Contact-guided Multi-Keypoint Affordance (CMKA), leveraging human grasping experience images for weak supervision combined with Large Vision Models for fine affordance feature extraction, achieving generalization while avoiding manual keypoint annotations. Additionally, we present a Keypoint-based Grasp matrix Transformation (KGT) method, ensuring spatial consistency between hand keypoints and object contact points, thus providing a direct link between visual perception and dexterous grasping actions. Experiments on public real-world FAH datasets, IsaacGym simulation, and challenging robotic tasks demonstrate that our method significantly improves affordance localization accuracy, grasp consistency, and generalization to unseen tools and tasks, bridging the gap between visual affordance learning and dexterous robotic manipulation. The source code and demo videos will be publicly available at \url{https://github.com/PopeyePxx/MKA}.
\end{abstract}

\section{Introduction}
Functional dexterous grasping is a key capability for robots to perform complex object manipulations based on human instructions. Unlike traditional simple grasping, it requires a robotic dexterous hand to generate diverse grasping postures and make contact with different object regions depending on the task. This involves intricate physical interactions between the fingers and the object. For instance, in the ``\textit{Hold Drill}'' task, the robot’s five fingers must firmly grasp the drill head, while in the ``\textit{Press Drill}'' task, the index finger presses the drill switch while the other four fingers stabilize the handle. Thus, how to infer task-relevant object contact regions and grasping postures from visual perception is a fundamental challenge in functional dexterous grasping.

\begin{figure}[t!]
\centerline{\includegraphics[width=0.5\textwidth]{img/moti1.pdf}}
\captionsetup{font=small}
\caption{\small Comparison between existing affordance-based grasping methods and our proposed Multi-Keypoint Affordance representation. 
(a) Existing methods identify only a rough interaction region, leading to uncertain interaction poses. 
(b) Our method localizes multiple keypoints corresponding to dexterous hand joints, enabling a unique and constrained grasping posture.}
\label{intro1}
\vskip-2ex
\end{figure}

In the field of vision, affordance-based methods~\cite{gibson1977theory, myers2015affordance,xu2021affordance, luo2022learning,xu2024weakly} have been widely explored to predict potential human interaction regions. Deep-learning-based approaches estimate heatmaps~\cite{luo2022learning,xu2024weakly} or segmentation masks~\cite{myers2015affordance,xu2021affordance} to indicate feasible interaction areas. However, existing methods~\cite{nguyen2024language,li2024learning} can only provide coarse region predictions given an image and a task. A rough affordance map cannot specify the exact interaction posture, leading to uncertainty in the grasping motion and insufficient constraints for functional dexterous grasping, as shown in Fig.~\ref{intro1}(a). Therefore, how to find a novel visual representation that not only identifies task-relevant contact areas but also directly constrains the dexterous grasping posture, ensuring a well-defined interaction between the hand and the object, remains a challenging problem.

Keypoint-based representations offer a potential solution by structuring high-dimensional visual data into a compact and interpretable form. 
Many studies~\cite{koppula2013learning,manuelli2019kpam,qin2020keto,luo2022skp} have demonstrated the effectiveness of keypoint-based approaches in robotic manipulation, often decomposing grasping tasks into object and environment keypoints. For instance, KETO~\cite{qin2020keto} defines three types of keypoints: grasp points, functional points, and operation points. 
SKP~\cite{luo2022skp} directly defines five keypoints on the object’s surface to support parallel grasping. 
However, these methods exhibit limitations in their visual representation: either the keypoints are manually defined for specific tasks, limiting generalization to novel objects and tasks, or they rely heavily on simulated environments for training, reducing their real-world applicability. Additionally, many of these approaches require extensive manual annotations, further increasing data collection costs.

To improve the generalization and applicability of keypoint-based representations, VRB~\cite{bahl2023affordances} introduces a more flexible visual representation by learning contact points and motion trajectories from human operation videos, demonstrating enhanced performance in robotic manipulation tasks. 
However, this method relies on post-processing steps, and its visual representation remains indirect. More recent advances in Large Vision Models (LVM) have significantly improved object feature extraction. 
For instance, ReKep~\cite{huang2024rekep} leverages LVMs to automatically extract candidate keypoints, and then filters them using vision-language models, directly guiding robotic operations. This approach enhances task generalization and establishes a more direct link between vision and action.

Despite successes, the above methods primarily focus on simple two-finger pinch grasps and do not extend to dexterous grasping tasks. In dexterous grasping, keypoints must not only determine the grasping location but also constrain the entire hand configuration, ensuring functional stability, as shown in Fig.~\ref{intro1}(b). Achieving this goal introduces three key challenges: (1) \textbf{Fine-grained feature extraction:} Dexterous grasping involves small, detailed interaction regions between fingers and the object. How can part-level keypoint features be extracted from the object? (2) \textbf{Data annotation cost:} Dexterous grasping requires precise keypoint annotations, which are costly to acquire. How can reliance on manual annotation be reduced? (3) \textbf{Keypoint correspondence:} Establishing a consistent mapping between object keypoints and hand keypoints is essential for stable grasping. How can robust keypoint correspondence be ensured?

To address the challenges, we propose the Multi-KeyPoint Affordance representation for Functional Dexterous Grasping. By localizing multiple keypoints on the object and the hand, a unique dexterous grasping posture with clear constraints is determined. 
First, we introduce the Contact-guided Multi-Keypoint Affordance (CMKA) learning, which leverages LVMs (\textit{e.g.}, SAM~\cite{kirillov2023segment} and DINOv2~\cite{oquab2023dinov2}) for fine-grained affordance feature extraction. The CMKA supervises Egocentric (Ego)-view images using hand-object interaction regions in Exocentric (Exo)-view images as contact priors via CAM~\cite{zhang2018adversarial}, guiding keypoint learning towards meaningful functional contact areas and eliminating the need for manual keypoint annotations. Then, we introduce the Keypoint-based Grasp matrix Transformation (KGT) method to ensure consistent mapping between hand and object keypoints. We observe that the wrist, functional fingers (index or thumb), and little finger MCP joints effectively reflect the relative contact posture between the hand and the object. The positional relationship of these three points forms a unique triangular structure, providing a direct connection between hand and object keypoints. We conduct comprehensive experiments to evaluate the proposed framework for multi-point affordance localization across 6 tasks and 18 tool shapes on the public FAH dataset~\cite{yang2024learning}, achieving a 45.35\% improvement over the state-of-the-art method in the KLD metric. In both IsaacGym~\cite{makoviychuk2021isaac} and real robot experiments, we successfully establish the geometric constraint relationship between tool and hand keypoints.

The main contributions of this work are as follows:
\begin{itemize}
    \item A novel multi-keypoint affordance representation is proposed, which constrains dexterous grasping postures through the geometric relationships of keypoints in the hand-object interaction region, directly establishing a link between vision and dexterous grasping actions.
    \item CMKA, a multi-keypoint affordance localization method based on a weakly-supervised framework, and KGT, a keypoint-based hand-object relative pose transformation method, are introduced, leveraging existing human interaction image data and LVMs for learning, effectively reducing data costs, and enabling functional dexterous grasping.
    \item The proposed algorithm is validated in both simulation and real robot experiments, demonstrating its ability to directly map tasks to grasping actions while exhibiting good generalization across tasks and objects, especially excelling in complex functional grasping scenarios.
\end{itemize}


\section{Related Work}

\begin{figure*}[t!]
\centerline{\includegraphics[width=1\textwidth]{img/flow.pdf}}
\captionsetup{font=small}
\caption{\small The key process of learning and connecting visual perception to functional dexterous grasping actions. The CMKA learning module learns from Exo images with human operation experience and transfers the knowledge to Ego images, locating three keypoints constrained by dexterous grasping. The KGT method maps the hand-object relative pose, calculating the dexterous hand's rotation and translation parameters (\(R\), \(T\)) to control the grasping task.}
\label{flow}
\vskip-2ex
\end{figure*}

\subsection{Object Representation for Dexterous Grasping}

Grasping and manipulation are fundamental topics in robotics. Traditional methods~\cite{srivastava2014combined, tyree20226, wen2024foundationpose} often rely on six Degrees of Freedom (6DoF) poses to represent objects for parallel gripper tasks. However, these methods are insufficient for dexterous grasping, which requires handling multiple contact points and complex interactions beyond simple position and orientation. Early methods such as rigid body modeling~\cite{rosales2012synthesis, el2015computing} and template matching~\cite{gabellieri2020grasp, kokic2020learning} are task-specific and lack generalization, limiting their applicability to diverse tasks. Recent studies have focused on object structure-based grasp affordance representations, such as ContactDB~\cite{brahmbhatt2019contactdb}, which annotates object-finger contact relationships;
the method in~\cite{zhu2023toward}, which maps contact points to finger regions and intent codes; 
and F2F~\cite{yang2024task}, which uses knowledge graphs to associate functional object parts with functional fingers. While these methods improve task performance, they depend on ideal perception systems that assume precise segmentation or localization of functional regions—an assumption rarely achievable in real-world settings. To address these limitations, we propose a robust object representation method specifically designed for dexterous grasping, eliminating the need for idealized perception inputs and enabling reliable handling of complex interactions.

\subsection{Keypoint Representation and Robotic Manipulation}
Keypoint-based methods have been widely used in computer vision tasks such as face recognition~\cite{mayo20093d, berretti20113d}, human pose estimation~\cite{belagiannis2017recurrent}, and tracking~\cite{chan2017robust}, where keypoints typically serve as low-level features or part-level object descriptors. In robotics, keypoints provide compact representations of the environment and objects. 
For example, KETO~\cite{qin2020keto} defines three types of keypoints—grasp points, functional points, and operation points—to describe tasks, whereas SKP~\cite{luo2022skp} defines five keypoints directly on the object surface to support parallel grasping. However, these methods are task-specific and struggle to generalize to new tasks. Recent advancements in large models have introduced generalizable representations for robotic manipulation, such as ReKep~\cite{huang2024rekep}, which employs  LVMs~\cite{oquab2023dinov2, kirillov2023segment} to extract candidate keypoints and vision-language models to filter task-relevant keypoints for direct operational guidance. 
However, ReKep~\cite{huang2024rekep} focuses on simple parallel gripper tasks and requires additional reasoning steps, making it unsuitable for dexterous manipulation. 
Inspired by human hand interactions~\cite{hang2024dexfuncgrasp}, we propose a multi-keypoint representation based on the wrist, functional fingers, and the little finger. This design directly constrains dexterous grasping postures, providing effective and robust solutions for complex manipulation tasks.

\subsection{Visual Affordance and Interaction}
Visual affordance learning explores potential object regions for specific actions and is a key topic in robotic grasping. 
Early fully supervised methods~\cite{nguyen2017object,yang2023grounding} relied on large-scale annotated datasets, which were both expensive and time-consuming to create. To reduce annotation costs, recent research has shifted toward weakly supervised methods, leveraging keypoints~\cite{sawatzky2017weakly,sawatzky2017adaptive} or image-level labels~\cite{luo2022learning,li2023locate,nagarajan2019grounded}. 
In this work, we utilize human interaction images to supervise Ego-view images through contact features, significantly reducing training data costs by leveraging existing resources. Existing affordance methods for robotic manipulation, such as VRB~\cite{bahl2023affordances}, learn contact points and trajectories from human operation videos, whereas Robo-ABC~\cite{ju2024robo} generates hand-object contact datasets to enable zero-shot generalization. Similarly, GAT~\cite{li2024learning} proposes pixel-level affordance learning to capture precise regions. However, these methods often depend on post-processing steps and additional modules, and their affordance regions are typically coarse, failing to provide the fine-grained constraints required for dexterous grasping. To address these limitations, we propose a multi-region keypoint affordance learning approach that directly provides fine-grained constraints tailored for dexterous grasping tasks.

\begin{figure*}[t!]
\centerline{\includegraphics[width=1\textwidth]{img/pip.pdf}}
\captionsetup{font=small}
\caption{\small Framework of the proposed CMKA. The framework includes: (1) a LVM-based Multi-Scale Clustering (LMSC) module for extracting candidate keypoints; (2) Keypoint feature extraction from egocentric (Ego) view; (3) Contact geometry knowledge transfer from exocentric (Exo) view to Ego view.}
\label{pipline}
\vskip-2ex
\end{figure*}

\section{Methodology}
In this study, our goal is to develop a complete system that establishes a direct visual representation for functional dexterous grasping and achieves cross-task and cross-object generalization. As shown in Fig.~\ref{flow}, during the training process, the proposed Contact-guided Multi-Keypoint Affordance (CMKA) learning module acquires human operation experience from exocentric (Exo) images and transfers it to egocentric (Ego) images, identifying three keypoints constrained by dexterous grasping in the Ego image. The details of this process will be explained in Sec.~\ref{cmak}. We then use the Keypoint-based Grasp matrix Transformation (KGT) method to map and constrain the hand-object relative pose using these keypoints, calculating the rotation and translation parameters (\(R\), \(T\)) of the dexterous hand, thereby enabling control of the grasping task. The relevant details will be further described in Sec.~\ref{kgt}. During the inference process, only Ego images are required as input, and the parameters learned by CMKA can be used to identify the three affordance keypoints of the object.

\subsection{Contact-guided Multi-KeyPoint Affordances Learning}
\label{cmak}

To identify the keypoint regions on the object surface where the fingers should make contact, robust fine-grained feature extraction is required. To achieve this, as shown in Fig.~\ref{pipline}, we first employ LVM-based Multi-Scale Clustering (LMSC) to obtain candidate keypoints from different parts of the object surface (see Sec. \ref{vmsc}). Next, we perform keypoint feature extraction from the Ego-view (see Sec. \ref{kfe}) and design a keypoint weighting learning mechanism, which computes a weighted score for each candidate keypoint and selects the top three keypoints. Then, a keypoint-guided feature extraction module is used to perform deep feature extraction on the selected regions in the Ego-view images. Finally, we leverage cues from the Exo-view for knowledge transfer of the contact geometry relationship (see Sec. \ref{cgkt}), using learnable Class Activation Mapping (CAM) technology~\cite{zhang2018adversarial} to extract hand-object interaction features from the Exo-view images, and employ cosine similarity loss to supervise keypoint selection in the Ego-view images, ensuring that the selected keypoints are concentrated around the hand-object contact regions.

\subsubsection{LVM-based Multi-Scale Clustering Module}\label{vmsc}

\begin{figure}[t!]
\centerline{\includegraphics[width=0.48\textwidth]{img/lvm.pdf}}
\captionsetup{font=small}
\caption{\small Candidate keypoint selection using the proposed LMSC module.}
\label{kp}
\vskip-2ex
\end{figure}
 
Inspired by the ReKep ~\cite{huang2024rekep}, we propose an LMSC module, which aims to focus clustering on finger contact regions, as shown in Fig.~\ref{kp}. 

First, we extract multi-level features from multiple intermediate layers of the DINOv2 model~\cite{oquab2023dinov2} to capture information from low to high levels. Specifically, we extract features from the first three layers, denoted as \( F_{li} \) (where \( i = 1, 2, 3 \)) and apply linear transformations and normalization to each layer. Next, a learnable weight vector \( \alpha_i \) is used to perform weighted fusion, where the weights are normalized using the softmax function. The final fused feature representation is obtained as:
\[
F = \sum_{i=1}^{3} \alpha_i F_{li}
.\]
Then, \( F \) is passed through a multi-layer perceptron (MLP) to obtain the feature representation \( f \). To further incorporate multi-scale information, we perform upsampling and downsampling on \( f \) to obtain \( f_{\uparrow} \) and \( f_{\downarrow} \), respectively. The final multi-scale feature representation is obtained by summation:
\[
f' = f + f_{\downarrow} + f_{\uparrow}.
\]
Meanwhile, recent vision foundation models like Segment Anything Model ~\cite{kirillov2023segment} (SAM) have demonstrated strong capacity to produce robust zero-shot segmentation in real-world scenes. we apply SAM ~\cite{kirillov2023segment} to the Ego-view image to obtain multi-region masks $ M_i$ and perform region-wise clustering on the multi-scale feature representation \( f' \) using the non-background masks. Specifically, we first apply PCA to reduce the dimensionality of each region's features, obtaining the reduced feature representation \( X_{\text{PCA}} \in \mathbb{R}^{n \times k} \), where \( n \) is the number of pixels in the region and \( k \) is the reduced dimension. Then, we perform K-means clustering on the reduced features to obtain multiple candidate keypoints \( K_{cn} \), selecting the center of each cluster as the final candidate keypoint. 
The clustering aims to minimize the distance between samples and cluster centers:
\[
K_{cn} = \arg\min_{\mathbf{C}} \sum_{i=1}^{n} \| X_{\text{PCA}}^{(i)} - C_j \|^2,
\]
where \( K_{cn} \) represents the \(n\)-th candidate keypoint, \( C_j \) is the center of the \(j\)-th cluster, and \( X_{\text{PCA}}^{(i)} \) is the feature of the \(i\)-th sample in the region. Finally, we select the center of each cluster as the candidate keypoint. If no valid candidates are found, the pixel closest to the object centroid is chosen as a fallback keypoint.

\subsubsection{Keypoint Feature Extraction from Egocentric View}\label{kfe}
To extract keypoint features from the Ego view image, we define a set of learnable weights \( W \in \mathbb{R}^{t \times n} \), where \( t \) represents the number of task types and \( n \) is the number of candidate keypoints. These weights are multiplied with the candidate keypoints \( K_{cn} \) to select the three keypoints \( K_i \) (where \( i = 1, 2, 3 \)) for feature extraction from the corresponding regions in the Ego view image.

For the selected keypoints \( K_i \), we define a circular region centered at each keypoint with a radius \( r \) and extract features from these regions. The extracted region features are denoted as \( F_{ki} \), representing the features from the regions centered on the selected keypoints.

To align the features from the Ego and Exo views in a unified feature space, we apply a linear transformation to the extracted keypoint features \( F_{ki} \), resulting in the final keypoint features \( f_{op} \):
\[
f_{ki} = \text{proj}(F_{ki}),
\]
where the projection layer \( \text{proj} \) is a linear transformation that maps the Ego view features to the feature space aligned with the Exo view.

\subsubsection{Contact Geometry Knowledge Transfer}\label{cgkt}
In the final step, we utilize the CAM technique~\cite{zhang2018adversarial} to classify Exo's features for the specific task, with the classification loss denoted as \( L_{\text{cls}} \). 
Additionally, we extract object features from the interactive regions using the Extract and PartSelect~\cite{li2023locate} modules, obtaining the prototype features \( f_{\text{op}} \) for the keypoint regions.

For the three keypoint features \( f_i \) extracted from the Ego view, we compute their sum to obtain the global keypoint feature \( f_{\text{gk}} \), which encapsulates the contact geometry information from the Ego perspective:
\[
f_{\text{gk}} = \sum_{i=1}^{3} f_{ki}.
\]
Next, we calculate the cosine similarity loss \( L_{\text{cos}} \) between the Exo prototype features \( f_{\text{op}} \) and the global Ego keypoint features \( f_{\text{gk}} \):
\[
L_{\text{cos}} = 1 - \frac{f_{\text{op}} \cdot f_{\text{gk}}}{\| f_{\text{op}} \| \| f_{\text{gk}} \|}.
\]

The final loss is the combination of the classification loss and the cosine similarity loss, ensuring that the contact geometry knowledge is accurately transferred between the two views:
\[
L = L_{\text{cls}} + L_{\text{cos}}.
\]

\subsection{Keypoint-based Grasp Matrix Transformation}\label{kgt}
After obtaining the three keypoints \( K_i \) on the object, we apply the KGT to obtain the relative pose transformation matrix \((R, T)\) between the dexterous hand and the tool. Specifically, as shown in the Fig. \ref{hto} (a), we take \( K_0 \) as the reference point, determine the direction from \( K_0 \) to \( K_1 \), and form a plane using \( K_0 \), \( K_1 \), and \( K_2 \). Based on the hand model (yellow triangle), we adjust the keypoints, resulting in the corrected contact points positions in the world coordinate system \( F_o \), \( L_o \), and \( W_o \). 

\begin{figure}[!t]
\centerline{\includegraphics[width=0.48\textwidth]{img/hto.pdf}}
\captionsetup{font=small}
\caption{\small Illustration of KGT method in IsaacGym \cite{makoviychuk2021isaac}, showing the keypoints on the object and the hand (functional finger, little finger, and wrist) and their role in coordinate frame construction.}
\label{hto}
\vskip-2ex
\end{figure}

Then, as shown in the Fig. \ref{hto} (b), we define the object coordinate system \( O \) with \( W_o \) as the origin, the x-axis as \( \mathbf{x}_o = \frac{\overrightarrow{W_oF_o}}{\|\overrightarrow{W_oF_o}\|} \), the z-axis as \( \mathbf{z}_o = \frac{\overrightarrow{W_oF_o} \times \overrightarrow{W_oL_o} }{\|\overrightarrow{W_oF_o} \times \overrightarrow{W_oL_o}  \|} \), and the y-axis as \( \mathbf{y}_o = \mathbf{z}_o \times \mathbf{x}_o \), leading to the object rotation matrix in the world coordinate system:
\[
R_O^I = [\mathbf{x}_o, \mathbf{y}_o, \mathbf{z}_o].
\]
At the same time, obtain the transformation matrix between the world coordinate system \( I \) and the object coordinate system \( O \):
\[
T_O^I =
\begin{bmatrix}
 R_O^I &   W_o  \\
 0 & 1
\end{bmatrix}
\]
Similarly, the hand coordinate system \( H \) is defined with \( W \) as the origin, the x-axis as \( \mathbf{x}_h = \frac{\overrightarrow{WF}}{\|\overrightarrow{WF}\|} \), the z-axis as \( \mathbf{z}_h = \frac{\overrightarrow{WF} \times \overrightarrow{WL} }{\|\overrightarrow{WF} \times \overrightarrow{WL}  \|} \), and the y-axis as \( \mathbf{y}_h = \mathbf{z}_h \times \mathbf{x}_h \), where \( F \), \( L \), and \( W \) represent the  keypoints positions on the hand in the world coordinate system, yielding the hand rotation matrix:
\[
R_H^I = [\mathbf{x}_h, \mathbf{y}_h, \mathbf{z}_h].
\]
The relative rotation matrix between the hand and the object is then computed as:
\[
R = (R_O^I)^{-1} R_H^I,
\]
while the translation vector is given by:
\[
T = (T_O^I)^{-1}( W-W_o).
\]

\section{Experiments}
\begin{figure*}[t!]
\centerline{\includegraphics[width=0.9\textwidth]{img/kp1.pdf}}
\captionsetup{font=small}
\caption{\small Qualitative comparison between our approach and the state-of-the-art multi-keypoint affordance grounding method (ReKep*~\cite{huang2024rekep}) on the FAH test set~\cite{yang2024learning}.
Our proposed method predicts keypoints that are more concentrated in the contact areas and captures the geometric information of the grasping posture. }
\label{vs1}
\vskip-2ex
\end{figure*}
\subsection{Setup}

\textbf{Datasets:} 
The public challenging FAH benchmark~\cite{yang2024learning} is a large-scale affordance-annotated dataset specifically designed for hand-object interactions. 
It contains $6$ functional grasp affordances and $18$ tools, with $5,858$ images spanning both exocentric (Exo) and egocentric (Ego) views. The dataset provides image-level affordance labels for weakly supervised learning and annotations for coarse dexterous grasp gestures targeting specific ``\textit{Task Tool}'' pairs. 
However, its test set only includes heatmap annotations for functional finger contact regions. To address this limitation, we sparsely annotate two additional contact points (little finger and wrist projection points). 
Specifically, polygons with up to five points are constructed around finger keypoints within a $5mm$ radius, and Gaussian kernels are applied at each point to generate dense annotations. During training, point annotations are added to the object regions in each Ego-view image to distinguish foreground and background during segmentation with SAM~\cite{kirillov2023segment}. 

\textbf{Implementation Details:} Experiments are conducted on an NVIDIA RTX A6000 GPU. The model is trained using the SGD optimizer with a learning rate of $0.01$ over $15$ iterations. Images are resized to a resolution of \( 448{\times}448 \). Following prior works~\cite{li2023locate,luo2022learning}, we evaluate the results using Kullback-Leibler Divergence (KLD), Similarity (SIM), and Normalized Scanpath Saliency (NSS) metrics.

\subsection{Results of Functional Affordance Grounding}
In this section, we present qualitative and quantitative results to demonstrate the effectiveness and efficiency of our method on the FAH test set~\cite{yang2024learning}. 
As weakly- or unsupervised methods for multi-region affordance localization are scarce in the state of the art, we use ReKep*~\cite{huang2024rekep}, a keypoint prediction method, as our baseline.

\textbf{Quantitative Results.} 
As shown in Tab.~\ref{pre_table}, our method significantly outperforms ReKep*~\cite{huang2024rekep} across multiple metrics. Specifically, it improves KLD by $45.35\%$, increases SIM by $54.19\%$, and improves NSS by $101.63\%$. 
These improvements stem from ReKep*'s lack of adaptation to dexterous grasping. 
While ReKep*~\cite{huang2024rekep} originally relies on manually selected keypoints, its modified version ReKep*~\cite{huang2024rekep} randomly generates three keypoints without explicit modeling of functional contact regions. In contrast, our method employs a learnable weighting mechanism to generate keypoints specifically for dexterous grasping, ensuring their alignment with functional contact regions. 

\begin{table}[!t]
\centering
\captionsetup{font=small}
\caption{\small Comparison to state-of-the-art method on the FAH test set~\cite{yang2024learning}. 
The \textbf{best} results are highlighted in bold. (↑/↓ means higher/lower is better).}
\label{pre_table}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model  }                           & \textbf{KLD} ($\downarrow$) & \textbf{SIM }($\uparrow$) & \textbf{NSS} ($\uparrow$) \\ \midrule
ReKep*~\cite{huang2024rekep} & 9.213              & 0.203      & 0.429   \\
Ours & \textbf{5.035}  & \textbf{0.313}    & \textbf{0.865}\\ \bottomrule
\end{tabular}
\vskip-2ex
\end{table}

\textbf{Hyperparameter Analysis.} 
We further investigate the impact of the candidate keypoint number $N{=}\{6,9,12,15\}$ on model performance,  
In Tab.~\ref{tab:results}, we show the effects of different values on KLD, SIM, and NSS. The results indicate that $N{=}12$ achieves the best performance across all metrics. This aligns with our design principle: too few keypoints lead to insufficient representation of affordance regions, hindering fine-grained grasp modeling, while too many introduce redundancy, diluting feature importance and reducing the model's focus on functional regions.

\textbf{Ablation Study.} The object priors provided by SAM \cite{kirillov2023segment} are crucial for constraining keypoint proposals to objects in the scene rather than the background. Thus, we focus on analyzing the critical visual feature extraction network in CMKA. As shown in Tab. \ref{ablation}, DINOv2 \cite{oquab2023dinov2}, as the backbone network combined with our designed multi-scale feature fusion (MSFF) module, achieves the best performance. In the backbone network, DINOv2 generates clearer features compared to DINO-ViT \cite{DBLP:journals/corr/abs-2112-05814}, better distinguishing fine-grained object regions and leading to improved performance. Furthermore, compared to replacing MSFF with a simple fully connected network, MSFF, with its multi-layer and multi-scale feature mapping, demonstrates superior potential.

\begin{table}[t!]
    \centering
    \captionsetup{font=small}
    \caption{\small Impact of the candidate keypoint number $N$.}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{$N$} & \textbf{KLD} ($\downarrow$) & \textbf{SIM} ($\uparrow$) & \textbf{NSS} ($\uparrow$) \\
        \midrule
        6 & 5.409 & 0.308 & 0.849 \\
        9 & 5.748 & 0.282 & 0.737 \\
        \textbf{12} & \textbf{5.035} & \textbf{0.313} & \textbf{0.865} \\
        15 & 5.766 & 0.279 & 0.723 \\
        \bottomrule
    \end{tabular}
    \label{tab:results}
    \vskip-2ex
\end{table}

\begin{table}[t!]
    \centering
    \captionsetup{font=small}
    \caption{\small Ablation study on different feature extractors. FFL: feed-forward layer. MSFF: multi-scale feature fusion.}
    \begin{tabular}{@{}l@{\hskip 5pt}l@{\hskip 5pt}l@{\hskip 5pt}l@{\hskip 5pt}|@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{}}
        \toprule
        \textbf{DINOv2} & \textbf{DINO-ViT}  & \textbf{MSFF} & \textbf{FFL} & \textbf{KLD} ($\downarrow$) & \textbf{SIM} ($\uparrow$) &\textbf{ NSS} ($\uparrow$) \\
        \midrule
        \textbf{\checkmark} & &\textbf{\checkmark}  & &\textbf{5.035} & \textbf{0.313} & \textbf{0.865} \\
      \textbf{\checkmark} & & &\textbf{\checkmark}& 5.517 & 0.302 & 0.67 \\
        &\textbf{\checkmark} &\textbf{\checkmark} & & 5.807 & 0.267 & 0.77\\
        &\textbf{\checkmark} & & \textbf{\checkmark}  & 6.075 & 0.253 & 0.65 \\
        \bottomrule
    \end{tabular}
    \label{ablation}
    \vskip-2ex
\end{table}

\textbf{Qualitative Analysis.} 
As shown in Fig.~\ref{vs1}, the visibility grounding visualizations include Ground Truth (GT), our method, and the baseline method ReKep*~\cite{huang2024rekep}. Compared to the baseline, our method localizes keypoints within the hand-object contact region while preserving the relative spatial relationships among the functional finger, little finger, and wrist projection, ensuring a meaningful distribution for dexterous grasping. For example, in the ``\textit{Click Flashlight}'' task, keypoint 0 is localized on the thumb and keypoint 1 on the little finger, accurately reflecting the contact regions. In the ``\textit{Press Drill}'' task, keypoint 0 is placed on the index finger, keypoint 1 on the little finger, and keypoint 2 on the wrist projection, maintaining a reasonable spatial relationship. In contrast, ReKep*~\cite{huang2024rekep} relies on manual post-processing, failing to constrain keypoints within the contact region and lacking spatial consistency, resulting in scattered keypoints and reduced accuracy of the affordance region.

\subsection{Evaluation of Keypoint-Based Grasp Transformation}
To validate the effectiveness of the keypoint-based dexterous grasp transformation method KGT, we conduct experiments on four ``\textit{Task Tool}'' combinations: \textit{``Press Drill''}, \textit{``Hold Drill''}, \textit{``Click Flashlight''}, and \textit{``Hold Flashlight''}. 
As shown in Fig.~\ref{issac}, we visualized the initial and final hand-object states in the simulation environment IsaacGym~\cite{makoviychuk2021isaac}. The results demonstrate that our method accurately computes the grasp transformation matrix, enabling precise hand-object interaction across different task-tool combinations with varying initial hand-object relative postures. For functional interaction tasks, such as \textit{``Press Drill''} and \textit{``Click Flashlight''}, the method ensures correct contact between the functional fingers and the target components. For general grasping tasks, such as \textit{``Hold''}, our method achieves a natural grasp, ensuring a reasonable hand posture.

\begin{figure}[t!]
\centerline{\includegraphics[width=0.5\textwidth]{img/issac_gym.pdf}}
\captionsetup{font=small}
\caption{\small Visualization of initial and final hand-object states in IsaacGym~\cite{makoviychuk2021isaac} for different ``\textit{Task Tool}'' combinations. The red spheres represent the three keypoints used for grasp transformation.}
\label{issac}
\vskip-1ex
\end{figure}

\subsection{Performance in Real-World Scenarios}

\begin{figure}[t!]
\centerline{\includegraphics[width=0.5\textwidth]{img/plat.pdf}}
\captionsetup{font=small}
\vskip-1ex
\caption{\small Experimental setup in a real-world scenario: (a) Hardware platform; (b) Calibration standards for the geometric relationship between keypoints when the functional finger is the index finger (upper) and the thumb (lower).}
\label{plat}
\end{figure}

\begin{figure}[!t]
  \centering
  \small
  \includegraphics[width=0.48\textwidth]{img/real.pdf}
  \captionsetup{font=small}
  \caption{\small Experiments with three typical ``\textit{Task Tools}'' in real-world scenarios: ``\textit{Click Flashlight}'', ``\textit{Press Drill}'' and ``\textit{Press Spraybottle}'' (from top to bottom). The upper right corner of the fourth column compares the functional dexterous grasping success rate with our method and GAAF-Dex~\cite{yang2024learning} (in bracket), where the total grasp number of each instance is $5$.}
  \label{fig:teaser}
\end{figure}

\textbf{Real-world Experiments Setup:} 
As shown in Fig.~\ref{plat}(a), the real-world platform consists of an Inspire hand, a UR5 industrial robotic arm, an Intel RealSense camera, a tool rack, and a control computer. To address real-world uncertainties, we introduce keypoint relative position calibration annotations based on the Inspire model during the grasping process, as shown in Fig.~\ref{plat}(b).

In the experiments, we use tool instances commonly found in daily life, which were unseen in the training set. As shown in Fig.~\ref{fig:teaser}, we selected three ``\textit{Task Tool}'' with strict functional grasping requirements for the experiment: ``\textit{Click Flashlight}'', ``\textit{Press Drill}'', and ``\textit{Press Spraybottle}''. We recorded four states: the initial state, followed by the localization of three affordance keypoints on the tool surface using the CMKA method to estimate their initial planar relationship. Simultaneously, we utilized an RGB-D camera to obtain the depth $z$ from the $(x,y)$ coordinates of the keypoints in the depth map, thus acquiring their $(x,y,z)$ coordinates in 3D space. We then adjusted the relative geometric positions of the keypoints on the dexterous hand palm using calibration standards (yellow triangles in the third column). Finally, we apply the KGT method to compute the final wrist grasp pose $W (R, T)$, and use the coarse gesture labels from the FAH~\cite{yang2024learning} to obtain the coarse grasp angle $J$ for each ``\textit{Task Tool}," which controls the final grasping of the dexterous hand.

The results demonstrate that for complex tasks requiring precise finger-object alignment, such as \textit{Press} and \textit{Click}, our method effectively bridges the gap between multi-point perception and dexterous grasping, showing broad practical value.  
Furthermore, due to the lack of direct methods combining perception and dexterous grasping, we compared our method with the state-of-the-art GAAF-Dex~\cite{yang2024learning} by the number of successful functional grasps. We define a successful grasp as the functional finger combining with the tool's functional component while the remaining fingers securely grasp other parts of the tool. As shown in the top left corner of Fig.~\ref{fig:teaser}, our method improves the grasp success rate by an average of $40\%$ across three complex tasks.  
GAAF-Dex~\cite{yang2024learning} is only effective when the initial and final grasp rotations are similar, such as in the \textit{Click Flashlight} scenario, due to its lack of adaptive rotation handling. In contrast, our method can handle arbitrary initial grasp poses.

\section{Conclusion and Future Work}
This work proposes a keypoint-based affordance representation for functional dexterous grasping. By leveraging human experience data for weak supervision and integrating the CMKA module with large visual models, our approach achieves precise multi-point contact localization, reducing data annotation costs and improving generalization. The KGT method enables the mapping of dexterous hand postures to object keypoints, ensuring a direct connection between perception and action. Experimental results demonstrate that the proposed method outperforms existing approaches in localization accuracy and functional grasp success rate. Practical experiments show that relying solely on 2D vision for localization fails to provide stable grasp constraints.
In the future, we aim to utilize multimodal information to enhance the accuracy and stability of multi-point 3D localization in real-world scenarios.

\bibliographystyle{IEEEtran}   
\bibliography{ref} 
\end{document}
