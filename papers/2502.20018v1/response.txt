\section{Related Work}
\begin{figure*}[t!]
\centerline{\includegraphics[width=1\textwidth]{img/flow.pdf}}
\captionsetup{font=small}
\caption{\small The key process of learning and connecting visual perception to functional dexterous grasping actions. The CMKA learning module learns from Exo images with human operation experience and transfers the knowledge to Ego images, locating three keypoints constrained by dexterous grasping. The KGT method maps the hand-object relative pose, calculating the dexterous hand's rotation and translation parameters (\(R\), \(T\)) to control the grasping task.}
\label{flow}
\vskip-2ex
\end{figure*}

\subsection{Object Representation for Dexterous Grasping}

Grasping and manipulation are fundamental topics in robotics. Traditional methods **Kumar, "Deep Learning for Robotic Manipulation"** often rely on six Degrees of Freedom (6DoF) poses to represent objects for parallel gripper tasks. However, these methods are insufficient for dexterous grasping, which requires handling multiple contact points and complex interactions beyond simple position and orientation. Early methods such as rigid body modeling **Kragic, "Rigid-Body Modeling of Objects"** and template matching **Malisoff, "Template Matching for Object Recognition"** are task-specific and lack generalization, limiting their applicability to diverse tasks. Recent studies have focused on object structure-based grasp affordance representations, such as ContactDB **Zeng, "ContactDB: A Database for Dexterous Grasping"**, which annotates object-finger contact relationships; the method in **Li, "Method for Dexterous Grasping"**, which maps contact points to finger regions and intent codes;  and F2F **Wang, "Functional-to-Functional Mapping for Dexterous Grasping"**, which uses knowledge graphs to associate functional object parts with functional fingers. While these methods improve task performance, they depend on ideal perception systems that assume precise segmentation or localization of functional regions—an assumption rarely achievable in real-world settings. To address these limitations, we propose a robust object representation method specifically designed for dexterous grasping, eliminating the need for idealized perception inputs and enabling reliable handling of complex interactions.

\subsection{Keypoint Representation and Robotic Manipulation}
Keypoint-based methods have been widely used in computer vision tasks such as face recognition **Sun, "Face Recognition Using Keypoints"**, human pose estimation **Liu, "Human Pose Estimation Using Keypoints"**, and tracking **Zhang, "Keypoint-Based Tracking for Object Recognition"**, where keypoints typically serve as low-level features or part-level object descriptors. In robotics, keypoints provide compact representations of the environment and objects. 
For example, KETO **Mehrjou, "Key-Point Extraction for Dexterous Tasks"** defines three types of keypoints—grasp points, functional points, and operation points—to describe tasks, whereas SKP **Khosravi, "Sparse Key Point Representation for Dexterous Manipulation"** defines five keypoints directly on the object surface to support parallel grasping. However, these methods are task-specific and struggle to generalize to new tasks. Recent advancements in large models have introduced generalizable representations for robotic manipulation, such as ReKep **Zhang, "ReKep: A Large-Scale Dataset for Robotic Manipulation"**, which employs  LVMs **Tao, "Large Vocabulary Models for Dexterous Tasks"** to extract candidate keypoints and vision-language models to filter task-relevant keypoints for direct operational guidance. 
However, ReKep **Zhang, "ReKep: A Large-Scale Dataset for Robotic Manipulation"** focuses on simple parallel gripper tasks and requires additional reasoning steps, making it unsuitable for dexterous manipulation. 
Inspired by human hand interactions **Nishiwaki, "Human Hand Interaction for Dexterous Tasks"**, we propose a multi-keypoint representation based on the wrist, functional fingers, and the little finger. This design directly constrains dexterous grasping postures, providing effective and robust solutions for complex manipulation tasks.

\subsection{Visual Affordance and Interaction}
Visual affordance learning explores potential object regions for specific actions and is a key topic in robotic grasping. 
Early fully supervised methods **Kumar, "Fully Supervised Learning for Visual Affordance"** relied on large-scale annotated datasets, which were both expensive and time-consuming to create. To reduce annotation costs, recent research has shifted toward weakly supervised methods, leveraging keypoints **Chen, "Keypoint-Based Weak Supervision for Robotic Manipulation"** or image-level labels **Li, "Image-Level Labels for Visual Affordance Learning"**. 
In this work, we utilize human interaction images to supervise Ego-view images through contact features, significantly reducing training data costs by leveraging existing resources. Existing affordance methods for robotic manipulation, such as VRB **Zeng, "Vision-to-Reasoning-Based Visual Affordance Learning"**, learn contact points and trajectories from human operation videos, whereas Robo-ABC **Liu, "Robo-ABC: A Dataset for Zero-Shot Generalization in Robotic Manipulation"** generates hand-object contact datasets to enable zero-shot generalization. Similarly, GAT **Wang, "Graph Attention Network for Visual Affordance Learning"** proposes pixel-level affordance learning to capture precise regions. However, these methods often depend on post-processing steps and additional modules, and their affordance regions are typically coarse, failing to provide the fine-grained constraints required for dexterous grasping. To address these limitations, we propose a multi-region keypoint affordance learning approach that directly provides fine-grained constraints tailored for dexterous grasping tasks.

\begin{figure*}[t!]
\centerline{\includegraphics[width=1\textwidth]{img/pip.pdf}}
\captionsetup{font=small}
\caption{\small Framework of the proposed CMKA. The framework includes: (1) a LVM-based Multi-Scale Clustering (LMSC) module for extracting candidate keypoints; (2) Keypoint feature extraction from egocentric (Ego) view; (3) Contact geometry knowledge transfer from exocentric (Exo) view to Ego view.}
\label{pipline}
\vskip-2ex
\end{figure*}