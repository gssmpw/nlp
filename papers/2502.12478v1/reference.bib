#mosi

@article{zadeh2016mosi,
  title={MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos},
  author={Zadeh, Amir and Zellers, Rowan and Pincus, Eli and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:1606.06259},
  year={2016}
}

#MOSEI

@inproceedings{zadeh2018multimodal,
  title={Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}

#sims v2

@inproceedings{liu2022make,
  title={Make Acoustic and Visual Cues Matter: CH-SIMS v2. 0 Dataset and AV-Mixup Consistent Module},
  author={Liu, Yihe and Yuan, Ziqi and Mao, Huisheng and Liang, Zhiyun and Yang, Wanqiuyue and Qiu, Yuanzhe and Cheng, Tie and Li, Xiaoteng and Xu, Hua and Gao, Kai},
  booktitle={Proceedings of the 2022 International Conference on Multimodal Interaction},
  pages={247--258},
  year={2022}
}

#sims

@inproceedings{yu2020ch,
  title={CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality},
  author={Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages={3718--3727},
  year={2020}
}

#meld

@article{poria2018meld,
  title={MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations},
  author={Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  journal={arXiv preprint arXiv:1810.02508},
  year={2018}
}

#iemocap

@article{busso2008iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  pages={335--359},
  year={2008},
  publisher={Springer}
}

#cherma

@inproceedings{sun2023layer,
  title={Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition},
  author={Sun, Jun and Han, Shoukang and Ruan, Yu-Ping and Zhang, Xiaoning and Zheng, Shu-Kai and Liu, Yulong and Huang, Yuxin and Li, Taihao},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={658--670},
  year={2023}
}

@article{zhu2023skeafn,
  title={SKEAFN: Sentiment knowledge enhanced attention fusion network for multimodal sentiment analysis},
  author={Zhu, Chuanbo and Chen, Min and Zhang, Sheng and Sun, Chao and Liang, Han and Liu, Yifan and Chen, Jincai},
  journal={Information Fusion},
  volume={100},
  pages={101958},
  year={2023},
  publisher={Elsevier}
}

@article{mai2023learning,
  title={Learning from the global view: Supervised contrastive learning of multimodal representation},
  author={Mai, Sijie and Zeng, Ying and Hu, Haifeng},
  journal={Information Fusion},
  volume={100},
  pages={101920},
  year={2023},
  publisher={Elsevier}
}

@article{li2022emocaps,
  title={EmoCaps: Emotion capsule based model for conversational emotion recognition},
  author={Li, Zaijing and Tang, Fengxiao and Zhao, Ming and Zhu, Yusen},
  journal={arXiv preprint arXiv:2203.13504},
  year={2022}
}



@article{han2023medalpaca,
  title={MedAlpaca--An Open-Source Collection of Medical Conversational AI Models and Training Data},
  author={Han, Tianyu and Adams, Lisa C and Papaioannou, Jens-Michalis and Grundmann, Paul and Oberhauser, Tom and L{\"o}ser, Alexander and Truhn, Daniel and Bressem, Keno K},
  journal={arXiv preprint arXiv:2304.08247},
  year={2023}
}

@article{toma2023clinical,
  title={Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding},
  author={Toma, Augustin and Lawler, Patrick R and Ba, Jimmy and Krishnan, Rahul G and Rubin, Barry B and Wang, Bo},
  journal={arXiv preprint arXiv:2305.12031},
  year={2023}
}

@article{bi2023accurate,
  title={Accurate medium-range global weather forecasting with 3D neural networks},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={Nature},
  volume={619},
  number={7970},
  pages={533--538},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{hu2022unimse,
  title={ UniMSE: Towards unified multimodal sentiment analysis and emotion recognition},
  author={Hu, Guimin and Lin, Ting-En and Zhao, Yi and Lu, Guangming and Wu, Yuchuan and Li, Yongbin},
  journal={arXiv preprint arXiv:2211.11256},
  year={2022}
}

@article{li2023unisa,
  title={UniSA: Unified Generative Framework for Sentiment Analysis},
  author={Li, Zaijing and Lin, Ting-En and Wu, Yuchuan and Liu, Meng and Tang, Fengxiao and Zhao, Ming and Li, Yongbin},
  journal={arXiv e-prints},
  pages={arXiv--2309},
  year={2023}
}



@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021}
}

@article{chen2023x,
  title={X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages},
  author={Chen, Feilong and Han, Minglun and Zhao, Haozhi and Zhang, Qingyang and Shi, Jing and Xu, Shuang and Xu, Bo},
  journal={arXiv preprint arXiv:2305.04160},
  year={2023}
}

@article{sun2023test,
  title={TEST: Text prototype aligned embedding to activate LLM's ability for time series},
  author={Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda},
  journal={arXiv preprint arXiv:2308.08241},
  year={2023}
}

@inproceedings{rahman2020integrating,
  title={Integrating Multimodal Information in Large Pretrained Transformers},
  author={Rahman, Wasifur and Hasan, Md Kamrul and Lee, Sangwu and Zadeh, Amir and Mao, Chengfeng and Morency, Louis-Philippe and Hoque, Ehsan},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={2020},
  pages={2359},
  year={2020},
  organization={NIH Public Access}
}

@article{kim2023aobert,
  title={AOBERT: All-modalities-in-One BERT for multimodal sentiment analysis},
  author={Kim, Kyeonghun and Park, Sanghyun},
  journal={Information Fusion},
  volume={92},
  pages={37--45},
  year={2023},
  publisher={Elsevier}
}

@article{hasan2023textmi,
  title={TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models},
  author={Hasan, Md Kamrul and Islam, Md Saiful and Lee, Sangwu and Rahman, Wasifur and Naim, Iftekhar and Khan, Mohammed Ibrahim and Hoque, Ehsan},
  journal={arXiv preprint arXiv:2303.15430},
  year={2023}
}

@article{zhang2023llama,
  title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  journal={arXiv e-prints},
  pages={arXiv--2303},
  year={2023}
}


@article{gao2023llama,
  title={LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{hu2023llm,
  title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author={Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya},
  journal={arXiv preprint arXiv:2304.01933},
  year={2023}
}

@article{liu2023gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  year={2023},
  publisher={Elsevier}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{liu2021p,
  title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@inproceedings{yu2021learning,
  title={Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis},
  author={Yu, Wenmeng and Xu, Hua and Yuan, Ziqi and Wu, Jiele},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={10790--10797},
  year={2021}
}

@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@article{li2023ga2mif,
  title={GA2MIF: Graph and Attention Based Two-Stage Multi-Source Information Fusion for Conversational Emotion Detection},
  author={Li, Jiang and Wang, Xiaoping and Lv, Guoqing and Zeng, Zhigang},
  journal={IEEE Transactions on Affective Computing},
  year={2023},
  publisher={IEEE}
}


@article{zadeh2017tensor,
  title={Tensor fusion network for multimodal sentiment analysis},
  author={Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:1707.07250},
  year={2017}
}

@article{liu2018efficient,
  title={Efficient low-rank multimodal fusion with modality-specific factors},
  author={Liu, Zhun and Shen, Ying and Lakshminarasimhan, Varun Bharadhwaj and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:1806.00064},
  year={2018}
}


@inproceedings{tsai2019multimodal,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={2019},
  pages={6558},
  year={2019},
  organization={NIH Public Access}
}

@inproceedings{hazarika2020misa,
  title={MISA: Modality-Invariant and-Specific Representations for Multimodal Sentiment Analysis},
  author={Hazarika, Devamanyu and Zimmermann, Roger and Poria, Soujanya},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={1122--1131},
  year={2020}
}

@article{han2021improving,
  title={Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis},
  author={Han, Wei and Chen, Hui and Poria, Soujanya},
  journal={arXiv preprint arXiv:2109.00412},
  year={2021}
}

@article{hu2021mmgcn,
  title={MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation},
  author={Hu, Jingwen and Liu, Yuchen and Zhao, Jinming and Jin, Qin},
  journal={arXiv preprint arXiv:2107.06779},
  year={2021}
}

@inproceedings{hu2022mm,
  title={MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations},
  author={Hu, Dou and Hou, Xiaolong and Wei, Lingwei and Jiang, Lianxin and Mo, Yang},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7037--7041},
  year={2022},
  organization={IEEE}
}

@inproceedings{lv2021progressive,
  title={Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences},
  author={Lv, Fengmao and Chen, Xiang and Huang, Yanyong and Duan, Lixin and Lin, Guosheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2554--2562},
  year={2021}
}

@inproceedings{generalpatternmachines2023,
  author    = {Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
  title     = {Large Language Models as General Pattern Machines},
  booktitle = {Proceedings of the 7th Conference on Robot Learning (CoRL)},
  year      = {2023},
}

@article{yu2023conki,
  title={ConKI: Contrastive knowledge injection for multimodal sentiment analysis},
  author={Yu, Yakun and Zhao, Mingjun and Qi, Shi-ang and Sun, Feiran and Wang, Baoxun and Guo, Weidong and Wang, Xiaoli and Yang, Lei and Niu, Di},
  journal={arXiv preprint arXiv:2306.15796},
  year={2023}
}

@inproceedings{yang2023confede,
  title={ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis},
  author={Yang, Jiuding and Yu, Yakun and Niu, Di and Guo, Weidong and Xu, Yu},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7617--7630},
  year={2023}
}



@inproceedings{lin-etal-2022-modeling,
    title = "Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis",
    author = "Lin, Zijie  and
      Liang, Bin  and
      Long, Yunfei  and
      Dang, Yixue  and
      Yang, Min  and
      Zhang, Min  and
      Xu, Ruifeng",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.622",
    pages = "7124--7135",
}

@article{zhang2023learning,
  title={Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis},
  author={Zhang, Haoyu and Wang, Yu and Yin, Guanghao and Liu, Kejun and Liu, Yuanyuan and Yu, Tianshu},
  journal={arXiv preprint arXiv:2310.05804},
  year={2023}
}

@inproceedings{yang2023code,
  title={i-Code: An Integrative and Composable Multimodal Learning Framework},
  author={Yang, Ziyi and Fang, Yuwei and Zhu, Chenguang and Pryzant, Reid and Chen, Dongdong and Shi, Yu and Xu, Yichong and Qian, Yao and Gao, Mei and Chen, Yi-Ling and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={10880--10890},
  year={2023}
}

@misc{szot2023large,
      title={Large Language Models as Generalizable Policies for Embodied Tasks}, 
      author={Andrew Szot and Max Schwarzer and Harsh Agrawal and Bogdan Mazoure and Walter Talbott and Katherine Metcalf and Natalie Mackraz and Devon Hjelm and Alexander Toshev},
      year={2023},
      eprint={2310.17722},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{he2017neural,
  title={Neural Factorization Machines for Sparse Predictive Analytics},
  author={He, Xiangnan and Chua, Tat-Seng},
  booktitle={Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval},
  pages={355--364},
  year={2017}
}

@inproceedings{lian2018xdeepfm,
  title={xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems},
  author={Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1754--1763},
  year={2018}
}

@inproceedings{wang2021dcn,
  title={DCN V2: Improved Deep \& Cross Network and Practical Lessons for Web-scale Learning to Rank Systems},
  author={Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed},
  booktitle={Proceedings of the web conference 2021},
  pages={1785--1797},
  year={2021}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{lewis2019bart,
  title={BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}


@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{yang2019xlnet,
  title={XLNET: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zhu2023minigpt,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{bapna2019simple,
  title={Simple, Scalable Adaptation for Neural Machine Translation},
  author={Bapna, Ankur and Arivazhagan, Naveen and Firat, Orhan},
  journal={arXiv preprint arXiv:1909.08478},
  year={2019}
}

@article{pfeiffer2020adapterhub,
  title={AdapterHub: A Framework for Adapting Transformers},
  author={Pfeiffer, Jonas and R{\"u}ckl{\'e}, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli{\'c}, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2007.07779},
  year={2020}
}

@inproceedings{guo2022dynamically,
  title={Dynamically adjust word representations using unaligned multimodal information},
  author={Guo, Jiwei and Tang, Jiajia and Dai, Weichen and Ding, Yu and Kong, Wanzeng},
  booktitle={Proceedings of the 30th ACM international conference on multimedia},
  pages={3394--3402},
  year={2022}
}


#talknet
@inproceedings{tao2021someone,
  title={Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection},
  author={Tao, Ruijie and Pan, Zexu and Das, Rohan Kumar and Qian, Xinyuan and Shou, Mike Zheng and Li, Haizhou},
  booktitle={Proceedings of the 29th ACM international conference on multimedia},
  pages={3927--3935},
  year={2021}
}

#openface

@inproceedings{eyben2010opensmile,
  title={Opensmile: the munich versatile and fast open-source audio feature extractor},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn},
  booktitle={Proceedings of the 18th ACM international conference on Multimedia},
  pages={1459--1462},
  year={2010}
}


@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

#wav2vec cherma
@inproceedings{zhang2022wenetspeech,
  title={Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition},
  author={Zhang, Binbin and Lv, Hang and Guo, Pengcheng and Shao, Qijie and Yang, Chao and Xie, Lei and Xu, Xin and Bu, Hui and Chen, Xiaoyu and Zeng, Chenchen and others},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6182--6186},
  year={2022},
  organization={IEEE}
}

#MTCNN
@article{zhang2016joint,
  title={Joint face detection and alignment using multitask cascaded convolutional networks},
  author={Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
  journal={IEEE signal processing letters},
  volume={23},
  number={10},
  pages={1499--1503},
  year={2016},
  publisher={IEEE}
}

# RAF-DB dataset
@inproceedings{li2017reliable,
  title={Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild},
  author={Li, Shan and Deng, Weihong and Du, JunPing},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2852--2861},
  year={2017}
}

@inproceedings{yang-etal-2024-clgsi,
    title = "{CLGSI}: A Multimodal Sentiment Analysis Framework based on Contrastive Learning Guided by Sentiment Intensity",
    author = "Yang, Yang  and
      Dong, Xunde  and
      Qiang, Yupeng",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.135",
    doi = "10.18653/v1/2024.findings-naacl.135",
    pages = "2099--2110",
    abstract = "Recently, contrastive learning has begun to gain popularity in multimodal sentiment analysis (MSA). However, most of existing MSA methods based on contrastive learning lacks more detailed learning of the distribution of sample pairs with different sentiment intensity differences in the contrastive learning representation space. In addition, limited research has been conducted on the fusion of each modality representation obtained by contrastive learning training.In this paper, we propose a novel framework for multimodal sentiment analysis based on Contrastive Learning Guided by Sentiment Intensity (CLGSI). Firstly, the proposed contrastive learning guided by sentiment intensity selects positive and negative sample pairs based on the difference in sentiment intensity and assigns corresponding weights accordingly.Subsequently, we propose a new multimodal representation fusion mechanism, called Global-Local-Fine-Knowledge (GLFK), which extracts common features between different modalities{'} representations. At the same time, each unimodal encoder output is separately processed by a Multilayer Perceptron (MLP) to extract specific features of each modality. Finally, joint learning of the common and specific features is used to predict sentiment intensity. The effectiveness of CLGSI is assessed on two English datasets, MOSI and MOSEI, as well as one Chinese dataset, SIMS. We achieve competitive experimental results, which attest to the strong generalization performance of our approach. The code for our approach will be released in https://github.com/AZYoung233/CLGSI",
}

@inproceedings{wang2024wisdom,
  title={WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge},
  author={Wang, Wenbin and Ding, Liang and Shen, Li and Luo, Yong and Hu, Han and Tao, Dacheng},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={2282--2291},
  year={2024}
}

@article{he2022sparseadapter,
  title={SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters},
  author={He, Shwai and Ding, Liang and Dong, Daize and Zhang, Miao and Tao, Dacheng},
  journal={arXiv preprint arXiv:2210.04284},
  year={2022}
}
