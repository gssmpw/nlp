@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{chen2023x,
  title={X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages},
  author={Chen, Feilong and Han, Minglun and Zhao, Haozhi and Zhang, Qingyang and Shi, Jing and Xu, Shuang and Xu, Bo},
  journal={arXiv preprint arXiv:2305.04160},
  year={2023}
}

@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{hasan2023textmi,
  title={TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models},
  author={Hasan, Md Kamrul and Islam, Md Saiful and Lee, Sangwu and Rahman, Wasifur and Naim, Iftekhar and Khan, Mohammed Ibrahim and Hoque, Ehsan},
  journal={arXiv preprint arXiv:2303.15430},
  year={2023}
}

@article{he2022sparseadapter,
  title={SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters},
  author={He, Shwai and Ding, Liang and Dong, Daize and Zhang, Miao and Tao, Dacheng},
  journal={arXiv preprint arXiv:2210.04284},
  year={2022}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{hu2021mmgcn,
  title={MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation},
  author={Hu, Jingwen and Liu, Yuchen and Zhao, Jinming and Jin, Qin},
  journal={arXiv preprint arXiv:2107.06779},
  year={2021}
}

@article{hu2022unimse,
  title={ UniMSE: Towards unified multimodal sentiment analysis and emotion recognition},
  author={Hu, Guimin and Lin, Ting-En and Zhao, Yi and Lu, Guangming and Wu, Yuchuan and Li, Yongbin},
  journal={arXiv preprint arXiv:2211.11256},
  year={2022}
}

@article{hu2023llm,
  title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author={Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya},
  journal={arXiv preprint arXiv:2304.01933},
  year={2023}
}

@article{kim2023aobert,
  title={AOBERT: All-modalities-in-One BERT for multimodal sentiment analysis},
  author={Kim, Kyeonghun and Park, Sanghyun},
  journal={Information Fusion},
  volume={92},
  pages={37--45},
  year={2023},
  publisher={Elsevier}
}

@article{li2023ga2mif,
  title={GA2MIF: Graph and Attention Based Two-Stage Multi-Source Information Fusion for Conversational Emotion Detection},
  author={Li, Jiang and Wang, Xiaoping and Lv, Guoqing and Zeng, Zhigang},
  journal={IEEE Transactions on Affective Computing},
  year={2023},
  publisher={IEEE}
}

@inproceedings{lin-etal-2022-modeling,
    title = "Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis",
    author = "Lin, Zijie  and
      Liang, Bin  and
      Long, Yunfei  and
      Dang, Yixue  and
      Yang, Min  and
      Zhang, Min  and
      Xu, Ruifeng",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.622",
    pages = "7124--7135",
}

@article{liu2021p,
  title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{liu2023gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  year={2023},
  publisher={Elsevier}
}

@article{mai2023learning,
  title={Learning from the global view: Supervised contrastive learning of multimodal representation},
  author={Mai, Sijie and Zeng, Ying and Hu, Haifeng},
  journal={Information Fusion},
  volume={100},
  pages={101920},
  year={2023},
  publisher={Elsevier}
}

@article{pfeiffer2020adapterhub,
  title={AdapterHub: A Framework for Adapting Transformers},
  author={Pfeiffer, Jonas and R{\"u}ckl{\'e}, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli{\'c}, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2007.07779},
  year={2020}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{rahman2020integrating,
  title={Integrating Multimodal Information in Large Pretrained Transformers},
  author={Rahman, Wasifur and Hasan, Md Kamrul and Lee, Sangwu and Zadeh, Amir and Mao, Chengfeng and Morency, Louis-Philippe and Hoque, Ehsan},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={2020},
  pages={2359},
  year={2020},
  organization={NIH Public Access}
}

@inproceedings{sun2023layer,
  title={Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition},
  author={Sun, Jun and Han, Shoukang and Ruan, Yu-Ping and Zhang, Xiaoning and Zheng, Shu-Kai and Liu, Yulong and Huang, Yuxin and Li, Taihao},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={658--670},
  year={2023}
}

@article{sun2023test,
  title={TEST: Text prototype aligned embedding to activate LLM's ability for time series},
  author={Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda},
  journal={arXiv preprint arXiv:2308.08241},
  year={2023}
}

@article{yang2019xlnet,
  title={XLNET: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{yang2023code,
  title={i-Code: An Integrative and Composable Multimodal Learning Framework},
  author={Yang, Ziyi and Fang, Yuwei and Zhu, Chenguang and Pryzant, Reid and Chen, Dongdong and Shi, Yu and Xu, Yichong and Qian, Yao and Gao, Mei and Chen, Yi-Ling and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={10880--10890},
  year={2023}
}

@inproceedings{yang2023confede,
  title={ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis},
  author={Yang, Jiuding and Yu, Yakun and Niu, Di and Guo, Weidong and Xu, Yu},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7617--7630},
  year={2023}
}

@article{zhang2023learning,
  title={Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis},
  author={Zhang, Haoyu and Wang, Yu and Yin, Guanghao and Liu, Kejun and Liu, Yuanyuan and Yu, Tianshu},
  journal={arXiv preprint arXiv:2310.05804},
  year={2023}
}

