\section{Related Work}
\subsection{Pre-trained language model for MSA and ERC}
	Currently, in the MSA and ERC communities, a number of outstanding works have emerged, including contrastive learning-based methods ____, graph-based methods ____, transformer-based methods ____, and methods based on Pre-trained Language Model (PLM). PLM-based methods typically use a designated PLM, such as BERT ____ or T5 ____, as their foundation. These methods convert non-textual modality features into tokens with equivalent dimensions to those of textual modalities, enabling training within the PLM framework.
	

	Rahman et al. \shortcite{rahman2020integrating} proposed the MAG, where completed word embeddings are fused with non-textual modality features to generate new embeddings. These embeddings are then fine-tuned within PLMs (such as BERT and XLNet ____) to achieve notable performance improvements.
	Similarly, Guo et al. \shortcite{guo2022dynamically} proposed CHFN, which uses its designed Multimodal Interaction layer to integrate non-textual modal information into textual embedding at the word level, and then fine-tunes the integrated multimodal information by feeding it into BERT.  
	%
	%Subsequently, Kim et al. \shortcite{kim2023aobert} introduced AOBERT, which combines the fusion results of textual and other non-textual modalities with the text modality itself prior to inputting it into BERT. AOBERT also incorporates additional pre-training tasks to enhance the interaction between modalities, leading to significant enhancements in performance.
	%
	Additionally, Hasan et al. \shortcite{hasan2023textmi} presented TextMI, a method that converts audio and vision information into corresponding textual descriptions. This approach links these descriptions with the textual content, transforming multimodal information into purely textual information. By inputting this enhanced text into BERT, TextMI achieves competitive performance results.
	%Rahman et al. ____ introduced Multimodal Adaptation Gates (MAG), where MAG fuses completed word embeddings with non-textual modality features as new embeddings, which are then fine-tuned within PLMs (tested with BERT and XLNet) to achieve notable performance. Kim et al. ____ proposed AOBERT , which, unlike MAG, merges the fusion results of textual and other non-textual modalities with the text modality itself before inputting it into BERT, and performs additional pre-training tasks to further strengthen the interaction between modalities, resulting in significant performance improvements. Hasan et al. ____ introduced TextMI , which converts acoustic and visual information into corresponding textual descriptions and connects them with the textual content itself, transforming multimodal information into purely textual information and achieving competitive performance when inputting this enhanced text into BERT. 
	Hu et al. \shortcite{hu2022unimse} introduced UniMSE, an approach that uses the T5 ____ model as its foundation. UniMSE encodes text using the initial layers of T5's encoder and then trains the remaining layers using a combination of non-textual and textual features. This training strategy incorporates contrastive learning to enhance the model's representation learning capabilities. Benefiting from its multitask training paradigm, UniMSE shows capabilities in both MSA and ERC tasks, demonstrating exceptional performance.
	%
	%Hu et al. ____ proposed UniMSE, which uses the T5 model as a base to first encode text with the initial layers of T5's encoder and then train the remaining layers with a mix of non-textual and textual features, employing contrastive learning to enhance the model's representation learning. Benefiting from multitask training, UniMSE is a model capable of performing both MSA and ERC tasks, demonstrating outstanding performance. 
	Li et al. \shortcite{li2023unisa} developed UniSA, a comprehensive framework for sentiment analysis. UniSA uses PLMs (GPT2-medium ____, T5 and BART) as a foundation, standardizing the data formats of various types of sentiment analysis sub-tasks for input into PLMs. It leverages pre-training tasks and contrastive learning to pre-train the PLM, followed by fine-tuning on downstream task datasets. The UniSA$_{\text{BART}}$ achieve comprehensive results across multiple sentiment analysis sub-tasks. 
	%
	%Our method differs from the aforementioned works; MSE-Adapter serves as a lightweight plugin requiring relatively fewer training parameters (with 2.6M-2.8M trainable parameters for base models sized 6/7B), without compromising the LLM's inherent generalization capability. This means that when there is a need for the LLM to perform MSA tasks, users only need to simply invoke the pre-trained MSE-Adapter plugin. This design not only optimizes parameter efficiency but also maintains the efficiency and flexibility of the LLM, providing a new and efficient solution for the application of LLMs in MSA.
	
	Compared to aforementioned works, our proposed approach, MSE-Adapter, is a lightweight plugin that requires fewer training parameters (approximately 2.6M to 2.8M trainable parameters for base models sized 6/7B). Notably, MSE-Adapter preserves the inherent generalization capability of the LLM without sacrificing efficiency. Therefore, when assigned MSA or ERC tasks, the user can invoke the relevant pre-trained MSE-Adapter plugin to carry out the designated task. This design enhances parameter efficiency while preserving the effectiveness and adaptability of LLM, offering a new and efficient solution for using LLM in MSA and ERC tasks.
		
	\begin{figure*}[ht]
		\centering
		\includegraphics[width=0.78\linewidth,trim=163 76 102 56,clip]{overall}
		\caption{The comprehensive framework integrating MSE-Adapter with LLM.} %%%这个说法不合适，这不是MSE-Adapter的框架，是使用MSE-Adapter与LLM结合的框架。
		\label{figure 1}
	\end{figure*}
	
	
	%\subsection{Adapter Make LLM perform non-plain text tasks}
	\subsection{Adapters enabling LLM to perform non-plain text tasks}
	Adapters were usually utilized for efficiently fine-tuning large pre-trained models ____. By freezing the main body of the pre-trained model and only training the Adapter, the essence is to use gradient backpropagation to let the Adapter generate pseudo tokens that can be recognized by the pre-trained model. This process is aimed at prompting the pre-trained models to further adapt to certain downstream tasks. Meanwhile, since the pre-trained model is frozen during the training phase, it retains its strong generalization capability, avoiding the issue of catastrophic forgetting ____. Inspired by these relevant works of Adapter, some researchers have argued that it is possible to convert information from non-textual modalities into information understandable by LLMs through an Adapter, enabling them to perform downstream tasks involving non-plain text modalities.
	Tsimpoukelli et al. \shortcite{tsimpoukelli2021multimodal} introduced Frozen, which utilizes a vision encoder to convert images into a series of tokens. These tokens are concatenated with a prompt and used to train a LLM for visual question answering (VQA) and captioning tasks, with gradient backpropagation guiding updates to the parameters of the vision encoder.
	%
	%Tsimpoukelli et al.  introduced "Frozen," which transforms images into a series of tokens through a vision encoder, concatenates these tokens with a prompt, and inputs them into an LLM for training. Gradient backpropagation guides the vision encoder in updating its parameters, enabling the LLM to perform visual question answering (VQA) and captioning tasks. 
	Similarly, Alayrac et al. \shortcite{alayrac2022flamingo} proposed Flamingo, which incorporates trainable cross-attention layers into a frozen LLM to fuse textual and vision modalities after embedding vision modality information using a pre-trained vision encoder. Flamingo exhibits remarkable performance across various video/visual-related tasks following training. 
	%Inspired by ``Frozen", Alayrac et al. ____ proposed "Flamingo," which embeds visual modality information using a frozen pre-trained vision encoder and then incorporates trainable cross-attention layers at each level of a frozen LLM to fuse textual and visual modalities. Flamingo, after training, demonstrates remarkable performance across various video/visual-related tasks.
	%
	Chen et al. \shortcite{chen2023x} presented X-LLM, a model that leverages X2L interfaces to convert vision, image, and audio modalities into ``foreign languages" that can be processed by the LLM. X-LLM demonstrates impressive performance after instruction-tuning on a high-quality multimodal instruction dataset.
	%Chen et al. ____ put forward "X-LLM", which converts video/image/audio modalities into "foreign languages" that the LLM can understand through proposed X2L interfaces . X-LLM achieves impressive performance on a high-quality multimodal instruction dataset. 
	Sun et al. \shortcite{sun2023test} developed TEST, which utilizes contrastive learning to train an encoder for time series (TS) data, applies similarity constraints to align it with text, and fine-tunes the LLM with a soft-prompt approach to effectively process TS-related tasks.
	%Sun et al. ____ developed "TEST", which first uses contrastive learning to train an encoder for embedding time series (TS) data, applies similarity constraints to align the TS encoder with text, and then fine-tunes the LLM using a soft-prompt approach to process TS-related tasks effectively. 
	
	In this paper, we introduce a lightweight plugin named MSE-Adapter, which enable the LLM's to perform MSA or ERC task without affecting its inherent capabilities. Unlike previous works, we introduce a novel module named TGM in the MSE-Adapter. TGM facilitates feature-level alignment between non-textual and textual modalities, which aids the LLM to better understand content from non-textual modalities.
	%Inspired by these works, this paper develops MSE-Adapter, which, without affecting the LLM's inherent capabilities, endows the LLM with the ability to perform MSA and ERC tasks. Unlike the aforementioned works, we introduce a novel feature-level text alignment method—TGM, which encourages non-text modalities to align with text modalities, thereby enhancing the LLM's understanding of content from non-textual modalities.