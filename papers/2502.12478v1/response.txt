\section{Related Work}
\subsection{Pre-trained language model for MSA and ERC}
	Currently, in the MSA and ERC communities, a number of outstanding works have emerged, including contrastive learning-based methods **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, graph-based methods **Kipf et al., "Variational Graph Autoencoders"**, transformer-based methods **Vaswani et al., "Attention Is All You Need"**, and methods based on Pre-trained Language Model (PLM). PLM-based methods typically use a designated PLM, such as BERT  **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** or T5  **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**, as their foundation. These methods convert non-textual modality features into tokens with equivalent dimensions to those of textual modalities, enabling training within the PLM framework.
	

	Rahman et al. \shortcite{rahman2020integrating} proposed the MAG, where completed word embeddings are fused with non-textual modality features to generate new embeddings. These embeddings are then fine-tuned within PLMs (such as BERT and XLNet  **Yang et al., "XLNet: Generalized Autoregressive Pretraining for Language Understanding"**) to achieve notable performance improvements.
	Similarly, Guo et al. \shortcite{guo2022dynamically} proposed CHFN, which uses its designed Multimodal Interaction layer to integrate non-textual modal information into textual embedding at the word level, and then fine-tunes the integrated multimodal information by feeding it into BERT.  
	%
	%Subsequently, Kim et al. \shortcite{kim2023aobert} introduced AOBERT, which combines the fusion results of textual and other non-textual modalities with the text modality itself prior to inputting it into BERT. AOBERT also incorporates additional pre-training tasks to enhance the interaction between modalities, leading to significant enhancements in performance.
	%
	Additionally, Hasan et al. \shortcite{hasan2023textmi} presented TextMI, a method that converts audio and vision information into corresponding textual descriptions. This approach links these descriptions with the textual content, transforming multimodal information into purely textual information. By inputting this enhanced text into BERT, TextMI achieves competitive performance results.
	%Rahman et al. ____ introduced Multimodal Adaptation Gates (MAG), where MAG fuses completed word embeddings with non-textual modality features as new embeddings, which are then fine-tuned within PLMs (tested with BERT and XLNet) to achieve notable performance. Kim et al. ____ proposed AOBERT , which, unlike MAG, merges the fusion results of textual and other non-textual modalities with the text modality itself before inputting it into BERT, and performs additional pre-training tasks to further strengthen the interaction between modalities, resulting in significant performance improvements. Hasan et al. ____ introduced TextMI , which converts acoustic and visual information into corresponding textual descriptions and connects them with the textual content itself, transforming multimodal information into purely textual information and achieving competitive performance when inputting this enhanced text into BERT. 
	Hu et al. \shortcite{hu2022unimse} introduced UniMSE, an approach that uses the T5  **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** model as its foundation. UniMSE encodes text using the initial layers of T5's encoder and then trains the remaining layers using a combination of non-textual and textual features. This training strategy incorporates contrastive learning to enhance the model's representation learning capabilities. Benefiting from its multitask training paradigm, UniMSE shows capabilities in both MSA and ERC tasks, demonstrating exceptional performance.
	%
	%Hu et al. ____ proposed UniMSE, which uses the T5 model as a base to first encode text with the initial layers of T5's encoder and then train the remaining layers with a mix of non-textual and textual features, employing contrastive learning to enhance the model's representation learning. Benefiting from multitask training, UniMSE is a model capable of performing both MSA and ERC tasks, demonstrating outstanding performance. 
	Li et al. \shortcite{li2023unisa} developed UniSA, a comprehensive framework for sentiment analysis. UniSA uses PLMs (GPT2-medium  **Brown et al., "Language Models play Hide and Seek with Adversarial Attacks"**, T5 and BART) as a foundation, standardizing the data formats of various types of sentiment analysis sub-tasks for input into PLMs. It leverages pre-training tasks and contrastive learning to pre-train the PLM, followed by fine-tuning on downstream task datasets. The UniSA$_{\text{BART}}$ achieve comprehensive results across multiple sentiment analysis sub-tasks. 
	%
	%Our method differs from the aforementioned works; MSE-Adapter serves as a lightweight plugin requiring relatively fewer training parameters (with 2.6M-2.8M trainable parameters for base models sized 6/7B), without compromising the LLM's inherent generalization capability. This means that when there is a need for the LLM to perform MSA tasks, users only need to simply invoke the pre-trained MSE-Adapter plugin. This design not only optimizes parameter efficiency but also maintains the efficiency and flexibility of the LLM, providing a new and efficient solution for the application of LLMs in MSA.
	
	Compared to aforementioned works, our proposed approach, MSE-Adapter, is a lightweight plugin that requires fewer training parameters (approximately 2.6M to 2.8M trainable parameters for base models sized 6/7B). Notably, MSE-Adapter preserves the inherent generalization capability of the LLM without sacrificing efficiency. Therefore, when assigned MSA or ERC tasks, the user can invoke the relevant pre-trained MSE-Adapter plugin to carry out the designated task. This design enhances parameter efficiency while preserving the effectiveness and adaptability of LLM, offering a new and efficient solution for using LLM in MSA and ERC tasks.
		
	\begin{figure*}[ht]
		\centering
		\includegraphics[width=0.78\linewidth,trim=163 76 102 56,clip]{overall}
		\caption{The comprehensive framework integrating MSE-Adapter with LLM.} %%%这个说法不合适，这不是MSE-Adapter的框架，是使用MSE-Adapter与LLM结合的框架。
		\label{figure 1}
	\end{figure*}
	
	
	%\subsection{Adapter Make LLM perform non-plain text tasks}
	\subsection{Adapters enabling LLM to perform non-plain text tasks}
	Adapters were usually utilized for efficiently fine-tuning large pre-trained models  **Vaswani et al., "Attention Is All You Need"**. By freezing the main body of the pre-trained model and only training the Adapter, the essence is to use gradient backpropagation to let the Adapter generate pseudo tokens that can be recognized by the pre-trained model. This process is aimed at prompting the pre-trained models to further adapt to certain downstream tasks. Meanwhile, since the pre-trained model is frozen during the training phase, it retains its strong generalization capability, avoiding the issue of catastrophic forgetting  **French et al., " Catastrophic Forgetting in Connectionist Temporal Classification"**. Inspired by these relevant works of Adapter, some researchers have argued that it is possible to use adapters to enable LLMs to perform MSA and ERC tasks.
	In this paper, we introduce a lightweight plugin named MSE-Adapter, which enables the LLM's to perform MSA or ERC task without affecting its inherent capabilities. Unlike previous works, we introduce a novel module named TGM in the MSE-Adapter. TGM facilitates feature-level alignment between non-textual and textual modalities, which aids the LLM to better understand content from non-textual modalities.
	%Inspired by these works, this paper develops MSE-Adapter, which, without affecting the LLM's inherent capabilities, endows the LLM with the ability to perform MSA and ERC tasks. Unlike the aforementioned works, we introduce a novel feature-level text alignment method—TGM, which encourages non-text modalities to align with text modalities, thereby enhancing the LLM's understanding of content from non-textual modalities.