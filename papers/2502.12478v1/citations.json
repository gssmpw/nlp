[
  {
    "index": 0,
    "papers": [
      {
        "key": "yang2023confede",
        "author": "Yang, Jiuding and Yu, Yakun and Niu, Di and Guo, Weidong and Xu, Yu",
        "title": "ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis"
      },
      {
        "key": "mai2023learning",
        "author": "Mai, Sijie and Zeng, Ying and Hu, Haifeng",
        "title": "Learning from the global view: Supervised contrastive learning of multimodal representation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2023ga2mif",
        "author": "Li, Jiang and Wang, Xiaoping and Lv, Guoqing and Zeng, Zhigang",
        "title": "GA2MIF: Graph and Attention Based Two-Stage Multi-Source Information Fusion for Conversational Emotion Detection"
      },
      {
        "key": "hu2021mmgcn",
        "author": "Hu, Jingwen and Liu, Yuchen and Zhao, Jinming and Jin, Qin",
        "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation"
      },
      {
        "key": "lin-etal-2022-modeling",
        "author": "Lin, Zijie  and\nLiang, Bin  and\nLong, Yunfei  and\nDang, Yixue  and\nYang, Min  and\nZhang, Min  and\nXu, Ruifeng",
        "title": "Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "sun2023layer",
        "author": "Sun, Jun and Han, Shoukang and Ruan, Yu-Ping and Zhang, Xiaoning and Zheng, Shu-Kai and Liu, Yulong and Huang, Yuxin and Li, Taihao",
        "title": "Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition"
      },
      {
        "key": "zhang2023learning",
        "author": "Zhang, Haoyu and Wang, Yu and Yin, Guanghao and Liu, Kejun and Liu, Yuanyuan and Yu, Tianshu",
        "title": "Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis"
      },
      {
        "key": "yang2023code",
        "author": "Yang, Ziyi and Fang, Yuwei and Zhu, Chenguang and Pryzant, Reid and Chen, Dongdong and Shi, Yu and Xu, Yichong and Qian, Yao and Gao, Mei and Chen, Yi-Ling and others",
        "title": "i-Code: An Integrative and Composable Multimodal Learning Framework"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "BERT: Pre-training of deep bidirectional transformers for language understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "raffel2020exploring",
        "author": "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yang2019xlnet",
        "author": "Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V",
        "title": "XLNET: Generalized autoregressive pretraining for language understanding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rahman2020integrating",
        "author": "Rahman, Wasifur and Hasan, Md Kamrul and Lee, Sangwu and Zadeh, Amir and Mao, Chengfeng and Morency, Louis-Philippe and Hoque, Ehsan",
        "title": "Integrating Multimodal Information in Large Pretrained Transformers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kim2023aobert",
        "author": "Kim, Kyeonghun and Park, Sanghyun",
        "title": "AOBERT: All-modalities-in-One BERT for multimodal sentiment analysis"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hasan2023textmi",
        "author": "Hasan, Md Kamrul and Islam, Md Saiful and Lee, Sangwu and Rahman, Wasifur and Naim, Iftekhar and Khan, Mohammed Ibrahim and Hoque, Ehsan",
        "title": "TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "raffel2020exploring",
        "author": "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hu2022unimse",
        "author": "Hu, Guimin and Lin, Ting-En and Zhao, Yi and Lu, Guangming and Wu, Yuchuan and Li, Yongbin",
        "title": " UniMSE: Towards unified multimodal sentiment analysis and emotion recognition"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language Models are Unsupervised Multitask Learners"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-Efficient Transfer Learning for NLP"
      },
      {
        "key": "pfeiffer2020adapterhub",
        "author": "Pfeiffer, Jonas and R{\\\"u}ckl{\\'e}, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli{\\'c}, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna",
        "title": "AdapterHub: A Framework for Adapting Transformers"
      },
      {
        "key": "he2022sparseadapter",
        "author": "He, Shwai and Ding, Liang and Dong, Daize and Zhang, Miao and Tao, Dacheng",
        "title": "SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters"
      },
      {
        "key": "hu2023llm",
        "author": "Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya",
        "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2021p",
        "author": "Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie",
        "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"
      },
      {
        "key": "liu2023gpt",
        "author": "Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie",
        "title": "GPT understands, too"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "chen2023x",
        "author": "Chen, Feilong and Han, Minglun and Zhao, Haozhi and Zhang, Qingyang and Shi, Jing and Xu, Shuang and Xu, Bo",
        "title": "X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sun2023test",
        "author": "Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda",
        "title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series"
      }
    ]
  }
]