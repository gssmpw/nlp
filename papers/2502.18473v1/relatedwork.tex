\section{Related Work}
(Dis)proving the functional equivalence of two pieces of code is known to be generally undecidable, but in many real-life code possible. There has been a large corpus of work in the programming languages research that investigates this problem which we omit here.
A substantial number of works has focused on the ``equivalence modulo inputs'' paradigm~\citep{le2014compiler} focusing on different implementations of some standard, such as compilers, or programs that perform code transformations~\citep{daniel2007automated}.
These methods use randomized equivalence testing and predate LLMs.
\citet{kommrusch2023self} was probably the first ML work that aimed to assist equivalence proofs of straight-line programs by suggesting rewrite rules.

More recently, research has looked into test generation, such as the work of \citet{lemieux2023codamosa}. CodeT~\citep{chen2022codet} generates tests as a means of exposing the functional differences of a set of candidate implementations, similar to semantic clustering.
As discussed in \autoref{sec:probe gen}, unit test generation is a special form of probe generation that requires providing both input usage of an interface, but also predicting the expected output.
This makes it a harder task for LLMs while also suffering from spurious counterexamples~\citep{alshahwan2024automated}.
\probgen circumvents this issue by comparing the output values of a probes that only provide inputs to the tested code, with the downside that a ground truth implementation needs to be present.