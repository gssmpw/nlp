
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.

\usepackage{hyperref}
\usepackage{url}
% My own packages
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{arydshln}
\usepackage{bigstrut}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{color}
\usepackage{comment}
\usepackage[english]{babel}
\usepackage{enumitem}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathabx}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{stmaryrd}
\usepackage{subfigure}
\usepackage{tablefootnote}
\usepackage{longtable}
\usepackage{threeparttable}
\usepackage{wrapfig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\title{Realistic Evaluation of Deep Partial-Label Learning Algorithms}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Wei Wang~$^{1,2}$\qquad Dong-Dong Wu~$^{3}$\qquad Jindong Wang~$^{4}$\qquad Gang Niu~$^{2,3}$\\ \bf Min-Ling Zhang~$^{3}$\qquad Masashi Sugiyama~$^{2,1}$ \\ 
  $^1$~The University of Tokyo, Chiba, Japan\qquad
  $^2$~RIKEN, Tokyo, Japan\\
  $^3$~Southeast University, Nanjing, China\qquad
  $^4$~William \& Mary, Williamsburg, VA, USA \\
  \texttt{wangw@g.ecc.u-tokyo.ac.jp}~~~ \texttt{\{dongdongwu1230,gang.niu.ml\}@gmail.com} \\ \texttt{jwang80@wm.edu}~~~ \texttt{zhangml@seu.edu.cn}~~~ \texttt{sugi@k.u-tokyo.ac.jp}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle
\begin{abstract}
Partial-label learning~(PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose \textsc{Plench}, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10~(PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on \textsc{Plench}. We hope that \textsc{Plench} will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.\footnote{The code implementation of \textsc{Plench} is available at \url{https://github.com/wwangwitsel/PLENCH}. The PLCIFAR10 dataset is available at \url{https://github.com/wwangwitsel/PLCIFAR10}.}
\end{abstract}
\section{Introduction}\label{intro_sec}
Partial-label learning~(PLL) is a weakly supervised learning problem that has attracted much attention recently~\citep{sugiyama2022machine,wang2022adaptive,tian2023partial}. In PLL, each training example is associated with multiple candidate labels ~\citep{jin2002learning,cour2011learning}. The true label for each example is hidden in the set of candidate labels, but not accessible to the learning algorithm. PLL has been successfully applied to computer vision~\citep{liu2012conditional,zeng2013learning,chen2018learning,gong2018regularization,tang2023disambiguated,wang2024learning}, natural language processing~\citep{garrette2013learning,zhou2018weakly,ren2016afet,ren2016label}, web mining~\citep{luo2010learning}, ecoinformatics~\citep{briggs2012rank,wang2019partial,li2021detecting,lyu2022self}, etc.

Among various strategies to address this problem, deep learning-based PLL algorithms have demonstrated satisfactory generalization performance due to the strong representation learning capabilities of deep neural networks~\citep{lv2020progressive,wang2022pico}. Despite the abundance of algorithms in this area, we find that there are several fundamental and critical issues that have received less attention in the PLL literature, including neglected model selection issues, inconsistent experimental settings, and lack of real-world image datasets, which will be discussed point by point.

\begin{figure*}[t]
  \centering
  \subfigure[Aggregate]{
    \includegraphics[width=0.23\textwidth]{fig/intro_aggregate_rebuttal.pdf}\label{intro_fig_a}
  }
  \subfigure[Vaguest]{
    \includegraphics[width=0.23\textwidth]{fig/intro_vaguest_rebuttal.pdf}\label{intro_fig_b}
  }
  \subfigure[Soccer Player]{
    \includegraphics[width=0.23\textwidth]{fig/intro_soccer_player.pdf}\label{intro_fig_c}
  }
  \subfigure[Yahoo! News]{
    \includegraphics[width=0.23\textwidth]{fig/intro_yahoo.pdf}\label{intro_fig_d}
  }
  \\
  \caption{The two left panels show the differences in using an ordinary-label dataset for validation~(lighter colors) and training~(darker colors) for a given algorithm. For validation~(lighter colors), we searched for the best hyperparameter configurations with the validation set for a given algorithm. For training~(darker colors), we considered the validation set as partial-label examples with a single partial label and added them to the training set for training, using default hyperparameters without tuning. For fair comparisons, we trained all models with the same number of iterations. The two right panels show the classification accuracies of some PLL algorithms on Soccer Player and Yahoo! News from papers A~\citep{zhang20222exploiting} and B~\citep{xu2023progressive}, respectively.}\label{intro_inconsistent}
\end{figure*}

\noindent{\textbf{Neglected model selection issues.}}~~~Most PLL algorithms select their hyperparameters by using a clean ordinary-label validation set~\citep{qiao2023decompositional,xu2023alim,xu2023progressive}. However, the original definition of PLL does not allow the existence of an ordinary-label dataset~\citep{jin2002learning,cour2011learning,zhang2017disambiguation}, indicating a mismatch between the problem definitions and experimental settings in the literature. This problem can even lead to \emph{unfair comparisons} if some algorithms follow the classical protocol of PLL to prohibit the use of ordinary-label data, while some algorithms do not. Moreover, \emph{if we have a clean ordinary-label dataset, why not use it for training?} In many cases, the use of clean labels is more valuable for weakly supervised learning than for ordinary supervised learning~\citep{hendrycks2018using,yu2023delving}. A pilot experiment was conducted to compare different approaches to using validation data with ordinary labels. Figures~\ref{intro_fig_a} and~\ref{intro_fig_b} illustrate the results of the experiment on two versions of our collected PLCIFAR10 dataset. We can observe that for many PLL algorithms without much need to tune hyperparameters, it is more beneficial to use validation data with ordinary labels for training. Therefore, it is imperative to standardize the use of validation data and model selection criteria to ensure the integrity and practicality of empirical studies. 

\noindent{\textbf{Inconsistent experimental settings.}}~~~We have found that the experimental settings used in different papers are often quite different, creating a dilemma when comparing the performance of different algorithms. We illustrate this point with an example in Figures~\ref{intro_fig_c} and~\ref{intro_fig_d}. We reported experimental results of several PLL algorithms on Soccer Player and Yahoo! News from papers A~\citep{zhang20222exploiting} and B~\citep{xu2023progressive}, respectively. The data points for the same algorithm are connected by a line, and the number of intersections between different lines indicates the times of inconsistency in the performance ranking. In particular, the algorithm implementations, network architectures, and datasets are identical. However, the relative ranking of the classification performance differed significantly between the different papers. It is hypothesized that the discrepancy is due to subtle variations in experimental settings, such as data partitioning and preprocessing, as well as hyperparameter configurations. Such an obstacle can hinder objective comparisons of different algorithms, making it difficult to determine the effectiveness of a developed technique. 

\noindent{\textbf{Lack of real-world image datasets.}}~~~Existing PLL works have mainly conducted experimental results on real-world tabular datasets or synthetic image datasets to demonstrate the effectiveness of the proposed methodology~\citep{lyu2021gmpll,wang2022pico}. However, on the one hand, tabular datasets may not be compatible with modern network architectures, such as convolutional neural networks~\citep{oquab2015object,he2016deep}. On the other hand, synthetic image datasets are generated by a human-made generation process~(see Section~\ref{background_sec}), which may not be consistent with complex annotation mechanisms in real-world applications~\citep{jiang2020beyond,wei2022learning}. This may lead to potential concerns about whether an algorithm that works well on synthetic image datasets will still work well on real-world complex image datasets~\citep{xu2021instance}.

\noindent{\textbf{Contributions.}}~~~To this end, we propose \textsc{Plench}, the first Partial-Label learning bENCHmark to standardize the experiments of PLL. The main contributions are as follows:
\begin{itemize}[leftmargin=1em, itemsep=1pt, topsep=0pt, parsep=-1pt]
\item We systematically investigate the model selection problem in PLL for the first time and propose new model selection criteria that are both theoretically and empirically validated.
\item We create PLCIFAR10, a new PLL benchmark dataset with human-annotated partial labels. PLCIFAR10 provides an effective testbed for evaluating the performance of PLL algorithms in more realistic scenarios. 
\item We present the first PLL benchmark that includes twenty-seven algorithms and eleven real-world datasets, to systematically compare state-of-the-art deep PLL algorithms.
\end{itemize}
\noindent{\textbf{Takeaways.}}~~~Based on our research, we suggest the following key takeaways:
\begin{itemize}[leftmargin=1em, itemsep=1pt, topsep=0pt, parsep=-1pt]
\item Many concise algorithms can outperform or be comparable to complicated algorithms with strong regularization techniques and higher computational requirements.
\item There is no algorithm that can outperform all other algorithms in all cases, suggesting that the algorithmic design should be tailored for different cases. 
\item Model selection is non-trivial for PLL, and should be specified when proposing a PLL algorithm or comparing different algorithms.
\end{itemize}
\section{Background}\label{background_sec}
\noindent{\textbf{Problem setting.}}~~~Let $\mathcal{X} = \mathbb{R}^d$ denote the $d$-dimensional feature space and $\mathcal{Y}=\left\{1,2,\ldots,q\right\}$ denote the label space with $q$ class labels. Let $\left(\bm{x},S\right)$ denote a partial-label example where $\bm{x} \in \mathcal{X}$ is a feature vector and $S \subseteq \mathcal{Y}$ is a candidate label set associated with $\bm{x}$. The basic assumption of PLL is that the ground-truth label $y$ of $\bm{x}$ is concealed within its candidate label set $S$, i.e., $y\in S$. The task of PLL is to learn a multi-class classifier $\bm{f}:\mathcal{X}\rightarrow [0,1]^q$ from a partial-label training set $\mathcal{D}^{\rm Tr}=\{\left(\bm{x}^{\rm Tr}_i,S^{\rm Tr}_i\right)\}_{i=1}^{n^{\rm Tr}}$ where $\bm{f}(\bm{x})=\left[f_{1}(\bm{x}),f_{2}(\bm{x}),\ldots,f_{q}(\bm{x})\right]$ is the estimated class-posterior probability vector for $\bm{x}$.  

\noindent{\textbf{Data generation process.}}~~~There are mainly two ways to generate synthetic \emph{instance-independent} partial labels, i.e., the uniform sampling strategy~(USS)~\citep{feng2020provably,feng2020learning} and the flipping probability strategy~(FPS)~\citep{zhang20222exploiting}. \citet{feng2020provably} proposed the USS that the candidate label set containing the ground-truth label is sampled from a uniform distribution, i.e., 
\begin{eqnarray}
p(S|\bm{x}, y)=\frac{1}{2^{q-1} - 1}\mathbb{I}\left(y\in S\right),
\end{eqnarray}
where $\mathbb{I}(\pi)$ returns 1 if predicate $\pi$ holds and returns 0 otherwise. \citet{wen2021leveraged} proposed the FPS that each false positive label is independently drawn into the candidate label set with a flipping probability $p(z\in S|y)$. Then, the class-conditional probability distribution of the candidate label set could be formulated as
\begin{eqnarray}
p(S|\bm{x}, y)=\prod_{m\in S, m\neq y}p(m\in S|y)\cdot \prod_{t\notin S}\left(1-p(t\in S|y)\right).
\end{eqnarray}
Although FPS seems more practical, the flipping probability is unknown. Many papers assume that the flipping probability is a constant value for different labels, which is often not true in real-world scenarios~\citep{wei2022learning}. \citet{xu2021instance} proposed \emph{instance-dependent} PLL, which assumes that the generation of partial labels also depends on the feature. However, the datasets are still synthesized by using the model outputs of an auxiliary network. In Section~\ref{plcifar10_section}, we present a more realistic benchmark dataset with human-annotated partial labels.

 

\section{Realistic Model Selection Criteria for PLL}\label{model_selection_section}
Model selection is a critical component in machine learning problems. However, it has received little attention in the context of PLL. Most PLL work assumes the existence of a clean ordinary-label dataset, which may be neither consistent with the original definition of PLL nor practical, as discussed in Section~\ref{intro_sec}. To promote fair and practical evaluation of PLL algorithms, it is important to systematically examine the model selection procedure for PLL. We follow the original definition of PLL to have only a single partial-label training set~\citep{cour2011learning,zhang2017disambiguation}. Then, following a widely used validation procedure in machine learning~\citep{raschka2018model,gulrajani2021in}, we divide a partial-label validation set $\mathcal{D}^{\rm Val}=\{\left(\bm{x}^{\rm Val}_i,S^{\rm Val}_i\right)\}_{i=1}^{n^{\rm Val}}$ from the training set for model selection. Next, we introduce each model selection criterion in turn. 
\subsection{Covering Rate}
\begin{definition}[Covering Rate~(CR)]The Covering Rate of a multi-class classifier $\bm{f}$ on the partial-label validation set $\mathcal{D}^{\rm Val}$ is defined as:
\begin{equation}
{\rm CR}(\bm{f})=\frac{1}{n^{\rm Val}}\sum_{i=1}^{n^{\rm Val}}\mathbb{I}\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x}^{\rm Val}_i)\in S^{\rm Val}_i\right).
\end{equation}
\end{definition}
CR indicates the fraction of validation data whose predicted label is included in its candidate label set. It is a natural metric in PLL, but its effectiveness may depend on the size of the candidate label sets. When the number of partial labels of each example is equal to 1, i.e., $|S|=1$, PLL is reduced to ordinary supervised learning and CR is reduced to the validation accuracy. However, as $|S|$ increases, more false positive labels are included in the candidate label set. In the most extreme case, when $|S|=q$, PLL is reduced to unsupervised learning and CR does not convey effective information. Before analyzing the gap between CR and the validation accuracy, we introduce the following definition.
\begin{definition}[Ambiguity Degree]\label{sadc}The Ambiguity Degree $\gamma$ is defined as 
\begin{equation}
\gamma = \sup_{(\bm{x},y)\sim p(\bm{x},y),S\sim p(S|\bm{x},y),\bar{y}\neq y} p(\bar{y}\in S),
\end{equation}
where $p(\bm{x},y)$ is the joint distribution over $\bm{x}$ and $y$.
\end{definition}
If $\gamma<1$, the small ambiguity degree is satisfied and the ERM learnability for PLL is guaranteed~\citep{cour2011learning,liu2014learnability}. We also define the expected accuracy as 
\begin{equation}
{\rm ACC}\left(\bm{f}\right)=\mathbb{E}_{p(\bm{x},y)}\mathbb{I}(\mathop{\arg\max}_{l}~f_{l}\left(\bm{x}\right)= y).
\end{equation}
Then, we have the following proposition.
\begin{proposition}\label{cr_bound}
Suppose that there is a constant $\epsilon\in\left(0,1\right)$ such that the expected accuracy of a classifier $\bm{f}$ satisfies ${\rm ACC}\left(\bm{f}\right)\geq\epsilon$. Then, we have $\mathbb{E}\left[{\rm CR}(\bm{f})\right]-{\rm ACC}\left(\bm{f}\right)\leq(1-\epsilon)\gamma$.
\end{proposition}
The proof is in Appendix~\ref{apd:proofs}. The gap between CR and the accuracy is affected by both the accuracy and the ambiguity degree. If the classifier is more accurate and the partial-label dataset is less ambiguous, the gap between the CR metric and the accuracy will be smaller. Furthermore, we show that the minimizers of both are the same under certain conditions.
\begin{theorem}\label{cr_consistency}
Suppose that the partial labels are generated by following the USS or the FPS with a constant flipping probability. Then, for any two classifiers $\bm{f}_1$ and $\bm{f}_2$ that satisfy $\mathbb{E}\left[{\rm CR}(\bm{f}_1)\right] < \mathbb{E}\left[{\rm CR}(\bm{f}_2)\right]$, we have ${\rm ACC}\left(\bm{f}_1\right) < {\rm ACC}\left(\bm{f}_2\right)$.
\end{theorem}
The proof is in Appendix~\ref{apd:proofs}. Theorem~\ref{cr_consistency} shows that when partial labels are generated by using the USS or the FPS with a constant flipping probability, the classifier that minimizes the expectation of CR will also minimize the expected accuracy. Therefore, the CR metric will serve as a \emph{consistent} model selection criterion for PLL under certain data distribution assumptions. However, this conclusion may not hold when partial labels are not generated by either strategy.

\subsection{Approximated Accuracy}
Next, we introduce the definition of the Approximated Accuracy metric.
\begin{definition}[Approximated Accuracy~(AA)]The Approximated Accuracy of a multi-class classifier $\bm{f}$ on the partial-label validation set $\mathcal{D}^{\rm Val}$ is defined as:
\begin{equation}
\mathrm{AA}(\bm{f})=\frac{1}{n^{\rm Val}}\sum_{i=1}^{n^{\rm Val}}\sum_{j\in S^{\rm Val}_i}\frac{f_{j}(\bm{x}^{\rm Val}_i)}{\sum_{k\in S^{\rm Val}_i}f_{k}(\bm{x}^{\rm Val}_i)}\mathbb{I}\left(\mathop{\arg\max}_{l}~f_{l}(\bm{x}^{\rm Val}_i)= j\right).
\end{equation}
\end{definition}
Then, we have the following theorem.
\begin{theorem}\label{ure_thm}
Suppose there is a function $C: \mathcal{X}\bigtimes2^{\mathcal{Y}}\mapsto\mathbb{R}$ such that the condition $p(S|\bm{x}, y)=C(\bm{x}, S)\mathbb{I}\left(y\in S\right)$ holds for partial-label data. Suppose further that the multi-class classifier $\bm{f}(\bm{x})$ is consistent with $p(y|\bm{x})$. Then, under mild conditions $\mathrm{AA}(\bm{f})$ is statistically consistent with the expected accuracy, i.e., $\mathbb{E}\left[{\rm AA}(\bm{f})\right]={\rm ACC}\left(\bm{f}\right)$.
\end{theorem}
The proof can be found in Appendix~\ref{apd:proofs}. The introduction of AA is inspired by data-generation-based strategies for PLL~\citep{wu2023learning}. Theorem~\ref{ure_thm} illustrates that AA can be a consistent metric w.r.t.~the expected classification accuracy on test data under certain assumptions. In particular, the data distribution assumption can hold for a wide range of types of partial labels~\citep{liu2012conditional,wu2023learning}. However, there are two factors that may affect its effectiveness. First, it may not be suitable for certain algorithms~\citep{wen2021leveraged} where the loss function is not \emph{strictly proper}~\citep{gneiting2007strictly,charoenphakdee2021on} and its modeling output is not calibrated to the posterior probabilities. Second, it requires that the modeling output to be accurate, which may not be satisfied in the early stages of training. 
\subsection{Oracle Accuracy}
Finally, we present the definition of the Oracle Accuracy metric.
\begin{definition}[Oracle Accuracy~(OA)]
The Oracle Accuracy of a multi-class classifier $\bm{f}$ on a partial-label validation set $\mathcal{D}^{\rm Val}$ is defined as
\begin{equation}
\mathrm{OA}(\bm{f})=\frac{1}{n^{\rm Val}}\sum_{i=1}^{n^{\rm Val}}\mathbb{I}\left(\mathop{\arg\max}_{l}~f_{l}(\bm{x}^{\rm Val}_i)= y^{\rm Val}_i\right),
\end{equation}
where $y^{\rm Val}_i$ is the underlying true label of $\bm{x}^{\rm Val}_i$.
\end{definition}
The OA metric is natural if we have access to true labels in supervised learning. However, such a condition is not realistic in real problems of PLL. In fact, the availability of true labels contradicts the original motivation of PLL, which is to reduce labeling costs at the expense of true label ignorance. Inspired by~\citet{gulrajani2021in}, we restrict the use of true labels by allowing only one query (the last checkpoint) for each hyperparameter configuration. This means that we do not allow early stopping when using the OA metric. We try to compensate for the unrealistic access to true labels by restricting the model selection space. We also include the results of OA with early stopping (ES), which can be considered as an upper bound of the model selection performance, for reference. Actually, OA with ES may be the most common model selection criterion for PLL papers.
\section{PLCIFAR10: A Dataset with Human-Annotated Partial Labels}\label{plcifar10_section}
In this section, we present a novel image dataset with human-annotated partial labels to compensate for the lack of real-world image datasets for PLL. We used Amazon Mechanical Turk~(MTurk) as our crowdsourced annotation platform. CIFAR-10~\citep{krizhevsky2009learning} was chosen as the base dataset because it has been widely used in the PLL literature. In addition, the difficulty and workload are relatively moderate, which can facilitate our experimental design and analysis. We posted the annotation tasks of the images from the training set of CIFAR-10 as Human Intelligence Tasks~(HITs), and the crowdsourced workers received salaries by completing the HITs. We created 5000 HITs, each containing 10 random images. Workers were allowed to select \emph{multiple candidate labels}, which could include the true label for an image. We then asked 10 different workers to perform the same HIT at the same time. As a result, each image could be annotated with multiple candidate label sets, and each candidate label set could contain multiple labels. More details on the data collection can be found in Appendix~\ref{apd:datasets}. 

\begin{figure*}[t]
  \centering
  \subfigure[t][Class Distribution]{
    \includegraphics[width=0.23\textwidth]{fig/label_count.pdf}\label{ds_fig_a}
  }
  \subfigure[t][Noise Rate Distribution]{
    \includegraphics[width=0.235\textwidth]{fig/noise_distribution.pdf}\label{ds_fig_b}
  }
  \subfigure[t][Aggregate]{
    \includegraphics[width=0.23\textwidth]{fig/transition_heatmap_aggregate.pdf}\label{ds_fig_c}
  }
  \subfigure[t][Vaguest]{
    \includegraphics[width=0.23\textwidth]{fig/transition_heatmap_vaguest.pdf}\label{ds_fig_d}
  }
  \caption{(a). The distribution of the collected partial labels of PLCIFAR10. (b) The noise rate with the increasing number of annotators. (c) The flipping probability matrix computed on PLCIFAR10-Aggregate. (d) The flipping probability matrix computed on PLCIFAR10-Vaguest.}\label{plcifar10}
\end{figure*}


In summary, we collected 502,190 candidate label sets with a total of 712,109 partial labels. Figure~\ref{ds_fig_a} shows the distribution of our collected partial labels. The imbalanced distribution of partial labels may not match the commonly used USS or FPS with a constant probability. Furthermore, we find that the noise rate, i.e., the proportion of training examples whose candidate label sets do not contain the true label, is high with few annotators, as shown in Figure~\ref{ds_fig_b}. As the number of annotators increases, the aggregation of partial labels may become less noisy. We consider two versions of PLCIFAR10 for experiments. The first is \textbf{PLCIFAR10-Aggregate}, which assigns the aggregation of all partial labels from all annotators to each example. The second is \textbf{PLCIFAR10-Vaguest}, which assigns to each example the largest candidate label set from the annotators. Since PLCIFAR10-Vaguest has a high noise rate, it actually serves as a dataset for noisy PLL~\citep{xu2023alim,lv2024on}, which is a more practical setting since it is hard to ensure that annotated partial labels always include the true label in real-world applications. Figures~\ref{ds_fig_c} and~\ref{ds_fig_d} show the flipping probability matrices $p(j\in S| y=i)$ computed based on the two versions of PLCIFAR10. We can see that the flipping probabilities are not equal and the diagonals are not all 1, which is more challenging and practical than current synthetic datasets generated with the USS or the FPS.
\section{Setup of \textsc{Plench}}
\subsection{Benchmark algorithms}
We have divided the benchmark algorithms into four main groups. The detailed descriptions and hyperparameter settings can be found in Appendix~\ref{apd:algos}. 

\noindent{\textbf{Vanilla deep PLL algorithms.}}~~~We considered deep PLL algorithms that used simple loss functions or disambiguation strategies without strong regularization techniques as vanilla deep PLL algorithms. Identification-based strategies included PRODEN~\citep{lv2020progressive}, CAVL~\citep{zhang20222exploiting}, and POP~\citep{xu2023progressive}. Note that RC~\citep{feng2020provably} is in the same form as PRODEN, so we only included PRODEN here to avoid repetitive comparisons. The averaging-based strategies included ABS-MAE~\citep{lv2024on} and ABS-GCE~\citep{lv2024on}, which showed favorable performance among the losses in the family. 
The data-generation-based strategies included EXP~\citep{feng2020learning}, MCL-GCE~\citep{feng2020learning}, MCL-MSE~\citep{feng2020learning}, CC~\citep{feng2020provably}, LWS~\citep{wen2021leveraged}, and IDGP~\citep{qiao2023decompositional}. Also, LOG~\citep{feng2020learning} is analogous to CC, so we only included CC here.

\noindent{\textbf{Vanilla deep CLL algorithms.}}~~~Complementary-label learning~(CLL) is a special case of PLL with only one label excluded from each candidate label set of training examples, i.e., $|S|=q-1$~\citep{wang2024climage}. If we consider each label outside the candidate label set as a complementary label, it is possible to apply CLL algorithms to solve PLL problems as well~\citep{feng2020learning}. Therefore, we included several vanilla CLL algorithms with simple loss functions in \textsc{Plench}. The vanilla CLL algorithms included PC~\citep{ishida2017learning}, Forward~\citep{yu2018learning}, NN~\citep{ishida2019complementary}, GA~\citep{ishida2019complementary}, SCL-EXP~\citep{chou2020unbiased}, SCL-NL~\citep{chou2020unbiased}, L-W~\citep{gao2021discriminative}, and OP-W~\citep{liu2023consistent}.

\noindent{\textbf{Holistic deep PLL algorithms.}}~~~We considered PLL algorithms that employ strong representation learning or regularization techniques to be holisitic PLL algorithms. We used five algorithms, including VALEN~\citep{xu2021instance}, PiCO~\citep{wang2022pico}, ABLE~\citep{xia2022ambiguity}, CRDPLL~\citep{wu2022revisiting}, and DIRK~\citep{wu2024distilling}.

\noindent{\textbf{Deep noisy PLL algorithms.}}~~~We also included three noisy PLL algorithms that explicitly considered tailored strategies to handle the cases where the true label might be outside the candidate label set. The deep noisy PLL algorithms included FREDIS~\citep{qiao2023fredis}, ALIM~\citep{xu2023alim}, and PiCO+~\citep{wang2024picoplus}.
\subsection{Benchmark Datasets}
To comprehensively evaluate the model performance of all algorithms, we included eleven real-world PLL benchmark datasets, including nine widely used tabular datasets and two image datasets that we collected. The tabular datasets included Lost~\citep{cour2011learning}, Soccer Player~\citep{zeng2013learning}, and Yahoo! News~\citep{guillaumin2010multiple} for the automatic face naming task, MSRCv2~\citep{liu2012conditional} for the object classification task, Mirflickr~\citep{huiskes2008mir} for the web image classification task, Birdsong~\citep{briggs2012rank} for the bird song classification task, Malagasy~\citep{garrette2013learning}, Italian~\citep{johan2009converting}, and English~\citep{zhou2018weakly} for the POS tagging task. A detailed description of the datasets can be found in Appendix~\ref{apd:datasets}.

\subsection{Experimental Settings}
In this paper, we mainly investigate the hyperparameter tuning problem in the context of model selection. For tabular datasets, we first divided a test set from the entire dataset. Since the datasets were not explicitly divided into training and validation parts, we manually divided them into a partial-label training set $\mathcal{D}^{\rm Tr}$ and a partial-label validation set $\mathcal{D}^{\rm Val}$. Then, we trained a model with $\mathcal{D}^{\rm Tr}$ and evaluated its validation performance on $\mathcal{D}^{\rm Val}$ with the model selection criteria proposed in Section~\ref{model_selection_section} as well as its test performance on the test set $\mathcal{D}^{\rm Te}$ with ordinary labels. We then selected the checkpoint with the best validation performance and returned the corresponding test accuracy as the final result. We randomly selected a set of hyperparameter configurations from a given pool for a given data split and recorded the mean accuracy as well as the standard deviations for the best performance with different dataset splits. We used ResNet~\citep{he2016deep} and DenseNet~\citep{huang2017densely} for image datasets and a multilayer perceptron~(MLP) with a hidden layer width of 500 equipped with the ReLU~\citep{nair2010rectified} activation function for tabular datasets. 
\section{Experiments}
\subsection{Experimental Results on Tabular Datasets}
Figure~\ref{tabular_res} shows the box plots of vanilla deep PLL and CLL algorithms on real tabular datasets, where NN and GA are not included in the figure because their performance was not satisfactory. Moreover, most holistic deep PLL algorithms are based on different data augmentation strategies and cannot be applied here. Detailed experimental results can be found in Appendix~\ref{apd:exp_res}. We can observe that AA does not work well in some cases where the classifier is not accurate enough. The CR metric is sometimes more effective and can serve as an alternative model selection criterion. Therefore, we may need to prepare different model selection criteria to effectively determine the hyperparameters in different cases. The classification performance of PRODEN is very strong in most cases, which clearly confirms its effectiveness. However, it still cannot outperform all other algorithms in all cases. In addition, some early vanilla CLL algorithms, such as Forward, SCL-EXP, and OP-W, can also achieve good performance and deserve to be considered when conducting comparative experiments for PLL. 
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig/tabular_exp.pdf}
  \caption{Experimental results of different algorithms on tabular datasets. The top, middle, and bottom figures correspond to box plots of experimental results using CR, AA, OA, and OA with ES for hyperparameter tuning, respectively. The colors of the bars indicate the mean accuracy.}\label{tabular_res}
\end{figure*}
\begin{table}[ht]
\centering
%\footnotesize
\scriptsize
\vspace{-10pt}
\caption{Classification accuracy (mean$\pm$std) of each algorithm on PLCIFAR10-Aggregate with ResNet, where the best performance w.r.t. each metric is shown in bold.}\label{aggregate_res}
%\setlength{\tabcolsep}{10pt}
%\vspace{-2pt}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}}&\multicolumn{1}{c}{\textbf{Venue}}                         & \textbf{w/ CR}                                     & \textbf{w/ AA}                                     & \textbf{w/ OA}                                     & \textbf{w/ OA \& ES}                               \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & \bf 85.95$\pm$0.08                                     & 85.77$\pm$0.11                                     & 85.91$\pm$0.18                                     & 86.03$\pm$0.13                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 68.09$\pm$0.25                                     & 65.92$\pm$1.15                                     & 68.23$\pm$0.17                                     & 69.12$\pm$0.47                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 84.94$\pm$0.08                                     & 85.27$\pm$0.34                                     & 85.04$\pm$0.27                                     & 85.53$\pm$0.24                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 55.13$\pm$1.13                                     & 45.51$\pm$4.38                                     & 54.31$\pm$1.22                                     & 55.32$\pm$1.15                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 74.21$\pm$1.08                                     & 71.28$\pm$1.20                                     & 74.68$\pm$0.80                                     & 76.00$\pm$0.44                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 10.00$\pm$0.00                                     & 10.00$\pm$0.00                                     & 10.00$\pm$0.00                                     & 10.00$\pm$0.00                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 60.57$\pm$0.60                                     & 58.26$\pm$1.14                                     & 32.31$\pm$9.73                                     & 62.26$\pm$0.72                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 61.03$\pm$1.25                                     & 10.00$\pm$0.00                                     & 55.09$\pm$1.96                                     & 63.17$\pm$0.59                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 80.66$\pm$0.23                                     & 80.77$\pm$0.46                                     & 81.44$\pm$0.29                                     & 81.87$\pm$0.11                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 55.31$\pm$0.32                                     & 26.44$\pm$5.35                                     & 55.50$\pm$0.58                                     & 56.17$\pm$0.51                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 82.80$\pm$0.30                                     & 80.72$\pm$0.66                                     & 83.43$\pm$0.17                                     & 83.65$\pm$0.44                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 71.45$\pm$0.71                                     & 70.34$\pm$0.39                                     & 71.06$\pm$0.56                                     & 72.24$\pm$0.75                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 81.19$\pm$0.24                                     & 79.51$\pm$0.33                                     & 80.98$\pm$0.49                                     & 81.57$\pm$0.38                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 30.68$\pm$0.39                                     & 25.97$\pm$1.17                                     & 29.50$\pm$0.51                                     & 31.65$\pm$0.38                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 37.81$\pm$1.00                                     & 37.51$\pm$1.19                                     & 36.84$\pm$1.37                                     & 38.26$\pm$0.79                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 79.50$\pm$0.33                                     & 79.57$\pm$0.37                                     & 79.34$\pm$0.53                                     & 80.30$\pm$0.15                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 81.87$\pm$0.07                                     & 81.09$\pm$0.55                                     & 79.96$\pm$0.56                                     & 81.75$\pm$0.14                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 76.76$\pm$0.49                                     & 74.04$\pm$0.87                                     & 74.76$\pm$0.60                                     & 76.76$\pm$0.49                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 78.91$\pm$0.22                                     & 78.71$\pm$0.39                                     & 79.64$\pm$0.21                                     & 81.15$\pm$0.49                                     \\
PiCO&ICLR 2022~\citep{wang2022pico}                & 79.20$\pm$0.61                                     & 75.19$\pm$0.59                                     & 79.37$\pm$0.50                                     & 79.88$\pm$0.77                                     \\
ABLE&IJCAI 2022~\citep{xia2022ambiguity}           & 85.86$\pm$0.18                                     & \bf 86.30$\pm$0.14                                     &\bf 86.07$\pm$0.08                                     & \bf 86.46$\pm$0.10                                     \\
CRDPLL&ICML 2022~\citep{wu2022revisiting}          & 81.60$\pm$0.55                                     & 79.82$\pm$0.16                                     & 81.66$\pm$0.52                                     & 82.36$\pm$0.19                                     \\
DIRK&AAAI 2024~\citep{wu2024distilling}            & 85.90$\pm$0.31                                     & 84.96$\pm$0.63                                     & 85.74$\pm$0.42                                     & 85.60$\pm$0.28                                     \\
FREDIS&ICML 2023~\citep{qiao2023fredis}            & 85.01$\pm$0.08                                     & 84.94$\pm$0.07                                     & 85.66$\pm$0.22                                     & 85.74$\pm$0.09                                     \\
ALIM&NeurIPS 2023~\citep{xu2023alim}               & 63.51$\pm$0.73                                     & 61.76$\pm$1.57                                     & 61.42$\pm$1.16                                     & 64.72$\pm$0.27                                     \\
PiCO+&TPAMI 2024~\citep{wang2024picoplus}          & 60.07$\pm$0.13                                     & 57.63$\pm$0.84                                     & 59.01$\pm$1.20                                     & 62.66$\pm$0.33                                     \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on PLCIFAR10-Vaguest with ResNet, where the best performance w.r.t. each metric is shown in bold.}\label{vaguest_res}
%\setlength{\tabcolsep}{10pt}
%\vspace{-2pt}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 74.95$\pm$0.09                                     & 68.54$\pm$0.39                                     & 74.78$\pm$0.64                                     & 76.94$\pm$0.41                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 63.62$\pm$0.27                                     & 61.76$\pm$0.75                                     & 63.70$\pm$0.62                                     & 64.68$\pm$0.18                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 75.17$\pm$0.53                                     & 67.71$\pm$0.54                                     & 74.35$\pm$0.25                                     & 76.08$\pm$0.18                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 55.58$\pm$0.53                                     & 32.25$\pm$7.18                                     & 55.60$\pm$0.66                                     & 55.84$\pm$0.50                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 75.17$\pm$0.34                                     & 72.86$\pm$0.17                                     & 74.13$\pm$0.68                                     & 75.95$\pm$0.35                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 63.93$\pm$0.39                                     & 61.77$\pm$0.16                                     & 63.04$\pm$0.58                                     & 64.14$\pm$0.36                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 71.74$\pm$0.21                                     & 64.53$\pm$1.67                                     & 70.78$\pm$0.32                                     & 73.28$\pm$0.19                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 67.99$\pm$0.96                                     & 65.91$\pm$1.44                                     & 64.94$\pm$0.27                                     & 69.98$\pm$0.49                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 71.78$\pm$0.50                                     & 61.05$\pm$0.22                                     & 70.11$\pm$0.43                                     & 73.26$\pm$0.51                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 60.21$\pm$0.59                                     & 44.12$\pm$10.76                                    & 60.98$\pm$0.46                                     & 61.78$\pm$0.67                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 76.14$\pm$0.18                                     & 71.36$\pm$2.72                                     & 76.05$\pm$0.31                                     & 77.74$\pm$0.19                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 64.46$\pm$1.88                                     & 66.02$\pm$0.48                                     & 66.59$\pm$0.34                                     & 68.13$\pm$0.29                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 70.01$\pm$0.43                                     & 43.09$\pm$12.96                                    & 70.52$\pm$0.07                                     & 70.98$\pm$0.23                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 33.23$\pm$0.32                                     & 25.31$\pm$1.05                                     & 30.35$\pm$0.37                                     & 33.12$\pm$0.48                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 33.94$\pm$1.01                                     & 33.64$\pm$1.20                                     & 31.35$\pm$0.86                                     & 34.00$\pm$0.98                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 71.13$\pm$0.51                                     & 65.45$\pm$0.58                                     & 71.33$\pm$0.50                                     & 72.86$\pm$0.28                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 69.66$\pm$0.50                                     & 60.46$\pm$1.54                                     & 69.56$\pm$0.47                                     & 71.52$\pm$0.14                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 69.81$\pm$0.59                                     & 59.19$\pm$1.45                                     & 71.12$\pm$0.19                                     & 72.44$\pm$0.45                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 70.53$\pm$0.51                                     & 69.64$\pm$2.67                                     & 73.20$\pm$0.26                                     & 73.21$\pm$0.10                                     \\
PiCO&ICLR 2022~\citep{wang2022pico}                & 74.06$\pm$0.22                                     & 68.33$\pm$0.58                                     & 73.27$\pm$0.46                                     & 74.70$\pm$0.33                                     \\
ABLE&IJCAI 2022~\citep{xia2022ambiguity}           & 75.49$\pm$0.58                                     & 68.14$\pm$0.25                                     & 74.87$\pm$0.49                                     & 76.27$\pm$0.13                                     \\
CRDPLL&ICML 2022~\citep{wu2022revisiting}          & 76.21$\pm$0.58                                     & 70.99$\pm$0.19                                     & 75.70$\pm$0.09                                     & 77.68$\pm$0.46                                     \\
DIRK&AAAI 2024~\citep{wu2024distilling}            & \bf 80.32$\pm$0.15                                     & \bf 75.02$\pm$2.36                                     & \bf 80.10$\pm$0.33                                     & \bf 81.08$\pm$0.32                                     \\
FREDIS&ICML 2023~\citep{qiao2023fredis}            & 74.57$\pm$0.90                                     & 67.72$\pm$0.13                                     & 73.45$\pm$0.32                                     & 76.94$\pm$0.24                                     \\
ALIM&NeurIPS 2023~\citep{xu2023alim}               & 65.49$\pm$1.05                                     & 66.51$\pm$0.83                                     & 67.57$\pm$1.93                                     & 70.59$\pm$1.21                                     \\
PiCO+&TPAMI 2024~\citep{wang2024picoplus}          & 61.59$\pm$1.55                                     & 59.65$\pm$0.84                                     & 61.27$\pm$0.12                                     & 64.75$\pm$0.30                                     \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on PLCIFAR10-Aggregate with DenseNet, where the best performance w.r.t. each metric is shown in bold.}\label{aggregate_res_dn}
%\setlength{\tabcolsep}{10pt}
%\vspace{-2pt}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 80.94$\pm$0.54                                     & \bf 81.54$\pm$0.72                                     & 80.94$\pm$0.90                                     & 81.30$\pm$0.74                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 61.12$\pm$2.71                                     & 60.17$\pm$2.34                                     & 63.52$\pm$1.80                                     & 63.82$\pm$1.68                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 80.99$\pm$0.31                                     & 80.90$\pm$0.53                                     & 81.20$\pm$0.41                                     & 81.38$\pm$0.43                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 53.93$\pm$2.12                                     & 50.56$\pm$1.85                                     & 52.56$\pm$1.68                                     & 53.85$\pm$2.05                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 72.48$\pm$0.34                                     & 71.50$\pm$0.46                                     & 73.05$\pm$0.71                                     & 74.28$\pm$0.70                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 10.00$\pm$0.00                                     & 10.00$\pm$0.00                                     & 10.00$\pm$0.00                                     & 10.00$\pm$0.00                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 58.82$\pm$0.31                                     & 57.81$\pm$0.24                                     & 36.47$\pm$10.98                                    & 61.34$\pm$0.03                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 57.29$\pm$0.22                                     & 10.00$\pm$0.00                                     & 54.27$\pm$2.35                                     & 59.37$\pm$0.38                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 78.28$\pm$0.82                                     & 77.64$\pm$0.67                                     & 78.52$\pm$0.38                                     & 78.97$\pm$0.49                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 47.57$\pm$0.20                                     & 41.64$\pm$1.91                                     & 48.48$\pm$0.42                                     & 48.90$\pm$0.41                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 77.49$\pm$0.86                                     & 76.07$\pm$0.80                                     & 78.41$\pm$0.84                                     & 79.03$\pm$0.86                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 65.60$\pm$0.13                                     & 65.95$\pm$0.54                                     & 66.12$\pm$0.34                                     & 66.55$\pm$0.47                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 78.74$\pm$0.48                                     & 78.02$\pm$0.30                                     & 78.38$\pm$0.39                                     & 79.39$\pm$0.19                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 31.51$\pm$0.98                                     & 26.01$\pm$1.37                                     & 30.22$\pm$0.48                                     & 32.97$\pm$0.79                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 37.21$\pm$0.58                                     & 36.85$\pm$0.46                                     & 37.28$\pm$0.18                                     & 37.86$\pm$0.20                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 78.67$\pm$0.71                                     & 78.27$\pm$0.69                                     & 78.38$\pm$0.34                                     & 78.83$\pm$0.32                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 79.26$\pm$0.59                                     & 78.40$\pm$0.98                                     & 78.29$\pm$0.27                                     & 79.26$\pm$0.59                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 71.68$\pm$0.53                                     & 71.77$\pm$0.29                                     & 72.24$\pm$0.69                                     & 73.78$\pm$0.56                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 79.95$\pm$0.36                                     & 78.86$\pm$0.43                                     & 80.04$\pm$0.16                                     & 80.36$\pm$0.32                                     \\
PiCO&ICLR 2022~\citep{wang2022pico}                & 74.89$\pm$2.14                                     & 74.23$\pm$0.65                                     & 75.81$\pm$1.58                                     & 76.37$\pm$1.61                                     \\
ABLE&IJCAI 2022~\citep{xia2022ambiguity}           & \bf 81.38$\pm$0.33                                     & 81.40$\pm$0.34                                     & \bf 81.21$\pm$0.49                                     & 81.28$\pm$0.61                                     \\
CRDPLL&ICML 2022~\citep{wu2022revisiting}          & 74.97$\pm$0.99                                     & 74.90$\pm$0.52                                     & 75.36$\pm$0.59                                     & 75.67$\pm$0.66                                     \\
DIRK&AAAI 2024~\citep{wu2024distilling}            & 77.74$\pm$0.64                                     & 77.83$\pm$0.53                                     & 77.86$\pm$0.67                                     & 77.85$\pm$0.76                                     \\
FREDIS&ICML 2023~\citep{qiao2023fredis}            & 81.25$\pm$0.56                                     & 81.08$\pm$0.80                                     & 81.11$\pm$0.69                                     & \bf 81.66$\pm$0.51                                     \\
ALIM&NeurIPS 2023~\citep{xu2023alim}               & 56.61$\pm$1.02                                     & 57.29$\pm$0.32                                     & 58.46$\pm$0.38                                     & 59.75$\pm$0.64                                     \\
PiCO+&TPAMI 2024~\citep{wang2024picoplus}          & 57.05$\pm$0.71                                     & 54.01$\pm$1.71                                     & 56.02$\pm$1.02                                     & 58.45$\pm$0.15                                     \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on PLCIFAR10-Vaguest with DenseNet, where the best performance w.r.t. each metric is shown in bold.}\label{vaguest_res_dn}
%\setlength{\tabcolsep}{10pt}
%\vspace{-2pt}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 72.73$\pm$0.61                                     & 67.42$\pm$1.06                                     & 72.53$\pm$0.48                                     & 73.98$\pm$0.11                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 55.93$\pm$1.95                                     & 58.85$\pm$1.00                                     & 59.01$\pm$1.29                                     & 59.81$\pm$1.49                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 71.92$\pm$0.87                                     & 69.18$\pm$1.64                                     & 71.90$\pm$0.13                                     & 72.85$\pm$0.21                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 57.07$\pm$2.16                                     & 45.50$\pm$5.21                                     & 57.69$\pm$1.98                                     & 57.94$\pm$2.01                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & \bf 73.30$\pm$0.46                                     & 69.85$\pm$1.27                                     & 72.75$\pm$0.48                                     & 73.63$\pm$0.42                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 61.50$\pm$0.69                                     & 55.26$\pm$2.27                                     & 61.44$\pm$0.54                                     & 62.56$\pm$0.52                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 66.57$\pm$1.37                                     & 61.90$\pm$1.36                                     & 67.88$\pm$1.10                                     & 69.57$\pm$0.70                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 63.37$\pm$1.09                                     & 63.79$\pm$0.92                                     & 63.08$\pm$0.42                                     & 65.26$\pm$0.55                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 68.77$\pm$0.29                                     & 60.92$\pm$0.05                                     & 69.62$\pm$0.33                                     & 71.49$\pm$0.48                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 57.85$\pm$1.47                                     & 54.65$\pm$1.65                                     & 58.16$\pm$2.38                                     & 59.09$\pm$2.00                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 72.02$\pm$0.64                                     & 70.55$\pm$0.91                                     & \bf 72.98$\pm$0.33                                     & \bf 74.19$\pm$0.41                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 59.99$\pm$0.93                                     & 60.28$\pm$0.28                                     & 63.11$\pm$0.48                                     & 63.93$\pm$0.90                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 66.35$\pm$0.97                                     & 60.12$\pm$1.52                                     & 66.54$\pm$0.58                                     & 67.99$\pm$0.14                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 31.08$\pm$0.91                                     & 26.52$\pm$0.37                                     & 30.23$\pm$0.63                                     & 33.54$\pm$0.12                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 35.55$\pm$0.25                                     & 35.52$\pm$0.46                                     & 34.52$\pm$0.55                                     & 35.75$\pm$0.30                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 68.99$\pm$0.34                                     & 65.59$\pm$0.82                                     & 69.41$\pm$0.84                                     & 70.72$\pm$0.49                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 66.90$\pm$0.37                                     & 61.42$\pm$0.22                                     & 65.87$\pm$1.03                                     & 68.71$\pm$0.64                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 68.28$\pm$0.83                                     & 63.52$\pm$0.71                                     & 68.20$\pm$0.48                                     & 69.66$\pm$0.43                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 69.91$\pm$1.42                                     & 70.25$\pm$0.84                                     & 71.55$\pm$0.49                                     & 72.55$\pm$0.41                                     \\
PiCO&ICLR 2022~\citep{wang2022pico}                & 71.76$\pm$0.28                                     & 69.30$\pm$0.93                                     & 70.89$\pm$0.59                                     & 72.27$\pm$0.48                                     \\
ABLE&IJCAI 2022~\citep{xia2022ambiguity}           & 71.68$\pm$1.01                                     & 68.22$\pm$1.08                                     & 72.10$\pm$0.20                                     & 73.44$\pm$0.51                                     \\
CRDPLL&ICML 2022~\citep{wu2022revisiting}          & 70.73$\pm$0.75                                     & 70.69$\pm$0.17                                     & 71.99$\pm$0.67                                     & 72.46$\pm$0.29                                     \\
DIRK&AAAI 2024~\citep{wu2024distilling}            & 70.47$\pm$0.32                                     & \bf 71.06$\pm$0.48                                     & 71.26$\pm$0.19                                     & 71.60$\pm$0.75                                     \\
FREDIS&ICML 2023~\citep{qiao2023fredis}            & 71.38$\pm$0.41                                     & 65.92$\pm$0.13                                     & 71.33$\pm$0.30                                     & 72.95$\pm$0.25                                     \\
ALIM&NeurIPS 2023~\citep{xu2023alim}               & 61.91$\pm$1.23                                     & 62.04$\pm$1.10                                     & 63.84$\pm$0.49                                     & 65.37$\pm$0.72                                     \\
PiCO+&TPAMI 2024~\citep{wang2024picoplus}          & 62.45$\pm$0.74                                     & 59.59$\pm$1.79                                     & 60.67$\pm$0.89                                     & 62.82$\pm$1.10                                     \\
\bottomrule
\end{tabular}}
\end{table}
\subsection{Experimental Results on Image Datasets}
Tables~\ref{aggregate_res} and~\ref{vaguest_res} report the experimental results on PLCIFAR10-Aggregate and PLCIFAR10-Vaguest with ResNet. Tables~\ref{aggregate_res_dn} and~\ref{vaguest_res_dn} report the experimental results on PLCIFAR10-Aggregate and PLCIFAR10-Vaguest with DenseNet. We do not include the results of VALEN because it requires a very large computational and memory budget. The following conclusions can be drawn. First, compared to the results with OA and ES, CR is very effective in most cases. The performance of AA decreases in some cases. We speculate that this is because the modeling results may not be reliable, and the results of AA may deviate from the true accuracy. Second, PRODEN and its variants are still strong in performance, but there is no algorithm that can outperform all other algorithms in all cases. Third, noisy partial labels have a significant impact on the model performance of most algorithms. Therefore, it is still promising to develop effective noisy PLL algorithms in more practical scenarios.

\begin{figure*}[t]
  \centering
  \subfigure[Running Time]{
    \includegraphics[width=0.48\textwidth]{fig/time_fig.pdf}\label{fig:time}
  }
  \subfigure[GPU Memory Utilization]{
    \includegraphics[width=0.48\textwidth]{fig/memory_fig.pdf}\label{fig:memory}
  }
  \caption{Running time and GPU memory utilization for each running step of different PLL algorithms on PLCIFAR10-Vaguest with DenseNet.}\label{fig:efficiency}
\end{figure*}

\subsection{Complexity Analysis}
Figure~\ref{fig:efficiency} shows the running time and GPU memory usage for each running step of different PLL algorithms on PLCIFAR10-Vaguest with DenseNet. We can see that some holistic deep PLL algorithms with strong regularization terms and complicated training strategies can achieve better performance, but they also take more time and use more memory. Therefore, it is important to consider the tradeoff between performance and resource consumption when deciding which algorithm to use given limited computational resources. 
\section{Conclusion}\label{conclusion_sec}
In this paper, we proposed the first PLL benchmark to standardize the performance evaluation of state-of-the-art deep PLL algorithms. We proposed new model selection criteria to fill in the gap of model selection problems for PLL. We also introduced PLCIFAR10, a novel image dataset of human-annotated partial labels. We hope that the availability of this benchmark can promote fair, practical, and standardized evaluation of PLL algorithms in the future. A limitation of our work is that the datasets are relatively small, and we will leave the exploration of large-scale PLL datasets as our future work. In addition, our experiments are conducted with simple deep neural networks. It is also interesting to explore the incorporation of foundation models to improve the model performance of PLL algorithms in the future.
\newpage
\subsubsection*{Acknowledgments}
The authors thank Wei-I Lin, Takashi Ishida, and Yu-Jie Zhang for their helpful discussions and suggestions, Wei-Xuan Bao for his help with the datasets, and Yuko Kawashima for her help with the funding management. The authors thank the anonymous reviewers for their helpful comments. WW was supported by the SGU MEXT Scholarship, by the Junior Research Associate~(JRA) program of RIKEN, and by Microsoft Research Asia. MLZ was supported by National Science Foundation of China (62225602). MS was supported by the Institute for AI and Beyond, UTokyo and by a grant from Apple, Inc. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc.

\subsubsection*{Ethics Statements}
This paper does not raise any ethical concerns. This paper contains a human subjects study where the data collection process was conducted on Amazon MTurk. The privacy of the annotators was strictly protected by Amazon MTurk. We strictly followed the Code of Ethics during the dataset collection process.
 
\bibliographystyle{iclr2025_conference}
\bibliography{plench}
\newpage
\appendix
\section{Proofs}\label{apd:proofs}
\subsection{Proof of Proposition~\ref{cr_bound}}
\begin{proof}[\unskip \nopunct]
\begin{align}
&\mathbb{E}\left[{\rm CR}(\bm{f})\right]-{\rm ACC}\left(\bm{f}\right) \nonumber \\
=&\mathbb{E}_{p(\bm{x},S)}\left[\mathbb{I}\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in S\right)\right]-\mathbb{E}_{p(\bm{x},y)}\left[\mathbb{I}\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})=y\right)\right] \nonumber \\
=&\mathbb{E}_{p(\bm{x},y,S)}\left[\mathbb{I}\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in {S\backslash \{y\}}\right)\right] \nonumber  \\
=&\mathbb{E}_{p(\bm{x},y,S)}\left[\mathbb{I}\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in {S\backslash \{y\}}\right)\mathbb{I}\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\neq y\right)\right] \nonumber \\
=&p\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in {S\backslash \{y\}},~\mathop{\arg\max}_{j}~f_{j}(\bm{x})\neq y\right)\nonumber \\
=&p\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\neq y\right)p\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in {S\backslash \{y\}}|\mathop{\arg\max}_{j}~f_{j}(\bm{x})\neq y\right) \nonumber \\
=&\left(1-{\rm ACC}\left(\bm{f}\right)\right)p\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in {S\backslash \{y\}}|\mathop{\arg\max}_{j}~f_{j}(\bm{x})\neq y\right) \nonumber \\
\leq&(1-\epsilon)\mathop{\max}_{\bar{y}\neq y}~p(\bar{y}\in S|\bm{x},y) \nonumber \\
\leq&(1-\epsilon)\gamma. \nonumber
\end{align}
Here, the third equation can be obtained by traversing the cases of $\mathop{\arg\max}_{j}~f_{j}(\bm{x})=y$, $\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in {S\backslash \{y\}}$, and $\mathop{\arg\max}_{j}~f_{j}(\bm{x})\notin S$. The last inequality results from the definition of the ambiguity degree. The proof is completed.
\end{proof}
\subsection{Proof of Theorem~\ref{cr_consistency}}
\begin{proof}[\unskip \nopunct]
Under the USS assumption or the FPS assumption with a constant flipping probability, the probability $p(\bar{y}\in S|\bm{x},\bar{y}\neq y)$ is a constant value for different $\bar{y}$. Suppose the constant is $c$. Then, we have 
\begin{align}
&\mathbb{E}\left[{\rm CR}(\bm{f})\right]-{\rm ACC}\left(\bm{f}\right) \nonumber \\
=&\left(1-{\rm ACC}\left(\bm{f}\right)\right)p\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})\in {S\backslash \{y\}}|\mathop{\arg\max}_{j}~f_{j}(\bm{x})\neq y\right) \nonumber \\
=&c\left(1-{\rm ACC}\left(\bm{f}\right)\right). \nonumber
\end{align}
Therefore, we have
\begin{equation}
{\rm ACC}\left(\bm{f}\right)=\frac{\mathbb{E}\left[{\rm CR}(\bm{f})\right]-c}{1-c}.
\end{equation}
Since we have $c<1$, for any two classifiers $\bm{f}_1$ and $\bm{f}_2$ that satisfy $\mathbb{E}\left[{\rm CR}(\bm{f}_1)\right] < \mathbb{E}\left[{\rm CR}(\bm{f}_2)\right]$, we have ${\rm ACC}\left(\bm{f}_1\right) < {\rm ACC}\left(\bm{f}_2\right)$. The proof is completed.
\end{proof}
\subsection{Proof of Theorem~\ref{ure_thm}}
The proof of Theorem~\ref{ure_thm} is mainly based on the theoretical results from~\citet{wu2023learning}. We introduce the following lemma. 
\begin{lemma}[\citet{wu2023learning}]\label{ure_lemma}
Assume that there exist a function $C: \mathcal{X}\bigtimes2^{\mathcal{Y}}\mapsto\mathbb{R}$ such that the condition $p(S|\bm{x}, y)=C(\bm{x}, S)\mathbb{I}\left(y\in S\right)$ holds for partial-label data. Then, the classification risk 
\begin{equation}\label{ordinary_risk}
\mathbb{E}_{p(\bm{x},y)}\left[\mathcal{L}(\bm{f}(\bm{x}), y)\right]
\end{equation}
is equivalent to 
\begin{equation}
\mathbb{E}_{p(\bm{x},y)}\left[\sum_{j\in S}\frac{p(y=j|\bm{x})}{\sum_{k\in S}p(y=k|\bm{x})}\mathcal{L}(\bm{f}(\bm{x}), k)\right].
\end{equation}
\end{lemma}
Then, we provide the proof of Theorem~\ref{ure_thm}.
\begin{proof}[Proof of Theorem~\ref{ure_thm}]
We set $\mathcal{L}(\bm{f}(\bm{x}), k)=\mathbb{I}\left(\mathop{\arg\max}_{j}~f_{j}(\bm{x})= y\right)$. Then, if the multi-class classifier $\bm{f}(\bm{x})$ is consistent with $p(y|\bm{x})$, $\mathrm{AA}(\bm{f})$ is statistically consistent with the expected accuracy. The proof is completed.
\end{proof}
\section{More Details of Benchmark Datasets}\label{apd:datasets}
In this section, we first describe the data sheet of PLCIFAR10, a novel dataset collected by us. Then, we provide more information of real-world datasets used in the paper. The summary of all the datasets is shown in Table~\ref{real_world_dataset}.
\begin{table*}[ht]
%\scriptsize
\caption{Characteristics of real-world PLL datasets used in \textsc{Plench}.}\label{real_world_dataset}\vspace{2pt}
\centering
\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
%\tabcolsep 0.05in
%%\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\toprule	 	
\textbf{Dataset} & \textbf{\# Examples} & \textbf{\# Features} & \textbf{\# Classes} & \textbf{Avg. \# CLs} & \textbf{Noise Rate} &\textbf{Type} &\textbf{Task Domain} \\\midrule
Lost  & 1,122  & 108   & 16    & 2.23 & 0\%& tabular&automatic face naming~\citep{cour2011learning} \\
MSRCv2 & 1,758 & 48 & 23 & 3.16 & 0\%&tabular&object classification~\citep{liu2012conditional}  \\
Mirflickr & 2,780 & 1,536 & 14 & 2.76 & 0\%&tabular&web image classification~\citep{huiskes2008mir}  \\
Birdsong & 4,998 & 38 & 13 & 2.18 & 0\%& tabular&bird song classification~\citep{briggs2012rank} \\ 
Malagasy & 5,303 & 384 & 44 & 8.35 &0.04\%& tabular&POS Tagging~\citep{garrette2013learning} \\ 
Soccer Player & 17,472 & 279   & 171   & 2.09 &0\%& tabular&automatic face naming~\citep{zeng2013learning} \\
Italian & 21,878 & 519   & 90   & 1.60 &0\%& tabular&POS Tagging~\citep{johan2009converting} \\
Yahoo! News & 22,991 & 163 & 219 & 1.91 &0\%& tabular&automatic face naming~\citep{guillaumin2010multiple} \\
English & 24,000 & 300 & 45 & 1.19 & 0.97\%& tabular & POS Tagging~\citep{zhou2018weakly} \\
PLCIFAR10-Aggregate (Ours) & 50,000 & 3,072   & 10   & 4.87 &0.13\%& image&image classification~\citep{krizhevsky2009learning} \\
PLCIFAR10-Vaguest (Ours) & 50,000 & 3,072   & 10   & 3.49 &17.56\%& image&image classification~\citep{krizhevsky2009learning} \\
\bottomrule
\end{tabular}%\vspace{15pt}
}
\end{table*}
\subsection{More Details of PLCIFAR10}
For each example, we have a list of lists, where each sublist contains partial labels given by a single annotator. Each image was resized to $256\times256$ for easy annotation. We also imposed several requirements on the annotation task to ensure the quality of the annotations. First, we asked the annotators to choose labels for all ten images. Second, we did not allow the same annotator to select the same label for more than a threshold number of images. Third, we did not allow annotators to select too many partial labels more than a threshold for a single image since the annotation may contain little supervision information. We set the thresholds to 6 for a HIT of 10 images. Crowdworkers were involved in the data collection process. We paid \$0.02 for a HIT, where \$0.01 was given to the crowdworkers and \$0.01 was given to the MTurk platform.

\subsection{More Information of Tabular Datasets}
For face age estimation, we considered ten crowdsourced labels of age numbers along with the true label as partial labels for a given human face. For automatic face naming, we considered the names in the corresponding captions or subtitles as partial labels for a face cropped from an image. For object classification, we considered object classes as partial labels for a segmentation part in an image. For web image classification, we considered tags on a web page as partial labels for a given image. For bird song classification, we considered bird species appearing in a ten-second bird song fragment as partial labels for the singing syllables of the fragment. For POS tagging, we considered all possible POS tags as partial labels for a given word with its contexts.
\begin{table*}[t]
%\scriptsize
\caption{Summary of benchmark algorithms.}\label{algorithms_table}\vspace{2pt}
\centering
\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
%\tabcolsep 0.05in
%%\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule	 	
\multirow{3}{*}{\textbf{Vanilla Deep PLL Algorithms}}&PRODEN~\citep{lv2020progressive}, CAVL~\citep{zhang20222exploiting}, POP~\citep{xu2023progressive}, ABS-MAE~\citep{lv2024on}, \\ & ABS-GCE~\citep{lv2024on}, EXP~\citep{feng2020learning}, MCL-GCE~\citep{feng2020learning},  \\ &MCL-MSE~\citep{feng2020learning}, CC~\citep{feng2020provably}, LWS~\citep{wen2021leveraged}, IDGP~\citep{qiao2023decompositional} \\\midrule
\multirow{2}{*}{\textbf{Vanilla Deep CLL Algorithms}}&PC~\citep{ishida2017learning}, Forward~\citep{yu2018learning}, NN~\citep{ishida2019complementary}, GA~\citep{ishida2019complementary},  \\ & SCL-EXP~\citep{chou2020unbiased}, SCL-NL~\citep{chou2020unbiased}, L-W~\citep{gao2021discriminative}, OP-W~\citep{liu2023consistent} \\\midrule
\multirow{2}{*}{\textbf{Holistic Deep PLL Algorithms}}& VALEN~\citep{xu2021instance}, PiCO~\citep{wang2022pico}, ABLE~\citep{xia2022ambiguity}, CRDPLL~\citep{wu2022revisiting}, \\
&DIRK~\citep{wu2024distilling} \\\midrule
\textbf{Deep Noisy PLL Algorithms} & FREDIS~\citep{qiao2023fredis}, ALIM~\citep{xu2023alim}, PiCO+~\citep{wang2024picoplus} \\
\bottomrule
\end{tabular}}
\end{table*}
\section{Usage of \textsc{Plench}}
PLCIFAR10 is located in \texttt{plench/data/plcifar10} in the code package. Tabular datasets can be downloaded from \url{https://palm.seu.edu.cn/zhangml/Resources.htm#data}. Researchers can easily add newly developed algorithms to \textsc{Plench} to verify their effectiveness. We mainly need to inherit the \texttt{Algorithm} class and implement the \texttt{update} and \texttt{predict} functions for the new algorithm in \texttt{algorithms.py}. For example, to implement CAVL~\citep{zhang20222exploiting}, we can write the following code:
\lstset{
    language=Python,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green}\ttfamily,
    morecomment=[l][\color{magenta}]{\#},
}
\newsavebox{\lsta}
\begin{lrbox}{\lsta}
\begin{lstlisting}
class CAVL(Algorithm):
    def __init__(self, input_shape, train_givenY, hparams):
        super(CAVL, self).__init__(input_shape, train_givenY, hparams)
        self.featurizer = networks.Featurizer(input_shape, self.hparams)
        self.classifier = networks.Classifier(
            self.featurizer.n_outputs,
            self.num_classes)

        self.network = nn.Sequential(self.featurizer, self.classifier)
        self.optimizer = torch.optim.Adam(
            self.network.parameters(),
            lr=self.hparams["lr"],
            weight_decay=self.hparams['weight_decay']
        )
        train_givenY = torch.from_numpy(train_givenY)
        tempY = train_givenY.sum(dim=1).unsqueeze(1).repeat(1, train_givenY.shape[1])
        label_confidence = train_givenY.float()/tempY
        self.label_confidence = label_confidence
        self.label_confidence = self.label_confidence.double()

    def update(self, minibatches):
        _, x, strong_x, partial_y, _, index = minibatches
        loss = self.rc_loss(self.predict(x), index)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.confidence_update(x, partial_y, index)
        return {'loss': loss.item()}

    def rc_loss(self, outputs, index):
        device = "cuda" if index.is_cuda else "cpu"
        self.label_confidence = self.label_confidence.to(device)
        logsm_outputs = F.log_softmax(outputs, dim=1)
        final_outputs = logsm_outputs * self.label_confidence[index, :]
        average_loss = - ((final_outputs).sum(dim=1)).mean()
        return average_loss

    def predict(self, x):
        return self.network(x)

    def confidence_update(self, batchX, batchY, batch_index):
        with torch.no_grad():
            batch_outputs = self.predict(batchX)
            cav = (batch_outputs*torch.abs(1-batch_outputs))*batchY
            cav_pred = torch.max(cav,dim=1)[1]
            gt_label = F.one_hot(cav_pred,batchY.shape[1])
            self.label_confidence[batch_index,:] = gt_label.double()
\end{lstlisting}
\end{lrbox}
\scalebox{0.75}{\usebox{\lsta}}

After implementing the algorithmic code, we can specify the necessary hyperparameters for the algorithm in \texttt{hparams\_registry.py}. Then we can train the model using the new algorithm with the following script:
\newsavebox{\lstd}
\begin{lrbox}{\lstd}
\begin{lstlisting}
python -m \textsc{Plench}.train --data_dir=your_data_path --algorithm CAVL \
--dataset PLCIFAR10_Aggregate --output_dir=your_output_path --steps 60000
\end{lstlisting}
\end{lrbox}

\scalebox{0.75}{\usebox{\lstd}}
\section{Related Work}
There are three main categories of deep PLL algorithms, including identification-based strategies, averaging-based strategies, and data-generation-based strategies. Identification-based strategies try to find the true label from the candidate label set and train a classifier simultaneously~\citep{yao2020deep,lv2020progressive,wang2020semi,zhang20222exploiting,wang2022partial,li2023learning,xu2023progressive,gong2022partial,he2024candidate,tian2024crosel,lv2024what}. Averaging-based strategies consider equal contributions from all candidate labels and use the average modeling output as the final output~\citep{lv2024on}. Data-generation-based strategies model the generative relationship between partial labels and true labels, and derive loss functions with theoretical guarantees~\citep{feng2020provably,feng2020learning,wen2021leveraged,qiao2023decompositional}. In addition, many works use strong representation learning techniques to improve model performance based on these basic strategies, such as contrastive learning and consistency regularization~\citep{wang2022pico,wu2022revisiting,xia2022ambiguity,wu2024distilling}. There are some work that also handles data annotations with multiple labels~\citep{khetan2018learning,peterson2019human,collins2022eliciting,schmarje2022is,gao2022learning,goswami2023aqua} .
\section{Details of deep PLL Algorithms}\label{apd:algos}
In this section, we first give detailed descriptions of the PLL algorithms used in this paper. Then, we provide the hyperparameter configurations for all the algorithms. Table~\ref{algorithms_table} shows the summary of the deep PLL algorithms included in this paper.
\subsection{Descriptions of Algorithms}
The vanilla deep PLL algorithms include:
\begin{itemize}[leftmargin=1em, itemsep=1pt, topsep=0pt, parsep=-1pt]
\item PRODEN~\citep{lv2020progressive}: An identification-based strategy that uses self-training to progressively estimate the true label distribution from the candidate label set.
\item CAVL~\citep{zhang20222exploiting}: An identification-based strategy that uses the class activation value to directly identify the true label from the candidate label set.
\item POP~\citep{xu2023progressive}: An identification-based strategy that progressively filters out false positive labels from the candidate label set based on PRODEN.
\item ABS-MAE~\citep{lv2024on}: An averaging-based strategy using the mean absolute error (MAE) loss function.
\item ABS-GCE~\citep{lv2024on}: An averaging-based strategy using the generalized cross-entropy (GCE) loss function.
\item CC~\citep{feng2020provably}: A data-generation-based strategy using a classifier-consistent loss function based on the uniform distribution assumption.
\item EXP~\citep{feng2020learning}: A data-generation-based strategy using exponential loss under the uniform distribution assumption.
\item MCL-GCE~\citep{feng2020learning}: A data-generation-based strategy using generalized cross-entropy (GCE) loss under the uniform distribution assumption.
\item MCL-MSE~\citep{feng2020learning}: A data-generation-based strategy using mean squared error (MSE) loss based on the uniform distribution assumption.
\item LWS~\citep{wen2021leveraged}: A data-generation-based strategy that uses a leveraged weighted loss function to account for losses from both candidate and non-candidate labels.
\item IDGP~\citep{qiao2023decompositional}: A data-generation-based strategy that performs Maximum A Posterior~(MAP) using a decomposed probability distribution model.
\end{itemize}

The vanilla deep CLL algorithms include
\begin{itemize}[leftmargin=1em, itemsep=1pt, topsep=0pt, parsep=-1pt]
\item PC~\citep{ishida2017learning}: A risk-consistent CLL algorithm using the pairwise-comparison loss based on the uniform distribution assumption.
\item Forward~\citep{yu2018learning}: A classifier-consistent CLL algorithm using a transition matrix to model the complementary-label generation process based on the biased distribution assumption.
\item NN~\citep{ishida2019complementary}: A risk-consistent CLL algorithm using a non-negative risk estimator based on the uniform distribution assumption.
\item GA~\citep{ishida2019complementary}: A risk-consistent CLL algorithm using the gradient ascent technique based on the uniform distribution assumption.
\item SCL-EXP~\citep{chou2020unbiased}: A discriminative CLL algorithm using exponential loss.
\item SCL-NL~\citep{chou2020unbiased}: A discriminative CLL algorithm using negative loss.
\item L-W~\citep{gao2021discriminative}: A discriminative CLL algorithm using weighted loss.
\item OP-W~\citep{liu2023consistent}: A classifier-consistent CLL algorithm by using the opposite number of logits to compute the loss.
\end{itemize}
The holistic deep PLL algorithms:
\begin{itemize}[leftmargin=1em, itemsep=1pt, topsep=0pt, parsep=-1pt]
\item VALEN~\citep{xu2021instance}: An identification-based strategy that uses variational inference to estimate the true label distribution.
\item PiCO~\citep{wang2022pico}: A PLL algorithm that uses the supervised contrastive learning module to improve model performance.
\item ABLE~\citep{xia2022ambiguity}: A PLL algorithm that uses an ambiguity-induced contrastive learning module to improve model performance.
\item CRDPLL~\citep{wu2022revisiting}: A PLL algorithm that uses consistency regularization to improve model performance.
\item DIRK~\citep{wu2024distilling}: A PLL algorithm that uses knowledge distillation and contrastive learning to improve model performance.
\end{itemize}
The deep noisy PLL algorithms include:
\begin{itemize}[leftmargin=1em, itemsep=1pt, topsep=0pt, parsep=-1pt]
\item FREDIS~\citep{qiao2023fredis}: A noisy PLL algorithm that filters out false positive labels while including false negative labels.
\item ALIM~\citep{xu2023alim}: A noisy PLL algorithm that uses a weighted sum of the labeling confidence of candidate and non-candidate label sets as the target. We assume that the true noise rate is accessible.
\item PiCO+~\citep{wang2024picoplus}: A noisy PLL algorithm using distance-based clean sample selection semi-supervised contrastive learning. We assume that the true noise rate is accessible.
\end{itemize}
\subsection{Implementation Details}
All the algorithms were implemented in PyTorch~\citep{paszke2019pytorch} and all experiments were conducted with a single NVIDIA Tesla V100 GPU. We used the Adam optimizer~\citep{kingma2015adam}. We ran 60,000 iterations for the image datasets, 20,000 iterations for the Soccer Player, Italian, Yahoo! News, and English datasets, and 10,000 iterations for the other datasets. We recorded the performance on validation and test sets per 1,000 iterations. We used three random data splits for PLCIFAR10 and five random data splits for tabular datasets. For each data split, we selected 20 random hyperparameter configurations from a given pool. Table~\ref{hyperparameter_table} shows the details of the hyperparameter configurations for all algorithms. 
\begin{table}[t]
\centering
%\footnotesize
\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on Lost with different model selection criteria.}\label{res_lost}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 71.33$\pm$2.45                                     & 70.09$\pm$2.33                                     & 71.33$\pm$2.41                                     & 71.33$\pm$2.29                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 60.00$\pm$3.30                                     & 58.41$\pm$2.99                                     & 64.42$\pm$2.80                                     & 61.95$\pm$2.40                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 69.38$\pm$1.96                                     & 69.38$\pm$1.96                                     & 70.44$\pm$2.45                                     & 73.10$\pm$2.16                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 68.50$\pm$2.70                                     & 67.43$\pm$2.10                                     & 68.85$\pm$3.07                                     & 69.03$\pm$2.99                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 62.65$\pm$2.64                                     & 62.30$\pm$2.04                                     & 62.48$\pm$2.66                                     & 67.43$\pm$2.68                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 71.33$\pm$1.27                                     & 70.27$\pm$2.32                                     & 72.21$\pm$1.59                                     & 70.97$\pm$1.26                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 60.71$\pm$2.41                                     & 58.94$\pm$2.62                                     & 61.95$\pm$1.99                                     & 65.49$\pm$2.14                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 58.58$\pm$2.26                                     & 59.47$\pm$2.71                                     & 58.76$\pm$2.52                                     & 59.47$\pm$3.06                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 71.33$\pm$2.60                                     & 70.62$\pm$2.92                                     & 72.57$\pm$2.80                                     & 72.21$\pm$2.42                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 72.92$\pm$3.59                                     & 71.33$\pm$3.75                                     & 72.21$\pm$2.41                                     & 74.34$\pm$2.68                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 69.73$\pm$2.87                                     & 70.97$\pm$1.59                                     & 75.04$\pm$1.61                                     & 71.68$\pm$2.39                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 66.37$\pm$2.54                                     & 63.01$\pm$2.89                                     & 66.37$\pm$2.33                                     & 67.96$\pm$2.29                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 71.68$\pm$3.70                                     & 71.50$\pm$3.77                                     & 73.27$\pm$2.91                                     & 72.74$\pm$3.51                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 26.73$\pm$1.04                                     & 17.52$\pm$1.49                                     & 22.48$\pm$1.24                                     & 26.37$\pm$1.04                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 18.94$\pm$3.40                                     & 13.63$\pm$2.57                                     & 6.73$\pm$1.14                                      & 20.00$\pm$2.06                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 69.73$\pm$2.50                                     & 70.80$\pm$2.66                                     & 73.81$\pm$1.95                                     & 73.81$\pm$1.85                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 71.68$\pm$2.54                                     & 70.80$\pm$3.03                                     & 74.69$\pm$2.97                                     & 72.74$\pm$1.93                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 63.72$\pm$2.36                                     & 63.36$\pm$1.19                                     & 63.01$\pm$2.81                                     & 65.31$\pm$2.33                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 73.10$\pm$3.14                                     & 62.30$\pm$2.61                                     & 76.28$\pm$2.78                                     & 76.99$\pm$2.68                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 66.02$\pm$2.43                                     & 65.31$\pm$2.55                                     & 64.42$\pm$2.98                                     & 66.19$\pm$3.32                                     \\
\bottomrule
\end{tabular}}
\end{table}
\begin{table}[ht]
%\scriptsize
\caption{Hyperparameters, their default values, and distributions for random search.}\label{hyperparameter_table}
\begin{center}
{
\resizebox{\textwidth}{!}{
\begin{tabular}{llll}
\toprule
\textbf{Condition} & \textbf{Parameter} & \textbf{Default Value} & \textbf{Random Distribution}\\
\midrule
\multirow{3}{*}{ResNet}       & learning rate & 0.001 & $10^{\text{Uniform}(-4.5, -2.5)}$\\
& batch size    & 256   & $2^{\text{Uniform}(6, 9)}$\\
& weight decay & 0.00001 & $10^{\text{Uniform}(-6, -3)}$\\ \midrule
\multirow{3}{*}{MLP}       & learning rate & 0.001 & $10^{\text{Uniform}(-4.5, -2.5)}$\\
& batch size    & 128   & $2^{\text{Uniform}(5, 8)}$\\
& weight decay & 0.00001 & $10^{\text{Uniform}(-6, -3)}$\\ \midrule
\multirow{4}{*}{POP}       & window size & 5 & $\text{RandomChoice}([3, 4, 5, 6, 7])$\\ & warm up epoch &20 & $\text{RandomChoice}([10, 15, 20])$ \\
& initial threshold & 0.001 & $10^{\text{Uniform}(-4.5, -2.5)}$\\ & step size & 0.001 & $10^{\text{Uniform}(-4.5, -2.5)}$\\ \midrule
\multirow{1}{*}{ABS-GCE}  & q & 0.7& 0.7\\ \midrule
\multirow{1}{*}{MCL-GCE}  & q & 0.7& 0.7\\ \midrule
\multirow{1}{*}{LWS}  & leveraged weight & 2& $\text{RandomChoice}([1,2])$\\ \midrule
\multirow{1}{*}{IDGP}  & warm up epoch & 10& $\text{RandomChoice}([5, 10, 15, 20])$\\ \midrule
\multirow{2}{*}{VALEN}       & warm up iteration & 5000 & $\text{RandomChoice}([1000,2000,3000,4000,5000])$\\ & number of nearest neighbors & 3 & $\text{RandomChoice}([3,4])$\\ 
\midrule
\multirow{6}{*}{PiCO}  & warm up iteration & 178 & 178\\ 
% & optimizer update momentum & 0.9 & $\text{RandomChoice}([0.8,0.9])$\\ 
& size of output representation & 128 & $\text{RandomChoice}([64,128,256])$\\ 
& size of the queue & 8192 & $\text{RandomChoice}([4096,8192])$\\ 
& model update momentum & 0.999 & $\text{RandomChoice}([0.9,0.999])$\\ 
&prototype calculation momentum & 0.99 & $\text{RandomChoice}([0.99,0.9])$\\ 
& weight of the contrastive loss & 0.5 & $\text{RandomChoice}([0.5,1.0])$\\ 
\midrule
\multirow{3}{*}{ABLE}       & size of output representation & 128 & $\text{RandomChoice}([64,128,256])$\\ 
& weight of the ambiguity-induced loss & 1 & $\text{RandomChoice}([0.5,1,2])$\\
& temperature & 0.07 & $\text{RandomChoice}([0.03,0.05,0.07,0.09])$\\ 
% & optimizer update momentum & 0.9 & 0.9 \\
\midrule
\multirow{1}{*}{CRDPLL}       & weight of the consistency loss & 1 & 1\\
\midrule
\multirow{1}{*}{DIRK}       & teacher model update momentum & 0.99 & $\text{RandomChoice}([0.5,0.9,0.99])$\\ \midrule
\multirow{9}{*}{FREDIS}       & initial refinement threshold & 0.000001& 0.000001\\ 
& initial disambiguation threshold & 1 & 1 \\
& step size of refinement threshold & 0.000001 & $0.1^{\text{RandomChoice}[6,5,4,3,2,1]}$\\
& step size of disambiguation threshold & 0.000001 & $0.1^{\text{RandomChoice}[6,5,4,3,2,1]}$\\
& number of changed entries & 500 & $\text{RandomChoice}([50,100,300,500,800,1000])$ \\
& time of the number of changed entries & 2 & $\text{RandomChoice}([1,2,3,4,5])$ \\
& epoch of updating intervals  & 20 & $\text{RandomChoice}([10,20])$ \\
& weight of the consistency loss & 10 & $\text{RandomChoice}([1,10])$ \\
& weight of the supervised loss & 1 & $\text{RandomChoice}([0.001, 0.01, 0.1, 1])$ \\
\midrule
\multirow{8}{*}{ALIM}  & warm up iteration & 178 & 178\\ 
& size of output representation & 128 & $\text{RandomChoice}([64,128,256])$\\ 
& size of the queue & 8192 & $\text{RandomChoice}([4096,8192])$\\ 
& model update momentum & 0.999 & $\text{RandomChoice}([0.9,0.999])$\\ 
&prototype calculation momentum & 0.99 & $\text{RandomChoice}([0.99,0.9])$\\ 
& weight of the contrastive loss & 0.5 & $\text{RandomChoice}([0.5,1.0])$\\ 
& starting epoch of denoising & 40 & $\text{RandomChoice}([20, 40, 80, 100, 140])$\\ 
& weight of the mixup loss & 1 & 1\\ 
\midrule
\multirow{12}{*}{PiCO+}       & warm up iteration & 250 & $\text{RandomChoice}([1,250])$\\ 
% & optimizer update momentum & 0.9 & $\text{RandomChoice}([0.8,0.9])$\\ 
& size of output representation & 128 & $\text{RandomChoice}([64,128,256])$\\ 
& size of the queue & 8192 & $\text{RandomChoice}([4096,8192])$\\ 
& model update momentum & 0.999 & $\text{RandomChoice}([0.9,0.999])$\\ 
&prototype calculation momentum & 0.99 & $\text{RandomChoice}([0.99,0.9])$\\ 
& weight of the contrastive loss & 0.5 & $\text{RandomChoice}([0.5,1.0])$\\ 
& selection ratio for clean sample & 1 & $\text{RandomChoice}([0.6,0.8,0.95,0.99,1])$\\
& start iteration of $k$NN augmentation & 5000 & $\text{RandomChoice}([2,100,5000])$\\
& number of nearest neighbors & 16 & $\text{RandomChoice}([8,16])$\\
& temperature of label guessing & 0.07 & $\text{RandomChoice}([0.1,0.07])$\\
& weight for the losses of unreliable examples & 0.1 & $\text{RandomChoice}([0.1,0.5])$ \\
& weight for the losses of mixup loss & 2 & $\text{RandomChoice}([2,3,5])$\\ 
\bottomrule
\end{tabular}}
}
\end{center}
\end{table}
\section{Details of Experimental Results}\label{apd:exp_res}
Table~\ref{res_lost} and Tables~\ref{table:msrcv2} to~\ref{res_english} show the experimental results on tabular datasets.

\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on MSRCv2 with different model selection criteria.}\label{table:msrcv2}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 52.50$\pm$0.83                                     & 53.41$\pm$1.11                                     & 51.82$\pm$0.65                                     & 52.39$\pm$1.16                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 46.14$\pm$0.84                                     & 45.00$\pm$1.26                                     & 48.64$\pm$1.56                                     & 46.14$\pm$1.63                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 51.36$\pm$0.92                                     & 52.05$\pm$0.73                                     & 52.61$\pm$1.10                                     & 54.09$\pm$1.01                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 45.68$\pm$1.82                                     & 44.09$\pm$1.80                                     & 46.48$\pm$1.91                                     & 48.18$\pm$1.47                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 48.30$\pm$1.80                                     & 46.93$\pm$1.37                                     & 49.55$\pm$1.31                                     & 50.34$\pm$1.11                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 44.77$\pm$0.73                                     & 45.57$\pm$0.94                                     & 46.14$\pm$1.36                                     & 45.57$\pm$1.26                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 44.43$\pm$1.16                                     & 44.77$\pm$1.19                                     & 47.16$\pm$0.28                                     & 46.93$\pm$1.10                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 45.68$\pm$1.33                                     & 33.64$\pm$2.08                                     & 43.75$\pm$1.27                                     & 45.80$\pm$1.79                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 52.16$\pm$1.32                                     & 52.27$\pm$1.51                                     & 52.27$\pm$1.60                                     & 53.07$\pm$1.25                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 47.50$\pm$0.80                                     & 46.59$\pm$0.53                                     & 49.77$\pm$1.35                                     & 49.32$\pm$1.03                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 52.61$\pm$0.81                                     & 50.45$\pm$2.07                                     & 52.05$\pm$1.14                                     & 52.84$\pm$1.56                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 41.82$\pm$1.33                                     & 43.98$\pm$2.02                                     & 44.09$\pm$0.73                                     & 44.89$\pm$1.24                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 49.89$\pm$1.71                                     & 51.59$\pm$1.64                                     & 51.14$\pm$0.53                                     & 50.57$\pm$1.80                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 18.86$\pm$1.79                                     & 16.82$\pm$1.33                                     & 16.70$\pm$0.83                                     & 20.23$\pm$1.14                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 12.95$\pm$1.01                                     & 13.52$\pm$0.67                                     & 10.91$\pm$0.74                                     & 14.55$\pm$1.22                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 48.86$\pm$1.41                                     & 49.89$\pm$1.20                                     & 48.98$\pm$1.70                                     & 49.32$\pm$1.67                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 51.25$\pm$1.02                                     & 51.70$\pm$1.41                                     & 49.32$\pm$1.11                                     & 51.36$\pm$1.24                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 44.77$\pm$1.63                                     & 41.93$\pm$0.74                                     & 45.34$\pm$0.90                                     & 44.55$\pm$0.89                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 49.89$\pm$0.98                                     & 27.50$\pm$9.01                                     & 51.59$\pm$1.55                                     & 51.93$\pm$1.42                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 48.30$\pm$1.43                                     & 49.09$\pm$1.29                                     & 48.41$\pm$0.93                                     & 49.89$\pm$0.81                                     \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on Mirflickr with different model selection criteria.}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 66.19$\pm$1.61                                     & 65.40$\pm$1.00                                     & 66.47$\pm$1.63                                     & 66.69$\pm$1.29                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 63.88$\pm$0.69                                     & 64.75$\pm$1.40                                     & 62.73$\pm$1.69                                     & 65.83$\pm$1.17                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 65.40$\pm$1.18                                     & 64.53$\pm$0.78                                     & 66.62$\pm$1.05                                     & 66.40$\pm$1.28                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 63.60$\pm$0.98                                     & 58.49$\pm$1.92                                     & 63.81$\pm$1.01                                     & 66.04$\pm$1.07                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 53.96$\pm$1.71                                     & 53.02$\pm$0.78                                     & 54.17$\pm$0.90                                     & 58.27$\pm$1.24                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 64.46$\pm$1.39                                     & 57.55$\pm$3.13                                     & 64.82$\pm$1.32                                     & 63.53$\pm$1.76                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 55.40$\pm$1.35                                     & 53.17$\pm$1.29                                     & 54.10$\pm$0.89                                     & 56.62$\pm$0.22                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 52.23$\pm$1.01                                     & 35.04$\pm$5.62                                     & 55.68$\pm$1.25                                     & 57.27$\pm$1.59                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 65.04$\pm$1.16                                     & 63.67$\pm$1.37                                     & 64.32$\pm$0.65                                     & 66.98$\pm$1.03                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 64.96$\pm$1.30                                     & 38.78$\pm$5.29                                     & 65.04$\pm$1.23                                     & 65.54$\pm$1.77                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 66.12$\pm$2.07                                     & 68.35$\pm$1.22                                     & 68.13$\pm$0.98                                     & 69.06$\pm$0.91                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 62.45$\pm$1.14                                     & 58.78$\pm$1.41                                     & 62.37$\pm$1.07                                     & 63.02$\pm$1.61                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 65.32$\pm$1.45                                     & 64.68$\pm$2.16                                     & 64.96$\pm$1.19                                     & 65.54$\pm$1.33                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 19.93$\pm$1.78                                     & 9.28$\pm$1.19                                      & 18.06$\pm$1.22                                     & 24.96$\pm$1.52                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 21.37$\pm$0.52                                     & 11.37$\pm$4.01                                     & 17.84$\pm$0.39                                     & 21.51$\pm$1.86                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 63.31$\pm$1.29                                     & 59.42$\pm$4.38                                     & 64.60$\pm$1.03                                     & 65.32$\pm$0.86                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 63.60$\pm$1.13                                     & 63.74$\pm$1.17                                     & 67.12$\pm$1.15                                     & 66.83$\pm$0.62                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 57.55$\pm$1.31                                     & 52.09$\pm$3.57                                     & 57.34$\pm$2.71                                     & 60.07$\pm$2.18                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 64.32$\pm$1.47                                     & 15.32$\pm$9.24                                     & 64.53$\pm$1.24                                     & 65.83$\pm$1.21                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 58.56$\pm$1.69                                     & 59.42$\pm$1.10                                     & 58.78$\pm$1.70                                     & 60.58$\pm$0.94                                     \\
\bottomrule
\end{tabular}
}
\end{table}
\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on Birdsong with different model selection criteria.}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 71.64$\pm$0.61                                     & 70.16$\pm$0.88                                     & 71.44$\pm$0.65                                     & 72.32$\pm$0.89                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 65.96$\pm$0.74                                     & 66.04$\pm$0.36                                     & 66.56$\pm$0.41                                     & 67.08$\pm$0.68                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 71.80$\pm$0.98                                     & 69.96$\pm$0.58                                     & 72.60$\pm$0.92                                     & 72.24$\pm$0.73                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 66.20$\pm$0.83                                     & 63.48$\pm$1.17                                     & 68.20$\pm$0.70                                     & 68.28$\pm$0.72                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 69.04$\pm$0.55                                     & 67.96$\pm$0.79                                     & 69.44$\pm$0.56                                     & 70.16$\pm$0.94                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 64.16$\pm$0.74                                     & 62.64$\pm$1.22                                     & 65.20$\pm$0.70                                     & 65.24$\pm$1.09                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 65.92$\pm$1.40                                     & 64.96$\pm$0.95                                     & 64.92$\pm$0.83                                     & 67.56$\pm$0.80                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 65.64$\pm$0.92                                     & 55.48$\pm$1.87                                     & 67.04$\pm$0.52                                     & 66.68$\pm$0.32                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 70.88$\pm$0.85                                     & 68.68$\pm$0.65                                     & 72.24$\pm$0.57                                     & 71.96$\pm$0.55                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 66.04$\pm$0.43                                     & 56.48$\pm$5.22                                     & 65.84$\pm$0.61                                     & 66.04$\pm$0.58                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 72.48$\pm$0.61                                     & 69.72$\pm$0.50                                     & 73.40$\pm$0.64                                     & 73.24$\pm$0.62                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 68.92$\pm$0.33                                     & 66.56$\pm$1.13                                     & 70.08$\pm$0.52                                     & 69.88$\pm$0.72                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 69.84$\pm$0.77                                     & 69.80$\pm$0.41                                     & 70.00$\pm$0.50                                     & 69.88$\pm$0.61                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 18.68$\pm$1.28                                     & 16.04$\pm$0.80                                     & 18.00$\pm$0.70                                     & 20.72$\pm$0.91                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 20.08$\pm$1.24                                     & 20.04$\pm$1.14                                     & 13.24$\pm$0.73                                     & 19.84$\pm$1.15                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 70.12$\pm$0.61                                     & 68.84$\pm$0.93                                     & 70.72$\pm$0.38                                     & 71.08$\pm$0.61                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 70.32$\pm$0.62                                     & 70.16$\pm$0.95                                     & 70.28$\pm$0.50                                     & 70.28$\pm$0.78                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 62.48$\pm$0.71                                     & 61.80$\pm$0.83                                     & 66.48$\pm$0.82                                     & 67.60$\pm$0.81                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 69.60$\pm$0.66                                     & 51.72$\pm$9.37                                     & 69.60$\pm$0.74                                     & 71.80$\pm$0.91                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 66.76$\pm$1.62                                     & 66.76$\pm$1.07                                     & 66.76$\pm$0.67                                     & 68.44$\pm$1.00                                     \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on Malagasy with different model selection criteria.}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 67.19$\pm$0.69                                     & 66.93$\pm$2.41                                     & 71.83$\pm$0.81                                     & 71.15$\pm$0.58                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 65.46$\pm$1.97                                     & 64.71$\pm$1.21                                     & 68.55$\pm$0.98                                     & 69.34$\pm$1.08                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 62.52$\pm$1.89                                     & 65.39$\pm$2.12                                     & 70.47$\pm$0.65                                     & 70.43$\pm$0.81                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 58.46$\pm$3.15                                     & 54.39$\pm$3.29                                     & 66.52$\pm$0.62                                     & 66.06$\pm$0.44                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 60.64$\pm$1.38                                     & 59.70$\pm$2.10                                     & 63.47$\pm$1.60                                     & 65.76$\pm$1.13                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      & 0.23$\pm$0.10                                      \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 57.82$\pm$0.78                                     & 60.53$\pm$2.32                                     & 70.66$\pm$0.66                                     & 71.41$\pm$1.00                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 57.74$\pm$1.33                                     & 45.50$\pm$4.37                                     & 63.54$\pm$1.56                                     & 65.08$\pm$1.02                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 69.94$\pm$0.70                                     & 67.50$\pm$1.45                                     & 70.06$\pm$0.62                                     & 70.70$\pm$0.83                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 67.01$\pm$1.36                                     & 62.52$\pm$1.38                                     & 69.15$\pm$1.08                                     & 69.38$\pm$0.82                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 61.51$\pm$2.55                                     & 62.79$\pm$2.60                                     & 69.72$\pm$0.44                                     & 69.38$\pm$0.79                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 19.89$\pm$1.75                                     & 14.43$\pm$2.04                                     & 20.34$\pm$1.00                                     & 25.54$\pm$0.75                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 18.83$\pm$1.51                                     & 19.47$\pm$1.68                                     & 9.04$\pm$0.65                                      & 19.55$\pm$1.62                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 61.73$\pm$2.40                                     & 57.48$\pm$1.38                                     & 69.76$\pm$0.36                                     & 69.72$\pm$0.87                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 57.36$\pm$0.69                                     & 61.77$\pm$2.34                                     & 68.66$\pm$1.05                                     & 68.89$\pm$0.62                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 57.33$\pm$1.74                                     & 58.38$\pm$2.46                                     & 64.71$\pm$1.11                                     & 66.70$\pm$1.03                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 58.95$\pm$0.77                                     & 0.64$\pm$0.18                                      & 70.21$\pm$0.39                                     & 69.34$\pm$0.74                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 65.76$\pm$1.23                                     & 60.23$\pm$2.71                                     & 68.78$\pm$0.75                                     & 68.63$\pm$0.64                                     \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on Soccer Player with different model selection criteria.}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 54.98$\pm$0.41                                     & 54.65$\pm$0.55                                     & 55.02$\pm$0.52                                     & 55.38$\pm$0.49                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 51.89$\pm$1.08                                     & 51.52$\pm$1.09                                     & 51.26$\pm$1.04                                     & 51.67$\pm$1.02                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 54.74$\pm$0.55                                     & 54.57$\pm$0.62                                     & 54.82$\pm$0.56                                     & 55.02$\pm$0.40                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 48.90$\pm$0.62                                     & 48.75$\pm$0.54                                     & 48.90$\pm$0.62                                     & 48.90$\pm$0.62                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 55.33$\pm$0.48                                     & 48.95$\pm$0.55                                     & 55.70$\pm$0.68                                     & 55.90$\pm$0.67                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 49.05$\pm$0.55                                     & 48.78$\pm$0.54                                     & 49.04$\pm$0.52                                     & 48.92$\pm$0.50                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 53.64$\pm$0.57                                     & 49.51$\pm$0.53                                     & 53.56$\pm$0.62                                     & 53.40$\pm$0.57                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 52.22$\pm$0.55                                     & 51.41$\pm$0.28                                     & 53.03$\pm$0.59                                     & 53.65$\pm$0.53                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 54.15$\pm$0.64                                     & 54.86$\pm$0.33                                     & 54.93$\pm$0.38                                     & 54.71$\pm$0.42                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 52.55$\pm$0.48                                     & 48.99$\pm$0.51                                     & 52.60$\pm$0.45                                     & 52.69$\pm$0.50                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 54.39$\pm$0.71                                     & 54.16$\pm$0.58                                     & 54.97$\pm$0.42                                     & 55.13$\pm$0.68                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 54.85$\pm$0.76                                     & 48.75$\pm$0.54                                     & 54.70$\pm$0.72                                     & 54.45$\pm$0.64                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 50.35$\pm$0.93                                     & 49.93$\pm$1.01                                     & 50.57$\pm$0.86                                     & 50.47$\pm$0.95                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 11.10$\pm$0.38                                     & 3.04$\pm$0.29                                      & 9.97$\pm$0.35                                      & 11.10$\pm$0.38                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 6.06$\pm$0.46                                      & 4.95$\pm$1.17                                      & 5.06$\pm$0.24                                      & 6.02$\pm$0.50                                      \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 49.32$\pm$0.87                                     & 49.35$\pm$0.87                                     & 49.51$\pm$0.89                                     & 49.42$\pm$0.84                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 50.57$\pm$0.83                                     & 49.98$\pm$1.06                                     & 50.58$\pm$0.91                                     & 50.55$\pm$0.95                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 49.41$\pm$0.69                                     & 48.84$\pm$0.59                                     & 49.60$\pm$0.60                                     & 49.83$\pm$0.66                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 50.77$\pm$0.48                                     & 28.54$\pm$10.31                                    & 51.12$\pm$0.61                                     & 50.93$\pm$0.51                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 52.13$\pm$0.54                                     & 51.97$\pm$0.52                                     & 52.15$\pm$0.54                                     & 52.30$\pm$0.59                                     \\
\bottomrule
\end{tabular}}
\end{table}
\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on Italian with different model selection criteria.}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 68.89$\pm$0.44                                     & 69.32$\pm$0.60                                     & 71.80$\pm$0.95                                     & 72.98$\pm$0.66                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 68.83$\pm$0.96                                     & 68.12$\pm$0.72                                     & 70.71$\pm$0.45                                     & 70.62$\pm$0.60                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 68.48$\pm$0.67                                     & 70.19$\pm$0.36                                     & 71.56$\pm$1.03                                     & 71.78$\pm$0.91                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 67.60$\pm$0.32                                     & 66.92$\pm$0.84                                     & 70.11$\pm$1.31                                     & 70.07$\pm$1.26                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 66.76$\pm$0.64                                     & 65.36$\pm$0.68                                     & 69.60$\pm$0.57                                     & 70.90$\pm$0.54                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 67.58$\pm$0.64                                     & 67.28$\pm$0.61                                     & 71.43$\pm$0.74                                     & 71.64$\pm$0.46                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 66.22$\pm$0.72                                     & 66.03$\pm$0.75                                     & 69.64$\pm$0.41                                     & 70.14$\pm$0.31                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 66.48$\pm$0.16                                     & 65.86$\pm$1.12                                     & 68.29$\pm$0.52                                     & 69.16$\pm$0.47                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 65.96$\pm$1.03                                     & 66.89$\pm$1.11                                     & 71.40$\pm$0.32                                     & 72.09$\pm$0.54                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 71.65$\pm$0.45                                     & 58.70$\pm$5.74                                     & 73.36$\pm$0.50                                     & 73.68$\pm$0.73                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 68.43$\pm$0.50                                     & 67.43$\pm$0.82                                     & 70.08$\pm$0.19                                     & 70.73$\pm$0.42                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 70.10$\pm$0.69                                     & 67.41$\pm$1.23                                     & 71.22$\pm$0.33                                     & 71.73$\pm$0.42                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 68.28$\pm$1.29                                     & 68.28$\pm$1.29                                     & 71.20$\pm$0.86                                     & 71.63$\pm$0.66                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 32.92$\pm$1.92                                     & 17.62$\pm$5.35                                     & 23.78$\pm$0.57                                     & 33.75$\pm$1.28                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 11.62$\pm$1.61                                     & 11.98$\pm$1.43                                     & 5.88$\pm$0.25                                      & 11.98$\pm$1.43                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 67.34$\pm$0.31                                     & 67.34$\pm$0.31                                     & 69.03$\pm$0.54                                     & 69.08$\pm$0.69                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 67.59$\pm$0.33                                     & 67.59$\pm$0.33                                     & 68.82$\pm$0.69                                     & 70.33$\pm$1.22                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 67.43$\pm$0.45                                     & 68.78$\pm$0.43                                     & 68.90$\pm$0.20                                     & 69.56$\pm$0.33                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 65.70$\pm$1.10                                     & 1.11$\pm$0.38                                      & 71.04$\pm$1.06                                     & 71.30$\pm$1.04                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 67.97$\pm$0.86                                     & 67.29$\pm$0.42                                     & 69.31$\pm$0.64                                     & 71.12$\pm$0.80                                     \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on Yahoo! News with different model selection criteria.}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 66.83$\pm$0.52                                     & 66.29$\pm$0.54                                     & 66.71$\pm$0.58                                     & 66.81$\pm$0.51                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 57.77$\pm$1.53                                     & 57.14$\pm$1.64                                     & 57.30$\pm$1.55                                     & 58.30$\pm$1.57                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 66.42$\pm$0.74                                     & 65.76$\pm$0.46                                     & 66.02$\pm$0.57                                     & 66.17$\pm$0.64                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 55.70$\pm$1.53                                     & 48.53$\pm$3.62                                     & 56.11$\pm$1.63                                     & 56.15$\pm$1.61                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 58.03$\pm$0.43                                     & 57.79$\pm$0.43                                     & 58.54$\pm$0.95                                     & 58.48$\pm$0.36                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 50.37$\pm$2.06                                     & 47.01$\pm$3.14                                     & 50.13$\pm$2.11                                     & 50.44$\pm$2.05                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 53.91$\pm$0.34                                     & 53.91$\pm$0.34                                     & 53.99$\pm$0.50                                     & 54.13$\pm$0.34                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 54.49$\pm$0.64                                     & 50.46$\pm$1.07                                     & 52.12$\pm$0.70                                     & 54.74$\pm$0.70                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 67.02$\pm$0.32                                     & 65.43$\pm$0.56                                     & 66.49$\pm$0.45                                     & 66.69$\pm$0.40                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 67.64$\pm$0.73                                     & 39.87$\pm$9.64                                     & 67.18$\pm$0.55                                     & 67.46$\pm$0.32                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 66.10$\pm$0.63                                     & 65.66$\pm$0.69                                     & 62.55$\pm$0.75                                     & 65.99$\pm$0.31                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 58.19$\pm$0.41                                     & 54.07$\pm$0.79                                     & 58.22$\pm$0.46                                     & 58.17$\pm$0.39                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 50.70$\pm$1.25                                     & 50.56$\pm$1.33                                     & 50.62$\pm$1.40                                     & 50.70$\pm$1.25                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 23.80$\pm$0.24                                     & 1.77$\pm$0.16                                      & 23.23$\pm$0.27                                     & 24.19$\pm$0.20                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 13.30$\pm$0.31                                     & 13.30$\pm$0.31                                     & 12.91$\pm$0.57                                     & 13.30$\pm$0.31                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 50.30$\pm$1.26                                     & 50.35$\pm$1.23                                     & 50.57$\pm$1.28                                     & 50.70$\pm$1.21                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 50.59$\pm$1.22                                     & 50.50$\pm$1.19                                     & 50.76$\pm$1.01                                     & 50.85$\pm$1.16                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 43.95$\pm$0.67                                     & 42.98$\pm$0.66                                     & 42.71$\pm$0.62                                     & 44.60$\pm$0.70                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 57.37$\pm$0.96                                     & 45.21$\pm$9.93                                     & 57.35$\pm$1.05                                     & 57.49$\pm$0.92                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 56.97$\pm$0.44                                     & 56.98$\pm$0.45                                     & 56.02$\pm$0.62                                     & 57.82$\pm$0.50                                     \\         
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[ht]
\centering
%\footnotesize
%\scriptsize
\caption{Classification accuracy (mean$\pm$std) of each algorithm on English with different model selection criteria.}\label{res_english}
%\setlength{\tabcolsep}{10pt}
%\tabcolsep 0.09in
%\renewcommand\arraystretch{1}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\multicolumn{1}{c}{\textbf{Algorithm}} & \multicolumn{1}{c}{\textbf{Venue}} & {\textbf{w/ CR}} & {\textbf{w/ AA}} &{\textbf{w/ OA}} & {\textbf{w/ OA \& ES}}  \\
\midrule
PRODEN&ICML 2020~\citep{lv2020progressive}         & 74.01$\pm$0.32                                     & 73.78$\pm$0.13                                     & 73.62$\pm$0.15                                     & 73.70$\pm$0.26                                     \\
CAVL&ICLR 2022~\citep{zhang20222exploiting}        & 73.71$\pm$0.35                                     & 73.58$\pm$0.35                                     & 74.15$\pm$0.50                                     & 74.19$\pm$0.25                                     \\
POP&ICML 2023~\citep{xu2023progressive}            & 74.04$\pm$0.32                                     & 73.99$\pm$0.17                                     & 73.78$\pm$0.31                                     & 74.44$\pm$0.36                                     \\
ABS-MAE&TPAMI 2024~\citep{lv2024on}                & 73.07$\pm$0.22                                     & 72.37$\pm$0.44                                     & 73.20$\pm$0.13                                     & 73.31$\pm$0.28                                     \\
ABS-GCE&TPAMI 2024~\citep{lv2024on}                & 73.52$\pm$0.30                                     & 73.12$\pm$0.26                                     & 74.47$\pm$0.16                                     & 74.57$\pm$0.23                                     \\
EXP&ICML 2020~\citep{feng2020learning}             & 73.18$\pm$0.23                                     & 72.73$\pm$0.24                                     & 73.75$\pm$0.27                                     & 73.61$\pm$0.33                                     \\
MCL-GCE&ICML 2020~\citep{feng2020learning}         & 73.52$\pm$0.24                                     & 73.52$\pm$0.51                                     & 74.09$\pm$0.26                                     & 74.35$\pm$0.48                                     \\
MCL-MSE&ICML 2020~\citep{feng2020learning}         & 73.70$\pm$0.24                                     & 73.37$\pm$0.48                                     & 74.22$\pm$0.39                                     & 73.96$\pm$0.24                                     \\
CC&NeurIPS 2020~\citep{feng2020provably}           & 73.83$\pm$0.25                                     & 73.77$\pm$0.29                                     & 73.62$\pm$0.08                                     & 73.87$\pm$0.19                                     \\
LWS&ICML 2021~\citep{wen2021leveraged}             & 73.31$\pm$0.34                                     & 64.11$\pm$3.99                                     & 73.98$\pm$0.38                                     & 73.91$\pm$0.48                                     \\
IDGP&ICLR 2023~\citep{qiao2023decompositional}     & 74.01$\pm$0.26                                     & 73.82$\pm$0.24                                     & 73.90$\pm$0.35                                     & 74.44$\pm$0.23                                     \\
PC&NeurIPS 2017~\citep{ishida2017learning}         & 73.47$\pm$0.29                                     & 72.82$\pm$0.30                                     & 73.38$\pm$0.45                                     & 73.74$\pm$0.35                                     \\
Forward&ECCV 2018~\citep{yu2018learning}           & 73.40$\pm$0.21                                     & 73.60$\pm$0.37                                     & 73.88$\pm$0.24                                     & 73.40$\pm$0.48                                     \\
NN&ICML 2019~\citep{ishida2019complementary}       & 54.18$\pm$0.89                                     & 53.82$\pm$0.76                                     & 39.25$\pm$1.43                                     & 54.18$\pm$0.89                                     \\
GA&ICML 2019~\citep{ishida2019complementary}       & 34.78$\pm$1.90                                     & 34.77$\pm$1.91                                     & 23.19$\pm$0.84                                     & 34.77$\pm$1.91                                     \\
SCL-EXP&ICML 2020~\citep{chou2020unbiased}         & 73.84$\pm$0.24                                     & 73.44$\pm$0.40                                     & 74.08$\pm$0.26                                     & 73.84$\pm$0.25                                     \\
SCL-NL&ICML 2020~\citep{chou2020unbiased}          & 74.01$\pm$0.22                                     & 73.78$\pm$0.44                                     & 74.04$\pm$0.31                                     & 74.17$\pm$0.24                                     \\
L-W&ICML 2021~\citep{gao2021discriminative}        & 73.84$\pm$0.44                                     & 73.13$\pm$0.54                                     & 74.02$\pm$0.39                                     & 74.14$\pm$0.29                                     \\
OP-W&AISTATS 2023~\citep{liu2023consistent}        & 73.88$\pm$0.39                                     & 31.83$\pm$15.01                                    & 73.97$\pm$0.35                                     & 74.05$\pm$0.36                                     \\
VALEN&NeurIPS 2021~\citep{xu2021instance}          & 73.20$\pm$0.41                                     & 72.86$\pm$0.40                                     & 73.39$\pm$0.23                                     & 73.77$\pm$0.24                                     \\
\bottomrule
\end{tabular}}
\end{table}
\end{document}
