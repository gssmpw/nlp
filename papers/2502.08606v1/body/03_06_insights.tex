\FloatBarrier
\section{Distilling language models in practice}
\label{sec:distilling-language-models-in-practice}

In the following analyses, we explore the sensitivity of student performance under modification of distillation hyperparameters.
We demonstrate that the pure distillation setting 
($\lambda=1$, \Cref{ssec:lambda-sensitivity}), 
unit temperature ($\tau=1$, \Cref{ssec:temperature-tau-sensitivity}), 
and learning rate $\eta=0.01$ (\Cref{ssec:lr-sensitivity}) under 
$\mu$P \citep{DBLP:conf/icml/YangH21,DBLP:journals/corr/abs-2308-01814,DBLP:journals/corr/abs-2203-03466,DBLP:journals/corr/abs-2309-14322,DBLP:journals/corr/abs-2310-17813} provides robust performance across model scales, while distribution truncation methods (Top-$k$, Top-$p$) degrade performance \emph{unless combined with ground-truth next-token prediction} (\Cref{ssec:top-k-top-p-sensitivity}). 
Finally, we verify that forward KL divergence distillation, $D_{\text{KL}}(\hat{p}_T || \hat{q}_S)$, consistently outperforms reverse KL (\Cref{ssec:forward-vs-backward}).

For ease of reference, we restate the components of the token-level loss for the student:
\begin{align}
	\Ls_{\text{NTP}}(x^{(i)},\vz^{(i)}) & =
	-\sum_{a=1}^V \ve(x^{(i)})_a\log \sigma_a(\vz^{(i)}),
	\label{eq:ntp-cross-entropy-app} & \textrm{(Next-token prediction)}\\
        \Ls_Z(\vz^{(i)})
        &=||\log Z(\vz^{(i)})||_2^2
        =\left|\left|\log \sum_{a=1}^V\exp(z_a^{(i)})\right|\right|_2^2   , & \textrm{(Z-loss)}\\
        \Ls_{\text{KD}}(\vz_T^{(i)},\vz_S^{(i)}) & =
        -\tau^2
        \sum_{a=1}^V \sigma_a
        \left(\frac{\vz_T^{(i)}}{\tau}\right)
        \log \sigma_a\left(\frac{\vz_S^{(i)}}{\tau}\right),  & \textrm{(Distillation loss)} \label{eq:distillation-loss-app}\\
        \Ls_{S}  (x^{(i)},\vz_T^{(i)},\vz_S^{(i)})&=
        (1-\lambda)\,\Ls_{\text{NTP}}(x^{(i)},\vz_S^{(i)})+\lambda\,\Ls_{\text{KD}}(\vz_T^{(i)},\vz_S^{(i)}) +\lambda_Z\,\Ls_{Z}(\vz_S^{(i)}). & \textrm{(Student loss)}
                                           \label{eqn:kd_and_nll_full_loss_app}        
\end{align}
See \Cref{sec:background} for a discussion of each of the terms.

\subsection{Mixing coefficient (\texorpdfstring{$\lambda$}{lambda}) sensitivity analysis}
\label{ssec:lambda-sensitivity}

The distillation process combines two loss components: knowledge transfer from the teacher, $\lambda \Ls_{\text{KD}}(\vz_T^{(i)},\vz_S^{(i)})$, and direct learning from data, $(1-\lambda) \Ls_{\text{NTP}}(x^{(i)},\vz_S^{(i)})$, weighted by the mixing coefficient $\lambda$ (\Cref{eqn:kd_and_nll_full_loss}). Our distillation scaling law analysis
is performed in the \emph{pure distillation} setting ($\lambda=1$).
Here we show this simple choice provides robust performance across a wide range of configurations.

\begin{figure}[h]
	\centering
	\subfloat[Mixing Coefficient $\bm \lambda$ Sensitivity.]{
		\includegraphics[width=0.45\textwidth]{plots/sensitivity_analysis_lambda.pdf}
		\label{fig:sensitivity-analysis-lambda}
	}
	\subfloat[Optimal Mixing Coefficients $\bm \lambda^*$]{
		\includegraphics[width=0.32\textwidth]{plots/sensitivity_analysis_lambda_optimal.pdf}
		\label{fig:sensitivity-analysis-lambda-optimal}
	}
	\caption{\textbf{Mixing Coefficients $\bm \lambda$.}
		\textbf{(a)} Students of six sizes $N_S\in\{198M,266M,\ldots,2.72B\}$ trained with a $M=D_S/N_S=20$ ratio are distilled from teachers of size sizes $N_T\in\{546M, 975M,\ldots,7.75B\}$ trained with a $M=D_T/N_T=20$ ratio with different values of loss mixing coefficient $\lambda\in[0,1]$. $\lambda=0$ and $\lambda=1$ correspond to supervised training and pure distillation cases respectively.
		\textbf{(b)} The mixing coefficients $\lambda^*=\argmin_\lambda \Ls(\lambda)$ that give the lowest student validation loss for each teacher-student combination shown in \Cref{fig:sensitivity-analysis-lambda}.}
	\label{fig:mixing-sensitivity}
\end{figure}

We examine various $\lambda$ values across different teacher-student configurations in \Cref{fig:sensitivity-analysis-lambda} and find that while the optimal mixing coefficients $\lambda^*$ vary based on the specific teacher-student combinations (\Cref{fig:sensitivity-analysis-lambda-optimal}),
the student cross-entropy $L_S$
remains mostly flat for choices of $\lambda > 0.5$,
with lower values of $\lambda$
only preferred in the cases where the teacher is particularly weak and where the supervised signal is more informative.
From \Cref{fig:sensitivity-analysis-lambda} it is also possible to get a sense of when distillation $\lambda > 0$ generally outperforms supervised learning $\lambda=0$ under the same token budget.\todo{Maybe add horizontal lines to indicate supervised performance.}

To guide practitioners, \Cref{fig:sensitivity-analysis-lambda-optimal} shows empirically derived optimal mixing coefficients, $\lambda^*$, though the simplicity and robustness of pure distillation makes it a reliable default choice for practical use and study.



\subsection{Temperature (\texorpdfstring{$\tau$}{tau}) sensitivity analysis}
\label{ssec:temperature-tau-sensitivity}
In distillation, the temperature $\tau$ controls the entropy of teacher predictions by scaling logits 
$\vz_T^{(i)}/\tau$ and $\vz_S^{(i)}/\tau$ in the knowledge distillation loss $\Ls_{\text{KD}}$ (\Cref{eqn:kd_and_nll_full_loss,eq:distillation-loss-app}). 
This scaling modulates the transfer of
\emph{dark knowledge} \citep{DBLP:journals/corr/HintonVD15} -- the log-probability ratios between incorrect categories encode the teacher's understanding of relationships between those categories.
Our analysis across $\tau \in [0.5,10]$ (\Cref{fig:sensitivity-analysis-temperature}) reveals that higher temperatures ($\tau > 3$) reduces performance by attenuating these ratios in $\sigma_a(\vz_T^{(i)}/\tau)$, particularly harming smaller students that rely heavily on this signal. Lower temperatures ($\tau < 1$) similarly reduce effectiveness by concentrating probability mass on argmax tokens, diminishing the transfer of relationships between lower-ranked predictions. 

We find optimal performance at $\tau=1$ across all model scales, suggesting this temperature best preserves log-probability structure.
Unlike the original distillation setting, 
which relied on dark knowledge to represents hierarchical relationships between incorrect classification predictions
in the presence of a \emph{true label},
language modeling is inherently ambiguous and complex, with many valid continuations.
\emph{It is precisely the understanding of the ambiguity of language we want to transfer to the student}, which is supported by our finding that maintaining the teacher's original probability ratios ($\tau=1$) produces the lowest student cross-entropies.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{plots/sensitivity_analysis_temperature.pdf}
	\caption{\textbf{Temperature $\bm \tau$ Sensitivity Analysis.} Students of four sizes $N_S\in\{198M,546M,975M,1.82B\}$ trained with a $M=D_S/N_S=20$ ratio are distilled from teachers of sizes $N_T\in\{546M, 1.82B,4.82B,7.75B\}$ trained with a $M=D_T/N_T=20$ ratio with different distillation temperatures $\tau\in[0.5,10]$.
	}
	\label{fig:sensitivity-analysis-temperature}
\end{figure}


\FloatBarrier

\subsection{Learning rate (\texorpdfstring{$\eta$}{eta}) sensitivity analysis, verification of \texorpdfstring{$\mu$}{mu}P for distillation} \label{ssec:lr-sensitivity}
The peak learning rate $\eta$ determines the scale of student parameter updates in distillation.
In our experiments we use a simplified version of
\gls{mup}~\citep{DBLP:conf/icml/YangH21,DBLP:journals/corr/abs-2308-01814,DBLP:journals/corr/abs-2203-03466,DBLP:journals/corr/abs-2309-14322,DBLP:journals/corr/abs-2310-17813},
described as \gls{mup} (simple) in~\cite{DBLP:conf/iclr/WortsmanLXEAACG24}.

In the supervised case,
in addition to improving the performance lower bound compared to the standard parameterization,
\gls{mup} simplifies experimental settings as it enables \emph{hyperparameter transfer}; the optimal peak learning rate $\eta$ and initialization scales found for a reference model size can be reused when changing model size\footnote{\gls{mup} only guarantees learning rate optimality when varying widths.
Empirically, the learning rate is also stable when changing the model depth within a reasonable range \citep{DBLP:journals/corr/abs-2203-03466}.
To \emph{guarantee} transfer \emph{across model depths} one can additionally employ depth-\gls{mup} \citep{DBLP:conf/iclr/YangYZH24},
although we do not use depth-\gls{mup} here.
}.

Here we validate that the optimal peak learning rate $\eta^*=0.01$
determined in the supervised case transfers to the distillation setting.
Sweeping values $\eta \in [0.001, 0.1]$ (\Cref{fig:sensitivity-analysis-learning-rate}) reveals that \gls{mup} achieves optimal performance at $\eta = 0.01$ uniformly across all configurations, from $198M$ to $1.82B$ parameter students and $546M$ to $7.75B$ parameter teachers,
consistent with the optimal peak learning rate in the supervised setting.

Performance varies smoothly and modestly around this optimum, with cross-entropy changing by less than 0.1 nats over one order of magnitude in learning rate.
This consistency validates $\mu$P's guarantee of scale-invariant training dynamics for distillation, confirming that our experimental setting for determining our distillation scaling law operates at the optimal learning rate or sufficiently close to it in all of our settings.
The observed moderate learning sensitivity in distillation partially alleviates the requirement for careful learning rate tuning,
showing that in practice the reference learning rate found in the supervised setting can be safely reused in the distillation setting.
\begin{figure}[h]
	\centering
        \vspace{-0.12cm}
	\includegraphics[width=0.45\textwidth]{plots/sensitivity_analysis_learning_rate.pdf}
        \vspace{-0.12cm}
	\caption{\textbf{Learning Rate $\bm \eta$ Sensitivity Analysis.} Students of four sizes $N_S\in\{198M,546M,975M,1.82B\}$ trained with a $M=D_S/N_S=20$ ratio are distilled from teachers of sizes $N_T\in\{546M, 1.82B,4.82B,7.75B\}$ trained with a $M=D_T/N_T=20$ ratio with different learning rates $\eta\in[0.001,0.1]$.}
        \vspace{-0.15cm}
	\label{fig:sensitivity-analysis-learning-rate}
\end{figure}

\FloatBarrier


\subsection{Distribution truncation methods: Top-\texorpdfstring{$k$}{k} and Top-\texorpdfstring{$p$}{p} sensitivity}
\label{ssec:top-k-top-p-sensitivity}

We investigate how the truncation of the teacher distributions affects student performance.
For these methods,
when the teacher produces a distribution $\hat p_T(x^{(i)}=a|\vx^{(<i)})$, $a\in\{1,\ldots,V\}$ over the vocabulary for the student to match,
only some entries in the distribution are used.
This is done primarily to reduce repeated inference and storage costs in the case teacher outputs are being stored for re-use in the multiple distillations scenario discussed in
\Cref{ssec:compute-optimal-distillation}.
In our case, the vocabulary size $V=32168$,
so assuming storage in \texttt{float32}, means each token requires $32168\times 4 \,\mathrm{bytes}\approx 129\mathrm{KB}$,
and storing all of C4 (approximately 2T tokens)
would take approximately 260 Petabytes,
a significant amount of data, roughly the total amount collected during the first
ten years of the \gls{lhc} \citep{CERNDataCentre2018}.

Given a truncation method $\gM$, can a \emph{truncated} teacher output $\hat p_T^{(\gM)}$
can be stored whilst still achieving the gains of distillation? 
Concretely, the truncation $p^{(\gM)}(x|c)$ of a distribution $p(x|c)$
with a truncation method $\gM$ is
{
		\medmuskip=1.5mu
		\thinmuskip=1.5mu
		\thickmuskip=1.5mu
\begin{align}\label{eq:truncation}
    p^{(\gM)}(x=a|c)
    &= \begin{cases}
        \frac{p(x=a|c)}
        {\sum\limits_{b\in \mathcal{S}_\mathcal{M}}
             p(x=b|c)},
         & a \in \mathcal{S}_\mathcal{M}(p(\,\cdot\,|c)), \\[6pt]
        0,
         & \text{otherwise},
    \end{cases}
\end{align}
}where $\mathcal{S}_\mathcal{M}(p(\,\cdot\,|c))$ represents the set of retained categories (i.e. non-zero probabilities)
in the truncated distribution,
which then undergoes renormalization over the retained categories.

We explore two complementary approaches: Top-$k$ and Top-$p$ (nucleus) sampling.
As in all of our settings, we evaluate the student cross-entropy against the data distribution with all categories, as this is the model property we are most interested in (a model can trivially match the target distribution if all categories except one are removed).
\begin{figure}[h]
	\centering
        \vspace{-0.12cm}
        \includegraphics[width=0.47\textwidth]{plots/combined_topk_and_topp.pdf}
        \vspace{-0.12cm}
        \caption{\textbf{Distribution truncation analysis.} Top-$k$ (left) and Top-$p$ (right) truncation of teacher logits $\vz_T^{(i)}$ for student-teacher pairs with $N_S$ in $\{198\text{M},546\text{M},1.82\text{B}\}$ and corresponding $N_T$ in $\{7.75\text{B},1.82\text{B},546\text{M}\}$. Standard truncation degrades performance: at $k=128$, validation loss increases by 0.11 nats compared to full distillation ($k=32768$), while Top-$p$ with $p=0.9$ degrades by 0.13 nats versus $p=1.0$. Using $\lambda=0.7$ with $k=128$ maintains performance within 0.01 nats while enabling efficient post-hoc training.}
        \vspace{-0.12cm}
	\label{fig:top-k-and-top-p}
\end{figure}

For Top-$k$,
we \emph{zero-out} all but the largest $k$
probabilities,
and Top-$p$,
we \emph{zero-out} all but the smallest set of
probabilities that sum to at least $p$.
The set defintions $\mathcal{S}_\mathcal{M}$ for Top-$k$ and Top-$p$ are
\begin{align}
    \mathcal{S}_k(\hat p) &= \mathrm{Top}(\hat p,\,k),&
    \mathcal{S}_p(\hat p) &= \{a : \sum\limits_{b \in \mathrm{sort}{\downarrow}(\hat p, a)} \hat p \leq p\}.
    \vspace{-0.1cm}
\end{align}
As the truncation parameters increase ($k \rightarrow V$ or $p \rightarrow 1$), both methods approach the full teacher distribution, and the student's cross-entropy converges to the baseline using the entire $\hat p_T$. 
Conversely, aggressive truncation (small $k$ or $p$) induces quantization that preserves only high-probability tokens while discarding information in the tail of the distribution.

Our empirical analysis (\Cref{fig:top-k-and-top-p}) reveals that both truncation methods directly correlate with reduced evaluation likelihoods. However, this performance degradation can be effectively mitigated through a combination of truncated distributions and ground truth next-token prediction using a mixing coefficient $\lambda \in (0,1)$ (\Cref{eqn:kd_and_nll_full_loss}). Specifically, with $k=128$ and $\lambda=0.7$, we achieve validation losses statistically indistinguishable from those obtained using the complete teacher distribution. For large-scale distillation scenarios where maintaining multiple models in memory is prohibitive, particularly with large teacher models, storing only the Top-$k$ teacher predictions (with $\lambda > 0$) enables efficient post-hoc distillation.








\FloatBarrier

\subsection{Forward and reverse KL divergence}
\label{ssec:forward-vs-backward}
We investigate both forward (mode spreading) and reverse (mode seeking) Kullback-Leibler divergences for distillation from $N_T=1.82$B to $N_S=546$M. The forward KLD $D_{\text{KL}}(\hat{p}_T || \hat{q}_S)$ (\Cref{eqn:kd_and_nll_full_loss}), minimizes $\mathcal{L}_{\text{forward}} = H(\hat{p}_T, \hat{q}_S) - H(\hat{p}_T)$,
where $H(\hat{p}_T)$ is dropped during optimization as it depends on only fixed teacher parameters. In contrast, the reverse KLD $D_{\text{KL}}(\hat{q}_S || \hat{p}_T)$ requires explicitly computing the student's entropy, $\mathcal{L}_{\text{reverse}} = H(\hat{q}_S, \hat{p}_T) - H(\hat{q}_S)$.

The forward KL achieves a lower data cross-entropy compared to the reverse KL (\Cref{tab:kl-comparison}), with an average improvement of 0.28 nats. This suggests that explicitly regularizing with respect to the student's entropy during training may not provide additional benefits for distillation quality. Given both the improved performance and reduced computational overhead of forward KL (which avoids computing student entropy), we recommend using standard forward KL for distillation.
\begin{table}[h]
	\centering
        \vspace{-0.1cm}
	\caption{Forward vs Reverse KL Divergence for $N_T=1.82$B to $N_S=546$M distillation. Reverse KL is slightly more expensive with respect to vocabulary size $V$ due to the entropy calculation.}
    \small
	\label{tab:kl-comparison}
	\begin{tabular}{lcc}
		\toprule
		Method     & Cross-Entropy & Computational Cost \\
		\midrule
		Forward KL & 2.42                & $\mathcal{O}(V)$   \\
		Reverse KL & 2.70                & $\mathcal{O}(2V)$  \\
		\bottomrule
	\end{tabular}
    \vspace{-0.15cm}
\end{table}



