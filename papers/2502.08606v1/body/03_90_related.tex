\section{Extended background}
\label{sec:extended-background}

This section reviews related work on knowledge distillation, capacity gap phenomena, neural scaling laws, and foundation models, highlighting their relevance to our study.

\subsection{Knowledge Distillation}
\label{ssec:knowledge-distillation}
\citet{DBLP:conf/kdd/BucilaCN06} provided strong evidence that the knowledge gained by a large ensemble of models can be effectively transferred to a single smaller model. Later, \citet{DBLP:journals/corr/HintonVD15} introduced knowledge distillation, where a smaller \emph{student} network learns from a larger \emph{teacher} network by mimicking its softened output probabilities, improving efficiency and generalization.
Building on this, \citet{DBLP:conf/nips/StantonIKAW21} studied both fidelity and student generalization, showing that while knowledge distillation often improves generalization, it frequently fails to achieve high fidelity, as student models do not fully match the teacher's predictive distribution. We study fidelity in terms of calibration in \Cref{ssec:model-calibration}, and show that when the learning signal is consistent with the calibration measure, then the student in our setup is well-calibrated both with respect to the teacher and the actual data. Addressing this, \citet{DBLP:conf/cvpr/BeyerZRMA022} demonstrated that knowledge distillation is most effective when the teacher is patient and consistent, providing stable targets over prolonged training to improve student generalization and fidelity. Our \gls{lm} setup automatically satisfies \emph{consistency}: both the teacher and student see the same data during the student's training.
However, our conclusions differ from those of \citet{DBLP:conf/cvpr/BeyerZRMA022} in that although distilling a student for longer does improve its performance, unless the teacher is chosen perfectly, distillation becomes less effective than supervised learning in the \emph{patient} setting, 
see \Cref{ssec:fixed-tokens-or-compute-best-case-app}
for a discussion.
Beyond empirical insights, \citet{DBLP:journals/corr/abs-2005-10419} established a bias-variance tradeoff for the student, quantifying how access to teacher logits can significantly enhance learning. Meanwhile, \citet{DBLP:journals/corr/abs-2407-04600} investigated self-distillation, where the student and teacher share the same architecture and size, to assess the potential gains from repeatedly applying knowledge distillation.
While most studies assume the teacher is a larger model, recent work explores weak-to-strong generalization, where a weaker model distills knowledge into a stronger one. This concept, introduced by \citet{DBLP:conf/icml/BurnsIKBGACEJLS24} and studied in \glspl{lm}, was further analyzed by \citet{DBLP:journals/corr/abs-2410-18837}, who extended the theoretical analysis to high-dimensional data and over-parameterized regression. Their findings show that distillation can provably outperform training with strong labels under the same data budget but does not improve the data scaling law. 
Our distillation scaling law (\Cref{eq:distillation-scaling-law}) confirms this finding, which for a fixed teacher cross-entropy does not improve the scaling law compared to the supervised one in \Cref{eq:supervised-scaling-law}. Moreover, in many previous works,  distillation happens with repeated data, that is, the student sees the same data as the teacher does during its training. 
In our setup, we do not repeat the data between teacher training and distillation, which allows us to examine only the effect of distillation rather than the possible diminishing returns of repeated data; see \citet{DBLP:journals/corr/abs-2305-16264} for more details on the effect of repeating data.\todo{Discuss SeqKD}



\subsection{Neural Scaling Laws}
\label{ssec:neural-scaling-laws}
Predictable scaling trends in neural networks were first empirically observed by \citet{DBLP:journals/corr/abs-1712-00409} and
later by \citet{DBLP:journals/corr/abs-2001-08361} who established empirical scaling laws for language model performance based on cross-entropy, which led to \citet{DBLP:journals/corr/abs-2203-15556} and the pursuit of compute-optimal training.
Beyond the empirical studies, there have been many theoretical works which provide explanations for why scaling laws should exist \citep{DBLP:journals/corr/abs-2102-06701,DBLP:journals/corr/abs-2405-15074,DBLP:journals/corr/abs-2411-06646}.
More recent works explore scaling laws across different distributions, closely related to knowledge distillation.
\citet{DBLP:journals/corr/abs-2102-01293} derived a scaling law for transfer learning, analyzing effective data transfer in low-data regimes and diminishing returns in high-data regimes.
Similarly, \citet{DBLP:journals/corr/abs-2408-16947} empirically studied pretraining on one distribution for optimizing downstream performance on another, showing that when the \emph{transfer gap} is low, pretraining is a cost-effective strategy. Finally, \citet{DBLP:journals/corr/abs-2402-04376} theoretically analyze how additional data from a \emph{surrogate model} affects generalization, demonstrating that surrogate data can reduce test error—even when unrelated—due to Stein's paradox \citep{stein1956inadmissibility}, with test error following a scaling law. 
This setup is related to tuning the coefficient $\lambda$ in our case, where we also observe a U-shape behavior depending on the teacher and student sizes (see \Cref{fig:sensitivity-analysis-lambda}). 
However, we are interested in studying the effect of distillation \emph{only} ($\lambda = 1.0$), which differs from their setup. 
While these works are closely related to knowledge distillation—since one can compare the distribution of the teacher logits to that of the student—they do not establish a distillation scaling law. Moreover, their setup differs from practical knowledge distillation, as it does not involve training a \emph{new} student model using a teacher but instead studies the effect of transferring training knowledge to a downstream task.
Our work is the first to determine and verify a distillation scaling law and examine the regions where one should distill as well as the regions where supervised pretraining outperforms distillation; see
\Cref{fig:fixedm-teacher-isoflop-students-strategies-data,fig:fixedm-teacher-isoflop-students-strategies-compute,fig:distillation-strategies-a-fixedtokens-xparams} in \Cref{ssec:fixed-tokens-or-compute-best-case-app,ssec:fixed-distillation-budget-given-a-teacher}.
Finally, 
for improving inference cost at a given model capability, the scaling behavior of \gls{moe}
\citep{DBLP:journals/corr/ShazeerMMDLHD17,DBLP:journals/corr/abs-2410-19034}
have been investigated in the context of scaling laws
\citep{DBLP:conf/icml/ClarkCGMPHDHCB022,DBLP:conf/icml/LudziejewskiKAP24,parameters-flops-scaling}
as one alternative to knowledge distillation.

\subsection{The Knowledge Distillation Capacity Gap}
\label{ssec:the-capacity-gap}
Despite extensive research on knowledge distillation, a persistent challenge is the curse of capacity gap, where a larger teacher does not necessarily produce a superior student compared to a smaller teacher.
This occurs because a large gap in model capacity makes it harder for the student to effectively learn from the teacher's outputs. As a result, there exists an optimal teacher size along the scaling trajectory that maximizes student performance. Our distillation scaling law in \Cref{eq:distillation-scaling-law} confirms this, revealing a u-shaped trend in the scaling law and validating the existence of an optimal teacher. However, our results further indicate that the capacity gap is influenced not only by the size of the teacher but also by its training tokens and, more generally, its loss. A theoretical analysis in the kernel regression setup (\Cref{sec:teacher-student-capacity-gaps}) supports these findings.
\citet{DBLP:journals/tmlr/LukasikBMK22} showed that distillation gains are not uniform and can even degrade performance when small teacher errors are amplified by the student. Similarly, \citet{DBLP:conf/nips/NagarajanMBMK23} found that deviations in predictive probabilities cause students to exaggerate the teacher's confidence levels. Several works \citep{DBLP:journals/corr/abs-2410-16215,DBLP:journals/corr/abs-2311-07052,DBLP:journals/corr/abs-2410-18779} observed the capacity gap in pre-training distillation for \gls{llm}s, affecting both large-to-small and small-to-large distillation. 
Notably, \citet{DBLP:journals/corr/abs-2311-07052} proposed an empirical law of the capacity gap, showing that the optimal teacher scale follows an approximately linear relationship with the student's scale. 
However, our findings suggest that scaling alone is insufficient—one must account for the complexity of the effective hypothesis space (\Cref{eq:distillation-scaling-law})
and we show that \citet{DBLP:journals/corr/abs-2311-07052}
is a special case of our work when the teachers are compute-optimal from a supervised perspective (see \Cref{ssec:compute-optimal-distillation}).
To address this issue, various strategies have been explored. \citet{DBLP:journals/kbs/YuanLQ24} studied temperature scaling, which simplifies the teacher's output into more learnable representations, aiding student generalization. We analyzed the effect of temperature and learning rate in distillation (\Cref{fig:sensitivity-analysis-temperature,fig:sensitivity-analysis-learning-rate}) and found that, contrary to existing literature, the optimal temperature is one. We hypothesize that this discrepancy arises because previous studies used repeated tokens, whereas our setup does not involve repeated data. Additionally, \citet{DBLP:conf/iccv/ChoH19} found that early stopping of the teacher's training mitigates the capacity gap, while \citet{DBLP:conf/aaai/MirzadehFLLMG20} proposed progressive distillation, where knowledge is transferred through intermediate models to improve student learning.
From a theoretical perspective, \citet{DBLP:conf/iclr/HarutyunyanRMKK23} analyzed the capacity gap in distillation using supervision complexity in kernel classifiers. Their findings highlight a trade-off between teacher accuracy, student margin with respect to teacher predictions, and teacher complexity, explaining why some teachers are easier for the student to learn. 
Earlier, \citet{DBLP:journals/corr/Lopez-PazBSV15} studied generalization error in distillation, proving that learning from a teacher can be beneficial under certain conditions, particularly when the teacher's capacity is small. Using similar techniques in \gls{lm}s, \citet{DBLP:conf/acl/ZhangYLWXWS23} demonstrated that among students of different capacities distilled from the same teacher, smaller students suffer from higher generalization error and lower performance, while larger teachers provide lower generalization error, reinforcing the trade-off in teacher-student capacity. Our distillation scaling law (\Cref{eq:distillation-scaling-law}) also confirms this trend, and we observe the effect of capacity gap in our scaling law terms, see \Cref{ssec:distillation-scaling-law-functional-form} for more details.

\paragraph{Foundation model pretraining}
Foundation models were initially undertrained \citep{DBLP:conf/nips/BrownMRSKDNSSAA20}, then followed the compute-optimal scaling law carefully \citep{DBLP:journals/corr/abs-2203-15556,DBLP:journals/corr/abs-2406-12907,DBLP:journals/corr/abs-2404-10102}, and soon after started overtraining heavily \citep{DBLP:conf/icml/SardanaPDF24,DBLP:journals/corr/abs-2401-02954,DBLP:journals/corr/abs-2404-06395,DBLP:journals/corr/abs-2403-08295,DBLP:journals/corr/abs-2310-06825}. The \llama family \citep{DBLP:journals/corr/abs-2302-13971,DBLP:journals/corr/abs-2307-09288,DBLP:journals/corr/abs-2407-21783} and Phi line \citep{DBLP:journals/corr/abs-2309-05463,DBLP:journals/corr/abs-2404-14219,DBLP:journals/corr/abs-2412-08905} is following the same trend, where smaller models are overtrained according to the original Chinchilla scaling laws. In all these cases, the models are designed to be best possible foundation model that is still cheap and fast to run on lower end hardware.
Besides overtraining, more recently, smaller foundation models tend to be distilled from larger models \citep{DBLP:journals/corr/abs-2407-21075,DBLP:journals/corr/abs-2408-00118,DBLP:journals/corr/abs-2403-05530} to further increase performance.
In these cases, the large model either specifically trained with the sole purpose of being a distillation teacher, or an existing model is re-used. In both these cases, there are no reports of how the exact teacher size is decided when taking total compute into account.
Determining the optimal allocation of compute budget in the distillation setting is one of the primary contributions of our work (see \Cref{ssec:compute-optimal-distillation}).

