\subsection{MLPs on the Mapping Problem}
\label{ssec:mlps-on-the-mapping-problem}

\subsubsection{Problem Definition}
\label{sssec:problem-definition}

Here we show a synthetic setting which exhibits the U-shape phenomenon. 
Matching the kernel regression analysis (\Cref{ssec:kernel-regression}), 
we find that the synthetic problem must include a class of problems that are easy for the student to learn, and ones that are harder, in order for the u-shape to appear.

The problem setting is the \emph{Mapping Problem}, and is 
similar in spirit to Pointer Value Retrieval \citep{DBLP:journals/corr/abs-2107-12580},
Here, the input is composed of small integers in \{0,1,2\}. 
The label for each sample is given by the code below, which shows the two cases: i) one where the label is simply given by a one-hot position, and ii) one where the label is given by the location of a matching element in the context portion of the input.  

\begin{minipage}{.6\textwidth}
	\input{algorithms/mapping.tex}
\end{minipage}
\begin{minipage}{.4\textwidth}
	\small
	\begin{verbatim}
Examples:
-----------------------------
2020210001000000, label = 1
    context [2 0 2 0 2 1 0 0]
    one-hot [0 1 0 0 0 0 0 0]
-----------------------------
1210120000000100, label = 2
    context [1 1 2 0 1 2 0 0]
    one-hot [0 0 0 0 0 1 0 0]
-----------------------------
0122221201000000, label = 6
    context [0 1 2 2 2 2 1 2]
    one-hot [0 1 0 0 0 0 0 0]
-----------------------------
\end{verbatim}
\end{minipage}

\subsubsection{Experimental Findings}
\label{sssec:experimental-findings}

We train \glspl{mlp} with two hidden layers of equal width, all non-linearities are \glspl{relu}. 
Teachers and students of different sizes are produced by varying the hidden layer width only.

All model are trained with Adam \citep{DBLP:journals/corr/KingmaB14}
using a peak learning rate of $3\times 10^{-4}$,
a single cycle cosine learning rate schedule with a linear warmup of $5\%$ of the total training steps.
A batch size of 512 is used for all models.
Training samples are never repeated.
Unless explicitly stated, model are trained on 
$500\times 512$, or $20N$ samples, where $N$ is the number of model parameters, whichever is larger.

In \Cref{fig:student-teacher-size-remap-loss}, we look at varying the size of the teacher.
For the width 256 model,
student performance improves as the teacher size increases to a point, and then student performance worsens.
This is observable in both the student cross-entropy (\Cref{fig:mlp-width-loss}) and accuracy (\Cref{fig:mlp-width-accuracy}).
Aligning with theory and large-scale experiments, the student cannot learn if it is too small, and can learns to match the teacher model when ther student is large enough. In the intermediate regime, where distillation is often used, we see an optimal teacher size and a capacity gap phenomenon.

\begin{figure}[h]
	\centering
	\subfloat[Cross-entropy]{
		\includegraphics[width=0.3\textwidth]{plots/fig_student_teacher_size_remap_loss_v2.pdf}
        \label{fig:mlp-width-loss}
	}
	\subfloat[Accuracy]{
		\includegraphics[width=0.29\textwidth]{plots/fig_student_teacher_size_remap_accuracy_v2.pdf}
        \label{fig:mlp-width-accuracy}
	}
	\caption{\textbf{Student performance when varying teacher width.}
	\textbf{(a)} Student cross-entropy as teacher width $\dffn$ is varied. 
	\textbf{(b)} Student accuracy as teacher width $\dffn$ is varied.
    Bands show the (25\%,75\%) values across four trials.}
    \label{fig:student-teacher-size-remap-loss}
\end{figure}

In \Cref{fig:student-teacher-steps-remap-loss}, a similar effect can be seen, when a large teacher ($\dffn=512$) is trained with on different amounts of data. This observation aligns with the idea that it is the teacher's completeness in modeling the problem that eventually harms the performance of a student with lesser capacity, and \emph{not} only the teacher size.


\begin{figure}[h]
	\centering
	\subfloat[Cross-entropy]{
		\includegraphics[width=0.3\textwidth]{plots/fig_teacher_data_remap_loss_v2.pdf}
        \label{fig:mlp-steps-loss}
	}
	\subfloat[Accuracy]{
		\includegraphics[width=0.29\textwidth]{plots/fig_teacher_data_remap_accuracy_v2.pdf}
        \label{fig:mlp-steos-accuracy}
	}
	\caption{\textbf{Student performance when varying teacher training data.}
	\textbf{(a)} Student cross-entropy as teacher training data is varied. 
	\textbf{(b)} Student accuracy as teacher training data is is varied.
    Bands show the (25\%,75\%) values across four trials.}
    \label{fig:student-teacher-steps-remap-loss}
\end{figure}
