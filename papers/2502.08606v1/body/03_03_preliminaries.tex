\section{Preliminaries}
\label{sec:preliminaries}

\paragraph{Notation}
For a sequence
$\vx$,
$\vx^{(i:j)}=(x^{(i)},x^{(i+1)},\ldots,$ $x^{(j)})$
returns a slice of the sequence,
and
$\vx^{(<i)}=\vx^{(1:i-1)}=(x^{(1)},\ldots,x^{(i-1)})$ is the \emph{context} of $x^{(i)}$.
We use the shorthand
$\gX^*=\cup_{n\in\mathbb{N}}\gX^n$
to denote the set of sequences
with arbitrary length $n\in\mathbb{N} =\{1,2,\ldots\}$.

\paragraph{Language modeling}
We focus on the \gls{lm} setting where the training objective is to model the probability of sequences $\vx$ of tokens $x_i$ drawn from a vocabulary
$\gV=\{1,2,\ldots,V\}$. Let
{
	\medmuskip=3.5mu
	\thinmuskip=3.5mu
	\thickmuskip=3.5mu
$f:\gV^*\times \Theta\rightarrow \mathbb R^V$}be a next-token classifier parameterized by
$\vtheta \in\Theta$
whose outputs define a predictive categorical
distribution over $\gV$ given a context $\vx^{(<i)}$
\begin{equation}
	\hat p(x^{(i)}=a|\vx^{(<i)};\vtheta)
	=\sigma_a (f(\vx^{(<i)};\vtheta))
	=\sigma_a (\vz^{(i)}),
\end{equation}
where
$
	\sigma_a(\vz)=
	\exp(z_a)/\sum_b \exp(z_b)
$
is the softmax function.
The next-token classifier outputs
$\vz^{(i)}=f(\vx^{(<i)};\vtheta)$
are the \emph{logits}.\footnote{
	We \emph{do not} write this as $\vz^{(<i)}$ to avoid confusion with the sequence
	$\vz^{(<i)}=(\vz^{(1)},\ldots,\vz^{(i-1)})$.
}
Autoregressive \glspl{lm} produce sequence likelihoods through $\hat p(\vx;\vtheta)
	=\prod_{i=1}^L\hat p(x^{(i)}|\vx^{(<i)};\vtheta)$
and are trained to maximize this likelihood on observed data through the \gls{ntp} loss
\begin{align}
	\Ls_{\text{NTP}}(x^{(i)},\vz^{(i)}) & =
	-\sum_{a=1}^V \ve(x^{(i)})_a\log \sigma_a(\vz^{(i)}),
	\label{eq:ntp-cross-entropy}
\end{align}
where $\ve(i)$ is the $i$-th basis vector.
It is common to also use the following token-level $Z$-loss to improve training stability \citep{DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23,DBLP:journals/corr/abs-2309-14322}
{
	\medmuskip=3.1mu
	\thinmuskip=3.1mu
	\thickmuskip=3.1mu
	\begin{align}
		\Ls_Z(\vz^{(i)})
		=||\log Z(\vz^{(i)})||_2^2
		=\left|\left|\log \sum_{a=1}^V\exp(z_a^{(i)})\right|\right|_2^2.
	\end{align}
}

\paragraph{Distillation}
In distillation, a \emph{teacher} with predicted next-token distribution
$\hat p_T(x^{(i)}|\vx^{(<i)};\vtheta_T)$
and corresponding logits $\vz_T^{(i)}$
replaces the one-hot basis vector in \Cref{eq:ntp-cross-entropy}
and is used as the target for a student predicted next-token distribution
$\hat q_S(x^{(i)}|\vx^{(<i)};\vtheta_S)$ and corresponding logits $\vz_S^{(i)}$.
The resulting \emph{knowledge distillation loss} is used to optimize the student parameters
	{
		\medmuskip=1.3mu
		\thinmuskip=1.3mu
		\thickmuskip=1.3mu
		\begin{align}
			\Ls_{\text{KD}}(\vz_T^{(i)},\vz_S^{(i)}) & =
			-\tau^2
			\sum_{a=1}^V \sigma_a
			\left(\frac{\vz_T^{(i)}}{\tau}\right)
			\log \sigma_a\left(\frac{\vz_S^{(i)}}{\tau}\right),
		\end{align}}and is equivalent to optimizing the \gls{kld} between the teacher and student predictions.
$\tau>0$ is the distillation \emph{temperature}.
Combining
the losses together results in a total token-level loss for the student:
	{
		\begin{align}
			\Ls_{S} & (x^{(i)},\vz_T^{(i)},\vz_S^{(i)})=
			(1-\lambda)\,\Ls_{\text{NTP}}(x^{(i)},\vz_S^{(i)})\nonumber
			\\                                  & +\lambda\,\Ls_{\text{KD}}(\vz_T^{(i)},\vz_S^{(i)}) +\lambda_Z\,\Ls_{Z}(\vz_S^{(i)}).
			                                   \label{eqn:kd_and_nll_full_loss}
		\end{align}
	}%

