\section{Background}
\label{sec:background}
Predicting model performance is essential when scaling
as it lets us understand i) the value of increasing the available compute ($C$),
and ii) how that compute should be distributed, typically between model parameters ($N$) and data ($D$), in order to achieve a model with desired properties.
These properties may be predicting the data distribution sufficiently well, measured in cross-entropy ($L$),
or achieving a level of performance on downstream tasks of interest.

Fortunately, cross-entropy \emph{is predictable}, with substantial empirical\todo{lots of cites here} and theoretical\todo{cites}
evidence that $L$ follows a power-law in parameters $N$ and data $D$ (measured in tokens)
{
		\medmuskip=2.1mu
		\thinmuskip=2.1mu
		\thickmuskip=2.1mu
\begin{align}
	\underbrace{L(N,D)}_\text{Model Cross-Entropy}=
	\underbrace{E}_{\text{Irreducible Error}}
	+
	\underbrace{\left(\frac{A}{N^\alpha}+\frac{B}{D^\beta}\right)^\gamma}_{\text{Model ability to mimic data}},
	\label{eq:supervised-scaling-law}
\end{align}
}where $\{E,A,B,\alpha,\beta,\gamma\}$ are task-specific positive coefficients\footnote{\citet{DBLP:journals/corr/abs-2203-15556} use $\gamma=1$
whereas \citet{DBLP:journals/corr/abs-2001-08361}
use $\beta=1$. We observe a significantly better fit and extrapolation without coefficient tying, which may be due to our use of \gls{mup} (see \Cref{ssec:experimental-setup}).} estimated from $n$ training runs $\{(N_i,D_i,L_i)\}_{i=1}^n$.

The choice of runs is critical; not all experiments enable identifying
the coefficients of \Cref{eq:supervised-scaling-law}.
One could use \emph{compute optimal} models whose size parameters $N^*$
and number of training tokens $D^*$ gives the lowest cross-entropy subject to a compute constraint $C$
{
		\medmuskip=2.1mu
		\thinmuskip=2.1mu
		\thickmuskip=2.1mu
		\begin{equation}
			N^*,D^*=\argmin_{N,D}L(N,D)\;\,\mathrm{s.t.}\;\,\mathrm{FLOPs}(N,D)=C.
		\end{equation}
	}This is tempting, as for a total experiment budget,
compute optimal models offer the largest loss variation.
Unfortunately, compute optimal models have a \emph{constant token to parameter ratio} $M\equiv D/N=\mathrm{const.}$  \citep{DBLP:journals/corr/abs-2203-15556},
removing a degree of freedom.

To achieve reliable identification of scaling coefficients,
\citet{DBLP:journals/corr/abs-2203-15556} uses two training strategies:
\begin{enumerate}
	\item \emph{(Fixed model, varied data)} The number of training tokens is varied for a fixed family of models.
	\item \emph{(IsoFLOP profiles)} Model size and training tokens are both varied subject to a total compute constraint.
\end{enumerate}
Data from both strategies is then combined for the fit.
See \Cref{sec:extended-background}
for an extended background.

The goal of this paper is predict the cross-entropy $L_S$ of a student produced by distillation.
This will tell us the value of increasing the compute for distillation and,
crucially,
which distillation produces the student of a given size with the lowest cross-entropy for a given compute budget.
