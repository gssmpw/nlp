\clearpage
\section{Scaling coefficients}
\label{sec:scaling-coefficients}

In this section, we analyze the process of deriving the coefficients for our scaling law. We follow the procedure outlined in \citep{DBLP:journals/corr/abs-2203-15556,DBLP:journals/corr/abs-2404-10102}, while incorporating our modified scaling laws

\subsection{Supervised scaling law coefficient estimation}
\label{ssec:supervised-scaling-law-coefficient-estimation}
First, let's tackle the supervised scaling law \Cref{eq:supervised-scaling-law} restated for convenience
\begin{equation}
	L(N,D)=
	E
	+
	\left(\frac{A}{N^\alpha}+\frac{B}{D^\beta}\right)^\gamma.
\end{equation}
To aid numerical stability, we write this expression in log space.
First note that for $a,b>0$
\begin{align}
	\log(a+b)=\log\left(\exp\log a+ \exp\log b\right)=\mathrm{LSE}(\log a, \log b),
\end{align}
where $\mathrm{LSE}$ is the log-sum-exp operator.
We can now proceed to write the supervised scaling law in log form
\begin{align}
	\log L(N,D;A,B,E,\alpha,\beta)
	 & =
	\log \left[E
		+
	\left(\frac{A}{N^\alpha}+\frac{B}{D^\beta}\right)^\gamma\right]                                             \\
	 & =\mathrm{LSE}\left[\log E, \gamma\log
	\left(\frac{A}{N^\alpha}+\frac{B}{D^\beta}\right)\right]                                                    \\
	 & =\mathrm{LSE}\left[\log E, \gamma\,\mathrm{LSE}\left(\log A - \alpha N, \log B - \alpha D\right)\right].
\end{align}
We make no assumptions about the relationships between the values (i.e. \emph{no parameter tying})
and optimize
\begin{align}
	(A^*,B^*,E^*,\alpha^*,\beta^*,\gamma^*) = \argmin_{\{A,B,E,\alpha,\beta,\gamma\}}\sum_i\mathrm{Huber}_\delta\left(\log L(N^{(i)},D^{(i)};A,B,E,\alpha,\beta)-L^{(i)}\right)
\end{align}
with a Huber $\delta=10^{-4}$,
where $N^{(i)}$, $D^{(i)}$ and $L^{(i)}$ are the model size, number of training tokens and loss achieved by the $i$-th run.
We fit on 73 samples over a grid of L-BFGS-B initializations given by:
$\log A\in\{0., 5., 10., 15., 20.\}$,
$\log B\in\{0., 5., 10., 15., 20.\}$,
$\log E \in\{-1., -0.5., 0., 0.5, 1., 1.5.\}$,
$\alpha\in\{0., 0.5, 1., 1.5\}$,
$\beta\in\{0., 0.5, 1., 1.5\}$,
$\gamma\in\{0., 0.5, 1., 1.5\}$.
The $L\geq 2.2$ case corresponds to 48 samples.

\subsection{Distillation scaling law coefficient estimation}
\label{ssec:distillation-scaling-law-coefficient-estimation}

Next, let's address the distillation scaling law \Cref{eq:distillation-scaling-law} restated for convenience
\begin{align}
	L_S(N_S,D_S,L_T)
	=L_T+
	\frac1{L_T^{c_0}}
	\left(1+\left(\frac{L_T}{\widetilde{L}_S d_1}\right)^{1/{f_1}}\right)^{-c_1*f_1}
	\left(\frac{A^\prime}{N_S^{\alpha^\prime}}+\frac{B^\prime}{D_S^{\beta^\prime}}\right)^{\gamma^\prime}.
\end{align}
As in \Cref{ssec:supervised-scaling-law-coefficient-estimation},
to aid numerical stability during optimization, we write this in log space
\begin{align}
	\log L_S(N_S,D_S,L_T;\theta) & =
	\log\left[L_T+
	\frac1{L_T^{c_0}}
	\left(1+\left(\frac{L_T}{\widetilde{L}_S d_1}\right)^{1/{f_1}}\right)^{-c_1*f_1}
	\left(\frac{A^\prime}{N_S^{\alpha^\prime}}+\frac{B^\prime}{D_S^{\beta^\prime}}\right)^{\gamma^\prime}\right]
	\\
	                             & =\mathrm{LSE}
	\left[
		\log L_T,
		-c_0 \log L_T
		-c_1f_1\log\left(1+\left(\frac{L_T}{d_1\widetilde{L}_S}\right)^{1/f_1}\right)
		+\gamma\log\left(\frac {A^\prime}{N_S^\alpha}+\frac {B^\prime}{D_S^\beta}\right)
		\right]
	\\
	                             & =\mathrm{LSE}
	\Bigg[
	\log L_T,
	\Bigg(
	-c_0 \log(L_T)
	-c_1f_1\,\mathrm{LSE}\left(0 , \frac1{f_1}\left(\log L_T - \log \widetilde{L}_S -\log d_1\right) \right)\nonumber \\
	                             & \hspace{2.8cm}+\gamma\,\mathrm{LSE}\left(
	\log A^\prime - \alpha^\prime \log N_S, \log B^\prime - \beta^\prime \log D_S\right)\Bigg)
	\Bigg],
\end{align}
where $\theta = \{A^\prime,B^\prime,\alpha^\prime,\beta^\prime,c_0,c_1,f_1,d_1\}$.
We make no assumptions about the relationships between the values
and optimize
\begin{align}
	\theta^* = \argmin_{\theta}\sum_i\mathrm{Huber}_\delta\left(\log L_S(N_S^{(i)},D_S^{(i)},L_T^{(i)};\theta)-L_S^{(i)}\right)
\end{align}
with a Huber $\delta=10^{-4}$,
where $N_S^{(i)}$, $D_S^{(i)}$, $L_T^{(i)}$ and $L_S^{(i)}$ are the student model size, number of training distillation tokens, the teacher pretraining loss and the student validation loss on the data achieved by the $i$-th run.
We fit on 697 samples over a grid of L-BFGS-B initializations given by:
$\log A^\prime\in\{0., 5., 10., 15., 20.\}$,
$\log B^\prime\in\{0., 5., 10., 15., 20.\}$,
$\alpha^\prime\in\{0., 0.5, 1.\}$,
$\beta^\prime\in\{0., 0.5, 1.\}$,
$\gamma^\prime\in\{0., 0.5, 1.\}$,
$c_0\in\{0., 0.5, 1., 1.5\}$,
$c_1\in\{0., 0.5, 1., 1.5\}$,
$f_1\in\{0., 0.5, 1., 1.5\}$,
$\log d_1\in\{-1., -0.5, 0., 0.5, 1.\}$.
The $L_S\geq 2.3$ case corresponds to 551 samples.

\subsection{Scaling law coefficients parameteric fit}
\label{ssec:scaling-law-coefficients-parameteric-fit}

The fitting procedure outlined in 
\Cref{ssec:supervised-scaling-law-coefficient-estimation,ssec:distillation-scaling-law-coefficient-estimation}
applied to data described in \Cref{ssec:distillation-scaling-law-experiments}
yields the scaling coefficients and associated confidence intervals shown in 
\Cref{tab:scaling-law-parameter-estimates}.
Note in the supervised case, our values of $a$ and $b$ are consistent with those of 
\citet{DBLP:journals/corr/abs-2203-15556}.

\begin{table}[h]
\centering
\rowcolors{2}{AppleChartGrey2}{white}
\caption{Scaling law parameter estimates accompanied by $90\%$ confidence intervals obtained by bootstrapping (4096 resamples) following the procedure of \citet{DBLP:journals/corr/abs-2404-10102}. $a=\beta/(\alpha+\beta)$ and $b=\beta/(\alpha+\beta)$ are the supervised  compute optimal scaling estimates for $N$ and $D$ respectively \citep{DBLP:journals/corr/abs-2203-15556}.}
\label{tab:scaling-law-parameter-estimates}
\begin{tabular}{lcc}
\toprule
 & Supervised & Distillation \\
\midrule
$A^{(\prime)}$ & 3355 (3346, 3360) & 2243 (2227, 2255) \\
$B^{(\prime)}$ & 18186 (18157, 18236) & 24181 (24084, 24266) \\
$E$ & 1.220 (1.190, 1.247) &  \\
$\alpha^{(\prime)}$ & 0.408 (0.405, 0.411) & 0.321 (0.319, 0.324) \\
$\beta^{(\prime)}$ & 0.431 (0.428, 0.433) & 0.637 (0.634, 0.640) \\
$\gamma^{(\prime)}$ & 0.452 (0.442, 0.461) & 0.764 (0.732, 0.788) \\
$c_0$ &  & 2.549 (2.425, 2.615) \\
$c_1$ &  & 522.6 (522.6, 522.6) \\
$f_1$ &  & 0.090 (0.088, 0.093) \\
$d_1$ &  & 1.315 (1.302, 1.327) \\
$a^{(\prime)}$ & 0.513 (0.513, 0.513) & 0.664 (0.662, 0.665) \\
$b^{(\prime)}$ & 0.486 (0.486, 0.486) & 0.335 (0.334, 0.337) \\
\midrule
Runs & 73 & 697 \\
\bottomrule
\end{tabular}
\end{table}

We also note that our irreducible error term is lower than the one in \citet{DBLP:journals/corr/abs-2203-15556}.
We suspect this is due to our use of \gls{mup}~\citep{DBLP:conf/icml/YangH21,DBLP:journals/corr/abs-2308-01814,DBLP:journals/corr/abs-2203-03466,DBLP:journals/corr/abs-2309-14322,DBLP:journals/corr/abs-2310-17813}.
