\section{Distillation scaling law applications}
\label{sec:distillation-scaling-law-applications}

Here, we apply our distillation scaling law (\Cref{eq:distillation-scaling-law})
and investigate scenarios of interest.
Typically, the resources in distillation pretraining include a compute budget, or a dataset containing a number of tokens.
For a distillation process, the compute cost can be approximated by
{
		\medmuskip=1.2mu
		\thinmuskip=1.2mu
		\thickmuskip=1.2mu
\begin{equation}
    \hspace{-.2cm}
    \mathrm{FLOPs}\approx
    \underbrace{3F(N_S)D_S}_{\substack{\mathrm{Student}\\\mathrm{Training}}}
    +F(N_T)(
    \underbrace{\delta_T^{\mathrm{Lgt}}D_S}_{\substack{\mathrm{Teacher}\\\mathrm{Logits}}} + \underbrace{\delta_T^{\mathrm{Pre}}3D_T}_{\substack{\mathrm{Teacher}\\\mathrm{Training}}})
    \label{eq:distillation-compute}
\end{equation}
}where 
$\delta_T^{\mathrm{Lgt}},\delta_T^{\mathrm{Pre}}\in[0,1]$ 
indicate if we account for the cost of teacher logit inference for the student targets\footnote{\Cref{ssec:top-k-top-p-sensitivity} evaluates distribution truncation via Top-$p$ and Top-$k$ to mitigate the overhead of computing these logits online.}, and teacher pretraining cost in the total compute budget (see \Cref{tab:compute-scenarios}).
$F(N)$ is the number of \flops a model with $N$ parameters
performs during a forward pass.
$F(N)\approx 2N$ is often used, giving supervised $\mathrm{FLOPs}\approx 6ND$.
We cannot use the $2N$ approximation, as i) using \emph{non-embedding} parameters $N$ induces systematic errors \citep{DBLP:journals/corr/abs-2406-19146},
and ii) we are interested in \emph{small models with large context sizes} where the FLOP contribution from attention is significant.
To resolve these issues.
we derive a simple expression $F(N)\approx 2N(1+c_1N^{-1/3}+c_2N^{-2/3})$
for \emph{fixed-aspect ratio} models
\Cref{ssec:alternative-approximation-for-flops-per-token-as-a-function-of-n}, and recommend the scaling community to consider adopting this hyperparameter setting.

\begin{table}[t]
    \centering
    \rowcolors{2}{AppleChartGrey2}{white}
    \caption{Scenarios considered in our scaling law applications.}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{p{3.cm}ccp{7.cm}}
        \toprule
        Compute Scenario & $\delta_T^{\mathrm{Lgt}}$ & $\delta_T^{\mathrm{Pre}}$ & Description \\ \midrule
        Best case (fully amortized teacher) & 0 & 0 & The teacher produces no additional FLOPs and so we are free to choose the teacher $L_T^*$ that minimizes the student cross-entropy. \\
        Teacher inference & 1 & 0 & We don't account for the teacher cost because the teacher already exists, or we intend to use the teacher as e.g. a server model. We still need to pay to use it for distilling a student. \\
        Teacher pretraining & 0 & 1 & The teacher needs training, but we store the logits for re-use, either during training, or after training for distilling into sufficiently many students.  \\        
        Teacher pretraining + inference & 1 & 1 & The teacher needs training and we pay for distilling into one student, the worst case scenario. \\ 
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.1cm}
    \label{tab:compute-scenarios}
\end{table}

\subsection{Fixed tokens or compute (best case)}
\label{ssec:fixed-tokens-or-compute-main}
To build intuition for when distillation may (and may \emph{not}) be beneficial, 
we ask \emph{how well can distillation do in the best case scenario, compared with supervised learning?}
We superimpose the data of \Cref{fig:fixedm-teacher-isoflop-students,fig:isoflop-teacher-fixedm-students} onto contours 
of distilled cross-entropy $L_S$ compared to a supervised model with the same resources $\widetilde L_S$ (\Cref{fig:fixedm-teacher-isoflop-students-strategies-data}).
\begin{figure}[h]
	\centering
	\includegraphics[width=0.43\textwidth]{plots/fixedm_teacher_isoflop_students_strategies_data.pdf}
    \vspace{-0.25cm}
	\caption{\textbf{Fixed-$\bm M$ Teacher/IsoFLOP students (data).} For a student size $N_S$ and token budget $D_S$, the cross-entropy difference between best case distillation and supervised learning. Blue indicates distillation outperforms supervised learning, red otherwise.
		The white horizontal dashed line indicates teacher size.
	}
    \vspace{-0.25cm}
	\label{fig:fixedm-teacher-isoflop-students-strategies-data}
\end{figure}
\paragraph{Supervised learning always outperforms distillation given enough student compute or tokens.}
For a modest token budget, distillation is favorable,
however, when a large number of tokens are available,
supervised learning outperforms distillation.
This is expected; in the large data regime, supervised learning can find the best solution limited by model size $N$ (\Cref{eq:supervised-scaling-law}),
whereas distillation \emph{only} finds this solution for the optimal teacher $L_T^*$
(see \Cref{ssec:distillation-with-infinite-data}),
and is otherwise limited by the distillation process.
This finding appears to contradict the \emph{patient teacher} finding of \citet{DBLP:conf/cvpr/BeyerZRMA022}.
A comment on this contradiction is provided in \Cref{sec:contradiction}.
Student compute constrained version of \Cref{fig:fixedm-teacher-isoflop-students-strategies-data}
and 
IsoFLOP Teacher/Fixed $M$ student contours are provided in
\Cref{ssec:fixed-tokens-or-compute-best-case-app}.


\FloatBarrier
\subsection{Fixed tokens or compute (teacher inference)}
\label{ssec:fixed-distillation-budget-given-a-teacher}

Next, we focus on the common scenario of planning to distill, and trying to decide between 
an existing set of teachers $\{(L_T^{(i)},N_T^{(i)})\}_{i=1}^n$.
A larger teacher may provide a better learning signal (lower cross-entropy)
but will also be more expensive to use because of the teacher logits cost (\Cref{eq:distillation-compute}, $\delta_T^{\mathrm{Lgt}}=1$),
inducing a trade-off.
Given a target student size $N_S$ and budget $D_S$ or $C_{\mathrm{Total}}$, the only degree of freedom is the choice of teacher.

\paragraph{For a fixed data budget, as the student size increases, teacher cross-entropy should be decreased as a power law.}
Here, the compute cost from $N_T$ is not relevant as we are considering a token budget.
Student cross-entropy at different distillation token budgets is shown in
\Cref{fig:distillation-strategies-a-fixedtokens-xparams}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{plots/distillation_strategies_a_given_teacher_fixedtokens_xparams.pdf}
    \vspace{-0.25cm}
	\caption{\textbf{Students given a teacher and token budget.}
		For four distillation token budgets
		the student cross-entropy for a range of students and teachers.
		The red line indicates the optimal teacher cross-entropy $L_T^*$ producing the lowest student cross-entropy.
	}
    \vspace{-0.25cm}
	\label{fig:distillation-strategies-a-fixedtokens-xparams}
\end{figure}
An equivalent plot for different student sizes whilst varying tokens is shown in \Cref{ssec:fixed-tokens-or-compute-teacher-inference-app}.
We see that the optimal teacher loss $L_T^*$ (red line) decreases as a power law with student size $N_S$ until $L_S$ matches $L_T^*$, when there is an inflection point in $L_T^*$, causing the decrease of teacher loss to sharpen with $N_S$.
This generalizes the observation of \citet{DBLP:journals/corr/abs-2311-07052}, that
\emph{
	``Optimal teacher scale almost consistently follows a linear scaling with the
	student scale across different model architectures and data scales.''}
which is a special case of our finding when the teachers are compute optimal (\Cref{fig:supervised-fixed-long}).
Note that our findings consistently show that teacher cross-entropy $L_T$ determines student cross-entropy $L_S$, \emph{not} $N_T$ itself (which leads to a given $L_T$).
We investigate a fixed compute budget setting for teacher inference only in \Cref{ssec:fixed-tokens-or-compute-teacher-inference-app}.

\subsection{Compute Optimal Distillation}
\label{ssec:compute-optimal-distillation}

We extend the analysis of \citet{DBLP:journals/corr/abs-2203-15556} to distillation, giving \emph{compute optimal distillation},
determining how to produce the student of a desired size $N_S$ with the lowest cross-entropy given a compute budget $C${
		\medmuskip=2.1mu
		\thinmuskip=2.1mu
		\thickmuskip=2.1mu
		\begin{align}
			D_S^*,N_T^*,D_T^*=\argmin_{D_S,N_T,D_T}&L_S(N_S,D_S,N_T,D_T)\nonumber \\
			\mathrm{s.t.} \quad \mathrm{FLOPs}&=C,
			\label{eq:distillation-optimal}
		\end{align}
	}To present the best and worst case for incorporating teacher inference into the compute constraints, we consider
\emph{all scenarios} presented in \Cref{tab:compute-scenarios}.
We also compare against the optimal \emph{supervised} performance.
To find the minima in \Cref{eq:distillation-optimal} we perform constrained numerical minimization using \gls{slsqp} \citep{kraft1988software} in \texttt{SciPy} \citep{DBLP:journals/corr/abs-1907-10121}.

\paragraph{Supervised learning always matches optimal  distillation at sufficient compute budget, with the  intersection favoring supervised learning increasing as student size grows.}
In \Cref{fig:compute-optimal-distillation-student-loss} we see that supervised learning always matches the best case distillation setting at some total compute budget, as anticipated from the asymptotic analysis in
\Cref{fig:scaling-law-d-infinity}.
The compute transition point when supervised learning becomes preferable to distillation increases
as a function of student size.
See also \Cref{fig:fixedm-teacher-isoflop-students-strategies-data}.
We also observe that \emph{smaller models are more likely to benefit from supervised pretraining}, whereas
\emph{larger models are more likely to benefit from distillation}.

\paragraph{When teacher training is included in the compute, the best student cross-entropy is always higher than in the supervised setting.}
This means that if your \emph{only} aim is to produce the best possible model of a target size and you do not have access to a teacher, then you should choose supervised learning, instead of training a teacher and then distilling.
Conversely, if the intention is to distill into a family of models, or use the teacher as a server model, distillation \emph{may} be more computationally beneficial than supervised learning.
On reflection, this finding should be expected, otherwise it would imply that for a total amount of compute, distillation can outperform direct maximum likelihood optimization.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.47\textwidth]{plots/compute_optimal_distillation_L_S_main.pdf}
        \vspace{-0.25cm}
	\caption{\textbf{Compute optimal distillation student performance.}
	For four student sizes , the best cross-entropy each student can achieve the five scenarios considered as total compute is varied.
	}
    \vspace{-0.25cm}
	\label{fig:compute-optimal-distillation-student-loss}
\end{figure}
A detailed discussion of the compute optimal configurations that produce $(N_S^*,N_T^*,D_T^*)$ for all scenarios is discussed in \Cref{ssec:compute-optimal-distillation-app}.

To build intuition for how quantities play off against each-other, we take the most complex scenario, \emph{teacher pretraining + inference}.
A view of the optimal distillation setup as compute varies is presented in \Cref{fig:distillation-pretraining-quantities}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.43\textwidth]{plots/compute_optimal_distillation_teacherpreinf_main.pdf}
    \vspace{-0.25cm}
	\caption{\textbf{Teacher pretraining + inference.} For four student sizes, the optimal student and teacher configurations when teacher logit inference and teacher pretraining cost is accounted for.}
    \vspace{-0.35cm}
	\label{fig:distillation-pretraining-quantities}
\end{figure}
Student and teacher tokens scale as a power law, with student tokens at a faster rate.
Optimal teacher size increases initially until it is slightly larger than the student, after which it plateaus.
This plateau occurs because inference with large teachers is expensive, and with the increase in number of student tokens,  overtraining the teacher becomes more efficient.

The values in \Cref{fig:distillation-pretraining-quantities}
can be recombined to produce the  compute terms in \Cref{eq:distillation-compute}
as shown in \Cref{ssec:compute-optimal-distillation-app}, \Cref{fig:compute-optimal-allocation-teacherpreinf-app}. We summarize the trend in \Cref{tab:compute-trends}.
\begin{table}[t]
    \centering
    \rowcolors{2}{AppleChartGrey2}{white}
    \caption{Optimal compute allocation trends.}
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{rrp{4.8cm}}
        \toprule
        Student size & Compute (FLOPs) & Allocation \\ \midrule
        Small ($\lesssim 3B$) & Small ($\lesssim 10^{21}$) & Mostly teacher pretraining. \\
        Small ($\lesssim 3B$) & Large ($\gtrsim 10^{25}$) & Evenly divided between student training and teacher inference, much less on teacher pretraining. \\
        Large ($\gtrsim 10B$) & Small ($\lesssim 10^{21}$) & Mostly standard student training.\\ 
        Large ($\gtrsim10B$) & Large ($\gtrsim10^{25}$) & Equally divided between student training and teacher inference and teacher pretraining. \\ 
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.2cm}
    \label{tab:compute-trends}
\end{table}

\FloatBarrier
