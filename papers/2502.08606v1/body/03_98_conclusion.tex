\section{Conclusion}
\label{sec:conclusion}
\glsresetall
We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher.
We then used our law to study practical distillation scenarios of interest,
and showed that distillation is only more efficient than supervised learning if:
i) the total compute or tokens used for distillation is not larger than a student size-dependent threshold,
\emph{and} ii) a teacher already exists, or the teacher to be trained has uses beyond single distillation.
Moreover, we use this law to determine optimal distillation scenarios that are able to outperform supervised learning,
enabling practitioners to select the best teacher for their use case.
This work represents the largest controlled empirical study of distillation we are aware of,
with systematic ablations of common distillation techniques.
Just as supervised scaling has mitigated risks in supervised pretraining, our findings offer a roadmap for producing smaller, more powerful models with lower inference costs, reducing carbon footprints, and enhancing the feasibility of test-time scaling.


