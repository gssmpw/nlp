\section{Parameters and Floating Operation Estimation}
\label{sec:parameters-and-floating-operation-estimation}

Here we outline the number of parameters (\Cref{ssec:model-parameters})
and the number of FLOPs per token (\Cref{ssec:flops-per-token}) for our experimental settings.
The symbol notation is provided in \Cref{tab:param-flop-notation}.
For our scaling laws, we find, as in \citet{DBLP:journals/corr/abs-2001-08361}
using that the number of \emph{non-embedding-parameters} provides the cleanest fit and extrapolation behavior.

Our expressions for approximate compute (FLOPs per token) differ from prior work in that we are interested in \emph{small models that are capable}.
This means we are unable to ignore the context-dependent term that arises from the quadratic computational complexity of the attention mechanism.
As our architectures are \emph{fixed aspect ratio}, there is a modified approximation we can use.
This expression is discussed in \Cref{ssec:alternative-approximation-for-flops-per-token-as-a-function-of-n}

For ease of reference, we provide a comparison of the expressions we use to commonly used existing expressions \citep{DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2203-15556,DBLP:conf/sc/NarayananSCLPKV21},
and provide comments for significant differences.

\input{body/05_03_flops_computation/notation_table}

\FloatBarrier
\input{body/05_03_flops_computation/alternative_compute}

\FloatBarrier
\subsection{Model parameters}
\label{ssec:model-parameters}

In \Cref{tab:param-count} we present our parameter counting compared to commonly used existing expressions \citep{DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2203-15556,DBLP:conf/sc/NarayananSCLPKV21}.
We present a convenient substitution in \Cref{tab:param-count-simple} which can be easier to work with analytically.
Our total expressions match the architecture we are using, which includes only gains for the normalization layers, whereas while \cite{DBLP:conf/sc/NarayananSCLPKV21} has both weights and biases.
We account for potential use of \citep{DBLP:conf/emnlp/AinslieLJZLS23} as well as
use of gated linear attention mechanisms which are becoming prevalent in modern architectures \citep{DBLP:journals/corr/abs-2002-05202}
including the one used in this work (\Cref{sec:model-architecture}).

\input{body/05_03_flops_computation/params_table}

This results in an approximation for the number of non-embedding parameters, dropping subleading terms
\begin{equation}
    N \approx \nlayers\dmodel^2\left(2+ \frac2\gsize + \nffn\rffn\right)
    \label{eq:non-embedding-parameters}
\end{equation}
which can be used to estimate forward \flops per token from the model size (\Cref{ssec:alternative-approximation-for-flops-per-token-as-a-function-of-n}).

\FloatBarrier
\subsection{FLOPs per token}
\label{ssec:flops-per-token}

In \Cref{tab:flops-count} we present our counting of the total number of \flops per token performed per token during a forward pass compared to commonly used existing expressions \citep{DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2203-15556,DBLP:conf/sc/NarayananSCLPKV21}.
We present a convenient substitution in \Cref{tab:flops-count-simple} which can be easier to work with analytically.

Beyond the potential accounting for gated linear layers and grouped query attention, the
most important discrepancy across methods is how the attention mechanism is handled.
As was also noted in \citet{DBLP:journals/corr/abs-2406-19146},
the expression used in \citet{DBLP:journals/corr/abs-2001-08361}
is consistent with efficiently computing a \emph{causal} attention mechanism \citep{DBLP:conf/nips/DaoFERR22,DBLP:conf/iclr/Dao24}
whereas \citet{DBLP:journals/corr/abs-2203-15556,DBLP:conf/sc/NarayananSCLPKV21}
are consistent with counting attention \flops for a bidirectional (non-causal) attention mechanism,
where the masked component of the attention matrix (zero by construction) is still being computed.
We adopt the efficient expression of assuming a causal computation as this more closely reflects best practice.







\input{body/05_03_flops_computation/compute_table}

This results in an approximation for the number of non-embedding floating operations per token, dropping subleading terms
\begin{equation}
    C_{\text{Forward}}\approx 2\nlayers\dmodel^2\left(2+ \frac2\gsize + \nffn\rffn\right)
	+2\nlayers\nctx\dmodel
    +2\nvocab\dmodel
    \label{eq:non-embedding-flops}
\end{equation}
which can be used to estimate forward \flops per token from the model size (\Cref{ssec:alternative-approximation-for-flops-per-token-as-a-function-of-n}).
