\clearpage
\section{Additional Results}
\label{sec:additional-results}

In this section, we provide an extensive list of studies, including downstream evaluations of distillation. We cover the models used as teachers, examine the \gls{kld} between teacher and student in fixed token-to-size ratios, and present supplementary materials to \Cref{ssec:experimental-setup}. Additionally, we investigate the limiting behavior of our scaling law, weak-to-strong generalization, and conduct a model calibration study to assess fidelity. These analyses offer a comprehensive view of the factors influencing distillation performance and the behavior of our proposed scaling laws.

\subsection{Downstream evaluations}
\label{ssec:downstream-evaluations}

In all settings, we optimize for and predict model cross-entropy on the validaiton set. 
To confirm that the validation cross-entropy $L_S$ is a good proxy for the downstream evaluation that we ultimately care about, we show how each downstream result is affected by the teacher and student loss.
\Cref{fig:downstream-evals-all} shows a set of English downstream evaluation tasks. ARC Easy \citep{DBLP:journals/corr/abs-2102-03315}, ARC Challenge \citep{DBLP:journals/corr/abs-2102-03315}, HellaSwag \citep{DBLP:conf/acl/ZellersHBFC19}, Piqa \citep{DBLP:conf/aaai/BiskZLGC20}, Sciq \citep{DBLP:conf/aclnut/WelblLG17}, WinoGrande \citep{DBLP:journals/cacm/SakaguchiBBC21} and Lambada OpenAI \citep{DBLP:conf/acl/PapernoKLPBPBBF16} are zero-shot tasks. TriviaQA \citep{DBLP:conf/acl/JoshiCWZ17} and WebQS \citep{DBLP:conf/emnlp/BerantCFL13} are one-shot tasks. TriviaQA evaluation is on the larger and more challenging \emph{Web} split. 
CoreEn is the average of both the zero-shot and one-shot tasks.

Finally, we have included GSM8K \citep{DBLP:journals/corr/abs-2110-14168} and MMLU \citep{DBLP:conf/iclr/HendrycksBBZMSS21,DBLP:conf/iclr/HendrycksBBC0SS21}. GSM8K is used in an 8-shot chain of thought setting, following \llama \citep{DBLP:journals/corr/abs-2302-13971,DBLP:journals/corr/abs-2307-09288,DBLP:journals/corr/abs-2407-21783}. MMLU is used in a five-shot setting. 
These perform near-random for most of the models, and only show a slightly upwards trend when decreasing student/teacher loss. 
This is due to the use of the C4 dataset in training, and we note that we do not aim for competitive downstream evaluation results.

All models are evaluated using an internal version of the open-source \texttt{lm-evaluation-harness} \citep{eval-harness}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.99\textwidth]{plots/downstream_evals_all.pdf}
	\caption{\textbf{All student downstream evaluations.} For a discussion of the individual metrics and datasets, see \Cref{ssec:downstream-evaluations}.
	}
	\label{fig:downstream-evals-all}
\end{figure}

\FloatBarrier
\input{body/05_04_additional_results/distillation_scaling_law_experiments}

\FloatBarrier
\subsection{Weak-to-strong generalization}
\label{ssec:weak-to-strong-generalization}

In \Cref{fig:distillation-fixedm-teacher-varydata-student}
we see that weak-to-strong generalization \citep{DBLP:conf/icml/BurnsIKBGACEJLS24,DBLP:journals/corr/abs-2410-18837} occurs \emph{only in the finite distillation data regime},
and when the number of tokens is sufficiently large, the student cross-entropy increases again, eventually matching the teacher cross-entropy.
This can be understood in the following way: i) when the student is larger than the teacher, the student contains in its hypothesis space the function represented by the teacher, ii)
when the student is shown the teacher outputs on enough of the data manifold, it
eventually matches what the teacher does on the whole data manifold.
We note this doesn't explain how and why the student outperforms its teacher, and only constrains its asymptotic (low and high distillation data) behaviors.
\begin{figure}[h]
	\centering
    \includegraphics[width=0.45\textwidth]{plots/fixedm_teacher_varydata_student.pdf}
	\caption{\textbf{Fixed $\bf M$-Ratio Teacher varying student data.} We look at \emph{strong to weak} generalization (left) and \emph{weak to strong} (right) distillation, varying distillation tokens $D_S\in[8B,512B]$.
	}
	\label{fig:distillation-fixedm-teacher-varydata-student}
\end{figure}

\FloatBarrier
\input{body/05_04_additional_results/model_calibration}
