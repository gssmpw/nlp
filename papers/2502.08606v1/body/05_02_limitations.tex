\section{Limitations}
\label{sec:limitations}
This work has several limitations that we are aware of:
\begin{itemize}
    \item Our work is performed in the language modeling setting only. 
    Although there is good evidence that the functional form of scaling laws applies across domains \citep{DBLP:journals/corr/abs-2010-14701},
    we cannot be absolutely certain that distillation behaves in the way we describe in this work in all domains.
    \item We perform our analysis on the English subset of C4 dataset (see \Cref{sec:model-architecture}).
    This means that for our larger token runs, data has been repeated.
    Although it was shown in \citet{DBLP:conf/nips/MuennighoffRBST23} that on the C4 dataset, repeating data up to $4$ times has negligible impact to loss compared to having unique data,
    this was shown in the supervised setting,
    and we cannot be absolutely certain that the same applies in the distillation setting.
    \item A second downside of using the C4 dataset is that we are limited in our ability to analyze downstream evaluations of students resulting from distillation. Our performance over standard English language downstream tasks closely follows cross-entropy, however, C4 is not as well suited for pretraining in order to probe aspects like reasoning performance (see \Cref{ssec:downstream-evaluations}).
    \item We focused on distillation as originally defined in \citet{DBLP:journals/corr/HintonVD15},
    where the teacher produces a full probability distribution for the student to target.
    More colloquially, \emph{distillation} has become 
    used to describe the more general process of using a teacher in order to produce a student.
    One popular approach for training language models is \emph{Sequence-Level Knowledge Distillation} \citep{DBLP:conf/emnlp/KimR16}
    where the teacher is sampled, e.g. with beam search, in order to produce sequences 
    for training the student on in a supervised way.
    This technique,
    also called \emph{synthetic data} or \emph{hard distillation} has been employed to great effect in the \llama families 
    \citep{DBLP:journals/corr/abs-2302-13971} and most recently, 
    the smaller models distilled from DeepSeek-R1 \citep{DBLP:journals/corr/abs-2412-19437}.
    While we anticipate that our broader findings also apply in the Sequence-Level Knowledge Distillation, we cannot be absolutely sure.
    We suggest that verifying the scaling properties of Sequence-Level Knowledge Distillation
    in a controlled, resource constrained manner as we have done here is important for future study.
    \end{itemize}
