\subsection{Alternative approximation for FLOPs per token as a function of \texorpdfstring{$N$}{N}}
\label{ssec:alternative-approximation-for-flops-per-token-as-a-function-of-n}

From \Cref{tab:param-count-simple} and \Cref{eq:non-embedding-parameters}
and
\Cref{tab:flops-count-simple}
we can read our approximate values for non-embedding parameters and total compute (dropping contributions from normalization layers) as\footnote{It was shown in \citet{DBLP:journals/corr/abs-2406-19146} that ignoring the embedding parameters and \flops can lead to systematic estimation bias for small models, and is one of the primary drivers between different exponents reported in \citet{DBLP:journals/corr/abs-2001-08361} and \citet{DBLP:journals/corr/abs-2203-15556}.
We find that the the \emph{non-embedding parameters} gives a tighter scaling behavior.
However, in the \emph{fixed-aspect-ratio} setting, we are able to use both the
\emph{non-embedding parameters} in the scaling law \emph{and} the \emph{approximate} total compute simultaneously, removing estimation bias.
Indeed, in the supervised setting, our coefficients $a$ and $b$ are consistent with those from \citet{DBLP:journals/corr/abs-2203-15556} (see \Cref{tab:scaling-law-parameter-estimates}).}
\begin{align}
	N                & =\nlayers\dmodel^2\left(2+ \frac2\gsize + \nffn\rffn\right) \\
	C_\text{Forward} & =
	2\nlayers\dmodel^2\left(2+ \frac2\gsize + \nffn\rffn\right)
	+2\nlayers\nctx\dmodel                                                         \\
	                 & =
	2N
	+2\nlayers\nctx\dmodel
    +2\nvocab\dmodel.
	\label{eq:c-forward-as-n}
\end{align}
Typically the term $2\nlayers\nctx\dmodel$ would be dropped, and the embedding parameters included into the total parameters \citep{DBLP:journals/corr/abs-2203-15556}
or discarded \citep{DBLP:journals/corr/abs-2001-08361}
yielding the expression $C_\text{Forward}$ and the familiar expression $C=6ND$ \citep{DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2203-15556}.
For our investigation we are interested in small, capable models, which may have a large context,
and so both of these terms cannot be ignored in general at the peril of making a systematic error in the region of configuration space we are most interested in.
Fortunately, we will see that our choice of \emph{fixed aspect ratio} $\rmodel=\dmodel/\nlayers$ architectures allows us a simple to use, more precise estimate.
The trick will be to use this fixed aspect ratio to come up with an approximation for $\nlayers$ and $\dmodel$ as a function of $N$ and $\rmodel$.
With these approximated, the term $2\nlayers\nctx\dmodel$ can be represented as a function of $N$.
First define\footnote{In our setting (\Cref{sec:model-architecture})
	$\omega$ takes values
	\begin{align}
		\omega
		 & =2+ \frac2\gsize + \nffn\rffn
		=2+ \frac21 + 3\times\frac83=12.
	\end{align}}
\begin{equation}
	\omega \equiv	2+ \frac2\gsize + \nffn\rffn
\end{equation}
so that
\begin{align}
	N
	 & =\nlayers\dmodel^2\omega.
\end{align}
Then we can substitute in $\rmodel\equiv\dmodel/\nlayers$ so that
\begin{align}
	N
	 & =\nlayers\dmodel^2\omega
	=\nlayers^3\rmodel^2\omega,
\end{align}
and solve for $\nlayers$ and $\dmodel$
\begin{align}
	\nlayers &= \left(\frac{N}{\rmodel^2\omega}\right)^{1/3},
    &
    \dmodel &= \left(\frac{N\rmodel}{\omega}\right)^{1/3},
\end{align}
The $C_\text{Forward}$ term can then be represented as a function of $N$.
The context-dependent term becomes
\begin{align}
	2\nctx\nlayers\dmodel
	 & =
	2\nctx\nlayers^2\rmodel
	=
	2\left(\frac{N}{\rmodel^2\omega}\right)^{2/3}\rmodel\nctx
	\equiv
    2\nctx\sigma_1 N^{2/3}
\end{align}
where
\begin{equation}
	\sigma_1
	=\left(\frac{1}{\rmodel^2\omega}\right)^{2/3}\rmodel
	=\left(\frac{1}{\rmodel\omega^2}\right)^{1/3}.
\end{equation}
The vocabulary projection term becomes
\begin{equation}
    2\nvocab\dmodel
    =2\nvocab\left(\frac{N\rmodel}{\omega}\right)^{1/3}
    =2\nvocab\left(\frac{\rmodel}{\omega}\right)^{1/3}N^{1/3}
    \equiv 2\nvocab\sigma_2N^{1/3},
\end{equation}
where
\begin{equation}
	\sigma_2
	=\left(\frac{\rmodel}{\omega}\right)^{1/3}.
\end{equation}
In total
\begin{equation}
	C_\text{Forward}
	=
	2N
    + 2\nctx\sigma_1 N^{2/3}
    + 2\nvocab\sigma_2 N^{1/3}
    =
    2N\left(1+\sigma_1 \frac{\nctx}{N^{1/3}} + \sigma_2 \frac{\nvocab}{N^{2/3}}\right),
    \label{eq:forward-flops-sigma}
\end{equation}
where $\sigma_1$ and $\sigma_2$ are independent of model and context size.
In the large $N$ limit, or the small $\nctx$ small $\nvocab$ limit this becomes the familiar $C_\text{Forward} = 2N$.
The backward FLOPS per token is taken as twice the forward FLOPs \citep{DBLP:journals/corr/abs-2403-14606}
\begin{equation}
	C_\text{Backward} = 2\,C_\text{Forward}.
\end{equation}

Given the simplicity of the compute expression as a function of $N$,
the better tightness of fit in the scaling law,
the improved intuition that the model size more directly corresponds to \emph{work being done by the model}, 
and the predictability of hyperparameters at larger scales,
we recommend the scaling law community consider adopting fixed aspect ratio models.






