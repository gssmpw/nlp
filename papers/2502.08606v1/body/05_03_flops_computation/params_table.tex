\begin{table}[h]
	\caption{Parameter counts for embedding projector, a single transformer layer, final normalization and output layer. \emph{Ours} indicates the expressions we use in the paper for the total number of parameters (note that the quantity $N$ that appears in our scaling laws is the number of \emph{non-embedding parameters}, but still includes parameters associated with normalization layers). \emph{Approx.} indicates taking the within-section total and dropping all terms that are not at least quadratic in one of $\dmodel,\nvocab$, and will be used for estimating the FLOPs per token from a given model size (\Cref{ssec:alternative-approximation-for-flops-per-token-as-a-function-of-n}), and does not differ significantly from the number of non-embedding parameters.}
	\label{tab:param-count}
	\centering
	\resizebox{1.0\textwidth}{!}{
		\begin{tabular}{lcccc}
			\toprule
			Parameters   & \cite{DBLP:journals/corr/abs-2001-08361} & \cite{DBLP:journals/corr/abs-2203-15556} & \cite{DBLP:conf/sc/NarayananSCLPKV21}                & Ours (Total)                                           \\ \midrule
			Embedding    & $(\nvocab+\nctx)\dmodel$                 & $(\nvocab+\nctx) \dmodel$                & $(\nvocab+\nctx) \dmodel$                            & $\nvocab \dmodel$                                      \\ \midrule
			\multicolumn{5}{l}{\emph{Attention (one transformer layer)}}                                                                                                                                                       \\ \midrule
			PreNorm      & ---                                      & ---                                      & $2 \dmodel$                                          & $\dmodel$                                              \\
			QKNorm       & ---                                      & ---                                      & ---                                                  & $2\dhead$                                              \\
			QKV          & $3 \nheads \dmodel \dhead$               & $3 \nheads \dmodel \dhead$               & $3 \nheads (\dmodel + 1) \dhead$                     & $ (\nheads+ 2\nkvheads) \dmodel \dhead$                \\
			Project      & $\nheads \dhead \dmodel$                 & $\nheads \dhead \dmodel$                 & $(\nheads \dhead + 1)\dmodel$                        & $\nheads \dhead \dmodel$                               \\
			Total        & $4\nheads \dhead \dmodel$                & $4\nheads \dhead \dmodel$                & $4\nheads \dhead \dmodel + 3(\nheads\dhead+\dmodel)$ & $2(\nheads +\nkvheads) \dhead \dmodel+2\dhead+\dmodel$ \\
			Approx.      & $4\nheads \dhead \dmodel$                & $4\nheads \dhead \dmodel$                & $4\nheads \dhead \dmodel + 3(\nheads\dhead+\dmodel)$ & $2(\nheads +\nkvheads) \dhead \dmodel$                 \\
			\midrule
			\multicolumn{5}{l}{\emph{Feed-forward (one transformer layer)}}                                                                                                                                                    \\ \midrule
			PreNorm      & ---                                      & ---                                      & $2 \dmodel$                                          & $\dmodel$                                              \\
			MLP          & $2 \dmodel \dffn$                        & $2 \dmodel \dffn$                        & $2\dmodel \dffn + \dffn + \dmodel$                   & $\nffn \dmodel \dffn$                                  \\
			Total        & $2 \dmodel \dffn$                        & $2 \dmodel \dffn$                        & $2\dmodel \dffn + \dffn + 3\dmodel$                  & $\nffn \dmodel \dffn+\dmodel$                          \\
			Approx.      & $2 \dmodel \dffn$                        & $2 \dmodel \dffn$                        & $2\dmodel \dffn + \dffn + 3\dmodel$                  & $\nffn \dmodel \dffn$                                  \\

			\midrule
			OutputNorm   & ---                                      & ---                                      & ---                                                  & $\dmodel$                                              \\
			Final logits & ---                                      & ---                                      & ---                                                  & ---                                                    \\
			\bottomrule
		\end{tabular}
	}
\end{table}

\begin{table}[h]
	\caption{Parameter counts displayed in \Cref{tab:param-count} using simplified notation $\nheads\dhead=\dmodel$, $\dffn=\rffn \dmodel$, and $\nheads=\gsize \nkvheads$.}
	\label{tab:param-count-simple}
	\centering
	\resizebox{1.0\textwidth}{!}{
		\begin{tabular}{lcccc}
			\toprule
			Parameters   & \cite{DBLP:journals/corr/abs-2001-08361} & \cite{DBLP:journals/corr/abs-2203-15556} & \cite{DBLP:conf/sc/NarayananSCLPKV21} & Ours (Total)                              \\ \midrule
			Embedding    & $(\nvocab+\nctx)\dmodel$                 & $(\nvocab+\nctx) \dmodel$                & $(\nvocab+\nctx) \dmodel$             & $\nvocab \dmodel$                         \\ \midrule
			\multicolumn{5}{l}{\emph{Attention (one transformer layer)}}                                                                                                                           \\ \midrule
			PreNorm      & ---                                      & ---                                      & $2 \dmodel$                           & $\dmodel$                                 \\
			QKNorm       & ---                                      & ---                                      & ---                                   & $2\dhead$                                 \\
			QKV          & $3 \dmodel^2$                            & $3 \dmodel^2$                            & $3 (\dmodel^2 + \dmodel)$             & $ (1+ 2/\gsize) \dmodel^2$                \\
			Project      & $\dmodel^2$                              & $ \dmodel^2$                             & $\dmodel^2 +\dmodel$                  & $\dmodel^2$                               \\
			Total        & $4\dmodel^2$                             & $4 \dmodel^2$                            & $4 \dmodel^2+6\dmodel$                & $2(1+ 1/\gsize)\dmodel^2+2\dhead+\dmodel$ \\
			Approx.      & $4\dmodel^2$                             & $4 \dmodel^2$                            & $4 \dmodel^2+6\dmodel$                & $2(1+ 1/\gsize)\dmodel^2$                 \\
			\midrule
			\multicolumn{5}{l}{\emph{Feed-forward (one transformer layer)}}                                                                                                                        \\ \midrule
			PreNorm      & ---                                      & ---                                      & $2 \dmodel$                           & $\dmodel$                                 \\
			MLP          & $2 \rffn \dmodel^2$                      & $2 \rffn \dmodel^2$                      & $2\rffn \dmodel^2 + (1+\rffn)\dmodel$ & $\nffn \rffn \dmodel^2$                   \\
			Total        & $2 \rffn \dmodel^2$                      & $2 \rffn \dmodel^2$                      & $2\rffn \dmodel^2 + (3+\rffn)\dmodel$ & $\nffn \rffn\dmodel^2+\dmodel$            \\
			Approx.      & $2 \rffn \dmodel^2$                      & $2 \rffn \dmodel^2$                      & $2\rffn \dmodel^2 + (3+\rffn)\dmodel$ & $\nffn \rffn\dmodel^2$                    \\

			\midrule
			OutputNorm   & ---                                      & ---                                      & ---                                   & $\dmodel$                                 \\
			Final logits & ---                                      & ---                                      & ---                                   & ---                                       \\
			\bottomrule
		\end{tabular}
	}
\end{table}
