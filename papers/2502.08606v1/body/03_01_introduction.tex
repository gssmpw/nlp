\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{plots/distillation_scatter.pdf}
        \vspace{-0.1cm}
	\caption{\textbf{Extrapolations of the Distillation Scaling Law.}
		The distillation scaling law (\Cref{eq:distillation-scaling-law}) is fitted on \emph{weak} students $(L_S > 2.3)$ for a range of teachers with losses $L_T$. Solid lines represent predicted model behavior for unseen teachers for a given student configuration (interpolation), and dashed lines represent predicted model behavior outside of seen teachers and for the \emph{strong} student region $(L_S \leq 2.3)$.
        As shown, the student can outperform the teacher (see \Cref{fig:fixedm-teacher-isoflop-students,fig:isoflop-teacher-fixedm-students,fig:distillation-fixedm-teacher-varydata-student} for details).
	}
        \vspace{-0.1cm}
	\label{fig:distillation-scaling-law-fig1}
\end{figure}

\section{Introduction}
\label{sec:introduction}

The study of scaling laws \citep{DBLP:journals/corr/abs-1712-00409,DBLP:conf/iclr/RosenfeldRBS20,DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2203-15556}
revealed that
previously trained \glspl{lm} could have been more capable if they had followed a \emph{compute optimal} training paradigm, which determines the model size and the number of training tokens that give the best performing model under a given compute budget.
Many subsequent works have followed compute optimal training \citep{DBLP:journals/corr/abs-2304-03208,DBLP:conf/nips/MuennighoffRBST23}.

The size of compute optimal models grows with compute \citep{DBLP:journals/corr/abs-2203-15556},
which makes them challenging to use due to the  \emph{growth in inference costs}.
In practice, this means compute optimal models are slow, expensive to serve, consume more battery life, provide high barriers to entry for academic study, and have a significant carbon footprint.
With inference volume up to billions of tokens per day \citep{openai_blog_gpt3},
the inference cost of an \gls{lm} is typically significantly larger than its pretraining cost  \citep{DBLP:conf/hotcarbon/ChienLNRSW23,DBLP:journals/micro/WuARH24}
and is going to further increase in an era of test-time compute scaling
\citep{DBLP:journals/corr/abs-2408-03314,DBLP:journals/corr/abs-2407-21787,DBLP:journals/corr/abs-2408-00724}.

Unsustainable inference costs have led to an alternative training paradigm, \emph{overtraining} \citep{DBLP:journals/corr/abs-2403-08540},
where the amount of training data used is much greater than in the compute optimal case,
enabling \emph{small, capable models}.
Overtrained models better satisfy compute optimality when compute is measured over a model's lifetime, rather than just the pretraining cost \citep{DBLP:conf/icml/SardanaPDF24}.
As supervised scaling laws follow power laws in model size and training data, diminishing returns in performance occur much sooner than in the compute-optimal case. To achieve reasonable capabilities, these models need to be trained on many trillions of tokens,
\citep{DBLP:journals/corr/abs-2408-03314,DBLP:journals/corr/abs-2407-21787,DBLP:journals/corr/abs-2408-00724}, which is expensive and time-consuming.

We seek models that match the performance of small overtrained models but at lower training cost. 
A popular candidate is \emph{distillation} \citep{DBLP:journals/corr/HintonVD15}, where a capable \emph{teacher} \gls{lm} produces targets for a smaller \emph{student} \gls{lm}.
When distillation is used for \gls{lm} pretraining, we will call this \emph{distillation pretraining}.
There are many explanations for \emph{why} distillation works,
from \emph{dark knowledge transfer},
where information is contained in the ratio of probabilities of incorrect classes \citep{DBLP:journals/corr/HintonVD15},
to being a form of regularization \citep{DBLP:conf/nips/MobahiFB20},
or reducing noise in the learning process \citep{DBLP:journals/corr/abs-2005-10419}, among many other explanations.
Despite a lack of consensus for why distillation works,
distillation pretraining has produced more capable models than supervised pretraining in the Gemma and Gemini \citep{DBLP:journals/corr/abs-2408-00118}, Minitron \citep{DBLP:journals/corr/abs-2407-14679,DBLP:journals/corr/abs-2408-11796} and AFM  \citep{DBLP:journals/corr/abs-2407-21075} families of \glspl{lm} in terms of both pretraining loss and downstream evaluations.
Yet, at the same time, \citet{DBLP:conf/icml/Liu0ILTFXCSKLC24} reported that distillation produces \emph{less} capable models than supervised pretraining does.

With such significant compute resources being devoted to distillation pretraining of \glspl{lm},
it is \emph{essential} to understand how to correctly allocate these resources, to produce the most capable models possible,
and to have an understanding if any gains are even possible
compared to supervised pretraining when both methods have access to the same resources \citep{DBLP:journals/corr/abs-2110-12894}.

To close this knowledge gap,
we perform an extensive controlled study of distillation,
with students and teachers ranging from 143M to 12.6B parameters,
trained on data of a few billion tokens, up to 512B tokens.
These experiments result in our \emph{distillation scaling law},
which estimates student performance as a function of resources (the teacher, the student size, and the amount of data used for distillation),
resolving questions about when distillation \emph{is} and \emph{is not} effective in terms of producing models of a desired capability under resource constraints of interest.
We find:
\begin{enumerate}
    \item The cross entropy of a student of size $N_S$ distilled on $D_S$ tokens from a teacher of size $N_T$ trained on $D_T$ tokens can be predicted using our distillation scaling law (\Cref{eq:distillation-scaling-law}).
    \item The teacher size $N_T$ and number of teacher training tokens $D_T$ determines the student cross-entropy \emph{only} through their determination of the teacher's cross-entropy $L_T=L_T(N_T,D_T)$ (\Cref{fig:isoflop-teacher-fixedm-students-teacher-loss}).
        \item The influence of the teacher cross-entropy upon the student loss follows a power law which transitions between two behaviors depending on the relative learning capacities of student and the teacher, reflecting a phenomenon in distillation called the \emph{capacity gap},
        where a stronger teacher produces a \emph{worse} student.
        Our parameterization resolves outstanding questions about the capacity gap, showing that it is a gap in learning capacity (both hypothesis space and ability to optimize) between the teacher and student, and not only about their relative sizes, which is a special case.
\end{enumerate}
Our results show that \emph{distillation can not produce lower model cross-entropies than supervised learning} when both learning processes are given enough data or compute.
However, distillation \emph{is more efficient} than supervised learning \emph{if both of the following are true}:
\begin{enumerate}
    \item The total compute or tokens used for the student is not larger than student size-dependent threshold given by our scaling law (\Cref{ssec:fixed-tokens-or-compute-main}).
    \item A teacher already exists, or the teacher to be trained has uses beyond a single distillation (\Cref{ssec:compute-optimal-distillation}).
\end{enumerate}


We hope the laws and analyses we provide will guide the community to produce even more capable models with lower inference cost and lower lifetime compute costs.



