\subsection{Kernel Regression}
\label{ssec:kernel-regression}
One of our main contributions is that the student loss follows a broken power law, where the transition between the two power law regions 
occur when the student becomes a stronger learner than the
teacher (\Cref{eq:distillation-scaling-law}). 
This implies that making the teacher too capable (relative to the student)
reduces student performance.
In this section we show how a capacity gap provably degrades student performance in the setting of kernel regression. 
While simple, we believe the underlying principle causing the student performance degradation in this case carry over to much more general settings involving neural networks.
\subsubsection{Setup}
\label{sssec:setup}
Let $\mathcal{H}$ denote a Hilbert space spanned by orthonormal bases functions 
$\{\phi_i\}_{i=1}^\infty$ 
such that $\langle \phi_i,\phi_j \rangle_{\mathcal{H}} = \delta_{ij}$. 
Let $f^* \in \mathcal{H}$ denote the \emph{target function}, identified by a set of coefficients $\bm{\alpha} = \{\alpha_i\}_{i=1}^\infty \in \R,~\|\alpha\| = M<\infty$ such that:
\begin{align}
	f^\star(x) = \sum_{i=1}^\infty \alpha_i\phi_i(x).
\end{align}
Let $\mathcal{H}_t^m,\mathcal{H}_s^n$ denote the teacher and student Hilbert spaces respectively:
\begin{align}
	\mathcal{H}_t^m & = \text{Span}\{\phi_1,\phi_2,...,\phi_m\}, \\
	\mathcal{H}_s^n & = \text{Span}\{\phi_1,\phi_2,...,\phi_n\},
\end{align}
which are the hypothesis spaces of the teacher and student.
Note that while the Hilbert space $\mathcal{H}$ is spanned by an infinite orthonormal basis, the teacher and student spaces are \emph{finite} and spanned by $m$ and $n$ basis functions respectively, where $|m-n|$ represents the teacher and student capacity gap. 

The process of training the teacher and student models involves solving the following constrained optimization problems:
\begin{align}
	g^\star & = \min_{g \in \mathcal{H}_t^m}\|g - f^\star\|_\mathcal{H} ~~~\text{s.t}~~~ \|g\|_\mathcal{H} \leq T, \\
	h^\star & = \min_{h \in \mathcal{H}_s^n}\|h - g^\star\|_\mathcal{H} ~~~\text{s.t}~~~ \|h\|_\mathcal{H} \leq D,
\end{align}
where $g^\star, h^\star$ are the optimal teacher and student respectively, and $D\leq T<M$. Note that we assume the teacher and student are exposed to an infinite amount of training data, hence our analysis is carried over entirely in function space.

\begin{lemma}\label{lem:teacher}
	The optimal teacher $g^\star$ is given by:
	\begin{align}\label{eqn:c}
		g^\star(x) = C(m,T)\sum_{i=1}^m\alpha_i \phi_i(x),~~~C(m,T) = \begin{cases}
			                                                              1                                       & \sqrt{\sum_{i=1}^m\alpha_i^2}\leq T \\
			                                                              \frac{T}{\sqrt{\sum_{i=1}^m\alpha_i^2}} & \text{otherwise.}
		                                                              \end{cases}
	\end{align}
	The teacher error $e^\star_\text{teacher}(m,T)$ is given by:
	\begin{align}
		e^\star_\text{teacher}(m,T) = \|g^\star - f^\star\|_\mathcal{H} = \sqrt{(C(m,T) - 1)^2\sum_{i=1}^m\alpha_i^2+ \sum_{i=m+1}^\infty \alpha_i^2}.
	\end{align}
	\begin{proof}
		By construction we may assume the teacher model takes the form $g^\star = \sum_{i=1}^m \beta_i \phi_i$.
		where $\sqrt{\sum_{i=1}^m\beta_i^2} \leq T$. We can write the error of $g^\star$ using:
		\begin{align}
			e_\text{teacher}(m,T, \bm{\beta}) & = \Big\|\big(\sum_{i=1}^m(\beta_i - \alpha_i)\phi_i + \sum_{i=m+1}^\infty \alpha_i\phi_i\Big\|_\mathcal{H}  = \sqrt{\sum_{i=1}^m(\beta_i - \alpha_i)^2 + \sum_{i=m+1}^\infty \alpha_i^2 }.\label{eqn:min}
		\end{align}
		Note that the minimizing coefficients $\bm{\beta}^\star$ of \cref{eqn:min} must take the form $\bm{\beta} = C\bm{\alpha}$ for some coefficient $C$. Considering the norm constraint on $g$, the constant $C$ takes the form in \cref{eqn:c}. Plugging the resulting $g^\star$ into the expression for $e_\text{teacher}(m,T, \bm{\beta}^\star)$ completes the proof.
	\end{proof}
\end{lemma}

Notably and intuitively, the teacher error decreases monotonically as $m$, which represents the teacher model capacity, increases.
\subsubsection{Distilling the Teacher}
\label{sssec:distilling-the-teacher}

We now pick our student function $h^\star$ by mimicking the teacher subject to a norm constraint:
\begin{align}
	h^\star(x) = \min_{h \in \mathcal{H}_t^n}\|h - g^\star\|_\mathcal{H} ~~~\text{s.t.}~~~ \|h\|_\mathcal{H} \leq D.
\end{align}
\begin{lemma}\label{lem:student}
	Let $k = \min(m,n)$ be the smaller of the teacher and student capacities. The optimal student $h^\star$ is given by:
	\begin{align}
		h^\star    & = Q(m,k,T,D)C(m,T)\sum_{i=1}^k\alpha_i\phi_i            \\
		Q(m,k,T,D) & = \begin{cases}
			               1                                             & C(m,T)\sqrt{\sum_{i=1}^k\alpha_i^2}<D \\
			               \frac{D}{C(m,T)\sqrt{\sum_{i=1}^k\alpha_i^2}} & \text{otherwise.}
		               \end{cases} \label{eqn:q}
	\end{align}
	The student error with respect to the target function is then:
	\begin{align}
		e_\text{student}(m,n,T,D) = \|h^\star - f^\star\|_\mathcal{H} = \sqrt{(C(m,T)Q(m,k,T,D) - 1)^2\sum_{i=1}^k\alpha_i^2+ \sum_{i=k+1}^\infty \alpha_i^2}
	\end{align}
	\begin{proof}
		The proof follows the exact same logic as in \cref{lem:teacher}. i.e, we can assume the optimal student is given by $h^\star = \sum_{i=1}^n \gamma_i \phi_i$. From the distillation loss, the optimal coefficients must match the teacher coefficients for the basis functions $\{\phi_i\}_{i=1}^n$, perhaps rescaled due to the norm constraint $\sqrt{\sum_{i=1}^n \gamma_i^2}\leq D$. This rescaling then gives rise to the additional $Q(m,k,T,D)$ multiplier in \cref{eqn:q}.
	\end{proof}
\end{lemma}

\subsubsection{U-shape in the student error}
\label{sssec:u-shape-in-the-student-error}

We will prove that the map
\[
	m \;\longmapsto\; e_{\text{student}}(m,n,T,D)
\]
is comprised of two distinct segments: i) where the student error monotonically decreases for $m < n$, and ii) where it monotonically increases for $m \geq n$, establishing a U-shape in the student error echoing the trend seen in \cref{fig:isoflop-teacher-fixedm-students,fig:fixedm-teacher-fixedm-students}.

\medskip

\noindent
\textbf{Case 1: $m < n$. \, (Student error is non-increasing in $\bm m$)}
\smallskip

\noindent
\emph{Claim.} For $1 \le m < n$, we have
\[
	e_{\text{student}}(m+1,n,T,D)
	\;\;\le\;\;
	e_{\text{student}}(m,n,T,D). \label{claim:1}
\]
In words, when $m < n$, the error does not increase (and typically decreases) as the teacher capacity $m$ increases.

\noindent
\emph{Proof.}

Let $\mathcal{H}_t^{m,T} \subseteq \mathcal{H}_t^{m}$ denote the space of functions in $\mathcal{H}_t^{m}$ that are norm constrained by $D$. i.e:
\begin{align}
	\mathcal{H}_t^{m,T} = \{f \in \mathcal{H}_t^{m} ~:~\|f\|_\mathcal{H}\leq T\}.
\end{align}
Since $\mathcal{H}_t^{m,T} \subseteq \mathcal{H}_t^{m+1,T}$, it follows that $g^\star_m \in \mathcal{H}_t^{m+1,T}$, which implies that the teacher error cannot increase as $m$ increases, hence it monotonically decreases. Now, let $h^\star_m$ denote the optimal student given the teacher $g^\star_m$. Since $D\leq T$, then for any $m < n$, we can equivalently write the optimal student $h^\star_m$ as the solution to the following optimization problem:
\begin{align}
	\forall_{m \leq n}~h^{\star}_m & = \min_{h \in \mathcal{H}_s^n}\|h - g^\star_m\|_\mathcal{H} ~~~\text{s.t}~~~ \|h\|_\mathcal{H} \leq D \\
	                               & = \min_{h \in \mathcal{H}_t^m}\|h - f^\star\|_\mathcal{H} ~~~\text{s.t}~~~ \|h\|_\mathcal{H} \leq D,
\end{align}
which corresponds exactly to the objective of finding the optimal teacher with with a norm constraint set to $D$. Therefore, from the fact that the teacher error monotonically decreases we can conclude that the student error monotonically decreases as well in the regime $m< n$.


\medskip

\noindent
\textbf{Case 2: $m \geq n$. \, (Student error eventually increases in $\bm m$)}

\smallskip

\noindent
\emph{Claim.} For $m \geq n$:
\[
	e_{\text{student}}(m+1,n,T,D)
	\;\;\ge\;\;
	e_{\text{student}}(m,n,T,D).
\]
Hence once $m$ exceeds $n$ the student error cannot decrease any further, the error eventually starts to rise.

\noindent
\emph{Proof.}

Let $\bm{\beta}^\star_m = \{\beta_1,...,\beta_m\}$ denote the coefficients of the optimal teacher $g^\star_m$. Note that in the regime $m\geq n$, as long as $\sqrt{\sum_{i=1}^n\beta_i^2}\geq D$ (i.e the norm of the coefficients corresponding to the basis $\{\phi_1,...,\phi_n\}$ is smaller than $D$), we have from \cref{eqn:q} that $Q(m,k,T,D) = 1$, which means that the optimal student doesnt change, hence its error remains constant. If however $\sqrt{\sum_{i=1}^n\beta_i^2}<D$, then we have from \cref{eqn:q}:
\begin{align}
	1 > Q(m,k,T,D) \geq Q(m+1,k,T,D),
\end{align}
where the second inequality becomes strict if $\alpha_{m+1}^2>0$. A strict inequality (i.e $Q(m,k,T,D) > Q(m+1,k,T,D)$) implies the optimal student is further scaled down due to the teacher having to "spread its capacity" to additional basis functions that are not learnable by the student, thereby strictly increasing its error.
Hence for $m\geq n$, we get
\[
	e_{\text{student}}(m+1,n,T,D)
	\;\;\ge\;\;
	e_{\text{student}}(m,n,T,D),
\]
demonstrating that the error increases monotonically with $m$ once $m\geq n$.
\qed

\medskip

\noindent
\textbf{Conclusion (U-shaped trend).} Combining these two cases:
\[
	\begin{cases}
		\text{For }1 \le m < n: &
		e_{\text{student}}(m,n,T,D)\text{ monotonically decreasing in }m, \\[4pt]
		\text{For }m\geq n:     &
		e_{\text{student}}(m,n,T,D)\text{ monotonically increasing in }m.
	\end{cases}
\]
Therefore, as a function of $m$, the student error $e_{\text{student}}(m,n,T,D)$
\emph{first decreases} and \emph{then increases} (for $m\geq n$) (for $m\le n$), giving a u-shape in student error due to a capacity gap between the teacher and the student.
\hfill\qedsymbol

We present an empirical verification of these conclusions in \Cref{fig:kernel}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.3\textwidth]{plots/kernel_error_v2.pdf}
	\caption{\textbf{Distillation in kernel regression.} We randomly sample the $\bf{\alpha} = \{\alpha_1,...,\alpha_{1000}\}$ coefficients of the target function uniformly in the range $[-1,1]$. We fix $T = 5, D = 4.5$ and compute the optimal student and teacher errors according to \cref{lem:teacher,lem:student} for various values of $n$ (dashed curves), and for $m \in [1...1000]$. As can be seen, the student error exhibits a U shaped error curve as predicted by the theory, where the error starts to increase when $m\geq n$. The black solid line indicates the teacher error, which always decreases with increasing $m$.}
    \label{fig:kernel}
\end{figure}
The above theoretical analysis points to an intuitive interpretation of the potentially adverse effect of a large teacher-student capacity gap; the degradation in student performance is due to the teacher learning basis functions that are unreachable by the student, at the expense of basis functions that are reachable by the student. In the following we provide empirical evidence in support of this picture in a controlled yet more realistic setting.
