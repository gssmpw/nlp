\clearpage
\subsection{Teachers used in distillation}
\label{ssec:teachers-used-in-distillation}

In \Cref{fig:supervised-models} we show the cross-entropies of the models used as teachers in \Cref{ssec:distillation-scaling-law-experiments},
and for fitting the supervised scaling law:
i) eleven of fixed-$M$ ratio models following the Chinchilla rule of thumb $D/N=M^*\approx20$ \citep{DBLP:journals/corr/abs-2203-15556},
ii) six models on $D=512B$ tokens (\Cref{fig:supervised-fixed-long}), and
iii) four IsoFLOP profiles (\Cref{fig:sub-supervised-isoflops}).
Together this produces 74 runs corresponding to tuples of $(N,D,L)$. 


\begin{figure}[h]
	\centering
        \vspace{-0.7cm}
        \subfloat[Fixed-$M$ and 512B Teachers.]{
		\includegraphics[width=0.305\textwidth]{plots/fixedm_long_teachers.pdf}
		\label{fig:supervised-fixed-long}
	}
        \hfill
	\subfloat[Supervised IsoFLOPs.]{
		\includegraphics[width=0.32\textwidth]{plots/isoflop_teachers.pdf}
		\label{fig:sub-supervised-isoflops}
	}
	\hfill
	\subfloat[Supervised IsoFLOP minima.]{
		\includegraphics[width=0.27\textwidth]{plots/isoflop_teachers_parabola.pdf}
		\label{fig:supervised-isoflop-minima}
	}
        \vspace{-0.1cm}
	\caption{\textbf{Supervised IsoFLOPs.}
        \textbf{(a)} The cross-entropy of supervised models trained with either a Chinchilla optimal $M=D/N\approx 20$ or on 512B tokens.
	\textbf{(b)} The cross-entropy supervised models trained with four ISOFLOP profiles $C\in\{3\times10^{19},10^{20},3\times10^{20},10^{21}\}$.
	\textbf{(c)} The optimal supervised parameters $N^*(C)=\argmin_{N} L(C)$ for each IsoFLOP profile, and the loss $L^*(C)$ achieved by that model.}
        \vspace{-0.1cm}
	\label{fig:supervised-models}
\end{figure}
Coefficient estimation (\Cref{ssec:supervised-scaling-law-coefficient-estimation})
yields the scaling coefficients shown in
\Cref{tab:scaling-law-parameter-estimates},
and a scaling law which has $\lesssim1\%$ relative prediction error, including when extrapolated from weaker to stronger models (see \Cref{fig:supervised-scaling-law}).

\FloatBarrier

\subsection{Fixed-\texorpdfstring{$M$}{M} teacher/fixed-\texorpdfstring{$M$}{M} students and the capacity gap}
\label{ssec:fixed-m-teacher-fixed-m-students}

\begin{figure}[h]
	\centering
        \vspace{-0.15cm}
		\includegraphics[width=0.67\textwidth]{plots/fixedm_teacher_fixedm_student_appendix.pdf}
        \vspace{-0.15cm}
	\caption{\textbf{Fixed $\bm M$ Teacher/Fixed $\bm M$ Student.} Students of three sizes trained with different $M_S=D_S/N_S=20$ ratios are distilled from teachers with $M_T=D_T/N_T\approx 20$.
	This is a more complete version of \Cref{fig:isoflop-teacher-fixedm-students}.}
        \vspace{-0.15cm}
	\label{fig:fixedm-teacher-fixedm-students-appendix}
\end{figure}

In \Cref{fig:fixedm-teacher-fixedm-students-appendix}, the \emph{capacity gap} in knowledge distillation can be seen.
Improving a teacher's performance does not always improve a student's, and even reduces the performance after a certain point.
The \gls{kld} between teacher and student is an increasing function of teacher size in all cases, which means as the teacher improves its own performance, the student finds the teacher more challenging to model, which eventually prevents the student from taking advantage of teacher gains.
See \Cref{sssec:198m-students-trained-on-20n-tokens} for an investigation using calibration to understand where this mismatch occurs.

\FloatBarrier
\subsection{Full distillation scaling law IsoFLOP profiles}
\label{ssec:distillation-isoflop-profiles}
In \Cref{fig:fixedm-teacher-isoflop-students-app} we provide the full six fixed $M$ Teacher/IsoFLOP Student profiles,
only two of which were shown in \Cref{fig:fixedm-teacher-isoflop-students}.
These experiments enable the reliable determination of $\alpha^\prime,\beta^\prime,\gamma^\prime,A^\prime$ and $B^\prime$.
In \Cref{fig:isoflop-teacher-fixedm-students-app}
we provide the full four IsoFLOP teacher/ fixed $M$ student,
only two of which were shown in \Cref{fig:isoflop-teacher-fixedm-students}.
These experiments enable the reliable determination of $c_0,c_1,f_1$ and $d_1$.

\paragraph{Strong-to-weak generalization occurs.} For the weaker teachers ($N_T\leq 2.72B$),
The horizontal dashed line in each pane shows the cross-entropy achieved by the teacher (\Cref{ssec:teachers-used-in-distillation}).
we see that for students larger than the teacher ($N_S>N_T$) and for sufficiently large compute budgets, 
\emph{the student is able to outperform the teacher}
(see \Cref{ssec:weak-to-strong-generalization} for a detailed one-dimensional slice).

\paragraph{A stronger teacher signal is needed in order for stronger students to outperfom the supervised baseline.}
The horizontal dashed line in each pane shows the cross-entropy achieved by the student if trained using supervised learning (\Cref{ssec:teachers-used-in-distillation}).
We see that weaker students benefit more from distillation, as e.g. the 198M student has all observed data below this dashed line, meaning all distillations outperform the supervised baseline.
However, for the 1.82B student, only $10^{21}$ FLOP teachers produce distilled students that outperform the supervised baseline.

\begin{figure}[h]
	\centering
    \vspace{-0.1cm}
	\subfloat[Fixed $\bm M$ Teacher/Student IsoFLOP profiles.]{
		\includegraphics[width=0.47\textwidth]{plots/fixedm_teacher_isoflop_students_app.pdf}
		\label{fig:fixedm-teacher-isoflop-students-app}
	}
	\subfloat[IsoFLOP Teacher/Fixed $\bm M$ Student profiles.]{
		\includegraphics[width=0.47\textwidth]{plots/isoflop_teacher_fixedm_students_app.pdf}
		\label{fig:isoflop-teacher-fixedm-students-app}
	}
    \vspace{-0.1cm}
	\caption{\textbf{Supervised IsoFLOPs.}
	\textbf{(a)} Teachers of six sizes with $M_T=D_T/N_T\approx 20$
    are distilled into Students with four IsoFLOP profiles, and a small number with $C_S=3\times 10^{21}$.
    The horizontal grey and vertical black dashed lines indicate teacher cross entropy $L_T$ and size $N_T$ respectively.
	\textbf{(b)} Students of four sizes
    trained with a $M=D_S/N_S=20$ are distilled from teachers with four IsoFLOP profiles.
    Horizontal (vertical) dashed lines indicate student supervised cross entropy $\widetilde{L}_S$ (student size $N_S$).}
    \vspace{-0.1cm}
	\label{fig:distillation-isoflops}
\end{figure}







\FloatBarrier


\subsection{Distillation scaling law IsoFLOP optima}
\label{ssec:distillation-scaling-law-isoflop-optima}
The optimal loss values of each IsoFLOP in \Cref{fig:fixedm-teacher-isoflop-students-app} are shown in \Cref{fig:isoflop-optima}.

\begin{figure}[h]
	\centering
	\subfloat[Fixed $M$-Ratio Teacher/Student ISOFlop optima.]{
		\includegraphics[width=0.3\textwidth]{plots/fixedm_teacher_isoflop_students_optima.pdf}
		\label{fig:fixedm-teacher-student-isoflop-optima}
	}
	\subfloat[Fixed $M$-Ratio Student/Teacher ISOFlop optima.]{
		\includegraphics[width=0.3\textwidth]{plots/isoflop_teacher_fixedm_students_optima.pdf}
		\label{fig:fixedm-student-teacher-isoflop-optima}
	}
	\caption{\textbf{ISOFlop optima.}
		\textbf{a)} The optimal student parameters $N_S^*=\argmin_{N_S} \Ls(N_S)$ that give the lowest student validation loss for each teacher-student combination shown in \Cref{fig:fixedm-teacher-isoflop-students-app}. The dashed lines correspond to the validation loss of the optimal supervised models trained with the four corresponding compute budget.
		\textbf{b)} The optimal teacher parameters $N_T^*=\argmin_{N_T} \Ls(T_S)$ that give the lowest student validation loss for each teacher-student combination shown in \Cref{fig:isoflop-teacher-fixedm-students}. The black dashed line correspond to the validation loss of a $M=D/N=20$ supervised model of the indicated student size.
		In both figures, the shaded region corresponds to where \emph{weak to strong generalization} may occur, as $N_S>N_T$ (see \Cref{ssec:weak-to-strong-generalization}).}
	\label{fig:isoflop-optima}
\end{figure}

\FloatBarrier

\subsection{Distillation with infinite data}
\label{ssec:distillation-with-infinite-data}

From the supervised scaling law (\Cref{eq:supervised-scaling-law})
a model with $N$ parameters
has a cross-entropy lower bound
\begin{equation}
    L(N)\equiv L(N,D=\infty)=E+(A N^{-\alpha})^\gamma
    \label{eq:supervised-lower-bound}
\end{equation}
which represents the best solution to the training objective
subject to constraints from that model's hypothesis space \citep{DBLP:journals/corr/abs-2203-15556}
and is achieved when the number of training tokens is large ($D\rightarrow\infty$).
As the hypothesis space of a model is independent of the procedure used to find the solutions,
we anticipate that the student with $N_S$ parameters has a cross-entropy lower bound
that is the same as the supervised one \Cref{eq:supervised-lower-bound}.
However, it not immediately clear if this is true in practice, since
\begin{align}
    L_S(N_S)
    &\equiv L_S(N_S,D_S=\infty,L_T=L_T^*)\\
    &=L_T^*+\frac{(A^\prime N_S^{-\alpha^\prime})^{\gamma^\prime}}{(L_T^*)^{c_0}}\left(1+\left(\frac{L_T^*d_1^{-1}}{L(N_S)}\right)^{1/{f_1}}\right)^{-c_1f_1},
    \label{eq:distillation-lower-bound}
\end{align}
where $L_T^*=\argmin_L(N_S,D_S=\infty,L_T)$ is the teacher cross-entropy that minimizes \Cref{eq:distillation-scaling-law}.
Upon checking numerically, we do find that \Cref{eq:supervised-lower-bound}
is consistent with \Cref{eq:distillation-lower-bound}
for a range of models $N,N_S\in[100M,100B]$
(\Cref{fig:scaling-law-d-infinity}).
We stress that unlike our three motivations for the equation properties (\Cref{ssec:distillation-scaling-law-functional-form}), this infinite data limit was imposed added by hand, and is only true for certain values scaling coefficients.
This lower bound consistency is evidence that that our distillation scaling law has desired behavior far outside of observed models, at least along the data and teacher axes.
We also note that only the optimal teacher for each student size produces a student cross-entropy lower bound that is consistent with the supervised one.
Any other choice produces higher student cross-entropies, either because the teacher is too weak, or due to the capacity gap.
\begin{figure}[h]
	\centering
		\includegraphics[width=0.27\textwidth]{plots/student_loss_d_infinity.pdf}
	\caption{\textbf{Scaling behavior in the infinite data regime.}
    For the \emph{optimal} choice of teacher, the loss achieved by all student sizes under distillation is consistent with the loss achievable by supervised learning. This is \emph{not} true for \emph{any} choice of teacher, \emph{only} the optimal one, which can be determined through numerical optimization of the provided distillation scaling laws (see \Cref{sec:distillation-scaling-law-applications}).}
	\label{fig:scaling-law-d-infinity}
\end{figure}

\FloatBarrier
