\newcommand{\calwidth}{0.48\textwidth}

\subsection{Model calibration}
\label{ssec:model-calibration}

Calibration in \gls{lm}s refers to the alignment between the model’s confidence in its predictions and the actual correctness of those predictions. 
Well-calibrated models provide confidence scores that accurately reflect their probability of correctness, enabling more decision-making. 
\gls{ece} is a common metric to quantify miscalibration, and measures the difference between \emph{predicted confidence} and \emph{actual accuracy} across multiple confidence intervals
\begin{align}\label{eq:ece}
	\mathrm{ECE} =
    \sum_{m=1}^{M} \frac{|\gB_m|}{N_{\mathrm{Samples}}} \left| \mathrm{Accuracy}(\gB_m) - \mathrm{Confidence}(\gB_m) \right|,
\end{align}
where $M$ is the number of bins,
$\gB_m$ is the set of samples whose confidence scores fall into the $m$-th bin, 
$|\gB_m|$ denotes the number of samples in bin $\gB_m$, 
$N_{\mathrm{Samples}}=\sum_{m=1}^M|\gB_m|$ is the total number of samples, 
$\mathrm{Accuracy}(\gB_m)$ and 
$\mathrm{Confidence}(\gB_m)$ are the empirical accuracy and average confidence of the model being evaluated in bin $m$ respectively.
Lower \gls{ece} indicates better model calibration.




To measure \gls{ece}, we use $M=21$ bins uniformly partitioned across the output probability space. Accuracy and confidence are computed in the standard manner: the predicted label is determined via the argmax over the output probabilities for each prediction, and the confidence is defined as the maximum probability assigned to the predicted label. Accuracy is then measured as the proportion of instances where the predicted label matches the ground truth. Notably, this approach focuses solely on the maximum probability prediction, disregarding the calibration of lower-probability predictions. To assess calibration across the entire output distribution rather than just the top prediction, alternative metrics could be considered.

\subsubsection{Teachers}
\label{sssec:teachers}

In \Cref{fig:calibration-teacher}, we see that the \gls{ece} value across different sizes of teachers. For all models, the \gls{ece} ranges between $0.4\%$ and $0.6\%$, suggesting that the models’ confidence estimates closely align with their actual accuracies. We also observe that for each plot, the blue points, i.e.~, the teacher's actual accuracy for predictions falling into specific confidence intervals, closely follow the diagonal, which shows that the models are well-calibrated. There is a small deviation at low and high confidence values denoted by the orange points.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{plots/calibration/c4_valid_teacher_20n_top0.0.pdf}
	\caption{\textbf{Teacher calibration.} The calibration of teachers of seven different sizes. The $x$-axis shows the teacher probability assigned to the most confident class, and the $y$-axis is the empirical accuracy of predictions within each confidence bin. Blue points represent the teacher accuracy for predictions falling into specific confidence intervals. Orange points represent the proportion of samples in each confidence bin (helpful for understanding sample distribution across confidence levels). The dashed line represents perfect calibration, where confidence matches empirical accuracy. The \gls{ece} (\Cref{eq:ece}) for each teacher is shown as the title of each plot.}
	\label{fig:calibration-teacher}
\end{figure}

\FloatBarrier
\subsubsection{198M students trained on 20N tokens}
\label{sssec:198m-students-trained-on-20n-tokens}

In this section we consider students trained on the teacher distribution, as in our main study.
We also study students trained on the teacher top-1 distribution, as described in \Cref{ssec:top-k-top-p-sensitivity},
as the qualitative difference in behavior can be informative for student design.

Evaluating the calibration of a student can be done in a number of ways:
\begin{enumerate}
    \item We can compare student outputs relative ground-truth data, as in \Cref{sssec:teachers} for the teachers.
    \item We can compare student outputs with the outputs of its teacher.
\end{enumerate}

\paragraph{Calibration against ground-truth.}
First, let's consider comparison against ground truth data.
In \Cref{fig:calibration-student-data-20n} we show  student calibration with respect to the dataset labels
for both \emph{teacher distribution} distillation and \emph{teacher top-1} distillation.
\begin{enumerate}
  \item \emph{Distilled on the full teacher distribution.} In \Cref{fig:calibration-student-data-20n-dist}, we observe that the student is well-calibrated against ground truth data. Similar to the teacher's calibration plot in \Cref{fig:calibration-teacher}, we see a small discrepancy at very low and very high confidence values, and the \gls{ece} value is low.
  \item \emph{Distilled on teacher top-1.} In \Cref{fig:calibration-student-data-20n-top1}, we see that \emph{a student trained only on its teacher's top-$1$ prediction, 
  is not calibrated against ground truth data.} 
  The blue points below the dashed line indicate an overconfident student, i.e.~, its predicted confidence is higher than the actual accuracy in that confidence range. 
  This is because training the student on top-$1$ assigns the student to the most plausible outcome rather than all the plausible outcomes with correct frequencies. 
  Confidence proportions are low for all bins that are not the most confident bin, and \gls{ece} is high, although decreases with increasing teacher size $N_T$.
\end{enumerate}
\Cref{fig:calibration-student-data-20n} shows that training the student on the teacher's distribution results in a calibrated student, whereas training on the teacher top-$1$
does not.
Indeed, optimizing against the teacher's top-$1$ is not a proper scoring metric, and that teacher top-$1$ is \emph{not} an \emph{unbiased} estimator for the data, while the teacher distribution is.
\begin{figure}[h]
  \centering
  \subfloat[Distillation target: teacher distribution.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_20n_top0.0.pdf}
      \label{fig:calibration-student-data-20n-dist}
  }
  \hfill
  \subfloat[Distillation target: teacher top-1.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_20n_top1.0.pdf}
      \label{fig:calibration-student-data-20n-top1}
  }
  \caption{\textbf{Student calibration (data).} Calibration of the student with respect to the actual data labels, trained with different teacher sizes ($N_T$), on \textbf{(a)} the teacher distribution and \textbf{(b)} the teacher's top-$1$. For axis definitions and the figure legend, refer to \Cref{fig:calibration-teacher}. Blue points below the dashed line indicate student overconfidence.}
  \label{fig:calibration-student-data-20n}
\end{figure}

\paragraph{Calibration against teacher top-1.} 
Next we investigate the first student calibration against the teacher.
In \Cref{fig:calibration-student-ttop1-20n} we show student calibration with respect to the teacher's top-$1$ label.
That is, the next-token label used for accuracy computation, and extract the students confidence
is the most probable next-token according to the teacher, instead of the label from data.
Here no next token labels are used at all.
These teacher top-1 labels are also used for the \gls{ece} calculation, which is still computed using \Cref{eq:ece}.
\begin{enumerate}
  \item \emph{Distilled on the full teacher distribution.} We see in \Cref{fig:calibration-student-ttop1-20n-dist} that when distilled from the full teacher distribution, the student is \emph{not} calibrated against the teacher top-1. The blue points are above the dashed line, which means that the empirical accuracy is higher than the model’s predicted confidence, i.e. with respect to the teacher top-1, the student is \emph{underconfident}.
  This can be understood by noting that the top-1 objective is an easier objective than modeling the full vocabulary at each step.
  \item \emph{Distilled on teacher top-1.} In \Cref{fig:calibration-student-ttop1-20n-top1} we observe that a student is distilled from its teacher's top-$1$ \emph{is calibrated with respect to teacher's top-1}.
\end{enumerate}
      \begin{figure}[h]
          \centering
          \subfloat[Distillation target: teacher distribution.]{
              \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v1_20n_top0.0.pdf}
              \label{fig:calibration-student-ttop1-20n-dist}
          }
          \hfill
          \subfloat[Distillation target: teacher top-1.]{
              \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v1_20n_top1.0.pdf}
              \label{fig:calibration-student-ttop1-20n-top1}
          }
          \caption{\textbf{Student calibration (teacher top-1).} Calibration of the student with respect to the teacher's top $1$, trained with different teacher sizes ($N_T$), on \textbf{(a)} the teacher distribution and \textbf{(b)} the teacher's top-1. For axis definitions and the figure legend, refer to \Cref{fig:calibration-teacher}. Blue points above the dashed line indicate the student is \emph{underconfident}.}
          \label{fig:calibration-student-ttop1-20n}
      \end{figure}
\Cref{fig:calibration-student-ttop1-20n} shows that training the student on teacher top-1 results in calibration against teacher top-1,
whereas a model trained on data, or distilled on the full teacher distribution is not calibrated against teacher top-1.
As above, this can be understood as now
teacher's top-$1$ is now a proper scoring metric, and teacher top-$1$ is an unbiased estimator for itself.

\paragraph{Calibration against teacher distribution.} 
Here we develop a modified calibration measure that will help us understand if the student matches the teacher in a distributional sense.
As we have two distributions to compare, we can ask, for a given teacher confidence, what is the expected student confidence.
This leads to $\mathrm{ECE}_{\mathrm{Dist}}$, a distributional form of \gls{ece}:
\begin{align}
\label{eq:ece-dist}
    \mathrm{ECE}_{\mathrm{Dist}}(A,B) =
    \sum_{m=1}^{M} \frac{|\gB_m|}{N_{\mathrm{Samples}}} \left| \mathrm{Confidence}(\gB_m; A) - \mathrm{Confidence}(\gB_m; B) \right|,
\end{align}
and is similar in spirit to divergence measures like \gls{kld}.
$\gB_m$, $|\gB_m|$, and $N_{\mathrm{Samples}}$ are defined as before,
and $\text{Confidence}_S(\gB_m;A|B)$
is the average confidence of model $A$ or $B$ in bin $m$ respectively. 
The bins $\gG_m$ are always witin the bins of confidence of model $B$.
In the current evaluation, we take $A$ as the teacher and $B$ as the student,
and we are measuring the average confidence of the teacher is measured within a student's confidence bin.
\begin{enumerate}
  \item \emph{Distilled on the full teacher distribution.} In \Cref{fig:calibration-student-tdist-20n-dist}, we see that when the student is confident, it matches the teacher confidence.
  However, as the teacher model grows in size, 
  when the student is less confident,
  it it systematically underestimates its confidence.
  This suggests that the student has not effectively learned low-probability outcomes, or that these outcomes are particularly challenging for the student to replicate. 
  The underconfidence in these regions may be a result of the distillation process not providing sufficient learning signal for these difficult cases, or the inherent difficulty of capturing the uncertainty associated with low-confidence predictions.
  This observation of confidence mismatch helps indicate which parts of the distribution the student finds challenging to model, giving rise to the increasing \gls{kld} and capacity gap observed in \Cref{fig:fixedm-teacher-fixedm-students} and \Cref{ssec:fixed-m-teacher-fixed-m-students}.
  \item \emph{Distilled on teacher top-1.} In \Cref{fig:calibration-student-tdist-20n-top1}, for small teachers, we observe student overconfidence.
  As the teacher increases in size, the student's overconfidence in low-confidence bins transitions to underconfidence. 
  At the same time, the student's overconfidence in high-confidence bins improves, leading to an overall reduction in distributional \gls{ece}. 
  This pattern of overconfidence in the student is similar to what we saw in \Cref{fig:calibration-student-data-20n-top1}, but the change in behavior at low-confidence bins as the teacher’s size varies is different. 
  This shift in the student's calibration behavior, especially in low-confidence bins, aligns with findings from \Cref{fig:calibration-student-tdist-20n-dist} and may highlight the difficulty the small student faces in learning rare events.
\end{enumerate}

\begin{figure}[h]
	\centering
    \vspace{-0.1cm}
	\subfloat[Train target: teacher distribution.]{
		\includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v2_20n_top0.0.pdf}
		\label{fig:calibration-student-tdist-20n-dist}
	}
	\hfill
	\subfloat[Train target: teacher top 1.]{
		\includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v2_20n_top1.0.pdf}
		\label{fig:calibration-student-tdist-20n-top1}
	}
    \vspace{-0.1cm}
	\caption{\textbf{Student calibration (teacher distribution).} Calibration of the student with respect to the teacher's distribution, trained with different teacher sizes ($N_T$), on \textbf{(a)} the teacher distribution and \textbf{(b)} the teacher's top-1. For \gls{ece} calculation on the full distribution, see \Cref{eq:ece-dist}. For axis definitions and the figure legend, refer to \Cref{fig:calibration-teacher}. Blue points below the dashed line indicate student overconfidence, while points above the dashed line indicate underconfidence.}
	\label{fig:calibration-student-tdist-20n}
    \vspace{-0.1cm}
\end{figure}

We can also inspect the student confidences within a bin of teacher confidences, and compute the distributional \gls{ece} (\Cref{eq:ece-dist}), swapping the roles of teacher and student (see \Cref{fig:calibration-teacher-tdist-20n}).
\begin{enumerate}
  \item \emph{Distilled on the full teacher distribution.} In \Cref{fig:calibration-student-tdist-20n-dist} we complete the picture from \Cref{fig:calibration-student-tdist-20n-dist} and see that the part of the distribution the student struggles to model is actually the place where teacher is most confident.
  \item \emph{Distilled on teacher top-1.} In \Cref{fig:calibration-student-tdist-20n-top1} we see that the student is systematically overconfident for all values of teaacher confidence, except for the largest teachers, where the student is underconfident when those teachers are most confident.
\end{enumerate}

\begin{figure}[h]
	\centering
	\subfloat[Train target: teacher distribution.]{
		\includegraphics[width=\calwidth]{plots/calibration/c4_valid_teacher_student_v2_20n_top0.0.pdf}
		\label{fig:calibration-teacher-tdist-20n-dist}
	}
	\hfill
	\subfloat[Train target: teacher top 1.]{
		\includegraphics[width=\calwidth]{plots/calibration/c4_valid_teacher_student_v2_20n_top1.0.pdf}
		\label{fig:calibration-teacher-tdist-20n-top1}
	}
	\caption{\textbf{Student calibration (under teacher confidence bins).} Calibration of the student with respect to the teacher's confidence bins, trained with different teacher sizes ($N_T$), on \textbf{(a)} the teacher distribution and \textbf{(b)} the teacher's top-1. For \gls{ece} calculation on the full distribution, see \Cref{eq:ece-dist}. For axis definitions and the figure legend, refer to \Cref{fig:calibration-teacher}. Blue points below the dashed line indicate the teacher is less confident than the student.}
	\label{fig:calibration-teacher-tdist-20n}
\end{figure}


\FloatBarrier
\subsubsection{198M Students trained on 128B tokens}
\label{sssec:198m-students-trained-on-128b-tokens}
In this section, we study the effect of increasing the number distillation tokens 
in \Cref{sssec:198m-students-trained-on-20n-tokens} from $D_S\approx 20N_S$ to $D_S\approx 512B$.
Here, we reserve discussion for the observed differences compared to \Cref{sssec:198m-students-trained-on-20n-tokens}.

\begin{figure}[h]
  \centering
  \vspace{-0.1cm}
  \subfloat[Train target: teacher distribution.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_128b_top0.0.pdf}
      \label{fig:calibration-student-data-128b-dist}
  }
  \hfill
  \subfloat[Train target: teacher Top 1.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_128b_top1.0.pdf}
      \label{fig:calibration-student-data-128b-top1}
  }
  \vspace{-0.1cm}
  \caption{\textbf{Student calibration (data).} Calibration of the student with respect to the actual data labels with increased training tokens. Compare to \Cref{fig:calibration-student-data-20n} for the effect of tokens and refer to \Cref{fig:calibration-teacher} for legend and axis explanations.}
  \label{fig:calibration-student-data-128b}
  \vspace{-0.1cm}
\end{figure}

\paragraph{Calibration against ground-truth.} 
As the number of distillation tokens increases, we observe a consistent decrease in the \gls{ece} when the student is trained on the teacher's distribution, as shown by the comparison between \Cref{fig:calibration-student-data-128b-dist} and \Cref{fig:calibration-student-data-20n-dist} across different teacher sizes. 
However, when the student is trained on the teacher's top-$1$ predictions, increasing the number of tokens \emph{negatively} impacts \gls{ece}, as evidenced by the comparison between \Cref{fig:calibration-student-data-128b-top1} and \Cref{fig:calibration-student-data-20n-top1}. 
This suggests that the teacher's top-$1$ predictions are not a reliable, unbiased estimator of the actual data, and increasing the number of training tokens only exacerbates this issue. See \Cref{ssec:top-k-top-p-sensitivity} for further discussion.

\paragraph{Calibration against teacher top-1.} 
Increasing the number of distillation tokens leads to worse calibration between the student and the teacher's top-$1$ predictions when the student is trained on the full distribution. 
This change primarily occurs in the low-confidence bins, and results in a higher \gls{ece} (compare \Cref{fig:calibration-student-ttop1-128b-dist} and \Cref{fig:calibration-student-ttop1-20n-dist}). 
However, when comparing the \gls{ece}s for the student trained on the teacher's top-$1$ predictions (\Cref{fig:calibration-student-ttop1-20n-top1,fig:calibration-student-ttop1-128b-top1}), there is an improvement across all teacher sizes. 
When the student is trained and evaluated using the same metric, increasing the training tokens helps improve calibration, demonstrating consistency between the learning objective and the evaluation metric.

\begin{figure}[h]
  \centering
  \subfloat[Train target: teacher distribution.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v1_128b_top0.0.pdf}
      \label{fig:calibration-student-ttop1-128b-dist}
  }
  \hfill
  \subfloat[Train target: teacher top 1.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v1_128b_top1.0.pdf}
      \label{fig:calibration-student-ttop1-128b-top1}
  }
  \caption{\textbf{Student calibration (teacher top 1).} Calibration of the student with respect to the teacher's top $1$ when the training tokens have increased. Compare to \Cref{fig:calibration-student-ttop1-20n} for the effect of tokens and refer to \Cref{fig:calibration-teacher} for legend and axis explanations.}
  \label{fig:calibration-student-ttop1-128b}
\end{figure}

\paragraph{Calibration against teacher distribution.} A comparison between \Cref{fig:calibration-student-tdist-128b-dist} and \Cref{fig:calibration-student-tdist-20n-dist} shows that when the student is trained on the teacher's full distribution and evaluated against the full distribution using \Cref{eq:ece-dist}, increasing the number of training tokens consistently improves calibration across all teacher sizes. 
However, when the student is trained on the teacher's top-$1$ predictions, a quick comparison between \Cref{fig:calibration-student-tdist-128b-top1} and \Cref{fig:calibration-student-tdist-20n-top1} reveals worse calibration uniformly across all confidence bins.

\begin{figure}[h]
  \centering
  \subfloat[Train target: teacher distribution.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v2_128b_top0.0.pdf}
      \label{fig:calibration-student-tdist-128b-dist}
  }
  \hfill
  \subfloat[Train target: teacher Top-1.]{
      \includegraphics[width=\calwidth]{plots/calibration/c4_valid_student_teacher_v2_128b_top1.0.pdf}
      \label{fig:calibration-student-tdist-128b-top1}
  }
  \caption{\textbf{Student calibration (teacher distribution).} Calibration of the student with respect to the teacher's distribution as the number of training tokens increases. Compare to \Cref{fig:calibration-student-tdist-20n} for the effect of tokens and refer to \Cref{fig:calibration-teacher} for legend and axis explanations.}
  \label{fig:calibration-student-tdist-128b}
\end{figure}

Similarly, when comparing within teacher confidence bins (\Cref{fig:calibration-teacher-tdist-128b})
increasing the number of distillation tokens from 20N to 128B primarily amplifies the observed phenomena at lower distillation token budgets,
and improving calibration in cases where there is a proper scoring metric present (\Cref{fig:calibration-teacher-tdist-128b-dist}).

\begin{figure}[h]
	\centering
	\subfloat[Train target: teacher distribution.]{
		\includegraphics[width=\calwidth]{plots/calibration/c4_valid_teacher_student_v2_128b_top0.0.pdf}
		\label{fig:calibration-teacher-tdist-128b-dist}
	}
	\hfill
	\subfloat[Train target: teacher top 1.]{
		\includegraphics[width=\calwidth]{plots/calibration/c4_valid_teacher_student_v2_128b_top1.0.pdf}
		\label{fig:calibration-teacher-tdist-128b-top1}
	}
	\caption{\textbf{Student calibration (teacher distribution).} Calibration of the student with respect to the teacher' confidence bins distribution as the number of training tokens increases. Compare to \Cref{fig:calibration-teacher-tdist-20n} for the effect of tokens.}
	\label{fig:calibration-teacher-tdist-128b}
\end{figure}

In general, increasing the number of training tokens has a positive effect when the training metric is an unbiased estimator of the actual data or the measured calibration quantities (see \Cref{fig:calibration-student-data-128b-dist,fig:calibration-student-ttop1-128b-top1,fig:calibration-student-tdist-128b-dist}) and reduces the \gls{ece}, while it has a negative impact when there is a mismatch between the learned and measured quantities (see \Cref{fig:calibration-student-data-128b-top1,fig:calibration-student-ttop1-128b-dist,fig:calibration-student-tdist-128b-top1}).
