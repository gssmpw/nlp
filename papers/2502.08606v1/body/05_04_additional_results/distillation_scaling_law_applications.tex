\newcommand{\bigwidth}{0.79\textwidth}


\section{Distillation scaling law applications (additional results)}
\label{sec:distillation-scaling-law-applications-extra-results}

In this section, we present results referenced in \Cref{sec:distillation-scaling-law-applications}. We explore the best-case scenario for distillation under fixed student tokens or compute, as well as under fixed teacher size or compute, while accounting for teacher inference. These results provide further insights into the optimal distillation strategies in different resource-constrained settings.

\subsection{A contradiction with patient teachers}
\label{sec:contradiction}
\citet{DBLP:conf/cvpr/BeyerZRMA022}
showed in computer vision that a good teacher is:
\begin{enumerate}
    \item \emph{Patient.} Distillation works best when training for a large number of epochs, and
    \item \emph{Consistent}. The teacher and the student see the \emph{same views} of the data under an augmentation policy.
\end{enumerate}
Our setting automatically satisfies \emph{consistency} as there is no augmentation policy.
There is a remaining question about patience, which in our scenario corresponds to the large $D_S$ limit.
We observe that for a given student size:
\begin{enumerate}
    \item If the teacher is optimally chosen for the student, distilling on a large number of tokens produces the same result as training the model in a supervised way on the same number of tokens (\Cref{ssec:distillation-with-infinite-data}).
    \item Otherwise supervised learning outperforms distillation (\Cref{ssec:compute-optimal-distillation}).
\end{enumerate}
The behavior we observe is expected if solutions a model can access are limited only by function space.
Uncertain of the driver behind our conclusion differences,
we note the differences between our experimental setups in \Cref{tab:setting-differences}.
\begin{table}[h]
    \centering
    \rowcolors{2}{AppleChartGrey2}{white}
    \caption{Experimental setting differences between \citet{DBLP:conf/cvpr/BeyerZRMA022} and ours.}
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{lll}
        \toprule
        Component & \citet{DBLP:conf/cvpr/BeyerZRMA022} & Ours \\ \midrule
        Data repetitions & Many repetitions & Minimal repetitions \\
        Data diversity & Low number of unique tokens & Large number of unique tokens \\
        Domain & Vision & Language \\
        Objective & Fewer categories, more unimodal & Many categories, highly multimodal \\
        Architecture & Different computer vision architectures & \gls{mup} optimized homogeneous  transformers \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.2cm}
    \label{tab:setting-differences}
\end{table}

\subsection{Fixed tokens or compute (best case)}
\label{ssec:fixed-tokens-or-compute-best-case-app}

\paragraph{Distillation can outperform supervised learning given enough teacher training tokens or compute.} As shown in \Cref{fig:isoflop-teacher-fixedm-students-strategies-data,fig:isoflop-teacher-fixedm-students-strategies-compute}, when the teacher size, student size, and number of student tokens are held constant, increasing the number of teacher training tokens makes distillation more favorable than supervised learning. 
This advantage arises because the teacher, with access to more training tokens, can better learn the approximation of the language distribution. 
As a result, the teacher's learned distribution become more informative for the student to follow, thus improving the student's performance. Note that for a fixed student size and compute, the teacher must be sufficiently large and well-trained; otherwise, supervised learning will outperform distillation. Without adequate teacher size or training, the student may not benefit from the distillation process, leading to inferior performance compared to direct supervised learning.

We also see that the scatter data matches up well with the contour colors, despite these contour beings a difference of two scaling laws, providing a verification of our setup.

\begin{figure}[h]
	\centering
	\subfloat[Fixed data]{
		\includegraphics[width=0.48\textwidth]{plots/isoflop_teacher_fixedm_students_strategies_data.pdf}
		\label{fig:isoflop-teacher-fixedm-students-strategies-data}
	}
	\subfloat[Fixed compute]{
		\includegraphics[width=0.48\textwidth]{plots/isoflop_teacher_fixedm_students_strategies_compute.pdf}
		\label{fig:isoflop-teacher-fixedm-students-strategies-compute}
	}
	\caption{\textbf{IsoFLOP Teacher Contours with Fixed $\bm M$ students.}
	\textbf{(a)} For a given teacher size $N_T$, for a given teacher token $D_T$, what is the difference between the loss achieved by distillation and supervised learning. Blue indicates distillation outperforms supervised learning, and red indicates when supervised learning outperforms distillation.
		The white horizontal dashed line indicates the student size. 
	\textbf{(b)} For a given teacher size $N_S$, for a given teacher compute budget, what is the difference between the loss achieved by distillation and supervised learning. Blue indicates distillation outperforms supervised learning, and red indicates when supervised learning outperforms distillation.
		The white horizontal dashed line indicates the student size.}
\end{figure}










\paragraph{Supervised learning always outperforms distillation given enough student compute or tokens.} The trend observed in \Cref{fig:fixedm-teacher-isoflop-students-strategies-compute} mirrors that of \Cref{ssec:fixed-tokens-or-compute-main}. It demonstrates that, for a fixed teacher size and compute, supervised learning can outperform distillation when the student's compute is sufficiently large. With enough resources allocated to the student, it can learn more effectively from the data directly, making distillation less advantageous in comparison.
This advantage only happens at a compute budget that grows with student size.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{plots/fixedm_teacher_isoflop_students_strategies_compute.pdf}
	\caption{\textbf{Fixed $\bm M$ Teacher Contours with IsoFLOP students (compute).} For a given student size $N_S$, for a given student compute budget, what is the difference between the loss achieved by distillation and supervised learning. Blue indicates distillation outperforms supervised learning, and red indicates when supervised learning outperforms distillation.
		The white horizontal dashed line indicates the teacher size.
	}
	\label{fig:fixedm-teacher-isoflop-students-strategies-compute}
\end{figure}

\FloatBarrier
\clearpage

\subsection{Fixed size or compute (teacher inference)}
\label{ssec:fixed-tokens-or-compute-teacher-inference-app}

\paragraph{Fixed student size} For a fixed student size, as the number of student tokens increases, the optimal teacher cross-entropy decreases slightly; see \Cref{fig:distillation-strategies-a-fixedparams-tokens}. This observation highlights an asymmetry between the growth of student size and student tokens (or their rates in the scaling law), as the behavior here differs from that observed in \Cref{ssec:fixed-tokens-or-compute-main}. Notably, when the student size is sufficiently large, such as $N_S = 30\text{B}$, increasing the student tokens initially leads to a decrease in the teacher's loss, followed by a saturation point and a slow decrease in the optimal teacher's loss.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{plots/distillation_strategies_a_given_teacher_fixedparams_xtokens.pdf}
	\caption{\textbf{Student performance given a teacher varying distillation tokens.}
		For four distillation student sizes
		$N_S\in\{1B, 3B, 10B, 30B\}$
		the validation loss achieved by a students distilled on $D_S\in[250B,16T]$ tokens under a teacher with loss $L_T\in[E,2.5]$.
		The red line indicates the value of the teacher loss resulting in the best performing student, and the vertical dashed line indicates the number of tokens at which supervised pretraining outperforms distillation.
	}
	\label{fig:distillation-strategies-a-fixedparams-tokens}
\end{figure}

\FloatBarrier
\paragraph{Fixed compute budget}
Given an inference budget $N_S$, a set of teachers $\{(L_T^{(i)},N_T^{(i)})\}_{i=1}^n$ and a total compute budget $C_{\mathrm{Total}}$,
the number of distillation tokens is determined from \Cref{eq:distillation-compute}
\begin{equation}
    D_S=C_{\mathrm{Total}}/(3F(N_S)+\delta_{\mathrm{T-Logits}}F(N_T)),
\end{equation}
where $F(N)$ is the forward \flops per token of a model of size $N$ (see \Cref{sec:parameters-and-floating-operation-estimation}).
If $\delta_{\mathrm{T-Logits}}=0$ then there is no price to pay for a larger teacher, and the conclusions are identical to those of the fixed token analysis of \Cref{ssec:fixed-distillation-budget-given-a-teacher}.
In the worst case scenario, $\delta_{\mathrm{T-Logits}}=1$, then using a larger teacher
will mean fewer distillation tokens are available for the student.
Due to the capacity gap phenomenon, at small compute budgets,
this means it is actually better to use a \emph{large weak teacher} rather than a \emph{large strong teacher}.
Once compute is sufficient to allow enough distillation tokens, a stronger teacher can be used for all student sizes (see \Cref{fig:distillation-strategies-fixed-compute}).

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{plots/distillation_strategies_a_given_teacher_fixedcompute.pdf}
	\caption{\textbf{Fixed compute distillation strategy.} The student performance obtained for four total compute budgets $C_{\mathrm{Total}}\in\{10^{21},10^{22},10^{23},10^{24}\}\,\mathrm{FLOPs}$ and four student sizes $N_S\in\{1B, 3B, 10B, 30B\}$ under a teacher of size $N_T\in[1B,1T]$ and teacher loss $L_T\in[E,2.5]$. The red line indicates the value of teacher loss $L_T^*(N_T)$ that results in the best student performance for each teacher size $N_T$.
	}
	\label{fig:distillation-strategies-fixed-compute}
\end{figure}

\FloatBarrier
\clearpage

\begin{table}[h]
    \centering
    \rowcolors{2}{AppleChartGrey2}{white}
    \caption{Scenarios considered in our scaling law applications. Same as \Cref{tab:compute-scenarios}.}
    \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{lccp{7.cm}}
        \toprule
        Compute Scenario & $\delta_T^{\mathrm{Lgt}}$ & $\delta_T^{\mathrm{Pre}}$ & Description \\ \midrule
        Best case (fully amortized teacher) & 0 & 0 & The teacher produces no additional FLOPs and so we are free to choose the teacher $L_T^*$ that minimizes the student cross-entropy. \\
        Teacher inference & 1 & 0 & We don't account for the teacher cost because the teacher already exists, or we intend to use the teacher as e.g. a server model. We still need to pay to use it for distilling a student. \\
        Teacher pretraining & 0 & 1 & The teacher needs training, but we store the logits for re-use, either during training, or after training for distilling into sufficiently many students.  \\        
        Teacher pretraining + inference & 1 & 1 & The teacher needs training and we pay for distilling into one student, the worst case scenario. \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:compute-scenarios-app}
\end{table}

\subsection{Compute optimal distillation}
\label{ssec:compute-optimal-distillation-app}

\subsubsection{Setup}

The solutions resulting in the losses give guidance on how to scale depending on the use case,
and are the result of constrained optimization
\begin{align}
    D_S^*,N_T^*,D_T^*=\argmin_{D_S,N_T,D_T}L_S(N_S,D_S,N_T,D_T)
    \qquad
    \mathrm{s.t.} \qquad \mathrm{FLOPs}(N_S,D_S,N_T,D_T)=C,
    \label{eq:distillation-optimal-appl}
\end{align}
where $L_S(N_S,D_S,N_T,D_T)$ is the distillation scaling law (\Cref{eq:distillation-scaling-law}),
and
\begin{equation}
    \mathrm{FLOPs}(N_S,D_S,N_T,D_T)\approx
    \underbrace{3F(N_S)D_S}_{\substack{\mathrm{Student}\\\mathrm{Training}}}
    +F(N_T)(
    \underbrace{\delta_T^{\mathrm{Lgt}}D_S}_{\substack{\mathrm{Teacher}\\\mathrm{Logits}}} + \underbrace{\delta_T^{\mathrm{Pre}}3D_T}_{\substack{\mathrm{Teacher}\\\mathrm{Training}}})
    \label{eq:distillation-compute-app}
    \vspace{-0.2cm}
\end{equation}
is the total number of floating operations performed in the entire distillation setup.
$F(N)$ is the forward \flops per token of a model of size $N$ (see \Cref{sec:parameters-and-floating-operation-estimation}),
and $\delta_T^{\mathrm{Lgt}},\delta_T^{\mathrm{Pre}}\in[0,1]$ 
indicate if we account for the cost of teacher logit inference for the student targets and teacher pretraining cost in the total compute budget.
For convenience, we restate our compute scenarios of interest in \Cref{tab:compute-scenarios-app}).
Constrained numerical minimization using \gls{slsqp} \citep{kraft1988software} in \texttt{SciPy} \citep{DBLP:journals/corr/abs-1907-10121}.
We allow numerical solutions for model sizes and tokens $N_T,D_S,D_T\in[1M,100P]$.
While this token upper-limit is larger than available resources \citep{epoch2023aitrends},
it simplifies discussions when comparing to supervised learning at large compute budgets, which otherwise, for smaller students, would only by using a fraction of the available compute.

We begin by looking at the student cross-entropy achievable in each compute scenarios alongside the corresponding teacher cross-entropies in \Cref{sssec:cross-entropy}.
We then investigate the compute-optimal distillation configurations for each scenario that produce those cross-entropies.
We look at \emph{best case} distillation in 
\Cref{sssec:distillation-best-case},
\emph{teacher inference} in
\Cref{sssec:distillation-teacher-inference},
\emph{teacher pretraining} in
\Cref{sssec:distillation-teacher-pretraining},
and 
\emph{teacher pretraining + inference}
in
\Cref{sssec:distillation-teacher-pretraining-inference}.
Finally, to aid comparisons across methods,
we present the token and parameter configurations for all methods in
\Cref{sssec:training-tokens} and \Cref{sssec:teacher-size} respectively.
For completeness, in the following sections, some of the findings of \Cref{ssec:compute-optimal-distillation} are restated.


\FloatBarrier
\subsubsection{Cross-entropy}
\label{sssec:cross-entropy}

In \Cref{fig:compute-optimal-distillation-student-loss-app}
we show the student cross-entropies achieved in the compute optimal case for each scenario in
\Cref{tab:compute-scenarios-app},
and the teacher cross-entropies that enable those student cross-entropies in \Cref{fig:compute-optimal-distillation-teacher-loss-app}.

\paragraph{Distillation and supervised learning produce the same student at large compute.}
The first thing to note in \Cref{fig:compute-optimal-distillation-student-loss-app}
is that at low compute,
in the \emph{best case} and \emph{teacher inference}
scenarios, distillation outperforms supervised learning,
consistent with our expectations from distillation and the existing literature (see \Cref{ssec:knowledge-distillation}).
However, once enough the compute is \emph{large enough}\footnote{The level of compute at which this happens is larger for larger models, see \Cref{fig:compute-optimal-distillation-student-loss-app} for specific values.},
distillation and supervised learning produce models with the same cross-entropy,
i.e. in general, \emph{distillation does not allow us to produce better models that supervised learning does},
however, \emph{distillation  does produce better models than supervised learning with modest resources}.
This behavior is consistent with the asymptotic analysis in
\Cref{ssec:distillation-with-infinite-data},
and can be understood through noting that although distillation modifies the learning process the student undergoes, \emph{distillation does not alter the hypothesis space of the student}, which is tied to the student size $N_S$, is the same hypothesis space in the supervised and distillation settings,
and can be explored in the limit of infinite compute or data.

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_L_S_app.pdf}
	\caption{\textbf{Compute optimal distillation student cross-entropies.} For eight student sizes, the optimal student validation cross-entropy $L_S^*$ in each of the distillation scenarios considered as the total compute is varied.
	}
	\label{fig:compute-optimal-distillation-student-loss-app}
\end{figure}

\paragraph{The compute at which distillation and supervised learning produce similar models grows with student size.}
Continuing the previous observation, 
we see in \Cref{fig:compute-optimal-distillation-student-loss-app}
that supervised cross-entropy approaches the
\emph{best case} and \emph{teacher inference}
student cross-entropies
at a value of compute which increases with compute,
meaning that \emph{larger students benefit from distillation for larger compute budgets than supervised learning}.
This implies that if your target student size is small and your compute budget is large,
then supervised learning is more likely to be beneficial than if your target student size is larger.
The phenomenon happens because larger supervised models saturate in performance at larger values of $D$ (\Cref{eq:supervised-scaling-law}),
and distillation accelerates progress towards this saturation with the correct choice of teacher
(\Cref{eq:distillation-scaling-law}),
with more capable teachers producing more gains per token.

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_L_T_app.pdf}
    \vspace{-0.25cm}
	\caption{\textbf{Compute optimal distillation teacher cross-entropies.} For eight student sizes, the optimal teacher validation loss $L_T^*$ resulting in lowest student validation loss $L_S^*$ in each of the distillation scenarios considered (\Cref{tab:compute-scenarios-app}) the total compute is varied.
	}
    \vspace{-0.25cm}
	\label{fig:compute-optimal-distillation-teacher-loss-app}
\end{figure}

\paragraph{Including teacher training in compute produces student cross-entropies higher than in the supervised setting.}
In \Cref{fig:compute-optimal-distillation-student-loss-app}
supervised cross-entropy is always below the
\emph{teacher pretraining} and \emph{teacher pretraining + inference}
scenarios, except at very large compute budgets, when supervised learning
and these distillation scenarios produce similar student cross-entropies.
This means that if your \emph{only} aim is to produce the model of a target size with the lowest cross-entropy and you do not have access to a teacher, then you should choose supervised learning, instead of training a teacher and then distilling.
Conversely, if the intention is to distill into a family of models, or use the teacher as a server model, distillation \emph{may} be more computationally beneficial than supervised learning.
This finding aligns with expectations, the alternative implies distillation can outperform direct maximum likelihood optimization given fixed compute.

\paragraph{The optimal teacher cross-entropy decreases with increasing total compute.} As shown in \Cref{fig:compute-optimal-distillation-teacher-loss-app}, the optimal teacher cross entropy loss has a decreasing trend with respect to the total compute. 
However, in the \emph{best case} scenarios, at low compute for larger student, 
where the number of student tokens is lower than the Chinchilla rule of thumb,
an inflection point happens in optimal teacher compute.

We now turn to investigating the optimal distillation configurations that achieve these student cross-entropies.

\FloatBarrier
\subsubsection{Distillation (best case)}
\label{sssec:distillation-best-case}

In the \emph{distillation (best case)} scenario, 
$\delta_T^{\mathrm{Lgt}}=\delta_T^{\mathrm{Pre}}=0$,
which means that we only account for compute associated with the standard supervised learning case
\begin{equation}
    \mathrm{FLOPs}(N_S,D_S,N_T,D_T)\approx
    \underbrace{3F(N_S)D_S}_{\substack{\mathrm{Student}\\\mathrm{Training}}}.
\end{equation}
We call this \emph{best case} as the scenario reflects a freedom to choose \emph{the best} distillation setting for a given student size $N_S$, with all of the compute being put into training the student for as long as possible (maximal $D_S$).
In this sense we can consider this the \emph{upper bound} in performance for distillation in our experimental setting.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{plots/compute_optimal_distillation_contours_bestcase.pdf}
    \vspace{-0.25cm}
	\caption{\textbf{Compute optimal configuration contours for distillation (best case).} The compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the student cross entropies for \emph{best case} in 
\Cref{fig:compute-optimal-distillation-student-loss-app} for a range of student sizes. $(N_T^*,D_T^*)$ are the supervised compute optimal combination giving rise to $L_T^*$ in \Cref{fig:compute-optimal-distillation-teacher-loss-app}.
	}
    \vspace{-0.25cm}
	\label{fig:compute-optimal-contours-bestcase-app}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_bestcase_app.pdf}
    \vspace{-0.25cm}
	\caption{\textbf{Compute optimal configurations for distillation (best case).} For eight student sizes, the compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the student cross entropies for \emph{best case} in 
\Cref{fig:compute-optimal-distillation-student-loss-app}. $(N_T^*,D_T^*)$ are the supervised compute optimal combination giving rise to $L_T^*$ in \Cref{fig:compute-optimal-distillation-teacher-loss-app}.
This is a one-dimensional slice of \Cref{fig:compute-optimal-contours-bestcase-app}.
	}
    \vspace{-0.25cm}
	\label{fig:compute-optimal-distillation-bestcase-app}
\end{figure}

This scenario represents the setting where a teacher already exists, or we will use the teacher for another purpose, for example a server model. 
In these scenarios, we do not need to worry about the teacher pretraining cost.
Additionally, this teacher may be used to produce the logits for many different students, or
we may have saved the logits from the teacher \emph{during its training}.
In these cases, the cost for producing the student logits can also be ignored.

The optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the cross entropies in 
\Cref{fig:compute-optimal-distillation-student-loss-app}
are shown in \Cref{fig:compute-optimal-distillation-bestcase-app,fig:compute-optimal-contours-bestcase-app}.
In the \emph{best case} scenario, $L_T^*$ is determined, however $N_T^*$ and $D_T^*$ are not determined because they do not enter into the compute constraint, yielding a one-dimensional family $(N_T(L_T^*,D_T),D_T)$ of valid solutions to the minimization problem (\Cref{eq:distillation-optimal-appl}).
To provide some guidance for producing $L_T^*$, in 
\Cref{fig:compute-optimal-distillation-teacher-loss-app}
we present the supervised compute optimal $(N_T(L_T^*,D_T),D_T)$,
i.e. the combination that minimizes $\mathrm{FLOPs}\propto F(N_T) D_T$ subject to $L(N_T,D_T)=L_T$.

In this scenario, all the compute goes into student tokens,
and so in \Cref{fig:compute-optimal-distillation-bestcase-app} we see optimal student tokens $D_S^*$ increases with compute at the same rate as we could for the supervised model, which is higher for smaller  students.
The optimal teacher parameters $N_T^*$ and tokens $D_T^*$ move together to produce the $L_T^*$ in 
\Cref{fig:compute-optimal-distillation-teacher-loss-app}.
Again, the exact values of $N_T^*,D_T^*$ in \Cref{fig:compute-optimal-distillation-bestcase-app}
represent the \emph{supervised compute optimal} solution for producing the $L_T^*$, but are not the only solution in this compute scenario,
since $N_T^*,D_T^*$  are not uniquely determined by the compute constraint.



\FloatBarrier
\subsubsection{Distillation (teacher inference)}
\label{sssec:distillation-teacher-inference}

In the \emph{distillation (teacher inference)} scenario, 
$\delta_T^{\mathrm{Lgt}}=1$ , $\delta_T^{\mathrm{Pre}}=0$,
which means that we  account for compute associated with the standard supervised learning case
as well as the cost for producing the logits for the student
\begin{equation}
    \mathrm{FLOPs}(N_S,D_S,N_T,D_T)\approx
    \underbrace{3F(N_S)D_S}_{\substack{\mathrm{Student}\\\mathrm{Training}}}
    +\underbrace{F(N_T)
    D_S}_{\substack{\mathrm{Teacher}\\\mathrm{Logits}}}.
\end{equation}
This scenario represents the setting where a teacher already exists, but logits for the distillation still need producing.
The optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the cross entropies in 
\Cref{fig:compute-optimal-distillation-student-loss-app}
are shown in \Cref{fig:compute-optimal-contours-teacherinfer-app,fig:compute-optimal-distillation-teacherinfer-app}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_contours_teacherinfer.pdf}
        \vspace{-0.25cm}
	\caption{\textbf{Compute optimal configuration contours for distillation (teacher inference).} The compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the student cross entropies for \emph{teacher inference} in \Cref{fig:compute-optimal-distillation-student-loss-app}.
	}
        \vspace{-0.25cm}
	\label{fig:compute-optimal-contours-teacherinfer-app}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_teacherinfer_app.pdf}
        \vspace{-0.25cm}
	\caption{\textbf{Compute optimal configurations for distillation (teacher inference).} For eight student sizes, the compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) producing the student cross entropies for \emph{teacher inference} in \Cref{fig:compute-optimal-distillation-student-loss-app}. This is a one-dimensional slice of \Cref{fig:compute-optimal-contours-teacherinfer-app}.
	}
        \vspace{-0.25cm}
	\label{fig:compute-optimal-distillation-teacherinfer-app}
\end{figure}

\paragraph{The teacher should be overtrained.}
In the \emph{teacher inference} scenario, $D_T^*$ does not contribute directly to compute
but instead indirectly $N_T^*$ subject to $L_T^*$.
To minimize $N_T^*$ at a given $L_T^*$, the solution is to maximize $D_T^*$
as is seen in \Cref{fig:compute-optimal-distillation-teacherinfer-app}; $D_T^*$ takes the largest value allowed in our numerical optimization, $10^{17}$ tokens.
Although not surprising, this demonstrates the benefit of producing \emph{overtrained teachers},
instead of taking the tempting strategy of using compute optimal teachers followed by a long distillation process into a smaller student model.

\paragraph{As compute is increased, relatively less should be spent on student training, and more on teacher logit inference.}
The compute allocations resulting from the optimal combination are shown in 
\Cref{fig:compute-optimal-allocation-teacherinfer-app}.
We see that in all cases, the student training term (blue) decreases as compute increases,
whereas the teacher logits (orange) increases.
This happens because as compute increases: i) optimal student tokens increases at a rate approximately independent of compute, ii) the teacher size increases with compute to provide a stronger signal, while iii) the student size is fixed (see \Cref{fig:compute-optimal-distillation-teacherinfer-app}).

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_allocation_teacherinfer_app.pdf}
	\caption{\textbf{Compute optimal allocations for distillation (teacher inference).} For eight student sizes, the compute optimal allocations corresponding to the terms in \Cref{eq:distillation-compute-app} for the compute optimal values in \Cref{fig:compute-optimal-distillation-teacherinfer-app}.
	}
	\label{fig:compute-optimal-allocation-teacherinfer-app}
\end{figure}

\FloatBarrier
\subsubsection{Distillation (teacher pretraining)}
\label{sssec:distillation-teacher-pretraining}

In the \emph{distillation (teacher pretraining)} scenario, 
$\delta_T^{\mathrm{Lgt}}=0$ , $\delta_T^{\mathrm{Pre}}=1$,
which means that we  account for compute associated with training the teacher, in addition to the standard training cost of the student, but \emph{not} the cost of producing the logits
\begin{equation}
    \mathrm{FLOPs}(N_S,D_S,N_T,D_T)\approx
    \underbrace{3F(N_S)D_S}_{\substack{\mathrm{Student}\\\mathrm{Training}}}
    +\underbrace{3F(N_T)D_T}_{\substack{\mathrm{Teacher}\\\mathrm{Training}}}.
    \label{eq:compute-teacher-pretraining}
\end{equation}
This scenario represents when we want to figure out which teacher to produce to distill into sufficiently many different students,
storing the teacher logits for reuse,
effectively ammortizing the cost of producing the logits.
Here, contrary to the previous two scenarios (\Cref{sssec:distillation-best-case,sssec:distillation-teacher-pretraining}),
the teacher size $N_T$ \emph{and} teacher tokens $D_T$ contribute directly to the compute accounting (\Cref{eq:compute-teacher-pretraining}).
The optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the cross entropies in 
\Cref{fig:compute-optimal-distillation-student-loss-app}
are shown in \Cref{fig:compute-optimal-contours-teacherpre-app,fig:compute-optimal-distillation-teacherpre-app}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{plots/compute_optimal_distillation_contours_teacherpre.pdf}
	\caption{\textbf{Compute optimal configuration contours for distillation (teacher pretraining).} The compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the student cross entropies for \emph{teacher pretraining} in \Cref{fig:compute-optimal-distillation-student-loss-app}.
	}
	\label{fig:compute-optimal-contours-teacherpre-app}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_teacherpre_app.pdf}
	\caption{\textbf{Compute optimal configurations for distillation (teacher pretraining).} For eight student sizes, the compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the student cross entropies for \emph{teacher pretraining} in \Cref{fig:compute-optimal-distillation-student-loss-app}. This is a one-dimensional size of \Cref{fig:compute-optimal-contours-teacherpre-app}.
	}
	\label{fig:compute-optimal-distillation-teacherpre-app}
\end{figure}

\paragraph{The compute optimal teacher for distillation is a supervised compute optimal teacher.}
In \Cref{fig:compute-optimal-distillation-teacherpre-app}
we see that the $M_T\equiv D_T/N_T$ ratio of the teacher is constant for all values of compute, 
and can be compared to the ratio in \Cref{fig:compute-optimal-contours-bestcase-app}.
This can be understood as there is no inference cost to pay for making the teacher large;
we are only minimizing the training compute budgets of two models,
and the most efficient way to produce a teacher with a given cross-entropy $L_T$ is a teacher that is compute-optimal in a supervised sense.
Note that this conclusion is the \emph{opposite} to the finding in \Cref{sssec:distillation-teacher-inference}.
There, the inference is expensive, and so the teacher should be \emph{overtrained}.
Here, teacher training is expensive, so teacher training should be \emph{compute optimal}.

\paragraph{As compute is increased, relatively less should be spent on teacher training, and more on student training.}
In \Cref{fig:compute-optimal-allocation-teacherpre-app} we see the compute allocations for the configurations shown in 
\Cref{fig:compute-optimal-distillation-teacherpre-app},
and see that student training relative compute (blue) increases with increasing compute budget, 
while the teacher training (green) decreases with increasing compute budget.
This happens because, as in all compute scenarios, with increasing compute, the optimal student tokens $N_S^*$ increases 
(\Cref{fig:compute-optimal-distillation-teacherpre-app}).
Teacher size and tokens are also increasing with increasing compute, providing a stronger signal for the student with more tokens to learn.
However, this increase in teacher size and tokens plateaus, while the student tokens continues to increase.
This is because here the teacher is compute optimal, and so the amount of compute needed to improve the learning signal for the student is much less than the amount of compute needed to train the student for to make use of that signal, due to the stronger diminishing returns with respect to $D_S$ at a fixed $N_S$ (\Cref{eq:distillation-scaling-law}).

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_allocation_teacherpre_app.pdf}
	\caption{\textbf{Compute optimal allocations for distillation (teacher pretraining).} For eight student sizes, the compute optimal allocations corresponding to the terms in \Cref{eq:distillation-compute-app} for the compute optimal values in \Cref{fig:compute-optimal-distillation-teacherpre-app}.}
	\label{fig:compute-optimal-allocation-teacherpre-app}
\end{figure}

\FloatBarrier
\subsubsection{Distillation (teacher pretraining + inference)}
\label{sssec:distillation-teacher-pretraining-inference}

In the \emph{distillation (teacher pretraining + inference)} scenario, 
$\delta_T^{\mathrm{Lgt}}=\delta_T^{\mathrm{Pre}}=1$,
which means that we account for all costs associated with distilling a single student 
\begin{equation}
    \mathrm{FLOPs}(N_S,D_S,N_T,D_T)\approx
    \underbrace{3F(N_S)D_S}_{\substack{\mathrm{Student}\\\mathrm{Training}}}
    +\underbrace{F(N_T)D_S}_{\substack{\mathrm{Teacher}\\\mathrm{Logits}}} + \underbrace{3F(N_T)D_T}_{\substack{\mathrm{Teacher}\\\mathrm{Training}}}.
    \label{eq:compute-teacher-pretraining-inference}
\end{equation}
This scenario can be thought of as the compute optimal \emph{worst case} scenario for distillation, i.e. \emph{one teacher} 
is trained \emph{only} for the purposes of \emph{one} student.
As in \Cref{sssec:distillation-teacher-inference}, teacher size $N_T$ \emph{and} teacher tokens $D_T$ contribute directly to the compute accounting (\Cref{eq:compute-teacher-pretraining-inference}).
The optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the cross entropies in 
\Cref{fig:compute-optimal-distillation-student-loss-app}
are shown in \Cref{fig:compute-optimal-contours-teacherpreinf-app,fig:compute-optimal-distillation-teacherpreinf-app}.

\paragraph{Compute optimal teachers should be used for lower compute budgets and overtrained teachers should be used for larger compute budgets.}
In \Cref{fig:compute-optimal-distillation-teacherpreinf-app}
we see a teacher configuration that interpolates between the \emph{teacher pretraining} (\Cref{sssec:distillation-teacher-pretraining})
and \emph{teacher inference} (\Cref{sssec:distillation-teacher-inference}) compute scenarios.
At low compute, the optimal number of student tokens $D_S^*$ is not too large, this means there is little penalty to increasing the teacher size,
resulting in an approximately supervised compute-optimal teacher given a teacher compute budget.
Once the optimal number of student tokens becomes higher than the optimal number of teacher tokens,
there is significant penalty to increasing the teacher size.
At this point, the teacher solution starts to become the overtrained solution seen in \emph{teacher inference},
the optimal teacher tokens continue to increase polynomially, but this is not followed with an increase in the teacher size.
For sufficiently high compute, corresponding to a large number of student distillation tokens, 
the compute penalty for teacher size is so large that optimal teacher size decreases with compute.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{plots/compute_optimal_distillation_contours_teacherpreinf.pdf}
	\caption{\textbf{Compute optimal configuration contours for distillation (teacher pretraining + inference).} The compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the student cross entropies for \emph{teacher pretraining + inference} in \Cref{fig:compute-optimal-distillation-student-loss-app}.
	}
	\label{fig:compute-optimal-contours-teacherpreinf-app}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_teacherpreinf_app.pdf}
	\caption{\textbf{Compute optimal configurations for distillation (teacher pretraining + inference).} For eight student sizes, the compute optimal quantities ($D_S^*$, $N_T^*$, $D_T^*$) giving rise to the student cross entropies for \emph{teacher pretraining + inference} in \Cref{fig:compute-optimal-distillation-student-loss-app}. This is a one-dimensional size of \Cref{fig:compute-optimal-contours-teacherpreinf-app}.
	}
	\label{fig:compute-optimal-distillation-teacherpreinf-app}
\end{figure}

\paragraph{For small students, as compute grows, more should be spent on training the student and producing logits for the student.}
In \Cref{fig:compute-optimal-allocation-teacherpreinf-app} we see the compute allocations for the configurations shown in 
\Cref{fig:compute-optimal-distillation-teacherpreinf-app}.
Compute optimal smaller models tend to have smaller teachers, 
and optimal teacher tokens always grow at a slower rate than student tokens,
and so teacher the training cost is relatively small.
As compute grows, the student is distilled on more tokens,
and the teacher always becomes slightly larger than the student,
which gives rise to most compute being allocated to standard student training compute component and producing the logits for this training.

\paragraph{For large students, as compute grows, more should be spent on training the teacher, until a transition happens where more should be spent on training the student and producing logits for the student.}
The explanation for the phenomenon is as above, except that the larger students need a more capable teacher to learn from as compute grows, and so initially compute needs to bused to produce the teachers required.
After a certain amount of compute, the large number of optimal student distillation tokens moves the optimal solution towards an overtrained teacher scenario, and more compute being allocated to student training and logit production.

\begin{figure}[h]
	\centering
	\includegraphics[width=\bigwidth]{plots/compute_optimal_allocation_teacherpreinf_app.pdf}
	\caption{\textbf{Compute optimal allocations for distillation (teacher pretraining).} For eight student sizes, the compute optimal allocations corresponding to the terms in \Cref{eq:distillation-compute-app} for the compute optimal values in \Cref{fig:compute-optimal-distillation-teacherpreinf-app}.
	}
	\label{fig:compute-optimal-allocation-teacherpreinf-app}
\end{figure}

\FloatBarrier
\subsubsection{Optimal teacher training and student distillation tokens}
\label{sssec:training-tokens}

To aid in comparing the different compute strategies presented in \Cref{sssec:distillation-best-case,sssec:distillation-teacher-inference,sssec:distillation-teacher-pretraining,sssec:distillation-teacher-pretraining-inference},
we now present each compute optimal value for all strategies, including supervised.
Here, we show compute-optimal distillation student tokens $D_S^*$ in 
\Cref{fig:compute-optimal-distillation-teacher-tokens-app}
and compute-optimal teacher pretraining tokens $D_T^*$ in \Cref{fig:compute-optimal-distillation-teacher-tokens-app}.

\begin{figure}[h]
	\centering
        \vspace{-0.2cm}
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_D_S_app.pdf}
        \vspace{-0.2cm}
	\caption{\textbf{Compute optimal distillation student tokens.} For eight student sizes, the compute optimal student tokens $D_S^*$ giving rise to the student cross-entropies for all compute scenarios, including supervised.
	}
        \vspace{-0.2cm}
	\label{fig:compute-optimal-distillation-student-tokens-app}
\end{figure}

\paragraph{In all scenarios, student tokens should be increased with compute similar to in the supervised case.}
We see in \Cref{fig:compute-optimal-distillation-student-tokens-app}
that, as in Chinchilla \citep{DBLP:journals/corr/abs-2203-15556},
supervised tokens are increased polynomially with compute.
\emph{Distillation (best case)} follows the exact same allocation,
as does \emph{distillation (pretraining)} with asymptotically large compute.
All other methods follow the same increase rate, but with scenario-dependent offsets.

\begin{figure}[h]
	\centering
        \vspace{-0.2cm}
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_D_T_app.pdf}
        \vspace{-0.2cm}
	\caption{\textbf{Compute optimal distillation teacher tokens.} For eight student sizes, the compute optimal teacher tokens $D_T^*$ giving rise to the student cross-entropies for all compute scenarios.
	}
	\label{fig:compute-optimal-distillation-teacher-tokens-app}
        \vspace{-0.2cm}
\end{figure}

\paragraph{Optimal teacher tokens interpolate between scenarios based on compute allocation.}
In \Cref{fig:compute-optimal-distillation-teacher-tokens-app}
we can see more clearly the interpolation behavior discussed in 
\Cref{sssec:distillation-teacher-pretraining-inference}.
At low compute, \emph{teacher pretraining} and \emph{teacher pretraining + inference}
share optimal solutions because the number of student tokens $N_S^*$ is small.
At high compute, 
\emph{teacher pretraining + inference} approaches \emph{teacher inference},
while 
\emph{teacher pretraining}
approaches \emph{best case},
as 
$N_S^*$ is large, and
costs associated with teacher pretraining become less important.

\FloatBarrier
\subsubsection{Optimal teacher size}
\label{sssec:teacher-size}

\begin{figure}[h]
	\centering
        \vspace{-0.2cm}
	\includegraphics[width=\bigwidth]{plots/compute_optimal_distillation_N_T_app.pdf}
        \vspace{-0.2cm}
	\caption{\textbf{Compute optimal distillation teacher size.} For eight student sizes, the compute optimal teacher size $N_T^*$ giving rise to the student cross-entropies for all compute scenarios.
	}
        \vspace{-0.2cm}
	\label{fig:compute-optimal-distillation-teacher-size-app}
\end{figure}

\paragraph{Optimal teacher size interpolate between scenarios based on compute allocation.}
As in the optimal teacher tokens $N_T^*$ in \Cref{fig:compute-optimal-distillation-teacher-tokens-app},
the same mechanism causes interpolation behavior in optimal teacher size (see \Cref{fig:compute-optimal-distillation-teacher-size-app}).

\subsection{Compute and data efficiency gains for distillation compared to supervised learning}
In this final section, we use the compute-optimal strategies developed through 
\Cref{sssec:distillation-best-case,sssec:distillation-teacher-pretraining,sssec:distillation-teacher-inference,sssec:distillation-teacher-pretraining-inference}
and understand, for each distillation compute scenario (\Cref{tab:compute-scenarios-app})
if it is more compute and/or data efficient to use distillation compared to supervised learning
in order to produce a desired model (i.e. of a given size $N_S$ with a desired performance, measured in cross-entropy $L_S$).

In \Cref{fig:compute-optimal-distillation-compute-ratios-app} we show the amount of compute needed to distill a student of a given size to a given cross-entropy as a multiple of the compute that supervised learning needs to produce the same result.
We do this for for each of the distillation compute scenarios, whose optimal configurations are given in
\Cref{sssec:distillation-best-case,sssec:distillation-teacher-pretraining,sssec:distillation-teacher-inference,sssec:distillation-teacher-pretraining-inference}.
In \Cref{fig:compute-optimal-distillation-data-ratios-app}
we show the same, except we show the number of tokens needed to distill a student of a given size to a given cross-entropy as a multiple of the number of tokens that supervised learning needs to produce the same result.
Our distillation token accounting depends on compute scenario:
\begin{equation}
    D_{\mathrm{Dist.}} = D_S + \delta_T^{\mathrm{Pre}} D_T,
\end{equation}
i.e. we only count teacher tokens if the teacher pretraining cost is also included in the compute cost (see \Cref{eq:distillation-compute-app}).

\begin{figure}[h]
	\centering
	\includegraphics[width=0.98\textwidth]{plots/supervised_distillation_compute_ratios.pdf}
	\caption{\textbf{Compute optimal distillation compute ratios.} For eight student sizes, the amount of supervised compute needed to produce a student of the indicated size and cross-entropy.
    The horizontal dashed line indicates the break-even point, when doing supervised leaning is as computationally efficient as the corresponding distillation compute scenario.
    Values greater (less) than one indicate distillation is more (less) expensive than supervised learning for producing a model of the indicated size and cross-entropy. 
    The vertical dashed line indicates the lowest cross-entropy achievable by that student.
	}
	\label{fig:compute-optimal-distillation-compute-ratios-app}
\end{figure}

\paragraph{When teacher training is discounted, distillation is often more efficient.}
In
\Cref{fig:compute-optimal-distillation-compute-ratios-app},
the \emph{base case} (blue) and 
\emph{teacher inference} (orange) compute scenarios
are below the grey dashed line for cross-entropies
slightly above the lowest possible cross-entropy (vertical grey dashed line), 
meaning less compute is needed for distillation 
than supervised learning.
This compute efficiency translates into data efficiency (see \Cref{fig:compute-optimal-distillation-data-ratios-app}).

\paragraph{To produce the strongest student possible, supervised learning is more efficient.}
In
\Cref{fig:compute-optimal-distillation-compute-ratios-app,fig:compute-optimal-distillation-data-ratios-app},
the \emph{base case} (blue) and 
\emph{teacher inference} (orange) compute scenarios
attain values larger than one as the target cross-entropy $L_S$
approaches the limiting value $L(N=N_S,D=\infty)$ for each student size $N_S$,
(vertical dashed line).
This suggests i) the existence of a more efficient training strategy where distillation is used as an initial training stage, with a transition to supervised learning based on a token or cross-entropy threshold, and ii)
potentially increased importance of data mixtures ($\lambda\leq1$, see \Cref{ssec:lambda-sensitivity}) when distilling with significant token and/or compute budgets. 
We leave this for future work.

\paragraph{In situations where teacher training is required, supervised learning is more efficient.}
As observed in \Cref{sssec:cross-entropy},
for all student sizes, if teacher pretraining is included in the computational cost of producing a student,
supervised learning is always more efficient than distilling.
This can be seen from
\Cref{fig:compute-optimal-distillation-compute-ratios-app}
as the \emph{teacher pretraining} (green) and 
\emph{teacher pretraining + inference} (red) compute scenarios
are above the grey dashed line, which means more compute is needed for distillation 
than supervised learning in those compute scenarios.
This compute efficiency translates into data efficiency (see \Cref{fig:compute-optimal-distillation-data-ratios-app}).

\begin{figure}[h]
	\centering
	\includegraphics[width=0.98\textwidth]{plots/supervised_distillation_data_ratios.pdf}
	\caption{\textbf{Compute optimal distillation data ratios.} For eight student sizes, the number of tokens compute needed to produce a student of the indicated size and cross-entropy.
    The horizontal dashed line indicates the break-even point, when doing supervised leaning is as data efficient as the corresponding distillation compute scenario.
    Values greater (less) than one indicate distillation is more (less) expensive than supervised learning for producing a model of the indicated size and cross-entropy. 
    The vertical dashed line indicates the lowest cross-entropy achievable by that student.
	}
	\label{fig:compute-optimal-distillation-data-ratios-app}
\end{figure}

\paragraph{Distillation is more efficient for larger students.}
In
\Cref{fig:compute-optimal-distillation-compute-ratios-app}
we see in the \emph{pretrain + inference} scenario, producing a $N_S=$500M student with a cross-entropy of 2.4
has roughly 3/4 the compute cost of producing the same model with supervised learning,
whereas producing a $N_S=$10B student with a cross-entropy of 2.2
has roughly 1/2 the compute cost of producing the same model with supervised learning.
In terms of data (\Cref{fig:compute-optimal-distillation-data-ratios-app}),
the 500M and 10B configurations
use roughly 2/3 and 1/2 the number of tokens of their supervised counterparts respectively.
\emph{The efficiency gains from distillation are potentially greater for larger students when considering compute or data.}
