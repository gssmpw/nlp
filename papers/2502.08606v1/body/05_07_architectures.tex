\section{Model architecture}
\label{sec:model-architecture}

All models are based on
\citet{DBLP:journals/corr/abs-2407-21075}
and are trained using \texttt{AXLearn} \citep{axlearn}.
All models use decoupled weight
decay~\citet{DBLP:conf/iclr/LoshchilovH19} of $10^{-4}$ for regularization, as well as a simplified version of
\gls{mup}~\citep{DBLP:conf/icml/YangH21,DBLP:journals/corr/abs-2308-01814,DBLP:journals/corr/abs-2203-03466,DBLP:journals/corr/abs-2309-14322,DBLP:journals/corr/abs-2310-17813},
following what is described as \gls{mup} (simple) in~\cite{DBLP:conf/iclr/WortsmanLXEAACG24}. Because of \gls{mup} (simple), we fix the learning rate to $1e-2$ across all model sizes.
Multi-headed attention (MHA) is used ($\gsize=1$), with
Pre-Normalization~\cite{DBLP:conf/iwslt/NguyenS19} using RMSNorm~\cite{DBLP:conf/nips/ZhangS19a}.
We train all models with a sequence length of $\nctx=4096$, with RoPE \citep{DBLP:journals/ijon/SuALPBL24} positional embeddings (base frequency set to $500\mathrm{k}$).
All model architectures in this work are presented in \Cref{tab:architectures},
have a \emph{fixed aspect ratio} $\dmodel=128$
and a \emph{fixed ffn ratio} $\rffn=8/3$ coupled with gated linear activation ($\nffn=3$).

\input{body/05_07_architectures_table}

We rescale the gradients, such that the maximum of the global norm is $1.0$. A cosine learning rate schedule is used with warmup (2000 steps), with a final learning rate of one thousandths of the peak learning rate. A Z-loss \citep{DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23} of $10^{-4}$ is used for stability, slightly decreasing norm growth at the end of the training.

For all experiments, the English-only subset of the C4 dataset \citep{DBLP:journals/jmlr/RaffelSRLNMZLL20} is used. The C4 dataset was chosen because of its wide usage in the research community. While C4 is big enough for larger-scale experiments, it is small enough to allow for reproduction of experiments.
For all distillation trainings, the teacher is trained on a different split as the student. The C4 dataset has roughly 180B tokens in total, which results in 90B unique tokens for the teacher training and 90B unique tokens for the student training. Except for the largest models, all Chinchilla-optimal models do not repeat data. Models that overtrain on more than 90B tokens will have data repetition too. \citet{DBLP:conf/nips/MuennighoffRBST23} has shown (on the C4 dataset) that repeating data up to $4$ times has negligible impact to loss compared to having unique data.
