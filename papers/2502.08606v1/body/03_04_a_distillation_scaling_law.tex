\section{Distillation Scaling Laws}
\label{sec:distillation-scaling-laws}

Here we outline the steps taken to arrive at our distillation scaling law.
First we describe the experimental setting (\Cref{ssec:experimental-setup})
and the experiments
needed to determine the scaling coefficients
(\Cref{ssec:distillation-scaling-law-experiments}).
Given the empirical observations, we discuss the form our distillation scaling law takes
(\Cref{ssec:distillation-scaling-law-functional-form}),
find the coefficients, and verify the law under extrapolation
(\Cref{ssec:distillation-scaling-law-parameteric-fit}).

\subsection{Experimental setup}
\label{ssec:experimental-setup}

All models are based on
\citet{DBLP:journals/corr/abs-2407-21075} and use decoupled weight
decay~\citet{DBLP:conf/iclr/LoshchilovH19} for regularization, as well as a simplified version of
\gls{mup}~\citep{DBLP:conf/icml/YangH21,DBLP:journals/corr/abs-2308-01814,DBLP:journals/corr/abs-2203-03466,DBLP:journals/corr/abs-2309-14322,DBLP:journals/corr/abs-2310-17813},
following \gls{mup} (simple) in~\cite{DBLP:conf/iclr/WortsmanLXEAACG24}.
\gls{mup} simplifies the scaling law experimental setup as it enables \emph{hyperparameter transfer} of the learning rate across model sizes.
We validate that \gls{mup} functions as expected for distillation
in \Cref{ssec:lr-sensitivity}.
Models have sizes which range from 143M to 12.6B parameters, and we allow the teacher to be smaller or larger than the student.
Multi-headed attention (MHA) is used, with
Pre-Normalization~\cite{DBLP:conf/iwslt/NguyenS19} using RMSNorm~\cite{DBLP:conf/nips/ZhangS19a}.
We train all models with a sequence length of 4096, with \gls{rope} \citep{DBLP:journals/ijon/SuALPBL24}. 
We use the English-only subset of the C4 dataset \citep{DBLP:journals/jmlr/RaffelSRLNMZLL20} for all experiments.
For all distillation trainings, the teacher is trained on a different split from the student. Except for the largest models, all Chinchilla-optimal models do not repeat data. Full hyperparameters and details can be found in \Cref{sec:model-architecture}.
As our goal is to understand the role of the teacher in the distillation process
we distill in the \emph{pure distillation} case ($\lambda=1$, \Cref{eqn:kd_and_nll_full_loss}) to avoid confounding coming from the data, as was done in \citet{DBLP:conf/nips/StantonIKAW21}.
We verify the choice $\lambda=1$ produces results statistically similar to the optimal $\lambda^*$ (see \Cref{ssec:lambda-sensitivity}).
Similarly, all experiments use distillation temperature ($\tau=1$),
as we found this produces the best performing students
(see \Cref{ssec:temperature-tau-sensitivity}).

\subsection{Distillation Scaling Law Experiments}
\label{ssec:distillation-scaling-law-experiments}
Here we discuss the experiments that produce the data for fitting our distillation scaling law.
\begin{table}[t]
    \centering
    \caption{Expressions related to scaling laws used in this work. In each case, $S$ always refers to \emph{student} and \emph{not} supervised.}    
    \rowcolors{2}{AppleChartGrey2}{white}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{rp{8.8cm}}
        \toprule
        Expression & Meaning \\ \midrule
        $N$\,/\,$N_{S}$\,/\,$N_{T}$ & The number of model/student/teacher \emph{non-embedding} parameters. Whenever we mention parameters in text, we always mean \emph{non-embedding} parameters unless explicitly stated otherwise. See \Cref{ssec:model-parameters} for more details. \\
        $D$\,/\,$D_T$ & The number of tokens the model/teacher is pretrained on. \\
        $D_S$ & The number of tokens the student is distilled on. \\
        $M\equiv D/N$ & The tokens per parameter ratio, or $M$-ratio. In \citet{DBLP:journals/corr/abs-2203-15556}, $M$ takes a compute optimal value $M^*\approx 20$ which is the \emph{Chinchilla rule of thumb}. \\
        $L\approx L(N,D)$ & The \emph{model cross-entropy}, which is the model validation cross entropy \emph{under data}, estimated by the supervised scaling law for a model with $N$ parameters trained on $D$ tokens. (\Cref{eq:supervised-scaling-law}). \\
        $L_T\approx L(N_T,D_T)$ & The \emph{teacher cross-entropy}, which is the teacher validation cross entropy \emph{under data}, estimated by the supervised scaling law for a teacher with $N_T$ parameters trained on $D_T$ tokens. \\
        $L_S\approx L_S(N_S,D_S,L_T)$ & The \emph{student cross-entropy}, which is the student validation cross entropy \emph{under data}, estimated by our distillation scaling law for a student with $N_S$ parameters distilled on $D_S$ tokens using a teacher with pretraining loss $L_T$ (\Cref{eq:distillation-scaling-law}). \\
        $\widetilde{L}_S\approx L(N_S,D_S)$ & The \emph{student supervised cross-entropy}, which is the student validation cross entropy \emph{under data if the student had been trained in a supervised way}, estimated by the supervised scaling law for a student with $N_S$ parameters trained on $D_S$ tokens. \\
        \bottomrule
    \end{tabular}
    \vspace{-0.1cm}
    }
    \label{tab:scaling-law-notation}
\end{table}
The distillation scaling law will estimate \emph{student cross-entropy} $L_S$\footnote{By \emph{cross-entropy}, we always mean with respect to \emph{data}, \emph{not} the teacher.
We summarize our scaling law notation in \Cref{tab:scaling-law-notation}.}, which in general depends on the student parameters $N_S$, number of distillation tokens $D_S$, the teacher parameters $N_T$ and the number of teacher training tokens $N_T$: $L_S\approx L_S(N_S,D_S,N_T,D_T)$.
As discussed in \Cref{sec:background},
only certain combinations of data support reliable identification of scaling law coefficients.
We combine three experimental protocols to produce data for our distillation scaling law fit.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.46\textwidth]{plots/fixedm_teacher_isoflop_students.pdf}
    \vspace{-0.2cm}
	\caption{\textbf{Fixed $\bm M$ Teacher/Student IsoFLOP profiles.}
    Two (of a total of six) teachers with $M_T=D_T/N_T\approx 20$ %
    are distilled into students with four IsoFLOP profiles, and a small number with $C_S=3\times 10^{21}$.
    Horizontal and vertical dashed lines indicate teacher cross entropy $L_T$ and size $N_T$ respectively.
    See \Cref{ssec:distillation-isoflop-profiles}, \Cref{fig:fixedm-teacher-isoflop-students-app} for all six profiles.
	}\vspace{-0.15cm}
	\label{fig:fixedm-teacher-isoflop-students}
\end{figure}

\paragraph{Fixed $\bm M$ Teachers/Student IsoFLOPs}
To simplify the experimental protocol we make the following assumption:
\emph{Training a student $(N_S,D_S)$ on the signal provided by a teacher $(N_T,D_T)$ is qualitatively similar to training that student on fixed dataset.}
As power law behavior has been observed in a wide variety of datasets and domains \citep{DBLP:journals/corr/abs-2010-14701}, it is expected that there should be a power law behavior in ($N_S$, $D_S$) given a fixed teacher.
To identify these coefficients correctly,
a similar protocol to the Chinchilla protocol described in \Cref{sec:background} should be performed.
However, we cannot only do this for \emph{only} one teacher, as the way student size and tokens affects downstream performance may be different for different teachers,
just as the scaling laws are different for different domains and dataset.
For distillation we anticipate this is the case so that different teachers produce different students.
To produce the widest range of teachers for a compute budget, we train six Chinchilla-optimal ($M_T=D_T/N_T\approx 20$) teachers ranging from 198M to 7.75B parameters.
\footnote{We generally refer to these as \emph{fixed-m} models rather than \emph{Chinchilla-optimal} models as we do not yet know whether $M\approx 20$ is a good choice in this specific setting.}
For each of those teachers, we distill into students with four IsoFLOP profiles, taking only the standard training cost into account.
The resulting student cross-entropies are in \Cref{fig:fixedm-teacher-isoflop-students}.
We note that in some cases, the student is able to outperform the teacher, i.e. exhibits \emph{weak-to-strong-generalization} \citep{DBLP:conf/icml/BurnsIKBGACEJLS24,DBLP:journals/corr/abs-2410-18837}
and investigate this further in \Cref{ssec:weak-to-strong-generalization}.


\begin{figure}[h]
    \vspace{-0.1cm}
    \subfloat[One teacher IsoFLOP set.]{
        \includegraphics[width=0.22\textwidth]{plots/isoflop_teacher_fixedm_students.pdf}
        \label{fig:isoflop-teacher-fixedm-students-size}
    }
    \hfill
    \subfloat[All teacher IsoFLOPs.]{
        \includegraphics[width=0.22\textwidth]{plots/isoflop_teacher_fixedm_students_tloss.pdf}
        \label{fig:isoflop-teacher-fixedm-students-teacher-loss}
    }
        \caption{\textbf{IsoFLOP Teacher/Fixed $\bm M$ Students.} \textbf{(a)} One (of four) student sizes
    trained with a $M_S=D_S/N_S=20$ are distilled from teachers with four IsoFLOP profiles.
        See \Cref{ssec:distillation-isoflop-profiles}, \Cref{fig:isoflop-teacher-fixedm-students-app} for all profiles. \textbf{(b)} All profiles against teacher cross-entropy.
        Horizontal (vertical) dashed lines show student supervised cross entropy $\widetilde{L}_S$ (student size $N_S$).
	}\vspace{-0.15cm}
\label{fig:isoflop-teacher-fixedm-students}
\end{figure}



\paragraph{IsoFLOP Teachers/Fixed $\bm M$ Students}
The fixed-$M$ teacher IsoFLOP student protocol is insufficient to identify how $N_T$ and $D_T$ \emph{independently} influence student cross-entropy.
To ensure our experiment can detect this influence,
we perform experiments where the student ($N_S$, $D_S$) is fixed, and vary $N_T$ and $D_T$ subject to a compute constraint, i.e. a teacher IsoFLOP.
We perform distillations into four Chinchilla-optimal ($M_S=D_S/N_S\approx 20$) students  ranging from 198M to 1.82B parameters
from teachers trained according to four IsoFLOP profiles.
The resulting student cross-entropies are in \Cref{fig:isoflop-teacher-fixedm-students}.

\paragraph{Fixed $\bm M$ Teachers/Fixed $\bm M$ Students}
Finally, although not necessary for fitting our distillation scaling law, it is instructive to see how student cross entropies vary over as large a range as possible.
To achieve this, we train fixed-$M$ teacher fixed-$M$ student combinations,
with ten teachers with $M_T\approx 20$,
and students of five sizes, with at least four choices of $M_S$ per student.
The resulting student cross-entropies for two of the students are in \Cref{fig:fixedm-teacher-fixedm-students}.\todo{We could still add the others in.}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.46\textwidth]{plots/fixedm_teacher_fixedm_student_main_nokld.pdf}
    \vspace{-0.2cm}
	\caption{\textbf{Fixed $\bm M$ Teacher/Fixed $\bm M$ Student.} Students of two sizes trained with different $M_S=D_S/N_S=20$ ratios are distilled from teachers with $M_T=D_T/N_T\approx 20$.
	}
    \vspace{-0.15cm}
	\label{fig:fixedm-teacher-fixedm-students}
\end{figure}
\paragraph{Capacity gap}
In \Cref{fig:fixedm-teacher-fixedm-students}, 
we observe the \emph{capacity gap}, where \emph{improving teacher performance does not always improve student performance, and even reduces student performance eventually}.
The capacity gap has been observed often in distillation (see \Cref{ssec:the-capacity-gap}).
The \gls{kld} between teacher and student is an increasing function of teacher capability in all cases (see \Cref{ssec:fixed-m-teacher-fixed-m-students}), which means as the teacher improves its own performance, the student finds the teacher more challenging to model, eventually preventing the student from taking advantage of teacher gains.
We use calibration metrics to investigate aspects that the student finds challenging to model in  \Cref{ssec:model-calibration}.
In \Cref{ssec:kernel-regression,ssec:mlps-on-the-mapping-problem} we offer a simple explanation in a kernel regression and synthetic \gls{mlp} setting and,  to the best our knowledge, are the first controlled demonstrations of the capacity gap.

\subsection{Distillation Scaling Law Functional Form}
\label{ssec:distillation-scaling-law-functional-form}
We need to determine the functional form of the distillation scaling law.
First, we observe that \emph{contributions from teacher size} $N_T$ \emph{and pretraining tokens} $D_T$ are summarized by the teacher cross-entropy $L_T$.
This can be seen from \Cref{fig:distillation-scaling-law-fig1,fig:isoflop-teacher-fixedm-students-teacher-loss} which contains the
IsoFLOP Teacher/Fixed $\bm M$ Students of \Cref{fig:isoflop-teacher-fixedm-students}, yet only smooth dependence as a function of $L_T$ is observed.
Next, the distillation scaling law should reflect the following properties:
\begin{enumerate}
    \item An \emph{infinitely capable student} should be able to model \emph{any} teacher: $\lim_{N_S,D_S\to\infty}L_S(N_S,D_S,L_T)\rightarrow L_T$.
    \item A \emph{random} teacher produces \emph{random} students \emph{independent} of how capable those students are: $\lim_{L_T\to\infty}L_S(N_S,D_S,L_T)\rightarrow L_T$.
    \item There is a \emph{capacity gap}: making a teacher too capable eventually reduces the student performance.
\end{enumerate}
A transition between two power law regions: i) where the student is a stronger learner than the teacher, and ii) where the student is a weaker learner than the teacher is described by a broken power law \citep{DBLP:conf/iclr/CaballeroGRK23}.
Together, we propose that student cross-entropy follows a broken power law in $L_T$ and a power law in $N_S$ and $D_S$:{
		\medmuskip=1.5mu
		\thinmuskip=1.5mu
		\thickmuskip=1.5mu
		\begin{align}
			\underbrace{L_S(N_S,D_S,L_T)}_{\text{Student cross-entropy}}\,
			=\underbrace{L_T}_{\text{Teacher cross-entropy}} \qquad \quad\nonumber \\+
			\underbrace{\frac1{L_T^{c_0}}
			\left(1+\left(\frac{L_T}{\widetilde{L}_S d_1}\right)^{1/{f_1}}\right)^{-c_1f_1}
			\left(\frac{A}{N_S^{\alpha^\prime}}+\frac{B}{D_S^{\beta^\prime}}\right)^{\gamma^\prime}}_{\text{Student ability to mimic teacher}}
			\label{eq:distillation-scaling-law}
		\end{align}
	}where $\{c_0,c_1,d_1,f_1,\alpha^\prime,\beta^\prime,\gamma^\prime\}$ are positive coefficients to be fitted following the procedure outlined in \Cref{ssec:distillation-scaling-law-coefficient-estimation} on the data produced in \Cref{ssec:distillation-scaling-law-experiments}.
The first two properties of our distillation scaling law can be readily checked.
For the third,
recall, $\widetilde{L}_S=L(N_S,D_S)$ is the cross-entropy a student would have achieved if it had been trained in a supervised way (\Cref{tab:scaling-law-notation}), and is determinable from the supervised scaling law (\Cref{eq:supervised-scaling-law}).
The capacity gap behavior follows from a transition based on the ratio of the \emph{algorithmic learning capacities} of the student and teacher, when
$
        L_T/\widetilde{L}_S
	\equiv
        L(N_T,D_T)/L(N_S,D_S)
	=
	d_1
$,
which can be interpreted as measure of the \emph{relative learning abilities} of the teacher and the student on a reference task.

\subsection{Distillation Scaling Law Parameteric Fit}
\label{ssec:distillation-scaling-law-parameteric-fit}

We use the teachers $(N_T, D_T)$ for fitting our supervised scaling law (\Cref{ssec:teachers-used-in-distillation}),
and all the data for fitting our distillation scaling law (\Cref{eq:distillation-scaling-law}).
Our fitting procedure is described in detail in \Cref{sec:scaling-coefficients}
and resulting scaling coefficients are presented in \Cref{ssec:scaling-law-coefficients-parameteric-fit}.
Our supervised and distillation scaling laws fit the observations at the level of $\lesssim1\%$ relative prediction error, including when extrapolated from weaker to stronger models (see \Cref{fig:distillation-scaling-law}).


\begin{figure}[h]
	\centering
	\subfloat[Supervised.]{
		\includegraphics[width=0.22\textwidth]{plots/scaling_law_supervised.pdf}
		\label{fig:supervised-scaling-law}
	}
	\hfill
	\subfloat[Distillation.]{
		\includegraphics[width=0.22\textwidth]{plots/scaling_law_distillation.pdf}
		\label{fig:distillation-scaling-law}
	}
    \vspace{-0.1cm}
	\caption{\textbf{Scaling law fits.}
		\textbf{(a)} The supervised scaling law (\Cref{eq:supervised-scaling-law}) applied to the data in \Cref{fig:supervised-fixed-long}.
		\textbf{(b)} Our distillation scaling law (\Cref{eq:distillation-scaling-law}) applied to the data in \Cref{fig:fixedm-teacher-isoflop-students,fig:isoflop-teacher-fixedm-students,fig:fixedm-teacher-fixedm-students}.}
        \vspace{-0.2cm}
	\label{fig:scaling-law-fits}
\end{figure}

As a further verification, we confirm that for a fixed model size, distillation in the infinite data regime is consistent with supervised learning on infinite data (\Cref{ssec:distillation-with-infinite-data}).
