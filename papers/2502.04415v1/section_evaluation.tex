\section{Evaluation}
\label{sec:eval}

To the best of our knowledge, there is no publically available dataset that is suitable for evaluating systems on the task of Text-to-SPARQL for Earth Observation archives. For this reason, we decided to deploy our engine as a pure geospatial QA engine and run an evaluation on the geospatial QA dataset GeoQuestions1089~\cite{geoquestions1089}. Although this did require some tinkering, since GeoQuestions1089 targets the YAGO2geo ontology, the process was straightforward and painless, and we believe that the resulting evaluation is useful for measuring the performance of most dimensions of our engine. Unfortunately, the questions in GeoQuestions1089 do not include temporal information.

To accept an answer as correct, it must match the gold result (included in GeoQuestions1089) exactly. We do not consider partially correct answers as correct. Likewise, for supersets of the answers in the gold set.

The results of our evaluation can be seen in Table~\ref{tab:geoq1089}. We benchmark \EngineName{} with the Query Enhancer disabled, since the model was fine-tuned on the same dataset, which would skew the results. We also compare our engine to GeoQA2 and the engine of Hamzei et al~\cite{hamzei}.

% \begin{table}[hbt]
% 	\centering
% 	\caption{Evaluation on GeoQuestions1089}\label{tab:geoq1089}
% 	\begin{tabular}{c c c c }
% 		\toprule
% 		\textbf{Category} & \textbf{GeoQA2 Accuracy} & \textbf{Hamzei Accuracy} & \textbf{\EngineName{} Accuracy} \\ \hline
%         A & 53.52\% & 28.16\% & 60.56\% \\ \hline
%         B & 62.68\% & 55.22\% & 73.13\% \\ \hline
%         C & 48.36\% & 30.06\% & 47.71\% \\ \hline
%         D & 9.09\% & 4.54\% & 22.73\% \\ \hline
%         E & 24.81\% & 6.56\% & 23.36\% \\ \hline
%         F & 28.57\% & 14.28\% & 38.10\% \\ \hline
%         G & 36.30\% & 12.32\% & 28.77\% \\ \hline
%         H & 23.07\% & 33.33\% & 40.17\% \\ \hline
%         I & 21.73\% &8.69\% & 26.00\% \\ \hline
%         ALL & 40.33\% & 25.92\% & 44.36\% \\
%         \bottomrule
% 	\end{tabular}
% \end{table}

\begin{table}[hbt]
    \centering
    \caption{Evaluation on GeoQuestions1089$_c$ v1.1}\label{tab:geoq1089}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{c c c c }
            \toprule
            \textbf{Category} & \textbf{GeoQA2 Accuracy} & \textbf{Hamzei Accuracy} & \textbf{\EngineName{} Accuracy} \\ \hline
            A & 53.52\% & 28.16\% & 60.56\% \\ \hline
            B & 62.68\% & 55.22\% & 73.13\% \\ \hline
            C & 48.36\% & 30.06\% & 47.71\% \\ \hline
            D & 9.09\% & 4.54\% & 22.73\% \\ \hline
            E & 24.81\% & 6.56\% & 23.36\% \\ \hline
            F & 28.57\% & 14.28\% & 38.10\% \\ \hline
            G & 36.30\% & 12.32\% & 28.77\% \\ \hline
            H & 23.07\% & 33.33\% & 40.17\% \\ \hline
            I & 21.73\% &8.69\% & 26.00\% \\ \hline
            ALL & 40.33\% & 25.92\% & 44.36\% \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

We can see that \EngineName{} outperforms the previous state of the art in most question categories without utilizing templates. All in all, there is 4\% uplift in performance over the entire dataset, which is translated to a 10\% improvement relative to GeoQA2. The categories with performance regressions are caused by \EngineName{}â€™s more dynamic nature. \EngineName{} does not use predefined query templates and employs heuristics for a number of processes as previously described, these heuristics are not performing as well for those particular question categories. 