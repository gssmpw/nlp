\section{Introduction}
\label{sec:intro}

LLMs have shown remarkable capabilities across various tasks. A key factor driving the rapid advancement is the availability of web-scale datasets. However, concerns have been raised regarding the use of such large-scale data, as it often includes copyrighted and privacy-protected data~\cite{hacker2023regulating, lucchi2024chatgpt}. 
{For instance, The New York Times sued OpenAI and Microsoft because their articles have been used in training GPT\footnote{\href{https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html}{https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html}}.}
{Meanwhile, the data is protected by General Data Protection Regulation (GDPR)~\cite{voigt2017eu}, and the data owners have the ``right to be forgotten''~\cite{rosen2011right}.}
Therefore, it is crucial to implement protections for these datasets. To address this, unlearning has been proposed to remove specific data from LLMs without requiring full retraining~\cite{liu2024large, liu2024rethinking}. The goal is to eliminate the influence of the \textbf{target data} or adjust the model behavior as if it had never encountered the target data.
% This method is post-training and is particularly valuable when certain data—referred to as \textbf{target data}—must be erased. The goal is to eliminate the influence of undesired content or adjust the model behavior as if it had never encountered the target data. 

LLM unlearning is typically a post-training method, with fine-tuning being widely adopted as an approach.
% To avoid full retraining, fine-tuning has been widely used.
Existing fine-tuning based unlearning methods can be roughly divided into two categories. 
% It falls into two main categories, each based on a distinct idea. 
One is \textbf{gradient ascent-based} ({GA-based}) methods, such as gradient ascent (GA)~\citep{jang2023knowledge, maini2024tofu} and its variants~\cite{yao2023large, liu2022continual, zhang2024negative, fan2024simplicity, veldanda2024llm, cha2024towards, liu2024towards, feng2024fine, bu2024unlearning, tian2024forget}. They negate the training impact of the target data by reversing the gradient descent loss. The other, \textbf{suppression-based} unlearning, does not aim to erase learned information directly~\cite{maini2024tofu, liwmdp, wang2024llm, huu2024effects, shi2024ulmr, liu2024towards, sinha2024unstar}. Instead, it explicitly tells the model about what constitutes target data and guides it to generate human-preferred outputs while suppressing those related to the target data~\footnote{{In addition to fine-tuning, other methods operating at the inference stage have also been proposed, such as in-context learning (ICL)~\cite{pawelczykcontext} and assistant models {\cite{huang2024offset}}. 
{Nonetheless, given the widespread adoption of fine-tuning, we focus on fine-tuning methods in this work.}
% However, fine-tuning-based approaches remain the most fundamental, straightforward, and widely adopted. Therefore, in this work we primarily focus on fine-tuning methods.
}}.

% \yue{[Need to discuss the following paragraph. Seems like existing observation and our new understandings are not clearly separated.]}

However, recent evaluations on fine-tuning-based methods reveal that {there is} a significant trade-off between unlearning and model utility, i.e., the model’s ability to respond to normal prompts unrelated to the target data~\cite{wang2024unlearning, si2023knowledge, wu2024evaluating}. 
% This trade-off arises likely because the target data or related knowledge is embedded within the large-scale model parameters, which have been optimized through extensive pre-training \yue{[\cite{}. 
% If no existing literature, need to rephrase the paragraph to state (1) what target we want to solve, (2) what is our conjecture, (3) what our preliminary finds]}. Effective unlearning may result in severe parameter distortion \yue{\cite{}}. 
This issue has been widely observed in LLM fine-tuning: as the fine-tuning dataset is small, it is likely to cause over-fitting and reduce the general ability~\cite{luo2023empirical, zhai2023investigating, howard2018universal}. 
% \jt{the above discussion is about fine-tuning method, I thought it includes both GA and suppression, why the following focus on GA?? the logic of this paragraph looks wired} {Furthermore, GA-based methods often reverse the training loss, thus, they may inadvertently hinder the training of normal text generation.}
Although they usually use retaining dataset to preserve the model utility~\citep{liu2022continual, shi2024muse}, its small size could limit the generalization.
% {The suppression-based method is also facing a similar utility challenge.}
% Additionally, GA-based methods suffer from the side effects of reversing the training loss, which may negatively impact the training of normal text generation. 
% \blue{Such a sacrifice may} reduce the practical value of unlearning. 




% \jie{Should we mentioned non-fine-tuning-based methods here?} \sw{you mean in-context unlearning? We can mention it in the related work, briefly introduce the high-level idea of in-context unlearning, and then mention that fine-tuning based is more popular. hence, we focus on fine-tuning based} \yue{+1. personally I don't believe the ICL-based unlearning method. It totally contradicts to my understanding in theoretical results in ICL.}

% \yue{Although various studies have proposed different methods to perform unlearning, there are still missing pieces in literature. First, there is limited understanding in the mechanism of the different types of unlearning methods: It is unknown whether these methods in principle have any differences/similarities, and what the key is in unlearning. Second, the utility of the unlearned model always drops compared to the original model. Therefore, in this paper, we aim to (1) investigate the mechanism of unlearning methods, and (2) propose a recipe applicable to these methods to enhance the utility. }

% \jt{in this paper, do we only focus on GA-based methods??? if yes, I think we should revise the second paragraph to only imention GA-based methods are one popular method while leaving other types in the related work. }

% Although GA-based seems solve the problem better, recent evaluations they cannot achieve what they claim

% \yue{[A bit confused: We say that the GA-based methods do not truly remove knowledge. But seems like suppression-based also does not truly remove knowledge? Why we mention two methods above but only one method here..? ]} 

% \yue{While suppression-based unlearning aim to distinguish the target data to other data, the GA-based methods seem more dedicated to ``unlearning'': since gradient descent aims to minimize the training loss, an ascent method intuitively reserves the learning process.} 

% Researcher recently find that although dedicated to negate the training on target data, GA-based methods may not truly remove target data~\cite{zhang2024catastrophic, wu2024evaluating, deeb2024unlearning, lynch2024eight}. For instance, \citet{deeb2024unlearning} demonstrates that fine-tuning with unrelated data can recover the knowledge that was supposed to be unlearned by GD. Observing this, 
% we conduct preliminary exploration to investigate GA-based unlearning and find
% demystify GA-based unlearning by showing 
% We find
% that these methods also operate by distinguishing and suppressing the target data. 

Therefore, we aim to develop a general framework to enhance the utility of fine-tuning-based LLM unlearning. However, the two types of fine-tuning-based methods are defined in totally different ways, posing a challenge in developing such a framework. Thus, 
we design a preliminary study to investigate the common property between GA-based and suppression-based methods ({Section~\ref{sec:prelim}}). We find that, although GA-based methods appear to be dedicated to negate the training of target data, the final GA-unlearned LLMs still recognize target data and actually treat it as a signal of unlearning. If target data is in the input, the representations exhibit a distinct pattern compared with the input irrelevant to target data. Then unlearned models suppress related generation. This suggests that
the GA-unlearned models also operate by distinguishing and suppressing target data, which closely resemble the models by suppression-based methods. 


% and the unlearned model will suppress target-related generation.

% exhibit a distinct pattern in the target data in the representation space compared with the input not related to target data. 

% This suggests that the target data's knowledge remains in the model and is just hidden by GA-based methods. {From this perspective, although GA-based methods demonstrate their effectiveness in literature, there is a need to explore the underlying mechanisms of these methods in order to better enhance them.}

% The underlying mechanisms of these methods remain largely unexplored.

% gate: utility & preliminary
% reft: utility
% Observing the above, \yue{we conduct preliminary exploration to investigate GA-based unlearning and find} 
% % demystify GA-based unlearning by showing 
% that these methods also operate by distinguishing and suppressing the target data. 
% In Section~\ref{}, we demonstrate that GA-unlearned LLMs exhibit a distinct pattern in the target data in the representation space compared with the input not related to target data. 
% % \yue{[a distinct pattern compared to what?]}.
% For GA-unlearned models, the target data acts as a signal. When the target data appears in the input prompt, the representations will exhibit this distinct pattern,
% If the target data exist in the input prompt, it works as an unlearning signal to trigger the distinct pattern.
% \yue{[rewrite to the following]If the target data exist in the input prompt at the inference stage, the corresponding representation will contain this distinct pattern.}
% and the unlearned model will suppress target-related generation.
% The final models by GA-based methods indeed closely resemble the models by suppression-based unlearning. 
% GA-based methods use a superfluous training objective, but only achieve an overclaimed unlearning. 

% Moreover, these methods cost considerable efforts to fine-tune the whole model and have been found severely sacrificing the model utility, i.e. its general text generation capabilities for non-target-related data~\cite{}.

% \yue{From the above results, for both GA-based methods and the suppression-based methods, the key is to distinguish the target data and other data. We are wondering: \textit{Can we design an unlearning method which achieve the key while is more simple and straightforward?} }

% This raises a question: \textit{Since we now understand that unlearning is merely a disguised form of distinguishing and suppression, is there a more efficient and straightforward approach to save efforts?}

% Inspired by the insights from our preliminary study we introduce \textbf{G}ated \textbf{R}epresentation \textbf{UN}learning (GRUN).  
 % \jt{we should first intro the key designs of GRUN with discussing how the designs are inspired by preliminary study and how the designs mitigate the mentioned challenges. Please adjust the following.} {The specific algorithm is designed because of the following two reasons: \textit{\textbf{First}},
% % In GRUN, 
% to overcome the limitations of fine-tuning and maintain utility, we avoid fine-tuning the model parameters. In contrast, we fine-tune the representations using Representation Fine-Tuning (ReFT)~\cite{wu2024reft}.} {Specifically, we freeze} the model to preserve utility, and unlearn the target data by redirecting the embeddings of target-data-related prompts toward suppression. {\textit{\textbf{Second}}, }leveraging the observation that GA-based (and suppression-based) methods rely on distinguishing target data for unlearning, we explicitly design two key components for distinguishing and suppression respectively: a soft gate function to distinguish, and a suppression module utilizing ReFT. The soft gate function selectively activates for target data, while the ReFT module adjusts prompt embeddings to steer outputs accordingly.

Inspired by the insights from our preliminary study
that both GA-based and suppression-based methods rely on distinguishing target data for unlearning, we introduce \textbf{G}ated \textbf{R}epresentation \textbf{UN}learning (GRUN). GRUN consists of two plug-and-play components designed explicitly for distinguishing and suppression: a soft gate function to distinguish, and a suppression module utilizing Representation Fine-Tuning (ReFT)~\cite{wu2024reft}. The ReFT module fine-tunes the representation instead of the model parameters, which can avoid distorting the parameters to preserve the utility. Meanwhile, its strength is controlled by the soft gate function, which further ensures the generation unrelated to the target data remains almost untouched. In essence, the soft gate function selectively activates for target data, while the ReFT module unlearns by redirecting the embeddings of target-data-related prompts toward suppression.

% leveraging a soft gate function and Representation Fine-Tuning (ReFT)~\cite{wu2024reft}. \yue{Our method is built upon the following:
% \begin{itemize}
%     \item Since the key of the unlearning methods is to distinguish target data and other data, we design a gate to selectively activate for target data.
%     \item Intuitively, knowledge, including both the target knowledge to forget and other LLM capabilities such as speaking as a human, is embedded in large-scale model parameters and optimized through extensive pre-training. As a result, fine-tuning might be ineffective at completely removing target data and often leads to a significant reduction in model utility. Therefore, we choose not to modify the model parameters. We freeze them to preserve utility and unlearn by redirecting the embeddings of target-data-related prompts toward suppression. The ReFT module works as a suppression module and adjusts prompt embeddings to steer outputs accordingly.
% \end{itemize}}
% Leveraging  we explicitly design two key components: a soft gate function to distinguish target data, and a suppression module utilizing Representation Fine-Tuning (ReFT)~\cite{wu2024reft}. The soft gate function selectively activates for target data, while the ReFT module adjusts prompt embeddings to steer outputs accordingly.

% GRUN is both effective and efficient. 

{We conduct extensive experiments to examine the effectiveness and efficiency of GRUN.} GRUN requires a lightweight additional module (less than 0.05 \% of the LLM’s size) and reduces training time by over 95\% compared to the original method, yet achieves near-perfect unlearning while maintaining utility. Moreover, GRUN is a general solution adaptable to various fine-tuning-based unlearning methods. Our experiments validate this across various models, including Llama 3.1 and Mistral, as well as across different datasets, such as TOFU focusing on the unlearning of fine-tuning data~\cite{maini2024tofu}, and WMDP focusing on unlearning pre-training data~\cite{liwmdp}.

% However, LLM unlearning has two problems:

% (1) GA-based overclaim. Some paper say xxx. To make it clear, we reveal that it first distinguish then suppression which is similar to suppression-based

% (2) Fine-tuning-based may have bad trade-off unlearning is not perfect but cause utility reduces. Due to fine-tuning's small retaining set and reverse loss. Since we have observe the two stages, can we avoid changing the model to distinguish data? We use GRUN. It freeze the model and controls unlearning by a gate. 
% Some inference stage suppression-based methods do this, but

% The ReFT is part of controlable module. For example, gate needs a reversable to control. Fine-tuning the whole module cannot aciehve this.

% Large model but limited data.