\section{Method}
\label{sec:method}

% In this section, we present our main framework of GRUN and introduce how to extend it when more than one unlearning process is needed (i.e., sequential unlearning).

In this section, we first present the design of GRUN and its training procedure. Lastly, we discuss how to extend GRUN to sequential unlearning, where multiple unlearning requests occur over time.

\subsection{GRUN}

% As we discussed in Section~\ref{sec:prelim}, the optimization of GA-based unlearning in LLMs relies on distinguishing the target data and other data, instead of fundamentally remove the target data.
{The observation in our preliminary study suggests that the mechanism of both GA-based and suppression-based methods is to distinguish the target data. Based on this, we proposed the ReFT-based Gated Representation UNlearning method to explicitly take the advantage of this finding.} 
% In this subsection, we explain some basic elements of ReFT and introduce the details of GRUN.
% Inspired by this, we propose Gated Representation UNlearning, which is based on ReFT, to enhance the LLM unlearning to explicitly take the advantage of the observation. In this subsection, we we first explain some basic elements of ReFT and introduce the details of GRUN.

\begin{figure}[t]
\centering
  \includegraphics[width=\linewidth]{pics/grun4.png}
  \caption{An overall of the framework of GRUN. }
  \label{fig:grun}
  \vspace{-0.1in}
\end{figure}

% TODO: since we cannot remove from model, we remove from representations

An overview of GRUN is in Figure~\ref{fig:grun}. GRUN consists of two plug-and-play components explicitly for distinguishing and suppression: a soft gate function to distinguish target data, and a ReFT module to suppress target-data-related generation. 
We first explain the elements of ReFT below.

% \paragraph{Motivation of the design.} {To explain why we consider ReFT instead of model fine-tuning, since the model parameters contain a variety of knowledge besides the target data, \jt{do we have evidence} we hypothesize that changing the model parameters in unlearning will change the other knowledge as well, sacrificing the utility of the model. Furthermore, \jt{do we have evidence to support the following?} combining with the gate functionality in GRUN, ReFT will be likely to only change the representation of the target data, distinguishing the representation between the target data and other data, and minimizing its impact on the utility of other data. }



\paragraph{ReFT.} As shown in Section~\ref{sec:related_works}, ReFT modifies a model by freezing its parameters while fine-tuning the intermediate representations of some layers. {Specifically, it applies the following transformation to update the $d$-dimensional representation $\boldsymbol{h}_i^{(l)}$ of the $i$-th token at layer $l$:}
\begin{align*}
    \Phi_{\text{ReFT}}(\boldsymbol{h}_i^{(l)})=\boldsymbol{h}_i^{(l)}+ \phi(\boldsymbol{h}_i^{(l)}),
\end{align*}
where $\phi(\boldsymbol{h}_i^{(l)})$ is a trainable low-rank linear transformation defined as 
\begin{align}
    \phi(\boldsymbol{h}_i^{(l)})=\mathbf{R}^{\top}(\mathbf{W} \boldsymbol{h}_i^{(l)}+\boldsymbol{b}-\mathbf{R} \boldsymbol{h}_i^{(l)}),
    \label{eq:transform}
\end{align}
where $\mathbf{R} \in \mathbb{R}^{r \times d}, W \in \mathbb{R}^{r \times d} \text{~and~} b\in \mathbb{R}^{r}$ are trainable parameters, with $r \ll d$. Intuitively, the term $\mathbf{W} \boldsymbol{h}_i^{(l)}+\boldsymbol{b}$ represents the target representation we aim to shift towards, while  $\phi(\boldsymbol{h}_i^{(l)})$ is the directional adjustment from $\boldsymbol{h}_i^{(l)}$ to the target representation in the space defined by $\mathbf{R}$. By replacing the original representation $\boldsymbol{h}_i^{(l)}$ with the new representation $\Phi_{\text{ReFT}}(\boldsymbol{h}_i^{(l)})$, ReFT modifies the embeddings of the input, thereby influencing the subsequent generation. 
A figure of ReFT is in Appendix~\ref{appd:reft}.
% \yue{[will the later representations also changed during the generation? Seems not consistent to the decription in GRUN?]}. 
% \blue{TODO: figure in appd}


\paragraph{GRUN.} 

% Inspired by the insights from Section~\ref{sec:prelim},  

On top of ReFT, we define GRUN as:
\begin{align}
    \Phi_{\text{GRUN}}(\boldsymbol{h}_i^{(l)})=\boldsymbol{h}_i^{(l)}+ g(\boldsymbol{h}_i^{(l)}) \phi(\boldsymbol{h}_i^{(l)}),
\end{align}
where $g$ is the gate function. More specifically, the soft gate $g$ is a single-output regression model (linear regression or Multi-Layer Perceptron neural network) with a sigmoid function following the output. Thus, the output value of $g$ is in the range of (0,1). As shown in Figure~\ref{fig:grun}, when the input representation $\boldsymbol{h}_i^{(l)}$ is related to the target data, $g(\boldsymbol{h}_i^{(l)})$ is closed to 1 which starts the low-rank transform for unlearning. In contrast, if the input is not about target data, then $g(\boldsymbol{h}_i^{(l)})$ is closed to 0 which passes limited changes on the representation.

% \yue{? Why textit?}\textit{The hyper-parameters of positions of token and layers for} GRUN. 

{While GRUN can be used in any token position and any Transformer layer, the configuration in our work is as follows:}

(1) The last token of input usually contains all the semantic information of the input and has a significant impact on the generation. Thus, we use GRUN at the last token position of input.
% We use GRUN at the last token of input usually contains all the semantic information of the input and has a significant impact on the generation. 
% \yue{A graphical illustration can be found in Figure~\ref{fig:grun}.} 
(2) 
To improve effectiveness, we use GRUN for multiple layers in a model instead of a single layer.
Since the later layers capture higher-level semantics than previous layers which are beneficial for the distinguishing of gate function, we choose to use GRUN for later layers~\cite{peng2018large, jin2025exploring}. To reduce the mutual influence (as discussed in Appendix~\ref{appd:layer}), we choose interval layers instead of successive layers. 
Specifically, for the LLMs studied in the following work, the layers are: the last layer, the last 7th layer and the last 12th layer.
% \yue{(1) is fixed, and (2) is changable. Seems the first sentence of this paragraph is not consistent?}

% \yue{Do we want to mention which layer of LLM we want to insert GRUN?}
% The is presented in Figure~\ref{fig:grun} and

% later layers: high-level semantics for gate and direct influence on generation. 

% plug-and-play property are two aspects, on the one hand, we can unlearn it b


\subsection{Training objective}

% The second plug-and-play property is that our method can be adapted into different unlearning loss such as GA, GD, NPO, IDK, RMU and all the fine-tuning based methods. In other words, GRUN can be seen as a new fine-tuning method that is very suitable in LLM unlearning task.

Our method is a unified method that can be adapted to different fine-tuning based unlearning loss such as GA~\cite{yao2023large}, GD~\cite{liu2022continual}, NPO~\cite{zhang2024negative}, IDK~\cite{maini2024tofu}, RMU~\cite{liwmdp} and other fine-tuning based methods. In other words, GRUN can be also seen as a new fine-tuning method that is tailored for the LLM unlearning task.

The training objective is represented as follows:
\begin{align}
    L = & L_{\text{u}} + L_{\text{G}} \\
      = & L_{\text{u}} + \mathbb{E}_{(x, y, \hat{y}) \in \mathcal{D}_{\mathrm{t}} \cup \mathcal{D}_{\mathrm{r}}} \mathbb{E}_{i,l} L_{\text{CE}} \left(g(\boldsymbol{h}_i^{(l)}), \hat{y}\right), \notag
\end{align}
% \blue{(TODO: check D t in all math)}
where $L_{\text{u}}$ is an unlearning loss which can be GA-based or suppression-based loss, $\hat{y}$ is the label to indicate target data ($\hat{y} = 1$) and retain data ($\hat{y} = 0$), and $L_{\text{G}}$ is the cross-entropy loss for the output of gate function. The unlearning loss $L_{\text{u}}$ is used to ensure the unlearning purpose. The term $L_{\text{G}}$ fine-tunes the gate function to open (closer to 1) for target data more and close (closer to 0) for the other data. {This training objective 
% explicitly follow the mechanism of fine-tuning-based methods which 
distinguishes the target data for unlearning and keeps the model utility by minimizing its impact on the normal input.}

\subsection{Sequential unlearning}

% \jt{is sequential unlearning a well-received concept?? if not we can combine with the previous sentence to define what is it? }

In real-world scenarios, the unlearning requests typically arise sequentially over time. To process this sequential unlearning, previous methods have to re-train the whole set or fine-tune multiple rounds which would largely reduce the model utility due to the accumulated parameter distortion~\cite{shi2024muse}. {In contrast,} in GRUN,  we mitigate this by using an independent ReFT for each unlearning request and combine them together. Specifically, if we have $M-1$ unlearning requests finished and get the new $M$-th request, we can fine-tune a separate gate for the new coming target set and combine multiple GRUNs by
\begin{align}
    \Phi_{\text{GRUN}}^{M}(\boldsymbol{h}_i^{(l)})=\boldsymbol{h}_i^{(l)}+ c ~\sum_{j=1}^{M}g_j(\boldsymbol{h}_i^{(l)}) \phi(\boldsymbol{h}_i^{(l)}), \notag
\end{align}
where $c$ is the coefficient to balance the strength. Each gate $g_j$ is fine-tuned independently on a requested target dataset $\mathcal{D}_{\mathrm{t}, j}$ and then combined. The coefficient $c$ reduces as the increasing of $M$ (the details to determine $c$ is in Appendix~\ref{appd:seq_c}). In this way, we can mostly preserve the model utility and save the training efforts. 
% \yue{[Provide some general guideline on how to determine $c$. When $M$ gets changed, $c$ should also be changed correspondingly?]}