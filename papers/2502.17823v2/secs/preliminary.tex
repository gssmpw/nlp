\section{Preliminary studies}
\label{sec:prelim}

% definitions and GA-based

% changes more

% combine both changes more

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.87\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{pics/pre_change_pca_legend.png}
        \label{fig:pre_change_pca_legend}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_overlap_pca_llama3_grad_diff_forget10_perturbed.png}
        \vspace{-0.37in}
        \caption{Llama 3.1 by GD}
        \label{fig:llama_gd}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_overlap_pca_llama3_npo_forget10_perturbed.png}
        \vspace{-0.37in}
        \caption{Llama 3.1 by NPO}
        \label{fig:llama_npo}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_overlap_pca_mistral_grad_diff_forget10_perturbed.png}
        \vspace{-0.37in}
        \caption{Mistral v0.1 by GD}
        \label{fig:mistral_gd}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_overlap_pca_mistral_npo_forget10_perturbed.png}
        \vspace{-0.37in}
        \caption{Mistral v0.1 by NPO}
        \label{fig:mistral_npo}
    \end{subfigure}
    % \vspace{-0.05in}
    \caption{PCA visualizations of embeddings (both before and after unlearning) of target data, retaining data, and never-seen data. We apply 2-component PCA to project the embeddings into a 2D space and visualize the distributions. Each subfigure corresponds to a separate PCA projection for an unlearned model.}
    \label{fig:pre_overlap}
    \vspace{-0.13in}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.87\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{pics/pre_mix_pca_legend.png}
        \label{fig:pre_change_pca_legend}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_mix_pca_llama3_grad_diff_forget10_perturbed.png}
        \vspace{-0.25in}
        \caption{Llama 3.1 by GD}
        \label{fig:llama_gd}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_mix_pca_llama3_npo_forget10_perturbed.png}
        \vspace{-0.25in}
        \caption{Llama 3.1 by NPO}
        \label{fig:llama_npo}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_mix_pca_mistral_grad_diff_forget10_perturbed.png}
        \vspace{-0.25in}
        \caption{Mistral v0.1 by GD}
        \label{fig:mistral_gd}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.249\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/pre_mix_pca_mistral_npo_forget10_perturbed.png}
        \vspace{-0.25in}
        \caption{Mistral v0.1 by NPO}
        \label{fig:mistral_npo}
    \end{subfigure}
    % \vspace{-0.05in}
    \caption{PCA visualization and the results of normal Q\&A mixed and not mixed with target data. PCA follows the same operation in Figure~\ref{fig:pre_overlap}. The ROUGE-L Recalls of retaining data/world fact are listed below each figure.}
    \label{fig:pre_mix}
    \vspace{-0.12in}
\end{figure*}

{In this section, we first introduce the definitions about fine-tuning-based unlearning, and then conduct experiments to investigate their common properties.}
% why existing GA-based LLM unlearning fails to eliminate the influence of the target data, but merely hide it from the generation process.



\subsection{Fine-tuning-based unlearning}



Given an LLM $f$ and a target dataset $\mathcal{D}_{\mathrm{t}}$, the goal of an unlearning task is to get a model $f_{\mathrm{u}}$ that behaves as if it was never trained on $\mathcal{D}_{\mathrm{t}}$. Besides, $f_{\mathrm{u}}$ should also retain the model utility, i.e. the general text generation capabilities. {To achieve this, various fine-tuning-based methods have been developed, such as GA-based and suppression-based methods.}

In \textbf{GA-based methods}, the unlearning objective is usually formulated as the following: \vspace{-0.05in}
\begin{align}
    \underset{\boldsymbol{\theta}}{\operatorname{argmin}} ~ & \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{t}}}\left[L_{\mathrm{f}}(y | x ; \boldsymbol{\theta})\right] \notag \\
    &+ \lambda  \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{r}}}\left[L_{\mathrm{r}}(y | x ; \boldsymbol{\theta})\right],
    \label{eq:ga}
\end{align}
where $\mathcal{D}_{\mathrm{r}}$ is the retaining dataset to preserve the model utility, and $(x, y)$ denotes an input-output pair. $\boldsymbol{\theta}$ represents the updated parameters, while $L_{\mathrm{f}}$ and $L_{\mathrm{r}}$ denote the forgetting and retaining loss functions, respectively, with $\lambda$ balancing them. Typically, $L_{\mathrm{f}}$ is the negative training loss (i.e., applying Gradient Ascent) or a variant, while $L_{\mathrm{r}}$ corresponds to the training loss on $\mathcal{D}{\mathrm{r}}$ or a regularization term (e.g., the KL divergence between the $f$ and $f_{\mathrm{u}}$). 

% Gradient Difference (GD) ~\cite{liu2022continual} is a typical method using negative of the standard training loss on target data as $L_{\mathrm{f}}$ and training loss on retaining data as $L_{\mathrm{r}}$. Another important method Negative Preference Optimization (NPO)~\cite{zhang2024negative} which derives from Direct Preference Optimization~\cite{rafailov2024direct} uses the initial checkpoint as a reference model to control the strength of unlearning by constraining the divergence from unlearned model to reference model.

We introduce two GA-based methods. Gradient Difference (GD) \cite{liu2022continual} applies negative standard training loss on $\mathcal{D}_{\mathrm{t}}$ as $L_{\mathrm{f}}$. Negative Preference Optimization (NPO)\cite{zhang2024negative}, derived from DPO~\cite{rafailov2024direct}, constrains divergence from the initial checkpoint to regulate strength of GA.



\textbf{Suppression-based methods} have a similar objective: \vspace{-0.2in}
\begin{align}
    \underset{\boldsymbol{\theta}}{\operatorname{argmin}} ~ & \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{t}}}\left[L_{\mathrm{s}}(y, x, \boldsymbol{\theta})\right] \notag \\
    &+ \lambda  \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{r}}}\left[L_{\mathrm{r}}(y | x ; \boldsymbol{\theta})\right], \notag
\end{align}
$L_{\mathrm{s}}$ is the suppression term. 
% For the suppression-based method IDK~\cite{maini2024tofu}, $L_{\mathrm{s}}$ guides the model to generate ignorant answers like ``I don't know'' for target data. In another method RMU~\cite{liwmdp}, it guides the representation of target data towards a random vector.
In IDK~\cite{maini2024tofu}, $L_{\mathrm{s}}$ encourages responses like “I don’t know” for target data, while in RMU~\cite{liwmdp}, it pushes target data representations toward a random vector to disturb target data.


% \yue{[Introduce GD and NPO]}

\subsection{Findings of GA-based unlearning}

% In this subsection, we conduct experiments to show that although GA-based methods are designed to fine-tune the LLMs to cancel training on target data and negate the learned knowledge of target data, they are actually similar to the preference-based methods that distinguish the forget and prevent from the related generation on them. We reveal this from the following perspectives.

% Although GA-based methods seems more dedicated to ``unlearning'', they ultimately produce final models similar to those derived from suppression-based methods. 
% In this section, 
% \jt{for the preliminary study, we should write it as a formal experimental sections. What is the purpose of this study, what is the setting, how to conduct the experiment, what are the results, from the results, what our observations and insights we get. We need to reorganize the following and add some missing content. It is the same as how you write the exerpimental sections.}

In this subsection, we investigate the common property between GA-based and suppression-based methods. 
We find that GA-based methods cannot remove target data as expected. Instead, the GA-unlearned models distinguish the target data and pretend to be unaware. It is actually the same strategy as suppression-based methods. 
Experiments are conducted by exploring following questions.

% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.8\columnwidth]{pics/pre_change_pca_llama3_grad_diff_forget05_perturbed.png}
%   \caption{Representaions of TOFU in Llama3.1.}
%   \label{fig:repre_llama3}
% \end{figure}

% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.8\columnwidth]{pics/pre_change_pca_mistral_grad_diff_forget05_perturbed.png}
%   \caption{Representaions of TOFU in Mistral-v0.1.}
%   \label{fig:repre_mistral}
% \end{figure}

\noindent\textbf{(1) Does reversing the training loss truly negate the target data's influence?} 

If the GA-based methods could remove the influence of target data, it is expected that the unlearned models should behave the same between the target data and the data it has never encountered.
To investigate this, we conduct an experiment to compare the model behaviors in these two data cases. 


\textit{Settings}.
We use TOFU dataset, which contains synthetic Q\&A pairs about non-existent writers and books. 
We split the dataset into three subsets: target data, retaining data and never-seen data.
We first fine-tune LLMs to learn the knowledge from the target data and retaining data. Then we unlearn the target data by GD and NPO. In Figure~\ref{fig:pre_overlap}, 
% \jt{what are these numbers below the figure "CSD" and "ROUGE", do we use tnse for visualization? what does the caption mean? } 
we plot the embeddings (both before and after unlearning) of target data, retaining data and never-seen data.

% \yue{(1) The embedding plotted is after PCA? (2) For PCA, how do you get the first two loadings? Are the loading the same across all figures? Please specify (if not space, mention one sentence and put details to the appendix.)} \jie{Will add in caption}

\textit{Results.} In Figure~\ref{fig:pre_overlap}, we observe that in embedding space, the unlearned models still recognize target data, and distinguish it with a special pattern. 
% GA-unlearned models exhibit a noticeably different behavior. 
\textit{Before} unlearning, the target data, retaining data, and never-seen data have similar embeddings, as all three sets are sampled from the same data distribution. In contrast, \textit{after} unlearning, the target data follows a significantly different pattern, distributed far from the retaining and never-seen data. This suggests that the model does not truly remove the target data. Instead, they still recognize it and distinguish it by pushing it into a distinct region.


\noindent\textbf{(2) Is unlearning performance associated with this distinct pattern?} 

% \textit{Observation: Better unlearning performance is associated with better distinction.} 
To further explore the connection between unlearning and distinct patterns, we quantify the distinction and the unlearning effectiveness in Table~\ref{tab:overlap}.
% we demonstrate that the more distinct the target data is, the better the unlearning performance. 

\textit{Settings.} We quantify the distinction using the degree of overlap between the embeddings of target and retained data, measured by Class-wise Separability Discriminant (CSD), i.e., the ratio of intra-class distance (samples within target and samples within retaining data) to inter-class distance (between target data and retaining data)~\cite{rentransferable, klecka1980discriminant}. Unlearning effectiveness is evaluated using ROUGE-L Recall, where a lower score on target data indicates better unlearning (as detailed in Section~\ref{sec:settings}). 

\textit{Observation.}
In Table~\ref{tab:overlap}, we observe that when the pattern is more distinct (i.e., lower CSD), the target data is more effectively unlearned (i.e., lower ROUGE-L Recall). For example, Mistral unlearned by GD has the lowest CSD and the lowest ROUGE-L Recall, while Llama unlearned by NPO has the highest CSD and the highest ROUGE-L Recall.
This implies that better unlearning performance is likely to be associated with better distinction.

\begin{table}[t]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
        & \multicolumn{2}{c}{Llama 3.1} & \multicolumn{2}{c}{Mistral v0.1} \\
        ~ & GD & NPO & GD & NPO \\ \midrule
        CSD & 0.45 & 3.21 & 0.13 & 1.72 \\
        ROUGE-L Recall & 0.016 & 0.197 & 0.001 & 0.127 \\ \bottomrule
    \end{tabular}
    }
    \caption{Unlearning effectiveness and distinction}
    \vspace{-0.15in}
  \label{tab:overlap}
\end{table}

% \jie{Should we put this in (2)? That would delay the explanation of ROUGE and overlap which might confuse reader? But it seems to relate with (2) more?} \yue{I think it is fine to keep here. Can remove the second bullet point itself but leave the paragraph here.}

% \yue{Need to know about how to control CSD first.}

% If the GA-based methods can entirely remove the influence of target data, it is expected that the embeddings of the target data would be similar to the embeddings of never-seen data. However, as shown in Figure~\ref{fig:pre_overlap}, GA-unlearned models exhibit a noticeably different behavior. \textit{Before} unlearning, the target data, retaining data, and never-seen data exhibit similar embeddings, as all three synthetic sets are sampled from the same data distribution. In contrast, \textit{after} unlearning, the target data follows a significantly different pattern, distributed far from the retaining and never-seen data. This suggests that the model does not truly remove the target data. Instead, they still recognize it and distinguish it by pushing to a distinct region.

\noindent\textbf{(3) How do GA-based methods unlearn?} 

To analyze how the GA-unlearned models process the target data, we compare the model behaviors between target data and normal Q\&A data (questions that should be correctly answered by unlearned models).

\textit{Settings.} We inject target data into normal Q\&A pairs {to form the mixed data} and compare the model's behaviors before and after {the injection}. 
We use retaining data and world fact Q\&A pairs as normal Q\&A data. 
% We refer to the data added with target data as mixed data. 
{For example, a mixed data instance is ``\textit{Where is Eiffel Tower? And \underline{who is} \underline{the author of Watermelon on the Moon}}?}'', where ``\textit{\underline{who is the author of $\ldots$}}'' is an instance in target data. 
We plot the embeddings and calculate ROUGE-L Recall (higher score means more correct answers) in Figure~\ref{fig:pre_mix}.

% \begin{table}[t]
%     \centering
%     \resizebox{0.9\linewidth}{!}{
%     \begin{tabular}{ccccc}
%     \toprule
%         \multirow{3}{*}{\makecell{Mixed with \\ target data}} & \multicolumn{2}{c}{{Llama 3.1}} & \multicolumn{2}{c}{Mistral v0.1} \\
%         ~ & GD & NPO & GD & NPO \\
%         ~ & retain/world & retain/world & retain/world & retain/world \\ \midrule
%         No & 0.483/0.785 & 0.551/0.851 & 0.687/0.403 & 0.842/0.494 \\
%         Yes & 0.010/0.000 & 0.157/0.050 & 0.013/0.000 & 0.225/0.030 \\ \bottomrule
%     \end{tabular}
%     }
%     \caption{ROUGE-L Recall of normal Q\&A}
%   \label{tab:mix}
% \end{table}

\textit{Results.} {From Figure~\ref{fig:pre_mix}, we can see that the unlearned models actually treat target data as the unlearning signal.} Specifically, before adding target data, the models correctly answer normal questions, achieving a high ROUGE-L Recall. However, once mixed with target data, the embeddings of normal data is dominated by target data (which is pulled toward the distinct area of target data). Consequently, the model's ability to answer normal questions deteriorates (lower ROUGE-L).
This implies that, instead of removing the target data, GA-unlearned models treat it as a suppression signal. 

In summary, our preliminary studies reveal that GA-based unlearning methods do not erase the target data as expected. Instead, the models still recognize it and distinguish it in the embeddings. Unlearning performance is likely to be associated with the distinction. When target data appears in the prompt, the model suppresses related generations—essentially employing the same strategy as suppression-based methods.

