\section{Related Work}
\label{sec5}
\subsection{Graph Neural Networks}
Graph neural networks (GNNs) are powerful models for learning node representations from graph data.
Existing GNNs can be categorized as spectral GNNs and spatial GNNs.
Spectral GNNs leverage eigenvectors and eigenvalues within the graph Laplacian matrix to design graph signal filters in the spectral domain **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"**.
Spatial GNNs **Scarselli**, "**Graph Neural Networks: A Review of the State-of-the-Art"** __**Hamilton, "Inductive Representation Learning on Large Graphs"** using first-order approximation, which aggregates features from neighboring nodes in the spatial graph to generate node embeddings.
%____.
% ____ simplified spectral graph filter ____ using first-order approximation, which aggregates features from neighboring nodes in the spatial graph to generate node embeddings.
% Building on this foundation, numerous spatial GNN variants with specialized aggregation methods have since been proposed **Gilmer, "Neural Message Passing for Quantum Chemistry"**,  achieving remarkable performance across various tasks.
% In recent years, GNNs have developed  sophisticated architectures to  handle complex task scenarios, such as structural distribution shifts **Perozzi, "DeepWalk: Online Learning of Graph Representations"**, continual graph learning **Kipf, "Learning Discrete Energy-Based Models of Image Content"**, and model explainability **Zhang, "Visualizing and Understanding Convolutional Neural Networks with Feature Maps"**.
However, existing GNNs assume that the input graph structure is sufficiently clean for learning, whereas real-world graphs are often noisy and incomplete, which limits GNN performance on downstream tasks **Liu, "Graph-Based Representation Learning for Graph Neural Networks"**.
In this paper, we propose uncertainty-aware graph structure learning, which effectively denoises the graph structure to alleviate the above limitation.



% \begin{figure}[t]
% \centering
% \includegraphics[width=1.0\linewidth]{effiency_cora.pdf}
% \caption{Time and space consumption of different methods on Cora dataset.}
% \label{fig:efficiencycora}
% \vspace{-10pt}
% \end{figure}

\subsection{Graph Structure Learning}
Graph Structure Learning (GSL) aims to learn an optimal graph that improves the accuracy and robustness of Graph Neural Networks (GNNs) on downstream tasks.
Early GSL methods **Knyazev, "EigTool: A C++ Library for Large-Scale Eigenvalue Decomposition"** directly treat the target adjacency matrix as learnable parameters, incurring substantial computational overhead and optimization challenge.
Mainstream GSL methods **Wilson, "Yatchy: Efficient Node Embeddings via Graph Signal Processing"** learn edge weights based on node-pair embedding similarities, employing various metrics such as cosine similarity __**, inner product ____ or neural networks ____.
These methods aim to increase graph homophily and typically achieve state-of-the-art performance, as these metrics can capture nodes with similar semantics.
However, these embedding-based GSL methods suffer from two limitations:
First, they construct edges solely based on embedding similarities while neglecting node uncertainty, which may introduce inferior information that poison the target node's embedding. 
Although some works__**Katzir, "Learning Graph Neural Networks with Predictive Uncertainty"**** incorporate predictive uncertainty in structure learning, they only use it for cross-view structure fusion without distinguishing nodes of varying uncertainty levels within a graph.
Second, embedding-based methods impose bidirectional edges between nodes, disregarding their unequal influence due to varying levels of uncertainty.
Recently, several methods propose constructing directed edges from labeled to unlabeled nodes to facilitate the propagation of supervision signals____.
However, these methods fail to learn reasonable asymmetric connections between the vast majority of unlabeled nodes.
% However, existing GSL models overlook the predictive uncertainty of neighboring nodes and the symmetry constraint in the structure modeling process, which  significantly impact node representations during aggregation.
% Although a few GSL methods ____  incorporate predictive uncertainty, it is used either for adaptive multi-view node representation fusion ____ or as a measure of similarity between nodes ____.
Different from the above works, we propose an uncertainty-aware structure learning (UnGSL) strategy that learns node-wise thresholds to differentiate low-uncertainty from high-uncertainty neighbors and adaptively refine the corresponding edges.
% Notably, UnGSL is able to  directly applied to most GSL models where edge weights updated through gradient descent,  and it is not compatible with a few GSL models ____ that do not meet this criterion.

\subsection{Uncertainty in GNNs}
GNNs inevitably present uncertainty towards their predictions, leading to unstable and erroneous prediction results ____.
In recent years, uncertainty in GNNs has been widely researched to adapt various tasks, including out-of-distribution (OOD) detection __**, trustworthy GNN learning **Pittoni, "Learning Trustworthiness of Graph Neural Networks"** , and GNN modeling **Kurakin, "Adversarial Examples, Multiple Perturbations, and the Role of Overfitting in Deep Learning"**.
However, these uncertainty-based GNNs assume that the input graph structure is clean and primarily focus on incorporating uncertainty into the model architecture.
In contrast, our proposed uncertainty-aware graph structure learning aims to refine the edge based on node uncertainty, assisting in improving graph quality. 
% However, these GSL methods  overlook the predictive uncertainty of neighboring nodes and the resulting asymmetry in the structure.
% In this paper, we use the predictive uncertainty to differentiate between nodes in structure learning process.
% Predictive uncertainty is typically quantified using maximum softmax probability and entropy in classification tasks.
% For accurate estimation and application of uncertainty, predictive uncertainty is commonly categorized into aleatoric and epistemic uncertainty based on their distinct sources. ____.
% Specifically, aleatoric uncertainty refers to data uncertainty from statistical randomness while epistemic uncertainty indicates model uncertainty due to limited knowledge in collected data ____.
% Notably, the entropy of a node's representation is influenced by both aleatoric and epistemic uncertainty____. 
% Therefore, we use predictive entropy to differentiate between nodes when constructing edges.