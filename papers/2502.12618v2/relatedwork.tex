\section{Related Work}
\label{sec5}
\subsection{Graph Neural Networks}
Graph neural networks (GNNs) are powerful models for learning node representations from graph data.
Existing GNNs can be categorized as spectral GNNs and spatial GNNs.
Spectral GNNs leverage eigenvectors and eigenvalues within the graph Laplacian matrix to design graph signal filters in the spectral domain \cite{bruna2013spectral,defferrard2016convolutional,li2021dimensionwise,he2022convolutional}.
Spatial GNNs \cite{kipf2017semisupervised,gilmer2017neural,veličković2018graph,wu2019simplifying,ding2022sketch,wu2021disenkgat} simplified spectral graph filter \cite{defferrard2016convolutional} using first-order approximation, which aggregates features from neighboring nodes in the spatial graph to generate node embeddings.
%\cite{kipf2017semisupervised,gilmer2017neural,veličković2018graph,wu2019simplifying,ding2022sketch}.
% \citet{kipf2017semisupervised} simplified spectral graph filter \citep{defferrard2016convolutional} using first-order approximation, which aggregates features from neighboring nodes in the spatial graph to generate node embeddings.
% Building on this foundation, numerous spatial GNN variants with specialized aggregation methods have since been proposed \citep{gilmer2017neural,veličković2018graph,wu2019simplifying,ding2022sketch},  achieving remarkable performance across various tasks.
% In recent years, GNNs have developed  sophisticated architectures to  handle complex task scenarios, such as structural distribution shifts \cite{gui2024joint}, continual graph learning \cite{zhang2022cglb}, and model explainability \cite{wang2024gnnboundary}.
However, existing GNNs assume that the input graph structure is sufficiently clean for learning, whereas real-world graphs are often noisy and incomplete, which limits GNN performance on downstream tasks \cite{geisler2021robustness,chen2023bias}.
In this paper, we propose uncertainty-aware graph structure learning, which effectively denoises the graph structure to alleviate the above limitation.



% \begin{figure}[t]
% \centering
% \includegraphics[width=1.0\linewidth]{effiency_cora.pdf}
% \caption{Time and space consumption of different methods on Cora dataset.}
% \label{fig:efficiencycora}
% \vspace{-10pt}
% \end{figure}

\subsection{Graph Structure Learning}
Graph Structure Learning (GSL) aims to learn an optimal graph that improves the accuracy and robustness of Graph Neural Networks (GNNs) on downstream tasks.
Early GSL methods \cite{franceschi2019learning,jin2020graph} directly treat the target adjacency matrix as learnable parameters, incurring substantial
computational overhead and optimization challenge.
Mainstream GSL methods \cite{chen2020iterative,yu2021graph,wang2023prose,in2024self} learn edge weights based on node-pair embedding similarities, employing various metrics such as cosine similarity \cite{wang2023prose,in2024self}, inner product \cite{yu2021graph} or neural networks \cite{liu2022compact}.
These methods aim to increase graph homophily and typically achieve state-of-the-art performance, as these metrics can capture nodes with similar semantics.
However, these embedding-based GSL methods suffer from two limitations:
First, they construct edges solely based on embedding similarities while neglecting node uncertainty, which may introduce inferior information that poison the target node's embedding. 
Although some works~\cite{liu2022compact, duan2024structural} incorporate predictive uncertainty in structure learning, they only use it for cross-view structure fusion without distinguishing nodes of varying uncertainty levels within a graph.
Second, embedding-based methods impose bidirectional edges between nodes, disregarding their unequal influence due to varying levels of uncertainty.
Recently, several methods propose constructing directed edges from labeled to unlabeled nodes to facilitate the propagation of supervision signals~\cite{song2022towards,song2024optimal,lu2024latent}.
However, these methods fail to learn reasonable
asymmetric connections between the vast majority of unlabeled nodes.
% However, existing GSL models overlook the predictive uncertainty of neighboring nodes and the symmetry constraint in the structure modeling process, which  significantly impact node representations during aggregation.
% Although a few GSL methods \cite{wang2021graph,liu2022compact}  incorporate predictive uncertainty, it is used either for adaptive multi-view node representation fusion \cite{liu2022compact} or as a measure of similarity between nodes \cite{wang2021graph,in2024self}.
Different from the above works, we propose an uncertainty-aware structure learning (UnGSL) strategy that learns node-wise thresholds to differentiate low-uncertainty from high-uncertainty neighbors and adaptively refine the corresponding edges.
% Notably, UnGSL is able to  directly applied to most GSL models where edge weights updated through gradient descent,  and it is not compatible with a few GSL models \cite{li2022reliable,zou2023se,wang2021graph} that do not meet this criterion.

\subsection{Uncertainty in GNNs}
GNNs inevitably present uncertainty towards their predictions, leading to unstable and erroneous prediction results \cite{wang2024uncertainty}.
In recent years, uncertainty in GNNs has been widely researched to adapt various tasks, including out-of-distribution (OOD) detection \cite{zhao2020uncertainty,stadler2021graph,yu2023uncertainty}, trustworthy GNN learning \cite{wang2021confident,hsu2022makes}, and GNN modeling \cite{um2023confidencebased}.
However, these uncertainty-based GNNs assume that the input graph structure is clean and primarily focus on incorporating uncertainty into the model architecture.
In contrast, our proposed uncertainty-aware graph structure learning aims to refine the edge based on node uncertainty, assisting in improving graph quality. 
% However, these GSL methods  overlook the predictive uncertainty of neighboring nodes and the resulting asymmetry in the structure.
% In this paper, we use the predictive uncertainty to differentiate between nodes in structure learning process.
% Predictive uncertainty is typically quantified using maximum softmax probability and entropy in classification tasks.
% For accurate estimation and application of uncertainty, predictive uncertainty is commonly categorized into aleatoric and epistemic uncertainty based on their distinct sources. \cite{hullermeier2021aleatoric}.
% Specifically, aleatoric uncertainty refers to data uncertainty from statistical randomness while epistemic uncertainty indicates model uncertainty due to limited knowledge in collected data \cite{zhao2020uncertainty}.
% Notably, the entropy of a node's representation is influenced by both aleatoric and epistemic uncertainty\cite{wang2024uncertainty}. 
% Therefore, we use predictive entropy to differentiate between nodes when constructing edges.