% !TeX root = ../all.tex

\section{Proofs of the lower bounds}\label{app:lb}
\subsection{Preliminary lemmas}

For the sake of completeness, we start by restating and proving some results from \citep{taoCollaborativeLearningLimited2019} in slightly more general language.
\begin{definition}
	For some integers $r$ and $n$, define $\tau_\delta^r$ the number of samples before the end of round $r$.
\end{definition}
\begin{lemma}[Generalization of Lemma 27 of \citep{taoCollaborativeLearningLimited2019}]\label{lem:27f}
	For an algorithm, two instances ${\bm\nu}$ and ${\bm\nu}'$ and $r\in\bN$, \[\bP_{{\bm\nu}'}\{R_\delta\geq r+1,\tau_\delta^{r+1} \leq n+m\}\geq \bP_{\bm\nu}\{R_\delta \geq r+1,\tau_\delta^r\leq m\}-\bP_{\bm\nu} \{\tau_\delta>n\} -\Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV}\] where $\cD^m_{\bm\nu}$ is the distribution of rewards the algorithm got from $\bm\nu$ over $m$ steps.
\end{lemma}

\begin{proof}
	Fix a deterministic algorithm. 
	
	
	First of all, \begin{equation}\label{eq:mpntomn}(R_\delta \geq r+1,\tau_\delta^{r} \leq m,\tau_\delta^{r+1}-\tau_\delta^r < n) \subseteq (R_\delta \geq r+1,\tau_\delta^{r+1}\leq n+m)\end{equation}
	
	And, since $(R_\delta \geq r+1,\tau_\delta^{r} \leq m,\tau_\delta^{r+1}-\tau_\delta^r < n)$ is determined by the first $m$ rewards (at the end of round $r$ using less than $m$ samples, the algorithm must choose the length of round $r+1$), \begin{equation}\label{eq:distprob} \bP_{{\bm\nu}'}\{R_\delta \geq r+1,\tau_\delta^{r} \leq m,\tau_\delta^{r+1}-\tau_\delta^r < n\} \geq \bP_{\bm\nu} \{R_\delta \geq r+1,\tau_\delta^{r} \leq m,\tau_\delta^{r+1}-\tau_\delta^r < n\}  -\Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV}\end{equation}

On the other hand, \begin{align*}
	(R_\delta \geq r+1,\tau_\delta^r\leq m)\setminus (R_\delta\geq r+1,\tau_\delta^r\leq m,\tau_\delta^{r+1}-\tau_\delta^r < n) &= (R_\delta\geq r+1,\tau_\delta^r\leq m, \tau_\delta^{r+1}-\tau_\delta^r \geq n) \\
	&\subseteq (\tau_\delta >n)
\end{align*} hence \begin{equation}\label{eq:ajoutround}\bP_{\bm\nu}\{R_\delta\geq r+1,\tau_\delta^r\leq m,\tau_\delta^{r+1}-\tau_\delta^r < n\}\geq \bP_{\bm\nu}\{R_\delta \geq r+1,\tau_\delta^r\leq m\}-\bP_{\bm\nu}\{\tau_\delta >n\}\end{equation}

	Hence, using Equations \eqref{eq:mpntomn},~\eqref{eq:distprob} then~\eqref{eq:ajoutround}, \begin{align*}
		\bP_{{\bm\nu}'}\{R_\delta \geq r+1,\tau_\delta^{r+1}\leq n+m\}&\geq \bP_{\bm\nu'} \{R_\delta \geq r+1,\tau_\delta^{r} \leq m,\tau_\delta^{r+1}-\tau_\delta^r < n\}\\
		&\geq \bP_{\bm\nu} \{R_\delta \geq r+1,\tau_\delta^{r} \leq m,\tau_\delta^{r+1}-\tau_\delta^r < n\} -\Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV} \\
		&\geq \bP_{\bm\nu}\{R_\delta \geq r+1,\tau_\delta^r\leq m\}-\bP_{\bm\nu} \{\tau_\delta>n\} -\Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV}
	\end{align*}
	
\end{proof}



\begin{lemma}[Generalization of Lemma 26 of \citep{taoCollaborativeLearningLimited2019}]\label{lem:26f_aux}
	For any $\delta$-correct algorithm, for all $m,r\in\bN$ and any two bandit instances ${\bm\nu}, {\bm\nu}'$, 
	we have
	\begin{align*}
	\bP_{\bm\nu}\{R_\delta\geq r+1,\tau_\delta^r\leq m\}
	\ge \bP_{\bm\nu}\{R_\delta\geq r,\tau_\delta^r\leq m\} - 2\delta - \Vert \cD_{\bm\nu}^m - \cD_{{\bm\nu}'}^m \Vert_{TV}
	\: .
	\end{align*}
\end{lemma}

\begin{proof}
Consider the event $\mathcal{F}_1=(R_\delta=r,\tau_\delta \leq m)$.
Denote by $\mathcal{F}_2$ the event that the algorithm returns the best arm of instance ${\bm\nu}$.
Then \(\bP_{\bm\nu}\{\mathcal{F}_1\}=\bP_{\bm\nu}\{\mathcal{F}_1\wedge \mathcal{F}_2\}+\bP_{\bm\nu}\{\mathcal{F}_1\wedge \overline{\mathcal{F}_2}\}\)
	
With $\mathcal{D}_{\bm\nu}^m$ the distribution of rewards over $m$ samples and some ${\bm\nu}'\in Alt_{\bm\nu}$,
\begin{align*}
\bP_{\bm\nu}\{\mathcal{F}_1\wedge \mathcal{F}_2\}
&\leq \bP_{{\bm\nu}'}\{\mathcal{F}_1\wedge \mathcal{F}_2\}+\Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV}
\\
&\leq \bP_{{\bm\nu}'}\{\mathcal{F}_2\} +\Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV}
\\
&\leq \delta+\Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV}
\: .
\end{align*}
On the other hand, $\mathbb{P}_{\bm\nu}\{\mathcal F_1 \wedge \overline{\mathcal F}_2\} \le \mathbb{P}_{\bm\nu}\{\overline{\mathcal F}_2\} \le \delta$.
Therefore $\bP_{\bm\nu}\{\mathcal{F}_1\}\leq 2\delta + \Vert\cD_{\bm\nu}^m -\cD_{{\bm\nu}'}^m\Vert_{TV}$.
Using \( \bP_{\bm\nu}\{R_\delta\geq r+1,\tau_\delta^r\leq m\}\geq \bP_{\bm\nu}\{R_\delta\geq r,\tau_\delta^r\leq m\}-\bP_{\bm\nu}(\mathcal{F}_1)\), we conclude.
\end{proof}


\begin{lemma}\label{lem:26f}
	For any $\delta$-correct algorithm, for all $m,r\in\bN$ and any bandit instance ${\bm\nu}$, 
	we have
	\begin{align*}
	\bP_{\bm\nu}\{R_\delta\geq r+1,\tau_\delta^r\leq m\}
	\ge \bP_{\bm\nu}\{R_\delta\geq r,\tau_\delta^r\leq m\} - 2\delta - \sqrt{\frac{m}{2} (T^\star(\bm\nu))^{-1}}
	\: .
	\end{align*}
\end{lemma}



\begin{proof}
First apply Lemma~\ref{lem:26f_aux} to an arbitrary instance ${\bm\nu}' \in Alt_{\bm\nu}$. Then using Pinsker's inequality yields
\begin{align*}
\Vert \cD_{\bm\nu}^m - \cD_{{\bm\nu}'}^m \Vert_{TV}
\leq \sqrt{\frac{1}{2}\KL(\cD_{\bm\nu}^m\Vert \cD_{{\bm\nu}'}^m)}
= \sqrt{\frac{1}{2} \sum_{i\in[K]} \bE_{\bm\nu}[N_{m,i}] \frac{(\mu_i-\mu_i')^2}{2}}
\end{align*} with $N_{m,i}$ the number of times arm $i$ is pulled before time $m$.

As this is true for all instances ${\bm\nu}'\in Alt_{{\bm\nu}}$, we can obtain an inequality using the infimum over those instances,
\begin{align*}
\inf_{{\bm\nu}' \in Alt_{\bm\nu}} \Vert \cD_{\bm\nu}^m - \cD_{{\bm\nu}'}^m \Vert_{TV}
&\le \sqrt{\frac{m}{2} \inf_{\bm\lambda \in Alt_{{\bm\nu}}} \sum_{i\in[K]} \frac{\bE_{\bm\nu}[N_{m,i}]}{m} \frac{(\mu_i-\lambda_i)^2}{2}}
\\
&\le \sqrt{\frac{m}{2} \sup_{w\in \Sigma_K} \inf_{\bm\lambda \in Alt_{{\bm\nu}}} \sum_{i\in[K]} w_i \frac{(\mu_i-\lambda_i)^2}{2}}
\\
&=   \sqrt{\frac{m}{2} (T^\star(\bm\nu))^{-1}}
\: ,
\end{align*}
by definition of $T^\star$.
\end{proof}





Finally, we also give a technical result to solve inequalities of the form $(k+N^2(a+b\ln N))^N\leq \rho$.

\begin{lemma}\label{lem:suffN}
	Let $\rho \ge e$, $a,b\geq 0$ and $k$ be real numbers, and let $A=\max\{e,k+a\}$.
	Then $N \coloneqq \left\lfloor \frac{\ln \rho}{\ln((\ln \rho)^2(A+b\ln \ln \rho))}\right\rfloor$ satisfies $(k+N^2(a+b\ln N))^N\leq \rho$~.
\end{lemma}

\begin{proof}
	If $N=0$, the equality is $1 \le \rho$, which is true since $\rho \ge e$. Otherwise, $N\geq 1$ and $(\ln \rho)^2(A+b\ln\ln \rho)\geq A\geq e$,
	so $N \le \lfloor \ln\rho / \ln e \rfloor \leq \ln \rho$. Therefore
	\begin{align*}
		N\ln(k+N^2(a+b\ln N))&\leq N\ln (N^2(A+b\ln N))
		\\
		&\leq N\ln((\ln \rho)^2(A+b\ln \ln \rho))
		\\
		&\leq \ln \rho
	\end{align*}
	and finally $(k+N^2(a+b\ln N))^N\leq \rho$~.
\end{proof}

\subsection{The lower bound in the general cases}

We give here a result for any sequence of instances.

\begin{restatable}[]{lemma}{lemrec}\label{lem:rec} 
	Let there be a sequence of instances $({\bm\nu}^n)_{0\leq n\leq N}$ such that the probability of error is bounded by $\delta$ and for any $n\in[0,N-1]$, $c_n \geq \bP_{{\bm\nu}^n} [\tau_\delta >x_n]$.
	Then \begin{align*} \bP_{{\bm\nu}^N}[R_\delta>N] &\geq 1-2N\delta -\sum_{i=0}^{N-1}\left[  c_n+ \sqrt{\frac{X_{n-1}}{2}} \left(\sqrt{ \frac{1}{T^\star(\bm\mu^n)} }  +\sqrt{\sum_{i\in[K]}\frac{\bE[N_{X_{n-1},i}]}{X_{n-1}} \frac{(\mu_i^{n+1}-\mu_i^n)^2}{2\sigma^2}}\right)\right]\end{align*} where $X_n=\sum_{i=-1}^n x_i$, $x_{-1}$ is any positive real number, and $N_{t,i}$ is the number of times arm $i$ is sampled before time $t$.
\end{restatable}

\begin{proof}[Proof of Lemma~\ref{lem:rec}] 
	By lemmas \ref{lem:27f} and \ref{lem:26f}, for any $m$, \begin{align*} 
		\bP_{{\bm\nu}^{n+1}}\{R_\delta\geq n+1,\tau_\delta^{n+1}\leq m+x_n\}  &\geq \bP_{{\bm\nu}^n}\{R_\delta\geq n+1,\tau_\delta^n\leq m\}-c_n-\Vert\cD_{{\bm\nu}^n}^m-\cD_{{\bm\nu}^{n+1}}^m\Vert_{TV}\\
		&\geq \bP_{{\bm\nu}^{n}}\{R_\delta \geq n,\tau_\delta^n\leq m\}-2\delta-\sqrt{\frac{m}{2} (T^\star(\bm\mu^n))^{-1}}\\
		&\hspace{1.5em}-c_n-\sqrt{\frac{1}{2}\sum_{i\in[K]}\bE[N_{m,i}] \frac{(\mu_i^{n+1}-\mu_i^n)^2}{2\sigma^2}}
	\end{align*} and with $X_n=\sum_{i=-1}^{n} x_i$, \begin{align*}\bP_{{\bm\nu}^{n+1}}\{R_\delta\geq n+1,\tau_\delta^{n+1}\leq X_n\} & \geq \bP_{{\bm\nu}^{n}}\{R_\delta\geq n,\tau_\delta^n\leq X_{n-1}\}-2\delta-c_n -\sqrt{\frac{X_{n-1}}{2} (T^\star(\bm\mu))^{-1}}\\
	&\hspace{1.5em} -\sqrt{\frac{X_{n-1}}{2}\sum_{i\in[K]}\frac{\bE[N_{X_{n-1},i}]}{X_{n-1}} \frac{(\mu_i^{n+1}-\mu_i^n)^2}{2\sigma^2}}\end{align*} So that finally \begin{align*}
	\bP_{{\bm\nu}^{N}}\{R_\delta\geq N,\tau_\delta^N\leq X_{N-1}\} &\geq \bP_{{\bm\nu}^{0}}\{R_\delta\geq 0,\tau_\delta^0\leq x_{-1}\}-2N\delta\\
	&\hspace{1.5em} -\sum_{i=0}^{N-1}\left[ c_n+ \sqrt{\frac{X_{n-1}}{2}}\left(\sqrt{ (T^\star(\bm\mu^n))^{-1} } +\sqrt{\sum_{i\in[K]}\frac{\bE[N_{X_{n-1},i}]}{X_{n-1}} \frac{(\mu_i^{n+1}-\mu_i^n)^2}{2\sigma^2}}\right)\right]\end{align*} and we conclude since for any $x_{-1}\geq 0$, $\bP_{{\bm\nu}^0}\{R_\delta\geq 1,\tau_\delta^0\leq x_{-1}\}=1$ (we always use at least 1 round).
\end{proof}

From there, we derive the result for $T^\star(\bm\mu^n)=\zeta^{-n} T^\star(\bm\mu^0)$.

\theorec* 


\begin{proof}[Proof of Lemma~\ref{th:theorec}]
	We apply Lemma~\ref{lem:rec} on the sequence $(\bm\nu^n)_{0\leq n\leq N}$ with $x_{-1}=\gamma T^\star(\bm\mu^0)\log(1/\delta)\frac{1}{\zeta^{-1}-1}$. That way, \begin{align*} X_{n} &= x_{-1} +\sum_{i=0}^n \gamma T^\star(\bm\mu^i)\log(1/\delta)\\
		&=\gamma T^\star(\bm\mu^0)\log(1/\delta)\left( \frac{1}{\zeta^{-1}-1}+\sum_{i=0}^n \zeta^{-i}\right)\\
		&=\gamma T^\star(\bm\mu^0)\log(1/\delta)\frac{\zeta^{-(n+1)}}{\zeta^{-1}-1}
	\end{align*}
	
\end{proof}

Under Assumption~\ref{asm:aff}, we can pick a sequence of instances of means $\bm\mu^{n+1}=x\bm\mu^n+(1-x)\bm y$ and control the sequence of $T^\star(\bm\mu^n)$. That way, we get the following result:
\begin{restatable}[Batch lower bound on affine sequences]{lemma}{lembar}\label{lem:bar}
	For problems on which Assumption~\ref{asm:aff} is satisfied;
	for any algorithm such that, for any Gaussian instance $\bm\nu$ satisfying $T^\star(\bm\mu)\in (T_{\min},T_{\max})$ the probability of error is smaller than $\delta$ and such that $\bP_{\bm\nu}(\tau_\delta>\gamma\log(1/\delta) T^\star(\bm\mu))\leq c$; we have for any $\sigma$-Gaussian instance $\bm\nu$ of complexity $T^\star(\bm\mu)\in (T_{\min},T_{\max})$, for the corresponding $y\in \R$ given by Assumption~\ref{asm:aff} for $\bm\mu$, that $\bP_{\bm\nu} (R_\delta\geq N)\geq 1/2$ for \[N = \min\left\{\frac{\ln \frac{T^\star(\bm\mu)}{T_{\min}}}{\ln\left( \left(\ln \frac{T^\star(\bm\mu)}{T_{\min}}\right)^2 \max\{e,C\} \right)},\frac{1}{2\delta+c}\right\}\] with $C=1+4\gamma\log(\frac{1}{\delta})\left(1+\sqrt{\frac{T^\star(\bm\mu)\Delta^2}{2\sigma^2}}\right)^2$ and $\Delta = \max_i |\mu_i -y|$.
\end{restatable}

\begin{proof}
	Fix some ($\sigma$-Gaussian) instance $\bm\nu^0=\bm\nu$ of complexity $T^\star(\bm\mu)=T_0\in(T_{\min},T_{\max})$. 
	
	For some $\zeta\in (0,1)$ to be fixed later, define the instance of mean $\bm\mu^{n+1}=\zeta^{-1/2}\bm\mu^n+(1-\zeta^{-1/2})\bm y$. We then have $\zeta^nT^\star(\bm\mu^0)= T^\star(\bm\mu^{n})$ by hypothesis. We can thus construct a sequence of instances of length $N$ as long as $\zeta^{N} > \frac{T_{\min}}{T^\star(\bm\mu^0)}$.
	
	\begin{align*} \frac{(\mu_i^{N-n-1}-\mu_i^{N-n})^2}{2\sigma^2} &= \frac{(\zeta^{-(N-n-1)/2}-\zeta^{-(N-n)/2})^2(\mu_i^0-y)^2}{2\sigma^2}\\ &\leq \frac{\zeta^{n-N}\Delta^2}{2\sigma^2} (1-\zeta^{1/2})^2\end{align*}
	We apply Theorem~\ref{th:theorec} on the reversed sequence $\left( \bm\nu_{N-i}\right)_{0\leq i\leq N}$:
	\begin{align*}\bP_{{\bm\nu}}(R_\delta>N) &\geq 1-N(2\delta+c)-\sqrt{\frac{\gamma\log(1/\delta)}{2(\zeta^{-1}-1)}}\times \sum_{i=0}^{N-1} \left[ 1 +\sqrt{T^\star(\bm\mu)\sup_{w\in \Delta_K}\sum_{i\in[K]}w_i \frac{\Delta^2}{2\sigma^2} (1-\zeta^{1/2})^2}\right]\\
		&\geq 1-N\left(2\delta+c+\sqrt{\frac{\gamma\log(1/\delta)}{2(\zeta^{-1}-1)}} \left(1+\sqrt{\frac{T^\star(\bm\mu^0)\Delta^2}{2\sigma^2}}\right)\right)\\
		&\geq 5/8-N(2\delta+c)\end{align*} for \[\zeta =\Bigg( 1+4N^2\gamma\log\left(\frac{1}{\delta}\right)\left(1+\sqrt{\frac{T^\star(\bm\mu^0)\Delta^2}{2\sigma^2}}\right)^2\Bigg)^{-1}\]
	
	
	We can apply Lemma~\ref{lem:suffN} with $\rho = \frac{T^\star(\bm\mu^0)}{T_{\min}}$, $k=1$, $b=0$, and $a=4\gamma\log(1/\delta)\left(1+\sqrt{\frac{T^\star(\bm\mu^0)\Delta^2}{2\sigma^2}}\right)^2$. We get that a sufficient condition is \begin{equation} \label{eq:Nbai} N\leq \frac{\ln \frac{T^\star(\bm\mu^0)}{T_{\min}}}{\ln\left( \left(\ln \frac{T^\star(\bm\mu^0)}{T_{\min}}\right)^2 \max\left\{e,C\right\} \right)}\end{equation} with $C=1+4\gamma\log(1/\delta)\left(1+\sqrt{\frac{T^\star(\bm\mu^0)\Delta^2}{2\sigma^2}}\right)^2$.
	
	Therefore, by picking $N$ that satisfies \eqref{eq:Nbai} and $N\leq \frac{1}{8(2\delta+c)}$, we have that $\bP_{\bm\nu}(R_\delta > N)>\frac{1}{2}$.
\end{proof}




\begin{restatable}[Batch lower bound on affine sequences, expectation constraint]{lemma}{lembarexp}\label{lem:barexp}
	For problems on which Assumption~\ref{asm:aff} is satisfied; 
	for any $\delta$-correct algorithm such that, for any Gaussian instance $\bm\nu$ satisfying $T^\star(\bm\mu)\in (T_{\min},T_{\max})$ $\bE_{\bm\nu}[\tau_\delta]\leq \gamma\log(1/\delta) T^\star(\bm\mu)$, we have for any $\sigma$-Gaussian instance $\bm\nu$ of complexity $T^\star(\bm\mu)\in (T_{\min},T_{\max})$, for the corresponding $y\in\R$ given by Assumption~\ref{asm:aff} for $\bm\mu$ that $\bP_{\bm\nu} (R_\delta\geq N)\geq 1/2$ for \[ N \geq \min\left\{ \frac{\ln \frac{T^\star(\bm\mu)}{T_{\min}}}{\ln\left( \left(\ln \frac{T^\star(\bm\mu)}{T_{\min}}\right)^2 \max\{e,C_\delta'\}  \right)},\frac{1}{3} \ln \frac{T^\star(\bm\mu)}{T_{\min}},\frac{1}{3\delta}\right\}\] with $C_\delta'=\max\left\{e,1+4\gamma\log(1/\delta)\ln \frac{T^\star(\bm\mu)}{T_{\min}}\left(1+\sqrt{\frac{T^\star(\bm\mu)\Delta^2}{2\sigma^2}}\right)^2 \right\}$ and $\Delta = \max_i |\mu_i -y|$.
\end{restatable}

\begin{proof}
For instance $\bm\nu$, for some algorithm satisfying $\bE_{\bm\nu}[\tau_\delta]\leq \gamma \log(1/\delta) T^\star(\bm\mu)$, we have by the Markov inequality that $\bP_{\bm\nu}(\tau_\delta \geq (\gamma/c) T^\star(\bm\mu) \log(1/\delta)) \leq c$.
Applying Lemma~\ref{lem:bar}, $\bP_{\bm\nu} (R_\delta\geq N)\geq 1/2$ for \[N = \min\left\{\frac{\ln \frac{T^\star(\bm\mu)}{T_{\min}}}{\ln\left( \left(\ln \frac{T^\star(\bm\mu)}{T_{\min}}\right)^2 \max\{e,C\} \right)},\frac{1}{2\delta+c}\right\}\] with $C=1+4\gamma/c\log(\frac{1}{\delta})\left(1+\sqrt{\frac{T^\star(\bm\mu)\Delta^2}{2\sigma^2}}\right)^2$ and $\Delta = \max_i |\mu_i -y|$.

Choosing $c = \max\left\{ \delta,\left( \log\frac{T^\star(\bm\mu)}{T_{\min}}\right)^{-1}\right\}$, if $\delta < \left( \log\frac{T^\star(\bm\mu)}{T_{\min}}\right)^{-1}$, then \[ N \geq \min\left\{ \frac{\ln \frac{T^\star(\bm\mu)}{T_{\min}}}{\ln\left( \left(\ln \frac{T^\star(\bm\mu)}{T_{\min}}\right)^2 \max\{e,C_\delta'\}  \right)},\frac{1}{3} \ln \frac{T^\star(\bm\mu)}{T_{\min}}\right\}\] with $C_\delta'=\max\left\{e,1+4\gamma\log(1/\delta)\ln \frac{T^\star(\bm\mu)}{T_{\min}}\left(1+\sqrt{\frac{T^\star(\bm\mu)\Delta^2}{2\sigma^2}}\right)^2 \right\}$.

If $\delta \geq \left( \log\frac{T^\star(\bm\mu)}{T_{\min}}\right)^{-1}$, \[N \geq \min\left\{\frac{\ln \frac{T^\star(\bm\mu)}{T_{\min}}}{\ln\left( \left(\ln \frac{T^\star(\bm\mu)}{T_{\min}}\right)^2 \max\{e,C_\delta'\} \right)},\frac{1}{3\delta}\right\}\]



\end{proof}











\subsection{The top-$k$ and BAI settings}

All that remains is to show that our problems satisfy Assumption~\ref{asm:aff}. We start with top-$k$, and first give a technical result giving a simple formula for $T^\star(\bm\mu)$.

\begin{lemma}\label{lem:baiw}
	For any $\bm w\in \Sigma_K$, \begin{align*}\inf_{\bm\lambda \in Alt_{\bm\mu}} \left( \sum_{i\in [K]} w_i d(\mu_i,\lambda_i)\right) = \min_{\substack{b\geq k+1 \\ a\leq k}} w_a d(\mu_a,\mu_{ab})+w_b d(\mu_b,\mu_{ab})\end{align*} where $\mu_{ab}=\frac{w_a\mu_a +w_b\mu_b}{w_a+w_b}$ (arms are assumed to be ordered, $\mu_1\geq\mu_2\geq \dots$).
\end{lemma}

\begin{lemma}\label{lem:topkgoodbar}
	In the top-$k$ problem, setting $\bm\mu' = x\bm\mu + (1-x)\bm y$ where $\bm y$ is a constant vector and $x>0$, $Alt_{\bm\mu'}=Alt_{\bm\mu}$, $\Delta_i^{\bm\mu'}=x\Delta_i^{\bm\mu}$ and $(T^\star(\bm\mu'))^{-1}=x^2(T^\star(\bm\mu))^{-1}$.
\end{lemma}
\begin{proof}
	First of all, for any two arms $i,j$, $\mu'_i-\mu'_j = x(\mu_i-\mu_j)$ with $x>0$. Therefore, the ordering of arms is conserved, and $Alt_{\bm\mu'}=Alt_{\bm\mu}$. Moreover, since $\Delta_i^{\bm\mu'} = \mu'_i - \mu'_{k+1} =x(\mu_i-\mu_{k+1})=x\Delta_i^{\bm\mu}$ for $i\leq k$ and $\Delta_i^{\bm\mu'} = \mu'_k-\mu'_i=x\Delta_i^{\bm\mu}$ otherwise, we do have $\Delta_i^{\bm\mu'}=x\Delta_i^{\bm\mu}$.
	
	Furthermore,
	\begin{align*}
		(T^\star(\bm\mu'))^{-1}&= \sup_{w\in \Sigma_K}\inf_{\bm\lambda \in Alt_{\bm\mu}} \left( \sum_{i\in [K]} w_i \frac{(\mu_i'-\lambda_i)^2}{2\sigma^2}\right)\\
		&= \sup_{w\in \Sigma_K} \min_{\substack{b\geq k+1 \\ a\leq k}} w_a \frac{(\mu_a'-\mu_{ab}')^2}{2\sigma^2}+w_b \frac{(\mu_b'-\mu_{ab}')^2}{2\sigma^2}
	\end{align*} with \begin{align*}\mu_{ab}'(w)&=\frac{w_a\mu_a' +w_b\mu_b'}{w_a+w_b}=x\mu_{ab}+(1-x)y\end{align*}
	So that \begin{align*}
		(T^\star(\bm\mu'))^{-1}&=x^2\sup_{w\in \Sigma_K}\min_{\substack{b\geq k+1 \\ a\leq k}} w_a \frac{(\mu_a-\mu_{ab})^2}{2\sigma^2}+w_b \frac{(\mu_b-\mu_{ab})^2}{2\sigma^2}\\
		&=x^2(T^\star(\bm\mu))^{-1}
	\end{align*}


\end{proof}

With these results, we can apply Lemmas~\ref{lem:bar} and \ref{lem:barexp}. We see that the value of $y$ does not impact the proof: we thus choose the value that minimizes $\max_i |\mu_i-y|$, which is $y = \frac{\max_i \mu_i+\min_i\mu_i}{2}$.

%\thlbbaib*
	

	
%\begin{proof}\textbf{of Theorem~\ref{th:lbbaib}}
%	Thanks to Lemma~\ref{lem:topkgoodbar}, it suffices to apply Lemma~\ref{lem:bar} with $\nu = \frac{\mu_1+\mu_K}{2}$. We have $\Delta = |\mu_1-\mu_K|/2$.
%\end{proof}	

\subsection{The thresholding setting}

\begin{lemma}\label{lem:tbpgoodbar}
	In the thresholding bandit problem, setting $\bm\mu' = x\bm\mu + (1-x)\bm \tau$ where $\bm \tau$ is the constant vector of value $\tau$ the threshold and $x>0$, $Alt_{\bm\mu'}=Alt_{\bm\mu}$, $\Delta_i^{\bm\mu'}=x\Delta_i^{\bm\mu}$ and $(T^\star(\bm\mu'))^{-1}=x^2(T^\star(\bm\mu))^{-1}$.
\end{lemma}

\begin{proof}
	First of all, for any arm $i$, $\mu'_i -\tau = x(\mu_i-\tau)$ with $x>0$. Therefore, $Alt_{\bm\mu}=Alt_{\bm\mu'}$. Moreover, $\Delta_i^{\bm\mu'}=|\mu'_i-\tau|=x|\mu_i-\tau|=x\Delta_i^{\bm\mu'}$.
	
	Furthermore, \begin{align*}
		(T^\star(\bm\mu'))^{-1}&=\sup_{w\in \Sigma_K} \inf_{\bm\lambda\in Alt_{\bm\mu}} \left( \sum_{i\in[K]} w_i\frac{(\mu'_i-\lambda_i)^2}{2\sigma^2}\right)\\
		&=\sup_{w\in \Sigma_K} \sup_{i\in [K]} w_i \frac{(\mu'_i-\tau)^2}{2\sigma^2}\\
		&=x^2 \sup_{w\in \Sigma_K} \sup_{i\in [K]} w_i \frac{(\mu_i-\tau)^2}{2\sigma^2}\\
		&=x^2(T^\star(\bm\mu))^{-1}
	\end{align*}
\end{proof}

