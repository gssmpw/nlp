% !TeX root = ../all.tex


\section{Introduction}

A Multi Armed Bandit (MAB) is a model of a sequential interaction that was introduced in \citep{thompsonLikelihoodThatOne1933} to create better medical trials.
This framework has since been expanded to various fields, and has seen applications to online advertising and recommendation systems.
In a MAB, an algorithm chooses at each time an \emph{arm} among a finite number (it \emph{pulls} it) and then observes a sample from a probability distribution associated with the arm.
The goal of the interaction will be to identify quickly which arm has the distribution with highest mean.

By making use of past observed rewards to continuously update the way they sample, MAB algorithms reach their objective faster than traditional fixed randomized trials.
For applications like online advertising, obtaining feedback can be quick, if for example the feedback is a click on an advertisement. Adapting after almost each interaction is then feasible.
However in applications like clinical trials, the delay between giving a treatment to a patient and seeing the effect can be on the order of months.
It is not possible to use a fully sequential algorithm, which would take too long.

This motivates looking into \textit{batched} algorithms: the algorithm pulls multiple arms within a single batch, and can only observe the results at the end of the batch.
An advantage is that all arms in a batch can be pulled in parallel, reducing greatly the length of the test in applications with large delay.
Of course, being less adaptive than a typical bandit method could lead to worse algorithms.
The key question is then how few batches one can use while keeping a performance comparable to a fully sequential algorithm.


\subsection{Fixed Confidence Pure Exploration}


We study $K$-armed bandit models, which we represent by a vector of real distributions $\bm\nu=(\nu_i)_{i\in [K]}$ with finite means, with the vector of means denoted by $\bm\mu=(\mu_i)_{i\in[K]}$. %=\left( \bE[\nu_i]\right)_{i\in [K]}$.
At each timestep $t \in \mathbb{N}$, an agent interacts with the bandit by choosing an arm $i_t\in [K]$ to sample.
It then observes $X_{t, i_t}$ a realization of $\nu_{i_t}$ and proceeds to the next step.
One common objective is regret minimization, where the agent must maximize $\bE[\sum_t X_{t, i_t}]$. See \citep{bubeck2012regret,lattimore2020bandit} for extensive surveys.
We focus on another goal, pure exploration, in which the agent must answer a question about $\bm\nu$, such as identifying the arm with the highest mean \citep{even-darPACBoundsMultiarmed2002,kaufmann2016complexity}.
In pure exploration problems, we have a set $\mathcal{I}$ of possible answers, and a function $\bm\mu\mapsto i^\star(\bm\mu)\in\mathcal{I}$ that depends on the means of the distributions and provides the unique correct answer for that instance.
For a given pure exploration problem and a vector of means $\bm\mu$, we denote by $Alt_{\bm \mu}$ the set of vectors of means $\bm{\mu'}$ such that $i^\star(\bm\mu') \ne i^\star(\bm\mu)$. Essentially, $Alt_{\bm\mu}$ is the set of means that disagree with $\bm\mu$. 

A pure exploration algorithm samples arms in a bandit interaction (following a \emph{sampling rule}) until a time at which it decides to stop (with a \emph{stopping rule}). Once it stops, it returns an answer $\hat{i} \in \mathcal I$.
A good pure exploration algorithm should have low probability of error $\bP_{\bm\nu}\{\hat{i}\neq i^\star(\bm\mu)\}$ and stop quickly (use few samples).
When the number of samples is fixed and the objective is to minimize the probability of error, we talk about fixed-budget pure exploration \citep{audibertBestArmIdentification2010}.
We consider instead the \emph{fixed confidence} objective, in which the probability of error is fixed and the agent aims to achieve it with the minimum number of samples.
We focus on \textbf{$\delta$-correct algorithms}. That is, algorithms that satisfy $\bP_{\bm\nu}\{\hat{i}\neq i^\star(\bm\mu)\} \le \delta$ \citep{garivierOptimalBestArm2016}.

We focus on bandit models where each arm $i\in[K]$ has a sub-Gaussian distribution $\nu_i$ of constant $\sigma^2$.
That is, for all $\lambda \in \mathbb{R}$, $\mathbb{E}_{\nu_i}[e^{\lambda (X - \mu_i)}] \le e^{\sigma^2 \lambda^2 / 2}$.
For example, both bounded and Gaussian distributions are sub-Gaussian.


Finally, we study \textbf{batched} algorithms \citep{agarwalLearningLimitedRounds2017,jinEfficientPureExploration2019}.
In practical settings, it is not always possible to adapt at every single timestep. 
Therefore, we want the agent to only observe results and take decisions a limited number of times.
At the beginning of a batch, the agent decides how many times to sample each arm $i \in [K]$, then observes all the realizations of those pulls.
It then can proceed to the next batch.
Crucially, the agent cannot change anything during a batch: it cannot change its sampling decision and cannot stop a batch early.
For $\delta$-correct batched algorithms, for each pure exploration problem, we want to control the sample complexity $\tau_\delta$ (the number of samples used), and the batch complexity $R_\delta$ (the number of batches used, which is the number of times the results were observed and the sampling rule was updated).


In all pure exploration problems, the expectation of the stopping time of $\delta$-correct strategies on an instance $\bm\nu$ is bounded from below as follows \citep{garivierOptimalBestArm2016}. Let $\Sigma_K$ be the simplex, $\KL$ the Kullback-Leibler divergence and $\kl$ the KL between Bernoulli distributions.
\begin{align*}
&\bE_{\bm\nu}[\tau_\delta]
\ge \tilde{T}^\star(\bm\mu) \kl(\delta,1-\delta)
\ge \tilde{T}^\star(\bm\mu) \log (1/(2.4 \delta))
\: , \\
&\text{in which }
(\tilde{T}^\star(\bm\mu))^{-1}
= \sup_{w\in \Sigma_K}\inf_{\bm\lambda\in Alt_{\bm\mu}} \sum_{i=1}^K w_i\KL(\nu_i,\lambda_i)
\: .
\end{align*}
 This result has been expanded to pure exploration problems with multiple correct answers by \citep{degennePureExplorationMultiple2019} in the asymptotic regime $\delta\rightarrow 0$. This $\tilde{T}^\star(\bm\mu)$ is the instance-dependent complexity of instance $\bm\mu$ in this problem. Our objective is to develop algorithms with a sample complexity approaching $\tilde{T}^\star(\bm\mu)\log(1/\delta)$ for all $\bm\mu$.
 As we are in the $\sigma$-subgaussian setting, $\tilde{T}^\star (\bm\mu) \leq T^\star(\bm\mu)$ where \[(T^\star(\bm\mu))^{-1} = \sup_{w\in \Delta_K}\inf_{\bm\lambda\in Alt_{\bm\mu}} \sum_i w_i\frac{(\mu_i-\lambda_i)^2}{2\sigma^2}\] (by Donsker-Varadhan duality, see for example \citep{wang2021sub}) with equality for Gaussians with variance $\sigma^2$.
Our goal is to obtain algorithms that have sample complexity as close to $T^\star(\bm\mu) \log(1/\delta)$ as possible while using few batches.


We will prove results for general identification problems, and specialize them to three particular cases.
Let $\sigma$ be a permutation such that $\mu_{\sigma(1)} \ge \ldots \ge \mu_{\sigma(K)}$.
\begin{itemize}
	\item In \textbf{Best Arm Identification} (BAI), $\mathcal{I}=[K]$ and, for some instance for which $\mu_{\sigma(1)}>\mu_{\sigma(2)}$, the correct answer is $i^\star(\bm\mu)=\sigma(1)$ the best arm.
	
	\item In \textbf{Top-$k$}, $\mathcal{I}=\mathcal{P}_k(K)$ the set of all subsets of $[K]$ of size $k$, and the correct answer for some instance for which $\mu_{\sigma(k)}>\mu_{\sigma(k+1)}$ is $i^\star(\bm\mu)=\{\sigma(1),\sigma(2),\dots,\sigma(k)\}$ the set of the best $k$ arms.

	Note that top-$1$ is BAI.
	
	\item In the \textbf{Thresholding Bandit Problem} (TBP) with threshold $\tau$ \citep{locatelliOptimalAlgorithmThresholding2016}, $\mathcal{I}$ is the set of all subsets of $[K]$, and the correct answer for some instance for which $\forall i,\mu_i\neq \tau$ is $i^\star=\{i:\mu_i >\tau\}$ the set of all arms that have their mean above the threshold. 
\end{itemize}




\subsection{Related Work}


The problem of batched bandit algorithms has been studied in fixed-confidence BAI and Top-k. Some of the works cited below consider a task with multiple agents that limit the number of rounds in which they communicate, but their methods also give batched bandit algorithms.
\citet{hillelDistributedExplorationMultiArmed2013} derived an algorithm for $\varepsilon$-BAI (the algorithm needs to return an arm with mean within $\varepsilon$ of the best) which progressively eliminates arms. They show high probability bounds on the batch and sample complexities: with probability $1 - \delta$, it uses less than $\log(1/\varepsilon)$ batches and uses $O\left( \sum_i (\Delta_i^\varepsilon)^{-2} \log\left(\frac{K}{\delta}\log(\Delta_i^\varepsilon)^{-1}\right)\right)$ samples with $\Delta_i^\varepsilon =\max\{\mu^\star - \mu_i,\varepsilon\}$.
\citet{agarwalLearningLimitedRounds2017} consider $\delta$-correct algorithms for BAI that use a given number of batches, and give a worst-case upper bound on the number of samples used by their algorithm (also elimination based), but their method requires the knowledge of a lower bound on the gaps of the arms.

\citet{jinEfficientPureExploration2019} and \citet{karpovCollaborativeTopDistribution2020} designed $\delta$-correct algorithms for the Top-k problem, and give each bounds with high probability on the batch and sample complexities.
\citep{jinOptimalBatchedBest2023} is the work which is closest to our approach when it comes to algorithms. They study the BAI setting and prove bounds on the expected batch and sample complexity. Compared to bounds with probability $1 - \delta$, that requires that they control the complexities also in the event of probability $\delta$. Their method for finite $\delta$ is also elimination based, but they additionally propose an algorithm with guarantees as $\delta \to 0$ that is inspired by the Track-and-Stop BAI algorithm \citep{garivierOptimalBestArm2016}.

All those works cover Top-k or its special case BAI. However, many other pure exploration problems have been studied in the fully sequential setting, like thresholding bandits \citep{locatelliOptimalAlgorithmThresholding2016} or the problem of detecting if any arm has mean larger than a threshold \citep{kaufmann2018sequential}.
We seek to analyze general pure exploration problems and answer the following questions: what is the minimal expected batch sample complexity needed to get an expected sample complexity close to what is achievable by fully sequential algorithms? Can we design a general pure exploration algorithm with near-optimal sample complexity and that reaches that minimal batch complexity?

\subsection{Contributions}

We show a link between sample and batch complexities.
We build in Section~\ref{sect:lb} a general method for computing batch complexities lower bounds for $\delta$-correct pure exploration algorithms.
We demonstrate how to apply that method to Top-k (including BAI) and thresholding bandits.
The lower bounds we obtain are instance dependent: contrary to previous work, we don't merely state that there exist an instance on which the algorithm requires some number of batches, but we give a lower bound for the batch complexity of each instance, function of its complexity $T^\star$.


In Section~\ref{sect:alg}, we construct a general batched algorithm for pure exploration problems, taking inspiration from Track-and-Stop \citep{garivierOptimalBestArm2016}.
The batch complexity of that algorithm is close to the lower bound under mild conditions that are satisfied on Top-k and thresholding bandits. Moreover, its sample complexity is close to optimal in the high confidence regime (small error probability $\delta$).