% !TeX root = ../all.tex


\section{Concentration and threshold for the stopping rule}
\label{app:concentration}

We suppose that each arm is sampled once during the first $K$ time steps.
\begin{theorem}
  \label{thm:bound_delta}
  Suppose that the arm distributions are $\sigma^2$-sub-Gaussian. Let $\hat{\mu}_{t,k}$ be the average of arm $k$ at time $t$ and $N_{t,k}$ be the number of times arm $k$ is sampled up to time $t$.
  With probability $1 - \delta$, for all $t > K$,
  \begin{align*}
  \frac{1}{2} \sum_{k=1}^K N_{t,k}\frac{(\hat{\mu}_{t,k}- \mu_k)^2}{2 \sigma^2}
  &\le \frac{K}{2} \overline{W}\left(2\ln \left(\frac{e\pi^2}{6}\right) + \frac{2}{K}\ln \left(\prod_{k=1}^K (1 + \ln N_{t,k})^2\right) + \frac{2}{K}\ln \frac{1}{\delta}\right)
  \: .
  \end{align*}
\end{theorem}



\subsection{Proof of the concentration theorem}

We can assume $w.l.o.g.$ that $\mu_k = 0$ for all $k$ and $\sigma^2 = 1$.

Let $S_{t,k} = \sum_{s=1}^t X_{s,k} \mathbb{I}\{k_s = k\}$.
We want a bound on $\frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k}}$.


We first remark that $\frac{1}{2}x^2 = \sup_{\lambda} \lambda x - \frac{1}{2}\lambda^2$~. Apply that to $x = S_{t,k}/\sqrt{N_{t,k}}$ to get
\begin{align*}
\sum_{k=1}^K \frac{1}{2}\frac{S_{t,k}^2}{N_{t,k}}
&= \sup_{\lambda_1, \ldots, \lambda_K} \sum_{k=1}^K \left( \lambda_k S_{t,k} - \frac{1}{2}N_{t,k} \lambda_k^2 \right)
\\
&= \sup_{\lambda_1, \ldots, \lambda_K} \sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2
\: .
\end{align*}
The advantage of that formulation is that we can concentrate the sum for any fixed value of $\lambda$ (or any distribution on $\lambda$) thanks to a martingale argument.

\begin{lemma}
For all $\rho \in \mathcal P(\mathbb{R}^K)$, the process $t \mapsto \mathbb{E}_{\lambda \sim \rho}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right]$ is a non-negative supermartingale with expectation bounded by 1.
\end{lemma}


\begin{corollary}\label{cor:prob_exists_log_ge_le}
For all $\rho \in \mathcal P(\mathbb{R}^K)$ and $x \ge 0$,
\begin{align*}
\mathbb{P}\left(\exists t, \ \ln \mathbb{E}_{\lambda \sim \rho}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right] \ge x\right) \le e^{-x}
\: .
\end{align*}
Equivalently, for all $\delta \in (0,1]$,
\begin{align*}
\mathbb{P}\left(\exists t, \ \ln \mathbb{E}_{\lambda \sim \rho}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right] \ge \ln\frac{1}{\delta}\right) \le \delta
\: .
\end{align*}
\end{corollary}

\begin{proof}
Use Ville's inequality and the fact that the process is a non-negative supermartingale.
\end{proof}

We don't want to bound an integral over $\lambda \sim \rho$, but the supremum over $\lambda$, so we need to relate the two quantities.
We do that for Gaussian priors over $\lambda$.

\begin{lemma}\label{lem:integral_exp_eq_log_add}
For $\rho = \mathcal N(0, \mathrm{diag}(\sigma_k^{-2}))$,
\begin{align*}
\ln \mathbb{E}_{\lambda \sim \rho}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right]
&= -\frac{1}{2}\sum_{k=1}^K \ln(1 + N_{t,k}\sigma_k^{-2}) + \frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{(N_{t,k} + \sigma_k^2)}
\: .
\end{align*}
\end{lemma}

\begin{proof}
\begin{align*}
&\mathbb{E}_{\lambda \sim \rho}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right]
\\
&= \prod_k \mathbb{E}_{\lambda_k \sim \mathcal N(0, \sigma_k^{-2})}\left[\exp\left(\lambda_k S_{t,k} - \frac{1}{2}N_{t,k} \lambda_k^2\right)\right]
\\
&= \prod_k \frac{1}{\sqrt{2 \pi \sigma_k^{-2}}}\int_{\lambda_k}\exp\left(\lambda_k S_{t,k} - \frac{1}{2}N_{t,k} \lambda_k^2 - \frac{\sigma_k^2}{2}\lambda_k^2\right)\mathrm{d}\lambda_k
\\ 
&= \prod_k \frac{1}{\sqrt{(1 + N_{t,k}\sigma_k^{-2})}} \frac{1}{\sqrt{2 \pi (N_{t,k} + \sigma_k^2)^{-1}}}
  \int_{\lambda_k} \exp\left(-\frac{1}{2}(N_{t,k} + \sigma_k^2)\left( \lambda_k - \frac{S_{t,k}}{(N_{t,k} + \sigma_k^2)} \right)^2 + \frac{1}{2}\frac{S_{t,k}^2}{N_{t,k} + \sigma_k^2}\right)\mathrm{d}\lambda_k
\\
&= \prod_k \frac{1}{\sqrt{(1 + N_{t,k}\sigma_k^{-2})}} \exp\left(\frac{1}{2}\frac{S_{t,k}^2}{N_{t,k} + \sigma_k^2}\right)
\\ 
\end{align*}

\end{proof}

\begin{corollary}\label{cor:sum_le_eta_mul}
Let $\rho = \mathcal N(0, \mathrm{diag}(\sigma_k^{-2}))$, $\eta_{t,\max} = \max_k \frac{\sigma_k^2}{N_{t,k}}$ and $\eta_{t,\min} = \min_k \frac{\sigma_k^2}{N_{t,k}}$. Then
\begin{align*}
\frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k}}
&\le (1 + \eta_{t,\max}) \left(\ln \mathbb{E}_{\lambda \sim \rho}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right]
  + \frac{K}{2}\ln(1 + \eta_{t,\min}^{-1})\right)
\end{align*}

\end{corollary}

\begin{proof}
Using Lemma~\ref{lem:integral_exp_eq_log_add},
\begin{align*}
\frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k} + \sigma_k^2}
&= \ln \mathbb{E}_{\lambda \sim \rho}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right]
  + \frac{1}{2} \sum_{k=1}^K \ln(1 + N_{t,k}\sigma_k^{-2})
\: .
\end{align*}
Then
\begin{align*}
\frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k} + \sigma_k^2}
\ge \frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k}(1 + \eta_{t,\max})}
= \frac{1}{1 + \eta_{t,\max}}\frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k}}
\end{align*}
Finally, $N_{t,k} \sigma_k^{-2} \le \eta_{t,\min}^{-1}$.
\end{proof}

If $N_{t,k}$ was a known, unchanging number, we could choose $\sigma_k^2 \propto N_{t,k}$ to get $\eta_{t,\max} = \eta_{t, \min}$, and we would choose it to minimize the right hand side.
The strategy to use that ``known $N_{t,k}$'' case even if they are random is to put geometric grids on the number of pulls of each arm, define distributions that are adapted to each cell of the grid, and combine them into a mixture of Gaussians.

Let $(\eta_{n_1, \ldots, n_K})_{n_1, \ldots, n_K \in \mathbb{N}}$ be non-negative real numbers that will be chosen later.
For $i \in \mathbb{N}$, let $w_i = \frac{6}{\pi^2}\frac{1}{(i+1)^2}$. The weights $(w_i)$ satisfy $\sum_{i \in \mathbb{N}} w_i = 1$, hence also $\sum_{n_1, \ldots, n_K} (\prod_{k=1}^K w_{n_k}) = 1$.

Let $\rho_{n_1, \ldots, n_K} = \bigotimes_{k=1}^K \mathcal N(0, e^{- n_k} \eta_{n_1, \ldots, n_K}^{-1})$. This is a product distribution, with each marginal being a Gaussian with mean 0 and variance that depends on the number of grid cell.

With probability $1 - \delta$, for all $(n_1, \ldots, n_K) \in \mathbb{N}^K$ and all $t$,
\begin{align*}
\ln \mathbb{E}_{\lambda \sim \rho_{n_1, \ldots, n_K}}\left[\exp\left(\sum_{s=1}^t \lambda_{k_s}X_{s,k_s} - \frac{1}{2}\lambda_{k_s}^2\right)\right]
\le \ln \frac{1}{\delta} + \sum_{k=1}^K \ln \frac{1}{w_{n_k}}
\: .
\end{align*}
This is simply an union bound using Corollary~\ref{cor:prob_exists_log_ge_le}, with weight $\prod_{k=1}^K w_{n_k}$ for $\rho_{n_1, \ldots, n_K}$.

In particular, there exists $(n_1, \ldots, n_k)$ such that for all $k \in [K]$, $e^{n_k} \le N_{t,k} \le e^{n_k+1}$.
For that choice, $e^{-1}\eta_{n_1, \ldots, n_K} \le \frac{e^{n_k}\eta_{n_1, \ldots, n_K}}{N_{t,k}} \le \eta_{n_1, \ldots, n_K}$.
For those values, using Corollary~\ref{cor:sum_le_eta_mul} with $\sigma_k^2 = e^{n_k}\eta_{n_1, \ldots, n_K}$, with probability $1 - \delta$,
\begin{align*}
\frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k}}
&\le (1 + \eta_{n_1, \ldots, n_K}) \left(\ln \frac{1}{\delta} + \sum_{k=1}^K \ln \frac{1}{w_{n_k}}
  + \frac{K}{2}\ln(1 + e \eta_{n_1, \ldots, n_K}^{-1})\right)
\\
&\le (1 + \eta_{n_1, \ldots, n_K}) \left(\ln \frac{e^{K/2}}{\delta} + \sum_{k=1}^K \ln \frac{1}{w_{n_k}}
  + \frac{K}{2}\ln(1 + \eta_{n_1, \ldots, n_K}^{-1})\right)
\\
&= (1 + \eta_{n_1, \ldots, n_K}) \left(\ln \frac{(\sqrt{e}\pi^2/6)^K \prod_{k=1}^K (n_k+1)^2}{\delta}
  + \frac{K}{2}\ln(1 + \eta_{n_1, \ldots, n_K}^{-1})\right)
\end{align*}

This is where we choose $\eta_{n_1, \ldots, n_K}$ to minimize the right hand side.

By Lemma A.3 of \citep{degenneImpactStructureDesign2019}, the minimal value is attained at some $\eta_{n_1, \ldots, n_K}$ such that
\begin{align*}
&(1 + \eta_{n_1, \ldots, n_K}) \left(\ln \frac{(\sqrt{e}\pi^2/6)^K \prod_{k=1}^K (n_k+1)^2}{\delta}
  + \frac{K}{2}\ln(1 + \eta_{n_1, \ldots, n_K}^{-1})\right)
\\
&= \frac{K}{2} \overline{W}\left( 1 + \frac{2}{K}\ln \frac{(\sqrt{e}\pi^2/6)^K \prod_{k=1}^K (n_k+1)^2}{\delta}\right)
\end{align*}

By the choice of $n_k$, it satisfies $n_k \le \ln N_{t,k}$.
We get that with probability $1 - \delta$, for all $t$,
\begin{align*}
\frac{1}{2} \sum_{k=1}^K \frac{S_{t,k}^2}{N_{t,k}}
&\le \frac{K}{2} \overline{W}\left( 1 + \frac{2}{K}\ln \frac{(\sqrt{e}\pi^2/6)^K \prod_{k=1}^K (1 + \ln N_{t,k})^2}{\delta}\right)
\\
&= \frac{K}{2} \overline{W}\left(\frac{2}{K}\ln \left((e\pi^2/6)^K \prod_{k=1}^K (1 + \ln N_{t,k})^2\right) + \frac{2}{K}\ln \frac{1}{\delta}\right)
\: .
\end{align*}

This ends the proof of the theorem.



\subsection{Upper bounds on $\beta(t, \delta)$ and on $\gamma_r$}

We choose the threshold
\begin{align*}
\beta(t, \delta)
&= \frac{K}{2} \overline{W}\left(2\ln \left(\frac{e\pi^2}{6}\right) + \frac{2}{K}\ln \left(\prod_{k=1}^K (1 + \ln N_{t,k})^2\right) + \frac{2}{K}\ln \frac{1}{\delta}\right)
\: .
\end{align*}

We can get an upper bound that is not random by maximizing over $(N_{t,k})_{k \in [K]}$ under the constraint $\sum_{k=1}^K N_{t,k} = t$. We get
\begin{align*}
\beta(t, \delta)
&\le \frac{K}{2} \overline{W}\left(2\ln \left(\frac{e\pi^2}{6}\right) + 4\ln \left(1 + \ln \frac{t}{K}\right) + \frac{2}{K}\ln \frac{1}{\delta}\right)
\: .
\end{align*}
We can get further upper bounds by using $\overline{W}(x) \le x + \ln x + 1/2 \le 2x$. This gives
\begin{align*}
\beta(t, \delta)
&\le 2 K\ln \left(\frac{e\pi^2}{6}\right) + 4 K\ln \left(1 + \ln \frac{t}{K}\right) + 2\ln \frac{1}{\delta}
\\
&\le 2 K\ln \left(\frac{e\pi^2}{6}\right) + 4 K\ln \frac{t}{K} + 2\ln \frac{1}{\delta}
\: .
\end{align*}
The right asymptotic for $\beta(t, \delta)$ as $\delta \to 0$ is $\ln(1/\delta)$. We lost a factor 2 in the upper bound above.




\begin{lemma}\label{lem:gamma_ub_bis}
Let $\gamma_r$ be the solution to $\beta(\bar{t}_r, \delta) = \gamma_r$ , for $\bar{t}_r = 2(K l_{1,r}/T_0 + \gamma_r) T_r$ and $l_{1,r} = 32 T_0 \ln (2\sqrt{2K} T_r)$. Then
\begin{align*}
\gamma_r \le 4 \ln \frac{1}{\delta} + 8K \ln(T_r) + 4K (11 + \ln K)
\: .
\end{align*}
\end{lemma}

\begin{proof}
We use an upper bound for $\beta(t, \delta)$: $\gamma_r$ is bounded from above by the solution $\gamma'_r$ to
\begin{align*}
\gamma = 2 K\ln \left(\frac{e\pi^2}{6}\right) + 4 K \ln \left(2(32 \ln (2\sqrt{2K} T_r) + \frac{\gamma}{K}) T_r\right) + 2\ln \frac{1}{\delta}
\: .
\end{align*}
Then either $\gamma'_r \le 8 K \ln (2\sqrt{2K} T_r)$ or $\gamma'_r$ is less than the solution to
\begin{align*}
\gamma
&= 2 K\ln \left(\frac{e\pi^2}{6}\right) + 4 K \ln \left(10\frac{\gamma T_r}{K}\right) + 2\ln \frac{1}{\delta}
\\
&= 2 K\ln \left(\frac{50 e\pi^2}{3}\right) + 4 K \ln \left(\frac{\gamma T_r}{K}\right) + 2\ln \frac{1}{\delta} 
\: .
\end{align*}


That is,
\begin{align*}
\gamma'_r
&= 4K \overline{W}\left( \frac{1}{2K} \ln \frac{1}{\delta} + \ln(T_r) + \frac{1}{2}\ln\frac{800 e \pi^2}{3} \right)
\\
&\le 4 \ln \frac{1}{\delta} + 8K \ln(T_r) + 4K \ln\frac{800 e \pi^2}{3}
\: .
\end{align*}

At this point, we get
\begin{align*}
\gamma_r
&\le \max\left\{ 4 \ln \frac{1}{\delta} + 8K \ln(T_r) + 4K \ln\frac{800 e \pi^2}{3},  8 K \ln (2\sqrt{2K} T_r)\right\}
\\
&\le 8K \ln(T_r) + \max\left\{ 4 \ln \frac{1}{\delta} + 4K \ln\frac{800 e \pi^2}{3}, 8 K \ln (2\sqrt{2K})\right\}
\\
&\le 8K \ln(T_r) + 4 \ln \frac{1}{\delta} + 4K \ln\frac{800 e \pi^2}{3} + 8 K \ln (2\sqrt{2K})
\\
&\le 8K \ln(T_r) + 4 \ln \frac{1}{\delta} + 4K \ln\left(\frac{6400 e \pi^2}{3}K\right)
\\
&\le 8K \ln(T_r) + 4 \ln \frac{1}{\delta} + 4K (11 + \ln K)
\: .
\end{align*}


\end{proof}