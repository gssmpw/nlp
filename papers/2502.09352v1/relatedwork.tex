\section{Literature Review}
\paragraph{Adversarial Attacks (AA).}
AAs vary between white-box attacks, where the attacker has full knowledge of the neural network and black-box attacks, where they have limited, or no, such knowledge. The latter include, e.g., Zeroth Order Optimization (ZOO) \citep{CZS+17}, Boundary Attack \citep{BRB18}, and Query-limited Attack \citep{IEAL18}. Of more relevance to us are the former attacks, originally focused on the \emph{pointwise} \(l_{p}\)-bounded image distortions, including Fast Gradient Sign Method (FGSM) \citep{GSS15}, Projected Gradient Descent (PGD) \citep{MMS+18}, CW attack \citep{CW17}. A useful benchmark for \(l_{p}\)-robustness of neural networks is provided by AutoAttack \citep{CH20}, an ensemble of various attack methods. More recently, a broader class of \emph{distributional threats} has been considered, see \citet{SJ17,SND18}, and \citet{BHJO23Wasserstein} proposed the first AA associated to such threats using Wasserstein distances. 

\paragraph{Adversarial Training.}
A variety of adversarial training methods, designed to withstand AAs,  have been introduced, including data augmentation, loss regularization, parameter fine-tuning, etc. 
To augment the training data, researchers have used adversarial examples, generated with AAs, see \citep{GSS15,MMS+18,TKP+18}, as well as random modifications, e.g., using generative models such as GANs and diffusion models, see \citep{GRW+21,XSC22,wang2023better}. 
\citet{zhang2019theoretically} consider the trade-off between robustness and accuracy of a neural network via TRADES, a regularized loss. Analogous research includes MART \citep{WZY+20} and SCORE \citep{PLY+22}. 
Loss regularization methods, e.g., adversarial weight perturbation \citep{WXW20}, have been shown to smooth the loss landscape and improve the robustness. 
In addition, various training techniques can be overlaid to improve robustness, including drop-out layers, early stopping and parameter fine-tuning \citet{SWMJ20}.
Among various robust training methods, \citet{SND18,GG22} are directly related to our work as they employ Wasserstein penalization and constraint respectively. In contrast, we use sensitivity analysis of \citet{BDOW21}, to design novel robust training methods. Conceptually close to our work is the adversarial distributional training (ADT) of \citep{DDP+20} with the key difference that we employ Wasserstein distances instead of an entropic regularization. 


\paragraph{Distributionally Robust Optimization (DRO).}
DRO provides a unifying language across many papers mentioned above. 
Mathematical formulation involves a min-max problem
\begin{equation}
    \label{eqn-dro}
    \inf_{\theta\in \Theta} \sup_{Q\in\scrP} \E_{Q}[f_{\theta}(Z)],
\end{equation}
where we minimize the worst-case loss over all possible distributions \(Q\in\scrP\). 
In financial economics, such criteria appear in the context of multi-prior preferences, see \citep{GS89,FW15}. We refer to \citep{RM19} for a survey of the DRO. 

Here, we focus on the ambiguity set \(\scrP=B_{\delta}(P)\) given by a ball centered at the reference distribution \(P\) with radius \(\delta\) in the Wasserstein distance, and refer to \citet{GK22} for a discussion of many advantages of this optimal-transport induced distance compared to other, e.g., divergence based, metrics. Importantly for capturing data perturbations, measures with different supports can be close in Wasserstein distance, see \citet{SND18}. Traditional \emph{pointwise} adversarial training can be seen as a special case of Wasserstein DRO (W-DRO) problem, \citet{SJ17}.
More recently, \citet{BLT+22Unified} unified various classical adversarial training methods, such as PGD-AT, TRADES, and MART, under the W-DRO framework. We also mention \citet{volpi2018generalizing} who used W-DRO to improve network performance on unseen data distributions.  

W-DRO, while very compelling, comes at a cost of an optimization over the space of probability measures, often intractable and/or computationally prohibitively expensive. To address this, one can use convex duality to rewrite \eqref{eqn-dro}, changing the $\sup$ to a univariate $\inf$ featuring a transform of $f_\theta$, see \citet{MK18} for the data-driven case, \citet{BM19,BDT20,GK22} for general probability measures and \citet{HHLD22} for a further application with coresets. Another approach, pioneered in \citet{BDOW21}, considers the first order approximation to the original W-DRO problem in the size of the uncertainty radius $\delta$. It was recently employed in \citet{BHJO23Wasserstein} to devise distributional AAs, and we use it here to construct novel robust training methods. 

\paragraph{Fine-tuning.} \label{subsec:finetuning-litreview}
Fine-tuning, in which an already trained model undergoes a limited additional training, often plays an important role in ML applications. 
In particular, a widely adopted strategy in transfer learning is to fine-tune only the final layers of a large pre-trained model, leaving earlier layers frozen. This approach is used not only in robustness tasks, but also in a broad range of scenarios such as domain adaptation \citep{DBLP:conf/cvpr/TzengHSD17} and continual learning \citep{DBLP:journals/nn/ParisiKPKW19}.
Recent work has shown that fine-tuning can substantially improve both standard and adversarial robustness of large pre-trained models \citep{yosinski2014transferable,raghu2019transfusion}. 
By \emph{randomizing} (or re-initializing) the last layer while freezing previous layers, one effectively learns a new ``head'' on top of features that were trained for robustness \citep{kornblith2019better}. 
Such partial re-initialization can avoid catastrophic forgetting of already-acquired robust representations \citep{DBLP:conf/eccv/LiH16}, while still adapting to new threats, including distributional attacks considered in this paper. 
Empirical results suggest that this strategy promotes better generalization in low-data regimes and helps preserve robust features, as noise or domain shifts are mainly absorbed by the newly learned final layer \citep{DBLP:conf/acl/RuderH18}. Furthermore, combining randomization of the last layer with additional regularization (for instance, adversarial weight perturbations or loss smoothing) can reduce overfitting and stabilize fine-tuning \citep{SWMJ20}. 
These observations align with the methods introduced in this paper, where we selectively re-initialize the final layer, or apply minimal random perturbations to all layers, to enhance distributional robustness while preserving core capabilities learned through previous training.