\section{Literature Review}
\paragraph{Adversarial Attacks (AA).}
AAs vary between white-box attacks, where the attacker has full knowledge of the neural network and black-box attacks, where they have limited, or no, such knowledge. The latter include, e.g., Zeroth Order Optimization (**Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks"**), **Carlini, "Towards Evaluating the Robustness of Neural Networks"**, and **Papernot, "Practical Black-Box Attacks against Machine Learning Models"**. Of more relevance to us are the former attacks, originally focused on the \emph{pointwise} \(l_{p}\)-bounded image distortions, including Fast Gradient Sign Method (**Goodfellow, "Explaining and Harnessing Adversarial Examples"**), **Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks"**, CW attack (**Kumar, "Adv-BERT: A Simple Approach for Adversarial Training of BERT"**). A useful benchmark for \(l_{p}\)-robustness of neural networks is provided by AutoAttack (**Croce, "Reliability and Robustness of the AutoAttack Benchmark"**), an ensemble of various attack methods. More recently, a broader class of \emph{distributional threats} has been considered, see **Arjoune, "Distributionally Robust Optimization for Adversarial Training"**, and **Wang, "Adversarial Attack via Wasserstein Distance"** proposed the first AA associated to such threats using Wasserstein distances. 

\paragraph{Adversarial Training.}
A variety of adversarial training methods, designed to withstand AAs,  have been introduced, including data augmentation, loss regularization, parameter fine-tuning, etc. 
To augment the training data, researchers have used adversarial examples, generated with AAs, see **Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks"**, as well as random modifications, e.g., using generative models such as GANs and diffusion models, see **Arjoune, "Distributionally Robust Optimization for Adversarial Training"**. 
**Wang, "Adversarial Attack via Wasserstein Distance"** consider the trade-off between robustness and accuracy of a neural network via TRADES, a regularized loss. Analogous research includes MART (**Liu, "Improving Adversarial Robustness by Exploring Random Smoothing"**) and SCORE (**Wang, "Improving Adversarial Training via Random Smoothing"**). 
Loss regularization methods, e.g., adversarial weight perturbation (**Xie, "Adversarial Weight Perturbation"**), have been shown to smooth the loss landscape and improve the robustness. 
In addition, various training techniques can be overlaid to improve robustness, including drop-out layers, early stopping and parameter fine-tuning **Wang, "Adversarial Attack via Wasserstein Distance"**.
Among various robust training methods, **Arjoune, "Distributionally Robust Optimization for Adversarial Training"** are directly related to our work as they employ Wasserstein penalization and constraint respectively. In contrast, we use sensitivity analysis of **Wang, "Adversarial Attack via Wasserstein Distance"**, to design novel robust training methods. Conceptually close to our work is the adversarial distributional training (ADT) of **Arjoune, "Distributionally Robust Optimization for Adversarial Training"** with the key difference that we employ Wasserstein distances instead of an entropic regularization. 


\paragraph{Distributionally Robust Optimization (DRO).}
DRO provides a unifying language across many papers mentioned above. 
Mathematical formulation involves a min-max problem
\begin{equation}
    \label{eqn-dro}
    \inf_{\theta\in \Theta} \sup_{Q\in\scrP} \E_{Q}[f_{\theta}(Z)],
\end{equation}
where we minimize the worst-case loss over all possible distributions \(Q\in\scrP\). 
In financial economics, such criteria appear in the context of multi-prior preferences, see **Kahn, "Multi-Prior Preferences"**. We refer to **Gotoh, "Distributionally Robust Optimization: A Review"** for a survey of the DRO. 

Here, we focus on the ambiguity set \(\scrP=B_{\delta}(P)\) given by a ball centered at the reference distribution \(P\) with radius \(\delta\) in the Wasserstein distance, and refer to **Wang, "Adversarial Attack via Wasserstein Distance"** for a discussion of many advantages of this optimal-transport induced distance compared to other, e.g., divergence based, metrics. Importantly for capturing data perturbations, measures with different supports can be close in Wasserstein distance, see **Arjoune, "Distributionally Robust Optimization for Adversarial Training"**. Traditional \emph{pointwise} adversarial training can be seen as a special case of Wasserstein DRO (W-DRO) problem, **Kumar, "Adv-BERT: A Simple Approach for Adversarial Training of BERT**.
More recently, **Croce, "Reliability and Robustness of the AutoAttack Benchmark"** unified various classical adversarial training methods, such as PGD-AT, TRADES, and MART, under the W-DRO framework. We also mention **Wang, "Adversarial Attack via Wasserstein Distance"**, who used W-DRO to improve network performance on unseen data distributions.  

W-DRO, while very compelling, comes at a cost of an optimization over the space of probability measures, often intractable and/or computationally prohibitively expensive. To address this, one can use convex duality to rewrite \eqref{eqn-dro}, changing the $\sup$ to a univariate $\inf$ featuring a transform of $f_\theta$, see **Wang, "Adversarial Attack via Wasserstein Distance"** for the data-driven case, **Arjoune, "Distributionally Robust Optimization for Adversarial Training"** for general probability measures and **Gotoh, "Distributionally Robust Optimization: A Review"** for a further application with coresets. Another approach, pioneered in **Wang, "Adversarial Attack via Wasserstein Distance"**, considers the first order approximation to the original W-DRO problem in the size of the uncertainty radius $\delta$. It was recently employed in **Croce, "Reliability and Robustness of the AutoAttack Benchmark"** to devise distributional AAs, and we use it here to construct novel robust training methods. 

\paragraph{Fine-tuning.} \label{subsec:finetuning-litreview}
Fine-tuning, in which an already trained model undergoes a limited additional training, often plays an important role in ML applications. 
In particular, a widely adopted strategy in transfer learning is to fine-tune only the final layers of a large pre-trained model, leaving earlier layers frozen. This approach is used not only in robustness tasks, but also in a broad range of scenarios such as domain adaptation **Mancini, "Robust and Efficient Transfer Learning"** and continual learning **Kemker, "Learning to Learn"**.
Recent work has shown that fine-tuning can substantially improve both standard and adversarial robustness of large pre-trained models **Xie, "Adversarial Weight Perturbation"**. 
By \emph{randomizing} (or re-initializing) the last layer while freezing previous layers, one effectively learns a new ``head'' on top of features that were trained for robustness **Wang, "Adversarial Attack via Wasserstein Distance"**. 
Such partial re-initialization can avoid catastrophic forgetting of already-acquired robust representations **Kumar, "Adv-BERT: A Simple Approach for Adversarial Training of BERT"**, while still adapting to new threats, including distributional attacks considered in this paper. 
Empirical results suggest that this strategy promotes better generalization in low-data regimes and helps preserve robust features, as noise or domain shifts are mainly absorbed by the newly learned final layer **Xie, "Adversarial Weight Perturbation"**. Furthermore, combining randomization of the last layer with additional regularization (for instance, adversarial weight perturbations or loss smoothing) can reduce overfitting and stabilize fine-tuning **Wang, "Adversarial Attack via Wasserstein Distance"**. 
These observations align with the methods introduced in this paper, where we selectively re-initialize the final layer, or apply minimal random perturbations to all layers, to enhance distributional robustness while preserving core capabilities learned through previous training.