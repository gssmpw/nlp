\section{Related Work}
\subsection{Semi-supervised Learning}
Current semi-supervised learning methods~\cite{van2020survey} can generally be divided into two categories: consistency regularization~\cite{laine2016temporal,ouali2020semi} and pseudo-labeling.~\cite{chen2021semi,lee2013pseudo,qiao2018deep}.Pseudo-labeling refers to using the model's predictions to generate labels for unlabeled samples, treating these predictions as approximate "ground truth" to expand the training dataset and further train the model. Consistency regularization enforces the modelâ€™s outputs to be consistent for the inputs under different perturbations. In addition, some studies~\cite{lu2023mutually,lu2023uncertainty} have combined the aforementioned methods and applied them to various tasks, showing superior performance.

\subsection{Semi-Supervised Medical Image Segmentation}
Semantic segmentation is the classification of every pixel in an image~\cite{hao2020brief,mo2022review}. It is a dense prediction task that requires substantial data and meticulous manual annotation in the training phase. Since semi-supervised methods can use unlabeled data to improve the performance of models, many semi-supervised methods have been applied to medical image segmentation in recent years. For example, SASSNet~\cite{li2020shape} introduces an edge distance function, calculating the signed distance from any voxel to its nearest natural boundary, and constructing a shape-aware semi-supervised semantic segmentation network. DTC~\cite{luo2021semi} proposes a dual-task consistency framework, introducing dual-task consistency regularization between segmentation maps derived from level sets for labeled and unlabeled data and directly predicted segmentation maps. UA-MT~\cite{yu2019uncertainty}, by incorporating uncertainty-aware consistency loss, guided by the estimated uncertainty, unreliable predictions are filtered out during the calculation of consistency loss, retaining only reliable predictions. However, removing unreliable predictions may result in the model ignoring features that are important but not easily detected, as well as meaning that they are underutilized, which may lead to loss of information.



\subsection{Uncertainty Estimation}
In semi-supervised learning, uncertainty estimates~\cite{NIPS2017_2650d608,shi2021inconsistency,mehrtash2020confidence} can be used to assess the quality of model predictions. Methods for estimating uncertainty mainly include: (1) Using the information entropy of the predicted probability distribution to measure uncertainty, the higher the entropy value, the greater the uncertainty. (2) Utilizing the bias of multiple prediction results of the same inputs under different perturbations to measure uncertainty, the higher the bias, the higher the uncertainty of the model's prediction of the input ~\cite{yu2019uncertainty}. (3) Calculate the variance of different prediction results for the same input to measure the uncertainty, the larger the variance, the higher the uncertainty of the model's prediction for that input~\cite{zheng2021rectifying}. Since entropy is calculated based on the entire probability distribution, it is insensitive to fluctuations in individual predictions, which means that the entropy can reflect the overall uncertainty of the model more stably, rather than being overly affected by individual anomalous predictions. On the other hand, calculating the entropy value only requires the already obtained predictive probability distributions without additional model inference or complex calculations. Therefore, in our approach, we perform uncertainty estimation utilizing the information entropy of the predicted probability distribution.