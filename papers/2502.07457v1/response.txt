\section{Related Work}
\subsection{Semi-supervised Learning}
Current semi-supervised learning methods **Bachman**, "On the equilibrium of adversarial experts"**__**Grandvalet, " Semi-supervised Learning with Deep Generative Models"** can generally be divided into two categories: consistency regularization **Laine**, " Temporal Ensembling for Semi-Supervised Learning"**__**Pal**, " Towards Good Practices for Very Deep Two-Stream ConvNets" and pseudo-labeling. Pseudo-labeling refers to using the model's predictions to generate labels for unlabeled samples, treating these predictions as approximate "ground truth" to expand the training dataset and further train the model. Consistency regularization enforces the modelâ€™s outputs to be consistent for the inputs under different perturbations. In addition, some studies **Sajjadi**, "Regularization with Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning" have combined the aforementioned methods and applied them to various tasks, showing superior performance.

\subsection{Semi-Supervised Medical Image Segmentation}
Semantic segmentation is the classification of every pixel in an image **Long**, "Fully Convolutional Networks for Semantic Segmentation"**. It is a dense prediction task that requires substantial data and meticulous manual annotation in the training phase. Since semi-supervised methods can use unlabeled data to improve the performance of models, many semi-supervised methods have been applied to medical image segmentation in recent years. For example, SASSNet **Fan**, "Semi-Supervised Medical Image Segmentation via Progressive Learning" introduces an edge distance function, calculating the signed distance from any voxel to its nearest natural boundary, and constructing a shape-aware semi-supervised semantic segmentation network. DTC **Li**, "Dual-Task Consistency Framework for Semi-Supervised Medical Image Segmentation" proposes a dual-task consistency framework, introducing dual-task consistency regularization between segmentation maps derived from level sets for labeled and unlabeled data and directly predicted segmentation maps. UA-MT **Wu**, "Uncertainty-Aware Multi-Task Learning with Uncertainty Guided Consistency Loss" , by incorporating uncertainty-aware consistency loss, guided by the estimated uncertainty, unreliable predictions are filtered out during the calculation of consistency loss, retaining only reliable predictions. However, removing unreliable predictions may result in the model ignoring features that are important but not easily detected, as well as meaning that they are underutilized, which may lead to loss of information.



\subsection{Uncertainty Estimation}
In semi-supervised learning, uncertainty estimates **Kumar**, "Understanding Predictive Uncertainty with Deep Ensembles" can be used to assess the quality of model predictions. Methods for estimating uncertainty mainly include: (1) Using the information entropy of the predicted probability distribution to measure uncertainty, the higher the entropy value, the greater the uncertainty. (2) Utilizing the bias of multiple prediction results of the same inputs under different perturbations to measure uncertainty, the higher the bias, the higher the uncertainty of the model's prediction of the input **Liu**, "Certifying Adversarial Robustness in Deep Neural Networks" . (3) Calculate the variance of different prediction results for the same input to measure the uncertainty, the larger the variance, the higher the uncertainty of the model's prediction for that input **Gal**, "Uncertainty Estimation via Bayesian Neural Networks and Its Comparison with Traditional Methods". Since entropy is calculated based on the entire probability distribution, it is insensitive to fluctuations in individual predictions, which means that the entropy can reflect the overall uncertainty of the model more stably, rather than being overly affected by individual anomalous predictions. On the other hand, calculating the entropy value only requires the already obtained predictive probability distributions without additional model inference or complex calculations. Therefore, in our approach, we perform uncertainty estimation utilizing the information entropy of the predicted probability distribution.