\section{Related Works}
% \paragraph{On-device Models.} 
% \paragraph{Small Language Models.} Small Language Models (SLMs) are designed for resource-efficient deployment on devices like desktops, smartphones, and wearables.
% Specifically, most SLMs are in the Transformer-based architectures, like Phi-3-mini~\cite{abdin2024phi}, TinyLlama~\cite{zhang2024tinyllama}, MobileLLM~\cite{liu2024mobilellm}, and Qwen-1.5B~\cite{bai2023qwen}, LiteLLaMa-460M, OPT-125M~\cite{zhang2022opt}, BLOOMZ~(560M, 1.1B, 1.7B, 3B)~\cite{le2023bloom}, SmolLM~(135M, 360M, 1.7B)~\cite{allal2024SmolLM}, OLMo~(1B)~\cite{Groeneveld2023OLMo}, OLMoE~(1B)~\cite{muennighoff2024olmoeopenmixtureofexpertslanguage}, MobiLlama~(0.5B, 1B)~\cite{thawakar2024mobillama}, MobileLLaMA~(1.4B, 2.7B)~\cite{chu2024mobilevlm}, OpenLLaMA~(3B)~\cite{openlm2023openllama}.
% These models are designed with lightweight architectures to operate effectively within the constraints of mobile devices and edge hardware.

% Recurrent Neural Networks (RNNs), like RWKV~(1B, 3B, 7B)~\cite{peng2023rwkv}, Mamba~(1.4B, 6.9B)~\cite{dao2024transformers}, and RecurrentGemma-2B~\cite{griffin2024recurrentgemma}, can provide promising solutions for on-device inference in resource-constrained environments. 
% These models leverage the recurrent nature of RNNs to process sequential data efficiently without requiring KV cache, which is suitable for resource-constrained on edge devices. 
% Specifically, RWKV introduces a hybrid RNN-Transformer backbone to capture long-term dependencies while maintaining computational efficiency. 
% Similarly, Mamba and RecurrentGemma design recurrent layers for low-power consumption and high throughput inference, which can significantly reduce memory and computational requirements, fostering low-latency applications directly on devices.

% visual language models.
% VLMs: MiniCPM-V \cite{yao2024minicpm}, MobileVLM\cite{chu2023mobilevlm}, MobileVLM-V2 \cite{chu2024mobilevlm}, 

% These models aim to offer practical machine intelligence without requiring significant computational resources. The vision behind SLMs is to democratize AI by making it universally accessible and affordable.



% 1.58 bits LLM \cite{ma2024era},  Survey \cite{chen2024llm, lu2024small}, Benchmark \cite{murthy2024mobileaibench}.
% Text-to-Image Diffusion Models: SnapFusion \cite{li2024snapfusion}, BitsFusion \cite{sui2024bitsfusion}, 
% Text-to-Video Diffusion Models: SnapGen-V \cite{wu2024snapgen}

% https://github.com/NexaAI/Awesome-LLMs-on-device