@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@article{mahaut2024factual,
  title={Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators},
  author={Mahaut, Mat{\'e}o and Aina, Laura and Czarnowska, Paula and Hardalov, Momchil and M{\"u}ller, Thomas and M{\`a}rquez, Llu{\'\i}s},
  journal={arXiv preprint arXiv:2406.13415},
  year={2024}
}

@article{yao2024minicpmvgpt4vlevelmllm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and Chen, Qianyu and Zhou, Huarong and Zou, Zhensheng and Zhang, Haoye and Hu, Shengding and Zheng, Zhi and Zhou, Jie and Cai, Jie and Han, Xu and Zeng, Guoyang and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024},
  url={https://arxiv.org/abs/2408.01800},
}

@article{wang2024chain,
  title={Chain-of-thought reasoning without prompting},
  author={Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.10200},
  year={2024}
}

@misc{thawakar2024mobillama,
      title={MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT}, 
      author={Omkar Thawakar and Ashmal Vayani and Salman Khan and Hisham Cholakkal and Rao Muhammad Anwer and Michael Felsberg and Timothy Baldwin and Eric P. Xing and Fahad Shahbaz Khan},
      year={2024},
      eprint={2402.16840},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
} 

@misc{muennighoff2024olmoeopenmixtureofexpertslanguage,
      title={OLMoE: Open Mixture-of-Experts Language Models}, 
      author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
      year={2024},
      eprint={2409.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02060}, 
}

@article{Groeneveld2023OLMo,
  title={OLMo: Accelerating the Science of Language Models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  journal={Preprint},
  year={2024}
}

@misc{allal2024SmolLM,
      title={SmolLM - blazingly fast and remarkably powerful}, 
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Leandro von Werra and Thomas Wolf},
      year={2024},
}

@article{le2023bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@misc{griffin2024recurrentgemma,
  title={Recurrentgemma: Moving past transformers for efficient open language models},
  author={Griffin, RLHF and Teams, Gemma},
  year={2024},
  publisher={ArXiv}
}

@article{peng2023rwkv,
  title={Rwkv: Reinventing rnns for the transformer era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@article{dao2024transformers,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{zhou2023mini,
  title={Mini-Giants:" Small" Language Models and Open Source Win-Win},
  author={Zhou, Zhengping and Li, Lezhi and Chen, Xinxi and Li, Andy},
  journal={arXiv preprint arXiv:2307.08189},
  year={2023}
}

@article{lu2024small,
  title={Small language models: Survey, measurements, and insights},
  author={Lu, Zhenyan and Li, Xiang and Cai, Dongqi and Yi, Rongjie and Liu, Fangming and Zhang, Xiwen and Lane, Nicholas D and Xu, Mengwei},
  journal={arXiv preprint arXiv:2409.15790},
  year={2024}
}

@article{chuang2024learning,
  title={Learning to Route with Confidence Tokens},
  author={Chuang, Yu-Neng and Zhou, Helen and Sarma, Prathusha Kameswara and Gopalan, Parikshit and Boccio, John and Bolouki, Sara and Hu, Xia},
  journal={arXiv preprint arXiv:2410.13284},
  year={2024}
}

@misc{phi2,
    title = {Phi-2: The surprising power of small language models},
    author = {Javaheripi, Mojan and Bubeck, SÃ©bastien},
    month = {December},
    year = {2023}
}

@article{chen2024role,
  title={What is the role of small models in the llm era: A survey},
  author={Chen, Lihu and Varoquaux, Ga{\"e}l},
  journal={arXiv preprint arXiv:2409.06857},
  year={2024}
}

@article{liu2024dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}

@article{brier1950verification,
  title={Verification of forecasts expressed in terms of probability},
  author={Brier, Glenn W},
  journal={Monthly weather review},
  volume={78},
  number={1},
  pages={1--3},
  year={1950}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@inproceedings{blasiok2023unifying,
  title={A unifying theory of distance from calibration},
  author={B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
  booktitle={Proceedings of the 55th Annual ACM Symposium on Theory of Computing},
  pages={1727--1740},
  year={2023}
}

@article{neeman2022disentqa,
  title={Disentqa: Disentangling parametric and contextual knowledge with counterfactual question answering},
  author={Neeman, Ella and Aharoni, Roee and Honovich, Or and Choshen, Leshem and Szpektor, Idan and Abend, Omri},
  journal={arXiv preprint arXiv:2211.05655},
  year={2022}
}

@article{li2022large,
  title={Large language models with controllable working memory},
  author={Li, Daliang and Rawat, Ankit Singh and Zaheer, Manzil and Wang, Xin and Lukasik, Michal and Veit, Andreas and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2211.05110},
  year={2022}
}

@article{liu2024uncertainty,
  title={Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach},
  author={Liu, Linyu and Pan, Yu and Li, Xiaocheng and Chen, Guanting},
  journal={arXiv preprint arXiv:2404.15993},
  year={2024}
}

@article{huang2023look,
  title={Look before you leap: An exploratory study of uncertainty measurement for large language models},
  author={Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Zhao, Shengming and Chen, Huaming and Juefei-Xu, Felix and Ma, Lei},
  journal={arXiv preprint arXiv:2307.10236},
  year={2023}
}

@article{zhou2023navigating,
  title={Navigating the grey area: How expressions of uncertainty and overconfidence affect language models},
  author={Zhou, Kaitlyn and Jurafsky, Dan and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.13439},
  year={2023}
}

@inproceedings{
    asai2024selfrag,
    author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
    title={Self-{RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=hSyW5go0v8}
}

@article{hendryckstest2021,
      title={Measuring Massive Multitask Language Understanding},
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      journal={Proceedings of the International Conference on Learning Representations (ICLR)},
      year={2021}
}

@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}

@inproceedings{wightman2023strength,
  title={Strength in numbers: Estimating confidence of large language models by prompt agreement},
  author={Wightman, Gwenyth Portillo and Delucia, Alexandra and Dredze, Mark},
  booktitle={Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)},
  pages={326--362},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{xiong2023can,
  title={Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  journal={arXiv preprint arXiv:2306.13063},
  year={2023}
}

@article{hu2024routerbench,
  title={ROUTERBENCH: A Benchmark for Multi-LLM Routing System},
  author={Hu, Qitian Jason and Bieker, Jacob and Li, Xiuyu and Jiang, Nan and Keigwin, Benjamin and Ranganath, Gaurav and Keutzer, Kurt and Upadhyay, Shriyash Kaustubh},
  journal={arXiv preprint arXiv:2403.12031},
  year={2024}
}

@article{yona2024can,
  title={Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?},
  author={Yona, Gal and Aharoni, Roee and Geva, Mor},
  journal={arXiv preprint arXiv:2405.16908},
  year={2024}
}

@article{zhao2024eagle,
  title={Eagle: Efficient training-free router for multi-llm inference},
  author={Zhao, Zesen and Jin, Shuowei and Mao, Z Morley},
  journal={arXiv preprint arXiv:2409.15518},
  year={2024}
}

@article{hendrycks2021ethics,
  title={Aligning AI With Shared Human Values},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}



@article{detommaso2024multicalibration,
  title={Multicalibration for confidence scoring in LLMs},
  author={Detommaso, Gianluca and Bertran, Martin and Fogliato, Riccardo and Roth, Aaron},
  journal={arXiv preprint arXiv:2404.04689},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{turpin2024language,
  title={Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yadkori2024believe,
  title={To Believe or Not to Believe Your LLM},
  author={Yadkori, Yasin Abbasi and Kuzborskij, Ilja and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  journal={arXiv preprint arXiv:2406.02543},
  year={2024}
}

@inproceedings{tian2023just,
  title={Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5433--5442},
  year={2023}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@article{stripelis2024polyrouter,
  title={PolyRouter: A Multi-LLM Querying System},
  author={Stripelis, Dimitris and Hu, Zijian and Zhang, Jipeng and Xu, Zhaozhuo and Shah, Alay and Jin, Han and Yao, Yuhang and Avestimehr, Salman and He, Chaoyang},
  journal={arXiv preprint arXiv:2408.12320},
  year={2024}
}

@article{huang2024context,
  title={Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models},
  author={Huang, Jerry and Parthasarathi, Prasanna and Rezagholizadeh, Mehdi and Chandar, Sarath},
  journal={arXiv preprint arXiv:2408.08470},
  year={2024}
}

@inproceedings{dinghybrid,
  title={Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing},
  author={Ding, Dujian and Mallick, Ankur and Wang, Chi and Sim, Robert and Mukherjee, Subhabrata and R{\"u}hle, Victor and Lakshmanan, Laks VS and Awadallah, Ahmed Hassan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{yue2023large,
  title={Large language model cascades with mixture of thoughts representations for cost-efficient reasoning},
  author={Yue, Murong and Zhao, Jie and Zhang, Min and Du, Liang and Yao, Ziyu},
  journal={arXiv preprint arXiv:2310.03094},
  year={2023}
}

@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}

@article{do2023hyperrouter,
  title={HyperRouter: Towards efficient training and inference of sparse mixture of experts},
  author={Do, Giang and Le, Khiem and Pham, Quang and Nguyen, Trungtin and Doan, Thanh-Nam and Nguyen, Bint T and Liu, Chenghao and Ramasamy, Savitha and Li, Xiaoli and Hoi, Steven},
  journal={arXiv preprint arXiv:2312.07035},
  year={2023}
}

@article{ong2024routellm,
  title={Routellm: Learning to route llms with preference data},
  author={Ong, Isaac and Almahairi, Amjad and Wu, Vincent and Chiang, Wei-Lin and Wu, Tianhao and Gonzalez, Joseph E and Kadous, M Waleed and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.18665},
  year={2024}
}

@article{srivatsa2024harnessing,
  title={Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing},
  author={Srivatsa, KV and Maurya, Kaushal Kumar and Kochmar, Ekaterina},
  journal={arXiv preprint arXiv:2405.00467},
  year={2024}
}

@article{tang2024science,
  title={The science of detecting llm-generated text},
  author={Tang, Ruixiang and Chuang, Yu-Neng and Hu, Xia},
  journal={Communications of the ACM},
  volume={67},
  number={4},
  pages={50--59},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{yang2024harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  journal={ACM Transactions on Knowledge Discovery from Data},
  volume={18},
  number={6},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY}
}

@article{wu2023autogen,
  title={Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}

@article{tang2023does,
  title={Does synthetic data generation of llms help clinical text mining?},
  author={Tang, Ruixiang and Han, Xiaotian and Jiang, Xiaoqian and Hu, Xia},
  journal={arXiv preprint arXiv:2303.04360},
  year={2023}
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{sai2023self,
  author       = {Akari Asai and
                  Zeqiu Wu and
                  Yizhong Wang and
                  Avirup Sil and
                  Hannaneh Hajishirzi},
  title        = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=hSyW5go0v8},
  timestamp    = {Mon, 29 Jul 2024 17:17:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/AsaiWWSH24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2023accelerating,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023}
}

@article{li2023classification,
  title={From classification to generation: Insights into crosslingual retrieval augmented icl},
  author={Li, Xiaoqian and Nie, Ercong and Liang, Sheng},
  journal={arXiv preprint arXiv:2311.06595},
  year={2023}
}

@article{kuhn2023semantic,
  title={Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation},
  author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  journal={arXiv preprint arXiv:2302.09664},
  year={2023}
}

@inproceedings{cortes2016learning,
  title={Learning with rejection},
  author={Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
  booktitle={Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings 27},
  pages={67--82},
  year={2016},
  organization={Springer}
}

@inproceedings{mozannar2020consistent,
  title={Consistent estimators for learning to defer to an expert},
  author={Mozannar, Hussein and Sontag, David},
  booktitle={International conference on machine learning},
  pages={7076--7087},
  year={2020},
  organization={PMLR}
}

@article{chen2023learning,
  title={Learning to make adherence-aware advice},
  author={Chen, Guanting and Li, Xiaocheng and Sun, Chunlin and Wang, Hanzhao},
  journal={arXiv preprint arXiv:2310.00817},
  year={2023}
}

@article{mao2024two,
  title={Two-stage learning to defer with multiple experts},
  author={Mao, Anqi and Mohri, Christopher and Mohri, Mehryar and Zhong, Yutao},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{mao2023structured,
  title={Structured prediction with stronger consistency guarantees},
  author={Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46903--46937},
  year={2023}
}

@article{hayou2024lora,
  title={Lora+: Efficient low rank adaptation of large models},
  author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  journal={arXiv preprint arXiv:2402.12354},
  year={2024}
}

@article{zhang2021knowing,
  title={Knowing more about questions can help: Improving calibration in question answering},
  author={Zhang, Shujian and Gong, Chengyue and Choi, Eunsol},
  journal={arXiv preprint arXiv:2106.01494},
  year={2021}
}

@inproceedings{chenlearning,
  title={Learning to Make Adherence-aware Advice},
  author={Chen, Guanting and Li, Xiaocheng and Sun, Chunlin and Wang, Hanzhao},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@article{kamath2020selective,
  title={Selective question answering under domain shift},
  author={Kamath, Amita and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:2006.09462},
  year={2020}
}

@article{li2023no,
  title={When no-rejection learning is optimal for regression with rejection},
  author={Li, Xiaocheng and Liu, Shang and Sun, Chunlin and Wang, Hanzhao},
  journal={arXiv preprint arXiv:2307.02932},
  year={2023}
}

@inproceedings{mohrilearning,
  title={Learning to Reject with a Fixed Predictor: Application to Decontextualization},
  author={Mohri, Christopher and Andor, Daniel and Choi, Eunsol and Collins, Michael and Mao, Anqi and Zhong, Yutao},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@inproceedings{duan2024shifting,
  title={Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models},
  author={Duan, Jinhao and Cheng, Hao and Wang, Shiqi and Zavalny, Alex and Wang, Chenan and Xu, Renjing and Kailkhura, Bhavya and Xu, Kaidi},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5050--5063},
  year={2024}
}

@article{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and LÃ©lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and TimothÃ©e Lacroix and William El Sayed},
    year={2023},
    journal={arXiv},
}

@inproceedings{roy2015solving,
  title={Solving General Arithmetic Word Problems},
  author={Roy, Subhro and Roth, Dan},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  year={2015},
  organization={Association for Computational Linguistics}
}

@misc{langchain_harrison2022, title={Langchain-ai/Langchain: build context-aware reasoning applications}, url={https://github.com/langchain-ai/langchain}, journal={LangChain}, author={Harrison, Chase}, year={2022}, month={Oct}} 

@inproceedings{krishna2021generating,
  title={Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques},
  author={Krishna, Kundan and Khosla, Sopan and Bigham, Jeffrey P and Lipton, Zachary C},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4958--4972},
  year={2021}
}

@article{schmucker2023ruffle,
  title={Ruffle\&riley: Towards the automated induction of conversational tutoring systems},
  author={Schmucker, Robin and Xia, Meng and Azaria, Amos and Mitchell, Tom},
  journal={arXiv preprint arXiv:2310.01420},
  year={2023}
}

@inproceedings{xiao2023evaluating,
  title={Evaluating reading comprehension exercises generated by LLMs: A showcase of ChatGPT in education applications},
  author={Xiao, Changrong and Xu, Sean Xin and Zhang, Kunpeng and Wang, Yufang and Xia, Lei},
  booktitle={Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
  pages={610--625},
  year={2023}
}

@misc{biswas2023chatgpt,
  title={ChatGPT and the future of medical writing},
  author={Biswas, Som},
  journal={Radiology},
  volume={307},
  number={2},
  pages={e223312},
  year={2023},
  publisher={Radiological Society of North America}
}

@misc{Databricks, url={https://www.databricks.com/solutions/accelerators/llms-customer-service-and-support}, journal={Databricks}} 

@article{kolasani2023optimizing,
  title={Optimizing natural language processing, large language models (LLMs) for efficient customer service, and hyper-personalization to enable sustainable growth and revenue},
  author={Kolasani, Saydulu},
  journal={Transactions on Latest Trends in Artificial Intelligence},
  volume={4},
  number={4},
  year={2023}
}

@article{wu2024snapgen,
  title={SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device},
  author={Wu, Yushu and Zhang, Zhixing and Li, Yanyu and Xu, Yanwu and Kag, Anil and Sui, Yang and Coskun, Huseyin and Ma, Ke and Lebedev, Aleksei and Hu, Ju and others},
  journal={arXiv preprint arXiv:2412.10494},
  year={2024}
}

@article{li2024snapfusion,
  title={Snapfusion: Text-to-image diffusion model on mobile devices within two seconds},
  author={Li, Yanyu and Wang, Huan and Jin, Qing and Hu, Ju and Chemerys, Pavlo and Fu, Yun and Wang, Yanzhi and Tulyakov, Sergey and Ren, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{sui2024bitsfusion,
  title={BitsFusion: 1.99 bits Weight Quantization of Diffusion Model},
  author={Sui, Yang and Li, Yanyu and Kag, Anil and Idelbayev, Yerlan and Cao, Junli and Hu, Ju and Sagar, Dhritiman and Yuan, Bo and Tulyakov, Sergey and Ren, Jian},
  journal={arXiv preprint arXiv:2406.04333},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{liu2024mobilellm,
  title={Mobilellm: Optimizing sub-billion parameter language models for on-device use cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
  journal={arXiv preprint arXiv:2402.14905},
  year={2024}
}

@article{yao2024minicpm,
  title={Minicpm-v: A gpt-4v level mllm on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@article{chen2024llm,
  title={Llm for mobile: An initial roadmap},
  author={Chen, Daihang and Liu, Yonghui and Zhou, Mingyi and Zhao, Yanjie and Wang, Haoyu and Wang, Shuai and Chen, Xiao and Bissyand{\'e}, Tegawend{\'e} F and Klein, Jacques and Li, Li},
  journal={ACM Transactions on Software Engineering and Methodology},
  year={2024},
  publisher={ACM New York, NY}
}

@article{murthy2024mobileaibench,
  title={MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases},
  author={Murthy, Rithesh and Yang, Liangwei and Tan, Juntao and Awalgaonkar, Tulika Manoj and Zhou, Yilun and Heinecke, Shelby and Desai, Sachin and Wu, Jason and Xu, Ran and Tan, Sarah and others},
  journal={arXiv preprint arXiv:2406.10290},
  year={2024}
}

@article{chu2023mobilevlm,
  title={Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices},
  author={Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
  journal={arXiv preprint arXiv:2312.16886},
  year={2023}
}

@article{chu2024mobilevlm,
  title={Mobilevlm v2: Faster and stronger baseline for vision language model},
  author={Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
  journal={arXiv preprint arXiv:2402.03766},
  year={2024}
}

@article{zhang2024tinyllama,
  title={Tinyllama: An open-source small language model},
  author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  journal={arXiv preprint arXiv:2401.02385},
  year={2024}
}

@article{ma2024era,
  title={The era of 1-bit llms: All large language models are in 1.58 bits},
  author={Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  journal={arXiv preprint arXiv:2402.17764},
  year={2024}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}


@inproceedings{fadeeva2023lm,
  title={LM-Polygraph: Uncertainty Estimation for Language Models},
  author={Fadeeva, Ekaterina and Vashurin, Roman and Tsvigun, Akim and Vazhentsev, Artem and Petrakov, Sergey and Fedyanin, Kirill and Vasilev, Daniil and Goncharova, Elizaveta and Panchenko, Alexander and Panov, Maxim and others},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={446--461},
  year={2023}
}

@article{lin2023generating,
  title={Generating with confidence: Uncertainty quantification for black-box large language models},
  author={Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
  journal={arXiv preprint arXiv:2305.19187},
  year={2023}
}

@article{cohen2024don,
  title={I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token},
  author={Cohen, Roi and Dobler, Konstantin and Biran, Eden and de Melo, Gerard},
  journal={arXiv preprint arXiv:2412.06676},
  year={2024}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@inproceedings{geng2024survey,
  title={A Survey of Confidence Estimation and Calibration in Large Language Models},
  author={Geng, Jiahui and Cai, Fengyu and Wang, Yuxia and Koeppl, Heinz and Nakov, Preslav and Gurevych, Iryna},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={6577--6595},
  year={2024}
}

@inproceedings{azaria2023internal,
  title={The Internal State of an LLM Knows When Itâs Lying},
  author={Azaria, Amos and Mitchell, Tom},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={967--976},
  year={2023}
}

@inproceedings{xiongcan,
  title={Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and LI, YIFEI and Fu, Jie and He, Junxian and Hooi, Bryan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}


@article{gal2016uncertainty,
  title={Uncertainty in deep learning},
  author={Gal, Yarin and others},
  year={2016},
  publisher={phd thesis, University of Cambridge}
}

@inproceedings{manakul2023selfcheckgpt,
  title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9004--9017},
  year={2023}
}

@inproceedings{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{mielke2022reducing,
  title={Reducing conversational agentsâ overconfidence through linguistic calibration},
  author={Mielke, Sabrina J and Szlam, Arthur and Dinan, Emily and Boureau, Y-Lan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={857--872},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â¦}
}

@article{lin2022teaching,
  title={Teaching models to express their uncertainty in words},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2205.14334},
  year={2022}
}

@inproceedings{dong2024survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Chang, Baobao and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={1107--1128},
  year={2024}
}

@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle={Advances in neural information processing systems},
  pages={24824--24837},
  year={2022}
}


@article{jiang2021can,
  title={How can we know when language models know? on the calibration of language models for question answering},
  author={Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={962--977},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â¦}
}


@article{fomicheva2020unsupervised,
  title={Unsupervised quality estimation for neural machine translation},
  author={Fomicheva, Marina and Sun, Shuo and Yankovskaya, Lisa and Blain, Fr{\'e}d{\'e}ric and Guzm{\'a}n, Francisco and Fishel, Mark and Aletras, Nikolaos and Chaudhary, Vishrav and Specia, Lucia},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={539--555},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â¦}
}

@inproceedings{vazhentsev2023efficient,
  title={Efficient out-of-domain detection for sequence to sequence models},
  author={Vazhentsev, Artem and Tsvigun, Akim and Vashurin, Roman and Petrakov, Sergey and Vasilev, Daniil and Panov, Maxim and Panchenko, Alexander and Shelmanov, Artem},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={1430--1454},
  year={2023}
}

@article{meta2024llama,
  title={Llama 3.2: Revolutionizing edge AI and vision with open, customizable models},
  author={Meta, AI},
  journal={Meta AI Blog. Retrieved December},
  volume={20},
  pages={2024},
  year={2024}
}

@article{pfeiffer2024h2o,
  title={H2o-danube3 technical report},
  author={Pfeiffer, Pascal and Singer, Philipp and Babakhin, Yauhen and Fodor, Gabor and Dhankhar, Nischay and Ambati, Sri Satish},
  journal={arXiv preprint arXiv:2407.09276},
  year={2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}

@misc{granite2024granite,
  title={Granite 3.0 Language Models},
  author={Granite Team, IBM},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={ACL},
  year={2017}
}

@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2924--2936},
  year={2019}
}

@article{reddy-etal-2019-coqa,
    title = "{C}o{QA}: A Conversational Question Answering Challenge",
    author = "Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    publisher = "MIT Press",
    pages = "249--266",
}

@inproceedings{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4149--4158",
}


@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{hendrycksmeasuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@article{roy2016solving,
  title={Solving general arithmetic word problems},
  author={Roy, Subhro and Roth, Dan},
  journal={arXiv preprint arXiv:1608.01413},
  year={2016}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}


@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@inproceedings{sap2019social,
  title={Social IQa: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4463--4473},
  year={2019}
}

@inproceedings{patel2021nlp,
  title={Are NLP Models really able to Solve Simple Math Word Problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2080--2094},
  year={2021}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{houdecomposing,
  title={Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling},
  author={Hou, Bairu and Liu, Yujian and Qian, Kaizhi and Andreas, Jacob and Chang, Shiyu and Zhang, Yang},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={46595--46623},
  year={2023}
}