\documentclass{article}

% [26 Jul 2024] Abdul Fatir Ansari: Adapted from the template for Score-based Methods Workshop from NeurIPS 2022

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading timeseries_workshop


% ready for submission
\usepackage[final]{timeseries_workshop}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{timeseries_workshop}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{timeseries_workshop}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{timeseries_workshop}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage[font=footnotesize,labelformat=simple]{subcaption}
\usepackage{float}
% \usepackage{caption}
% \usepackage{subfig}
\usepackage{wrapfig} 
\usepackage{fancyhdr}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\title{Masking the Gaps: An Imputation-Free Approach to Time Series Modeling with Missing Data}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{Abhilash Neog$^{1}$, Arka Daw$^{2}$, Sepideh Fatemi Khorasgani$^{1}$, Anuj Karpatne$^{1}$ \\
$^{1}$ Virginia Tech,
$^{2}$ Oak Ridge National Labs}


\begin{document}
\maketitle
\begin{abstract}
 A significant challenge in time-series (TS) modeling is the presence of missing values in real-world TS datasets. Traditional two-stage frameworks, involving imputation followed by modeling, suffer from two key drawbacks: (1) the propagation of imputation errors into subsequent TS modeling, (2) the trade-offs between imputation efficacy and imputation complexity. While one-stage approaches attempt to address these limitations, they often struggle with scalability or fully leveraging partially observed features. To this end, we propose a novel imputation-free approach for handling missing values in time series termed \textbf{Miss}ing Feature-aware \textbf{T}ime \textbf{S}eries \textbf{M}odeling (\textbf{MissTSM}) with two main innovations. \textit{First}, we develop a novel embedding scheme that treats every combination of time-step and feature (or channel) as a distinct token. \textit{Second}, we introduce a novel \textit{Missing Feature-Aware Attention (MFAA) Layer} to learn latent representations at every time-step based on partially observed features. We evaluate the effectiveness of MissTSM  in handling missing values over multiple benchmark datasets.
\end{abstract}

% \input{introduction}
\section{Introduction}

Multivariate time-series (TS) modeling is important in a number of real-world applications. However, a persistent challenge is the presence of missing values on arbitrary sets of features at varying time-steps, introducing ``gaps'' in the data that can impair the application of State-of-the-art (SOTA) models unless specific adaptations are made. A common approach for handling missing data is to use imputation methods \cite{ahn2022comparison, batista2002study, acuna2004treatment}.
% These methods fall into two broad categories: those that leverage cross-channel correlations \cite{batista2002study, acuna2004treatment} and those that exploit temporal dynamics \cite{box2015time}. 
Recent deep learning (DL)-based imputation techniques \cite{tashiro2021csdi, cini2021multivariate, liu2019naomi} can learn complex, nonlinear temporal dynamics which are difficult for simple imputation techniques (like interpolation). 
% However, their reliance on a single entangled representation \cite{woo2022cost} limits capturing multifaceted time-series features, which matrix factorization \cite{liu2022multivariate} based techniques help overcome by offering disentangled temporal representations. 
However all such frameworks rely on a two-stage process, imputation of missing values, followed by feeding the imputed time-series to a TS model. This introduces two critical challenges: \textit{first}, the propagation of imputation errors into subsequent TS modeling performance, and \textit{second}, the inherent trade-offs between imputation efficacy and imputation complexity. 

In this regard, several approaches have been proposed to model time-series with missing values, such as \cite{grud} embed time intervals between observations as additional auxiliary features to handle irregular sequences, but relies on recurrent networks which struggle with long-term dependencies. ODE-based methods \cite{neuralode, latentode} offer a continuous-time framework for irregular sampling but are computationally demanding and difficult to scale. Recent methods, like \cite{chronos} implicitly handle missing values via attention mask, or use an additional mask channel (\cite{timesfm}), but in a univariate scenario.
% The time-series model learned on imputed time-series can be only as good as the imputation error.
%  \begin{wrapfigure}{r}{0.49\textwidth} % Adjust 'r' for right, 'l' for left, and width as necessary
%     \centering
%     \includegraphics[width=0.49\textwidth]{figures/toc_figure.pdf} % Adjust width as needed
%     \caption{Comparing the traditional two-stage approach of time-series modeling with missing values with our single-stage MissTSM framework that does not require imputations.} 
%     \label{fig:toc}
%     \vspace{-1ex}
% \end{wrapfigure}

To address the above limitations, we ask the question: \emph{``can we circumvent the need for imputation by designing a DL framework that can directly model multivariate TS with missing values?''} To answer this question, we draw inspiration from the recent success of masked modeling approaches in domains including vision \cite{he2022masked} and language \cite{devlin2018bert} where \emph{``masked-attention''} operations embedded in Transformer blocks are effectively utilized to reconstruct data from partial observations. Based on this insight, we propose a novel \textbf{Miss}ing Feature-aware \textbf{T}ime \textbf{S}eries \textbf{M}odeling (\textbf{MissTSM}) Framework, which capitalizes on the information contained in partially observed features to perform downstream TS modeling tasks without explicitly imputing the missing values.
% derive meaningful latent representations through a novel application of masked-attention. 
 It uses two main innovations. \textit{First}, we develop a novel embedding scheme, termed \textit{Time-Feature Independent (TFI) Embedding}, which treats every combination of time-step and feature (or channel) as a distinct token, encoding them into a high-dimensional space. \textit{Second}, we introduce a novel \textit{Missing Feature-Aware Attention (MFAA) Layer} to learn latent representations at every time-step based on partially observed features. Additionally, we use the framework of Masked Auto-encoder (MAE) \cite{he2022masked} to perform self-supervised learning of latent representations, which can be re-used for downstream tasks such as forecasting and classification. 
To evaluate the ability of MissTSM  to model TS with missing values, we consider two synthetic masking techniques: missing completely at random (MCAR), and periodic masking, to simulate varying scenarios of missing values.  We show that MissTSM achieves consistently competitive performance as SOTA models on multiple benchmark datasets without using any imputation techniques.
% Our main contributions can be summarized as follows:
% First, we propose a novel embedding scheme named as Time-Feature Independent Embedding which considers each individual time-step and feature (or channel) as an independent token, and embeds them into a high-dimensional representation. Second, we propose a Missing Feature-Aware Attention (MFAA) Layer, where we first obtain \emph{attention scores} based on the partially observed features at a given time-step $t$, which can then be utilized to perform a weighted aggregation over the observed features space to obtain the latent representations. Additionally, we adapt a Masked Auto-encoder (MAE) framework for time-series data to perform training in a self-supervised fashion with the goal of learning robust representations. These representations can then be utilized for downstream tasks such as time-series forecasting and classification. To evaluate the performance of the MissTSM approach, we propose synthetic masking techniques to generate missing-values in benchmark datasets. \textcolor{red}{We empirically demonstrate that for ..... }
% Paragraph 3: Towards the development of a deep learning framework for time-series that can handle missing values. motivation from MAE. ..
% Contributions:
% "What we are doing using MFAA is model the cross-channel correlations!"
% Primary idea of the paper:
% \\
% \begin{enumerate}
%     \item Is the observed space of partially observed time-series data sufficient for time-series modeling? => this is our hypothesis and we prove our hypothesis through empirical evaluations
%     \item Now, the next question is, how do we utilize the available space of data? -> In this regard, we propose a Feature-point embedding approach along with Missing feature-aware attention to generate efficient feature representations at each time-step that can capture the correlation of the observed space of features
%     \item Through this, we show that, this approach allows us to have a one-shot modeling (or end-to-end) for time-series data, thus eliminating the need for any kind of imputation
% \end{enumerate}
% Motivation.
% \begin{enumerate}
%     \item Real-world time-series data suffer from missing values due to multiple reasons, sensor failures, human errors, etc. And the go-to method to deal with missing data has always been to perform some kind of imputation. However, this results in a two-stage process: imputation, then the actual time-series modeling (forecasting, classification, anomaly detection, etc.). The two-stage process has a significant trade-off.
%     \begin{enumerate}
%         \item  While simple/traditional imputation techniques, like bi-linear interpolation or spline interpolation work very fast, the efficiency of these techniques is very low, especially with complex time-series and larger missing data
%         \item This can be compensated with complex time-series imputation techniques like DeepMVI, BRITS, etc. which provides a significant performance boost over traditional interpolation techniques. However, these imputation techniques are not generalizable and hence, require training (SAITS and DeepMVI are deep learning models), which further adds to the time-complexity in this 2-stage pipeline.
%         \item Using complex imputation models also require a certain degree of hyper-parameter tuning that further adds to the computational time complexity
%     \end{enumerate}
%     \item Imputation error dictates the modeling errors. This creates a strong dependency between time-series modeling and the type of imputation technique used <plot-modeling-error-interpolation-error>
%     \item We have witnessed a huge surge in time-series models recently, with a shift towards Masked modeling for self-supervised learning in time-series. While these techniques achieve State-of-the-art results, they work with the underlying assumption of complete data with no missing values. They expect data to be complete and do not have any explicit handling of missing values.
%     \item Often people resort to zero-imputation (SimMTM fills in the masked values as zeros), but this often leads to sub-optimal performance <\href{https://openreview.net/pdf?id=BylsKkHYvH}{reference}>
%     \item Furthermore, this 2-stage pipeline has it's own challenges. Firstly, imputation techniques are not generalizable and does not necessarily preserve the original manifold of the time-series data. Secondly, it induces an extra imputation error besides the modeling error. For exmaple, if we consider the forecasting task we have,  
%     \[\text{\textit{Imputation\ error}} = \sum(iTS - gTS)^2\]
%     \[\text{\textit{Modeling\ error}} = \sum(y - \hat{y})^2\]
%     \[\text{\textit{Prediction\ error}} = \text{\textit{Imputation\ error}} + \text{\textit{Modeling\ error}}\]
%     \center{where, \small{$iTS = Imputed\ Time\ Series$, \\$gTS = Ground\ truth\ TS\ values$, \\ $\hat{y} = Ground\ truth\ forecast\ values$}}
% \end{enumerate}
% Contribution
% \begin{enumerate}
%     \item We propose an imputation-free approach for time-series modeling by regarding each time-feature combination as an independent token and transforming them into one multi-dimensional feature vector capturing the feature correlations.   
%     \item We integrate this idea with the masked modeling paradigm to perform implicit imputation by utilizing self-supervised learning's built-in imputation ability for time-series modeling
%     \item Our approach combines the 2-stage pipeline into an end-to-end training solution for partial time-series data, while also achieving state-of-the-art results on benchmark datasets and higher stability and robustness with increasing missing values.
%     \item \textbf{We show that the observed space in partially observed data can alone be utilized to generate efficient feature representations for time-series modeling}
% %     \item We reflect on the architecture of Transformer and refine that the competent capability of native Transformer components on multivariate time series is under-explored.
% %     \item We propose iTransformer that regards independent time series as tokens to capture multivariate correlations by self-attention and utilize layer normalization and feed-forward network modules to learn better series-global representations for time series forecasting.
% %     \item Experimentally, iTransformer achieves comprehensive state-of-the-art on real-world benchmarks. We extensively analyze the inverted modules and architecture choices, indicating a
% % promising direction for the future improvement of Transformer-based forecasters.
% \end{enumerate}

% \input{related_works}
% \input{methods}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\linewidth, scale=0.1]{figures/Encoding.jpg}
%     \caption{Schematic of the Time-Feature Independent (TFI) Embedding of MissTSM that learns a different embedding for every combination of time-step and variate, in contrast to the time-only embeddings of Transformer \cite{vaswani2017attention} and the variate-only embeddings of iTransformers \cite{liu2023itransformer}.} 
%     % MissTSM uses a novel  for every , allowing it to handle time-steps with missing values using masked cross-attention.}
%     \label{fig:tfi}
% \end{figure}

\vspace{-2ex}
\section{Missing Feature Time-Series Modeling (MissTSM)}

% \textcolor{red}{Figure 1: Two stage approach vs One-stage approach (Box-and-arrow diagram)}

% \textcolor{red}{* learning good representation using the available data (unmasked data). Not relying on imputations.}

\subsection{Notations and Problem Formulations}
Let us represent a multivariate TS as $\mathbf{X} \in \mathbb{R}^{T \times N}$, where $T$ is the number of time-steps, and $N$ is the dimensionality (number of variates) of the TS. We assume a subset of variates (or features) to be missing at some time-steps of $\mathbf{X}$, represented in the form of a missing-value mask $\mathcal{M} \in [0, 1]^{T \times N}$, where $\mathcal{M}_{(t, d)}$ represents the value of the mask at $t$-th time-step and $d$-th dimension. 
% $\mathcal{M}_{(t, d)} = 1$ denotes that the corresponding value in $\mathbf{X}_{(t, d)}$ is missing, while $\mathcal{M}_{(t, d)} = 0$ denotes that $\mathbf{X}_{(t, d)}$ is observed. 
Let us denote $\mathbf{X}_{(t, :)} \in \mathbb{R}^N$ as the multiple variates of the TS at a particular time-step $t$, and $\mathbf{X}_{(:, d)} \in \mathbb{R}^T$ as the uni-variate time-series for the variate $d$. In this paper, we consider two downstream tasks for TS modeling: forecasting and classification. For forecasting, the goal is to predict the future $S$ time-steps of $\mathbf{X}$ represented as $\mathbf{Y} \in \mathbb{R}^{S \times N}$, and, for TS classification, the goal is to predict output labels $\mathbf{Y} \in \{1, 2, ..., C\}$ given $\mathbf{X}$, where $C$ is the number of classes.

% [DESCRIBE THE FORECASTING AND CLASSIFICATION PROBLEMS.]
% Note that the missing values in time-series can be 

\subsection{Learning Embeddings for Time-Series with Missing Features using TFI Embedding}
% \par \noindent \textbf{Limitations of Existing Methods:}
% The first step in time-series modeling using transformer-based architectures is to learn an embedding of the time-series $\mathbf{X}$, which is then fed into the transformer encoder. Traditionally, this is done using an Embedding-layer (typically implemented using a multi-layered perceptron) as $\texttt{Embedding}:\mathbb{R}^N \mapsto \mathbb{R}^D$ that maps $\mathbf{X} \in \mathbb{R}^{T \times N}$ to the embedding  $\mathbf{H} \in \mathbb{R}^{T \times D}$, where $D$ is the embedding dimension. The Embedding layer operates on every time-step independently such that the set of variates observed at time-step $t$, $\mathbf{X}_{(t, :)}$, is considered as a single token and mapped to the embedding vector $\mathbf{h}_{t} \in \mathbb{R}^{D}$ as $\mathbf{h}_{t} = \texttt{Embedding}(\mathbf{X}_{(t, :)})$ (see Figure \ref{fig:tfi}(a)). An alternate embedding scheme was recently introduced in the framework of inverted Transformer \cite{liu2023itransformer},  where the uni-variate time-series for the $d$-th variate, $\mathbf{X}_{(:, d)}$, is considered as a single token and mapped to the embedding vector: $\mathbf{h}_{d} = \texttt{Embedding}(\mathbf{X}_{(:, d)})$ (see Figure \ref{fig:tfi}(b)). 
% % Schematic representations of the embedding scheme for original transfomer and iTransformer are depicted in \textcolor{red}{Figure XX}. 
% While both these embedding schemes have their unique advantages, they are unsuitable to handle time-series with arbitrary sets of missing values at every time-step. In particular, the input tokens to the Embedding layer of Transformer or iTransformer requires all components of $\mathbf{X}_{(t, :)}$ or $\mathbf{X}_{(:, d)}$ to be observed, respectively.
% % Assuming that the time-series $\mathbf{X}$ has missing values, these arbitrary tokens might have missing values in them as well. 
% % This in-turn would prevent us from embedding the entire token using the aforementioned $\texttt{Embedding}$ layers. 
% If any of the components in these tokens are missing, we will not be able to compute their embeddings and thus will have to discard either the time-step or the variate, leading to loss of information. 
% for the entire token cannot be obtained. This would drastically affect the number of embedding inputs that can be fed into the subsequent transformer layers, especially as the missing value fraction increases.

% \par \noindent \textbf{Time-Feature Independent (TFI) Embedding:} 
Prior embedding techniques such as in Transformer or iTransfomer models cannot handle missing values (See Appendix \ref{appendix:A.1} for more details) directly. To address this challenge, we propose a novel \emph{Time-Feature Independent (TFI) Embedding} scheme for TS with missing features, where the value at each combination of time-step $t$ and variate $d$ is considered as a single token $\mathbf{X}_{(t, d)}$, and is independently mapped to an embedding using $\texttt{TFIEmbedding}:\mathbb{R} \mapsto \mathbb{R}^D$ as: $\mathbf{h}_{(t, d)} = \texttt{TFIEmbedding}(\mathbf{X}_{(t, d)})$ 
% \begin{equation}
%     \mathbf{h}_{(t, d)} = \texttt{TFIEmbedding}(\mathbf{X}_{(t, d)})
% \end{equation}
In other words, the $\texttt{TFIEmbedding}$ Layer maps $\mathbf{X} \in \mathbb{R}^{T \times N}$ into the TFI embedding $\mathbf{H}^{\mathrm{TFI}} \in \mathbb{R}^{T \times N \times D}$ ({see Figure \ref{fig:tfi}(c) in the Appendix \ref{appendix:A.1}}). The $\texttt{TFIEmbedding}$ is applied only on tokens $\mathbf{X}_{(t, d)}$ that are observed (for missing tokens, i.e., $\mathcal{M}_{(t, d)} = 0$, we generate a dummy embedding that gets masked out in the MFAA layer). The advantage of such an approach is that even if a particular value in the TS is missing, other observed values in TS can be embedded \emph{``independently''} without being affected by missing values. Later, we demonstrate how our Missing Feature-Aware Attention Layer takes advantage of TFI embedding scheme to compute masked cross-attention among observed features at a time-step to account for missing features.

% \textbf{2D Positional Encodings:} We add Positional Encoding vectors $\mathbf{PE}$ to the TFI embedding $\mathbf{H}^{\mathrm{TFI}}$ to obtain positionally-encoded embeddings, $\mathbf{Z} = \mathbf{PE} + \mathbf{H}^{\mathrm{TFI}}$.
% % Since the TFI Embedding scheme maps each time-feature combination $\mathbf{X}_{(t, d)}$ into a higher-dimensional embedding,
% Since TFI embeddings treat every time-feature combination as a token, we use a 2D-positional encoding scheme  defined as follows:

% \begin{align}
%     &\texttt{PE}(t, d, 2i) = \sin \big(\frac{t}{10000^{(4i/D)}} \big) ; \quad \texttt{PE}(t, d, 2i+1) = \cos \big(\frac{t}{10000^{(4i/D)}} \big), \\ 
%     &\texttt{PE}(t, d, 2j+D/2) = \sin \big(\frac{d}{10000^{(4j/D)}} \big) ; \quad \texttt{PE}(t, d, 2j+1+D/2) = \cos \big(\frac{d}{10000^{(4j/D)}} \big),
% \end{align}
% where $t$ is the time-step, $d$ is the feature, and $i, j \in [0, D/4)$ are integers. 
% We add the 2D-positional embedding to our input embeddings to obtained the positionally-encoded embeddings denoted as $\mathbf{Z}$.

% is added to the TFI Embedding to obtain the hidden representation that is fed into subsequent layers: $\mathbf{Z}=\mathbf{H}^{\mathrm{TFI}} + \texttt{PE}$.

% To account for the extra dimension added due to the TFI embedding scheme, we adopt the 2D positional encoding scheme introduced in \textcolor{red}{XX. [ADD Formula for 2d PE.]}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth, height=6cm]{figures/MissTSM_framework.pdf}
    \caption{A schematic illustration of the overall MissTSM Framework with a zoomed-in view of the Missing Feature-Aware Attention (MFAA) Layer on the left.}
    \label{fig:mfa}
\end{figure}
\subsection{Missing Feature-Aware Attention (MFAA) Layer}
% \par \noindent \textbf{Motivation:} 
% Having learned TFI embeddings for every non-missing time-feature combination, 
We propose a novel \emph{Missing Feature-Aware Attention (MFAA) Layer} (see Figure \ref{fig:mfa}) to leverage the power of \emph{``masked-attention''} for learning latent representations at every time-step using partially observed features.
% account for missing values in some time-feature combinations. Specifically,
% % Recent advances of masked modeling in various domains including vision, language and time-series, have demonstrated that \emph{``masked-attention''} can be effectively utilized to reconstruct partially observed sequences. Motivated by it, 
% we ask the question: \emph{can we leverage the partial information of observed features at time-step $t$ to obtain a latent representation $\mathbf{L}_t$  using masked attention?} In response to this question, we introduce a novel \emph{Missing Feature-Aware Attention (MFAA) Layer} illustrated in Figure \ref{fig:mfa}. 
MFAA works by computing \emph{attention scores} based on the partially observed features at a time-step $t$, which are then used to perform a weighted sum of observed features to obtain the latent representation $\mathbf{L}_t$. 
These latent representations are later fed into an encoder-decoder based self-supervised learning framework to reconstruct the TS. 
% This is analogous to traditional imputation techniques that utilize cross-channel correlations for imputations. However, the difference in our framework is that once we have learned latent representations $\mathbf{L}_t$ using the paradigm of self-supervised learning, we can directly use them for downstream tasks without explicitly imputing the missing values. 
% \textcolor{red}{Mention that this technique is analogous to traditional imputation techniques that utilize cross-channel correlations for imputations.}

% at a given time-step $t$ using the missing-value mask $\mathcal{M}$. The weighted aggregation is performed using a method which is similar to a Masked Multi-Head Attention (MHAA) with some subtle differences.

\par \noindent \textbf{Mathematical Formulation:} To obtain \emph{attention scores} from partially-observed features at a time-step, we apply a masked scaled-dot product operation followed by a softmax operation. We first define a learnable query vector $\mathbf{Q} \in \mathbb{R}^{1 \times D}$ which is independent of the variates and time-steps. The positionally-encoded embeddings at time-step $t$, $\mathbf{Z}_{(t,:)}$, are used as key and value inputs in the MFAA Layer. Specifically, the query, key, and value vectors are obtained using linear projections as, $\hat{\mathbf{Q}}=\mathbf{Q}\mathbf{W^Q}, \quad \hat{\mathbf{K}}_t=\mathbf{Z}_{(t, :)}\mathbf{W^K}, \quad \hat{\mathbf{V}}_t=\mathbf{Z}_{(t, :)}\mathbf{W^V}$.
% \begin{equation}
%     \hat{\mathbf{Q}}=\mathbf{Q}\mathbf{W^Q}, \quad \hat{\mathbf{K}}_t=\mathbf{Z}_{(t, :)}\mathbf{W^K}, \quad \hat{\mathbf{V}}_t=\mathbf{Z}_{(t, :)}\mathbf{W^V},
% \end{equation}
Here, $\hat{\mathbf{Q}} \in \mathbb{R}^{1 \times d_k}$ and $\hat{\mathbf{K}}_t, \hat{\mathbf{V}}_t \in \mathbb{R}^{N \times d_k}$, where $d_k$ is the dimension of the vectors after linear projection. The linear projection matrices for the query, key, and values are defined as: $\mathbf{W^Q}, \mathbf{W^K}, \mathbf{W^V} \in \mathbb{R}^{D \times d_k}$ respectively. Note that the key $\hat{\mathbf{K}}_t$ and value $\hat{\mathbf{V}}_t$ vectors depend on the time-step $t$, while the query vector is learnable and doesn't change with time. This separation of roles is inspired by similar architectures in multi-modal grounding, for example, in \cite{detr}, learnable object queries are kept independent of the image content that is sent as keys and values. In our setting, the learnable queries capture the interactions among variates independent of time, enabling the model to attend to the most informative aspects of observed variates at any time-step fed through keys and values. We then define the MFAA Score at a given time-step $t$ as a masked scalar dot-product of query and key vector followed by normalization of the scores, defined as follows:
\begin{equation}
    \mathbf{A}_t = \texttt{MFAAScore}(\hat{\mathbf{Q}}, \hat{\mathbf{K}}_t, \mathcal{M}_{(t,:)}) = \texttt{Softmax}\Big( \frac{\hat{\mathbf{Q}}\hat{\mathbf{K}}_t^\top}{\sqrt d_k} + \eta \mathcal{M}_{(t,:)} \Big),
\end{equation}
where $\mathbf{A}_t \in \mathbb{R}^N$ is the MFAA Score vector of size $N$ corresponding to the $N$ variates, and $\eta$ is a large negative value. The negative bias term $\eta$ forces the masked-elements that correspond to the missing variates in TS to have an attention score of zero. Thus, by definition, the $i$-th element of the MFAA Score $\mathbf{A}_{(t, i)} \neq 0 \implies \mathcal{M}_{(t,:)} \neq 0$.
% by definition the attention scores of variates that are missing are zero, and the attention scores of the observed variated are normalized using the softmax operation. the final latent representations is a weighted aggregation of the observed variables for that particular time-step.
We then compute the latent representation $\mathbf{L}_t$ using $\mathbf{A}_t$ and the Value vector $\hat{\mathbf{V}}_t$ as, $\mathbf{L}_t = \texttt{MFAA}(\mathbf{A}_t, \hat{\mathbf{V}}_t) = \mathbf{A}_t\hat{\mathbf{V}}_t \in \mathbb{R}^{d_k}. $
% \begin{equation}
%     \mathbf{L}_t = \texttt{MFAA}(\mathbf{A}_t, \hat{\mathbf{V}}_t) = \mathbf{A}_t\hat{\mathbf{V}}_t \in \mathbb{R}^{d_k}
% \end{equation}
Similar to MHA used in transformers, we extend MFAA to multiple heads as: $\texttt{MultiHeadMFAA}(\mathbf{Q}, \mathbf{Z}_{(t, :)}, \mathcal{M}_{(t,:)}) = \mathrm{Concat}(\mathbf{L}^0_t, \mathbf{L}^1_t, ..., \mathbf{L}^h_t)\mathbf{W^O}$
% \begin{equation}
%     \texttt{MultiHeadMFAA}(\mathbf{Q}, \mathbf{Z}_{(t, :)}, \mathcal{M}_{(t,:)}) = \mathrm{Concat}(\mathbf{L}^0_t, \mathbf{L}^1_t, ..., \mathbf{L}^h_t)\mathbf{W^O}
% \end{equation}
, where $h$ is number of heads, $\mathbf{W^0} \in \mathbb{R}^{hd_k \times D_{o}}$, $\mathbf{L}^i_t$ is latent representation obtained from $i$-th $\texttt{MHAA}$ Layer, and $D_{o}$ is output-dimension of $\texttt{MultiHeadMFAA}$ Layer.


% \subsection{Putting Everything Together: Overall Framework of MissTSM}
% Figure \ref{fig:mfa} shows the overall framework of MissTSM. For an input time-series $\mathbf{X}$, we apply the TFI embedding layer followed by the MFAA layer to learn a latent representation for every time-step. We then integrate the latent representations into a Masked Auto-Encoder (MAE) \cite{he2022masked} framework adapted for time-series (similar to Ti-MAE \cite{li2023ti}). 
% % Specifically, we apply temporal masking to the latent representations before sending them to the encoder block. The encoder embeddings for the unmasked time-steps are then fed to the decoder block that learns the tokens for the masked time-steps in order to finally reconstruct the original time-series $\mathbf{X}$. 
% Although the design of the proposed TFI-Embedding and MFAA Layers are flexible enough that they can be integrated with any transformer-based time-series modeling framework (e.g., AutoFormer \cite{autoformer}), we opted for a masked time-series modeling approach (such as Ti-MAE \cite{li2023ti} and SimMTM \cite{simmtm}) due to their recent success in time-series modeling. Further, out of the several state-of-the-art masked time-series modelling techniques, we intentionally chose the simplest variation of MAE, namely Ti-MAE \cite{li2023ti}, to highlight the power of TFI and MFAA layers in handling missing values. 
% To the best of our knowledge, MissTSM is the first end-to-end framework for modeling time-series with missing values without explicitly imputing the time-series. Like a typical masked time-series modeling approach, MissTSM  has two main stages: (1) \emph{Self-Supervised Learning Stage:} where the multivariate time-series (with missing values) is reconstructed using an encoder-decoder architecture, with the goal of learning meaningful representations using just the encoder, and (2) \emph{Fine-tuning Stage:} where the latent representations learned by the encoder are fed into a multi-layer perceptron to perform downstream tasks of forecasting and classification. 

\subsection{Putting Everything Together: Overall Framework of MissTSM}
Figure \ref{fig:mfa} shows the MissTSM framework. We opted for a masked TS modeling approach (such as Ti-MAE \cite{li2023ti}) due to their recent success.
% , and intentionally chose the simplest variation of MAE, namely Ti-MAE \cite{li2023ti}.
% , to highlight the power of TFI and MFAA layers in handling missing values. 
MissTSM  has two main stages: (1) \emph{Self-Supervised Learning Stage:} where multivariate TS (with missing values) is reconstructed using an encoder-decoder architecture, with the goal of learning meaningful representations, (2) \emph{Fine-tuning Stage:} where latent representations learned by encoder are fed into a MLP to perform downstream tasks. 

% Our self-supervised training architecture is similar to Ti-MAE.


% In addition to the TFI Embedding scheme, and MFAA Layer to obtain latent representations at every time-step,
% \subsection{Motivation/ Problem Definition}

% Theoretical insights about the advantages of end-to-end learning with missing values as compared to two-stage approaches? Look into maths of bias/variance trade-offs.

% Should we add some experiment for motivation?



% \subsection{Method Innovations/ Structure Overivew}

% Step-by-step techniques:

% \textbf{Pretraining Model Architecture}
% \begin{enumerate}
%     \item Using the time-series instance-normalization technique (cite paper)
%     \item \textcolor{red}{Feature Time-Independent Embedding:} 
%     $B, L, n_{feat} \rightarrow B, L, n_{feat}, D$, this projects each point to a higher dimensional representation (Add figure similar to iTransformer) [\href{https://arxiv.org/pdf/2203.05556}{Reference for embedding scalar numerical features}]
%     Motivation is missing values.
%     \item 2D-positional Embedding (add citation of paper from github)
%     \item \textcolor{red}{Missing Feature-Aware Attention (MFAA)} 
%     \item MAE style Masking
%     \item Transformer Encoder
%     \item Transformer Decoder
% \end{enumerate}

% \textbf{Fine-tuning Model}: MAE style Flattened MLP for prediction. (similar to SimMTM)

% Add schematic for MAE-style model architecture. the MFAA can be part of the schematic





% \input{experiments}
\section{Experiments}

% We compare MissTSM with two-stage baseline methods on two downstream tasks: forecasting and classification, under varying scenarios of missing values.
\par
\textbf{Datasets and Baselines:} We consider three popular TS forecasting datasets: ETTh2 \cite{informer}, ETTm2 \cite{informer} and Weather \cite{weather}. For classification, we use three real-world datasets, namely, Epilepsy, EMG, and Gesture. We follow the evaluation setup proposed in AutoFormer \cite{autoformer} for the forecasting datasets and evaluation setup proposed in TF-C \cite{tfc} for the classification ones.
. For our experiments, we consider five SOTA TS-modeling baselines, SimMTM \cite{simmtm}, PatchTST \cite{patchtst}, Autoformer \cite{autoformer}, iTransformer \cite{liu2023itransformer} and DLinear \cite{dlinear}. In order to apply these methods on data with missing values, we consider four imputation techniques---a simple $2^\text{nd}$-order spline imputation, k-nearest neighbors (kNN) and two state-of-the-art imputation techniques, SAITS \cite{saits} and BRITS \cite{cao2018brits}. 
% We compare MissTSM's performance against time-series models trained on data imputed using the imputation methods. For forecasting, we used a default lookback window of $L$ = 336, and vary the horizon windows as $T \in$ \{96, 192, 336, 720\}. 
For more experimental details, refer to Appendix \ref{appendix:B.3}

% \par
% \textbf{Baselines:} For all of our experiments, we consider two state-of-the-art time-series modeling baselines, SimMTM \cite{simmtm} and PatchTST \cite{patchtst}. In order to apply these methods on data with missing values, we also consider two imputation techniques---a simple $2^\text{nd}$-order spline imputation, and a state-of-the-art transformer-based imputation technique, SAITS \cite{saits}. We compare MissTSM's performance against SimMTM and PatchTST models trained and tested on data imputed using both SAITS and spline interpolations. The same evaluation setup is used for all baselines. For forecasting, we used a default lookback window of $L$ = 336, while we varied the horizon windows as $T \in$ \{96, 192, 336, 720\}.  

% \begin{table}[t]
%  \caption{Comparing forecasting performance of baseline methods using mean squared error (MSE) as the evaluation metric under no masking, MCAR masking, and periodic masking. For every dataset, we consider multiple forecasting horizons, $T \in \{96, 192, 336, 720\}$. We highlight the best performing model in dark blue and the $2^\text{nd}$-best model using light blue for every masking experiment.}
% \label{tab:main_table}
% \renewcommand{\arraystretch}{1.5}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lllllllllllllll}
% \hline
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{3}{l}{\cellcolor[HTML]{FFFFFF}}                                                                                                                    & \multicolumn{5}{c}{\cellcolor[HTML]{FFFFFF}MCAR Masking}                                                                                                                                                                                                                                 & \multicolumn{5}{c}{\cellcolor[HTML]{FFFFFF}Periodic Masking}                                                                                                                                                                                    \\ \cline{6-15} 
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{3}{c}{\cellcolor[HTML]{FFFFFF}No Masking}                                                                                                          & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}}                          & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}SimMTM}                                                     & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}PatchTST}                                                    & \cellcolor[HTML]{FFFFFF}                          & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}SimMTM}                                                     & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}PatchTST}                               \\ \cline{3-5} \cline{7-10} \cline{12-15} 
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}MissTSM} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SimMTM} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}PatchTST} & \multicolumn{1}{c}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}MissTSM}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS}  & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}MissTSM} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & SAITS                         \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & \cellcolor[HTML]{9698ED}0.255                       & 0.295                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.274}   & 0.250                                                                 & 0.317                                              & \cellcolor[HTML]{DAE8FC}0.243                     & 0.299                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.224} & \cellcolor[HTML]{9698ED}0.259                     & 0.382                                              & 0.357                                             & \cellcolor[HTML]{DAE8FC}0.314                      & 0.343                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & \cellcolor[HTML]{9698ED}0.234                       & 0.356                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.338}   & \cellcolor[HTML]{DAE8FC}0.303                                         & 0.373                                              & 0.315                                             & 0.36                                               & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.298} & \cellcolor[HTML]{9698ED}0.321                     & 0.422                                              & 0.47                                              & \cellcolor[HTML]{DAE8FC}0.411                      & 0.431                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{9698ED}0.316                       & 0.375                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.33}    & \cellcolor[HTML]{9698ED}0.277                                         & 0.390                                              & 0.342                                             & 0.336                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.28}  & \cellcolor[HTML]{9698ED}0.303                     & 0.427                                              & 0.475                                             & \cellcolor[HTML]{DAE8FC}0.383                      & 0.443                         \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}ETTh2}   & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.305                       & 0.404                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.378}   & \cellcolor[HTML]{9698ED}0.318                                         & 0.404                                              & 0.36                                              & 0.377                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.336} & \cellcolor[HTML]{9698ED}0.382                     & 0.440                                              & 0.497                                             & \cellcolor[HTML]{DAE8FC}0.43                       & 0.49                          \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & 0.183                                               & \cellcolor[HTML]{DAE8FC}0.172                      & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.164}   & 0.216                                                                 & 0.169                                              & 0.173                                             & \cellcolor[HTML]{9698ED}0.155                      & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.161} & 0.252                                             & 0.170                                              & 0.178                                             & \cellcolor[HTML]{9698ED}0.159                      & \cellcolor[HTML]{DAE8FC}0.165 \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & \cellcolor[HTML]{9698ED}0.209                       & 0.223                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.22}    & 0.275                                                                 & 0.228                                              & 0.233                                             & \cellcolor[HTML]{9698ED}0.215                      & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.223} & 0.254                                             & \cellcolor[HTML]{DAE8FC}0.228                      & 0.24                                              & \cellcolor[HTML]{9698ED}0.218                      & 0.233                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{9698ED}0.261                       & 0.282                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.277}   & \cellcolor[HTML]{DAE8FC}0.281                                         & 0.284                                              & 0.286                                             & \cellcolor[HTML]{9698ED}0.274                      & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.286} & 0.299                                             & \cellcolor[HTML]{DAE8FC}0.285                      & 0.297                                             & \cellcolor[HTML]{9698ED}0.276                      & 0.296                         \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}ETTm2}   & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.311                       & 0.374                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.367}   & \cellcolor[HTML]{9698ED}0.300                                         & 0.373                                              & 0.386                                             & \cellcolor[HTML]{DAE8FC}0.366                      & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.383} & \cellcolor[HTML]{9698ED}0.310                     & 0.377                                              & 0.406                                             & \cellcolor[HTML]{DAE8FC}0.369                      & 0.404                         \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & 0.164                                               & \cellcolor[HTML]{DAE8FC}0.158                      & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.151}   & 0.191                                                                 & 0.208                                              & \cellcolor[HTML]{DAE8FC}0.18                      & 0.208                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.175} & 0.198                                             & \cellcolor[HTML]{DAE8FC}0.142                      & 0.177                                             & \cellcolor[HTML]{9698ED}0.141                      & 0.177                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & 0.210                                               & \cellcolor[HTML]{DAE8FC}0.199                      & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.196}   & \cellcolor[HTML]{DAE8FC}0.233                                         & 0.260                                              & \cellcolor[HTML]{DAE8FC}0.233                     & 0.257                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.224} & 0.236                                             & \cellcolor[HTML]{9698ED}0.192                      & 0.225                                             & \cellcolor[HTML]{DAE8FC}0.193                      & 0.226                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & 0.254                                               & \cellcolor[HTML]{9698ED}0.246                      & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.249}   & \cellcolor[HTML]{DAE8FC}0.284                                         & 0.314                                              & \cellcolor[HTML]{DAE8FC}0.281                     & 0.31                                               & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.277} & 0.279                                             & \cellcolor[HTML]{DAE8FC}0.249                      & 0.277                                             & \cellcolor[HTML]{9698ED}0.246                      & 0.279                         \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}Weather} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & 0.324                                               & \cellcolor[HTML]{9698ED}0.317                      & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.319}   & \cellcolor[HTML]{9698ED}0.310                                         & 0.388                                              & \cellcolor[HTML]{DAE8FC}0.352                     & 0.381                                              & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.352} & \cellcolor[HTML]{DAE8FC}0.333                     & \cellcolor[HTML]{9698ED}0.327                      & 0.35                                              & \cellcolor[HTML]{9698ED}0.327                      & 0.355                         \\ \hline
% \end{tabular}}
% \end{table}

% \par
% \textbf{Patch Masking:} This setting simulates real-world applications where sensors are prone to random failures, which is then followed by a stretch of repair time (in our case, the masked patch), the length of which is again, randomly selected within a fixed set of boundaries.


% \begin{table*}[ht]
% \caption{Comparing forecasting performance of baseline methods under MCAR and Periodic Masking. Best performing model in dark blue and the $2^\text{nd}$-best model in light blue}
% \label{tab:main_table}
% \renewcommand{\arraystretch}{1.0}
% \scriptsize
% \resizebox{\textwidth}{!}{\begin{tabular}{llllllllllll}
% \hline
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{5}{c}{\cellcolor[HTML]{FFFFFF}MCAR Masking}                                                                                                                                                                                                                                          & \multicolumn{5}{c}{\cellcolor[HTML]{FFFFFF}Periodic Masking}                                                                                                                                                                                             \\ \cline{3-12} 
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}}                          & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}SimMTM}                                                     & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}PatchTST}                                                             & \cellcolor[HTML]{FFFFFF}                          & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}SimMTM}                                                     & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}PatchTST}                                        \\ \cline{4-7} \cline{9-12} 
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{1}{c}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}MissTSM}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS}           & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}MissTSM} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & SAITS                                  \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & \cellcolor[HTML]{9698ED}0.266  0.0146                                & 0.349  0.0191                                     & \cellcolor[HTML]{DAE8FC}0.267  0.0308            & 0.309  0.0087                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.413  0.0339} & \cellcolor[HTML]{9698ED}0.268  0.0151            & 0.646  0.1716                                     & \cellcolor[HTML]{DAE8FC}0.319  0.0659            & 0.492  0.0825                                     & 0.599  0.2110                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & \cellcolor[HTML]{9698ED}0.283  0.0123                                & 0.414  0.0232                                     & \cellcolor[HTML]{DAE8FC}0.360  0.0465            & 0.370  0.0076                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.495  0.0376} & \cellcolor[HTML]{9698ED}0.295  0.0298            & 0.729  0.1913                                     & \cellcolor[HTML]{DAE8FC}0.408  0.0865            & 0.510  0.0578                                     & 0.672  0.2113                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{9698ED}0.287  0.0142                                & 0.435  0.0297                                     & 0.394  0.0542                                    & \cellcolor[HTML]{DAE8FC}0.352  0.0062             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.492  0.0450} & \cellcolor[HTML]{9698ED}0.319  0.0185            & 0.751  0.2034                                     & \cellcolor[HTML]{DAE8FC}0.436  0.1056            & 0.469  0.0423                                     & 0.671  0.2260                         \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}ETTh2}   & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.323  0.0125                                & 0.447  0.0250                                     & 0.413   0.0464                                   & \cellcolor[HTML]{DAE8FC}0.394  0.0089             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.564  0.0574} & \cellcolor[HTML]{9698ED}0.356  0.0310            & 0.735  0.1836                                     & \cellcolor[HTML]{DAE8FC}0.443  0.0746            & 0.502  0.0367                                     & 0.723  0.2251                         \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & 0.222  0.0058                                                        & 0.170  0.0030                                     & \cellcolor[HTML]{9698ED}0.166  0.0082            & 0.170  0.0005                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.168  0.0025} & 0.241  0.0105                                    & \cellcolor[HTML]{DAE8FC}0.166  0.0050             & \cellcolor[HTML]{9698ED}0.156  0.0155            & 0.175  0.0032                                     & 0.187  0.0038                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & 0.274  0.0023                                                        & 0.230  0.0031                                     & \cellcolor[HTML]{9698ED}0.223  0.0115            & 0.228  0.0005                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.225  0.0035} & 0.260  0.0109                                    & \cellcolor[HTML]{DAE8FC}0.224  0.0050             & \cellcolor[HTML]{9698ED}0.210  0.0207            & 0.233  0.0021                                     & 0.244  0.0050                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{DAE8FC}0.279  0.0018                                & 0.286  0.0042                                     & \cellcolor[HTML]{9698ED}0.276  0.0111            & 0.286  0.0009                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.282  0.0051} & \cellcolor[HTML]{DAE8FC}0.279  0.0333            & 0.281  0.0056                                     & \cellcolor[HTML]{9698ED}0.262  0.0242            & 0.291  0.0024                                     & 0.300  0.0040                         \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}ETTm2}   & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.316  0.0144                                & 0.377  0.0046                                     & \cellcolor[HTML]{DAE8FC}0.369  0.0128            & 0.378  0.0014                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.373  0.0062} & \cellcolor[HTML]{9698ED}0.321  0.0139            & 0.372  0.0061                                     & 0.353  0.0346                                    & 0.382  0.0020                                     & 0.394  0.0064                         \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & 0.191  0.0033                                                        & 0.200  0.0475                                     & 0.178  0.0025                                    & \cellcolor[HTML]{DAE8FC}0.189  0.0095             & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.172  0.0010} & 0.199  0.0021                                    & 0.208  0.0648                                     & \cellcolor[HTML]{DAE8FC}0.176  0.0019            & 0.190  0.0110                                     & \cellcolor[HTML]{9698ED}0.171  0.0018 \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & \cellcolor[HTML]{DAE8FC}0.227  0.0068                                & 0.250  0.0488                                     & 0.230  0.0043                                    & 0.227  0.0094                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.215  0.0008} & 0.238  0.0059                                    & 0.258  0.0654                                     & \cellcolor[HTML]{DAE8FC}0.225  0.0019            & 0.227  0.0109                                     & \cellcolor[HTML]{9698ED}0.214  0.0015 \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{DAE8FC}0.282  0.0048                                & 0.304  0.0484                                     & 0.282  0.0079                                    & 0.275  0.0090                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.264  0.0008} & 0.286  0.0080                                    & 0.313  0.0639                                     & 0.278  0.0019                                    & \cellcolor[HTML]{DAE8FC}0.274  0.0102             & \cellcolor[HTML]{9698ED}0.263  0.0018 \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}Weather} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.322  0.0088                                & 0.377  0.0449                                     & 0.355  0.0065                                    & 0.345  0.0089                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.335  0.0009} & \cellcolor[HTML]{DAE8FC}0.339  0.0112            & 0.388  0.0610                                     & 0.352  0.0015                                    & 0.345  0.0099                                     & \cellcolor[HTML]{9698ED}0.334  0.0023 \\ \hline
% \end{tabular}}
% \end{table*}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth, scale=0.9]{figures/effectofmasking.pdf}
%     \caption{Effect of masking fractions in MCAR on forecasting performance (MSE) under different prediction horizons $T \in \{192, 336, 720\}$ on three datasets, ETTm2, ETTh2, and Weather. }
%     \label{effectofmasking}
%     \vspace{-2ex}
% \end{figure}



% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth, scale=0.9]{figures/effect_of_masking_2.pdf}
%     \vspace{0.5cm} % Add some vertical space between the images
%     % \includegraphics[width=0.66\textwidth]{effect_of_masking_2_2.pdf}
%     \vspace{-4ex}
%     \caption{Effect of varying masking types: MCAR and Periodic Masking on the ETTh2 and ETTm2 datasets for long-term forecasting ($T=720$).}
%     \label{fig:effectofmasking2}
%     \vspace{-2ex}
% \end{figure}
% \begin{figure}[htbp!]
%     % \centering
%     \begin{minipage}[b]{0.5\textwidth}
%         \centering
%         \begin{subfigure}[b]{0.6\textwidth}
%             \centering
%             \includegraphics[width=0.95\textwidth]{figures/rebuttal_figures/ts_baselines_etth2_t_192_mcar_saits.pdf}
%             % \caption{ETTh2, T=192, SAITS}
%             \caption{ETTh2,T=192}
%             \label{fig:first_subfigure_4}
%             % \vspace{-20pt}
%         \end{subfigure}
%         \begin{subfigure}[b]{0.6\textwidth}
%             \centering
%             \includegraphics[width=0.97\textwidth]{figures/rebuttal_figures/ts_baselines_weather_t_720_mcar_saits.pdf}
%             % \caption{Weather, T=720, SAITS}
%             \caption{Weather,T=720}
%             \label{fig:first_subfigure_5}
%             % \vspace{-20pt}
%         \end{subfigure}
%         \caption{Performance comparison with multiple TS Baselines imputed with SAITS}
%         \label{fig:fig4}
%     \end{minipage}
%     \hspace{0.001\textwidth}
%     \begin{minipage}[b]{0.49\textwidth}
%         \centering
%         \begin{subfigure}[b]{\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/rebuttal_figures/classification_ts_baselines.pdf}
%             % \caption{ETTm2, T=720}
%             \caption{Datasets (Left to Right)=EMG,Epilepsy,Gesture}
%             \label{fig:first_subfigure_5}
%         \end{subfigure}
%         \caption{Classification F1 scores across varying masking fractions: $\{0.0, 0.2, 0.4, 0.6, 0.8\}$.}
%         \label{fig:fig5}
%     \end{minipage}
% \end{figure}


\begin{figure}[htbp]

    % First column (left) with two rows
    \begin{minipage}{0.49\textwidth}
        % First row of first column: two subfigures
        \begin{subfigure}[htbp]{0.49\textwidth}
            % \centering
            \includegraphics[width=\textwidth]{figures/rebuttal_figures/imp_baselines_weather_t_720_mcar.pdf}
            \caption{Weather, T=720}
        \end{subfigure}
        \hfill
        \begin{subfigure}[htbp]{0.49\textwidth}
            % \centering
            \includegraphics[width=0.9\textwidth]{figures/rebuttal_figures/imp_baselines_ETTh2_t_720_mcar.pdf}
            \caption{ETTh2, T=720}
            \label{fig:imp_baseline_b}
        \end{subfigure}
        \caption{\textbf{Multiple Imputation Baselines}. Performance comparison across multiple imputation models. Imputation models considered: kNN, Spline, SAITS. TS Baselines: iTransformer}
        \label{fig:imp_baselines_main}
        % \vspace{0.5cm} % Space between rows
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
    % \centering
        % First row of second column: two subfigures
        \begin{subfigure}[htbp]{0.49\textwidth}
        % \label{ts_baselines_main}
            \includegraphics[width=\textwidth]{figures/rebuttal_figures/ts_baselines_ettm2_t_720_mcar.pdf}
            \caption{ETTm2, T=720}
        \end{subfigure}
        % \vspace{0.09cm} % Space between rows
        \hfill
        % Second row of second column: two subfigures
        \begin{subfigure}[htbp]{0.49\textwidth}
            \includegraphics[width=\textwidth]{figures/rebuttal_figures/ts_baselines_ettm2_t_720_periodic.pdf}
            \caption{ETTm2, T=720}
        \end{subfigure}
        \caption{\textbf{Multiple TS Baselines.} Performance comparison with multiple TS Baselines imputed with SAITS, under MCAR and Periodic setting}
        \label{fig:ts_baselines_main}
    \end{minipage}
% TS Baselines considered: Autoformer \cite{autoformer}, PatchTST \cite{patchtst}, iTransformer \cite{liu2023itransformer}, DLinear \cite{dlinear}, SimMTM \cite{simmtm}

    % \caption{Layout with two columns. The left column has two rows, one with subfigures and one with an independent figure. The right column has two rows, both with subfigures.}
    \label{fig:forecasting_results}
\end{figure}

\vspace{-1.5pt}
% In For classification, we go with random masking.
\par \noindent \textbf{Forecasting Results:} For comparing the forecasting performance we consider the Mean Squared Error (MSE) metric. The performance of MissTSM is compared across an array of missing data fractions (60\%, 70\%, 80\%, 90\%), along varying forecasting horizons (T=96,192,336,720) and under different masking strategies. Figure \ref{fig:imp_baselines_main} compares MissTSM with time-series baselines imputed with multiple imputation techniques. The results shown are on Weather and ETTh2 dataset and along the longest forecasting horizon. We can see that MissTSM performs better or similar in comparison to all the baselines considered. It is also interesting to note the significant drop in performance of iTransformer trained on kNN imputed data, which demonstrates the influence of imputation methods on the final performance of time-series baselines. We observe that BRITS results in significantly high MSE, and therefore not considered in result plots. 
 \begin{wrapfigure}{r}{0.49\textwidth} % Adjust 'r' for right, 'l' for left, and width as necessary
    % \centering
    \includegraphics[width=0.49\textwidth]{figures/rebuttal_figures/classification_2_ts_baselines.pdf} % Adjust width as needed
    \caption{Classification F1 scores across varying masking fractions: $\{0.2, 0.4, 0.6, 0.8\}$.} 
    \label{fig:clf}
    \vspace{-1ex}
\end{wrapfigure}
We also compare MissTSM's performance against different SOTA time-series baselines (see Figure \ref{fig:ts_baselines_main})  imputed with SAITS, and across different masking strategies. MissTSM shows significant improvement over the baselines on ETTh2 and ETTm2, under both, MCAR and periodic masking settings (Refer to additional results in Appendix \ref{appendix:C.2}). Overall, we observe that MissTSM is consistently better than the baselines for ETTh2 dataset, while it shows competitive performance on ETTm2 and Weather datasets.
% For the MCAR masking experiments, we observe a trend across all the datasets that the MissTSM framework performs significantly better than the baselines for longer-term forecasting, i.e., for larger forecasting horizons such as 336 and 720. Additionally, we observe on ETTh2 and Weather datasets that the baselines with SAITS-based imputation performs better than the corresponding spline interpolation counterpart. 
% Add setting to appendix
% \par
% ETTh2: MCAR 80\%, Periodic 90\%
% \\
% ETTm2: MCAR 60\%, Periodic 80\%
% \\
% Weather: MCAR 60\%, Periodic 70\%








% \subsection{Analyzing the Impact of Missing Value Fractions}

% To understand the effect of varying masking fractions on the forecasting performance of MissTSM and baseline methods, Figure \ref{effectofmasking} shows variations in the MSE of comparative methods as we increase the missing value fraction in MCAR masking scheme from $0.6$ to $0.9$ for different forecasting horizons and datasets.  We can see that MissTSM mostly performs at par or better than the time-series baselines, especially for higher missing fractions and longer forecasitng horizons. For example, we can see that while MissTSM does not achieve similar MSE as baselines on ETTm2 for $T = 336$, it performs significantly better than baselines when we increase the forecasting horizon to $T = 720$ on the same dataset. Similarly, for the Weather and ETTh2 datasets, we can see that MissTSM shows similar performance as state-of-the-art baselines across all settings of missing value fractions. 
% It is interesting to see that, for ETTh2, MissTSM achieves similar MSE as the stronger baseline methods (with more complex architectures both for imputation and time-series modeling) even with 90\% missing values. 


% Figure \ref{fig:effectofmasking2} shows the effect of varying the masking scheme along with the missing value fractions on ETTh2 and ETTm2 datasets. We can see that with periodic masking, there are sharp changes in MSE as we vary the missing fractions. While this can be attributed to the presence of an implicit pattern in the masked data for generating periodic missing values, it is interesting to note that MissTSM is relatively more stable, with similar or better performance than the baselines.  
% Moreover, it is quite interesting to see the amount of fluctuations in the baselines, which reflect the strong dependency of the modeling errors with the imputation errors. 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth, scale=0.9]{effect_of_masking_2.pdf}
%     \caption{Forecasting Performance under different masking types - MCAR, Periodic, Patch. For ETTm2 Patch, SimMTM baselines were not trained, hence, not shown in the figure}
%     \label{effectofmasking2}
% \end{figure}
% \subsection{Classification}


\textbf{Classification Results:} For fine-tuning on the classification tasks, we add a multi-layer perceptron to the encoder as the classification layer. The same is done for the SimMTM baseline. The classification performance is reported with F1-score metric and under randomly masked (MCAR) fractions of data (20\%, 40\%, 60\%, 80\%). Figure \ref{fig:clf} compares MissTSM with two variants of SimMTM - one, trained on Spline imputed data, and the other, trained on SAITS imputed data. As seen in the figure, MissTSM achieves roughly similar or better performance as SimMTM on EMG (performance improvement is visible over increasing masked fractions), and outperforms SimMTM on the Gesture Dataset (Refer to \ref{appendix:C.3} for results on all three datasets). These results demonstrate the effectiveness of our proposed MissTSM framework to circumvent the need for explicit imputation of missing values while achieving similar predictive performance as state-of-the-art.






% Results to add in the paper:
% \begin{enumerate}
%     \item \textcolor{red}{Table 1: Forecasting Results} \begin{itemize}
%         \item \textcolor{red}{Datasets: ETTh1, ETTh2, ETTm1, ETTm2, Weather}
%         \item \textcolor{red}{Baselines: SimMTM+Spline, SimMTM+SAITS, PatchTST+Spline, PatchTST+SAITS, ours}
%         \item \textcolor{red}{Choose mask fraction : XX (This can be same for ETT datasets - say 0.9 or 0.8, but for weather we can choose a different one.)}
%         \item \textcolor{red}{Masking Types: MCAR, Periodic}
%         \item Summary of Table: 5 rows for each dataset, and each row will have 5 baselines $\times$ 2 masking types = 10 columns
%         \item Should we report both MAE and MSE like SimMTM/PatchTST/iTransformer? If yes, we can change the structure of the Table. Suggestion: (10 Rows: 5 dataset $\times$ 2 blocks for each masking type, and 10 columns: 5 baselines $\times$ 2 metrics)
%         \item Also, we thought about showing the percentage decrease from the unmasked error (mask fraction=0\%). That will increase the number of values. 
%     \end{itemize}

%     \item \textcolor{red}{Figure 1: Effect of masking and type of masking} \begin{itemize}
%         \item \textcolor{red}{Datasets: ETTh1, ETTh2}
%         \item \textcolor{red}{Type of masking: MCAR, Periodic, Patch (if we have) - SimMTM + Spline/SAITS for Patch missing}
%         \item \textcolor{red}{Baselines: Can we show the same 5 baselines??}
%         \item \textcolor{red}{Each plot: Mask Fractions (along x-axis) vs MSE (along y-axis) for every baseline}
%         \item Summary: Ideally 3 columns (type of masking) $\times$ 2 rows (datasets) plot Figure.
%         \item If not, we can show 4 datasets (all EET datasets) for 2 type of masking.
%     \end{itemize}

%     \item \textcolor{red}{Figure 2: Ablation Study of our method for forecasting} \begin{itemize}
%         \item \textcolor{red}{Datasets: ETTh1, ETTh2}
%         \item \textcolor{red}{Baselines: Ours, MAE + Spline, MAE + SAITS}
%         \item \textcolor{red}{Type of masking: MCAR, Periodic??}
%         \item \textcolor{red}{Each plot: Mask Fractions (along x-axis) vs MSE (along y-axis) for every baseline}
%         \item Summary: 2 x 2 plots.
%     \end{itemize}

    % \item \textcolor{red}{Figure 3: Classification} \begin{itemize}
    %     \item \textcolor{red}{Datasets: Epilepsy, Gesture, EMG}
    %     \item \textcolor{red}{Baselines: SimMTM+Spline, SimMTM+SAITS, MAE+Spline, MAE+SAITS, ours, [MAYBE IF WE CAN ADD PATCH-TST]}
    %     \item \textcolor{red}{Type of masking: MCAR only?}
    %     \item \textcolor{red}{Each plot: Mask Fractions (along x-axis) vs F1 score (along y-axis) for every baseline} 
    %     \item Summary: 3 plots each with 5 baselines
    %     \item MAE + Spline and MAE + SAITS can be an ablation study separate Figure.
    % \end{itemize}
    
    % \item Time complexity Figure to show speed improvements due to single stage approach.
    
%     \item Do we have results with different forecasting and lookback window variations? (TODO: for rebuttal.)
% \end{enumerate}


% \input{conclusion}
\section{Conclusions and Future Work}

% To the best of our knowledge, our proposed MissTSM framework is the first end-to-end framework for TS modeling with missing values that does not require any explicit imputations. 
% We introduce a novel Time-Feature Independent Embedding scheme in MissTSM to represent every time-feature combination as an independent token. We also propose a novel Missing Feature-Aware Attention (MHAA) Layer that utilizes partially observed variates (or channels) at a given time-step $t$ to learn latent representations.
% , echoing the essence of traditional imputation methods that utilize cross-channel correlations to infer missing values. 
We empirically demonstrate the effectiveness of the MissTSM framework across multiple benchmark datasets and synthetic masking strategies. 
However, a limitation of MFAA layer is that it does not explicitly learn the non-linear temporal dynamics, and relies on subsequent transformer encoder blocks to learn the dynamics. Future work can explore modifications of MFAA layer to address this limitation.

\subsubsection*{Acknowledgments}
This work was supported in part by NSF awards IIS-2239328 and DEB-2213550.
This manuscript has been authored by UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of Energy (DOE). The US government retains and the publisher, by accepting the article for publication, acknowledges that the US government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for US government purposes. DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (https://www.energy.gov/doe-public-access-plan).
\bibliography{main}
% \bibliographystyle{plainnat}
\bibliographystyle{unsrt}



% \subsection{Retrieval of style files}


% The style files for this workshop and other information are available on
% the at
% \begin{center}
%   \url{https://neurips-time-series-workshop.github.io/}
% \end{center}

% The only supported style file for this workshop is \verb+timeseries_workshop.sty+,
% rewritten for \LaTeXe{}.


% The \LaTeX{} style file contains three optional arguments: \verb+final+, which
% creates a camera-ready copy, \verb+preprint+, which creates a preprint for
% submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
% \verb+natbib+ package for you in case of package clash.


% \paragraph{Preprint option.}
% If you wish to post a preprint of your work online, e.g., on arXiv, using the
% NeurIPS style, please use the \verb+preprint+ option. This will create a
% nonanonymized version of your work with the text ``Preprint. Work in progress.''
% in the footer. This version may be distributed as you see fit. Please \textbf{do
%   not} use the \verb+final+ option, which should \textbf{only} be used for
% papers accepted to the workshop.


% At submission time, please omit the \verb+final+ and \verb+preprint+
% options. This will anonymize your submission and add line numbers to aid
% review. Please do \emph{not} refer to these line numbers in your paper as they
% will be removed during generation of camera-ready copies.


% The file \verb+main.tex+ may be used as a ``shell'' for writing your
% paper. All you have to do is replace the author, title, abstract, and text of
% the paper with your own.


% The formatting instructions contained in these style files are summarized in
% Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


% \section{General formatting instructions}
% \label{gen_inst}


% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
% type with a vertical spacing (leading) of 11~points.  Times New Roman is the
% preferred typeface throughout, and will be selected for you by default.
% Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
% indentation.


% The paper title should be 17~point, initial caps/lower case, bold, centered
% between two horizontal rules. The top rule should be 4~points thick and the
% bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
% below the title to rules. All pages should start at 1~inch (6~picas) from the
% top of the page.


% For the final version, authors' names are set in boldface, and each name is
% centered above the corresponding address. The lead author's name is to be listed
% first (left-most), and the co-authors' names (if different address) are set to
% follow. If there is only one co-author, list both author and co-author side by
% side.


% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% \section{Headings: first level}
% \label{headings}


% All headings should be lower case (except for first word and proper nouns),
% flush left, and bold.


% First-level headings should be in 12-point type.


% \subsection{Headings: second level}


% Second-level headings should be in 10-point type.


% \subsubsection{Headings: third level}


% Third-level headings should be in 10-point type.


% \paragraph{Paragraphs}


% There is also a \verb+\paragraph+ command available, which sets the heading in
% bold, flush left, and inline with the text, with the heading followed by 1\,em
% of space.


% \section{Citations, figures, tables, references}
% \label{others}


% These instructions apply to everyone.


% \subsection{Citations within the text}


% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.


% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}


% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+timeseries_workshop+ package:
% \begin{verbatim}
%    \PassOptionsToPackage{options}{natbib}
% \end{verbatim}


% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%    \usepackage[nonatbib]{timeseries_workshop}
% \end{verbatim}


% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous.''


% \subsection{Footnotes}


% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).


% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}


% \subsection{Figures}


% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}


% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.


% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.


% \subsection{Tables}


% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.


% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.


% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}


% \section{Final instructions}


% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.


% \section{Preparing PDF files}


% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''


% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.


% \begin{itemize}


% \item You should directly generate PDF files using \verb+pdflatex+.


% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.


% \item The IEEE has recommendations for generating PDF files whose fonts are also
%   acceptable for this workshop. Please see
%   \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}


% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.


% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%    \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%    \newcommand{\RR}{I\!\!R} %real numbers
%    \newcommand{\Nat}{I\!\!N} %natural numbers
%    \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


% \end{itemize}


% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.


% \subsection{Margins in \LaTeX{}}


% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.


% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}


% \section*{References}


% References follow the acknowledgments. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip


% {
% \small


% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.


% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.


% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
% }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
% \include{appendix}
\newpage
\appendix
\section{Additional Details: Methodology}
% \par \noindent \textbf{Limitations of Existing Methods:}
\subsection{Limitations of Existing Methods}
\label{appendix:A.1}
The first step in time-series modeling using transformer-based architectures is to learn an embedding of the time-series $\mathbf{X}$, which is then fed into the transformer encoder. Traditionally, this is done using an Embedding-layer (typically implemented using a multi-layered perceptron) as $\texttt{Embedding}:\mathbb{R}^N \mapsto \mathbb{R}^D$ that maps $\mathbf{X} \in \mathbb{R}^{T \times N}$ to the embedding  $\mathbf{H} \in \mathbb{R}^{T \times D}$, where $D$ is the embedding dimension. The Embedding layer operates on every time-step independently such that the set of variates observed at time-step $t$, $\mathbf{X}_{(t, :)}$, is considered as a single token and mapped to the embedding vector $\mathbf{h}_{t} \in \mathbb{R}^{D}$ as $\mathbf{h}_{t} = \texttt{Embedding}(\mathbf{X}_{(t, :)})$ (see Figure \ref{fig:tfi}(a)). An alternate embedding scheme was recently introduced in the framework of inverted Transformer \cite{liu2023itransformer},  where the uni-variate time-series for the $d$-th variate, $\mathbf{X}_{(:, d)}$, is considered as a single token and mapped to the embedding vector: $\mathbf{h}_{d} = \texttt{Embedding}(\mathbf{X}_{(:, d)})$ (see Figure \ref{fig:tfi}(b)). 
% Schematic representations of the embedding scheme for original transfomer and iTransformer are depicted in \textcolor{red}{Figure XX}. 
While both these embedding schemes have their unique advantages, they are unsuitable to handle time-series with arbitrary sets of missing values at every time-step. In particular, the input tokens to the Embedding layer of Transformer or iTransformer requires all components of $\mathbf{X}_{(t, :)}$ or $\mathbf{X}_{(:, d)}$ to be observed, respectively.
% Assuming that the time-series $\mathbf{X}$ has missing values, these arbitrary tokens might have missing values in them as well. 
% This in-turn would prevent us from embedding the entire token using the aforementioned $\texttt{Embedding}$ layers. 
If any of the components in these tokens are missing, we will not be able to compute their embeddings and thus will have to discard either the time-step or the variate, leading to loss of information.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth, scale=0.1]{figures/Encoding.pdf}
    \caption{Schematic of the Time-Feature Independent (TFI) Embedding of MissTSM that learns a different embedding for every combination of time-step and variate, in contrast to the time-only embeddings of Transformer \cite{vaswani2017attention} and the variate-only embeddings of iTransformers \cite{liu2023itransformer}.} 
    % MissTSM uses a novel  for every , allowing it to handle time-steps with missing values using masked cross-attention.}
    \label{fig:tfi}
\end{figure}
\subsection{2D Positional Encodings}
\label{appendix:A.2}
We add Positional Encoding vectors $\mathbf{PE}$ to the TFI embedding $\mathbf{H}^{\mathrm{TFI}}$ to obtain positionally-encoded embeddings, $\mathbf{Z} = \mathbf{PE} + \mathbf{H}^{\mathrm{TFI}}$.
% Since the TFI Embedding scheme maps each time-feature combination $\mathbf{X}_{(t, d)}$ into a higher-dimensional embedding,
Since TFI embeddings treat every time-feature combination as a token, we use a 2D-positional encoding scheme  defined as follows:

\begin{align}
    &\texttt{PE}(t, d, 2i) = \sin \big(\frac{t}{10000^{(4i/D)}} \big) ; \quad \texttt{PE}(t, d, 2i+1) = \cos \big(\frac{t}{10000^{(4i/D)}} \big), \\ 
    &\texttt{PE}(t, d, 2j+D/2) = \sin \big(\frac{d}{10000^{(4j/D)}} \big) ; \quad \texttt{PE}(t, d, 2j+1+D/2) = \cos \big(\frac{d}{10000^{(4j/D)}} \big),
\end{align}
where $t$ is the time-step, $d$ is the feature, and $i, j \in [0, D/4)$ are integers. 

\section{Experimental Setup}
\subsection{Dataset Description}
\label{appendix:B.1}
\textbf{Forecasting Dataset Details} 

\textbf{ETT.} The ETT \cite{informer} dataset captures load and oil temperature data from electricity transformers. ETTh2 includes 17,420 hourly observations, while ETTm2 comprises 69,680 15-minute observations. Both datasets span two years and contain 7 variates each. 

\textbf{Weather.} Weather \cite{weather} is a 10-minute frequency time-series dataset recorded throughout the year 2020 and consists of 21 meteorological indicators, like humidity, air temperature, etc.  

Following previous works in this area, we use a train-validation-test split of 6:2:2 for the ETT datasets and 7:1:2 for the Weather dataset. We standardized the input features by subtracting off the mean and dividing by the standard deviation for every feature over the training set. Again, following the approach used in previous works, we compute the MSE in the normalized space of all features considering all features together. 

% \textcolor{red}{\textbf{Lake Datasets}. Falling Creeks Reservoir (FCR) data extraction and processing details. Mendota data extraction and processing details.}

\textbf{Classification Dataset Details}

\textbf{Epilepsy.} Epilepsy \cite{epilepsy} contains univariate brainwaves (single-channel EEG) sampled from 500 subjects (with 11,500 samples in total), with each sample classified as having epilepsy or not (binary classification).  

\textbf{Gesture.} Gesture \cite{gesture} dataset consists of 560 samples, each having 3 variates (corresponding to the accelerometer data) and each sample corresponding to one of the 8 hand gestures (or classes) 

\textbf{EMG.} EMG \cite{emg} dataset contains 163 EMG (Electromyography) samples corresponding to 3-classes of muscular diseases. 

We make use of the following readily available data splits (train, validation, test) for each of the datasets: 
\textbf{Epilepsy} = 60 (30 samples per each class)/20 (10 samples per each class)/11420 (Train/Val/Test) 
\textbf{Gesture} = 320/20/120 (Train/Val/Test) 
\textbf{EMG} = 122/41/41 (Train/Val/Test)

\subsection{Synthetic Masked Data Generation}
\label{appendix:B.3}
\textbf{Random Masking}: We generated masks by randomly selecting data points across all variates and time-steps, assigning them as missing with a likelihood determined by p (masking fraction). The selected data points were then removed, effectively simulating missing values at random. For multiple runs, we created multiple such versions of the synthetic datasets and compared all baseline methods and MissTSM on the same datasets. 

\textbf{Periodic Masking}: We use a sine curve to generate the masking periodicity with given phase and frequency values for different features. Specifically, the time-dependent periodic probability of seeing missing values is defined as $\hat{\texttt{p}}(t) = \texttt{p} + \alpha(1-\texttt{p}){\sin(2\pi \nu \texttt{t} + \phi)}$, where, $\phi$ and $\nu$ are randomly chosen across the feature space, $\alpha$ is a scale factor, and $\texttt{p}$ is an offset term. We vary $\texttt{p}$ from low to high values to get different fractions of periodic missing values in the data. To implement this masking strategy, each feature in the dataset was assigned a unique frequency, randomly selected from the range [0.2, 0.8]. This was done to reduce bias and increase randomness in periodicity across the feature space. Additionally, the phase shift was chosen randomly from the range [0, 2$\pi$]. This was applied to each feature to offset the sinusoidal function over time. Like frequency, the phase value was different for different features. This generated a periodic pattern for the likelihood of missing data. 


\subsection{Implementation Details}
\label{appendix:B.4}
The experiments have been implemented in PyTorch using NVIDIA TITAN 24 GB GPU. The baselines have been implemented following their official code and configurations. We consider Mean Squared Error (MSE) as the metric for time-series forecasting and F1-score for the classification tasks.

\textbf{Forecasting experiments}. MissTSM was trained with the MSE loss, using the Adam \cite{adam} optimizer with a learning rate of 1e-3 during pre-training for 50 epochs and a learning rate of 1e-4 during finetuning with an early stopping counter of 3 epochs. Batch size was set to 16. All the reported missing data experiment results are obtained over 5 trials (5 different masked versions). During fine-tuning for different Prediction lengths (96, 192, 336, 720), we used the same pre-trained encoder and added a linear layer at the top of the encoder.  

\textbf{Classification experiments}. MissTSM was trained using the Adam \cite{adam} optimizer, with MSE as the loss function during pre-training and Cross-Entropy loss during fine-tuning. During fine-tuning, we plugged a 64-D linear layer at the top of the pre-trained encoder. We pre-trained and fine-tuned for 100 epochs. 

\subsection{Hyper-parameter Details}
\label{appendix:B.5}
For MissTSM, we start with the same set of hyper-parameters as reported in the SimMTM paper as initialization (see Table \ref{tab:params}), and then search for the best learning rate in factors of 10, and encoder/decoder layers in the range [2, 4]. Note that we only perform hyper-parameter tuning on 100\% data, and use the same hyper-parameters for all experiments involving the dataset, such as different missing value probabilities. 
Our goal is to show the generic effectiveness of our MissTSM framework even without any rigorous hyper-parameter optimization. Additionally, we would also like to note that our model sizes are relatively very small (number of parameters for ETTh2=28,080, Weather= 149,824, and ETTm2= 28,952), compared to other baselines such as SimMTM (ETTh2=4,694,186), iTransformer (ETTh2=254,944), and PatchTST (ETTh2=81,728).

\begin{table*}[htbp]
\caption{Hyperparameters for Forecasting and Classification Tasks}
\centering
\renewcommand{\arraystretch}{1.0}
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Task} & \textbf{\# Enc. Layers} & \textbf{\# Dec. Layers} & \textbf{\# Enc. Heads} & \textbf{\# Dec. Heads} & \textbf{Enc. Embed Dim} & \textbf{Dec. Embed Dim} \\
\midrule
\textbf{Forecasting} & & & & & & \\
ETTh2      & 2  & 2  & 8  & 4  & 8   & 32  \\
ETTm2      & 3  & 2  & 8  & 4  & 8   & 32  \\
Weather    & 2  & 2  & 8  & 4  & 64  & 32  \\
\midrule
\textbf{Classification} & & & & & & \\
All Datasets & 3  & 2  & 16 & 16 & 32  & 32  \\
\bottomrule
\end{tabular}
\label{tab:params}
}
\end{table*}

% \subsection{Hyper-parameter sensitivity}
% \label{appendix:B.6}
\begin{table}[htbp]
\caption{Hyper-parameter sensitivity of MissTSM on ETTh2 with 70\% Masking Fraction, MCAR. Best results shown in bold, second best underlined. Hyper-parameter settings used in the remainder of experiments in the paper are italicized.}
\vspace{2pt}
\centering
\small
\begin{tabular}{llllllllll}
% \cline{2-10}
\toprule
    & \multicolumn{3}{l}{Enc. Heads}                         & \multicolumn{3}{l}{Enc. Layers}                        & \multicolumn{3}{l}{Enc. Embed Dim}                     \\ \cline{2-10} 
    & 1              & 4              & 8                    & 1              & 2                       & 3           & 8                       & 16             & 32          \\ \cline{2-10} 
96  & {\ul 0.246}    & \textbf{0.245} & {\ul \textit{0.246}} & 0.249          & \textit{\textbf{0.243}} & {\ul 0.244} & \textit{\textbf{0.243}} & {\ul 0.248}    & 0.285       \\
192 & \textbf{0.261} & 0.273          & {\ul \textit{0.266}} & 0.287          & \textit{\textbf{0.267}} & {\ul 0.271} & {\ul \textit{0.267}}    & \textbf{0.266} & 0.340       \\
336 & 0.312          & \textbf{0.279} & {\ul \textit{0.310}} & \textbf{0.294} & \textit{0.392}          & {\ul 0.307} & \textit{0.392}          & \textbf{0.316} & {\ul 0.369} \\
720 & \textbf{0.326} & 0.346          & {\ul \textit{0.333}} & {\ul 0.351}    & \textit{\textbf{0.323}} & 0.355       & \textit{\textbf{0.323}} & {\ul 0.338}    & 0.446      
% \midrule
\vspace{8pt}
\end{tabular}
% \hfill
% \vspace{3pt}
\begin{tabular}{llllllllll}
% \cline{2-10}
% \toprule
    & \multicolumn{3}{l}{Dec. Heads}                         & \multicolumn{3}{l}{Dec. Layers}                        & \multicolumn{3}{l}{Dec. Embed Dim}                     \\ \cline{2-10} 
    & 1              & 4              & 8                    & 1              & 2                       & 3           & 8                       & 16             & 32          \\ \cline{2-10} 
96  & 0.261    & \textit{\textbf{0.243}} & {\ul 0.252} & 0.276          & \textit{\textbf{0.242}} & {\ul 0.248} & {\ul 0.250} & 0.259    & \textbf{\textit{0.243}}       \\
192 & 0.276 & \textbf{\textit{0.267}}          & {\ul 0.272} & \textbf{0.266}          & \textit{{\ul 0.268}} & {\ul 0.268} & \textbf{0.257}    & 0.272 & \textit{{\ul 0.267}}       \\
336 & {\ul 0.319}          & \textit{0.392} & \textbf{0.301} & \textbf{0.262} & \textit{0.352}          & {\ul 0.271} & {\ul 0.289}          & \textbf{0.266} & \textit{0.392} \\
720 & {\ul 0.324} & \textit{\textbf{0.323}}          & 0.330 & \textbf{0.323}   & \textit{0.364} & {\ul 0.341}       & {\ul 0.353} & 0.384   & \textbf{\textit{0.323}}     
% \bottomrule
\end{tabular}
\end{table}

% \subsection{Synthetic Masking Schemes}
% % \par
% To simulate varying scenarios of missing values appearing in real-world time-series datasets, we propose two synthetic masking schemes that we apply on benchmark datasets, namely missing completely at random (MCAR) masking and periodic masking, described in the following. 

% \textbf{Missing Completely At Random (MCAR) Masking:} In this scheme, we randomly mask out data from a benchmark dataset based on a uniform probability of seeing missing values at any time-feature combination. We vary the probability value to generate synthetically masked datasets with different fractions of missing values. 
% % \par

% \textbf{Periodic Masking:} Since missing values in time-series follow periodic patterns in many real-world applications (e.g., the seasonal cycles in weather and environmental datasets), we introduce a periodic masking scheme described as follows. We use a sine curve to generate the masking periodicity with given phase and frequency values for different features. Specifically, the time-dependent periodic probability of seeing missing values is defined as $\hat{\texttt{p}}(t) = \texttt{p} + \alpha(1-\texttt{p}){\sin(2\pi \nu \texttt{t} + \phi)}$, where, $\phi$ and $\nu$ are randomly chosen across the feature space, $\alpha$ is a scale factor, and $\texttt{p}$ is an offset term. We vary $\texttt{p}$ from low to high values to get different fractions of periodic missing values in the data.

\section{Additional Results}
\subsection{Embedding of 1D data}
\label{appendix:C.1}
To understand the usefulness of mapping 1D data to multi-dimensional data in TFI embedding, we present (in Table \ref{tab:embed}) an ablation comparing performances on ETTh2 with and without using high-dimensional projections in TFI Embedding under the no missing value scenario. Projecting 1D scalars independently to higher-dimensional vectors may look wasteful at the time of initialization of TFI Embedding, when the context of time and variates are not incorporated. However, it is during the cross-attention stage (using MFAA layer or later using the Transformer encoder block) that we can leverage the high-dimensional embeddings to store richer representations bringing in the context of time and variate in which every data point resides. 

From Table \ref{tab:embed}, we can see that TFI embedding with 8-dimensional vectors consistently outperform the ablation with 1D representations, empirically demonstrating the importance of high-dimensional projections in our proposed framework.

\begin{table}[htbp]
\centering
\caption{Effect of TFI Embedding with embedding size=1 and embedding size=8 under no masking scenario. Dataset=ETTh2}
\renewcommand{\arraystretch}{1.0}
\scriptsize
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l>{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm}}
\toprule
\textbf{Time Horizon} & \textbf{TFI Embedding with embedding size = 1} & \textbf{TFI Embedding with embedding size = 8} \\
\midrule
96  & 0.283  0.048 & \textbf{0.245  0.011} \\
192 & 0.285  0.078 & \textbf{0.260  0.023} \\
336 & 0.319  0.023 & \textbf{0.300  0.016} \\
720 & 0.378  0.022 & \textbf{0.334  0.032} \\
\bottomrule
\end{tabular}}
\label{tab:embed}
\end{table}

\subsection{Forecasting}
\label{appendix:C.2}
Table \ref{tab:main_table} compares the forecasting performance of MissTSM with five SOTA baseline methods in terms of the Mean Squared Error (MSE) metric on three datasets (ETTh2, ETTm2 and Weather) with varying forecasting horizons, imputation techniques (Spline and SAITS), and masking schemes. We provide the mean and standard deviations over 5 different samples of the masking schemes. We choose a missing value probability of 60\% for MCAR masking and 70\% for periodic masking to simulate scenarios with varying (and often extreme) amounts of missing information. 
We can see that in the no masking experiment, the performance of all methods (with the exception of AutoFormer) are mostly comparable to each other across all three datasets, with MissTSM and PatchTST having a slight edge on the ETTh2/ETTm2 and Weather datasets, respectively. 
For the MCAR masking experiments, we observe a trend across all the datasets that the MissTSM framework performs slightly better than the baselines for longer-term forecasting (such as forecasting horizon of 720), and comparable to the best-performing baselines on other forecasting horizons. For the Periodic masking experiment, we can see that MissTSM is consistently better than the baselines for ETTh2 dataset, while for the ETTm2 and Weather datasets, the forecasting performance is comparable to the other baselines. These results demonstrate the effectiveness of our proposed MissTSM framework to circumvent the need for explicit imputation of missing values while achieving comparable  performance as SOTA. 

% \sout{Additionally, we observe on the three datasets that the baselines with SAITS-based imputation is comparable or slightly better than the corresponding spline interpolation counterpart.}

By being imputation-free, MissTSM  does not suffer from the propagation of imputation errors  (from the imputation scheme) to forecasting errors  (from the time-series models). In Appendix Figure \ref{fig:err_propagation}, we provide empirical evidence of this error propagation, where we see a positive correlation between imputation errors and forecasting errors of baseline methods, indicating that reducing imputation errors is crucial for improving forecasting accuracy. This finding underscores the limitations of traditional two-stage approaches and suggests that using more sophisticated imputation models is necessary to achieve lower forecasting errors. We also report the computation time of SimMTM (with Spline and SAITS) and MissTSM in Appendix Table \ref{tab:computation}, where we demonstrate that MissTSM is significantly faster as it does not involve any expensive interpolations as an additional advantage.

\begin{table}[!t]
  \caption{Comparing forecasting performance of baseline methods using mean squared error (MSE) as the evaluation metric under no masking, MCAR masking, and periodic masking. For every dataset, we consider multiple forecasting horizons, $T \in \{96, 192, 336, 720\}$. Results are color-coded as \colorbox{blue!35} {Best}, \colorbox{blue!15} {Second best}. We report the mean and standard deviations (in brackets) across 5 random sampling of the masking schemes. Subscript $_{SP}$ refer to Spline and $_{SA}$ refer to SAITS}
  \vspace{-2ex}
  \label{tab:main_table}
  \renewcommand{\arraystretch}{1.4}
  \setlength{\tabcolsep}{3pt}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \Large
    \begin{tabular}{cllllrlllrllllr}
                                                                                                 &                                      &                                      &                                             &                                      & \multicolumn{1}{l}{}                                      &                                                   &                                                   &                                             & \multicolumn{1}{l}{}                                          &                                             &                                          &                                          &                                          \\ \hline
      &                                       & \multicolumn{4}{c|}{\textbf{ETTh2}}                                                                                                                                                    & \multicolumn{4}{c|}{\textbf{ETTm2}}                                                                                                                                                          & \multicolumn{4}{c|}{\textbf{Weather}}   &  
      \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}{\textbf{Avg}} \\ {\textbf{Rank}}\end{tabular}}}                                                                                                                          
      \\ 
      \cline{3-14}
      \multicolumn{1}{l}{}                                                                       &                                      & \textbf{96}                          & \textbf{192}                                & \textbf{336}                         & \multicolumn{1}{l|}{\textbf{720}}                         & \textbf{96}                                       & \textbf{192}                                      & \textbf{336}                                & \multicolumn{1}{l|}{\textbf{720}}                             & \textbf{96}                                 & \textbf{192}                             & \textbf{336}                             & \multicolumn{1}{l|}{\textbf{720} }                            
      \\ 
      \cline{1-15}
      \multicolumn{1}{c|}{\multirow{6}{*}{\rotatebox[origin=c]{90}{\textbf{No Masking}}}}        & \textbf{MissTSM}                     & \cellcolor{blue!35}${0.255}$         & \cellcolor{blue!35}${0.234}$                & \cellcolor{blue!35}${0.316}$         & \multicolumn{1}{l|}{\cellcolor{blue!35}${0.305}$}         & $0.183$                                           & \cellcolor{blue!35}${0.209}$                      & \cellcolor{blue!35}${0.261}$                & \multicolumn{1}{l|}{\cellcolor{blue!35}${0.311}$}             & $0.164$                                     & $0.210$                                  & \cellcolor{blue!15}$0.254$               & \multicolumn{1}{l|}{$0.324$}          & \multicolumn{1}{c}{$1.9$}                        \\
      \multicolumn{1}{c|}{}                                                                      & \textbf{SimMTM}                      & $0.295$                              & $0.356$                                     & $0.375$                              & \multicolumn{1}{l|}{$0.404$}                              & $0.172$                                           & $0.223$                                           & $0.282$                  & \multicolumn{1}{l|}{$0.374$}                                  & \cellcolor{blue!15}$0.163$                  & \cellcolor{blue!15}$0.203$               & $0.255$                                  & \multicolumn{1}{l|}{$0.326$} & \multicolumn{1}{c}{$2.9$}
      
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{PatchTST}                    & \cellcolor{blue!15}$0.274$           & \cellcolor{blue!15}$0.338$                  & \cellcolor{blue!15}0.330             & \multicolumn{1}{l|}{\cellcolor{blue!15}$0.378$}           & \cellcolor{blue!35}$0.164$                        & \cellcolor{blue!15}$0.220$                        & \cellcolor{blue!15}$0.277$                                     & \multicolumn{1}{l|}{\cellcolor{blue!15}$0.367$}               & \cellcolor{blue!35}${0.151}$                & \cellcolor{blue!35}${0.196}$             & \cellcolor{blue!35}${0.249}$             & \multicolumn{1}{l|}{\cellcolor{blue!35}${0.319}$}             
      & \multicolumn{1}{c}{$1.7$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{AutoFormer}                  & $0.501$                              & $0.516$                                     & $0.565$                              & \multicolumn{1}{l|}{$0.462$}                              & $0.352$                                           & $0.337$                                           & $0.494$                                     & \multicolumn{1}{l|}{$0.474$}                                  & $0.306$                                     & $0.434$                                  & $0.437$                                  & \multicolumn{1}{l|}{$0.414$}                                  & \multicolumn{1}{c}{$5.9$}
      
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{DLinear}                     & $0.288$                              & $0.383$                                     & $0.447$                              & \multicolumn{1}{l|}{$0.605$}                              & \cellcolor{blue!15}$0.168$                        & $0.224$                                           & $0.299$                                     & \multicolumn{1}{l|}{$0.414$}                                  & \textbf{$0.175$}                            & \textbf{$0.219$}                         & $0.265$                                  & \multicolumn{1}{l|}{\cellcolor{blue!15}$0.323$}               & \multicolumn{1}{c}{$4.1$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{iTransformer}                & $0.304$                              & $0.392$                                     & $0.425$                              & \multicolumn{1}{l|}{$0.415$}                              & $0.176$                                           & $0.246$                                           & $0.289$                                     & \multicolumn{1}{l|}{$0.379$}                                  & \cellcolor{blue!15}{$0.163$}                & \cellcolor{blue!15}$0.203$               & $0.256$                                  & \multicolumn{1}{l|}{$0.326$}                                  & \multicolumn{1}{c}{$4.5$}
      
      \\ 
      
      \cline{2-15}
      
      \multicolumn{1}{c|}{\multirow{11}{*}{\rotatebox[origin=c]{90}{\textbf{MCAR Masking}}}}     & \multicolumn{1}{l}{\textbf{MissTSM}} & $\cellcolor{blue!35}{0.243_{0.006}}$ & $\cellcolor{blue!35}0.259_{0.002}$          & $\cellcolor{blue!35}0.283_{0.009}$   & \multicolumn{1}{r|}{$\cellcolor{blue!35}0.329_{0.011}$}   & $0.224_{0.005}$                                   & $0.253_{0.009}$                                   & \cellcolor{blue!15}$0.293_{0.019}$                             & \multicolumn{1}{r|}{\cellcolor{blue!35}$0.316_{0.014}$}       & $0.191_{0.003}$                             & $0.234_{0.006}$                          & $0.281_{0.004}$                          & \multicolumn{1}{l|}{\cellcolor{blue!35}$0.322_{0.008}$}       
      & \multicolumn{1}{c}{$2.7$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{SimMTM$_{\text{SP}}$}        & $0.309_{0.001}$                      & \textbf{$0.372_{0.005}$}                    & \textbf{$0.396_{0.01}$}             & \multicolumn{1}{r|}{\textbf{$0.418_{0.008}$}}             & $0.185_{0.001}$                                   & \cellcolor{blue!15}$0.243_{0.002}$                                   & { $0.298_{0.001}$}                       & \multicolumn{1}{r|}{\textbf{$0.388_{0.005}$}}                 & $0.203_{0.009}$                            & $0.242_{0.010}$                          & $0.284_{0.008}$                          & \multicolumn{1}{l|}{\textbf{$0.386_{0.008}$}}                 
      & \multicolumn{1}{c}{$5.0$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{SimMTM$_{\text{SA}}$}        & \textbf{$0.457_{0.06}$}             & \textbf{$0.510_{0.061}$} & \textbf{$0.503_{0.055}$}             & \multicolumn{1}{r|}{$0.472_{0.066}$}                      & \textbf{$0.287_{0.037}$}       & { \textbf{$0.320_{0.035}$}} & \textbf{$0.342_{0.017}$} & \multicolumn{1}{r|}{{ $0.413_{0.014}$}} & { $0.187_{0.002}$}    & { $0.240_{0.001}$}                    & \textbf{$0.280_{0.001}$}                 & \multicolumn{1}{l|}{$0.385_{0.004}$}                          & \multicolumn{1}{c}{$6.2$}
      
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{PatchTST$_{\text{SP}}$}      & \textbf{$0.290_{0.003}$}             & \cellcolor{blue!15}\textbf{$0.355_{0.003}$}                    & $\cellcolor{blue!15}0.345_{0.003}$   & \multicolumn{1}{r|}{$\cellcolor{blue!15}0.390_{0.003}$}   & { \textbf{\cellcolor{blue!35}$0.169_{0.001}$}}                    & \cellcolor{blue!35}\textbf{$0.228_{0.001}$}                          & { $\cellcolor{blue!35}0.286_{0.001}$}    & \multicolumn{1}{r|}{\cellcolor{blue!15}$0.378_{0.001}$}                          & { \cellcolor{blue!15}$0.183_{0.009}$}                       & \cellcolor{blue!15}\textbf{$0.226_{0.009}$}                 & { $0.277_{0.009}$}                    & \multicolumn{1}{l|}{$0.339_{0.008}$}                          
      & \multicolumn{1}{c}{$2.1$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{PatchTST$_{\text{SA}}$}      & \textbf{$0.440_{0.059}$}             & $0.484_{0.057}$                              & $0.434_{0.059}$                      & \multicolumn{1}{r|}{$0.436_{0.075}$}                      & { \textbf{$0.324_{0.05}$}} & { $0.362_{0.045}$}                             & $0.410_{0.049}$                             & \multicolumn{1}{r|}{$0.462_{0.047}$}                          & \textbf{$\cellcolor{blue!35}0.175_{0.002}$} & { $\cellcolor{blue!35}0.211_{0.000}$} & $\cellcolor{blue!35}0.264_{0.002}$       & \multicolumn{1}{l|}{$0.335_{0.001}$}      
      & \multicolumn{1}{c}{$4.6$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{AutoFormer$_{\text{SP}}$}    & $0.559_{0.05}$                      & $0.628_{0.101}$                             & $0.525_{0.037}$                      & \multicolumn{1}{r|}{$0.550_{0.143}$}                      & { $0.280_{0.006}$}                             & $0.390_{0.158}$                                   & $0.360_{0.018}$                             & \multicolumn{1}{r|}{$0.475_{0.033}$}                          & { $0.321_{0.008}$}                       & \textbf{$0.413_{0.013}$}                 & $0.508_{0.036}$                          & \multicolumn{1}{l|}{$0.467_{0.032}$}                          
      & \multicolumn{1}{c}{$8.9$}
      \\

      
      \multicolumn{1}{c|}{}                                                                      & \textbf{AutoFormer$_{\text{SA}}$}    & $0.767_{0.126}$                      & $0.526_{0.06}$                             & $0.550_{0.019}$                      & \multicolumn{1}{r|}{$0.449_{0.010}$}                      & $0.610_{0.312}$                                   & $0.850_{0.365}$                                   & $0.615_{0.151}$                             & \multicolumn{1}{r|}{$1.045_{0.262}$}                          & \textbf{$0.353_{0.013}$}                    & $0.413_{0.006}$                           & $0.474_{0.028}$                           & \multicolumn{1}{l|}{$0.504_{0.049}$}                          
      & \multicolumn{1}{c}{$10.2$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{DLinear$_{\text{SP}}$}       & \cellcolor{blue!15}$0.296_{0.003}$                      & $0.401_{0.018}$                             & $0.445_{0.006}$                      & \multicolumn{1}{r|}{$0.607_{0.013}$}                      & $0.458_{0.169}$                                   & \cellcolor{blue!35}$0.228_{0.001}$                                   & $0.302_{0.000}$                             & \multicolumn{1}{r|}{$0.531_{0.144}$}                          & $0.205_{0.007}$                             & $0.241_{0.007}$                          & $0.282_{0.008}$                          & \multicolumn{1}{l|}{$0.373_{0.009}$}                          
      & \multicolumn{1}{c}{$6.5$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{DLinear$_{\text{SA}}$}       & $0.454_{0.053}$   & $0.514_{0.053}$                             & $0.542_{0.064}$                      & \multicolumn{1}{r|}{$0.680_{0.084}$}                      & $0.330_{0.065}$                                   & \textbf{$0.365_{0.062}$}       & $0.427_{0.058}$                             & \multicolumn{1}{r|}{$0.538_{0.063}$}                          & $0.190_{0.001}$                             & $0.233_{0.000}$                          & $0.276_{0.000}$                          & \multicolumn{1}{l|}{\cellcolor{blue!15}$0.333_{0.001}$}                          
      & \multicolumn{1}{c}{$6.8$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{iTransformer$_{\text{SP}}$}  & $0.313_{0.004}$                      & $0.394_{0.014}$                             & $0.436_{0.005}$                      & \multicolumn{1}{r|}{$0.429_{0.005}$}                      & \cellcolor{blue!15}\textbf{$0.178_{0.001}$}                          & \cellcolor{blue!15}$0.243_{0.0004}$                                   & \cellcolor{blue!15}$0.293_{0.001}$                             & \multicolumn{1}{r|}{$0.384_{0.008}$}                          & $0.197_{0.006}$                             & $0.260_{0.007}$                          & $0.315_{0.008}$                          & \multicolumn{1}{l|}{$0.349_{0.006}$}                          
      & \multicolumn{1}{c}{$4.9$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{iTransformer$_{\text{SA}}$}  & $0.492_{0.058}$                       & $0.545_{0.048}$                             & $0.579_{0.049}$                      & \multicolumn{1}{r|}{$0.540_{0.094}$}                      & $0.369_{0.080}$                                   & $0.432_{0.083}$                                   & $0.482_{0.083}$                             & \multicolumn{1}{r|}{$0.541_{0.075}$}                          & $0.191_{0.002}$                             & $0.228_{0.002}$                          & \cellcolor{blue!15}$0.273_{0.002}$                          & \multicolumn{1}{l|}{$0.348_{0.003}$}                          
      & \multicolumn{1}{c}{$7.7$}
      \\ 
      
      \cline{2-15}
      
      \multicolumn{1}{c|}{\multirow{11}{*}{\rotatebox[origin=c]{90}{\textbf{Periodic Masking}}}} & \multicolumn{1}{l}{\textbf{MissTSM}} & \cellcolor{blue!35}${0.246}_{0.018}$ & \cellcolor{blue!35}${0.263}_{0.017}$        & \cellcolor{blue!35}${0.301}_{0.042}$ & \multicolumn{1}{r|}{\cellcolor{blue!35}${0.353}_{0.015}$} & ${0.227}_{0.006}$                                 & $0.249_{0.006}$                                   & \cellcolor{blue!35}{${0.282}_{0.011}$}      & \multicolumn{1}{r|}{\cellcolor{blue!35}{${0.337}_{0.036}$}}   & $0.212_{0.007}$                             & $0.256_{0.008}$                          & $0.313_{0.009}$                          & \multicolumn{1}{l|}{$0.379_{0.019}$}                          
      & \multicolumn{1}{c}{$4.1$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{SimMTM$_{\text{SP}}$}        & \textbf{$0.372_{0.122}$}             & \textbf{$0.469_{0.198}$}                    & \textbf{$0.496_{0.198}$}             & \multicolumn{1}{r|}{$0.510_{0.200}$}                      & $0.192_{0.010}$                                   & \textbf{$0.247_{0.009}$}                          & \textbf{$0.301_{0.008}$}                    & \multicolumn{1}{r|}{$0.391_{0.008}$}                          & $0.182_{0.004}$                             & $0.248_{0.003}$                          & $0.291_{0.009}$                          & \multicolumn{1}{l|}{$0.344_{0.005}$}                           
      & \multicolumn{1}{c}{$4.7$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{SimMTM$_{\text{SA}}$}        & \textbf{$0.591_{0.132}$}             & \textbf{$0.666_{0.152}$}                    & $0.681_{0.182}$                      & \multicolumn{1}{r|}{$0.667_{0.222}$}                      & \textbf{$0.389_{0.071}$}                          & \textbf{$0.409_{0.054}$}                          & $0.436_{0.076}$                             & \multicolumn{1}{r|}{{ $0.505_{0.055}$}} & { \cellcolor{blue!15}$0.178_{0.002}$}    & { \cellcolor{blue!15}$0.214_{0.001}$} & \cellcolor{blue!35}$0.261_{0.001}$                          & \multicolumn{1}{l|}{$0.354_{0.003}$}                          
      & \multicolumn{1}{c}{$6.0$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{PatchTST$_{\text{SP}}$}      & \cellcolor{blue!15}$0.328_{0.047}$   & \cellcolor{blue!15}$0.389_{0.040}$          & \cellcolor{blue!15}$0.381_{0.050}$   & \multicolumn{1}{r|}{\cellcolor{blue!15}$0.426_{0.058}$}   & { \cellcolor{blue!35}{$0.174_{0.004}$}}        & \cellcolor{blue!15}$0.231_{0.003}$                                   & { \cellcolor{blue!15}$0.289_{0.004}$}    & \multicolumn{1}{r|}{\cellcolor{blue!15}$0.381_{0.004}$}                          & { $0.181_{0.004}$}                       & $0.227_{0.005}$                          & $0.267_{0.005}$                          & \multicolumn{1}{l|}{$0.346_{0.003}$}                          
      & \multicolumn{1}{c}{$2.4$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{PatchTST$_{\text{SA}}$}      & $0.581_{0.120}$                     & $0.620_{0.132}$                            & $0.592_{0.170}$                      & \multicolumn{1}{r|}{$0.644_{0.230}$}                      & $0.423_{0.054}$                                   & { $0.457_{0.042}$}                             & $0.493_{0.037}$                             & \multicolumn{1}{r|}{$0.527_{0.027}$}                          & {\cellcolor{blue!35}${0.171}_{0.002}$}      & {\cellcolor{blue!35}${0.212}_{0.001}$}   & \cellcolor{blue!15}{${0.263}_{0.005}$}   & \multicolumn{1}{l|}{\cellcolor{blue!15}$0.334_{0.001}$} 
      & \multicolumn{1}{c}{$5.2$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{Autoformer$_{\text{SP}}$}    & $0.482_{0.041}$                      & $0.685_{0.165}$                             & $0.621_{0.166}$                      & \multicolumn{1}{r|}{$0.546_{0.035}$}                      & { $0.329_{0.109}$}                             & $0.315_{0.010}$                                   & $0.398_{0.090}$                             & \multicolumn{1}{r|}{$0.456_{0.021}$}                          & {$0.333_{0.0176}$}                           & {$0.387_{0.035}$}                        & { $0.406_{0.025}$}                    & \multicolumn{1}{l|}{$0.453_{0.016}$}                          
      & \multicolumn{1}{c}{$7.5$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{Autoformer$_{\text{SA}}$}    & $1.415_{0.807}$                      & $0.810_{0.269}$                             & $1.364_{0.760}$                      & \multicolumn{1}{r|}{$0.820_{0.467}$}                      & $1.303_{1.278}$                                   & $0.933_{0.444}$                                   & $1.788_{0.538}$                              & \multicolumn{1}{r|}{$0.809_{0.431}$}                          & {$0.335_{0.009}$}                           & { $0.387_{0.017}$}                    & $0.435_{0.035}$                          & \multicolumn{1}{l|}{$0.467_{0.017}$}                          
      & \multicolumn{1}{c}{$10.8$}
      \\

      \multicolumn{1}{c|}{}                                                                      & \textbf{DLinear$_{\text{SP}}$}       & $0.346_{0.069}$                      & $0.475_{0.108}$                             & $0.477_{0.044}$                      & \multicolumn{1}{r|}{$0.649_{0.068}$}                      & $0.327_{0.188}$                                   & \cellcolor{blue!35}$0.230_{0.002}$          & $0.305_{0.003}$                             & \multicolumn{1}{r|}{$0.473_{0.038}$}                          & { $0.215_{0.018}$}                       & $0.244_{0.013}$                          & $0.284_{0.008}$                          & \multicolumn{1}{l|}{$0.339_{0.007}$}                          
      & \multicolumn{1}{c}{$5.0$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{DLinear$_{\text{SA}}$}       & $0.605_{0.109}$                      & $0.674_{0.11}$                            & $0.728_{0.138}$                      & \multicolumn{1}{r|}{$0.911_{0.158}$}                      & { {$0.447_{0.049}$}}        & ${0.475}_{0.043}$              & $0.523_{0.042}$                             & \multicolumn{1}{r|}{$0.626_{0.032}$}                          & $0.190_{0.001}$                             & $0.233_{0.000}$                          & $0.276_{0.001}$ & 
      \multicolumn{1}{l|}{\cellcolor{blue!35}${0.333}_{0.001}$}     
      & \multicolumn{1}{c}{$7.4$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{iTransformer$_{\text{SP}}$}  & $0.358_{0.070}$                      & $0.435_{0.067}$                             & $0.488_{0.096}$                      & \multicolumn{1}{r|}{$0.497_{0.119}$}                      & \cellcolor{blue!15}\textbf{$0.180_{0.005}$}                          & $0.245_{0.006}$                                   & $0.296_{0.007}$                             & \multicolumn{1}{r|}{$0.384_{0.007}$}                          & $0.197_{0.009}$                             & { $0.233_{0.006}$}                    & \textbf{$0.288_{0.01}$}                  & \multicolumn{1}{l|}{$0.351_{0.010}$}    
      & \multicolumn{1}{c}{$4.2$}
      \\
      
      \multicolumn{1}{c|}{}                                                                      & \textbf{iTransformer$_{\text{SA}}$}  & $0.691_{0.143}$                      & $0.715_{0.140}$                             & $0.763_{0.153}$                      & \multicolumn{1}{r|}{$0.773_{0.201}$}                      & $0.512_{0.055}$                                   & $0.578_{0.052}$                                   & $0.662_{0.05}$                             & \multicolumn{1}{r|}{$0.680_{0.029}$}                          & { $0.194_{0.001}$}                       & \textbf{$0.229_{0.004}$}                 & $0.274_{0.002}$                          & \multicolumn{1}{l|}{$0.350_{0.003}$}   
      & \multicolumn{1}{c}{$8.2$}
      \\ 
      \hline
      % \cline{1-15}
    \end{tabular}}
\end{table}

% \begin{table*}[hbtp]
% \caption{Comparing forecasting performance of baseline methods under MCAR and Periodic Masking. Best performing model in dark blue and the $2^\text{nd}$-best model in light blue}
% \label{tab:main_table}
% \renewcommand{\arraystretch}{1.5}
% \scriptsize
% \resizebox{\textwidth}{!}{\begin{tabular}{llllllllllll}
% \hline
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{5}{c}{\cellcolor[HTML]{FFFFFF}MCAR Masking}                                                                                                                                                                                                                                          & \multicolumn{5}{c}{\cellcolor[HTML]{FFFFFF}Periodic Masking}                                                                                                                                                                                             \\ \cline{3-12} 
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}}                          & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}SimMTM}                                                     & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}PatchTST}                                                             & \cellcolor[HTML]{FFFFFF}                          & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}SimMTM}                                                     & \multicolumn{2}{c}{\cellcolor[HTML]{FFFFFF}PatchTST}                                        \\ \cline{4-7} \cline{9-12} 
% \rowcolor[HTML]{FFFFFF} 
%                                                   &                                                  & \multicolumn{1}{c}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}MissTSM}} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS}           & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}MissTSM} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}SAITS} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}Spline} & SAITS                                  \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & \cellcolor[HTML]{9698ED}0.266  0.0146                                & 0.349  0.0191                                     & \cellcolor[HTML]{DAE8FC}0.267  0.0308            & 0.309  0.0087                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.413  0.0339} & \cellcolor[HTML]{9698ED}0.268  0.0151            & 0.646  0.1716                                     & \cellcolor[HTML]{DAE8FC}0.319  0.0659            & 0.492  0.0825                                     & 0.599  0.2110                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & \cellcolor[HTML]{9698ED}0.283  0.0123                                & 0.414  0.0232                                     & \cellcolor[HTML]{DAE8FC}0.360  0.0465            & 0.370  0.0076                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.495  0.0376} & \cellcolor[HTML]{9698ED}0.295  0.0298            & 0.729  0.1913                                     & \cellcolor[HTML]{DAE8FC}0.408  0.0865            & 0.510  0.0578                                     & 0.672  0.2113                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{9698ED}0.287  0.0142                                & 0.435  0.0297                                     & 0.394  0.0542                                    & \cellcolor[HTML]{DAE8FC}0.352  0.0062             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.492  0.0450} & \cellcolor[HTML]{9698ED}0.319  0.0185            & 0.751  0.2034                                     & \cellcolor[HTML]{DAE8FC}0.436  0.1056            & 0.469  0.0423                                     & 0.671  0.2260                         \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}ETTh2}   & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.323  0.0125                                & 0.447  0.0250                                     & 0.413   0.0464                                   & \cellcolor[HTML]{DAE8FC}0.394  0.0089             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.564  0.0574} & \cellcolor[HTML]{9698ED}0.356  0.0310            & 0.735  0.1836                                     & \cellcolor[HTML]{DAE8FC}0.443  0.0746            & 0.502  0.0367                                     & 0.723  0.2251                         \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & 0.222  0.0058                                                        & 0.170  0.0030                                     & \cellcolor[HTML]{9698ED}0.166  0.0082            & 0.170  0.0005                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.168  0.0025} & 0.241  0.0105                                    & \cellcolor[HTML]{DAE8FC}0.166  0.0050             & \cellcolor[HTML]{9698ED}0.156  0.0155            & 0.175  0.0032                                     & 0.187  0.0038                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & 0.274  0.0023                                                        & 0.230  0.0031                                     & \cellcolor[HTML]{9698ED}0.223  0.0115            & 0.228  0.0005                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}0.225  0.0035} & 0.260  0.0109                                    & \cellcolor[HTML]{DAE8FC}0.224  0.0050             & \cellcolor[HTML]{9698ED}0.210  0.0207            & 0.233  0.0021                                     & 0.244  0.0050                         \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{DAE8FC}0.279  0.0018                                & 0.286  0.0042                                     & \cellcolor[HTML]{9698ED}0.276  0.0111            & 0.286  0.0009                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.282  0.0051} & \cellcolor[HTML]{DAE8FC}0.279  0.0333            & 0.281  0.0056                                     & \cellcolor[HTML]{9698ED}0.262  0.0242            & 0.291  0.0024                                     & 0.300  0.0040                         \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}ETTm2}   & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.316  0.0144                                & 0.377  0.0046                                     & \cellcolor[HTML]{DAE8FC}0.369  0.0128            & 0.378  0.0014                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.373  0.0062} & \cellcolor[HTML]{9698ED}0.321  0.0139            & 0.372  0.0061                                     & 0.353  0.0346                                    & 0.382  0.0020                                     & 0.394  0.0064                         \\ \hline
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}96}  & 0.191  0.0033                                                        & 0.200  0.0475                                     & 0.178  0.0025                                    & \cellcolor[HTML]{DAE8FC}0.189  0.0095             & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.172  0.0010} & 0.199  0.0021                                    & 0.208  0.0648                                     & \cellcolor[HTML]{DAE8FC}0.176  0.0019            & 0.190  0.0110                                     & \cellcolor[HTML]{9698ED}0.171  0.0018 \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}192} & \cellcolor[HTML]{DAE8FC}0.227  0.0068                                & 0.250  0.0488                                     & 0.230  0.0043                                    & 0.227  0.0094                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.215  0.0008} & 0.238  0.0059                                    & 0.258  0.0654                                     & \cellcolor[HTML]{DAE8FC}0.225  0.0019            & 0.227  0.0109                                     & \cellcolor[HTML]{9698ED}0.214  0.0015 \\
% \rowcolor[HTML]{FFFFFF} 
% \cellcolor[HTML]{FFFFFF}                          & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}336} & \cellcolor[HTML]{DAE8FC}0.282  0.0048                                & 0.304  0.0484                                     & 0.282  0.0079                                    & 0.275  0.0090                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{9698ED}0.264  0.0008} & 0.286  0.0080                                    & 0.313  0.0639                                     & 0.278  0.0019                                    & \cellcolor[HTML]{DAE8FC}0.274  0.0102             & \cellcolor[HTML]{9698ED}0.263  0.0018 \\
% \rowcolor[HTML]{FFFFFF} 
% \multirow{-4}{*}{\cellcolor[HTML]{FFFFFF}Weather} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}720} & \cellcolor[HTML]{9698ED}0.322  0.0088                                & 0.377  0.0449                                     & 0.355  0.0065                                    & 0.345  0.0089                                     & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.335  0.0009} & \cellcolor[HTML]{DAE8FC}0.339  0.0112            & 0.388  0.0610                                     & 0.352  0.0015                                    & 0.345  0.0099                                     & \cellcolor[HTML]{9698ED}0.334  0.0023 \\ \hline
% \end{tabular}}
% \end{table*}
% \vspace{-10pt}
% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}[b]{\textwidth}
%         \centering
%         \begin{subfigure}[b]{0.44\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/rebuttal_figures/fig4_ettm2_t_336_mcar.pdf}
%             \caption{ETTm2, T=336}
%             \label{fig:first_subfigure_4}
%             % \vspace{-20pt}
%         \end{subfigure}
%         \begin{subfigure}[b]{0.44\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/rebuttal_figures/fig4_ettm2_t_720_mcar.pdf}
%             \caption{ETTm2, T=720}
%             \label{fig:second_subfigure_4}
%             % \vspace{-20pt}
%         \end{subfigure}
%         \caption{Effect of Random masking on forecasting performance under different prediction horizons}
%         \label{fig:fig4}
%     \end{minipage}
%     % \hspace{0.1\textwidth}
% \end{figure}
% \vspace{10cm}
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=0.95\linewidth]{figures/fig4_MCAR_ETTh2_T720.pdf}
        \caption{ETTh2, MCAR}
        \label{fig:fig4_MCAR_ETTh2_T720}
    \end{subfigure}
     \begin{subfigure}{0.24\textwidth}
     \centering
         \includegraphics[width=\linewidth]{figures/fig4_MCAR_ETTm2_T720.pdf}
        \caption{ETTm2, MCAR}
        \label{fig:fig4_MCAR_ETTm2_T720}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
     \centering
         \includegraphics[width=0.95\linewidth]{figures/fig4_Periodic_ETTh2_T720.pdf}
        \caption{ETTh2, Periodic}
        \label{fig:fig4_Periodic_ETTh2_T720}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
     \centering
         \includegraphics[width=\linewidth]{figures/fig4_Periodic_ETTm2_T720.pdf}
        \caption{ETTm2, Periodic}
        \label{fig:fig4_Periodic_ETTm2_T720}
    \end{subfigure}
    \caption{\textbf{Multiple Time-series Baselines.} Performance comparison between MCAR and Periodic masking with multiple TS Baselines imputed with SAITS. TS Baselines considered: Autoformer \cite{autoformer}, PatchTST \cite{patchtst}, iTransformer \cite{liu2023itransformer}, DLinear \cite{dlinear}, SimMTM \cite{simmtm}}
    \label{fig:fig4}
    \vspace{-4ex}
\end{figure}


\begin{figure}[t]
    \centering
    \begin{subfigure}{0.27\textwidth}
    \centering
        \includegraphics[width=\linewidth]{figures/fig5_MCAR_ETTh2_p6.pdf}
        \caption{ETTh2 60\% MCAR}
        \label{fig:fig5_MCAR_ETTh2_p6}
    \end{subfigure}
     \begin{subfigure}{0.27\textwidth}
     \centering
         \includegraphics[width=\linewidth]{figures/fig5_MCAR_ETTm2_p6.pdf}
        \caption{ETTm2 60\% MCAR}
        \label{fig:fig5_MCAR_ETTm2_p6}
    \end{subfigure}
    \begin{subfigure}{0.27\textwidth}
     \centering
         \includegraphics[width=\linewidth]{figures/fig5_MCAR_weather_p6.pdf}
        \caption{Weather 60\% MCAR}
        \label{fig:fig5_MCAR_weather_p6}
    \end{subfigure}
    \caption{Forecasting performance with the horizon length \textit{T} $\in$ {96, 192, 336, 720} and fixed lookback length S = 336. Baseline models are imputed with SAITS}
    \label{fig:fig5}
\end{figure}

% \begin{figure}[hbtp]
%     \centering
%     \begin{minipage}[b]{\textwidth}
%         \centering
%         \begin{subfigure}[b]{0.32\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/rebuttal_figures/imp_baselines_ETTh2_t_192_mcar.pdf}
%             \caption{ETTh2, T=192}
%             \label{fig:first_subfigure_imp_baseline}
%             % \vspace{-20pt}
%         \end{subfigure}
%         \begin{subfigure}[b]{0.32\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/rebuttal_figures/imp_baselines_ETTh2_t_720_mcar.pdf}
%             \caption{ETTh2, T=720}
%             \label{fig:second_subfigure_imp_baseline}
%             % \vspace{-20pt}
%         \end{subfigure}
%         \begin{subfigure}[b]{0.33\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/rebuttal_figures/imp_baselines_weather_t_720_mcar.pdf}
%             \caption{Weather, T=720}
%             \label{fig:third_subfigure_imp_baseline}
%             % \vspace{-20pt}
%         \end{subfigure}
%         \caption{\textbf{Multiple Imputation Baselines}. Performance comparison across multiple imputation models on ETTh2 and Weather. Imputation models considered: BRITS \cite{cao2018brits}, kNN, Spline, SAITS \cite{saits}. TS Baselines: iTransformer \cite{liu2023itransformer} and PatchTST \cite{patchtst}}
%         \label{fig:imp_baselines}
%     % \begin{minipage}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{figures/rebuttal_figures/imp_baselines_weather_t_720_mcar.pdf}
%     %     \captionof{figure}{\small{Performance comparison across multiple imputation models on weather, MCAR}}
%     %     \label{fig:cls_imbalance}
%     \end{minipage}
%     % \begin{minipage}[b]{0.24\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{figures/rebuttal_figures/Weather_Error_Plot.pdf}
%     %     \captionof{figure}{\small{Imputation error vs Forecasting error across 5 trials for 4 missing fractions, 0.6, 0.7, 0.8, 0.9}}
%     %     \label{fig:cls_imbalance}
%     % \end{minipage}
    
% \end{figure}


\begin{figure}[t]
    \centering
    \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=0.85\linewidth]{figures/fig6_itrans_MCAR_ETTh2_720.pdf}
        \caption{iTransformer, ETTh2}
        \label{fig:fig6_itrans_MCAR_ETTh2_720}
    \end{subfigure}
     \begin{subfigure}{0.24\textwidth}
     \centering
         \includegraphics[width=0.95\linewidth]{figures/fig6_itrans_MCAR_weather_720.pdf}
        \caption{iTransformer, Weather}
        \label{fig:fig6_itrans_MCAR_weather_720}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
     \centering
         \includegraphics[width=0.84\linewidth]{figures/fig6_ptst_MCAR_ETTh2_720.pdf}
        \caption{PatchTST, ETTh2}
        \label{fig:fig6_ptst_MCAR_ETTh2_720}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
     \centering
         \includegraphics[width=\linewidth]{figures/fig6_ptst_MCAR_weather_720.pdf}
        \caption{PatchTST, Weather}
        \label{fig:fig6_ptst_MCAR_weather_720}
    \end{subfigure}
    \caption{\textbf{Multiple Imputation Baselines}. Performance comparison across multiple imputation models. Imputation models considered: kNN, Spline, SAITS \cite{saits}. TS Baselines: iTransformer \cite{liu2023itransformer} and PatchTST \cite{patchtst}}
    \label{fig:fig6}
\end{figure}


% \newpage
\subsection{Classification}
\label{appendix:C.3}
Full classification results (on all the datasets) are shown in Figure \ref{fig:clf_full}
\begin{figure}[hbtp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/rebuttal_figures/classification_ts_baselines.pdf}
    \caption{Classification F1 scores on three datasets - EMG, Epilepsy, Gesture. Masking fractions considered: {0.2, 0.4, 0.6, 0.8}.}
    \label{fig:clf_full}
\end{figure}

\textbf{Real-world results on Physio-Net:} 
We compare the performance of the MissTSM framework with six imputation baselines M-RNN \cite{mrnn}, GP-VAE \cite{fortuin2020gp}, BRITS \cite{cao2018brits}, Transformer \cite{vaswani2017attention}, and SAITS \cite{saits}on the real-world PhysioNet classification dataset \cite{silva2012predicting} that is highly sparse with 80\% missing values (see Appendix for additional details), as shown in Figure \ref{fig:physio_net}. We follow the same evaluation setup as proposed in \cite{saits}. MissTSM achieves an F1-score of 57.84\%, representing an approximately 15\% improvement over SAITS, the best-performing imputation model, which scored 42.6\%. 
% \vspace{-1ex}
\begin{figure}[hbtp]
% {0.3\textwidth}
    \centering
    \includegraphics[width=0.45\linewidth]{figures/F1_score.pdf}
    \caption{Classification Performance of MissTSM and other imputation baselines on PhysioNet Dataset \cite{silva2012predicting}.}
    \label{fig:physio_net}
    % \vspace{-4ex}
\end{figure}
This substantial performance gain on a real-world dataset with missing values highlights the advantages of MissTSMs single-stage approach compared to traditional two-stage methods, beyond synthetic masking schemes used to simulate missing values in other datasets.


% \newpage
\subsection{Ablations on Forecasting and Classification task}
\label{appendix:C.4}
\begin{figure}[h]
    \includegraphics[width=\linewidth, scale=1]{figures/MAE_compare.pdf}
    \caption{Ablations of MissTSM with and without MFAA layer on Forecasting datasets.}
    \label{fig:forecast_ablation}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.85\linewidth, scale=0.5]{figures/classification_model_ablation.pdf}
    \caption{Ablations of MissTSM with and without the TFI+MFAA layer on the classification tasks.}
    \label{fig:classification_model_ablation}
\end{figure}

In the ablation experiments, our goal is to quantify the effectiveness of the TFI-Embedding scheme and the MFAA Layer on MissTSM. To achieve this, we compare MissTSM with Ti-MAE, which can be viewed as an ablation of MissTSM without the TFI-Embedding and MFAA Layers. We refer to this ablation of MissTSM as MAE. For both the forecasting (see Fig. \ref{fig:forecast_ablation}) and classification (see Fig. \ref{fig:classification_model_ablation}) tasks, we compare the MissTSM framework with MAE trained on spline and SAITS imputation techniques. For forecasting on ETTh2, we observe that our proposed MissTSM framework consistently outperforms the MAE ablations without the MFAA Layer. On the other hand for the classification, we show that for all the three datasets, we are either comparable or better than the MAE ablations. This demonstrates the efficacy of the TFI-Embedding and MFAA Layer for time-series modeling with missing values.
% \newpage
\subsection{Experiment on Computational cost comparison}
\label{appendix:C.5}
We consider a case study of a classification task on the Epilepsy dataset. Dataset is 80\% masked under MCAR. Spline and SAITS are the imputation techniques and SimMTM is the time-series model used. We report the total modeling time as the sum of imputation time and the time-series model training time. 

In Table \ref{tab:computation}, we observe that, while SimMTM integrated with SAITS achieves the highest F1 score, the total imputation time for SAITS is significantly higher than that of Spline. This additional computational overhead substantially increases the overall modeling time. Moreover, SAITS has approximately 1.3 million trainable parameters, further increasing the overall model complexity of the time-series modeling task. This highlights the potential trade-off between imputation efficiency and complexity (by imputation complexity we are referring to both model and time complexity). 

In the case of our proposed method, we do not have the extra overhead of imputation complexity. Simultaneously, MissTSM also achieves competitive performance. 
\begin{table}[htbp!]
\centering
\caption{Comparison of total computational cost between MissTSM and SimMTM integrated with Spline and SAITS}
% {\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm}
\renewcommand{\arraystretch}{1.5}
\scriptsize
\normalsize
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Time-Series Model} & \textbf{Imp. Model} & \textbf{Imp. Time (sec)} & \textbf{TS Model Train Time (sec)} & \textbf{Total Time (sec)} & \textbf{F1 Score} \\
\midrule
\textbf{SimMTM} & SAITS  & 949  42.9 & 397.59  2.64 & 1346.59  45.54 & \underline{61.0  9.20} \\
                & Spline & 8.74  0.38 & 397.59  2.64 & \underline{406.33  3.02}  & 59.16  3.67 \\
\textbf{MissTSM} & N/A & N/A & 346.8  7.32 & \textbf{346.8  7.32} & \textbf{64.93  4.57} \\
\bottomrule
\end{tabular}}
\label{tab:computation}
\end{table}
% \newpage
\subsection{Imputation error propagation}
\label{appendix:C.6}
Figure \ref{fig:err_propagation} captures the propagation of imputation errors and forecasting errors for the weather dataset (at 720 forecasting horizon). It demonstrates that there is an overall positive correlation between the imputation error and forecasting errors, thereby demonstrating propagation of the imputation errors into the downstream time-series models.
\begin{figure}[hbtp]
    \centering
    \includegraphics[width=0.6\linewidth, scale=0.5]{figures/rebuttal_figures/Weather_Error_Plot.pdf}
    \caption{\small{Imputation error vs Forecasting error across 5 trials for 4 missing fractions, 0.6, 0.7, 0.8, 0.9}}
    \label{fig:err_propagation}
\end{figure}
% \newpage
\subsection{Analysis of impact of frequency and phase parameters}
\label{appendix:C.7}
In the following, we provide additional details regarding an ablation we conducted to understand the impact of frequency and phase parameters. Given the varying frequency and phase for each feature, we modify the intervals of both to assess their impact on the results. Dataset=ETTh2, Fraction=90\% 

\textbf{Case 1}. With the phase interval held constant, we lower the frequency range and examine two intervals: one in the high frequency region ([0.6, 0.9]) and one in the low frequency region ([0.1, 0.3]). The performance comparison between these new strategies and the original configuration is shown in Table \ref{tab:periodic1}.

\begin{table}[htbp!]
\centering
\caption{Effect of sampling from different frequency intervals. The best results are in bold and second-best are italicized}
\small
\begin{tabular}{l>{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm}}
\toprule
\textbf{Time Horizon} & \textbf{Original Periodic Masking MSE} & \textbf{High Frequency MSE} & \textbf{Low Frequency MSE} \\
\midrule
96  & \textbf{0.268  0.0151} & \textit{0.281  0.028} & 0.285  0.023 \\
192 & \textbf{0.295  0.0298} & \textit{0.301  0.037} & 0.316  0.049 \\
336 & 0.319  0.0185 & \textit{0.308  0.014} & \textbf{0.307  0.011} \\
720 & 0.356  0.0310 & \textbf{0.339  0.043} & \textit{0.351  0.058} \\
\bottomrule
\end{tabular}
\label{tab:periodic1}
\end{table}

\par
We observe that with a reduced frequency range, for both high and low frequency intervals, the performance improves as the prediction window increases.  

\textbf{Case 2}. Following a similar approach as Case 1, we keep the frequency interval constant and lower the range of phase values. We examine the following intervals: the positive half-cycle [0, $\pi$] and the negative half-cycle [$\pi$, 2$\pi$]. Table \ref{tab:half_cycle_comparison} presents the results of this ablation 

\begin{table}[H]
\centering
\caption{Effect of sampling from different phase intervals. The best results are in bold and second-best are italicized}
\small
\begin{tabular}{l>{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm}}
\toprule
\textbf{Time Horizon} & \textbf{Original Periodic Masking MSE} & \textbf{(+) Half Cycle MSE} & \textbf{(-) Half Cycle MSE} \\
\midrule
96  & \textbf{0.268  0.0151} & \textit{0.287  0.037} & 0.293  0.04 \\
192 & \textbf{0.295  0.0298} & \textit{0.309  0.05}  & 0.313  0.057 \\
336 & 0.319  0.0185 & \textit{0.316  0.022} & \textbf{0.311  0.013} \\
720 & 0.356  0.0310 & \textit{0.343  0.035} & \textbf{0.340  0.040} \\
\bottomrule
\end{tabular}
\label{tab:half_cycle_comparison}
\end{table}

We observe a similar pattern here as well, with the performance improving as the prediction window increases when we sample from either the positive or negative cycle. 

As shown in the tables above, frequency and phase values clearly impact model performance. The new strategies reduce frequency or phase-related randomness among the variates of the dataset, resulting in more consistent values. This appears to enhance the models ability in long-term forecasting. 
% \begin{table}[h]
%   \caption{The table provides description of all the datasets used in the experiments}
%   \label{tab:dataset detail}
%   \centering
%   \begin{threeparttable}
%   \begin{small}
%   \renewcommand{\multirowsetup}{\centering}
%   \setlength{\tabcolsep}{3.5pt}
%   \renewcommand\arraystretch{2.0}
%   \begin{tabular}{c|c|c|c|c|c|c|c}
%     \toprule
%     \scalebox{0.9}{Tasks} & \scalebox{0.9}{Datasets} & \scalebox{0.9}{Channels} & \scalebox{0.9}{Length} & \scalebox{0.9}{Samples} & \scalebox{0.9}{Classes} & \scalebox{0.9}{Information} & \scalebox{0.9}{Frequency} \\
%     \toprule
%     \multirow{5}{*}{\rotatebox{90}{\scalebox{0.9}{Forecasting}}} & \scalebox{0.9}{ETTh2} & 7 & \scalebox{0.9}{\{96,192,336,720\}} & \scalebox{0.9}{8545/2881/2881} & - & \scalebox{0.9}{Electricity} & \scalebox{0.9}{1 Hour} \\
%     & \scalebox{0.9}{ETTm2} & \scalebox{0.9}{7} & \scalebox{0.9}{\{96,192,336,720\}} & \scalebox{0.9}{34465/11521/11521} & - & \scalebox{0.9}{Electricity} & \scalebox{0.9}{15 Mins} \\
%     & \scalebox{0.9}{Weather} & \scalebox{0.9}{21} & \scalebox{0.9}{\{96,192,336,720\}} & \scalebox{0.9}{36792/5271/10540} & - & \scalebox{0.9}{Weather} & \scalebox{0.9}{10 Mins} \\
%     % & \scalebox{0.9}{Electricity} & \scalebox{0.9}{321} & \scalebox{0.9}{\{96,192,336,720\}} & \scalebox{0.9}{18317/2633/5261} & - & \scalebox{0.9}{Electricity} & \scalebox{0.9}{1 Hour} \\
%     % & \scalebox{0.9}{Traffic} & \scalebox{0.9}{862} & \scalebox{0.9}{\{96,192,336,720\}} & \scalebox{0.9}{12185/1757/3509} & - & \scalebox{0.9}{Transportation} & \scalebox{0.9}{1 Hour} \\
%     \midrule
%     \multirow{5}{*}{\rotatebox{90}{\scalebox{0.9}{Classification}}} & \scalebox{0.9}{SleepEEG} & \scalebox{0.9}{1} & \scalebox{0.9}{200} & \scalebox{0.9}{371005/-/-} & \scalebox{0.9}{5} & \scalebox{0.9}{EEG} & \scalebox{0.9}{100 Hz} \\
%     & \scalebox{0.9}{Epilepsy} & \scalebox{0.9}{1} & \scalebox{0.9}{178} & \scalebox{0.9}{60/20/11420} & \scalebox{0.9}{2} & \scalebox{0.9}{EEG} & \scalebox{0.9}{174 Hz} \\
%     & \scalebox{0.9}{FD-B} & \scalebox{0.9}{1} & \scalebox{0.9}{5120} & \scalebox{0.9}{60/21/135599} & \scalebox{0.9}{3} & \scalebox{0.9}{Faulty Detection} & \scalebox{0.9}{64K Hz} \\
%     & \scalebox{0.9}{Gesture} & \scalebox{0.9}{3} & \scalebox{0.9}{315} & \scalebox{0.9}{320/120/120} & \scalebox{0.9}{8} & \scalebox{0.9}{Hand Movement} & \scalebox{0.9}{100 Hz} \\
%     & \scalebox{0.9}{EMG} & \scalebox{0.9}{1} & \scalebox{0.9}{1500} & \scalebox{0.9}{122/41/41} & \scalebox{0.9}{3} & \scalebox{0.9}{Muscle responses} & \scalebox{0.9}{4K Hz} \\
%     \bottomrule
%   \end{tabular}
%   \end{small}
%   \end{threeparttable}
% \end{table}


% Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.


\end{document}
