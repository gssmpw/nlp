\section{Related Work }
Decoding images from brain activity and aligning vision models with human perception are two independently researched directions. However, perceptually aligned models have yet to be explored for visual decoding from the brain. This section reviews prior work in both areas, highlighting the gap our study addresses.
\vspace{-5pt}
\subsection{Decoding Images from Brain Activity}

Understanding how humans perceive images and decode this information from brain signals has been a long-standing question in neuroscience and, more recently, in computer vision~\cite{hubel1962receptive, kamitani2005decoding, scottimindeye2}. Early studies focused on decoding simple visual features like stimulus orientation~\cite{kamitani2005decoding} and basic geometric shapes~\cite{du2017sharing}. Advances in computer vision~\cite{radford2021learning, dosovitskiy2020image, rombach2022high} and large-scale datasets~\cite{gifford2022large, allen2022massive, hebart2023things} have enabled the retrieval and reconstruction of complex images from brain activity. Research has explored natural image and video decoding using fMRI~\cite{scotti2024reconstructing, scottimindeye2}, MEG~\cite{benchetritbrain}, and EEG~\cite{songdecoding, li2024visual}. While fMRI achieves higher decoding accuracy, MEG and EEG offer better temporal resolution and accessibility. A common approach in these studies maps brain signals into pre-trained image representation spaces like CLIP~\cite{radford2021learning} and DINO~\cite{caron2021emerging} using a self-supervised loss, such as InfoNCE (Equation~\ref{eq:clip-loss}). These mappings enable image retrieval and can serve as conditioning signals for image generative models like Stable Diffusion~\cite{rombach2022high} to reconstruct viewed images. Unlike prior work focusing on brain encoders and auxiliary modalities like text, we investigate the role of image encoders. Specifically, we examine how aligning image encoders with human perception impacts visual decoding from brain signals.
\vspace{-5pt}
\subsection{Perceptual Alignment of Vision Models}

Early computer vision models were inspired by human perception~\cite{fukushima1980neocognitron, serre2007feedforward, krizhevsky2012imagenet}, but modern models often diverge from it~\cite{golan2020controversial, lindsay2021convolutional, kumarbetter, fel2022harmonizing, muttenthalerhuman}. Recent research has sought to bridge this gap by aligning machine vision models with human perception. \citet{fel2022harmonizing} proposed a neural harmonizer that co-trains deep networks to align with human visual strategies while maintaining task accuracy. Their approach leverages datasets with human judgments on salient image features from ImageNet~\cite{russakovsky2015imagenet}. Similarly, \citet{muttenthaler2024improving} introduced \emph{gLocal}, a linear transformation that aligns the global structure of neural representations with human similarity judgments while preserving local structure. Unlike naive transformations that degrade downstream task performance, \emph{gLocal} maintains or slightly improves performance while enhancing alignment with human perception. More recently, \citet{fu2024dreamsim} introduced \emph{Dreamsim}, a metric fine-tuning vision models on a human image similarity dataset. \citet{sundaramdoes} demonstrated that \emph{Dreamsim}-aligned models outperform unaligned ones on several downstream vision tasks, including object counting, instance retrieval, and image segmentation.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_dreamsim.pdf}
        \caption{}
        \label{fig:dreamsim}
    \end{subfigure}
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_glocal.pdf}
        \caption{}
        \label{fig:glocal}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/performance_comparison_braindream_nice_harmonization.pdf}
        \vspace{0.5pt}
        \caption{}
        \label{fig:harmonization}
    \end{subfigure}\\
    \caption{\textbf{200-way image retrieval performance from EEG signals}. For this task, we employ the \emph{NICE} EEG encoder~\cite{songdecoding} and evaluate different human-alignment methods and image encoders. Each vertical pair of panels demonstrates the results for one human-alignment method. The results highlight a consistent improvement in image decoding performance when considering human-aligned image encoders. Stars represent significant difference confirmed by a paired T-test~($p<0.05$). Best viewed with zoom.}
    \label{fig:alignment_methods}
    \vspace{-2ex}
\end{figure*}