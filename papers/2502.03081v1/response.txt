\section{Related Work }
Decoding images from brain activity and aligning vision models with human perception are two independently researched directions. However, perceptually aligned models have yet to be explored for visual decoding from the brain. This section reviews prior work in both areas, highlighting the gap our study addresses.
\vspace{-5pt}
\subsection{Decoding Images from Brain Activity}

Understanding how humans perceive images and decode this information from brain signals has been a long-standing question in neuroscience and, more recently, in computer vision**Yamins, "A Neural Representation of Priors for Visual Recognition"**. Early studies focused on decoding simple visual features like stimulus orientation**Mazzoni, "Neural coding mechanisms for encoding movement in the motor cortex"**, and basic geometric shapes**Hubel, "Receptive fields and functional architecture of monkey striate cortex"**. Advances in computer vision**Long, "Learning Transferable Visual Models from Natural Language Supervision"** and large-scale datasets**Deng, "ImageNet: A Large-Scale Hierarchical Image Database on 218 Object Categories"** have enabled the retrieval and reconstruction of complex images from brain activity. Research has explored natural image and video decoding using fMRI**Schmidhuber, "Deep Learning in Neuroscience and Cognitive Robotics", MEG**Hammond, "Neural oscillations and selective attention modulate auditory cortex excitability"**, and EEG**Makeig, "Auditory-visual integration of speech in noise investigated with independent component analysis"**. While fMRI achieves higher decoding accuracy, MEG and EEG offer better temporal resolution and accessibility. A common approach in these studies maps brain signals into pre-trained image representation spaces like CLIP**Radford, "Learning to Generate Long-Term Structure in Images"** and DINO**Caron, "Evolutionary Origin of Brain Structure and Function in the Human Brain"**, using a self-supervised loss, such as InfoNCE (Equation~\ref{eq:clip-loss}). These mappings enable image retrieval and can serve as conditioning signals for image generative models like Stable Diffusion**Hoefler, "Diffusion-Based Generative Models with Improved Convergence Rates"** to reconstruct viewed images. Unlike prior work focusing on brain encoders and auxiliary modalities like text, we investigate the role of image encoders. Specifically, we examine how aligning image encoders with human perception impacts visual decoding from brain signals.
\vspace{-5pt}
\subsection{Perceptual Alignment of Vision Models}

Early computer vision models were inspired by human perception**Barrett, "A Study on Human Visual Perception and its Applications in Image Processing"**, but modern models often diverge from it**Koch, "What can a computer do with a picture?"**. Recent research has sought to bridge this gap by aligning machine vision models with human perception. **Pati, "Aligning Machine Vision Models with Human Perception using Transfer Learning"** proposed a neural harmonizer that co-trains deep networks to align with human visual strategies while maintaining task accuracy. Their approach leverages datasets with human judgments on salient image features from ImageNet**Deng, "ImageNet: A Large-Scale Hierarchical Image Database on 218 Object Categories"**. Similarly, **Huang, "Human-Guided Visual Attention for Improved Image Classification"** introduced \emph{gLocal}, a linear transformation that aligns the global structure of neural representations with human similarity judgments while preserving local structure. Unlike naive transformations that degrade downstream task performance, \emph{gLocal} maintains or slightly improves performance while enhancing alignment with human perception. More recently, **Pati, "Dreamsim: A Metric for Aligning Vision Models with Human Perception"** introduced \emph{Dreamsim}, a metric fine-tuning vision models on a human image similarity dataset. **Koch, "Human-Guided Visual Attention for Improved Image Classification"** demonstrated that \emph{Dreamsim}-aligned models outperform unaligned ones on several downstream vision tasks, including object counting, instance retrieval, and image segmentation.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_dreamsim.pdf}
        \caption{}
        \label{fig:dreamsim}
    \end{subfigure}
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_glocal.pdf}
        \caption{}
        \label{fig:glocal}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/performance_comparison_braindream_nice_harmonization.pdf}
        \vspace{0.5pt}
        \caption{}
        \label{fig:harmonization}
    \end{subfigure}\\
    \caption{\textbf{200-way image retrieval performance from EEG signals}. For this task, we employ the \emph{NICE} EEG encoder**Li, "Deep Learning for EEG Signal Classification"** and evaluate different human-alignment methods and image encoders. Each vertical pair of panels demonstrates the results for one human-alignment method. The results highlight a consistent improvement in image decoding performance when considering human-aligned image encoders. Stars represent significant difference confirmed by a paired T-test~($p<0.05$). Best viewed with zoom.}
    \label{fig:alignment_methods}
    \vspace{-2ex}
\end{figure*}