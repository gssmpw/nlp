%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{multicol}
\usepackage{multirow}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% to make lists easier
\usepackage[shortlabels]{enumitem}

\usepackage{lipsum}

\usepackage[most]{tcolorbox}
\tcbset{
    definitionstyle/.style={
        colback=gray!05,        % Light grey background
        colframe=black,         % Border color
        boxrule=1.0pt,          % Border thickness
        width=\columnwidth,     % Box width matches column width
        arc=1mm,                % Rounded corners
        left=2mm,               % Left padding
        right=2mm,              % Right padding
        top=1mm,                % Top padding
        bottom=1mm,             % Bottom padding
        before skip=10pt,       % Space before the box
        after skip=10pt,        % Space after the box
        enhanced,               % Enable advanced tcolorbox features
    }
}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% Extra packages
\usepackage{multirow} % For merging rows

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Human-Aligned Image Models Improve Visual Decoding from the Brain}

\begin{document}

\twocolumn[
\icmltitle{Human-Aligned Image Models Improve Visual Decoding from the Brain}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nona Rajabi}{yyy}
\icmlauthor{Ant\^onio H. Ribeiro}{sch,equal}
\icmlauthor{Miguel Vasco}{yyy,equal}
\icmlauthor{Farzaneh Taleb}{yyy}
\icmlauthor{M\r{a}rten Bj\"{o}rkman}{yyy}
\icmlauthor{Danica Kragic}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Division of Robotics, Perception, and Learning, KTH Royal Institute of Technology, Stockholm, Sweden}
\icmlaffiliation{sch}{Department of Information Technology, Uppsala University, Uppsala, Sweden}


\icmlcorrespondingauthor{Nona Rajabi}{nonar@kth.se}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Visual Decoding,Brain-Computer Interface,EEG,Contrastive Learning,Human-Alignment}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Decoding visual images from brain activity has significant potential for advancing brain-computer interaction and enhancing the understanding of human perception. Recent approaches align the representation spaces of images and brain activity to enable visual decoding. In this paper, we introduce the use of human-aligned image encoders to map brain signals to images. We hypothesize that these models more effectively capture perceptual attributes associated with the rapid visual stimuli presentations commonly used in visual brain data recording experiments. Our empirical results support this hypothesis, demonstrating that this simple modification improves image retrieval accuracy by up to 21\% compared to state-of-the-art methods. Comprehensive experiments confirm consistent performance improvements across diverse EEG architectures, image encoders, alignment methods, participants, and brain imaging modalities.
\end{abstract}



\section{Introduction}
\label{sec:introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/intro_fig.pdf}
    \caption{\textbf{Performance of Image decoding from EEG data}. We show how the use of human-aligned versions of standard image encoders consistently leads to an increase in performance across image retrieval tasks from brain activity. The bars represent the top-1 retrieval performance averaged over subjects.}
    \label{fig:first_fig}
    \vspace{-3pt}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/alignment_method.pdf}
    \caption{\textbf{Learning to map brain activity to images}. 
    Image embeddings \(\mathbf{v} \in \mathbb{R}^d\) are extracted from frozen image encoders \(f_{\theta}(.)\), while brain signal embeddings \(\mathbf{w} \in \mathbb{R}^d\) are computed using modality-specific encoders \(g_{\phi}(.)\). A contrastive loss then aligns brain-signal representations to the image space. We consider both original pre-trained image encoders and human-aligned versions fine-tuned on human image similarity judgments. Images for the similarity judgment dataset and retrieval results are borrowed from \emph{NIGHTS}~\cite{fu2024dreamsim} and \emph{THINGS}~\cite{hebart2019things} datasets.}
    \label{fig:overview}
    \vspace{-3pt}
\end{figure*}

Decoding visual information from brain activity has long been a key goal in neuroscience and computer vision, with the potential to revolutionize brain-computer interfaces, deepen our understanding of neural processes, and enable advanced assistive technologies. Current state-of-the-art methods for visual decoding rely on three core components: \textit{(i)} a \emph{brain-signal encoder} that processes high-dimensional, multivariate time-series data from the brain; \textit{(ii)} a pre-trained \emph{image encoder} capable of generating semantically rich image embeddings; and \textit{(iii)} a \emph{self-supervised loss function} that aligns brain signals and image embeddings within a shared representation space.

Despite progress, visual decoding—especially from low signal-to-noise ratio data like electroencephalogram (EEG)—remains a significant challenge. Previous approaches have explored various brain signal encoders~\cite{benchetritbrain, songdecoding, li2024visual}, additional encoding models~\cite{scotti2024reconstructing, li2024visual}, and auxiliary loss functions~\cite{ferrante2024decoding}. Yet, overall performance remains limited; for example, state-of-the-art EEG-based models achieve only 28\% top-1 retrieval accuracy from a database of 200 unseen test images. Here, we take another approach and focus on the nature of the image encoder and how it affects the performance of visual decoding methods.

We investigate the use of \emph{human-aligned} image representation models~\cite{fel2022harmonizing, muttenthaler2024improving, sundaramdoes} for visual decoding from brain signals. These models are specifically designed to ensure that images perceived as similar by humans are closer in the representation space, effectively aligning the embeddings with human perception. This alignment is particularly relevant for decoding visual information from brain signals, as human judgments emerge from underlying brain processes.

We demonstrate that incorporating pre-trained, human-aligned image encoders significantly improves visual decoding performance from brain signals. Through an extensive empirical study spanning multiple brain modalities, brain-signal encoders, and image foundation models, we show consistent and substantial improvements achieved through human alignment. Furthermore, we analyze the differences between models trained with human-aligned and baseline image encoders, revealing that human-aligned models focus on signal components known to be relevant to visual processing. By leveraging human-aligned image encoders, we improve the accuracy of state-of-the-art models by up to 21\%, establishing a new direction for visual decoding from the brain (\cref{fig:first_fig}).

Our key contributions are as follows:
\begin{itemize} %[itemsep=-4pt, topsep=-2pt]
    \item We propose and demonstrate that \emph{human-aligned models consistently and significantly improve visual decoding from the brain}.
    \item We conduct an \emph{extensive empirical study} of various human-alignment methods, image and brain encoders, and neuroimaging modalities.
    \item We hypothesize that human \emph{similarity judgment datasets capture early human impressions} of images and are more closely aligned with the brain signals. To support this hypothesis, we present experiments that provide further evidence.
\end{itemize}

\section{Brain-to-Image Decoding via Latent Space Alignment}
\label{sec:background}
We frame the problem of visual decoding from the brain as retrieving the corresponding viewed image from a given image database based on brain activity. We exploit an architecture composed of a brain-signal encoder and an image encoder. After training the brain encoder, test brain activity is embedded and its distances to all images in the test database are computed. The images are then ranked by their distance to the brain embedding, and the top-$k$ images are retrieved. The procedure, and overall architecture of the method, are summarized in \cref{fig:overview}.

Formally, let $\mathbf{x}$ denote an image from a subset of an image dataset, and let $\mathbf{b}$ represent the brain-activity measurement recorded from a subject while observing $\mathbf{x}$. We define two neural network-parameterized functions, \(f_{\theta}\) and \(g_{\phi}\), that map images and brain signals, respectively, to a shared latent vector space \( Z \in \mathbb{R}^{d}\):  
\begin{equation}
\mathbf{x} \mapsto f_{\theta}(\mathbf{x}) = \mathbf{v}, \quad  
\mathbf{b} \mapsto g_{\phi}(\mathbf{b}) = \mathbf{w}, \quad \mathbf{v}, \mathbf{w} \in \mathbb{R}^{d}.
\end{equation}

Our goal is to train the brain encoder \(g_{\phi}\) such that the embeddings $\mathbf{v}, \mathbf{w}$ for each pair ($\mathbf{x}, \mathbf{b}$) are \emph{aligned} in the shared latent space \(Z \in \mathbb{R}^{d}\). To achieve this, we use frozen image encoders \(f_{\theta}\) derived from pre-trained image or image-text models, which provide rich, generalizable representations without requiring fine-tuning. Aligning brain embeddings $\mathbf{w}$ with the image embeddings $\mathbf{v}$ enables retrieval of the corresponding viewed image $\mathbf{x}$ from the brain signal.
This alignment is enforced using the multimodal InfoNCE loss~\cite{radford2021learning}:
   \begin{equation}
    \small
    \label{eq:clip-loss}
    \begin{split}
    \mathcal{L}_{C} =- \frac{1}{N} \sum_{i=1}^{N} \bigg[
    & \log \frac{\exp(\text{sim}(\mathbf{w}_i, \mathbf{v}_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(\mathbf{w}_i, \mathbf{v}_j) / \tau)}\\
    & + \log \frac{\exp(\text{sim}(\mathbf{v}_i, \mathbf{w}_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(\mathbf{v}_i, \mathbf{w}_j) / \tau)}
    \bigg],
    \end{split}
    \end{equation}
where $\tau$ is the temperature parameter and $\text{sim}(a, b)$ is a distance metric between the two embeddings: in our case, we employ cosine similarity. The same subscripts denote matching pairs. We investigate the impact of image encoders on visual decoding from brain activity, specifically assessing whether human perceptual biases in image representation spaces influence performance. Our hypothesis is:

\begin{tcolorbox}[definitionstyle]
\textbf{Hypothesis}: Replacing the image encoder \(f_{\theta}(\mathbf{x})\) with its perceptually aligned version significantly improves visual decoding performance from brain signals.
\end{tcolorbox}

We based our hypothesis on the intuition that human similarity judgments originate from brain processes and thus reflect the similarities between brain signals corresponding to image stimuli. Additionally, we propose that this improvement is linked to the experimental design of the image-similarity dataset. Specifically, since brain signals are elicited by a rapid visual stimulus (100 ms viewing time), they primarily capture early impressions of images, such as shape, orientation, and color. These features are better represented in image-similarity datasets focused on early perceptual attributes. We corroborate this hypothesis through the experiments provided in later sections.

\section{Related Work }
Decoding images from brain activity and aligning vision models with human perception are two independently researched directions. However, perceptually aligned models have yet to be explored for visual decoding from the brain. This section reviews prior work in both areas, highlighting the gap our study addresses.
\vspace{-5pt}
\subsection{Decoding Images from Brain Activity}

Understanding how humans perceive images and decode this information from brain signals has been a long-standing question in neuroscience and, more recently, in computer vision~\cite{hubel1962receptive, kamitani2005decoding, scottimindeye2}. Early studies focused on decoding simple visual features like stimulus orientation~\cite{kamitani2005decoding} and basic geometric shapes~\cite{du2017sharing}. Advances in computer vision~\cite{radford2021learning, dosovitskiy2020image, rombach2022high} and large-scale datasets~\cite{gifford2022large, allen2022massive, hebart2023things} have enabled the retrieval and reconstruction of complex images from brain activity. Research has explored natural image and video decoding using fMRI~\cite{scotti2024reconstructing, scottimindeye2}, MEG~\cite{benchetritbrain}, and EEG~\cite{songdecoding, li2024visual}. While fMRI achieves higher decoding accuracy, MEG and EEG offer better temporal resolution and accessibility. A common approach in these studies maps brain signals into pre-trained image representation spaces like CLIP~\cite{radford2021learning} and DINO~\cite{caron2021emerging} using a self-supervised loss, such as InfoNCE (Equation~\ref{eq:clip-loss}). These mappings enable image retrieval and can serve as conditioning signals for image generative models like Stable Diffusion~\cite{rombach2022high} to reconstruct viewed images. Unlike prior work focusing on brain encoders and auxiliary modalities like text, we investigate the role of image encoders. Specifically, we examine how aligning image encoders with human perception impacts visual decoding from brain signals.
\vspace{-5pt}
\subsection{Perceptual Alignment of Vision Models}

Early computer vision models were inspired by human perception~\cite{fukushima1980neocognitron, serre2007feedforward, krizhevsky2012imagenet}, but modern models often diverge from it~\cite{golan2020controversial, lindsay2021convolutional, kumarbetter, fel2022harmonizing, muttenthalerhuman}. Recent research has sought to bridge this gap by aligning machine vision models with human perception. \citet{fel2022harmonizing} proposed a neural harmonizer that co-trains deep networks to align with human visual strategies while maintaining task accuracy. Their approach leverages datasets with human judgments on salient image features from ImageNet~\cite{russakovsky2015imagenet}. Similarly, \citet{muttenthaler2024improving} introduced \emph{gLocal}, a linear transformation that aligns the global structure of neural representations with human similarity judgments while preserving local structure. Unlike naive transformations that degrade downstream task performance, \emph{gLocal} maintains or slightly improves performance while enhancing alignment with human perception. More recently, \citet{fu2024dreamsim} introduced \emph{Dreamsim}, a metric fine-tuning vision models on a human image similarity dataset. \citet{sundaramdoes} demonstrated that \emph{Dreamsim}-aligned models outperform unaligned ones on several downstream vision tasks, including object counting, instance retrieval, and image segmentation.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_dreamsim.pdf}
        \caption{}
        \label{fig:dreamsim}
    \end{subfigure}
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_glocal.pdf}
        \caption{}
        \label{fig:glocal}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/performance_comparison_braindream_nice_harmonization.pdf}
        \vspace{0.5pt}
        \caption{}
        \label{fig:harmonization}
    \end{subfigure}\\
    \caption{\textbf{200-way image retrieval performance from EEG signals}. For this task, we employ the \emph{NICE} EEG encoder~\cite{songdecoding} and evaluate different human-alignment methods and image encoders. Each vertical pair of panels demonstrates the results for one human-alignment method. The results highlight a consistent improvement in image decoding performance when considering human-aligned image encoders. Stars represent significant difference confirmed by a paired T-test~($p<0.05$). Best viewed with zoom.}
    \label{fig:alignment_methods}
    \vspace{-2ex}
\end{figure*}

\section{Materials and Methods}
Here we describe the datasets, pre-trained image encoders, and EEG encoder architectures employed in our study. 
\subsection{Datasets}
\label{sec:dataset}
We used the \emph{Things EEG2} dataset~\cite{gifford2022large} to train and evaluate our framework. \emph{Things EEG2} is, to our knowledge, the only dataset with a sufficient number of participants and trials per participant. It also employs a random-order design, avoiding biases associated with block-order designs~\cite{li2020perils}. EEG data was recorded from 10 participants exposed to visual stimuli via a rapid serial visual presentation (RSVP) paradigm (100 ms per image). The training set includes 1,654 unique concepts, each with 10 images shown in random order and repeated 4 times, totaling \(1654 \times 10 \times 4\) samples per participant. The test set contains 200 distinct concepts, each with 1 image shown 80 times, yielding \(200 \times 1 \times 80\) samples per participant. Training and test concepts are disjoint. To improve the signal-to-noise ratio, repetitions were averaged, reducing training and test samples to \(1654 \times 10\) and \(200 \times 1\) per participant. Models were trained with a 90/10\% split and evaluated on the test set.

For the results in Section~\ref{sec:meg}, we used preprocessed MEG data from~\cite{hebart2023things}, recorded from 4 participants in response to 1,854 unique concepts (500 ms per image). The training set contains \(1854 \times 12 \times 1\) samples, while the test set includes \(200 \times 1 \times 12\) samples per participant. We excluded the 200 test concepts from training and averaged the 12 repetitions per test concept, resulting in \(1654 \times 12\) training samples and \(200 \times 1\) test samples per participant. For both EEG and MEG, models were trained on signals from 0 to 1000 ms relative to stimulus onset. Further dataset details are in Appendix~\ref{supl:dataset}.

\begin{table*}[t]
\centering
\caption{\textbf{200-way top-1 image retrieval performance from EEG signals.} We consider different EEG and image encoders during training of the models. The results show that using human-aligned (HA) image encoders consistently improves the performance of the overall model, regardless of the specific image and EEG encoder employed. Best viewed with zoom.}
\label{tab:eeg_encoders}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l*{12}{c}} % 13 columns
\toprule
& \multicolumn{2}{c}{\emph{Ensemble}} & \multicolumn{2}{c}{\emph{SynCLR}} & \multicolumn{2}{c}{\emph{CLIP}} & \multicolumn{2}{c}{\emph{OpenCLIP}} & \multicolumn{2}{c}{\emph{DINO}} & \multicolumn{2}{c}{\emph{DINOv2}} \\
\cmidrule(lr){2-13}
& HA & Base & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base \\
\midrule
\emph{EEGNetV4} & $\boldsymbol{39.51\pm1.42}$ & $33.94\pm1.80$ & $\boldsymbol{38.98\pm1.27}$ & $31.52\pm1.64$ & $\boldsymbol{28.03\pm1.40}$ & $20.55\pm0.96$ & $\boldsymbol{31.31\pm1.74}$ & $24.10\pm1.49$ & $\boldsymbol{37.46\pm1.42}$ & $31.34\pm1.43$ & $\boldsymbol{27.45\pm1.12}$ & $15.93\pm1.07$ \\
\emph{EEGConformer} & $\boldsymbol{23.78\pm1.25}$ & $19.62\pm1.13$ & $\boldsymbol{26.25\pm1.01}$ & $18.05\pm1.37$ & $\boldsymbol{19.02\pm1.01}$ & $13.59\pm0.81$ & $\boldsymbol{21.96\pm0.95}$ & $14.61\pm0.89$ & $\boldsymbol{23.92\pm1.30}$ & $19.48\pm0.89$ & $\boldsymbol{18.14\pm1.04}$ & $9.26\pm0.63$\\
\emph{NICE} & $\boldsymbol{54.96\pm2.37}$ & $47.27\pm2.27$ & $\boldsymbol{58.44\pm2.29}$ & $44.19\pm2.02$ & $\boldsymbol{42.52\pm2.17}$ & $28.59\pm1.52$ & $\boldsymbol{49.59\pm2.40}$ & $36.20\pm2.02$ & $\boldsymbol{54.24\pm2.19}$ & $45.99\pm1.92$ & $\boldsymbol{41.87\pm1.67}$ & $20.64\pm1.55$ \\
\emph{ATM-S} & $\boldsymbol{56.36\pm2.14}$ & $48.89\pm1.97$ & $\boldsymbol{62.53\pm1.59}$ & $46.15\pm1.59$ & $\boldsymbol{46.06\pm1.62}$ & $29.87\pm1.41$ & $\boldsymbol{53.25\pm1.93}$ & $40.13\pm1.77$ & $\boldsymbol{56.30\pm1.92}$ & $48.04\pm1.61$ & $\boldsymbol{44.73\pm1.61}$ & $22.91\pm1.49$ \\
\bottomrule
\end{tabular}%
}
\vspace{-4pt}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{\textbf{Cross-subject 200-way top-1 image retrieval performance from EEG signals.} We encode EEG using the \emph{NICE} encoder and consider different image encoders during the training of the models. HA refers to the human-aligned models. Best viewed with zoom.}
    \label{tab:cross_eeg}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l*{12}{c}} % 13 columns
    \toprule
    & \multicolumn{2}{c}{\emph{Ensemble}} & \multicolumn{2}{c}{\emph{SynCLR}} & \multicolumn{2}{c}{\emph{CLIP}} & \multicolumn{2}{c}{\emph{OpenCLIP}} & \multicolumn{2}{c}{\emph{DINO}} & \multicolumn{2}{c}{\emph{DINOv2}} \\
    \cmidrule(lr){2-13}
    & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base \\
    \midrule
    S1 & $11.4\pm1.71$ & $12.2\pm1.36$ & $9.4\pm1.16$ & $10.0\pm0.84$ & $10.3\pm1.96$ & $6.0\pm1.05$ & $1.5\pm0.55$ & $9.6\pm0.37$ & $10.6\pm1.11$ & $10.0\pm0.71$ & $9.8\pm1.21$ & $9.5\pm1.00$ \\
    S2 & $15.0\pm1.67$ & $15\pm1.48$ & $13.1\pm1.83$ & $13.2\pm1.63$ & $12.8\pm1.12$ & $8.5\pm0.95$ & $11.8\pm2.64$ & $10.4\pm1.85$ & $15.1\pm1.25$ & $13.5\pm1.73$ & $13.5\pm1.38$ & $10.8\pm1.21$ \\
    S3 & $8.1\pm0.98$ & $9.5\pm4.07$ & $12.8\pm1.91$ & $8.7\pm1.50$ & $11.9\pm1.98$ & $7.5\pm1.00$ & $10.5\pm3.18$ & $7.1\pm0.73$ & $9.1\pm1.31$ & $9.4\pm1.24$ & $8.4\pm1.88$ & $7.8\pm0.75$ \\
    S4 & $9.5\pm1.58$ & $12.0\pm1.05$ & $10.2\pm1.50$ & $9.4\pm1.36$ & $9.9\pm1.24$ & $6.8\pm0.40$ & $10.0\pm2.14$ & $6.1\pm1.24$ & $9.3\pm1.03$ & $9.2\pm0.98$ & $9.6\pm1.32$ & $10.8\pm1.21$ \\
    S5 & $7.3\pm2.42$ & $7.9\pm0.86$ & $7.6\pm0.58$ & $8.6\pm1.28$ & $7.4\pm1.36$ & $4.4\pm1.39$ & $6.1\pm1.88$ & $6.2\pm1.29$ & $7.8\pm1.17$ & $8.1\pm1.68$ & $7.1\pm0.97$ & $6.4\pm1.16$ \\
    S6 & $21.8\pm1.60$ & $18.9\pm1.24$ & $17.0\pm1.22$ & $14.4\pm1.36$ & $16.7\pm1.47$ & $12.0\pm0.48$ & $13.0\pm2.39$ & $10.9\pm1.39$ & $13.0\pm2.55$ & $15.5\pm1.97$ & $11.87\pm1.78$ & $11.3\pm1.66$\\
    S7 & $10.8\pm1.72$ & $10.5\pm1.45$ & $10.7\pm2.48$ & $8.8\pm1.03$ & $8.5\pm1.52$ & $5.9\pm0.97$ & $7.6\pm1.71$ & $7.0\pm1.38$ & $9.5\pm1.30$ & $10.2\pm1.86$ & $9.5\pm1.30$ & $5.5\pm0.45$\\
    S8 & $14.5\pm1.45$ & $13.6\pm0.86$ & $15.9\pm1.53$ & $12.7\pm0.98$ & $12.4\pm1.56$ & $10.5\pm1.00$ & $13.6\pm0.71$ & $10.0\pm0.55$ & $14.2\pm1.54$ & $13.2\pm1.03$ & $13.8\pm0.93$ & $9.6\pm1.35$\\
    S9 & $13.9\pm2.44$ & $13.3\pm2.18$ & $13.8\pm0.51$ & $12.4\pm1.65$ & $10.0\pm1.45$ & $7.6\pm0.66$ & $11.4\pm1.46$ & $9.7\pm1.91$ & $14.2\pm0.93$ & $11.8\pm1.12$ & $10.0\pm0.71$ & $9.8\pm1.81$ \\
    S10 & $15.8\pm2.96$ & $16.4\pm0.73$ & $13.3\pm1.36$ & $12.0\pm1.30$ & $14.1\pm1.20$ & $10.2\pm1.29$ & $12.3\pm1.50$ & $9.2\pm1.94$ & $12.6\pm2.30$ & $13.2\pm1.03$ & $10.1\pm1.65$ & $11.9\pm1.32$\\
    \midrule
    Avg. & $12.8\pm1.37$ & $12.9\pm1.04$ & $12.4\pm0.92$ & $11.0\pm0.68$ & $\boldsymbol{11.4\pm0.87}$ & $7.9\pm0.75$ & $\boldsymbol{10.8\pm0.74}$ & $8.6\pm0.58$ & $11.5\pm0.82$ & $11.4\pm0.75$ & $10.2\pm0.72$ & $9.3\pm0.67$ \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-5pt}
\end{table*}

\subsection{Pretrained Vision Models}
\label{sec:vision_models}
We train the complete EEG encoder by aligning its final layer representations to a pretrained vision model's representations. For this purpose, we use image embeddings from vision models aligned to human perceptual judgments proposed by~\citet{sundaramdoes} (which we refer to as \emph{Dreamsim}) and compare them with their original unaligned version. Specifically, we use vision transformer (ViT)~\cite{dosovitskiy2020image} backbones including \emph{CLIP}~\cite{radford2021learning}, \emph{OpenCLIP}~\cite{cherti2023reproducible}, \emph{DINO}~\cite{caron2021emerging}, \emph{DINOv2}~\cite{oquabdinov2}, and \emph{SynCLR}~\cite{tian2024learning}. We also include results obtained by concatenated features from \emph{DINO}, \emph{CLIP}, and \emph{OpenCLIP} referred to as the \emph{Ensemble} model~\cite{sundaramdoes}.

\subsection{EEG Architectures and Training}
We used the \emph{NICE} EEG encoder proposed by \citet{songdecoding} to embed EEG signals in the experiments described in Sections \ref{sec:abl_img_align}, \ref{sec:cross_eeg}, and \ref{sec:meg}. \emph{NICE} is a recent convolutional neural network (CNN) that applies temporal and spatial convolutions to multivariate time-series input data. Additionally, in Section~\ref{sec:abl_eeg}, we evaluated our method with other EEG encoders by training three recent architectures: \emph{EEGNetv4}~\cite{lawhern2018eegnet}, a widely used CNN for EEG tasks; \emph{EEGConformer}~\cite{song2022eeg}, a transformer-based model designed for EEG signals; and \emph{ATM-S}~\cite{li2024visual}, which combines a transformer with a CNN. 

To speed up the training, we pre-computed the image embeddings using the frozen image encoders. \emph{Dreamsim} models were used as human-aligned image encoders throughout the paper unless explicitly mentioned otherwise. We trained all models using the InfoNCE loss (Equation~\ref{eq:clip-loss}) with five different seeds (which also resulted in five different train/val splits). All results presented were averaged over all seeds. More training details are presented in the Appendix~\ref{supl:training}.

\begin{table*}[t]
\centering
\caption{\textbf{200-way top-1 image retrieval performance from MEG signals.} We encode MEG using the \emph{NICE} encoder and consider different image encoders during the training of the models. The results show that using human-aligned (HA) image encoders consistently improves the performance of the overall system, regardless of the specific image encoder employed. Best viewed with zoom.}
% \caption{200-way top-1 retrieval performance on MEG data for human-aligned and original embeddings.}
\label{tab:meg_top1}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l*{12}{c}} % 13 columns
\toprule
& \multicolumn{2}{c}{\emph{Ensemble}} & \multicolumn{2}{c}{\emph{SynCLR}} & \multicolumn{2}{c}{\emph{CLIP}} & \multicolumn{2}{c}{\emph{OpenCLIP}} & \multicolumn{2}{c}{\emph{DINO}} & \multicolumn{2}{c}{\emph{DINOv2}} \\
\cmidrule(lr){2-13}
& HA & Base & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base \\
\midrule
S1 & $13.4\pm1.82$ & $12.1\pm1.88$ & $15.5\pm1.14$ & $13.0\pm1.30$ & $12.6\pm0.86$ & $7.1\pm0.49$ & $11.4\pm1.11$ & $8.7\pm0.68$ & $10.7\pm1.03$ & $11.8\pm1.75$ & $11.7\pm2.02$ & $7.5\pm1.10$ \\
S2 & $36.7\pm2.56$ & $29.6\pm0.97$ & $36.8\pm2.54$ & $24.5\pm1.52$ & $28.3\pm2.75$ & $13.8\pm1.81$ & $29.6\pm2.11$ & $20.6\pm2.27$ & $33.6\pm1.56$ & $24.75\pm2.61$ & $30.6\pm1.28$ & $12.3\pm1.17$\\
S3 & $22.7\pm1.08$ & $20.6\pm1.02$ & $25.6\pm2.41$ & $16.9\pm1.62$ & $22.2\pm1.63$ & $13.4\pm1.32$ & $21.4\pm0.97$ & $15.4\pm1.39$ & $21.1\pm1.77$ & $19.12\pm1.02$ & $17.5\pm0.89$ & $7.6\pm0.97$ \\
S4 & $14.8\pm1.96$ & $12.2\pm1.72$ & $14.7\pm2.94$ & $12.7\pm2.45$ & $14.3\pm1.33$ & $8.1\pm0.73$ & $11.1\pm1.68$ & $10.9\pm2.08$ & $10.2\pm1.17$ & $10.2\pm1.48$ & $10.8\pm1.75$ & $5.8\pm1.78$ \\
\midrule
Avg. & $\boldsymbol{21.9\pm5.34}$ & $18.6\pm4.16$ & $\boldsymbol{23.2\pm5.18}$ & $16.8\pm2.75$ & $\boldsymbol{19.3\pm3.64}$ & $10.6\pm1.75$ & $\boldsymbol{18.4\pm4.44}$ & $13.9\pm2.66$ & $\boldsymbol{18.9\pm5.51}$ & $16.5\pm3.37$ & $\boldsymbol{17.7\pm4.56}$ & $8.3\pm1.40$ \\
\bottomrule
\end{tabular}%
}
\vspace{-6pt}
\end{table*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.39\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/dreamsim_synclr_vitb16_gradcam_histogram.pdf}
        \caption{}
        \label{fig:time_hist}
    \end{subfigure}
    \begin{subfigure}[b]{0.39\textwidth}
        \centering
        \includegraphics[width=0.98\textwidth]{figures/dreamsim_synclr_vitb16_gradcam_eeg_bands.pdf}
        \caption{}
        \label{fig:psd}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/dreamsim_synclr_vitb16_gradcam_topomap.pdf}
        \caption{}
        \label{fig:topomap}
    \end{subfigure}\\
    \caption{\textbf{Analyzing Gradient Maps for \emph{NICE} EEG Encoder}. We computed the gradients for trained EEG encoders with human-aligned and original unaligned image embeddings and analyzed them from (a) temporal, (b) spectral, and (c) spatial perspectives. The results are presented for a representative participant and image backbone.}
    \label{fig:mainfigure}
    \vspace{-6pt}
\end{figure*}
\section{Results}
We conduct an extensive empirical study comparing the performance of human-aligned and unaligned image encoders in visual decoding from brain activity. Specifically, we assess their impact across various image encoder architectures and alignment methods (Section~\ref{sec:abl_img_align}), as well as various EEG encoders (Section~\ref{sec:abl_eeg}). We also assess whether human-aligned models generalize better to unseen subjects (Section~\ref{sec:cross_eeg}) and examine their effectiveness in other neuroimaging modalities, such as MEG (Section~\ref{sec:meg}). Finally, we perform an interpretability analysis (Section~\ref{sec:interpret}) to highlight differences between EEG models trained with human-aligned and original image embeddings. Overall, our results highlight the consistent improvement of using human-aligned models for visual decoding from the brain.

\subsection{Image Encoders and Alignment Methods}
\label{sec:abl_img_align}
We start by evaluating how different image encoders and alignment methods affect visual decoding performance from EEG activity. In particular, we consider the \emph{Dreamsim}~\cite{fu2024dreamsim, sundaramdoes}, \emph{gLocal}~\cite{muttenthalerhuman} and \emph{Harmonization}~\cite{fel2022harmonizing} alignment methods. For consistency, we employ the NICE EEG encoder~\cite{songdecoding} across all evaluations. We present the main results in Figure~\ref{fig:alignment_methods}.

EEG encoders trained with human-aligned embeddings using \emph{Dreamsim} consistently and significantly outperform those trained with unaligned embeddings, regardless of the type of image encoder used (Fig.~\ref{fig:dreamsim}). Notably, these human-aligned models achieve up to 21\% higher top-1 and 29\% higher top-5 accuracy in 200-way retrieval tasks. A paired T-test further confirms the significance of these improvements (\(p < 0.001\) for all except \emph{DINO} and \emph{Ensemble}, where \(p < 0.01\) and \(p < 0.05\), respectively for top-1 performance).

Both \emph{gLocal} and \emph{Harmonization} perform comparably to original unaligned models and sometimes even surpass them (Fig.~\ref{fig:glocal}, Fig.~\ref{fig:harmonization}). However, they still underperform relative to \emph{Dreamsim}. We attribute this to differences in the datasets used to align these models with human perception. \emph{Dreamsim} is trained on image similarity judgment datasets that capture mid-level similarities, which are more likely to be reflected in brain data collected via RSVP. In contrast, \emph{gLocal} and \emph{Harmonization} emphasize similarities that require more attention and longer viewing times. We further discuss these differences in Section~\ref{sec:discussion}.

\subsection{EEG Architecture}  
\label{sec:abl_eeg}
We now assess the impact of different EEG architecture for the overall performance of the method. We evaluate the \emph{EEGNetV4}~\cite{lawhern2018eegnet}, \emph{EEGConformer}~\cite{song2022eeg}, \emph{NICE}~\cite{songdecoding}, and \emph{ATM-S}~\cite{li2024visual} as EEG encoders in our evaluation. As before, baseline embeddings are compared with human-aligned embeddings from multiple image encoders. We consider \emph{Dreamsim} as the alignment method for this evaluation. The results are summarized in Table~\ref{tab:eeg_encoders}. 

Once again, we observe a consistent and significant improvement when we consider the human-aligned version of the image embeddings during training. In particular, human-alignment allows us to achieve a 62\% top-1 accuracy, a significant improvement over the 46\% accuracy achieved by non-aligned versions. As expected from prior literature, certain architectures (\emph{NICE}, \emph{ATM-S}) perform better overall than others (\emph{EEGNetV4}, \emph{EEGConformer}). Nonetheless, encoders trained with human-aligned embeddings consistently outperformed those trained with unaligned embeddings, regardless of the underlying EEG architecture.

\subsection{Transferring to New Subjects}
\label{sec:cross_eeg}
Generalization of trained models to unseen participants is a critical yet challenging task in brain data studies, as the subjective nature of brain signals often leads to poor performance on new participants' data. In this experiment, we compared the naive generalization capability of the \emph{NICE} EEG encoder trained with human-aligned embeddings to its unaligned counterpart. Specifically, the \emph{NICE} encoder was trained using data from 9 participants and tested on the held-out participant's test data. The results for each test participant and image encoder are presented in Table~\ref{tab:cross_eeg}. 

The results highlight the challenge of transferring brain signals to unseen participants. A paired T-test showed human-aligned models significantly outperformed original models for \emph{CLIP} (\(p<0.001\)) and \emph{OpenCLIP} (\(p<0.05\)), while no significant differences were found for other architectures. Though improving transferability is beyond the scope of this work, we plan to explore strategies for enhancing generalization in future research.

\subsection{Validation on MEG Signals}
\label{sec:meg}
To evaluate whether the performance improvement of using human-aligned image encoders extends to other neuroimaging modalities, we replicate the experimental procedure of Section~\ref{sec:abl_img_align}, this time using MEG data. Specifically, we utilized the MEG dataset from~\citet{hebart2023things}, as described in Section~\ref{sec:dataset}, and employed a similar architecture to the \emph{NICE} encoder. We present the top-1 retrieval performances, discriminated by subject, in Table~\ref{tab:meg_top1}, and top-5 performance in Appendix~\ref{supl:meg_top5}. 

Results consistently show improved retrieval performance across all participants using human-aligned embeddings with \emph{Dreamsim} over original image encoders. While statistical tests were not conducted due to the limited number of participants, the findings highlight the potential of human-aligned image encoders to enhance visual decoding across brain activity modalities.

\subsection{Biological Interpretation}
\label{sec:interpret}
To investigate whether the improved performance with perceptually aligned embeddings is linked to attributes of visual processing in the brain, we analyzed the gradients of the EEG encoder from spatial, temporal, and spectral perspectives. Specifically, we applied gradient-weighted class activation mapping (Grad-CAM)~\cite{selvaraju2017grad} to identify the regions in the signals that contributed most to the model's predictions. Figure~\ref{fig:time_hist} presents the histogram of gradient values exceeding the $99^{th}$ percentile over time for a representative participant. Notably, EEG encoders trained with human-aligned image embeddings exhibit a higher peak at earlier time points ($0-200$ ms relative to stimulus onset). This period overlaps with the image stimulus presentation time and is associated with low-level visual processing, such as orientation and color~\cite{lamme2000distinct, hillyard1998event}. Figure~\ref{fig:retrieved_images} further supports this, demonstrating that human-aligned models, compared to original ones, more accurately retrieve attributes like texture, pattern, color, and shape, which are consistently shared among the top-5 retrieved images. Additional examples are provided in Appendix~\ref{supl:retrived_large}.

EEG signals are typically decomposed into five frequency bands linked to cognitive processes: Delta ($[0-4]$ Hz), Theta ($[4-8]$ Hz), Alpha ($[8-12]$ Hz), Beta ($[12-30]$ Hz), and Gamma ($>30$ Hz)). To assess each band's contribution, we computed the power spectral density of the gradients and normalized them by total power. Fig.~\ref{fig:psd} compares energy of each band across human-aligned and original models, with statistical significance assessed via an unpaired T-test ($p < 0.005$ for Delta, Alpha, and Beta; $p<0.05$ for Gamma). Models trained with original embeddings focused more on Delta, while human-aligned models emphasized Alpha, Beta, and Gamma, aligning with prior research linking these bands to visual processing~\cite{bastos2015visual, michalareas2016alpha}. 

We also examined which EEG electrodes contribute more to the predictions of the model. Both EEG encoders, whether trained with human-aligned or original image embeddings, relied on information from electrodes positioned over the occipital and parietal brain regions, which are known to be involved in visual processing~\cite{grill2004human, wandell2007visual} (Figure~\ref{fig:topomap}). From this perspective, no significant differences were observed between the models. More results and details are provided in Appendix~\ref{supl:interpret}.


\vspace{-5pt}
\section{Discussion}
\label{sec:discussion}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/correct_retrievals.pdf}
    \caption{\textbf{Top-5 retrieved images of unseen classes from EEG signals using human-aligned (top) and base (bottom) models.} We use \emph{Dreamsim} for the aligned model with \emph{CLIP} backbone for both. The cosine similarity between EEG and image embeddings is shown above the images. Images retrieved from human-aligned models emphasize attributes like texture, pattern, color, and shape.}
    \label{fig:retrieved_images}
    \vspace{-6pt}
\end{figure*}


% P1: Quick summary and main takeaways.
In this paper, we investigate the use of human-aligned image models for decoding visual information from brain signals. We demonstrate that brain encoders trained with human-aligned image embeddings consistently outperform those trained with unaligned embeddings in image retrieval tasks. This finding is validated through extensive evaluations across various image and brain encoders, alignment methods, and brain modalities. Finally, we provide some insights on spatial, temporal, and spectral differences between the models by analyzing the gradients of the brain encoder.

% P2-P3: Explain our hypothesis and construct a story around the plots.
\paragraph{Perceptual Alignment and Brain Decoding.} Human similarity judgments stem from brain processes and reflect the similarities between brain signals elicited by image stimuli. To evaluate the effect of using alignment, in \cref{sec:interpret} we analyze gradient maps produced by the brain encoder across three domains: spatial, temporal, and spectral. The spatial analysis showed that both human-aligned and non-human-aligned models focus on similar brain regions, particularly the visual cortex. However, differences emerged in the temporal and spectral domains, with the human-aligned model placing greater emphasis on early stimulus responses and frequency components in Alpha, Beta, and Gamma bands.

We attribute this behavior to the experimental design of the image-similarity dataset. Brain signals, elicited by rapid visual stimuli (100 ms for EEG and 500 ms for MEG), primarily capture early impressions of images, such as shape, orientation, and color. These features are better represented in image-similarity datasets like \emph{NIGHTS}, which emphasize mid-level similarities, compared to those focusing on fine-grained categorical differences. As a result, image encoders fine-tuned with such datasets are more aligned to differences between images that are also reflected in the recorded brain signals. We visualized some of the retrieved images from EEG signals in~\Cref{fig:retrieved_images}, showing that the retrieved images for the human-aligned models indeed share some attributes such as color, shape, and orientation.

% P3: Discuss the ablations. Why dreamsim is so good

\paragraph{Why \emph{Dreamsim} outperforms other alignment methods.} We also evaluated embeddings generated by human-aligned image encoders trained with earlier methods. While these models performed similarly to those using unaligned embeddings, they did not achieve the substantial improvements observed with \emph{Dreamsim} models. This difference likely arises from variations in datasets and training methodologies. For example, \emph{gLocal} relies on \emph{THINGS} odd-one-out triplets~\cite{hebart2019things}, which emphasize high-level category differences, whereas \emph{Dreamsim} uses the \emph{NIGHTS}~\cite{fu2024dreamsim} similarity judgment dataset, capturing mid-level attributes such as object appearance, viewing angles, camera poses, and layout. Unlike earlier datasets focused on low-level perturbations (e.g., blurring, noise) or high-level categorical distinctions, \emph{NIGHTS} prioritizes features that are also relevant to rapid visual processing, aligning more closely with brain-computer interaction tasks in this study. Finally, \emph{Harmonization} leverages the \emph{ClickMe}~\cite{linsleylearning} and \emph{Clicktionary}~\cite{linsley2017visual} datasets, which use human-annotated saliency maps to train image encoders aligned with human perception. However, tasks in \emph{ClickMe} and \emph{Clicktionary} require careful inspection and attention to images—conditions that do not reflect the rapid visual stimulus used in brain data collection experiments. As a result, \emph{Harmonization} models are less likely to capture the same features as the brain datasets used in this study. 

\paragraph{Future Work.} Our work opens several new possibilities for future work. We mainly looked at this problem from the perspective of image retrieval, another important downstream task is reconstructing viewed images from brain activity using image generative models like Stable Diffusion~\cite{rombach2022high}. This venue of exploration could be achieved by fine-tuning the image generation process on human-aligned conditioning representation space. Moreover, methods for quantifying the alignment~\cite{sucholutsky2023getting} can provide significant insight and could also be explored. Finally, to generalize our findings, future work should validate the results on datasets featuring brain responses to images from other databases. We plan to extend this study to fMRI, which offers higher spatial resolution and longer stimulus times, to test the consistency of our findings.

\paragraph{Conclusion.} In this work we proposed that human-aligned models consistently and significantly improve visual decoding from the brain. Our extensive empirical study highlighted the pivotal role of image representations and experimental design in visual decoding from brain data. We hope our study introduces a new perspective on learning brain-to-image mappings and lays the groundwork for future advances in visual decoding from brain activity.

\section*{Acknowledgments}
This work was supported by the Knut and Alice Wallenberg Foundation, the Swedish Research Council,
European Research Council (ERC) D2Smell 101118977 and ERC BIRD 88480 grants. The computations and data handling were enabled mainly by the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre and partly by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%\section{Appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Datasets Details}
\label{supl:dataset}
\paragraph{EEG Data} We used EEG data from \citet{gifford2022large} and preprocessed it in the same way as \citet{songdecoding, li2024visual}. Particularly, first, a multivariate noise normalization~\cite{guggenmos2018multivariate} was applied to the data from each session before epoching the signals.  Then, signals from 63 EEG electrodes were filtered to the range [0.1–100] Hz and downsampled to 250 Hz. EEG data was epoched from 0 to 1000 ms relative to the stimulus onset, with baseline correction applied using the 200 ms pre-stimulus interval.  

\paragraph{MEG Data} For the MEG experiments, we used the preprocessed MEG data from \citet{hebart2023things}. The dataset includes recordings from four participants with 271 channels. Signals were band-passed between 0.1 and 40 Hz and baseline corrected by subtracting the mean and dividing by the standard deviation of the 100 ms period before stimulus onset. While the original data was epoched from -100 to 1300 ms relative to stimulus onset, we used a subset from 0 to 1000 ms. The data was initially sampled at 1200 Hz but was downsampled to 200 Hz during preprocessing.  

\section{Implementation Details}
\label{supl:training}

\paragraph{Aligned Image Encoders} We obtained human-aligned image encoders directly from the \emph{Dreamsim}, \emph{gLocal}, and \emph{Harmonization} repositories provided by the authors, ensuring they function exactly as reported in their respective papers without any retraining. The code for generating embeddings for images in the \emph{THINGS-EEG2} and \emph{THINGS-MEG2} datasets is available in the project repository.

\paragraph{Unaligned Image Encoders} For the original unaligned encoders, we used publicly available pretrained models using \emph{Hugging Face transformers} (Table~\ref{tab:model_mappings_hf}) or \emph{timm} (Table~\ref{tab:model_mappings_timm}) libraries.
\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Model Name} & \textbf{Hugging Face Repository Path} \\
        \midrule
        \texttt{CLIP\_ViT-B32} & \texttt{openai/clip-vit-base-patch32} \\
        \texttt{CLIP\_ViT-B16} & \texttt{openai/clip-vit-base-patch16} \\
        \texttt{CLIP\_ViT-L14} & \texttt{openai/clip-vit-large-patch14} \\
        \texttt{DINO\_ViT-B16} & \texttt{facebook/dino-vitb16} \\
        \texttt{DINO\_ViT-B8} & \texttt{facebook/dino-vitb8} \\
        \texttt{DINOv2\_ViT-B14} & \texttt{facebook/dinov2-base} \\
        \texttt{DINOv2\_ViT-L14} & \texttt{facebook/dinov2-large} \\
        \texttt{OpenCLIP\_ViT-L14\_laion2b} & \texttt{hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K} \\
        \texttt{OpenCLIP\_ViT-L14\_laion400m} & \texttt{hf-hub:timm/vit\_large\_patch14\_clip\_224.laion400m\_e32} \\
        \texttt{OpenCLIP\_ViT-B32\_laion400m} & \texttt{hf-hub:timm/vit\_base\_patch32\_clip\_224.laion400m\_e31} \\
        \texttt{OpenCLIP\_RN50} & \texttt{hf-hub:timm/resnet50\_clip.openai} \\
        \bottomrule
    \end{tabular}
    \caption{Model names and their corresponding Hugging Face repository paths.}
    \label{tab:model_mappings_hf}
\end{table}


\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Model Type} & \textbf{Timm Model Name} \\
        \midrule
        \texttt{ViT-B16} & \texttt{vit\_base\_patch16\_224} \\
        \texttt{ResNet50} & \texttt{resnet50} \\
        \texttt{VGG16} & \texttt{vgg16} \\
        \texttt{ConvNEXT} & \texttt{convnext\_tiny} \\
        \texttt{LeViT} & \texttt{levit\_conv\_128} \\
        \bottomrule
    \end{tabular}
    \caption{Mapping of model types to their corresponding \texttt{timm} model names.}
    \label{tab:model_mappings_timm}
\end{table}



\paragraph{Training}
We performed a grid search on a sample subject to determine the optimal learning rate, batch size, and temperature parameter, selecting the best values based on minimum validation loss.  
For per-participant EEG experiments, \emph{NICE} encoders were trained for up to 50 epochs with a batch size of 128, a learning rate of 0.0002, and a temperature of 0.04. The same hyperparameters were used for \emph{ATM-S}, except for the number of epochs, which was set to 80. \emph{EEGNet} and \emph{EEGConformer} were both trained for 200 epochs. \emph{EEGConformer} used a learning rate of 0.0002, a batch size of 128, and a temperature of 0.07, while \emph{EEGNet} used 0.01, 512, and 0.1, respectively.  
For cross-participant training, \emph{NICE} was trained for up to 150 epochs with a batch size of 512 and a learning rate of 0.0001.  
For MEG experiments, we used a learning rate of 0.00005, a batch size of 256, and a temperature of 0.1, training for up to 50 epochs. Training was halted in all models if validation loss did not improve for 25 consecutive epochs.

\section{Cross-Subject EEG Top-5 Retrieval Performances}
\cref{tab:cross_eeg_top5} presents the top-5 performance of the experiment presented in \cref{sec:cross_eeg}.
\begin{table}[h]
    \centering
    \caption{Cross-subject 200-way top-5 image retrieval performance of \emph{NICE} encoder on EEG signals using human-aligned or original image embeddings.}
    \label{tab:cross_eeg_top5}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l*{12}{c}} % 13 columns
    \toprule
    & \multicolumn{2}{c}{Ensemble} & \multicolumn{2}{c}{SynCLR} & \multicolumn{2}{c}{CLIP} & \multicolumn{2}{c}{OpenCLIP} & \multicolumn{2}{c}{DINO} & \multicolumn{2}{c}{DINOv2} \\
    \cmidrule(lr){2-13}
    & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base \\
    \midrule
    S1 & $33.6\pm2.6$ & $31.5\pm1.0$ & $32.6\pm2.7$ & $27.0\pm1.7$ & $33.7\pm2.8$ & $25.0\pm1.9$ & $33.9\pm3.2$ & $25.4\pm0.9$ & $32.3\pm2.9$ & $28.2\pm1.3$ & $29.3\pm0.9$ & $29.1\pm1.6$ \\
    S2 & $38.1\pm1.6$ & $38.4\pm1.4$ & $34.1\pm3.4$ & $32.8\pm0.7$ & $32.0\pm0.8$ & $25.2\pm2.0$ & $30.1\pm3.4$ & $27.8\pm2.3$ & $35.3\pm1.3$ & $33.3\pm1.7$ & $34.2\pm2.4$ & $27.6\pm1.6$ \\
    S3 & $28.3\pm1.4$ & $31.9\pm4.7$ & $33.5\pm1.9$ & $31.9\pm3.2$ & $30.7\pm3.3$ & $23.9\pm2.3$ & $33.2\pm4.4$ & $24.6\pm0.9$ & $30.8\pm1.4$ & $29.9\pm2.4$ & $26.6\pm1.6$ & $25.0\pm2.5$ \\
    S4 & $30.1\pm3.9$ & $33.6\pm1.9$ & $29.8\pm2.4$ & $30.1\pm3.6$ & $28.6\pm2.0$ & $24.8\pm1.5$ & $26.7\pm1.6$ & $24.2\pm1.5$ & $27.9\pm1.7$ & $27.3\pm2.2$ & $27.3\pm1.8$ & $27.6\pm1.5$ \\
    S5 & $23.5\pm0.8$ & $23.0\pm1.1$ & $21.9\pm2.0$ & $21.6\pm1.4$ & $22.1\pm2.1$ & $16.5\pm1.3$ & $23.3\pm3.7$ & $20.5\pm3.6$ & $21.6\pm1.9$ & $22.2\pm1.8$ & $19.3\pm3.4$ & $16.9\pm1.4$ \\
    S6 & $46.9\pm1.3$ & $41.1\pm1.4$ & $41.2\pm2.6$ & $36.7\pm3.7$ & $38.2\pm2.8$ & $32.9\pm1.4$ & $36.5\pm3.4$ & $32.3\pm1.6$ & $36.2\pm1.9$ & $36.8\pm1.3$ & $34.7\pm2.8$ & $26.3\pm1.7$ \\
    S7 & $35.2\pm2.6$ & $32.6\pm1.1$ & $29.8\pm1.6$ & $29.1\pm2.0$ & $28.8\pm3.2$ & $24.5\pm1.6$ & $27.3\pm0.9$ & $27.4\pm2.4$ & $32.0\pm3.3$ & $31.2\pm2.5$ & $25.9\pm3.1$ & $20.0\pm1.8$ \\
    S8 & $35.6\pm2.9pm$ & $35.3\pm3.4$ & $35.3\pm2.5$ & $34.5\pm1.3$ & $31.6\pm3.2$ & $28.2\pm1.3$ & $35.9\pm2.3$ & $31.4\pm0.9$ & $35.1\pm2.1$ & $31.5\pm3.2$ & $33.6\pm2.0$ & $26.3\pm1.2$ \\
    S9 & $33.8\pm2.9$ & $33.6\pm5.8$ & $29.5\pm2.0$ & $30.2\pm2.0$ & $27.1\pm3.5$ & $24.1\pm2.6$ & $26.4\pm1.2$ & $26.1\pm0.9$ & $32.4\pm1.8$ & $31.2\pm1.1$ & $28.1\pm2.1$ & $25.3\pm2.3$ \\
    S10 & $40.8\pm4.3$ & $40.2\pm1.9$ & $33.4\pm2.6$ & $32.7\pm1.3$ & $33.1\pm1.5$ & $28.4\pm2.7$ & $36.2\pm1.6$ & $32.2\pm2.3$ & $32.1\pm3.4$ & $33.9\pm3.2$ & $28.2\pm2.3$ & $31.9\pm2.5$ \\
    \midrule
    Avg. & $34.6\pm2.4$ & $34.1\pm2.4$ & $32.1\pm2.4$ & $30.7\pm2.1$ & $\boldsymbol{30.6\pm2.5}$ & $25.3\pm1.9$ & $\boldsymbol{30.9\pm2.6}$ & $27.2\pm1.7$ & $31.6\pm2.2$ & $30.5\pm2.1$ & $28.7\pm2.2$ & $25.6\pm1.8$\\
    \bottomrule
    \end{tabular}%
    }
\end{table}


\section{MEG Top-5 Retrieval Performances}
\label{supl:meg_top5}
\cref{tab:meg_top5} presents the top-5 performance of the experiment presented in \cref{sec:meg}.
\begin{table}[h]
\centering
\caption{200-way zero-shot top-5 performance of the \emph{NICE} encoder on MEG data using human-aligned or original image embeddings.}
\label{tab:meg_top5}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l*{12}{c}} % 13 columns
\toprule
& \multicolumn{2}{c}{Ensemble} & \multicolumn{2}{c}{SynCLR} & \multicolumn{2}{c}{CLIP} & \multicolumn{2}{c}{OpenCLIP} & \multicolumn{2}{c}{DINO} & \multicolumn{2}{c}{DINOv2} \\
\cmidrule(lr){2-13}
& HA & Base & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base \\
\midrule
S1 & $40.1\pm3.7$ & $40.1\pm1.8$ & $39.6\pm1.2$ & $35.6\pm1.8$ & $35.7\pm3.1$ & $26.6\pm0.9$ & $35.9\pm2.4$ & $29.0\pm1.1$ & $35.7\pm1.1$ & $34.1\pm2.3$ & $33.4\pm1.4$ & $21.2\pm1.9$ \\
S2 & $72.7\pm1.4$ & $62.9\pm1.3$ & $75.2\pm1.6$ & $62.1\pm2.0$ & $62.5\pm2.3$ & $44.7\pm1.4$ & $64.5\pm1.2$ & $53.1\pm1.5$ & $66.5\pm1.9$ & $58.1\pm1.9$ & $60.7\pm3.6$ & $34.1\pm1.5$\\
S3 & $58.6\pm2.0$ & $49.6\pm1.8$ & $55.0\pm2.5$ & $47.7\pm2.3$ & $48.8\pm1.9$ & $36.0\pm1.9$ & $53.3\pm3.0$ & $41.8\pm2.1$ & $52.1\pm1.9$ & $42.9\pm2.8$ & $46.8\pm4.0$ & $23.1\pm2.5$ \\
S4 & $36.4\pm2.1$ & $35.1\pm1.4$ & $38.6\pm0.8$ & $31.9\pm3.7$ & $36.7\pm1.8$ & $26.4\pm2.1$ & $32.9\pm1.3$ & $33.5\pm3.2$ & $37.3\pm4.0$ & $30.6\pm2.6$ & $32.4\pm1.0$ & $19.4\pm1.1$ \\
\midrule
Avg. & $\boldsymbol{51.9\pm2.3}$ & $46.9\pm1.6$ & $\boldsymbol{52.1\pm1.5}$ & $44.3\pm2.5$ & $\boldsymbol{45.9\pm2.3}$ & $33.4\pm1.6$ & $\boldsymbol{46.6\pm2.0}$ & $39.3\pm2.0$ & $\boldsymbol{47.9\pm2.2}$ & $41.4\pm2.4$ & $\boldsymbol{43.3\pm2.5}$ & $24.4\pm1.8$ \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{More Examples of Retrieved Images}
\label{supl:retrived_large}
In addition to the original \emph{Things EEG2} test set of 200 samples, we explored retrieved images from a larger dataset. Specifically, using EEG data from the test set, we computed the distances between EEG embeddings and all image embeddings from both the training and test sets, expanding the search space to 16,740 images with a chance performance of ~0.006\%. This broader search provides deeper insights into the neighboring images of an EEG sample and highlights differences between the representation spaces learned with human-aligned and unaligned image embeddings.  

Figure~\ref{fig:retrieved_large} presents examples of the top-5 retrieved images from this larger dataset. The results suggest that human-aligned (\emph{Dreamsim}) models are more sensitive to both color and pattern. As seen in Figures ~\ref{subfig:5},~\ref{subfig:6},~\ref{subfig:7}, and~\ref{subfig:9}, while both the top (human-aligned) and bottom (original) rows contain images with similar patterns to the ground truth (query) image, those retrieved by human-aligned models also share similar colors with the ground truth. Additionally, while the original models sometimes better estimate object categories, human-aligned models appear to be more effective in retrieving the correct orientation and overall shape of the object (see Figures~\ref{subfig:1},~\ref{subfig:2},~\ref{subfig:3},~\ref{subfig:4},~\ref{subfig:8},~\ref{subfig:10}). Table~\ref{tab:retreived_large} show the retrieval performance when using the large image set for a representative subject. The results show that for human-aligned models the top-1 performance can achieve up to 12.5\% which is significantly higher than the chance performance (0.006\%).

We also visualized some of the retrieved images using human-aligned models with \emph{gLocal} as the alignment method, as shown in \cref{fig:retrieved_large_glocal}. Interestingly, images retrieved from \emph{gLocal} model embeddings exhibit greater consistency in object category compared to both the original unaligned versions and \emph{Dreamsim} models. However, unlike \emph{Dreamsim}, images retrieved from \emph{gLocal} embeddings do not necessarily match the ground truth image in terms of orientation and color (see Figures~\ref{subfig:g2},~\ref{subfig:g3},~\ref{subfig:g4}, and~\ref{subfig:g6}).  

This highlights an important point: each human-aligned model prioritizes attributes emphasized in the dataset used for alignment with human perception, which directly influences their learned representations (as discussed in \cref{sec:interpret}). Additionally, the way participants experience image stimuli during visual brain data collection affects their perception of the images. Currently available datasets have been collected using the RSVP paradigm, which primarily captures low-level and mid-level features. As a result, they align more closely with \emph{Dreamsim}'s approach. Future work should explore alternative experimental designs for brain data collection and investigate their relationship with different human-alignment methods.

\begin{table}[h]
\centering
\caption{200-way zero-shot performance of the \emph{NICE} encoder on EEG data of a representative subject using a larger image database for retrieval. numbers show top-1 accuracies and top-5 in paranthesis.}
\label{tab:retreived_large}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l*{12}{c}} % 13 columns
\toprule
& \multicolumn{2}{c}{Ensemble} & \multicolumn{2}{c}{SynCLR} & \multicolumn{2}{c}{CLIP} & \multicolumn{2}{c}{OpenCLIP} & \multicolumn{2}{c}{DINO} & \multicolumn{2}{c}{DINOv2} \\
\cmidrule(lr){2-13}
& HA & Base & HA & Base & HA & Base & HA & Base & HA & Base & HA & Base \\
\midrule
S10 & $9.5~(22.0)$ & $3.0~(12.0)$ & $12.5~(28.0)$ & $3.0~(12.0)$ & $4.5~(14.5)$ & $3.5~(8.5)$ & $6.0~(19.5)$ & $4.5~(11.5)$ & $7.5~(24.0)$ & $3.5~(12.5)$ & $3.0(10.0)$ & $0.5(2.0)$ \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Grad-CAM Analysis Per Participant}
\label{supl:interpret}
\subsection{Temporal Analysis}
In \cref{sec:interpret}, we plotted a histogram of large gradient values over time for a representative participant and image encoder. We extracted gradients from the projection layer of the \emph{NICE} model's convolutional encoder (\texttt{encoder.0.projection.0}) for models trained with five different seeds on one participant's data. Only positive gradient values were kept, highlighting features that positively contributed to predictions. These gradients were then normalized between 0 and 1 and interpolated to match the EEG input size. For each model (with a specific seed), we determined the 99th percentile of gradient amplitudes across all 200 test samples and used this as the amplitude threshold for that model, resulting in five different thresholds. Finally, we computed a histogram showing the number of gradients exceeding their respective thresholds across time for all images and seeds (1,000 samples in total).

We extended this analysis to all participants and image encoders, with the results shown in Figures~\ref{fig:kde_time_all_part1} to~\ref{fig:kde_time_all_part3}. The findings consistently indicate that human-aligned models focus more on earlier time windows, associated with low-level visual processing, compared to their unaligned counterparts, further corroborating the main paper's results.  

Finally, \cref{fig:kde_time_all_gLocal} illustrates the same procedure applied to two sample models (the top performers in the retrieval task) trained with \emph{gLocal} image embeddings. The results indicate that there is no consistent difference in the time windows attended to by \emph{gLocal} models compared to the original models.

\subsection{Spectral Analysis}
To analyze the frequency contributions to the model's predictions, we computed the energy of the Fourier transform of the gradient maps.  
Following the same procedure as in the previous section, we extracted gradients from \texttt{encoder.0.projection.0}, retained only positive values, normalized them between 0 and 1, and interpolated them to match the EEG input size. The five frequency bands were defined as described in \cref{sec:interpret}.  
We measured the power spectral density (PSD) using the periodogram method. The periodogram of a signal is obtained by squaring its Fourier transfoem values, as shown in~\cref{eq:periodogram}.
\begin{equation}
\label{eq:periodogram}
S_{xx}(f_k) = \frac{1}{N} \left| X_k \right|^2, \quad k = 0, 1, ..., N-1~~,
\end{equation}
where $X$ is the discrete Fourier transform of the signal and is computed as: 
\begin{equation}
X_k = \sum_{n=0}^{N-1} x_n e^{-i 2\pi k n / N}
\end{equation}
The normalized energy of each frequency band was calculated by summing the power values within that band and dividing by the total signal energy (sum of power values across all frequencies). This process was performed separately for each seed, and the bars in \cref{fig:psd} and Figures~\ref{fig:eeg_bands_all_part1}-\ref{fig:eeg_bands_all_part3} represent the mean and standard error across all seeds.

We repeated the procedure for different subjects and image encoders. As shown in Figures~\ref{fig:eeg_bands_all_part1}-\ref{fig:eeg_bands_all_part3}, models trained with human-aligned embeddings consistently exhibited greater focus on the Alpha and Beta bands, while original models emphasized the Delta band. This trend was observed across nearly all participants and image encoders. Although the Gamma band contributed more to predictions in some cases for human-aligned models, its effect was less consistent than the differences observed in other frequency bands.

Following the same approach, we repeated the analysis for the two best-performing \emph{gLocal} models. The results, visualized in \cref{fig:eeg_bands_all_glocal}, show that, in most cases, the original models attend more to Alpha and Beta frequencies compared to the \emph{gLocal} models.

As discussed in the paper, we believe the difference between \emph{gLocal} and \emph{Dreamsim} stems from the datasets used to train them. The superior performance of \emph{Dreamsim} in visual decoding from brain activity may be due to its focus on information that is emphasized by the experimental design used to collect brain data. In contrast, other models are optimized to highlight features such as categorical differences, which may not be well-captured in brain data collected using RSVP experiments. 


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_10_retrieved.png}
        \caption{}
        \label{subfig:1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_9_retrieved.png}
        \caption{}
        \label{subfig:2}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_34_retrieved.png}
        \caption{}
        \label{subfig:3}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_40_retrieved.png}
        \caption{}
        \label{subfig:4}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_42_retrieved.png}
        \caption{}
        \label{subfig:5}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_62_retrieved.png}
        \caption{}
        \label{subfig:6}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_80_retrieved.png}
        \caption{}
        \label{subfig:7}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_149_retrieved.png}
        \caption{}
        \label{subfig:8}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_157_retrieved.png}
        \caption{}
        \label{subfig:9}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk/query_85_retrieved.png}
        \caption{}
        \label{subfig:10}
    \end{subfigure}
    \caption{\textbf{Top-5 retrieved images from a larger image database (Dreamsim).} Query EEG signals were selected from the test set but the search for the corresponding image happened at the combined test and train image sets. Top rows show human-aligned (HA) and bottom rows show original models. The numbers are cosine similarity between the EEG and image embeddings. \emph{Dreamsim} was used as the alignment method.}
    \label{fig:retrieved_large}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_10_retrieved.png}
        \caption{}
        \label{subfig:g1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_9_retrieved.png}
        \caption{}
        \label{subfig:g2}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_23_retrieved.png}
        \caption{}
        \label{subfig:g3}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_33_retrieved.png}
        \caption{}
        \label{subfig:g4}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_38_retrieved.png}
        \caption{}
        \label{subfig:g5}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_62_retrieved.png}
        \caption{}
        \label{subfig:g6}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_160_retrieved.png}
        \caption{}
        \label{subfig:g7}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topk_glocal/query_124_retrieved.png}
        \caption{}
        \label{subfig:g8}
    \end{subfigure}
    \caption{\textbf{Top-5 retrieved images from a larger image database (gLocal).} Query EEG signals were selected from the test set but the search for the corresponding image happened at the combined test and train image sets. Top rows show human-aligned (HA) and bottom rows show original models. The numbers are cosine similarity between the EEG and image embeddings.}
    \label{fig:retrieved_large_glocal}
\end{figure}

% Part 1
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_synclr_vitb16_gradcam_histogram_all_participants.pdf}
        \caption{SynCLR}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_ensemble_gradcam_histogram_all_participants.pdf}
        \caption{ENSEMBLE}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Temporal Analysis (Part 1).} EEG encoders are trained with original unaligned and human-aligned image embeddings using Dreamsim as the alignment method.}
    \label{fig:kde_time_all_part1}
\end{figure}

% Part 2
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_CLIP_ViT-B32_gradcam_histogram_all_participants.pdf}
        \caption{CLIP}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_OpenCLIP_ViT-B32_gradcam_histogram_all_participants.pdf}
        \caption{OpenCLIP}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Temporal Analysis (Part 2).}EEG encoders are trained with original unaligned and human-aligned image embeddings using Dreamsim as the alignment method.}
    \label{fig:kde_time_all_part2}
\end{figure}

% Part 3
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_DINO_ViT-B16_gradcam_histogram_all_participants.pdf}
        \caption{DINO}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_DINOv2_ViT-B14_gradcam_histogram_all_participants.pdf}
        \caption{DINOv2}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Temporal Analysis (Part 3).} EEG encoders are trained with original unaligned and human-aligned image embeddings using Dreamsim as the alignment method.}
    \label{fig:kde_time_all_part3}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/gLocal_clip_rn50_gradcam_histogram_all_participants.pdf}
        \caption{CLIP-RN50}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/gLocal_DINO_ViT-B16_gradcam_histogram_all_participants.pdf}
        \caption{DINO-ViT$\backslash$B16}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Temporal Analysis (gLocal).} EEG encoders are trained with original unaligned and human-aligned image embeddings using gLocal as the alignment method.}
    \label{fig:kde_time_all_gLocal}
\end{figure}

% Part 1
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_synclr_vitb16_gradcam_eeg_bands_all_participants.pdf}
        \caption{SynCLR}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_ensemble_gradcam_eeg_bands_all_participants.pdf}
        \caption{ENSEMBLE}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Spectral Analysis (Part 1).} EEG encoders are trained with original unaligned and human-aligned image embeddings using Dreamsim as the alignment method. Stars over bars represent significant difference (greater) with $p<0.05$.}
    \label{fig:eeg_bands_all_part1}
\end{figure}

% Part 2
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_CLIP_ViT-B32_gradcam_eeg_bands_all_participants.pdf}
        \caption{CLIP}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_OpenCLIP_ViT-B32_gradcam_eeg_bands_all_participants.pdf}
        \caption{OpenCLIP}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Spectral Analysis (Part 2).} EEG encoders are trained with original unaligned and human-aligned image embeddings using Dreamsim as the alignment method. Stars over bars represent significant difference (greater) with $p<0.05$.}
    \label{fig:eeg_bands_all_part2}
\end{figure}

% Part 3
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_DINO_ViT-B16_gradcam_eeg_bands_all_participants.pdf}
        \caption{DINO}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dreamsim_DINOv2_ViT-B14_gradcam_eeg_bands_all_participants.pdf}
        \caption{DINOv2}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Spectral Analysis (Part 3).} EEG encoders are trained with original unaligned and human-aligned image embeddings using Dreamsim as the alignment method. Stars over bars represent significant difference (greater) with $p<0.05$.}
    \label{fig:eeg_bands_all_part3}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/gLocal_clip_rn50_gradcam_eeg_bands_all_participants.pdf}
        \caption{CLIP-RN50}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/gLocal_DINO_ViT-B16_gradcam_eeg_bands_all_participants.pdf}
        \caption{DINO-ViT$\backslash$B16}
    \end{subfigure}
    \caption{\textbf{Comparing Gradient Maps for EEG Models--Spectral Analysis (gLocal).} EEG encoders are trained with original unaligned and human-aligned image embeddings using gLocal as the alignment method. Stars over bars represent significant difference (greater) with $p<0.05$.}
    \label{fig:eeg_bands_all_glocal}
\end{figure}


\end{document}
