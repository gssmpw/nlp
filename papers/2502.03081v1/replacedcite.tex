\section{Related Work }
Decoding images from brain activity and aligning vision models with human perception are two independently researched directions. However, perceptually aligned models have yet to be explored for visual decoding from the brain. This section reviews prior work in both areas, highlighting the gap our study addresses.
\vspace{-5pt}
\subsection{Decoding Images from Brain Activity}

Understanding how humans perceive images and decode this information from brain signals has been a long-standing question in neuroscience and, more recently, in computer vision____. Early studies focused on decoding simple visual features like stimulus orientation____ and basic geometric shapes____. Advances in computer vision____ and large-scale datasets____ have enabled the retrieval and reconstruction of complex images from brain activity. Research has explored natural image and video decoding using fMRI____, MEG____, and EEG____. While fMRI achieves higher decoding accuracy, MEG and EEG offer better temporal resolution and accessibility. A common approach in these studies maps brain signals into pre-trained image representation spaces like CLIP____ and DINO____ using a self-supervised loss, such as InfoNCE (Equation~\ref{eq:clip-loss}). These mappings enable image retrieval and can serve as conditioning signals for image generative models like Stable Diffusion____ to reconstruct viewed images. Unlike prior work focusing on brain encoders and auxiliary modalities like text, we investigate the role of image encoders. Specifically, we examine how aligning image encoders with human perception impacts visual decoding from brain signals.
\vspace{-5pt}
\subsection{Perceptual Alignment of Vision Models}

Early computer vision models were inspired by human perception____, but modern models often diverge from it____. Recent research has sought to bridge this gap by aligning machine vision models with human perception. ____ proposed a neural harmonizer that co-trains deep networks to align with human visual strategies while maintaining task accuracy. Their approach leverages datasets with human judgments on salient image features from ImageNet____. Similarly, ____ introduced \emph{gLocal}, a linear transformation that aligns the global structure of neural representations with human similarity judgments while preserving local structure. Unlike naive transformations that degrade downstream task performance, \emph{gLocal} maintains or slightly improves performance while enhancing alignment with human perception. More recently, ____ introduced \emph{Dreamsim}, a metric fine-tuning vision models on a human image similarity dataset. ____ demonstrated that \emph{Dreamsim}-aligned models outperform unaligned ones on several downstream vision tasks, including object counting, instance retrieval, and image segmentation.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_dreamsim.pdf}
        \caption{}
        \label{fig:dreamsim}
    \end{subfigure}
    \begin{subfigure}[b]{0.36\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/performance_comparison_braindream_nice_glocal.pdf}
        \caption{}
        \label{fig:glocal}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/performance_comparison_braindream_nice_harmonization.pdf}
        \vspace{0.5pt}
        \caption{}
        \label{fig:harmonization}
    \end{subfigure}\\
    \caption{\textbf{200-way image retrieval performance from EEG signals}. For this task, we employ the \emph{NICE} EEG encoder____ and evaluate different human-alignment methods and image encoders. Each vertical pair of panels demonstrates the results for one human-alignment method. The results highlight a consistent improvement in image decoding performance when considering human-aligned image encoders. Stars represent significant difference confirmed by a paired T-test~($p<0.05$). Best viewed with zoom.}
    \label{fig:alignment_methods}
    \vspace{-2ex}
\end{figure*}