\clearpage
\onecolumn
\section*{Appendix A. Proof of Theorem 1}

Theorem 1 shows that a point attractor exists at the origin of the feature space.
We first define the attracting set $A$ and its basin $B$, and then provide a proof of Theorem 1.

\vspace{5pt}
\noindent \textit{\textbf{Definition 3.}
Let $\mathcal{X}$ be a Hausdorff space
and $\varphi(t, x)$ be a dynamical system on $\mathcal{X}$, where $t \in \mathbb{N}$ is a time index, $x \in \mathcal{X}$ is an initial point, and all $\varphi(t, \cdot): \mathcal{X} \to \mathcal{X}$ are continuous functions.
We say that a closed set $A \subset \mathcal{X}$ is an attracting set if there exists a neighborhood $U$ of $A$ that satisfies the following two conditions.
{
\setlength{\leftmargini}{20pt}
\begin{enumerate}
\item [(a)] There exists $T$ such that ~$\bigcap_{t \ge T}\{\varphi(t, x) : x \in U\} = A$.
\item [(b)]
There exists $T$ such that,
for every neighborhood $V$ of $A$,~$t \ge T \Rightarrow\{\varphi(t, x) : x \in U\} \subset V$.
\end{enumerate}
}
\noindent We define the basin $B$ of attraction of $A$ as $B = \bigcup_{t \ge 0} \{x : \varphi(t, x) \in U\}$.
}

\vspace{5pt}
\noindent \textit{\textbf{Theorem 1.}
Suppose that activation functions $f$ and $g$ in the dynamical system of Eqs.~(\ref{eq:de_v}, \ref{eq:de_h}) are given by the derivatives of RecLag $\Lagh$ in Eq.~(\ref{eq:reclag}) and the Lagrangian $\Lagv$ in Eq.~(\ref{eq:lag_modern}), respectively.
For any interaction matrix $\xi \in \mathbb{R}^{\Numh \times \Numv}$, a trivial point attracting set $A = \{\bm{0}\}$ exists at the origin
$\bm{0} \in \mathbb{R}^{\Numv}$ in the feature space when $\gamma > \Numh$ under the adiabatic limit $\tauv = dt$.}

\renewcommand{\proofname}{Proof}
\begin{proof}
With RecLag, the activation function $f_{\nu}$ is given by
\begin{align}
f_{\nu}(h)
&= \frac{\partial}{\partial h_{\nu}} \max \left(\log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)^{\frac{1}{\beta}}, 0 \right)\\
&= \chi \left( \log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)^{\frac{1}{\beta}} \right) \cdot \frac{\partial}{\partial h_{\nu}} \log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)^{\frac{1}{\beta}}\\
&= 
\chi \left(
\frac{1}{\beta}
\log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)
\right)
\cdot
\frac{1}{\beta}
\left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)^{-1} \cdot \frac{\beta}{\gamma} \exp \left({\beta h_{\nu}}\right)\\
&= 
\chi \left(
\log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)
\right)
\cdot
\left( \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)^{-1} \cdot  \exp \left({\beta h_{\nu}}\right)\\
&= 
\chi \left(
\log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right)
\right)
\cdot
\mathrm{softmax}_{\nu} (\beta h) \label{eq:23}
\end{align}
where
\begin{align}
\chi (x) =
\begin{cases}
1 & (x \geq 0)\\
0 & (x < 0)
\end{cases}.
\end{align}
Under the adiabatic limit, {\it i.e.}, when the dynamics of memory neurons is changing rapidly, we have
\begin{align}
h_{\mu} = \sum_{j=1}^{\Numv} 
\xi_{\mu j} v_{j}.
\end{align}
Thus, we obtain
\begin{align}
\text{Eq.~(\ref{eq:23})}
&=
\chi \left(
\log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta \sum_{j=1}^{\Numv} 
\xi_{\mu j} v_{j}}\right) \right)
\right)
\cdot
\mathrm{softmax}_{\nu} \left(\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v_{j} \right)\\
&= \chi(G(v)) \cdot
\mathrm{softmax}_{\nu} \left(\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v_{j} \right).
\end{align}
where
\begin{align}
G(v) =
\log
\left(
\frac{1}{\gamma}
\sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} v_{j}}\right)
\right).
\end{align}
The differential equation in Eq.~(\ref{eq:de_v}) is then written by
\begin{align}
\label{eq:29}
\tauv \frac{d v_{i}(t)}{d t} &= \sum_{\mu=1}^{\Numh} \xi_{i \mu} f_{\mu}(h(t)) - v_{i}(t)\\
&= \sum_{\mu=1}^{\Numh} \xi_{i \mu} \chi(G(v(t)))
~\mathrm{softmax}_{\mu} \left(\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v_{j}(t) \right) - v_{i}(t)\\
&= \chi(G(v(t))) \sum_{\mu=1}^{\Numh} \xi_{i \mu} ~\mathrm{softmax}_{\mu} \left(\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v_{j}(t) \right) - v_{i}(t).\label{eq:31}
\end{align}
To derive the update rule, we consider the first-order Taylor approximation
\begin{align}
v_{i}(t + \Delta t)
=
v_{i}(t) + \frac{d v_{i}(t)}{d t} \Delta t,
\end{align}
where $\Delta t$ is a small time step.
From Eq.~(\ref{eq:31}), we have 
\begin{align}
v_{i}(t + \Delta t)
&=
v_{i}(t)
+
\frac{\Delta t}{\tauv}
\left(
\chi(G(v(t))) \sum_{\mu=1}^{\Numh} \xi_{i \mu} ~\mathrm{softmax}_{\mu} \left(\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v_{j}(t) \right) - v_{i}(t)
\right).
\end{align}
Therefore, when $\tauv = \Delta t$, we have
\begin{align}
v_{i}(t + \Delta t)
&=
\chi(G(v(t))) \sum_{\mu=1}^{\Numh} \xi_{i \mu} ~\mathrm{softmax}_{\mu} \left(\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v_{j}(t) \right)
.
\end{align}
This yields the update rule with discrete time steps $k \in \mathbb{N}$ as follows:
\begin{align}
v^{(k+1)}_{i}
=
\chi
\left(
G(v^{(k)})
\right)
\sum_{\mu=1}^{\Numh}
\xi_{i\mu}
~\mathrm{softmax}_{\mu}
\left(
\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v^{(k)}_{j}
\right).
\end{align}

Finally, we show that $A = \{\bm{0}\}$ is an attracting set for every fixed $\xi$.
Suppose that $\mathcal{X} = \mathbb{R}^{\Numv}$ is the feature space.
We consider the Euclidean distance $d(x, y) = \|x - x^{\prime}\|_{2}$ between two points $x, x^{\prime} \in \mathcal{X}$.
Clearly, with the topology induced by the open balls
\begin{align}
U_{\epsilon}(x) = \{ x^{\prime} \in \mathcal{X} : d(x, x^{\prime}) < \epsilon\}~~(\epsilon > 0),
\end{align}
the space $\mathcal{X}$ is a Hausdorff space. The dynamic system $\varphi(k, x)$ is then given by
\begin{align}
\varphi(k, x) =
\begin{cases}
x & (k = 0)\\
\chi
\left(
G(\varphi(k, x))
\right)
\sum_{\mu=1}^{\Numh}
\xi_{i\mu}
~\mathrm{softmax}_{\mu}
\left(
\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} \varphi_{j}(k, x)
\right) & (k > 0)
\end{cases}.
\end{align}
Below, we show that the two conditions (a) and (b) in Definition 3 are satisfied.

\noindent \textit{Proof of (a).}
Let $U = U_{\epsilon}(\bm{0})$ be an open ball with
\begin{align}
\label{eq:38}
\epsilon = \frac{1}{\Numv \beta~\Xi} \log \frac{\gamma}{\Numh},
\quad
\Xi = \max_{\mu, j} |\xi_{\mu j}|,
\end{align}
where $\gamma > \Numh$.
For every $x \in U$, we have
\begin{align}
G(\varphi (0, x))
&=
\log
\left(
\frac{1}{\gamma}
\sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} x_{j}}\right)
\right)\\
&=
\log \left(
\sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} x_{j}}\right)
\right)
- \log \gamma\\
&\leq
\log \left(
\Numh
\max_{\mu} \exp \left( {\beta \sum_{j=1}^{\Numv} \xi_{\mu j} x_{j}}\right)\right)
- \log \gamma\\
&\leq
\max_{\mu} \left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} x_{j}}
\right) - \log \frac{\gamma}{\Numh}\\
&\leq
\Numv \beta \max_{\mu} \max_{j} \left({\xi_{\mu j} x_{j}}
\right) - \log \frac{\gamma}{\Numh}\\
& \leq
\Numv \beta~\Xi~\|x\|_{\infty}
 - \log \frac{\gamma}{\Numh}\\
 & \leq
\Numv \beta~\Xi~\|x\|_{2}
 - \log \frac{\gamma}{\Numh}\\
& <
\Numv \beta~\Xi~\epsilon
 - \log \frac{\gamma}{\Numh}\\
&= 0.
\end{align}
Thus, when $T = 1$, we have
\begin{align}
\bigcap_{t \ge T} \{\varphi(t, x) : x \in U_{\epsilon}\}
= \bigcap_{t \ge T} \{\bm{0}\} =
A.
\end{align}

\noindent \textit{Proof of (b).}
Suppose that $V = \{x : d(x, \bm{0}) < \epsilon^{\prime}\}$ is a neighborhood of $A$.
With the open ball $U = U_{\epsilon}(\bm{0})$ defined by Eq.~(\ref{eq:38}) and when $T = 1$, we have
\begin{align}
\{\varphi(t, x) : x \in U\} = A \subset V,
\end{align}
when $t \geq T$.

This shows that $A$ is an attracting set when $\gamma > \Numh$ for every fixed $\xi$.
\end{proof}

\section*{Appendix B. Proof of Theorem 2}
\textit{\textbf{Theorem 2.}
Let $v_{\mathrm{A}}$ and $v_{\mathrm{B}}$ be feature neurons of a vanilla MHN and a RecLag-based MHN, respectively. Suppose that $v_{\mathrm{A}}^{(0)} = v_{\mathrm{B}}^{(0)}$. For every $\epsilon > 0$,
a small $\gamma > 0$ exists such that $\sup_{k} \|v_{\mathrm{A}}^{(k)} - v_{\mathrm{B}}^{(k)}\|_{2} < \epsilon$.
}

\begin{proof}
The update rules for $v_{\mathrm{A}}$ and $v_{\mathrm{B}}$ are given by
\begin{align}
v^{(k+1)}_{\mathrm{M},i}
&=
\sum_{\mu=1}^{\Numh}
\xi_{i\mu}
~\mathrm{softmax}_{\mu}
\left(
\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v^{(k)}_{\mathrm{M},j}
\right),\\
v^{(k+1)}_{\mathrm{R},i}
&=
\chi
\left(
G(v^{(k)}_{\mathrm{B}})
\right)
\sum_{\mu=1}^{\Numh}
\xi_{i\mu}
~\mathrm{softmax}_{\mu}
\left(
\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v^{(k)}_{\mathrm{R},j}
\right).
\end{align}
Let $0 < \delta < 1$ be a small constant and
\begin{align}
\gamma = \delta \min_{k} \sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} v^{(k)}_{\mathrm{M},j}}\right).
\end{align}
When $v^{(k)}_{\mathrm{B}} = v^{(k)}_{\mathrm{A}}$, we have
\begin{align}
\chi(G(v^{(k)}_{\mathrm{B}}))
&=
\chi
\left(
\log
\left(
\frac{1}{\gamma}
\sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} v^{(k)}_{\mathrm{R},j}}\right)
\right)
\right)\\
&=
\chi
\left(
\log
\left(
\sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} v^{(k)}_{\mathrm{M},j}}\right)
\right)
- \log{\gamma}
\right)\\
&= \chi
\left(
\log
\frac{
\sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} v^{(k)}_{\mathrm{M},j}}\right)
}{
\min_{k} \sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} v^{(k)}_{\mathrm{M},j}}\right)
}
- \log \delta\right)\\
&= 1
,
\end{align}
and we have
\begin{align}
\|v_{\mathrm{A}}^{(k+1)} - v_{\mathrm{B}}^{(k+1)}\|^{2}_{2}
&=
\sum_{i=1}^{\Numv}
\left(
\left( 1 - \chi
\left(
G(v^{(k)}_{\mathrm{B}})
\right)
\right)
\sum_{\mu=1}^{\Numh}
\xi_{i\mu}
~\mathrm{softmax}_{\mu}
\left(
\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v^{(k)}_{\mathrm{M},j}
\right)
\right)^{2}\\
&=
\left( 1 - \chi
\left(
G(v^{(k)}_{\mathrm{B}})
\right)
\right)^{2}
\sum_{i=1}^{\Numv}
\left(
\sum_{\mu=1}^{\Numh}
\xi_{i\mu}
~\mathrm{softmax}_{\mu}
\left(
\beta \sum_{j=1}^{\Numv} 
\xi_{\cdot j} v^{(k)}_{\mathrm{M},j}
\right)
\right)^{2}\\
&= 0.
\end{align}
This assumption gives us $v^{(0)}_{\mathrm{B}} = v^{(0)}_{\mathrm{A}}$, and thus, for every $\epsilon > 0$,
\begin{align}
\sup_{k} \|v_{\mathrm{A}}^{(k)} - v_{\mathrm{B}}^{(k)}\|_{2} = 0< \epsilon
\end{align}
\end{proof}

\section*{Appendix C. Proof of Theorem 3}
\textit{\textbf{Theorem 3.}
The basin $B_{0} = \{ v : G(v) < 0\}$ is identical to the set of points that have low probability density values, {\it i.e.}, a threshold $\delta$ exists such that
\begin{align}
B_{0} = \{x : p_{H}(X = x) < \delta\}.
\end{align}
}
\begin{proof}
With the joint probability distribution $p_{H}(X=x, M=\mu)$ given by Definition~2, the marginal distribution $p_{H}(X=x)$ is given by
\begin{align}
p_{H}(X = x)
=
\sum_{\mu=1}^{\Numh}
\frac{1}{Z} \exp \left(
{\beta \sum_{j=1}^{\Numv} \xi_{\mu j} x_{j}}
\right)
=
\frac{\gamma}{Z} \exp(G(x)).
\end{align}
Therefore, for fixed values of $\xi$ and $\gamma$, we have
\begin{align}
\delta = \frac{\gamma}{Z}
\end{align}
satisfying
\begin{align}
B_{0}
= \{v: G(v) < 0\}
= \{v: \exp(G(v)) < 1\}
= \left\{x: p_{H}(X = x) < \delta \right\}.
\end{align}
This shows that the basin is a set of data samples that have a probability density lower than $\delta$.
\end{proof}

\section*{Appendix D. Implementation details}

Three image classification networks were used: ResNet-18~\cite{he2016resnet},
ResNet-34~\cite{he2016resnet}, and
WideResNet40-2~\cite{zagoruyko2016wide}.
Each network was trained on an ID dataset using cross-entropy loss for 200 epochs with an SGD momentum optimizer. The initial learning rate was set to 0.1, and it was decayed by a factor of 0.1 at 100 and 150 epochs. The batch size was set to 128. Random cropping and horizontal flipping were used to augment the training images.
The dimension of the output representation was 512, and thus the number of feature neurons was set as $N_{V} = 512$.
During the training of the interaction matrix, normalization was applied to the output representations so that the L2 norms are 10.0.
The interaction matrix was trained for 100 epochs by following the training method for SFNN proposed by \citet{tang13sfnn}, where the number of memory neurons is set as $N_{H} = 250$, the inverse temperature parameter $\beta$ is set to 5.0, and the number of samples for Monte Carlo approximation is set to 5.
The objective function was computed using the input features as targets, as described in Eq.~(\ref{eq:objective_sfnn}). The OOD datasets were prepared following \citet{shen2023posthoc}, with all images resized to $32 \times 32$. We used the official implementation of Energy~\cite{liu2020energy}, ReAct~\cite{sun2021react}, MHE and SHE~\cite{shen2023posthoc} to report their results.
