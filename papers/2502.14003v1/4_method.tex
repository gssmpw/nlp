
\section{Rectified Lagrangian}

This section introduces RecLag, a Lagrangian function that creates a point attractor for OOD samples in the dynamical system of HMNs.
As shown in Figure~\ref{fig:energy},
RecLag creates a point attractor in the feature space.
This attractor is designed to exist for any interaction matrix $\xi$, enabling OOD detection by identifying data samples that fall into it as OOD.

\subsection{Definition}

To incorporate a point attractor for OOD samples in the dynamical system, we propose a minimal yet effective modification to the Lagrangian function of memory neurons.
Specifically, we introduce an {\it inverse memory strength constant} $\gamma$, which determines the strength of ID samples stored in memory, with a $\max$ function to screen out negative values, which is applied in the same way as ReLU.
The proposed RecLag is defined as follows.

\vspace{5pt}
\noindent \textit{\textbf{Definition 1.}
We define RecLag as
\begin{align}
\label{eq:reclag}
\Lagh(h) = \max \left(
\frac{1}{\beta}
\log \left( \frac{1}{\gamma} \sum^{\Numh}_{\mu=1} \exp \left({\beta h_{\mu}}\right) \right), 0 \right),
\end{align}
where $\beta, \gamma \in \mathbb{R}_{\ge 0}$ are constants.
}

\subsection{Existence of a Trivial Point Attractor}
With the dynamical system using RecLag $L_{H}$ in Eq.~(\ref{eq:reclag}) for memory neurons and the Lagrangian $\Lagv$ in Eq.~(\ref{eq:lag_modern}) for feature neurons, Theorem 1 shows that there exists a trivial point attractor at the origin of the feature space for any interaction matrix.

\vspace{5pt}
\noindent \textit{\textbf{Theorem 1.}
Suppose that activation functions $f$ and $g$ in the dynamical system of Eqs.~(\ref{eq:de_v},\ref{eq:de_h}) are given by the derivatives of RecLag $\Lagh$ in Eq.~(\ref{eq:reclag}) and the Lagrangian $\Lagv$ in Eq.~(\ref{eq:lag_modern}), respectively.
For any interaction matrix $\xi \in \mathbb{R}^{\Numh \times \Numv}$,
a trivial point attracting set $A = \{\bm{0}\}$ exists at the origin
$\bm{0} \in \mathbb{R}^{\Numv}$ in the feature space when $\gamma > \Numh$ under the adiabatic limit $\tauv = dt$.}
\vspace{-4pt}
\renewcommand{\proofname}{Sketch of proof}
\begin{proof}
With RecLag, writing the differential equations of the dynamical system in finite differences with $\frac{dv_{i}}{dt} \simeq \frac{v^{(k+1)}_{i} - v^{(k)}_{i}}{\Delta t}$ and $\tauv = \Delta t$
gives the following update rule for feature neurons:
\begin{align}
v^{(k+1)}_{i}
\hspace{-2pt}
=
\hspace{-1pt}
\chi
\hspace{-2pt}
\left(
\hspace{-1pt}
G(v^{(k)})
\hspace{-1pt}
\right)
\hspace{-2pt}
\sum_{\mu=1}^{\Numh}
\xi_{i\mu}
\mathrm{softmax}
\hspace{-2pt}
\left(
\hspace{-2pt}
\beta \sum_{j=1}^{\Numv} 
\xi_{\mu j} v^{(k)}_{j}
\hspace{-3pt}
\right)\hspace{-3pt},\hspace{-3pt}
\end{align}
where $k \in \mathbb{N}$ is a discrete time step, and 
\begin{align}
G(v) &=
\log
\left(
\frac{1}{\gamma}
\sum_{\mu = 1}^{\Numh}
\exp\left({\beta \sum_{j=1}^{\Numv} \xi_{\mu j} v_{j}} \right)
\right),\\
\chi (x) &=
\begin{cases}
1 & (x \geq 0)\\
0 & (x < 0)
\end{cases}.
\end{align}
When $v^{(k)} = \bm{0}$, we have $\chi(G(v^{(k)})) = 0$, and thus we have $v^{(k+1)} = \bm{0}$.
This shows that $\bm{0} \in \mathbb{R}^{\Numv}$ is a fixed point of the dynamical system in the feature space.
Further, with the epsilon neighborhood of the origin $U_{\epsilon} = \{u : \| u \|_{2} < \epsilon\}$, we have $\chi(G(u)) = 0$ for every $u \in U_{\epsilon}$ if $\epsilon$ is small enough. This shows that $A = \{\bm{0}\}$ is an attracting set for every fixed interaction matrix $\xi$.
A full proof is given in Appendix A.
\end{proof}

\subsection{Reduction to Vanilla MHNs}
Along with the existence of the trivial point attractor, it is also worth noting the limit where it disappears.
Theorem 2 shows that RecLag-based MHNs reduce to vanilla MHNs when the memory strength of ID samples is infinitely large, that is, when the inverse memory strength constant $\gamma \to 0$.
This theoretical result indicates that our approach is a natural extension of MHNs.
A proof is given in Appendix B.

\vspace{5pt}
\noindent \textit{\textbf{Theorem 2.}
Let $v_{\mathrm{A}}$ and $v_{\mathrm{B}}$ be feature neurons of a vanilla MHN and a RecLag-based MHN, respectively. Suppose $v_{\mathrm{A}}^{(0)} = v_{\mathrm{B}}^{(0)}$. For every $\epsilon > 0$,
there exists a small $\gamma > 0$ such that $\sup_{k} \|v_{\mathrm{A}}^{(k)} - v_{\mathrm{B}}^{(k)}\|_{2} < \epsilon$.
}

\subsection{Visualization and Discussion}
\noindent \textbf{Visualization.}
Figure~\ref{fig:energy} compares the energy distributions of a vanilla MHN and a RecLag-based MHN, where each red point indicates a fixed point $\xi_{\mu} \in \mathbb{R}^{N_{V}}$ at a local minimum of the energy function.
As shown, RecLag creates an attractor at the origin of the feature space. This attractor is associated with OOD samples as described in the next section.
The 3D visualization of these energy functions is shown in Figure~\ref{fig:teaser} with trajectories of a test sample (white diamond-shaped point) over time.
As shown, with the vanilla MHN, the test sample falls into one of the attractors even if it is an OOD sample. %with high Hopfield energy.
In contrast, with the RecLag-based MHN, the same test sample falls into the created attractor, indicating that none of the memory patterns are associated with it. This shows that the RecLag-based MHN can explicitly manage OOD samples in the dynamical system.

\noindent \textbf{Memory Strength.}
The size of the created attractor increases as the inverse memory strength constant $\gamma$ increases. Consequently, the number of samples identified as OOD samples also increases with $\gamma$.
This indicates that $\gamma$ can serve as a threshold parameter that adjusts the sensitivity of RecLag-based MHNs to OOD samples.
In practice, to draw a receiver operating characteristic (ROC) curve, one could vary $\gamma$ to generate different true positive rates (TPRs) and false positive rates (FPRs) for OOD detection.

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{fig_energy_v2.pdf}
\caption{
(a) MHN with an interaction matrix $\xi_{\mu i}$ between memory neurons $h_{\mu}$ and feature neurons $v_{i}$.
(b) Energy distribution of a vanilla MHN using the Lagrangians in Eq.~(\ref{eq:lag_modern}).
(c) Energy distribution of a RecLag-based MNH. The point attractor in Theorem 1 created by RecLag is marked by the yellow star.
(d) Probability density distribution in
Eq.~(\ref{eq:reclagprob}).
Data samples with low probability density values fall into the created attractor.
}
\label{fig:energy}
\end{figure*}

