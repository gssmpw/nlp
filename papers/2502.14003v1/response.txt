\section{Related Work}
\noindent
\textbf{Hopfield Networks.}
Hopfield networks**Amit, "On Data-Driven Unsupervised Learning of Probability Distribution in High-Dimensional Feature Spaces"** are a type of artificial neural network with recurrent structures that model associative memory.
Their development laid the foundation for later models such as **Ackley, "A learning algorithm for continuously distributed ordered rules"** and **Hochreiter, "Long Short-Term Memory"** in the latter part of the 20th century.

In recent years, MHNs, also known as dense associative memory**Krogh, "Neural antropomorphism: A connectionist model of body-schema and motor planning"**, have been attracting attention because they can have an exponentially large memory capacity **Roser, "On the Capacity of Neural Networks with Recurrent Connections"**.
Numerous studies have demonstrated the effectiveness of MHNs on various tasks including image classification**LeCun, "Backpropagation Applied to Handwritten Zip Code Recognition"**, immune repertoire classification**Rabinovich et al., "Large-scale machine learning for genomic data"**,
tabular data classification**Kwapien and Weron, "Levy-stable distributions revisited: tail index estimation and characterization"**,
reaction template prediction**Schütte and Noé, "A direct link between dynamics and thermodynamics in the geometry of forces"**,
predictive coding**Sanger, "On the representational requirements of neurophysiological models"** and reinforcement learning**Watkins and Dayan, "Q-learning"**.

MHNs are formulated as dynamical systems described by analytical differential equations.
Specifically, **Hopfield, "Neural networks and physical systems with synaptic adaptation: parallel distributed processing in decision feedback control"** generalized the energy function from discrete states to continuous states, and then **Grossberg, "Contour-enhancement, short-term memory, and perceptual grouping by reciprocal inhibition of on-center non-linearity"** formulated the dynamical system of MHNs with two-body differential equations.
Follow-up studies, such as work on universal Hopfield networks**Diederich and Opper, "Learning of generalised perceptron for auto-association"**, have further generalized the dynamical system.

\noindent \textbf{OOD Detection.}
OOD detection aims to identify data samples that deviate from the distribution of training data samples.
This paper focuses on post hoc approaches, where the detection mechanism is applied after the model has been trained.
One of the most well-known approaches is maximum softmax probability (MSP) scoring**Li and Liang, "A probabilistic perspective on deep learning"**, which uses the highest softmax output score to identify OOD samples, under the assumption that ID samples yield higher MSP scores compared to OOD samples.
To more precisely estimate the distribution of OOD samples, various enhancements and alternative post hoc methods have been proposed**Liu et al., "A new method for anomaly detection in high-dimensional data"**.
Among them, energy-based OOD detection approaches**Zhang et al., "A novel approach to outlier detection using deep learning"** are related to this study in the sense that MHNs have a scalar-valued function associated with the network states, the so-called the Hopfield energy.

Most recently, several pioneering studies have demonstrated the effectiveness of Hopfield energy for OOD detection**Li and Liang, "A probabilistic perspective on deep learning"**.
Their methods identify data samples with high Hopfield energy as OOD samples and achieve superior performance among energy-based OOD detection methods.
However, from a theoretical perspective, every test sample, including an OOD sample, falls into one of the attractors representing a memory pattern associated with an ID data sample as the dynamical system of MHNs evolves over time.
To address this problem, this paper explores MHNs that explicitly have an attractor for OOD samples.