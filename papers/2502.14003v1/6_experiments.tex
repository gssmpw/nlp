\section{Experiments}

We focus on evaluating OOD detection performance of our proposed method along with strong baselines in this work.

\subsection{Experimental Settings}

%\textcolor{red}{Conduct an experiment to compare the OOD detection capability of RecLag with other baselines. In order to verify the accuracy of only the OOD detection function of the proposed method in this experiment, we do not actually run the entire RecLag algorithm, but rather the OOD detection with the MHE and SHE functions of the \citet{zhang23she} method replaced by the Lagrangian $L_{H}$ is the subject of the experiment. Note that although \citet{lu2024learning} exists as a method that performs OOD by applying multiple proxy-based distributional representations to a single class in the same way as the proposed method, it is not included in this comparison. This is because RecLag is an extension of MHN and its OOD decision  is post-hoc approaches. Therefore, this comparison is limited to energy-based OOD detection, which is a post-hoc approach. Details of the training in this experiment can be found in Appendix D.}

\noindent \textbf{Datasets.}
Eleven image datasets were used to conduct OOD detection experiments:
CIFAR-10~\cite{krizhevsky2009cifar}, CIFAR-100~\cite{krizhevsky2009cifar},
SVHN~\cite{netzer2011svhn}, LSUN-C~\cite{yu2015lsun}, LSUN-R~\cite{yu2015lsun}, iSUN~\cite{Xu2015iSun}, Places365~\cite{zhou2017places}, DTD~\cite{cimpoi2014DTD}, TinyImageNet (TIN)~\cite{deng2009imagenet}, SUN~\cite{xiao2010sun}, and iNaturalist~\cite{van2018inaturalist}.
The CIFAR-10 or CIFAR-100 dataset was used as the ID dataset, and the other nine datasets were used as OOD datasets.

\noindent \textbf{Evaluation Measure.}
We used FPR95 as the primary evaluation measure, which is the FPR of OOD samples when the TPR for ID samples is 95.0\%.
ROC curves and the area under the curve (AUC) are also reported.

\noindent \textbf{Baselines.}
We chose five baseline methods:
MSP scoring \cite{hendrycks2017msp},
energy-based detection (Energy)~\cite{liu2020energy},
rectified activations applied to energy (ReAct)~\cite{sun2021react},
MHE~\cite{zhang23she}, and SHE~\cite{zhang23she}.
Note that the last four methods are energy-based OOD detection methods, with MHE and SHE being state-of-the-art using MHNs.
Also note that these methods, including ours, process representations from a frozen encoder. 
For a fair comparison, we use the same encoder in each experiment.
Another type of ODD detection methods (such as \citet{zhang23she}) that jointly optimize encoder and OOD module in a specific fashion is excluded.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{figure_roc_v2.pdf}
\caption{ROC curves (log-scale) and AUC$\uparrow$. Yellow background indicates that RecLag performed the best in terms of AUC.}
\label{fig:roc}
\end{figure*}
\noindent \textbf{Neural networks.}
Three image classification networks were used: ResNet18~\cite{he2016resnet},
ResNet34~\cite{he2016resnet}, and
WideResNet40-2 (WRN40-2)~\cite{zagoruyko2016wide}.
They were trained on an ID dataset using cross-entropy loss. The OOD benchmark was conducted with no dynamics simulations (no extra computational costs compared to the baselines). Other implementation details are given in Appendix D.

\subsection{Experimental Results}
\noindent \textbf{Comparison With Energy-Based Methods.}
Table~\ref{tab:fpr} shows the OOD detection performance on the nine OOD datasets with the three neural networks trained on the CIFAR-10 dataset.
As shown, our RecLag-based MHN (RecLag) achieved the best average performance across all neural networks.
This demonstrates the effectiveness of our approach, which incorporates an attractor for OOD samples in post-hoc OOD detection scenarios.

\noindent \textbf{ROC Curves.}
Figure~\ref{fig:roc} reports the ROC curves with the AUC values. As shown, RecLag exhibited the best AUC value in 23 out of 27 comparisons (highlighted with the yellow background), indicating its consistent superiority in OOD detection performance.

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{8.5pt}
{\footnotesize
\begin{tabular}{l|rrrrrrrrr|r}
\toprule
Method & SVHN & LSUN-C & LSUN-R & iSUN & Places & DTD & TIN & SUN & iNaturalist & Average \\
\midrule
%\multirow{6}{*}{\rotatebox[origin=c]{90}{ ResNet18}}
MSP & 73.51 & 67.42 & 88.65 & 86.73 & 66.61 & 81.79 & 83.06 & 73.57 & 72.27 & 77.07\\
Energy & 66.00 & 54.96 & 82.88 & 82.23 & 56.55 & 78.85 & 77.49 & 66.21 & 70.86 & 70.67\\
ReAct & 61.33 & 52.73 & 83.36 & 83.48 & 53.61 & 74.92 & 77.27 & 62.67 & 66.29 & 68.41\\
MHE & 16.24 & 41.21 & 67.61 & 56.08 & 9.99 & 40.80 & 61.79 & 10.35 & 17.22 & 35.70\\
SHE & 16.15 & 41.07 & 67.78 & 56.42 & \textbf{9.91} & 40.37 & 61.89 & \textbf{10.08} & \textbf{16.90} & 35.61\\
%RecLag & \textbf{13.07} & \textbf{39.59} & \textbf{61.50} & \textbf{51.81} & 10.87 & \textbf{36.94} & \textbf{55.17} & 12.81 & \textbf{18.44} & \textbf{33.36} \\
RecLag & \textbf{15.50} & \textbf{39.94} & \textbf{65.28} & \textbf{55.67} & 11.57 & \textbf{39.18} & \textbf{59.02} & 12.17 & 19.29 & \textbf{35.29} \\
& \textbf{$\pm$ 3.09} & \textbf{$\pm$ 0.80} & \textbf{$\pm$ 3.96} & \textbf{$\pm$ 4.52} & $\pm$ 0.54 & \textbf{$\pm$ 1.13} & \textbf{$\pm$ 4.43} & $\pm$ 0.64 & $\pm$ 1.29 & \textbf{$\pm$ 1.93} \\
\bottomrule
\end{tabular}
}
\caption{OOD detection performance as
FPR95(\%) $\downarrow$ with CIFAR-100 images being ID samples.
%Our RecLag-based MHN (RecLag) is compared with MSP~\cite{hendrycks2017msp}, Energy~\cite{liu2020energy}, ReAct~\cite{sun2021react}, MHE~\cite{zhang23she}, and SHE~\cite{zhang23she}.
WRN40-2 arch.~was used.
For other descriptions, see the caption of Table~\ref{tab:fpr}.
}
\label{tab:c100}
\end{table*}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{fig_score4_v2.pdf}
\caption{Comparison detection scores over time on LSUN-R. ResNet18 trained on CIFAR-10 was used.}
\label{fig:score}
\vspace{14pt}
\centering
\includegraphics[width=0.95\linewidth]{fig_auc_v2.pdf}
\caption{FPR95 $\downarrow$ and AUC $\uparrow$ over time on LSUN-R.}
%ResNet18 trained on CIFAR-10 is used.}
\label{fig:auc}
\end{figure}

\noindent \textbf{In-Distribution Data.}
To investigate how OOD detection performance is affected when a neural network is trained on a more complex task, Table~\ref{tab:c100} shows the results for WRN40-2 trained on CIFAR-100.
As shown, the OOD performance decreases for all methods compared to those in Table 1. This is because the variance of features in ID samples increased, making OOD detection more challenging.
However, even in this case, our RecLag-based MHN outperformed the other methods. This result indicates that the relative effectiveness of our approach is robust against differences in ID samples.

\noindent \textbf{Time Evolution.}
Figure~\ref{fig:score} analyzes how the detection scores change as the dynamical system of MHNs evolves over time.
With MHE, OOD samples have higher energy scores than ID samples at time $t = 0$; however, the scores decrease over time, making it almost impossible to distinguish between ID samples and OOD samples at a discrete time step of 4.
In contrast, RecLag-based MHN can distinguish between ID samples and OOD samples even after the score converges, thereby maintaining OOD detection performance over time as shown in Figure~\ref{fig:auc}.
This demonstrates that our approach successfully managed OOD samples within the dynamical system of MHNs.

\begin{figure}
    \centering
\includegraphics[width=0.9\linewidth]{fig_ood.pdf}
\caption{OOD samples incorrectly identified as ID samples. ID: CIFAR-100, OOD: TIN.}
\label{fig:analysis}
\end{figure}

\noindent \textbf{Visual Analysis.}
Figure~\ref{fig:analysis} analyzes the OOD samples incorrectly identified as ID samples.
As shown, images of animals, people, and foods were difficult to detect as OOD. This study focused on post-hoc OOD detection, but in the future, it would be interesting to simultaneously train MHNs and classification networks to further improve the performance.

