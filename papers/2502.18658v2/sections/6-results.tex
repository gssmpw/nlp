\section{Results}
%Below we report quantitative results from the study sessions and survey. 
Among 54 task instances, participants successfully completed the programming task in 50 instances, passing all test cases. 
In 4 instances, the task was halted as participants did not pass all test cases within 30 minutes.
The mean task completion time was 16 minutes 46 seconds, with no significant differences across system conditions, task orders, or tasks.
To understand the effects of proactivity on human-AI programming collaboration (RQ2), we first report participants' user experience comparison between prompt-based AI tools (e.g. ChatGPT), their perceived effort of use, and the sense of disruption.
We then describe participants' evaluation of the \sys{} probe's key design features, including the timing of proactive interventions, the AI agent presence, and context management.
Analyzing the 1004 human-AI interaction episodes, we illustrate how users interacted with the AI agent under different programming processes, as well as discuss participants' preference to utilize proactive AI in different task contexts and workflows (RQ3).
We also discuss the human-AI interplay between users and different versions of the system, covering their reliance and trust towards AI, and their own sense of control, ownership, and level of code understanding while using the tools.
% From a software engineering perspective, we discuss participants' preference to use proactive AI in different programming task contexts and workflow processes (RQ3).
% Finally, we report on task-based metrics to quantitatively evaluate the three versions of the system, as well as an analysis of the 1004 human-AI interaction episodes to illustrate how users interacted and made use of the AI agent under different programming processes. 

% Potential results we can report:
% Talk about the effect of proactivity on disruption and how our design helped
% Awareness of our AI agent design and effect on collaboration experience
% Which stages of programming were most suitable for proactivity?
% Time and effort with proactivity
% CONTEXT: At which stage, which subtask, what type of AI actions are preferred by the users
% PERCEPTION: Is acceptance or usability connected to user's biases or perceptions of AI tools
% Can lead to design principles of proactive AI in different domains



\subsection{\sys{} Reduces Expression Effort and Alleviates Disruptions}
% Increased productivity, compare to base line
Overall, participants found the increased AI proactivity in the CodeGhost and \sys{} conditions led to higher efficiency (P1, P2, P13, P15, P18). 
% P2 commented \textit{``I couldn't accomplish this [task] in a short time, so that's the reason I use that [AI support].'' }
% Similarly, P15 remarked: 
% \begin{quote}
%    \textit{ ``Now that I have experienced this AI assistant, I think that the arguments about AIs are out there taking programming jobs...has some merit to it... Just for the convenience of programming, I would love to have one of these in my home. (P15)''}
% \end{quote}
Participants commented that prompt-based tools, like Github Copilot or the PromptOnly in the study, required more effort to interact with (P7, P8, P10, P12, P14).
This was due to the proactive systems' ability to provide suggestions preemptively (P7), making the interaction feel more natural (P8).
After experiencing the CodeGhost and \sys{} conditions, P10 felt that \textit{``in the third one [PromptOnly], there was not enough [proactivity]. Like I had to keep on prompting and asking.''}

% This result was supported quantitatively based on time to convey and interpret...
The proactive agent interventions also resulted in less effort for the user to interpret each AI action in both CodeGhost and \sys{} compared to in PromptOnly (Figure \ref{fig:time_convey_interpret}). 
Among 857 recorded episodes where both the user and the AI agent had at least one turn of interaction (i.e. AI responded to the user's query or the user engaged with AI proactive intervention), we observed a significant difference in the amount of time to interpret the AI agent's actions (e.g., chat messages, editor code changes, presence cues) per interaction across three conditions (\textit{F}(2,856)= 41.1, \textit{p} < 0.001) using one-way ANOVA.
Using pairwise T-test with Bonferroni Correction, we found the interpretation time significantly higher in PromptOnly ($\mu$ = 34.5 seconds, $\sigma$ = 30.1) than in CodeGhost ($\mu$ = 19.8 seconds, $\sigma$ = 17.2; \textit{p} < 0.001) and \sys{} ($\mu$ = 18.7 seconds, $\sigma$ = 14.9; \textit{p} < 0.001). 
There was no significant difference in the time to interpret between the CodeGhost and \sys{} conditions (\textit{p} = 0.398; Figure \ref{fig:time_convey_interpret}). 
This indicates that when the system was proactive, participants spent less time interpreting AI's response and incorporating them into their own code, potentially due to the context awareness of the assistance to present just-in-time help.
We did not find a significant difference in the time to express user intent to the AI agent per interaction (e.g. respond to AI intervention via chat message, in-line comment, or breakout chat) (\textit{F}(2,652) = 2.36, \textit{p} = 0.095), despite qualitative feedback that the PromptOnly without proactivity was the most effortful to communicate with.


\begin{figure}[t]

\centering
\includegraphics[width=\columnwidth]{figures/Time_to_convey_and_interpret.pdf}
%https://docs.google.com/drawings/d/1CWPtvVLIvQhpS99MhPdVq3V5_PZS9gdYvgYkQgENFII/edit
\caption{\textbf{The time to express user intentions to the AI and the time to interpret the AI response per interaction.} \textnormal{Users' expression time was not significantly different across conditions \textit{F}(2,652) = 2.36, \textit{p} = 0.095). Users' interpretation time varied (\textit{F}(2,856)= 41.1, \textit{p} < 0.001), and was significantly lower for CodeGhost and \sys{} conditions than in PromptOnly.}}
\label{fig:time_convey_interpret}
% Link to google drawing: https://docs.google.com/drawings/d/1CWPtvVLIvQhpS99MhPdVq3V5_PZS9gdYvgYkQgENFII/edit?usp=sharing
\end{figure}

While proactivity allowed participants to feel more productive and efficient, they also experienced an increased sense of disruption.
% A Chi-squared test (\textit{$\chi^2$} = 13.8, \textit{df} = 2, \textit{p} < 0.01) shows significant difference in the number of disruptions 
This was especially prominent in the CodeGhost condition, when the AI agent did not exhibit its presence and provide context management (P1, P9, P10, P14).
% Overall, introducing proactive AI support led to an increased sense of disruption.
Disruptions occurred in different patterns across the three conditions.
In PromptOnly, the scarce disruptions arose from users accidentally triggering AI responses via the in-line comments (similar to Github Copilot's autocompletion) while documenting code or making manual changes during system feedback, leading to interruptions. 
% P10 specifically disliked using comments as instructions for AI: ``I feel like when I think of comments, I think of just writing helpful little notes for myself. Like I don't see them necessarily as instructions. So I feel like it would have been a little distracting right now.''
In CodeGhost, disruptions were due to users' lack of awareness of the AI's state, leading to unanticipated AI actions while they attempted to manually code or move to another task, making interventions feel abrupt. 
For example, P14 found the lack of visual feedback on which part of the code the AI modified made the collaboration chaotic.
Similarly, P12 felt that the automatic response disrupted their flow of thinking, leading to confusion.
In \sys{}, similar disruptions occurred less frequently with the addition of AI presence and threaded interaction.
% However, the additional agent visual signals and organizations of breakout chat messages could be overwhelming and lead to disruptions.
% However, miscommunications about turn-taking between the user and AI sometimes arose, resulting in both parties acting simultaneously and causing interruptions.
% Add quantitative support

Analyzing the Likert-scale survey data (Fig. \ref{fig:survey}) using the Friedman test, participants perceived different levels of disruptions among three conditions (\textit{$\chi^2$} = 22.1, \textit{df} = 2, \textit{p} < 0.001, Fig.\ref{fig:survey} Q1), with the highest in CodeGhost ($\mu$ = 4.61, $\sigma$ = 1.58), then \sys{} ($\mu$ = 3.78, $\sigma$ = 1.86) and PromptOnly ($\mu$ = 1.56, $\sigma$ = 1.15).
Using Wilcoxon signed-rank test with Bonferroni Correction, we found higher perceived disruption in CodeGhost than PromptOnly (\textit{Z} = 3.44, \textit{p} < 0.01), and in \sys{} than PromptOnly (\textit{Z} = 3.10, \textit{p} < 0.01).
We did not find a statistically significant difference in perceived disruption between CodeGhost and \sys{} (\textit{Z} = -1.51, \textit{p} = 0.131).
The perceived disruptions in \sys{} might be due to the additional visual cues exhibited by the AI agent and the breakout chat, which we further discuss in Section \ref{Results:presence_context}.

% The results presented a multifaceted outcome of using proactive AI assistance in programming.
% On the one hand, the AI reduced users' effort to specify and initiate help-seeking, enhancing productivity and efficiency by leveraging LLM's generative capabilities.
% Meanwhile, AI's increased involvement in user workflows created disruptions, but this was alleviated to an extent by our design of \sys{}, although participants perceived level of disruption varied widely (Fig.\ref{fig:survey} Q1).
% We further discuss designs to adapt the salience of the AI presence in the user interface 
% % and improvements to the timing of service 
% in the Discussion.



\begin{figure*}[h]

\centering
\includegraphics[width=0.85\textwidth]{figures/Codellaborator_Timing_Heuristics.pdf}
%https://docs.google.com/drawings/d/1CWPtvVLIvQhpS99MhPdVq3V5_PZS9gdYvgYkQgENFII/edit
\caption{\textbf{Summary of Heuristics for Proactive Assistance Timing.} Overall, we recorded 398 instances of AI proactivity defined by our timing heuristics (Table \ref{table:proactive-features}), with 212 (53.3\%) instances leading to effective user engagement, 48 (12.1\%) instances of disruptions, and 138 (34.7\%) instances of ignored AI proactivity.}
\label{fig:timing_heuristics}
% Link to google drawing: https://docs.google.com/drawings/d/1UW-dSkpbG0QZd4AoKhKW00GZha-VE8aNPmTWE58lVqA/edit
\end{figure*}


\subsection{Measuring Programming Sub-Task Boundary Is Effective to Time Proactive AI Assistance}
% Talk about the frequency, effectiveness of each design
% Discuss any qualitative feedback on each timing of design
To evaluate the design heuristics for the timing of proactive AI assistance (DG1, Table \ref{table:proactive-features}), we analyzed how each system feature and heuristic was utilized. 
Derived from the interaction data, we summarize the frequency, duration, and outcome of each heuristic design (Fig.\ref{fig:timing_heuristics}). 
Overall, we recorded 398 proactivity instances, with 212 (53.3\%) interactions leading to the user's effective engagement (e.g. adapting AI code changes, respond to the agent's message), 48 disruptions (12.1\%), and 138 interactions (34.7\%) where the user did not engage with the proactive agent (i.e. ignored or did not notice).
The most frequently triggered heuristics were code block completion (107 times), program execution (102 times), and user-written in-line comment (78 times). 
Additionally, the most effective heuristics that led to user engagement are multi-line change (73.1\%), user-written comment (69.2\%) and program execution (66.7\%).
Reflecting on the proactivity features, Design Rationale 2 --- intervening at programmer's task boundary --- was the most effective design principle overall.
The only exception is the heuristic of intervening at code block completion, which resulted in \revise{excessive AI responses. Many were affirmatory messages} to acknowledge the completed code and ask if the user needs further help. This led users to ignore around \revise{50\% of the proactive agent signals (Fig.\ref{fig:timing_heuristics})} to avoid disruption to their workflow.

% Add a couple lines on the user's comments on those
\revise{On the other hand}, the implementation of Design Rationale 3 --- intervening based on the user's implicit signals of adding a code comment or selecting a range of code --- resulted in many false positives that led to workflow disruptions.
Code comments and cursor selections conveyed different utilities for different users, which led to misinterpretation of user intent.
For example, P10 did not perceive comments as instructions for AI: ``\textit{I feel like when I think of comments, I think of just writing helpful little notes for myself. Like I don't see them necessarily as instructions. So I feel like it would have been a little distracting right now.}''
Code selection, similarly, was used by some participants as a habitual behavior to focus their own attention on a part of code. Therefore, the agent's proactivity could be perceived as unexpected and unnecessary.

Design Rationale 1 --- intervening at moments of low mental workload --- \revise{was not effectively operationalized}. 
Participants reported that when they were inactive for an extended period, they were likely thinking through the code design or solving an issue, which represents high mental workload. 
While it is likely that idleness is a signal to assist, participants preferred to initiate the help-seeking after they could not resolve the issue themselves, rather than having the AI agent intervene at a potentially mentally occupied moment.
The design rationale requires more involved modeling of the programmer's mental state to render it effective.
We outline the design implications from these finding in the discussion.



\begin{figure*}[t]

\centering
\includegraphics[width=\textwidth]{figures/Codellaborator_User_interaction_journey.pdf}

\caption{\textbf{Human-AI interaction timelines for P1.} \textnormal{For each task, we visualized each interaction initiated by the user and the AI, along with the time spent expressing the user's intent and interpreting the AI agent's response. We also visualized the annotated programming stages over the task. 
The Misc stage colored in black represents when the user was not actively engaged in the task (e.g. performing think-out-loud).
In PromptOnly, we observed the traditional command-response interaction paradigm where the user initiated most interactions. However, P1 unexpectedly triggered the AI agent when documenting code with comments, causing disruptions. In the CodeGhost condition, AI initiated most interactions, but this caused 6 disruptions, mainly during the Organize stage when P1 was making low-level edits and did not expect AI intervention. 
In the \sys{} condition, AI remained proactive but caused fewer disruptions, as P1 engaged in more back-and-forth interactions with higher awareness of the AI's actions and processes. See supplementary material for all timelines.}}
\label{fig:timeline}
% link to google drawing: https://docs.google.com/drawings/d/1Gz6nBWUN-ja1zI0tPYowdSLWafotLV5jUtYCOJjdFmw/edit?usp=sharing
\end{figure*}


\subsection{Users Adapted to AI Proactivity and Established New Collaboration Patterns}
% Describe how the users develop trust to the system, and some users become reliant or over-reliant on the AI
% Throughout the user study session, participants demonstrated calibration of their mental model of the AI agent's capabilities in the editor.
% Users naturally developed more trust and reliance as they used the AI to aid with their tasks, especially after they were exposed to proactive assistance.
Throughout the user study, participants calibrated their mental model of the AI agent's capabilities in the editor, developing a level of trust and reliance after experiencing proactive assistance.
% P15 used an analogy to a leader in a software engineering team, and that as the team establishes a \textit{``good track record of performing well, you're just naturally going to trust it.''}
Half of the participants ($N$ = 9) exhibited a level of reliance on the AI's generative power to tackle the coding task at hand and resorted to an observer and code reviewer role.
% P7 described their mentality shift as such: 
% \textit{``As programmers you're never really going to do extra work if you don't have to...You might as well take a little bit of a backseat on it and kind of only start working on it yourself once it's like complex logic that you need to understand yourself.''}
With this role change, participants shifted their mental process to focus more on high-level task design and away from syntax-level code-writing.
P3 reflected on their shift: \textit{``I kind of shifted more from `I want to try and solve the problem' to what are the keywords to use to get this [AI agent] to solve the problem for me... I could also feel myself paying less attention to what exactly was being written...So I think my shift focus from less like problem-solving and more so like prompts.''}
% P15 had a similar view of the proactive system: 
% \begin{quote}
%    \textit{ ``It [the AI] was the driver and I was the tool. Basically, I was the post-mortem tool, right. I was checking whether his code is correct, right. But it was the driver writing it. So in that case, I was not disrupted at all.. because the paradigm of the workflow has shifted. I am not in a position to be disrupted anymore, right. It was doing the heavy lifting. I was just doing the code review. (P15)''}
% \end{quote}
P10 expressed optimism toward developers' transition from code-writing to more high-level engineering and designing tasks:
\begin{quote}
    \textit{``I think with the increase of... low code, or even no code sort of systems, I feel like the coding part is becoming less and less important. And so I really do see this as a good thing that can really empower software engineers to do more. Like this sort of more wrote software engineering, more wrote code writing is just... it's not needed anymore.''}
\end{quote}

Under this trend of allowing the AI to drive the programming tasks, four participants (P3, P6, P7, P15) commented that they were still able to maintain overall control of the programming collaboration and steer the AI toward their goal.
P7 described their control as they adjusted to the level of AI assistance and navigated division of labor: \textit{``It's great to the point where you have the autonomy and agency to tell it if you want it to implement it for you, or if you want suggestions or something like you can tell [the AI] with the way it's written. It's always kind of like asking you, do you want me to do this for you? And I think that's like, perfect.''}
These findings highlight the potential for users to adopt proactive AI support in their programming workflows, fostering productive and balanced collaborations, provided the systems show clear signals of its capabilities for users to align their understanding to.






\subsection{Users Desired Varying Proactivity at Different Programming Processes}
\label{Results:programming_process}
% Some users expressed ambivalence to using proactive AI, good for efficiency, but bad for code understanding
Through analyzing the 1004 human-AI interaction episodes, we found that participants engaged with the AI the most (38.2\%) during the implement stage, 
followed by debug (26.4\%), 
analyze (i.e., examine existing code or querying technical questions like how to use an API; 11.5\%), design (i.e., planning the implementation; 10.9\%), organize (i.e., formatting, re-arranging code; 6.67\%), refactor (5.48\%), and miscellaneous interactions (e.g., user thanks the AI agent for its help; 0.697\%). 
We visualize P1's user interaction timeline as an example to illustrate different interaction types and frequencies under different programming stages (Fig.\ref{fig:timeline}).
% To perform this analysis, we referenced CUPS, an existing process taxonomy on AI programming usage \cite{Mozannar2022ReadingBT}, and adapted it to our research questions and applicable stages observed in our tasks.
% We acknowledge that the listed programming processes do not comprehensively represent different tasks and software engineering contexts.
% Rather, we cross-reference our analysis with qualitative feedback to identify user experiences in different stages of programming at a high level.
% With this broad categorization, we probed participants during the post-interview and identified processes of programming where proactive AI assistance was desired, and where it was disruptive and unhelpful.
To conduct this analysis, we adapted CUPS, an existing process taxonomy on AI programming usage \cite{Mozannar2022ReadingBT}, to align with our research questions and the stages observed in our tasks. 
By cross-referencing the interaction analysis with qualitative feedback, \revise{we identified programming stages where proactive AI assistance was most desired or disruptive.}
% This broad categorization allowed us to identify where proactive AI assistance was most desired and where it could be disruptive or unhelpful, providing valuable insights into optimizing AI support in programming.

In general, participants preferred to engage with the AI during well-defined boundaries between high-level processes, like providing scaffolds to the initial design or executing the code, and repetitive processes, such as refactoring.
They additionally desired AI intervention when they were stuck, for example during debugging.
In contrast, for more low-level tasks that require high mental focus, like implementing \revise{planned} functionality, participants were more often disrupted by proactive AI support and would prefer to take control and initiate interactions themselves.

This was corroborated by our interaction analysis results.
When examining the number of disruptions, we found that most disruptions occurred during the implementation process (32.7\%, 18 disruptions).
% and the implement (25.7\%, 9 disruptions). A disproportional amount of organize stage disruptions took place when participants were not intentionally moving forward with their processes or trying to advance the task. 
% Instead, participants were conducting low-level code organization (e.g., moving blocks around or adding empty lines between code blocks) to improve readability or satisfy personal preferences. 
% The AI agent's intervention during this stage was considered unnecessary. P3, who often documented and re-arranged their code, lamented about proactivity in P condition intruding their organization process: ``\textit{I would like type a comment, and then like, it would appear...giving me like three different text messages}.''
In contrast, \revise{very few} disruptions occurred during the debugging (7.27\%, 4 disruptions) and refactor phases (1.82\%, 1 disruption), which comprised 26.3\% and 5.48\% of all the interactions, respectively.
Most participants expressed the need to seek help from the AI agent in these stages and anticipated AI intervention as there were clear indications of turn-taking (i.e., program execution) and information to act on (i.e., program output, code to be refactored). 
After experiencing proactive assistance, P9 felt that ``\textit{[PromptOnly] wasn't responsive enough in the sense that when I ran the tests, I was kind of looking for immediate feedback regarding what's wrong with my tests and how I can fix it.}''
This corresponds with our proactivity design guideline to initiate intervention during subtask boundaries (Table \ref{table:proactive-features}, Design Rationale 2). 
In a sense, participants desired meaningful actions to be taken before AI intervened.
As P13 described, \textit{``If I'm like paste [code], something big, I run the program, the proactivity in that way, it's good. But if it's proactive because I'm idle or proactive because of a tiny action or like a fidget, then I don't really like that [AI] initiation.''}
% In different context, they would want different levels of proactivity, the action should match the level of severity
% Despite general agreement on preferred and less preferred programming processes to engage with proactive AI, participants did not reach a consensus and often expressed conflicting views on specific processes.
% For example, while P9, P16, and P17 desired proactive feedback after executing their programs and receiving errors, P14 and P18 were against using proactive AI for debugging as it might recurse into more errors, making the program harder to debug.
% Thus, in addition to adhering to general trends, future systems should also aim to be adaptive to the user's preferences, exhibiting different levels of proactive AI assistance according to best fit the user's personal needs and use cases.
% We propose a detailed design suggestion in Section \ref{Discussion:design_implication}.
\revise{Participants generally expressed preferences for programming processes where they wanted proactive support, but their opinions varied regarding which specific processes required more or less proactive assistance.}
For instance, while P9, P16, and P17 welcomed proactive feedback after program errors, P14 and P18 opposed it, fearing it could lead to more errors and complicate debugging. 
Therefore, future systems should adapt to individual user preferences, offering varying levels of proactive AI assistance based on personal needs and use cases. A detailed design suggestion is provided in Section \ref{Discussion:design_implication}.



\subsection{\sys{} and CodeGhost Feel \revise{More} Like Programming with a Partner than a Tool}
% Another effect of presence and context is a different sense of collaboration
% Another effect we observed from participants using the proactive conditions was an elevated sense of collaboration rather than using the programming assistant as a tool.
% While in all three conditions, the AI agent was initialized with the same prompt that enforces pair programming practices (Appendix \ref{appendix:prompt}), some participants expressed that working with the \sys{} and CodeGhost conditions felt more like collaborating with a more human-like agent with presence ($N$ = 6) than the PromptOnly.
We observed that participants in the proactive conditions perceived an elevated sense of collaboration with the AI, rather than viewing it as just a tool. 
Despite all three conditions using the same pair programming prompt (Appendix \ref{appendix:prompt}), six participants noted that \sys{} and CodeGhost felt more like collaborating with a human-like agent compared to PromptOnly.
% P1 commented that \textit{``it's like a person that's on your side [and says] `that's over here. You add that here' and kind of felt that way.''}
P6 reflected that \textit{``just the fact that it was talking with me and checking in with a code editor. I maybe treated it more like an actual human.''}
A part of this is due to the \revise{local scope of interaction with the agent in} the code editor (DG3), as P14 reflected \textit{``by changing the code that I'm working on instead of like on the side window...it feels more like physically interacting with my task.''}
Even the disruptions arising from the proactive AI actions facilitated a human-like interaction experience.
P9 recalled an interaction where they encountered a conflict in turn-taking with the AI: \textit{``[AI agent] was like, `Do you want to read the import statement? Or should I?' I was like, `No, I'll write it' and it [AI agent] said `Great I'll do it' and it just did it. Okay, yeah. True to the human experience.''}

This different sense of collaboration was \revise{reflected in} the survey results ((\textit{$\chi^2$} = 22.1, \textit{df} = 2, \textit{p} < 0.001, Fig.\ref{fig:survey} Q8).
Participants rated the AI assistant in the PromptOnly to be much like a tool ($\mu$ = 5.67, $\sigma$ = 1.58), while both the \sys{} ($\mu$ = 3.61, $\sigma$ = 1.65) and the CodeGhost conditions ($\mu$ = 4.17, $\sigma$ = 1.72) felt more like a programming partner (both \textit{p} < 0.001 compared to PromptOnly).
This more humanistic collaboration experience introduced by proactive AI systems naturally brings questions to its implications for programmers' workflow. We further share our analysis across programming processes in Section \ref{Results:programming_process} and the corresponding design suggestions in the Discussion.



\begin{figure*}[]

\centering
\includegraphics[width=0.8\textwidth]{figures/Codellaborator_survey.pdf}
% new link to google drawing:
% https://docs.google.com/drawings/d/1oiOvDttY3Xibk0P3hm0y1BJWbRh3cKMkh6or_Gu4XB4/edit?usp=sharing
% link to google drawing: https://docs.google.com/drawings/d/1rELkSVN1rroPm8ccWyVykW72SDOhkBKn3MjoaZZq4ec/edit?usp=sharing
\caption{\textbf{Likert-scale Response displayed in box and whisker plots comparing three conditions}. \textnormal{Anchors are 1 - Strongly disagree and 7 - Strongly agree. The green dotted lines represent the mean values for each question. Using the Friedman test, we identified significant differences in rating in Q1 for disruption, Q5 for awareness, and Q8 for partner versus tool use experience.}}
\label{fig:survey}
\end{figure*}



\subsection{Presence and \revise{Local Scope of Interaction} Increase User Awareness on AI Action and Process}
\label{Results:presence_context}
% To specifically evaluate our design of the \sys{} technology probe, we collected qualitative feedback on the AI presence and context management features, and their effects on the user experience compared to other conditions.
% Eight participants expressed that the AI agent's presence in the editor increased their awareness of the AI's actions, intentions, and processes.
We gathered qualitative feedback on the \sys{} technology probe's AI presence and \revise{breakout} features to assess their impact on user experience. 
Eight participants noted that the AI's presence in the editor enhanced their awareness of its actions, intentions, and processes (DG2).
Visualizing the AI's edit traces in the editor using a caret and cursor helped guide the users (P1, P4, P7, P12, P18) and allowed them to understand the system's focus and thinking (P12, P13, P18).
As P13 commented \textit{``I... like the cursor implementation of like, be able to see what it's highlighting, be able to move that cursor all the way just to see like, what part of the file it's focusing on.''}
The presence features also helped users identify the provenance of code and clarified the human-AI turn-taking.
As P10 remarked, \textit{``it was really clear when the AI was taking the turn with writing out the text and like the cursor versus when I was writing it.''}
On the other hand, the different scopes of interaction further increased users' awareness by reducing their cognitive load and enhancing the granularity of control (DG3).
For example, compared to a standard chat interface where \textit{``everything is just one very long line of like, long stream of chat''}, P6 preferred the threaded breakout conversations that decomposed and organized past exchanges.
P4 also found that the breakout \textit{``could be sort of like a plus towards steerability because you can really highlight what you want it to do.''}

% In survey, more awareness...
% Analyzing the survey response, we found that participants generally found the system to be highly aware of the user's actions, with no significant difference across conditions (\textit{$\chi^2$} = 5.83, \textit{df} = 2, \textit{p} = 0.054, Fig.\ref{fig:survey} Q6).
% Conversely, participants rated their own awareness of the AI differently (\textit{$\chi^2$} = 12.7, \textit{df} = 2, \textit{p} < 0.001, Fig.\ref{fig:survey} Q5), with the highest in PromptOnly ($\mu$ = 6.56, $\sigma$ = 0.511), then \sys{} ($\mu$ = 5.44, $\sigma$ = 1.79), then CodeGhost ($\mu$ = 4.17, $\sigma$ = 1.86).
% Specifically, the users felt like they were less aware of the CodeGhost condition prototype compared to the non-proactive PromptOnly (\textit{Z} = 2.5, \textit{p} < 0.001).
% We did not identify any other significant pairwise comparisons after Bonferroni Correction.
% This suggests that proactivity alone in CodeGhost induced more workflow interruptions, which in turn lowered users' perceived awareness of the AI system's action and process. 
% But similar to alleviating workflow disruptions, the \sys{} condition with visual presence and context management also improved user awareness.
Analyzing the survey responses, we found that participants generally rated the system as highly aware of their actions, with no significant difference across conditions (\textit{$\chi^2$} = 5.83, \textit{df} = 2, \textit{p} = 0.054, Fig.\ref{fig:survey} Q6). 
However, participants' \revise{own} awareness of the AI \revise{agent's actions} varied significantly (\textit{$\chi^2$} = 12.7, \textit{df} = 2, \textit{p} < 0.001, Fig.\ref{fig:survey} Q5), with the highest ratings in PromptOnly ($\mu$ = 6.56, $\sigma$ = 0.511), followed by \sys{} ($\mu$ = 5.44, $\sigma$ = 1.79), and the lowest in CodeGhost ($\mu$ = 4.17, $\sigma$ = 1.86). 
Specifically, participants felt less aware of the AI in CodeGhost compared to the non-proactive PromptOnly condition (\textit{Z} = 2.5, \textit{p} < 0.001). 
No other significant pairwise differences were found after applying Bonferroni correction. 
\revise{This can be attributed to CodeGhost's increased proactivity, which, in the absence of sufficient presence signals and a manageable local interaction scope, resulted in more frequent workflow interruptions. These interruptions, in turn, diminished users' ability to remain aware of the AI's actions and interpret its signals effectively, ultimately reducing their sense of control and understanding during the interaction.}

% These findings suggest that CodeGhost's proactivity led to more workflow interruptions and reduced users' perceived awareness of the AI's actions, presenting drawbacks to proactive assistance.

% However, not everyone was fond of presence and context, some find it distracting, and some don't want to go back to old conversations
% While the \sys{} condition with visual presence and context management improved user awareness,  not every participant found the agent design helpful. 
% Four participants thought that the AI presence could be distracting (P8, P10, P17), and two participants did not prefer the integration of AI in the editor, as they took up screen real estate (P6, P9).
% Similarly, P16 pointed out that their personal workflow would not involve using breakout chats for managing past interactions: \textit{``I also don't think I would have that many discussions with the AI once the coding is done and I have this working, then I'm probably not gonna look back at the discussions I've taken.''}
% The mixed findings on the system design indicate that different users, given different programming styles, workflows, preferences, and task contexts, desire different types of systems. We detail our design implications and suggestions in Section \ref{Discussion:design_implication}.
While the \sys{} condition showed improvements on user awareness, not all participants found the agent design helpful. 
Four participants felt the AI presence was distracting (P8, P10, P17), and two thought it occupied too much screen space (P6, P9). 
P16 noted that their workflow wouldn't involve using breakout chats to manage \revise{interaction context}: \textit{“Once the coding is done and I have this working, then I'm probably not gonna look back at the discussions I've taken.”} 
These mixed responses suggest that users with different programming styles, workflows, and preferences require varied system designs. Design implications are discussed in Section \ref{Discussion:design_implication}.




\subsection{Over-Reliance on Proactive Assistance Led to Loss of Control, Ownership, and Code Understanding}
% Discuss how some users are against overly trusting and relying on AI
% These users express concerns on their loss of control
Despite optimism in adopting proactive AI support in many participants, some participants (P6, P10, P11, P13, P16, P18) voiced concerns about over-relying on AI help, \revise{citing} a loss of control.
P10 felt like they were \textit{``fighting against the AI''} in terms of planning for the coding task, as the agent proactively makes coding changes during the implementation phase.
% P11 described that the AI agent \textit{``didn't let me implement it the way I wanted to implement it, it just kind of implemented it the way it felt fit.''}
They further expanded on the potential limitations of LLM code generation, particularly with regards to devising innovative solutions: \textit{``If it was too proactive with that, it would almost force you into a box of whatever data it's already been trained on, right?... It would probably give you whatever is the most common choice, as opposed to what's best for your specific project (P11).''}

% The effects also affect ownership of the code
The capability to understand rich task context and quickly generate solutions also lowered users' sense of ownership of the completed code.
P7 concluded that \textit{``the more proactivity there was, the less ownership I felt...it feels like the AI is kind of ahead of you in terms of its understanding.''}
% Code understanding, maintainability
This lack of code understanding was referenced by multiple participants (P11, P16, P18), raising issues on the maintainability of the code (P11, P14, P15) and security risks (P18).
As P11 suggested: \textit{``It's... not facilitating code understanding or your knowledge transfer. And yes, it's not very easily understood by others, if they just take a look at it.''}
Additionally, some participants believed that programmers should still invest time and effort to cultivate a deep understanding of the codebase, even if AI took the initiative to write the code.
P4 commented \textit{``I think the more that you leave it up to the AI, the more that you sort of have to take it upon yourself to understand what it's doing, assuming that you're being you know, responsible as [a] programmer.''}
Upon noticing that the AI was overtaking the control, P14 adjusted the way they utilized the proactive assistance and found a more balanced paradigm: \textit{``It's more like a conversation, like I gave him [AI] something so it did something, and then step by step I give another instruction and then you [AI] did something. I was being more involved, which allows me to like step by step understand what the AI is doing also to oversee, I was able to check it.''}

However, not all participants shared this concern. P10 expressed a different opinion as they felt like they are not \textit{``emotionally attached''} to their code, and that in the industry setting, code has been written and modified by many stakeholders anyways, so that \textit{``me typing it versus me asking the AI to type it, it's just not that much of a difference.''}
% This prompts an adaptive and balanced design that emphasizes user's control and reliance...
% This part of our findings uncovered different trade-offs between convenience and productivity from the utilization of more proactive and autonomous AI tools, and the potential loss of control during the programming process and less ownership of the end result.
\revise{This finding highlights the trade-offs between convenient productivity and potential risks in user control and code quality.}
While the system can be used to increase efficiency and free programmers from low-level tasks like learning syntax, documentation, and debugging minor issues, it remains a challenge to design balanced human-AI interaction, where the users' influences are not diminished and developers can work with AI, not driven by AI, to tackle new engineering problems.
We condense our findings into design implications in Section \ref{Discussion:design_implication}.


% \subsection{Old Disruption Subsection}

% % \subsection{Social Transparency Cues Reduced the Number of Disruptions Caused by Proactivity}
% Analyzing the Likert-scale survey data using the Friedman test, participants perceived different levels of disruptions among three conditions (\textit{$\chi^2$} = 17.4, \textit{df} = 2, \textit{p} < 0.001, Fig.\ref{fig:survey} Q1). 
% Using Wilcoxon signed-rank test with Bonferroni Correction, we found higher perceived disruption in the P condition ($\mu$ = 4.75, $\sigma$ = 1.42) than in the B condition ($\mu$ = 1.50, $\sigma$ = 0.90, \textit{Z} = 3.1, \textit{p} < 0.01), as well as a higher level of perceived disruption in condition S than B ($\mu$ = 3.75, $\sigma$ = 1.91, \textit{Z} = 2.9, \textit{p} < 0.01).
% We did not find a significant difference in the perceived level of disruption between condition P and condition S ($\mu$ = 3.75, $\sigma$ = 1.9, \textit{Z} = -1.7, \textit{p} = 0.085).
% The higher level of perceived disruptiveness compared to the baseline might be due to the added AI agent's visual signals burdening users' cognitive load. P6 commented that the presence of the AI cursor and thought bubble ``\textit{take up some, I guess real estate of like the editor itself.}'' Similarly, P7 found that the AI agent is S condition is ``\textit{very proactive and very like present with what you're doing. I think that can be potentially a tad bit overwhelming at times because especially with like the highlighting if you can see like what it's doing and it's doing everything.}''
% This indicates that while social transparency cues allow for more visibility of the AI agent's process, it can be overwhelming for the user to perceive all the visual feedback.

% Comparing all three system conditions using a one-way ANOVA test, we observe a significant difference in the mean number of disruptions participants experience per task (\textit{F(2,33)}= 5.86, \textit{p} < 0.01).
% We then conduct post-hoc analysis using Tukey's Honestly Significant Difference (HSD) for multi-group pairwise comparisons with control in overall familywise error rate.
% The P condition ($\mu$ = 2.08, $\sigma$ = 2.11) resulted in significantly more disruptions than the B condition ($\mu$=0.33, $\sigma$ = 0.89; \textit{p} < 0.01). 
% % Disruptions during the B condition were mainly due to the user unintentionally triggering an AI response when documenting code (i.e., activating a comment action), or the user attempting to manually make code changes while waiting for system feedback but then being disrupted by system feedback. 
% % Disruptions during the P condition stemmed from the lack of awareness the user had about the AI agent's state. Participants perceived no AI actions and desired to take control (i.e., manually coding or examining the file), potentially advancing to the next sub-task with a different context. However, the AI agent reacted to user actions without visibility about the interaction process and existing context, causing the interventions to be jarring and hard to interpret.
% In contrast, participants experienced significantly fewer disruptions during the S condition ($\mu$ = 0.58, $\sigma$ = 0.51) than the P condition (\textit{p} < 0.05). There was not a significant difference between the S and B conditions (\textit{p} = 0.89). 
% This result suggests a mediating effect of social transparency cues for alleviating the disruptions brought upon by a proactive AI agent.

% Disruptions occurred in different patterns across the three conditions.
% In condition B, disruptions were mainly due to the user unintentionally triggering an AI response when documenting code (i.e., activating a comment action), or the user attempting to manually make code changes while waiting for system feedback but then being disrupted by system feedback. 
% Disruptions during the P condition stemmed from the lack of awareness the user had about the AI agent's state. Participants perceived no AI actions and desired to take control (i.e., manually coding or examining the file), potentially advancing to the next sub-task with a different context. However, the AI agent reacted to user actions without visibility about the interaction process and existing context, causing the interventions to be jarring.
% Disruptions during the S condition are similar to those of the P condition, where users expected to take control but were interrupted, but they occurred less frequently. Another type of disruption with the S condition occurs when the user perceives and communicates with the AI about the turn-taking, but the coordination is ambiguous, so both the human and the AI agent act, causing disruptions.
% % Disruptions during the B condition occurred more often when neither the participant nor the AI agent was active, but both attempted to initiate interaction with poor coordination. Due to a period of participant inactivity, the AI agent could not accurately identify the participant's needs and current thought process. Thus, the timing of system intervention appeared more unforeseen.



% % \subsection{Proactivity and Social Transparency's Effects on Disruptions}
% % Comparing all three system conditions using a one-way ANOVA test, we observe a significant difference in the mean number of disruptions participants experience per task (\textit{F}(2,33)= 5.86, \textit{p} < 0.01).
% % We then conduct pairwise comparisons using T-test and Bonferroni Correction, judging statistical significance at \textit{p} < 0.0167 [0.05 / 3]. With this metric, we found that the P condition ($\mu$ = 2.08, $\sigma$ = 2.11) resulted in significantly more disruptions than the B condition ($\mu$=0.33, $\sigma$ = 0.89; \textit{p} < 0.0167). 
% % Participants experienced fewer disruptions during the S condition ($\mu$ = 0.58, $\sigma$ = 0.51), but we did not identify statistical significance when comparing to the P condition (\textit{p} = 0.0256). 
% % There was also not a significant difference between the S and B conditions (\textit{p} = 0.408). 

% % Disruptions during the B condition were mainly due to the user unintentionally triggering an AI response when documenting code (i.e., activating a comment action), or the user attempting to manually make code changes while waiting for system feedback but then being disrupted by system feedback. 
% % Disruptions during the P condition stemmed from the lack of awareness the user had about the AI agent's state. Participants perceived no AI actions and desired to take control (i.e., manually coding or examining the file), potentially advancing to the next sub-task with a different context. However, the AI agent reacted to user actions without visibility about the interaction process and existing context, causing the interventions to be jarring and hard to interpret.
% % Disruptions during the S condition are similar to those of the P condition, where users expected to take control but were interrupted, although less frequent. Another type of disruption with the S condition occurs when the user perceives and communicates with the AI about the turn-taking, but the coordination is ambiguous, so both the human and the AI agent act, causing disruptions.
% % nor the AI agent was active, but both attempted to initiate interaction with poor coordination. Due to a period of participant inactivity, the AI agent could not accurately identify the participant's needs and current thought process. Thus, the timing of system intervention appeared more unforeseen.

% % Analyzing the Likert-scale survey using the Friedman test, participants perceived different levels of disruptions among three conditions (\textit{$\chi^2$} = 17.4, \textit{df} = 2, \textit{p} < 0.001, Fig.\ref{fig:survey} Q1). 
% % Using Wilcoxon signed-rank test with Bonferroni Correction, we found higher perceived disruption in the P condition ($\mu$ = 4.75, $\sigma$ = 1.42) than in the B condition ($\mu$ = 1.50, $\sigma$ = 0.90, \textit{Z} = 3.1, \textit{p} < 0.01), and higher perceived disruption in condition S than B ($\mu$ = 3.75, $\sigma$ = 1.91, \textit{Z} = 2.9, \textit{p} < 0.01).
% % However, we did not find a significant difference in the perceived level of disruption between condition P and condition S (\textit{Z} = -1.7, \textit{p} = 0.085).

% % Despite a lower mean number of disruptions and lower mean rating for perceived disruption in condition S than P, we did not identify an effect of social transparency cues reducing disruptions significantly. 
% % This might be due to the visual signals indicating AI agent status in S condition adds to users' cognitive load. P6 commented that the presence of AI cursor and chat bubble ``\textit{take up some, I guess real estate of like the editor itself.}'' Similarly, P7 found that the AI agent is S condition is ``\textit{very proactive and very like present with what you're doing. I think that can be potentially a tad bit overwhelming at times because especially with like the highlighting if you can see like what it's doing and it's doing everything.}''
% % This indicates that while social transparency cues allow for more visibility of the AI agent's process, it can be overbearing for the user to perceive all the visual feedback.



% \subsection{Time and Effort to Convey to, and Comprehend, the AI}
% % \yc{is there a statement to make in the title? should have one takeaway msg per result as a paragraph-header}
% To examine the effects of proactivity on the facilitation of productivity and better human-AI collaboration, we analyzed the overall completion time for each condition. We also draw from interaction-level data to inspect the effort users needed to convey their intentions to the AI agent and to comprehend the system response.

% All tasks were completed within the 30-minute time allotment except for three. During these three instances, the participants did not come close to fulfilling the specifications of the tasks and were halted without passing all test cases. We observe one failed instance of each coding task, with two occurring during the P condition and one during the S condition. 
% The interaction episodes in the failed instances are still included in the data analysis, as they were natural interactions from first-time users.

% A one-way ANOVA did not reveal a significant difference in overall completion time for each condition (\textit{F}(2,33)= 0.425, \textit{p} = 0.657). This demonstrates that despite feeling more disrupted in proactive AI systems, the interventions do not slow down participants from completing the tasks.
% % , consistent with prior studies \cite{vaithilingam2022expectation}.
% % \yc{this is consistent with prior studies}.
% % or each task (\textit{F(2,33)}= 1.77, \textit{p} = 0.19). 
% % Thus, there is no evidence that the difference in task difficulties was a confounding factor. 
% % There was also no significant effect of ordering (\textit{F(X,X)} = 1.16, \textit{p} = 0.32). However, the range of task completion time is drastic, with the fastest completion being only 169 seconds (task order 1, condition P, budget tracker), which was lower than the time allotted and deviated from the overall mean completion time of 991 seconds. This could have been due to the inconsistent code generation quality from the LLM despite our effort to configure it with the least randomness.
% Despite no significant differences in task completion time, the proactive agent interventions in the P and S conditions resulted in less effort for the user to interpret each AI action than during the B condition (Figure \ref{fig:time_convey_interpret}). 
% We observe a significant difference in the amount of time to interpret the AI agent's actions (e.g., chat messages, editor code changes, presence cues) per interaction across three conditions (\textit{F}(2,637)= 46.7, \textit{p} < 0.001) from ANOVA.
% Using Tukey's HSD, we found the time to interpret per interaction was significantly higher in condition B ($\mu$ = 36.4 seconds, $\sigma$ = 30.8) than in condition P ($\mu$ = 17.7 seconds, $\sigma$ = 16.1; \textit{p} < 0.001) and S ($\mu$ = 17.0 seconds, $\sigma$ = 15.9; \textit{p} < 0.001). There was no significant difference in the time to interpret between the P and S conditions (\textit{p} = 0.90; Figure \ref{fig:time_convey_interpret}). 


% There was also not a significant effect of condition on the time to convey user intention to the AI agent (\textit{F}(2,432) = 0.744, \textit{p} = 0.476). The degree of freedom is lower as only user-initiated interactions required users to convey their intentions. However, five participants expressed that out of the three conditions, they felt that they spent the most effort communicating with the AI agent in condition B since they had to manually draft a message with a specific context (P1, P3, P6, P7, P12).

% Overall, these findings indicate that proactivity in AI programming systems might not result in a significant increase in productivity or efficiency, however, AI-initiated contextualized assistance could potentially contribute to more explainable and interpretable AI, with the decreased effort to understand AI responses and no extra cost to convey user intentions, at the scope of each interaction.

% \subsection{Proactivity Affects Awareness and Collaboration Experience}
% Further analyzing the survey results using Friedman test, we found that across three conditions,
% participants rated different levels on their sense of awareness of the AI agent (\textit{$\chi^2$} = 8.06, \textit{df} = 2, \textit{p} < 0.05, Fig.\ref{fig:survey} Q5). 
% Using Wilcoxon signed-rank tests with Bonferroni Correction, we found that compared to the baseline condition ($\mu$ = 6.33, $\sigma$ = 0.49), participants felt the AI agent was less aware of their actions in P condition ($\mu$ = 4.33, $\sigma$ = 2.06, \textit{Z} = -2.4, \textit{p} < 0.0167 [0.05 / 3]).
% We did not identify any significant difference between conditions B and S ($\mu$ = 5.58, $\sigma$ = 1.62, \textit{Z} = -1.2, \textit{p} = 0.202), nor between conditions P and S (\textit{Z} = 1.7, \textit{p} = 0.092).


% On the other hand, we found a significant difference in users' rating of the AI agent's awareness of the user's actions (\textit{$\chi^2$} = 8.21, \textit{df} = 2, \textit{p} < 0.05, Fig.\ref{fig:survey} Q6).
% % participants perceived different levels of disruptions among three conditions (\textit{$\chi^2$} = 17.61, \textit{df} = 2, \textit{p} < 0.001, Fig.\ref{fig:survey} Q1). 
% % Using Wilcoxon signed-rank tests, we found a similar pattern in higher perceived disruption in P condition ($\mu$ = 4.8, $\sigma$ = 1.4) than in B condition ($\mu$ = 1.5, $\sigma$ = 0.90, \textit{Z} = 3.1, \textit{p} < 0.01).
% Performing pairwise condition comparisons, we found that compared to the baseline condition ($\mu$ = 5.33, $\sigma$ = 0.98), participants felt the AI agent was more aware of their actions in P condition ($\mu$ = 6.42, $\sigma$ = 0.79, \textit{Z} = 2.4, \textit{p} < 0.0167). 
% We did not identify a significant difference between condition B and S ($\mu$ = 6.17, $\sigma$ = 0.83, \textit{Z} = 2.0, \textit{p} = 0.0442) nor between P and S conditions (\textit{Z} = -0.89, \textit{p} = 0.37).


% These results demonstrate that the proactivity in conditions P without social transparency cues simultaneously made users feel like the AI agent was more aware of the user's actions, while they were less aware of the AI agent's actions themselves. 
% % Whereas in condition S, participants did not necessarily feel less aware of the AI. 
% % This potentially indicates that the social transparency cues alleviated the abruptness of the AI agent's proactive actions. While we did not identify a significant pairwise difference between conditions P and S, 

% Interestingly, proactivity also affects users' perspectives on whether the collaboration felt like a partnership or merely utilizing a tool. 
% When asked whether they felt like the AI agent was more like a tool than a programming partner (Fig.\ref{fig:survey} Q8), we discovered significant differences across conditions (\textit{$\chi^2$} = 8.04, \textit{df} = 2, \textit{p} < 0.05). 
% % Similar to the pattern in system awareness, the conditions P ($\mu$ = 3.83, $\sigma$ = 1.59, \textit{Z} = -2.21, \textit{p} < 0.05) and S ($\mu$ = 4.0, $\sigma$ = 1.86, \textit{Z} = -2.35, \textit{p} < 0.05) were both perceived as closer to a programming partner than a tool when compared to the baseline ($\mu$ = 5.25, $\sigma$ = 1.71, \textit{Z} = 2.4, \textit{p} < 0.05). we did not find a significant pairwise difference between conditions P and S (\textit{Z} = 0.42, \textit{p} = 0.67).
% While we did not identify any significant pairwise comparisons between conditions, we received qualitative feedback that conditions P ($\mu$ = 3.83, $\sigma$ = 1.59) and S ($\mu$ = 4.0, $\sigma$ = 1.86) felt less like using a tool and more like a partnership experience than in baseline condition ($\mu$ = 5.25, $\sigma$ = 1.71).
% From the interviews, participants expressed that proactivity and social transparency cues led to a human-like programming collaboration. P1 in condition S commented that ``\textit{it's like a person that's on your side that [says] `here, you add that [code] here'}.'' P8 also expressed that in condition S, ``\textit{it felt a bit more human in the way that it was kind of interrupting you}.'' Similarly, P6 recalled that in conditions P and S, ``\textit{the fact that it was talking with me and checking in with a code editor. I maybe treated it more like an actual human}.'' 

% Based on these findings, AI proactivity could lead to users feeling less aware of the system's actions but more observed by the system with higher AI-to-user awareness. 
% When introduced with social transparency cues in condition S, participants did not feel less aware of the AI compared to the baseline. 
% However, more data needs to be collected to further explore if social transparency mediates the awareness gap when users interact with proactive AI tools.
% Proactivity and social transparency also potentially serve the purpose of facilitating a social presence, creating a partnership-like experience. 
% Future research can explore the implications of proactive AI design and how to best utilize this human-like collaboration experience.

% % The Likert-scale responses revealed that participants felt that the AI agent was more aware of the users' actions during the P ($\mu$ = 6.42, $\sigma$ = XXX; \textit{p} = 0.0071) and S conditions ($\mu$ = 6.17, $\sigma$ = XXX; \textit{p} = 0.035) compared to the B ($\mu$ = 5.33, $\sigma$ = XXX). On the other hand, the participants were less aware of the AI agent's actions during condition P ($\mu$ = 4.33, $\sigma$ = XXX) than condition B ($\mu$ = 6.33, $\sigma$ = XXX; \textit{p} = 0.0035). 

% % During the S condition, participants felt more aware of the AI agent's actions ($\mu$ = 5.58, $\sigma$ = XXX) than condition P ($\mu$ = XXX $\sigma$ = XXX; \textit{p} = 0.11) but less aware than in condition B ($\mu$ = XXX $\sigma$ = XXX; \textit{p} = 0.14). We did not identify statistical significance in the difference. A potential explanation is that social transparency cues mediated the disruptive effects of system proactivity, and provided more visual signals compared to the baseline, adding to users' cognitive load.



% % \subsection{Time and Completion}



