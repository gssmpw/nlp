\section{Evaluation}
To study the benefits and drawbacks of our design probe (RQ2) and understand the human-AI interaction workflows in different programming processes (RQ3), we conducted an in-person user study where participants collaborated with three versions of \sys{} in pair-programming sessions to specifically evaluate and compare each designed mechanisms (DG4). 
% We used the usage evaluation strategy in the HCI toolkit to guide our study design \cite{ledo2018evaluation}. 

\subsection{Participants}
We recruited 18 upper-level CS students from our university (8 female, 10 male; mean = 21.3 years, SD = 1.49 years, range = 19-24 years; denoted as P1-P18). Participants had a mean coding experience of 5.6 years. Fifteen of the participants had used LLM-based AI tools like ChatGPT \cite{chatgpt}, and thirteen participants used AI tools for programming at least occasionally. Participants were recruited via a posting in the Discord and Slack channels for CS students at the university. Each study session lasted around 90 minutes and participants were compensated with \$40. The study was approved by the ethics review board at our institution.

% \begin{itemize}
%     \item Recruit students who have Python programming experience. Note whether they have prior AI tool usage for later analysis
%     \item Agent fixed on a level of proactivity and expertise that the researchers deemed appropriate
%     \item Design 2 Python coding tasks (~20 mins each). Use test-driven development (provide test case and let users implement code to pass test case), encourage discussion with AI 
%     \item Conduct 20 mins of story-focused semi-structured interviews to gather qualitative feedback on the use experience (e.g. how do people think about the proactive features, presence, and context? What were the challenges of collaborating with the system).
%     Ask questions about turn-taking, engagement, and cooperation.
%     \item Record quantitatively each episode of interaction (two types: user conveys to AI, and user tries to understand AI), and the flow of interaction (e.g. chat -> AI suggest code -> user writes code).
%     \item Note the number of times each user utilized each feature. Combine with the TAM and NASA-TLX in post-survey to see the effect of each feature.
%     \item Collect basic quantitative data such as completion time and accuracy
% \end{itemize}

\subsection{Study Design}
\label{Evaluation:study-design}
\revise{We conducted a within-subject study involving} three conditions on three system prototypes to examine the effects of different proactivity designs compared to a fully user-initiated baseline (RQ2, DG4).
% To account for ordering and learning effects, the presentation order of three conditions was counterbalanced. 
The condition orders were counterbalanced to account for ordering and learning effects.
The underlying LLM in all three conditions was initialized with the same system prompt (Appendix \ref{appendix:prompt}) and parameters (e.g. version, temperature, etc.).
The there conditions are described below:
\begin{itemize}
    \item \textbf{The PromptOnly condition} was an ablated version of \sys{}, similar to existing AI programming tools like Github Copilot \cite{githubcopilot} and ChatGPT \cite{chatgpt} where users prompt using code comments or chat messages to receive AI response. The system only reacts to users' explicit requests. This system did \underline{not} have access to proactively make code changes in the editor.
    \item \textbf{The CodeGhost condition} constructed an AI agent that takes proactive actions, such as sending messages or writing code in the editor, to provide help based on user activity and timing principles. In this condition, the ablated system did not include any additional indicators of the AI agent's presence, and did not \revise{support the localized, threaded, scope of interactions}.
    \item \textbf{The \sys{} condition} utilized the same AI agent and proactivity features found in the CodeGhost condition and additionally utilized the \revise{AI agent's visual features and interacted with the user at different scopes of context}. In this condition, the full system represented the AI via its autonomous cursor, caret, and intention signal bubble (Fig.\ref{fig:ui}.b,d). Moreover, users were allowed to use breakouts to start localized threads of conversations at different parts of the code (Fig.\ref{fig:ui}.a). \sys{} also automatically grouped relevant messages and organized them into breakouts to manage interaction context (Fig.\ref{fig:ui}.e).
\end{itemize}


\subsection{Tasks}
% Three programming tasks of similar difficulty were used during the study. The tasks involved implementing a small-scale project in Python (full descriptions in Appendix \ref{appendix:task}; Task 1: event scheduler, Task 2: word guessing game, Task 3: budget tracker). 
% We adopted test-driven development by providing users with a unit test suite for each task, where they had to implement the specification to pass the test cases. The tasks were intentionally open-ended and can be completed through various designs so that the LLM could not solve them deterministically. 
% In an attempt to maintain consistent generation quality across participants, the backend GPT-4 model was set to 0 temperature to reduce randomness. 
% Based on a pilot study with 6 users using the \sys{} condition prototype, we found that participants were able to complete each task within 20-30 minutes, thus showcasing similar task difficulties. 
% During the study, the tasks were randomly assigned to each condition to reduce bias.
Three programming tasks involving implementing a small-scale project in Python (full descriptions in Appendix \ref{appendix:task}; Task 1: event scheduler, Task 2: word guessing game, Task 3: budget tracker) were used in the study. 
\revise{The tasks were derived from LeetCode \cite{leetcodeMeetingScheduler, leetcodeWordGuessing, leetcodeBudgetSpending} coding problems, which present adequate challenges for our participant pool within the scope of the study. The particular tasks are selected to reflect typical programming activities commonly encountered by developers, including working with data structures, control flow, and basic algorithms.} 
We adopted test-driven development by providing users with a unit test suite for each task, where they had to implement the specification to pass the test cases. 
\revise{The tasks were designed to balance practical relevance and study feasibility, allowing participants to showcase problem-solving skills and creative design choices within a constrained time frame.} 
Although LeetCode problems are often designed with one or two optimized solutions, we modified the problems so users could take multiple approaches. For instance, one of our tasks involved implementing an event scheduler. While a brute-force approach could solve the problem, participants could also explore different data structures (e.g., priority queues, dictionaries) to optimize efficiency, or modularize their solution to enhance readability and maintainability. This flexibility allowed us to observe variations in user decision-making.
The tasks were intentionally open-ended and can be completed through various designs so that participants could not \revise{directly use task specification as LLM prompt to solve the problem deterministically}. 
In an attempt to maintain consistent generation quality across participants, the backend GPT-4 model was set to 0 temperature to reduce randomness. 
Based on a pilot study with 6 users using the \sys{} condition prototype, we found that participants were able to complete each task within 20-30 minutes, thus showcasing similar task difficulties. 
During the study, the tasks were randomly assigned to each condition to reduce bias.


\subsection{Procedure}
After signing a consent form, participants completed each of the three coding tasks. Before each task, participants were shown a tutorial about the system condition they would use for the task. Participants were asked to discuss the task with the AI agent at the start of each task to calibrate participants' expectations of the AI agent and to reduce biases from prior AI tool usage. Participants were given 30 minutes per task and were asked to adopt a think-aloud protocol. 
% We did not enforce a strict time limit as we did not want participants to rush to complete the task, however they were paused if the task was incomplete at the time limit. 
After each task, participants completed a Likert-scale survey (anchors: 1 strongly disagree to 7 strongly agree) about their experience in terms of the sense of disruption, awareness, control, etc. (Fig.\ref{fig:survey}). After completing all three tasks, participants underwent a semi-structured interview for the remainder of the study. Each session was screen- and audio-recorded and lasted around 90 minutes.

\subsection{Data Analysis}
We conduct in-depth qualitative and quantitative analysis \revise{on the collected data.}
Qualitatively, the semi-structured interview responses were individually coded by three researchers. 
Subsequently, thematic analysis \cite{braun2006using, vaismoradi2013content} was employed by one researcher to distill participants' \revise{key feedback on the dynamics of human-AI collaboration across different programming processes.}
% This qualitative approach enabled us to capture the complexities and subtleties of participants' experiences, perspectives, and the overarching themes that emerged from their interactions with the different design components of the AI system.
% By focusing on the qualitative data, we aimed to uncover the underlying factors that influenced participants' engagement with the AI agent and specific parts of the system design that affected their workflow, experience, and perception of AI programming tools.

Additionally, we conducted quantitative analysis on task-level and interaction-level statistics. We recorded the task duration and the number of test cases completed for all (3 X 18 = 54) task instances. 
We further logged and analyzed each human-AI interaction episode during the user study sessions.
An episode starts when either the user or the AI sends a message, and ends when the user moves on from the interaction (e.g. starts writing code after reading AI comments, or writes a response message to initiate a new interaction). 
The interaction data was then labeled with the timestamp, the duration, the expression time (e.g. the time lapsed to write a direct message to the AI agent), the interpretation time (e.g. the time lapsed for the user to read the AI agent's response or code edit), the current programming process (e.g., design, implement, or debug), and a description of the workflow between the human and the AI agent. 
This process resulted in 1004 human-AI interaction episodes for our analysis. 
Aggregating these interaction episodes, we recorded the number of disruptions, defined as instances when, during a system-initiated intervention, the user switched their context to process AI's actions but found them unhelpful and interruptive, resulting in the dismissal or reversion of the AI actions. 
% We also identified the number of ignored AI agent actions and signals, where the user does not spend any time, intentionally or unintentionally, to interpret the AI's action and communication. 
We further analyze the utility and the effectiveness of each design heuristic for the timing of AI assistance.
% This quantitative analysis offered a structured overview of the interaction patterns as well as the utilization and efficiency of individual design features at different stages of the programming workflow.
