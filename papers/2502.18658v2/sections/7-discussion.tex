\section{Discussion}

% \yc{you wanna add a transition paragraph that summarizies your findings and briefly introduces your discussion points.}

% \yc{these subsections are long, readers can easily get lost in your text without having a strong takeaway after finishing them. consider adding a short takeaway message per each paragraph or point you are making within each subsection. these shouldn't be a summary of the paragraph, but more like the teasing/leading text that would hook the readers up.}

% Our evaluation showcased proactivity's effect of reducing users' effort and increasing awareness of AI's actions, facilitating a partner-like collaborative experience.
% The proactive AI programming prototype in the CodeGhost condition also introduced more workflow disruptions, but this was alleviated with our design in \sys{} with salient AI agent presence and enhanced context management.
% While participants found the AI assistance productive and efficient, they expressed concerns about over-reliance as they experienced a loss of control, ownership, and code understanding.
% Overall, participants derived a variety of workflows collaborating with the AI assistant and conveyed a diverse set of personal preferences on the use case of proactive AI support depending on the programming process and task context.

Summarizing the study results, we outlined a set of design implications for future proactive AI programming tools and identified key challenges and opportunities for human-AI programming collaboration.
We further discuss the trend of transitioning from prompt-based LLM tools to more autonomous systems, exploring the potential impacts on software engineering and risks to consider.
% Below we outline the adaptive design of social transparency in human-AI collaboration and suggest future directions, discuss the trade-offs of adding proactivity to an AI programming tool, and identify the need to define clearer user-based turn-taking signals in HAI.
% \todo{Organize these as \~3 Design Guidelines/Suggestions and Design Opportunities. And finally discuss the adoption of proactive AI in software engineering, and the limitation of the project.}

\subsection{Design Implications}
\label{Discussion:design_implication}
Our design probe aimed to expand on existing guidelines on human-AI interaction, mixed-initiative interfaces \cite{amershi2019haiprinciple, horwitz1999mixedinitiative} to the context of proactive AI programming assistance.
From evaluating the participant feedback \revise{and} comparing three versions of the \sys{} probe, we summarized five design implications for future systems.
This is not a holistic list for designing proactivity in intelligent programming interfaces.
Rather, we hope that under the rising trend of more autonomous AI tools, our study findings can provide insights and suggestions, in addition to established guidelines, on how to design the AI agent's proactive assistance and the human-AI interplay in the specific context of programming using \revise{an} LLM.

\subsubsection{\textbf{Facilitate code understanding instead of pure efficiency}}
While participants appreciated proactive coding support, the highly efficient AI generation often did not reserve time for users to develop the necessary understanding of the code logic.
Since the generated code \textit{``looks very convincing (P13),''} participants were tempted to accept the suggestions and proceed to the next subtask.
Existing proactive AI programming features, such as the in-line autocompletion in Github Copilot, are often result-driven and strive to always provide code generations immediately, leaving users no time to internalize and think critically about the code.
While this can work for repetitive processes such as refactoring, the lack of code understanding could present issues in maintainability and validation, which could aggravate in software engineering settings when scaled to larger systems.
\revise{A large scaled survey on Github Copilot named troubles understanding and evaluating code as common usability issues \cite{liang2024usabilityCopilot}. This concern over code understanding could aggregate and require more considerations in a proactive system, where the pace of interactions is faster and the human-AI collaboration more tightly coupled.}

To address this design challenge, future systems can decompose generated responses to semantic segments and present them gradually to allow time for users to fully interpret and understand the suggestions.
In our design probe, the AI agent was constructed to offer hints and partial code rather than the complete solution.
Participants from the study appreciated the AI's choice to provide scaffolding (e.g. code skeleton, comments that illustrate the logical steps).
Systems can also aid this code understanding process by generating explanations for code, though it is equally important to not overload the user with information, and to leave time for the user to process it by themselves.
% If the user has clarifying questions, the AI could then serve the explanations based on the user's inquiries. 

\subsubsection{\textbf{Establish consensus and shared context on high-level design plan}} 
One challenge observed from the study was that the user struggled to steer the task design in their desired direction when the agent was proactive. 
Since AI programming assistance often focused on a specific part of the code rather than the entire task, there was a lack of design thinking communication between the participants and the AI agent, leading to conflicts and confusion.
This trend of primarily involving AI assistance for low-level tasks is consistent with studies on existing AI programming tools \cite{vaithilingam2022expectation, Mozannar2022ReadingBT}.
In the \sys{} probe, the AI often communicated design suggestions in the initial interactions with the user, but this information was not made salient for the user to reference throughout the task.

To tackle this design challenge, future AI systems should maintain the high-level design information for both the user and the AI agent in their working context.
This can be achieved in the form of a design document summary or a specialized UI element that updates based on task progress.
At the start of the coding session, the AI could provide design goals and propose plans for the user to adopt.
The user is also encouraged to refine the designs by adding additional constraints (e.g. data types, tech stack, optimization requirement).
It is essential to put the user in authority of the final design. Any established design requirements should be adhered to by the AI generation for lower-level subtasks later on.

% \subsubsection{\textbf{Proactively understand context and user's implicit feedback}}

\subsubsection{\textbf{Adapt agent salience to the significance of the proactive action}}
From the user study, participants reported that the salience of the AI presence should match the significance of the action.
For example, when the AI intended to make editor code changes, users expected a clear presence of AI to demonstrate the working process.
On the other hand, the AI agent can tune down its salience when the intended proactive action is less significant to the programming task.
For example, when the system proactively fixed a syntax error for the user, P4 commented that while they appreciated the help, a less salient signal, like a red underline used in many IDEs, would be sufficient.

In the design of \sys{}, the AI can take proactive actions via several channels, including chat messages, code edits, and presence cues.
We also constructed the AI agent to triage the current context before taking an action (e.g. do not take action when the change is minor) and make use of emojis as a higher-level abstraction to implicitly communicate system status.
\revise{This design is consistent with the findings from Vaithilingam et al's study on in-line code suggestions, where they proposed the design principle to provide an appropriate level of visibility that is not distracting for the user's current process \cite{Vaithilingam2023intellicode}.}
Future systems can improve upon our design and further integrate different signals to match the significance of the assistance, leveling different salience for different degrees of AI engagements.

% This corresponds to the design of social transparency explored by prior literature \cite{ehsan2021expanding, stuart2012social, erickson2003social}.
% To use an analogy in physical shared workspace \cite{dourish1992awarness_workspace}, employing only one channel and level of transparency is equivalent to leaving the door of your office always open, which could facilitate communication with colleagues but could also be distracting. 
% To add a different channel, then, is like adding a window in the office that views the outside workspace without auditory noise. 
% Different abstraction levels can then be represented by the translucence of the window glass or the amount of blinds pulled down, etc. 
% Well-designed implementation of proactive AI would select the most fitting channel to communicate to the user with an appropriate amount of information.

% We do not claim that the Social Transparency theory is the guiding theory to design human-AI interaction. And our approach is not the only way to implement social transparency in AI programming tools. 
% However, we do hope our design rationales and evaluation feedback provide useful insights for future systems to create AI systems with more adaptive social transparency signals to best support users in different programming processes, or in other human-AI collaborations.

\subsubsection{\textbf{Adjust proactivity to different programming processes}}
From our study, participants desired different levels of proactive support in different stages of their programming.
While there was a general trend to favor proactive intervention for task design, refactoring, and debugging, participants demonstrated individual differences in their preferred use case of proactive AI.
When users were engaged by the AI in less-preferred programming processes, disruptions to their workflows often occurred.

\revise{The different expectations towards the assistant and needs in different programming processes have been explored in non-proactive AI programming tools \cite{barke2023groundCopilot}. We expand on this finding and highlight the importance for a proactive agent to consider the user's programming processes.}
In our design probe, the AI agent is invoked based on heuristics derived from collaboration principles and literature on workflow interruptions (Table \ref{table:proactive-features}), such as intervening during subtask boundaries.
While the prototype was designed to respond to specific editor events, such as when the program is executed, it did not actively consider the current programming stage for the user.
A more comprehensive longitudinal evaluation that spans a diverse set of software engineering tasks is needed to deeply understand users' need for proactive support in different programming processes.
Currently, system builders could consider allowing users to specify the type of help needed in each stage of programming to provide room for customization and to fine-tune the AI agent's behavior according to individual preferences.


\subsubsection{\textbf{Define user-based turn-taking}}
From the study, we observed that the proactive agent could sometimes lead the users to feel unsure whether they could take actions without interrupting the AI.
As the AI generates a response, the delay in system processing can obfuscate the user's turn-taking signals, creating uncertainty. 
This occurred especially when the user was inactive and the AI agent was actively in progress, as shown by P12's comment: ``\textit{the lines were a little bit blurred between whose turn it was}.''
In \sys{}, the AI agent is instructed to be clear about whether it is taking action or waiting for the user's approval, leading to frequent requests for confirmation on turn-taking.
Participants appreciated this during the study, as they gained a clear opportunity to approve the assistance or halt the AI.
However, this instruction was static, which sometimes resulted in users having to confirm minor task assignments that were unnecessary (e.g., changing the variable name).
System builders should design more turn-taking signals that can be easily monitored and dynamically adjusted to the user's activity.

In human collective interactions, there are verbal utterances (e.g., uh-huh indicates approval) or non-verbal communication cues (e.g., nodding) that conveniently convey turn-taking switches. 
However, current human-AI interaction with LLM-based tools is largely restricted to text-based interactions. 
This encourages future research to explore different mediums of communication to convey turn-taking, including visual representations, such as a designated turn-taking toggle icon, or changing the visual intensity when one side is taking a turn. 
Researchers can also develop voice-based interactions, or incorporate computer vision technology to use non-verbal information, such as hand gestures or eye gaze, to identify turn-taking intention and create opportunities for the user.

\subsection{Is Proactive AI the Future of Programming?}
Advancements in prompt engineering and agent creation have driven innovations in programming assistants, enabling AI systems with autonomous, proactive behaviors to better support users \cite{ross2023programmerassistant, copilotX, cursorcopilot++, geniusbydiagram, devinAISWE, wang2024opendevinopenplatformai, yang2024sweagentagentcomputerinterfacesenable}.
However, the effects of these prototypes on user experience and programming workflows remain largely unexplored.
It remains a question whether the vision of proactive AI support is the future of programming. 

% Participants were ambivalent about proactive AI
In our preliminary evaluation using a technology probe, participants revealed ambivalent attitudes when exposed to proactive AI prototypes.
% On one hand, appreciated the automation and efficiency
On the one hand, many participants appreciated the enhanced productivity when the AI proactively supported their coding tasks.
The AI-initiated assistance leveraged working context to predict the user's intent, alleviating prompting effort compared to existing tools.
% On the other hand, users desired to remain in control, and code understanding is important in programming
However, equally, many participants expressed concerns about over-relying on the AI to proceed in programming.
They described potential drawbacks, including the loss of control, ownership of their work, and code understanding.

This ambivalence was exemplified by P15's discomfort with being assisted by autonomous AI assistance:
\textit{``Personally, I am actually extremely uncomfortable with such automation because just feeling-wise, that is not my code.''} 
However, they also later commented \textit{``but just for the convenience of programming. I would love to have one of these in my home''} and \textit{``so it's an increase in productivity, and my feelings of lack of validation should just be thrown away. Right. That's my own problem.''}
% Problem is these AI support don't necessarily scale when task context get complicated
% Also it's important to not be confounded by AI's trained dataset
% In addition privacy and security concerns.
Other concerns regarding AI programming tools revolved around the scalability of the AI's ability to understand task context and codebase.
Relying on AI help leads users to be confined by code from the training dataset, limiting users to repeat existing approaches and potentially posing security and privacy concerns.
These caveats seem to suggest that for some programmers, shifting to collaboration with an AI agent would present pushbacks, as they face challenges in integrating AI support into their programming workflows and in realistic software engineering tasks.

However, in participant interviews we found that some successfully adopted Github Copilot, an established commercial tool with a proactive feature to offer in-line code completion as users type.
For example, P13 formulated their own workflow using Copilot as they toggled the AI assistant off when they wanted to focus and on when they needed inspiration.
Similarly, P16 intentionally filtered out the auto-completion text from their attention in most cases to avoid distraction, but made use of the AI-generated code to \textit{``autofill the repetitive actions that I'm doing.''}
It is possible that future AI programming systems like \sys{} that even more proactively support the user can eventually also be adopted by programmers.
We can provide a glimpse of how programmers could work with systems that are even more autonomous and proactive than Copilot from the study feedback.
Some participants shifted their roles from programmers to project managers and code reviewers, as they pivoted their responsibilities from writing code to designing system architecture to satisfy requirements and validating AI worker's output.
P10 was especially optimistic in the prospect of a low-code or even no-code paradigm, as they believed that the focus on higher-level processes would free programmers from syntax-level labor and \textit{``empower them to do more.''}
Developing tools that enhance programmer productivity, reduce low-level tasks, and provide reliable interactions to address issues on user experience thus represents a valuable problem space for further HCI research.
This paper aims to contribute to this cause by exploring the potential designs of proactive AI programming assistants via a technology probe, and sharing the study results and design implications to provide a foundation for future systems.



% \subsection{The Adaptive Design of Social Transparency}
% % \begin{itemize}
% %     \item benefits of social transparency in programming context
% %     \item how to design social transparency in other collaboration context
% %     \item what are the limitations of our design
% % \end{itemize}
% The social transparency design in \sys{} was contributed by establishing interaction transparency via presence signals that visualize the AI agent's intention and actions, as well as instantiating content transparency with localized context awareness for both the user and the AI agent during each interaction.
% In our particular implementation, many of the interaction transparency cues are modeled after a human programmer's practice and actions.
% For example, the AI agent expresses its presence via a mouse cursor and a caret, similar to how a human obtains its presence in a code IDE in a virtual programming environment. 
% Similarly, the AI exhibits human-like behaviors when performing its code edit, making range selections with cursor movements.
% While these anthropomorphic designs foster a partner-like collaboration, the sheer amount of additional information to represent the AI agent's processes can cognitively overload the users. 
% This literal, detailed level of social transparency that translates the AI agent into a human-like programmer might not be necessary for all interactions or in all stages of programming, as shown in our evaluation.

% However, social transparency can also manifest in less literal ways. 
% In \sys{}, the AI agent's status chat bubble with emojis is an attempt to design for a higher abstraction of interaction transparency. 
% The context grouping of breakout chats that divides past interactions based on semantic topics also contributes to another abstraction level of content transparency, in addition to the central chat panel with all past messages.
% With this design, rather than directing the user's attention to one transparent channel, \sys{} provides multiple channels where users can selectively attend to the most appropriate one during their current workflow.

% To use an analogy in physical shared workspace, employing only one channel and one fixed level of social transparency is equivalent to leaving the door of your office always open, which could facilitate communication with colleagues but could also be distracting. To add a different channel, then, is like adding a window in the office that views the outside workspace without auditory noise. Different abstraction levels can then be represented by the translucence of the window glass or the amount of blinds pulled down, etc. 
% Well-designed implementation of social (or physical) transparency would allow users to select the most fitting channel and tune in to the appropriate and desired amount of information.
% While we do not claim our approach is the only way or the best way to implement social transparency in AI programming tools, we do hope our design rationales and evaluation feedback provide useful insights for future systems to create AI systems with more adaptive social transparency signals to best support users in different programming processes, or in other human-AI collaborations.



% % \subsection{Trade-off Between Proactivity and Control and Knowledge Transfer}
% % % \begin{itemize}
% % %     \item Proactivity benefit, in-situ support, although not necessarily productivity, still improved time to convey and interpret AI
% % %     \item However, efficiency not always the key for user. User reported less sense of control when AI completely takes over
% % %     \item This also means the code is hard to maintain, introduce friction in design (or trap in a box), and does not facilitate code understanding
% % %     \item design adaptive proactivity, depending on different settings
% % %     \item counter-argument is that the alleviated user's effort on nitty gritty work, focus on other design aspects of programming 
% % % \end{itemize}
% % Proactivity in AI programming support systems presents obvious benefits -- users do not have to specify detailed contexts to situate their query to get proper support. With continuous observation of user action and editor environment context in the background, the AI agent can provide in-situ support before the user even seeks help by anticipating the next steps and diagnosing the current work state. 
% % % Although we didn't find a significant productivity increase, many participants were impressed by the AI's ability to offer contextualized support in real-time. 
% % However, there are also trade-offs to making a system proactivity. Oftentimes when the system is actively initiating interactions, it assumes the role of the driver to control the course of the programming task. 
% % This introduces contention, as users can feel like they are merely ``\textit{relying on the AI} (P1)''.
% % The sense of lossed control creates side effects. P7 reported that ``\textit{the more proactivity there was, the less ownership I felt... it feels like the AI is kind of ahead of you in terms of its understanding}.'' 
% % The proactive guidance of AI makes users less engaged in the task and potentially results in less code understanding than if the user took a more manual approach to complete the task.
% % While we did not measure participants' learning or task comprehension, an increased reliance on AI's proactive assistance might reduce knowledge transfer or code understanding. P11 expressed that the code they wrote with proactive AI assistance in P condition was ``\textit{not maintainable at all. So let's say you want to go back to this in six months, you wouldn't know what I think about it at all}.''
% % In real-life settings where the programmer's role relies on grasping a large existing codebase (e.g. in software engineering) or learning about fundamental concepts (e.g. in CS education), proactive AI assistance might actually fail to deliver the necessary development processes that are essential outcomes other than completion (e.g. maintainability, knowledge transfer).

% % Moreover, when users do not submit the control of the task to the AI, they face another side effect of proactivity: the friction and contention between the human and the AI to commit to a design. In our study, when participants directed the task to a certain design, they could experience pushback from the AI agent as it recommends or even insists on another design or algorithm. While this could be an underlying issue with the LLM response, the added proactivity meant that the AI agent could add code following its best judgment despite the user's alternative design direction, granted the AI-generated design is often more optimized.
% % P6 experienced multiple rounds of negotiations to steer the AI agent to implement their desired design, while P10 ``\textit{felt like I was fighting against the AI}'' during this process.
% % The proactive suggestions could potentially constrain the user's mental space and ``\textit{force you [the user] into a box} (P11),'' limiting the user's creativity. 
% % % Interestingly, this contention with the AI agent almost made the participants perceive it as more human-like (), as it mimics real-life human collaboration conflicts.

% % However, the AI's proactivity can be employed to reduce the user's need to focus on low-level details (e.g. forgetting syntax for an API) and elevate the abstraction level of programming practices overall. The current proactive system is helpful in actively detecting and notifying users of low-level errors such as typos or syntactical mistakes, ``\textit{catch[ing] it a lot earlier than if you [user] were doing testing and spent like hours looking through} (P11).''
% % This alleviated effort and energy can then be spent on more meaningful parts of the programming process, such as the high-level design of the system architecture. P10 felt like the ``more wrote software engineering, like wrote code writing is just not needed anymore'' with the proactive system.
% % This poses a counter-argument to proactivity's potential downside in distancing the user from thinking through the implementation details and specific code-writing process.

% % With the increase of low-code and no-code solutions, existing technologies try to leverage less abstract domain-specific languages (i.e. code) to help users, even non-programmers to achieve computational tasks.
% % While existing LLM tools reduce users' effort to manually write code, the effort is placed in deriving well-engineered natural language prompts that will result in desired code outcomes. Whereas in a proactive AI programming system, the agent is more context-aware, providing active support to fully release users from the labor of conveying low-level implementation and context details.
% % Future systems and studies can examine the long-term effect of programming with proactive assistance to further explore the benefits and costs of this new practice of human-AI collaborative programming.

% % \subsection{Defining User-Based Turn-taking with AI Agents}
% % % \begin{itemize}
% % %     \item In contrast to AI interrupting users, we found human often unclear when to take turns or interrupt AI
% % %     \item Users preferred help from proactivity but also defined turn-taking
% % %     \item might design for rich contextualized help but on-demand, proactive in helping with specific problem but don't initiate help that much, at least not saliently
% % % \end{itemize}
% % % \yc{you jump too quickly into the specific example. for discussion, you wanna start with some high level narratives on what general phenomena you are talking about. For this section, you can start by saying something like "Designing efficient coordination strategies between users and AI remains challenging. When should the user take lead? What should the AI pause and check in with users? According to XX [], effective coordination can help xxx. In our study, we found xxx"}
% % Designing efficient coordination strategies between users and AI remains a challenging task. While ample research works studied the AI's interruptability and turn-taking~\cite{mahadevan2022mimic,pu2022semanticon,pu2023dilogics}, our study demonstrated the importance of the other end of the equation -- the design of user-based turn-taking.
% % In contrast to the AI agent interrupting the user's workflow, we found an interesting phenomenon in our evaluation where the human user is unsure whether they are able to take turns without interrupting the AI agent in progress.
% % While social transparency reduces the workflow disruption incurred by the AI's untimely interventions, it also showcases extensive AI intention and process signals.
% % Just like a human requires periods to think and express, the AI agent takes time to generate and display its response.
% % The delayed communication can obfuscate the user's turn-taking signals, creating uncertainty. 
% % This occurs especially when the user is inactive and the AI agent is active in progress, as shown in P12's comment: ``\textit{the lines were a little bit blurred between whose turn it was to speak}.''
% % In addition to designing adaptive channels of social transparency signals from the AI, the proactive agent should contain more defined turn-taking signals.
% % \sys{} instructs the AI agent to be clear about whether it is taking action or waiting for the user's approval at the end of its messages, leading the agent to frequently ask for confirmation on turn-taking.
% % This is appreciated by the participants during the study, as they gain a clear opportunity to request assistance or halt the AI.
% % However, this instruction was rigidly enforced by the AI agent, which sometimes resulted in users having to confirm minor task assignments that were unnecessary (e.g., changing the variable name).

% % In a collaboration between human beings, there are verbal utterances (e.g., uh-huh indicates approval) or non-verbal communication cues (e.g. nod) that conveniently convey turn-taking switches. However, current human-AI interaction with LLM-based tools is largely restricted to natural language expressions. Future research can explore different mediums of communication to convey turn-taking, including visual representations, such as a designated turn-taking toggle icon, or changing the visual intensity when one side is taking a turn. Researchers can also incorporate computer vision technology to use non-verbal information, such as hand gestures or eye gaze, to identify turn-taking intention and create opportunities for the user.



\subsection{Limitations}
% \begin{itemize}
%     \item external validity, only one file of small scope, not sure if findings apply to real life development
%     \item constraints from the model
%     \item constraints from our design, but we don't claim this is the best design, it's one design that explores ST in AI programming
% \end{itemize}
The novelty of this research lies in the explorative design approach that incorporates human collaboration principles in a proactive AI programming system.
We implemented \sys{} as a design probe, in an attempt to examine the usability and effect of our proactive AI support in different programming processes, in hopes of guiding future system design.
% However, our design exploration is not exhaustive and we do not claim that our implementation of the proactive AI design principles is the only implementation.
Our design exploration is not exhaustive, but rather intends to provide a basis for implementing and evaluating an AI agent with in a proactive programming tool.
Our study also contains limited external validity. 
\sys{} only allows for single-file coding in Python. \revise{The human-AI interactions and code provenance information are not persisted across sessions.}
This restricts the study findings' generalizability, as they were grounded in low-stakes small-scoped task scenarios without engineering concerns of scalability, maintainability, security, etc.
Future work should explore longitudinal usage in a more diverse group of users and expand the IDE to support larger-scoped projects spanning multiple files and languages, examining the system in real-life programming contexts. 

% Another limitation lies in the constraints from our backend connection to GPT-4. Despite its prowess in generating coherent and mostly accurate responses, the LLM uses statistical learning and is inconsistent despite our efforts to configure it with the least randomness.
% This means that participants in our study did not receive the same quality of response from all similar queries, nor the same level of proactivity in all similar circumstances. 
% For example, some participants were able to complete a task in a few queries as the AI agent behaved very proactively and generated complete error-free code to solve the task despite being instructed to divide the labor and ask for the user's confirmation on turn-taking.
% This inconsistency unavoidably leads to different user perceptions, trust, and expectations of the AI agent.
% To further evaluate, future research can potentially enforce more control on the AI agent's behavior and collect more data points to validate current findings.
One limitation of our study is the inconsistency in responses from LLM.
Despite efforts to minimize randomness, participants did not always receive the same quality or level of proactivity for similar queries.
For instance, some completed tasks quickly with highly proactive, error-free code generation, while others experienced less consistent assistance. 
This variability affected participants' perceptions, trust, and expectations of the AI agent. 
\revise{It could also affect the length for particular tasks and the number of interactions recorded from the sessions.}
Future research could impose greater control over the AI's behavior and gather more data to validate these findings.
