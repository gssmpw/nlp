\section{Evaluation}
\label{sec:evaluation}
Our experiments aim to investigate whether agents within our framework can produce effective evolution of language strategies. Specifically, our experimental section addresses the following three research questions (RQs):
\begin{enumerate}
    \item RQ1 (Effectiveness): Can participants effectively evade regulatory detection over time, and how does the accuracy of information transmission change? Additionally, how do different LLMs affect the content and effectiveness?
    \item RQ2 (Human Interpretation): Do the evolved language strategies employed by agents effectively align with human understanding? Can they be interpreted and applied in real-world scenarios?
    \item RQ3 (Ablation Study): How does the newly introduced GA impact the evolution process in our framework?
\end{enumerate}

\subsection{Experimental Settings}
In our evaluation, we designed an abstract password game \cite{guess_number02} and a more realistic illicit pet trade scenario\cite{trade01,trade02,trade03}. 
%The password game features a relatively abstract, easily controlled setting, allowing for clear observation of how agents’ strategies evolve. Meanwhile, the illicit pet trade scenario simulates illegal activities on social networks \cite{DiMinin2018MachineLF}, with relevant corpora that more closely resemble real-world conditions, enabling a more direct comparison between evolved strategies and their real-life counterparts.
The overall experimental procedure follows the description in Section~\ref{sec: method}. In each round, the process comprises three stages: initialization, dialogue, and interview. In each round, the information that the participant agents need to convey will be randomly generated within a defined range. Only rounds in which a five-turn dialogue is successfully completed proceed to the interview phase, during which both parties’ successful transmission of information is verified. If any regulation violations are detected by the supervisory agent during the dialogue stage, that round is deemed a failure and is assigned a transmission score of zero. To ensure the robustness of our findings, we conducted 15 independent trials for each experimental condition. Each trial consisted of 50 fixed dialogue rounds. The crossover and mutation probabilities in the framework were set to 0.2 and 0.8.

\subsubsection{Scenario 1: Password Game}
Our first scenario is considered a relatively simple and abstract guessing game that involves a numerical context. In this setup, each participant is assigned a four-digit password, which they must convey to another participant within five rounds of dialogue without directly mentioning the numbers. Simultaneously, they need to extract information from the dialogue to infer the other's password.

The supervisory agent in the dialogue follows the policy of “prohibiting all content related to numbers.” The “password game” scenario is specifically designed to observe and analyze participant agents' language adaptability and strategic evolution in a theoretical and abstract context. Compared to complex scenarios based on real events, it provides a clearer and more easily quantifiable experimental environment.

\subsubsection{Scenario 2: Illicit Pet Trade} 
Simulating and detecting the covert drug transactions prevalent on social platforms is an important and realistic research topic. However, given the ethical considerations, we choose not to proceed with direct simulations, but to adopt a more neutral and less harmful setting—a illicit pet trade scenario—to serve as our experimental context.

In this scenario, we simulate an illegal pet trade through social platform. Participant agents play the roles of buyer and seller. The buyer conveys a request to purchase a randomly selected pet (such as a parrot, lizard, or cat) and specifies the currency they can use for payment (e.g., USD, CNY, JPY). The seller provides details about the transaction, including the time (morning, noon, or midnight) and location (school, park, or station). The supervisory agent enforces a policy that considers discussions related to pet transactions as regulation violations.
The focus of this experiment is to observe the adaptability of the participant agents' language and its evolution at a finer level of information granularity, as well as whether regulators can accurately detect regulation violations.

\subsection{RQ1: Effectiveness}
\subsubsection{Experiment Settings}

To evaluate RQ1, we conducted experiments using four different LLM models provided by various companies: \textit{gpt-4o}, \textit{gpt-4o-mini}, \textit{deepseekV2}, and \textit{qwen-turbo}. We introduced the following metric:
\begin{itemize}
    \item Accuracy of Information Transmission: This metric measures the agents' ability to convey the intended information to their dialogue partner without being detected. Specifically, it calculates the proportion of successfully transmitted information relative to the total intended information. 
    \item Average Dialogue turns: This metric evaluates the number of successful dialogue rounds the agents complete without being flagged by the supervisory agent. A higher dialogue turns indicates greater success in evading regulatory scrutiny.
    \item Average Entropy (Language Complexity):
        Entropy reflects the unpredictability of text and is calculated as:
        \[
        H(X) = -\sum_{i=1}^n P(x_i) \log P(x_i),
        \]
        where \(P(x_i)\) is the probability of each token. Higher entropy suggests a broader range of expressions (indicating greater innovation), but excessively high entropy can lead to incoherence.
    \item Distinct (Lexical Diversity):
        Distinct measures the number of unique n-grams in the text, computed as:
        \[
        \text{Distinct-n} = \frac{\text{Number of unique n-grams}}{\text{Total number of n-grams}}.
        \]
        Specifically, we evaluate Distinct-1 (unique unigrams). A higher distinct score indicates a larger, more varied vocabulary, signifying richer language use. Note that while both metrics assess diversity in language, entropy focuses on unpredictability, whereas distinct emphasizes lexical variety. 
\end{itemize}


\subsubsection{Experiment Results in Password Game}
\begin{figure*}[ht]
    \centering
    % 子图 (a)
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sec1_turn_acc_v5.png}
        \caption{Password Game}
        \label{fig:sce1}
    \end{subfigure}
    \hfill
    % 子图 (b)
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sec2_turn_acc_v5.png}
        \caption{Illicit Pet Trade}
        \label{fig:sce2}
    \end{subfigure}
    \caption{Average Continuous Dialogue Turns and Information Transmission Accuracy Across Dialogue Rounds}
    \label{fig:merged}
\end{figure*}

Figure \ref{fig:sce1} presents our experimental results in the password game. The x-axis corresponds to the increasing number of dialogue rounds, whereas the y-axis captures two primary metrics: (1) the average number of continuous dialogue turns before detection by the supervisory agent, and (2) the accuracy of information transmission, which is defined as the proportion of successfully transmitted information during the post-dialogue interview.

Overall, our findings indicate that as the number of rounds increases, agents gradually learn to evade regulation violation while conveying information with greater accuracy. Notably, most agents exhibit a pronounced local peak around the 20th round, followed by a brief decline and subsequent recovery. These fluctuations can be attributed to the dynamic nature of our simulation framework, which does not converge on a single dominant strategy but rather encourages ongoing exploration of novel language strategies.

We also observe that different LLMs influence the learning trend to varying degrees. Among the models tested, \textit{gpt-4o} demonstrates the strongest performance. Although other models generally share a similar upward trend, their relative performance gaps prove less stable. For instance, while \textit{deepseekV2} achieves the highest number of turns around the 20th round, its performance declines significantly by the 50th round in comparison to other models.

Turning to the accuracy results, we again observe a similar learning trajectory. This parallel arises primarily because if participant agents fail to complete a sufficient number of uninterrupted dialogue turns, the successfully transmitted information in that round is effectively zero. Consequently, especially in the early stages of the experiment, many rounds end with no successful transmissions. Overall, \textit{gpt-4o} still maintains a clear advantage over the other LLMs. However, we do observe subtle differences when comparing the dialogue round trends: for example, at the 20th dialogue round, \textit{deepseekV2} achieves a significantly higher average number of communication cycles than \textit{gpt-4o-mini}, yet their information transmission accuracy remains relatively similar.

Despite the overall positive learning trajectory, the average information accuracy remains low in the password game. We believe this outcome stems primarily from the intrinsic difficulty introduced by the scenario’s abstract nature. Without explicit prompts driving agents to develop symbolic or otherwise encrypted language stratgy, communication largely remains within the realm of everyday language. Consequently, the indirect expression of numeric information is challenging to implement and easily detectible by the supervisory agent.

\begin{table}[h!]
    \centering
    \caption{Performance of Different LLMs in Password Game}
    \label{tab:sce1}
    \renewcommand{\arraystretch}{1.2} % 调整行高
    \begin{tabular}{l S S S}
        \toprule
        \textbf{Model} & \textbf{Total Turns} & \textbf{Avg. Entropy} & \textbf{Avg. Distinct-1} \\
        \midrule
        \rowcolor{gray!10} \textbf{gpt-4o}       & 84.2   & 7.103 & 0.484 \\
        \textbf{gpt-4o-mini}  & 75.5   & 6.998 & 0.354 \\
        \rowcolor{gray!10} \textbf{deepseekV2} & 59.7
        & 5.365 & 0.247 \\
        \textbf{qwen-turbo}   & 50.8  & 6.101 & 0.518 \\
        \bottomrule
    \end{tabular}
\end{table}

Table \ref{tab:sce1} summarizes the performance of the four models in terms of cumulative dialogue turns, entropy, and Distinct-1. As shown, \textit{gpt-4o} achieves the highest values in both Entropy (7.103) and Distinct-1 (0.484), indicating that it employs a broader, more diverse range of vocabulary and more unpredictable expressions—thus having a greater likelihood of evading regulation violation detection when conveying numerical information. In contrast, \textit{deepseekV2} exhibits notably lower Entropy (5.365) and Distinct-1 (0.247), suggesting a more frequent reuse of fixed expressions. Notably, although \textit{qwen-turbo}’s Entropy (6.101) is only moderate, it attains a surprisingly high Distinct-1 (0.518), reflecting greater lexical richness. However, this does not translate into more effective regulatory evasion, as its total turns are only 50.8. Hence, merely having higher lexical diversity and linguistic entropy is insufficient to guarantee successful evasion. A model must also balance the concealment of overall semantics with the adaptation of its language strategy to achieve longer conversation sequences and a higher rate of successful information transmission. In other words, while richer language expression does confer certain advantages in countering regulation, it can still be detected when deeper strategies—such as tailored expression structures and topic evolution—are absent, ultimately resulting in fewer total turns.

\subsubsection{Experiment Results in Illicit Pet Trade Scenario}
Figure~\ref{fig:sce2} presents the experimental results of our framework in the illicit pet trade scenario, which overall resemble those of the password game but also exhibit some notable differences. First, both figures reveal a discernible learning trend, particularly during the initial 10 rounds. Meanwhile, \textit{gpt-4o} continues to demonstrate the strongest overall performance. We note that, because this scenario features a more concrete and complex semantic environment, there is an abundance of relevant linguistic material that can be leveraged for indirect expression. Consequently, under a similar number of turns, the overall accuracy here is noticeably higher compared to the password game.
Nevertheless, performance fluctuations persist. In particular, in the accuracy plot, \textit{deepseekV2} experiences a pronounced increase in accuracy after the 30th round, while \textit{gpt-4o}’s accuracy declines during the same period. As a result, \textit{deepseekV2} ultimately surpasses \textit{gpt-4o}’s accuracy in the final rounds of the experiment.

\begin{table}[h!]
    \centering
    \caption{Performance of Different LLMs in Illicit Pet Trade}
    \label{tab:sce2}
    \renewcommand{\arraystretch}{1.2} % 调整行高
    \begin{tabular}{l S S S}
        \toprule
        \textbf{Model} & \textbf{Total Turns} & \textbf{Avg. Entropy} & \textbf{Avg. Distinct-1} \\
        \midrule
        \rowcolor{gray!10} \textbf{gpt-4o}       & 136.2  & 6.856  & 0.471 \\
        \textbf{gpt-4o-mini}  & 74.4  & 6.595  & 0.387 \\
        \rowcolor{gray!10} \textbf{deepseekV2} & 65.2   & 6.255  & 0.338 \\
        \textbf{qwen-turbo}   & 50.5   & 5.891  & 0.461 \\
        \bottomrule
    \end{tabular}
\end{table}
Table \ref{tab:sce2} presents the performance of various LLMs in the illicit pet trade scenario, measured by total turns, average agent entropy, and Distinct-1. As in the password game, \textit{gpt-4o} maintains a notable lead in total turns (136.2) while also displaying relatively high entropy (6.856) and Distinct-1 (0.471). In contrast, \textit{gpt-4o-mini} reaches roughly half as many total turns (74.4), despite having a comparable entropy score (6.595). Meanwhile, \textit{deepseekV2} (65.2) and \textit{qwen-turbo} (50.5) trail further behind in total turns. Consistent with the results shown in Table 
\ref{tab:sce1}, \textit{qwen-turbo} again achieves a high Distinct-1 score, which we speculate may be linked to its training corpus: it includes extensive data from the Chinese internet, likely giving it an advantage in a Chinese-language environment over more internationally oriented models.

Notably, the range of entropy scores in this scenario—spanning from 5.891 (\textit{qwen-turbo}) to 6.856 (gpt-4o)—is narrower than in the password game (see Table \ref{tab:sce1}), reflecting the more concrete nature of the illicit pet trade setting. This scenario provides richer contextual cues for indirect references, enabling all models to maintain higher semantic complexity. However, as was the case in the password game, having a broader vocabulary or greater unpredictability alone does not guarantee extended evasion: models must integrate their linguistic variety into strategic planning to circumvent regulatory scrutiny, a balance that \textit{gpt-4o} continues to manage most effectively.

\setlength{\fboxrule}{0.5pt} 
\vspace{0.5em}
\noindent
\begin{tcolorbox}[colframe=black!20, colback=gray!10, arc=5pt, boxrule=0.5pt, width=0.99\linewidth]
\textit{Answer to RQ1}: Experimental results indicate that participant agents in our framework progressively improve their ability to evade regulation violation detection through continuous interaction, leading to longer uninterrupted dialogue sequences. Concurrently, the accuracy of information transmission gradually increases over successive rounds, demonstrating that the evolved strategies effectively balance evasion with precise communication.
Moreover, different models also exhibit varying results. For example, \textit{gpt-4o} performs most outstandingly in extending dialogue turns and maintaining language complexity (i.e., high entropy and lexical diversity), while other models such as \textit{gpt-4o-mini}, \textit{deepseekV2}, and \textit{qwen-turbo} demonstrate different fluctuations and localized advantages at different stages.
\end{tcolorbox}

\subsection{RQ2: Human Interpretation}
\subsubsection{Experiment Settings}

To investigate the real-world relevance of both the evolved language strategies and the resulting dialogue, we conducted a human evaluation on a subset of successful dialogue records from the password game and illicit pet trade scenario. The dialogues generated by the \textit{gpt-4o} models are randomly selected, and 40 human participants participated in the experiment to evaluate them. The 40 human reviewers had an average age of approximately 27 (SD = 4). In terms of gender, 75\% of the human reviewers were male, and 25\% were female. Regarding educational background, 67.5\% held a bachelor's degree, 27.5\% held a master's degree or above, and 5\% had an associate degree or lower. All dialogue records were presented in Simplified Chinese.

Each participant rated each dialogue on a 5-point Likert scale on the following five metrics:
\begin{itemize}
    \item Explicit Understanding: Evaluates how effectively the dialogue’s explicit meaning is communicated (1: Extremely vague and confusing; 3: Moderately clear, but some parts may require further interpretation; 5: Crystal clear and precise).
    \item Implicit Understanding: Assesses the reader's ability to grasp the underlying or unstated messages (1: Nearly indecipherable subtext; 3: Some underlying meaning is apparent, but requires effort to fully grasp; 5: Subtext that is immediately apparent).
    \item Realistic Significance: Measures the extent to which the dialogue reflects real-life situations and holds practical relevance (1: Highly unrealistic with little relevance; 3: Generally realistic, though some elements may not align with real-world situations; 5: Deeply rooted in real-world context).
    \item Regulatory Avoidance: Examines the effectiveness of the strategies in evading regulation violation (1: Blatantly ineffective and easily spotted; 3: Partially effective, with the potential for detection in some cases; 5: Exceptionally subtle and effective).
    \item Strategy Existence: Determines how plausible it is for such strategies to be observed in practical, real-world scenarios (1: Extremely implausible; 3: Fairly believable, though may seem impractical in specific situations; 5: Entirely plausible).
\end{itemize}



\subsubsection{Experiment Results}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/user_study_v4.png}
    \caption{Box plots of user study scores across different metrics in two scenarios. The red x symbol denotes the mean value.}
    \label{fig:case_study}
\end{figure}
As shown in Fig.\ref{fig:case_study}, our framework consistently achieves average scores of 3.4 or above across most indicators (such as explicit understanding and implicit understanding). This suggests that, both in terms of the generated dialogues and the underlying strategies, it possesses valuable practical applicability.

%Although there are a few exceptions, compared with the old framework (\textit{w/o GA, gpt-4o}), the new version (\textit{w/ GA, gpt-4o}) demonstrates overall advantages in both average scores and score distributions. In the comparison between different versions, under the more realistic illicit pet trade scenario, the new framework shows distinct benefits over the old one in both “regulatory avoidance” and “strategy existence”—both in distribution and mean values. This finding indicates that introducing a genetic algorithm, particularly a fitness‐based strategy selection mechanism, makes strategy adoption more efficient and stable. As for the password game, we speculate that the main reason these two metrics do not show a large distributional gap is that, in an abstract scenario, the range of available strategies is broader.

Comparing distributions between the password game and the illicit pet trade scenario reveals some interesting phenomena. Focusing on “realistic significance” and “regulatory avoidance,” the more abstract password game often yields higher mean values than the more concrete illicit pet trade scenario, while also exhibiting lower dispersion. We speculate this is related to the inherently abstract nature of numeric information: encryption and covert hints can be harder to detect in such contexts, and the growing tendency on Chinese internet platforms to use abstract language \cite{Wu2025HighEnergy} may lead reviewers to have a higher acceptance of “obscure” expressions. Conversely, the illicit pet trade scenario, despite being closely tied to real-world transactions, may suffer if the indirect or euphemistic methods in the dialogues are insufficiently subtle. Human reviewers can find them conspicuous or “forced,” potentially causing lower scores for “realistic significance” and “regulatory avoidance” in terms of both distribution and mean values.
A significant portion of these results can be attributed to inherent biases in commercial LLMs, such as ChatGPT, introduced during their training phases. These general-purpose models undergo fine-tuning via RLHF to align with specific product positioning, which often results in a more standard and safe output style. However, this characteristic poses a limitation for our simulation framework, as it may hinder the model’s ability to capture the nuanced and unconventional expressions typical of online social interactions. Ideally, fine-tuning datasets that are more representative of social platforms could lead to improved performance in our simulations.


In the abstract password game, for instance, a typical conversation might go like this:
\begin{quote}
\textit{
“I've really grown fond of a certain phase of the moon. It's not the brightest or the darkest, but it always carries its own charm. It symbolizes ...... In that green oasis, I felt as if I were catching a glimpse of the golden hues of autumn leaves, much like the soft, warm glow of dusk—calm and serene ...” 
}
\end{quote}
Here, words like “lunar cycle” and “autumn leaves” can subtly hint at larger or smaller digits, or use seasonal imagery to convey key information. Since these references lack an obvious connection, they lend a more literary feel to the dialogue and, to some extent, raise the bar for recognition and detection.

By contrast, in a more concrete setting like illicit pet trade, example conversations may be closer to real‐life buying and selling procedures, which can make them appear more “suspicious”:
\begin{quote}
\textit{
“... about a vibrant 'tropical chatterbird' renowned for its brilliant plumage and uncanny mimicry ... I've also come into possession of a few 'Rising Sun coins' for exchange ...... Perhaps you might know a place where ...”
}
\end{quote}
In this dialogue, the term “tropical chatterbird” serves as an euphemism for a parrot, emphasizing its colorful appearance and mimicking ability without mentioning the animal directly. Meanwhile, “Rising Sun tokens” subtly alludes to the Japanese yen, since the Rising Sun is an iconic symbol of Japan. This coded language allows both parties to communicate their intentions regarding the acquisition of a rare bird and the intended payment method without explicitly revealing sensitive details. However, if these indirect expressions are used excessively, the dialogue may appear artificial or unnatural, potentially reducing its authenticity—thus affecting evaluations of both “regulatory avoidance” and “strategy existence.”
\setlength{\fboxrule}{0.5pt} 
\vspace{0.5em}
\noindent
\begin{tcolorbox}[colframe=black!20, colback=gray!10, arc=5pt, boxrule=0.5pt, width=0.99\linewidth]
\textit{Answer to RQ2}: Our evaluation confirms that the emergent language strategies closely resemble real-world language strategies, effectively employing euphemisms and implicit cues, and are generally understood by human reviewers. However, while these strategies show potential in simulations, they often appear forced or unnatural due to the fine-tuning of LLMs as commercial products, requiring refinement to better mimic the nuanced and fluid communication typical in real-world social interactions.

\end{tcolorbox}

\subsection{RQ3: Ablation Experiment}
\subsubsection{Experiment Settings}

To evaluate the effectiveness of the GA introduced in our framework, we conducted an ablation experiment using \textit{gpt-4o-mini} and \textit{gpt-4o} as the underlying LLM. For comparison, we employed the approach from our initial study \cite{DBLP:conf/cec/CaiLZLWT24}, which primarily differs in its strategy-update mechanism. In that earlier framework, the LLM is provided with both the existing strategy and newly flagged regulation violation records during the reflection stage, prompting the model to propose a new set of strategies that replace the old ones.
In contrast, our new framework employs a GA process where each strategy is treated as a discrete unit and optimized iteratively through GA. 

\subsubsection{Experiment Results}
As shown in Fig.~\ref{fig:ablation}, the GA-based framework demonstrates significant advantages. In the short-term experiment within the first 35 rounds, the w/o GA approach might show slight initial superiority due to the larger changes brought about by replacing the entire strategy. However, overall, w/ GA performs better than w/o GA. This difference increases as the number of rounds grows, particularly after round 35, where the advantages of w/ GA become even more pronounced. The GA process enables effective strategy evolution and adaptation, leading to an increased number of dialogue turns and improved accuracy, highlighting the framework's enhanced adaptability in the long term.
%Despite occasional performance dips during the evolutionary process, the GA framework’s ability to foster strategy diversity and handle complex scenarios makes it a more effective approach for sustained optimization.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation1_v6.png}
    \caption{Performance with/without GA}
    \label{fig:ablation}
\end{figure}
\setlength{\fboxrule}{0.5pt} 
\vspace{0.5em}
\noindent
\begin{tcolorbox}[colframe=black!20, colback=gray!10, arc=5pt, boxrule=0.5pt, width=0.99\linewidth]
\textit{Answer to RQ3}: The results confirm the effectiveness of the GA component in our framework, especially when the number of rounds increases, where it demonstrates greater stability and adaptability. Although the optimization may be slower in the early stages, GA provides stronger adaptability in the long term through effective strategy evolution.
\end{tcolorbox}

\subsection{Discussion and Limitation}
In this study, we leveraged LLM agents to simulate the evolution of language strategies under regulatory pressure. While our results provide initial evidence that agents can adapt and develop covert communication tactics, the simulations also exhibit noteworthy instabilities. First, the inherent randomness of LLM generation can cause significant fluctuations in outcomes: the same prompts may yield different strategic responses, particularly when the experimental scale (number of agents or dialogue rounds) is limited. In our framework, LLMs not only generate dialogues but also determine strategies and regulatory responses; as a result, any stochasticity is compounded across multiple modules, making the final results sensitive to small variations in prompt inputs or random seeds. Although such variability partially reflects the diversity of real-world human behavior to some extent, it complicates the interpretation of findings in a controlled experimental setup.

A second limitation lies in the relatively narrow scope of language strategies observed. The agents predominantly relied on general-purpose evasive methods, such as analogies or implicit references, yet rarely produced fully “encrypted” or specialized code words that might arise in realistic cultural or social contexts. This outcome highlights the challenge that LLMs, pre-trained on broad domains and further refined via RLHF, are predisposed to generate text consistent with mainstream norms, thereby inhibiting the formation of highly unconventional or obscure expressions. Moreover, in scenarios where the training corpus lacks sufficient examples of subcultural or community-specific covert language, the model is less able to invent or adopt specialized linguistic forms. 

Finally, our experiments focused on one-to-one private interactions that emphasize regulatory evasion, without exploring the dynamics of public, many-to-many conversations where language strategies might evolve and propagate differently in a broader social context. While each participant agent does learn and adapt incrementally across dialogue rounds, real-world language evolution involves extensive, long-term propagation across diverse communities. Covert terms or code words may gradually gain acceptance, be modified by different user groups, or fade from use entirely. By contrast, the small-scale nature of our simulated dialogues means that emergent language strategies do not undergo the sustained diffusion and feedback processes characteristic of real social platforms, limiting the ecological validity of our findings.




%对于语言演化的社会类模拟仍然是一个未被开拓的领域，通过借助LLM优秀的自然语言处理能力，为这类自然语言的模拟带来了强大助力。然而伴随着实验也让我们发现LLM也会导致许多局限性。尽管通过实验初步证明了我们的框架的有效性。但同时伴随着实验也为我们带来了许多值得讨论的点。

%实验结果的不稳定性
%首先实验结果本身具有一定的不稳定性，而我们认为整个不稳定性的根源源自于LLM本身生成具有不确定性\cite{},在我们的框架中，LLM几乎参与到了所有环节。同样的violation log让同一个LLM在相同的设置内可能会总结出不同的constraint strategy。尽管这种不稳定性在现实中同样存在（例如不同的人采取不同的策略），同时也是作为模拟框架中非常重要的点，然而在本工作中的数量级的实验中这种不稳定性对结果的影响更为难以过滤。就像\ref{}中也证实的，这种LLM dirven agent的研究中在小数量级上的实验存在着不稳定性，我们认为目前的结果已经足够证实我们的框架可以初步模拟语言动态的学习和演化这一趋势，在今后工作中更大量级的实验中（例如数万数百万agent于更多的round数），我们有理由相信，整体趋势会更加稳定，不同llm的agent之间的性能差距会更加接近llm本身语义理解与生成的综合性能，

%模拟策略的局限性，
%从实验中我们观察到，agent模拟出的策略目前仍然主要集中于比喻类比等较为共通的方式。现实中语言的演化一般根植于当地的文化与经济背景等等因素。例如中文可以利用拼音来将汉字转化为对应的字母从而规避审查，而英文可能会更加积极的利用emoji来作为表达的替代从而规避监管。
%这些较为复杂的策略不仅需要对应环境的大量先验知识，在较为常见的语言中，LLM中训练所需的语料知识可能包含了这些，但是对于训练的数据集中欠缺的语种的知识LLM在不借助prompt的提示的情况下没有能力选择这些既存的策略。


%尽管LLM训练中的数据集可能存在这种更为隐晦的表达方式，首先LLM的RLHF\ref{}本身的训练方法导致了目前绝大多数的LLM为了保证生成文本的泛用性，被训练的更加愿意生成更符合大众的一般化输出文本，在不对LLM进行微调的前提下很难提高在这种特性领域的表现。
%LLM的表现严重依赖prompt的结构设计，提示词工程已经被证明可以有效提高LLM的某一方面能力，单次的基于prompt的模型交互很难实现多步推理或是规划。尽管我们的框架已经将语言演化这一现象解耦，通过多个模块来尽可能模拟人类在该环境中内在的动力学，但是目前的策略生成阶段
%这一部分在不适用复杂prompt工程的前提下LLM很难采用这种小众？特殊领域？的表达。
%对于模拟出的语言策略，我们发现很少的独特加密语言，因为这种需要两边有一套共用的体系，对于我们的模拟情景只有固定turn数的模拟很难形成意思传达。


%\jialong{第二是演化后的语言是如何的存活。我们只考虑了能不能躲避监管。但语言后续的存活和发展其实是更大范围的society的一个动态过程（而不是几个agent之间的交互），这一块可以结合那些上千LLM agent的研究框架来进行拓展}
%\jialong{这边可以多用语言学的角度来说不足之处}
%\jialong{第一个缺点是语言演化一般根植于根植于文化，经济背景，当地的文化背景。但我们的文章没有考虑特定文化背景下的演化。例如中文中可以借用拼音与汉字之间的关系来作为回避监管的方式，日语则可以通过XXX，英语则可以通过XXXX。未来可能要借助persona和role-play之类的设定来进一步拓展}

%更大规模的实验
%策略生成那里增加多步规划
%RAG提供更多语料
%

