\section{Background and Related Work}
\label{sec: RelatedWork}
This section provides an overview of relevant background and previous work that underpins this study. We begin by discussing the advances in LLMs, move on to research related to slang detection and identification, and conclude by exploring the application of LLMs in evolutionary game theory and social simulations.

\subsection{Large Language Models}
The advent of LLMs has revolutionized the field of natural language processing (NLP), with models like  GPT-4~\cite{openai2024gpt4technicalreport} and LLaMA~\cite{touvron2023llama} showcasing state-of-the-art performance in various linguistic tasks. These models, built upon the Transformer architecture~\cite{vaswani2017attention}, leverage self-attention mechanisms to handle sequential data efficiently, enabling them to capture complex linguistic patterns, such as syntax and semantics, across vast corpora of text.

% , PaLM~\cite{chowdhery2022palm}, and Bard

The training of these models is based on large-scale datasets, which allows them to generalize across diverse linguistic contexts, including different languages, genres, and registers. A noteworthy aspect of LLMs is their ability to exhibit zero-shot and few-shot learning, which empowers them to perform well on tasks they have not been explicitly trained on~\cite{li2024exploring,Zhao2023ASO,Wang2023ASO,10.1145/3686803}. Additionally, techniques like Reinforcement Learning from Human Feedback (RLHF)~\cite{instructGPT} enhance their capability to align with human ethical norms, improving both the quality and appropriateness of generated content. As a result, these models have been deployed in various real-world applications, ranging from content creation to decision-making in social contexts.

\subsection{Slang Detection and Identification}
The detection and identification of slang have long been significant challenges in NLP due to the constantly evolving nature of informal language. Early study relied on traditional rule-based approaches and static slang dictionaries to identify non-standard expressions in text~\cite{Wang_icceasia23}. These methods, while effective in detecting known slang, often struggled to keep pace with rapidly changing linguistic trends, especially in online communities where new slang emerges frequently.

More recent approaches have incorporated machine learning models, such as Naive Bayes and Support Vector Machines (SVMs), to detect informal language~\cite{10308036,9961254}. While these models offered more flexibility, they still faced limitations when confronted with novel or context-dependent slang terms. In response to this challenge, cognitive approaches to slang prediction have been developed, such as the work by~\cite{sun2019slang}, which explores the use of categorization models to predict the emergence of slang based on the selection of new vocabulary. This method emphasizes the role of cognitive processes in slang generation and demonstrates superior performance over random guessing.

Further refinement came with the introduction of frameworks like the Semantically Informed Slang Interpretation (SSI) model~\cite{sun2022semantically}, which leverages semantic and cognitive theories to better understand how slang evolves within specific contexts. This approach not only improves the interpretation of slang but also sheds light on the mechanisms underlying its evolution, providing a more dynamic view of language change in informal settings. However, these studies primarily focus on detecting and predicting existing slang, while the generation and adaptation of new slang remain relatively unexplored—a gap that this study seeks to address to some extent.

\subsection{LLMs in Evolutionary Game Theory and Social Simulation}
The intersection of LLMs with evolutionary game theory and social simulation has opened new avenues for studying complex interactions in controlled environments. Research has demonstrated that LLMs can simulate sophisticated strategies in negotiation-based games, as evidenced by~\cite{fu2023improving}, where models refine their bargaining strategies through iterative self-play. This iterative process mirrors the real-world adaptation of communication strategies, showing the potential of LLMs to autonomously improve their decision-making capabilities.

LLMs have also shown promise in social deduction games, such as Werewolf, where they analyze historical communication patterns to develop effective game strategies~\cite{xu2023exploring}. This study highlights the models’ ability to evolve their behaviors and responses based on the context and previous interactions. Additionally, combining LLMs with reinforcement learning, as discussed by~\cite{xu2023language}, has enabled the development of agents that make competitive decisions while maintaining linguistic coherence. Such advancements illustrate the growing role of LLMs in not only simulating language but also in evolving strategic behavior in complex scenarios.

Beyond game theory, LLMs have been applied to broader social simulations, including modeling historical and social dynamics. In~\cite{Park2023GenerativeAI}, LLM-driven agents were used to simulate interactions in a Wild West-style setting, demonstrating how these models can autonomously generate diverse behaviors without relying on real-world data. Similarly, the S3 framework~\cite{gao2023s3} simulates social media interactions by predicting user demographics and behaviors, providing a realistic model of social network dynamics. 
In \cite{tang2024gensim}, a general-purpose, error-correcting social simulation platform based on large model agents is proposed. This platform supports large-scale simulations involving up to 100k participants and has been tested in various scenarios, including labor market simulations and network user behavior simulations, with a discussion of its effectiveness.
Similarly, \cite{yang2024oasis} takes this further by introducing OASIS, a scalable and extensible social media simulator. OASIS extends the number of simulated users to the million level.
LLM-based simulations have also been used to reconstruct historical events, as seen in~\cite{hua2023war}, where multi-agent systems were employed to simulate military confrontations and decision-making processes in historical contexts.

These diverse applications highlight the versatility of LLMs in simulating social interactions. However, existing study has primarily focused on open or historically constrained scenarios, whereas we examine the trade-offs users make between expression needs and platform moderation in regulated environments. Understanding this dynamic is crucial for optimizing moderation strategies and balancing freedom of expression with compliance requirements. Our goal is to uncover the evolutionary mechanisms of language strategies in such environments and provide feasible simulation approaches.


%\paragraph{Differences from Prior Research}
%Our previous study \cite{DBLP:conf/cec/CaiLZLWT24} shares the same overall framework as \method{}; however, significant differences exist in the design of the Reflection Module. Specifically, the prior research did not incorporate the concept of genetic algorithms. Instead, both Constraint Strategies and Expression Strategies were directly managed by a LLM. During each reflection cycle, the LLM would generate several new strategies based on violation logs or dialogue history and replace the existing strategies entirely. Through experiments and further investigation, we identified a notable issue with this approach: when the LLM updates strategies by capturing new information from the context each time, it struggles to balance global information with local details. For example, when maintaining Constraint Strategies, the LLM might focus more on frequently occurring errors in the violation logs, thereby neglecting certain highly valuable individual records.

%In contrast, \method{} significantly enhances the strategy optimization process within the Reflection Module by introducing genetic algorithms. Specifically, genetic algorithms enable the LLM to generate new strategies targeting small batches of violation records during the Mutation phase, allowing it to concentrate more effectively on local information features. Concurrently, the Selection process, based on fitness evaluation, assesses each strategy's contribution to the overall dialogue performance, ensuring that the selection process accounts for global effectiveness. Through this approach, \method{} not only preserves strategies that perform well at the local level within the strategy pool but also maintains overall coordination and optimization of strategies. 