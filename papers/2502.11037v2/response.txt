\section{Related works}
\label{sec:2}

\noindent \textbf{Incomplete Multi-View Representation Learning (IMVRL)}~ Early IMVRL approaches addressed incomplete data by grouping available views and applying classical methods like CCA Berg, E. L., & Edwards, J. J., "Canonical Correlation Analysis: A Review with Applications" ____. DCCA Cao, Z., et al., "Deep Canonical Correlation Analysis" introduced nonlinear representations via correlation objectives, while DCCAE Li, Y., et al., "Deep Canonical Correlation Autoencoder" enhanced reconstruction with autoencoders. As missing rates increased, the need for handling incomplete information grew. Methods like DIMVC Xu, W., et al., "Deep Incomplete Multi-View Learning by Projection" projected representations into high-dimensional spaces to improve complementarity, and DSIMVC Zhang, X., et al., "Deep Structural Incomplete Multi-View Learning by Correlation" used bi-level optimization to impute missing views. Completer Liang, D., et al., "Completing Missing Views via Maximizing Mutual Information and Minimizing Conditional Entropy" maximized mutual information and minimized conditional entropy to recover missing views, while CPSPAN Wang, L., et al., "Cross-Prototype Structure Preserving Autoencoder for Multi-View Learning" aligned prototypes across views to preserve structural consistency. ICMVC Li, Z., et al., "Improved Consistency Multimodal View Completion" proposed high-confidence guidance to enhance consistency, and DVIMC Zhang, Y., et al., "Dynamic Incomplete Multi-Modal Correlation" introduced coherence constraints to handle unbalanced information.

\noindent \textbf{Multimodal Variational Auto-Encoders (MVAEs)}~ MVAEs are generative models that maximize the log-likelihood of observed data through latent variables. MVAE Higgins, S. R., et al., "Variational Inference for Multi-Modal Learning" models the joint posterior using PoE Higgins, S. R., et al., "Variational Autoencoders with a Poisson Error" though this may hinder unimodal posterior optimization. MMVAE Sohn, K., et al., "Learning and Evaluating Cross-Modality Similarity Models" and mmJSD Mathieu, M., et al., "Discrete Flows: Invertible Implications of Non-Invertible Architectures" use MoE for the joint posterior, with MMVAE applying pairwise optimization for reconstructing all views, but struggling to aggregate information efficiently. mmJSD addresses this with a dynamic prior, replacing regularization with Jensen-Shannon Divergence. Wang, Z., et al., "Multi-Modal Variational Autoencoder with Mixture-of-Product-of-Experts Prior" propose Mixture-of-Product-of-Experts (MoPoE) to decompose KL divergence into $2^V$ terms, while MVTCAE Zhang, H., et al., "Multi-View Transformable Correlation Autoencoder" introduces an information-theoretic objective using forward KL divergences. MMVAE+ Chen, J., et al., "Multimodal Variational Autoencoders Plus: Separating Shared and View-Peculiar Information in Latent Subspaces and Incorporating Cross-View Reconstructions" extends MMVAE by separating shared and view-peculiar information in latent subspaces and incorporating cross-view reconstructions.

\noindent \textbf{Informational Priors in VAE Formulations}~ Sohn, K., et al., "Learning Structured Output Representation using Deep Conditional Generative Models" first introduced a data-dependent prior into VAE, which was later extended to multimodal VAEs for better inter-view consistency. Mathieu, M., et al., "Discrete Flows: Invertible Implications of Non-Invertible Architectures" employed a dynamic prior combined with the joint posterior to define Jensen-Shannon divergence regularization. Chen, T., et al., "Variational Autoencoders for Multi-Modal Learning with View-Specific Posteriors as Priors" used view-specific posteriors as priors, regularizing the joint posterior to ensure representations could be inferred from all views. Li, Y., et al., "Mixture-of-Experts Prior for Soft-Sharing of Information Across View-Specific Representations in Multi-Modal Variational Autoencoders" develop an MoE prior for soft-sharing of information across view-specific representations rather than simply aggregation. They relies on fusion of posteriors and enforce strict alignment, while we encourage soft consistency between views after transformations.