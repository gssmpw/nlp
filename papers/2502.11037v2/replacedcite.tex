\section{Related works}
\label{sec:2}

\noindent \textbf{Incomplete Multi-View Representation Learning (IMVRL)}~ Early IMVRL approaches addressed incomplete data by grouping available views and applying classical methods like CCA ____. DCCA ____ introduced nonlinear representations via correlation objectives, while DCCAE ____ enhanced reconstruction with autoencoders. As missing rates increased, the need for handling incomplete information grew. Methods like DIMVC ____ projected representations into high-dimensional spaces to improve complementarity, and DSIMVC ____ used bi-level optimization to impute missing views. Completer ____ maximized mutual information and minimized conditional entropy to recover missing views, while CPSPAN ____ aligned prototypes across views to preserve structural consistency. ICMVC ____ proposed high-confidence guidance to enhance consistency, and DVIMC ____ introduced coherence constraints to handle unbalanced information.

\noindent \textbf{Multimodal Variational Auto-Encoders (MVAEs)}~ MVAEs are generative models that maximize the log-likelihood of observed data through latent variables. MVAE ____ models the joint posterior using PoE ____, though this may hinder unimodal posterior optimization. MMVAE ____ and mmJSD ____ use MoE for the joint posterior, with MMVAE applying pairwise optimization for reconstructing all views, but struggling to aggregate information efficiently. mmJSD addresses this with a dynamic prior, replacing regularization with Jensen-Shannon Divergence. ____ propose Mixture-of-Product-of-Experts (MoPoE) to decompose KL divergence into $2^V$ terms, while MVTCAE ____ introduces an information-theoretic objective using forward KL divergences. MMVAE+ ____ extends MMVAE by separating shared and view-peculiar information in latent subspaces and incorporating cross-view reconstructions.

\noindent \textbf{Informational Priors in VAE Formulations}~ ____ first introduced a data-dependent prior into VAE, which was later extended to multimodal VAEs for better inter-view consistency. ____ employed a dynamic prior combined with the joint posterior to define Jensen-Shannon divergence regularization. ____ used view-specific posteriors as priors, regularizing the joint posterior to ensure representations could be inferred from all views. ____ develop an MoE prior for soft-sharing of information across view-specific representations rather than simply aggregation. They relies on fusion of posteriors and enforce strict alignment, while we encourage soft consistency between views after transformations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%