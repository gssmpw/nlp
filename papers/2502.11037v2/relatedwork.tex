\section{Related works}
\label{sec:2}

\noindent \textbf{Incomplete Multi-View Representation Learning (IMVRL)}~ Early IMVRL approaches addressed incomplete data by grouping available views and applying classical methods like CCA \citep{hotelling1992relations}. DCCA \citep{DCCA} introduced nonlinear representations via correlation objectives, while DCCAE \citep{DCCAE} enhanced reconstruction with autoencoders. As missing rates increased, the need for handling incomplete information grew. Methods like DIMVC \citep{DIMVC} projected representations into high-dimensional spaces to improve complementarity, and DSIMVC \citep{DSIMVC} used bi-level optimization to impute missing views. Completer \citep{completer1, completer2} maximized mutual information and minimized conditional entropy to recover missing views, while CPSPAN \citep{CPSPAN} aligned prototypes across views to preserve structural consistency. ICMVC \citep{ICMVC} proposed high-confidence guidance to enhance consistency, and DVIMC \citep{DVIMVC} introduced coherence constraints to handle unbalanced information.

\noindent \textbf{Multimodal Variational Auto-Encoders (MVAEs)}~ MVAEs are generative models that maximize the log-likelihood of observed data through latent variables. MVAE \citep{wu2018multimodal} models the joint posterior using PoE \citep{hinton2002training}, though this may hinder unimodal posterior optimization. MMVAE \citep{shi2019variational} and mmJSD \citep{mmJSD} use MoE for the joint posterior, with MMVAE applying pairwise optimization for reconstructing all views, but struggling to aggregate information efficiently. mmJSD addresses this with a dynamic prior, replacing regularization with Jensen-Shannon Divergence. \cite{suttergeneralized} propose Mixture-of-Product-of-Experts (MoPoE) to decompose KL divergence into $2^V$ terms, while MVTCAE \citep{MVTCAE} introduces an information-theoretic objective using forward KL divergences. MMVAE+ \citep{palumbo2023mmvae+} extends MMVAE by separating shared and view-peculiar information in latent subspaces and incorporating cross-view reconstructions.

\noindent \textbf{Informational Priors in VAE Formulations}~ \citet{tomczak2018vae} first introduced a data-dependent prior into VAE, which was later extended to multimodal VAEs for better inter-view consistency. \citet{mmJSD} employed a dynamic prior combined with the joint posterior to define Jensen-Shannon divergence regularization. \citet{MVTCAE} used view-specific posteriors as priors, regularizing the joint posterior to ensure representations could be inferred from all views. \citet{sutter2024unity} develop an MoE prior for soft-sharing of information across view-specific representations rather than simply aggregation. They relies on fusion of posteriors and enforce strict alignment, while we encourage soft consistency between views after transformations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%