% Jupyter Notebook for education
@INPROCEEDINGS{9648674,
  author={Liubchenko, Vira and Parkhomenko, Hlib},
  booktitle={2021 IEEE 16th International Conference on Computer Sciences and Information Technologies (CSIT)}, 
  title={The Involvement of Jupyter Notebooks as an Educational Tools: A Case Study}, 
  year={2021},
  volume={2},
  number={},
  pages={147-150},
  keywords={Training;Data analysis;Correlation;Data visualization;Data models;Frequency measurement;Task analysis;Jupyter notebook;exploratory analysis;measurement scale;descriptive statistics;frequency distribution;correlation analysis},
  doi={10.1109/CSIT52700.2021.9648674}
}

% Jupyter notebook for STEM education
@INPROCEEDINGS{9782924,
  author={Ochkov, Valery F. and Stevens, Alan and Tikhonov, Anton I.},
  booktitle={2022 VI International Conference on Information Technologies in Engineering Education (Inforino)}, 
  title={Jupyter Notebook, JupyterLab – Integrated Environment for STEM Education}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  keywords={Visualization;Publishing;Debugging;Writing;Kernel;Information technology;Engineering education;STEM;STEAM;Jupyter project;JupyterLab;Jupyter Notebook;computational document;integrated environment;Python;Markdown;Latex},
  doi={10.1109/Inforino53888.2022.9782924}
}


% jupyter notebook for teaching chemistry
@article{BASCUNANA2023155,
    title = {Impact of Jupyter Notebook as a tool to enhance the learning process in chemical engineering modules},
    journal = {Education for Chemical Engineers},
    volume = {44},
    pages = {155-163},
    year = {2023},
    issn = {1749-7728},
    doi = {https://doi.org/10.1016/j.ece.2023.06.001},
    url = {https://www.sciencedirect.com/science/article/pii/S174977282300026X},
    author = {J. Bascuñana and S. León and M. González-Miquel and E.J. González and J. Ramírez},
    keywords = {Unit operations, Chemical reactors, Mass balances, Python, Jupyter Notebook},
    abstract = {Jupyter Notebook (JN) is an example of an innovative and efficient digital tool that can be used effectively in higher education. In this work, a set of JNs was developed and implemented in the module “Chemical Processes” of the Master’s in Industrial Engineering at Universidad Politécnica de Madrid (Spain) to strengthen key Chemical Engineering concepts and enhance the learning process of students. Specifically, five interactive JN activities related to mass balances, reactor design, and separation operations (i.e., distillation, absorption, and extraction) were created, including a set of self-assessment tests to measure the effect of this tool on the knowledge and understanding of students. In addition, the final grades obtained by the students were used to evaluate the impact of the interactive activities proposed in the learning process. Finally, the opinions of the students were collected through an anonymous questionnaire containing closed and open questions. The results obtained are discussed in terms of numerical indicators and student feedback to assess overall performance and engagement. In summary, this innovative teaching approach based on JN was considered a successful initiative to promote student motivation and learning experience in Chemical Engineering-related modules.}
}

% improving jupyter notebook to make its use easier for students
@misc{casseau2023moonassistingstudentscompleting,
      title={MOON: Assisting Students in Completing Educational Notebook Scenarios}, 
      author={Christophe Casseau and Jean-Rémy Falleri and Thomas Degueule and Xavier Blanc},
      year={2023},
      eprint={2309.16201},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2309.16201}, 
}

% nbgrader
@article{Jupyter2019, 
    title = {nbgrader: A Tool for Creating and Grading Assignments in the Jupyter Notebook}, 
    author = {Project Jupyter and Douglas Blank and David Bourgin and Alexander Brown and Matthias Bussonnier and Jonathan Frederic and Brian Granger and Thomas Griffiths and Jessica Hamrick and Kyle Kelley and M Pacer and Logan Page and Fernando Pérez and Benjamin Ragan-Kelley and Jordan Suchow and Carol Willing}, 
    journal = {Journal of Open Source Education},
    doi = {10.21105/jose.00032}, 
    url = {https://doi.org/10.21105/jose.00032}, 
    year = {2019}, 
    publisher = {The Open Journal}, 
    volume = {2}, 
    number = {16}, 
    pages = {32}, 
}

% UNCode notebook auto-grader
@Article{su132112050,
AUTHOR = {González-Carrillo, Cristian D. and Restrepo-Calle, Felipe and Ramírez-Echeverry, Jhon J. and González, Fabio A.},
TITLE = {Automatic Grading Tool for Jupyter Notebooks in Artificial Intelligence Courses},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {12050},
URL = {https://www.mdpi.com/2071-1050/13/21/12050},
ISSN = {2071-1050},
ABSTRACT = {Jupyter notebooks provide an interactive programming environment that allows writing code, text, equations, and multimedia resources. They are widely used as a teaching support tool in computer science and engineering courses. However, manual grading programming assignments in Jupyter notebooks is a challenging task, thus using an automatic grader becomes a must. This paper presents UNCode notebook auto-grader, that offers summative and formative feedback instantaneously. It provides instructors with an easy-to-use grader generator within the platform, without having to deploy a new server. Additionally, we report the experience of employing this tool in two artificial intelligence courses: Introduction to Intelligent Systems and Machine Learning. Several programming activities were carried out using the proposed tool. Analysis of students’ interactions with the tool and the students’ perceptions are presented. Results showed that the tool was widely used to evaluate their tasks, as a large number of submissions were performed. Students expressed positive opinions mostly, giving feedback about the auto-grader, highlighting the usefulness of the immediate feedback and the grading code, among other aspects that helped them to solve the activities. Results remarked on the importance of providing clear grading code and formative feedback to help the students to identify errors and correct them.},
DOI = {10.3390/su132112050}
}

% Otter-Grader
@misc{ottergrader,
  author = {{Data Science Education Program at UC Berkeley}},
  title = "Otter-Grader Documentation",
  year = 2025,
  url = "https://otter-grader.readthedocs.io/en/latest/index.html",
  note = "Accessed: 2025-02-17"
}

% okpy => sieht unserem sehr ähnlich... :/ => verwendet aber kein LLM
@misc{okpy,
  author = {{UC Berkeley}},
  title = "OKpy: Automate Grading \& Personalize Feedback.",
  year = 2025,
  url = "https://okpy.org/",
  note = "Accessed: 2025-02-17"
}

%
@inproceedings{zabala2024development,
  title={Development and Evaluation of an AI-Enhanced Python Programming Education System},
  author={Zabala, Eric and Narman, Husnu S},
  booktitle={2024 IEEE 15th Annual Ubiquitous Computing, Electronics \& Mobile Communication Conference (UEMCON)},
  pages={787--792},
  year={2024},
  organization={IEEE}
}

% kritisches paper zu KI in der Lehre (insb Fobizz) ... siehe ccc talk
@misc{muehlhoff2025chatbotsimschulunterrichtwir,
      title={Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur automatischen Bewertung von Hausaufgaben}, 
      author={Rainer Muehlhoff and Marte Henningsen},
      year={2025},
      eprint={2412.06651},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2412.06651}, 
}

% large language models in education => die haben eine ziemlich coole überblick-grafik (taxonomy tree)
% => da sollte ich auch noch nach ein paar relevanten arbeiten gucken
@misc{wang2024largelanguagemodelseducation,
      title={Large Language Models for Education: A Survey and Outlook}, 
      author={Shen Wang and Tianlong Xu and Hang Li and Chaoli Zhang and Joleen Liang and Jiliang Tang and Philip S. Yu and Qingsong Wen},
      year={2024},
      eprint={2403.18105},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.18105}, 
}

% AI for learning how to code (and its pitfalls)
@misc{kazemitabaar2023novicesusellmbasedcode,
      title={How Novices Use LLM-Based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment}, 
      author={Majeed Kazemitabaar and Xinying Hou and Austin Henley and Barbara J. Ericson and David Weintrop and Tovi Grossman},
      year={2023},
      eprint={2309.14049},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2309.14049}, 
}

% huggingface api
@misc{huggingface_tgi,
        title = {Text Generation Inference (Github Repository)},
        url = {https://github.com/huggingface/text-generation-inference},
        author = {{Huggingface}},
        year = {2025}
}

% Mistral 7B => gibt es auch noch ein paper zu mistral large?
@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

% deepseek
@misc{deepseekai2025,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={{DeepSeek-AI}},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
@misc{deepseekai2025_full,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

% original Transformer paper
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

% original ChatGPT paper?

% GPT-4 paper
@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

% medpalm
@misc{singhal2022largelanguagemodelsencode,
      title={Large Language Models Encode Clinical Knowledge}, 
      author={Karan Singhal and Shekoofeh Azizi and Tao Tu and S. Sara Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and Ajay Tanwani and Heather Cole-Lewis and Stephen Pfohl and Perry Payne and Martin Seneviratne and Paul Gamble and Chris Kelly and Nathaneal Scharli and Aakanksha Chowdhery and Philip Mansfield and Blaise Aguera y Arcas and Dale Webster and Greg S. Corrado and Yossi Matias and Katherine Chou and Juraj Gottweis and Nenad Tomasev and Yun Liu and Alvin Rajkomar and Joelle Barral and Christopher Semturs and Alan Karthikesalingam and Vivek Natarajan},
      year={2022},
      eprint={2212.13138},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.13138}, 
}

% medpalm2
@misc{singhal2023expertlevelmedicalquestionanswering,
      title={Towards Expert-Level Medical Question Answering with Large Language Models}, 
      author={Karan Singhal and Tao Tu and Juraj Gottweis and Rory Sayres and Ellery Wulczyn and Le Hou and Kevin Clark and Stephen Pfohl and Heather Cole-Lewis and Darlene Neal and Mike Schaekermann and Amy Wang and Mohamed Amin and Sami Lachgar and Philip Mansfield and Sushant Prakash and Bradley Green and Ewa Dominowska and Blaise Aguera y Arcas and Nenad Tomasev and Yun Liu and Renee Wong and Christopher Semturs and S. Sara Mahdavi and Joelle Barral and Dale Webster and Greg S. Corrado and Yossi Matias and Shekoofeh Azizi and Alan Karthikesalingam and Vivek Natarajan},
      year={2023},
      eprint={2305.09617},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.09617}, 
}

% large language models in Law
@misc{lai2023largelanguagemodelslaw,
      title={Large Language Models in Law: A Survey}, 
      author={Jinqi Lai and Wensheng Gan and Jiayang Wu and Zhenlian Qi and Philip S. Yu},
      year={2023},
      eprint={2312.03718},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03718}, 
}

% LLMs for coding?
@misc{jiang2024surveylargelanguagemodels,
      title={A Survey on Large Language Models for Code Generation}, 
      author={Juyong Jiang and Fan Wang and Jiasi Shen and Sungju Kim and Sunghun Kim},
      year={2024},
      eprint={2406.00515},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.00515}, 
}

% datasets
% for code review:
@inproceedings{li_code_review,
author = {Li, Zhiyu and Lu, Shuai and Guo, Daya and Duan, Nan and Jannu, Shailesh and Jenks, Grant and Majumder, Deep and Green, Jared and Svyatkovskiy, Alexey and Fu, Shengyu and Sundaresan, Neel},
title = {Automating code review activities by large-scale pre-training},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549081},
doi = {10.1145/3540250.3549081},
abstract = {Code review is an essential part to software development lifecycle since it aims at guaranteeing the quality of codes. Modern code review activities necessitate developers viewing, understanding and even running the programs to assess logic, functionality, latency, style and other factors. It turns out that developers have to spend far too much time reviewing the code of their peers. Accordingly, it is in significant demand to automate the code review process. In this research, we focus on utilizing pre-training techniques for the tasks in the code review scenario. We collect a large-scale dataset of real-world code changes and code reviews from open-source projects in nine of the most popular programming languages. To better understand code diffs and reviews, we propose CodeReviewer, a pre-trained model that utilizes four pre-training tasks tailored specifically for the code review scenario. To evaluate our model, we focus on three key tasks related to code review activities, including code change quality estimation, review comment generation and code refinement. Furthermore, we establish a high-quality benchmark dataset based on our collected data for these three tasks and conduct comprehensive experiments on it. The experimental results demonstrate that our model outperforms the previous state-of-the-art pre-training approaches in all tasks. Further analysis show that our proposed pre-training tasks and the multilingual pre-training dataset benefit the model on the understanding of code changes and reviews.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1035–1047},
numpages = {13},
keywords = {Code review, datasets, deep learning, pre-training},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}
@inproceedings{guo_code_review,
author = {Guo, Qi and Cao, Junming and Xie, Xiaofei and Liu, Shangqing and Li, Xiaohong and Chen, Bihuan and Peng, Xin},
title = {Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623306},
doi = {10.1145/3597503.3623306},
abstract = {Code review is an essential activity for ensuring the quality and maintainability of software projects. However, it is a time-consuming and often error-prone task that can significantly impact the development process. Recently, ChatGPT, a cutting-edge language model, has demonstrated impressive performance in various natural language processing tasks, suggesting its potential to automate code review processes. However, it is still unclear how well ChatGPT performs in code review tasks. To fill this gap, in this paper, we conduct the first empirical study to understand the capabilities of ChatGPT in code review tasks, specifically focusing on automated code refinement based on given code reviews. To conduct the study, we select the existing benchmark CodeReview and construct a new code review dataset with high quality. We use CodeReviewer, a state-of-the-art code review tool, as a baseline for comparison with ChatGPT. Our results show that ChatGPT outperforms CodeReviewer in code refinement tasks. Specifically, our results show that ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively, while the state-of-the-art method achieves only 15.50 and 62.88 on a high-quality code review dataset. We further identify the root causes for ChatGPT's underperformance and propose several strategies to mitigate these challenges. Our study provides insights into the potential of ChatGPT in automating the code review process, and highlights the potential research directions.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {34},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

% AI tools
@misc{fobizz,
  author = {{fobizz | 101skills GmbH}},
  title = "Fobizz: AI-Powered Learning Platform",
  year = 2025,
  url = "https://fobizz.com/en/artificial-intelligence-at-school-and-in-the-classroom/",
  note = "Accessed: 2025-02-17"
}

% AI tools - blog post
@misc{chatgpthelperorcheating,
  author = "Michelle Schwaf",
  title = "ChatGPT & Beyond: Revolutionizing homework or enabling cheating?",
  year = 2024,
  url = "https://fobizz.com/en/chatgpt-homework-helper-or-cheating-tool/",
  note = "Authored: 2024-05-02; Accessed: 2025-02-17"
}

@misc{gotFeedback,
  author = {{Growth Over Time Learning, Inc. (gotLearning)}},
  title = "gotFeedback: AI Feedback for Teachers and Students",
  year = 2025,
  url = "https://www.gotlearning.com/ai-toolsold",
  note = "Accessed: 2025-02-17"
}

@misc{Fellofish,
  author = {{Fiete Education GmbH}},
  title = "FelloFish: AI Feedback for Teachers and Students",
  year = 2025,
  url = "https://www.fellofish.com/",
  note = "Accessed: 2025-02-17"
}

@misc{myTAI,
  author = "Patrick Rexroth",
  title = "MyTAI: AI Tool for Teachers",
  year = 2025,
  url = "https://www.mytai.net/",
  note = "Accessed: 2025-02-17"
}
https://www.khanmigo.ai/

@misc{khanmigo,
  author = {{Khan Academy}},
  title = "Khanmigo: Khan Academy's AI-powered teaching assistant and tutor",
  year = 2025,
  url = "https://www.khanmigo.ai/",
  note = "Accessed: 2025-02-17"
}

@misc{personify,
  author = {{Harness AI Inc.}},
  title = "Personify: AI Teaching Assistant Tailored to Your Course",
  year = 2025,
  url = "https://personifyai.app/",
  note = "Accessed: 2025-02-17"
}

@misc{cocalc,
  author = {{Sagemath Inc.}},
  title = "CoCalc: Collaborative Calculation and Data Science",
  year = 2025,
  url = "https://cocalc.com/",
  note = "Accessed: 2025-02-18"
}

@misc{vocareum,
  author = {{Vocareum Inc.}},
  title = "Vocareum: AI for Educators and Teaching",
  year = 2025,
  url = "https://www.vocareum.com/highered/",
  note = "Accessed: 2025-02-18"
}



% regular quizzes are good for final exam:
@article{El-Hashash_2022, title={Weekly Quizzes Reinforce Student Learning Outcomes and Performance in Biomedical Sciences in-course Assessments}, volume={2}, url={https://www.scipublications.com/journal/index.php/ojer/article/view/273}, abstractNote={Studies have highlighted the benefits of frequent quizzing in class. Frequent quizzing can promote more student attendance, engagement, practice and review, and achievement. Conversely, the opponents of frequent quizzing suggest that too frequent testing might hinder learning by frustrating anxious students and inhibiting larger units of instructional material. Notably, most studies have used degree examinations to evaluate the impact of quizzes on student learning and performance, yet little is known about whether quizzes can reinforce student performance in the in-course assessments (ICAs) despite ICA importance in student learning. The present study aimed to test the hypothesis that administration of weekly MCQ quizzes can enhance the leaning outcomes and performance of biomedical science students in assessment methods such as essay and oral presentation that can directly measure and provide information about student learning. It was therefore limited to in-course assessments. We found that the performance of the weekly quiz student group is remarkably better than that of the control student group in both the essay and oral presentation ICAs, which are two measures and indicators of student learning, suggesting improved student learning outcomes and performance after administrating weekly MCQ quizzes that also promoted student attendance in classrooms. The findings of this research study have implications for students, teachers, and curriculum designers in higher education.}, number={4}, journal={Open Journal of Educational Research}, author={El-Hashash, Ahmed}, year={2022}, month={Jun.}, pages={168–178} 
}

@article{kim2022homework,
  title={Homework, in-class assignments, and midterm exams: Investigating the predictive utility of formative and summative assessments for academic success},
  author={Kim, Alice SN and Stevenson, Cassandra R and Park, Lillian},
  journal={Open Scholarship of Teaching and Learning},
  volume={2},
  number={1},
  pages={92--102},
  year={2022}
}

@article{latif2020impact,
  title={The impact of assignments and quizzes on exam grades: A difference-in-difference approach},
  author={Latif, Ehsan and Miles, Stan},
  journal={Journal of Statistics Education},
  volume={28},
  number={3},
  pages={289--294},
  year={2020},
  publisher={Taylor \& Francis}
}

% open source language models:
@article{touvron2023llama_1,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama_2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
      year={2024},
      journal={arXiv e-prints},
      pages={arXiv--2407},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}


@article{mistralnemoblog,
  title={Mistral NeMo},
  author={{Mistral AI team}},
  year={2024},
  journal={Mistral Blog},
  url = {https://mistral.ai/news/mistral-nemo/}
}



@misc{mistrallarge,
  author = "Mistral.ai",
  title = "Mistral Large",
  year = 2022,
  url = "https://mistral.ai/en/news/mistral-large",
  note = "Accessed: 2025-02-18"
}

@misc{rfc6455,
    series =    {Request for Comments},
    number =    6455,
    howpublished =  {RFC 6455},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC6455},
    url =       {https://www.rfc-editor.org/info/rfc6455},
    author =    {Alexey Melnikov and Ian Fette},
    title =     {{The WebSocket Protocol}},
    pagetotal = 71,
    year =      2011,
    month =     dec,
    abstract =  {The WebSocket Protocol enables two-way communication between a client running untrusted code in a controlled environment to a remote host that has opted-in to communications from that code. The security model used for this is the origin-based security model commonly used by web browsers. The protocol consists of an opening handshake followed by basic message framing, layered over TCP. The goal of this technology is to provide a mechanism for browser-based applications that need two-way communication with servers that does not rely on opening multiple HTTP connections (e.g., using XMLHttpRequest or \textless{}iframe\textgreater{}s and long polling). {[}STANDARDS-TRACK{]}},
}

% AI shows similar performance to students in warehousing studies
@Article{computers14020052,
AUTHOR = {Franke, Sven and Pott, Christoph and Rutinowski, Jérôme and Pauly, Markus and Reining, Christopher and Kirchheim, Alice},
TITLE = {Can ChatGPT Solve Undergraduate Exams from Warehousing Studies? An Investigation},
JOURNAL = {Computers},
VOLUME = {14},
YEAR = {2025},
NUMBER = {2},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2073-431X/14/2/52},
ISSN = {2073-431X},
ABSTRACT = {The performance of Large Language Models, such as ChatGPT, generally increases with every new model release. In this study, we investigated to what degree different GPT models were able to solve the exams of three different undergraduate courses on warehousing. We contribute to the discussion of ChatGPT’s existing logistics knowledge, particularly in the field of warehousing. Both the free version (GPT-4o mini) and the premium version (GPT-4o) completed three different warehousing exams using three different prompting techniques (with and without role assignments as logistics experts or students). The o1-preview model was also used (without a role assignment) for six runs. The tests were repeated three times. A total of 60 tests were conducted and compared with the in-class results of logistics students. The results show that the GPT models passed a total of 46 tests. The best run solved 93% of the exam correctly. Compared with the students from the respective semester, ChatGPT outperformed the students in one exam. In the other two exams, the students performed better on average than ChatGPT.},
DOI = {10.3390/computers14020052}
}

% AI smarter than humans in economics (micro and macro-economics)
@article{geerling2023chatgpt,
  title={ChatGPT has mastered the principles of economics: now what?},
  author={Geerling, Wayne and Mateer, G Dirk and Wooten, Jadrian and Damodaran, Nikhil},
  journal={Available at SSRN 4356034},
  year={2023}
}

% ai in medicine
@article{singhal2025toward,
  title={Toward expert-level medical question answering with large language models},
  author={Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Amin, Mohamed and Hou, Le and Clark, Kevin and Pfohl, Stephen R and Cole-Lewis, Heather and others},
  journal={Nature Medicine},
  pages={1--8},
  year={2025},
  publisher={Nature Publishing Group US New York}
}
@article{DAE231006,
author = {Jung, Leonard B. and Gudera, Jonas A. and Wiegand, Tim L. T. and Allmendinger, Simeon and Dimitriadis, Konstantinos and Koerte, Inga K.},
title = {{ChatGPT Passes German State Examination in Medicine With Picture Questions Omitted}},
journal = {Dtsch Arztebl International},
volume = {120},
number = {21-22},
pages = {373-374},
doi = {10.3238/arztebl.m2023.0113},
year = {2023},
abstract = {},
URL = {https://www.aerzteblatt.de/int/article.asp?id=231006},
eprint = {https://www.aerzteblatt.de/pdf.asp?id=231006}
}
@article{lai2023evaluating,
  title={Evaluating the performance of ChatGPT-4 on the United Kingdom medical licensing assessment},
  author={Lai, U Hin and Wu, Keng Sam and Hsu, Ting-Yu and Kan, Jessie Kai Ching},
  journal={Frontiers in Medicine},
  volume={10},
  pages={1240915},
  year={2023},
  publisher={Frontiers Media SA}
}

% AI in law
@article{katz2024gpt,
  title={Gpt-4 passes the bar exam},
  author={Katz, Daniel Martin and Bommarito, Michael James and Gao, Shang and Arredondo, Pablo},
  journal={Philosophical Transactions of the Royal Society A},
  volume={382},
  number={2270},
  pages={20230254},
  year={2024},
  publisher={The Royal Society}
}

% computer science
@article{bordt2023chatgpt,
  title={Chatgpt participates in a computer science exam},
  author={Bordt, Sebastian and von Luxburg, Ulrike},
  journal={arXiv preprint arXiv:2303.09461},
  year={2023}
}

% coding

@misc{openaio3mini,
  author = "OpenAI",
  title = "OpenAI o3 mini",
  year = 2025,
  url = "https://openai.com/index/openai-o3-mini/",
  note = "Accessed: 2025-02-17"
}


% physics
@article{pimbblet2024can,
  title={Can ChatGPT pass a physics degree? Making a case for reformation of assessment of undergraduate degrees},
  author={Pimbblet, Kevin A and Morrell, Lesley J},
  journal={European Journal of Physics},
  volume={46},
  number={1},
  pages={015702},
  year={2024},
  publisher={IOP Publishing}
}
@article{revalde2025can,
  title={Can ChatGPT Pass a Physics Test?},
  author={Revalde, Gita and Zholdakhmet, Madi and Abola, Anda and Murzagaliyeva, Aliya},
  journal={Technology, Knowledge and Learning},
  pages={1--20},
  year={2025},
  publisher={Springer}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}