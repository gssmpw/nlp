% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorlinks]{hyperref}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\usepackage[svgnames]{xcolor}
\definecolor{highlightgrey}{rgb}{0.93,0.93,0.93} 
\NewDocumentCommand{\codeword}{v}{\colorbox{highlightgrey}{\texttt{#1}}}
\newcommand*{\red}{\textcolor{red}}

\begin{document}
%
%\title{PyEvalAI: Automated Evaluation of Jupyter Notebooks with Large Language Models}
%\title{PyEvalAI: Evaluating Jupyter Notebooks with AI}
%\title{PyEvalAI: AI-based evaluation of Jupyter Notebooks for scalable and efficient grading} % scalable stört mich ein bisschen, weil wir noch keine Evaluation auf grossen Gruppen haben...
%\title{PyEvalAI: AI-based evaluation of Jupyter Notebooks for efficient personalized feedback}
\title{PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate personalized feedback}


%
\titlerunning{PyEvalAI: AI-assisted evaluation of Jupyter Notebooks}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Anonymous Authors}
%\authorrunning{Anonymous Authors}

%\iffalse
\author{
Nils Wandel%\inst{1}%\orcidID{0000-1111-2222-3333} 
\and
David Stotko%\inst{1}%\orcidID{1111-2222-3333-4444} 
\and
Alexander Schier%\inst{1}%\orcidID{2222-3333-4444-5555} 
\and
Reinhard Klein%\inst{1}%\orcidID{2222-3333-4444-5555}
}
%
\authorrunning{N. Wandel et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{
Rheinische Friedrich-Wilhelms-Universität Bonn, Bonn NRW, Germany \\
\email{wandeln@cs.uni-bonn.de}
}
%\fi
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}

%Setting:
%- grading jupyter notebooks for student tutorials in STEM disciplines

%Grading Jupyter notebooks for student tutorials in STEM disciplines is a time-consuming and labor-intensive task for tutors. % => das geht zu direkt auf tutor probleme ein. ich würde gerne zuerst auf studenten probleme eingehen

% fange vllt am besten mit "recent advances in AI based grading ..." haben bereits viele Probleme gelöst... jedoch ...
% oder vllt doch besser:
% 1. Probleme für Tutoren mit Korrektur (allgemein)
% 2. Probleme für Studis (allgemein)
% 3. Recent advances ...
% 4. But Problems (privacy / mixture unit tests and LLMs / Latex and Code / open source and free)
% 5. Our solution
% 6. mention evaluation of case study

% 1.+2. Problem for Tutors & Students
Grading student assignments in STEM courses is a laborious and repetitive task for tutors, often requiring a week to assess an entire class. 
%This workload reduces the time available for personalized feedback where increased attention is required. 
For students, this delay of feedback prevents iterating on incorrect solutions,  hampers learning, and increases stress when exercise scores determine admission to the final exam. % einleitung evtl noch zu lang => sollte vllt schneller zum Punkt kommen. Nachteile vllt noch etwas klarer formulieren

% 3.+4. recent advancees and their problems
Recent advances in AI-assisted education, such as automated grading and tutoring systems, aim to address these challenges by providing immediate feedback and reducing grading workload. 
However, existing solutions often fall short due to privacy concerns, reliance on proprietary closed-source models, lack of support for combining Markdown, LaTeX and Python code, or excluding course tutors from the grading process. 

%Many systems also lack seamless integration of unit testing and AI-based assessment while keeping tutors in control of final grades. => das gehört in "unsere solution"

To overcome these limitations, we introduce \textbf{PyEvalAI}, an AI-assisted evaluation system, which automatically scores Jupyter notebooks using a combination of unit tests and a locally hosted language model to preserve privacy. 
Our approach is free, open-source, and ensures tutors maintain full control over the grading process. % and a structured overview. % oversight = full-control
% case study
A case study demonstrates its effectiveness in improving feedback speed and grading efficiency for exercises in a university-level course on numerics. % achtung: das mit der grading efficiency müssten wir dann wrsl auch quantitativ zeigen... (was wir eigtl nicht können)
% das mit "for exam preparation" ist noch etwas irritierend => setze das besser nach vorne... promising?
%We evaluate our system in a university-level numerics course, demonstrating its effectiveness in improving feedback speed and grading efficiency. % Future work will explore its deployment in large-scale STEM courses, bridging AI-driven automation with human expertise to enhance education at scale. / create database


\iffalse

Problems:
Students: 
- get feedback on exercise sheets they worked on only after 1-2 weeks
=> difficult to correct misunderstandings in time for subsequent exercises.
=> By the time they get feedback, they may have forgotten their reasoning behind specific solutions.
=> iterating on their solutions in real-time not possible. 
=> no trial and error
=> delayed feedback creates uncertainty and stress if admission to final exam depends on exercise performance

Tutors: 
- grading exercises is cumbersome / time-intensive
- repetitive (Even when students make common mistakes, tutors must repeatedly provide the same feedback.)
- doesn't scale to very large groups => (a tutor can grade 10-20 exercise sheets a week => you need a lot of tutors)
- organize grades in spreadsheet / identify difficult exercises
- Due to high grading workload, tutors have less time to provide individual feedback or mentoring to students.

Related work:
=> recently fast growing field of using AI / unit tests to help students during learning and facilitate grading of exercises
Problems:
- input should be in Markdown / Latex / Python Code
- Tutor must remain in the loop!
- privacy (chatgpt not possible / local llm needed)

Solution:
- Jupyter handles Markdown / Latex / Python Code
- effective approach to automatically evaluate jupyter notebooks
- complement tutorials with AI system (Tutor remains in full control over grades)
- open source
- privacy (local LLM)
- unit tests allow local LLM to give better informed grades
- scalability als outlook into the future (z.B. für MOOCs... => hier könnte man dann aber auch chatgpt verwenden... Khan academy macht's vor... trotzdem könnte man das erwähnen... Kombi mit unit tests ist ja auch ganz nett...)
- outlook 2: dataset generation of anonymized student solutions and tutor grades to evaluate accuracy of different LLMs for exercise grading.

Evaluation:
- case study: preparation for numerics exam at a university
- outlook: establish in exercise tutorials next semester


The abstract should briefly summarize the contents of the paper in
150--250 words.

\fi

\keywords{Personalized Feedback  \and AI-assisted grading \and LLM.}
\end{abstract}
%
%
%

\section{Introduction}

% allgemein auf unser Problem hinführen

% Laut Modulhandbuch sollen die Studierenden in der Lage sein, fortgeschrittene numerische Verfahren auf Problemstellungen der Informatik in konkreten Anwendungen anzuwenden. Als integrativ vermittelte Schlüsselkompetenzen werden explizit Transfer- und Teamfähigkeit sowie Leistungsbereitschaft, fachliche Flexibilität und Kreativität genannt. Die Bearbeitung der Übungsaufgaben soll in Gruppen erfolgen und schriftlich eingereicht werden, um auch Teamarbeit und Fertigkeiten zur schriftlichen Präsentation zu üben. Mündliche Präsentationsfähigkeiten können nur in einer Übungsgruppe erworben werden, ebenso ist eine Überprüfung des Anteils eines Studenten an einer schriftlich eingereichten und in der Gruppe bearbeiteten Übungsaufgabe nur durch (stichprobenartige) Überprüfung durch Vorstellung der Lösung möglich. Damit kann auch in Übungsgruppen auf die Einübung und Einhaltung guter wissenschaftlicher Praxis Wert gelegt werden, während ohne entsprechende Teilnahme aller Gruppenmitglieder an den Übungen einer Übungsgruppe keine Kontrolle darüber besteht, ob ein Teilnehmer tatsächlich an der Gruppenarbeit zur Lösung der Aufgaben teilgenommen hat oder ob die anderen Gruppenmitglieder nur zugelassen haben, dass der Name des Teilnehmers auf dem Übungsblatt steht, ohne dass dieser tatsächlich einen Beitrag geleistet hat, womit ein wissenschaftliches Fehlverhalten keine Konsequenzen hätte.

\iffalse
To help students to keep up with the lecture instead of only learning shortly before the exam, it is common % citation?
to have them solve weekly exercises. These exercises are usually corrected by tutors and often a certain number of points is required to be admitted to the exam. This does not only help lecturers to ensure that only students who prepared themselves are writing the exam, but also motivates students to keep up with the lecture during the course of the semester instead of starting to learn shortly before the exam, which often is not sufficient for good grades. % Citation?
Solving exercises also prepares students for other academic works like writing seminal papers or eventually their thesis in a well-organized manner. % TODO: Is writing a thesis comparable? => halte ich vllt für etwas weit hergeholt. aber der punkt mit dem regelmässigen lernen ist gut!
\fi


Weekly student assignments play an important role in STEM (Science Technology Engineering Mathematics) education as they help students to apply curriculum content in practice. % (i.e. learning by doing). %, thereby reinforcing their understanding of new concepts. % "because they" hört sich komisch an
Furthermore, exercise sheets motivate students to learn on a regular basis %, rather than procrastinating until just before the exam 
and are a good predictor for the final exam performance \cite{El-Hashash_2022,kim2022homework,latif2020impact}. 
%
% Alex: Fuer "Widely used" koennten Reviewer eine Citation wollen. Vielleicht besser primaer mit Nuetzlichkeit argumentieren?
A widely used option to create exercise sheets are Jupyter notebooks \cite{BASCUNANA2023155,9648674,9782924}, since they provide an easy way to combine Markdown and Latex instructions with interactive Python code. %, making them ideal for undergraduate courses in STEM disciplines.
%Problems for Tutors:
%- cumbersome / repetitive / laborious task
%- takes time (about a week)
However, grading and providing feedback on student assignments is a cumbersome and repetitive task for teaching assistants, typically taking around a week to complete for an entire class. 
%Problems for Students:
%- delay prevents iterating on solutions
%- hampers learning (students might forget exercises before feedback is given)
%- increased stress (admission to exam)
%- copy from fellow students or cheat with AI => studierende setzen sich nicht mehr mit Stoff auseinander
For students, such a delay of feedback prevents iterating on incorrect solutions and hampers the learning experience. % citation? because students might forget important details about exercises before feedback is given. 

On the other hand, recent breakthroughs in large language models are currently transforming traditional education in groundbreaking ways. 
% vllt zunächst allgemeine tests, in denen KI mittlerweile gut ist...
LLM-based solutions like ChatGPT already pass the bar exam for lawyers \cite{katz2024gpt}, state examinations for medicine in Germany and the UK \cite{singhal2025toward,lai2023evaluating} or exams in micro- and macro-economics \cite{geerling2023chatgpt}.
In STEM subjects such as computer science \cite{bordt2023chatgpt}, physics \cite{pimbblet2024can,revalde2025can} or warehousing studies \cite{computers14020052}, LLMs are often en par or exceed human baselines. 
Lately, OpenAI's o3 models outperformed most humans in coding and maths competitions \cite{openaio3mini}. 
% => KI sollte gut darin sein, dinge zu erklären, siehe...
Given these impressive general problem solving capabilities, it is a logical next step to apply LLMs also in the context of education \cite{wang2024largelanguagemodelseducation} and help students to solve problems on their own. 
% KI könnte oben genannte Probleme mit korrektur von jupyter notebooks lösen
The immediate feedback of LLMs has not only the potential to provide a more interactive learning experience for students \cite{khanmigo} but can also relieve tutors of some of the repetitive grading work \cite{personify}. 
% ... aber hat noch Probleme (z.B. Privacy / accuracy / consistency / adversarial attacks / cheating with AI )
Unfortunately, there are still several points that must be considered when incorporating AI systems in education such as data privacy concerns, reliance on expensive proprietary closed-source software, grading accuracy and consistency, hallucinations, and adversarial attacks \cite{muehlhoff2025chatbotsimschulunterrichtwir}.%,chatgpthelperorcheating}. % or cheating with AI

% deshalb schlagen wir vor: local LLM & keep tutors in the loop. jupyter notebook = natural environment
Therefore we introduce \textbf{PyEvalAI}, a novel and open-source\footnote{All code and prompts for this project will be released as open source under terms of the MIT licence upon acceptance. } AI-assisted evaluation system that automatically generates scores and feedback for Jupyter assignments using a combination of unit tests and a language model hosted by the university itself.
While the AI generated feedback provides a more interactive learning experience for students and reduces the workload for tutors, tutors can still easily correct inaccurate or inconsistent AI feedback at any time to mitigate the current shortcomings of LLMs. 
%dataset. % -> das sollte man vielleicht nur in den outlook schreiben?



% Paper is structured as follows: ...
% - related work
% - methods (architecture / UI / backend)
% - evaluation
% - conclusion
This paper is structured as follows: First, we cover related work. Next, we explain the architecture of PyEvalAI as well as its user interface and back-end in more detail. Then, we present the results of a case study in context of a university-level numerics course and conclude our work.


\iffalse
Recent Large Language Models (LLMs), on the other hand promise to provide immediate, helpful feedback to students. 
and take some of the pressure off from tutors \cite{}.

%Thus, with the recent advent of powerful language models, AI is increasingly being used to provide immediate, helpful feedback to students and releave some of the pressure from tutors \cite{}. % TODO: insert citations

%Often, exam admission is conditioned on the performance in assignments leading to increased stress levels and incentives to copy from fellow students or cheat by using AI.

% allgemeine Advancements in der KI beschreiben (KI ist extrem gut in Programmieren / schafft Medizin-Tests etc)
%AI in particular is in the midst of revolutionizing 
%has become a very tempting...
%(man kann dem quasi nichts entgegensetzen... detektion von AI schlecht möglich (paper))

But there are several concerns that need to be addressed when introducing AI generated feedback for student assignments:
First, privacy is a major issue since universities often do not allow student data to be transmitted outside the institution. This eliminates most solutions based on external services such as for example ChatGPT from OpenAI. Second, ...
% 
, accuracy and alignment with the course material. 
Mühlhoff et al. \cite{muehlhoff2025chatbotsimschulunterrichtwir} raise serious concerns about the accuracy and consistency of AI feedback from current models and ...?

... might not provide the solutions that were asked for / not in correspondence with or grounded in or based on the teaching material => could result in new questions

% extrem schnelle verbesserung ...
% => turn AI into something positive:
% wird nun auch immer häufiger in der Lehre eingesetzt
more engaging / interactive / personal
AI is not perfect yet (cite ccc paper) => we have to keep tutors involved in the grading process


% Motivation: statt schummeln mit KI => besser KI produktiv zum Lernen einsetzen
Since the advent of services that provide easily accessible LLM for end users, people started to use them to replace their own work in many ways, from simple grammar checking, to improving their writing style, up to full plagiarism. As one can neither full prevent the use for solving exercises, nor have a good way to prove cheating, as current AI detection systems are still not reliable % Citation => \cite{chatgpt_helper_or_cheating} ... ich würde aber versuchen, den Punkt nur kurz anzureissen
educators have to find a way to deal with students possibly using such systems as help. Furthermore, we acknowledge that the right use of these systems can be beneficial. From a supplementing text books, over generating examples for complicated topics up to learning by discussing own approaches with the text models, there are many valid use-cases for learning that one should not try to prevent.

For these reasons, we aim to embrace LLMs as a helpful tools for students and encourage them to use them without cheating, while not wanting them to use them for generating a full solution.

Our system provides an own feedback loop that is specially tailored for our execises, which means it is both able to better help with the specific problem as it knows the example solution of what we expect students to know, and has guardrails that prevent the students from just asking for a full solution. While this does not prevent the use of external tools that are able to solve the exercises, it encourages a responsible use for honest students by having a chatbot that does not eagerly supplement every explanation with the full solution as many other systems do. %Citation?
% Kurzer Abriss from Feedback Loop schon mal, oder auf naechste Kapitel verweisen?


ensure, chatbots do not simply tell the solution but actively involve students in the solution process
- wir wollen anreize zum schummeln reduzieren, indem wir eine KI an die Hand geben, welche motivierend beim auseinandersetzen mit dem Stoff unterstützt. 
Schnelles und motivierendes Feedback soll Studierende in die Lage versetzen, falsche Lösungen selbst zu korrigieren und sich intensiver mit den Inhalten auseinanderzusetzen.
Zusätzlich sollen Tutoren entlastet werden und privacy bewahrt werden (=> das passt eigtl besser zu "our approach")


Our approach:

This paper is structured as follows:
- related work
% - motivation, goals, ...? 
- methods (architecture / UI / backend)
- evaluation
- conclusion

by keeping human tutors in the loop, 
- we can prevent students from cheating the AI with adversarial prompts
- more accurate solutions for students / ensure quality standard are met (cite study that ai is not perfect yet)
- studierende sollen lösungen der KI hinterfragen können und mit tutoren darüber diskutieren
- dataset generation
- umso besser die KI wird, desto weniger arbeit entsteht für die Tutoren
- 

% TODO: wie mixed man die related work hier am besten rein? (nochmal von anderen arbeiten inspirieren lassen)
% => evtl etwas allgemeiner halten / nicht zu detailliert in die Nachteile eingehen
\fi

\section{Related Work}

%In this section, commercial as well as open-source educational tools for automated feedback and grading are covered. 
%Furthermore, we report on some of the latest developments in open-source large language models that can be hosted locally. % diese Einleitung und insb der letzte Satz gefallen mir nicht... Vllt einfach genz weglassen
\iffalse
Current solutions, ... => put into related work
- privacy issues: models on foreign cloud servers such as chatGPT by openai
- often rely on proprietary, closed source models
- tutors are put out of "the loop" / student solutions are stored in different files (unnecessary overhead for swapping between solutions) / feedback not immediate
- limited support to directly grade code / combine grading with unit tests
\fi

\subsubsection{Commercial Tools}
of numerous educational start-ups nowadays incorporate AI-based features in their products. For example, Khanmigo by Khan academy \cite{khanmigo} offers students interactive learning experiences by chatting with historical figures or by giving hints on questions without giving away the answer. Personify \cite{personify} allows teachers to create tailored teaching assistants for their courses. myTAI \cite{myTAI} helps teachers create teaching materials, and tools like gotFeedback \cite{gotFeedback}, Fellofish \cite{Fellofish} or Fobizz \cite{fobizz} help with automatic grading and feedback for writing exercises. Cocalc \cite{cocalc} and Vocareum \cite{vocareum} integrate generative AI into collaborative notebooks to help students. 
% problems with privacy / costly
However, commercial tools usually outsource the AI to external providers such as OpenAI potentially leading to conflicts with privacy policies of universities. 
Furthermore, licenses can be quite expensive for lower-budget universities and students. %( wrsl schwaches argument... A100 ist auch teuer)
Finally, it is important to keep tutors in charge of the final grades since the quality of AI-only feedback without human oversight is often still insufficient \cite{muehlhoff2025chatbotsimschulunterrichtwir}.

%\cite{fobizz,gotFeedback,Fellofish,myTAI,khanmigo,personify}
\iffalse
- AI in education: e.g. Khanmigo by Khan academy etc
    - most edu tools are based on ChatGPT
    - A few words about commercial viability of these tools?
    - And about why commercialization means less use for the education sector
- https://personifyai.app/
- CCC zu KI in der Lehre / automatische Korrektur: https://www.youtube.com/watch?v=o6DBGdnA1P4\&ab\_channel=media.ccc.de
    - fobizz => basiert auf Chat-GPT4 (zitiere evtl auch: Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur automatischen Bewertung von Hausaufgaben https://arxiv.org/abs/2412.06651 )
    - Fiete => Fellofish
    - myTAI
- Development and Evaluation of an AI-Enhanced Python Programming Education System (https://hsnarman.github.io/CONF/24-UEMCON-AIPython.pdf) => wen zitieren die noch so? (basieren auf OpenAI / Google)
- we want privacy => open source on premise LLM
    - local / open source LLMs:
- Mistral / LLama / 
- AI for coding: 
    - Codestral
    - qwen
    - ...
- latest developments:
    - deepseek => pretty large. quantized version didn't work that great. thinking process takes too long.
- unit testing
\fi


\subsubsection{Open-source tools} 
have been developed on multiple occasions to facilitate grading Jupyter Notebooks. Otter-Grader \cite{ottergrader} as well as OKpy \cite{okpy} were implemented at UC Berkley to serve in computer science and data science courses. UNCode \cite{su132112050} provides immediate feedback for students of an introductory lecture for "Intelligent Systems and Machine Learning" and NB grader \cite{Jupyter2019} is widely used for creating and grading assignments in computer science. 
However, these tools primarily focus on testing code solutions with rigorous unit-tests and do not support grading text exercises (for example mathematical derivations). Grading such exercises requires a more flexible LLM as a backbone.

\iffalse
- NB grader \cite{Jupyter2019}
    => Problems: grades by tutors take time
    => no automation for "text exercises" / proofs
- https://notebookgrader.com/
    (see \url{https://www.youtube.com/watch?v=yvLWbpgnspM\&ab\_channel=LucadeAlfaro-InstructionalVideos})
    => based on nbgrader
    => grading with unit tests / no LLMs (AI feedback only at UCSC)
- Otter-grader \cite{ottergrader} (https://discourse.jupyter.org/t/autograding-notebooks-with-otter-grader/4627?utm_source=chatgpt.com)
- UNCode \cite{su132112050}
- OKpy \cite{okpy}
\fi

% Model vs. Service (ChatGPT / GPT-4)
\subsubsection{Large language models} like ChatGPT (OpenAI), Gemini (Google), Claude (Anthropic) have shown tremendous success in recent years and offer convenient APIs for easy access in the cloud. However, sending sensitive student data to external providers without explicit consent of the students might violate privacy policies of the university. 
Fortunately, more and more powerful open weight models become publicly available, such as Llama \cite{touvron2023llama_1,touvron2023llama_2,dubey2024llama3herdmodels}, Mistral \cite{jiang2023mistral7b,mistralnemoblog,mistrallarge}, %TODO: Mistral Small, Mistral Large Citations? Is there a full paper about Nemo by now?
DeepSeek \cite{deepseekai2025}, and many more. Such models can run on local infrastructure to avoid any privacy concerns.


%But open alternatives have become increasingly powerful ...

% \subsubsection{Focus on Privacy / local open-source models} % => 
% Mention TGI and open WebUI?


% User privacy should have utmost importance for responsible use of LLM and is also required by laws like GDPR. Many universities in addition have strict regulations on how student data may be processed by external entities and may require prior approval of external systems before they can be used. This means that some of the popular service cannot readily be used without full consent of the students and the university.


%\subsubsection{Datasets}
%mostly: Q and A. not Q and A and feedback for A
% => datasets wären eher etwas für future work...

\section{PyEvalAI}
\iffalse
In this section, we'll first provide an architectural overview. Then, we describe the userinterface for students, tutors and administrators. Finally, we give details about the server, login authentification, database, unit tests and the local language model that automatically generates feedback for the student assignments.
\subsection{Architecture Overview}
\fi
The core of PyEvalAI consists of a Tornado server. As shown in Figure \ref{fig:architecture}, this server provides a front-end for students, tutors and administrators (more information is given in Section \ref{sec:user_interface}) and coordinates different back-end services such as user authentification, data-storage, unit-tests or the local LLM to generate automated feedback (see Section \ref{sec:backend}).



\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/architecture3.pdf}
    \caption{Architecture of PyEvalAI. Section \ref{sec:user_interface} describes the front-end, Section \ref{sec:backend} provides details about the back-end.} % TODO: versuche das quer hinzubekommen...
    \label{fig:architecture}
\end{figure}

% Authentication details (LDAP?), security considerations (TLS encryption? University VLAN, etc.)


\subsection{User-Interface / Front-end}
\label{sec:user_interface}

The user interface of PyEvalAI allows for two different kinds of interactions: First, a web-interface for tutors and students to review grades,
% that is provided via "Hypertext Transfer Protocol Secure" (https), 
and second, Jupyter Notebooks for students and administrators to work on, hand in and register exercises on the server via WebSockets \cite{rfc6455}. In the following sections, we explain the different interfaces for students, tutors and administrators in detail.

\subsubsection{Student Interface:}
\label{sec:student_interface}
%Intuitive and efficient interfaces are key for a usable system. As we want to improve on previous solutions, we aim to provide a better experience in two ways. 
%First, we provide an accessible web-interface for tutors and students for the grading, and second we provide Jupyter notebooks, which are a common choice for exchanging programming code together with notes in STEM fields that provides a nicer interface than just exchanging code files, which many STEM students are already familiar with.  %argumentiere, dass das eine Gewohnte Umgebung für STEM fächer ist => haben wir schon vorher... hier kein Platz mehr dafür


Assignment sheets are given to students in the familiar form of Jupyter notebooks. Figure \ref{fig:jupyter_students} shows how students can access all the necessary functionality to hand in exercises through the pip package \codeword{pyevalai}.
% Alex: Importieren die Studis denn oft das Package aktiv selber oder nutzen sie eher ein Template / Gegebenes Notebook mit Aufgaben? => sie nutzen ein vorgegebenes Template
% Vorschlag: Exercise sheets given as Jupyter notebooks import our pyevalai module, which provides the functionality needed for ...
% 
This module provides a simple API with functions like \codeword{login("server.url")} for signing in, entering a course via \codeword{enter_course("course name")} and handing in exercises via \codeword{handin_exercise("exercise name", solution)}. 
Depending on the exercise type, students can hand in text solutions given as a string % TODO: Hand written = typed? Sounds like scanned paper
or code solutions given as a python function. 
% TODO: Wenn wir hier nur den Funktionsnamen angeben und z.B. in einer Tabelle oder im Appendix die ganze Funktionssignatur (ggf. auch mit Rueckgabetyp?) bricht der Text besser um.
% TODO: Wie viel technische Details will das Journal ueberhaupt? Vielleicht eher darauf konzentrieren die Funktionen zu beschreiben als wie der genaue API-Call heisst?
% 
After handing in an exercise, PyEvalAI needs %on our test system (see sec. XXX for details) 
about 1-2 minutes (depending on the exercise type and current load of the server) to generate feedback and compute a grade which is directly displayed inside the Jupyter Notebook. % the student used to submit the exercise solution. 
The \codeword{handin_exercise} call is asynchronous, allowing students to proceed working on subsequent exercises without having to wait for the feedback of prior exercises to be computed.

%TODO: zeige Beispiel Jupyter code für login / enter course / handin ex / feedback direkt in jupyter notebook => für Text und Code exercise!

\begin{figure}
\centering
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{imgs/jupyter_student.pdf}
  \captionof{figure}{Example of Jupyter notebook for students. 1. block: login and enter numerics course. 2. block: exercise description. 3. block: student solution. 4. block: hand in exercise and obtain feedback by pyevalai. For best clarity, please view this figure digitally and zoom in as needed.}
  \label{fig:jupyter_students}
\end{minipage}%
\hspace{0.03\textwidth}
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{imgs/jupyter_admin2.pdf}
  \captionof{figure}{Jupyter notebook for admins. 1. block: login and enter course. 2. block: specify task, solution, unit tests and register exercise. 3. block: remove exercise}
  \label{fig:jupyter_admins}
\end{minipage}
\end{figure}

To get an overview of all exercises from all exercise sheets together with the achieved grades, numbers of attempts and deadlines in one place, students can visit the PyEvalAI website (see Figure \ref{fig:student_overview}). By selecting a specific exercise, students can recap the task assignment, all handed-in solution attempts and view corresponding feedback and grades provided by the LLM and tutors (see Figure \ref{fig:exercise_student}).

%CODO: ersetze Beispiele durch Beispiele, in denen nicht volle Punktzahl erreicht wurde und AI-Grader sinnvolles feedback gibt

\begin{figure}[h!]
\centering
\begin{minipage}{.48\textwidth}
  \centering
  %\includegraphics[trim={0 7.5cm 0 0},width=1\linewidth,clip]{imgs/student_overview.pdf}
  %\includegraphics[trim={0 5.8cm 0 0},width=1\linewidth,clip]{imgs/student_overview.pdf}
  \includegraphics[trim={0 18cm 0 0},width=1\linewidth,clip]{imgs/student_overview.pdf}
  \captionof{figure}{Students can easily overview all exercises, already achieved scores, corresponding deadlines and numbers of attempts.}
  \label{fig:student_overview}



  \includegraphics[trim={0 2cm 0 0},width=1\linewidth,clip]{imgs/tutor_table3.pdf}
  \captionof{figure}{Tutors can oversee in real-time all grades achieved by the students for the individual assignments in a table. By clicking on a grade, tutors can access further details and fix incorrect grades (see Figure \ref{fig:exercise_tutor}).}
  \label{fig:table_tutor}
  
\end{minipage}%
\hspace{0.03\textwidth}
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{imgs/exercise_student.pdf}
  \captionof{figure}{For every exercise, students can transparently examine their handed in solutions as well as grades provided by PyEvalAI or tutors.}
  \label{fig:exercise_student}
\end{minipage}
\end{figure}

\subsubsection{Tutor Interface:}
\label{sec:tutor_interface}

PyEvalAI provides tutors with a comprehensive table that contains grades for all students and all exercises (see Figure \ref{fig:table_tutor}). This allows to immediately identify exercises with poor grades that need special attention in subsequent tutorials. %Grades marked in bold still need to be checked by a human tutor. 
By selecting a table entry, tutors can view the task assignment, the handed-in solution attempts by the student as well as the corresponding achieved grades and feedback (see Figure \ref{fig:exercise_tutor}). If the AI feedback is inadequate, tutors can easily rectify incorrect grades and adjust the feedback.


%Why that interface? => TODO: move to introduction / motivation!
%Did someone give feedback (we count ourselves of course!) => das ist mehr oder weniger nach "Gefühl" / Erfahrung entstanden / wir haben keine Daten oder AB-Testing, auf deren Basis wir das Interface verbessert haben

%Jupyter Notebook:
%- allows for Markdown / Latex / Python Code
%- python allows to import pyevalai package for automatic grading / connection to server
% => see above

%Website:
%- students: all exercises / solutions ordered in one place => useful if there are multiple sheets


%- tutors:
%    - overview => which exercises are particularly hard?
%    - tutors can easily oversee and fix faulty grades by PyEvalAI


\begin{figure}[htp!]
\centering
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{imgs/exercise_tutor.pdf}
  \caption{Tutors can review all submitted assignments and feedback generated by PyEvalAI. Feedback that does not meet the required quality standards can be easily overwritten by the tutors.}
  \label{fig:exercise_tutor}
\end{minipage}
\hspace{0.03\textwidth}
\begin{minipage}{.47\textwidth}
  \centering
  % \includegraphics[width=1\linewidth]{imgs/tutor_table.pdf}
  % \captionof{figure}{Tutors can oversee in real-time all grades achieved by the students for the individual assignments in a table. By clicking on a grade, tutors can access further details and fix incorrect grades (see Figure \ref{fig:exercise_tutor} on the right). (TODO: translate / anonymize).}
  % \label{fig:table_tutor}
\begin{subfigure}[t]{1\textwidth}
    \includegraphics[width=1\linewidth]{imgs/evaluation/grading_comparison.pdf}
    \caption{This scatter plot shows a high correlation of AI grades and human grades.
    The point sizes indicate the number of occurences. }
    \label{fig:grading_comparison}
\end{subfigure}
\begin{subfigure}[t]{1\textwidth}
    \includegraphics[width=1\linewidth]{imgs/evaluation/grading_differences.pdf}
    \caption{This histogram shows how differences between AI and human grades are distributed. Most solutions were graded the same by the AI and human tutors.
    The mean difference is $-0.14\%$ with a standard deviation of $20.73\%$.}
    \label{fig:grading_differences}
\end{subfigure}
\caption{Comparison of AI grades with average grades provided by human tutors.}
\label{fig:comparison_plots}
\end{minipage}%
\end{figure}


\subsubsection{Administrator Interface:}
Course administrators have special privileges to register new exercises or remove existing exercises from a course. 
Typically, this is the role of
%an experienced PhD student
a teaching assisstant
or the professor who supervises the tutors. 
The interface for registering new exercises or removing existing ones is similar to the student interface for handing in exercises. 
As shown in Figure \ref{fig:jupyter_admins}, administrators create a Jupyter Notebook, import the \codeword{pyevalai} module, login with their credentials and enter the course they want to work on. 
Administrators can then register new exercises or update existing exercises with \codeword{register_exercise()}. Here, they can specify parameters like the name of the exercise, task description, sample solution, maximum number of achievable points, number of attempts (in our case, students always had 3 attempts), deadline or whether the exercise is a "text" exercise expecting a string for the solution or a "code" exercise expecting a python function. On top of that, a list of (unit-) tests can be provided for a more detailed specification of the evaluation criteria (more information on that is provided in Section \ref{sec:backend}). 
\iffalse
This function takes the following arguments:
\begin{itemize}
    \item \codeword{name}: a unique identifier name for the exercise that is also used by students to hand in their solutions.
    \item \codeword{task}: a string that contains the task description of the student assignment
    \item \codeword{solution}: a string that contains a sample solution for the assignment
    \item \codeword{points}: the maximum number of points that can be achieved in this exercise
    \item \codeword{tests}: a list of (unit-)tests that are conducted to provide additional information for the LLM. More information is given below in Section \ref{sec:backend}. (optional)
    \item \codeword{n_tries}: number of attempts for students to improve their submission. In our case, this parameter was always set to 3. (optional)
    \item \codeword{deadline}: a date-time string to specify a handin deadline (optional)
    \item \codeword{ex_type}: The type of an exercise is either "text" or "code". Text exercises expect the solution to be a string (as shown for example in Figure \ref{fig:jupyter_students}), Code exercises require the solution to be a python function.
\end{itemize}
\fi
By calling \codeword{remove_exercise("exercise name")}, administrators can remove existing exercises.

%separate from tutors. reasons:
%- unit tests have access to server => should be handled carefully
%- admin creates exercises and sample solutions (usually, this is the role of the Professor or PhDs)




% TODO: Backend, Frontend, ueberall ohne Bindestrich? An sich ich das denke ich ueblicher, auch wenn die "korrekte" Schreibweise eigentlich einen hat.
\subsection{Assignment grading / Back-end}
\label{sec:backend}

The back-end of PyEvalAI consists of multiple components that are coordinated by a Tornado server. % Citation / Footnote with link
Tornado is a lightweight and open source python web framework that can handle many concurrent connections efficiently. For user authentication, the "Lightweight Directory Access Protocol" (LDAP) is used to check login credentials of students, tutors and administrators affilitated with the university. The datastorage consists of a Pickle file which is updated whenever new data comes in and loaded whenever the server restarts. This simple approach yields sufficient performance and facilitates handling the data in python for later evaluation and dataset generation tasks. To grade the handed in solutions, unit tests and a large language model come into play, which we'll discuss in the following sections in more detail.

%Core: Tornado Server => why: lightweight python server. can handle many connections efficiently. easy integration with AI / LDAP / database

\subsubsection{Large Language Model (LLM)}
To ensure data privacy for the students, the language model is hosted locally on a single NVidia A100 GPU on an in-house GPU server. % How many? => we only use 1 of 6 GPUs. Thus I wouldn't mention the number...
For inference, we rely on the "Text Generation Inference" (TGI) toolkit by Huggingface \cite{huggingface_tgi} as it provides fast and efficient text generation and provides the commonly used OpenAI API for accessing text generation. % which is also compatible with other LLM providers such as OpenAI.
% Alex: Ich waere vorsichtig mit standardized, ich denke es ist eher so ein de-factor Standard als etwas wo jemand einem garantiert dass sich server/clients dran halten muessen
This gives us full flexibility for different models that may come up in the future. % CODO: das könnte evtl noch begründet / motiviert werden.
During the initial development phase, we tested several models such as quantized versions of LLaMa \cite{dubey2024llama3herdmodels} and distilled versions of Deepseek \cite{deepseekai2025} % TODO: Citation
but found that an AWQ-quantized version \cite{lin2024awq} of Mistral Large \cite{mistrallarge} produced qualitatively the most convincing results while running at acceptable inference speed.
Using Mistral Large, PyEvalAI took 88.2 seconds on average to compute feedback for a submission and around half of all responses were computed within 1 minute. 
%In the future, as we collect more data, we'll test / compare more different models more rigorously => move to outlook

To grade exercises and generate feedback, we first provide the model with the task description, a sample solution and the student solution. %finally the student solution together with instructions on how to use the sample solution to grade the student's solution.
%Next, we inquire about the specific aspects in which help is needed to correct the student's solution. 
The LLM is then prompted with optional (unit-) test questions, which are described in more detail in the next section, to check for correctness and award partial points. 
%After these targeted checks, it is asked to compare the student's answer with the sample solution, identifying any differences or mistakes without yet assigning a score. 
Then, we ask the LLM to carefully compare the sample and student solution and comment on the severeness of detected differences. 
%It also questions whether any critical steps from the sample solution were omitted and, if so, requests details on what was missed.
Furthermore, we ask the LLM if critical steps from the sample solution were ommited and, if so, request details on what was missed.
Finally, the LLM is tasked with determining a final score based on these insights and generating a concise, encouraging feedback message that explains the points awarded and any deductions. 
To achieve deterministic outputs, we use greedy sampling. 

%This allows the automatic verification of submissions using a sample solution or unit-tests, allowing for detailed comparisons between student and sample solutions, specific guidance on individual parts of the submission and prevents feedback from using yet unknown concepts to the students.
This step-by-step approach, combined with a detailed comparison between the sample and student solutions, generates precise and constructive feedback for the student and ensures that feedback is both comprehensive and actionable, giving students clear insights into their work and concrete steps for improvement.

%The advantage of this step by step approach lies in the automatic verification of submissions using a sample solution or unit-tests.
%This enables detailed comparisons between student and sample solutions, specific guidance on individual parts of the submission and prevents feedback from using yet unknown concepts to the students.





\iffalse

In addition to grading the exercise using the LLM, we run optional unit-tests, which are described in more detail in the next section.
Then, we ask the LLM to carefully compare the sample and student solution and comment on the severeness of detected differences.
Furthermore, we ask the LLM if important steps are missing in the student solution and, if so, to describe these missing parts.
%This allows the automatic varification of submissions using a sample solution or unit-tests, allowing for detailed comparisons between student and sample solutions, specific guidance on individual parts of the submission and prevents feedback from using yet unknown concepts to the students.
%The main advantage compared to using ... is the automatic varification of submissions using a sample solution or unit-tests.
%This enables detailed comparisons between student and sample solutions, specific guidance on individual parts of the submission and prevents feedback from using yet unknown concepts to the students.
%Finally, we ask the model to return a grade between 0 and the specified maximum number of achievable points as well as to write a short and constructive feedback without revealing the sample solution. To achieve deterministic outputs, we use deterministic sampling. 
Finally, the LLM is tasked with determining a final score based on these insights and generating a concise, encouraging feedback message that explains the points awarded and any deductions.
To achieve deterministic outputs, we use greedy sampling. 
\fi

\subsubsection{Unit-Tests}
By specifying unit tests (see Figure \ref{fig:jupyter_admins}), we can provide additional input for the LLM to improve grading accuracy. There are 2 types of tests supported in pyevalai:
\begin{enumerate}
    \item \textbf{Text:} These tests consist of yes-no questions and corresponding points that should be granted if the LLM replies with yes or no (for example "Is the student cheating by using prohibited libraries in his code?" or "Did the student document the code properly?"). This allows administrators to specify certain evaluation criteria more clearly.
    \item \textbf{Code:} To assist LLMs in distinguishing between correct and incorrect code, code-tests consist of a question (e.g. "Does the function return correct values?") and a test function that takes the student's submitted code as input. After testing the student code, the test function returns a reply to the initial question (e.g. "The function returns correct values!") and a number of points that should be taken into account for the final score. For security reasons, the student code is not executed on the server but inside the student's notebook. To this end, the websocket connection is used to transmit input and return values between the student notebook and the tornado server.
\end{enumerate}

After the execution of these tests, the specified questions, corresponding replies, as well as the sum of achieved test-points are given as additional input to the language model to achieve better informed grades.



\section{Evaluation}

To validate PyEvalAI, % before future use in mandatory tutorials, 
we conducted a case study in a university level numerics course where volunteer Bachelor of Science in Computer Science students used interactive sample questions to prepare for the course exam. 
To obtain quantitative results about the AI's grading accuracy, we manually checked and rectified the AI generated feedback for all handed in solutions. 
Furthermore, we collected qualitative data through an anonymous feedback survey.
In the following section, we present our key findings.

%case study: exam preparation for numerics course.


\subsection{Quantitative}

In our case study, 20 volunteer students participated by solving exercises from a pool of 19 practice tasks. 
%In total, they handed in 277 solutions that were graded by the AI and human tutors.
%These solutions can be grouped into 113 exercise submissions that took only 1 attempt, 43 submissions with 2 attempts, and 26 submissions with a total of 3 attempts.
In total, they handed in 277 solutions, which were graded by both the AI and human tutors. These solutions resulted from varying numbers of attempts: 113 exercises were solved in a single attempt, 43 required two attempts, and 26 took three attempts to reach the correct solution.
In the following, we investigate the accuracy of PyEvalAI and how it helped students to improve their scores over multiple attempts.

\subsubsection{Grading Accuracy}
\label{sec:grading_accuracy}

% how accurate are grades of handed in solutions? (comparison to tutor grades)?
% - correlation plot (percent)
% - histogram over / under estimation of points
First, we evaluate the accuracy of the AI model by comparing its generated scores with scores of human numerics tutors. 
Figure \hyperref[fig:grading_comparison]{8a} shows human gradings and AI gradings as coordinates in a 2D diagram where the number of occurrences is indicated by the point sizes. 
%We further indicate the region of perfect agreement between AI and human grading by a black dashed line. % platz sparen
The majority of all points is located at the top right corner where AI and humans agree to give a large fraction of the total points.
% Lots of submissions are indeed correct and graded with 100\% by the AI as well as human tutors. 
% der nachfolgende abschnitt sollte evtl besser in die nächste section >>
\iffalse
Moreover, we can observe that an incomplete or incorrect submission by a student often leads to an additional attempt to improve the result.
This is visible as the majority of the green and yellow points (indicating the second or third last attempt) did not get a score of $100\%$ by the AI.
Similarly, when graded with all possible points (top border of the diagram), the students do no make use of additional attempts resulting in purple points in the diagram.
We will focus more on the addiational attempts in the next subsection.
\fi
% <<
As indicated by the small purple dots scattered across the top of the plot, in some cases the AI gave the highest possible score even though the submission was incomplete (e.g. if the solution is correct but the calculations were not presented in the submission). 
Figure \hyperref[fig:grading_differences]{8b} depicts a histogram of differences between AI and human gradings. 
It shows that 182 (or 65.7\%) of all 277 human gradings where identical to the AI. 
Furthermore, the distribution has a mean of $-0.14\%$ with a standard deviation of 20.73\% indicating that our model does not expose a significant bias.
Nonetheless, variations between automated and human gradings are still present. To some extent, however, this is also typical of human gradings. %but some fluctuations will always be present even within several human tutors. %TODO: fluctuations hört sich sehr nach inconsistencies an... kann man das noch etwas anders formulieren?
%Unfortunately, we have not been able to evaluate on human variations in our study as this will multiply the workload for all the tutors. \textcolor{red}{Kann man das so stehen lassen oder wirkt das zu sehr nach einer Ausrede?}% => TODO: liest sich tatsächlich recht holprig... sollte man vllt nicht näher drauf eingehen...
In our study, we have not yet evaluated human variations, as this would significantly increase the workload for all tutors.
However, this remains an interesting aspect for future research.

Lastly, we compare how often human corrections occured for the scores and the feedback text of the AI.
%In order to quantify this, we instructed the tutors to add a comment by themselves if the AI feedback is insufficient.
Table \ref{tab:human_corrections} presents how often the tutors agreed with the AI, only changed the grading, only corrected the feedback text or changed both. 
In 57.8\% of all submissions we have full agreement between the tutors and the AI. % such that tutors only have to comfirm the already existing feedback and grading. %<-das ist eher ein studien-detail, welches für die Ergebnisse und in der praktischen anwendung keine grosse Rolle mehr spielen sollte
In 25.6\% of all cases the tutors changed the AI score as well as the feedback text. 
The remaining 16.6\% contain the cases in which the AI response is somewhat close to the human grading, such that only either the grading or the text had to be adjusted.
In these cases the AI provides a baseline for the feedback on which the tutors can build on.

%Regarding that all of our exercises give at most 4 points, this usually implies differences of less than 1 point.
%In summary, while the AI model may still produce incorrect scores, it often matches human scores and has a low bias. 
The results support our approach of using LLMs as a semi-automatic tool to assist human tutors. % instead of replacing them entirely.
%\textcolor{red}{Ich hätte gerne so eine kleine mini conclusion, aber die jetzige gefällt mir noch nicht so ganz.}
%In summary, although the AI model still produces occasional incorrect gradings, it aligns very well with human assessments on average. This supports our approach of using LLMs as a semi-automatic tool to assist human tutors, 
In summary, although the AI model still produces occasional incorrect gradings, it aligns very well with human assessments on average. This supports our approach of using LLMs as a semi-automatic tool to assist human tutors, reducing their overall grading workload.

\begin{table}[t]
\centering
\caption{Human corrections on AI scores and feedback texts.}
\label{tab:human_corrections}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{ccc}
& keep AI feedback & fix AI feedback \\
\hline
keep AI score & 160 (57.8\%) & 22 (7.9\%) \\
fix AI score & 24 (8.7\%) & 71 (25.6\%) \\
\end{tabular}
%\vspace{-10pt}
\end{table}


% \begin{figure}[htp!]
% \begin{subfigure}[t]{0.49\textwidth}
%     \includegraphics[width=1\linewidth]{imgs/evaluation/grading_comparison.pdf}
%     \caption{Comparison between the grading of the AI and humans.
%     The point sizes depict the number of occurences. This scatter plot shows ...}
%     \label{fig:grading_comparison}
% \end{subfigure}
% \hspace{0.01\textwidth}
% \begin{subfigure}[t]{0.49\textwidth}
%     \includegraphics[width=1\linewidth]{imgs/evaluation/grading_differences.pdf}
%     \caption{Differences in grading between the AI and human corrections.
%     The mean difference within all samples is $29.5 \pm 52.8$. This histogram shows how differences between AI and human grades are distributed...}
%     \label{fig:grading_differences}
% \end{subfigure}
% \caption{Comparison of AI grades with average grades provided by human tutors.}
% \label{fig:comparison_plots}
% \end{figure}

\subsubsection{Improved student performance through multiple graded attempts}
\label{sec:student_improvement}

One important benefit of our system is the quick response by the LLM in comparison to traditional grading workflows which enables additional attempts to repeat and improve on tasks.
Figure \ref{fig:student_improvement} depicts the AI's grading of exercises over all attempts.
This time, the frequency of the data is visible as increasing opacity of the lines.
We again observe that a large number of last submissions are graded with 100\% of all points.
Moreover, when using multiple attemps, there is a significant improvement from the second last attempt to the last one.
To make this more clear, we compute the mean and standard deviation of all AI gradings for each attempt separately and report the numbers in Table \ref{tab:percentage_per_attempt}.
The last attempt always yields the most points with the lowest standard deviation. % TODO: is this really true? was bedeutet die standard deviation?
Students improve on average by 25\% -- 30\% compared to the second last attempt.
Table \ref{tab:n_improvements_per_attempt} displays the number of students that improved, worsened or stayed equal during their attempts. 
Also here we see that the vast majority of students indeed improved in grading.
Performing the same analysis on human grades instead of AI grades resulted in very similar statistics. 
%We have collected the same statistics for the tutor gradings as well with lots of similarities compared to the AI's grading.
%Hence, we have only mentioned the latter here (\textcolor{red}{Bilder in den Appendix?}).
Sometimes the students submitted solutions that were very similar to the previous attempts such that no change or even worsened gradings occured. 
Although our language model is configured to be deterministic, minor changes in the submission can still cause variations that may reduce the amount of points the model returns as the final grading.
This property is part of the LLM, which is difficult to prevent and requires further development in the future.

% \begin{table}
% \centering
% \caption{Average percentage of points per attempt.}
% \label{tab:percentage_per_attempt}
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{ccccc}
% Total attempts & \# solutions & Last & Second last & Third last \\
% \hline
% 1 & 37 & $93.9 \pm 16.3$ & & \\
% 2 & 20 & $93.9 \pm 16.3$ & $93.9 \pm 16.3$ & \\
% 3 & 16 & $93.9 \pm 16.3$ & $93.9 \pm 16.3$ & $93.9 \pm 16.3$ \\
% \end{tabular}
% \end{table}


\begin{figure}[t]
\centering
\begin{minipage}{.42\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{imgs/evaluation/student_improvements.pdf}
  \captionof{figure}{Improvement of students on AI scores over multiple attempts.}
  \label{fig:student_improvement}
\end{minipage}%
\hspace{0.01\textwidth}
\begin{minipage}{.56\textwidth}
    \centering

    \captionof{table}{Average percentage of points per attempt.}
    \label{tab:percentage_per_attempt}
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccccc}
    Total & number of & $3^{rd}$ & $2^{nd}$ & \\
    attempts & solutions & last & last & Last \\
    \hline
    1 & 37 & & & 91.4\% \\
    2 & 20 & & 67.2\% & 95.4\% \\
    3 & 16 & 58.0\% & 63.3\% & 87.2\% \\
    \end{tabular}

    
    \captionof{table}{Number of students that performed better, equal and worse with multiple attempts.}
    \label{tab:n_improvements_per_attempt}
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccccc}
    \#attempts & from $\rightarrow$ to & better & equal & worse \\
    \hline
    2 & $1 \rightarrow 2$ & 36 & \phantom{0}6 & \phantom{0}1 \\
    3 & $1 \rightarrow 2$ & 11 & \phantom{0}5 & 10 \\
    3 & $2 \rightarrow 3$ & 19 & \phantom{0}5 & \phantom{0}2 \\
    \end{tabular}
\end{minipage}
\end{figure}


\subsection{Qualitative}

%Finally, 
We conducted an anonymous survey in which 14 students participated.
The most important results are visualized in Figure \ref{fig:bars_students_compressed}.
In addition to the proportions of the responses, we compute a mean and standard deviation by converting all possible choices into equally-spaced numerical values between 0 (strongly disagree) and 1 (strongly agree) and visualizing these results as error bars for each question. % TODO: sind diese error-bars valide? müssten sie nicht kleiner werden, wenn wir mehr daten bekommen?

We observe that PyEvalAI is well received by the students and that the grading is perceived as fair and understandable.
Similarly, the students acknowledge the quick AI feedback and consider it helpful, motivating and useful for making quick progress.
These results are in accordance with the quantitative evaluation in Section \ref{sec:grading_accuracy} for the gradings and the improvements with several attempts. 
Most students do not find the average waiting time of 88.2 seconds disruptive, but faster responses will be an obvious improvement to the PyEvalAI user experience. 
Finally, we ask how much benefit the AI tool provides when learning in groups versus when learning alone. 
Here, the students found PyEvalAI more useful when learning alone.
We suppose this could be due to group members already providing helpful feedback to each other, reducing the need for additional feedback from the AI.
%We expect the LLM being used similar to a member in a learning group and as such being replacable by other human group members. 
%However, constructive discussions are not present when learning alone. % TODO: das liest sich noch holprig... (kann man das kürzer und klarer schreiben?)
%Hence, the AI model may be more valueable in the latter case.
%This hypothesis is mostly confirmed by our questionnaire although only 4 students responded to the assessment about learning in a group.
%One reason for the low number of responses may be the study being conducted mainly as a tool for preparing for the final exam which is written without any support by other people. % würde ich eher weglassen, da ziemlich hypothetisch... => TODO: könnte man falsch verstehen und denken, dass damit eine geringe Anzahl von Antworten auf alle Fragen gemeint ist
%However only few students learn regularly in groups
In summary, most students rate PyEvalAI positively and would like to use it in future tutorials.

We also conducted a survey among the tutors with predominantly positive feedback. 
Although regular adjustments still had to be made by the tutors through the web-interface, % Nevertheless
all tutors agree or strongly agree that PyEvalAI is a helpful addition for students and confirm that the grading and feedback is mostly correct. 
This validates our findings during the quantitative evaluation in Section \ref{sec:grading_accuracy}.
However, the sample size (4 tutors) was too small to make meaningful statements about individual results. 
More data from further employment of PyEvalAI will increase statistical significance in future assessments. % TODO: kann man das so stehen lassen?

\begin{figure}[t]
    \includegraphics[width=1\linewidth]{imgs/evaluation/bars_students_compressed.pdf}
    \caption{Results of the student survey regarding the experience with PyEvalAI.}
    \label{fig:bars_students_compressed}
\end{figure}


\section{Conclusion}

In this work, we presented PyEvalAI, a novel tool to provide students with immediate personalized AI generated feedback and support tutors in the grading process. PyEvalAI accepts text and code exercises, ensures privacy and is open source. A thorough evaluation of a case study in a numerics course demonstrates that the AI system often produces accurate and useful feedback that helps students to improve their solutions and tutors in the grading process.

% outlook
In the future, PyEvalAI will be introduced in further courses to complement tutorials. 
A growing database of exercises, student solutions, AI and tutor feedback will enable thorough comparisons of different LLMs and help to improve prompting strategies and fine-tune models. 
We believe that, as local LLMs become increasingly more powerful and accurate, the learning experience for students can be further improved and tutors will be able to shift their focus from repetitive grading to providing individual feedback to students in the tutorials. 


%System could be also used in large scale groups (e.g. MOOCs with 100s or 1000s of participants)

%\newpage

\iffalse
\section{Appendix}
\begin{verbatim}
- Example Prompts (but they are german)
- Example Outputs
- Output comparision side by side
(different prompts tested, different models tested, ...)
\end{verbatim}



\newpage
\section{First Section}
\subsection{A Subsection Sample}
Please note that the first paragraph of a section or subsection is
not indented. The first paragraph that follows a table, figure,
equation etc. does not need an indent, either.

Subsequent paragraphs, however, are indented.

\subsubsection{Sample Heading (Third Level)} Only two levels of
headings should be numbered. Lower level headings remain unnumbered;
they are formatted as run-in headings.

\paragraph{Sample Heading (Fourth Level)}
The contribution should contain no more than four levels of
headings. Table~\ref{tab1} gives a summary of all heading levels.

\begin{table}
\caption{Table captions should be placed above the
tables.}\label{tab1}
\begin{tabular}{|l|l|l|}
\hline
Heading level &  Example & Font size and style\\
\hline
Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
\hline
\end{tabular}
\end{table}


\noindent Displayed equations are centered and set on a separate
line.
\begin{equation}
x + y = z
\end{equation}
Please try to avoid rasterized images for line-art diagrams and
schemas. Whenever possible, use vector graphics instead (see
Fig.~\ref{fig1}).

\begin{figure}
\includegraphics[width=\textwidth]{fig1.eps}
\caption{A figure caption is always placed below the illustration.
Please note that short captions are centered, while long ones are
justified by the macro package automatically.} \label{fig1}
\end{figure}

\begin{theorem}
This is a sample theorem. The run-in heading is set in bold, while
the following text appears in italics. Definitions, lemmas,
propositions, and corollaries are styled the same way.
\end{theorem}
%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%
\begin{proof}
Proofs, examples, and remarks have the initial word in italics,
while the following text appears in normal font.
\end{proof}
For citations of references, we prefer the use of square brackets
and consecutive numbers. Citations using labels or the author/year
convention are also acceptable. The following bibliography provides
a sample reference list with entries for journal
articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
and a homepage~\cite{ref_url1}. Multiple citations are grouped
\cite{ref_article1,ref_lncs1,ref_book1},
\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
\fi

\iffalse
\begin{credits}
\subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
used for general acknowledgments, for example: This study was funded
by X (grant number Y).

\subsubsection{\discintname}
It is now necessary to declare any competing interests or to specifically
state that the authors have no competing interests. Please place the
statement with a bold run-in heading in small font size beneath the
(optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
system, is used, then the disclaimer can be provided directly in the system.},
for example: The authors have no competing interests to declare that are
relevant to the content of this article. Or: Author A has received research
grants from Company W. Author B has received a speaker honorarium from
Company X and owns stock in Company Y. Author C is a member of committee Z.
\end{credits}
\fi
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{mybibliography}
%

\iffalse
\begin{thebibliography}{8}
\bibitem{ref_article1}
Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\bibitem{ref_lncs1}
Author, F., Author, S.: Title of a proceedings paper. In: Editor,
F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
Springer, Heidelberg (2016). \doi{10.10007/1234567890}

\bibitem{ref_book1}
Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
Location (1999)

\bibitem{ref_proc1}
Author, A.-B.: Contribution title. In: 9th International Proceedings
on Proceedings, pp. 1--2. Publisher, Location (2010)

\bibitem{ref_url1}
LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
\end{thebibliography}
\fi

\end{document}
