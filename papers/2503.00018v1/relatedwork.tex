\section{Related Work}
\paragraph{LLM-Based Patient Simulation in Mental Health.} 
Recent work has explored using LLMs to simulate therapy clients for clinician training \cite{wang2024patient, louie2024roleplay}. Early approaches relied on generic LLM prompting \cite{qiu2024interactive}, but concerns about clinical validity and ethical risks \cite{haltaufderheide2024ethics, zidoun2024artificial} have led to structured modeling efforts. Patient-$\psi$ \cite{wang2024patient} integrates cognitive modeling from clinical frameworks to enhance realism, while Roleplay-doh \cite{louie2024roleplay} applies principle-adherence prompting to improve consistency. However, these methods struggle with generating nuanced, profile-consistent responses, highlighting the need for systematic alignment strategies.

\paragraph{Preference Optimization for Alignment.}
Optimizing LLMs with human preference data has been widely studied \cite{christiano2017deep}, with Direct Preference Optimization (DPO) emerging as an efficient alternative to reinforcement learning \cite{rafailov2023direct}. While DPO has been applied in general chatbot alignment and some scientific domains \cite{cheng2024decomposed,savage2024fine}, its use in simulation for clinical psychology practice remains underexplored. Recent methods propose augmenting preference data through automated techniques \cite{pi2024strengthening, lu2024step}, which aligns with our approach of leveraging model-based augmentation to enhance preference learning for profile-guided mental health simulations.