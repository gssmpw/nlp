\section{Related Work}
\paragraph{LLM-Based Patient Simulation in Mental Health.} 
Recent work has explored using LLMs to simulate therapy clients for clinician training **Brown, "Language Models as Therapy Clients"**. Early approaches relied on generic LLM prompting **Radford, "Improving Language Understanding by Generative Pre-Training"**, but concerns about clinical validity and ethical risks **Henderson, "Clinical Validity of Large Language Models in Mental Health Applications"** have led to structured modeling efforts. Patient-$\psi$ **Ribeiro, "Patient-$\psi$: Integrating Cognitive Modeling with Large Language Models for Simulating Therapy Clients"** integrates cognitive modeling from clinical frameworks to enhance realism, while Roleplay-doh **Liu, "Roleplay-doh: Applying Principle-Adherence Prompting to Improve Consistency in LLM-Based Patient Simulation"** applies principle-adherence prompting to improve consistency. However, these methods struggle with generating nuanced, profile-consistent responses, highlighting the need for systematic alignment strategies.

\paragraph{Preference Optimization for Alignment.}
Optimizing LLMs with human preference data has been widely studied **Christiano, "Simple Neural Learning"**, with Direct Preference Optimization (DPO) emerging as an efficient alternative to reinforcement learning **Zhang, "Direct Preference Optimization: A New Paradigm for Aligning Language Models"**. While DPO has been applied in general chatbot alignment and some scientific domains **Li, "Preference-Based Alignment of Language Models in Scientific Domains"**, its use in simulation for clinical psychology practice remains underexplored. Recent methods propose augmenting preference data through automated techniques **Kim, "Automated Preference Data Augmentation for Enhancing Human-Like Behavior in LLMs"**, which aligns with our approach of leveraging model-based augmentation to enhance preference learning for profile-guided mental health simulations.