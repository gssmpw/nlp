% \clearpage
% \newpage
\appendices

\section{Time complexity analysis}
\label{appendix:A}
We denote the cardinality of proxy column set  as $l$ and the cardinality per proxy column as $m$. During the offline stage, the complexity of pre-computing all the column embeddings is $\mathcal{O}(ml|\overline{C}||\mathcal{R}|) = \mathcal{O}(|\overline{C}||\mathcal{R}|)$, where $|\mathcal{R}|$ denotes the number of columns in the repository, and $|\overline{C}|$ denotes the average size of columns in the repository; and the time complexity of index construction using HNSW is $\mathcal{O}(|\mathcal{R}| \operatorname{log}|\mathcal{R}|)$. During the online stage, the complexity of computing the query column embedding is $\mathcal{O}(ml|C_Q|) = \mathcal{O}(|C_Q|)$, where $|C_Q|$ is the size of the query column $C_Q$; and the time complexity of ANN search is  $\mathcal{O}(\operatorname{log}|\mathcal{R}|)$. 
Consequently, the overall time complexity of online processing is $\mathcal{O}(|C_Q| + \operatorname{log}|\mathcal{R}|)$.



\section{Effectiveness of equi-join discovery}
\label{appendix:B}
To demonstrate the effectiveness of \textsf{Snoopy} in equi-join search, we compare its R@25 with Deepjoin, which can also deal with approximate equi-join discovery.
We omit the exact algorithms such as JOSIE~\cite{JOSIE} and MATE~\cite{MATE}, as their accuracy is 1. The approximate algorithm like LSH Ensemble~\cite{LSH} is also excluded, as it has been validated that Deepjoin outperforms LSH Ensemble in equi-join search~\cite{Deepjoin}. The results are shown in Table~\ref{tab:equi-join}. We can see that the accuracy of $\textsf{Scorpion}$  also outperforms Deepjoin.




\begin{table}[h]
\small
\centering
\caption{Accuracy of equi-join search. The best are in bold.}
\vspace{-3mm}
\renewcommand{\arraystretch}{1.2} % 增加行高
\setlength{\tabcolsep}{1.4mm}{
\begin{tabular}{l|cc|cc|cc} 
\specialrule{.12em}{.06em}{.06em}
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Methods}}} & \multicolumn{2}{c}{\textbf{WikiTable}} & \multicolumn{2}{c}{\textbf{Opendata}} & \multicolumn{2}{c}{\textbf{WDC Small}}  \\
\cline{2-7}
                         & \textbf{R@25}     & \textbf{N@25}               & \textbf{R@25}   & \textbf{N@25}                & \textbf{R@25}   & \textbf{N@25}                 \\ 
\hline
\multicolumn{1}{c|} {Deepjoin}  & 0.4984 &  0.6498   & 0.8168 &  0.8992    &   0.6960 &  0.9026                   \\ 
  {$\textsf{Snoopy}_\text{bs}$}          &   0.7288     &   0.8985   &   0.8380    &   0.9297               &      0.7840     &       0.9474                                       \\
  {\textsf{Snoopy}}                       &   \textbf{0.7416}      &    \textbf{0.9077}               &   \textbf{0.8576}   &    \textbf{0.9449}               &    \textbf{0.8096}    &     \textbf{0.9632}                 \\
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\label{tab:equi-join}
\vspace{-3mm}
\end{table}


\newpage
\section{Offline Efficiency Evaluation}
\label{appendix:C}
The runtime (in minutes) of the offline process is shown in Table~\ref{tab:offline_efficiency}. We can observe that $\textsf{Snoopy}$ exhibits the shortest per-epoch training time due to its lightweight AGM-based column mapping. PEXESO requires the least encoding time, as it simply invokes fastText to encode each cell within the query column.
The encoding time of Snoopy is also short
due to the lightweight AGM-based column projection.  Since all column-level methods employ HNSW for indexing, their indexing times are comparable. The indexing time of PEXESO is long 
due to the necessity of indexing the mapped vectors of all cells in hierarchical grids~\cite{Pexeso}.


\begin{table}[h]
\small
\centering
\caption{Offline processing time (min). The training is evaluated on WDC Small while encoding and indexing are on WDC Large.}
\vspace{-2mm}
\renewcommand{\arraystretch}{1.1} % 增加行高
\begin{threeparttable}
\setlength{\tabcolsep}{3.3mm}{
\begin{tabular}{c|c|c|c} 
\specialrule{.12em}{.06em}{.06em}
\textbf{Methods}  & training/epoch & encoding & indexing  \\ 
\hline
PEXESO   &      --             &   \textbf{8.75}        &       298.44    \\
WarpGate &      --             &    115.98        &    0.29        \\
BERT*    &       36.54      &    110.01        &     0.32        \\
Starmie &         41.60   &   128.54       &    0.35          \\
Deepjoin &         70.46   &   120.52        &    0.39          \\ 
\hline
\textsf{Snoopy} &   \textbf{2.03} &      14.81              &     \textbf{0.28}           \\
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\end{threeparttable}
    \vspace{-2mm}
    \label{tab:offline_efficiency}
\end{table}

