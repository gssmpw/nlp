\section{Introduction}
\label{sec:intro}

\IEEEPARstart{T}he drastic growth of open and shared datasets (e.g., government open data)
% \footnote{https://www.govdata.de/})
has brought unprecedented opportunities for data analysis. However, when confronted with massive datasets, users often struggle to find 
relevant ones for their specific requirements~\cite{WarpGate}.
Hence, the data management community has been developing dataset discovery systems to find related tables given a query table~\cite{TabelDiscovery}.
% dedicated to exploring solutions for effective and efficient dataset discovery~\cite{TabelDiscovery,paton2023dataset}.
% systems, with table search as the primary application~\cite{TabelDiscovery,paton2023dataset}.


In this work, we focus on semantic join discovery.
Given a table repository $\mathcal{T}$, a query table $T_Q$ with the specified column $C_Q$,  semantic join discovery aims to find column $C$  from  $\mathcal{T}$ that exhibits a large number of semantically matched cells between column $C$ and $C_Q$. Thus, the table with the column $C$ can be joined with the query table $T_Q$ to enrich the features for data analysis/machine learing tasks.
% One of the main benefits of semantic join discovery is dataset enrichment for enhanced data analysis~\cite{DongO22}.
% For example, as illustrated in Figure~\ref{fig:exm1}, by joining the top-1 joinable table $\text{T}_\text{A}$, the extra \texttt{Unemployment} information  can be obtained for enhanced data analysis.

% are not only in the database fields (e.g., data integration~\cite{KhatiwadaSGM22}) but also in the machine learning fields such as feature augmentation~\cite{DataAugmentation} and data enrichment~\cite{DongO22}. 
% It is beneficial to data integration~\cite{KhatiwadaSGM22}, feature augmentation~\cite{DataAugmentation}, data enrichment~\cite{DongO22}, etc.


\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{Example1.pdf} \vspace{-5mm}
 %  \caption{An illustrative example of semantically joinable table search for enhanced data analysis.
 % The data scientist \mytextcircled{1} searches the top-1 joinable table with $T_Q$ and \mytextcircled{2} performs join to merge rows in Table $T_Q$ and $T_A$ based on the matched cells, so that the \texttt{Unemployment} information can be obtained for enhanced analysis of housing sales rate in cities of $T_Q$.}
   \caption{An example of semantic join discovery. The cells in columns $C_Q$, $C_1$ and $C_2$ with the same grayscale are matched.}
  \label{fig:exm1}
 \vspace{-6mm}
\end{figure}





\begin{example}\label{example-1}
As illustrated in Fig.~\ref{fig:exm1}, a data scientist wants to analyze the housing sales rate of the cities listed in Table $T_Q$. To enhance the understanding of these cities, she/he searches for tables with high joinability to Table $T_Q$ on the \texttt{City} column ($C_Q$). 
% Since performing join could be time-consuming~\cite{autofuzzyjoin, Auto-Join}, she/he expects the discovery method to return only top-$k$ columns ranked by joinabilities. 
Since three (resp. two) cells in  $C_Q$ can find matched ones 
in $C_1$ (resp. $C_2$), the joinability of $C_Q$ and $C_1$ is $\frac{3}{3}$, while for $C_Q$ and $C_2$, it is $\frac{2}{3}$.
An ideal join discovery method returns $C_1$ as the top-1 column (assuming that only top-1 is requested). Then, the data scientist can employ any join algorithms~\cite{semanticjoin,autofuzzyjoin,Auto-Join} to merge the rows in Table $T_Q$ and $T_A$ based on the joinable columns, so that the \texttt{Unemplpyment} rate can be obtained for enhanced data analysis.
% \footnote{In this work, we just focus on the semantic join discovery process.}.
% returns Table $T_A$ to the data scientist (assuming that only top-1 is required). Subsequently,  she/he can employ any join algorithms~\cite{semanticjoin,autofuzzyjoin,Auto-Join} to \textnormal{\circled{2}} merge the rows in Table $T_Q$ and $T_A$ based on the matched cells, so that the extra attributes can be obtained for enhanced data analysis\footnote{In this paper, we  focus on \circled{1} joinable table search process.}.
% Subsequently, she/he can employ any join algorithms~\cite{semanticjoin,autofuzzyjoin} to join Table T with the returned target tables, so that she/he can get the extra information (e.g.,  ``\texttt{Unemployment}" and ``\texttt{Population}") of cities in table T.
\end{example}


% \begin{figure}
% \centering
% \subfigure[DeepJoin]{
% \begin{minipage}[t]{0.4\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{exp_fig/splashes_opendata_deepjoin.pdf}
%     \vspace{-3mm}
% \end{minipage}}
% \hspace{7mm} 
% \subfigure[\textsf{Scorpion}]{
% \begin{minipage}[t]{0.4\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{exp_fig/splashes_opendata_scorpion.pdf}
%     \vspace{-3mm}
% \end{minipage}}
% \vspace{-4mm}
% \caption{
% % t-SNE visualization of randomly selected 1K column embeddings learned by (a) Deepjoin and (b) our proposed \textsf{Scorpion} from Opendata dataset.
% t-SNE visualization of column embeddings learned by Deepjoin and our proposed \textsf{Scorpion}. Ideally, the joinable column embeddings should be close while the non-joinable ones should be far apart. However, the representation distribution of Deepjoin is too uniform, making it difficult to determine joinability through similarity comparison.
% }

% \label{fig:visual_embed}
% \vspace{-3mm}
% \end{figure}

\definecolor{DeepGreen}{RGB}{0, 88, 36}


% \begin{figure}
% \centering

% \subfigure[DeepJoin vs. ground truth]
% {
% \begin{minipage}[t]{0.45\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{exp_fig/frequency_opendata_deepjoin.pdf}
%     \label{fig:visualC}
%     \vspace{-3mm}
% \end{minipage}}
% \hspace{3mm}
% \subfigure[\textsf{Scorpion} vs. ground truth]{
% \begin{minipage}[t]{0.45\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{exp_fig/frequency_opendata_scorpion.pdf}
%     \label{fig:visualD}
%     \vspace{-3mm}
% \end{minipage}}
% \vspace{-4mm}
% \caption{
% Similarity distribution between query column embeddings and their top-25 joinable column embeddings vs. ground truth joinability distribution (\textcolor{DeepGreen}{in green}) on Opendata.
% % Illustration of the distribution of similarities of the tested query column embeddings and the top-25 joinable column embeddings vs. the ground true  joinability distribution on the Opendata dataset.
% }
% \label{fig:visual_dist}
% \vspace{-3mm}
% \end{figure}




Most existing join discovery methods~\cite{JOSIE,LSH,DatasetDiscovery,CrossDataDis} limit the join type to the equi-join, overlooking numerous semantically matched cells (e.g., ``New York" semantically matches ``Big Apple"),  thus underestimating column joinabilities. To take semantics into consideration, 
cell-level semantic join discovery methods (e.g. PEXESO~\cite{Pexeso}) determine cell matching via cell embeddings, and compute the joinability based on the cell matching results. 
They are exact solutions under the proposed semantic joinability measure~\cite{Deepjoin,Pexeso}, but require online evaluation of all cell pairs between the query column and each repository column to assess column-to-column (c2c) joinabilities, as shown in Fig.~\ref{fig:exm2}(a).
Thus, they show ideal effectiveness but poor efficiency.
% the worst time complexity is linear in the product of query column size and table repository size~\cite{Deepjoin}.
To enhance efficiency,  column-level  solutions~\cite{Deepjoin,WarpGate} propose to encode the entire column to a fixed-dimensional embedding via transformer-based pre-trained models (PTMs), as shown in Fig.~\ref{fig:exm2}(b),
and returns approximate results by performing similarity search on the column embeddings. While efficiency is improved, they suffer poor effectiveness due to the following reasons.
\begin{itemize}  
\item{} \textbf{Semantics-joinability-gap}: PTMs (e.g. SBERT adopted by DeepJoin~\cite{Deepjoin}) encode each column to an embedding independently, as shown in Fig.~\ref{fig:exm2}(b), without essential c2c interactions. This results in column embeddings inferior in capturing the cell information between columns, but focusing more on the semantic type of the input column~\cite{Watchog}. However, similarity in column semantic type does not necessarily imply high joinability. For instance, columns $C_Q$ and $C_3$ in Fig.~\ref{fig:exm1} share the same semantic type (\texttt{City}) but are not joinable, as no cells can be semantically matched.
\item{} \textbf{Size limit}: existing column-level methods concatenate cells within a column into a single sequence as input for PTMs. However, transformer-based PTMs typically impose an input size constraint, resulting in the truncation  of cells that exceed this limit, even if they contribute to the joinability. 
\item{} \textbf{Permutation sensitivity}: the embedding  obtained by these PTMs is sensitive to the permutation of tokens in the input sequence~\cite{tableembed}, however, the joinability of two columns is agnostic to the permutation of cells within each column.
\end{itemize}
% \noindent \underline{\textit{(1) Semantics-joinability-gap.}} PTM-based encoders (e.g. SBERT used by DeepJoin~\cite{Deepjoin}) are designed for capturing the semantics of the individual input sequence (column), yet they are used for assessing joinability between columns. This gap leads to less discriminative column representations in the embedding space, as shown in Figure~\ref{fig:visual_embed}~\subref{fig:visualA}, resulting 
% in an inferior fit of joinability distribution, as shown in Figure~\ref{fig:visual_dist}~\subref{fig:visualC}, and thus, hampers the effectiveness of joinable table search.


% \noindent \underline {\textit{(2) Size limit to sequences. }} Transformer-based PTMs typically impose an input size constraint (e.g. 512 tokens) due to the high complexity of the self-attention mechanism~\cite{badaro2023transformers}. However, long tables are common in real data lakes. For example, the longest table in WDC Corpus has 18,106 rows, exceeding the size limit of these PTMs for input token sequences. Either cutting off the ends of columns or sampling sub-columns to fit the size limit will degrade the effectiveness (see experiments in Sec.~\ref{sec:exp_effectiveness} ), as each cell may contribute to the joinability. 


% \noindent \underline{\textit{(3) Permutation sensitivity.}} The output of PTMs is sensitive to the permutation of tokens in the input sequence~\cite{tableembed}, however, the joinability of two columns is agnostic to the permutation of cells within each column. The permutation sensitivity of PTMs leads to inconsistent results after shuffling rows (see  experiments in Sec.~\ref{sec:exp_effectiveness}), which affects the effectiveness of joinability determination.


\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{Example2.pdf} \vspace{-5mm}
   \caption{(a) The column-to-column  joinabilities that exact solutions should assess. (b) The existing column-level solutions encode columns independently without column-to-column interactions. (c)  Our proposed proxy columns capture column-to-proxy-column joinabilities and are used to derive column embeddings. For all repository columns $C_1, \dots, C_x$, c2pc values can be pre-computed offline. During the online stage,  only the computation $C_Q - P_i$ is needed to generate the embedding for the query column $C_Q$.}
   \vspace{-3mm}
   \label{fig:exm2}
\end{figure}

To address these issues, 
we introduce a novel concept termed \textbf{proxy columns}, which serve as representative columns in the column space.
Then, we define a  column projection function to capture the column-to-proxy-column (c2pc) joinabilities. 
The column embedding is derived as the concatenation of the column projection values guided by different proxy columns.
Introducing proxy-column-based column embedding offers several advantages:  (i) we can model c2pc joinabilities (cf. Fig.~\ref{fig:exm2}(c)) to represent the more general c2c  joinabilities, which can better estimate the joinabilities than the existing column-level methods; bridging the semantics-joinability-gap.  (ii) the derived column embeddings can overcome size limit and permutation sensitivity inherent in PTM-based encoders, as long as we guarantee the column projection function is size-unlimited and permutation-invariant; and  (iii) unlike c2c  joinabilities, which need online computations, c2pc joinabilities for all the repository columns can be pre-computed offline, making the online search highly efficient.
Under this main idea, two challenges arise.

% \textcolor{purple}{In this paper, we propose a novel column-level method that learns high-quality column representations using a set of representative columns, which we call the pivot columns, and expressing the column embeddings as the mapping results via these pivot columns.
% % we present \textsf{Scorpion}, a novel \textit{column-level} joinable table search framework. In contrast to utilizing the existing PTMs as column encoder~\cite{Deepjoin,WarpGate},
% % which serves as a proxy to map each column in the table repository to a low-dimensional embedding.
% % \textsf{Scorpion} first transforms each column $C$ to a column matrix by independently encoding each cell into an embedding vector, then maps the column matrix to a fixed-dimensional embedding vector $f(C)$ via pivot columns.
% % Instead of utilizing PTMs as column encoders~\cite{Deepjoin,WarpGate}, we propose a novel concept of \textbf{pivot column}, which serves as a proxy to map each column $C$ in the table repository to a low-dimensional embedding $f(C)$.
% The concept of pivot column is inspired by pivot-based techniques in metric space~\cite{metricsurvey,ZhuCGJ22,ZezulaADB06}, and introducing it to the joinable table search is of great benefits:  i) \textit{effectiveness}: through pivot column mapping, column representations well model the implicit relationships between specific columns and each pivot column, and overcomes the shortcomings of size limitation and permutation sensitivity; and ii) 
% % \textcolor{purple}{in terms of efficiency, lightweight pivot column mapping surpasses PTMs in the online computation of query column embeddings, thereby enhancing the overall efficiency of existing column-level methods.}
% \textit{efficiency} : the pivot column mapping is more lightweight than the PTM-based column encoder, enhancing the online encoding time of the query column and amplifying the efficiency superiority of existing column-level methods.
% % column embeddings can be pre-computed via pivot column mapping, and thus, during the online stage, only the query embedding is required to be computed, followed by efficient identification of its $k$ nearest neighbors, thus maintaining and amplifying the efficiency superiority of column-level methods. 
% Under this idea, two challenges arise.}

\textbf{Challenge I: }\textit{How to design the column projection function to well model the c2pc joinabilities?}
% The column representation for joinable table search is expected to overcome the SL and OS problems mentioned before.
The core of proxy-column-based column representation lies in how to design the column projection function.
A straightforward approach is to model c2pc joinabilities just like the c2c  joinabilities, i.e., defining the joinability score as the column projection value.
However, the joinability scores are typically low due to the strict join definition~\cite{Pexeso,Deepjoin}. Thus, the column embeddings which obtained by concatenating the projection values would be sparse and similar to each other, limiting their ability to capture different inter-column  joinabilities and impeding the effectiveness of subsequent similarity-based search.
% To address this challenge, we first formalize and analyze the desirable properties of column representations for joinable table search. Then, we design a approximate-graph-matching-based mapping function, which well models the correlations between the specified column and each pivot column, and enables size unlimited and  permutation invariant column representations.


\textbf{Challenge II: }\textit{How to obtain good proxy columns for similarity-based join search?} 
Since proxy columns directly affect the final column embeddings, it is crucial to find ``good" proxy columns that can map the joinable columns closely while effectively pushing non-joinable columns far apart in the column embedding space, facilitating the join discovery through similarity search of column embeddings.
% Although some pivot selection techniques in metric spaces~\cite{ZhuCGJ22,pca,fft} can be easily extended to proxy column selection, these methods are not designed for that purpose and result in sub-optimal results.
In addition, good proxy columns should be aware of the   rank differences among joinable columns. For example, in Fig.~\ref{fig:exm1}, compared to the column $C_2$, good proxy columns should map the column $C_1$ closer to the query column $C_Q$.
% To solve this challenge, we provide a novel perspective that treats the pivot columns as learnable parameters and introduce a pivot column contrastive learning paradigm. We further design a rank-aware optimization to make pivot columns cognizant of the ranks of different joinable columns. Additionally, we design two different training data generation methods to enable self-supervised training.
% In this way, \textsf{Scorpion} is thoroughly self-supervised, getting rid of any human labeling or expensive data labeling through exact algorithm~\cite{Deepjoin}.

% Instead of using the traditional pivot selection methods, we provide a novel perspective that treats the pivot columns as learnable parameters.
% We design a pivot column learning paradigm via contrastive loss, which enables \textsf{Scorpion} to learn the optimal pivot columns which is able to map joinable columns in close proximity, while effectively positioning non-joinable columns at a greater distance. However, traditional contrastive learning~\cite{Moco} only includes one positive during training. Thus, it cannot distinguish the ranking of different joinable columns. For example, in Figure~\ref{fig:exm1}, three tuples in Table A can be joined with Table T, while only two in Table B can be joined with Table T. Hence, the rank of Table A should be higher than Table B. To this end, we present a ranking-aware contrastive objective to take a list of joinable columns into considerations to further improve the effectiveness of top-$k$ joinable table search. Additionally, we design two different training data preparation methods, i.e. text-level column synthesis and embedding-level column synthesis. In this way, \textsf{Pivo} is thoroughly self-supervised, getting rid of any human labeling or expensive data labeling through exact algorithm~\cite{Deepjoin}.


% To surmount the above challenges, we first formalize and analyze desired properties of column representations for joinable table search. Then, we design a neat but effective column representation method powered by pivot columns, which is able to capture the join ability between column pairs. After that, we provide a novel perspective that treats the pivot columns as learnable parameters and introduce a pivot column learning paradigm via contrastive loss.  We also dsign a rank-aware optimization to improve the effectiveness of top-kk search. Additionally, we design two different training data generation methods, i.e. text-level column synthesis and embedding-level column synthesis for pivot column learning. In this way, \textsf{Scorpion} is thoroughly self-supervised, getting rid of any human labeling or expensive data labeling through exact algorithm~\cite{Deepjoin}.

To surmount these challenges, we present \textsf{Snoopy}, an effective and efficient column-level \underline{s}ema\underline{n}tic  j\underline{o}in  disc\underline{o}very framework via \underline{p}rox\underline{y} columns. Firstly, we formalize and analyze the desirable properties of column representations for column-level semantic join discovery. Then, we propose to construct a bipartite graph using the specific column and proxy column, and devise an approximate-graph-matching (AGM)-based column projection function to model the c2pc joinabilities. Since it is non-trivial to select a real textual column as a representative proxy column from the huge textual column space, we directly learn its matrix form, i.e., proxy column matrix. 
We regard proxy column matrices as learnable parameters, and present a rank-aware contrastive learning paradigm to learn the proxy column matrices which can derive column representations whose similarities imply joinabilities.  Additionally, we design two different training data generation methods to enable self-supervised training.


Our key contributions can be summarized as follows:
\begin{itemize}  
\vspace{-1mm}
\item{} \textit{Proxy-column-powered framework.} We propose a novel concept termed proxy columns. Based on this, we present a column-level semantic join discovery framework, \textsf{Snoopy}, which bridges effectiveness and efficiency.

\item{} \textit{Lightweight and effective column projection.}
% We formalize and analyze the desirable properties of column representations for joinable table search. Guided by that,
We present a lightweight column projection function based on approximate graph matching, which effectively models the c2pc joinabilities and ensures the size-unlimited and permutation-invariant column representations.

% graph matching-based column representation using pivot columns, which overcomes the shortcomings of PTM-based column encoders and well-capture column joinabilities.

\item \textit{Rank-aware learning paradigm.} We introduce a novel perspective that regards proxy column matrices as learnable parameters, and present a rank-aware contrastive learning paradigm to obtain good proxy columns.
% , which can derive column representations whose similarities imply joinabilities.
Additionally, we design two training data generation strategies for self-supervised training.

\item{} \textit{Extensive experiments.} Extensive experiments on real-world table repositories demonstrate
that \textsf{Snoopy} achieves both high effectiveness and efficiency.
% Experimental results demonstrate that (1) \textsf{Scorpion} outperforms SOTA column-level methods by 16.1\% in Recall@25 and 10.1\% in NDCG@25, and (2) the online processing speed of \textsf{Scorpion} is at least 5 orders of magnitude faster than cell-level methods, and 3.5x faster than existing column-level methods on average.

\end{itemize}

The rest of this paper is organized as follows.
Section~\ref{sec:pre} presents preliminaries. Section~\ref{sec:pivot} introduces proxy columns.  Section~\ref{sec:Snoopy} proposes the framework $\textsf{Snoopy}$. Section~\ref{sec:exp} reports experimental results. 
% Section~\ref{sec:case} presents a case study. 
Section~\ref{sec:relatedwork} reviews the related work. Finally, Section~\ref{sec:conlusion} concludes the paper.

% review related work in Section 2, and provide the problem statement in Section 3.  Section 4 details the proposed
% synthesis framework. The experimental results are reported in
% Section 5. Finally, we conclude the paper, and offer directions for future work in Section 6.