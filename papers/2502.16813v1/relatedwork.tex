\section{Related Work}
\label{sec:relatedwork}
\subsection{Dataset Discovery} 
Dataset discovery has been widely studied in the data management community~\cite{paton2023dataset, TabelDiscovery}, with table search  as the primary application. 
% A prevalent line  is query-driven discovery that aims to search for the tables from data lakes or large open data repositories, according to a user's query.  
% Earlier studies~\cite{AdelfioS13,BrickleyBN19} aim to find web tables related to the given keywords with the help of metadata. 
The main sub-tasks of table search are joinable table search and table union search.
% When given a query table for search, the main sub-tasks are joinable table search and table union search.

\textbf{Joinable table search.}
To support joinable table search, most studies~\cite{Aurum, DatasetDiscovery,LSH,JOSIE,CrossDataDis,Correlation,MATE} focus on equi-join and utilize syntactic similarity measures to determine joinanility between columns. 
% Aurum~\cite{Aurum} and D3L~\cite{DatasetDiscovery} use Jaccard distance, while LSH Ensemble~\cite{LSH} and Josie~\cite{JOSIE} adopt Jaccard set containment to alleviate the bias to shorter columns. Some recent studies extend the equi-join to correlated table discovery~\cite{Correlation} or n-ary joins~\cite{MATE}.
% These methods do not consider semantics of columns.
To take semantics into consideration, PEXESO~\cite{Pexeso} proposes a semantic joinability measure, and designs a cell-level exact algorithm under this measure using word embeddings. To enhance efficiency, the following column-level solutions, such as DeepJoin~\cite{Deepjoin} and WarpGate~\cite{WarpGate}, perform coarser computation at column-level to approximate the results of cell-level solutions. However, the effectiveness is poor due to the suboptimal column embeddings. In contrast, our $\textsf{Snoopy}$ is an effective column-level framework powered by the proxy-column-based column embeddings.
Recent works~\cite{koios,SilkMoth} on semantic overlap set search are related to join discovery, but adopt a different semantic overlap (join) measure from the previous studies~\cite{Deepjoin,Pexeso} and ours. 
OmniMatch~\cite{omnimatch}, a concurrent work with ours, detects both equi-joins and fuzzy-joins by combining multiple similarity measures. However, it views join discovery as an offline procedure~\cite{omnimatch}, unlike online procedures where high efficiency is a crucial demand.


\textbf{Table union search.} The goal of table union search is to find tables that can be unioned with the query table to extend it with tuples. TUS~\cite{TUS} defines table union search based on attribute unionability, and formalizes three probabilistic models to describe how unionable attributes are
generated from different domains.
D3L~\cite{DatasetDiscovery} further adds in measures that include formatting similarity and attribute names. SANTOS~\cite{santos}
% and Starmie~\cite{starmine} consider the relationships between columns and achieve better accuracy.
considers the relationships between columns and uses a knowledge base to identify  unionable tables. Starmie~\cite{starmine} extends the notion of capturing binary relationships to use the context of the table to determine union-ability. 
 
 





% \noindent\textbf{Joinable table search.} 
% % \subsection{Joinable Table Search}
% To find joinable tables from large table repositories or data lakes, most studies~\cite{Aurum, DatasetDiscovery,LSH,JOSIE,CrossDataDis,Correlation,MATE,CrossDataDis} focus on equi-join and utilize syntactic similarity metrics to measure joinability between columns. Aurum~\cite{Aurum} and D3L~\cite{DatasetDiscovery} use Jaccard distance as the similarity measure. Since Jaccard similarity has a bias to shorter columns, solutions based on Jaccard set containment~\cite{LSH,JOSIE,CrossDataDis} have been proposed.
% % , which are more robust under different cardinalities.
% Some recent studies extend the requirement of column joinability, such as the Join-Correlation Search~\cite{Correlation} which aims to find joinable and correlated tables, and MATE~\cite{MATE} which aims to find n-ary joinable tables.
% To take semantics into consideration, PEXESO~\cite{Pexeso}, the first semantically joinable table search solution, uses word embeddings to capture the semantics of cell values as multi-dimensional vectors, and finds tables that can be fuzzy joined using similarity predicates.




% All the studies above, however, exclude the semantic-joinable tables, and would miss substantial join opportunities. To take semantics into consideration, PEXESO~\cite{Pexeso}, the first semantically joinable table search solution, uses word embeddings to capture the semantics of cell values as multi-dimensional vectors, and finds tables that can be fuzzy joined using similarity predicates. Although the pivot-based index and filtering techniques are adopted to accelerate the search procedure, the efficiency is still limited due to the complex cell-level similarity computations. By contrast, the column-level solutions perform coarser computation at the column level, improving efficiency by transforming the entire column to a fixed-dimensional embedding.
% To the best of our knowledge, DeepJoin~\cite{Deepjoin} and WarpGate~\cite{WarpGate} are the only existing column-level methods.
% DeepJoin~\cite{Deepjoin} encodes each column via the pre-trained language models. WarpGate~\cite{WarpGate} suggests using the pre-trained table embedding models~\cite{tableembed} to encode columns. 
% The main shortcoming of existing column-level methods is the sub-optimal column representation derived by transformer-based column encoders. 
% Our work falls into the column-level category, but differs
% from existing methods mainly in the column representation, i.e., we design a novel column representation for effective and efficient joinable table search via the the proposed pivot columns.

% \vspace{1mm}
\subsection{Table Representation}
% \subsection{Table Representation}
Many researchers are exploring how to represent tabular data (i.e., structured data) with neural models~\cite{tableembed, badaro2023transformers}. Due to the huge success in natural language processing (NLP), pre-trained language models (e.g., BERT~\cite{bert}, SBERT~\cite{sentencebert}, E5-base~\cite{E5-Base-4k}, etc.) have been widely applied to represent different levels of tabular data, including entity matching (row-level)~\cite{camper,ditto}, column type annotation (column-level)~\cite{doduo,Watchog}, etc. To model the row-and-column structure as well as integrate the heterogeneous information from different components of tables, transformer-based table embedding models have been proposed, such as TURL~\cite{turl}, TaBERT~\cite{tabert}, TAPAS~\cite{tapas}, etc. These models are based on Transformer architecture, and thus, enforce a length limit to token sequences (e.g. 512) due to the high computational complexity of the self-attention mechanism~\cite{attention}. In contrast, our proposed column representation is size unlimited, which can well handle the long columns in the real table repositories.

% \textbf{Pivot-based techniques.} The pivot (proxy) concept has been widely used in diverse fields, such as representation learning~\cite{KimKCK20}, data management in metric spaces~\cite{metricsurvey}, etc. In the field  of representation learning, proxies represent the classes of data points and are used to better represent the objects in continuous spaces ~\cite{LiangZWA21}, improve the model generalization~\cite{yao2022pcl}, etc. In the field  of metric spaces, to reduce the number of distance computations and accelerate the similarity search, pivot-based filtering and indexing techniques~\cite{ChavezNBM01,metricsurvey} have been proposed to pre-compute and store distances from every object in the metric space to a set of so-called pivots, and then utilize these distances to prune unqualified objects during search. 
% To improve the effectiveness of pivot-based techiniques in metric space,
% various pivot selection methods~\cite{fft,pca,ZhuCGJ22} have been proposed.
% % For example, Farthest First Traversal (FFT)~\cite{fft} iteratively identifies a new pivot, which is the farthest from the current pivot set, and utilizes it to expand the existing pivot set. PCA for Pivot Selection (PCA)~\cite{pca} performs dimensionality reduction to select high-quality pivots, based on the FFT.
% For more information on pivot selection, please refer to the survey~\cite{ZhuCGJ22}. Although these pivot selection methods can be easily extended to select pivot columns in our problem, we provide a novel perspective to regard pivot columns as learnable parameters. We show that our learning-based method can achieve better performance than the traditional pivot selection strategies via experimental validation in Section~\ref{sec:exp_ablation}. 

\subsection{Contrastive Learning}
Contrastive learning~\cite{Moco} (CL) is a discriminative approach that aims to pull similar samples closer and push apart dissimilar ones in the embedding space, and has achieved huge success in diverse domains.
In data discovery and preparation, CL is an effective method for learning high-quality data representations. Pylon~\cite{Pylon} and Starmie~\cite{starmine} leverage CL to learn column representations for table discovery. Ember~\cite{Ember}  enables a general keyless join operator by constructing an index populated with task-specific embeddings via CL. Sudowoodo~\cite{Sudowoodo} applies CL to learn entity, column, and cell representations to address multiple tasks in data preparation. Instead of directly learning the data items, in this paper, we leverage CL to learn proxy column matrices, which are then used to derive column representations efficiently. In contrast to the standard CL that requires a strict binary separation of the training pairs into similar and dissimilar samples, RINCE~\cite{2022ranking} first proposes a new mechanism that preserves a ranked ordering of positive samples. Inspired by that, we incorporate rank awareness into the pivot column matrix learning process, focusing on designing a data generation method to synthesize ranked joinable columns to enable rank-aware CL.