\section{Experiments}
\label{sec:exp}

In this section, we conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed \textsf{Snoopy}.

\subsection{Experiment setup}
\subsubsection{Datasets}

We use three real-world table repositories to evaluate the effectiveness of \textsf{Snoopy} and baseline methods. 
% Since there are no open-sourced benchmark datasets for semantic join discovery, we follow the previous studies~\cite{Deepjoin, starmine, Pexeso} to construct datasets from real table repositories. Table~\ref{tab:dataset} lists the corresponding statistics.
% Since effectiveness evaluation requires ground truth, we follow previous work~\cite{Deepjoin} to label the datasets by running the cell-level method PEXESO~\cite{Pexeso} with ideal effectiveness. However, running PEXESO is time and memory-consuming (cf. Table~\ref{tab:online_efficiency}), and thus, we follow~\cite{starmine} to construct sampled datasets for effectiveness evaluation.
Wikitable is a dataset of relational tables from Wikipedia~\cite{Wikidataset}. Opendata is a data lake table repository from Canadian and UK open datasets~\cite{LSH,santos}. WDC Small is a sample dataset with long tables from the WDC Web Table Corpus\footnote{http://webdatacommons.org/webtables/2015/downloadInstructions.html}. For each dataset $\mathcal{T}$, we remove whitespace and only selected columns with a length greater than 5 and not numeric to form column repository $\mathcal{R}$. Since not all datasets contain metadata, for fairness, we only use the cells within each column. To generate queries and avoid data leaks, we randomly sample 50 columns for WikiTable and WDC Small, and 100 columns for Opendata from the original corpus except those in $\mathcal{T}$, following previous studies~\cite{Deepjoin, starmine}. 
For efficiency evaluation, we use a large dataset, WDC Large, which consists of 1 million columns extracted from 186,744 tables in the WDC Web Table Corpus.




\begin{table}[t] \small
\centering
\caption{Statistics of datasets. ``size" denotes \# of cells per column.}
\renewcommand{\arraystretch}{1.15} % 增加行高
\vspace{-2mm}
\label{tab:dataset}
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{l|ccccl} 
\specialrule{.12em}{.06em}{.06em}
Dataset   & $|\mathcal{T}|$  & $|\mathcal{R}|$ & Min. size & Max. size & Avg. size  \\ 
\hline
WikiTable &  32,614       &   228,299     &   5       &    999      &   29.41        \\
Opendata  &  2,310       &   13,918     &   5       &    1,417     &   151.74        \\
WDC Small &   17,763      &   97,703     &    27      &     810     &    102.44      \\ 
\hline
WDC Large &    186,744     &   1,000,000      &     1     &     1,974     &      115.90     \\

\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\vspace{-3mm}
\end{table}



% \begin{table*}[t]
% \small
% \centering
% \caption{Performance comparison of different methods on three datasets. The best results are in bold and second best underlined. R@k and N@k refer to Recall@k and NDCG@k, respectively.}
% \vspace{-2mm}
% \label{tab:effectiveness}
% \renewcommand{\arraystretch}{1.3} % 增加行高
% \setlength{\tabcolsep}{0.55mm}{
% \begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}  
% % \toprule[1pt]
% \specialrule{.12em}{.06em}{.06em}
% \multirow{2}{*}{\textbf{Methods}}  & \multicolumn{6}{c|}{\textbf{Wikitable}}         &\multicolumn{6}{c|}{\textbf{Opendata}}         & \multicolumn{6}{c}{\textbf{WDC Small}}           \\ 
% \cline{2-19}
%  & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} \\ 
% \hline
% WarpGate      &  0.3720   &   0.5240   &  0.6072    &   0.8265  &  0.7806    &  0.7548   & 0.3440  &  0.6560  &   0.7416   &  0.9069   &  0.8824    &   0.8427   &  0.5360   &   0.6427   &   0.7032   &  0.9360   &    0.9319  &   0.9148    \\
% BERT   &  0.3720   &   0.5067   &   0.5896   &   0.7851  &  0.7806    &   0.7548  & 0.3100    &   0.6000   &   0.7328   &  0.8780   &   0.8577   &  0.8397    &  \underline{0.6000}   &    0.6213  &   0.6624   &  0.9092   &   0.8847   &   0.8717    \\
% $\text{BERT}^* $  &  0.3920   &   0.5267   &   0.5840   &  0.8038   &   0.7912   &  0.7591  &  0.3500   &   0.6580   &   0.7688   &  0.8955   &  0.8853    &   0.8653    & \textbf{0.6160}   &   0.6400   &    0.6832  & 0.9298    &   0.9117   &    0.8973   \\
% SBERT   &  0.3480   &   0.4147   &   0.4800   &  0.7226   &  0.6705    &   0.6446 & 0.3120 & 0.5593  & 0.6728  & 0.8511  &   0.8176  &  0.7892    &   0.5480  &   0.5400   &   0.5544   & 0.8906    &  0.8405    &    0.8032   \\
% $\text{DeepJoin}^\text{--}$       &   0.3680  &   0.5253   &  0.5664    &   0.7913  &   0.7575   &   0.7265  &   0.3480  &  0.6507    &  0.7732    &   0.8965  &   0.8845   &  0.8697    &  0.5920   & 0.6347     &   0.7040   &   0.9243  &    0.9157  &    0.9095   \\
% $\text{DeepJoin}$     &   0.3520  &  0.4827    &   0.5600   &   0.7347  &  0.7099    &    0.6984  &  \textbf{0.4340}   &   0.6967  &    0.8188  &  0.9051   &   0.9017   &  0.9040     &  0.5960   &   0.6333   &   0.7184   &    0.9190 &   0.9158   &    0.9112   \\
% % TURL      & 0.3440  &  0.6560  &   0.7416   &  0.9069   &  0.8824    &   0.8427      &  0.3720   &   0.5240   &  0.6072    &   0.8265  &  0.7806    &  0.7548    &  0.5360   &   0.6427   &   0.7032   &  0.9360   &    0.9319  &   0.9148    \\
% % TURL*      &     &      &      &     &      &      &     &      &      &     &      &      &     &      &      &     &      &       \\


% \specialrule{.12em}{.06em}{.06em}
% $\textsf{\textbf{Snoopy}}_\text{bs}$      &  \underline{0.4600}   &    \underline{0.6587}  &   \underline{0.7360}   &   \underline{0.9103}  &   \underline{0.9231}   &  \underline{0.8945}   &   0.3860    &  \underline{0.7113}    &   \underline{0.8552}   &  \underline{0.9259}   &  \underline{0.9380}    &  \underline{0.9370}     &  0.5920   &   \underline{0.6813}   &   \underline{0.7960}   &  \underline{0.9503}   &   \underline{0.9500}   &    \underline{0.9472}   \\
% \textsf{\textbf{Snoopy}}   & \textbf{0.5000}  &   \textbf{0.6600}   &  \textbf{0.7728}   &  \textbf{0.9187}   &   \textbf{0.9243}   &   \textbf{0.9122}  & \underline{0.3920}    &  \textbf{0.7180}    &  \textbf{0.8716}    &  \textbf{0.9300}   &   \textbf{0.9440}   &   \textbf{0.9458}   &   0.5760   &   \textbf{0.6827}   &  \textbf{0.8230}    &  \textbf{0.9621}   &  \textbf{0.9600}    &    \textbf{0.9642}   \\

% % $\textsf{Snoopy}_\text{ran}$    &  0.3260   &   0.6640   & 0.7592     &  0.9036   &   0.8973   &   0.8678   &   0.3720  &   0.5787   &    0.6184  &   0.8345  &    0.8129  &   0.7747   &    0.5200 &    0.5640  &  0.6080    &   0.9086  &   0.8737   &    0.8432   \\
% % $\textsf{Snoopy}_\text{fft}$       &   0.3800  &   0.6427   &   0.7188   &   0.9106  &   0.8823   &  0.8377    &   0.3360  &    0.5467  &   0.6108   &   0.8115  &   0.7931   &   0.7677   &   0.5000  &   0.6000   &    0.6344  &   0.9239  &  0.8957    &   0.8652    \\
% % $\textsf{Snoopy}_\text{pca}$       &  0.3600   &     0.6373 &    0.7008  &   0.8971  &    0.8714  &   0.8267   &   0.3720  &  0.5573    &   0.6224   &    0.8334  &   0.8031   &  0.7759    &   0.5004  &   0.5907   &    0.6064  &   0.9046  &     0.8877  &  0.8499     \\

% % \bottomrule 
% \specialrule{.12em}{.06em}{.06em}
% \end{tabular}}
% \vspace{-2mm}
% \end{table*}




\begin{table*}[t]
\footnotesize
\centering
\caption{Performance comparison of different methods on three datasets. The best results are in bold and the second best underlined. R@k and N@k refer to Recall@k and NDCG@k, respectively.}
\vspace{-2mm}
\label{tab:effectiveness}
\renewcommand{\arraystretch}{1.3} % 增加行高
\setlength{\tabcolsep}{0.75mm}{
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}  
% \toprule[1pt]
\specialrule{.12em}{.06em}{.06em}
\multirow{2}{*}{\textbf{Methods}}  & \multicolumn{6}{c|}{\textbf{Wikitable}}         &\multicolumn{6}{c|}{\textbf{Opendata}}         & \multicolumn{6}{c}{\textbf{WDC Small}}           \\ 
\cline{2-19}
 & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} \\ 
\hline
WarpGate      &  0.3720   &   0.5240   &  0.6072    &   0.8265  &  0.7806    &  0.7548   & 0.3440  &  0.6560  &   0.7416   &  0.9069   &  0.8824    &   0.8427   &  0.5240   &   0.6360   &   0.7040   &  0.9364   &    0.9307  &   0.9154    \\
BERT   &  0.3720   &   0.5067   &   0.5896   &   0.7851  &  0.7806    &   0.7548  & 0.3100    &   0.6000   &   0.7328   &  0.8780   &   0.8577   &  0.8397    &  0.5919   &    0.6107  &   0.6592   &  0.9065   &   0.8821   &   0.8709    \\
$\text{BERT}^* $  &  0.3920   &   0.5267   &   0.5840   &  0.8038   &   0.7912   &  0.7591  &  0.3500   &   0.6580   &   0.7688   &  0.8955   &  0.8853    &   0.8653    &  0.6080   &   0.6373   &    0.7168  & 0.9296    &   0.9236   &    0.9124   \\
SBERT   &  0.3480   &   0.4147   &   0.4800   &  0.7226   &  0.6705    &   0.6446 & 0.3120 & 0.5593  & 0.6728  & 0.8511  &   0.8176  &  0.7892    &   0.5560  &   0.5507   &   0.5816   & 0.9007    &  0.8547    &    0.8229   \\
$\text{SBERT}_\text{ck}$  &   0.3080 &	0.4133	& 0.4736   & 0.6796 &	0.6503	& 0.6316 &  0.3440	& 0.5927	& 0.7220 & 0.8620 &	0.8430 &	0.8265 &  0.5360	 & 0.5147 &	0.5456 & 0.8856 & 0.8320 & 0.7993  \\
$\text{Starmie}$       &   0.3760  &   0.4947   &  0.5496    &   0.7257  &   0.7070   &   0.6820  &    0.4240 &  0.6680    &  0.7936    &   0.9023  &   0.8872   &  0.8859    &  \underline{0.6120}   & 0.6267     &   0.7024   &   0.9159  &    0.9094  &    0.9007   \\
$\text{DeepJoin}$     &   0.3520  &  0.4827    &   0.5600   &   0.7347  &  0.7099    &    0.6984  &  \underline{0.4340}   &   0.6967  &    0.8188  &  0.9051   &   0.9017   &  0.9040     &  \textbf{0.6200}   &   0.6507   &   0.7240   &    0.9238 &   0.9227   &    0.9170   \\
$\text{DeepJoin}_\text{ck}$ & 0.3720 &	0.4827&	0.5384	&0.7312&	0.7039&	0.6790& \textbf{0.4400}&	0.6940&	0.8264 &\underline{0.9282}	&0.9095	&0.9117&0.6040&	0.6400&	0.7320&	0.9406	& 0.9273&	0.9242 \\
\hline
CellSamp&0.4360 & 0.6027 & 0.7264 & 0.8935	& 0.8960 & 0.8938 & 0.3620 & 0.6687& 0.8480 & 0.9157	& 0.9211 & 0.9281  & 0.3320 &	0.4813 &	0.5864 &	0.8314 &	0.8351	 & 0.8228 \\
% TURL      & 0.3440  &  0.6560  &   0.7416   &  0.9069   &  0.8824    &   0.8427      &  0.3720   &   0.5240   &  0.6072    &   0.8265  &  0.7806    &  0.7548    &  0.5360   &   0.6427   &   0.7032   &  0.9360   &    0.9319  &   0.9148    \\
% TURL*      &     &      &      &     &      &      &     &      &      &     &      &      &     &      &      &     &      &       \\


\specialrule{.12em}{.06em}{.06em}
$\textsf{\textbf{Snoopy}}_\text{bs}$      &  \underline{0.4600}   &    \underline{0.6587}  &   \underline{0.7360}   &   \underline{0.9103}  &   \underline{0.9231}   &  \underline{0.8945}   &   0.3860    &  \underline{0.7113}    &   \underline{0.8552}   &   0.9259  &  \underline{0.9380}    &  \underline{0.9370}     &  0.5920  &   \underline{0.6813}   &   \underline{0.7960}   &  \underline{0.9503}   &   \underline{0.9500}   &    \underline{0.9472}   \\
\textsf{\textbf{Snoopy}}   & \textbf{0.5000}  &   \textbf{0.6600}   &  \textbf{0.7728}   &  \textbf{0.9187}   &   \textbf{0.9243}   &   \textbf{0.9122}  &  0.3920     &  \textbf{0.7180}    &  \textbf{0.8716}    &  \textbf{0.9300}   &   \textbf{0.9440}   &   \textbf{0.9458}   &   0.5760   &   \textbf{0.6827}   &  \textbf{0.8230}    &  \textbf{0.9613}   &  \textbf{0.9600}    &    \textbf{0.9632}   \\

% $\textsf{Snoopy}_\text{ran}$    &  0.3260   &   0.6640   & 0.7592     &  0.9036   &   0.8973   &   0.8678   &   0.3720  &   0.5787   &    0.6184  &   0.8345  &    0.8129  &   0.7747   &    0.5200 &    0.5640  &  0.6080    &   0.9086  &   0.8737   &    0.8432   \\
% $\textsf{Snoopy}_\text{fft}$       &   0.3800  &   0.6427   &   0.7188   &   0.9106  &   0.8823   &  0.8377    &   0.3360  &    0.5467  &   0.6108   &   0.8115  &   0.7931   &   0.7677   &   0.5000  &   0.6000   &    0.6344  &   0.9239  &  0.8957    &   0.8652    \\
% $\textsf{Snoopy}_\text{pca}$       &  0.3600   &     0.6373 &    0.7008  &   0.8971  &    0.8714  &   0.8267   &   0.3720  &  0.5573    &   0.6224   &    0.8334  &   0.8031   &  0.7759    &   0.5004  &   0.5907   &    0.6064  &   0.9046  &     0.8877  &  0.8499     \\

% \bottomrule 
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
% \vspace{-2mm}
\end{table*}



\subsubsection{Baselines} The following baselines are evaluated. 
\begin{itemize} 

\item{} \textbf{WarpGate}~\cite{WarpGate} is the latest system prototype for dataset discovery, which suggests using pre-trained table embedding models as column encoders. Since the embedding model~\cite{0002TGL21} used in the original paper is not available, we choose TURL~\cite{turl}, a well-adopted table embedding model.
\item{} \textbf{BERT}~\cite{bert} is a pre-trained language model. We use \textit{bert-base-uncased}\footnote{https://huggingface.co/bert-base-uncased} to get the embedding for each column, and use the default input length limit of 512.
\item{} $\textbf{BERT}^*$  adopts the contrastive loss to fine-tune BERT for the semantic join discovery task.
\item{} \textbf{SBERT}~\cite{sentencebert} is a specialized variant of BERT that is designed for sentence-level embeddings. We use MPNet~\cite{mpnet} as the backbone and set the input size limit as 512.
\item{}  $\textbf{SBERT}_\text{ck}$ is a variant of  SBERT , which divides the unique column values by chunks to ensure the 512-token limit, and averages chunk embeddings from SBERT to derive final column embeddings. 
\item{} \textbf{DeepJoin}~\cite{Deepjoin} is a state-of-the-art join discovery method, which fine-tunes SBERT to obtain column embeddings and samples the most frequent cells for each column to ensure the 512-token limit. We use the best-performance MPNet~\cite{mpnet} as the backbone, following~\cite{Deepjoin}.
\item{} $\textbf{DeepJoin}_\text{ck}$ is a chunking-based variant of DeepJoin. It employs the chunking strategy only during inference, as mini-batch training is not supported with chunking. 
% \item{} $\textbf{DeepJoin}^\textbf{--}$ is a variant that removes the frequency-based sampling technique of DeepJoin and directly truncates the cells exceeding the size limit.


\item{} $\textbf{Starmie}$~\cite{starmine} is a state-of-the-art dataset discovery method. It contextualizes column representations via fine-tuning RoBERTa to facilitate
unionable table search in data lakes. For fairness, in our evaluation, we employ the single-column encoder and fine-tune it for semantic join discovery.  

\item{} $\textbf{\textsf{Snoopy}}_\text{bs}$ is a base version of \textsf{Snoopy}, which adopts the traditional contrastive loss in Equation (\ref{eq:cl}) for training.
\item{} \textbf{CellSamp} is a cell-level method with sampling  by randomly selecting $n_s = 20$ cells per column to improve efficiency. It compute pairwise similarity scores of sampled cells, and the joinability score is determined as the average of the maximum similarity scores between each cell in the query column and all cells in the candidate column.
% It then computes the joinability score as the average of the maximum similarity scores between each cell in the query column and all cells in the candidate column.
 
% \item{} \textcolor{blue}{\textbf{PEXESO}~\cite{Pexeso} is a cell-level semantic-joinable table search solution. We follow~\cite{Deepjoin} to treat it as the exact algorithm (with IDEAL effectiveness) and use it to obtain ground truth. Hence, it only serves as a baseline in efficiency experiments.}
% \item{} $\textsf{Snoopy}^\text{--}$ is a base version of \textsf{Snoopy} which adopts the traditional contrastive loss in Equation (\ref{eq:cl}).
\end{itemize}

We implement baselines following their original settings and tune the parameters for the best-performing results. Since the PTM-based methods need textual sequences as input for fine-tuning, we use the proposed text-level synthesis strategy to construct the same training data for $\text{BERT}^*$, DeepJoin, Starmie, and $\textsf{Snoopy}_\text{bs}$. 
We did not observe accuracy improvements by fine-tuning TURL with our training data, as it requires some extra information such as captions and entities in the knowledge graph~\cite{turl}. Consequently, we directly utilize the pre-trained TURL to implement WarpGate.

% We also replace the pivot column learning module of \textsf{Snoopy} with the following pivot selection methods in metric space~\cite{ZhuCGJ22}.

% \begin{itemize} [leftmargin=*]
% \item{} $\textbf{\textsf{Snoopy}}_\text{ran}$ is a naive method to randomly select columns in the repository as the proxy columns.
% \item{} $\textbf{\textsf{Snoopy}}_\text{fft}$ extends the traditional FFT~\cite{fft} to iteratively identify a new pivot column that is the farthest from the current pivot column set, and utilize it to expand the existing pivot column set.
% \item{}  $\textbf{\textsf{Snoopy}}_\text{pca}$ extends the PCA-based pivot selection method~\cite{pca}, which performs dimensionality reduction to select high-quality proxy columns.
% \end{itemize}







\subsubsection{Metrics} Following Deepjoin~\cite{Deepjoin}, we adopt Recall@$k$ and NDCG@$k$ to evaluate effectiveness, where $k$ is set to 5, 15, and 25 by default. Recall@$k$ is defined as $\frac{|\hat{\mathcal{S}} \cap \mathcal{S}|}{k}$, where $\hat{\mathcal{S}}$ and  $\mathcal{S}$ denote the top-$k$ result obtained by the specific method and an exact solution, respectively.
NDCG@$k$ is defined as  $\frac{\text{DCG@}k}{\text{IDCG@}k}$,
where $\text{DCG@}k=\sum_{i=1}^k \frac{J(C_Q, \hat{C}_i)}{\operatorname{log}_2(i+1)}$ and $\text{IDCG@}k=\sum_{i=1}^k \frac{J(C_Q, {C}_i)}{\operatorname{log}_2(i+1)}$, and $\hat{C}_i$ and $C_i$ denote the columns ranked in the $i$-th position in the results obtained by the specific method and an exact solution, respectively. For efficiency, we evaluate the runtime of all the methods. The above metrics are averaged over all the queries.


\subsubsection{Implementation Details} We implement \textsf{Snoopy} in PyTorch. We set the batch size to 64, the length $\beta$ of the negative queue to 32, and the momentum coefficient $\alpha$  to 0.9999. We use Adam~\cite{adam} as the optimizer, and set the learning rate to 0.01. We set the number $l$ of proxy column matrix to 90, and the cardinality $m$ per proxy column matrix to 50 by default.  For $\textsf{RCL}$, we first generate
a list of sorted joinability scores, where each score is randomly generated from (0.6, 0.9), and then apply the embedding-level synthesis strategy to generate positive columns. The length of the positive ranking list is set to 3. For cell matching, we follow previous studies~\cite{Deepjoin,Pexeso} to use fastText~\cite{fasttext} as  cell embedding function, and normalize all the vectors to unit length. 
We use Euclidean distance as the distance function $d$, and the threshold $\tau$ of cell matching is set to 0.2 by default. All experiments were conducted on a computer with an Intel Core i9-10900K CPU, an NVIDIA GeForce RTX3090 GPU, and 128GB memory.  The programs were implemented in Python\footnote{
The source code and datasets are available at 
https://github.com/ZJU-DAILY/Snoopy.}.




\subsection{Effectiveness Evaluation}
\label{sec:exp_effectiveness}

Table~\ref{tab:effectiveness} presents the Recall@$k$ and NDCG@$k$ of $\textsf{Snoopy}$ and other baseline methods.

\noindent \textbf{$\textsf{Snoopy}_\text{bs}$ vs. competitors.}
The first observation is that the proposed $\textsf{Snoopy}_\text{bs}$ consistently outperforms other PTM-based methods across almost all evaluation metrics on the three datasets. 
Specifically, $\textsf{Snoopy}_\text{bs}$ demonstrates an average improvement of  12\% in Recall@25 and 8\% in NDCG@25, compared with the best column-level competitor. We contribute this improvement to the high-quality column embeddings derived by AGM-based column projection, which is capable of capturing the implicit relationships between column pairs. Note that, while $\textsf{Snoopy}_\text{bs}$ exhibits slightly lower Recall@5 compared to certain baselines on specific datasets, its NDCG@5 consistently outperforms them. The discrepancy stems from the fact that some joinable columns with the same joinability are ordered sequentially in the returned results. When $k$ is small, Recall@$k$ overlooks columns ranked beyond the $k$-th position, despite their equivalent joinability to those ranked at $k$. 
The second observation is that chunking does not always lead to performance improvements. Specifically, $\text{SBERT}_\text{ck}$ outperforms $\text{SBERT}$ on Opendata but performs worse on the other two datasets.  Similarly, $\text{DeepJoin}_\text{ck}$ surpasses $\text{DeepJoin}$ on Opendata and WDC Small but underperforms on WikiTable. This is because the chunking-and-averaging strategy inevitably leads to information loss due to the naive averaging operation, sometimes negating the benefits of incorporating additional cells.
The third observation is that the sampling-based cell-level method, CellSamp, performs well on the Wikitable and Opendata datasets due to its finer-grained computations compared to column-level methods.
However, it underperforms on the WDC Small dataset, where the shortest column contains 27 cells, exceeding CellSamp's sampling threshold of 20. While increasing the sampling threshold could incorporate more cells, this would substantially reduce the efficiency of the online search, as even with 20 cells, the computational cost remains orders of magnitude higher than column-level methods (see Table~\ref{tab:online_efficiency}). 
 
 

\noindent \textbf{$\textsf{Snoopy}$ vs. competitors.}
With rank-aware optimization, $\textsf{Snoopy}$ outperforms the existing SOTA column-level methods by  16\%  in Recall@25 and 10\% in NDCG@25 on average. This is because the rank-aware optimization takes a list of joinable columns into consideration, which enables $\textsf{Snoopy}$ to better distinguish the ranks of different joinable columns.
% The improvement is more pronounced as $k$ increases, as lower-ranked columns in the positive list inevitably introduce some noise to contrastive learning, especially affecting those with high ranks.
% However, as $k$ increases, the evaluation metrics extend the focus beyond high-ranking columns, leading to more apparent benefits.
 

To demonstrate the superiority of our column embeddings in bridging the semantics-joinability-gap, being size-unlimited, and permutation-invariant, we conduct in-depth analyses.
\noindent \textbf{Bridge the semantics-joinability-gap.}
First, we randomly select 1K columns from Opendata and use the trained Deepjoin and   $\textsf{Snoopy}$ to encode these columns into embeddings. We then visualize these embeddings using t-SNE, as shown in Fig.~\ref{fig:visual_embed}. The embedding distribution of Deepjoin is more uniform, indicating less evident similarity differences compared to $\textsf{Snoopy}$. This is because Deepjoin's embeddings focus more on the column semantic types rather than cell semantics, resulting in many columns having similar and indistinguishable embeddings.
We then compute the joinability of each query column with its top-25 joinable columns from Opendata, and the similarity between each query column's embedding and its top-25 joinable columns' embeddings obtained by Deepjoin and $\textsf{Snoopy}$.
Fig.~\ref{fig:visual_dist} depicts the distributions of joinability and column embedding similarity. It is observed that, the similarity distribution of Deepjoin's embeddings poorly fits the joinability distribution. In contrast, our $\textsf{Snoopy}$ demonstrates a closer alignment with the joinability distribution, effectively bridging the semantics-joinability-gap. Visualizations of other
datasets are similar and omitted.

\begin{figure}
\centering
\subfloat[DeepJoin]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=1\linewidth]{splashes_opendata_deepjoin.pdf}
    \vspace{-3mm}
\end{minipage}}
\hspace{7mm} 
\subfloat[\textsf{Snoopy}]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=1\linewidth]{splashes_opendata_snoopy.pdf}
    \vspace{-3mm}
\end{minipage}}
% \vspace{-2mm}
\caption{Visualization of column embeddings of Opendata learned by Deepjoin and our proposed \textsf{Snoopy}.
}
\label{fig:visual_embed}
\vspace{-3mm}
\end{figure}


\begin{figure}
\centering

\subfloat[DeepJoin]
{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width=1\linewidth]{frequency_opendata_deepjoin.pdf}
 
    \vspace{-3mm}
\end{minipage}}
\hspace{3mm}
\subfloat[\textsf{Snoopy}]{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width=1\linewidth]{frequency_opendata_snoopy.pdf}
 
    \vspace{-3mm}
\end{minipage}}
\caption{
Joinability distribution (in green) vs. column embedding similarity distribution obtained by Deepjoin and $\textsf{Snoopy}$.
% between query column embeddings and their top-25 joinable column embeddings vs. ground truth joinability distribution (\textcolor{DeepGreen}{in green}) on Opendata.
}
\label{fig:visual_dist}
\vspace{-4mm}
\end{figure}

% \begin{figure}
% \centering

% \subfloat[DeepJoin]
% {
% \begin{minipage}[t]{0.45\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{frequency_opendata_deepjoin.pdf}
 
%     \vspace{-3mm}
% \end{minipage}}
% \hspace{3mm}
% \subfloat[\textsf{Snoopy}]{
% \begin{minipage}[t]{0.45\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{frequency_opendata_snoopy.pdf}
 
%     \vspace{-3mm}
% \end{minipage}}
% \vspace{-2mm}
% \caption{
% Joinability distribution (in green) vs. column embedding similarity distribution obtained by Deepjoin and $\textsf{Snoopy}$.
% % between query column embeddings and their top-25 joinable column embeddings vs. ground truth joinability distribution (\textcolor{DeepGreen}{in green}) on Opendata.
% }
% \label{fig:visual_dist}
% \vspace{-4mm}
% \end{figure}





\begin{figure*}
\vspace{-3mm}
\centering
\begin{minipage}{\linewidth}
    \centering
       \includegraphics[width=0.5\textwidth]{head.pdf}\\
       \vspace{-2mm}
  \end{minipage}
   
\subfloat[Recall@25 of different column sizes]{
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=1\linewidth]{Bar_Recall_25.pdf}
    \vspace{-3mm}
\end{minipage}}
\subfloat[NDCG@25 of different column sizes]{
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=1\linewidth]{Bar_NDCG_25.pdf}
    \vspace{-3mm}
\end{minipage}
}

\vspace{-2mm}
\caption{ Effectiveness evaluation in grouping columns of different sizes on the Opendata dataset.}  
\label{fig:impact of size}
\vspace{-4mm}
\end{figure*}




\noindent \textbf{Impact of column size.}
Then, we divide query columns of Opendata into four groups based on the column size ($<$50, 50-100, 100-500, and $>$500), and show the results on each group in Fig.~\ref{fig:impact of size}. 
This experiment is conducted on the Opendata dataset due to the limited number of long columns in Wikitable and short columns in WDC Small, respectively.
As observed, when the column size grows to more than 500, $\textsf{Snoopy}$ consistently maintains high performance, while most other baselines show a decline.
Notably, $\text{SBERT}_\text{ck}$ mitigates performance degradation of long columns on Opendata dataset. However, $\text{DeepJoin}_\text{ck}$ shows lower accuracy on long columns compared with $\text{DeepJoin}$. This discrepancy may stem from the inconsistency between fine-tuning (without chunking) and inference (with chunking). Unfortunately, the chunking strategy cannot be applied during training, limiting the overall performance.
Although DeepJoin employs a frequency-based sampling strategy to mitigates the negative impact of long columns, the performance is still not as stable as $\textsf{Snoopy}$. 

% \textcolor{blue}{We also explore some alternatives to overcome the size limits. (i) We apply the long-context model E5-Base-4k~\cite{E5-Base-4k}, which supports 4k tokens, to encode columns. The results compared with the best-performing 512-token-limit models are shown in Table~\ref{tab:long_ctx}.
% % Since the sentence-transformers\footnote{https://huggingface.co/sentence-transformers} do not yet support long-context models, we test the accuracy using the pre-trained model, expecting similar trends if applied to DeepJoin. 
% As observed, the accuracy increases on Opendata but decreases on the other two datasets. This is because,  while the long-context model overcomes the size limit, the semantics-joinability gap persists. Specifically, treating the entire column as a sequential input for PTMs prioritizes column semantics over individual cell semantics. As more cells are added, non-matching cells may dominate the column's embedding, resulting in reduced accuracy. (ii) We explore TF-IDF and BM25 to sample cells within each column to ensure the 512-token limit for DeepJoin. The results compared with the original DeepJoin (frequency-based) are shown in Table~\ref{tab:tfidf}. It is observed that the impact of different strategies on accuracy is quite limited.}
 



\noindent \textbf{Impact of permutation.} Finally, we explore the impact of cell permutation on effectiveness. Specifically, we randomly permutate the order of cells within each column in each dataset 50 times, and plot the distributions of Recall@25 in Fig.~\ref{fig:impact of order}. We omit the results of \textsf{Snoopy} because 
they are theoretically unaffected by permutations.
We can observe that the results of all the PTM-based methods are affected by the order of cells. In particular, SBERT exhibits the largest spread on all the datasets, attributed to its specialized sentence-level optimization, which considers sentence structure as an ordered sequence. Furthermore, the chunking strategy cannot mitigate the sensitivity, as each chunk is still processed as a sequential input by the PTMs, resulting in chunk embeddings being influenced by the cell order within each chunk.
% DeepJoin shows relatively stronger robustness to the cell permutation than other methods as it adopts the frequency-based sampling optimization~\cite{Deepjoin}. However, once removing the sampling technique ($\text{DeepJoin}^-$), the sensitivity becomes evident.
In contrast, the  column representation of \textsf{Snoopy} is theoretically permutation-invariant, which is consistent with the definition of joinability that is agnostic to the cell orders.


Although $\textsf{Snoopy}$ primarily focuses on semantic join, 
it can also be applied to equi-join discovery by configuring the cell matching threshold $\tau = 0$ and removing data augmentation during training data generation. The effectiveness evaluation of equi-join discovery is detailed in Appendix B. 


\begin{figure}
\centering

\subfloat{
% \centering
\includegraphics[width=1\linewidth]{prek_f.xlsx22.pdf}\hfill
    % \vspace{-1mm}
}
\vspace{-4mm}
\caption{ Order sensitivity study by random permutation of cells. }
\label{fig:impact of order}
\vspace{-2mm}
\end{figure}


% \begin{table} \small
% \centering
% \caption{Accuracy of equi-join search. The best are in bold.}
% \vspace{-3mm}
% \renewcommand{\arraystretch}{1.2} % 增加行高
% \setlength{\tabcolsep}{1.4mm}{
% \begin{tabular}{l|cc|cc|cc} 
% \specialrule{.12em}{.06em}{.06em}
% \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Methods}}} & \multicolumn{2}{c}{\textbf{WikiTable}} & \multicolumn{2}{c}{\textbf{Opendata}} & \multicolumn{2}{c}{\textbf{WDC Small}}  \\
% \cline{2-7}
%                          & \textbf{R@25}     & \textbf{N@25}               & \textbf{R@25}   & \textbf{N@25}                & \textbf{R@25}   & \textbf{N@25}                 \\ 
% \hline
% \multicolumn{1}{c|} {Deepjoin}  & 0.4984 &  0.6498   & 0.8168 &  0.8992    &   0.6960 &  0.9026                   \\ 
%   {$\textsf{Snoopy}_\text{bs}$}          &   0.7288     &   0.8985   &   0.8380    &   0.9297               &      0.7840     &       0.9474                                       \\
%   {\textsf{Snoopy}}                       &   \textbf{0.7416}      &    \textbf{0.9077}               &   \textbf{0.8576}   &    \textbf{0.9449}               &    \textbf{0.8096}    &     \textbf{0.9632}                 \\
% \specialrule{.12em}{.06em}{.06em}
% \end{tabular}}
% \label{tab:equi-join}
% \vspace{-3mm}
% \end{table}







\subsection{Ablation Study}
\label{sec:exp_ablation} 

We conduct an ablation study of key components of \textsf{Snoopy}, with results shown in Table~\ref{tab:ablation}.

First, we replace the the AGM-based column projection (CP) with the widely-used pooling techniques, i.e., max-pooling  (MaxP),  min-pooling (MinP), and average-pooling (AvgP), to obtain the column embeddings. 
% \textcolor{blue}{We also explore using MPNet as the cell embedding function with mean-pooling to generate column embeddings (MPA).}
We can observe that the search accuracy dramatically drops.
% \textcolor{blue}{Specifically, it results in an average drop of 14.8\% in Recall@25 for Snoopy compared to MaxP, 15.2\% compared to MinP, and 5.3\% compared to Avg across the three datasets.}
This is because the widely used pooling methods result in much information loss when transforming the column matrices to column embeddings.  In contrast, our
AGM-based column projection well maintains the informative signals for joinability determination. 

Then, we remove the rank-aware contrastive learning (RCL) module, and extend the pivot selection methods in metric space~\cite{ZhuCGJ22} to the proxy column selection process:
% i.e., selecting columns from the table repository and transforming them into column matrices:
(i) RAN selects columns from the repository $\mathcal{R}$ randomly as proxy columns; (ii) FFT~\cite{fft}  iteratively adds the new column  which is the most different from the current selected columns to the proxy column set; and (iii) PCA~\cite{pca} performs dimensionality reduction to select representative proxy columns based on FFT mechanism.
We can observe that without RCL, the Recall@25 drops at least 19.5\% on WikiTable, 12.9\% on Opendata, and 22.9\% on WDC Small. This demonstrates the effectiveness of RCL that is able to identify good proxy columns which yields promising effectiveness.
% We can find several observations: (1) without CL, the Recall@25 drops at least 19.5\% on WikiTable, 12.9\% on Opendata, and 22.9\% on WDC Small. This demonstrates the effectiveness of CL that is able to identify good proxy columns which yields promising results.
% (2) Different traditional pivot selection methods have little impact on effectiveness, and the optimal method varies for each dataset.
% This confirms the superiority of treating proxy columns as learnable parameters.
% (3) Even with the traditional pivot selection methods, the pivot-column-based column representation yields  the  comparable NDCG@25 to the previous SOTA column-level methods (see Table~\ref{tab:effectiveness}).
% This is a promising observation since the traditional pivot selection methods do not need any model training, which demonstrates the superiority of pivot-column-based column representation.

\begin{table} \small
\centering
\caption{Ablation study on three datasets.  Bold score indicates the performance under the default setting.}
\vspace{-2mm}
\renewcommand{\arraystretch}{1.2} % 增加行高
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{l|cc|cc|cc} 
\specialrule{.12em}{.06em}{.06em}
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Methods}}} & \multicolumn{2}{c}{\textbf{WikiTable}} & \multicolumn{2}{c}{\textbf{Opendata}} & \multicolumn{2}{c}{\textbf{WDC Small}}  \\
\cline{2-7}
                         & \textbf{R@25}     & \textbf{N@25}               & \textbf{R@25}   & \textbf{N@25}                & \textbf{R@25}   & \textbf{N@25}                 \\ 
\hline
\multicolumn{1}{c|}{\textsf{Snoopy}}                & \textbf{0.7728} &  \textbf{0.9122}                   & \textbf{0.8716} &  \textbf{0.9458}                    & \textbf{0.8230} &    \textbf{0.9632}                   \\ 
\hline
  w/o CP + MaxP          &  0.6344      & 0.7791    &  0.7716     & 0.8725                 &    0.6984        & 0.9087                                            \\
  w/o CP + MinP                       &   0.6440       &  0.7784                  &     0.7268   &   0.8462                &   0.7216     &   0.9102                   \\
  w/o CP + AvgP                       &     0.7210     &   0.8780                 &   0.8300     &   0.9199                  &  0.7856      & 0.9500                     \\ 
  % \textcolor{blue}{w/o CP + MPAP}                       &     0.6827     &   0.8550                 &   0.8188     &   0.9007                  &  0.7776      & 0.9505                     \\ 
\hline
  w/o RCL + RAN                       &  0.6184        &   0.7747                 &  0.7592      &  0.8678                   &  0.6080      &    0.8432                  \\
  w/o RCL + FFT                       &   0.6168       &  0.7677                  &  0.7188      &  0.8377                   &  0.6344      &   0.8652                   \\
  w/o RCL + PCA                       &   0.6224       &  0.7759                  &   0.7008     &  0.8267                   &   0.6064     &   0.8500                   \\
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\label{tab:ablation}
\vspace{-3mm}
\end{table}






\subsection{Efficiency Evaluation}
\label{subsec:efficiency}



We report the runtime of $\textsf{Snoopy}$ and baseline methods. We omit the results of some methods because they demonstrate similar results to the specific methods already included. We also include PEXESO~\cite{Pexeso}, which is a cell-level exact solution.

% \noindent \textbf{Online process.}
We vary the number of columns in the WDC Large dataset from 100K to 1M, and report the average online processing time per query over 50 independent tests, as shown in Table~\ref{tab:online_efficiency}. 
% Specifically, we report the query column encoding time, and the total online processing time (query column encoding time + search time).
Note that, for all column-level methods, we apply HNSW~\cite{HNSW} for ANN search.
We have the following observations:
(i) PEXESO and $\textsf{Snoopy}$ demonstrate high efficiency in online encoding. However, other PTM-based methods need a relatively longer online encoding time (10x longer for BERT*,  Starmie and DeepJoin, and 20x longer for WarpGate compared with \textsf{Snoopy}). This is because these transformer-based pre-trained models have complex architectures~\cite{FCS}, while \textsf{Snoopy} has a lightweight column projection mechanism. 
(ii) All the column-level methods significantly outperform the cell-level PEXESO in total time. Furthermore, \textsf{Snoopy} is 3.5x faster than other column-level methods on average, due to its shorter online encoding time.
(iii) CellSamp improves efficiency over PEXESO, but remains orders of magnitude slower than column-level methods due to the requirement of online pairwise cell similarity computations. 
(iv) Column-level methods demonstrate stable total time, even with an increase in dataset size. This is because the time of ANN search using HNSW is relatively stable, which is consistent with the prior study~\cite{Deepjoin}.
We also report the runtime of the offline stage in Appendix C.
% \noindent \textbf{Offline process.} 
% The runtime of the offline process is shown in Table~\ref{tab:offline_efficiency}. We can observe that $\textsf{Snoopy}$ exhibits the shortest per-epoch training time due to its lightweight AGM-based column mapping. PEXESO requires the least encoding time, as it simply invokes fastText to encode each cell within the query column.
% Since all column-level methods employ HNSW~\cite{HNSW} for indexing, their indexing times are comparable. The indexing time of PEXESO is long 
% due to the necessity of indexing the mapped vectors of all cells in hierarchical grids~\cite{Pexeso}.
 


\begin{table}[t] \small
\centering
\caption{Online processing time (ms) on WDC Large. Total time comprises query column encoding time and online search time.}
\vspace{-2mm}
\label{tab:online_efficiency}
\renewcommand{\arraystretch}{1.1} % 增加行高
\begin{threeparttable}
\setlength{\tabcolsep}{0.42mm}{
\begin{tabular}{c|c|ccccc} 
\specialrule{.12em}{.06em}{.06em}
\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}query column \\ encoding  \end{tabular}} & \multicolumn{5}{c}{total online processing}                      \\ 
\cline{3-7}
                        &                                 & 100K      & 200K      & 300K  & 500K  & 1M     \\ 
\hline
PEXESO                  & 0.82                           & 311,761 & 655,011 &   \textsf{OOM}    &  \textsf{OOM}     &    \textsf{OOM}    \\
CellSamp    &  0.68         & 21,645    & 44,462 & 64,346 & 104,115 & 203,537 \\
\hline
WrapGate                & 20.56                           & 24.55     & 24.68     & 24.74 & 24.54 & 24.46  \\
BERT*                   & 8.89                            & 12.85     & 12.80     & 12.61 & 12.45 & 12.74  \\
Starmie                   & 9.22                          & 14.69     & 14.57     & 14.65 & 14.76 & 14.68     \\
DeepJoin                & 11.57                           & 15.56     & 15.54     & 15.56 & 15.54 & 15.57  \\
$\text{DeepJoin}_\text{ck}$             & 15.08     & 19.25      & 19.33      &19.22  &19.43   &19.82    \\
\hline
\textsf{Snoopy}               & 1.10                            & \textbf{5.10}      & \textbf{4.91}      & \textbf{5.00}  & \textbf{5.17}  & \textbf{4.97}   \\
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\end{threeparttable}
\begin{tablenotes}\small
    \item  $^1$``\textsf{OOM}'' indicates out of memory under 128GB memory.
\end{tablenotes}
    \vspace{-2mm}
\end{table}

\begin{table}[t]
\small
\centering
\caption{ Ground truth shifts with different cell embedding functions.} 
\vspace{-2mm}
\label{tab:gt_shifts}
\renewcommand{\arraystretch}{1.1} % 增加行高
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{c|c|c|c} 
\specialrule{.12em}{.06em}{.06em}
\textbf{Cell embed. func.} & WikiTable & Opendata & WDC Small  \\ 
\hline
Word2vec          &       0.0341 &	0.0194 &	0.0212    \\
MPNet             &       0.0279 &  0.0085        &   0.0103         \\
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\vspace{-6mm}
\end{table}


\begin{table*}[t]
\footnotesize
\centering
\vspace{-5mm}
\caption{ Performance  of \textsf{Snoopy} in the dynamic scenario. The best results are in bold.} 
\vspace{-2mm}
\label{tab:dynamic}
\renewcommand{\arraystretch}{1.3} % 增加行高
\setlength{\tabcolsep}{0.75mm}{
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}  
% \toprule[1pt]
\specialrule{.12em}{.06em}{.06em}
\multirow{2}{*}{\textbf{Repository}}  & \multicolumn{6}{c|}{\textbf{Wikitable}}         &\multicolumn{6}{c|}{\textbf{Opendata}}         & \multicolumn{6}{c}{\textbf{WDC Small}}           \\ 
\cline{2-19}
 & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} & \textbf{R@5} & \textbf{R@15} & \textbf{R@25} & \textbf{N@5} & \textbf{N@15} & \textbf{N@25} \\ 
\hline
$\mathcal{R}_0$      &  \textbf{0.5320}   &   \textbf{0.7347}   &  0.7576    &   \textbf{0.9270}  &  0.9194    &  0.8843   & \textbf{0.4860}   &  \textbf{0.8113}   &  0.7744    &  \textbf{0.9311}  & 0.9416   & 0.9338   & \textbf{0.6640} &  \textbf{0.7453}    &  0.7832    &   \textbf{0.9682}  &  0.9601     & 0.9524   \\


% $\mathcal{R}^0_1$      &  0.4880   &   0.7080   &  0.7672    &   0.9234  &  0.9233    &  0.8852   & 0.4480   &  0.7913   &  0.8056    & 0.9300   & 0.9445   &  0.9411  & 0.6480 & 0.7267     &  0.7920    &  0.9682   & 0.9634     &  0.9564  \\ 

$\mathcal{R}_1$      &  0.5160   &   0.7090   &  0.7752    &   0.9255  &  \textbf{0.9253}    &  0.8909   &  0.4460  &  0.8000   &   0.8120   &  0.9310  &  \textbf{0.9448}  & 0.9420   & 0.6560 &  0.7360    &  0.8176    &  0.9590   &  \textbf{0.9605}     & 0.9568   \\   


% $\mathcal{R}^0_2$      &  0.4600   &   0.6840   &  0.7768    &   0.9193  &  0.9243    &  0.8917   &  0.4240  &  0.7527   &  0.8452    & 0.9290   &  0.9438  & 0.9437   & 0.6000 & 0.7067 & 0.8096 & 0.9650 & 0.9660 &0.9594    \\


$\mathcal{R}_2$      &  0.4760   &   0.6813   &  0.7704    &   0.9200  &  0.9251    &  0.8841   &  0.4300  &  0.7567   &   0.8504   &  0.9348  & 0.9454   &  0.9475  & 0.6160 & 0.7053     & 0.8192     & 0.9668    &   0.9604    &  0.9618  \\

% $\mathcal{R}^0_3$   &  0.4880   &   0.6680    &  0.7528   &   0.9086   &   0.9230   &   0.9028  &  0.400  &  0.7180   &  0.8656    &  0.9300  &  0.9436  & 0.9449   &0.5920  & 0.6947     & 0.8032     &  0.9660   &  \textbf{0.9668}     &  0.9628  \\

$\mathcal{R}_3$   &  0.5000   &   0.6600    &  \textbf{0.7728}   &   0.9187   &   0.9243   &   \textbf{0.9122}   & 0.3920     &   0.7180    &  \textbf{0.8716}    &   0.9300    &   0.9440    &   \textbf{0.9458}   &   0.5760   &   0.6827   &  \textbf{0.8230}    &  0.9613   &  0.9600    &    \textbf{0.9632}   \\


% \bottomrule 
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\vspace{-6mm}
\end{table*}





\subsection{Parameter Sensitivity}


% \begin{figure*}[t]
% \centering
% \subfigure[Wikitable]{
%     \includegraphics[width=0.22\linewidth]{cellsamp-wikitable.pdf}
% }\hfill
% \subfigure[Opendata]{
% \includegraphics[width=0.22\linewidth]{cellsamp-opendata.pdf}
% }\hfill
% \subfigure[WDC Small]{
%     \includegraphics[width=0.22\linewidth]{cellsamp-wdcsmall.pdf}
% }\hfill
% \subfigure[WDC Large]{
%     \includegraphics[width=0.22\linewidth]{cellsamp-time-wdclarge.pdf}
% }
% \vspace{-2mm}
% \caption{Recall@25 and online search time of CellSamp varying $n_s$ compared with our \textsf{Snoopy}.}
% \label{fig:sense}
% \vspace{-6mm}
% \end{figure*}




\begin{figure}[t]
\centering
\subfloat[Varying $m$]{
    \includegraphics[width=0.45\linewidth]{vary-m.pdf}
}\hfill
% \hspace{1.5mm}
\subfloat[Varying $l$]{
\includegraphics[width=0.45\linewidth]{vary-l.pdf}
}

\subfloat[Varying $\tau$]{
    \includegraphics[width=0.45\linewidth]{vary-t.pdf}
}\hfill
% \hspace{1.5mm}
\subfloat[Varying $s$]{
    \includegraphics[width=0.45\linewidth]{vary-s.pdf}
}
\vspace{-2mm}
\caption{Sensitivity study of parameters of \textsf{Snoopy}.}
\label{fig:sense}
\vspace{-6mm}
\end{figure}






We study the influence of four important hyper-parameters on the performance of \textsf{Snoopy}.



% \noindent \textbf{Impact of the cardinality $m$ of each proxy column.} 
First, we explore the impact of the number $m$ of elements in each proxy column. We vary the value of $m$ and show the result in Fig.~\ref{fig:sense}(a).
It is observed that the performance of \textsf{Snoopy} is not sensitive to the hyper-parameter $m$. 


% \noindent \textbf{Impact of the number $l$ of proxy columns.}
Next, we vary the number $l$ of used proxy columns, and show the results in Fig.~\ref{fig:sense}(b).
It is observed that as $l$ increases from 30 to 90, Recall@25  exhibits a noticeable improvement, which demonstrates that more information can be captured by increasing the number of proxy columns. When $l$ continues to increase, the recall no longer increases but tends to be stable. Thus, it is advisable to set $l$ to a relatively large value. For best performance across all datasets, we set $l$ to 90.


% \noindent \textbf{Impact of the threshold $\tau$ of cell matching.} 
Then, we vary the threshold $\tau$ of cell matching from 0.1 to 0.3, and show the Recall@25 in Fig.~\ref{fig:sense}(c). It is observed that the performance is relatively stable under different thresholds $\tau$,  indicating that \textsf{Snoopy} is capable of accommodating different degrees of semantic join. Note when $\tau$ is set to 0, semantic-join degrades to equi-join.

% \noindent \textbf{Impact of the length $s$ of the positive ranking list.} 
Finally, we explore the impact of the length $s$ of the positive ranking list on the search results, as depicted in Fig.~\ref{fig:sense}(d). It is observed that as $s$ grows, the Recall@25 first increases and then gradually decreases. This is because treating the low-ranked columns in the ranking list as positive examples sometimes hurts the contrastive learning process, especially for the columns with high ranks. Hence, we set $s$ to 3 for the best performance on all the datasets.

\subsection{ Further Experiments} 
\label{subsec:further_exp}
We further (i) explore how ground truth in evaluation shifts when using different cell embedding functions; (ii) evaluate the effectiveness of \textsf{Snoopy} under the dynamic scenario; and (iii) explore some optimizations to overcome the size limits in existing PTM-based methods.
% (3)  explore the performance of existing column-level methods (e.g., DeepJoin) that use the long-context (e.g., 4k context) PTM or column compression techniques.
 



\noindent{ (i) \underline{\textit{Impact of cell embedding function}}}. 
We explore two cell embedding functions: Word2vec\footnote{https://huggingface.co/LoganKilpatrick/GoogleNews-vectors-negative300}, which is less powerful, and MPNet-based sentence-embedding model\footnote{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}, which is more powerful than the default fastText.
Specifically, for each query column, we first obtain its top-25 ranked column list $L_f$ based on the semantic joinability using fastText. Then, we recompute joinabilities between the query column and those 25 columns using Word2vec and MPNet to generate new ranked lists $L_w$ and $L_m$, respectively. 
Finally, we measure ground truth shifts by quantifying the similarity between rankings $L_f$ and $L_w$ (resp. $L_m$) using $1-\rho =  \frac{6 \sum_{i=1}^s d_i^2}{s(s^2 - 1)} \in [0,1] $, where $\rho$ is the Spearman's rank correlation coefficient~\cite{Spearman}, $d_i$ represents the rank difference of column $C_i$ between $L_f$ and $L_w$ (resp. $L_m$), and $s$ is the list length.
The results are presented in Table~\ref{tab:gt_shifts}. We can observe that the shifts are small on all three datasets, demonstrating that while absolute embedding values may differ, the relative rankings derived by different cell embedding functions are similar. Notably, MPNet exhibits smaller shifts than Word2vec, indicating that the default fastText is closer to the MPNet than Word2vec.






\begin{table} \small
\centering
\caption{ Performance of E5-base-4k. $\Delta$ indicates the difference relative to the best-performing 512-token-limit PTMs.} 
\label{tab:long_ctx}
\vspace{-2mm}
\renewcommand{\arraystretch}{1.2} % 增加行高
\setlength{\tabcolsep}{0.9mm}{
\begin{tabular}{c|cc|cc|cc} 
\specialrule{.12em}{.06em}{.06em}
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{WikiTable}} & \multicolumn{2}{c}{\textbf{Opendata}} & \multicolumn{2}{c}{\textbf{WDC Small}}  \\
\cline{2-7}
                         & \textbf{R@25}     & \textbf{N@25}               & \textbf{R@25}   & \textbf{N@25}                & \textbf{R@25}   & \textbf{N@25}                 \\ 
\hline
\multicolumn{1}{c|} {E5-base-4k}            &  0.5488	& 0.7306 & 0.8416	& 0.9165 & 0.6424 &	0.8646 \\
\hline
  $\Delta$          &  $\downarrow$0.0584      & $\downarrow$0.0285    &  $\uparrow$0.1196    & $\uparrow$0.0512                 &    $\downarrow$0.0616        & $\downarrow$0.0508                                            \\ 
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\label{tab:ablation}
\vspace{-3mm}
\end{table}

\begin{table}[t] \small
\centering
\caption{ Online efficiency (ms) comparison of E5-base-4k and \textsf{Snoopy}.} 
\vspace{-2mm}
\label{tab:online_efficiency_E5}
\renewcommand{\arraystretch}{1.1} % 增加行高
\begin{threeparttable}
\setlength{\tabcolsep}{0.9mm}{
\begin{tabular}{c|c|ccccc} 
\specialrule{.12em}{.06em}{.06em}
\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}query column \\ encoding  \end{tabular}} & \multicolumn{5}{c}{total online processing}                      \\ 
\cline{3-7}
                        &                                 & 100K      & 200K      & 300K  & 500K  & 1M     \\ 
\hline
E5-base-4k                  & 21.23                           & 25.12 & 25.49 &   25.38    &  25.67     &   25.71    \\
 
\hline
\textsf{Snoopy}               & 1.10                            & \textbf{5.10}      & \textbf{4.91}      & \textbf{5.00}  & \textbf{5.17}  & \textbf{4.97}   \\
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\end{threeparttable}
\vspace{-6mm}
\end{table}

\begin{table}[t]
\small
\centering
\caption{ Performance of different cell sampling strategies applied to DeepJoin. The best results are in bold.} 
\label{tab:tfidf}
\vspace{-2mm}
\renewcommand{\arraystretch}{1.2} % 增加行高
\setlength{\tabcolsep}{0.9mm}{
\begin{tabular}{c|cc|cc|cc} 
\specialrule{.12em}{.06em}{.06em}
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Strategies}}} & \multicolumn{2}{c}{\textbf{WikiTable}} & \multicolumn{2}{c}{\textbf{Opendata}} & \multicolumn{2}{c}{\textbf{WDC Small}}  \\
\cline{2-7}
                         & \textbf{R@25}     & \textbf{N@25}               & \textbf{R@25}   & \textbf{N@25}                & \textbf{R@25}   & \textbf{N@25}                 \\ 

\hline
  {Frequency}          &  0.5600      & 0.6984    &  \textbf{0.8188}    & \textbf{0.9040} &      \textbf{0.7240} & \textbf{0.9170}                     \\ 
\hline
\multicolumn{1}{c|} {TF-IDF}       &  0.5608	 & 0.7061       &   0.8128 &	0.9016	    & 0.7104	& 0.9078 \\

  \hline
\multicolumn{1}{c|} {BM25}            &  \textbf{0.5611}	& \textbf{0.7066} 	& 0.8129 &	0.8955  &   0.7112	 & 0.9079	 \\
 
\specialrule{.12em}{.06em}{.06em}
\end{tabular}}
\label{tab:ablation}
\vspace{-3mm}
\end{table} 


\noindent{ (ii){\underline{\textit{ Effectiveness in the dynamic scenario}}}.} To simulate a dynamic scenario where data are constantly added while fixing the number of proxy column matrices, we create an initial repository $\mathcal{R}_0$ by randomly sampling 70\% of the columns from the original dataset.
The remaining 30\%  are evenly divided into three batches $\mathcal{B}_1$, $\mathcal{B}_2$, and $\mathcal{B}_3$, which are incrementally added to the repository.
We denote the repository after adding the first $j$ batches as  $\mathcal{R}_j = \mathcal{R}_0 \cup \dots \cup \mathcal{B}_j (j \geq 1)$.
Table~\ref{tab:dynamic} presents the accuracy of \textsf{Snoopy} in the dynamic scenario. 
We observe some fluctuations of  R@$k$ in different sizes of data corpus. This is because the metric R@$k$ can be influenced by the cut-off errors due to the constraint of the fixed value of $k$, as mentioned in Sec.~\ref{sec:exp_effectiveness}. However, \textsf{Snoopy} demonstrates relatively stable and high N@$k$ performance as new columns are added, highlighting its effectiveness in dynamic scenarios. We also observe that as more data are added, N@$5$ decreases slightly, while N@$25$ increases. 
This arises from the sparsity of joinable columns in the data repository. Adding more data enhances the likelihood of identifying additional joinable columns, improving N@$25$. However, this also increases the difficulty of identifying the most joinable columns, resulting in a slight decrease in N@$5$. 
 



% under two settings: (i) without re-training (w/o rt), reusing the  proxy column matrices learned using $\mathcal{R}_0$, and (ii) with re-training (rt), where new proxy matrices are learned after each new batch insertion. As observed, \textsf{Snoopy} performs quite well in dynamic scenarios, even without re-training the pivot column matrices, demonstrating that the learned proxy column matrices are representative and well capture data joinability. We observe a slight increase in accuracy as the repository size grows. This is because, with a small data size, there are few columns with high joinablities. For instance, the joinablities of the top-100 columns may all be around 0.1, making it challenge to distinguish the top-25. As the data lake expands, more highly joinable columns become candidates, leading to a slight improvement in accuracy.
 

\noindent{(iii) {\underline{\textit{PTM-based methods optimized for size limits}}}.} First, we apply the long-context model E5-Base-4k~\cite{E5-Base-4k}, which supports 4k tokens, to encode columns. The results compared with the best-performing 512-token-limit models are shown in Table~\ref{tab:long_ctx}.
As observed, the accuracy increases on Opendata but decreases on the other two datasets. This is because,  while the long-context model overcomes the size limit, the semantics-joinability gap persists. As more cells are added, non-matching cells may dominate the column's embedding, resulting in reduced accuracy. Furthermore, the long-context model's online encoding is time-consuming, requiring on average 20$\times$ more query column encoding time compared with \textsf{Snoopy}, as shown in Table~\ref{tab:online_efficiency_E5}.
Since the sentence-transformers\footnote{https://huggingface.co/sentence-transformers} does not yet support long-context models, we test the accuracy using the pre-trained E5-Base-4k, expecting similar trends if applied to DeepJoin.
Second, we explore TF-IDF and BM25 to sample cells within each column to ensure the 512-token limit for DeepJoin. The results compared with the original DeepJoin (frequency-based) are shown in Table~\ref{tab:tfidf}. As observed, the impact of different strategies on accuracy is quite limited. This is because these sampling strategies are designed for text similarity and information retrieval tasks, which do not align well with the definition of joinability.


Based on these observations, we compare the pros and cons of \textsf{Snoopy} versus DeepJoin with a long-context encoder. Both \textsf{Snoopy} and long-context-model-powered DeepJoin can overcome size limitations, however, \textsf{Snoopy} has two advantages: (i) it effectively bridges the semantics-joinability gap through proxy column matrices, whereas the long-context model struggles with this gap and may even exacerbate it; and (ii) \textsf{Snoopy} is much more efficient than long-context-model during online search. The primary limitation of \textsf{Snoopy} is its reliance on the cell embedding function and join definition, whereas DeepJoin offers greater flexibility by adapting to various PTMs and join definitions.


 
% \begin{table}
% \footnotesize
% \centering
% \caption{\textcolor{blue}{Recall@25 on three datasets under dynamic scenario.}}
% \vspace{-2mm}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{1mm}{
% \begin{tabular}{c|c|cc|cc|cc} 
% \specialrule{.12em}{.06em}{.06em}
% \multirow{2}{*}{\textbf{Datasets}} & {$\mathcal{R}_0$} & \multicolumn{2}{c|} {$\mathcal{R}_1$} & \multicolumn{2}{c|} {$\mathcal{R}_2$} & \multicolumn{2}{c}{$\mathcal{R}_3$}  \\ 
% \cline{2-8}
%                           & --   & w/o rt &  rt          & w/o rt & rt          & w/o rt & rt          \\ 
% \hline
% WikiTable                 &  0.7576  &   0.7672     &     0.7752           &   0.7768     &     0.7704           &    0.7528    &        0.7728        \\ 
% \hline
% Opendata                  &  0.7744  &   0.8056     &     0.8120           &   0.8452     &     0.8504           &    0.8656    &       0.8716         \\ 
% \hline
% WDC Small                 &  0.7832  &   0.7920     &     0.8176           &   0.8096     &    0.8048            &   0.8032     &       0.8230         \\
% \specialrule{.12em}{.06em}{.06em}
% \end{tabular}}
% \end{table}

% \begin{table}
% \footnotesize
% \centering
% \caption{\textcolor{blue}{NDCG@25 on three datasets under dynamic scenario.}}
% \vspace{-2mm}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{1mm}{
% \begin{tabular}{c|c|cc|cc|cc} 
% \specialrule{.12em}{.06em}{.06em}
% \multirow{2}{*}{\textbf{Datasets}} & {$\mathcal{R}_0$} & \multicolumn{2}{c|} {$\mathcal{R}_1$} & \multicolumn{2}{c|} {$\mathcal{R}_2$} & \multicolumn{2}{c}{$\mathcal{R}_3$}  \\ 
% \cline{2-8}
%                           & --   & w/o rt &  rt          & w/o rt & rt          & w/o rt & rt          \\ 
% \hline
% WikiTable                 &  0.8843  &  0.8852     &     0.8909           &   0.8917     &     0.8841           &    0.9028    &        0.9122        \\ 
% \hline
% Opendata                  &  0.9338  &   0.9411     &     0.9420           &   0.9437     &     0.9475           &    0.9449    &      0.9458         \\ 
% \hline
% WDC Small                 &  0.9524  &   0.9563     &     0.9568           &   0.9594     &    0.9561            &  0.9628     &       0.9632         \\
% \specialrule{.12em}{.06em}{.06em}
% \end{tabular}}
% \end{table}