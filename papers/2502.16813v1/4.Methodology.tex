\section{The \textsf{Snoopy} Framework}
\label{sec:Snoopy}
In this section, we first  
overview the \textsf{Snoopy} framework. Then, we design the column representation via proxy column matrices, and present a rank-aware contrastive learning paradigm to obtain good proxy column matrices. After that, we illustrate the index and online search process. Finally, we devise two training data generation strategies for self-supervised training.


% \begin{myDef}
% \textnormal{\textbf{(Column Mapping)}}.
% Given an input column $\mathbf{C}$, a column mapping operation guided by the proxy column $\mathbf{P}$ maps the column $\mathbf{C}$ to a real number $\phi(\mathbf{C}, \mathbf{P}) \in \mathbb{R}$.
% \end{myDef}



\subsection{Overview}
\label{subsec:overview}

\textsf{Snoopy} framework is composed of two stages: offline and online, as illustrated in Fig.~\ref{fig:framework}.

\noindent\textbf{Offline stage.} Given a table repository $\mathcal{T}$, the textual columns are extracted to form the column repository $\mathcal{R}$. In the \ding{172} training phase, the column representation process transforms each column in $\mathcal{R}$ into a column embedding by concatenating the proxy-guided column projection values. Proxy column matrices are treated as learnable parameters and are updated via rank-aware contrastive learning.
% , which aims to identify good proxy columns that can
% map the joinable columns closely while pushing non-joinable ones far apart in the column embedding space.
% pull joinable columns closer, while pushing non-joinable
% columns far away in the embedding space.
\ding{173} After training, \textsf{Snoopy} uses the learned proxy column matrices to pre-compute all the column embeddings, and stores them in a Vector Storage. The indexes (e.g. HNSW~\cite{HNSW}) are constructed to accelerate the subsequent online search.

\noindent\textbf{Online stage.} Given a  query column $C_Q$ from table $T_Q$, \textsf{Snoopy} first computes the embedding of $C_Q$ using the previously learned proxy column  matrices. 
Then, it uses the embedding of $C_Q$ to search the top-$k$ similar embeddings from the Vector Storage. Finally, \textsf{Snoopy} returns the joinable columns according to the retrieved embeddings. 

% \subsection{Properties of Column Representation}
% Recall that, the effectiveness of column-level methods highly depends on column representations.
% % Given a column $C\in \mathcal{R}$, we aim to design an embedding function $f(\cdot)$ to transform the column $C$ to an embedding $f(C)\in \mathbb{R}^l$.
% Thus, we formalize and analyze several desirable properties of column representations for column-level semantically joinable table search. 


% \begin{figure}
%   \centering
%   \includegraphics[width=1\linewidth]{Framework - 副本 - 副本.pdf} \vspace{-5mm}
%   \caption{{Overview of \textsf{Snoopy}.}
%   \label{fig:framework}}
%   \vspace{-4mm}
% \end{figure}

% % \begin{myProp}
% % \textnormal{\textbf{(\textcolor{red}{Size Unlimited})}.} Given a column $C =\{ c_1, c_2, \dots,\\ c_{|C|} \}$, and another column $C' =\{ c'_1, c'_2, \dots, c'_{|C'|} \}$, the extension of $C$ by $C'$ is denoted as $C||C' = \{c_1, c_2, \dots, c_{|C|}, c'_1, c'_2, \dots, c'_{|C'|} \}$. The column representation is expected to satisfy:
% % \begin{equation}
% % \nexists C, \text{s.t.} \; f(C) \equiv f(C||C'), \forall C^{\prime} \in \mathcal{R}
% % \end{equation}
% % \end{myProp}

% % If a column embedding function has an input size limit $\epsilon$, any column $C$ with size $\epsilon$ satisfies $f(C) \equiv f(C||C')$, as any extension of $C$ exceeds the size $\epsilon$ and would be ignored.



% \begin{myProp}
% \label{prop:1}
%     \textnormal{\textbf{(Size Unlimited)}}. Given a column $C =\{ c_1, c_2, \dots,\\ c_n \} \in \mathcal{R}$, and the embedding function $f: \mathcal{R} \rightarrow \mathbb{R}^l$, the size $n$ of the input column $C$ is arbitrary.
% \end{myProp}


 
% % \subsubsection{Permutation Invariance }
% \begin{myProp}
% \label{prop:2}
% \textnormal{\textbf{(Permutation Invariance)}.} Given a column $C = \{c_1, c_2, \dots, c_n \}$, and an arbitrary bijective function $\delta: \{1, 2, \dots, n\} \rightarrow \{1, 2, \dots, n\}$, a permutation of column $C$ is denoted as $\Tilde{C} = \{c_{\delta(1)}, c_{\delta(2)},\\ \dots, c_{\delta(n)} \}$. The column representation is expected to satisfy:
% \begin{equation}
%     f(C) \equiv f(\Tilde{C}) 
% \end{equation}
% \end{myProp}
% % If a column embedding function treats the input column as an ordered sequence of cells, the output varies with permutations, thereby not satisfying the permutation invariance property.

% % \subsubsection{Rank Preserving}
% \begin{myProp}
% \label{prop:3}
% \textnormal{\textbf{(Order Preserving)}.} 
% % Recall that the embedding-based methods aim to find the top-$k$ results through the embedding of columns.
% % Thus, the ideal column embedding function $f(\cdot)$ is expected 
% % to preserve the ranks of columns obtained by computing the joinability defined in Equation~\ref{eq:js}. We formalize this expected property as follows.
% Given a query column $Q$, two candidate columns $C_1$ and $C_2$, the joinability function $J(\cdot)$, and the column-level similarity measurement $\operatorname{sim}(\cdot)$, the IDEAL column representation is expected to satisfy:
% \begin{equation}
% \small
%     \operatorname{sim} \bigl(f(Q), f(C_1)\bigr) \geq \operatorname{sim} \bigl(f(Q), f(C_2) \bigr)   \Leftrightarrow  J(Q, C_1) \geq J(Q, C_2)
% \end{equation}
% \end{myProp}
% % \textcolor{blue}{We design a column representation via proxy columns (Section~\ref{subsec:ColRep}) which satisfies the \textsc{Property}~\ref{prop:1} and \textsc{Property}~\ref{prop:2}. 
% Note that, \textsc{Property}~\ref{prop:3} is the ultimate goal ensuring that column-level methods achieve the same effectiveness as cell-level ones.
% However, it is challenging due to inevitable information loss resulting from coarse computations at the column level.  
% Thus, we design a training paradigm to learn the optimal proxy columns capable of deriving column representations that approximate this goal (Section~\ref{subsubsec:PivotLearn}).



\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{Framework.pdf} \vspace{-4mm}
  \caption{Overview of \textsf{Snoopy}. ``Column" is abbreviated as ``Col".}
  \label{fig:framework}
  \vspace{-4mm}
\end{figure}

\subsection{Column Representation}
The effectiveness of column-level methods highly depends on column representations. However, existing methods just adopt suboptimal PTMs as column encoders, and the desirable properties of column representations for column-level methods remain under-explored. Thus, we first formalize several desirable properties of column representations, and then propose the AGM-based column projection function to deduce the column embeddings.

\subsubsection{Desirable Properties}
% \vspace{1mm}
% \noindent\textbf{Desirable Properties.}
We provide some key observations based on the definition of joinability and formalize the desirable properties of column representations for column-level semantic join discovery.




The first observation is that each cell in the column may contribute to the joinability score. Consider the example in Fig.~\ref{fig:exm1}, the joinability between  $C_Q$ and $C_1$ is $\frac{3}{3}$=1. If we neglect any cell in the  $C_1$, the join score declines. 
This observation implies that the column embedding function should consider all the cells within the column and not be
% consider all cells within the column, without being
constrained by the column size, which is formalized as follows.

 
\begin{myProp}
\label{prop:1}
 \textnormal{\textbf{(Size-unlimited)}.}
     Given a column $C =\{ c_1, c_2,\\ \dots, c_n \} \in \mathcal{R}$, which is the input of the embedding function $f: \mathcal{R} \rightarrow \mathbb{R}^l$, the size $n$ of the input column $C$ is arbitrary.
\end{myProp}


% \noindent  \underline{\textit{Size-unlimited.}} The first observation is that each cell in the column may contribute to the joinability score. Consider the example in Fig.~\ref{fig:exm1}, the joinability between  $C_Q$ and $C_1$ is $\frac{3}{3}$=1. If we neglect any cell in the  $C_1$, the join score declines. 
% This observation implies that the column embedding function should consider all cells within the column, without being constrained by the   column size. 

 
% \begin{myProp}
% \label{prop:1}
%      Given a column $C =\{ c_1, c_2, \dots, c_n \} \in \mathcal{R}$, which is the input of the embedding function $f: \mathcal{R} \rightarrow \mathbb{R}^l$, the size $n$ of the input column $C$ is arbitrary.
% \end{myProp}



% \noindent \underline{\textit{Permutation-invariant.}} The second observation is that the joinability between two columns is agnostic to the permutations of cells within each column. For instance, if we permute the column  $C_Q$  in Fig.~\ref{fig:exm1} from \{``Los Angeles", ``New York", ``Washington"\} to \{``Washington", ``Los Angeles", ``New York"\}, the joinability between $C_Q$  and $C_1$ is still $\frac{3}{3}$=1. This suggests that the column embedding should be permutation-invariant, which is formalized as follows.
% \begin{myProp}
% \label{prop:2}
% Given a column $C = \{c_1, c_2, \dots, c_n \}$, and an arbitrary bijective function $\delta: \{1, 2, \dots, n\} \rightarrow \{1, 2, \dots, n\}$, a permutation of column $C$ is denoted as $\Tilde{C} = \{c_{\delta(1)}, c_{\delta(2)}, \dots, c_{\delta(n)} \}$. The column representation is expected to satisfy:
% \begin{equation}
%     f(C) \equiv f(\Tilde{C}) 
% \end{equation}
% \end{myProp}

The second observation is that the joinability between two columns is agnostic to the permutations of cells within each column. For instance, if we permute the column  $C_Q$  in Fig.~\ref{fig:exm1} from \{``Los Angeles", ``New York", ``Washington"\} to \{``Washington", ``Los Angeles", ``New York"\}, the joinability between $C_Q$  and $C_1$ is still $\frac{3}{3}$=1. This suggests that the column embedding should be permutation-invariant, which is formalized as follows.
\begin{myProp}
\label{prop:2}
\textnormal{\textbf{(Permutation-invariant)}.} 
Given a column $C = \{c_1, c_2, \dots, c_n \}$, and an arbitrary bijective function $\delta: \{1, 2, \dots, n\} \rightarrow \{1, 2, \dots, n\}$, a permutation of column $C$ is denoted as $\Tilde{C} = \{c_{\delta(1)}, c_{\delta(2)}, \dots, c_{\delta(n)} \}$. The column representation is expected to satisfy:
\begin{equation}
    f(C) \equiv f(\Tilde{C}) 
\end{equation}
\end{myProp}


% \noindent \underline{\textit{Order-preserving.}} Recall that the column-level methods return the top-$k$ results through the similarity comparison of column embeddings. 
% Hence, an ideal column embedding function $f(\cdot)$ needs to preserve the ranking order of columns as determined by their joinabilities in Definition~\ref{def:js}.


% \begin{myProp}
% \label{prop:3}
% % \textnormal{\textbf{(Order Preserving)}.} 
% Given a query column $C_Q$, two candidate columns $C_1$ and $C_2$, the joinability $J(\cdot)$ as defined in Definition~\ref{def:js}, and the column-level similarity measurement $\operatorname{sim}(\cdot)$, the ideal column representation satisfies:
% \begin{equation}
% \begin{gathered}
% \operatorname{sim}\left(f\left(C_Q\right), f\left(C_1\right)\right) \geq \operatorname{sim}\left(f\left(C_Q\right), f\left(C_2\right)\right) \\
% \Leftrightarrow J\left(C_Q, C_1\right) \geq J\left(C_Q, C_2\right)
% \end{gathered}
% \end{equation}
% \end{myProp}

Recall that the column-level methods return the top-$k$ results through the similarity comparison of column embeddings. 
Hence, an ideal column embedding function $f(\cdot)$ needs to preserve the ranking order of columns as determined by their joinabilities in Definition~\ref{def:js}.


\begin{myProp}
\label{prop:3}
\textnormal{\textbf{(Order-preserving)}.} 
Given a query column $C_Q$, two candidate columns $C_1$ and $C_2$, the joinability $J(\cdot)$ as defined in Definition~\ref{def:js}, and the column-level similarity measurement $\operatorname{sim}(\cdot)$, the ideal column representation satisfies:
\begin{equation}
\begin{gathered}
\operatorname{sim}\left(f\left(C_Q\right), f\left(C_1\right)\right) \geq \operatorname{sim}\left(f\left(C_Q\right), f\left(C_2\right)\right) \\
\Leftrightarrow J\left(C_Q, C_1\right) \geq J\left(C_Q, C_2\right)
\end{gathered}
\end{equation}

\end{myProp}


The property~\ref{prop:3} is the ultimate goal ensuring that column-level methods achieve the same effectiveness as exact solutions.
However, it is challenging to achieve this due to the inevitable information loss resulting from coarse computations at the column level.  
Thus, we design a training paradigm to learn good proxy column matrices capable of deriving column representations that approximate this goal (Section~\ref{subsubsec:PivotLearn}). Note that the defined joinability in Definition~\ref{def:js} relies on a cell embedding function $h(\cdot)$. If $h(\cdot)$ is inferior, then the ground truth in evaluation and the learned  column embedding function $f(\cdot)$ may be affected.
% Several alternatives are available for the cell embedding function, including pre-trained language models~\cite{fasttext, sentencebert} and models tailored for entity matching~\cite{camper, ditto}. Designing a good cell embedding function is orthogonal to this work.
In this paper, we follow~\cite{Pexeso} to employ fastText~\cite{fasttext} as a cell embedding function, and show how the ground truth shifts when adopting different cell embedding functions (see Section~\ref{subsec:further_exp}). 




\subsubsection{AGM-based Column Projection}
\label{subsec:ColRep}
% \vspace{1mm}
% \noindent \textbf{AGM-based Column Projection.}
In order to consider all the cells within the column $C$, 
\textsf{Snoopy} first transforms it into the column matrix $\mathbf{C} = h(C)$ by the cell embedding function $h(\cdot)$. 
Then, it computes the column
representation using $\mathbf{C}$ as the input. 
The cell embedding function $h(\cdot)$ is utilized to capture the cell semantics, so that the cells that are not exactly the same but semantically equivalence can be matched and contribute to the joinability.  
Several alternatives are available for the cell embedding function, including pre-trained language models~\cite{fasttext, sentencebert} and models tailored for entity matching~\cite{camper, ditto}.
Note that, designing a good cell embedding function is orthogonal to this work.
% In this paper, we follow~\cite{Pexeso} to employ fastText~\cite{fasttext} as a non-parametric cell embedding function.  


% Given a column $C$, \textsf{Snoopy} first transforms it into the column matrix $\mathbf{C} = h(C)$ by the cell embedding function $h(\cdot)$. 
% Then, it computes the column
% representation using $\mathbf{C}$ as the input. 
% Note that, designing a good cell embedding function is orthogonal to this work. In this paper, we follow~\cite{Pexeso} to employ fastText~\cite{fasttext} as a non-parametric cell embedding function. 
% We now define the concept of pivot column, and column mapping which is the core of pivot-column-based column representation. 



As mentioned in Section~\ref{sec:intro}, the core of proxy-column-based column representation lies in the design of column projection function  $\pi_{\mathbf{P}}(\cdot)$  that aims to well capture the c2pc relationships. Recall that, the semantic joinability in Equation (\ref{eq:js}) represents the c2c relationship captured by the cell-level methods, and it is naturally \textit{size-unlimited} and \textit{permutation-invariant} by definition. 
Thus, a straightforward way is to define $\pi_{\mathbf{P}}(\mathbf{C}) = J(\mathbf{C}, \mathbf{P})$. However, it results in significant information loss.  We begin our analysis by rewriting the joinability in Equation~(\ref{eq:js}) into the following equivalent  mathematical form:
\begin{equation}
\label{eq:js2}
    J(\mathbf{C}, \mathbf{P})= \sum_{i=1}^n \mathds{1}\left(\min _{\mathbf{p}_j \in \mathbf{P}}  d\left(\mathbf{c}_i, \mathbf{p}_j\right) \leq \tau\right) /|\mathbf{C}|
\end{equation}
\noindent
where $\mathds{1}(\mathbf{x})$ is a binary indicator function that returns 1 if the  predicate $\mathbf{x}$ is true  otherwise returns 0. Since the threshold $\tau$ is typically small~\cite{Pexeso} (otherwise, it would introduce many falsely matched cells), the predicate  $\left(\min _{\mathbf{p}_j \in \mathbf{P}}  d\left(\mathbf{c}_i, \mathbf{p}_j\right)\leq\tau\right)$  is typically NOT true.  Consequently, $\mathds{1}(\cdot)$ frequently returns a value of 0, resulting in $J(\mathbf{C}, \mathbf{P})$ being close to 0 for numerous different $\mathbf{C}$. Hence, the projection values tend to be small and similar to each other, resulting in a significant information loss.
\begin{example}
\label{example-4}
Consider the example in Fig.~\ref{fig:exm1}. According to the cells within the columns,  the column matrix $\mathbf{C}_Q$ is supposed to be similar to $\mathbf{C}_1$ while dissimilar to $\mathbf{C}_3$. Thus, we assume that  $\mathbf{C}_Q = [[-0.1, 0.2], [0.2, 0.3], [-0.2,$ $ 0.3]], \mathbf{C}_1 = [[-0.1, 0.25], [0.15, 0.35], $ $ [-0.15, 0.3]]$, and $\mathbf{C}_3 = [[-0.8, 0.2],$ $[0.3, 0.8], [0.5, 0.3]]$. We also assume that a proxy column matrix $\mathbf{P} = [[1, -1], [-1, -1]]$ is used, the Euclidean distance is adopted as $d(\cdot)$ and the threshold $\tau = 0.1$. If we set $\pi_{\mathbf{P}}(\mathbf{C}) = J(\mathbf{C}, \mathbf{P})$, we would get $\pi_{\mathbf{P}}(\mathbf{C}_Q) = \pi_{\mathbf{P}}(\mathbf{C}_1) = \pi_{\mathbf{P}}(\mathbf{C}_3) = 0$, despite the differences of the three columns.
\end{example}

This observation motivates us to explore a column projection mechanism to better capture the c2pc joinabilities. 
Since both the column $\mathbf{C}$ and proxy column $\mathbf{P}$ can be treated as order-insensitive cell sets, we resort to the maximum bipartite matching which has been extensively employed in set similarity measurement~\cite{SilkMoth,TokenJoin}. Given an input column $\mathbf{C} = \{\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_n\}$, a proxy column $\mathbf{P} = \{\mathbf{p}_1, \mathbf{p}_2, \dots, \mathbf{p}_m\}$, and a similarity measurement $\operatorname{sim}(\cdot)$, we construct a bipartite graph $\mathcal{G} = (\mathbf{C}, \mathbf{P}, E)$, where $\mathbf{C}$ and $\mathbf{P}$ are two disjoint vertex sets, $E = \{e_{ij}\}$ is an edge set where $e_{ij}$ connects vertices $\mathbf{c}_i$ and $\mathbf{p}_j$ and associated with a weight $\operatorname{sim}(\mathbf{c}_i, \mathbf{p}_j)$.
\begin{myDef}
    \textnormal{\textbf{(Maximum Weighted Bipartite Matching)}}. Given a bipartite graph $\mathcal{G} = (\mathbf{C}, \mathbf{P}, E)$, the maximum weighted bipartite matching aims to
    find a subset of edges   $M \subset E$ that maximizes the sum of edge weights while ensuring no two edges in $M$ share a common vertex. The matching value $\mathcal{M}(\mathbf{C}, \mathbf{P})$ is formulated as follows:
\begin{equation}
\label{eq:BM}
\begin{aligned}
&  {\mathcal{M}}(\mathbf{C}, \mathbf{P})=\max \sum_{i=1}^n \sum_{j=1}^m u_{i j} \operatorname{sim}\left(\mathbf{c}_{i},\mathbf{p}_{j}\right),  \text{subject to:} \\
% & \text{subject to:}\\
& \textnormal{(i)} \; \forall  i \in\{1,2, \ldots, n\}, \sum_{j = 1}^m  u_{i j} \leq 1,  u_{i j} \in\{0,1\}\\
& \textnormal{(ii)} \; \forall j \in\{1,2, \ldots, m\}, \sum_{i = 1}^n  u_{i j} \leq 1,  u_{i j} \in\{0,1\}
\end{aligned}
\end{equation}
\end{myDef}
\noindent
where $u_{ij} = 1$ (or $0$) indicates the edge $e_{i j}$ is (or not) in $M$.

However, since the time complexity of weighted bipartite matching is 
% $\mathcal{O}\left((n+m)^3 \log (n+m)\right)$,
$\mathcal{O}(\{\max(m,n)\}^3)$ using the classical Hungarian algorithm, it is rather inefficient to perform the computation.
% Moreover, deriving gradients with respect to the proxy columns in the subsequent learning phase is not straightforward.
To tackle this,
% instead of using $\mathcal{M}$ as the column mapping operation,
we remove the second constraint (ii) in Equation~(\ref{eq:BM}) to formulate an approximate graph matching (AGM) problem. In this way, two edges are allowed to have common vertices in set $\mathbf{P}$. Thus, the AGM can be solved by a lightweight greedy mechanism and we define the projection $\pi_{\mathbf{P}}(\mathbf{C})$ as the result  $\mathcal{M}'(\mathbf{C}, \mathbf{P})$ of AGM as follows:
% for each vertex $\mathbf{c}_i$, select the edge with the maximum weight $sim(\mathbf{c}_i, \mathbf{p}_j)$:
\begin{equation}
\label{eq:ABM}
     \pi_{\mathbf{P}}(\mathbf{C}) = \mathcal{M}'(\mathbf{C}, \mathbf{P})=\sum_{i=1}^n  \max _{\mathbf{p}_j\in \mathbf{P}} \operatorname{sim}\left(\mathbf{c}_{i},\mathbf{p}_{j}\right)
\end{equation}
\noindent This approximation reduces the time complexity to $\mathcal{O}(mn)$, and the operations can be executed on a GPU, further enhancing efficiency. We use dot product as the measurement $\operatorname{sim}(\cdot)$.



Finally, given a proxy column set $\mathcal{P} = \{\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_l \}$, we can obtain the column embedding of column $C$:
% To incorporate more information from different proxy columns, we apply a set of proxy columns $\mathcal{P} = \{\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_l \}$ and concatenate the column mapping values to obtain the column representation as follows:
\begin{equation}
\label{eq:pi}
   % f(C) = \phi(\mathbf{C}) = [\mathcal{M}'(\mathbf{C}, \mathbf{P}_1), \mathcal{M}'(\mathbf{C}, \mathbf{P}_2), \cdots, \mathcal{M}'(\mathbf{C}, \mathbf{P}_l)]
    f(C)  =  \phi(\mathbf{C}) = \left[\pi_{\mathbf{P}_1} (\mathbf{C}), \pi_{\mathbf{P}_2}(\mathbf{C}), \ldots \pi_{\mathbf{P}_l} (\mathbf{C})\right] 
\end{equation}

\noindent where $\pi_{\mathbf{P}_i}(\cdot)$ is desinged as Equation (\ref{eq:ABM}).



% \begin{prop}
%     The projection value $\pi_{\mathbf{P}}(\mathbf{C})$ in Equation\textnormal{~(\ref{eq:ABM})} is an upper bound of $\mathcal{M}(\mathbf{C}, \mathbf{P})$ defined in Equation\textnormal{~(\ref{eq:BM})}.
% \end{prop}

\begin{myThm}
    The proposed column representation $f(C)$ is size-unlimited  and permutation-invariant.
\end{myThm}

\begin{proof}
   The  transformation $h(\cdot$) from the input column $C$ to column matrix $\mathbf{C}$ is size-unlimited, as all cells are processed independently. The function $\pi_{\mathbf{P}_i} (\cdot)$ in Equation (\ref{eq:ABM}) is also size-unlimited, as it will go through each cell embedding $\mathbf{c}_i$. Thus, $f(C)$ is size-unlimited.
    Permuting the order of cells in $C$ changes the order of $\{\mathbf{c}_i\}$ in column matrix $\mathbf{C}$. However, $\pi_{\mathbf{P}_i} (\cdot)$ is independent of this order,  making $\phi(\mathbf{C})$ permutation-invariant. Thus, $f(C)$ is permutation-invariant.
\end{proof}

% \subsubsection{Design Analysis}
\noindent \textbf{Discussion.}
We now analyze the correlation between the column projection in Equation (\ref{eq:ABM}) and the joinability definition in Equation (\ref{eq:js2}).
First, we substitute the distance function $d(\cdot)$ in Equation (\ref{eq:js2}) with a similarity function $\operatorname{sim}(\cdot)$ and correspondingly replace the distance threshold $\tau$ with a similarity threshold $\alpha$. Since the smaller distance means the higher similarity, we can rewrite the Equation (\ref{eq:js2}) as follows:
\begin{equation}
\label{eq:js3}
    J(\mathbf{C}, \mathbf{P})= \sum_{i=1}^n \mathds{1}\left(\max _{\mathbf{p}_j \in \mathbf{P}}  \operatorname{sim}\left(\mathbf{c}_i, \mathbf{p}_j\right)>\alpha\right) /|\mathbf{C}|
\end{equation}

It is observed that we can simplify 
% our designed column projection function is a smoothy version of
the term $\sum_{i=1}^n \mathds{1}\left(\max _{\mathbf{p}_j \in \mathbf{P}}  \operatorname{sim}\left(\mathbf{c}_i, \mathbf{p}_j\right)>\alpha\right)$ in Equation (\ref{eq:js3}) by eliminating the inequality comparison condition $> \alpha$ and the step-like indicator function $\mathds{1}(\cdot)$, and obtain the Equation (\ref{eq:ABM}).
This indicates that our designed column projection is a
smoothed version of the primary component of Equation (\ref{eq:js3}).
It mitigates information loss from the step-like indicator function, preserving more c2pc relationship details.
 

\begin{example}
\label{example-6}
    Continuing with Example~\ref{example-4}, applying the AGM-based column projection yields  $\pi_{\mathbf{P}}(\mathbf{C}_1) = -0.3$, $\pi_{\mathbf{P}}(\mathbf{C}_2) = -0.55$, and $\pi_{\mathbf{P}}(\mathbf{C}_3) = 0.3$. Thus, the phenomenon of information loss is alleviated.
\end{example}


\subsection{Rank-Aware Contrastive Learning}
\label{subsubsec:PivotLearn}
To achieve Property~\ref{prop:3}, it is crucial to identify good proxy column matrices $\mathcal{P} \in \mathbb{R}^{l \times m \times d }$ that can map the joinable columns closer and push the non-joinable columns far apart in the column embedding space. Traditional pivot selection methods in metric spaces~\cite{ZhuCGJ22} can be extended to select proxy columns from the given column repository $\mathcal{R}$, and then derive the corresponding proxy column matrices. However, these methods are constrained in the subspace $\mathcal{R}$ of the column space  $\mathbb{C}$, and are not designed for order-preserving property, resulting in inferior accuracy (see ablation study in Section~\ref{sec:exp_ablation}). Also, it is non-trivial to directly select good proxy columns that can achieve order-preserving, as columns consist of discrete cell values. To this end, we present a novel perspective that regards the continuous proxy column matrices $\mathcal{P} \in \mathbb{R}^{l \times m \times d }$ as learnable parameters, and introduce the rank-aware contrastive learning paradigm 
to learn $\mathcal{P}$
guided by the order-preserving objective.


% \subsubsection{Contrastive Learning Paradigm}

% \noindent \textbf{Contrastive Learning Paradigm.}
Order-preserving entails high similarity for joinable column embeddings and low similarity for non-joinable ones. We introduce the contrastive learning paradigm to achieve this objective. Specifically, given an anchor column $C$, a positive column $ C^{+}$ with high joinability $J(C, C^+)$,  a negative column set $\{C_i^-\}_{i=1}^u$ with low joinabilities $\{J(C, C_i^-)\}_{i=1}^u$, and the column embedding function $f(\cdot)$, we have InfoNCE loss~\cite{Moco} as follows:
\begin{equation}
\label{eq:cl}
\mathcal{L} =-\log \frac{e^{  f(C)^{\top} f(C^+)/ t}}{ e^{ f(C)^{\top} f({C}^+)/ t} + {\sum_i e^{  f({C})^{\top} f({C_i}^-)/ t}}}
\end{equation}
\noindent
where $t$ is a  temperature scaling parameter and we fix it to be 0.08 empirically. Note $ f(C)^{\top} f(C_i)$ denotes the  cosine similarity, as we normalize the column embedding to ensure that $\| f(\cdot) \| = 1$.  

\noindent \textbf{Rank-Aware Optimization.}
% \subsubsection{Rank-Aware Optimization}
In traditional InfoNCE loss, each anchor column has just one positive column, and thus, it is difficult to distinguish the ranks of different positive columns~\cite{rank}. To incorporate the rank awareness of positive
columns, we introduce a positive ranking list $L_C = [C_1^+, C_2^+, \dots, C_s^+]$ for each anchor $C$. The positive columns within $L_C$ are sorted in descending order by joinabilities, i.e., $J(C, C_1^+) > J(C, C_2^+) > \dots > J(C, C_s^+)$. During training, each column $C_j^+ \in \mathcal{R}$ is regarded as a positive column or pseudo-negative column. Specifically, we recursively regard $C_j^+$ as a positive column and $\{C_{j+1}^+, C_{j+2}^+, \dots, C_s^+\}$ as pseudo-negative columns.
The pseudo-negative columns, along with the true negative columns $\{C_i^-\}_{i=1}^u$, are combined to form the negative columns for training, as shown in Fig.~\ref{fig:rank}.
Note that, the negative column set $\{C_i^-\}_{i=1}^u$ is generated from the negative queue $\mathcal{Q}$, which will be detailed  in Algorithm~\ref{algo:algo1} later.
% It is a common generation method~\cite{Deepjoin, Moco}, since most column pairs in the huge repository have low joinabilities.
As for the positive ranking list, we design two kinds of data generation strategies to automatically construct it (see Section~\ref{subsec:data_gen}).
We adopt the rank-aware contrastive loss~\cite{rank} $\mathcal{L}_{rank} = \sum_{j=1} ^ s \ell_j$, where $\ell_j$ is defined as follows:
% \begin{equation}
% \label{eq:rcl}
% \ell_j =- \log \frac{e^{  f(C)^{\top}f(C_j^+)/ \tau}}{{\sum \limits_{t \geq j} e^{  f({C})^{\top} f({C_t}^+)/ \tau}} + {\sum_i e^{  f({C})^{\top} f({C_i}^-)/ \tau}}}
% \end{equation} 
\begin{equation}
\label{eq:rcl}
\ell_j =- \log \frac{e^{  f(C)^{\top}f(C_j^+)/ t}}{{ Z+\sum \limits_{r \geq (j+1)} e^{  f({C})^{\top} f({C_r}^+)/ t}}  }
\end{equation} 


\noindent where $Z = { e^{ f(C)^{\top} f({C}{_j}^+)/ t} + {\sum_i e^{  f({C})^{\top} f({C_i}^-)/ t}}}$ in Equation~(\ref{eq:cl}). Equation~(\ref{eq:rcl}) takes the ranks into consideration. Specifically, minimizing $\ell_j$ requires
% increasing $f(C)^{\top} f(C_j^+)$ and
decreasing $f(C)^{\top} f(C_{j+1}^+)$; while minimizing $\ell_{j+1}$ requires increasing $f(C)^{\top} f(C_{j+1}^+)$. Thus, $\ell_j$ and $\ell_{j+1}$ will compete for the  $f(C)^{\top} f(C_{j+1}^+)$. Since $C_{j+1}^+$ would be regarded as negative columns $j$ times while $C_j^+$ would be regarded as negative columns only $(j-1)$ times,  it would guide a ranking of positive columns with $f(C)^{\top} f(C_j^+) > f(C)^{\top} f(C_{j+1}^+)$.




\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{Rank.pdf} \vspace{-7mm}
  \caption{An illustration of rank-aware contrastive learning.}
  \label{fig:rank}
  \vspace{-1mm}
\end{figure}




\begin{algorithm}[!tb]

	\small
	\LinesNumbered
	\caption{\protect\mbox {\textsf{Rank-aware Contrastive Learning (RCL)}}}
 \label{algo:algo1}
        \KwIn{a column repository $\mathcal{R}$, the training epoch $\mathcal{E}$, the list $L$ of positive ranking lists for each column,  the length $s$ of each ranking list,
        % the target/momentum pivot columns $\mathcal{P}^\text{t}$/ $\mathcal{P}^\text{m}$,
        the length $\beta$ of the queue $\mathcal{Q}$}
	\KwOut{the learned proxy column matrices  $\mathcal{P}^\text{t}$}
	Initialize target proxy column matrices  $\mathcal{P}^\text{t}$ and momentum proxy column matrices $\mathcal{P}^\text{m}$ in the same way\\
	\ForEach{ e $\in \left \{0, 1, ..., \mathcal{E}\right \}$}
    {     Get a $batch$ of columns from $\mathcal{R}$\\
	   $\mathcal{Q} \leftarrow$ Enqueue($\mathcal{Q}$, $batch$) \\
           \If{$len(\mathcal{Q}) >= \beta+1$}
           {
              CurrentBatch $B \leftarrow$ Dequeue($\mathcal{Q}$)\\
              \ForEach{ column $C$ in $B$  }
              {     Get the positive ranking list $L_C$ of $C$ from $L$\\
                   \ForEach{ j $\in \left \{0, 1, ..., s\right \}$  }
                   { 
                     $C_j^+ \leftarrow L_C\left[j\right]$\\
                     $\{C_r^+\} \leftarrow L_C\left[j+1:\right]$\\
                     $\{C_i^-\} \leftarrow$ All the columns in the current $\mathcal{Q}$\\
                     Compute $\ell_j$ using $C$, $C_j^+$,$\{C_r^+\}$, and $\{C_i^-\}$ \text{// Equation (\ref{eq:rcl}) }\\
                   }
                   Compute $\mathcal{L}_C$ using $\{\ell_j\}$
              }
              Compute $\mathcal{L}_B$ using $\{\mathcal{L}_C\}$\\
              Compute the gradients $\nabla_{{\mathcal{P}}^\text{t}}\mathcal{L}_B$\\
              Update $\mathcal{P}^\text{t}$ using stochastic gradient descent\\
              Update $\mathcal{P}^\text{m}$ with momentum\text{// Equation (~\ref{eq:mom}) }\\
              
           }
	
	}
	\Return{$\mathcal{P}^\text{t}$}
  
\end{algorithm}


In practice, considering the training efficiency, we freeze the cell embedding function $h(\cdot)$, and thus, the only learnable parameters are the proxy column  matrices $\mathcal{P} = \{\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_l \}$.
% \textcolor{blue}{Note that, since the computation involves a non-differentiable operation -- $\operatorname{max(\cdot)}$, we apply straight-through gradient estimation~\cite{sun2024learning} to back-propagate the gradient from rank-aware contrastive loss.}
Since enlarging the number of negative samples typically brings performance improvement in contrastive learning~\cite{camper}, we maintain a  negative column queue $\mathcal{Q}$ with the pre-defined length $\beta$ to consider more negative columns.
We present the rank-aware contrastive learning process (\textsf{RCL}) in Algorithm~\ref{algo:algo1}.  At the beginning, \textsf{RCL} would not implement the gradient update until the  queue reaches the pre-defined length $\beta$ + 1 (line 5). Next, the oldest batch is dequeued and becomes the current batch (line 6).
For each column $C$ in the current batch, \textsf{RCL} gets the positive ranking list $L_C$ (line 8), and there are $s$ iterations to be performed to compute the loss $\mathcal{L}_C$. 
In each iteration, \textsf{RCL} first gets the positive column $C_j^+$, the pseudo-negative columns $\{C_r^+\}$, and  the true negative columns $\{C_i^-\}$ (line 10--12).
% Note that the true negative columns are from the current negative queue, which is a common strategy~\cite{starmine,selfKG}, as most column pairs in the huge repository have low joinabilities.
Then, \textsf{RCL} computes the loss $\ell_j$ using Equation (\ref{eq:rcl}). After $s$ iterations,  the loss $\mathcal{L}_C$ can be obtained (line 14). 
After getting losses of each column in the current batch, \textsf{RCL} computes $\mathcal{L}_B$ and the gradients of proxy column  matrices (line 15--16).
We adapt the momentum technique~\cite{Moco} to mitigate the obsolete column embeddings. Specifically, two sets of proxy column  matrices (i.e., the target proxy column  matrices ${\mathcal{P}}^\text{t}$ and the momentum proxy column matrices ${\mathcal{P}^\text{m}}$) are maintained. While ${\mathcal{P}}^\text{t}$ is instantly updated with the backpropagation, ${\mathcal{P}}^\text{m}$ is updated with momentum as follows:
\begin{equation}
\label{eq:mom}
    \mathbf{\mathcal{P}}^\text{m} \leftarrow \alpha \cdot \mathbf{\mathcal{P}}^\text{m}+(1-\alpha) \cdot \mathbf{\mathcal{P}}^\text{t}, \alpha \in[0,1)
\end{equation}

\noindent where  $\alpha$ is the momentum coefficient. Note that ${\mathcal{P}}^\text{t}$ and ${\mathcal{P}}^\text{m}$ are initialized in the same way before training (line 1).
% The learned ${\mathcal{P}}^\text{t}$ are utilized to compute the embeddings for both the columns in the repository and the query column during online process.
The learned $\mathcal{P}^\text{t}$ is used to compute the column embeddings during offline and online processes.

% In practice, considering the training efficiency, we freeze the cell embedding function $h(\cdot)$, and thus, the only learnable parameters are the proxy columns $\mathcal{P} = \{\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_l \}$.
% Since enlarging the number of negative samples typically brings performance improvement in contrastive learning~\cite{selfKG,ContrastiveBox}, we maintain a  negative column queue $\mathcal{Q}$ with the pre-defined length $\beta$ to consider more negative columns.
% % during the mini-batch training, a negative column queue $\mathcal{Q}$ is maintained to store the columns in previous batches. At the beginning, we would not implement the gradient update until the  queue reaches the pre-defined length $\beta$ + 1. Then, the oldest batch is dequeued to become the current batch, and all the other columns in $\mathcal{Q}$ form the negative column set.
% We adapt the momentum technique~\cite{Moco} to avoid the obsolete column representations. Specifically, two sets of proxy columns (i.e., the target proxy columns ${\mathcal{P}}^\text{t}$ and the momentum proxy columns ${\mathcal{P}^\text{m}}$) are maintained. While ${\mathcal{P}}^\text{t}$ is instantly updated with the backpropagation, ${\mathcal{P}}^\text{m}$ is updated with momentum as follows:
% \begin{equation}
% \label{eq:mom}
%     \mathbf{\mathcal{P}}^\text{m} \leftarrow \alpha \cdot \mathbf{\mathcal{P}}^\text{m}+(1-\alpha) \cdot \mathbf{\mathcal{P}}^\text{t}, \alpha \in[0,1)
% \end{equation}

% Note ${\mathcal{P}}^\text{t}$ and ${\mathcal{P}}^\text{m}$ are initialized in the same way before training.
% The learned ${\mathcal{P}}^\text{t}$ are utilized to compute the embeddings for both the columns in the repository and the query column.






\subsection{Index and Search}
\label{subsec:OnlineSearch}
After contrastive learning, \textsf{Snoopy} uses the learned $\mathcal{P}^\text{t}$ to pre-compute the embeddings of all the columns in the repository $\mathcal{R}$.
% To further accelerate the search process, \textsf{Snoopy} adopts the approximate nearest neighbor (ANN) search methods. Thus, during the offline stage, 
Then, \textsf{Snoopy} can construct the indexes for column embeddings using any prevalent indexing techniques, to boost the online approximate nearest neighbor (ANN) search. Since graph-based methods are a proven superior trade-off in terms of accuracy versus efficiency~\cite{WangXY021}, \textsf{Snoopy} adopts HNSW~\cite{HNSW} to construct indexes.

For online processing, when a query column $C_Q$ comes, \textsf{Snoopy} uses the learned $\mathcal{P}^\text{t}$ to get $f(C_Q)$. Compared with the existing PTM-based methods, \textsf{Snoopy}'s online encoding process is more efficient due to the proposed lightweight AGM-based column projection. Then, \textsf{Snoopy}  performs the ANN search using the indexes constructed offline and finds the top-$k$ similar column embeddings to $f(C_Q)$ from the Vector Storage. Finally, the corresponding top-$k$ joinable columns in the table repository are returned. We analyze the time complexity of  the online and offline stages in Appendix A.

% \vspace{1mm}
% \noindent \textbf{Time Complexity}. 
% We denote the cardinality of proxy column set  as $l$ and the cardinality per proxy column as $m$. During the offline stage, the complexity of pre-computing all the column embeddings is $\mathcal{O}(ml|\overline{C}||\mathcal{R}|) = \mathcal{O}(|\overline{C}||\mathcal{R}|)$, where $|\mathcal{R}|$ denotes the number of columns in the repository, and $|\overline{C}|$ denotes the average size of columns in the repository; and the time complexity of index construction using HNSW is $\mathcal{O}(|\mathcal{R}| \operatorname{log}|\mathcal{R}|)$. During the online stage, the complexity of computing the query column embedding is $\mathcal{O}(ml|C_Q|) = \mathcal{O}(|C_Q|)$, where $|C_Q|$ is the size of the query column $C_Q$; and the time complexity of ANN search is  $\mathcal{O}(\operatorname{log}|\mathcal{R}|)$. 
% Consequently, the overall time complexity of online processing is $\mathcal{O}(|C_Q| + \operatorname{log}|\mathcal{R}|)$.


\subsection{Training Data Generation}
\label{subsec:data_gen}
During contrastive learning, the negative column set is generated from the negative queue $\mathcal{Q}$, as most column pairs in the huge repository have low joinabilities.
As for the positive columns,
% previous work~\cite{Deepjoin}   
% utilizes the exact algorithms to label some column pairs in the table repository with a joinability score exceeding a specified high threshold.
% However, positive column pairs with high joinability scores are rare in the real table repository. Moreover,
% in our rank-aware paradigm, we require each anchor to have a list of positives, incurring numerous anchor columns struggling to find enough positive columns. Additionally, relying on the data in the repository, it tends to find positive columns lacking diversity.
% To address the aforementioned limitations,
we design two kinds of data generation strategies.
Our objective is to synthesize positive columns with expected joinability scores. In this way, we can generate a ranked 
positive list $L_C$ according to given ranked  scores.

% \begin{algorithm}[!tb]
 
% 	\small
% 	\LinesNumbered
%  \caption{\textsf{Text-level Column Synthesis (TCS)}}
% 	\label{alg:tcs}
%         \KwIn{a column $C$, the sample rate $x\%$, the distance function $d$ and threshold $\tau$}
% 	\KwOut{the anchor column $C_a$ with positive column $C_a^+$}
% 	Divide $C$ into two subsequences $C_a$ and $C_b$\\
%     $S_a \leftarrow$Randomly sample $x\%$ cells from $C_a$\\
%     $S_a' \leftarrow \emptyset$\\
  
% 	\ForEach{ c in $S_a$}
%     {     Randomly sample an augmentation operator $\operatorname{aug}(\cdot)$\\
% 	   $c' \leftarrow \operatorname{aug}(c)$ \\
%           $Flag \leftarrow d(h(c), h(c')) \geq \tau$\\ 
%            \If{$Flag == 1$}
%            {
%               $c' = c$
%            }
%            $S'_a.append(c')$
	
% 	}
%     $C_a^+ \leftarrow \text{shuffle}(S'_a\|C_b)$\\
% 	\Return{$C_a$, $C_a^+$}
 
% \end{algorithm}




% \begin{figure}
%   \centering
%   \includegraphics[width=1\linewidth]{DataGen.pdf} \vspace{-7mm}
%   \caption{{Illustration of text-level positive column synthesis.}
%   \label{fig:text-level}}
%   \vspace{-5mm}
% \end{figure}

% \subsubsection{Text-level Synthesis}
\vspace{1mm}
\noindent \textbf{Text-level Synthesis}.
Given a column $C$, we randomly divide it into two sub-columns $C_a$ and $C_b$. We use $C_a$ as the anchor column, and $C_b$ as the residual column. 
The reason why we maintain the residual column $C_b$ will be detailed later.
Now we illustrate how to synthesize a positive column $C_a^+$ for $C_a$ with a joinability approximated to a specified score $x\%$.


We randomly sample $x\%$ cells from the anchor $C_a$ to form a sampling column $S_a$. However, directly using $S_a$ as the positive column has two shortcomings: (i) the matched cells between $C_a$ and $S_a$ are exactly the same, without covering the semantically-equivalent cases, and (ii) each cell in $S_a$ can find matched cell in the anchor $C_a$, which is not realistic.
To tackle (i), we follow~\cite{Watchog,starmine} to apply straightforward augmentation operations to each cell $c \in S_a$ in text-level.  Note we should guarantee that the augmentation operator would not be too aggressive that $c$ and the augmented $c'$ are no longer matched under the distance threshold $\tau$ in Definition~\ref{def:cellmatch}. If that happens, we give up this augmentation and let $c'= c$. After that, we obtain an augmentation version $S'_a$ of $S_a$.
To tackle (ii), we concatenate $S'_a$ with the residual sub-column $C_b$ to obtain $S'_a || C_b$. Since we can assume that duplicates are few in the original column~\cite{autofuzzyjoin},  most cells in the sub-column $C_b$ do not have matches in the sub-column $C_a$, effectively simulating non-matched cells between the positive column and the anchor column.
Finally, we apply shuffling to $S'_a || C_b$ to obtain the required positive column $C_a^+$.
% Note that, $J(C_a, C_a^+)$ may slightly exceed $x\%$, but it would not affect the efficacy of our model training. In this way, we can easily construct $L_C$ according to a given sorted list of joinability scores.



% \subsubsection{Embedding-level Synthesis}
\vspace{1mm}
\noindent\textbf{Embedding-level Synthesis.}
An obstacle to text-level synthesis is that it is hard to determine the granularity of the applied augmentation operator, especially in an extremely small threshold $\tau$. To this end, we propose another embedding-level column synthesis strategy.

Given a column matrix $\mathbf{C}$, analogous to the text-level method, we first horizontally divide $\mathbf{C}$ into two sub-matrices $\mathbf{C}_a$ and $\mathbf{C}_b$. Then we randomly samples $x\%$ rows from $\mathbf{C}_a$, and denote it as $\mathbf{S}_a$. Note that each row of $\mathbf{S}_a$ is a cell embedding $\mathbf{c}$. Next, we  augment each cell embedding vector $\mathbf{c}$ by random perturbation. Specifically, we generate a random vector $\mathbf{r} $ following normal distribution $\mathcal{N}(0, \sigma)$, and gets the augmented $\mathbf{c}' = \mathbf{c} + \mathbf{r}$. Here, we also need a validation step to ensure that $d(\mathbf{c}, \mathbf{c}') \leq \tau$. But the advantage is that we can reduce $\sigma$ by multiplying a coefficient $\gamma \in (0,1)$ until the augmentation is proper to make $\mathbf{c}'$ matches $\mathbf{c}$ under the threshold $\tau$. Since the augmentation is operated in the continual embedding space, it is more flexible than the text-level method.



% \begin{algorithm}[!tb]
 
% 	\small
% 	\LinesNumbered
%  \caption{\protect\mbox{\textsf{Embedding-level Column Synthesis (ECS)}}}
% 	\label{alg:mcsa}
%          \KwIn{a column matrix $\mathbf{C}$, the sample rate $x\%$, the distance function $d$ and threshold $\tau$}
% 	\KwOut{\protect\mbox{the anchor matrix $\mathbf{C}_a$ with positive matrix $\mathbf{C}_a^+$}}
% 	Divide $\mathbf{C}$ into two matrices $\mathbf{C}_a$ and $\mathbf{C}_b$\\
%     $\mathbf{S}_a \leftarrow$Randomly sample $x\%$ rows from $\mathbf{C}_a$\\
%     $\mathbf{S}_a' \leftarrow \emptyset$ \\
  
% 	\ForEach{ \text{row} $\mathbf{c}$  in $\mathbf{S}_a$}
%     {     $Flag \leftarrow 1$ \\
%           \While{Flag == \textnormal{1}} 
%           {Generate a random vector $\mathbf{r}\sim \mathcal{N}(0, std)$ \\
% 	   $\mathbf{c}' \leftarrow \mathbf{c}+\mathbf{r}$ \\
%           $Flag \leftarrow d(\mathbf{c}, \mathbf{c}') \geq \tau$\\ 
%           std=std⋅γstd = std \cdot \gamma
%           } 
%           S′a.append(c′)\mathbf{S}'_a.append(\mathbf{c}')
% 	}
%     C+a←shuffle(S′a‖C_a^+ \leftarrow \text{shuffle}(\mathbf{S}'_a\|\mathbf{C}_b)\\
% 	\Return{\mathbf{C}_a\mathbf{C}_a, \mathbf{C}_a^+\mathbf{C}_a^+}
 
% \end{algorithm}



% Given a column matrix \mathbf{C}\mathbf{C}, analogous to the text-level method, \textsf{ECS} first horizontally divides \mathbf{C}\mathbf{C} into two matrices \mathbf{C}_a\mathbf{C}_a and \mathbf{C}_b\mathbf{C}_b (line 1). Then it randomly samples x\%x\% rows from \mathbf{C}_a\mathbf{C}_a and denote it as \mathbf{S}_a\mathbf{S}_a (line 2). Note each row of \mathbf{S}_a\mathbf{S}_a is a cell embedding \mathbf{c}\mathbf{c}. Next, \textsf{ECS}  augments each cell embedding vector \mathbf{c}\mathbf{c} by random perturbation. Specifically, it generates a random vector \mathbf{r} \mathbf{r}  following normal distribution \mathcal{N}(0, \sigma)\mathcal{N}(0, \sigma) (line 7), and gets the augmented \mathbf{c}' = \mathbf{c} + \mathbf{r}\mathbf{c}' = \mathbf{c} + \mathbf{r} (line 8). Here it also needs a validation step to ensure that d(\mathbf{c}, \mathbf{c}') \leq \taud(\mathbf{c}, \mathbf{c}') \leq \tau (line 9). But the advantage is that it can reduce \sigma\sigma by multiplying a coefficient \gamma < 1\gamma < 1 (line 10) until the augmentation is proper to make \mathbf{c}'\mathbf{c}' matches \mathbf{c}\mathbf{c} under the threshold \tau\tau and the Flag is equal to 0. Since the augmentation is operated in the continual embedding space, it is more flexible than the text-level method.
