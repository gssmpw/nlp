\section{Preliminaries}
\label{sec:pre}

In this section, we first present the the scope of semantic join discovery, and then provide the problem statement.

% \subsection{\textcolor{blue}{The Scope of Joinable Table Search}}
\subsection{The Scope of Semantic Join Discovery}
\label{subsec: scope}
Our goal is to design a framework that takes a query column as input, and searches for the top-$k$ semantically joinable columns from a large table repository. Then, it is easy to locate the tables with these joinable columns.
How to find the best way of joining column elements after discovery, as explored in studies~\cite{semanticjoin,autofuzzyjoin,Auto-Join} is out of our scope.
% The scope of semantic join discovery is discovering joinable columns, thus dose not include performing joins like studies~\cite{NobariR22, Singh16, Auto-Join}. 
Note that, the join relationship is not a simple binary relationship (i.e., joinable or not joinable); rather, it often involves determining which column is more joinable to the query.
Since performing join is time-consuming, users and data scientists prefer to prioritize columns with higher joinability, rather than considering all columns labeled as joinable without any indication of their relative join probabilities.
Thus, it is beneficial for discovery methods to return the top-$k$ columns ranked by joinabilities.
% Since not all joins can bring beneficial attributes (e.g., \texttt{Unemp}. shown in Fig.~\ref{fig:exm1}) for data analysis, it typically needs a feature selection step after joining~\cite{GalhotraGF23}, which is beyond our focus. 


\subsection{Problem Statement}
\label{subsec: prob}

Given a table repository $\mathcal{T}$, each table $T \in \mathcal{T}$ consists of several columns $\{C_1, C_2, \dots, C_{|T|}\}$, where $|T|$ denotes the number of columns in $T$.
% Let $\mathcal{T} = \{ C_1, C_2, \dots, C_{|\mathcal{T}|}\} $ be a  table repository of size $|\mathcal{T}|$, where each $C_i$ denotes a column from a table $T$ in the repository $\mathcal{T}$.
In this paper, we focus on  textual columns, following the recent study~\cite{Deepjoin}.
For simplicity, we extract all the textual columns from $\mathcal{T}$ into a column repository $\mathcal{R} =\{C_1, C_2, \dots, C_{|\mathcal{R}|} \}$.  Each column $C_i$ consists of a set of cells $\{c_j^i\}$. Since metadata (e.g., column names) are typically missing or incomplete in real scenarios~\cite{NargesianZMPA19}, we rely solely on cell information.

\begin{myDef}
\label{def: colmat}
    \textnormal{\textbf{(Column Matrix)}.}  Given a column $C= \{c_1, c_2,  \cdots, c_n\}$, and a cell embedding function $h(\cdot)$ which transforms each $c_i \in C$ to an embedding $\mathbf{c}_i = h(c_i) \in \mathbb{R}^d$,  we extend the notation of cell embedding function $h(\cdot)$ to represent the column matrix $\mathbf{C} = h(C) = \{\mathbf{c}_1, \mathbf{c}_2, \cdots, \mathbf{c}_n\}$.
\end{myDef}
% For simplicity, we use the term "column" to refer to a "column matrix"  in this paper when the context is clear.

\begin{myDef}
\label{def:cellmatch}
    \textnormal{\textbf{(Cell Matching)}.} Given two cells $c_i$ and $c_j$, and the corresponding cell embeddings $\mathbf{c}_i$ and $\mathbf{c}_j$,  we denote $c_i$ matches $c_j$ as  
    $c_i$ $\cong$ $c_j$ if and only if $ d\big(\mathbf{c}_i, \mathbf{c}_j\big) \leq \tau $, where $d(\cdot)$ is a distance function, and $\tau$ is a pre-defined threshold.
% , each cell is transformed to an embedding vector  $\bm{c}_i = h(c_i)$ and $\bm{c}_j = h(c_j)$. 
%  We denote $c_i$ matches $c_j$ as $M_\tau^d(c_i, c_j)=1$  if and only if $ d(\bm{c_i}, \bm{c_j}) \leq \tau $.
\end{myDef}


% The cell embedding function $h(\cdot)$ is utilized to capture the semantics of each cell, so that some cells that are not exactly the same but semantically equivalence (e.g. the cell "Los Angeles" and "LA" in Example~\ref{example-1}) can contribute to the join score. Cell matching can be seen as the well-studied entity matching problem~\cite{EM}. Thus, we can choose any pre-trained language models~\cite{fasttext, sentencebert}, or other fine-tuned embedding models specifically designed for entity matching~\cite{camper, ditto} as the cell embedding function.
% Note that, designing a good cell embedding function is orthogonal to this work. We follow~\cite{Deepjoin} to employ fastText~\cite{fasttext} as the cell embedding function in this paper.


% \begin{myDef}
 
% \textnormal{\textbf{(Semantic-Join Set)}} Given a column $Q$ from a query table, and a target column $C$ from the repository $\mathcal{R}$. The semantic-join set is defined as follows.

% \begin{equation}
%     Q_M=\left\{q \mid q \in Q \wedge \exists c \in C \text { s.t. } M_\tau^d(q, c)=1\right\}
% \end{equation}


% \end{myDef}

% \begin{example}\label{example-2}
 
% Consider the query column "\textsf{City}" of Table Q and the target column "\textsf{City}"  of Table A in Fig.~\ref{fig:exm1}. We assume that cell "Los Angeles" matches cell "LA", cell "New York" matches cell "Big Apple", and  cell "Washington D.C." matches cell "The District", according to Def.~\ref{def:cellmatch}. Then, the semantic-join set is
% $\{$"Los Angeles", "New York", "Washington D.C."$\}$.
% \end{example}

\begin{myDef}
\label{def:js}
\textnormal{\textbf{(Semantic Joinability}).} Given a query column $C_Q$ from a table $T_Q$, and a column $C \in \mathcal{R}$, semantic joinability~\cite{Pexeso,Deepjoin} from $C_Q$ to $C$ is defined as follows:
\begin{equation}
\label{eq:js}
    J(C_Q, C)= \left|\left\{c_q \in C_Q \mid  \exists c \in C \text { s.t. } c_q \cong c\right\}\right|/\ |C_Q|  
\end{equation}
\end{myDef}



% \begin{example}
% Consider the \texttt{City} column in Table $T_Q$ and the \texttt{Place} column in Table $T_B$, as shown in Figure~\ref{fig:exm1}.  Since only two cells (i.e., "LA" and " New York") in \texttt{City}  have matched cells in \texttt{Place}, $J(\texttt{City}, \texttt{Place}) \\= |\{\text{"LA"}, \text{"New York"}\}| /\ |\texttt{City}| = 2 /\ 3.$
% \end{example}
 
With the definition of semantic joinability measure, we can now formalize the problem we aim to solve.

\begin{Prob}
\label{prob}
    \textnormal{\textbf{(Top-$k$ Semantic Join Discovery)}\footnote{Equi-join discovery can be viewed as a specific case of semantic join discovery by setting the threshold $\tau$ in Definition~\ref{def:cellmatch} to 0.}.}  Given a query column $C_Q$ from a table $T_Q$, and a column repository $\mathcal{R}$, top-$k$  semantic join discovery aims to find a subset $\mathcal{S} \subset \mathcal{R}$ where $|\mathcal{S}|=k$  and $ \forall C \in \mathcal{S}$ and $C' \in \mathcal{R} - \mathcal{S}$, $J(C_Q, C) \geq J(C_Q, C')$.
\end{Prob}

Cell-level methods  (e.g. PEXESO~\cite{Pexeso}) design exact algorithms to Problem~\ref{prob} under the defined semantic joinability measure~\cite{Deepjoin}. These methods perform fine-grained cell-level computations to directly compute $J(C_Q, C_i)$ for the query column and each repository column $C_i \in \mathcal{R}$. Despite optimizations such as pruning and filtering, the worst-case time complexity remains linear with respect to the product of the query column size and the sum of repository column size.
In contrast, column-level methods perform a coarser computation and return approximate results. Specifically, they design a column embedding function $f: \mathcal{R} \rightarrow \mathbb{R}^l$ to obtain the query column embedding $f(C_Q)$ and repository column embeddings $f(C_i)$ for $C_i \in \mathcal{R}$, and perform similarity search to efficiently find the top-$k$ joinable columns $\hat{\mathcal{S}}$ ranked by $\operatorname{sim}\bigl(f(C_Q), f(C_i)\big)$, where $\operatorname{sim}(\cdot)$ is a  column-level similarity measurement. 
Hence, the primary challenge of column-level approaches is \textit{how to devise a column embedding function $f(\cdot)$ that makes the result $\hat{\mathcal{S}}$ as close as possible to $\mathcal{S}$.} 


% \begin{figure}
%   \centering
%   \includegraphics[width=1\linewidth]{PC_Def.pdf} \vspace{-6mm}
%   \caption{{Illustration of pivot mapping in metric space and pivot column mapping in our problem.}
%   \label{fig:pcdef}}
%   \vspace{-2mm}
% \end{figure}

% \subsection{\textcolor{blue}{Pivot-based Techniques}}

% \subsection{Pivot-based Techniques}
\label{subsec: pivot_tech}

% The pivot concept has been widely used in different fields, such as metric learning in AI community~\cite{KimKCK20,FCS2}, similarity search in DB community~\cite{metricsurvey,ChavezNBM01}, etc. In metric learning, pivots (proxies) represent the classes of data points and are used to better represent the objects in continuous spaces~\cite{LiangZWA21}, improve the model generalization~\cite{yao2022pcl}, etc. In the field of similarity search in metric spaces, to reduce the number of distance computations and accelerate the similarity search, pivot-based filtering and indexing techniques~\cite{ChavezNBM01,metricsurvey,PivotIndex} have been proposed 
% to pre-compute and store distances from every object $o$ in the metric space to a set of pivots $v_i$, and then utilize these distances to prune unqualified objects during search. 
% % Pivot-based techniques~\cite{ChavezNBM01, metricsurvey, ZhuCGJ22} have been widely used in accelerating similarity search in metric space. 
% % A metric space is a two-tuple ($M$, $d$), in which $M$ is an object domain and $d$ is a distance function that measures the  similarity between objects in $M$.
% % % proposed to accelerate the similarity search by reducing the amount of distance computations. The main idea is to pre-compute and store distances from every object to a set of pivots and then utilize the distances to prune unqualified objects during search. 
% Specifically, given a pivot set $V = \{v_1, v_2, \dots, v_l\}$ and a distance function $\textsf{dist}(\cdot)$ in the metric space, an object $o$ is represented as a  point in the vector space, via the mapping function $\phi(\cdot)$  formulated as follows: 
% \begin{equation}
%      \phi(o) = \:[\pi_{v_1}(o), \pi_{v_2}(o), \dots, \pi_{v_l}(o)] \in \mathbb{R}^l
% \end{equation}
% where $\pi_{v_i}(o) = \textsf{dist}(o, v_i)$ is a projection operation that projects the object $o$ to a specific dimension by pivot object $v_i$. 
% % Figure~\ref{fig:pcdef} (a) and (b) show  an example of pivot mapping.

% % In the offline phase, the mappings can be pre-computed for every objects in the metric space, and during the online stage, the pivot-based filtering~\cite{ChenGLJC17,metricsurvey} can be used to avoid unnecessary similarity computations, if the distance function $d$ satisfies the triangle inequality. Note that, since the joinability in Equation (\ref{eq:js}) does not satisfy the triangle inequality, we can not use the filtering techniques but resort to the advanced approximate
% % nearest neighbor (ANN) search techniques to efficiently search for the results. The aim of extending the concept of pivot to our work is to borrow the idea of pivot mapping, which can naturally overcome the size limitation and permutation sensitivity, which will be detailed in the following sections.

% % Given a query object $q$ and a search radius $r$, the goal is to find all the objects $o$ in the metric space that $d(q, o) < r$. Instead of computing distances between $q$ and all the objects in the metric space, an object $o$ can be safely pruned if $\exists p_i \in P$ s.t. $|\pi_{p_i}(o) - d(q, p_i)|> r$, according to the triangle inequality. 

% % Note that $\pi_{p_i}(o)$ is pre-computed in offline stage, and thus, in the online stage, the needed distance computations are $d(q, p_i) (1 \leq i \leq t)$ where $t \ll |O|$. 

% % Although PEXESO~\cite{Pexeso} adopts the filtering technique to the computation process of \textit{cell matching}, the worst-case time complexity is still  $O(|\mathcal{R}_v|\cdot|Q|)$ where $\mathcal{R}_v$ denotes the multiset of all the cell vectors in the repository.
% % In contrast to PEXESO~\cite{Pexeso} which regards the  cell as a pivot, our proposed pivot columns treat the entire column as a pivot, significantly improving efficiency.
 
% % The performance of pivot-based methods depends on the quality of the pivots selected. To enhance the quality of pivots, various pivot-selection methods~\cite{fft,pca,ZhuCGJ22} have been proposed. 
% % % For example, Farthest First Traversal (FFT)~\cite{fft} iteratively identifies a new pivot, which is the farthest from the current pivot set, and utilizes it to expand the existing pivot set. PCA for Pivot Selection (PCA)~\cite{pca} performs dimensionality reduction to select high-quality pivots, based on the FFT.
% % % For more information on pivot selection, please refer to the survey~\cite{ZhuCGJ22}.
% % Although these methods can be easily extended to select pivot columns in our problem, we provide a novel perspective for pivot column selection, which regards pivot columns as learnable parameters. We show that our pivot column learning method can achieve better performance than the traditional pivot selection strategies via experimental validation in Sec.\textcolor{blue}{xx}. 

% To improve the effectiveness of pivot-based techniques in metric spaces,
% various pivot selection methods~\cite{fft,pca,ZhuCGJ22} have been proposed.
% % For example, Farthest First Traversal (FFT)~\cite{fft} iteratively identifies a new pivot, which is the farthest from the current pivot set, and utilizes it to expand the existing pivot set. PCA for Pivot Selection (PCA)~\cite{pca} performs dimensionality reduction to select high-quality pivots, based on the FFT.
% % For more information on pivot selection, please refer to the survey~\cite{ZhuCGJ22}.
% Although these pivot selection methods can be easily extended to select pivot columns in our problem, we provide a novel perspective that regards pivot columns as learnable parameters, and demonstrate better performance (see Section~\ref{sec:exp_ablation}). 

