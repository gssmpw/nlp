\documentclass[accepted]{arxiv}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref} 

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2024}

\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{mathtools}
\usepackage{xspace}	
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{comment}
\usepackage{amsmath,amssymb}
\usepackage{tablefootnote}
\usepackage{array, longtable}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{epigraph}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{array}
\usepackage{hyperref}
\usepackage{footnotehyper}
\usepackage{fnbreak}
\usepackage{enumitem}
\usepackage{threeparttable}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}

\interfootnotelinepenalty=10000 % предотвращает перенос сноски на другую страницу




\definecolor{maroon}{cmyk}{0,0.87,0.68,0.32}
\definecolor{yellow}{cmyk}{0,0,1,0}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
	
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

\newcommand{\blue}[1]{{\color{blue}#1}}
\usepackage{tcolorbox}
\usepackage{pifont}
\definecolor{mydarkgreen}{RGB}{39,130,67}
\definecolor{mydarkred}{RGB}{192,47,25}
\definecolor{mypurple}{RGB}{250, 150, 250}
\newcommand{\green}{\color{mydarkgreen}}
\newcommand{\red}{\color{mydarkred}}
\newcommand{\cmark}{{\green\ding{51}}}
\newcommand{\xmark}{{\red\ding{55}}}

\newcommand{\R}{\mathbb{R}}

\newlist{assumlist}{enumerate}{1}
\setlist[assumlist,1]{
    label=(\alph*),
    ref=\theassumption(\alph*),
    align=left,
    leftmargin=0.4cm
}

\usepackage{xargs}                      % Use more than one optional parameter in a new commands
 
\usepackage[textsize=tiny]{todonotes}
\newcommandx{\mt}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\mti}[2][1=]{\todo[inline,linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\title{Variance Reduction Methods Do Not Need to Compute Full Gradients: Improved Efficiency through Shuffling}

% Add authors
\author[1,2]{\href{mailto:<mediakov.do@phystech.edu>?Subject=Your No Full Grad paper}{Daniil Medyakov}{}}
\author[1,2]{Gleb Molodtsov}
\author[1,2]{Savelii Chezhegov}
\author[1]{Alexey Rebrikov}
\author[2,1]{Aleksandr Beznosikov}
% Add affiliations after the authors
\affil[1]{%
    Moscow Institute of Physics and Technology
}
\affil[2]{%
    Ivannikov Institute for System Programing of the Russian Academy of Sciences
}

\begin{document}

\maketitle

\begin{abstract}
In today's world, machine learning is hard to imagine without large training datasets and models. This has led to the use of stochastic methods for training, such as stochastic gradient descent (SGD). SGD provides weak theoretical guarantees of convergence, but there are modifications, such as Stochastic Variance Reduced Gradient (\textsc{SVRG}) and StochAstic Recursive grAdient algoritHm (\textsc{SARAH}), that can reduce the variance. These methods require the computation of the full gradient occasionally, which can be time consuming. In this paper, we explore variants of variance reduction algorithms that eliminate the need for full gradient computations. To make our approach memory-efficient and avoid full gradient computations, we use two key techniques: the shuffling heuristic and idea of \textsc{SAG/SAGA} methods. As a result, we improve existing estimates for variance reduction algorithms without the full gradient computations. Additionally, for the non-convex objective function, our estimate matches that of classic shuffling methods, while for the strongly convex one, it is an improvement. We conduct comprehensive theoretical analysis and provide extensive experimental results to validate the efficiency and practicality of our methods for large-scale machine learning problems.
\end{abstract}

\section{Introduction}\label{sec:introduction}

In recent years, machine learning has experienced remarkable progress. It has been driven by the pursuit of enhanced performance and the ability to tackle increasingly complex challenges, which have become central focuses for researchers and engineers alike. This focus has led to a substantial expansion in both the volume of data samples and the scale of models. These development is crucial, as it contributes to greater result stability and enhanced overall task performance. The majority of machine learning tasks are based on the finite-sum minimization problem:
%\vspace{-4mm}
\begin{equation}
\label{eq:finite-sum}
\underset{x\in\mathbb R^d}{\min}\left[f(x) = \frac{1}{n}\sum\nolimits_{i=1}^n f_i(x)\right],
\end{equation}
%\vspace{-3mm}
where $f_i: \mathbb R^d \rightarrow \mathbb R$ and the number of functions $n$ is large. To illustrate, in the context of training machine learning models, let $n$ represent the size of the training set and $f_i(x)$ denote the loss of the model on the $i$-th data point, where $x$ is the vector of model parameters.

Stochastic methods are especially suitable for this problem because they can avoid calculating the full gradient at each algorithm iteration. This is because such an action is prohibitively expensive in real-world learning problems due to the large values of $n$. One of the most well-known stochastic methods for solving this problem is \textsc{SGD} \citep{robbins1951stochastic, moulines2011non} and its multiple modifications \citep{lan2020first}. At iteration $t$, the algorithm selects a single index $i_t$ from the set $\{1, \ldots, n\}$ and performs the following step with the stepsize $\gamma$.
%\vspace{-0.1cm}
\begin{equation*}
    x^{t+1} = x^t - \gamma\nabla f_{i_t} (x^t).
\end{equation*}
%\vspace{-0.1cm}

$\bullet$ \textbf{Variance reduction technique.} Despite the simplicity of classical \textsc{SGD} and the fact that its properties and evaluation are deeply studied, it has one significant drawback: the classical variance of its native stochastic estimators of full gradients remains large throughout the learning process. Thus, \textsc{SGD} with constant learning rate converges linearly only to the neighborhood of the optimal solution, the size of which is proportional to the step and variance \citep{gower2020variance}. Fortunately, the technique of variance reduction (VR) \citep{johnson2013accelerating} was proposed to address this issue. Some of the most popular methods based on this technique are \textsc{SVRG} \citep{johnson2013accelerating}, \textsc{SARAH} \citep{nguyen2017sarah, hu2019efficient}, \textsc{SAG} \citep{roux2012stochastic}, \textsc{SAGA} \citep{defazio2014saga}, \textsc{Finito} \citep{defazio2014finito}, \textsc{SPIDER} \citep{fang2018spider}. In this paper, we analyze two of these algorithms: Stochastic Variance Reduced Gradient (\textsc{SVRG}) and StochAstic Recursive grAdient algoritHm (\textsc{SARAH}). Let us start with \textsc{SVRG} that comprises the following steps:
%\vspace{-4mm}
\begin{align}
\label{eq:vr}
\begin{split}
    &v^t = \nabla f_{i_t} (x^t) - \nabla f_{i_t} (\omega^t) + \nabla f (\omega^t),\\
    &x^{t+1} = x^t - \gamma v^t,
\end{split}
\end{align}
%\vspace{-6mm}
where index $i_t$ is selected at the $t$-th iteration. Let us pay attention to the reference point $\omega^t$. This point should be updated periodically, either after a fixed number of iterations (e.g., once per epoch) or randomly (as in loopless versions; see \citep{kovalev2020don}). The goal of update mechanisms like \eqref{eq:vr} is to move beyond the limitations of naive gradient estimators. It employs an iterative process to construct and apply a gradient estimator whose variance is progressively diminished. This approach allows for the safe use of larger learning rates, thereby accelerating the training process. Now let us look at another approach, \textsc{SARAH}, which updates the estimator of the full gradient recursively:
\begin{align}
\label{eq:sarah}
\begin{split}
    &v^t = \nabla f_{i_t} (x^t) - \nabla f_{i_t} (x^{t-1}) + v^{t-1},\\
    &x^{t+1} = x^t - \gamma v^t. 
\end{split}
\end{align}
The problem with this procedure is that it fails to converge to the optimal solution $x^*$. To resolve this, we need to periodically update $v^t$ using the full gradient. Like \textsc{SVRG}, this process restarts after either a fixed number of iterations or randomly \citep{li2020convergence}. Furthermore, there is a practical version of \textsc{SARAH} that surpasses the performance of \textsc{SVRG}. In that version, the idea of the automatic choice of iterations to update the full gradient estimation is applied by computing the ratio $\frac{\|v^t\|}{\|v^0\|}$ (\textsc{SARAH+} \citep{nguyen2017sarah}).

$\bullet$ \textbf{Shuffling heuristic.} Now let us present an heuristic for selecting the $i_t$-th indexes at each iteration of the algorithms. Note that a comprehensive examination of such approaches has a potential to result in the development of more stable and efficient algorithms. In this paper, we utilize the shuffling heuristic \citep{bottou2009curiously, mishchenko2020random, safran2020good}, which plays a crucial role in our algorithms. The technique is as follows: rather than selecting the index $i_t$ randomly and independently at each iteration, as in classical stochastic methods, we take a more practical approach. We first permute the sequence of indexes ${1, \ldots, n}$ and then select an index based on its position in this permutation during and epoch. There is a number of different shuffling techniques. Some of the most popular approaches include Random Reshuffle (RR), which involves shuffling data before each epoch, Shuffle Once (SO), which shuffles data once at the beginning of training, and Cyclic Permutation, which accesses data deterministically in a cyclic manner In our study, we do not explore the differences between these approaches; instead, we highlight one important common property among them: during one epoch, we calculate the stochastic gradient for each data sample exactly once. Let us formalize this setting. At each epoch $s$, we have a set of indexes $\{\pi_s^0, \pi_s^1, \ldots, \pi_s^{n-1}\}$ that is a random permutation of the set $\{0, 1, \ldots, n-1\}$. Then, e.g. the \textsc{SVRG} update \eqref{eq:vr} of the full gradient estimator at the $t$-th iteration of this epoch transforms into
%\vspace{-1mm}
\begin{equation*}
    v^t = \nabla f_{\pi_s^t} (x_s^t) - \nabla f_{\pi_s^t} (\omega^t) + \nabla f (\omega^t).
\end{equation*}
%\vspace{-6mm}
Nevertheless, the analysis of shuffling methods has some specific details. The key difference between shuffling and independent choice is that shuffling methods do not have one essential property -- the unbiasedness of stochastic gradients derived from the i.i.d. sampling:
%\vspace{-0mm}
\begin{equation*}
    \mathbb{E}_{\pi_s^t} \left[\nabla f_{\pi_s^t}(x_s^t)\right] \neq \frac{1}{n}\sum\nolimits_{i = 1}^n \nabla f_{\pi_s^i}(x) = \nabla f(x).
\end{equation*}
This restriction leads to a more complex analysis and non-standard techniques for proving the shuffling methods.

%This work was inspired by the list of following works, the SVRG method and Shuffling approach was investigated: \cite{mishchenko2020random, koloskova2023shuffle, gorbunov2020unified, li2020unified, malinovsky2023random}

\section{Brief literature review}
$\bullet$ \textbf{No full gradient methods.} \textsc{SVRG} \eqref{eq:vr} and \textsc{SARAH} \eqref{eq:sarah} are now the standard choices for solving a finite sum problem. Nevertheless, they suffer from one significant drawback -- the necessity for the full gradient computation of the target function. There is considerable interest in variance reduction methods that avoid calculating full gradients. While \textsc{SAG} \citep{roux2012stochastic} and \textsc{SAGA} \citep{defazio2014saga} address this issue, they necessitate substantial additional memory allocation, with a complexity of $\mathcal{O}(nd)$. Separately, it is worth noting the attempts to remove the computation of the full gradient in \textsc{SARAH}. First approach was supposed by \citep{nguyen2021inexact}. The authors proposed inexact \textsc{SARAH} algorithm, where computation of the full gradient obviates with mini-batch gradient estimate $\frac{1}{|S|}\sum_{i\in S} f_i(x), S\subset \{1, \ldots, n\}$. To converge to the point with $\varepsilon$-accuracy, this algorithm needs the batch size $|S|\sim \mathcal{O}\left(\frac{1}{\varepsilon}\right)$ and the stepsize $\gamma\sim\mathcal{O}\left(\frac{\varepsilon}{L}\right)$. Another approach actively uses the recursive \textsc{SARAH} update of the full gradient estimator and in the set of works authors proposed the following hybrid scheme without any restarts:
\begin{equation*}
    v^t = \beta_t\nabla f_{i_t} (x^t) + (1-\beta_t)(\nabla f_{i_t} (x^t) - \nabla f_{i_t} (x^{t-1}) + v^{t-1}).
\end{equation*}
In the work \citep{liu2020optimal}, it is used with constant parameter $\beta_t = \beta$, the \textsc{STORM} method \citep{cutkosky2019momentum} considers $\beta_t$ decreasing to zero and \textsc{ZeroSARAH} \citep{li2021zerosarah} combines it with \textsc{SAG/SAGA}. Now let us take a look on the approaches that build methods without computing the full gradient in combination with shuffling techniques, since it is of particular interest in the context of our work. There is a list of works in which research has been conducted in this direction. Methods \textsc{IAG} \citep{gurbuzbalaban2017convergence}, \textsc{PIAG} \citep{vanli2016stronger}, \textsc{DIAG} \citep{mokhtari2018surpassing}, \textsc{SAGA} \citep{park2020linear, ying2020variance}, \textsc{Prox-DFinito} \citep{huang2021improved} naively store stochastic gradients and even \textsc{PIAG}, \textsc{DIAG}, \textsc{Prox-DFinito} that provide the best oracle complexity for the methods with shuffling heuristic from the current state of the theory \footnote{In the work \citep{malinovsky2023random} there are estimates, that outperform those from the discussed works, but they were obtained in the big data regime: $n \gg \frac{L}{\mu}$. It is quite a specific assumption that we do not consider in the current work, so it is out of scope.}, still require $\mathcal{O}(nd)$ of extra memory that is suboptimal in the large-scale problems. Methods \textsc{AVRG} \citep{ying2020variance} and the modification of \textsc{SARAH} \citep{beznosikov2023random} eliminates this issue, however, it deteriorates the oracle complexity. Thus, there are still no variance reduction methods that obviate the need for computing full gradients and are optimal in terms of additional memory and acceptable oracle complexity. We, in turn, address this gap.

$\bullet$ \textbf{Shuffling methods.} As our methods utilize a shuffling heuristic, we will review existing general shuffling techniques and compare their complexity to ours. Conceptually, the use of shuffling techniques has been a focus of research attention for a long time. In fact, sampling without replacement in finite-sum problems guarantees that within a single epoch, we account for the gradient of every function precisely once. It has been also demonstrated that Random Reshuffle converges more rapidly than \textsc{SGD} on a multitude of practical tasks \citep{bottou2009curiously, recht2013parallel}. 

Nevertheless, the theoretical estimates of shuffling methods remained significantly worse than the theoretical estimates of \textsc{SGD}-like methods \citep{rakhlin2012making, drori2020complexity, nguyen2019tight}. A breakthrough was the work \citep{mishchenko2020random} which presented new proof techniques and approaches to understanding shuffling. In particular, the results for strongly convex problems coincided with the results of \textsc{SGD} with an independent choice of indexes \citep{moulines2011non, stich2019unified}. However, in the non-convex case, the results themselves were inferior to those of \textsc{SGD} with an independent choice index selection \citep{ghadimi2013stochastic}. Furthermore, the major problem was the requirement for a large number of epochs to obtain convergence estimates in the non-convex case. This requirement is unnatural for real modern neural networks, where models are trained on a fairly small number of epochs. The solution of this problem was presented in the work \citep{koloskova2024convergence}. The authors built their analysis on the basis of convergence over a shorter period, called the correlation period, rather than on the basis of convergence over one epoch. This offers a more rigorous assessment and provides a foundation for future research.

In all these works, the shuffling heuristic was considered in relation to vanilla \textsc{SGD}, which is not an optimal method for the finite-sum problem under consideration \citep{zhang2020tight, allen2018katyusha} and generally speaking is out of the scope of this paper.
The research was also continued in a more general formulation of the problem as variational inequalities \citep{beznosikov2023smooth}. Thus, \citep{medyakov2024shuffling} applied a shuffling heuristic to the \textsc{Extragradient} method \citep{juditsky2011solving} and achieved a similar estimate in a linear term with that for the method without shuffling.
Consequently, researchers directed their attention to the shuffling heuristic in conjunction with the variance reduction methods and have obtained linear convergence of the methods. Most of all, the \textsc{SVRG} method was investigated.
The first theoretical guarantees for the \textsc{Extragradient} method with the variance reduction scheme was obtained in \citep{medyakov2024shuffling}, who succeeded in obtaining a linear convergence estimate for methods with shuffling in the VI problem.
In the case of the finite-sum minimization, we pay spacial attention on the works \citep{malinovsky2023random, sun2019general}. But the theoretical estimates in these papers are still far from the estimates of the methods with an independent choice of indexes \citep{allen2016variance}. As for this paper, we improve the estimates in the case of a strongly convex objective.

%\vspace{-1mm}
The convergence results from the mentioned and other works are presented in Table \ref{table1}. One can observe that we compare our results only with studies using a shuffling setting where linear convergence was achieved.

%\vspace{-0.5cm}
\begin{table*}[ht]
\centering
\begin{threeparttable}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Algorithm} & 
\begin{tabular}{@{}l@{}}
\textbf{No} \\ \textbf{Full} \\  \textbf{Grad.?}
\end{tabular}
 & \textbf{Memory} & \textbf{Non-Convex} & \textbf{Strongly Convex} \\ \hline
SAGA \cite{park2020linear} & \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
IAG \cite{gurbuzbalaban2017convergence}& \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n^{\red{2}}\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
PIAG \cite{vanli2016stronger} & \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
DIAG \cite{mokhtari2018surpassing} & \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
Prox-DFinito \cite{huang2021improved}& \cmark & \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
AVRG \cite{ying2020variance} & \cmark &  \(\mathcal{O}(d)\) & \(\backslash\) & \(\mathcal{O}\left(n\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
SVRG \cite{sun2019general} & \xmark &  \(\mathcal{O}(d)\) & \(\backslash\) & \(\mathcal{O}\left(n^{\red{3}}\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
SVRG \cite{malinovsky2023random} & \xmark &  \(\mathcal{O}(d)\) & \(\mathcal{O}\left(\frac{nL}{\varepsilon^2}\right)\) & \(\mathcal{O}\left(n\frac{L^{\red{\nicefrac{3}{2}}}}{\mu^{\red{\nicefrac{3}{2}}}}\log\left(\frac{1}{\varepsilon}\right)\right)^{\red{\text{(1)}}}\) \\ \hline
SARAH \cite{beznosikov2023random} & \cmark & \(\mathcal{O}(d)\) & \(\backslash\) & \(\mathcal{O}\left(n^{\red{2}}\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\  \hline
\rowcolor{yellow} SVRG (Algorithm \ref{alg2} in this paper) & \cmark & \(\mathcal{O}(d)\) & \(\mathcal{O}\left(\frac{nL}{\varepsilon^2}\right)\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
\rowcolor{yellow} SARAH (Algorithm \ref{alg2} in this paper) & \cmark & \(\mathcal{O}(d)\) & \(\mathcal{O}\left(\frac{nL}{\varepsilon^2}\right)\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
\end{tabular}%
\vspace{-2mm}
\caption{Comparison of the convergence results.}
\label{table1}
\begin{tablenotes}
\small
    \item [] {\em Columns:} No Full Grad.? = whether the method computes full gradients, Memory = amount of additional memory.
    \item [] {\em Notation:} $\mu$ = constant of strong convexity, $L$ = smoothness constant, $n$ =  size of the dataset, $d$ = dimension of the problem, $\varepsilon$ = accuracy of the solution.
    \item [] {\red{(1)}}: In this work, there are also improved results that hold in the big data regime: $n \gg \mathcal{O}\left(\frac{L}{\mu}\right)$, but it is out of the scope of this work.
\end{tablenotes}    
\end{threeparttable}
%\vspace{-5mm}
\vspace{-2mm}
\end{table*}

\section{Contributions}
Our contributions are as follows. 

$\bullet$ \textbf{Full gradient approximation.} We explore an approach whose key feature is approximating the full gradient of the objective function using a shuffling heuristic \citep{beznosikov2023random}. Moreover, this method does not require any additional $\mathcal{O}(nd)$ memory. Instead, we iteratively build the approximation over an epoch.\\
$\bullet$ \textbf{No need to compute full gradient in variance reduction algorithms.} In a sense, we construct a universal analysis for our gradient approximation and it allows us to equally apply it to two variance reduction methods:
\begin{itemize}
    \item [(a)] Classical \textsc{SVRG} \citep{johnson2013accelerating},
    \item [(b)] \textsc{SARAH} in the closest form to one in \citep{beznosikov2023random}, however, we transform their algorithm to improve convergence estimates.
\end{itemize}
$\bullet$ \textbf{Convergence results.} We obtain convergence results under different assumptions on the objective function. We consider the non-convex case, which is of the greatest interest in modern machine learning problems, as well as the strongly convex case. We obtain state-of-the-art guarantees of convergence in the class of variance reduction algorithms, that do not need the computation of the full gradient at all, for both of our methods. Additionally, we improve upper estimate of shuffling methods in the strongly convex case.\\
$\bullet$ \textbf{Experiments.} We conduct experiments on specific tasks and compare the performance of the considered methods.

\section{Assumptions} 

We provide a list of assumptions below.

%\vspace{-0mm}
\begin{assumption}\label{as1}
    Each function $f_i$ is $L$-smooth, i.e., it satisfies $\|\nabla f_i(x) - \nabla f_i(y)\| \leq L\|x - y\|$ for any $x, y \in \mathbb{R}^d$.
\end{assumption}
%\vspace{-2mm}
\begin{assumption}\label{as2}
    \ 
   %\vspace{-2mm}
    \begin{assumlist}
        \item \label{as2stronglyconvex}
        \text{Strong Convexity:} Each function $f_i$ is $\mu$-strongly convex, i.e., for any $x, y \in \mathbb{R}^d$, it satisfies 
        $$ f_i(y) \geqslant f_i(x) + \langle \nabla f_i(x), y - x \rangle + \frac{\mu}{2}\|y - x\|^2 .$$  
        %\vspace{-6mm}
        \item \label{as2nonconvex} 
        \text{Non-Convexity:}
        The function $f$ has a (may be not unique) finite minimum, i.e. $f^* = \inf\limits_{x \in \mathbb{R}^d} f(x) > - \infty$.  
    \end{assumlist}
\end{assumption}

\section{Algorithms and convergence analysis}\label{sec:main}

\subsection{Full gradient approximation}\label{subsection:gradientapprox}

In this section, we introduce an approach based on the shuffling heuristic and \textsc{SAG/SAGA} ideas that not only approximates the full gradient, but also optimizes memory usage by avoiding the storage of past gradient values. 

The \textsc{SAG} algorithm is one of the early methods designed to improve the convergence speed of stochastic gradient methods by reducing the variance of gradient updates. In \textsc{SAG}, the update step can be written as
\begin{align}\label{eq:sag}
    &\scalebox{0.85}{$x^{t+1} = x^t - \frac{\gamma}{n} \left( \nabla f_{i_t}(x^t) - \nabla f_{i_t}(\phi_{i_t}^t) +  \sum_{j=1}^n \nabla f_j(\phi_j^t) \right),$}
\end{align}
where \(\phi_j^t\) represents the past iteration at which the gradients for the $j$-th function are considered. The core idea is that we store old gradients for each function (essentially storing gradients $\nabla f_j(\phi_j)$ rather than points \(\phi_j\)), and at each iteration, we update one component of this sum with a newly computed gradient.
%However, \textsc{SAG} introduces bias in the gradient estimation, which can affect convergence properties.

%This led to the development of the \textsc{SAGA} algorithm, which improves upon \textsc{SAG} by maintaining an unbiased estimate of the gradient. 

%In \textsc{SAGA}, the gradient update is computed as follows:
%$$\textstyle{x^{t+1} = x^t - \gamma \left( \nabla f_{j_t}(x^t) - \nabla f_{j_t}(\phi_{j_t}^t) + \frac{1}{n} \sum_{i=1}^n \nabla f_i(\phi_i^t) \right).}$$
%Here, \(\phi_j^t\) is updated at each step, ensuring that the gradient estimator remains unbiased. While this approach avoids the need for full gradient computations, it requires storing past gradients \(\nabla f_j(\phi_j^t)\), increasing the memory usage to $\mathcal{O}(nd)$.

In \textsc{SAG}, \(i_t\) are sampled independently at each iteration, which makes it unclear when the gradient for a specific index was last computed. However, in the case of shuffling, we know that during an epoch, the gradient for each $\nabla f_j$ is computed. Thus, at the beginning of an epoch, we reliably approximate the gradient in the same way as in \textsc{SAG}:
\begin{align} \label{eq:withoutfullgrad}
    &\textstyle{v_{s+1} = \frac{1}{n} \sum_{t=1}^n \nabla f_{\pi_s^t}(x_s^t),}
\end{align}
where \(\pi_s^t\) denotes the sequence of data points after shuffling at the beginning of epoch \(s\). This computation is done without additional memory, using a simple moving average during the previous epoch:
\begin{align}
\label{eq:withoutfullgradmovingaverage}
\begin{split}
    \quad~~~&\textstyle{\widetilde{v}_s^{t+1}} = \textstyle{\frac{t}{t+1} \widetilde{v}_s^{t} + \frac{1}{t+1} \nabla f_{\pi_s^t}(x_s^t)},\\
    \quad~~~&v_{s+1} = \widetilde{v}_s^n.
\end{split}
\end{align}
It is a simple exercise to prove \eqref{eq:withoutfullgradmovingaverage} matches \eqref{eq:withoutfullgrad} and we show it in the proof of Lemma \ref{ngl3}.

\subsection{SVRG without full gradients}\label{subsection:svrgalgorithm}

As we already mentioned, variance reduced methods suffer from the major obstacle — they have to compute the full gradient once at each epoch or use additional memory of a larger size. To address this limitation, in this section, we propose a novel algorithm, based on the classical \textsc{SVRG} that not only eliminates the need for full gradient computation but also optimizes memory usage by avoiding the storage of past gradient values.

Previously, in Section \ref{subsection:gradientapprox}, we defined the way we can approximate the full gradient: \eqref{eq:withoutfullgrad}, \eqref{eq:withoutfullgradmovingaverage}. However, this approach introduces a question: how should we alter and use the gradient approximation \(v_s\) throughout an epoch? In \textsc{SAG} \eqref{eq:sag}, we added a new \(\nabla f_{i_t}(x^t)\) and removed its old version $\nabla f_{i_t}(\phi_{i_t}^t)$, but this requires memory for all \(\nabla f_{i_t}\) (\(\phi_{i_t}^t\)). Building on the concept from \textsc{SVRG} \eqref{eq:vr}, rather than computing $\nabla f_{i_t}$ ($\phi_{i_t}^t$), our update uses the gradient at a reference point $\omega_s$. Unlike \textsc{SVRG}, which uses the full gradient at this reference point as shown in \eqref{eq:vr}, we use an approximation provided by \eqref{eq:withoutfullgrad}:
\begin{align*}
    v_s^{t} &= \nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(\omega_s) + v_s.
\end{align*}
Finally, we do the step of the algorithm in Line \ref{alg2:line8}, where the approximation $v_s$ is calculated in the previous epoch as \eqref{eq:withoutfullgradmovingaverage} in Line \ref{alg2:line6}. We now present the formal description of \textsc{No Full Grad SVRG} (Algorithm \ref{alg2}).


\begin{algorithm}[ht]
\caption{\textsc{No Full Grad SVRG}}\label{alg2}
\begin{algorithmic}[1]
    \State \textbf{Input:} Initial points $x_0^0\in\mathbb{R}^d, \omega_0 = x_0^0$; Initial gradients $\widetilde{v}_0^0 = 0^d, v_0 = 0^d$
    \State \textbf{Parameter:} Stepsize $\gamma > 0$
    \For {epochs $s = 0, 1, 2, \ldots, S$}
    \State Sample a permutation $\pi^0_s, \dots, \pi^{n-1}_s$ of $\overline{0, n-1} $ \\ \scriptsize{\hspace{30mm} // Sampling depends on shuffling heuristic}
    \For {$t = 0, 1, 2, \ldots, n-1$}
    \State \label{alg2:line6} $\widetilde{v}_s^{t+1} = \frac{t}{t+1} \widetilde{v}_s^{t} + \frac{1}{t+1} \nabla f_{\pi_s^t}(x_s^t)$
    \State \label{alg2:line7} $v_s^{t} = \nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(\omega_s) + v_s$
    \State \label{alg2:line8} $x_s^{t+1} = x_s^t - \gamma v_s^t$
    \EndFor
    \State $x_{s+1}^0 = x_s^n$
    \State \label{alg2:line11} $\omega_{s+1} = x_s^n$
    \State \label{alg2:line12} $\widetilde{v}_{s+1}^0 = 0^d$
    \State $v_{s+1} = \widetilde{v}_s^n$
    \EndFor
\end{algorithmic}
\end{algorithm}

The outstanding issue is selecting the point $\omega_s$ (Line \ref{alg2:line11}). The approximation $v_s$, representing the full gradient at $\omega_s$, is derived from gradients evaluated at points between $x_{s-1}^1$ and $x_{s-1}^n$. A reasonable choice for $\omega_s$ seems to be the average of these points. However, during this epoch, we are continuously moving away from this point, computing \(\nabla f_{\pi_s^t}(x_s^t)\) and adding to the reduced gradient. Consequently, the average point changes over time. By the end of the current epoch, it evolves into an average calculated from points ranging from $x_s^1$ to $x_s^n$. Thus, choosing \(\omega_s\) as the last point from the previous epoch is a logical compromise, since we estimate not only how far we can move during the past epoch but also how far we have moved during the current one (see Lemma \ref{ngl2}).
An intriguing question for future research is whether more frequent or adaptive updates of $\omega_s$ could further improve convergence rates \citep{allen2016variance}.

By integrating this dynamic strategy, our approach not only improves the efficiency of gradient updates but also optimizes memory usage, making it suitable for large-scale optimization problems. Now let us move to the theoretical analysis. We consider the problem in two formulations: in the non-convex and strongly convex cases.

\subsubsection{Non-convex setting}\label{subsection:svrgnonconvex}

For a more detailed analysis of this method, let us examine the interim results. We structure our analysis as follows: first, we analyze convergence over a single epoch, and then we extend this analysis recursively across all epochs. The crucial aspect here is understanding how gradients change from the start to different points within an epoch. To begin with, we need to show that these changes depend on two critical factors: first, how well we approximate the true full gradient at the start of each epoch; second, how far our updates stray from this initial reference point as we move through it. To obtain this, we present a lemma.

\begin{lemma}\label{ngl2}
Suppose Assumptions \ref{as1}, \ref{as2} hold. Then for Algorithm \ref{alg2} a valid estimate is
    \begin{align*}
    \left\| \nabla f(\omega_s) - \frac{1}{n}\sum\nolimits_{t=0}^{n-1} v_s^t\right\|^2 \hspace{-1mm}\leqslant& ~~2\|\nabla f(\omega_{s}) - v_s \|^2 \\
    & + \frac{2L^2}{n}\sum\nolimits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2.
    \end{align*}
\end{lemma}

In contrast to classical SVRG, where only one term matters due to exact computation of full gradients ($v_s = \nabla f(\omega_s)$), our algorithm avoids such computations. Instead, it introduces an additional term representing errors in approximating these gradients. This error fundamentally reflects discrepancies between our approximation and actual gradients at $\omega_s$. Given that $v_s$ averages stochastic gradients from previous epochs (as per Equation~\eqref{eq:withoutfullgrad}) with $\omega_s$ set as their final point, this error quantifies shifts among those points relative to future reference points. Thus, beginning each epoch at $\omega_s$, we gauge both potential movement within upcoming epochs and progress made during past ones. This perspective underscores a strategic balance involved when selecting $\omega_s$, aligning with discussions found in Section \ref{subsection:svrgalgorithm}. We now present estimates for these deviations through a subsequent lemma.
\begin{lemma}\label{ngl3}
Suppose Assumptions \ref{as1}, \ref{as2} hold. Let the stepsize $\gamma \leqslant \frac{1}{2Ln}$. Then for Algorithm \ref{alg2} a valid estimate is
\begin{eqnarray*}
        \left\| \nabla f(\omega_s) - \frac{1}{n}\sum\nolimits_{t=0}^{n-1} v_s^t\right\|^2 & \leqslant& 8\gamma^2L^2n^2\|v_{s}\|^2 \\
        & & + 32\gamma^2L^2n^2\|v_{s-1}\|^2.
\end{eqnarray*}
\end{lemma}

Now we are ready to present the final result of this section.

\begin{theorem}\label{nfgt1}
   Suppose Assumptions \ref{as1}, \ref{as2nonconvex} hold. Then Algorithm \ref{alg2} with $\gamma\leqslant\frac{1}{20L n}$ to reach $\varepsilon$-accuracy, where $\varepsilon^2 = \frac{1}{S}\sum\nolimits_{s=1}^{S} \|\nabla f(\omega_s)\|^2$, needs
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\varepsilon^2}\right)~~ \text{iterations and oracle calls.}
    \end{equation*}
\end{theorem}
%\vspace{-3mm}

Detailed proofs of the results obtained can be found in Appendix, Section \ref{nfgsvrg_appendix}. Let us delve into the result of this theorem. We obtained the first variance reduction method without calculating the full gradient and optimal on additional memory in the non-convex setting. Our results showed that the score is by an order of magnitude inferior, compared to the classical \textsc{SVRG} using independent sampling: $\mathcal{O}(n L)$ versus $\mathcal{O}(n^{\nicefrac{2}{3}}L)$ \citep{allen2016variance}. In a sense, this is payback for the fact that we approximate the full gradient at the reference point, rather than considering at the current state. As for the \textsc{Shuffle SVRG}, we replicate the current optimal estimate \citep{malinovsky2023random}. Taking into account that our method does not require the calculation of full gradients at all, it is valid and worth considering for the further research. Finally, the development of no-full-grad option for non-convex problems is a strong contribution, as this area has not been extensively explored before (Table \ref{table1}).

\subsubsection{Strongly convex setting}\label{subsection:svrgstronglyconvex}

Let us analyze this algorithm for the strongly convex case. Based on the proof of Theorem \ref{nfgt1}, we can construct an analysis for strongly convex setting, actively using Polyak-Lojasiewicz condition (see Appendix, Section \ref{sec:basicineq}). In that way, we present the following guarantees of convergence.
\begin{theorem}\label{nfgt2}
Suppose Assumptions \ref{as1}, \ref{as2stronglyconvex} hold. Then Algorithm \ref{alg2} with $\gamma\leqslant\frac{1}{20Ln}$ to reach $\varepsilon$-accuracy, where $\varepsilon = f(\omega_{S+1})-f(x^*)$, needs
   %\vspace{-3mm}
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\mu}\log \frac{1}{\varepsilon}\right)~~ \text{iterations and oracle calls.}
    \end{equation*}
\end{theorem}
Our results for the \textsc{No Full Grad SVRG} algorithm under strong convexity conditions are similar to those observed in non-convex settings. Furthermore, it is much better, than currently available estimates of no-full-grad methods. When comparing our results to those of other shuffling methods (as shown in Table \ref{table1}), it is evident that our algorithm improves convergence guarantees, while keeping optimal extra memory. In that way, it is a contribution to the whole class of shuffling algorithms.

\subsection{SARAH without full gradients}\label{subsection:sarahalgorithm}

We have earlier discussed that \textsc{SARAH} was designed as the variance reduction method, which outperforms \textsc{SVRG} on practice and has a lot of interesting practical applications \citep{nguyen2017sarah}. It would be a great omission on our part not to do an analysis for this method. This section discusses how to modify the \textsc{SARAH} method to avoid restarts. We consider two approaches: taking steps based on an accurate full gradient or developing a version of \textsc{SARAH} that does not require full gradient computations.

There also exists a version of \textsc{SARAH}, that obviates the need of the full gradient computation \citep{beznosikov2023random}, however its upper estimate, $\mathcal{O}\left(n^2\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)$, is far from the one we obtain for \textsc{SVRG} in the previous section. Let us show what we can change in their method to improve this estimate. The authors in the mentioned work used the same technique to approximate the full gradient as \eqref{eq:withoutfullgrad}. The algorithm used the standard \textsc{SARAH} update formula \eqref{eq:sarah} throughout each epoch. To initiate a new recursive cycle at the start of every epoch, an extra step was taken with an approximated full gradient. In this way, they also used the \textsc{SAG/SAGA} idea but provided a recursive reduced gradient update to avoid storing all stochastic gradients during the epoch.

To continue the analysis, we want to shade some light on the differences between \textsc{SAG} and \textsc{SAGA} algorithms, since it is crucial for our modifications. We have already discussed the \textsc{SAG} update \eqref{eq:sag}. The
\textsc{SAGA} update is almost the same, except for the absent factor $\frac{1}{n}$. Thus, maintaining the notation we used for \textsc{SAG}, we can write the SAGA step as
\begin{align}\label{eq:saga}
    &\scalebox{0.85}{$x^{t+1} = x^t - \gamma\left( \nabla f_{i_t}(x^t) - \nabla f_{i_t}(\phi_{i_t}^t) +  \frac{1}{n}\sum_{j=1}^n \nabla f_j(\phi_j^t) \right).$}
\end{align}
The key difference hides in the reduction of the variance of the \textsc{SAG} update in $n^2$ times compared to \textsc{SAGA} with the same $\phi$'s, however, the payback for such a gain is a non-zero bias in \textsc{SAG}. The choice towards unbiasedness of reduced operators was made in \textsc{SAGA} mainly in order to obtain a simple and tight theory for variance reduction methods and to yield theoretical estimates for proximal operators \citep{defazio2014saga}.

Now we can specify and say that the idea behind \textsc{SAGA} was applied in \citep{beznosikov2023random}. Nevertheless, attempting to increase variance for the sake of zero bias appears illogical here because the shuffling heuristic remains in use, thereby inherently introducing bias. As a result, achieving convergence requires very small step sizes, leading to significantly worse estimates. In contrast, we propose leveraging the concept of \textsc{SAG}, similar to what we did in \textsc{No Full Grad SVRG}, and modifying the \textsc{SARAH} update during each epoch, as
\begin{align*}
    v_s^{t+1} &= \frac{1}{n}\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) + v_s^{t-1}.
\end{align*}
This helps us to safely use bigger steps and improve convergence rates. We provide the formal description of the \textsc{No Full Grad SARAH} method (Algorithm \ref{alg:sarah}).

\vspace{-2mm}
\begin{algorithm}
\caption{\textsc{No Full Grad SARAH}}\label{alg:sarah}
\begin{algorithmic}[1]
    \State \textbf{Input:} Initial points $x_0^0\in\mathbb{R}^d$; Initial gradients $\widetilde{v}_0^0 = 0^d, v_0 = 0^d$
    \State \textbf{Parameter:} Stepsize $\gamma > 0$
    \For {epochs $s = 0, 1, 2, \ldots, S$}
    \State Sample a permutation $\pi^1_s, \dots, \pi^{n}_s$ of $\overline{1, n}$ \\ \scriptsize{\hspace{20.5mm} // Sampling depends on shuffling heuristic}
    \State $v_s^0 = v_s$
    \State $x_s^1 = x_s^0 - \gamma v_s^0$
    \For {$t = 1, 2, 3, \ldots, n$}
    \State \label{algsarah:line8} $\widetilde{v}_s^{t+1} = \frac{t-1}{t} \widetilde{v}_s^{t} + \frac{1}{t} \nabla f_{\pi_s^t}(x_s^t)$
    \State \label{algsarah:line9} $v_s^{t} = \frac{1}{n}\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) + v_s^{t-1}$
    \State \label{algsarah:line10} $x_s^{t+1} = x_s^t - \gamma v_s^t$
    \EndFor
    \State $x_{s+1}^0 = x_s^{n+1}$
    \State \label{algsarah:line13} $\widetilde{v}_{s+1}^1 = 0^d$
    \State $v_{s+1} = \widetilde{v}_s^{n+1}$
    \EndFor
\end{algorithmic}
\end{algorithm}
\vspace{-2mm}

One can observe that we slightly modify the coefficients in the full gradient approximation scheme (Line \ref{algsarah:line8} of Algorithm \ref{alg:sarah}) compared to Algorithm \ref{alg2}. The discrepancy stems solely from differences in indexing. In this case, the full gradient approximation begins at iteration $t=1$ rather than $t=0$, as we incorporate an extra restart step not present in \textsc{SVRG}. Therefore, to prevent the factor from becoming $\frac{1}{n+1}$ instead of $\frac{1}{n}$ in \eqref{eq:withoutfullgrad}, we make this adjustment. Now we proceed to the theoretical analysis of Algorithm \ref{alg:sarah} under both non-convex and strongly convex assumptions on the objective function.

\subsubsection{Non-convex setting}\label{subsection:sarahnonconvex}

During the proof of \textsc{SARAH}'s convergence estimates, we proceed analogously to how we did for \textsc{SVRG}. Initially, we focus on a single epoch and demonstrate convergence within it. To achieve this, we estimate the difference between the gradient at the start of the epoch and the average of reduced gradients used for updates throughout that epoch.
\begin{lemma}\label{l1:sarahmain}
    Suppose that Assumptions \ref{as1}, \ref{as2} hold. Then for Algorithm \ref{alg:sarah} a valid estimate is
    \begin{align*}
        \textstyle{\left\| \nabla f(x_s^0) - \frac{1}{n+1}\sum\nolimits_{t=0}^n v_s^t\right\|^2 \leqslant}& \textstyle{~2\|\nabla f(x_{s}^0) - v_s \|^2} \\
        & \textstyle{\hspace{-1mm}+ \frac{2L^2}{n+1}\sum\nolimits_{t=1}^n \|x_s^t - x_s^{t-1}\|^2.}
    \end{align*}
\end{lemma}
The first term is identical to the one we previously encountered in the analysis of Algorithm \ref{alg2}: the difference between the true full gradient and its approximation. Additionally, note that the second term conveys a similar meaning to its counterpart in Lemma \ref{ngl2}, representing the difference between current and reference points during an epoch (where the reference point is consistently set as the previous one). In that way, we proceed analogically to \textsc{SVRG} and move to the following lemma.
\begin{lemma}\label{l2:sarahmain}
    Suppose that Assumptions \ref{as1}, \ref{as2} hold. Let the stepsize $\gamma \leqslant \frac{1}{3L}$. Then for Algorithm \ref{alg:sarah} a valid estimate is
    \begin{eqnarray*}
            \left\| \nabla f(x_s^0) - \frac{1}{n+1}\sum\nolimits_{t=0}^n v_s^t\right\|^2 &\leqslant& 9\gamma^2L^2\|v_{s}\|^2 \\
            & & + 36\gamma^2L^2n^2\|v_{s-1}\|^2.
    \end{eqnarray*}
\end{lemma}
Obtaining this crucial lemma, we can now present the final result of this section.
\begin{theorem}\label{th1:sarahmain}
   Suppose Assumptions \ref{as1}, \ref{as2nonconvex} hold. Then Algorithm \ref{alg:sarah} with $\gamma\leqslant\frac{1}{20L(n+1)}$ to reach $\varepsilon$-accuracy, where $\varepsilon^2 = \frac{1}{S}\sum\nolimits_{s=1}^{S} \|\nabla f(x_s^0)\|^2$, needs
   %\vspace{-3mm}
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\varepsilon^2}\right) ~~ \text{iterations and oracle calls.}
    \end{equation*}
\end{theorem}

We obtain the expected result. Notably, the upper bound for the convergence of \textsc{No Full Grad SARAH} aligns with that of \textsc{No Full Grad SVRG}, as stated in Theorem \ref{nfgt1}. Our comparison with earlier estimates remains consistent and is detailed in Section \ref{subsection:svrgnonconvex}.

\subsubsection{Strongly convex setting}\label{subsection:sarahstringlyconvex}
We can extend our analysis on the strongly convex objective function, using \eqref{PL} (see Appendix, Section \ref{sec:basicineq} for details). We move straight to the final result.
\begin{theorem}\label{th2:sarahmain}
    Suppose Assumptions \ref{as1}, \ref{as2stronglyconvex} hold. Then Algorithm \ref{alg:sarah} with $\gamma\leqslant\frac{1}{20L(n+1)}$ to reach $\varepsilon$-accuracy, where $\varepsilon = f(x_{S+1}^0)-f(x^*)$, needs
   %\vspace{-3mm}
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\mu}\log \frac{1}{\varepsilon}\right)~~ \text{iterations and oracle calls.}
    \end{equation*}
\end{theorem}
The discussion of the result is the same as for \textsc{SVRG} (see Section \ref{subsection:svrgstronglyconvex}).


\section{Lower bounds}
A natural question arises: is Algorithm \ref{alg2} optimal? At least, we can consider its optimality in the non-convex case (in fact, we can also consider the strongly convex case, but the concept remains the same). A comprehensive explanation requires understanding the essence of \textit{smoothness} assumptions, which are used to study variance-reduced schemes.

As a result, we present the lower bound for non-convex finite-sum problem \eqref{eq:finite-sum} under Assumption \ref{as1}. What is more, we propose the explanation why it is impossible to construct the lower bound which matches the result of Theorem \ref{nfgt1}.
\begin{theorem}[\textbf{Lower bound}]
\label{thm:lower}
    For any $L > 0$ there exists a problem \eqref{eq:finite-sum} which satisfies Assumption \ref{as1}, such that for any output of first-order algorithm, number of oracle calls $N_c$ required to reach $\varepsilon$-accuracy is lower bounded as
    \begin{align*}
        N_c = \Omega\left(\frac{L\Delta}{\varepsilon^2}\right).
    \end{align*}
\end{theorem}
In fact, this result has already been stated (see Theorem 4.7 in \citep{zhou2019lower}), but in order to make it more convenient to perceive the lower bound, we have constructed it in a different form. Moreover, the interpretation of the obtained result (see Remark 4.8 in \citep{zhou2019lower}) is not completely correct. For example, the comparison with the upper bound from \citep{fang2018spider} is inconsistent since the smoothness parameters are considered different, and as a consequence, the problem classes \textit{do not coincide}.
\begin{theorem}[\textbf{Non-optimality}]
\label{thm: quest}
    For any $L > 0$ there is \textbf{no} problem \eqref{eq:finite-sum} which satisfies Assumption \ref{as1}, such that for any output of first-order algorithm, number of oracle calls $N_c$ required to reach $\varepsilon$-accuracy is lower bounded with $p > \frac{1}{2}$:
    \begin{align*}
        N_c = \Omega\left(\frac{n^pL\Delta}{\varepsilon^2}\right).
    \end{align*}
    \end{theorem}
This theorem means that the best result that can potentially be obtained in terms of lower bounds is $\Omega\left(\frac{\sqrt{n}L\Delta}{\varepsilon^2}\right)$. Therefore, the results for upper bound (Theorems \ref{nfgt1} and \ref{th1:sarahmain}) are non-optimal, and lower bound (Theorem \ref{thm:lower}) could be non-optimal in the class of problems inducted by Assumption \ref{as1}.
As a result, Theorem \ref{thm: quest} means that despite the superior performance compared to existing results (Table \ref{table1}), there remains a gap between the upper and lower bounds, which is an open question for research. For details see Section \ref{sec:lower_bound}.
%\vspace{-2mm}
\section{Experiments}\label{sec:experiments}

\textbf{ResNet-18 on CIFAR-10 classification.}
We address a classification task on the \textsc{CIFAR-10} dataset \citep{krizhevsky2009learning} using the \textsc{ResNet18} architecture \cite{he2016deep}, a standard benchmark model for evaluating optimization algorithms due to its balance of complexity and performance. To explore the robustness of the optimizers, we reformulate the standard minimization problem into a min-max optimization framework. Specifically, let 
$f(w, x, y)$  denote the loss function, where $w \in \mathbb{R}^d$ represents the model parameters, $x \in \R^n$ is the input, and  $y \in \R$ is the corresponding label. We consider the following optimization problem:
\begin{equation*}
    \min \limits_{w} \max \limits_{\sigma} \frac{1}{M} \sum \limits_{i = 1}^M f(w, x_i + \sigma, y_i) + \frac{\lambda_1}{2}\|w\|^2 - \frac{\lambda_2}{2}\|\sigma\|^2,
\end{equation*}
where $f$ is the cross-entropy loss function, $\sigma$ represents adversarial noise introduced to model data perturbations, and $\lambda_1, \lambda_2$ are regularization parameters. This formulation can be expressed as a variational inequality with:
\begin{equation*}
    z = \begin{pmatrix}
        w \\
        \sigma
    \end{pmatrix}, \quad F_i(z) = \begin{pmatrix}
        \nabla_w f(w, x_i + \sigma, y_i) + \lambda_1 w \\
        -\nabla_\sigma f(w, x_i + \sigma, y_i) + \lambda_2 \sigma
    \end{pmatrix}.
\end{equation*}

The results are presented in Figures \ref{fig:svrg_10} and \ref{fig:sarah_10}.

\begin{figure}[H]
\centering
\vspace{-0.4cm}
\begin{minipage}[][][b]{\columnwidth}
\centering
\resizebox{\columnwidth}{!}{%
\includegraphics[width=\columnwidth]{plots/svrg_10.pdf}
}
\end{minipage}
\vspace{-0.4cm}
\caption{\textsc{No Full Grad SVRG} and \textsc{SVRG}.}
\label{fig:svrg_10}
\end{figure}

\begin{figure}[H]
\centering
\vspace{-0.8cm}
\begin{minipage}[][][b]{\columnwidth}
\centering
\resizebox{\columnwidth}{!}{%
\includegraphics[width=\columnwidth]{plots/sarah_10.pdf}
}
\end{minipage}
\vspace{-0.4cm}
\caption{\textsc{No Full Grad SARAH} and \textsc{SARAH}.}
\vspace{-0.4cm}
\label{fig:sarah_10}
\end{figure}

The \textsc{No Full Grad SVRG} algorithm reduces training loss oscillations compared to \textsc{SGD}, particularly in low-diversity datasets. Despite batch fluctuations, convergence remains smooth. On the test set, \textsc{NFG SVRG} shows better loss reduction, and test accuracy surpasses SGD, stabilizing from epoch 150.

As for the \textsc{No Full Grad SARAH} algorithm, it ensures stable training loss convergence, surpassing the standard \textsc{SARAH} in speed in terms of the full gradient computations. On the test set, the loss reaches a similar minimum as SGD but continues decreasing, while SGD started to fluctuate. Final test loss is lower, and test accuracy improves gradually, accelerating in later stages.

Experiments on CIFAR-100 as well as the experiments on the least squares regression are presented in Appendix \ref{sec:additionalexp}.

\section*{Acknowledgments}

This work was partially done while D. Medyakov and G. Molodtsov were visiting research assistants in Mohamed bin Zayed University of Artificial Intelligence (MBZUAI).

\bibliography{refs}
\bibliographystyle{plainnat}
\addcontentsline{toc}{section}{References}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\onecolumn
\newpage

\title{Variance Reduction Methods Do Not Need to Compute Full Gradients: Improved Efficiency through Shuffling \\ (Supplementary Material)}
\maketitle

\appendix
% \section*{Contents}
% \newpage

\allowdisplaybreaks
%\tableofcontents

%\newpage 
\vspace{8mm}
\section{Additional experiments}\label{sec:additionalexp}

\subsection{Least squares regression.}
We consider the non-linear least squares loss problem:
%\vspace{-3mm}
\begin{equation} \label{eq:9}
   \textstyle{ f(x) = \frac{1}{n} \sum_{i=1}^{n} (y_i - h_i)^2,}
\end{equation} 
%\vspace{-5mm}
where $n$ is the number of samples, $y_i$ is the true value for sample $i$, $h_i$ is value for sample $i$, calculated as $ h_i = \frac{1}{1 + \exp(-z_i)}$, with $ z_i = A_i \cdot x $, addressing problem \eqref{eq:9}. Based on our theoretical estimates, which suggest inferior performance compared to standard \textsc{SVRG} and \textsc{SARAH}, we expect less favorable convergence. To address this limitation, we expand our investigation to examine the convergence of this method by tuning the stepsize, a topic that falls outside the scope of our current theoretical framework. The plots are shown in Figures \ref{fig:nfglog}-\ref{fig:nfglog2}.

\begin{figure}[H]
\centering
%\vspace{-0.4cm}
\begin{minipage}[][][b]{\columnwidth}
\centering
\resizebox{\columnwidth}{!}{%
\includegraphics[width=0.5\columnwidth]{plots/ijcnn1_svrg.pdf}
\includegraphics[width=0.5\columnwidth]{plots/a9a_svrg.pdf}
}
\end{minipage}
\caption{\textsc{No Full Grad SVRG} and \textsc{SVRG} convergence with theoretical and tuned step sizes on problem \eqref{eq:9} on the \texttt{ijcnn1} (left) and \texttt{a9a} (right) datasets.}
\label{fig:nfglog}
\end{figure}


\begin{figure}[H]
\centering
%\vspace{-0.4cm}
\begin{minipage}[][][b]{\columnwidth}
\centering
\resizebox{\columnwidth}{!}{%
\includegraphics[width=0.5\columnwidth]{plots/ijcnn1_sarah.pdf}
\includegraphics[width=0.5\columnwidth]{plots/a9a_sarah.pdf}
}
\end{minipage}
\caption{\textsc{No Full Grad SARAH} and \textsc{SARAH} convergence with theoretical and tuned step sizes on problem \eqref{eq:9} on the \texttt{ijcnn1} (left) and \texttt{a9a} (right) datasets.}
\label{fig:nfglog2}
\end{figure}

Upon examining the plots, we notice that although \textsc{No Full Grad} versions may converge slightly slower although comparable than its regular counterpart when using the theoretical step size, it significantly outperforms \textsc{SVRG} and \textsc{SARAH}, respectively, when the step size is optimally tuned. This highlights the potential of our method to achieve superior convergence rates with proper parameter adjustments, providing a robust alternative for large-scale optimization.


\subsection{ResNet-18 on CIFAR-10/CIFAR-100 classification.}

\subsubsection*{Optimizers and Experimental Design.}
 The experiments were implemented in Python using the PyTorch library \citep{paszke2019pytorch}, leveraging both a single CPU (Intel Xeon 2.20 GHz) and a single GPU (NVIDIA Tesla P100) for computation. To emulate a distributed environment, we split batches across multiple workers, simulating a decentralized optimization setting.

Our algorithms are evaluated in terms of accuracy and the number of full gradient computations. The experiments are conducted with the following setup:
\begin{itemize}
    \item number of workers $M = 5$;
    \item learning rate $\gamma = 0.01$ for both optimizers;
    \item regularization parameters $\lambda_1 = \lambda_2 = 0.0005$.
\end{itemize}

The experiments on CIFAR-100 are presented below.

The SVRG algorithm follows a similar trend, with test loss stabilizing instead of increasing, unlike SGD. While SGD rebounds, SVRG maintains a plateau before further improvement. Test accuracy surpasses SGD from epoch 50 onward.

\begin{figure}[H]
\centering
\vspace{-0.4cm}
\begin{minipage}[][][b]{0.8\textwidth}
\centering
\resizebox{\columnwidth}{!}{%
\includegraphics[width=0.8\textwidth]{plots/svrg_10.pdf}
}
\end{minipage}
\vspace{-0.4cm}
\caption{\textsc{No Full Grad SVRG} and \textsc{SVRG}  on CIFAR-100 convergence.}
\label{fig:svrg_100}
\end{figure}

The SARAH algorithm stabilizes training loss convergence and outpaces standard SARAH. Test loss decreases beyond SGD’s minimum, leading to a lower final loss. Test accuracy initially rises slowly but later accelerates, reducing overfitting.

\begin{figure}[H]
\centering
\vspace{-0.4cm}
\begin{minipage}[][][b]{0.8\textwidth}
\centering
\resizebox{\columnwidth}{!}{%
\includegraphics[width=0.8\textwidth]{plots/sarah_10.pdf}
}
\end{minipage}
\vspace{-0.4cm}
\caption{\textsc{No Full Grad SARAH} and \textsc{SARAH} on CIFAR-100 convergence.}
\label{fig:sarah_100}
\end{figure}



% \subsection{Image denoising}
% We consider the problem of image denoising \citep{chambolle2011first} as an applied example of using the shuffling techniques. Indeed, the shuffling technique is a key one in such a task because, by splitting the image into butches, we want to go through all the butches for each epoch. If we use independent sampling of indexes, the image processing may turn out to be unequal. We present a minimization problem suitable for denoising:
% %\vspace{-4mm}
% \begin{equation}\label{eq:denoising}
%     f(u) = \left[h^2\|\nabla u\|_1 + \frac{\lambda}{2}\|u - g\|^2_2\right].
% \end{equation}
% %\vspace{-2mm}
% In this setting, we consider regular Cartesian grid of size $M\times N$: $\{(h \cdot i, h \cdot j): 1\leqslant i\leqslant M, 1\leqslant j\leqslant N\}$; $u, g$ are an unknown solution and the given noisy data; h denotes the size of the spacing; $\|\cdot\|_1$ is $l_1$ norm; $\nabla u$ is defined as $(\nabla u)_{i, j} = \begin{pmatrix}
%   (\nabla u)_{i, j}^1\\
%   (\nabla u)_{i, j}^2
% \end{pmatrix}$,
% where 
% \vspace{-2mm}
% \begin{align*}
% (\nabla u)_{i, j}^1 &= \begin{cases}
%     \frac{u_{i+1,j} - u_{i, j}}{h}, &\text{~~, if~~} i < M\\
%     0 &\text{~~, if~~} i = M
% \end{cases};\\
% (\nabla u)_{i, j}^2 &= \begin{cases}
%     \frac{u_{i,j+1} - u_{i, j}}{h} &\text{~~, if~~} j < N\\
%     0 &\text{~~, if~~} j = N
% \end{cases}.
% \end{align*}
% For this problem, we have selected two images with different additive zero mean Gaussian noise ($\sigma$). We compare the performance of \textsc{SVRG} with independent choice and shuffling heuristics on them. The results are shown in Figure \ref{fig:denoising}.

% \begin{figure}[ht]
% \centering
% \vspace{-0.4cm}
% \begin{minipage}[][][b]{\textwidth}
% \resizebox{\textwidth}{!}{%
% \centering
% \includegraphics[width=0.48\textwidth]{plots/girl_conv.jpg}
% \includegraphics[width=0.48\textwidth]{plots/pyramid_conv.jpg}
% }
% \resizebox{\textwidth}{!}{%
% \centering
% \includegraphics[width=0.24\textwidth]{plots/girl_noise.jpg}
% \includegraphics[width=0.24\textwidth]{plots/girl_denoise.jpg}
% \includegraphics[width=0.24\textwidth]{plots/pyramid_noise.jpg}
% \includegraphics[width=0.24\textwidth]{plots/pyramid_denoise.jpg}
% }
% \end{minipage}
% \captionof{figure}{Comparison of \textsc{SVRG} convergence with independent choice and shuffling heuristics on the problem \eqref{eq:denoising}. The left column shows convergence results on the image with $\sigma = 0.05$, as well as the noisy image and its processed version. The right column shows the same for the image with $\sigma = 0.1$.}
% \label{fig:denoising}
% \end{figure}

% %\vspace{-4mm}
% These plots show us the superiority of shuffling over the classical stochastic optimization independent index selection for this task.

\newpage
\section{General Inequalities}\label{sec:basicineq}
We introduce important inequalities that are used in further proofs. Let $f$ adhere to \text{Assumption~\ref{as1}}, $g$ adhere to \text{Assumption~\ref{as2stronglyconvex}}. Then for any real number $i$ and for all vectors $x, y, \{x_i\}\in\mathbb{R}^d$ with a positive scalars $\alpha, \beta$, the following inequalities hold:
\begin{align}
\label{ineq3} \tag{Scalar} 2\langle x, y \rangle & \leqslant \frac{\|x\|^2}{\alpha} + \alpha \|y\|^2, \\
\label{ineq:norm} \tag{Norm} 2\langle x, y \rangle & = \|x + y\|^2 - \|x\|^2 - \|y\|^, \\
\label{ineq:square} \tag{Quad} \|x + y\|^2 & \leqslant (1 + \beta)\|x\|^2 + (1 + \frac{1}{\beta})\|y\|^2, \\
\label{ineq4} \tag{Lip} f(x) & \leqslant f(y) + \langle \nabla f(y), x-y \rangle + \frac{L}{2} \|x-y\|^2,\\
\label{ineq1} \tag{CS}  \left\|\sum_{i=1}^{n} x_i\right\|^2 & \leqslant  n \sum_{i=1}^{n} \|x_i\|^2  \quad \quad (\text{Cauchy-Schwarz}),\\
\label{PL} \tag{PL}  g(x) - \inf g  &\leqslant \frac{1}{2\mu} \|\nabla g(x)\|^2 \quad \quad (\text{Polyak-Lojasiewicz}).
\end{align}
Here, \eqref{ineq4} was derived in \citep{nesterov2018lectures} in Theorem 2.1.5.

\newpage
\section{No Full Grad SVRG}\label{nfgsvrg_appendix}

For the convenience of the reader, we provide here the short description of Algorithm \ref{alg2}. If we consider it in epoch $s \neq 0$, one can note that the update rule is nothing but
\begin{align}
\label{svrg:update}
\begin{split}
\begin{cases}
    &\text{initial initialization:}\\
    &\quad\omega_s = x_s^0 = x_{s-1}^n\\
    &\quad v_s = \frac{1}{n}\sum\limits_{t = 0}^{n-1} \nabla f_{\pi_{s-1}^t} (x_{s-1}^t)\\
    &\text{for all iterations during the epoch}:\\
    &\quad v_s^t = \nabla f_{\pi_{s}^t} (x_s^t) - \nabla f_{\pi_{s}^t}(\omega_s) + v_s\\
    &\quad x_s^{t+1} = x_s^t - \gamma v_s^t
\end{cases}
\end{split}
\end{align}
\subsection{Non-convex setting}
\begin{lemma}\label{lemma1}
    Suppose that Assumptions \ref{as1}, \ref{as2} hold. Let the stepsize $\gamma \leqslant \frac{1}{Ln}$. Then for Algorithm \ref{alg2} it holds
    \begin{equation*}
        f(\omega_{s+1}) \leqslant f(\omega_s) - \frac{\gamma n}{2}\|\nabla f(\omega_s)\|^2 + \frac{\gamma n}{2}\left\|\nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2.
    \end{equation*}
    \begin{proof}
    Using the iteration of Algorithm \ref{alg2} \eqref{svrg:update}, we have
        \begin{align*}
            f(\omega_{s+1}) &~= f(\omega_s - (\omega_s - \omega_{s+1})) \\
            &\overset{\eqref{ineq4}}{\leqslant} f(\omega_s) + \langle \nabla f(\omega_s), \omega_{s+1} - \omega_s\rangle + \frac{L}{2}\|\omega_{s+1} - \omega_s\|^2 \\
            & ~= f(\omega_s) - \gamma n\left\langle \nabla f(\omega_s), \frac{1} n\sum\limits_{t=0}^{n-1} v_s^t\right\rangle  + \frac{\gamma^2n^2L}{2}\left\|\frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 \\
            &~ \overset{\eqref{ineq:norm}}{=} f(\omega_s) - \frac{\gamma n}{2}\left[\|\nabla f(\omega_s)\|^2 + \left\|\frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 - \left\|\nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 \right] \\
            & \quad ~+ \frac{\gamma^2n^2L}{2}\left\|\frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2  \\
            &~ = f(\omega_s) - \frac{\gamma n}{2}\left[\|\nabla f(\omega_s)\|^2 - \left\|\nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 \right] \\
            & \quad ~- \frac{\gamma n}{2} \cdot \left(1 - \gamma nL \right)\left\|\frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2,
        \end{align*}
    Choosing \(\gamma : \frac{\gamma n}{2}\left(1 - \gamma nL\right) > 0 \) and note that such a choice is followed by \( \gamma\leqslant \frac{1}{Ln}\). In that way, we make the last term is negative and obtain the result of the lemma.  
    \end{proof}
\end{lemma}

Now we want to address the last term in the inequality of Lemma \ref{lemma1}. We prove the following lemma.

\begin{lemma}[\textbf{Lemma \ref{ngl2}}]\label{lemma2}
Suppose that Assumptions \ref{as1}, \ref{as2} hold. Then for Algorithm \ref{alg2} a valid estimate is
    \begin{align*}
    \left\| \nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 &\leqslant 2\|\nabla f(\omega_{s}) - v_s \|^2 + \frac{2L^2}{n}\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2.
    \end{align*}

\begin{proof}
We straightforwardly move to estimate of the desired norm:
\begin{align}
    \notag\left\| \nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 &\overset{\eqref{svrg:update}}{=} \left\|\nabla f(\omega_s) - \frac{1}{n}\left(nv_s + \sum\limits_{t=0}^{n-1}\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(\omega_s)\right)\right)\right\|^2\\
    \notag & \overset{\eqref{ineq1}}{\leqslant} 2\|\nabla f(\omega_s) - v_s\|^2 + \frac{2}{n^2}\left\|\sum\limits_{t=0}^{n-1}\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(\omega_s)\right)\right\|^2\\
    \notag & \overset{\eqref{ineq1}}{\leqslant} 2\|\nabla f(\omega_s) - v_s\|^2 + \frac{2}{n} \sum\limits_{t=0}^{n-1}\left\|\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(\omega_s)\right\|^2\\
    & \overset{\text{Ass. \ref{as1}}}{\leqslant} 2\|\nabla f(\omega_s) - v_s\|^2 + \frac{2L^2}{n} \sum\limits_{t=0}^{n-1}\left\|x_s^t - \omega_s\right\|^2,
\end{align}
which ends the proof.
\end{proof}    
\end{lemma}

\begin{lemma}[\textbf{Lemma \ref{ngl3}}]\label{lemma3}
Suppose that Assumptions \ref{as1}, \ref{as2} hold. Let the stepsize $\gamma \leqslant \frac{1}{2Ln}$. Then for Algorithm \ref{alg2} a valid estimate is
\begin{align*}
        \left\| \nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 & \leqslant 8\gamma^2L^2n^2\|v_{s}\|^2 + 32\gamma^2L^2n^2\|v_{s-1}\|^2.
\end{align*}
\begin{proof}
        To begin with, in Lemma \ref{lemma2}, we obtain
        \begin{equation}
        \label{l3:ineq1}
            \left\| \nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 \leqslant 2\|\nabla f(\omega_{s}) - v_s \|^2 + \frac{2L^2}{n}\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2.
        \end{equation}
        Let us show what $v_s$ is (here we use Line  \ref{alg2:line6} of Algorithm \ref{alg2}):
        \begin{align}
            \notag v_s & = \widetilde{v}_{s-1}^{n} = \frac{n-1}{n} \widetilde{v}_{s-1}^{n-1} + \frac{1}{n}\nabla f_{\pi_{s-1}^{n-1}} (x_{s-1}^{n-1}) \\
            \notag & = \frac{n-1}{n}\cdot\frac{n-2}{n-1} \widetilde{v}_{s-1}^{n-2} + \frac{n-1}{n}\cdot\frac{1}{n-1}\nabla f_{\pi_{s-1}^{n-2}}(x_{s-1}^{n-2})
            + \frac{1}{n}\nabla f_{\pi_{s-1}^{n-1}} (x_{s-1}^{n-1})\\
            \notag& = \frac{n-1}{n}\cdot\frac{n-2}{n-1}\cdot\ldots\cdot 0\cdot \widetilde{v}_{s-1}^0 + \frac{1}{n}\sum\limits_{t = 0}^{n-1}\nabla f_{\pi_{s-1}^t}(x_{s-1}^t)\\
            \label{l3:ineq2}& \overset{(i)}{=} \frac{1}{n}\sum\limits_{t = 0}^{n-1}\nabla f_{\pi_{s-1}^t}(x_{s-1}^t),
        \end{align}
        where equation (\textit{i}) is correct due to initialization $\widetilde{v}_{s-1}^0 = 0$ (Line \ref{alg2:line12} of Algorithm \ref{alg2}). In that way, using \eqref{l3:ineq1} and \eqref{l3:ineq2},
        \begin{align*}
            \left\| \nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 & \leqslant 2\left\|\nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t = 0}^{n-1}\nabla f_{\pi_{s-1}^t}(x_{s-1}^t) \right\|^2 \\
            & ~~+ \frac{2L^2}{n}\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2.
            \end{align*}
            Then, using \eqref{eq:finite-sum},
            \begin{align}
            \notag \left\| \nabla f(\omega_s) - \frac{1}{n}\sum\limits_{t=0}^{n-1} v_s^t\right\|^2 & \leqslant ~~2\left\|\frac{1}{n}\sum\limits_{t = 0}^{n-1}\left(\nabla f_{\pi_{s-1}^t}(\omega_{s}) - \nabla f_{\pi_{s-1}^t}(x_{s-1}^t) \right)\right\|^2 + \frac{2L^2}{n}\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2 \\
            \notag & \overset{\eqref{ineq1}}{\leqslant} \frac{2}{n}\sum\limits_{t=0}^{n-1}\|\nabla f_{\pi_{s-1}^t}(\omega_{s}) - \nabla f_{\pi_{s-1}^t}(x_{s-1}^t)\|^2 + \frac{2L^2}{n}\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2 \\
            \notag & \overset{\text{Ass. \ref{as1}}}{\leqslant} \frac{2L^2}{n}\sum\limits_{t=0}^{n-1}\|x_{s-1}^t - \omega_s\|^2 + \frac{2L^2}{n}\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2 \\
            \notag & \overset{\eqref{ineq:square}}{\leqslant} \frac{4L^2}{n}\sum\limits_{0}^{n-1}\|x_{s-1}^t - \omega_{s-1}\|^2 + \frac{4L^2}{n}\sum\limits_{t=0}^{n-1}\|\omega_s - \omega_{s-1}\|^2  \\
            \label{l3:ineq3}& ~~+ \frac{2L^2}{n}\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2.
        \end{align}
        Now we have to bound these three terms. Let us begin with $\sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2$.
        \begin{align*}
            \sum\limits_{t=0}^{n-1} \|x_s^t - \omega_s\|^2 &= \gamma^2\sum\limits_{t=0}^{n-1} \left\|\sum\limits_{k = 0}^{t-1} v_s^k\right\|^2 \overset{\eqref{svrg:update}}{=}\gamma^2\sum\limits_{t=0}^{n-1} \left\|tv_s + \sum\limits_{k = 0}^{t-1} \left(\nabla f_{\pi_s^k}(x_s^k) - \nabla f_{\pi_s^k}(\omega_s)\right)\right\|^2\\
            &\overset{\eqref{ineq1}}{\leqslant} 2\gamma^2\sum\limits_{t=0}^{n-1}t^2\|v_s\|^2 + 2\gamma^2\sum\limits_{t=0}^{n-1}t\sum\limits_{k = 0}^{t-1}\|\nabla f_{\pi_s^k}(x_s^k) - \nabla f_{\pi_s^k}(\omega_s)\|^2\\
            &\overset{\text{Ass. \ref{as1}}}{\leqslant} 2\gamma^2n^3\|v_s\|^2 + 2\gamma^2L^2n\sum\limits_{t=0}^{n-1}\sum\limits_{k=0}^{t-1}\|x_s^k - \omega_s\|^2\\
            &~~\leqslant ~~2\gamma^2n^3\|v_s\|^2 + 2\gamma^2L^2n^2\sum\limits_{t=0}^{n-2}\|x_s^t - \omega_s\|^2\\
            &~~\leqslant ~~2\gamma^2n^3\|v_s\|^2 + 2\gamma^2L^2n^2\sum\limits_{t=0}^{n-1}\|x_s^t - \omega_s\|^2.
        \end{align*}
        Expressing $\sum\limits_{t = 0}^{n-1}\|x_s^t - \omega_s\|^2$ from here, we get
        \begin{equation*}
            \sum\limits_{t = 0}^{n-1}\|x_s^t - \omega_s\|^2 \leqslant \frac{2\gamma^2 n^3\|v_s\|^2}{1 - 2\gamma^2L^2n^2}.
        \end{equation*}
         To finish this part of proof it remains for us to choose appropriate $\gamma$. In Lemma \ref{lemma1} we require $\gamma \leqslant\frac{1}{Ln}$. There we choose smaller values of $\gamma: \gamma\leqslant\frac{1}{2Ln}$ (with that values all previous transitions is correct). Now we provide final estimation of this norm:
        \begin{align}
        \label{l3:ineq4}
            \sum\limits_{t=0}^{n-1}\|x_s^t - \omega_s\|^2 \leqslant 4\gamma^2n^3\|v_s\|^2.
        \end{align}
        One can note the boundary of the $\sum\limits_{t = 0}^{n-1}\|x_{s-1}^t - \omega_{s-1}\|^2$ term is similar because it involves the same sum of norms from the previous epoch.
        \begin{align}
        \label{l3:ineq5}
            \sum\limits_{t = 0}^n\|x_{s-1}^t - \omega_{s-1}\|^2 \leqslant 4\gamma^2n^3\|v_{s-1}\|^2.
        \end{align}
        It remains for us to estimate the $\sum\limits_{t=0}^{n-1}\|\omega_s - \omega_{s-1}\|^2$ term.
        \begin{align*}
            \sum\limits_{t=0}^{n-1} \|\omega_s - \omega_{s-1}\|^2 & ~~=~~ \gamma^2\sum\limits_{t=0}^{n-1} \left\|\sum\limits_{k = 0}^{n-1} v_{s-1}^k\right\|^2 \\
            & ~~\overset{\eqref{svrg:update}}{=}~~\gamma^2\sum\limits_{t=0}^{n-1} \left\|nv_{s-1} + \sum\limits_{k = 0}^{n-1} \left(\nabla f_{\pi_{s-1}^k}(x_{s-1}^k) - \nabla f_{\pi_{s-1}^k}(\omega_{s-1})\right)\right\|^2\\
            &~~\overset{\eqref{ineq1}}{\leqslant}~~ 2\gamma^2\sum\limits_{t=0}^{n-1}n^2\|v_{s-1}\|^2 + 2\gamma^2\sum\limits_{t=0}^{n-1}n\sum\limits_{k = 0}^{n-1}\|\nabla f_{\pi_{s-1}^k}(x_{s-1}^k) - \nabla f_{\pi_{s-1}^k}(\omega_{s-1})\|^2\\
            &\overset{\text{Ass. \ref{as1}}}{\leqslant}~~ 2\gamma^2n^3\|v_{s-1}\|^2 + 2\gamma^2L^2n\sum\limits_{t=0}^{n-1}\sum\limits_{k=0}^{t-1}\|x_{s-1}^k - \omega_{s-1}\|^2\\
            &\quad\leqslant ~~2\gamma^2n^3\|v_{s-1}\|^2 + 2\gamma^2L^2n^2\sum\limits_{t=0}^{n-2}\|x_{s-1}^t - \omega_{s-1}\|^2\\
            &\quad\leqslant ~~2\gamma^2n^3\|v_{s-1}\|^2 + 2\gamma^2L^2n^2\sum\limits_{t=0}^{n-1}\|x_{s-1}^t - \omega_{s-1}\|^2\\
            &\quad\overset{\eqref{l3:ineq5}}{\leqslant}~~ 2\gamma^2n^3\|v_{s-1}\|^2 + 8\gamma^4L^2n^5\sum\limits_{t=0}^{n-1}\|x_{s-1}^t - \omega_{s-1}\|^2.
        \end{align*}
        Using our choice $\gamma\leqslant\frac{1}{2Ln}$, we derive the estimate of the last term:
        \begin{align}\label{l3:ineq6}
            \sum\limits_{t=0}^{n-1} \|\omega_s - \omega_{s-1}\|^2 \leqslant 2\gamma^2n^3\|v_{s-1}\|^2 + 2\gamma^2n^3\|v_{s-1}\|^2 = 4\gamma^2n^3\|v_{s-1}\|^2.
        \end{align}
        Now we can apply the upper bounds obtained in \eqref{l3:ineq4} -- \eqref{l3:ineq6} to \eqref{l3:ineq3} and have
        \begin{align*}
            \left\| \nabla f(\omega_s) - \frac{1}{n} \sum\limits_{t=0}^{n-1} v_s^t\right\|^2 & \leqslant 8\gamma^2L^2n^2\|v_{s}\|^2 + 16\gamma^2L^2n^2\|v_{s-1}\|^2 + 16\gamma^2L^2n^2\|v_{s-1}\|^2 \\
            & = 8\gamma^2L^2n^2\|v_{s}\|^2 + 32\gamma^2L^2n^2\|v_{s-1}\|^2,
        \end{align*}
        which ends the proof.
        \end{proof}
\end{lemma}

\begin{theorem}[\textbf{Theorem \ref{nfgt1}}]
Suppose Assumptions \ref{as1}, \ref{as2nonconvex} hold. Then Algorithm \ref{alg2} with $\gamma\leqslant\frac{1}{20L n}$ to reach $\varepsilon$-accuracy, where $\varepsilon^2 = \frac{1}{S}\sum\limits_{s=1}^{S} \|\nabla f(\omega_s)\|^2$, needs
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\varepsilon^2}\right)~~ \text{iterations and oracle calls.}
    \end{equation*}
               \begin{proof}
        We combine the result of Lemma \ref{lemma1} with the result of Lemma \ref{lemma3} and obtain
        \begin{align*}
            f(\omega_{s+1}) &\leqslant f(\omega_s) - \frac{\gamma n}{2}\|\nabla f(\omega_s)\|^2 \\
            & \quad + \frac{\gamma n}{2}\left(8\gamma^2L^2n^2\|v_{s}\|^2 + 32\gamma^2L^2n^2\|v_{s-1}\|^2\right).
        \end{align*}
        We subtract $f(x^*)$ from both parts:
        \begin{align*}
            f(\omega_{s+1}) - f(x^*) &\leqslant f(\omega_s) - f(x^*) - \frac{\gamma n}{2}\|\nabla f(\omega_s)\|^2 \\
            & \quad + \frac{\gamma n}{2}\left(8\gamma^2L^2n^2\|v_{s}\|^2 + 32\gamma^2L^2n^2\|v_{s-1}\|^2\right)\\
            & = f(\omega_s) - f(x^*) - \frac{\gamma n}{4}\|\nabla f(\omega_s)\|^2 \\
            & \quad + \frac{\gamma n}{2}\left(8\gamma^2L^2n^2\|v_{s}\|^2 + 32\gamma^2L^2n^2\|v_{s-1}\|^2\right) \\
            & \quad - \frac{\gamma n}{4}\|\nabla f(\omega_s)\|^2.
            \end{align*}
       Then, transforming the last term by using \eqref{ineq:square} with $\beta=1$, we get
        \begin{align*}
            f(\omega_{s+1}) - f(x^*) &\leqslant f(\omega_s) - f(x^*) - \frac{\gamma n}{4}\|\nabla f(\omega_s)\|^2 \\
           & \quad  + \frac{\gamma n}{2}\left(8\gamma^2L^2n^2\|v_{s}\|^2 + 32\gamma^2L^2n^2\|v_{s-1}\|^2\right) \\
            & \quad- \frac{\gamma n}{8}\|v_s\|^2 + \frac{\gamma n}{4}\|v_s - \nabla f(x
            \omega_s)\|^2.
            \end{align*}
        Using Lemma \ref{lemma3} to $\|v_s - \nabla f(\omega_s)\|^2$ (specially $\frac{4L^2}{n}\cdot\eqref{l3:ineq5} + \frac{4L^2}{n}\cdot\eqref{l3:ineq6}$),
        \begin{align*}
            f(\omega_{s+1}) - f(x^*) &\leqslant f(\omega_s) - f(x^*) - \frac{\gamma n}{4}\|\nabla f(\omega_s)\|^2 \\
            & \quad + \frac{\gamma n}{2}\left(8\gamma^2L^2n^2\|v_{s}\|^2 + 32\gamma^2L^2n^2\|v_{s-1}\|^2\right) \\
            & \quad- \frac{\gamma n}{8}\|v_s\|^2 + \frac{\gamma n}{4}\cdot 32\gamma^2L^2n^2\|v_{s-1}\|^2.
        \end{align*}
        Combining alike expressions,
        \begin{align}
            \notag f(\omega_{s+1}) - f(x^*) + \frac{\gamma n}{4}\|\nabla f(\omega_s)\|^2 &\leqslant f(\omega_s) - f(x^*) - \frac{\gamma n}{8}\left(1 - 32 \gamma^2L^2n^2\right)\|v_s\|^2 \\ 
            \label{t1:ineq1}& \quad + \gamma n\cdot 24\gamma^2L^2n^2\|v_{s-1}\|^2. 
        \end{align}
        Using $\gamma \leqslant \frac{1}{20Ln}$ (note it is the smallest stepsize from all the steps we used before, so all previous transitions are correct), we get
        \begin{align*}
            f(\omega_{s+1}) - f(x^*) &+ \frac{1}{10}\gamma n\|v_s\|^2 + \frac{\gamma(n+1)}{4}\|\nabla f(\omega_s)\|^2\\
            &\leqslant f(\omega_s) - f(x^*) + \frac{1}{10}\gamma n\|v_{s-1}\|^2.
        \end{align*}
        Next, denoting $\Delta_{s} = f(\omega_{s+1}) - f(x^*) + \frac{1}{10}\gamma n\|v_{s}\|^2$, we obtain
        \begin{equation*}
            \frac{1}{S}\sum\limits_{s=1}^{S} \|\nabla f(\omega_s)\|^2 \leqslant \frac{4\left[\Delta_0 - \Delta_{S}\right]}{\gamma nS}.
        \end{equation*}    
        We choose $\varepsilon^2 = \frac{1}{S}\sum\limits_{s=1}^{S} \|\nabla f(\omega_s)\|^2$ as criteria. Hence, to reach $\varepsilon$-accuracy we need $\mathcal{O}\left(\frac{L}{\varepsilon^2}\right)$ epochs and $\mathcal{O}\left(\frac{nL}{\varepsilon^2}\right)$ iterations. Additionally, we note that the oracle complexity of our algorithm is also equal to $\mathcal{O}(\frac{nL}{\varepsilon^2})$, since at each iteration the algorithm computes the stochastic gradient at only two points. This ends the proof.
\end{proof}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strongly convex setting} 
\begin{theorem}[\textbf{Theorem \ref{nfgt2}}]\label{theorem2}
Suppose Assumptions \ref{as1}, \ref{as2stronglyconvex} hold. Then Algorithm \ref{alg2} with $\gamma\leqslant\frac{1}{20Ln}$ to reach $\varepsilon$-accuracy, where $\varepsilon = f(x_{S+1}^0)-f(x^*)$, needs
   %\vspace{-3mm}
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\mu}\log \frac{1}{\varepsilon}\right)~~ \text{iterations and oracle calls.}
    \end{equation*}
        
\begin{proof}
Under Assumption \ref{as2stronglyconvex}, which states that the function is strongly convex, the \eqref{PL} condition is automatically satisfied. Therefore, 
\begin{align*}
    f(\omega_{s+1}) - f(x^*) + \frac{\gamma\mu n}{2}\left( f(\omega_s) - f(x^*)\right) &\leqslant  f(\omega_{s+1}) - f(x^*) + \frac{\gamma n}{4}\|\nabla f(\omega_s)\|^2.
\end{align*}
Thus, using \eqref{t1:ineq1},
\begin{align*}
f(\omega_{s+1}) - f(x^*) &+ \frac{\gamma\mu n}{2}\left( f(\omega_s) - f(x^*)\right) \leqslant f(\omega_s) - f(x^*) \\
& - \frac{\gamma n}{8}\left(1 - 32\gamma^2L^2n^2\right)\|v_s\|^2 + \gamma n\cdot 24\gamma^2L^2n^2\|v_{s-1}\|^2.
\end{align*}
Using $\gamma \leqslant \frac{1}{20L(n+1)}$ and assuming $n \geqslant 2$, we get
\begin{align*}
    f(\omega_{s+1}) - f(x^*) + \frac{1}{10}\gamma n\|v_s\|^2
    &\leqslant \left(1-\frac{\gamma\mu n}{2}\right)\left(f(\omega_s) - f(x^*)\right) \\
    & \quad + \frac{1}{10}\gamma n\cdot \left(1-\frac{\gamma\mu n}{2}\right)\|v_{s-1}\|^2.
\end{align*}
Next, denoting $\Delta_{s} = f(\omega_{s+1}) - f(x^*) + \frac{1}{10}\gamma n\|v_{s}\|^2$, we obtain the final convergence over one epoch:
\begin{align*}
    \Delta_{s+1} \leqslant \left(1-\frac{\gamma\mu n}{2}\right) \Delta_s.
\end{align*}
Going into recursion over all epoch,
\begin{align*}
    f(\omega_{S+1}) - f(x^*)\leqslant\Delta_{S} \leqslant \left(1-\frac{\gamma\mu n}{2}\right)^{S+1}\Delta_0.
\end{align*}
We choose $\varepsilon = f(\omega_{S+1})-f(x^*)$ as criteria. Then to reach $\varepsilon$-accuracy we need $\mathcal{O}\left(\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)$ epochs and $\mathcal{O}\left(\frac{nL}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)$ iterations. Additionally, we note that the oracle complexity of our algorithm is also equal to $\mathcal{O}\left(\frac{nL}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)$, since at each iteration the algorithm computes the stochastic gradient at only two points.
\end{proof} 
\end{theorem}
    
\section{No Full Grad SARAH}\label{nfgsarah_appendix}

For the convenience of the reader, we provide here the short description of Algorithm \ref{alg:sarah}. If we consider it in epoch $s \neq 0$, one can note that the update rule is nothing but
\begin{align}
\label{sarah:update}
\begin{split}
\begin{cases}
    &\text{if iteration~} t=0:\\
    &\quad x_s^0 = x_{s-1}^{n}\\
    &\quad v_s^0 = v_s = \frac{1}{n}\sum\limits_{t = 1}^n \nabla f_{\pi_{s-1}^t} (x_{s-1}^t)\\
    &\quad x_s^1 = x_s^0 - \gamma v_s^0\\
    &\text{for rest iterations during the epoch}:\\
    &\quad v_s^t = v_s^{t-1} + \frac{1}{n} \left(\nabla f_{\pi_{s}^t} (x_s^t) - \nabla f_{\pi_{s}^t} (\omega_s)\right)\\
   &\quad x_s^{t+1} = x_s^t - \gamma v_s^t
\end{cases}
\end{split}
\end{align}
\subsection{Non-convex setting}
\begin{lemma}\label{lemma4}
    Suppose that Assumptions \ref{as1}, \ref{as2} hold. Let the stepsize $\gamma \leqslant \frac{1}{L(n+1)}$. Then for Algorithm \ref{alg:sarah} it holds
    \begin{equation*}
        f(x_{s+1}^0) \leqslant f(x_s^0) - \frac{\gamma(n+1)}{2}\|\nabla f(x_s^0)\|^2 + \frac{\gamma(n+1)}{2}\left\|\nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{i=0}^n v_s^i\right\|^2.
    \end{equation*}
    \begin{proof}
    Using the iteration of Algorithm \ref{alg:sarah} \eqref{sarah:update}, we have
        \begin{align*}
            f(x_{s+1}^0) &~= f(x_s^0 - (x_s^0 - x_{s+1}^0)) \\
            &\overset{\eqref{ineq4}}{\leqslant} f(x_s^0) + \langle \nabla f(x_s^0), x_{s+1}^0 - x_s^0\rangle + \frac{L}{2}\|x_{s+1}^0 - x_s^0|^2 \\
            & ~= f(x_s^0) - \gamma(n+1)\left\langle \nabla f(x_s^0), \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\rangle  + \frac{\gamma^2(n+1)^2L}{2}\left\|\frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 \\
            &~ \overset{\eqref{ineq:norm}}{=} f(x_s^0) - \frac{\gamma(n+1)}{2}\left[\|\nabla f(x_s^0)\|^2 + \left\|\frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 - \left\|\nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 \right] \\
            & \quad ~+ \frac{\gamma^2(n+1)^2L}{2}\left\|\frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2  \\
            &~ = f(x_s^0) - \frac{\gamma(n+1)}{2}\left[\|\nabla f(x_s^0)\|^2 - \left\|\nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 \right] \\
            & \quad ~- \frac{\gamma(n+1)}{2} \cdot \left(1 - \gamma(n+1)L \right)\left\|\frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2.
        \end{align*}
    It remains for us to choose \(\gamma : \frac{\gamma(n+1)}{2}\left(1 - \gamma(n+1)L\right) > 0 \) and note that such a choice is followed by \( \gamma\leqslant \frac{1}{L(n+1)}\). In that way we make the last term is negative and obtain the result of the lemma. 
    \end{proof}
\end{lemma}

Now we want to address the last term in the result of Lemma \ref{lemma4}. We prove the following lemma.

\begin{lemma}[\textbf{Lemma \ref{l1:sarahmain}}]\label{lemma5}
Suppose that Assumptions \ref{as1}, \ref{as2} hold. Then for Algorithm \ref{alg:sarah} a valid estimate is
\begin{align*}
    \left\| \nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 &\leqslant 2\|\nabla f(x_{s}^0) - v_s \|^2 + \frac{2L^2}{n+1}\sum\limits_{t=1}^n \|x_s^t - x_s^{t-1}\|^2.
\end{align*}

\begin{proof}
    We claim that
    \begin{equation}
    \label{l5:ineq1}
        \sum\limits_{t=k}^n v_s^t = \frac{1}{n} \sum\limits_{t = k+1}^n (n-t+1)\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) + (n-k+1)v_s^k.
    \end{equation}
    Let us prove this. We use the method of induction. For \( k = n \) it is obviously true. We suppose that it is true for some some fixed $k = \widetilde{k} \geqslant 1$ ($k=0$ is the first index in the epoch, i.e. start of the recursion) and want to prove that it is true for $k = \widetilde{k}-1$.
    \begin{align*}
        \sum\limits_{t=\widetilde{k}-1}^n v_s^t &= v_s^{\widetilde{k}-1} + \sum\limits_{t=\widetilde{k}}^n v_s^t \\
        &= v_s^{\widetilde{k}-1} + \frac{1}{n} \sum\limits_{t = \widetilde{k}+1}^n (n-t+1)\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) + (n-\widetilde{k}+1)v_s^{\widetilde{k}} \\
        & \overset{(i)}{=}  v_s^{\widetilde{k}-1} + \frac{1}{n} \sum\limits_{t = \widetilde{k}+1}^n (n-t+1)\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) \\
        & \quad + (n-\widetilde{k}+1)\left(v_s^{\widetilde{k}-1} + \frac{1}{n} \left(\nabla f_{\pi_s^{\widetilde{k}}}(x_s^{\widetilde{k}}) - \nabla f_{\pi_s^{\widetilde{k}}}(x_s^{\widetilde{k}-1})\right)\right) \\
        &= \frac{1}{n} \sum\limits_{t = \widetilde{k}}^n (n-t+1)\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) + (n-\widetilde{k}+2) v_s^{\widetilde{k}-1},
    \end{align*}
    where equation (\textit{i}) is correct due to \eqref{sarah:update} and $\widetilde{k} \geqslant 1$. In that way, the induction step is proven. It means that \eqref{l5:ineq1} is valid. We substitute $ k = 0 $ in \eqref{l5:ineq1} and, utilizing $v_s^0 = v_s$, get
    \begin{align}
    \label{l5:ineq2}
        \sum\limits_{t=0}^n v_s^t &= \frac{1}{n} \sum\limits_{t = 1}^n (n-t+1)\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) + (n+1) v_s.
    \end{align}
    Hence, estimating the desired term gives
    \begin{align*}
        \Biggl\|\nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\Biggr\|^2 
        & ~= \frac{1}{(n+1)^2}\left\|(n+1)\nabla f(x_{s}^0) -\sum\limits_{t=0}^n v_s^t\right\|^2\\
        & ~\overset{\eqref{l5:ineq2}}{=} \frac{1}{(n+1)^2}\Biggl\|(n+1)\nabla f(x_{s}^0) \\
        &\quad- \frac{1}{n}\sum\limits_{t = 1}^n (n-t+1)\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right) - (n+1)v_s\Biggr\|^2 \\
        &~~ \overset{\eqref{ineq1}}{\leqslant} 2\|\nabla f(x_{s}^0) - v_s\|^2 \\
        &\quad+ \frac{2}{(n+1)^2}\left\|\frac{1}{n} \sum\limits_{t = 1}^n (n-t+1)\left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right)\right\|^2\\
        &~~\overset{(i)}{\leqslant} 2\|\nabla f(x_{s}^0) - v_s\|^2 + \frac{2}{(n+1)^2}\left\|\sum\limits_{t=1}^n \left(\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right)\right\|^2\\
        &~~\overset{\eqref{ineq1}}{\leqslant} 2\|\nabla f(x_{s}^0) - v_s\|^2 + \frac{2}{n+1}\sum\limits_{t=1}^n\left\|\nabla f_{\pi_s^t}(x_s^t) - \nabla f_{\pi_s^t}(x_s^{t-1})\right\|^2\\
        &~~\overset{\text{Ass. \ref{as1}}}{\leqslant} 2\|\nabla f(x_{s}^0) - v_s\|^2 + \frac{2L^2}{n+1}\sum\limits_{t=1}^n\left\|x_s^t - x_s^{t-1}\right\|^2,
    \end{align*}
    where inequality (\textit{i}) is correct due to $t\geqslant 1$ holds during the summation. The obtained inequality finishes the proof of the lemma.
\end{proof}    
\end{lemma}

\begin{lemma}[\textbf{Lemma \ref{l2:sarahmain}}]\label{lemma6}
Suppose that Assumptions \ref{as1}, \ref{as2} hold. Let the stepsize $\gamma \leqslant \frac{1}{3L}$. Then for Algorithm \ref{alg:sarah} a valid estimate is
\begin{align*}
        \left\| \nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 & \leqslant 9\gamma^2L^2\|v_{s}\|^2 + 36\gamma^2L^2n^2\|v_{s-1}\|^2.
\end{align*}
\begin{proof}
        To begin with, in {Lemma \ref{ngl2}}, we obtain
        \begin{equation}
        \label{l6:ineq1}
            \left\| \nabla f(x_s^0) - \frac{1}{n+1} \sum\limits_{t=0}^n v_s^t\right\|^2 \leqslant 2\|\nabla f(x_s^0) - v_s \|^2 + \frac{2L^2}{n+1}\sum\limits_{t=1}^n \|x_s^t - x_s^{t-1}\|^2.
        \end{equation}
        Let us show what $v_s$ is (here we use Line  \ref{algsarah:line8} of Algorithm \ref{alg:sarah}):
        \begin{align}
            \notag v_s & = \widetilde{v}_{s-1}^{n+1} = \frac{n-1}{n} \widetilde{v}_{s-1}^{n} + \frac{1}{n}\nabla f_{\pi_{s-1}^{n}} (x_{s-1}^{n}) \\
            \notag & = \frac{n-1}{n}\cdot\frac{n-2}{n-1} \widetilde{v}_{s-1}^{n-1} + \frac{n-1}{n}\cdot\frac{1}{n-1}\nabla f_{\pi_{s-1}^{n-1}}(x_{s-1}^{n-1})
            + \frac{1}{n}\nabla f_{\pi_{s-1}^{n}} (x_{s-1}^{n})\\
            \notag& = \frac{n-1}{n}\cdot\frac{n-2}{n-1}\cdot\ldots\cdot 0\cdot \widetilde{v}_{s-1}^1 + \frac{1}{n}\sum\limits_{t = 1}^n\nabla f_{\pi_{s-1}^t}(x_{s-1}^t)\\
            \label{l6:ineq2}& \overset{(i)}{=} \frac{1}{n}\sum\limits_{t = 1}^n\nabla f_{\pi_{s-1}^t}(x_{s-1}^t),
        \end{align}
        where equation (\textit{i}) is correct due to initialization $\widetilde{v}_{s-1}^1 = 0$ (Line \ref{algsarah:line13} of Algorithm \ref{alg:sarah}). In that way, using \eqref{l6:ineq1} and \eqref{l6:ineq2},
        \begin{align*}
            \left\| \nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 & \leqslant 2\left\|\nabla f(x_s^0) - \frac{1}{n}\sum\limits_{t = 1}^n\nabla f_{\pi_{s-1}^t}(x_{s-1}^t) \right\|^2 \\
            & ~~+ \frac{2L^2}{n+1}\sum\limits_{t=1}^n \|x_s^t - x_s^{t-1}\|^2.
            \end{align*}
            Then, using \eqref{eq:finite-sum},
            \begin{align}
             \notag \left\| \nabla f(x_s^0) - \frac{1}{n+1}\sum\limits_{t=0}^n v_s^t\right\|^2 & ~~~~~\leqslant ~~2\left\|\frac{1}{n}\sum\limits_{t = 1}^n\left[\nabla f_{\pi_{s-1}^t}(x_s^0) - \nabla f_{\pi_{s-1}^t}(x_{s-1}^t) \right]\right\|^2 \\
             \notag&\quad\quad+ \frac{2L^2}{n+1}\sum\limits_{t=1}^n \|x_s^t - x_s^{t-1}\|^2 \\
            \notag & \overset{\eqref{ineq1}, \text{Ass. \ref{as1}}}{\leqslant} \frac{2L^2}{n}\sum\limits_{t=1}^n\|x_{s-1}^t - x_s^0\|^2 + \frac{2L^2}{n+1}\sum\limits_{t=1}^n \|x_s^t -x_s^{t-1}\|^2 \\
            \notag& ~~\overset{\eqref{ineq:square}}{\leqslant} \frac{4L^2}{n}\sum\limits_{t=1}^n\|x_{s-1}^t - x_{s-1}^0\|^2 + \frac{4L^2}{n}\sum\limits_{t=1}^n\|x_s^0 - x_{s-1}^0\|^2 \\
            \label{l6:ineq3}&\quad\quad+ \frac{2L^2}{n+1}\sum\limits_{t=1}^n \|x_s^t - x_s^{t-1}\|^2.
        \end{align}
        Now we have to bound these three terms. Let us begin with the $\sum\limits_{t=1}^n\|x_s^t - x_s^{t-1}\|^2$ norm.
        \begin{align}
        \label{l6:ineq4}
            \sum\limits_{t=1}^n\|x_s^t - x_s^{t-1}\|^2 = \gamma^2\sum\limits_{t=1}^n\|v_s^{t-1}\|^2 = \gamma^2\sum\limits_{t=0}^{n-1}\|v_s^{t}\|^2.
        \end{align}
        Now we estimate $\|v_{s}^t\|^2$. For $t\geqslant 1$:
        \begin{align*}
            \|v_{s}^t\|^2 &~~~= \left\|v_{s}^{t-1} + \frac{1}{n}\left(\nabla f_{\pi_{s}^t}(x_{s}^t) - \nabla f_{\pi_{s}^t}(x_{s}^{t-1})\right) \right\|^2 
            \\ & \overset{\eqref{ineq:square}}{\leqslant}  \left(1 + \frac{1}{\beta}\right)\|v_{s}^{t-1}\|^2 + \frac{(1 + \beta)L^2}{n^2}\|x_{s}^t - x_{s}^{t-1}\|^2  \\
            & \overset{\eqref{ineq:square}}{\leqslant} \left(1 + \frac{1}{\beta}\right)^2\|v_{s}^{t-2}\|^2 + \frac{1}{n^2}\left(1 + \frac{1}{\beta}\right)(1 + \beta)L^2\|x_{s}^{t-1} - x_{s}^{t-2}\|^2 \\
            &\quad+ \frac{1}{n^2}(1 + \beta)L^2\|x_{s}^t - x_{s}^{t-1}\|^2 \\
            &~ \overset{\eqref{ineq:square}}{\leqslant} \left(1 + \frac{1}{\beta}\right)^t\|v_{s}\|^2 + \frac{1}{n^2}(1 + \beta)L^2 \sum\limits_{k = 1}^t \left(1 + \frac{1}{\beta}\right)^{k-1}\|x_{s}^{t-k+1} - x_{s}^{t-k}\|^2 \\
            & \underset{\beta = t}{\overset{\eqref{ineq:square}}{\leqslant}} ~\left(1 + \frac{1}{t}\right)^t \|v_{s}\|^2 +\frac{1}{n^2} (1+t)\left(1 + \frac{1}{t}\right)^t L^2\sum\limits_{k = 1}^t\|x_{s}^k - x_{s}^{k-1}\|^2.
        \end{align*}
        Then, utilizing the property of the exponent $\left(\left(1 + \frac{1}{t}\right)^t \leqslant e\right)$ and $t\leqslant n-1$ \eqref{l6:ineq4}, we get an important inequality (for $0\leqslant t\leqslant n-1$, since for $t=0$ we have $\|v_s^t\|^2 = \|v_s\|^2$ and desired inequality becomes trivial):
        \begin{align}
        \label{l6:ineq5}
            \|v_{s}^t\|^2 & ~\leqslant e \|v_{s}\|^2 + \frac{e L^2}{n}\sum\limits_{k = 1}^t\|x_{s}^k - x_{s}^{k-1}\|^2.
        \end{align}
        Now we substitute \eqref{l6:ineq5} to \eqref{l6:ineq4} and obtain
        \begin{align*}
            \sum\limits_{t=1}^n\|x_s^t - x_s^{t-1}\|^2 & = \gamma^2\sum\limits_{t=0}^{n-1}\|v_s^{t}\|^2 \leqslant e\gamma^2 n \|v_s\|^2 + \frac{e\gamma^2 L^2}{n}\sum\limits_{t=0}^{n-1}\sum\limits_{k=1}^{t} \|x_{s}^k - x_{s}^{k-1}\|^2\\
            & \leqslant e\gamma^2 n \|v_s\|^2 + e\gamma^2 L^2\sum\limits_{t=1}^{n-1}\|x_{s}^t - x_{s}^{t-1}\|^2\\
            & \leqslant e\gamma^2 n \|v_s\|^2 + e\gamma^2 L^2\sum\limits_{t=1}^{n}\|x_{s}^t - x_{s}^{t-1}\|^2.
        \end{align*}
        Straightforwardly expressing $\sum\limits_{t=1}^n\|x_s^t - x_s^{t-1}\|^2$ we obtain desired estimation:
        \begin{align*}
            \sum\limits_{t=1}^n\|x_s^t - x_s^{t-1}\|^2 \leqslant \frac{e\gamma^2 n \|v_s\|^2}{1 - e\gamma^2 L^2} \overset{e<3}{\leqslant}\frac{3\gamma^2 n \|v_s\|^2}{1 - 3\gamma^2L^2}.
        \end{align*}
        To finish this part of proof it remains for us to choose appropriate $\gamma$. In Lemma \ref{lemma4} we require $\gamma \leqslant\frac{1}{L(n+1)}$. There we are satisfied with even large values of $\gamma$. Let us estimate obtained expression with $\gamma\leqslant\frac{1}{L(n+1)}\leqslant\frac{1}{3L}$. Now we provide final estimation of this norm:
        \begin{align}
        \label{l6:ineq6}
            \sum\limits_{t=1}^n\|x_s^t - x_s^{t-1}\|^2 \leqslant \frac{9}{2}\gamma^2 n\|v_s\|^2.
        \end{align}
        Let us proceed our estimation of \eqref{l6:ineq3} with the $\sum\limits_{t=1}^n \|x_{s-1}^t - x_{s-1}^0\|^2$ term.
        \begin{equation}
            \label{l6:ineq7}
            \sum\limits_{t=1}^n \|x_{s-1}^t - x_{s-1}^0\|^2 = \gamma^2\sum\limits_{t=1}^n \left\|\sum\limits_{k = 0}^{t-1} v_{s-1}^k\right\|^2 \overset{\eqref{ineq1}}{\leqslant} \gamma^2\sum\limits_{t=1}^n t \sum\limits_{k = 0}^{t-1} \|v_{s-1}^k\|^2 \leqslant \gamma^2 n^2 \sum\limits_{t = 0}^{n-1}  \|v_{s-1}^t\|^2.
        \end{equation}
        Note, that we have already estimated $\|v_s^t\|^2$ term for $0\leqslant t\leqslant n-1$ \eqref{l6:ineq5}. Furthermore, we can make the same estimate for the terms in the $(s-1)$-th epoch and write 
        \begin{align}
        \label{l6:ineq8}
            \|v_{s-1}^t\|^2 & ~\leqslant e \|v_{s-1}\|^2 + \frac{e L^2}{n}\sum\limits_{k = 1}^t\|x_{s-1}^k - x_{s-1}^{k-1}\|^2.
        \end{align}
        Now we substitute \eqref{l6:ineq8} to \eqref{l6:ineq7} to obtain
        \begin{align}
            \notag\sum\limits_{t=1}^n \|x_{s-1}^t - x_{s-1}^0\|^2 &\leqslant \gamma^2 n^2\sum\limits_{t=0}^{n-1} \left(e\|v_{s-1}\|^2 + \frac{e L^2}{n}\sum\limits_{k = 1}^t\|x_{s-1}^k - x_{s-1}^{k-1}\|^2\right)\\
            \notag& \leqslant \gamma^2 n^3 e \|v_{s-1}\|^2 + e\gamma^2 L^2n\sum\limits_{t = 0}^{n-1}\sum\limits_{k = 1}^t\|x_{s-1}^k - x_{s-1}^{k-1}\|^2\\
            \notag& \leqslant \gamma^2 n^3 e\|v_{s-1}\|^2 + e\gamma^2L^2 n^2 \sum\limits_{t = 1}^{n-1}\|x_{s-1}^t - x_{s-1}^{t-1}\|^2\\
            \label{l6:ineq9}
            & \leqslant \gamma^2 n^3 e\|v_{s-1}\|^2 + e\gamma^2L^2 n^2 \sum\limits_{t = 1}^{n}\|x_{s-1}^t - x_{s-1}^{t-1}\|^2.
        \end{align}
        Note, that we have already estimated the $\sum\limits_{t = 1}^{n}\|x_{s}^t - x_{s}^{t-1}\|^2$ term \eqref{l6:ineq6}. Furthermore, we can make the same estimate for the term in the $(s-1)$-th epoch and write 
        \begin{align}
        \label{l6:ineq10}
            \sum\limits_{t=1}^n\|x_{s-1}^t - x_{s-1}^{t-1}\|^2 \leqslant \frac{9}{2}\gamma^2 n\|v_{s-1}\|^2.
        \end{align}
        Substituting \eqref{l6:ineq10} to \eqref{l6:ineq9} we derive
        \begin{align*}
            \sum\limits_{t=1}^n \|x_{s-1}^t - x_{s-1}^0\|^2 \leqslant 3\gamma^2 n^3 \|v_{s-1}\|^2 + \frac{27}{2}\gamma^4L^2 n^3\|v_{s-1}\|^2.
        \end{align*}
        Using our $\gamma\leqslant\frac{1}{3L}$ choice,
        \begin{align}
        \label{l6:ineq11}
            \sum\limits_{t=1}^n \|x_{s-1}^t - x_{s-1}^0\|^2 \leqslant 3\gamma^2 n^3 \|v_{s-1}\|^2 + \frac{3}{2}\gamma^2 n^3\|v_{s-1}\|^2 = \frac{9}{2}\gamma^2 n^3\|v_{s-1}\|^2.
        \end{align}
        It remains for us to estimate the $\sum\limits_{t=1}^n\|x_s^0 - x_{s-1}^0\|^2$ term. The estimate is quite similar to the previous one:
        \begin{align*}            
            \sum\limits_{t=1}^n \|x_s^0 - x_{s-1}^0\|^2  & = \gamma^2\sum\limits_{t=1}^n \left\|\sum\limits_{k = 0}^{n-1} v_{s-1}^{k-1}\right\|^2 \overset{\eqref{ineq:square}}{\leqslant} \gamma^2\sum\limits_{t=1}^n n \sum\limits_{k = 0}^{n-1} \|v_{s-1}^k\|^2 \leqslant \gamma^2 n^2 \sum\limits_{t = 0}^{n-1}  \|v_{s-1}^t\|^2. 
        \end{align*}
        We obtain the estimate as in \eqref{l6:ineq7}. Thus, proceed similarly as we did for the previous term, we obtain
        \begin{align}
        \label{l6:ineq12}
            \sum\limits_{t=1}^n \|x_{s}^0 - x_{s-1}^0\|^2 \leqslant \frac{9}{2}\gamma^2 n^3\|v_{s-1}\|^2.
        \end{align}
        Now we can apply the upper bounds obtained in \eqref{l6:ineq6}, \eqref{l6:ineq11}, \eqref{l6:ineq12} to \eqref{l6:ineq3} and have
        \begin{align*}
            \left\| \nabla f(\omega_s) - \frac{1}{n+1} \sum\limits_{i=0}^n v_s^i\right\|^2 & \leqslant 18\gamma^2L^2n^2\|v_{s-1}\|^2 + 18\gamma^2L^2n^2\|v_{s-1}\|^2 + 9\gamma^2L^2\|v_s\|^2 \\
            & = 9\gamma^2L^2\|v_{s}\|^2 + 36\gamma^2L^2n^2\|v_{s-1}\|^2,
        \end{align*}
        which ends the proof.
\end{proof}
\end{lemma}
        
\begin{theorem}[\textbf{Theorem \ref{th1:sarahmain}}]\label{theorem3}
Suppose Assumptions \ref{as1}, \ref{as2nonconvex} hold. Then Algorithm \ref{alg:sarah} with $\gamma\leqslant\frac{1}{20L(n+1)}$ to reach $\varepsilon$-accuracy, where $\varepsilon^2 = \frac{1}{S}\sum\limits_{s=1}^{S} \|\nabla f(x_s^0)\|^2$, needs
   %\vspace{-3mm}
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\varepsilon^2}\right) ~~ \text{iterations and oracle calls.}
    \end{equation*}
        \begin{proof}
        We combine the result of Lemma \ref{lemma4} with the result of Lemma \ref{lemma6} and obtain
        \begin{align*}
            f(x_{s+1}^0) &\leqslant f(x_s^0) - \frac{\gamma(n+1)}{2}\|\nabla f(x_s^0)\|^2 \\
            & \quad + \frac{\gamma(n+1)}{2}\left(9\gamma^2L^2\|v_{s}\|^2 + 36\gamma^2L^2n^2\|v_{s-1}\|^2\right).
        \end{align*}
        We subtract $f(x^*)$ from both parts:
        \begin{align*}
            f(x_{s+1}^0) - f(x^*) &\leqslant f(x_s^0) - f(x^*) - \frac{\gamma(n+1)}{2}\|\nabla f(x_s^0)\|^2 \\
            & \quad + \frac{\gamma(n+1)}{2}\left(9\gamma^2L^2\|v_{s}\|^2 + 36\gamma^2L^2n^2\|v_{s-1}\|^2\right)\\
            & = f(x_s^0) - f(x^*) - \frac{\gamma(n+1)}{4}\|\nabla f(x_s^0)\|^2 \\
            & \quad + \frac{\gamma(n+1)}{2}\left(9\gamma^2L^2\|v_{s}\|^2 + 36\gamma^2L^2n^2\|v_{s-1}\|^2\right) \\
            & \quad - \frac{\gamma(n+1)}{4}\|\nabla f(x_s^0)\|^2.
            \end{align*}
       Then, transforming the last term by using \eqref{ineq:square} with $\beta=1$, we get
        \begin{align*}
            f(x_{s+1}^0) - f(x^*) &\leqslant f(x_s^0) - f(x^*) - \frac{\gamma(n+1)}{4}\|\nabla f(x_s^0)\|^2 \\
           & \quad  + \frac{\gamma(n+1)}{2}\left(9\gamma^2L^2\|v_{s}\|^2 + 36\gamma^2L^2n^2\|v_{s-1}\|^2\right) \\
            & \quad- \frac{\gamma(n+1)}{8}\|v_s\|^2 + \frac{\gamma(n+1)}{4}\|v_s - \nabla f(x_s^0)\|^2.
            \end{align*}
        Using Lemma \ref{lemma6} to $\|v_s - \nabla f(x_s^0)\|^2$ (specially $\frac{4L^2}{n}\cdot\eqref{l6:ineq11} + \frac{4L^2}{n}\cdot\eqref{l6:ineq12}$),
        \begin{align*}
            f(x_{s+1}^0) - f(x^*) &\leqslant f(x_s^0) - f(x^*) - \frac{\gamma(n+1)}{4}\|\nabla f(x_s^0)\|^2 \\
            & \quad + \frac{\gamma(n+1)}{2}\left(9\gamma^2L^2\|v_{s}\|^2 + 36\gamma^2L^2n^2\|v_{s-1}\|^2\right) \\
            & \quad- \frac{\gamma(n+1)}{8}\|v_s\|^2 + \frac{\gamma(n+1)}{4}\cdot 36\gamma^2L^2n^2\|v_{s-1}\|^2.
        \end{align*}
        Combining alike expressions,
        \begin{align}
            \notag f(x_{s+1}^0) - f(x^*) + \frac{\gamma(n+1)}{4}\|\nabla f(x_s^0)\|^2 &\leqslant f(x_s^0) - f(x^*) - \frac{\gamma(n+1)}{8}\left(1 - 36 \gamma^2L^2\right)\|v_s\|^2 \\ 
            \label{t3:ineq1}& \quad + \gamma(n+1)\cdot 27\gamma^2L^2n^2\|v_{s-1}\|^2. 
        \end{align}
        Using $\gamma \leqslant \frac{1}{20L(n+1)}$ (note it is the smallest stepsize from all the steps we used before, so all previous transitions are correct), we get
        \begin{align*}
            f(x_{s+1}^0) - f(x^*) &+ \frac{1}{10}\gamma(n+1)\|v_s\|^2 + \frac{\gamma(n+1)}{4}\|\nabla f(\omega_s)\|^2\\
            &\leqslant f(x_s^0) - f(x^*) + \frac{1}{10}\gamma(n+1)\|v_{s-1}\|^2.
        \end{align*}
        Next, denoting $\Delta_{s} = f(x_{s+1}^0) - f(x^*) + \frac{1}{10}\gamma(n+1)\|v_{s}\|^2$, we obtain
        \begin{equation*}
            \frac{1}{S}\sum\limits_{s=1}^{S} \|\nabla f(x_s^0)\|^2 \leqslant \frac{4\left[\Delta_0 - \Delta_{S}\right]}{\gamma(n+1)S}.
        \end{equation*}    
        We choose $\varepsilon^2 = \frac{1}{S}\sum\limits_{s=1}^{S} \|\nabla f(x_s^0)\|^2$ as criteria. Hence, to reach $\varepsilon$-accuracy we need $\mathcal{O}\left(\frac{L}{\varepsilon^2}\right)$ epochs and $\mathcal{O}\left(\frac{nL}{\varepsilon^2}\right)$ iterations. Additionally, we note that the oracle complexity of our algorithm is also equal to $\mathcal{O}(\frac{nL}{\varepsilon^2})$, since at each iteration the algorithm computes the stochastic gradient at only two points. This ends the proof.
\end{proof}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strongly convex setting} 
\begin{theorem}[\textbf{Theorem \ref{th2:sarahmain}}]\label{theorem4}
Suppose Assumptions \ref{as1}, \ref{as2stronglyconvex} hold. Then Algorithm \ref{alg:sarah} with $\gamma\leqslant\frac{1}{20L(n+1)}$ to reach $\varepsilon$-accuracy, where $\varepsilon = f(x_{S+1}^0)-f(x^*)$, needs
   %\vspace{-3mm}
    \begin{equation*}
        \mathcal{O} \left(\frac{nL}{\mu}\log \frac{1}{\varepsilon}\right)~~ \text{iterations and oracle calls.}
    \end{equation*}
        
\begin{proof}
Under Assumption \ref{as2stronglyconvex}, which states that the function is strongly convex, the \eqref{PL} condition is automatically satisfied. Therefore, 
\begin{align*}
    f(x_{s+1}^0) - f(x^*) + \frac{\gamma\mu(n+1)}{2}\left( f(x_s^0) - f(x^*)\right) &\leqslant  f(x_{s+1}^0) - f(x^*) + \frac{\gamma(n+1)}{4}\|\nabla f(x_s^0)\|^2.
\end{align*}
Thus, using \eqref{t3:ineq1},
\begin{align*}
f(x_{s+1}^0) - f(x^*) &+ \frac{\gamma\mu(n+1)}{2}\left( f(x_s^0) - f(x^*)\right) \leqslant f(x_s^0) - f(x^*) \\
& - \frac{\gamma(n+1)}{8}\left(1 - 36\gamma^2L^2\right)\|v_s\|^2 + \gamma(n+1)\cdot 27\gamma^2L^2n^2\|v_{s-1}\|^2.
\end{align*}
Using $\gamma \leqslant \frac{1}{20L(n+1)}$ and assuming $n \geqslant 2$, we get
\begin{align*}
    f(x_{s+1}^0) - f(x^*) + \frac{1}{10}\gamma(n+1)\|v_s\|^2
    &\leqslant \left(1-\frac{\gamma\mu(n+1)}{2}\right)\left(f(\omega_s) - f(x^*)\right) \\
    & \quad + \frac{1}{10}\gamma(n+1)\cdot \left(1-\frac{\gamma\mu(n+1)}{2}\right)\|v_{s-1}\|^2.
\end{align*}
Next, denoting $\Delta_{s} = f(x_{s+1}^0) - f(x^*) + \frac{1}{10}\gamma(n+1)\|v_{s}\|^2$, we obtain the final convergence over one epoch:
\begin{align*}
    \Delta_{s+1} \leqslant \left(1-\frac{\gamma\mu(n+1)}{2}\right) \Delta_s.
\end{align*}
Going into recursion over all epoch,
\begin{align*}
    f(x_{S+1}^0) - f(x^*)\leqslant\Delta_{S} \leqslant \left(1-\frac{\gamma\mu(n+1)}{2}\right)^{S+1}\Delta_0.
\end{align*}
We choose $\varepsilon = f(x_{S+1}^0)-f(x^*)$ as criteria. Then to reach $\varepsilon$-accuracy we need $\mathcal{O}\left(\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)$ epochs and $\mathcal{O}\left(\frac{nL}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)$ iterations. Additionally, we note that the oracle complexity of our algorithm is also equal to $\mathcal{O}\left(\frac{nL}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)$, since at each iteration the algorithm computes the stochastic gradient at only two points.
\end{proof} 
\end{theorem}

\section{Lower bounds} \label{sec:lower_bound}

In this section we provide the proof of the lower bound on the amount of oracle calls in the class of the first-order algorithms with shuffling heuristic that find the solution of the non-convex objective finite-sum function. We follow the classical way by presenting the example of function and showing the minimal number of oracles needs to solve the problem. We consider the following function:
\begin{align*}
    l(x) = -\Psi(1)\Phi([x]_1) + \sum\limits_{j=2}^d \left(\Psi(-[x]_{j-1})\Phi(-[x]_j) - \Psi([x]_{j-1})\Phi([x]_j)\right),
\end{align*}
where $[x]_j$ is the $j$-th coordinate of the vector $x\in\mathbb R ^d$,
\begin{align*}
    \Psi(z) = \begin{cases}
        0, &\text{~if~} z\leqslant\frac{1}{2}\\
        \exp\left(1 - \frac{1}{(2z-1)^2}\right), &\text{~if~} z > \frac{1}{2}
    \end{cases}
\end{align*}
and
\begin{align*}
    \Phi(z) = \sqrt{e}\int\limits_{-\infty}^z \exp\left(-\frac{t^2}{2}\right) dt.
\end{align*}
We also define the following function:
\begin{align*}
    \text{prog}(x) = \begin{cases}
        0, &\text{~if~} x=0\\
        \underset{1\leqslant j \leqslant d}{\max} \left\{j : [x]_j \neq 0\right\}, &\text{~otherwise~}
    \end{cases},
\end{align*}
where $x\in\mathbb R ^d$. In the work \citep{arjevani2023lower} it was shown, that function $l(x)$ satisfies the following properties:
\begin{align*}
    &\forall x\in\mathbb R^d ~~ l(0) - \underset{x}{\inf}~ l(x) \leqslant \Delta_0 d,\\
    &l(x) \text{~is~} L_0\text{-smooth with~} L_0=152,\\
    &\forall x\in\mathbb R^d ~~ \|l(x)\|_{\infty}\leqslant G_0 \text{~with~} G_0=23,\\
    &\forall x\in\mathbb R^d : [x]_d = 0 ~~ \|l(x)\|_{\infty} \geqslant 1,\\
    &l(x) \text{~is the zero-chain function, i.e.,~} \text{prog}(\nabla l(x)) \leqslant \text{prog}(x) + 1.
\end{align*}

\begin{lemma}[Lemma C.9 from \citep{metelev2024decentralized}]\label{lemma_L_0-smooth}
    Each $l_j(x)$ is $L_0$-smooth, where
    \begin{align*}
        l_j(x) = \begin{cases}
                    -\Psi(1)\Phi([x]_1), &\text{~if~} j=1\\
                    \Psi(-[x]_{j-1})\Phi(-[x]_j) - \Psi([x]_{j-1})\Phi([x]_j), &\text{~otherwise}
                \end{cases}.
    \end{align*}
\end{lemma}

\subsection{Proof of Theorem \ref{thm:lower}}
\begin{theorem}[\textbf{Theorem \ref{thm:lower}}]
    For any $L > 0$ there exists a problem \eqref{eq:finite-sum} which satisfies Assumption \ref{as1}, such that for any output of first-order algorithm, number of oracle calls $N_c$ required to reach $\varepsilon$-accuracy is lower bounded as
    \begin{align*}
        N_c = \Omega\left(\frac{L\Delta}{\varepsilon^2}\right).
    \end{align*}
\end{theorem}
        \begin{proof}
            To begin with, we need to decompose the function $l(x)$ to the finite-sum:
            \begin{align*}
                l(x) = \sum\limits_{j=1}^d l_j(x),
            \end{align*}
            where index $j$ responds the definition of $l(x)$, i.e.,
            \begin{align*}
                l_j(x) = \begin{cases}
                    -\Psi(1)\Phi([x]_1), &\text{~if~} j=1\\
                    \Psi(-[x]_{j-1})\Phi(-[x]_j) - \Psi([x]_{j-1})\Phi([x]_j), &\text{~otherwise}
                \end{cases}.
            \end{align*}
            Now we design the following objective function:
            \begin{align*}
                f(x) = \frac{1}{n}\sum\limits_{i=1}^n f_i(x),
            \end{align*}
            where $f_i(x) = \frac{LC^2}{L_0}\sum\limits_{j\equiv i \mod n} l_j\left(\frac{x}{C}\right)$.
            Since each $l_j\left(\cdot\right)$ is $L_0$-smooth (according to Lemma \ref{lemma_L_0-smooth}) and for $j\equiv i\mod n$ the gradients $\nabla l_j(x)$ are separable, than for any $x_1, x_2 \in\mathbb R^d$ it implies
            \begin{eqnarray*}
                \|\nabla f_i(x_1) - \nabla f_i(x_2)\|^2 &=& \frac{L^2 C^2}{L_0^2}\left\|\sum\limits_{j\equiv i \mod n} \left(\nabla l_j\left(\frac{x_1}{C}\right) - \nabla l_j\left(\frac{x_2}{C}\right)\right)\right\|^2 \\
                &\leqslant& \frac{L^2 C^2}{L_0^2}\frac{L_0^2}{C^2} \|x_1 - x_2\|^2 = L^2\|x_1 - x_2\|^2.
            \end{eqnarray*}
            It means, each function $f_i(x)$ is $L$-smooth. Moreover, since $f(x) = \frac{LC^2}{nL_0} l\left(\frac{x}{C}\right)$,
            \begin{eqnarray}
                \Delta = \notag f(0) - \underset{x}{\inf}~ f(x) &=& \frac{LC^2}{nL_0} \left(l(0) - \underset{x}{\inf}~ l\left(\frac{x}{C}\right)\right)\\
                \label{thlb:ineq1}&=& \frac{LC^2}{nL_0} \left(l(0) - \underset{x}{\inf}~ l(x)\right) \leqslant \frac{LC^2\Delta_0 d}{nL_0}.
            \end{eqnarray}
            Now we show, how many oracle calls we need to have progress in one coordinate fo vector $x$. At the current moment, we need a specific piece of function, because according to structure of $l(x)$, each gradient estimation can "defreeze" at most one component and only a computation on a certain block makes it possible. Formally, since $\frac{1}{n}\sum\limits_{i=1}^n f_i(x) = \frac{LC}{dL_0} l\left(\frac{x}{C}\right)$,
            \begin{align*}
                \text{prog}(\nabla f_i(x)) \begin{cases}
                    =\text{prog}(x) + 1, &\text{~if~} i = \text{prog}(x)\mod n\\
                    \leqslant \text{prog}(x), &\text{~otherwise}
                \end{cases}.
            \end{align*}
            Now, we need to show the probability of choosing the necessary piece of function, according to the shuffling heuristic. This probability at the first iteration of the epoch, i.e., iteration $t$, such that $t\mod n = 1$, is obviously $\frac{1}{n}$. At the second iteration of the epoch -- $\frac{n-1}{n}\cdot\frac{1}{n-1} = \frac{1}{n}$. Thus, at the $k$-th iteration of the epoch, the desired probably is $\frac{n-1}{n}\cdot\frac{n-2}{n-1}\cdot\ldots\cdot\frac{1}{n-k+1} = \frac{1}{n}$. In that way, the expected amount of gradient calculations though the epoch is
            \begin{align*}
                \sum\limits_{i=1}^n \frac{i}{n} = \frac{n+1}{2} \leqslant n.
            \end{align*}
            Since epochs is symmetrical in a sense of choosing indexes, we need to perform $n$ oracle calls at each moment of training. Thus, after $T$ oracle calls, we can change only $\frac{T}{n}$ coordinate of vector $x$. Now, we can write the final estimate:
            \begin{eqnarray*}
                \mathbb E\|\nabla f(\hat{x})\|^2_2 &\geqslant& \mathbb E\|\nabla f(\hat{x})\|^2_\infty \geqslant \underset{[x]_d=0}{\min} \|\nabla f(\hat{x})\|^2_\infty = \frac{L^2C^2}{n^2L_0^2}\left\|\nabla l\left(\frac{\hat{x}}{C}\right)\right\|^2_\infty \geqslant \frac{L^2C^2}{n^2L_0^2}\\
                &\overset{\eqref{thlb:ineq1}}{\geqslant}& \frac{L\Delta}{n L_0 \Delta_0 d} = \frac{L\Delta}{L_0 \Delta_0 T}.
            \end{eqnarray*}
            Thus, lower bound on $T$ is $\Omega\left(\frac{L\Delta}{\varepsilon^2}\right)$.
        \end{proof}
\subsection{Proof of Theorem \ref{thm: quest}}
Before we start the proof, let us introduce other assumptions of smoothness for the complete analysis.
\begin{assumption}[Smoothness of each $f_i$]
\label{asm: each}
    Each function $f_i$ is $L_i$-smooth, i.e., it satisfies $$\|\nabla f_i(x) - \nabla f_i(y)\| \leq L_i\|x - y\|$$ for any $x, y \in \mathbb{R}^d$.
\end{assumption}
\begin{assumption}[Average smoothness of $f$]
\label{asm: average}
    Function $f$ is $\hat{L}$-average smooth, i.e., it satisfies $$\mathbb{E}_i\left[\|\nabla f_i(x) - \nabla f_i(y)\|^2\right] \leq \hat{L}^2\|x - y\|^2$$ for any $x, y \in \mathbb{R}^d$.
\end{assumption}
Here we also assume that $L_i$ with $i = 1, \ldots, n$ and $\hat{L}$ are \textit{effective}: it means that these constants cannot be reduced.

If $\{f_i\}_{i=1}^n$ satisfies Assumption \ref{as1}, it automatically leads to the satisfaction of Assumption \ref{asm: each}, since $L_i$ can be chosen as $L$. Nevertheless, the effective constant of smoothness for $f_i$ can be less than $L$. As a consequence, we obtain the next result.
\begin{lemma}
    \label{lem: ineq-L}
    Suppose that Assumption \ref{as1} holds. Then, the set $\{f_i\}_{i=1}^n$ satisfies Assumptions \ref{asm: each} and \ref{asm: average}. Moreover,
    \begin{align*}
        \hat{L} \leq L,
    \end{align*}
    where $\hat{L}$ and $L$ are chosen effectively.
\end{lemma}
\begin{proof}
    Let $L_i$ be the constant of smoothness of $f_i$. Therefore, $L_i \leq L$, and $L$ is defined as $\max_{i} L_i$. Moreover, $\hat{L}^2$ is defined as
    \begin{align*}
        \hat{L}^2 = \sum\limits_{i=1}^n w_i L_i^2,
    \end{align*}
    where $\{w_i\}_{i=1}^n$ is probabilities for the sampling of $f_i$, i.e. $w = (w_1, \ldots, w_n)$ formalizes the discrete distribution over indices $i$ (the most common case: $w_i = \frac{1}{n}$; nevertheless, we consider an unified option). As a result, we have
    \begin{align*}
        \hat{L}^2 = \sum\limits_{i=1}^n w_i L_i^2 \leq \sum\limits_{i=1}^n w_i L^2 = L^2.
    \end{align*}
    This concludes the proof.
\end{proof}
Now we are ready to proof the Theorem \ref{thm: quest}.
\begin{theorem}[\textbf{Theorem \ref{thm: quest}}]
    For any $L > 0$ there is \textbf{no} problem \eqref{eq:finite-sum} which satisfies Assumption \ref{as1}, such that for any output of first-order algorithm, number of oracle calls $N_c$ required to reach $\varepsilon$-accuracy is lower bounded with $p > \frac{1}{2}$:
    \begin{align*}
        N_c = \Omega\left(\frac{n^pL\Delta}{\varepsilon^2}\right).
    \end{align*}
    \end{theorem}
\begin{proof}
    Let us assume that we can find the problem \eqref{eq:finite-sum} which satisfies Assumption \ref{as1}, such that for any output of first-order algorithm, number of oracle calls $N_c$ required to reach $\varepsilon$-accuracy is lower bounded as
    \begin{align*}
        N_c = \Omega\left(\frac{n^pL\Delta}{\varepsilon^2}\right)
    \end{align*}
    with $p > \frac{1}{2}$. Applying Lemma \ref{lem: ineq-L}, one can obtain
    \begin{align*}
        N_c = \Omega\left(\frac{n^pL\Delta}{\varepsilon^2}\right) \geq \Omega\left(\frac{n^p\hat{L}\Delta}{\varepsilon^2}\right), 
    \end{align*}
    which contradict existing results of upper bound in terms of $n$ under Assumption \ref{asm: average} (e.g. \cite{fang2018spider}). This finishes the proof.
\end{proof}
\end{document}
