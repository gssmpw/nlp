Reinforcement learning (RL) has made significant strides in training agents to perform complex tasks~\citep{lee2020LearnQuadrupedalLoco, kumar2021rma, miki2022perceptiveLoco, hoeller2024parkour, li2023learning}, achieving high performance in controlled settings.
However, RL agents often struggle when faced with tasks outside their training distributionâ€”a limitation known as the out-of-distribution (OOD) generalization challenge.
This limitation restricts the applicability of RL policies to a broader range of tasks beyond those explicitly encountered during training~\citep{li2023versatile, li2024fld}.

% \setcounter{figure}{0}
\begin{figure}
\centering
\includegraphics[trim=0 60 20 40, clip, width=1.0\linewidth]{images/introduction/intro.pdf}
\caption{Improving task generalization through memory augmentation. Top row: RL agents often struggle with tasks not encountered during training, leading to poor performance.
Middle row: Experience augmentation expands the range of training scenarios to include potential test-time tasks; however, in partially observable settings, agents may fail to identify the current task parameterizations, resulting in over-conservative behavior.
Bottom row: Incorporating memory mechanisms enables agents to infer task context, facilitating context-aware decision-making and improved adaptability.}
\label{fig:intro}
\end{figure}

A common way to address this challenge is to increase training diversity to better encompass potential testing conditions. 
Domain randomization~\citep{tobin2017DRforSim2Real, sadeghi2016cad2rl, peng2018DynRand}, for instance, achieves this by varying environment parameters, thereby exposing agents to a wide range of scenarios during training.
However, as the extent of randomization increases, this method often suffers from reduced sample efficiency~\citep{kirk2023surveyZSG}. 
Additionally, increased variations during training can result in over-conservative strategies, especially in environments where these variations are not fully observable.
In such settings, the agent may struggle to discern the current task parameterizations and only executes conservative moves, thereby sacrificing optimality.
This raises a crucial question: how can an RL agent achieve robust performance across diverse environment variations, or \textit{tasks}, without being explicitly exposed to them during training?

A promising method to improve the generalization of the policy is through experience augmentation \cite{andrychowicz2017HER}, where training experiences are augmented to simulate a broader range of potential test-time conditions.
However, pure augmentation, without properly accounting for task context, may similarly cause over-conservativeness resulting from exposing the policy to excessive unobservable variations.
Building on the contextual modeling capability of Recurrent Neural Networks (RNNs), which have been shown to effectively address partial observability~\citep{heess1512memoryRNN}, we enhance experience augmentation in RL by incorporating memory mechanisms that respect the necessary task context, as shown in~\figref{fig:intro}.

In this work, we introduce \textit{memory augmentation}, a novel approach to improve task generalization by augmenting training experience to simulate plausible OOD tasks and incorporating memory mechanisms to facilitate zero-shot task adaptation.
Our approach trains a single unified policy that can perform well across both in-distribution (ID) and unseen OOD tasks.

In summary, our contributions include:
\begin{itemize}
    \item We propose \textit{memory augmentation}, a novel approach to improve task generalization without additional interactions with the environment.
    \item We validate our approach across a variety of legged locomotion experiments. Results demonstrate that our memory-augmented policies generalize effectively to unseen OOD tasks while maintaining robust ID performance. Additionally, while maintaining comparable performance, they achieve superior sample efficiency compared to randomization-based policies, where the agent is explicitly trained across all conditions.
    \item We demonstrate the sim-to-real transfer of the learned memory-augmented policy on a quadrupedal robot for position tracking under joint failure.
    The robot successfully tracks the goals while effectively adapting to both ID and OOD joint failures.
\end{itemize}