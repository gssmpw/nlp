In this work, we have presented an RL framework that integrates task-structured experience augmentation with memory mechanisms to enhance task generalization. 
By simulating varied task conditions through augmented training experiences and incorporating memory-based task inference, our approach trains a single unified policy capable of handling both ID and OOD tasks without additional environment interactions.

This framework effectively addresses the limitations of policies lacking context-awareness, which tend to adopt over-conservative strategies in highly randomized and partially observable environments.
Through extensive simulation experiments, we demonstrate that our approach achieves zero-shot generalization to OOD tasks while maintaining robust ID performance. 

Furthermore, our approach demonstrates performance comparable to domain randomization in both ID and OOD tasks, while the latter requiring explicit environment interactions for these tasks. 
Notably, our method achieves this without requiring additional environment interactions for the OOD tasks, thereby overcoming the sample inefficiency associated with domain randomization.

Crucially, the memory mechanism proves essential in partially observable settings with substantial task diversity, allowing the agent to infer task context from past interactions. 
This facilitates zero-shot task adaptation and ensures reliable performance across both ID and OOD tasks. 
Moreover, our hardware experiments on a quadruped robot validate the sim-to-real transfer, demonstrating that our policy performs well on both ID and OOD tasks in real-world scenarios.

We believe this work inspires further advancements in developing adaptive RL agents capable of handling the complexities and uncertainties of real-world environments.
