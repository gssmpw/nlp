\subsection{Task generalization in RL}
In RL, task generalization arises as a challenge when the agent is required to perform well on tasks that differ from those encountered during training. 
These differences may stem from variations in scenes, states, or reward structures~\citep{zhou2022DGSurvey}.
Domain randomization, originally introduced for sim-to-real transfer in robotics~\citep{tobin2017DRforSim2Real, sadeghi2016cad2rl, peng2018DynRand}, addresses this issue by randomizing environment parameters during training to encompass a broader range of potential test-time conditions. 
In robotic control, this often involves dynamics randomization, where parameters like masses, friction coefficients, joint damping, and actuator gains are varied during training to enhance policy robustness~\citep{peng2018DynRand, lee2020LearnQuadrupedalLoco, kumar2021rma, miki2022perceptiveLoco, andrychowicz2020DexInHandManipulation}. 
While this effectively broadens the agentâ€™s exposure to diverse conditions, increased randomization often leads to reduced sample efficiency~\citep{kirk2023surveyZSG}.

Alternatively, many works incorporate known physics principles or state structures into task design to approach task generalization.
Examples include foot-placement dynamics~\citep{yang2020data}, object invariance~\citep{sancaktar2022curious}, granular media interactions~\citep{choi2023learning}, frequency domain parameterization~\citep{li2024fld}, rigid body dynamics~\citep{song2024learning}, and semi-structured Lagrangian dynamics models~\citep{levy2024learning}.

\subsection{Experience augmentation}
While many data augmentation techniques in RL have focused on augmenting image-based observations to improve generalization across visual changes~\citep{laskin2020RLwithAugData, raileanu2021autoDataAug, hansen2021SoftDataAug, wang2020mixreg}, fewer studies have investigated augmentation at the induced state-action trajectory level---referred to as \textit{experience augmentation}---which is the focus of our work.

One example of experience augmentation is PlayVirtual~\citep{yu2021playvirtual}, which augments cycle-consistent virtual trajectories by leveraging forward and backward dynamics models in a latent space to improve feature representation learning.
Meanwhile, in HER~\citep{andrychowicz2017HER}, which aims to address the sparse reward problem in RL, failed trajectories are transformed into meaningful learning signals through goal relabeling. 
This relabeling mechanism, as implemented in HER and its extensions~\citep{fang2019curriculumHER, lin2020invariantTER, yang2021MHER, packer2021HERMetaRL}, can also be viewed as a form of experience augmentation.
However, these approaches primarily focus on improving learning efficiency with limited training data rather than explicitly addressing task generalization.

In legged locomotion control, symmetry-based augmentation has been applied to state-action trajectories to encourage symmetry-invariant behavior~\citep{Mittal2024SymmetryCF, Su2024LeveragingSym}. 
Inspired by these works, our approach leverages the inherent task structure to augment state-action trajectories, generating synthetic experience of novel scenarios to enhance task generalization.

\subsection{Meta-RL}
Meta-RL enables agents to quickly adapt to new tasks by leveraging experience gained during meta-training. 
Through exposure to multiple related tasks, the agent learns a context-aware policy~\citep{duan2016RL2, wang2016learnToRL, mishra2017snail, melo2022transformersMetaRL} or a good initialization~\citep{finn2017MAML, li2017metaSGD, zintgraf2019CAVIA}, which allows it to adapt to novel, unseen tasks with minimal additional training~\citep{beck2023surveyMetaRL}. 
In the field of memory-based meta-RL, RNNs have been widely employed for their ability to capture task context from past interactions, enabling zero-shot adaptation~\citep{duan2016RL2, wang2016learnToRL}.
Building on this framework,~\citet{nagabandi2018adaptMetaRL} developed a dynamics model prior to enable rapid online adaptation in dynamic environments, while~\citet{zargarbashi2024metaloco} trained a morphology-agnostic locomotion policy for legged robots.