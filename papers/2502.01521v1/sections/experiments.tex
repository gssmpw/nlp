\label{sec:experiments}
\subsection{Environment and task setup}
We evaluate the performance of our approach across eight legged locomotion experiments with the ANYmal D quadruped and Unitree G1 humanoid robots in Issac Lab~\citep{mittal2023orbit}.
The experiments are designed around two core task settings: \textit{position tracking} and \textit{velocity tracking}, with variations introduced to simulate joint failures and payload distributions:
\begin{itemize}
    \item \textit{Joint failure}:  A joint failure is simulated by disabling the motor torque.
    \begin{itemize}
        \item \textbf{Quadruped robot:} During training, a joint in the left front leg (LF HAA, LF HFE, or LF KFE) is randomly disabled, categorized as an ID joint failure. 
        Joint failures in other legs are considered OOD failures.
        \item \textbf{Humanoid robot:} A joint in the left hip (pitch, roll, or yaw) is randomly disabled during training (ID). 
        Joint failures in the right hip are considered as OOD.
    \end{itemize}
    For both robots, a joint failure occurs with an 80\% probability during training.
    \item \textit{Payload}:  A payload is simulated by imposing a vertical external force.
    \begin{itemize}
        \item \textbf{Quadruped Robot:} During training, a payload with a mass uniformly sampled from \([30\,\text{kg}, 40\,\text{kg}]\) is applied at a random position on the left front section of the robot's base (ID). 
        Payloads with the same mass range applied to other sections of the base are considered OOD.
        
        \item \textbf{Humanoid Robot:} During training, a payload with a mass uniformly sampled from \([40\,\text{kg}, 50\,\text{kg}]\) is applied at a random position on the left shoulder of the robot (ID). 
        Payloads applied to the right shoulder are considered OOD.

    \end{itemize}
\end{itemize}

We train a memory-based policy both with and without experience augmentation, referred to as \textbf{Memory-Aug} and \textbf{Memory-ID}, respectively. 
To assess the role of memory-based task inference, we also train a standard MLP policy with experience augmentation~\citep{Mittal2024SymmetryCF, Su2024LeveragingSym}, referred to as \textbf{Baseline-Aug}, and a standard MLP policy without augmentation, referred to as \textbf{Baseline-ID}.

Additionally, we include randomization-based policies, where the agent is exposed to all variations mentioned above (ID + OOD) during training. 
These are referred to as \textbf{Baseline-Rand} for the MLP-only architecture and  \textbf{Memory-Rand} for the memory-enabled architecture.

We include all implementation details in the Appendix, including observations, rewards, network architectures and hyperparameter values.

\subsection{Task performance and data efficiency}
\begin{figure}
\centering
    % First pair of images
    \begin{subfigure}[b]{1.0\linewidth} % Ensure the pair fits the column width
        \centering
        \includegraphics[width = 1.0\linewidth]{images/experiments/anymal_eval_id_aug.pdf}
        \caption{Mean episodic returns with minimum and maximum values of policies trained \textbf{with} and \textbf{without augmentation}.}
        \label{fig:anymal_pos_tracking_joint_id_aug}
    \end{subfigure}

    % Second pair of images
    \begin{subfigure}[b]{1.0\linewidth}
        \centering
        \includegraphics[width = 1.0\linewidth]{images/experiments/anymal_eval_rand_aug.pdf}
        \caption{Mean episodic returns with minimum and maximum values of \textbf{augmentation-} and \textbf{randomization-based policies}.}
        \label{fig:anymal_pos_tracking_joint_rand_aug}
    \end{subfigure}

\caption{Evaluation of quadruped position tracking under joint failure for different policies.}
\label{fig:anymal_pos_joint_eval}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{images/experiments/id_ood_results.pdf}
\caption{Normalized mean episodic returns on ID and OOD tasks, with error
bars indicating minimum and maximum values.}
\label{fig:all_exp_results}
\end{figure*}

We evaluate performance using the average episodic return, calculated as the mean of undiscounted cumulative rewards across 1000 episodes.
Training for each policy was conducted using five different seeds.
\Figref{fig:anymal_pos_joint_eval} presents detailed evaluations of different policies in quadruped position tracking under both ID and OOD joint failure scenarios. 

As shown in~\figref{fig:anymal_pos_tracking_joint_rand_aug}, \textbf{Baseline-Aug} exhibits ID performance comparable to \textbf{Baseline-Rand}, but falls short of matching the ID performance of \textbf{Baseline-ID} (see~\figref{fig:anymal_pos_tracking_joint_id_aug}).
These findings underscore how variations introduced through randomization or augmentation can cause a simple MLP policy to adopt suboptimal behaviors in partially observable settings.
Lacking the ability to infer latent task context from partial observations, such policies resort to over-conservative strategies, limiting their ability to optimize performance in specific tasks.

In contrast, \textbf{Memory-Aug} achieves performance comparable to \textbf{Memory-ID} on ID tasks, as shown in~\figref{fig:anymal_pos_tracking_joint_id_aug}.
This result highlights how the memory module effectively captures task context from partial observations and utilizes this capability in experience augmentation, thereby preserving robust ID performance.

In~\figref{fig:anymal_pos_tracking_joint_rand_aug}, \textbf{Memory-Aug} demonstrates performance on par with \textbf{Memory-Rand} in both ID and OOD tasks.
Notably, \textbf{Memory-Aug} achieves this without requiring any additional environment interactions for these OOD tasks, demonstrating significantly higher sample efficiency than \textbf{Memory-Rand}.

To verify that these findings are not coincidental, we evaluate the approaches across all experiments.
As shown in~\figref{fig:all_exp_results}, the trend of ID performance degradation in \textbf{Baseline-Rand} and \textbf{Baseline-Aug} persists, particularly in experiments involving the quadruped robot, where task diversity is more pronounced.
In contrast, \textbf{Memory-Rand} and \textbf{Memory-Aug} consistently achieve comparable ID performance to \textbf{Memory-ID}.
This again highlights the importance of the memory mechanism in partially observable settings with substantial task diversity.
By leveraging its context inference capability, this memory module allows the agent to maintain robust performance in ID tasks.
Furthermore, \textbf{Memory-Aug} achieves performance comparable to \textbf{Memory-Rand} in both ID and OOD tasks.
However, unlike \textbf{Memory-Rand}, \textbf{Memory-Aug} eliminates the need for additional environment interactions, effectively improving sample efficiency.

\subsection{Behavior analysis with memory augmentation}

\begin{figure*}
\centering
\includegraphics[trim=10 180 10 140, clip, width=\linewidth]{images/experiments/behavior_analysis.pdf}
\caption{Behavior analysis of different policies for quadruped position tracking under an ID KFE joint failure.
The yellow marker denotes the impaired joint, the blue arrow represents the current base velocity, and the green marker indicates the goal position.
Left: The \textbf{Baseline-ID}-trained agent actively turns sideways toward the goal within the first second, then moves laterally while dragging the impaired leg to minimize reliance on the failed joint. 
In contrast, agents trained with \textbf{Baseline-Rand} and \textbf{Baseline-Aug} attempt to walk directly toward the goal using very small steps. 
Failing to adapt to the joint failure, these agents often stumble and passively change their body orientation during tracking. 
Right: Agents trained with memory-enabled policies actively adjust their body orientation within the first second, effectively compensating for the impaired joint and maintaining stable locomotion.}
\label{fig:behavior_analysis}
\end{figure*}

We observe distinct strategies adopted by different policies for quadruped position tracking with impaired joint, particularly in response to HFE and KFE joint failures.
Agents trained with \textbf{Baseline-Rand} and \textbf{Baseline-Aug} often attempt to walk directly toward the goal using very small steps, as shown in the left part of~\figref{fig:behavior_analysis}.
This over-cautious behavior reflects a tendency to uniformly reduce reliance on all joints rather than identifying and compensating for the specific failed joint. 
Despite their careful movements, the joint impairment often leads to stumbling or passive change in body orientation during tracking.

In contrast, agents trained with \textbf{Baseline-ID} and \textbf{Memory-ID} demonstrate a more adaptive strategy under ID joint failure.
They tend to first turn sideways toward the goal, then move laterally while dragging the impaired leg to minimize reliance on the failed joint (see upper row of~\figref{fig:behavior_analysis}).
This intentional body orientation adjustment at the start enables them to maintain greater stability while effectively progressing toward the goal.
Notably, agents trained with \textbf{Memory-Rand} and \textbf{Memory-Aug} exhibit similar adaptive behavior, as shown in the middle and bottom rows on the right side of~\figref{fig:behavior_analysis}.

To further support these findings, we conduct a quantitative analysis of feet air time, calculated as the total duration the feet remain off the ground, and feet contact frequency, measured as the total number of instances the feet make contact with the ground.
These metrics provide insight into how effectively the agents adapted their motion to compensate for joint failures.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/experiments/feet_motion_analysis.pdf}
    \caption{Motion analysis. Normalized episodic feet air time and feet contact frequency for different policies in quadruped position tracking under ID and OOD joint failures, with error bars indicating minimum and maximum values.}
    \label{fig:quantitative_motion_analysis}
\end{figure}

As illustrated in~\figref{fig:quantitative_motion_analysis}, in ID tasks, \textbf{Baseline-Rand} and \textbf{Baseline-Aug} demonstrate significantly shorter feet air time and more frequent ground contact compared to \textbf{Baseline-ID}.
This aligns with our observation that agents employing these policies tend to take smaller, more cautious steps during goal tracking.
This conservative behavior persists in OOD tasks, as evidenced by the low feet air time and high contact frequency of \textbf{Baseline-Rand} and \textbf{Baseline-Aug} under OOD joint failures. 
In contrast, \textbf{Memory-Rand} and \textbf{Memory-Aug} exhibit feet air time and contact frequency comparable to \textbf{Memory-ID} in ID tasks, while maintaining consistent levels in OOD tasks.
This highlights the ability of \textbf{Memory-Rand} and \textbf{Memory-Aug} to effectively adapt their motion strategies in response to both ID and OOD joint failures.

We attribute this context-aware behavior to the memory-based task inference capability provided by the RNN. 
By leveraging past experience, the RNN enables the agent to infer the underlying task context from partial observations, including the presence and nature of joint failures. 
This capability enables the agent to dynamically adapt its behavior across diverse scenarios, adopting strategies that compensate for impairments and optimize task performance.

\section{Hardware experiments}
\begin{figure*}
\centering
\includegraphics[trim=10 100 10 100, clip, width=\linewidth]{images/experiments/hardware_rnn.pdf}
\caption{Hardware experiment. Position tracking under ID (LF HAA) and OOD (RH HAA) joint failures with \textbf{Memory-Aug} policy.}
\label{fig:hardware_rnn}
\end{figure*}
We tested our \textbf{Memory-Aug} policy in position tracking under joint failure on the ANYmal D robot. 
To facilitate sim-to-real transfer, we introduced randomization in terrain profiles with a noise range of \([0.0, 0.05]\) during training.

As shown in Figure \ref{fig:hardware_rnn} and the supplementary video, the robot successfully tracked multiple goals consecutively, while adapting effectively to both ID and OOD joint failures.
These results demonstrate the ID robustness and OOD generalization of the memory-augmented policy on real robots.

\begin{figure}
\centering
\includegraphics[trim=240 200 260 190, clip, width=1.0\columnwidth]{images/experiments/hardware_mlp.pdf}
\caption{Hardware experiment. Position tracking under ID (LF HAA) joint failure with \textbf{Baseline-Aug} policy.}
\label{fig:hardware_mlp}
\end{figure}

Additionally, we also tested \textbf{Baseline-Aug} policy trained under the same terrain profiles. 
As shown in Figure \ref{fig:hardware_mlp}, the robot exhibited limited adaptability under ID joint failure, frequently losing balance during goal tracking.
