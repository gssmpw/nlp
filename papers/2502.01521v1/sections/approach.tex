\subsection{Task-structured augmentation}
To augment the training experience, our approach leverages a set of transformation functions, denoted by \( \mathcal{G} \), where each \( g \in \mathcal{G} \) represents a transformation that reflects the underlying structure that we assume to exist in the task space.
These transformations are used to generate synthetic experience in \( \mathcal{T_{\text{OOD}}^{\text{aug}}} \) from the original data collected in \( \mathcal{T_{\text{ID}}} \).

Formally, let \( \tau = \{(o_t, a_t, o_{t+1}, r_t)\}_{t=0}^{T} \) represent a trajectory collected by the current policy \( \pi_{\theta_k} \) within the training set \( \mathcal{T_{\text{ID}}} \). 
Each transformation \( g = (g_o, g_a) \in \mathcal{G} \) consists of functions \( g_o : \mathcal{O} \rightarrow \mathcal{O} \) and \( g_a : \mathcal{A} \rightarrow \mathcal{A} \), which map an original observation-action pair \( (o, a) \) to a new pair \( (o^g, a^g) = (g_o(o), g_a(a)) \).
We denote \( s_t^g\) as the latent state corresponding to the transformed observation \( o_t^g\).
For all \( g \in \mathcal{G}, \, s \in \mathcal{S}, \, a \in \mathcal{A}\), it is assumed that the following properties hold:
\begin{enumerate}[label=(\alph*)]
    \item \label{item:transition_invariance} \textbf{Transition probability invariance:} The probability of reaching the next state from a given state-action pair remains identical in their respective tasks. Formally, this is expressed as:
    \[
    T(s_{t+1}^g | s_t^g, a_t^g) = T(s_{t+1} | s_t, a_t).
    \]
    \item \label{item:init_state_invariance} \textbf{Initial state distribution invariance}: The initial state distribution remains invariant across original and transformed trajectories. Formally, this is expressed as:
    \[
    p_0(s^g) = p_0(s).
    \]
    \item \label{item:reward_invariance} \textbf{Reward invariance}: Each original state-action pair \( (s_t, a_t) \) and its transformed counterpart \( (s_t^g, a_t^g) \) receive the same rewards in their respective tasks. Formally, this is expressed as:
    \[
    r(s_t^g, a_t^g) = r(s_t, a_t).
    \]
\end{enumerate}
These properties ensure alignment between the original and transformed tasks, maintaining the coherence of task dynamics. 
The resulting augmented trajectory \( \tau_g = \{(o_t^g, a_t^g, o_{t+1}^g, r_t)\}_{t=0}^{T} \) represents the agent’s simulated experience within an augmented task \( T \in \mathcal{T}_{\text{OOD}}^{\text{aug}} \), generated without real environment interactions.
The training dataset is then constructed as the union of the original and augmented trajectories:
\[
\mathcal{D}_{\text{train}} = \mathcal{D}_{\text{ID}} \cup \mathcal{D}_{\text{OOD}}^{\text{aug}} = \{\tau_i\}_{i=1}^N \cup \bigcup_{g \in \mathcal{G}} \{\tau_i^g\}_{i=1}^N.
\]

An example of such transformations in \( \mathcal{G} \) includes symmetry transformations, which have been used in RL for manipulation tasks~\citep{lin2020invariantTER} and legged locomotion control~\citep{Mittal2024SymmetryCF, Su2024LeveragingSym}. 
Specifically, for quadrupedal robots, symmetry transformations can simulate mirrored conditions (e.g., joint failures) across left-right and front-back body parts. 
Similarly, for humanoid robots, they can replicate mirrored conditions across left-right body parts. 
These transformations leverage the morphological symmetry inherent to legged robots and uphold the essential invariance properties~\ref{item:transition_invariance}--\ref{item:reward_invariance}~\citep{ordonez2024morphological}.

\subsection{Training with augmented data}
Following the method of~\citet{Mittal2024SymmetryCF}, we approach data augmentation in PPO by constructing augmented policies \( \pi_{\theta_k}^g \) such that
\(
\pi_{\theta_k}^g(a^g | s^g) = \pi_{\theta_k}(a | s), \forall g \in \mathcal{G}, s \in \mathcal{S}, a \in \mathcal{A}.
\)
Based on properties~\ref{item:transition_invariance}--\ref{item:reward_invariance}, it can be shown that for a state-action pair \( (s, a)\) and its transformed counterpart \((s^g, a^g)\), the following invariances hold~\citep{Mittal2024SymmetryCF}:
\begin{equation}
    \label{eq:advantage_state_prob_invariance}
     A^{\pi_{\theta_k}^g}(s^g, a^g) = A^{\pi_{\theta_k}}(s, a)\text{, and }
    p_{\pi_{\theta_k}^g}(s^g) = p_{\pi_{\theta_k}}(s).
\end{equation}
Using~\eqnref{eq:advantage_state_prob_invariance}, the policy update rule in~\eqnref{eq:policy_grad_ppo} becomes
\begin{align}
    \label{eq:policy_update_final}
    \nabla_{\theta} J(\pi_{\theta}) &= \mathbb{E}_{\tau \sim p_{\pi_{\theta_k}}} \left[ \sum_{t=0}^{\infty} \eta_t(\theta) A^{\pi_{\theta_k}} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right] \nonumber \\
    &\hspace{-3em} + \sum_{g \in G} \mathbb{E}_{\tau \sim p_{\pi_{\theta_k}}} \left[ \sum_{t=0}^{\infty} \eta_t^g(\theta) A^{\pi_{\theta_k}} \nabla_{\theta} \log \pi_{\theta}(a_t^g | s_t^g) \right],
\end{align}
with
\(
\eta_t^g(\theta) = \frac{p_{\pi_{\theta}}(s_t^g)}{p_{\pi_{\theta_k}}(s_t)} \frac{\pi_{\theta}(a_t^g | s_t^g)}{\pi_{\theta_k}(a_t | s_t)}
\), which retains the action probability of the original data.
For further details, we refer readers to~\citep{Mittal2024SymmetryCF}.

For each gradient update, we sample a mini-batch of trajectories from the original rollouts along with their corresponding transformed trajectories:
\[
\mathcal{D}_{\text{minibatch}} = \{\tau_i\}_{i=1}^{N_{\text{minibatch}}} \cup \bigcup_{g \in \mathcal{G}} \{\tau_i^g\}_{i=1}^{N_{\text{minibatch}}}.
\]
This approach ensures balanced learning across both the original and augmented tasks, promoting equitable representation for each task during the policy update.


\subsection{Memory augmentation}

\begin{figure}
\centering
% \includegraphics[trim=120 50 120 50, clip,width=1.0\linewidth]{images/approach/training_arch.pdf}
\includegraphics[trim=40 90 65 50, clip,width=1.0\linewidth]{images/approach/training_arch.pdf}
\caption{Overview of our training framework.}
\label{fig:training_framework}
\end{figure}

\begin{figure*}
\centering
\includegraphics[trim=80 160 80 160, clip,width=1\linewidth]{images/approach/mem_aug.pdf}
\caption{Memory augmentation. Transformation \( g = (g_o, g_a) \in \mathcal{G} \) is applied to observations \(o_t\) and actions \(a_t^g\) to generate augmented observations \(o_t^g\) and actions \(a_t^g\). The augmented observation sequence is forward passed through the RNN, producing hidden states \(h_t^g\).
During the \(k\)-th policy update, the initial augmented hidden state \(h_0^{g,k}\) is set to the last hidden state from the previous update \(h_T^{g,k-1}\), ensuring continuity and context retention across updates.}
\label{fig:mem_aug}
\end{figure*}

While experience augmentation can enhance generalization by introducing greater training diversity, the resulting variations, combined with the partially observable nature of the environment, may cause the agent to adopt over-conservative strategies as it struggles to infer the latent task context.
To address this limitation, we extend augmentation to the agent's memory by incorporating an RNN into the training framework, as illustrated in \figref{fig:training_framework}.
The RNN, leveraging its context modeling capability, acts as an implicit task encoder.
At each time step, the RNN updates its hidden state \( h_t \) based on the observations, progressively capturing task-specific information accumulated so far.
From this hidden state, the RNN generates a latent task embedding \( z_t \), which conditions the policy, allowing it to adapt decisions based on the inferred context.

In addition to augmenting the trajectories, we extend the RNN’s hidden states to incorporate memory for the augmented tasks.
As shown in \figref{fig:mem_aug}, this is achieved by performing a forward pass with the transformed observation sequence \( (o_t^g)_{t=0}^{T} \) through the RNN.
During this process, the RNN generates a sequence of hidden states \( (h_t^g)_{t=0}^{T} \) that progressively encode the evolving dynamics of the augmented observations.
To ensure continuity in the context encoding, the initial hidden state \(h_0^{g,k}\) for the current update \(k\) is set to the final hidden state \(h_T^{g,k-1}\) from the previous update. 
The forward pass is performed continuously throughout the episode to ensure that the RNN’s hidden states capture the cumulative dynamics of the augmented observations at each step, providing an up-to-date context encoding for subsequent updates.
The latent task embeddings \( (z_t^g)_{t=0}^{T} \), generated from these hidden states, provide the policy with the inferred context of the augmented tasks.

This memory augmentation allows the policy to infer unobservable context information for both ID and augmented scenarios. 
In contrast, a simple feedforward policy without memory relies solely on immediate partial observations and lacks the ability to infer latent task context, which may lead to degraded ID performance when augmented data is introduced.

Our training process incorporating memory augmentation is outlined in~\Algref{alg:memory_augmented_ppo}.

\begin{algorithm}[H]
\caption{Recurrent PPO with Memory Augmentation}
\label{alg:memory_augmented_ppo}
\begin{algorithmic}[1]
\Require Number of iterations $ N_{\text{iter}} $
\Ensure Optimal policy and RNN parameters $ \theta^*, \psi^*$
\State Initialize environment, policy $ \pi_\theta $ and RNN$_{\psi}$;
\State Initialize hyperparameters $ \epsilon, \gamma $;
\State Define transformation set $ \mathcal{G} $ for experience augmentation;

\For{\( k = 1, \dots, N_{\text{iter}} \)}
    \State Collect rollouts \( \mathcal{D}_{\text{ID}} \) using \( \pi_\theta \);
    \State Generate  \(\mathcal{D}_{\text{aug}}^{\text{OOD}}\) by applying transformations \(g \in \mathcal{G}\);
    \State Update RNN hidden states via forward pass; 
    % \Comment {No gradient}
    \State Combine rollouts: \(\mathcal{D}_{\text{train}}  \gets \mathcal{D}_{\text{ID}} \cup \mathcal{D}_{\text{aug}}^{\text{OOD}}\);
    \For{\(\mathcal{D}_{\text{minibatch}}\) sampled from \(\mathcal{D}_{\text{train}}\)}
        \State Compute surrogate objective \( L_{\text{clip}} \);
        \State Update \( \theta \) and \(\psi\);
    \EndFor
\EndFor

\State \Return \( \theta^*, \psi^* \)
\end{algorithmic}
\end{algorithm}