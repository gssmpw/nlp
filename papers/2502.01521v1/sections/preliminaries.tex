\subsection{Partially observable Markov Decision Process and Meta-RL}
We model the problem as a Partially Observable Markov Decision Process (POMDP).
We define a POMDP as the tuple 
\(
\mathcal{M} = \langle \mathcal{S}, \mathcal{O}, \mathcal{A}, p, r, \gamma, \rho_0 \rangle,
\)
where \( \mathcal{S} \) is the set of states, \( \mathcal{O} \) is the set of observations, \( \mathcal{A} \) is the set of actions, 
\( p : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1] \) is the state transition function, 
\( r : \mathcal{S} \times \mathcal{A} \to \mathbb{R} \) is the reward function, 
\( \gamma \) is the discount factor, and 
\( \rho_0 : \mathcal{S} \to [0, 1] \) is the initial state distribution.

In POMDPs, observations provide only partial information about the true underlying state, requiring the agent to reason about the task context.
In conventional RL, the agent aims to learn a policy \( \pi \) that maximizes the expected cumulative reward:
\[
J(\pi) = \mathbb{E}_{\tau \sim p_\pi(\tau)} \left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \right],
\]
where \( \tau = (s_0, a_0, s_1, \dots) \) represents a trajectory sampled from the POMDP \( \mathcal{M} \) under the policy \( \pi \).

In this work, we frame the problem of learning across multiple tasks as a meta-RL problem, with the objective of optimizing over a distribution of tasks \( p(\mathcal{T}) \).
In our setting, this corresponds to a distribution of POMDPs \( \mathcal{M}(\mathcal{T}) \). 
We focus on the scenario where each task \( T \in \mathcal{T} \) exhibits unique dynamics while sharing a common reward function.
The goal is to learn a policy \( \pi \) that maximizes the expected cumulative reward across tasks: 
\[
J(\pi) = \mathbb{E}_{T \sim p(\mathcal{T}), \tau \sim p_\pi(\tau \mid T)} \left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \right].
\]

\subsection{Proximal Policy Optimization}
In this work, we apply Proximal Policy Optimization (PPO)~\citep{schulman2017PPO} as the RL algorithm.
At the \( k \)-th learning step, the agent collects trajectories by executing its policy, parameterized by \( \theta_k \).
To optimize the policy, PPO employs importance sampling to address the discrepancy between the current policy  \( \pi_\theta(a_t \mid s_t) \) and the policy from the previous update \( \pi_{\theta_k}(a_t \mid s_t) \) used to collect the data. 
The policy gradient is expressed as:

\begin{equation}
\label{eq:policy_grad_ppo}
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim p_{\pi_{\theta_k}}} \left[ \sum_{t=0}^{\infty} \eta_t(\theta) A^{\pi_{\theta_k}} \nabla_\theta \log \pi_\theta(a_t | s_t) \right],
\end{equation}
where \( \eta_t(\theta) = \frac{p_{\pi_{\theta}}(s_t)}{p_{\pi_{\theta_k}}(s_t)} \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_k}(a_t | s_t)}\) is the importance sampling weight,
and \( A^{\pi_{\theta_k}} = A^{\pi_{\theta_k}}(s_t, a_t)\) is the advantage function under the policy \( \pi_{\theta_k} \).
To stabilize training, PPO optimizes a clipped surrogate loss \(L_{\text{clip}}(\theta)\)~\citep{schulman2017PPO}, which constrains the magnitude of policy updates.
Additionally, Generalized Advantage Estimation (GAE)~\citep{schulman2015GAE} is commonly used to compute the advantage function, balancing bias and variance for efficient learning.


\subsection{Problem statement}
We consider a set of tasks \( \mathcal{T} \), where \( \mathcal{T}_{\text{ID}} \subset \mathcal{T} \) represents the subset of tasks accessible during training, referred to as ID tasks. The remaining tasks, \( \mathcal{T}_{\text{OOD}} = \mathcal{T} \setminus \mathcal{T}_{\text{ID}} \), constitute the set of unseen tasks that differ from the training tasks. 
In this work, we focus on a subset of these unseen tasks, \( \mathcal{T}_{\text{OOD}}^{\text{aug}} \subset \mathcal{T}_{\text{OOD}} \), which can be effectively simulated through task-structured augmentations without requiring additional interactions with the environment.
These tasks exhibit different dynamics from \( \mathcal{T}_{\text{ID}} \) while maintaining the same reward structure. 
Our goal is to train a single policy that can adapt and perform well across both \( \mathcal{T}_{\text{ID}} \) and \( \mathcal{T}_{\text{OOD}}^{\text{aug}} \). 
This is formalized by maximizing the expected cumulative reward:
\[
J(\pi) = \mathbb{E}_{T \sim p(\mathcal{T}_{\text{ID}} \cup \mathcal{T}_{\text{OOD}}^{\text{aug}}), \tau \sim p_\pi(\tau \mid T)} \left[ \sum_{t=0}^\infty \gamma^t r_{T}(s_t, a_t) \right].
\]
During training, only partial observations are available for \( \mathcal{T}_{\text{ID}}\). 
In this setting, the agent must infer the task context implicitly based on past observations and actions.

