\section{Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For AI technology to gain widespread acceptance, its reliability and performance must be rigorously tested (\shortciteNP{hong2023statistical}). Several studies have evaluated LLMs' programming capabilities. \shortciteN{atkinson2023chatgpt} analyzed GPT 3.5's performance across multiple programming languages. \shortciteN{bubeck2023sparks} examined GPT 4.0's performance on 40 coding problems, showing it outperforms GPT 3.5 in code generation. Similarly, \shortciteN{yeadon2024comparison} compared the coding performance of human students, GPT 3.5, and GPT 4.0 on university-level programming tasks. \shortciteN{Songetal2025-coderating} conducted human evaluations of LLMs' performance in SAS programming across various statistical tasks.

Benchmarks have been developed to assess LLMs' programming abilities, primarily for widely used languages like Python. \shortciteN{chen2021evaluatinglargelanguagemodels} introduced HumanEval, a benchmark for evaluating LLMs on Python function-level code generation. Other notable benchmarks include MBPP (\shortciteNP{austin2021program}), APPS (\shortciteNP{hendrycks2021measuring}), Multi-HumanEval (\shortciteNP{athiwaratkun2022multi}), and CoderEval (\shortciteNP{yu2024codereval}). The ``BigCodeBench'' study (\shortciteNP{zhuo2024bigcodebenchbenchmarkingcodegeneration}) highlights the need for evaluating LLMs on diverse function calls and complex instructions to enhance real-world applicability.

Class-level benchmarks, such as ClassEval (\shortciteNP{du2024evaluating}), provide a more realistic assessment by examining how functions interact within a class. Multilingual benchmarks like MBXP and Multi-HumanEval1 have attempted to address language diversity by translating Python into other languages (\shortciteNP{athiwaratkun2022multi}). CoderEval (\shortciteNP{yu2024codereval}) further advanced real-world testing by incorporating non-standalone functions. RepoCoder introduced a repository-level framework for code completion, incorporating context-aware evaluation through the RepoEval benchmark (\shortciteNP{zhang2023repocoder}). ConvCodeWorld's multi-turn interaction paradigm (\shortciteNP{han2025convcodeworld}) better simulated iterative statistical workflows, while CodeEditorBench (\shortciteNP{guo2024codeeditorbench}) focused on code editing tasks, revealing LLMs' challenges in preserving context during incremental modifications. Despite these advancements, benchmarks for statistical languages such as SAS and R remain underrepresented.



Establishing data repository is crucial for research at the intersection of AI and statistics (\shortciteNP{Zhengetal2025-datareview}). Several efforts have focused on developing benchmarks for statistical analysis. \shortciteN{hu2024infiagent} introduced InfiAgent-DABench, a benchmark for assessing LLMs' data analysis capabilities in Python. \shortciteN{zhu2024largelanguagemodelsgood} presented StatQA, which evaluates LLM performance in elementary statistical analysis. \shortciteN{liu2024llmscapabledatabasedstatistical} proposed a benchmark to assess LLMs' ability to perform statistical and causal analysis with real-world data. \shortciteN{huang2024evaluating} compared ChatGPT 4.0's data analysis capabilities to traditional statistical software. Unlike these studies, StatLLM offers a novel contribution by providing a comprehensive dataset specifically designed to evaluate LLMs' proficiency in statistical programming, with a primary focus on SAS.









\begin{figure}%[htbp]
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/DataStructure.pdf}
\caption{An overview of the structure of the StatLLM dataset.}\label{fig:data.structure}
\end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%