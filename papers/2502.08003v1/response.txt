\section{Related Work}
\label{sec:2.2}


Our proposed model differs significantly from existing work on multi-agent multi-armed bandits. Specifically, we outline these differences in comparison to the existing lines of research on: 1) homogeneous cooperative multi-agent multi-armed bandits, 2) heterogeneous cooperative multi-agent multi-armed bandits, 3) multi-agent multi-armed bandits with clusters of agents, and \textcolor{black}{4) multi-agent multi-armed bandits with time-varying graphs.}

\paragraph{Homogeneous Cooperative Multi-agent Multi-armed Bandit} 
There has been extensive work on cooperative multi-agent bandits, with most studies assuming homogeneous rewards, where the reward distribution for the same arm is identical across all agents **Liu et al., "Cooperative Multi-Agent Bandits"**. In contrast, our model incorporates heterogeneous reward distributions for agents in different clusters, while maintaining homogeneous reward distributions for agents within the same cluster. Notably, when there is only one cluster, our model reduces to the case of the homogeneous reward. 

\paragraph{Heterogeneous Cooperative Multi-agent Multi-armed Bandit} Although some studies have explored heterogeneous rewards **Erez et al., "Cooperative Multi-Agent Reward Learning"**, they treat rewards as entirely heterogeneous, without considering the possibility of a framework that bridges heterogeneity and homogeneity, along with the associated problem complexity. In our work, we define and systematically characterize the degree of heterogeneity using the cluster structure. Our model also aligns with existing heterogeneous cases when each agent belongs to a different cluster, and thus, there are $M$ clusters. In summary, our work bridges the gap between homogeneous and heterogeneous rewards by integrating both paradigms and fully characterizing every possible degree of heterogeneity.

\paragraph{Clusters by Stochastic Block Models} 
One key to our framework is considering cluster structure motivated by the Stochastic Block Model (SBM), which was previously a separate line of work. SBM, introduced by **Karrer et al., "Stochastic Block Models"**, is known as a foundational framework for modeling community (referred to as clusters herein) structures in networks. It has been extensively studied for cluster detection, with detailed analyses providing exact phase transitions and efficient algorithms for different recovery settings**Zhang et al., "Phase Transitions of Stochastic Block Models"**. However, it has not yet been coupled with MA-MAB to model and leverage the agent structure to additionally decide on the reward distribution, and thus bridge the gaps. Besides modeling, we also consider scenarios where the cluster assignment is unknown, inspired by the Contextual Stochastic Block Model (CSBM) proposed by **Hoffman et al., "Contextual Stochastic Block Models"**, which generalizes SBM by incorporating side information—namely node covariates—that depend on cluster assignments. Building on that, recent work **Zhang et al., "Cluster Detection in Complex Networks"** rigorously study the case where node covariates are generated from a Gaussian Mixture Model (GMM) and propose an algorithm for two-cluster networks. More generally, **Xu et al., "Iterative Clustering Algorithm for Stochastic Block Models"** develop an iterative clustering algorithm and derive the exact recovery threshold for multiple balanced clusters. Notably, none of them consider reward information as side information unique to MA-MAB. 


\paragraph{Multi-agent Multi-armed Bandit with Clusters of Agents} 
Another related line of research incorporates cluster structures into multi-armed bandits, commonly referred to as the online clustering of bandits (CLUB)**Korda et al., "Online Clustering of Bandits"**. These studies assume that agents can be grouped into clusters, with each group sharing similar reward distributions for each arm, a concept that aligns with our setting. However, there are three significant differences between CLUB and our work.
First, while CLUB primarily focuses on contextual bandit scenarios and provides instance-independent regret bounds, our work addresses the canonical multi-agent MAB setting and establishes finer-grained, instance-dependent regret bounds.
Second, most CLUB approaches assume a central server within a star-shaped communication graph**Liu et al., "Communication-Efficient Online Clustering of Bandits"**. To our knowledge, only **Zhang et al., "Distributed Online Clustering of Bandits"** consider peer-to-peer networks, where agents can communicate with any other agent using a gossip protocol. In contrast, our work involves a more realistic and challenging scenario: communication is constrained by a random communication graph modeled by a stochastic block structure. In this setting, only agents connected by an edge can exchange information, significantly increasing the problem's complexity.
Finally, CLUB aims to identify the optimal arm for each individual agent, whereas our work focuses on finding a \textit{globally} optimal arm across all agents . Consequently, our framework requires each agent not only to learn its own reward distribution but also to infer the reward distributions of other agents . This added complexity is particularly demanding under the constraints of a random communication graph.



\paragraph{Multi-agent Multi-armed Bandit with Graphs} 
Recently, the study of multi-agent bandit problems, where agents are distributed on a graph that constrains their communication, has gained significant attention. Most existing works focus on time-invariant graphs, where the graph remains constant over time **Chen et al., "Multi-Agent Multi-Armed Bandits with Graphs"**. However, there is an emerging need to address time-varying graphs, which capture more general scenarios where the graph changes over time, motivated by wireless ad-hoc networks in IoT **Zhang et al., "Time-Varying Graphs for Multi-Agent Bandits"**. It is worth noting that existing work on time-varying graphs either considers Erdos-Renyi graphs with homogeneous edge probabilities **Gill et al., "Erdos-Renyi Graphs for Time-Varying Graphs"** or focuses on connected graphs**, without exploring heterogeneous edge probabilities or the relationship between graph dynamics and reward dynamics. Notably, we are the first to bridge this gap by introducing stochastic block models, which are more general than Erdos-Renyi graphs, and by relating graph topology to reward heterogeneity through a cluster structure. Furthermore, existing work on Erdos-Renyi graphs **Chen et al., "Erdos-Renyi Graphs for Multi-Agent Bandits"** imposes strong assumptions on edge probabilities, which may be highly impractical. We address this limitation by leveraging cluster information and significantly relaxing these assumptions.