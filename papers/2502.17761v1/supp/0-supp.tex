
\hheading{Datasets}

\noindent\textbf{Prostate Cancer} Archived formalin-fixed and paraffin-embedded (FFPE) prostatectomy specimens were collected from $n=11$ patients and imaged with micro-computed tomography (microCT). Following imaging, either one or two histological sections per sample were prepared using a microtome depending on the sample thickness, stained with H\&E, and subsequently subjected to Visium spatial transcriptomics profiling. Overall, we obtained 65,715 spots. 
In addition to the internal cohort, we compiled data from multiple public sources.  The dataset includes three sources: (1) a study where Visium profiling was applied to 22 frozen H\&E-stained histological sections from 2 patients with prostate adenocarcinoma, yielding 58,906 spots\cite{erickson2022spatially}, (2) a study that used Spatial Transcriptomics technology on 24 needle biopsies snap frozen from three patients with prostate adenocarcinoma, resulting in 3,969 spots\cite{marklund2022spatio}, and (3) the public 10x Genomics repository, which provides three FFPE prostate samples from both healthy and diseased individuals, totaling 9,957 spots. Collectively, the public cohort we compile consisted of 72,832 spots from 49 histological sections. Overall, from public and in-house cohorts, we obtain 138,547 spots from 68 sections and 20 patients. Further details can be found in \textbf{Extended Data Table \ref{tab:dataset}}.


\noindent\textbf{Breast Cancer} We compiled 58,263 H\&E image patches with associated transcriptomic profiles from 101 histological sections and 35 patients from three previous works\cite{andersson2021spatial, staahl2016visualization, he2020integrating} as well as the 10x Genomics public repository. The samples are a mixture of frozen and FFPE histological sections with associated transcriptomics profiles from a 
sequencing-based ST technology (Visium or Spatial Transcriptomics). 
2.5D tissue images consisting of serial tissue sections were obtained from a previous study \cite{andersson2021spatial}. The cohort includes samples from 8 patients (A-H): four patients (A-D) with 6 serial sections each, and four patients (E-H) with 3 serial sections each. The histological sections were acquired at the thickness of 16~\textmu m, with a spacing of 32~\textmu m between each section.  Overall, 6,577 paired H\&E image patches with associated 2.5D morphology and ST spots from 16 histological sections were availble. Further details can be found in \textbf{Extended Data Table \ref{tab:dataset}}.

\noindent\textbf{Colorectal Cancer} We curated a dataset of paired ST-H\&E samples from colon, rectum, and cecum areas of human donors that had been subjected to a sequencing-based ST technology (Visium or Spatial Transcriptomics). The dataset is a compilation of three sources of publicly available data:  two studies on colorectal cancer \cite{valdeolivas2024profiling,mirzazadeh2023spatially}, and the publicly available samples on the 10x Genomics website. Overall, the dataset consists of 72,042 ST spots with paired H\&E image patches from 26 histological sections and 13 patients. 
For validation, we used the public cohort \cite{valdeolivas2024profiling} which includes 
samples from 7 patients, each with 2 histological sections, totaling 20,708 spatial transcriptomic spots (Dataset CRC I in \textbf{Extended Data Table \ref{tab:dataset}}). 

We additionally apply the model to publicly available large CRC tissue volume sample~\cite{lin2023multiplexed}. The tissue volume has dimensions of 1.6~cm~x~1.6~cm~x~0.5~cm and consists of 22 serial H\&E-stained histological sections obtained from a specimen with poorly differentiated stage IIIB adenocarcinoma and adjacent normal tissue resected from the cecum of a 69-year old male. 



\hheading{Data acquisition}

\noindent\textbf{MicroCT} MicroCT imaging was performed on the collection of FFPE prostate cancer tissue blocks using the Versa 620 X-ray Microscope (Carl Zeiss, Inc., Pleasanton, California, USA) at a resolution of 4 $\mu m$/voxel. For each scan, a microfocus X-ray source operating at a tube voltage of 40~kV and a filament current of 75~mA (3~watts) was utilized. A total of 4,501 projection images were acquired, with the sample rotated by 0.08 degrees per projection (360 degrees/4,501). The images were captured using a 16-bit flat panel detector with a resolution of 3,064 $\times$ 1,928 pixels, resulting in a stack of 1,300 2D images along the depth dimension. Each projection was averaged from 15 frames, with an exposure time of 0.5 seconds per frame (totaling 7.5 seconds per projection) to enhance the signal-to-noise ratio. The detector recorded raw grayscale intensity values for each voxel. Scanning each sample required 11 hours, to cover a field-of-view measuring 12.8~mm~$\times$~ 7.68~mm$\times$~ 5.2~mm (3,200~$\times$~1,920~$\times$~1,300 voxels). 
To ensure a consistent image intensity scale across samples, we normalize the 3D tissue image for each sample with the lower threshold of 25,000 intensity value and the upper threshold to the top 1\% of each tissue’s intensity value.


\noindent\textbf{Visium spatial transcriptomics on Prostate samples}
Following the imaging with microCT, two sections spaced 250 $\mu$m apart were obtained from samples P1~$\sim$~P5, and one section was cut in samples P6 $\sim$ P11. Each section was subject to Visium ST, where the RNA expression measurements were performed in spots with a diameter of 55~$\mu$m arranged in a grid with a centre-to-centre distance of 100~$\mu$m. The number of spots in each section ranged from 3,402 to 4,760. DV200 was first performed on the tissue sections for quality control, followed by the Tissue Adhesion Test outlined in the 10x Genomics protocol. The sections (5 $\mu$m in thickness) were placed on a Visium Spatial Gene Expression Slide following the Visium Spatial Protocols–Tissue Preparation guide. Sections were left drying  and deparaffinized following the  protocol for Visium Spatial Gene Expression for FFPE – Deparaffinization, H\&E Staining, Imaging \& Decrosslinking
(10x Genomics, CG000409 Rev D). Tissue sections were stained with H\&E and imaged at 0.62 $\mu$m/pixel resolution. Decrosslinking was immediately carried out for H\&E-stained sections. Subsequently, human whole transcriptome probe panels were applied to the tissue. After probe hybridization and target gene ligation, the ligation products were released through RNase treatment and tissue permeabilization. Ligation probes were finally hybridized to the spatially barcoded oligonucleotides on the capture area.  Spatial transcriptomics libraries were constructed from the probes and sequenced using an Illumina NovaSeq 6000 system 300 cycle with an S4 flow cell. High-resolution tissue images were captured with Olympus BX51 scope and DP74 camera.



\hheading{3D image data preprocessing}

\noindent\textbf{Cross-modal registration} To align genomic expression data with volumetric microCT images, a registration pipeline was deployed to map H\&E images (and corresponding spot coordinates) onto microCT volumes. Given the intensity and color differences between microCT and H\&E-stained histologies, a feature-based approach was chosen over intensity-based registration. The registration paradigm consists of two main steps: first, identifying the (virtual) plane within the microCT volume that corresponds to the H\&E-stained image thereby determining the exact tilt angle during the sectioning process; and second, achieving precise 2D alignment between the H\&E-stained image and its corresponding microCT plane. In more detail, we first downscaled the H\&E images to match the resolution of the microCT images (4 $\mu$m/pixel). Then,
we followed a previous work\cite{chicherova2014histology} to extract Speeded Up Robust Features (SURF) descriptors \cite{bay2006surf} for each axial microCT plane and matched those to the histology image using a second-nearest-neighbor-criteria. Following this, RANSAC plane fitting \cite{fischler1981random, dong2022deciphering} was used to fit a plane to the 3D point cloud of matching points. The resulting plane matching the H\&E-stained histological section was virtually cut from the microCT volume. For the second step, non-elastic registration\cite{gatenbee2023virtual} was applied to the microCT plane to achieve pixel-wise alignment between corresponding images from both modalities. After microCT – histology registration, the coordinates of the spots containing the gene expression levels were readily aligned to the volumetric microCT scans.
 
\noindent\textbf{Image patches} We downsample all H\&E whole-slide-images (WSIs) to 1 $\mu m$/pixel to ensure the image resolutions are consistent. Next, we crop $112\times112$ pixel image patch centered around each ST spot to obtain data pairs of 2D patch and 2D ST. 
Following image co-registration of prostate microCT images to their corresponding H\&E images with associated ground truth ST expression, we crop $112\times 112$ pixel patch ($4 \mu m$/pixel) at the same axial locations as in the H\&E images. In the \textit{2D} training setting, we only consider the 2D microCT plane image for which the ST measurement data is available, resulting in 2D patches of $112\times 112\times 1$ voxels ($448~\mu m \times 448~\mu m \times 4~\mu m $). In the \textit{3D} and \textit{3D+VOI} settings, 20 adjacent planes (10 planes above and below the central plane with 2D ST data) are incorporated to capture tissue context, forming 3D patches with $112\times 112\times 21$ voxels ($448~\mu m \times 448~\mu m \times 84~\mu m$). The intensity in each patch is then normalized to [0,1].  

\hheading{2.5D image data preprocessing}

\subsection{Image registration - Serial H\&E sections}
Serial histological sections from the breast and colorectal cancer cohorts were co-registered to generate 2.5D digital tissue samples. The images were 
first downsampled to 1~$\mu$m/pixel and then aligned sequentially, with the middle section of the image stack serving as the reference image. Alignment was performed using a two-step process: a landmark-based rigid registration followed by a non-rigid registration for enhanced precision.  For the breast cohort, where each histological section has corresponding 2D ST data, ST spots of consecutive sections were aligned by applying the same registration transformation that had been used for image alignment, using the VALIS registration framework \cite{gatenbee2023virtual}.

\subsection{Image patches}
Upon serial registration of tissue sections, in the breast cancer cohort, we construct 2.5D patch of $112\times 112\times 3$ pixel from three consecutive neighboring sections that are $32\mu m$ apart. Specifically, the tissue section for 2D ST prediction was assigned the central section, and the sections immediately above and below it were considered. Consequently, for samples with six sections, we only considered ST prediction for four middle sections for which 2.5D context was available (S2$\sim$S5). For samples with three sections, we only considered the single middle section. In the colorectal cancer samples used for model training and evaluation, we crop
2D image patches of $112\times 112$ pixels at 1 $\mu m$/pixel resolution. For the colorectal cancer sample consisting of 22 serial histological sections with no ST data available, we crop non-overlapping $112~\mu m\times112~\mu m$ image patches on each plane.


\hheading{Transcriptomic data preprocessing}

\noindent\textbf{ST spot filtering and expression normalization} Spatial transcriptomics spots are first filtered based on the number and type of expressed genes. Spots containing gene expression of at least 25 genes and with less than 20\% of mitochondrial genes were considered. For ST prediction, we preprocess the gene counts with a series of two transformations. First, we normalize the total gene expression of each spot to a library size of 10,000 to equalize the sequencing depth across different samples and spots. This is then followed by a log transformation. The normalized gene expression for each spot is smoothed by averaging its expression values with that of its immediate spot neighbors ($\sim$10 closest neighbors) for removing spot-specific measurement noise\cite{he2020integrating, chung2024accurate, zhang2024inferring}.


\noindent\textbf{Gene expression panel for evaluation} 
Different ST technologies provide the expression levels of different sets of genes. For example, in the prostate cancer cohort, comprised of data from four sources, the number of sequenced genes per spot ranges from 17,943 to 33,538 (\textbf{Extended Data Table \ref{tab:dataset}}). To integrate data from multiple sources, we first identified the intersection of gene expression panels for each cancer cohort, considering only genes common to all spots. This process resulted in 8,136 genes for the prostate cohort, 8,034 for the breast cohort, and 10,765 for the colorectal cancer cohort. For the main experiments, we first curated the 250 genes with the highest mean expression (HEG) for a given tissue cohort, in line with the previous studies\cite{he2020integrating, chung2024accurate}. We subsequently incorporated a set of genes with prognostic value for each tissue cohort into the 250 gene set. For the prostate cancer cohort, we utilized the genes from Oncotype DX and Decipher, which are molecular assays used for evaluating prostate cancer risk, resulting in \textit{All genes} panel with 264 genes. For the breast cancer cohort, we incorporated the genes from the prognostic score HER2DX\cite{prat2020multivariable}, as well as a set of genes involved in evading anti-HER2 therapy\cite{smith2021her2+}, yielding an \textit{All genes} panel with 269 genes. For the colorectal cancer cohort, we considered a set of mutated driver genes and genes significant in several key pathways from two studies\cite{nunes2024prognostic,valdeolivas2024profiling}, resulting in \textit{All genes} panel with 276 genes. To assess the robustness of $\ours$ on different gene expression panels, we also analyze a gene expression panel with 1,000 HEGs and also with 250 highly-variable genes (HVG). Marker gene names for each cancer cohort are included in \textbf{Extended Data Table \ref{tab:marker_genes}}.

\noindent\textbf{Gene expression input processing} To prepare the transcriptomics for being encoded with the scGPT transcriptomics encoder\cite{cui2024scgpt}, we applied the default preprocessing transformations required for fine-tuning this gene encoder. Specifically, the 1,200 most HVGs were selected from each cancer cohort and filtered for each spot. This was followed by a log1p transformation and a value binning technique to convert expression counts into relative values.

\hheading{$\ours$ architecture}

$\ours$ combines the two directions of ST evaluation, the direct regression-based approaches\cite{he2020integrating} and cross-modal alignment approaches\cite{xie2024spatially, min2024multimodal}, resulting in the model with ST prediction and image-ST alignment branch, similar to CoCa\cite{yu2022coca} in vision-language literature. Aligning ST to the corresponding image modalities, 2D H\&E image patches and 3D microCT patches, allows $\ours$ to perform cross-modal retrieval tasks in addition to ST prediction, making $\ours$ a flexible framework for diverse tasks. 
$\ours$ is comprised of four main components: \textit{2D image encoder}, \textit{transcriptomics encoder}, \textit{3D image encoder}, and \textit{transcriptomics predictor}. 

\noindent\textbf{2D image encoder} We choose CONCH\cite{lu2024visual} as the \textit{2D image encoder} for two reasons. First, it was pretrained on histology regions with diverse types and stains, including frozen tissue, FFPE, and immunohistochemistry, yielding image features robust to different tissue processing protocols across data sources. Next, it was shown to be one of the most competitive models for predicting transcriptomic profiles for diverse cancer types on public HEST-1K benchmark\cite{jaume2024hest} and showing robust performance across different tissue stains and textures\cite{filiot2025distilling}. 
Instead of directly using 512-dimensional image patch embedding from CONCH, we use embeddings from the pre-contrastive module, which is a set of 196 ($= 14 \times 14$) patch token embeddings, each of which is dimension $\mathbb{R}^{768}$. This provides additional flexibility in using image encoder output embeddings for different downstream tasks, as further mentioned in \textbf{Attentional Poolers}.
To aggregate training pairs from diverse sources, $\ours$ needs to handle batch effects arising from integration of diverse data sources. Besides using image and transcriptomics encoders pretrained on diverse data sources, we include a lightweight MLP to encode the source/batch ID to distill biological variations while removing batch-associated variations during training through a domain adaptation loss, following previous work\cite{ganin2015unsupervised, vaidya2024demographic, cui2024scgpt}. Upon training, the MLP module is discarded for downstream tasks. 

\noindent\textbf{3D image encoder} To encode a 3D patch, we use a transformer-based architecture (ViT-B/16) pretrained on natural images (ImageNet-1K) as the backbone of the 3D image encoder. Instead of treating 3D patch as a volume, we treat it as a stack of 2D patches. Specifically, the image encoder first extracts a set of 196 2D patch token features for every 2D section of the 3D patch. A depth-specific learnable embedding is then added to each set of token features. Following the works in video processing\cite{alayrac2022flamingo}, the same learnable embedding is added to all the token features in the same depth, without additional 2D positional embeddings. Subsequently, the sets are merged to form a larger set of patch token embeddings\cite{alayrac2022flamingo}. For example, a 3D patch with depth 21 would result in 4,116 ($=196\times 21$) patch token features. The choice of ImageNet-pretrained VIT as the image backbone is motivated by the prior study\cite{song2024analysis}, which demonstrated that image encoders pretrained on natural images provided better transfer performance for the microCT data compared to other radiology-specific image encoders, due to inherent texture and resolution differences between MRI/CT and microCT. Notably, $\ours$ is flexible in its components, allowing easy replacement with more powerful modality-specific 3D imaging foundation models as they become available. For encoding 2.5D patches from the serial tissue sections dataset, we instead use the CONCH image encoder, as both 2D and 2.5D modalities are the same. 

\noindent\textbf{Attentional Poolers} The output of the image encoders is a set of token features for each image patch. For the 2D image encoder, this amounts to 196 ($=14\times 14$) token features per 2D image patch. For the 3D image encoder, this amounts to 4,116 ($196\times 21$ sections) token features per 3D image patch. A single-layer Transformer, termed \textit{attentional pooler}, facilitates the encoding of interactions between the token features set and a set of learnable embeddings (queries), each of which is dimension $\mathbb{R}^{768}$. Next, each query is projected to a lower dimension of $\mathbb{R}^{512}$ through a linear layer. The encoded queries are then used for subsequent downstream tasks.
In $\ours$, we introduce two attentional poolers, one for the ST reconstruction task and the other for the contrastive task, inspired by CoCa framework\cite{yu2022coca}. For a given $i^{\text{th}}$ image patch, the \textit{contrastive attentional pooler} for cross-modal alignment with constrastive learning uses a single query ($n_{\text{contrast}}=1$) to encapsulate the global representation of the patch, resulting in $\mathbf{h}_i^{\text{2D, cont.}}\in\mathbb{R}^{512}$ and $\mathbf{h}_i^{\text{3D, cont.}}\in\mathbb{R}^{512}$, for 2D and 3D patch, respectively. The \textit{reconstruction attentional pooler} uses $n_{\text{recon}}=32$ queries to capture more localized and fine-grained image details for ST prediction, resulting in $\{\mathbf{h}_{i,j}^{\text{2D, rec.}}\}_{j=1}^{32}$ and $\{\mathbf{h}_{i,j}^{\text{3D, rec.}}\}_{j=1}^{32}$ with $\mathbf{h}_{i,j}^{\text{2D, rec.}}, \mathbf{h}_{i,j}^{\text{3D, rec.}}\in\mathbb{R}^{512}$, for 2D and 3D patch,respectively.

\noindent\textbf{Transcriptomics encoder}
We encode ST data using a modified version of scGPT \cite{cui2024scgpt}, a single-cell foundation model pretrained on transcriptomics data from millions of cells of various cancer types. While initially developed for single-cell data, we adapt scGPT to encode transcriptomics data from Visium and Spatial Transcriptomics spots, which typically contain about 10 and 20 cells, respectively\cite{elosua2021spotlight}. This follows the successful adaptations of single-cell foundation models to encode transcriptomics data beyond single-cell, such as tissue bulk RNA expression and spatial transcriptomics data through model fine-tuning\cite{vaidya2025molecular,lee2024pathomclip}. ScGPT features three key components: a \textit{gene-name} encoder, an \textit{expression-value} encoder, and a \textit{transformer} encoder. The \textit{gene-name} encoder comprises an embedding layer that maps each gene to a fixed-length embedding vector of dimension 512. The \textit{expression-value} encoder consists of two fully connected layers with ReLU activation, which transform each gene expression value into a 512-dimensional vector. The output of the \textit{gene-name} encoder and the \textit{expression-value} encoder are then combined through element-wise addition, forming the input to the \textit{transformer} encoder, which is a stack of 12 Transformer layers, each with eight attention heads. The \texttt{<CLS>} token from the last transformer layer in fed into a single fully-connected layer for the transcriptomics embedding $\mathbf{g}_i\in\mathbb{R}^{512}$ for $i^{\text{th}}$ sequencing spot.
scGPT encoders are initialized from the \texttt{pancancer} checkpoint (pretrained on 5.7 million cells of various cancer types) and the projection head is randomly initialized.

\noindent\textbf{Transcriptomics predictor} We use a Transformer with a single layer followed by a single fully-connected layer as the transcriptomics predictor $f_{\text{pred.}}$, which takes $\{\mathbf{h}_{i,j}^{\text{3D, rec.}}\}_{j=1}^{32}$ as the input to predict the ST expression levels, i.e., $\widehat{\mathbf{y}}_i = f_{\text{pred.}}(\{\mathbf{h}_{i,j}^{\text{3D, rec.}}\}_{j=1}^{32})$. Consequently, $\widehat{\mathbf{y}}_{i,j}$ corresponds to the $j^{\text{th}}$ gene expression prediction for $i^{\text{th}}$ spot. The predictor can also operate on 2D patch embeddings $\{\mathbf{h}_{i,j}^{\text{2D, rec.}}\}_{j=1}^{32}$, for earlier pretraining stages.

\hheading{$\ours$ training}

$\ours$  is trained over three stages designed to gradually build the capacity of 3D ST prediction for the volume-of-interest (VOI). The first two stages utilize 2D and 3D images of all the volumes except VOI in the same cancer cohort. If the 2D ST measurements from VOI are available, the third stage is performed to fine-tune the model. All three stages use adaptations of loss functions used in CoCa\cite{yu2022coca} designed to predict transcriptomics profiles from image embeddings while also aligning them with transcriptomics embeddings.

\noindent\textbf{Stage I: 2D Pretraining on cancer-specific heterogeneous samples} During the pretraining stage, we leverage all available paired 2D H\&E-stained histology and ST data for a given cancer cohort. At this stage, $\ours$ takes three types of data as inputs: 2D morphology from 112$\times$112~\textmu m histology image patches (112$\times$112 pixels) centered at the location of each of the ST spots, transcriptomics expression data after preprocessing, and the source ID for correcting batch effects. The 2D morphology and transcriptomics data are encoded with the 2D image encoder and transcriptomics encoder, respectively. The contrastive and reconstruction attentional poolers are randomly initialized and trained. The last three transformer layers from the 2D image encoder and transcriptomic encoder are also fine-tuned to provide task-specific embeddings. We apply a conventional data augmentation scheme to image patches such as horizontal flip, vertical flip, and color jittering. 

We use a combination of three loss functions: symmetric cross-modal contrastive learning objective ($\mathcal{L}_{\text{cont.}, \mathrm{I}}$), ST reconstruction loss ($\mathcal{L}_{\text{rec.},\mathrm{I}}$), and domain adaptation loss ($\mathcal{L}_{\text{da}}$).

\textbf{Contrastive loss ($\mathcal{L}_{\text{cont.}, \mathrm{I}}$)} We align the embedding space of the 2D image encoder and transcriptomic encoder using a symmetric cross-modal contrastive learning objective.  Specifically, for a batch of $M$ pairs $\left\{ \left( \mathbf{h}_{i}^{\text{cont.}}, \mathbf{g}_i \right) \right\}_{i=1}^{M}$ with $\mathbf{g}_{i}\in\mathbb{R}^{512}$ and $\mathbf{h}^{\text{cont.}}_{i}\in\mathbb{R}^{512}$ denoting the $i^{\text{th}}$ transcriptomic and histology (single query from the contrastive attentional pooler) normalized embeddings respectively, the loss function is defined as:
% Contrastive loss
\begin{equation}
\mathcal{L}_{\text{cont.}, \mathrm{I}} = 
- \frac{1}{2M} \sum_{i=1}^{M} \log \frac{\exp\left(\tau (\mathbf{h}_{i}^{\text{2D, cont.}})^{\top} \mathbf{g}_i \right)}{\sum_{j=1}^{M} \exp\left(\tau (\mathbf{h}_{i}^{\text{2D, cont.}})^{\top} \mathbf{g}_j \right)}
- \frac{1}{2M} \sum_{j=1}^{M} \log \frac{\exp\left(\tau \mathbf{g}_j^{T} \mathbf{h}_{j}^{\text{2D, cont.}} \right)}{\sum_{i=1}^{M} \exp\left(\tau \mathbf{g}_j^{T} \mathbf{h}_{i}^{\text{2D, cont.}} \right)},
\end{equation}
where $\tau$ is the temperature parameter. The first term represents histology-to-gene loss, and the second represents gene-to-histology loss. The loss function $\mathcal{L}_{\text{cont.,I}}$ aims to minimize the distance between paired embeddings while maximizing the distance between unpaired embeddings.

\textbf{Reconstruction loss ($\mathcal{L}_{\text{rec.},\mathrm{I}}$)} In addition to the contrastive loss, we use the reconstruction loss to minimize the error between the predicted gene expression and the ground truth ST profiles. Specifically, we minimize the mean squared error (MSE) between the (smoothed) ground truth gene expression ($\mathbf{y}_i$) and the predicted expression obtained from the histology image embeddings $\{\mathbf{h}_{i,j}^{\text{2D, rec.}}\}_{j=1}^{32}$,
% Reconstruction loss
\begin{equation}
\mathcal{L}_{\text{rec., I}} = \frac{1}{M} \sum_{i=1}^{M} \left\| \mathbf{y}_i - f_{\text{pred.}}\left(\{\mathbf{h}_{i,j}^{\text{2D, rec.}}\}_{j=1}^{32}\right) \right\|_2^2.
\end{equation}

\textbf{Domain adaptation loss ($\mathcal{L}_{\text{da}}$)} To address potential batch effects by integrating ST samples from multiple data sources, we train the MLP classifier to infer the batch source ID from the transcriptomic embedding ($\mathbf{g_i}$) and use a cross-entropy loss. Batch source IDs are defined based on the data sources included in \textbf{Extended Data Table \ref{tab:dataset}}. As the aim is to make the model invariant to the batch attribute, the negative of the attribute prediction loss is back-propagated, making the model poor in predicting the data source. 

The total loss minimized during this stage is defined as:
\begin{equation}
    \mathcal{L}_{\mathrm{I}} = \lambda_{\text{cont.},\mathrm{I}}\cdot\mathcal{L}_{\text{cont.},\mathrm{I}} + 
\lambda_{\text{rec.},\mathrm{I}}\cdot\mathcal{L}_{\text{rec.},\mathrm{I}} +
\lambda_{\text{da}}\cdot\mathcal{L}_{\text{da}}, 
\end{equation}
where we use $\lambda_{\text{cont.},\mathrm{I}}=\lambda_{\text{rec.},\mathrm{I}}=1$ and $\lambda_{\text{da}}=0.1$.
The model is trained with a batch size of 512 for 25 epochs. The initial five epochs are used for warmup, where the learning rate is linearly increased from 0 to $1 \times 10^{-5}$. Next, the cosine scheduler is applied with the learning rate decaying from $1 \times 10^{-5}$ down to 0 by the end of training. The weight decay is set to 0.01 and the AdamW optimizer is used with $\beta$ values of (0.9, 0.999). Further details on hyperparameters and training settings are provided in \textbf{Extended Data Table \ref{tab:stageI}}. 


\noindent\textbf{Stage II: 3D pretraining}
Upon establishing the morphomolecular link between 2D H\&E histology and transcriptomics, the second stage focuses on further fine-tuning $\ours$ to capture the relationship between the morphology present in 3D tissue imaging data, and transcriptomics.
Specifically, we encode the morphology of 3D tissue image data with the 3D image encoder and align the embedding to the corresponding 2D H\&E and ST embeddings. In doing so, we also fine-tune the transcriptomics predictor such that the model can transition from predicting ST from 2D H\&E patch to predicting ST from 3D microCT patch. To preserve the morphology-transcriptomics embedding space from the previous stage, the 2D image encoder and transcriptomics encoder are kept frozen. To account for the smaller size of microCT and ST data pairs compared to the pretraining dataset, we also keep the 3D image encoder frozen to prevent overfitting, instead opting to train the randomly initialized contrastive and reconstruction attentional poolers.


We use a combination of three loss functions: a symmetric cross-modal contrastive learning objective ($\mathcal{L}_{\text{cont., II}}$), a direct alignment loss ($\mathcal{L}_{\text{dir}}$), and a reconstruction loss ($\mathcal{L}_{\text{rec., II}}$).

\textbf{Contrastive loss ($\mathcal{L}_{\text{cont.}, \mathrm{II}}$)} Similar to the alignment between histology and transcriptomic embeddings in the first stage, we align the embedding space of the 3D image encoder to that formed between the 2D image encoder and transcriptomic encoder, using a dual symmetric cross-modal contrastive learning objective
% Contrastive loss
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{cont., II}} = 
& - \frac{1}{2M} \sum_{i=1}^{M} \log \frac{\exp\left(\tau (\mathbf{h}_{i}^{\text{3D, con.}})^{\top} \mathbf{g}_i \right)}{\sum_{j=1}^{M} \exp\left(\tau (\mathbf{h}_{i}^{\text{3D, con.}})^{\top} \mathbf{g}_j \right)} 
- \frac{1}{2M} \sum_{j=1}^{M} \log \frac{\exp\left(\tau \mathbf{g}_j^{\top} (\mathbf{h}_{j}^{\text{3D, con.}}) \right)}{\sum_{i=1}^{M} \exp\left(\tau \mathbf{g}_j^{\top} (\mathbf{h}_{i}^{\text{3D, con.}}) \right)} \\
& - \frac{1}{2M} \sum_{k=1}^{M} \log \frac{\exp\left(\tau (\mathbf{h}_{k}^{\text{3D, con.}})^{\top} (\mathbf{h}_{k}^{\text{2D, con.}}) \right)}{\sum_{l=1}^{M} \exp\left(\tau (\mathbf{h}_{k}^{\text{3D, con.}})^{\top} (\mathbf{h}_{l}^{\text{2D, con.}}) \right)} 
- \frac{1}{2M} \sum_{l=1}^{M} \log \frac{\exp\left(\tau (\mathbf{h}_{l}^{\text{2D, con.}})^{\top} (\mathbf{h}_{l}^{\text{3D, con.}}) \right)}{\sum_{k=1}^{M} \exp\left(\tau \mathbf{h}_{l}^{\text{2D, con.}})^{\top} (\mathbf{h}_{k}^{\text{3D, con.}}) \right)}.
\end{aligned}
\end{equation}

\textbf{Direct Alignment ($\mathcal{L}_{\text{dir.}}$)} The 3D microCT modality presents different intensity, texture, and resolved structures compared to the 2D H\&E images. Therefore, for accurate ST prediction with microCT data, it is imperative to minimize the gap between different imaging modalities and significantly leverage the first pretraining stage based on 2D H\&E imaging modality. To this end, we introduce a second alignment loss ($\mathcal{L}_{\text{dir}}$) that minimizes the Euclidean distance between the 2D image patch token embeddings $\{\mathbf{h}_{i,j}^{\text{2D, rec.}}\}_{j=1}^{32}$ and 3D image patch token embeddings $\{\mathbf{h}_{i,j}^{\text{3D, rec.}}\}_{j=1}^{32}$ from the reconstruction attentional pooler. The alignment of two modalities through minimizing Euclidean distance, instead of the contrastive approach, is inspired by an alternate approach for aligning multiple modalities\cite{yang2021multi}. The loss can be written as
% Direct alignment loss 
\begin{equation}
\mathcal{L}_{\text{dir.}} = \frac{1}{M} \sum_{i=1}^{M} \sum_{j=1}^{32} \left\| \mathbf{h}_{i,j}^{\text{2D, rec.}} - \mathbf{h}_{i,j}^{\text{3D, rec.}} \right\|_2^2.
\end{equation}

\textbf{ST reconstruction ($\mathcal{L}_{\text{rec.},\mathrm{II}}$)} We minimize the MSE between the ground truth and the $\ours$-predicted gene expression. The loss can be written as
% Reconstruction loss
\begin{equation}
\mathcal{L}_{\text{REC}, \mathrm{II}} = \frac{1}{M} \sum_{i=1}^{M} \left\| \mathbf{y}_i - f_{\text{pred.}}\left(\{\mathbf{h}_{i,j}^{\text{3D, rec.}}\}_{j=1}^{32}\right) \right\|_2^2.
\end{equation}

The total loss minimized during this stage is defined as
\begin{equation}
    \mathcal{L}_{\mathrm{II}} = \lambda_{\text{cont.},\mathrm{II}}\cdot\mathcal{L}_{\text{cont.},\mathrm{II}} + 
\lambda_{\text{dir.}}\cdot\mathcal{L}_{\text{dir.}} +
\lambda_{\text{rec.},\mathrm{II}}\cdot\mathcal{L}_{\text{rec.}, \mathrm{II}},
\end{equation}
where $\lambda_{\text{cont.},\mathrm{II}}=\lambda_{\text{rec.},\mathrm{II}}=1$. The strength of direct alignment, $\lambda_{\text{dir.}}$ is set to 1 for microCT and 0 for 2.5D serial tissue sections as the 2D and 2.5D image data are composed of the same imaging modality. 
To maintain the number of training iterations consistent with the previous stage, the model is trained with a reduced batch size of 128. This is to account for a smaller number of available 3D morphology and ST pairs compared to the H\&E morphology and ST pairs curated from diverse sources. The model is trained for 15 epochs using a cosine scheduler that decays the learning rate from $1 \times 10^{-5}$ down to 0 by the end of training. The weight decay and optimizer settings are the same as in the first stage. Hyperparameters and training settings are provided in \textbf{Extended Data Table \ref{tab:stageII}}.


\noindent\textbf{Stage III: VOI fine-tuning}
In the final stage, we fine-tune $\ours$ with \textit{sample-specific} data to better capture the morphomolecular links of the VOI. During this stage, we fine-tune all layers that were trainable during previous stages. This covers the last three transformer layers of the 2D image encoder, the last three transformer layers of the transcriptomics encoder, the trainable layers of the 3D image encoder from the previous stage (none for microCT or three for serial tissue sections), the contrastive and reconstruction attentional poolers in both the 2D image encoder and 3D image encoder, and the transcriptomics predictor. 

During this stage, $\ours$ is trained using the same direct ($\mathcal{L}_{\text{dir.}}$) loss and reconstruction ($\mathcal{L}_{\text{rec.}, \mathrm{II}}$) loss as in training stage II. In addition, the contrastive loss is defined as the sum of the symmetric contrastive losses from stages I and II: $\mathcal{L}_{\text{cont.}} 
 = \mathcal{L}_{\text{cont.},\mathrm{I}} + \mathcal{L}_{\text{cont.},\mathrm{II}}$. %This setup enforces cross-modal alignment across the three data modalities: histology, volumetric imaging, and transcriptomics.
To have a consistent number of training iterations as in the earlier stages, we use a reduced batch size of 16. This adjustment ensures a comparable number of batches for each training epoch, with data pairs from VOI significantly smaller than those from previous stages.
The model is fine-tuned for 10 epochs using a cosine scheduler that decays the learning rate from $1 \times 10^{-5}$ down to 0 by the end of training. The weight decay and optimizer settings are the same as in the first stage. Hyperparameters and training settings are provided in \textbf{Extended Data Table \ref{tab:stageIII}}.

\hheading{$\ours$ evaluation scenarios} 

We design three training strategies to evaluate $\ours$ to understand the effect of two important data scaling trends for 3D ST prediction for prostate cancer cohort: (1) the benefit of 3D morphological context to predict ST profiles (\textit{3D} or \textit{3D + VOI} scenario) as opposed to considering the 2D morphology (\textit{2D} scenario), and (2) the benefit of integrating VOI-specific training pairs (ST acquired on another tissue section from the VOI, 250 $\mu m$ from the section being evaluated) on top of training pairs from other volumes in the same cancer cohort (\textit{3D + VOI} scenario).

For the three scenarios, $\ours$ training stage I is commonly used. For the \textit{2D} setting, the model is further trained in stage II with 2D microCT sections and the corresponding 2D ST measurements obtained from the samples, excluding VOI. For encoding 2D microCT sections, we use the 3D image encoder with the input depth of 1, instead of 21. For the \textit{3D} setting, 3D microCT volumes of depth 21, instead of 2D microCT sections, around the 2D ST section are considered to incorporate 3D morphological context in stage II. 
For the \textit{3D+VOI} setting, we further fine-tune the model through Stage III with \textit{3D setting} on one of the two tissue sections with 2D ST measurement from VOI (P1-P5). We use the other remaining tissue section for evaluation. Next, we swap the roles of these two sections and re-perform fine-tuning and evaluation to obtain two measurements for each sample.  
All models are evaluated for each section with a tissue section leave-out-cross-validation strategy, resulting in ten distinctive results (two tissue sections each for five patients).


\hheading{Molecular queries for cross-modal retrieval}

To design a molecular query based on transcriptomic profiles of biological functions of interest, such as \textit{tumor cellular growth}, we start by combining two sets of genes. First, we identify one or two key genes (\textit{genes of interest}) involved in the biological process based on literature. Then, for these \textit{genes of interest}, we identify the set of  \textit{correlated genes}, defined as the genes whose expression levels across the ST spots are correlated with each of the \textit{genes of interest} with a Pearson Correlation Coefficient (PCC) greater than 0.5. PCC is evaluated for all genes that commonly exist across all samples in the same cancer cohort. In the prostate cancer cohort, for example, this consists of 8,136 genes, as explained in the previous  \textbf{Gene expression panel for evaluation} section. \textit{Genes of interest} and \textit{Correlated genes} for each molecular query in the prostate cancer cohort can be found in \textbf{Extended Data Table~\ref{tab:query_genes}}.

The union of \textit{genes of interest} and \textit{correlated genes}, which we refer to as \textit{spatial filter genes}, are then used for ST spot filtering within VOI to curate coherent expression profiles defining the molecular query. Specifically, we retain only the ST spots where at least half of the genes in the \textit{spatial filter genes} exhibit high expression, defined as being above the 75th percentile of expression values across all ST spots in VOI. Finally, we obtain the molecular query embedding by averaging the transcriptomic embeddings of the filtered spots.
The averaging operation is intended to remove inherent noise in individual measurements, similar to how text prompt embeddings are averaged in vision-language zero-shot cross-modal tasks\cite{radford2021learning, lu2024visual, ding2024multimodalslidefoundationmodel}. 
The molecular query is defined per VOI if ST data is available for the VOI or across the whole training set, otherwise. The former is the case of the prostate cancer cohort (analysis in \textbf{Figure \ref{fig:recall}}) and the latter is the case of the large CRC with serial histological sections (analysis in \textbf{Extended Data Figure \ref{fig:ext_CRC_query}}).
%The genes in the \textit{spatial filter genes} set can be found in \textbf{Extended Data Table~\ref{tab:query_genes}}.
%for which the expression levels of the genes included in the molecular query are in the top (bottom) 25 percentile if the genes are set to be upregulated (downregulated) for that biological process.

To identify morphological regions in the VOI that are most representative of the molecular queries, we divide the VOI into 3D patches and compute the cosine similarity between the normalized molecular query embedding and 3D patch embeddings. To minimize the sharp transition in similarity values for voxels at the boundary of neighboring patches, we employ the following sequence of operations: 3D image patches are created with 75\% overlap, cosine similarity is computed per patch, and the similarity values are averaged in the overlapping regions to achieve a smoother appearance in the similarity heatmaps. 
A coolwarm colormap, with red and blue colors indicating high and low similarity values, is then applied to the cosine similarity values and overlaid on the raw 3D image with a transparency value of 0.35. 
The minimum and maximum values for the colormap are set to the 10\textsuperscript{th} and 90\textsuperscript{th} percentile of the cosine similarity values for each molecular query and volume. The patches with the highest cosine similarity are also visualized as representative of the molecular query.



\hheading{Spatial domain identification}

We identify spatial domains in the tissue volumes by clustering the 3D patches based on 
their predicted gene expression information\cite{hu2021spagcn, dong2022deciphering, xiao2024integrating}. Specifically, we use the transcriptomic embedding before the last fully-connected layer of the transcriptomis predictor, immediately after the single-layer Transformer module. This yields 512-dimensional embedding, the dimensionality of which is independent of the final number of predicted genes. We subsequently cluster the set of transcriptomic embeddings in the tissue, by aggregating all the embeddings in the tissue volume and performing \textit{k}-means algorithm with four or five clusters, depending on the sample. This approach divides the tissue into functionally distinct regions in an unsupervised manner, guided by their transcriptomic expression profiles.
% ARI
We evaluate the quality of the spatial domains by computing the Adjusted Rand Index (ARI) \cite{steinley2004properties} between the morphological segmentation masks and the spatial domains. We compute the ARI metric only across the locations of predicted gene expression and separately per each tissue section. Specifically, for a given axial section in the 3D image, we first obtain the morphological cluster assignment for each spot by referring to the corresponding morphological segmentation class. ARI metric is then computed between the morphological clusters and the spatial domains across all the spots within the tissue section. This process is repeated for every axial section in the 3D tissue image.


\hheading{3D visualization}

3D visualization was used for visualzing the $\ours$-predicted volumetric gene expression and the corresponding 3D morphological data. 3D renders were generated using Napari from 2D image stacks (along the z-axis) that represents the 3D tissue image data. 
The 3D ST prediction visualizations were generated from the stacks of 2D tissue images of gene predictions. For the prostate cohort, the \textit{3D+VOI} model was used for prediction of the spatial gene expression. 
To visualize spatial gene expression at high resolution, TESLA algorithm was used with a resolution factor of 15\cite{hu2023deciphering}. For each gene, the predicted transcriptomics expression levels were clipped at the 1st and 99th percentiles of the predicted expression levels in the central plane of the volume. This ensures a consistent scale of intensity across the volume. The data was then overlaid onto each plane with a transparency value of 0.7.

\hheading{3D morphological segmentation}

To generate 3D segmentations of microCT images, we leveraged the Segment Anything Model 2 (SAM2), a state-of-the-art video segmentation model trained on spatiotemporal datasets\cite{ravi2024sam2segmentimages}. Following the previously proposed methodology\cite{shen2024interactive3dmedicalimage}, we treated the sequential planes of CT volumes as video frames, enabling SAM2 to propagate segmentation masks annotated on a subset of planes to the entire 3D volume. The initial annotations were provided by a pathologist (A.K.) who labeled two evenly distributed H\&E tissue planes within each volume using polygon masks, capturing key anatomical structures. The annotations were then transferred to the registered microCT planes for each H\&E section. The adaptation and extension of a video-based segmentation paradigm to medical imaging significantly reduced the annotation burden without requiring domain-specific model retraining.

Within our pipeline, minimal pre- and post-processing steps (including normalization, clipping, morphological closing, and the application of a threshold-based foreground mask) were applied to refine the propagated masks, ensuring cleaner and more continuous boundaries across planes. For each sections annotated by the pathologist, the model propagated the segmentations both forward and backward, either to the next annotated section or to the end of the volume, whichever came first. This approach produced two sets of predictions for planes located between annotated sections. To ensure consistency in the model’s predictions when transitioning between annotated planes, we combined the two sets of predictions by taking a pixel-wise weighted average of the output logits for each class. The weights were scaled linearly, starting at one at the annotated plane where the propagation began and linearly decreasing to 0 at the next annotated plane, seamlessly blending the propagated segmentations.

\hheading{Evaluation metrics}

We evaluate $\ours$ using five metrics: Pearson Correlation Coefficient (PCC), Structural Similarity Index Measure (SSIM), Spearman's $\rho$, Moran's I, and Geary's C. These metrics are computed on a per-plane basis. When multiple planes with ground truth ST are available for a given VOI, we calculate the metrics for each plane, and then average values across all planes across the given cohort.

\noindent\textbf{PCC} offers insights into the linear relationship
between predicted and ground truth values, both in strength
and direction. It is one of the most common metrics for evaluating the quality of ST prediction from morphology\cite{zhang2024inferring,he2020integrating,chung2024accurate,wang2025benchmark}. We evaluate PCC for each gene across all spots in the plane. We compute the average across $M$ genes as
\begin{equation}
\operatorname{PCC} = \frac{1}{M} \sum_{i=1}^{M} \operatorname{PCC}_i.
\end{equation}
% -----------------------------------------------------

\noindent\textbf{SSIM} measures the similarity between the spatial structures of the ground truth and the predicted gene expression values. SSIM is an image similarity metric and has been applied to evaluate ST prediction tasks\cite{zhang2024inferring,wang2025benchmark}. A higher SSIM
indicates a higher degree of similarity between two images. For a given gene in each tissue section, we generate two single-channel images, one for the ground truth and the other for predicted ST expression values. The ST spot coordinates are first downscaled by the factor equivalent to the center-to-center distance of ST spots, which yields a dense 2D pixel grid. Each pixel corresponds to a ST spot, with the expression values scaled to $[0,1]$ using min-max normalization. 
SSIM is then computed for each gene and averaged across gene sets, following a procedure similar to that used for the PCC metric.

% Spearman rho
\noindent\textbf{Spearman's $\mathbf{\rho}$} To better assess how effectively the different $\ours$ training strategies recapitulate the variance of the genes being predicted, we compare the variance of each gene in the ground truth ST data to that of the predicted gene expression, following previous work\cite{xie2024spatially}.
Genes are ranked based on their ground truth variance (from smallest to largest), and curves are generated for both the original and predicted gene expression variances. Spearman's rank correlation coefficient (Spearman's $\rho$) is then computed to quantify the similarity between the two distributions.

\noindent\textbf{Moran's I and Geary's C} While PCC and SSIM evaluate the correct prediction of each gene mean expression, Spearman's $\rho$ assesses the variance of the predictions. In addition, to evaluate how well $\ours$'s training strategies capture the distribution of gene expression patterns across spatial locations in the tissue, we consider Moran's I \cite{moran1950notes} and Geary's C \cite{geary1954contiguity}, two classical spatial autocorrelation metrics widely employed to identify spatially variable genes. We evaluate these metrics per gene and we compute the average across all genes. We then report the difference of each metric between the ground truth and $\ours$-prediction, with smaller values indicating better captures of the expression heterogeneity. 


\hheading{Statistical analysis}

For each training scenario (\textit{2D}, \textit{3D} and \textit{3D+VOI}), we evaluate the model performance on each of the two sections available per patient (P1$\sim$P5) separately. We report the mean performance and the standard deviations across all 10 planes. We use a one-sided Wilcoxon signed-rank test to evaluate the statistical significance between the three settings, for all evaluation metrics.

\hheading{Computational hardware and software.}
3D Spatial Transcriptomics on volumetric images via $\ours$ was performed on AMD Ryzen multicore CPUs (central processing units). Two NVIDIA GeForce RTX 3090 GPUs (graphics processing units) were used for the 2D pretraining, and one GPU of the same specifications was used for the following training stages (3D pretraining and VOI fine-tuning). $\ours$ was implemented in Python (version 3.10.13). All deep learning implementations were performed with PyTorch (version 2.1.2). The implementation of scGPT from 
(\texttt{\href{https://github.com/bowang-lab/scGPT}
{\url{https://github.com/bowang-lab/scGPT}}}) was used, which required flash-attn (version 1.0.4). The loss function for contrastive alignment was adapted from (\texttt{\href{ https://github.com/moein-shariatnia/OpenAI-CLIP.git}{\url{ https://github.com/moein-shariatnia/OpenAI-CLIP.git}}}). Processing and analysis of spatial transcriptomics data was performed using scanpy (version 1.10.1). Generation of ST super-resolution data for visualization was generated with TESLA from (\texttt{\href{https://github.com/jianhuupenn/TESLA}{\url{ https://github.com/jianhuupenn/TESLA}}}). Valis-wsi (version 1.0.4) was used for serial section registration and pyRANSAC from (\texttt{\href{https://github.com/leomariga/pyRANSAC-3D/}{\url{https://github.com/leomariga/pyRANSAC-3D/}}}) was used for 3D point cloud fitting during cross-modal registration.
Evaluation metrics for $\ours$ predictive performance used numpy (version 1.26.4), and scikit-image (version 0.19.3).
Other Python libraries used to support data analysis include slideio (version 2.5.0), tifffile (version 2024.5.10), pandas (version 2.2.2), scipy (version 1.13.0), pillow (version 9.5.0), opencv-python (version 4.9.0), torchvision (version 0.16.0), and timm (version 1.0.3). Plots were generated in Python using matplotlib (version 3.9.0). 3D visualization was accomplished via napari (version 0.4.16). The interactive demo website was developed using THREE.js (version 0.152.2) and jQuery (version 3.6.0).




