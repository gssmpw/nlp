% SIAM Article Template
%\documentclass[review,hidelinks,onefignum,onetabnum]{siamart220329}
\documentclass[onefignum,onetabnum]{siamart220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

%\input{ex_shared}
% SIAM Shared Information Template
% This is information that is shared between the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.


% Packages and macros go here
\usepackage{lipsum}
\usepackage{amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{ulem}
\usepackage{comment}
\usepackage{bm}
\usepackage{tikz}
\usepackage{mathtools}
%\mathtoolsset{showonlyrefs=true} %参照してない式番号を消す

\usepackage{subcaption}
\usepackage{multirow}
\usepackage{threeparttable}

\usetikzlibrary{intersections,calc,arrows.meta}

\usepackage{booktabs} % For better table formatting

\newcommand{\tred}{\textcolor{red}}

%\usepackage{mathtools}
%\mathtoolsset{showonlyrefs=true} %参照してない式番号を消す

\newcommand*{\diff}[1]{\mathrm{d}#1}
\newcommand*{\T}{\mathsf{T}}
\newcommand*{\A}{\mathrm{A}}
\newcommand*{\ddel}[2]{\frac{\partial #1}{\partial #2}}

\newcommand*{\tab}{\hspace*{5pt}}
\newcommand*{\paren}[1]{\left(#1\right)}
\newcommand*{\parsq}[1]{\left[#1\right]}
\newcommand*{\parbr}[1]{\left\{#1\right\}}
\newcommand*{\parang}[1]{\langle#1\rangle}
\newcommand*{\norm}[1]{\left\|#1\right\|}
\newcommand*{\floor}[1]{\lfloor#1\rfloor}
\newcommand*{\ceil}[1]{\lceil#1\rceil}
\newcommand*{\bceil}[1]{\big\lceil#1\big\rceil}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\image}{Im}
\DeclareMathOperator{\vecspan}{span}
\renewcommand{\figurename}{Figure }
\renewcommand{\tablename}{Table }
% \renewcommand{\bibname}{References}

%ここだけのコマンド%
\newcommand*{\averes}{\bar{\mathcal{R}}}
\newcommand*{\gdir}{\mathcal{G}_{\mathrm{dir}}}
\newcommand*{\gundir}{\mathcal{G}_{\mathrm{undir}}}
\newcommand*{\din}{\delta_{\mathrm{in}}}
\newcommand*{\tbcell}[1]{\begin{tabular}{l}#1\end{tabular}}

\newcommand{\toi}[1]{\vspace{10pt}\par\noindent\textbf{#1.}\par}
\newcommand{\tois}[1]{\noindent\textbf{#1.}\par}

% \newtheorem{thm}{Theorem}
% \newtheorem{theorem}{Theorem}
% \newtheorem{dfn}{Definition}
% \newtheorem{definition}{Definition}
% \newtheorem{lemma}{Lemma}
% \newtheorem{proposition}{Proposition}
% \newtheorem{corollary}{Corollary}

\theoremstyle{remark}
% \newtheorem{remark}{Remark}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\rd}{\mathrm{d}}
\DeclareMathOperator*{\minimize}{minimize}

%\newcommand{\imag}{\mathop{\rm im}\nolimits}
%\renewcommand{\labelenumi}{\arabic{enumi}).}
%\usepackage{framed}

\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

% Add a serial/Oxford comma by default.
\newcommand{\creflastconjunction}{, and~}
\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\BREAK}{\STATE \algorithmicbreak}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\crefname{ALC@unique}{Step}{Steps}


% Used for creating new theorem and remark environments
\newsiamremark{remark}{Remark}
\newsiamthm{problem}{Problem}
\crefname{problem}{Problem}{Problems}
\newsiamthm{assumption}{Assumption}
\crefname{assumption}{Assumption}{Assumptions}

% Sets running headers as well as PDF title and authors
\headers{Consensus Algorithm on SC Graph}{T.~YONAIYAMA AND K.~SATO}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Analysis of Linear Consensus Algorithm on Strongly Connected Graph Using Effective Resistance\thanks{Submitted to the editors DATE.
\funding{This work was supported by Japan Society for the Promotion of Science KAKENHI under 	23K28369.}
}}

% Authors: full names plus addresses.
\author{Takumi Yonaiyama\thanks{Department of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo (T.~Yonaiyama: \email{yonaiyama-takumi844@g.ecc.u-tokyo.ac.jp}, K.~Sato: \email{kazuhiro@mist.i.u-tokyo.ac.jp}).}
\and Kazuhiro Sato\footnotemark[2]}

\usepackage{amsopn}
% \DeclareMathOperator{\diag}{diag}


%%% Local Variables: 
%%% mode:latex
%%% TeX-master: "ex_article"
%%% End: 

% Optional PDF information
% \ifpdf
% \hypersetup{
%   pdftitle={An Example Article},
%   pdfauthor={D. Doe, P. T. Frank, and J. E. Smith}
% }
% \fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.


% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle
% REQUIRED
\begin{abstract}
We study the performance of the linear consensus algorithm on strongly connected graphs
using the linear quadratic (LQ) cost as a performance measure.
 In particular, we derive bounds on the LQ cost by leveraging effective resistance. Our results extend previous analyses---which were limited to reversible cases---to the nonreversible setting. To facilitate this generalization, we introduce novel concepts, termed the back-and-forth path and the pivot node, which serve as effective alternatives to traditional techniques that require reversibility. Moreover, we apply our approach to geometric graphs to estimate the LQ cost without the reversibility assumption. The proposed approach provides a framework that can be adapted to other contexts where reversibility is typically assumed.
\end{abstract}

% REQUIRED
\begin{keywords}
  Directed graph, Effective resistance, Linear consensus, Reversiblization
\end{keywords}

% REQUIRED
% \begin{AMS}
%   	93A15, 93B05, 93C05
% \end{AMS}



\section{Introduction}
\label{sec: introduction}
The analysis of multiagent networks is applied to a variety of subjects, such as multi-robot systems \cite{robot}, wireless sensor networks \cite{WSN}, and large language models \cite{LLM}. In information processing of multiagent networks, one of the most simple and fundamental algorithms is linear consensus, also known as average consensus. The linear consensus algorithm is used when each agent has a scalar value, and agents have to match their values to the weighted average of their initial value with limited and decentralized communications. Despite its simplicity, linear consensus algorithm is the basis for many different tasks related to network coordination, such as formation control \cite{formation1,multivehicle2}, distributed optimization \cite{optim1}, and cooperative leader following \cite{leader0,leader1}.

The performance measurement of linear consensus algorithm has been studied in a various way. Linear consensus network can be understood as a linear discrete-time dynamical system, which is closely related to Markov chain \cite{cayley}, so Markov chain is an important tool for analysis. In fact, the performance of linear consensus algorithm has been analyzed by the speed of convergence using spectral analysis of a transition matrix of a Markov chain \cite{spectral1,spectral2}. On the other hand, recent studies often adopt approaches from a control theoretic point of view, such as linear quadratic (LQ) cost, which is classical in control. The LQ cost is derived by the summation of $L_2$-norm of the difference between the state in each moment and the final state. Moreover, the LQ cost also appears in the error estimation of noisy consensus, a variant of linear consensus with additive random noise. With such background, there are some studies using the LQ cost or similar functions as a performance measure \cite{cayley,aboutlqcost,mainp}. In these studies, various approaches are employed:
\begin{itemize}
  \item \textbf{Eigenvalue calculation}: Eigenvalue are calculated for special graph types, including Cayley graphs, grid graphs, and random geometric graphs \cite{cayley,aboutlqcost}.
  \item \textbf{Effective resistance}: The concept, which bridges electrical networks with Markov chains \cite{randomwalks}, has been employed in several recent studies \cite{citeres1,satokazu}. Notably, previous research has estimated the LQ cost using the effective resistance \cite{mainp}, enabling the assessment of LQ cost based on the network's graph topology.
  \item \textbf{Hitting time of a Markov chain} (see \cite{revmarkov}): Recent research \cite{novelnoisy} has provided an exact formulation of a variant of the LQ cost by leveraging hitting times.
\end{itemize}

In this paper, we analyze bounds on the LQ cost for nonreversible cases using effective resistance. This generalizes the estimation of the LQ cost for reversible linear consensus networks, as studied in \cite{mainp}, to nonreversible networks. A key advantage of nonreversible Markov chains is its applicability to directed networks. While the reversibility is a useful assumption, it does not hold when communications within the network are directed. Therefore, investigating nonreversible cases is beneficial for understanding linear consensus in directed networks. Discussions on nonreversible Markov are also present in recent studies \cite{nonrev1,nonrev2,nonrev3}. In the research of a nonreversible Markov chains, a technique called \textit{reversiblization} of the transition matrix is used to exploit results of studies on reversible Markov chains \cite{reversiblization}.

Our main contribution is the establishment of an upper bound on the LQ cost for nonreversible cases, and a lower bound under an assumption weaker than reversibility using the effective resistance of reversiblization of the underlying Markov chain. To obtain these bounds on the LQ cost, we use the following methods:
\begin{enumerate}
  \item Although the analogy between the effective resistance and Markov chain does not hold for nonreversible cases, we demonstrate that through the reversiblization of a nonreversible chain, the effective resistance can be applied to a nonreversible linear consensus system.
  \item We relate the reversiblization and the original nonreversible Markov chain by introducing new notions named the \textit{back-and-forth path} and \textit{pivot node}. These concepts serve as alternatives to the notion of $2$-\textit{fuzz} of an undirected graph \cite{hfuzz}, which is instrumental in reversible cases, but cannot directly apply to nonreversible cases.
\end{enumerate}
The differences between our approach to upper and lower bounds and those of previous studies are summarized in Table \ref{table:appr}. 
 In all methods except our own, reversibility is typically assumed in the analysis of linear consensus, owing to the availability of numerous results pertaining to reversible Markov chains.
In addition, we apply our main results to random geometric graphs, which model real-world communication networks and have been extensively studied in the literature (see \cite{rggs}). For linear consensus on geometric graphs, we derive asymptotic bounds on the LQ cost, yielding results analogous to those in \cite{mainp} for reversible cases.

\begin{table}[h]
  \caption{Approaches on the LQ cost}
  \label{table:appr}
  \centering
  \tabcolsep = 3pt
    \begin{tabular}{c|lll}
    \hline
    Method & Target class & Main tool & Key technique \\
    \hline\hline
    \tbcell{\cite{cayley}} & \tbcell{Cayley, grid,\\and random\\geometric graphs} & \tbcell{Calculation of\\eigenvalues} & \tbcell{Defining an appropriate\\trigonometric polynomial}\\
    \hline
    \tbcell{\cite{mainp}} & \tbcell{Reversible} & \tbcell{Effective resistance} & \tbcell{$2$-fuzz of a graph}\\
    \hline
    \tbcell{\cite{novelnoisy}} & \tbcell{Reversible} & \tbcell{Hitting time of\\a Markov chain} & \tbcell{Markov's inequality}\\
    \hline
    \tbcell{Proposed\\method} & \tbcell{\textbf{Nonreversible}} & \tbcell{Effective resistance} & \tbcell{``Back-and-forth path''\\and ``Pivot node''\\of a graph}\\
    \hline
    \end{tabular}
\end{table}


The remainder of this paper is organized as follows: In Section 2, we introduce the mathematical formulation of linear consensus algorithm, effective resistance, and the relationship between them. In Section 3, we  present our main result on the estimation of the LQ cost, along with its proofs. Section 4 demonstrates the application of our result to geometric graphs, and Section 5 concludes the paper.

\subsection*{Notations}
Let $\mathbb{R}$ be a set of real numbers, $\mathbb{R}^n$ be a set of $n$-dimensional real vectors, and $\mathbb{R}^{n\times m}$ be a set of $n\times m$ real matrices. We denote $i$-th entry of $\bm{v}\in\mathbb{R}^n$ by $v_i$, and $(i,j)$ entry of $A\in\mathbb{R}^{n\times m}$ by $A_{ij}$. Let $\bm{e}_i\in\mathbb{R}^n$ be a vector whose $i$-th entry is $1$ and other entries are $0$, and $\bm{1}\in\mathbb{R}^n$ be a vector with all entries equal to $1$. We denote an identity matrix by $I$ and zero matrix by $O$. For a vector $\bm{v}\in\mathbb{R}^n$, let $\bm{v}^\T$ be a transpose of $\bm{v}$, $\vecspan\{\bm{v}\}$ be a vector space spanned by $\bm{v}$, and $\diag(\bm{v})\in\mathbb{R}^{n\times n}$ be a diagonal matrix satisfying $\diag(\bm{v})_{ii}=v_i$ for all $i=1,2,\dots,n$ and $\diag(\bm{v})_{ij}=0$ if $i\neq j$. For a matrix $A$, let $A^\T$ be a transpose of $A$, $\tr A$ be a trace of $A$, $\ker A$ be a kernel of $A$, and $A^{\dagger}$ be a Moore-Penrose pseudoinverse of $A$. For two matrices $A$ and $B$, $A\geq B$ means $A_{ij}\geq B_{ij}$ for all $i$ and $j$. We denote a graph with a set of node $V$ and a set of edge $\mathcal{E}$ by $(V,\mathcal{E})$. For an undirected graph, we denote an edge between nodes $u$ and $v$ by $\{u,v\}$, while for a directed graph, we denote an edge from node $u$ to node $v$ by $(u,v)$.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
\label{sec:preliminaries}
\subsection{Definition of linear consensus algorithm}
We model a communication network of $n$ agents labeled $1$ to $n$. Each pair of agents can communicate according to the directed graph $\mathcal{G}=(V,\mathcal{E})$, where $V=\{1,2,\dots,n\}$ represents the label of each agent, and $(i,j)\in\mathcal{E}$ means that $i$ can get information from $j$. We call $\mathcal{G}$ a communication graph. In this paper, the information is represented by a real number.

The linear consensus algorithm ensures that all agents converge to the weighted average of their initial real numbers by exchanging information according to a predefined graph. In this algorithm, at each iteration, each agent transmits its current number to all connected agents and subsequently updates its own number to a convex combination of the received numbers using predetermined coefficients. More precisely, the linear consensus algorithm repeats the following update:
\[\bm{x}(t+1)=P\bm{x}(t),\]
indicating that
\begin{eqnarray}\label{fmlxt}
  \bm{x}(t)=P^t\bm{x}(0),
\end{eqnarray}
where $P\in\mathbb{R}^{n\times n}$ is a row stochastic matrix, namely, satisfying $P\bm{1}=\bm{1}$, and $\bm{x}(t)\in\mathbb{R}^n$ represents the number held by each agent at time $t$. In this paper, we refer to row stochastic matrices simply by \textit{stochastic matrices}.

The constraint that agents can only communicate according to the communication graph $\mathcal{G}$ is reflected to the place of nonzero elements in $P$, which defines the directed graph $\gdir(P)=(V,\mathcal{E})$, letting $(u,v)\in\mathcal{E}$ if and only if $P_{uv}\neq 0$. Due to the constraint, $\gdir(P)$ should be a subgraph of the communication graph $\mathcal{G}$. In this paper, we assume $\gdir(P)=\mathcal{G}$ for simplicity. Furthermore, we define the \textit{undirected graph associated with matrix} $P$, $\mathcal{G}_{\mathrm{undir}}(P)=(V,\mathcal{E})$, letting $\{u,v\}\in\mathcal{E}$ if and only if $P_{uv}\neq 0$ or $P_{vu}\neq 0$.

Throughout this paper, we assume two properties: $P$ is \textit{irreducible}, and the diagonal elements of $P$ are positive, where $P$ is \textit{irreducible} if and only if $\gdir(P)$ is strongly connected. The positive diagonal assumption leads to the \textit{aperiodicity} of $P$, which means that the greatest common divisor of the lengths of all cycles in $\gdir(P)$ is $1$.

By these two assumptions, Perron-Frobenius theorem \cite{perronfrobenius} implies that $P$ has the eigenvalue $1$ with the multiplicity $1$, the corresponding right eigenvector is $\bm{1}$, and corresponding left eigenvector is a strictly positive vector. We denote the right eigenvector by $\bm{\pi}$ normalized so that $\sum_u \pi_u=1$, and call it the \textit{invariant measure} of $P$. It also holds that, when $t$ grows to infinity, $P^t$ converges to $\bm{1}\bm{\pi}^\T$. Because of (\ref{fmlxt}), we obtain $\lim_{t\to\infty}\bm{x}(t)=\bm{1}\bm{\pi}^\T\bm{x}(0)$. We also use a diagonal matrix $\varPi:=\diag(\bm{\pi})$.

In this paper, we call a stochastic and irreducible matrix whose diagonal elements are strictly positive, a \textit{consensus matrix}.

\subsection{Performance measure of linear consensus algorithm}
\label{subsec:perf_lc}
As a performance measure, we use the linear quadratic (LQ) cost \cite{mainp}:
\begin{eqnarray}\label{lqcost}
  J(P):=\sum_{t\geq 0}\|P^t-\bm{1}\bm{\pi}^\T\|_{\mathrm{F}}^2=\frac{1}{n}\tr\left[\sum_{t\geq 0}(I-\bm{\pi}\bm{1}^\T)(P^\T)^t P^t(I-\bm{1}\bm{\pi}^\T)\right],
\end{eqnarray}
where $\|\cdot\|_\mathrm{F}$ means the Frobenius norm of a matrix.

The cost (\ref{lqcost}) is obtained by evaluating $\mathrm{E}\left[\sum_{t\geq 0}\|\bm{x}(t)-\bm{x}(\infty)\|^2\right]$, where $\mathrm{E}[\cdot]$ means the expected value, under the assumption that $\bm{x}(0)$ is a random vector with covariance $\mathrm{E}\left[\bm{x}(0)\bm{x}(0)^\T\right]=I$ \cite{aboutlqcost,mainp}. In addition, the cost (\ref{lqcost}) also appears in the noisy consensus \cite{aboutlqcost,mainp,novelnoisy}. The noisy consensus repeats the following update:
\[\bm{x}(t+1)=P\bm{x}(t)+\bm{n}(t),\]
where $\bm{n}(t)$ is an independent and identically distributed process with the average $\mathrm{E}\left[\bm{n}(t)\right]=\bm{0}$ and the covariance $\mathrm{E}\left[\bm{n}(t)\bm{n}(t)^\T\right]=I$. Assume that $\bm{x}(0)$ is a random vector with covariance $\mathrm{E}\left[\bm{x}(0)\bm{x}(0)^\T\right]=I$ and is not correlated with $\bm{n}(t)$. To measure the distance between $\bm{x}(t)$ and its weighted average $\bm{1}\bm{\pi}^\T\bm{x}(t)$, we define $\tilde{\bm{e}}(t):=(I-\bm{1}\bm{\pi}^\T)\bm{x}(t)$. Then, we can show that
\begin{eqnarray}\label{noisy1}
  \frac{1}{n}\lim_{t\to\infty}\mathrm{E}\parsq{\|\tilde{\bm{e}}\|^2}&=&J(P).
\end{eqnarray}

From the context of noisy consensus, we also use another variant \cite{novelnoisy}:
\begin{eqnarray}\label{lqcost2}
  J_{\mathrm{w}}(P):=\tr\left[\sum_{t\geq 0}(I-\bm{\pi}\bm{1}^\T)(P^\T)^t\varPi P^t(I-\bm{1}\bm{\pi}^\T)\right],
\end{eqnarray}
which is obtained by substituting $\|\tilde{\bm{e}}\|^2$ in (\ref{noisy1}) by $\sum_i \pi_i\tilde{e}_i^2$. Different from (\ref{lqcost}), which sums up the errors uniformly, (\ref{lqcost2}) sums up the errors with weights according to $\bm{\pi}$.

\subsection{Effective resistance}
We consider a resistor network as an undirected connected graph $\mathcal{G}=(V,\mathcal{E})$ and each edge represents the resistor that connects both nodes of the edge. A resistor network with $n$ nodes is determined by assigning a matrix $C=(C_{ab})\in\mathbb{R}^{n\times n}$ whose element $C_{ab}$ means the conductances of the edge between $a$ and $b$ if $C_{ab}\neq 0$, or that $a$ and $b$ are not connected by an edge if $C_{ab}=0$. We call $C=(C_{ab})$ a \textit{conductance matrix} if $C$ is a symmetric, nonnegative and irreducible matrix.

We consider a voltage vector of the nodes $\bm{v}$, where $v_a$ represents the voltage of the node $a$, and a current vector $\bm{i}$, where $i_a$ represents the current flowing out of (or into when negative) the node $a$. By Ohm's law, the current from $a$ to $b$ through an edge $\{a,b\}$ is $C_{ab}(v_a-v_b)$. Therefore, by Kirchhoff's law, $i_a$ is determined by:
\begin{eqnarray}
  i_a=\sum_{b\in V}C_{ab}(v_a-v_b)\quad
{\rm or}\quad
\bm{i}=L(C) \bm{v}, \label{kirch1}
\end{eqnarray}
where $L(C):=\diag(C\bm{1})-C$ is the Laplacian of $C$. Notice that the diagonal elements of $C$ are not concerned with $L(C)$.

Because the current injection and extraction are balanced, we can assume $\bm{i}^{\T}\bm{1}=0$. Under this condition, we can show the following lemma.
\begin{lemma}\label{v-uniqueness}
  If $\bm{i}^{\T}\bm{1}=0$, then (\ref{kirch1}) has a unique solution for $\bm{v}$ up to the fundamental solution term $\bm{1}$. That is, for each current vector $\bm{i}$, the potential difference between nodes $a$ and $b$ is unique.
\end{lemma}
\begin{proof}
  Notice that $\diag(C\bm{1})$ is nonsingular and \[\paren{I-\diag(C\bm{1})^{-1}C}=\diag(C\bm{1})^{-1}L(C).\] Because $\mathcal{G}$ is connected, $\diag(C\bm{1})^{-1}C$ is an irreducible stochastic matrix. Therefore, Perron-Frobenius theorem \cite{perronfrobenius} ensures that $\diag(C\bm{1})^{-1}C$ has an eigenvalue $1$ with multiplicity $1$, so the rank of $I-\diag(C\bm{1})^{-1}C$ is $n-1$, which means that the rank of $L(C)$ is also $n-1$. Because $L(C)\bm{1}=\bm{0}$, $L(C)$ has an eigenvalue $0$ with multiplicity $1$. Therefore, $\bm{l}_1,\dots,\bm{l}_{n-1}$ are linearly independent, where $\bm{l}_k^\T$ is the $k$-th row of $L(C)$. Taking into account only $i_1,\dots,i_{n-1}$ and $\bm{l}_1^\T,\dots,\bm{l}_{n-1}^\T$ in (\ref{kirch1}), we can obtain the unique solution $\bm{v}$ for (\ref{kirch1}) up to the fundamental solution term $\bm{1}$. This $\bm{v}$ automatically satisfies $i_n=\bm{l}_n^\T\bm{v}$, because $i_n=-(i_1+\dots+i_{n-1})$ and $\bm{l}_n=-(\bm{l}_1+\dots+\bm{l}_{n-1})$.
\end{proof}

Consider the situation that a voltage source is connected between nodes $a,b$, and adjust the voltage so that a unit current flows through the source. In this setting, the currect vector $\bm{i}$ is $\bm{e}_a-\bm{e}_b$, and $v_a-v_b$, the potential difference between nodes $a,b$, is unique. Thus, we can define \textit{effective resistance} between $a,b$:
\begin{definition}[\cite{mainp}]
  Let $\mathcal{G}=(V,\mathcal{E})$ be an undirected graph with the conductance matrix $C$. The \textit{effective resistance} between nodes $a$ and $b$, denoted by $\mathcal{R}_{ab}(C)$, is defined by $v_a-v_b=(\bm{e}_a-\bm{e}_b)^\T\bm{v}$, where $\bm{v}$ is an arbitrary solution of (\ref{kirch1}) when $\bm{i}=\bm{e}_a-\bm{e}_b$.
\end{definition}

The effective resistance $\mathcal{R}_{ab}(C)$ can also be expressed as
\begin{eqnarray}
  \mathcal{R}_{ab}(C)=(\bm{e}_a-\bm{e}_b)^\T L(C)^{\dagger}(\bm{e}_a-\bm{e}_b),
\end{eqnarray}
because $\bm{v}=L(C)^\dagger\bm{i}$ \cite{kronred}.

In our paper, we also consider resistor networks whose resistors have a unit conductance. For such networks, we only have to indicate the unweighted graph $\mathcal{G}$, so we denote the effective resistance between $a,b$ for such networks by $\mathcal{R}_{ab}(\mathcal{G})$, which is the property determined only by the graph topology.

In addition, we define \[\averes(C):=\frac{1}{2n^2}\sum_{u,v\in V}\mathcal{R}_{uv}(C)\] as the average effective resistance and $\averes(\mathcal{G})$ is defined using $\mathcal{R}_{uv}(\mathcal{G})$ in the same way. $\averes(\mathcal{G})$ is also determined only by the graph topology.

\subsection{The relationships between linear consensus algorithm and effective resistance}
In this section, we summarize some relationships between \textit{reversible} consensus matrices and conductance matrices.

A consensus matrix $P$ can be treated as the transition matrix of a discrete time Markov chain with the stationary distribution $\bm{\pi}$. A Markov chain with the transition matrix $P$ is called \textit{reversible} if and only if $\pi_xP_{xy}=\pi_{y}P_{yx}$ holds for all $x,y$, that is, the matrix $\varPi P$ is symmetric. Analogously, we call a consensus matrix $P$ \textit{reversible} if and only if $\varPi P$ is symmetric. For reversible $P$, we can associate $P$ with a conductance matrix $C$:
\begin{lemma}
  Let $S_{\mathrm{cs}}\subseteq \mathbb{R}^{n\times n}$ be the set of reversible consensus matrices, and for $\alpha>0$, let \[S_{\alpha}=\{C\in\mathbb{R}^{n\times n}\mid C^\T =C,\ C\geq 0,\ C_{ii}> 0\ (\forall i),\ \bm{1}^\T C\bm{1}=\alpha,\ C\textrm{ is irreducible}\}\]be the set of conductance matrices whose sum of all elements is $\alpha$. Then, $\Phi_{\alpha}(P):=\alpha\varPi P$ is a bijection between $S_{\mathrm{cs}}$ and $S_{\alpha}$, and the inverse is $\Psi(C):=\diag(C\bm{1})^{-1}C$.
\end{lemma}
The proof is analogous to \cite{mainp}.

Let $\Phi(P):=\Phi_n(P)=n\varPi P$, where $n$ denotes the number of rows of $P$. Then, we can see a clear relationship between the effective resistance and the random walk with the transition matrix $P$ \cite{randomwalks}.
\begin{lemma}\label{reswalk}
  Let $\mathcal{G}$ be a resistor network with the conductance matrix $C:=\Phi(P)$. Consider the Markov chain with the transition matrix $P$, and let $p_{\mathrm{esc}}$ be the probability that the walker on the Markov chain starting at $a$ returns $a$ before reaching $b$. Then $p_{\mathrm{esc}}=\frac{1}{\mathcal{R}_{ab}\sum_x{C_{ax}}}$.
\end{lemma}

There are also random walk interpretations of the electrical network other than resistance, such as the voltage and the current of the network (see \cite{randomwalks}). In the analogy of random walk, a matrix called Green matrix \cite{mainp} plays an important role. The Green matrix of consensus matrix $P$ is defined as:
\begin{eqnarray}\label{defgp}
  G(P):=\sum_{t\geq 0}(P^t-\bm{1}\bm{\pi}^\T).
\end{eqnarray}
This matrix is also called the fundamental matrix \cite{revmarkov}. For a consensus matrix $P$, this matrix is well defined (see Chapter 11.4 in \cite{greenconv}).
\begin{remark}
  Because $P^t-\bm{1}\bm{\pi}=(P-\bm{1}\bm{\pi})^t$ for $t\geq 1$, \eqref{defgp} can be reformulated as follows:
  \begin{eqnarray}\label{defgpref}
    G(P)+\bm{1}\bm{\pi}=I+\sum_{t\geq 1}(P-\bm{1}\bm{\pi}^\T)^t.
  \end{eqnarray}
  Moreover, since $(I-A)^{-1}=\sum_{t\geq 0}A^t$ if the sum is well defined, we obtain \begin{eqnarray}\label{defgpfinal}
    G(P)+\bm{1}\bm{\pi}=(I-P+\bm{1}\bm{\pi}^\T)^{-1}.
  \end{eqnarray}
  A fundamental matrix often refers not to $G(P)$, but to the right hand of \eqref{defgpref} or \eqref{defgpfinal} (e.g. \cite{greenconv,novelnoisy}).
\end{remark}

Now we show another representation of effective resistance using the Green matrix $G(P)$.
\begin{lemma}\label{resGreen}
  For a reversible consensus matrix $P$, let $C=\Phi(P)$. Then, \[\mathcal{R}_{ab}(C)=\frac{1}{n}(\bm{e}_a-\bm{e}_b)^\T G(P)\varPi^{-1}(\bm{e}_a-\bm{e}_b).\]
\end{lemma}
The proof can be found in \cite{mainp}. If we start the discussion from a conductance matrix $C$, we can obtain the following corollary.
\begin{corollary}
  For a conductance matrix $C$, let $P=\Psi(C)$. Then, \[\mathcal{R}_{ab}(C)=(\bm{e}_a-\bm{e}_b)^\T G(P)\diag(C\bm{1})^{-1}(\bm{e}_a-\bm{e}_b).\]
\end{corollary}
The proparties in this section will be used to relate $J(P)$ and the effective resistance of the graph. In particular, $G(P)$ plays an important role in evaluating $J(P)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance analysis of linear consensus algorithm using effective resistance}

In this section, we give upper and lower bounds for $J(P)$ using the concept of effective resistance. A bound for $J(P)$ has been known for reversible $P$ \cite{mainp,novelnoisy}, but not for nonreversible cases. To provide bounds for nonreversible $P$, we use a reversible matrix $P^*P$, where $P^*:=\varPi^{-1}P^\T\varPi$. Our result is a generalization of \cite{mainp}, which relates $J(P)$ and $\averes(\Phi(P^2))$.

\subsection{Properties of $P^*P$}
We show some properties of $P^*P$. For every consensus matrix $P$, $P^*P$ is a reversible consensus matrix with the same invariant measure as $P$, so it is called a multiple reversiblization of $P$ \cite{reversiblization}. When $P$ is a reversible consensus matrix, we have $P^*=\varPi^{-1}(\varPi P)^\T=\varPi^{-1}(\varPi P)=P$ and $P^*P=P^2$.

The following lemma plays an important role in this paper.
\begin{lemma}\label{tracekey}
  For every consensus matrix $P$ and nonnegative integer $t$, 
  \begin{eqnarray}\label{tracekeymain}
    \tr\paren{(P^*)^tP^t}\leq\tr \paren{(P^*P)^t}.
  \end{eqnarray}
\end{lemma}
\begin{proof}
  Let $Q:=\varPi^{1/2}P\varPi^{-1/2}$. Then $\tr\paren{(P^*)^tP^t}=\tr\paren{(Q^t)^\T Q^t}=\sum_{i}\sigma_i^2(Q^t)$, where $\sigma_i(Q)$ is the $i$-th singular value of $Q$. Moreover, $\tr \paren{(P^*P)^t}=\tr \paren{(Q^\T Q)^t}=\sum_{i}\sigma_i^{2t}(Q)$, because $(Q^\T Q)^t$ is symmetric. Using $\sigma_i(Q^t)\leq \sigma_i^t(Q)$, as shown in 9.H.2.a of \cite{traceineq}, we obtain \eqref{tracekeymain}.
\end{proof}
% \begin{remark}
%   We have asssumed that the diagonal elements of $P$ are positive. Without this assumption, even if $P$ is apperiodic and irreducible, $P^*P$ can not be irreducible matrix. Let
%   \begin{eqnarray*}
%     P:=\begin{pmatrix}
%       0 & 1 & 0 \\
%       0 & 0 & 1 \\
%       \frac{1}{2} & 0 & \frac{1}{2}
%     \end{pmatrix},
%   \end{eqnarray*}
%   then $\bm{\pi}=\begin{pmatrix}\frac{1}{4}&\frac{1}{4}&\frac{1}{2}\end{pmatrix}^\T$ and
%   \begin{eqnarray*}
%     P^*:=\begin{pmatrix}
%       0 & 0 & 1 \\
%       1 & 0 & 0 \\
%       0 & \frac{1}{2} & \frac{1}{2}
%     \end{pmatrix}.
%   \end{eqnarray*}
%   Therefore,
%   \begin{eqnarray*}
%     P^*P:=\begin{pmatrix}
%       \frac{1}{2} & 0 & \frac{1}{2} \\
%       0 & 1 & 0 \\
%       \frac{1}{4} & 0 & \frac{3}{4}
%     \end{pmatrix},
%   \end{eqnarray*}
%   then $P^*P$ is not irreducible. This matrix has an eigenvalue $1$ with multiplicity $2$.

%   Similarly, if some of the diagonal elements of $P$ is very small, 
% \end{remark}

The equality of \eqref{tracekeymain} holds not only for reversible matrices. For example, normal consensus matrices satisfy the property $P^*P=PP^*$. The class of normal consensus matrices includes Cayley graphs, and $J(P)$ on Cayley graphs have been studied (e.g. \cite{cayley}).
\begin{proposition}
  If $P$ is a normal consensus matrix, then $P^*P=PP^*$.
\end{proposition}
\begin{proof}
  Because $P$ is normal, $(I-P)(I-P^\T)=(I-P^\T)(I-P)$. Multiplying $\bm{1}$ from the right, we obtain $(I-P)(I-P^\T)\bm{1}=\bm{0}$, which means $(I-P^\T)\bm{1}\in\ker (I-P)$. Therefore, there exists some real value $a$ that satisfies $(I-P^\T)\bm{1}=a\bm{1}$, because $\ker(I-P)=\vecspan\{\bm{1}\}$. Then we multiply $\bm{1}^\T$ from the left and obtain $\bm{0}=an$. This means that $a=0$ and $(I-P^\T)\bm{1}=\bm{0}$. Thus, $P$ is doubly-stochastic and the invariant measure $\bm{\pi}$ is $\frac{1}{n}\bm{1}$. Therefore, we obtain $P^*=P$ and $P^*P=P^\T P=PP^\T=PP^*$.
\end{proof}

\begin{figure}
  \centering
  \includegraphics[keepaspectratio, scale=0.35]{consensus.png}
  \caption{Inclusion properties among classes of consensus matrices.}
  \label{incl}
\end{figure}

Inclusion properties among classes of consensus matrices are shown in \Cref{incl}. Notice that there are also a matrix $P$ which is not reversible, nor normal, but satisfies $P^*P=PP^*$, such as the following example \eqref{matrixexample}. Thus, the condition $P^*P=PP^*$ is a weeker assumption than reversibility or normality.
\begin{eqnarray}\label{matrixexample}
  P=\frac{1}{2+\sqrt{10}}\begin{pmatrix}
    2 & 1 & -1+\sqrt{10} & 0 \\
    1 & 2 & 0 & -1+\sqrt{10} \\
    0 & 1+\sqrt{10} & 1 & 0 \\
    1+\sqrt{10} & 0 & 0 & 1
  \end{pmatrix}.
\end{eqnarray}

\subsection{A bound for $J(P)$ and $J_{\mathrm{w}}(P)$ using $\averes(C)$}

In this section, we prove the following \Cref{mainres1} that provides upper and lower bounds for $J(P)$ using the effective resistance of $\Phi(P^*P)$. \Cref{mainres1} is a generalization of Theorem 3.1 in \cite{mainp} for reversible $P$.
\begin{theorem}
  \label{mainres1}
  Let $P$ be a consensus matrix with invariant measure $\bm{\pi}$, and let $C_{P^*P}:=\Phi(P^*P)=nP^\T\mathit{\Pi}P$ be the conductance matrix. Then,
  \begin{eqnarray*}
    J(P)\leq\frac{\pi_{\max}^3n^2}{\pi_{\min}}\bar{\mathcal{R}}(C_{P^*P}),\quad
    J_{\mathrm{w}}(P)\leq\pi_{\max}^3n^3\averes(C_{P^*P}),
  \end{eqnarray*}
  where $\pi_{\min}$ and $\pi_{\max}$ are, respectively, the minimum and maximum entries of $\bm{\pi}$. Moreover, if $PP^*=P^*P$, then
  \begin{eqnarray*}
    J(P)\geq\frac{\pi_{\min}^3n^2}{\pi_{\max}}\bar{\mathcal{R}}(C_{P^*P}),\quad
    J_{\mathrm{w}}(P)\geq\pi_{\min}^3n^3\bar{\mathcal{R}}(C_{P^*P}).
  \end{eqnarray*}
\end{theorem}

To prove \Cref{mainres1}, in the same manner as the approach in \cite{mainp}, we define the \textit{weighted average effective resistance}
\begin{eqnarray}\label{weff}
  \averes_{\mathrm{w}}(C):=\frac{1}{2}\bm{\pi}^\T\mathcal{R}(C)\bm{\pi}=\frac{1}{2}\sum_{(u,v)\in V\times V}\mathcal{R}_{uv}(C)\pi_u\pi_v,
\end{eqnarray}
where $\bm{\pi}$ is the invariant measure of reversible $P$ and $C:=\Phi(P)$.

\begin{lemma}\label{rwn}
  For a reversible consensus matrix $P$, let $C:=\Phi(P)$ and $\bm{\pi}$ be the invariant measure of $P$. Then,
  \[n^2\pi_{\min}^2\averes(C)\leq \averes_{\mathrm{w}}(C)\leq n^2\pi_{\max}^2\averes(C).\]
\end{lemma}
\begin{proof}
  Because $\pi_{\min}\leq \pi_u\leq \pi_{\max}$,
  \begin{eqnarray*}
    \averes_{\mathrm{w}}(C)\leq \frac{1}{2}\pi_{\max}^2\sum_{(u,v)\in V\times V}\mathcal{R}_{uv}(C)=n^2\pi_{\max}^2\averes(C),
  \end{eqnarray*}
  and
  \begin{eqnarray*}
    \averes_{\mathrm{w}}(C)\geq \frac{1}{2}\pi_{\min}^2\sum_{(u,v)\in V\times V}\mathcal{R}_{uv}(C)=n^2\pi_{\min}^2\averes(C).
  \end{eqnarray*}
\end{proof}

\begin{lemma}\label{rw}
  For a reversible consensus matrix $P$, let $C:=\Phi(P)$. Then,
  \[\averes_{\mathrm{w}}(C)=\frac{1}{n}\tr G(P),\]
  where $G(P)$ is the Green matrix of $P$ defined in \eqref{defgp}.
\end{lemma}
\begin{proof}
  The similar statement appears in the proof of Lemma 5.8 of \cite{mainp}. By changing $P^2$ to $P$ in \cite{mainp}, we obtain the lemma.
\end{proof}

% \begin{remark}\label{remhitting}
%   $\averes_{\mathrm{w}}(C)$ can be written in terms of hitting time of the Markov chain associated with $P$ (see Appendix A.1).
% \end{remark}

Using $\averes_{\mathrm{w}}(C)$, we prove Theorem \ref{mainres1}.
\begin{proof}[Proof of \Cref{mainres1}]
  First, we show the upper bound of $J_{\mathrm{w}}(P)$ in \Cref{mainres1}. Let $G_{\mathrm{w}}:=\sum_{t\geq 0}(I-\bm{\pi}\bm{1}^\T)(P^\T)^t\mathit{\Pi}P^t(I-\bm{1}\bm{\pi}^\T)$. Because $G_{\mathrm{w}}$ is positive semidefinite, its diagonal elements are nonnegative. Therefore, the matrix $\varPi^{-1} G_{\mathrm{w}}$ satisfies
  \[\pi_{\max}^{-1}(G_{\mathrm{w}})_{ii}\leq (\varPi^{-1} G_{\mathrm{w}})_{ii}\leq \pi_{\min}^{-1}(G_{\mathrm{w}})_{ii}.\]
  By summation from $i=1$ to $n$, we obtain
  \begin{eqnarray}\label{jwpb1}
    \pi_{\max}^{-1}J_{\mathrm{w}}(P)\leq \tr\paren{\varPi^{-1}G_{\mathrm{w}}}\leq \pi_{\min}^{-1}J_{\mathrm{w}}(P),
  \end{eqnarray}
  because $J(P)=\tr G_{\mathrm{w}}$ by definition. Since
  \begin{eqnarray*}
    \tr\paren{\varPi^{-1}G_{\mathrm{w}}}=\tr\paren{\varPi^{-1}\sum_{t\geq 0}\paren{(P^\T)^t\varPi P^t-\bm{\pi}\bm{\pi}^\T}}=\tr\paren{\sum_{t\geq 0}\paren{(P^*)^tP^t-\bm{1}\bm{\pi}^\T}},
  \end{eqnarray*}
  \eqref{jwpb1} yields
  \begin{eqnarray}\label{jweval}
    \pi_{\max}^{-1}J_{\mathrm{w}}(P)\leq \tr\paren{\sum_{t\geq 0}\paren{(P^*)^tP^t-\bm{1}\bm{\pi}^\T}}\leq \pi_{\min}^{-1}J_{\mathrm{w}}(P).
  \end{eqnarray}
  By \Cref{tracekey}, we obtain
  \begin{eqnarray}\label{jweval2}
    J_{\mathrm{w}}(P) \leq\pi_{\max}\tr\paren{\sum_{t\geq 0}\paren{(P^*)^tP^t-\bm{1}\bm{\pi}^\T}}\leq\pi_{\max}\tr G(P^*P).
  \end{eqnarray}
  Since $P^*P$ is a reversible consensus matrix, Lemmas \ref{rwn} and \ref{rw} imply that
  \[J_{\mathrm{w}}(P)\leq n\pi_{\max}\averes_{\mathrm{w}}(C_{P^*P})\leq n^3\pi_{\max}^3\averes(C_{P^*P}).\]
  Thus, the upper bound of \Cref{mainres1} is obtained.

  Then we show the lower bound of $J_{\mathrm{w}}(P)$ in \Cref{mainres1}. When $PP^*=P^*P$, we can rewrite (\ref{jweval}) as below:
  \begin{eqnarray}\label{jweval3}
    J_{\mathrm{w}}(P)&\geq&\pi_{\min}\tr\paren{\sum_{t\geq0}\paren{(P^*)^tP^t-\bm{1}\bm{\pi}^\T}}=\pi_{\min}\tr G(P^*P).
  \end{eqnarray}
  Lemmas \ref{rwn} and \ref{rw} implies that \[J_{\mathrm{w}}(P)\geq n\pi_{\min}\averes_{\mathrm{w}}(C_{P^*P})\geq n^3\pi_{\min}^3\averes(C_{P^*P}).\] Thus, we obtain the bound of $J_{\mathrm{w}}(P)$.

  Finally, we give a bound $J(P)$ by $J_{\mathrm{w}}(P)$. Comparing (\ref{lqcost}) and (\ref{lqcost2}), we obtain
  \begin{eqnarray}\label{compare12}
    \frac{1}{n\pi_{\max}}J_{\mathrm{w}}(P)\leq J(P)\leq\frac{1}{n\pi_{\min}}J_{\mathrm{w}}(P).
  \end{eqnarray}
  Applying \eqref{jweval2} and \eqref{jweval3} to \eqref{compare12}, the proof is completed.
\end{proof}

\begin{remark}
  It seems to be hard to give a lower bound for $\tr \paren{(P^*)^tP^t}$ using $\tr (P^*P)^t$, because $\tr (P^*P)^t$ can be much larger than $\tr (P^*P)^t$. Let us consider the following consensus matrix:
  \begin{eqnarray*}
    P_{\varepsilon}=\begin{pmatrix}
      \varepsilon & 1-\varepsilon & 0 \\
      0 & \varepsilon & 1-\varepsilon \\
      \frac{1}{2} & 0 & \frac{1}{2}
    \end{pmatrix}.
  \end{eqnarray*}
  $P^t-\bm{1}\bm{\pi}$ converges exponentially with respect to the second largest eigenvalue of $P$. The eigenvalues of $P_{\varepsilon}$ are $1$ and $\alpha_{\pm}:=-\paren{\frac{1}{4}-\varepsilon}\pm\frac{\mathrm{i}}{2}\sqrt{\frac{7}{4}-2\varepsilon}$. The eigenvalues of $P_{\varepsilon}^*$ are the same due to similarity. Therefore, $(P_{\varepsilon}^*)^tP_{\varepsilon}^t-\bm{1}\bm{\pi}$ converges exponentially to some real matrix with respect to the rate near $1/2$.

  On the other hand, $P^*P$ has the eigenvalue $1-O(\varepsilon)$. This means that if $\varepsilon$ is small enough, the convergence of $(P^*P)^t$ is very slow, and its trace can be far larger than $\tr (P_{\varepsilon}^*)^tP_{\varepsilon}^t$.
\end{remark}

\subsection{A bound for $J(P)$ using the graph structure of network}
In this section, we establish bounds for $J(P)$ using the structure of undirected associated graph $\mathcal{G}(P)$, the maximum and minimum elements of $\bm{\pi}$, and the minimum element of $P$. Notice that we do not consider the edge weights of $\mathcal{G}(P)$ and focus only on its graph topology of $\mathcal{G}(P)$.

Although \Cref{mainres1} provides bounds when all elements of $P$ are known, \Cref{mainres2} offers bounds even if complete information about the elements of $P$ and $\bm{\pi}$ is unavailable.

\begin{theorem}
  \label{mainres2}
  Let $P$ be a consensus matrix with invariant measure $\pi$, and let $\mathcal{G}(P)$ be the associated undirected graph with $P$. Then it holds that
  \begin{eqnarray*}    J(P)\leq\frac{\pi_{\max}^3n}{p_{\min}^2\pi_{\min}^2}\averes(\mathcal{G}(P)),\quad
    J_{\mathrm{w}}(P)\leq\frac{\pi_{\max}^3n^2}{p_{\min}^2\pi_{\min}}\averes(\mathcal{G}(P)),
  \end{eqnarray*}
  where $\pi_{\min}$ and $\pi_{\max}$ are respectively the minimum and maximum entries of $\bm{\pi}$, $p_{\min}$ is the minimum nonzero entry of $P$, and $\averes(\mathcal{G}(P))$ is the average effective resistance of a network, associated with $\mathcal{G}(P)$, such that each edge has a unit conductance. Moreover, if $PP^*=P^*P$, then
  \begin{eqnarray*}
    J(P)\geq\frac{\pi_{\min}^3n}{4p_{\max}^2\din^2\pi_{\max}^2}\averes(\mathcal{G}(P)),\quad
    J_{\mathrm{w}}(P)\geq\frac{\pi_{\min}^3n^2}{4p_{\max}^2\din^2\pi_{\max}}\averes(\mathcal{G}(P)).
  \end{eqnarray*}
  where $p_{\max}$ is the maximum nonzero entry of $P$, and $\din$ is the maximum indegree (excluding self loops) of the associated directed graph $\gdir(P)$.
\end{theorem}

By Theorem \ref{mainres1}, it is enough for the proof to show the following lemma, which is the generalization of Lemma 5.9 in \cite{mainp}.
\begin{lemma}\label{mainlemma2}
  Let $P$ be a consensus matrix with invariant measure $\bm{\pi}$ and $C_{P^*P}:=\Phi(P^*P)=nP^\T\varPi P$. Then
  \begin{eqnarray*}
    \averes(C_{P^*P})\leq \frac{1}{n\pi_{\min}p_{\min}^2}\averes(\mathcal{G}(P)).
  \end{eqnarray*}
  Moreover, if $PP^*=P^*P$, then
  \begin{eqnarray*}
    \averes(C_{P^*P})\geq \frac{1}{4n\pi_{\max}\din p_{\max}^2}\averes(\mathcal{G}(P)).
  \end{eqnarray*}
\end{lemma}
Since the associated graph of the consensus matrix $P$ can be directed, we evaluate $\averes(C_{P^*P})$ using the maximum indegree $\din$ of the graph nodes of $\gdir(P)$, unlike previous research \cite{mainp}.

We employ the undirected graph $\mathcal{G}(P)$ instead of the directed graph $\gdir$, because the following property is essential for proving \Cref{mainres2}.
\begin{lemma}[Rayleigh's monotonicity law]\label{rayleigh}
  Let $C$ and $C'$ be conductance matrices such that $C\leq C'$. Then, $\mathcal{R}(C)\geq \mathcal{R}(C')$.
\end{lemma}
In plain words, if the resistance of any edge increases, then the resistance between any pair of nodes also increases. The proof can be found in \cite{randomwalks}.

Using \Cref{rayleigh}, we will compare $\averes(C_{P^*P})$ and $\averes(\mathcal{G}(P^*P))$. Recall that $\averes(\mathcal{G}(P^*P))$ is decided only by the graph topology of $\mathcal{G}(P^*P)$, and is independent of each element of $P^*P$. In contrast, $\averes(C_{P^*P})$ is affected by each element of $C_{P^*P}$, which can vary to each other.

Therefore, we first consider the range of the elements of $C_{P^*P}$. When $(C_{P^*P})_{uv}\neq 0$, there is an index $w$ such that $P_{wu}$ and $P_{wv}$ are both positive, so it holds that $(C_{P^*P})_{uv}=n\sum_w\pi_wP_{wu}P_{wv}\geq n\pi_{\min}p_{\min}^2$. Moreover, the number of nonzero entries of $P_{wu}$ for any fixed $u$ does not exceed $\din+1$. Therefore, we obtain $(C_{P^*P})_{uv}\leq n\pi_{\max}(\din+1) p_{\max}^2$. Applying \Cref{rayleigh} to $(C_{P^*P})_{uv}$, we obtain
\begin{eqnarray}\label{simple_monotonicity}
  \frac{1}{n\pi_{\max}(\din+1) p_{\max}^2}\averes(\mathcal{G}(P^*P))\leq \averes(C_{P^*P})\leq \frac{1}{n\pi_{\min}p_{\min}^2}\averes(\mathcal{G}(P^*P)).
\end{eqnarray}

Next, we evaluate $\averes(\mathcal{G}(P^*P))$ using $\averes(\mathcal{G}(P))$, both of which are determined by the positions of the nonzero elements in $P$. Because $(P^*P)_{uv}=\sum_w\pi_u^{-1}\pi_wP_{wu}P_{wv}$, $(P^*P)_{uv}$ is positive if and only if there exists an index $w$ which $P_{wu}$ and $P_{wv}$ are positive. From a graph theoretic perspective, this means that $\mathcal{G}(P^*P)$ has an edge $\{u,v\}$ if and only if there exists a node $w$ with edges $(w,u)$ and $(w,v)$ in $\gdir(P)$. 

To compare the places of edges in $\mathcal{G}(P^*P)$ and $\mathcal{G}(P)$, we introduce the novel concepts of a \textit{back-and-forth path} and its \textit{pivot node}. A \textit{back-and-forth path} between $u$ and $v$ is a path which consists of $\{u,w\},\{w,v\}\in\mathcal{G}(P)$ where $(w,u)$ and $(w,v)$ are edges in $\gdir(P)$. We refer to $w$ as the \textit{pivot node} of the path (see Figure \ref{newedge}). When $\gdir(P)$ is symmetric, namely, $P_{ij}\neq 0$ if and only if $P_{ji}\neq 0$, back-and-forth paths reduce to paths of length $2$ in $\mathcal{G}(P)$. In such cases, including when $P$ is reversible, $\mathcal{G}(P^*P)$ has the same graph topology as $\mathcal{G}(P^2)$. Generally, the graph $\mathcal{G}(P^h)$ is called $h$-fuzz, and previous research has shown the relationship between $\averes(\mathcal{G}(P))$ and $\averes(\mathcal{G}(P^h))$ \cite{hfuzz}. However, for nonreversible $P$, it is worthwhile to compare $\mathcal{G}(P)$ and $\mathcal{G}(P^*P)$.

Using the concept of a back-and-forth path, we can obtain an upper bound by a simple way. For each edge $(u,v)$ in $\gdir(P)$, we can choose a back-and-forth path between $u$ and $v$, because there are edges $(u,u)$ (a self loop) and $(u,v)$ in $\gdir(P)$. Therefore, any edge appearing in $\mathcal{G}(P)$ also appears in $\mathcal{G}(P^*P)$. By this property, we can obtain the following upper bound for $\averes(\mathcal{G}(P^*P))$:
\begin{lemma}
  \label{upper_bound_GPP}
  $\mathcal{R}_{uv}\paren{\mathcal{G}(P)}\geq\mathcal{R}_{uv}\paren{\mathcal{G}(P^*P)}.$
\end{lemma}
\begin{proof}
  Because all edges in $\mathcal{G}(P)$ appear in $\mathcal{G}(P^*P)$, this lemma is a direct result from Rayleigh's monotonicity law.
\end{proof}

We can also obtain a lower bound for $\averes(\mathcal{G}(P^*P))$. The following lemma is a variation of an argument for $h$-fuzz shown in Lemma 5.5.1. of \cite{hfuzz}.
\begin{lemma}
  \label{lower_bound_GPP}
  Let $\mathcal{G}(P)$ and $\mathcal{G}(P^*P)$ be the associated undirected graph with $P$ and $P^*P$, respectively. Then, it holds that
  \begin{eqnarray*}
    \frac{1}{4(\din-1)}\mathcal{R}_{uv}\paren{\mathcal{G}(P)}\leq \mathcal{R}_{uv}\paren{\mathcal{G}(P^*P)},
  \end{eqnarray*}
  where $\din$ is the maximum indegree of the graph nodes of $\gdir(P)$.
\end{lemma}
\begin{proof}
  Let $\mathcal{G}(P)=(V,\mathcal{E})$ and $\mathcal{G}(P^*P)=(V,\mathcal{E}^*)$. Observe that all the edges in $\mathcal{E}$ are also present in $\mathcal{E}^*$. We refer to the edges that are in $\mathcal{E}^*$ but not in $\mathcal{E}$ as `new edges' (see Figure \ref{newedge}). Consider replacing each new edge in $\mathcal{G}(P^*P)$ with a series of two edges and an intermediate node, resulting in a modified graph $\bar{\mathcal{G}}=(\bar{V},\bar{\mathcal{E}})$. In this transformation, the resistances of the new edges are doubled. By Rayleigh's monotonicity law, the resistance of every pair of two nodes in $V$ becomes equal to or greater than the original value and at most twice the original value, that is, $R_{uv}(\mathcal{G}(P^*P))\leq R_{uv}(\bar{\mathcal{G}})\leq 2R_{uv}(\mathcal{G}(P^*P))$ holds for every pair of nodes $u,v\in V$.

  \begin{figure}
    \centering
    \includegraphics[keepaspectratio, scale=0.25]{pivotne.png}
    \caption{Notions of back-and-forth path, pivot, and new edge.}
    \label{newedge}
  \end{figure}

  Next, focus on an intermediate node $x_{uv}$ added when replacing a new edge $\{u,v\}$. For every new edge $\{u,v\}$ in $\mathcal{G}(P^*P)$, there exists a back-and-forth path in $\mathcal{G}(P)$. We then short-circuit the connection between $x_{uv}$ in $\bar{\mathcal{G}}$ and the pivot of the back-and-forth path. To short-circuit a connection means to reduce the resistance of a pair of nodes from infinity to zero. Applying Rayleigh's monotonicity law again, we obtain $R_{\bar{u}\bar{v}}(\mathcal{G}')\leq R_{\bar{u}\bar{v}}(\bar{\mathcal{G}})$ for every pair of nodes $\bar{u},\bar{v}\in \bar{V}$, where $\mathcal{G}'$ is the resulting graph after shortening (see Fig.\,\ref{Fig:evaluation}).

  The graph $\mathcal{G}'$ differs from $\mathcal{G}(P)$ only in the multiplicity of the edges, so we compare edges of both graphs. Notice that $\mathcal{G}'$ has one self loop for each node, which is the same as $\mathcal{G}(P)$. The number of edges between $u,v\in V\ (u\neq v)$ in $\mathcal{G}'$ is bounded by the number of back-and-forth paths through $\{u,v\}$ in $\mathcal{G}(P)$. Let $\eta$ be the upper bound of the number of such paths. Because an $n$-multiple edge is equivalent to a single edge of resistance $\frac{1}{n}$, we can use Rayleigh's monotonicity law for $\mathcal{G}'$ and obtain $R_{uv}(\mathcal{G}')\geq \frac{1}{\eta}R_{uv}(\mathcal{G}(P))$. Combining the inequalities, we have
  \[\frac{1}{\eta}R_{uv}(\mathcal{G}(P))\leq R_{uv}(\mathcal{G}')\leq R_{uv}(\bar{\mathcal{G}})\leq 2R_{uv}(\mathcal{G}(P^*P))\]
  for all $u,v\in \mathcal{V}$.

  \begin{figure}
    \centering
    \includegraphics[keepaspectratio, scale=0.25]{evalpp.png}
    \caption{Graphs used to evaluate $\averes(\mathcal{G}(P^*P))$.}
    \label{Fig:evaluation}
  \end{figure}
  
  Finally, we evaluate $\eta$. For a back-and-forth path through $\{u,v\}$ in $\mathcal{G}(P)$, the pivot is $u$ or $v$. If $u$ is the pivot, then the possible choices of the other end of the path except $v$ are at most $\din-1$. The same is true for $v$. Therefore, we obtain $\eta\leq 2(\din-1)$ and $\frac{1}{2(\din-1)}R_{uv}(\mathcal{G}(P))\leq 2R_{uv}(\mathcal{G}(P^*P))$, which proves the lemma.
\end{proof}

We are now in a position to prove \Cref{mainres2}.
\begin{proof}[Proof of \Cref{mainres2}]
  Applying Lemmas \ref{upper_bound_GPP} and \ref{lower_bound_GPP} to \eqref{simple_monotonicity}, we obtain
  \begin{eqnarray*}
    \frac{1}{4n\pi_{\max}(\din-1)(\din+1) p_{\max}^2}\averes(\mathcal{G}(P^*P))\leq \averes(C_{P^*P})\leq \frac{1}{n\pi_{\min}p_{\min}^2}\averes(\mathcal{G}(P^*P)).
  \end{eqnarray*}
  Using \Cref{mainres1} and $(\din+1)(\din-1)\leq \din^2$, the proof is completed.
\end{proof}

\section{Application and numerical experiment}
In this section, we apply the bound obtained in \Cref{mainres2} to geometric graphs. Geometric graphs model the position in a $d$-dimensional space, so the result can be useful for controlling objects in the real world. This is a generalization of the result shown in \cite{mainp}.

A geometric graph is a connected, undirected, and unweighted graph $\mathcal{G}(V,\mathcal{E})$ such that $V\subseteq \mathcal{Q}$, where $\mathcal{Q}:=[0,l]^d\subseteq \mathbb{R}^d$ and $|V|=n$. Notice that there are no constraint on $\mathcal{E}$ at first, but the distances between connected pairs of two nodes are used to define parameters.

For such graphs, we can define the following parameters \cite{mainp}:
\begin{enumerate}
  \item the minimum node distance \[s=\underset{u,v\in V,\ u\neq v}{\min}d_{\mathrm{E}}(u,v),\]where $d_{\mathrm{E}}$ denotes Euclidean distance;
  \item the maximum connected range \[r=\underset{(u,v)\in \mathcal{E}}{\max}d_{\mathrm{E}}(u,v);\]
  \item the maximum uncovered diameter \[\gamma=\sup\parbr{r>0 \mid \exists\bm{x}\in \mathcal{Q},\ B(\bm{x},r)\cap V=\emptyset\in \mathcal{E}},\] where $B(\bm{x},r)$ is a $d$-dimensional ball centered in $\bm{x}\in\mathbb{R}^d$ and with radius $r$;
  \item the minumum ratio between Euclidean distance and graphical distance \[\rho=\min\parbr{\frac{d_{\mathrm{E}}(u,v)}{d_{\mathcal{G}}(u,v)}\mid u,v\in V,\ u\neq v},\]where $d_{\mathcal{G}}$ is the length of the shortest path in $\mathcal{G}$.
\end{enumerate}
There are some relationships among these parameters as follows:
\begin{eqnarray}\label{paramsrelations}
  s\leq r,\ s\leq 2\gamma,\ \rho\leq r,\ \delta\leq\paren{\frac{3r}{s}}^d.
\end{eqnarray}
Inequalities $s\leq r$ and $s\leq 2\gamma$ can be easily checked by definition. The third inequality comes from
\[\rho\leq \frac{d_{\mathrm{E}}(u,v)}{d_{\mathcal{G}}(u,v)}\leq\frac{d_{\mathrm{E}}(u,v)}{1}\leq r.\]
The last inequality is obtained by comparing volumes of spheres. For a node $u$, let $B\paren{u,r+\frac{s}{2}}$ be a $d$-dimensional ball centered in $u$ and with radius $r+\frac{s}{2}$. This ball includes a ball $B\paren{v,\frac{s}{2}}$ for all $v$ which is a neighbor node of $u$. By definition of $s$, balls $B\paren{v,\frac{s}{2}}$ are not crossing with each other. Therefore, the volume of $B\paren{u,r+\frac{s}{2}}$ is larger than the sum of the volumes of $B\paren{v,\frac{s}{2}}$ for all $v$. Then we obtain
\[\paren{r+\frac{s}{2}}^d\geq \delta\paren{\frac{s}{2}}^d,\]
and, by $s\leq r$,
\[(3r)^d\geq \delta s^d,\]
which leads the last inequality of \eqref{paramsrelations}.

The previous study has shown an asymptotic behavior of $J(P)$ with respect to $n$ when $P$ is on geometric graphs and the parameters above are fixed.
\begin{proposition}[Theorem 4.1 in \cite{mainp}]\label{geo}
  Let $P\in\mathbb{R}^{n\times n}$ be a reversible consensus matrix with invariant measure $\bm{\pi}$, associated with a graph $\mathcal{G}=(V,\mathcal{E})$. Assume that all nonzero entries of $P$ belong to the interval $[p_{\min},p_{\max}]$and that $\mathcal{G}$ is a geometric graph with parameters $(s,r,\gamma,\rho)$ and nodes lying in $\mathcal{Q}=[0,l]^d$ in which $\gamma<l/4$. Then
  \[k_1+q_1f_\mathrm{d}(n)\leq J(P)\leq k_2+q_2f_\mathrm{d}(n),\]
  where
  \[f_\mathrm{d}(n)=\begin{cases}
    n & \textrm{if }d=1,\\
    \log n & \textrm{if }d=2,\\
    1 & \textrm{if }d\geq 3,\\
  \end{cases}\]
  and where $k_1,k_2,q_1$, and $q_2$ are positive numbers which are functions of the following parameters only: the dimension $d$, the geometric parameters of the graph ($s,r,\gamma$,and $\rho$), the maximum degree $\delta$, $p_{\min}$,and $p_{\max}$, and the products $\pi_{\min} n$ and $\pi_{\max} n$.
\end{proposition}

\Cref{geo} is useful when we consider a growing family of geometric graphs whose parameters $s,r,\gamma,\rho$ are bounded. However, \Cref{geo} is not for nonreversible consensus matrices.

Using \Cref{mainres2}, \Cref{geo} can be generalized as follows.

\begin{theorem}\label{maingeo}
  Let $P\in\mathbb{R}^{n\times n}$ be a consensus matrix with invariant measure $\bm{\pi}$, and let $\mathcal{G}(P)$ be the undirected associated graph of $P$. Assume that all the nonzero entries of $P$ belong to the interval $[p_{\min},p_{\max}]$and that $\mathcal{G}(P)$ is a geometric graph with parameters $(s,r,\gamma,\rho)$ and nodes lying in $\mathcal{Q}=[0,l]^d$ in which $\gamma<l/4$. Then
  \begin{eqnarray*}\label{geo1}
    J(P)\leq k'_2+q'_2f_\mathrm{d}(n),
  \end{eqnarray*}
  and particularly, when $P^*P=PP^*$ holds then
  \begin{eqnarray*}\label{geo2}
    J(P)\geq k'_1+q'_1f_\mathrm{d}(n),
  \end{eqnarray*}
  where
  \[f_\mathrm{d}(n)=\begin{cases}
    n & \textrm{if }d=1,\\
    \log n & \textrm{if }d=2,\\
    1 & \textrm{if }d\geq 3,\\
  \end{cases}\]
  and where $k'_1,k'_2,q'_1$, and $q'_2$ are positive numbers which are functions of the following parameters only: the dimension $d$, the geometric parameters of the graph $(s,r,\gamma,\rho)$, the maximum degree $\delta$ of $\mathcal{G}(P)$, $p_{\min}$ and $p_{\max}$, and the products $\pi_{\min} n$ and $\pi_{\max} n$.
\end{theorem}

\begin{proof}
  Theorem 4.1 in \cite{mainp} has shown that for reversible $P$,
  \begin{eqnarray}
    \label{jpgeo}c_l\averes(\mathcal{G})\leq J(P)\leq c_u\averes(\mathcal{G})
  \end{eqnarray}
  with $c_l$ and $c_u$ dependent on $p_{\min},p_{\max},\delta,\pi_{\max}n,\pi_{\min}n$. Using \Cref{mainres2} in this paper, \eqref{jpgeo} can be generalized to
  \[J(P)\leq c'_u\averes(\mathcal{G}(P)),\]
  without the assumption of reversibility of $P$, where $c'_u$ depends on $p_{\min}$,$p_{\max}$,$\delta$,$\pi_{\max}n$, and $\pi_{\min}n$. Moreover, if $P^*P=PP^*$, then
  \[J(P)\geq c'_l\averes(\mathcal{G}(P)),\]
  with $c'_l$ dependent on $p_{\min},p_{\max},\delta,\pi_{\max}n,\pi_{\min}n$. Notice that the lower bound of \Cref{mainres2} uses $\din$, but because $\din\leq \delta$, we can define $c'_l$ as a variable dependent on $\delta$ instead of $\delta_{\mathrm{in}}$.

  The rest of the proof is totally analogous to Section 6 in \cite{mainp}.
\end{proof}

\Cref{maingeo} is useful for a growing family of geometric graphs $\mathcal{G}_n=(V_n,\mathcal{E}_n)$ where $V_n\subseteq [0,l_n]^d$ and with the geometric parameters $(s_n,r_n,\gamma_n,\rho_n)$ which are bounded as
\begin{eqnarray*}\label{boundedparam}
  s_n\geq s,\ r_n\leq r,\ \gamma_n\leq\gamma,\ \rho_n\geq\rho.
\end{eqnarray*}
This family of graphs is called \textit{a family of geometric graphs with bounded parameters}, including regular grids, $R$-disks and Delaunay graphs with appropriate parameter bounds \cite{mainp}. For a family of geometric graphs which satisfies \eqref{boundedparam}, $s_n,r_n,\gamma_n,\rho_n$ and the maximum degree $\delta$ are lower and upper bounded as
\begin{eqnarray*}
  s\leq s_n\leq r_n\leq r,\ \rho\leq\rho_n\leq r,\ \frac{s}{2}\leq\gamma_n\leq\gamma,\ 1\leq\delta_n\leq\paren{\frac{3r}{s}}^d,
\end{eqnarray*}
which follows from \eqref{paramsrelations}. In addition to the assumption of \eqref{boundedparam}, if the ranges of nonzero entries of $P$ and $n\bm{\pi}$ are bounded, then all parameters determining $k'_1,k'_2,q'_1$ and $q'_2$ in \Cref{maingeo} are lower and upper bounded. Therefore, we obtain the following corollary.
\begin{corollary}\label{maingeo2}
  Let $P_n\in\mathbb{R}^{n\times n}$ be a family of consensus matrices with invariant measure $\bm{\pi}_n$, and let $\mathcal{G}(P_n)$ be the undirected associated graph of $P_n$. Assume that all nonzero entries of $P_n$ belong to the interval $[p_{\min},p_{\max}]$ and all entries of $n\bm{\pi}$ belong to the interval $[\bar{\pi}_{\min},\bar{\pi}_{\max}]$ and that the family of $\mathcal{G}(P_n)$ is a family of geometric graphs with bounded parameters $(s_n,r_n,\gamma_n,\rho_n)$ and nodes lying in $\mathcal{Q}=[0,l_n]^d$ in which $\gamma_n<l_n/4$. Then
  \begin{eqnarray*}\label{geo1asymp}
    J(P)\leq k''_2+q''_2f_\mathrm{d}(n),
  \end{eqnarray*}
  and particularly, when $P^*P=PP^*$ holds then
  \begin{eqnarray*}\label{geo2asymp}
    J(P)\geq k''_1+q''_1f_\mathrm{d}(n),
  \end{eqnarray*}
  where $f_\mathrm{d}(n)$ is the same as in \Cref{maingeo}, and $k''_1,k''_2,q''_1$, and $q''_2$ are positive numbers which are functions of the following parameters only: the dimension $d$, the bound of geometric parameters $(s,r,\gamma,\rho)$ which satisfy \eqref{boundedparam}, $p_{\min},p_{\max},\bar{\pi}_{\min}$ and $\bar{\pi}_{\max}$.
\end{corollary}
\begin{remark}\label{growthln}
  For a family of geometric graphs with bounded parameters, $l_n$ must grow linearly to $n^{1/d}$ in $n\to\infty$ (Lemma 6.3 in \cite{mainp}). This feature is used for setting of $l_n$ in the following numerical experiment.
\end{remark}



Based on the above discussion, we check the behavior of $J(P)$ on a family of geometric graphs with bounded parameters by numerical experiment. The dimension $d$ is set to $2$ or $3$. The experiment is conducted in the following way:
\begin{itemize}
  \item \textbf{Construct a geometric graph}
  \begin{itemize}
    \item We fix the dimension $d$, the geometric parameters $(s,r,\gamma,\rho)$, an edge making probability $p_{\mathrm{e}}$, an edge direction probability $p_{\mathrm{d}}$, and the number of nodes $n$.
    \item We fix a constant $c$ and set $l_n:=cn^{1/d}$.
    \item We choose $n$ nodes in $[0,l_n]^d$ in the following way:
    \begin{enumerate}
      \item We choose the first node in $[0,l_n]^d$ according to a uniform distribution.
      \item We repeat selecting the next node in $[0,l_n]^d$ according to a uniform distribution independently to the previous nodes. If the Euclidean distance between the picked node and any previous node is less than $s$, then we discard this node. We stop repeating when the number of accepted nodes becomes $n$.
    \end{enumerate}
    \item We construct the edges in the following way: For all pairs of nodes $\{u,v\}$, if the distance between $u$ and $v$ is greater than $r$, we do not draw any edge between them. Otherwise, we draw an edge between $u$ and $v$ with probability $p_{\mathrm{e}}$ (independently to other edges).
    \item If the graph is not connected, we discard it.
    \item We calculate $\gamma_n$ and $\rho_n$, and if $\gamma_n>\gamma$ or $\rho_n<\rho$, we discard the graph. The method of judgment of $\gamma_n>\gamma$ and $\rho_n<\rho$ is in Appendix.
  \end{itemize}
  \item \textbf{Construct a consensus matrix}
  \begin{itemize}
    \item We fix the constants $b$, $\bar{\pi}_{\min}$, and $\bar{\pi}_{\max}$.
    \item We construct a matrix $P_n\in\mathbb{R}^{n\times n}$, where $P_{uv}=1$ if $u$ and $v$ are connected by an edge or $u=v$, and otherwise $P_{uv}=0$. This is a symmetric matrix and not a stochastic matrix.
    \item For all undirected edges $\{u,v\}$, we change $P_{uv}$ to $0$ with probability $p_{\mathrm{d}}$, or $P_{vu}$ to $0$ with probability $p_{\mathrm{d}}$, or do nothing to $P_{uv}$ and $P_{vu}$ with the probability $1-2p_{\mathrm{d}}$.
    \item If $P$ is not irreducible, we discard $P$ and return to ``construct a geometric graph'' step.
    \item We change all nonzero entries in $P_n$ to random values chosen by a uniform distribution on $[b,1]$. Then we normalize each row of $P_n$ so that $P_n$ becomes a stochastic matrix. By this process, the elements of $P_n$ are guaranteed to be greater than or equal to $\frac{b}{b+\delta}$.
    \item We calculate the invariant measure $\bm{\pi}$ of $P$. If $n\pi_{\min}<\bar{\pi}_{\min}$ or $n\pi_{\max}>\bar{\pi}_{\min}$, we discard $P$ and return to ``construct a geometric graph'' step.
  \end{itemize}
  \item \textbf{Calculate $J(P)$}
  \begin{itemize}
    \item For $P$ constructed by ``construct a consensus matrix'' step, we calculate $J(P)$ by \eqref{lqcost}. The summation ends at $t=10^4$, or the first point where the change of sum is less than $10^{-5}$ for $10$ consective $t$.
  \end{itemize}
\end{itemize}


\begin{table}[t]
  \caption{$n$ used in the experiment}
  \label{table:expn}
  \centering
    \begin{tabular}{c|l}
    \hline
    $d$ & $n$ \\
    \hline
    2 & 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300 \\
    3 & 50, 150, 250, 350, 450, 550, 600, 650, 700, 750, 800 \\
    \hline
    \end{tabular}
\end{table}
\begin{table}[t]
  \caption{Parameters used in the experiment}
  \label{table:exprm}
  \centering
    \begin{tabular}{cccccccccc}
    \hline
    $s$ & $r$ & $\gamma$ & $\rho$ & $p_{\mathrm{e}}$ & $p_{\mathrm{d}}$ & $c$ & $b$ & $\bar{\pi}_{\min}$ & $\bar{\pi}_{\max}$ \\
    \hline \hline
    0.1 & 1.0 & 1.0 & 0.052 & 0.8 & 0.1 & 0.5 & 0.8 & 0.1 & 3.0\\
    \hline
    \end{tabular}
\end{table}

\begin{figure}[htbp]
  \begin{minipage}[htbp]{0.48\linewidth}
    \centering
    \includegraphics[keepaspectratio, scale=0.38]{d2result.png}
  \end{minipage}
  \begin{minipage}[htbp]{0.48\linewidth}
    \centering
    \includegraphics[keepaspectratio, scale=0.38]{d3result.png}
  \end{minipage}
  \caption{The values of $J(P)/\log n$ in $d=2$ (left panel), and $J(P)/\log n$ in $d=3$ (right panel). The solid lines in both panels are the average of $J(P)$ (divided by $\log n$ in the left panel).}
  \label{figure:expres}
\end{figure}


We have run the construction and calculation for $d=2$ and $d=3$. In both cases, we have constructed $15$ consensus matrices for each $n$ listed in Table \ref{table:expn}\@. We also show the parameters used for the experiment in Table \ref{table:exprm} (common in $d=2$ and $d=3$).

Notice that we only specify $\gamma_n$ as equal or less than $\gamma$, so $\gamma_n$ can be larger than $l_n/4$ if $\gamma>l_n/4$. In fact, when $n$ is small, the constraint $\gamma_n<l_n/4$ can be violated. Particularly, in our parameter setting, the results in $n\leq 64$ of $d=2$ case and $n\leq 512$ of $d=3$ case are not guaranteed to obey \Cref{maingeo2}.

The result is shown in Fig.\,\ref{figure:expres}. In the figure of $d=2$, the vertical axis shows $J(P)$ divided by $\log n$. The results suggest that the asymptotic growth of $J(P)$ is bounded as \Cref{maingeo2}. In fact, in small $n$, it seems that there is a decreasing feature in $d=2$ and an increasing feature in $d=3$, but in larger $n$ which satisfies $\gamma<l_n/4$, the asymptotic behavior predicted in \Cref{maingeo2} can be read. In addition, the lower bound for $J(P)$ is not guaranteed since our experiment does not assume $P^*P=PP^*$, but in the results, the value of $J(P)$ appears not to become much smaller. There may be some lower bound that holds for a random geometric graph with high probability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

In this paper, we presented an estimation of the LQ cost for the linear consensus algorithm applied to nonreversible matrices. Our approach leverages the reversible matrix $P^*P$, which remains reversible even when $P$ is not, to derive performance bounds using effective resistance. We further introduced novel concepts---the \textit{back-and-forth path} and \textit{pivot node}---to establish a relationship between the effective resistances of $\mathcal{G}(P)$ and $\mathcal{G}(P^*P)$. An application to geometric graphs was also demonstrated, underscoring the practical relevance of our results.

We believe that the methodology based on $P^*P$ can be extended to existing research on reversible cases. In reversible settings, since $P^*P = P^2$, studies that employ $P^2$ can be adapted to nonreversible cases using $P^*P$ instead. Future work may focus on establishing a more refined lower bound for the LQ cost in nonreversible cases, under assumptions that are less restrictive than those used in our current analysis, by leveraging effective resistance or alternative analytical tools.



\bibliographystyle{siamplain}
\bibliography{all}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

% \section{Relationship between $\averes_{\mathrm{w}}(C)$ and hitting time of the Markov chain}

% This section is used to note what we have mentioned in \Cref{remhitting}. In \cite{novelnoisy}, an explicit expression of $J_{\mathrm{w}}(P)$ using hitting time of the Markov chain is suggested.

% \begin{definition}
%   Consider a random walk on $n$ nodes with a transition matrix $P\in\mathbb{R}^{n\times n}$. Then, we call \textit{hitting time} from $i$ to $j$, the expected time to visit $j$ for the first time starting $i$. We also define a matrix $H_P$ whose $(i,j)$ element is hitting time from $i$ to $j$.
% \end{definition}

% \begin{proposition}[Explicit expression using hitting time]\label{newjwp}
%   For a reversible stochastic matrix,
%   \[J_{\mathrm{w}}(P)=\bm{\pi}^\T H_{P^2}\varPi^2\bm{1}.\]
% \end{proposition}

% \Cref{newjwp} is a variant of Theorem 1 in \cite{novelnoisy}. Notice that we have introduced $J_{\mathrm{w}}(P)$ from the point of view of noisy consensus, and for simplicity, we have assumed that the covariance of the noise is an identity matrix. The original expression in \cite{novelnoisy} takes the covariance of the noise into account, but we abbreviate here.

% As the same manner in Chapter 2 of this paper, \Cref{newjwp} can be generalized to nonreversible cases. From \textit{Proof of Theorem 1} of \cite{novelnoisy}, we obtain that
% \[\sum_{k=0}^\infty (P^k-\bm{1}\bm{\pi}^\T)=\bm{1}\bm{\pi}^\T H_P\varPi-H_P\varPi.\]
% Therefore, by \eqref{jweval2} and \eqref{jweval3}, we obtain bounds of $J_{\mathrm{w}}(P)$ for nonreversible $P$ using hitting time as below:
% \begin{eqnarray*}
%   J_{\mathrm{w}}(P)&\leq& \pi_{\max}\tr G(P^*P)=\pi_{\max}\tr\paren{\bm{1}\bm{\pi}^\T H_{P^*P}\varPi-H_{P^*P}\varPi}=\pi_{\max}\bm{\pi}^\T H_{P^*P}\bm{\pi},
% \end{eqnarray*}
% since the diagonal elements of $H_{P^*P}$ are zero by definition. Moreover, when $P^*P=PP^*$,
% \begin{eqnarray*}
%   J_{\mathrm{w}}(P)&\geq& \pi_{\min}\tr G(P^*P)=\pi_{\min}\tr\paren{\bm{1}\bm{\pi}^\T H_{P^*P}\varPi-H_{P^*P}\varPi}=\paren{\bm{\pi}^\T H_{P^*P}\bm{\pi}}.
% \end{eqnarray*}

\section{Calculation of $\gamma_n$ and $\rho_n$ in the numerical experiment}
In the numerical experiment in Section 4, we have to judge whether $\gamma_n>\gamma$, and whether $\rho_n<\rho$. This condition is checked in the following way.
\begin{itemize}
  \item $\gamma_n>\gamma$ \\
  We take all points in $\mathcal{Q}=[0,l]^d$ so that each coordinate is an integer multiple of $\tilde{l}:=l/30$. Notice that the number of points is $31^d$. For all of these $31^d$ points, we check if they are more than $\gamma-\tilde{l}\sqrt{d}/2$ away from all the points in $V$. If not, it is guaranteed that all the points in $\mathcal{Q}$ are within distance $\gamma$ of any point in $V$, so we judge that $\gamma_n\leq\gamma$. Otherwise, there may be a point further than $\gamma$ from all points in $V$, so we judge that $\gamma_n>\gamma$ and discard the constructed graph. It is possible to be discarded even if $\gamma_n\leq\gamma$, but if the graph is not discarded, then $\gamma_n\leq\gamma$ necessarily holds.
  \item $\rho_n<\rho$ \\
  We use Floyd-Warshall (see Section 23.2 in \cite{cormen2022introduction}) method to calculate the distance between all pairs of points. Then we calculate $\rho_n$ and judge whether $\rho_n<\rho$.
\end{itemize}



\end{document}
