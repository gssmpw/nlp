%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{color}
\usepackage{tikz}
\usepackage{forest}
\usepackage{makecell}
% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Recent Advances on Generalizable Diffusion-generated Image Detection
}

%Towards Generalizable \textcolor{red}{AI-Generated} Image Detection in the Diffusion Era: A Survey

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\author{
Qijie Xu$^1$\and
Defang Chen$^2$\thanks{Corresponding author.}\and
Jiawei Chen$^{1}$\and
Siwei Lyu$^{2}$\And
Can Wang$^{1}$
\affiliations
$^1$State Key Laboratory of Blockchain and Data Security, Zhejiang University\\
$^2$University at Buffalo, State University of New York\\
\emails
\{qijxu, sleepyhunt, wcan\}@zju.edu.cn,
\{defchern, siweilyu\}@buffalo.edu
}

\begin{document}

\maketitle

\begin{abstract}
The rise of diffusion models has significantly improved the fidelity and diversity of generated images. %, expanding their applications across various domains. 
With numerous benefits, these advancements also introduce new risks. Diffusion models can be exploited to create high-quality Deepfake images, which poses challenges for image authenticity verification. In recent years, research on generalizable diffusion-generated image detection has grown rapidly. However, a comprehensive review of this topic is still lacking. To bridge this gap, we present a systematic survey of recent advances and classify them into two main categories: (1) data-driven detection and (2) feature-driven detection. Existing detection methods are further classified into six fine-grained categories based on their underlying principles. Finally, we identify several open challenges and envision some future directions, with the hope of inspiring more research work on this important topic. Reviewed works in this survey can be found at \url{https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detection}.
\end{abstract}

\section{Introduction}
Recent years have witnessed explosive growth in generative models. Beyond traditional Generative Adversarial Networks (GANs)~\cite{goodfellow2020generative}, diffusion models \cite{ho2020denoising,rombach2022high} have considerably improved image generation by modeling the gradients of image distributions. This advancement has led to remarkable gains in fidelity and diversity, making diffusion models widely adopted across various applications.
However, the increasing ability of generative models in synthesizing highly realistic images has raised significant societal and ethical concerns. These advanced technologies can be potentially exploited for malicious purposes, particularly in the creation of Deepfake images, facilitating various illegal activities including fake news dissemination, blackmail, and financial fraud~\cite{lyu2020deepfake}. Figures \ref{fig:harm} illustrates two examples of the significant harm caused by the malicious purposes of diffusion-generated images.\footnote{\url{https://edition.cnn.com/2023/05/22/tech/twitter-fake-image-pentagon-explosion/index.html}}\footnote{\url{https://x.com/AmyKremer/status/1841928828576272548}}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/harm.pdf}
    \caption{
    Generated images can now easily mislead the public, leading to serious consequences such as panic and economic losses.
    }
    \label{fig:harm}
\end{figure*}

With the growing importance of detection for diffusion-generated images, this field has recently attracted increasing attention, leading to a surge in related research~\cite{wang2023dire,tan2024rethinking,ricker2024aeroblade,brokman2025manifold,rajan2025effectiveness}. These works aim to develop detection methods that generalize across images synthesized by different models. In real-world scenarios, identifying the specific model behind a given image is often impractical, and continuously collecting or frequently retraining detectors to accommodate new generative models is infeasible. Therefore, detection methods must exhibit strong generalization capabilities. Although generalizability has long been a crucial goal and extensively studied in the detection of GAN-generated images \cite{wang2020cnn,gragnaniello2021gan}, these previously established methods often struggle to extend to diffusion models, even when retrained on diffusion-generated images \cite{sha2023fake,cazenavette2024fakeinversion}, probably due to distinct artifacts present in these two types of models~\cite{corvi2023detection,ojha2023towards}.

In this paper, given the rapid increase of detection methods, we provide a comprehensive survey to help researchers effectively navigate the overall landscape of generalizable diffusion-generated image detection. While several surveys \cite{wang2024security,lin2024detecting,deng2024survey} cover a broad range of Deepfake detection topics, including multiple modalities of AI-generated content and different types of Deepfake generation methods, they only briefly touch on generalizable diffusion-generated image detection. Besides, these surveys only encompass limited early works, either lacking a taxonomy or providing only a coarse and incomplete classification based on  the spatial or frequency domains. Such oversimplified categorizations, along with the absence of numerous recent studies, significantly hinder researchers from grasping the latest developments and understanding key strategies for improving detection methods.

To bridge this gap, this work presents a systematic review of state-of-the-art methods. We analyze the main ideas behind existing methods and categorize them into two types based on whether their generalization ability arises from explicit hand-crafted features for generated image detection: 
(1) \textit{Data-driven detection methods}. These methods do not rely on explicit hand-crafted features to differentiate between real and generated images, but instead enhance the capability of detectors to capture implicit generalizable features through refining training strategies in a data-driven manner. We further categorize these methods into three types based on the specific training aspect they improve. 
(2) \textit{Feature-driven detection methods}. These methods analyze differences between real and diffusion-generated images in specific feature spaces. We further classify these methods into three categories based on whether the features are perceptible to humans and can be extracted from the image itself. This taxonomy provides a structured, comprehensive framework that covers a wide range of existing works, offering insights beneficial to future works in this field. Our taxonomy is illustrated in Figure \ref{fig_overall_structure}.

Beyond the systemic taxonomy, we also identify several open challenges and discuss future directions to inspire further advancements on this topic: (1) Robustness to post-processing. Post-processing operations, such as compression, resizing, are very common in digital image processing. They introduce perturbations that can weaken generalizable features used for detection. (2) Stronger theoretical foundations. Most of existing methods depend on empirical observations or heuristics, without providing a clear theoretical understanding of their underlying principle. This raises concerns regarding their generalizability across diverse generative models. Consequently, Building theoretical foundation for this field is important and promising. (3) High-quality and diverse datasets. The conventional datasets employed in this field present specific limitations, particularly in terms of image quality and dataset biases, challenging the training and accuracy assessment of existing methods. Therefore, the development of more diverse and high-quality datasets is of paramount importance. (4) Alternative paradigm for generalizable detection. Existing methods typically utilize a single detection model for generalizable detection across diverse architectures, which is highly challenging. There lies promise in the exploration of alternative paradigms, e.g, developing specialized models tailored to specific architectures and fusing multi-model's capabilities.

This survey is organized as follows. Section \ref{sec:preliminaries} provides the background on diffusion models and reviews representative methods for detecting GAN-generated images. Section \ref{sec:definition} formally defines the problem of detecting generated images by distinguishing between the distribution of real images and that learned by generative models. Section \ref{sec:data-driven} and \ref{sec:feature-driven} summarize existing data-driven and feature-based detection methods, respectively. Finally, we discuss open problems and potential future research directions in \ref{sec:future}.

\input{structure}

\section{Preliminaries}
\label{sec:preliminaries}
In this section, we first introduce some core concepts of diffusion models in Section \ref{subsec:preliminaries_diffusion}, and then introduce GAN-generated image detection methods and discuss why these methods struggle to extend to diffusion-generated images in Section \ref{sec:detectGAN}.

\subsection{Diffusion Models}
\label{subsec:preliminaries_diffusion}

Diffusion-based generative modeling defines a Markov chain that gradually adds Gaussian noise to data in $T$ steps, which is termed as the forward diffusion process~\cite{sohl2015deep,ho2020denoising}. Given an image sampled from the real image distribution $\mathbf{x}_0\sim q(\mathbf{x})$, the Markov transition kernel is defined as
$q(\mathbf{x}_t|\mathbf{x}_{t-1})=\mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1},\beta_t\mathbf{I})$, where $\mathbf{x}_t$ denotes the noisy image at the $t$-th step and $\{\beta_t\in (0,1)\}_{t=1}^T$ is a predefined schedule~\cite{ho2020denoising}. We can also sample $\mathbf{x}_t$ at any arbitrary step $t$ from $\mathbf{x}_0$ using
\begin{equation}
    q(\mathbf{x}_t|\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha}_t}\mathbf{x}_0,(1-\bar{\alpha}_t)\mathbf{I}),
    \label{marginal}
\end{equation}
where $\alpha_t=1-\beta_t$ and $\bar{\alpha}_t=\prod_{i=1}^t\alpha_i$. We usually opt for a large value of $T$, \textit{e.g.}, 1000, to approximate the isotropic Gaussian distribution $\mathcal{N}(\mathbf{0},\mathbf{I})$ with $q(\mathbf{x}_T)$. Images are synthesized from the noise $\mathbf{x}_T\sim \mathcal{N}(\mathbf{0},\mathbf{I})$ by reversing the forward process, which is termed as the reverse generative process.
The reversal transition kernel is tractable if conditioned on $\mathbf{x}_0$, \textit{i.e.}, $q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t-1};\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,\mathbf{x}_0),\tilde{\beta}_t\mathbf{I})$, where $\tilde{\boldsymbol{\mu}}_t$ has a closed-form expression and $\tilde{\beta}_t$ depends solely on the $\beta_t$. This kernel is approximated by $p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)=\mathcal{N}(\mathbf{x}_{t-1};\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,\frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t))),\tilde{\beta}_t\mathbf{I})$, where the noise-prediction model $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)$ estimates the noise $\boldsymbol{\epsilon}=\frac{\mathbf{x}_t-\sqrt{\bar{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}}$ added in $\mathbf{x}_t$ via \eqref{marginal}, and $\mathbf{x}_0$ is estimated given $\mathbf{x}_t$:
\begin{equation}
    \mathbf{x}_0=\frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)).
    \label{predict_x0}
\end{equation}

Besides, a family of sampling processes $q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}(\boldsymbol{\mu}(\mathbf{x}_t,\mathbf{x}_0),\sigma_t^2\mathbf{I})$ exists, sharing the same marginal distribution $q(\mathbf{x}_t|\mathbf{x}_0)$ as the reverse process above, where
\begin{equation}
\begin{aligned}
    \boldsymbol{\mu}(\mathbf{x}_t,\mathbf{x}_0)=\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0
    +\sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2}\cdot\frac{\mathbf{x}_t-\sqrt{\bar{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}}.
    \label{DDIM_reverse}
\end{aligned}
\end{equation}
This process reduces to the preceding reverse process if $\sigma_t=\sqrt{(1-\bar{\alpha}_{t-1})/(1-\bar{\alpha}_t)}\sqrt{1-\bar{\alpha}_t/\bar{\alpha}_{t-1}}$. If $\sigma_t=0$, this process becomes deterministic and we can reduce the number of sampling steps only at a minor cost in sample quality~\cite{song2021denoising}. The noise-prediction model remains applicable for predicting $\mathbf{x}_0$ in this process via Eq. \eqref{predict_x0}. 
The deterministic sampling is named as DDIM~\cite{song2021denoising}, and 
we can derive the DDIM inversion from $\mathbf{x}_0$ to $\mathbf{x}_T$:
\begin{equation}
    \frac{\mathbf{x}_{t+1}}{\sqrt{\bar{\alpha}_{t+1}}}=\frac{\mathbf{x}_t}{\sqrt{\bar{\alpha}_t}}+\left(\sqrt{\frac{1-\bar{\alpha}_{t+1}}{\bar{\alpha}_{t+1}}}-\sqrt{\frac{1-\bar{\alpha}_t}{\bar{\alpha}_t}}\right)\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t).
    \label{DDIM_inversion}
\end{equation}

The framework of diffusion models was later generalized to continuous-time differential equations, and various numerical solvers were employed to achieve sample synthesis~\cite{song2021sde,chen2024trajectory}. We can also train a conditional noise-prediction model $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t,\mathbf{c})$ with the signal $\mathbf{c}$ to achieve conditional sample synthesis.
A practical challenge for training diffusion models in a high-dimensional pixel space and sampling from them is the huge computational cost, which motivates the use of latent diffusion models (LDMs)~\cite{rombach2022high}.
Specifically, given an image $\mathbf{x}_0$, an antoencoder $\mathcal{E}$ is used to encode $\mathbf{x}_0$ into a low-dimensional latent representation $\mathbf{z}_0=\mathcal{E}(\mathbf{x}_0)$. The forward process of diffusion models is performed in the latent space, evolving from $\mathbf{z}_0$ to $\mathbf{z}_T$. For sampling, the reverse process begins with $\tilde{\mathbf{z}}_T\sim\mathcal{N}(\mathbf{0},\mathbf{I})$ to obtain a latent representation $\tilde{\mathbf{z}}_0$, which is then decoded into an image $\tilde{\mathbf{x}}_0=\mathcal{D}(\tilde{\mathbf{z}}_0)$.

Another important concept is reconstruction, which generally refers to the process of adding noise to an input image $\mathbf{x}_0$ to obtain its latent representation $\mathbf{x}_T$ and then performing a sampling process from $\mathbf{x}_T$ to generate the reconstructed output $\mathbf{x}_0'$. This process can be implemented by solving the continuous-time ordinary differential equations (ODEs) of diffusion models using any solvers. When employing DDIM and DDIM inversion for the reconstruction, we refer to it as \textit{DDIM reconstruction}. For LDMs~\cite{rombach2022high}, we need to obtain the low-dimensional latent representation $\mathbf{z}_0=\mathcal{E}(\mathbf{x}_0)$ of the input image, then execute the aforementioned reconstruction process to obtain $\mathbf{z}_0'$, and finally convert it back to pixel space via $\mathbf{x}_0'=\mathcal{D}(\mathbf{z}_0')$. Some works utilize only the autoencoder for reconstruction, \textit{i.e.}, $\mathbf{x}_0'=\mathcal{D}(\mathcal{E}(\mathbf{x}_0))$.

\subsection{Detecting Images Generated by GANs}
\label{sec:detectGAN}
%zhang2019detecting

Before the emergence of diffusion models, GAN-generated images were widely used in practical applications, accompanied by various detection methods. Some works revealed that the essential up-sampling operators in GANs cause distortions in the high-frequency domain of the generated images, which can be leveraged to train detectors~\cite{frank2020leveraging,durall2020watch,tan2024frequency}. Additionally, GANs leave specific patterns in the noise residuals or re-synthesis errors of generated images~\cite{marra2019gans,he2021beyond}, which can be utilized for forgery detection.

However, diffusion models present new challenges due to their fundamental structural differences from GANs~\cite{song2021sde}. Diffusion-generated images exhibit greater realism, with fewer and different artifacts compared to those found by GAN-generated detection methods \cite{wang2020cnn,durall2020watch}. Therefore, many existing GAN-based detectors struggle to distinguish real from diffusion-generated images. Even when retrained on diffusion-generated images, these detectors often fail to generalize effectively across different diffusion models, as their feature space and discriminative capability may not align with diffusion-specific artifacts~\cite{corvi2023detection}. This underscores the need for more generalizable and effective detection methods for diffusion-generated images.

\section{Problem Definition}
\label{sec:definition}
We categorize the generation of Deepfake images utilizing generative models into two types: (1) editing a portion of a real image, referred to as \textit{edited images}, and (2) synthesizing an entire image in a single sampling process, referred to as \textit{fully generated images}. Some detection methods are specialized for edited images and do not rely on the intrinsic characteristics of real and generated images, instead leveraging inconsistencies between edited and unedited regions~\cite{pei2024deepfake}. However, more works focus on distinguishing between the distribution learned by generative models and the distribution of real images. This approach applies not only to the detection of fully generated images but to edited images when combined with localization methods. Since localization is not directly related to generative models, it is not covered in this survey.

We denote ``natural distribution'' as the distribution of optical projections of real-world scenes onto a two dimensional plane, without any content processed by AI models. This distribution is denoted as $q(\mathbf{x})$. In contrast, we denote the image distribution learned by a generative model with the parameter $\theta_{i}$ as $p(\mathbf{x};\theta_{i})$, which varies depending on generative model architecture and its specific parameterization.
In real-world scenarios, an image is not necessarily a direct sample $\mathbf{x}_0$ drawn from $q(\mathbf{x})$ or $p(\mathbf{x};\theta_{i})$; it may undergo a sequence of post-processing operations:
\begin{equation}
    H(\mathbf{x}_0)=h_n(h_{n-1}(...h_1(\mathbf{x}_0))),
\end{equation}
where $H$, $h_i$ and $n$ denotes the full post-processing sequence, the $i$-th post-processing operation, and the number of post-processing steps, respectively. If no post-processing occurs ($n=0$), $H$ is simply an identity function. 

The post-processing operations considered must (1) be applied to the entire image, (2) preserve all semantic content, and (3) not alter the fundamental distinction between $q(\mathbf{x})$ and $p(\mathbf{x};\theta_{i})$. Such operations include color manipulation, blurring/sharpening, resizing, compression, and similar transformations. Notably, we consider camera capture to be a special form of post-processing, as it encompasses multiple post-processing operations, including extra color correction mechanisms to compensate for sensor limitations. Post-processing operations introduce image perturbations that can disrupt features leveraged by detection methods, making it more challenging to distinguish real from generated images.

Given this, we define the detection problem as follows. Given an image $H(\mathbf{x})$, which \textit{is known to be either real or fully generated}, but without any knowledge of the specific generative model $i$, our goal is to train a model $f_{\phi}(\cdot)$ that takes only $H(\mathbf{x})$ as input and determines whether it originates from the nature distribution or generative model:
\begin{equation}
    f_{\phi}(H(\mathbf{x}))=
    \begin{cases}
        0, & \mathrm{if}~\mathbf{x} \sim q(\mathbf{x}) \\
        1, & \mathrm{if}~\mathbf{x} \sim p(\mathbf{x},\theta_i)
    \end{cases}.
\end{equation}

\section{Data-driven Detection}
\label{sec:data-driven}

Data-driven detection methods do not rely on specifically hand-crafted features to distinguish diffusion-generated images from real ones~\cite{ojha2023towards,liu2024forgery,chen2024drct,cozzolino2024zero,rajan2025effectiveness}. Instead, they extract implicit generalizable features through detectors, and refine training strategies in a data-driven manner to enhance the capability of the detectors to capture these features. In this section, we classify existing data-driven detection methods into three categories: (1) advanced model architectures, (2) reduced dataset bias, and (3) improved training objectives beyond traditional binary classification. These categories are not mutually exclusive, and a single detection method may incorporate multiple types of improvements.

\subsection{Advanced Model Architectures}
Following the standard object classification paradigm, an AI-generated image detector extracts relevant features and make decisions based on them. To achieve better performance, a natural way is to employ more powerful architectures for feature extraction, such as Vision Transformer (ViT) \cite{dosovitskiy2020image}. ViT converts the input image into patch embeddings and then extracts features through multiple cascaded transformer blocks, each comprising a multi-head self-attention block and a Multi-Layer Perception (MLP) block. To adapt ViT for AI-generated image detection, we can fine-tune a pretrained model using ViT as the backbone network, such as CLIP-ViT \cite{radford2021learning}.

Some methods freeze the pretrained CLIP-ViT parameters and adapts the CLIP-ViT outputs for AI-generated image detection.
Although the feature space of a pretrained CLIP-ViT is not inherently aligned with this task, \citeauthor{ojha2023towards} \shortcite{ojha2023towards} argues that leveraging non-specialized features for distinguishing real from generated images improves model generalization, since it alleviates the risk of overfitting to forgery clues unique to generated images. Thus, an extra trainable linear layer attached after the final transformer block of CLIP-ViT is sufficient for binary classification~\cite{ojha2023towards}. Besides, the last block typically captures high-level semantic information, whereas most artifacts in generated images manifest as low-level features. To address this, the outputs from shallow transformer blocks can also be integrated, with an importance estimator trained to adjust their impact to the final decision~\cite{koutlis2024leveraging}.

Other methods modify the model structure and fine-tune CLIP-ViT parameters to acquire a feature space that effectively captures generalizable artifacts left by generative models. \citeauthor{liu2024forgery} \shortcite{liu2024forgery} introduces a forgery-aware adapter between several adjacent transformer blocks to incorporate the forgery traces from both pixel and frequency domains into extracted features. \citeauthor{liu2024mixture} \shortcite{liu2024mixture} adopts a mixture-of-experts framework to fine-tune parameters of the MLP blocks, using a combination of a shared low-rank adaptation (LoRA) and multiple specialized LoRAs. Additionally, a trained router determines which specialized LoRA to be utilized for each image alongside the shared LoRA.

\subsection{Reduced Dataset Bias}
In the binary classification paradigm, detectors can distinguish real from generated images based on various distinctions in the training set. These distinctions often include not only intrinsic characteristics of generated images but also unintended dataset bias, such as content, image style~\cite{yu2024semgir,rajan2025effectiveness}. As diffusion-generated images become increasingly realistic, identifying discriminative features for generated image detection is getting more challenging. This may cause detectors to be easily misled by dataset bias. Reducing it encourages detectors to focus on intrinsic differences between real and generated images, thereby improving the model generalization.

One approach to mitigate dataset bias is disrupting the irrelevant information, such as applying random masks to images \cite{doloriel2024frequency}. However, since generated images are easily obtainable and masks may obscure important forgery clues, dataset augmentation is a more effective strategy to minimize distinctions caused by known biases~\cite{chen2024drct,rajan2025effectiveness,yu2024semgir}.

Augmented generated images can be incorporated into the training set before model optimization. DRCT~\cite{chen2024drct} reduces content bias by reconstructing all images in the training set with Stable Diffusion~\cite{rombach2022high} and text prompt guidance. \citeauthor{rajan2025effectiveness} \shortcite{rajan2025effectiveness} reconstructs real images solely with the LDM autoencoder. Since the latent space preserves essential semantics in reconstruction results, such as content, overall structure and color tone, this method effectively reduces the bias in semantic content.

Augmented generated images can also be utilized during both training and inference. SemGIR \cite{yu2024semgir} generates a counterpart image with the idential content for each input image and then concatenates their features extracted by CLIP-ViT. Training on these features enables the classifier to compare corresponding representations and focus on information beyond the content, improving detection robustness.

\subsection{Improved Training Objectives}
The failure of detectors to extract generalizable features may stem from inherent limitations of the binary classification paradigm. Since the detector only needs to find the simplest classification criterion to distinguish real from generated images within the training set, it is not encouraged to explore deeper, intrinsic features of each category~\cite{ojha2023towards}. While binary classification remains the ultimate goal, alternative training objectives can help exploit additional discriminative features \cite{chen2024drct,cozzolino2024zero}.

Some studies \cite{khan2024clipping,liu2024forgery} utilize text-image alignment as a metric for detecting ai-generated images. Specifically, each category is represented by a text prompt and an image is classified based on the highest cosine similarity between its feature representation and the text embeddings. A straightforward adaptation to CLIP-ViT involves training specialized text embeddings to represent real and generated images~\cite{khan2024clipping}. However, these embeddings may not align well with features that are discriminative for detection. To address this, \citeauthor{liu2024forgery} \shortcite{liu2024forgery} proposes training a patch-based enhancer to generate a context-specific token set for each image, and develops an extra text-guided interactor that allows text embeddings to influence the image features bidirectionally. Beyond text-based approaches, DRCT \cite{chen2024drct} incorporates contrastive learning to enhance feature robustness. For each image pair, features of images with the same label (real or generated) are pulled closer together, while those with different labels are pushed further apart. This ensures that both real and generated images possess common properties in the feature space, making these features more likely to be generalizable.

GenDet \cite{zhu2023gendet} reframes AI-generated image detection as an anomaly detection problem and introduces an adversarial teacher-student framework. The training objective minimizes the discrepancy between the teacher and the student outputs for real images, while maximizing it for generated images. To further improve generalization, a feature augmenter is applied to generated images during training to minimize output discrepancies. The final decision is based on the differences between the teacher and the student outputs. Besides, \citeauthor{cozzolino2024zero} \shortcite{cozzolino2024zero} proposes a probabilistic method that predicts the probability density of pixel values under the real image distribution. Given a down-sampled real image, a model is trained to estimate pixel values in the original resolution. The probability density is then used to determine the likelihood of an image belonging to the real image distribution, which is finally used for the final decision.

\section{Feature-driven Detection}
\label{sec:feature-driven}
Feature-driven methods analyze differences between real and diffusion-generated images in specific feature spaces and train detectors based on these observations \cite{sarkar2024shadows,tan2024rethinking,wang2023dire}. We classify existing methods into three categories based on whether the features are perceptible to humans and can be extracted from the image itself: (1) perceptible image features, (2) imperceptible image features, and (3) features beyond images.

\subsection{Perceptible Image Features}
Some forgery clues in generated images are directly observable by humans, such as projective geometry inconsistencies~\cite{sarkar2024shadows} and text-image mismatches~\cite{sha2023fake}, which can be utilized for training detectors.

Most current generative models do not explicitly incorporate projective geometric principles during training. Exploiting this limitation, \citeauthor{sarkar2024shadows} \shortcite{sarkar2024shadows} assesses geometric adherences in generated images from three aspects: (1) object-shadow relationship, (2) perspective field consistency, (3) structural lines and vanishing points, and train three separate detectors for each of them.

Content-level forgery clues extend beyond factual errors to inconsistencies between an image and its text description. DE-FAKE \cite{sha2023fake} reveals that the widely used text-to-image generation tends to generate images strictly adhering to user-provided prompts, whereas real images carry richer details beyond textual descriptions. Inspired by this observation, DE-FAKE utilizes the description attached to the image, which is quite common for images found on the Internet, or employs BLIP \cite{li2022blip} to generate textual descriptions of input images, and trains a detector using concatenated image features and its corresponding text embeddings. 

\subsection{Imperceptible Image Features}
Discrepancies between real and generated images are often more noticeable in feature spaces that are imperceptible to humans, such as the frequency domain \cite{zhang2024leveraging}, local correlations \cite{tan2024rethinking} and noise patterns \cite{chen2024single}. These discrepancies can be identified through analysis of generative model pipelines or image transformations such as Fourier analysis and filtering.

\paragraph{Frequency Domain.}
Previous works on GAN-generated image detection have identified artifacts in the frequency domain, as discussed in Section \ref{sec:detectGAN}. While diffusion-generated images also exhibit such artifacts, their characteristics differ: high-frequency components in diffusion-generated images are lower than those in real images \cite{zhang2024leveraging}, whereas GAN-generated images contain higher high-frequency components than those in real images~\cite{durall2020watch}.
To leverage these differences, 
%Inspired by the observation that the discrepancy is increasingly discriminative from low- to high-frequency bands, 
\citeauthor{zhang2024leveraging} \shortcite{zhang2024leveraging} proposes a frequency-selective function that refines the spectrum by removing low-frequency components and amplifying mid-to-high frequency components proportional to their discrepancy between real and generated images. The enhanced spectrum is then mapped back to pixel space for detector training.

\paragraph{Local correlations.}
Existing works have demonstrated that up-sampling operations, which are essential for converting low-resolution latent representations into high-resolution images, introduce frequency artifacts in generated images \cite{zhang2019detecting,durall2020watch}. These operations also affect the pixel domain \cite{tan2024rethinking}, since they create dependencies between local pixels, referred to as \textit{local correlations}, which persist through subsequent convolutional layers. % as the convolution kernel is consistent for any position.
To extract and leverage the local correlations, \citeauthor{li2024improving} \shortcite{li2024improving} forces the detector to focus on local correlations by performing a patch-based random masking on the image. \citeauthor{tan2024rethinking} \shortcite{tan2024rethinking} proposes an artifact representation method called NPR. An image is divided into $l\times l$ patches, denoted as $v=\{w_1,...,w_i,...,w_n\},n=l\times l$. The NPR is derived by subtracting any element $w_j$ in the whole patch $v$, which is denoted as $\hat{v}=\{w_1-w_j,...,w_i-w_j,...,w_n-w_j\}$. The detector is then trained on the set of all patches. 


\paragraph{High-frequency noise.}
Prior works also have observed that GAN-generated images contain unique high-frequency noise patterns detectable via high-pass filtering, as discussed in Section \ref{sec:detectGAN}, and these patterns vary across different GANs~\cite{marra2019gans}. Similarly, training diffusion model detectors solely on the noise patterns results in poor generalization ability~\cite{sinitsa2024deep}.
Recent approaches~\cite{yan2024sanity,zhong2023rich,chen2024single} instead focus on the relationship between high-frequency noise and texture richness of image patches, measured by pixel fluctuation~\cite{zhong2023rich} or high-frequency components. Patchcraft \cite{zhong2023rich} exploits the observation that noise discrepancies between rich and poor texture region are more significant in generated images. It thus divides images into rich- and poor-texture patches, extracts noise features from both, and trains the detector on the feature residuals. Similarly, \citeauthor{chen2024single} \shortcite{chen2024single} argues that when generating regions with the simplest textures, generative models tend to produce an area with similar colors, thereby neglecting noise. Consequently, their method trains the detector on noise extracted from the lowest-texture patches, where missing noise signals serve as a forgery clue.

\subsection{Features Beyond Images}
Apart from perceptible and imperceptible features in the images themselves, there are also some discriminative features for diffusion-generated image detection that can only be identified when incorporating additional information. 

One widely used approach leverages the image distribution learned by diffusion models \cite{wang2023dire,brokman2025manifold}. Generated images typically cluster near the local maxima of the learned distribution and exhibit higher likelihoods compared to real images. Although these methods have demonstrated a certain degree of generalization ability in practice, the theoretical guarantee behind remains unclear, as images generated from different diffusion models may not conform to the same learned distribution~\cite{brokman2025manifold}. \citeauthor{cazenavette2024fakeinversion} \shortcite{cazenavette2024fakeinversion} estimates image likelihood under a given diffusion model by utilizing the decoding result of noise, \textit{i.e.}, $\mathcal{D}(\mathbf{z}_T)$ along with $\mathbf{x}_0$ and its reconstructed counterpart $\mathbf{x}_0'$ from the LDM reconstruction process. The authors demonstrate that these three inputs suffice to estimate the likelihood of $\mathbf{x}_0$. Similarly, \citeauthor{brokman2025manifold} \shortcite{brokman2025manifold} introduces a method to assess whether an image lies near a local maximum of the learned distribution by analyzing the difference between the curvature and gradient of the score function~\cite{song2021sde} learned by diffusion models in a small local neighborhood of the input image.

Another line of research utilizes reconstruction error, the difference between an input image $\mathbf{x}_0$ and its reconstruction version $\mathbf{x}_0'$, to determine whether an image belongs to the learned distribution. They are motivated by the observation that generated images are reconstructed more accurately than real images, as both the original and reconstructed generated images align with the learned distribution, whereas real images do not~\cite{wang2023dire}. For example, DIRE \cite{wang2023dire} applies reconstruction within the DDIM reconstruction framework, and trains a classifier on the reconstruction error $|\mathbf{x}_0-\mathbf{x}_0'|$. AEROBLADE \cite{ricker2024aeroblade} focuses on the detection of LDM-generated images, and reconstructs input images by the autoencoder used in LDMs to assess whether the image belongs to the distribution learned by autoencoders. As a training-free method, AEROBLADE directly uses a distance metric (e.g., LPIPS \cite{zhang2018unreasonable}) to measure the reconstruction error for threshold-based classification.
Besides, ZeroFake \cite{sha2024zerofake} introduces an approach based on text-image inconsistency, where the reconstruction process is guided by a modified prompt that is generated via BLIP~\cite{li2022blip} by replacing the first noun in the image description with another noun from a predefined list. This discrepancy increases reconstruction errors in real images more than in diffusion-generated images. ZeroFake also utilizes a distance metric to measure the reconstruction error.
\citeauthor{luo2024lare} \shortcite{luo2024lare} adopts a different approach by amplifying extracted features in regions with significant reconstruction errors. This is achieved by adopting a multi-head attention module, where the reconstruction error modulates the attention score. To accelerate computation, it further estimates reconstruction error using a one-step noise addition and one-step denoising, instead of a full-step reverse process.

\section{Future Directions}
\label{sec:future}
Despite significant progress in diffusion-generated image detection, several vital challenges still need to be addressed. 

\paragraph{Robustness to post-processing.}
As discussed in Section~\ref{sec:definition}, post-processing operations introduce perturbations to generated images that can obscure generalizable features for detection. Given their widespread use and ease of implementation, real-world detectors must be robust and reliable against these operations. A common strategy to improve the robustness is data augmentation, which simulates post-processing operations during training. However, some current methods still experience performance degradation under post-processing~\cite{chen2024drct,zhang2024leveraging}. Exploring alternative solutions remains an open question. One promising attempt is autoencoder-based reconstruction, where real images are reconstructed before training, using the LDM autoencoder without changing resolutions. This method enhances robustness of detectors against resizing artifacts by ensuring the real images and their reconstructions exhibit similar scaling artifacts~\cite{rajan2025effectiveness}. 

\paragraph{Stronger theoretical foundations.}
The field still lacks rigorous theoretical research on the intrinsic differences between real and generated images. Many existing methods rely on empirical observations~\cite{brokman2025manifold,wang2023dire} or extracted discriminative features without a clear understanding of their underlying principles \cite{liu2024forgery,ojha2023towards}. This raises concerns about their generalizability across different generative models.
A recent study reveals that some existing methods, despite strong performance in commonly used experimental settings, still suffer significant accuracy drops on the latest generative models~\cite{imanpour2024visual}. Strengthening the theoretical foundations of these methods by systematically analyzing intrinsic distinctions between real and generated images could enhance their generalization and facilitate the development of more robust detection methods.

\paragraph{High-quality and diverse datasets.}
Many studies in diffusion-generated image detection train and evaluate detectors on several popular datasets, such as GenImage \cite{zhu2024genimage}, DiffusionForensics \cite{wang2023dire} and others~\cite{ojha2023towards}. However, these datasets exhibit two key limitations. (1) \textit{Limited image quality}. Real images often originate from datasets designed for other domains, which restricts their diversity and complexity and may have undergone heavy post-processing. Besides, some generated images lack sufficient realism, affecting the effectiveness of datasets in training and evaluation. As shown in~\cite{zhang2024leveraging}, the same detector tested on images from the same generative model can yield significantly varying accuracies across different datasets. (2) \textit{Dataset biases}. Many datasets contain biases related to JPEG compression and resolution~\cite{grommelt2024fake}. Detectors trained on biased datasets may perform well in controlled benchmarks, but fail in other benchmarks or real-world scenarios. For example, DIRE \cite{wang2023dire} was initially reported to be highly robust, but later studies found its performance degraded significantly due to JPEG compression bias \cite{ricker2024aeroblade}.

To develop more effective datasets for diffusion-generated image detection, we can explore the following aspects: (1) increasing the semantical diversity of real images while ensuring they have not undergone extensive post-processing, (2) verifying the fidelity of generated images, (3) incorporating outputs from state-of-the-art generative models, and (4) mitigating common dataset biases.


\paragraph{Alternative paradigm for generalizable detection.}
Except for a few works focused specifically on LDM-generated image detection~\cite{ricker2024aeroblade,rajan2025effectiveness} or diffusion-generated image detection~\cite{sha2024zerofake,cazenavette2024fakeinversion}, most current works aims to develop methods trained on images generated by one model while generalizing well across not only all diffusion models but also GAN-generated images~\cite{ojha2023towards,tan2024rethinking,brokman2025manifold}. Despite notable progress, 
achieving full generalization across all generative models using a single method remains an open challenge. A more pragmatic approach could involve a hybrid framework: (1) categorizing existing generative models into major groups based on architectural similarities, (2) developing specialized detection methods tailored to each category, ensuring strong intra-class generalization, and (3) finally integrating multiple detection methods within a mixture-of-experts framework for robust real-world performance. This strategy balances the need for a generalizable detector with practical generalization capability constraints of a single method by prioritizing generalization across variants of existing models~\cite{abdullah2024analysis} rather than across entirely different model families, as the emergence of new families of generative models is very slow, thus reducing the need for frequent retraining while maintaining adaptability to new generative models.

\section{Conclusion}
With the emergence of powerful diffusion models, security concerns stemming from generated images have become increasingly significant. This comprehensive survey presents a systematic review of generalizable diffusion-generated image detection methods, categorizing existing approaches into data-driven and feature-driven detection methods based on the explicit incorporation of hand-crafted features. Moreover, fine-grained principles are utilized to further classify existing methods into six fine-grained categories. Given the nascent nature of this field, we also identify several open problems and research directions that merit further investigation. We anticipate that this survey will be beneficial for researchers and practitioners interested in generative image detection and will inspire additional research in this field.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}