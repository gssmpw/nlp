\begin{figure*}[t]
\vspace{-10pt}
\includegraphics{fig_igarashi_v3.pdf}
\caption{FastAtlas box packing: Given a set of boxes (a), we rotate them so that they are at least as tall as they are wide (b), and then hash sort them in descending size while maintaining a deterministic order (c). We then find a scale factor $S$ such that the boxes, scaled and integerized, will fit in the atlas (d). We do this by testing candidate scales in parallel. For each scale we {\em fold} boxes into rows that fit the atlas width. Boxes are placed alternately in one row from left to right, and two rows from right to left, to avoid congestion on the sides of the atlas. We scale all boxes to avoid horizontal overflow. We {\em push} boxes upwards in the atlas to minimize wasted space. The largest scale that does not overflow the atlas is selected (e). All steps are executed on the GPU, and evaluated in parallel.}
\vspace{-10pt}
\label{fig:igarashi_packing}
\end{figure*}

\section{FastAtlas Packing}
\label{sec:pack}
\label{sec:atlasing}
\label{sec:packing}

Given a set of charts with associated bounding boxes, FastAtlas packs these bounding boxes into texture atlases of fixed user-specified dimensions $\omega \times \omega$ texels. We assume that chart bounding boxes have target dimensions in texel space that are specified relative to $\omega$. For TSS applications, these are typically the chart's pixel-space dimensions in the frame buffer, which captures that each chart should ideally have a 1:1 texel-to-pixel ratio.

Our packing must address two inter-related sub-problems: a scale factor such that, after scaling the boxes by this factor and rounding their size to integer coordinates, they can indeed be packed into our atlas; and computing the actual packing. We seek atlases that are packed as efficiently as possible, minimizing memory waste. A key additional requirement for TSS applications is that atlases for identical charts must be strictly identical; this avoids subtexel jitter across frames if the scene and camera remain stationary. This requirement means that we must use purely deterministic algorithms when computing our atlases, including consistent handling of identically sized boxes.
 
We address this challenging problem using a two-step process inspired by traditional packing methods \cite{levy2002least,igarashi2001adaptive}. These methods start by ordering charts by size using traditional sorting methods, then place charts one at a time, choosing the optimal location for each chart based on the locations of the previously placed charts. This ordering is motivated by the observation that larger charts get harder and harder to place as the atlas fills up, and thus should be packed first. The chart placement strategies used by these methods are too slow for real-time computation and cannot be directly parallelized across multiple threads, as placing one chart requires awareness of the placement of all prior charts. We adopt the general two-step framework these methods use (sorting charts first, then placing them leveraging the sorted order), but execute both steps using parallizable variants of the respective algorithms.

We first order our boxes in a deterministic, monotonically decreasing size order (Sec.~\ref{sec:order}). We then leverage GPU parallelism to generate atlases with different scaling factors, and pick the atlas and factor that provides the best packing efficiency as the solution (Sec~\ref{sec:scale}). To generate the actual atlases we introduce a parallel framework inspired by the packing method of \cite{igarashi2001adaptive} (Sec~\ref{sec:actual_packing}).

\subsection{Box Ordering via Cuckoo Hashing.}
\label{sec:order}
We orient all per-chart boxes so that they are at least as tall as they are wide (rotating them by 90 degrees if necessary), and then order them by height. To ensure deterministic results across multiple identical frames, we use height as the primary ordering criterion, and the lowest triangle index in each box's corresponding chart as the secondary ordering criterion (Fig.~\ref{fig:igarashi_packing}bc).

Sorting on the GPU can be efficiently implemented by a prefix sum-based radix sort (e.g. \cite{merrill2011high}). However, ordering our boxes lexicographically requires 36-bit keys: 13 bits for the box height (assuming box height is integer and the height of a box is at most equal to the maximal atlas height, here $\max_H=8192$), and 23 bits for the per-box root triangle index (assuming a maximum triangle index of $~8M = 2^{23}$). Using a GPU radix sort, which can sort 4 bits per pass, ordering such keys would require 9 radix sort passes, which is prohibitively expensive even with hand-optimized compute shaders. We avoid explicit lexicographic sorting by leveraging two observations. First, as we have a fixed number of box heights (1 to $\max_H$), we can simply bucket charts by height into $\max_H$ buckets, rather than sort them. Second, when bucketing charts, it is not necessary for boxes of the same height to be strictly ordered by triangle ID, so long as the ordering of boxes inside a bucket is identical across identical frames.

We place boxes into one of $\max_H$ possible buckets by employing a {\em cuckoo hashing} scheme \cite{pagh2004cuckoo}. At the start of program execution, we allocate a scratch buffer on the GPU which contains enough memory for four times the maximum number of charts on screen. Each frame we compute, for each bucket, the total number of boxes it will contain; this can be done efficiently by using GPU atomic operations. Boxes are then assigned to a bucket, based on their screen-space integer height. Each bucket is allocated enough scratch buffer memory for four times its number of boxes; this prevents hash contention and unnecessary evictions during the cuckoo hashing. Using the lowest triangle ID of each chart, we attempt to place each box in its bucket using GPU atomic compare-and-swap operations, initially at position $(4 \cdot \operatorname{chart\ ID})\ \mathrm{mod}\operatorname{bucket\ size}$ in the bucket. If a box is already placed in a specific bucket location, it is evicted; the evicting thread in the compute shader is then responsible for placing the evicted box in an empty location in the same bucket. A final compute shader removes empty buckets and empty space in buckets. This approach reduces overall runtime by one order of magnitude versus a radix-based sort, while still being deterministic. 

\subsection{Parallel Atlas and Scale Factor Computation.}
\label{sec:scale}
Packing boxes into a fixed size atlas requires us to find a suitable scale factor $S$ such that the boxes can indeed be packed once their dimensions are scaled by this factor and rounded upward to the closest integer values. While one can easily pick a conservative scale factor for which packing is guaranteed, minimizing undersampling (or oversampling) requires searching for a scale factor $S$ that is as close as possible to one.

We compute the suitable factor by leveraging GPU parallelism. We observe that on the GPU we can compute packings in parallel at multiple possible scales with the same efficiency as computing one box packing at a fixed scale. 
We consequently find the desired scale-factor through brute-force search over a range of scales, choosing the scale that maximizes packing efficiency while ensuring that all boxes fit in the atlas. We test box packing over a set of 64 scale factors (ranging from $\frac{1}{64}, \frac{2}{64}, ..., \frac{64}{64}=1$) in parallel as this provided a suitable speed versus quality trade-off.
Candidate scale factors are rejected if not all boxes fit in the fixed-size atlas, and the largest scale factor not rejected is chosen.

\subsection{Box Packing.}
\label{sec:actual_packing}
Given a candidate scale factor $S$, we scale all bounding boxes by this factor, then round box sizes upwards to integer coordinates for packing. We then attempt to pack the boxes into the $\omega \times \omega$ texture atlas. We achieve this goal by observing that while it is notoriously challenging to parallelize standard box packing problems, it {\em is} possible to solve this problem by relaxation. Specifically, we parallelize the 2D box packing problem by allowing the atlas to slightly overflow. We then decrease the scale factor $S$ to scale the packed boxes down so as to fit back in the atlas. While this is not an acceptable trade-off for most bin-packing problems (as bins are not normally allowed to shrink), it is acceptable in our problem context. Our relaxed packing method extends the approach of Igarashi and Cosgrove ~\shortcite{igarashi2001adaptive}; one advantage of this method is that much of it can be parallelized and only one data structure - the array of charts, or boxes - is operated on, reducing cache and register pressure. While it might be possible to parallelize the more popular Tetris Packing method \cite{levy2002least} on the GPU, it is not clear how to do so as Tetris Packing requires maintaining multiple sets of active horizons during packing. 

Given appropriately scaled, rotated and pre-ordered charts, the CPU-based algorithm of Igarashi and Cosgrove ~\shortcite{igarashi2001adaptive} 
proceeds using two operations: {\em folding} and {\em pushing}. During folding, charts are placed sequentially, from the tallest to shortest, starting from the top left corner of the atlas ($X=0,Y=0$) and advancing along the X axis until there is no room left for the next chart. The algorithm then forms the next row by placing charts in a reverse direction from right to left: the first chart in the second row is placed at the $Y$ value directly below the tallest chart placed and at $X= \omega - \operatorname{chartWidth}$ (flush with the right side of the atlas); the rest of the charts are placed one after the other to the left of it. This alternating row layout is repeated until all charts are placed. After the initial placement is complete, a final {\em pushing} operation moves charts upwards in the atlas as far as possible to minimize wasted space. The key idea behind this heuristic approach is that large charts, which are most likely to be difficult to pack efficiently, are placed first; the alternating row layout serves to ensure that the charts with the tallest heights in each row do not bunch on the left hand side of the atlas, increasing packing efficiency during the pushing operation. 

Our key challenge is to efficiently implement the folding step on the GPU: placing boxes in a linear fashion, one at a time, is too slow for real-time performance. Unfortunately, computing the exact location where the algorithm needs to switch rows is an inherently sequential operation: detecting when a box being placed triggers an overflow, necessitating starting a new row, requires knowing where all previous boxes were placed. 

We overcome this challenge and parallelize folding by employing a relaxation strategy which is based on prefix-sums and which provides a good approximation of folding for our purposes. We place our boxes in order, left to right, along a straight line, with the top left corner of each box snug against the right top corner of the previous one.  
Recall that the {\em prefix sum} of a set of numbers $w_1,...,w_n$ is $w_1,(w_1+w_2),...,\sum_{i=1}^{n}  {w_i}$. The prefix sums of the horizontal locations of the top-left corners of our laid-out boxes (represented as 32-bit integers) can be efficiently computed in parallel on the GPU \cite{harris2007parallel}. We observe that, if we restrict our texture atlas width to be a power of two (as most textures already are!), and writing $\omega=2^k$, the prefix sum yields an approximate folding for all boxes at once: the top $32-k$ bits of its corner's prefix-sum determine the row each box belongs to, and the bottom $k$ bits determine each box's horizontal position within the row (mirrored for odd rows). The only caveat is that the right-most box on each row may overflow by some amount. We therefore use prefix sums to place boxes into the atlas (Fig~\ref{fig:igarashi_packing}) expecting an overflow, which we account for by reducing the tested scale factor $S$: we compute the maximum horizontal overflow $m$ (the largest amount, in texels, that any row is over $\omega$), and multiply $S$ by the ratio $\frac{\omega}{\omega + m}$), ensuring that all boxes fit horizontally into the given size atlas. In practice we found our atlases were dominated by a few extremely large charts, and therefore alternate one left-starting row with {\em two} right-starting rows to accomodate this domain-specific knowledge.
 
After folding, we "push up" each box in a row-by-row fashion, using an advancing front approach. We start by setting the vertical offset of each box to zero. Each box in the row then writes its height to a buffer of size $\omega$, for each of the texels that it covers horizontally; this represents the current frontline. Subsequent rows then set their vertical offset to the maximum value that they cover with the current frontline, then write that value, plus their height, to any frontline texels that they cover. 
