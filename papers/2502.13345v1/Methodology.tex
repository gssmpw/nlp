\section{Methodology}
\label{sec-3.2}
% \subsection{Threat Model}
% There are two agents involved in our threat model, i.e. a model producer and a model user. The model producer provides image generation services via model distribution. The model users deploy the diffusion model to generate images. We formally summarize the threat model as follows:


% \noindent\textbf{Model producers} inject messages into the latent diffusion model to watermark the generated images. 
% % The watermark encoder injects the message in multi-bit form into the generated images while preserving image quality. 
% The goal for model producers is to ensure that watermarks are successfully injected and extracted from watermarked images.
    
% \noindent\textbf{Model users} own the fine-tuned VAE-Decoder, message encoder and U-Net. When the message embedding is evaded, the security mechanism causes degradation in the quality of generated images. The goal of untrustworthy users is to obtain images without watermark through image processing or evade the security mechanism.
    

%水印编码器根据用户对应的信息输出特定的分布，并采样获得初始隐变量。图像初始隐变量通过去噪网络U-Net后获得图像在潜在空间内的表示Z_0。VAE-Decoder进一步将隐变量Z_0转化为图片，同时初始隐变量通过Skip-connection影响VAE-Decoder的输出，当初始隐变量为随机生成时，将无法正常生成图像。带有语义水印的图像可通过VAE-Encoder转化为隐变量后通过扩散反演获得Revered Latent。水印解码器将提取出反演隐变量中的水印信息.由于初始隐变量的选取会影响到U-Net去噪过程，进一步使得图像在语义层面的差异，故本方法的水印体现在语义层面。
\subsection{Framework of DistriMark} 

In this work, we extend to achieve the security and embedding efficiency of watermarks in the model distribution scenarios. Our Distrimark embed the watermark into the latent variables of the diffusion model and enforce the mandatory embedding of the watermark whenever the model is utilized by leveraging the watermark-network controller.
To guarantee the security of watermark distribution and maintain the unpredictability and fidelity of watermark, we propose a novel watermark distribution method Secure Latent Watermark Distribution. This method establishes a unified representation of latent variables and watermark information as shown in Figure \ref{fig2}. The watermark region follows a specific distribution, from which watermarked latent variables are sampled. The variability in latent variables across different outputs increases randomness and unpredictability, which ensures the security of watermark distribution. To safeguard the security of watermark embedding, we introduce Watermark-Network Controller, a security mechanism integrated into latent diffusion model components which binds the variational autoencoder with watermarked latent variables to prevent users from evading the watermark embedding process. This module binds the VAE-Decoder with watermarked latent variables through skip connections. The image quality will significantly deteriorate when the model user escape the watermark. Distirimark utilizes a three-step progressive training strategy with the following objectives:
    %这一步骤不涉及扩散模型。消息编码器的训练目标是在标准高斯分布下构建消息和隐变量的统一表示。
% (1) Pre-training message encoder-decoder. The training objective of the message encoder is to construct a unified representation of messages and latent variables under a standard Gaussian distribution. 
%     %这一步的作用为通过跳接结构建立变分自编码器的解码器和水印隐变量的关系。当输入为带水印隐变量时，最小化VAE-Decoder输出图像与原模型输出图像的感知差异的情况，当输入为其他时，产生随机噪声的输出。
% (2) Finetuning VAE-decoder. The aim is to minimize the disparity of output images between fine-tuned VAE-Decoder and original model when input contains watermarked latent variables, and maximize it for non-watermarked inputs.
% %这一步的作用为针对图像处理进行对抗训练、修复扩散反演误差带来的影响。
% (3) Finetuning message decoder. This step aims to perform adversarial training against image processing and to mitigate the effects of diffusion inversion.
% % In this section, we will initially present the overview of our proposed DistriMark method and subsequently elaborate on the training strategy.

% \noindent\textbf{Watermarking Embedding.}
% %为保证水印水印流通的安全性，同时保证水印的随机度和保真度，我们提出了一种()的水印分发方法。
% %具体来说，我们假定有一系列确定性的函数$f(z;\theta)$由某个空间$\Phi$中的向量$\theta$参数化,当\theta固定而z\sim\mathbb{N1}(1,0)，$f(z;\theta)$可以生成服从特定分布潜在变量.具体来说，Encoder将模拟确定性函数输出均值向量和方差向量，并通过重参数化输出初始隐变量。
% To guarantee the security of watermark distribution and maintain the unpredictability and fidelity of watermark, we propose a novel watermark distribution method: Watermarked Latent Sampling from Distribution (DistriMark for short). This method establishes a unified representation of latent variables and watermark information. The watermark region follows a specific distribution, from which watermarked latent variables are sampled. The variability in latent variables across different outputs increases randomness and unpredictability, which ensures the security of watermark distribution.

% To safeguard the security of watermark embedding, we introduce Latent-VAE Skip-Binder, a security mechanism integrated into latent diffusion model components which binds the variational autoencoder with watermarked latent variables to prevent users from evading the watermark embedding process. Specially, this module binds the VAE-Decoder with watermarked latent variables through skip connections. The image quality will significantly deteriorate when the model user escape the watermark.

%具体来说，

%As figure \ref{fig2} shows, the message encoder outputs a specific distribution based on the message and samples to obtain initial latent variables. The initial latent variables of the image are denoised in the latent space $Z_0$ after denoising through the U-Net and the prompt. The VAE-Decoder further transfers latent variables $Z_0$ into images, with the initial latent variables influencing the VAE-Decoder's output via skip connections. When initial latent variables are randomly generated, the security mechanism will cause degradation in the quality of generated images. 

% \noindent \textbf{Latent-VAE Skip-Binder.} To ensure that watermark embedding is resistant to evasion and that the watermark information is robust at the model level and the image level, this module utilizes skip connections to bind the latent variables with the VAE-Decoder and finetunes the VAE-Decoder to establish security mechanism.  This approach effectively combines the robustness of semantic watermarks with the coupling features of model integration watermarks. Since the watermark information is not embedded in the model parameters, it provides robustness against adversarial attacks targeting the model. 


% \noindent\textbf{Watermarking Verification.} Watermark extraction involves diffusion inversion, an approximate process for obtaining initial hidden variables from generated images. To mitigate the effects of diffusion inversion and raise the robustness of image processing, we introduce the compensation module which employs adversarial training to raise performance of the message decoder.
% % Watermark extraction involves diffusion inversion, an approximate process for obtaining initial hidden variables from generated images. 

% % Watermarked images can be transformed into latent variables through VAE-Encoder and then inverted through diffusion inversion to obtain the reversed latent. The message decoder can extract the message from the reversed latent. Due to the selection of initial latent variables affecting the denoising process of U-Net, this method introduces semantic differences in the generated images. Therefore, the watermark in this approach manifests at the semantic level.

% %水印提取涉及扩散反演的过程，这是通过扩散模型生成的图像获得初始隐变量的算法，与扩散模型去噪过程相反，基于以下假设：扩散反演过程可公式化为：
% Diffusion inversion \cite{dhariwal2021diffusion} algorithmically retrieves the initial latent variables from images generated by a diffusion model. This algorithm is the inverse process of denoising in the diffusion model. Based on the assumption $x_{t-1} - x_t 
% \approx x_{t+1} - x_t$ , diffusion inversion of the Denoising Diffusion Implicit Model (DDIM) \cite{song2020denoising} can be formalized as follows:
% \begin{equation}
% \hat{x}_{t+1}=\sqrt{\bar{\alpha}_{t+1}} x_{0}+\sqrt{1-\bar{\alpha}_{t+1}} \epsilon_{\theta}(x_{t})
% \end{equation}
% where $\bar{\alpha}$ is the parameter of the diffusion model. $t$ denotes the denoising timestep.  $\epsilon_\theta(x_t)$ is the estimated noise for timestep $t$.
% $\hat{x_{0}}$ represents the prediction of the image at the current timestep and can be formulated as follows:
% \begin{equation}
%     \hat{x_{0}}=\frac{x_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{\theta}\left(x_{t}\right)}{\sqrt{\bar{\alpha}_{t}}}.
% \end{equation}

% \noindent\textbf{Training Strategy.}
% Distirimark utilizes a three-step progressive training strategy with the following objectives:
%     %这一步骤不涉及扩散模型。消息编码器的训练目标是在标准高斯分布下构建消息和隐变量的统一表示。
%     (1) Pre-training message encoder-decoder. The training objective of the message encoder is to construct a unified representation of messages and latent variables under a standard Gaussian distribution. 
%     %这一步的作用为通过跳接结构建立变分自编码器的解码器和水印隐变量的关系。当输入为带水印隐变量时，最小化VAE-Decoder输出图像与原模型输出图像的感知差异的情况，当输入为其他时，产生随机噪声的输出。
%     (2) Finetuning VAE-decoder. The aim is to minimize the disparity of output images between fine-tuned VAE-Decoder and original model when input contains watermarked latent variables, and maximize it for non-watermarked inputs.
% %这一步的作用为针对图像处理进行对抗训练、修复扩散反演误差带来的影响。
%     (3) Finetuning message decoder. This step aims to perform adversarial training against image processing and to mitigate the effects of diffusion inversion.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.9\columnwidth]{3.pdf} % 
% \caption{Message-encoder and message-decoder structure.}
% \label{fig3}
% \end{figure}

\subsection{Watermark-Network Controller}
%该步骤将初始隐变量和VAE-Decoder的相关组件通过Skip connection连接，微调VAE-Decoder保证Message Encoder输出的隐变量所对应的图像与原模型一致，随机隐变量对应的图像为随机噪声。
To enforce the embedding of watermarks during model usage, the watermark-network controller directs image generation by using watermarked initial latent as control signals. watermark-network controller connects the watermarked initial latent variables and relevant components of the VAE-Decoder through skip connections. Through fine-tuning the VAE-Decoder, images corresponding to the watermarked latent variables consistent with the original model, while corresponding to random latent variables are transformed to random noise. In the implementation of skip-connection, we design a network association to bind the watermarked initial variable to the intermediate layer variables.

%损失函数使用的图像间的LPIPS损失和L2距离。具体如下：
The loss function employs the LPIPS loss and L2 distance between images, denoted as $L_1$ and $L_2$, respectively. $\mathcal{L}_2(D_o(z),D_v(z))=||D_o(z)-D_v(z)||_2^2.$ $D(\cdot$) denotes the decoding process of the variational autoencoder $D_o$ and $D_v$ denote the original and the fine-tuned VAE-Decoder with skip connections respectively. When the VAE-Decoder is connected to the initial latent variable, the loss function is:
\begin{equation}
\mathcal{L}_w=L_1(D_o(z),D_v(z))+  L_2(D_o(z),D_v(z)))
\end{equation}

% \begin{equation}
% \begin{aligned}
% \label{step2}
% &\mathcal{L}_v=\theta\cdot(L_1(D_o(z),D_v(z))+  L_2(D_o(z),D_v(z)))\\&+(1-\theta) \cdot (L_2(D_o(z_{r}),D_v(z))-\lambda_v \cdot  L_2(D_o(z),D_v(z)))\\&
% \end{aligned}
% \end{equation}
%其中D(\cdot)表示变分自编码器的解码过程，D_o,D_v分别为原模型，加入Skip Connection的微调模型。\delta=1,-1表示初始隐变量是否来自message encoder.
\noindent where $Z_r$ denotes the random noise. The factor $\lambda_v$ is a constant. When the VAE-Decoder is connected to the random latent variable, the loss function is:
\begin{equation}
\mathcal{L}_u=(L_2(D_o(z_{r}),D_v(z))-\lambda_v \times  L_2(D_o(z),D_v(z)))
\end{equation}

To prevent pixels with smaller values from being excessively altered, We calculate the difference across multiple channels between the output images of the original model and the fine-tuned model as the loss:
\begin{equation}
    \mathcal{L}_i=\frac{1}{c\times h\times w}\sum_{k=1}^c\sum_{i=1}^h\sum_{j=1}^w\frac{\left|D_v(z)_{(k,i,j)}-{D_o(z)}_{(k,i,j)}\right|}{D_v(z)_{(k,i,j)}+max(D_v(z))}
\end{equation}



\noindent where $\theta$ indicates whether the initial latent variables are from the message encoder. $\varepsilon,\delta$ is the balancing weight. The overall loss for this step is as follows:
\begin{equation}
\mathcal{L}_v=\theta\times\mathcal{L}_w+\varepsilon\times(1-\theta) \times \mathcal{L}_u+\delta\times \mathcal{L}_i
\end{equation}

\begin{figure*}[t]
\centering
\includegraphics[width=1.7\columnwidth]{10.pdf} % 
\caption{Generated image comparison under the security mechanism. Images in each sample from left to right are Watermarked Initial Latent Variables (WIL for short) without VAE-based fine-tuning (fine-tuning for short), WIL with fine-tuning, non-WIL without fine-tuning, and non-WIL with fine-tuning, representatively.}
\label{fig8}
\end{figure*}

\subsection{Secure Latent-Watermark Distribution}

We assume a series of deterministic functions $f(z;\theta)$ parameterized by a vector $\theta$. When $\theta$ is fixed and $z \sim \mathcal{N}(1,0)$, $f(z; \theta)$ can generate latent variables that conform to a specific distribution. Specially, the encoder outputs the mean vector and variance vector to simulate the deterministic function $f(z;\theta)$ and generates the initial latent variables through reparameterization. 
% The structure of the message encoder-decoder is depicted in Figure \ref{fig3}. 

%在这一步骤中，编码后的消息直接输入消息解码器，用于在自监督范式下进行训练。消息损失是消息 ( m ) 与经过 sigmoid 函数处理后的 ( \delta(m') ) 之间的二元交叉熵（BCE）。
The watermarked latent variables are put into the message decoder directly to train them in the self-supervision paradigm. The message loss is the Binary Cross Entropy (BCE) between $m$ and the sigmoid $\sigma(m')$:

% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_m&=\sum_{i=0}^{B-1}\sum_{k=0}^{n-1}BCE(m_i^k,m_i^{k^{'}})\\
%     &=-\sum_{i=0}^{B-1}\sum_{k=0}^{n-1}m_i^k\cdot\log\sigma(m_i^{k^{'}})+(1-m_i^k)\cdot\log(1-\sigma(m_i^{k^{'}}))
% \end{aligned}
% \end{equation}

% \begin{equation}
% \begin{aligned}
%     &\mathcal{L}_m=\sum_{k=0}^{n-1}BCE(m_k,m_k')\\
%     &=-\sum_{k=0}^{n-1}m_k\log\sigma(m_k')+(1-m_k)\log(1-\sigma(m_k'))
% \end{aligned}
% \label{step1_BCE}
% \end{equation}

\begin{equation}
\begin{aligned}
    &\mathcal{L}_m
    =-\sum_{k=0}^{n-1}m_k\log\sigma(m_k')+(1-m_k)\log(1-\sigma(m_k'))
\end{aligned}
\label{step1_BCE}
\end{equation}



%同时，由于扩散模型的训练样本是通过不断加入噪声直到符合标准正态分布，在推理阶段，初始的隐变量应尽可能符合标准正态分布。我们以初始隐变量和标准正态分布的KL损失作为损失函数，故一阶段损失函数如下：
Since the training samples of the diffusion model are generated by progressively adding noise until they conform to a standard normal distribution, during the inference stage, the message encoder will output initial latent variables that follow the same distribution. The Kullback-Leibler (KL) divergence between the initial latent variables and the standard normal distribution is utilized as the loss function. The output follows a normal distribution, denoted as $q(z) \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and the standard normal distribution is denoted as $p(z) \sim \mathcal{N}(0, 1)$. The distribution loss is as follows:
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_d&=D_{KL}(q(z)||p(z))\\&
% =\int_xq(x)log\frac{q(x)}{p(x)}dx
%     \\&=-\frac{1}{2}(1+\log\sigma_{1}^{2}-\sigma_{1}^{2}-\mu_{1}^{2})
% \end{aligned}
% \end{equation}

\begin{equation}
\begin{aligned}
    \mathcal{L}_d=D_{KL}(q(z)||p(z))
=\int_xq(x)log\frac{q(x)}{p(x)}dx
\end{aligned}
\end{equation}

% When the average bit accuracy and KL divergence reach the preset thresholds, this step concludes.



% % Watermark extraction involves diffusion inversion, an approximate process for obtaining initial hidden variables from generated images. 



\subsection{Watermark Robustness Enhancement Module}
\noindent\textbf{Watermarking Verification.} Watermark extraction involves diffusion inversion, an approximate process for obtaining initial hidden variables from generated images. 
Diffusion inversion \cite{dhariwal2021diffusion} algorithmically retrieves the initial latent variables from images generated by a diffusion model. $x_t$ represents the image at the timestep $t$. Based on the assumption $x_{t-1} - x_t 
\approx x_{t+1} - x_t$ , diffusion inversion of the Denoising Diffusion Implicit Model (DDIM) \cite{song2020denoising}  is formalized as follows:
\begin{equation}
\hat{x}_{t+1}=\sqrt{\bar{\alpha}_{t+1}} x_{0}+\sqrt{1-\bar{\alpha}_{t+1}} \epsilon_{\theta}(x_{t})
\end{equation}
where $\bar{\alpha}$ is the parameter of the diffusion model. $t$ denotes the denoising timestep.  $\epsilon_\theta(x_t)$ is the estimated noise for timestep $t$.
$\hat{x_{0}}$ represents the prediction of the image at the current timestep and is defined as:
\begin{equation}
    \hat{x_{0}}=\frac{x_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{\theta}\left(x_{t}\right)}{\sqrt{\bar{\alpha}_{t}}}.
\end{equation}


%由于扩散反演中存在多步近似存在误差，且图像经过处理，Step3主要目的为经过对抗训练减小误差并对抗图像处理攻击。
To mitigate the effects of diffusion inversion and raise the robustness of image processing, we introduce the watermark robustness enhancement module which employs adversarial training to raise performance of the message decoder.
% Due to the presence of multi-step approximations in inverse diffusion and considering image processing effects, 
% the primary objective of this step is to mitigate errors through adversarial training and counteract image processing attacks. 
The loss function is binary cross entropy between the message $m$ and the sigmoid $\delta(m')$ which is the same as Equation \ref{step1_BCE}.

%由于在图像使用的实际场景中存在各种类型的攻击，因此在训练过程中，我们利用攻击层对带水印的图像进行处理，然后进行水印提取算法。该攻击层包含 7 种常见的攻击，即模糊、高斯噪声、亮度调整、对比度调整、饱和度调整、和JPEG。





\noindent\textbf{Attack Simulation for Adversarial Training.} Various attacks are common in practical image usage. Therefore, during the training process, we deploy an attack layer to watermarked images before employing a watermark extraction algorithm. This attack layer encompasses six common types of attacks: blur, Gaussian noise, brightness adjustment, contrast adjustment, saturation adjustment, and JPEG compression. To remain the differentiable of attack during training, we employ the differentiable simulation method to perform JPEG attack \cite{zhu2018hidden}.

