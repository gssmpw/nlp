
\input{table_3}

\input{table_1}

\section{Experiments}
%\subsection{Experiment Settings}

\noindent\textbf{Implementation details.} In this paper, we focus on text-to-image generation, hence we utilized Stable Diffusion-v2 \cite{rombach2022high}. The number of inference steps is 25 for both generation and detection process. Following the settings of existing works \cite{wen2024tree}, we employ
the prompt from StableDiffsionDB \cite{wang2022diffusiondb} with guidance
scale of 5 during inference and an empty prompt during DDIM inversion. 
We utilize AdamW with a learning rate of $5 \times 10^{-4}$ and weight decay of 0.01 during finetuning. All experiments are conducted on a single NVIDIA L40.





\noindent\textbf{Watermarking baselines.} We select six typical baselines: three official watermark of Stable Diffusion \cite{rombach2022high} for cloud services called DwtDct \cite{cox2007digital}, DwtDctSvd \cite{cox2007digital}, and RivaGAN \cite{zhang2019robust}, two multi-bit watermarking methods named FSwatermark \cite{xiong2023flexible} and Stable Signature \cite{fernandez2023stable}, and a fine-tuning-free semantic watermarking method called Tree-Ring \cite{wen2024tree}.


\begin{figure*}[t]
\centering
\includegraphics[width=1.9\columnwidth]{9v2.pdf} % 
\caption{Adversarial samples obtained from adaptive adversarial sample attack and reconstrctive attack. }
\label{fig6}
\end{figure*}


%\noindent\textbf{Robustness evaluation.} The parameter settings in the attack layer are as follows: Gauss Blur with kernel size $7\times7$, Gauss Noise(std.) in [0.1,0.8], Brightness in [0.5,1.5], Contrast in [0.5,1.5] and JPEG in [50,90]. 

\noindent\textbf{Evaluation metrics.} We measure the detection performance by the true positive rate (TPR) when the false
positive rate (FPR) is at 1\%. We measure the traceability performance by the bit accuracy. To measure the image generation quality, 
we compute the Peak Signalto-Noise Ratio (PSNR) \cite{hore2010image} and  Structural Similarity score (SSIM) \cite{wang2004image} for image distortion evaluation, the Fr$\acute{\text{e}}$chet Inception Distance (FID) \cite{heusel2017gans} and CLIP score \cite{Radford2021LearningTV} for image diversity and semantic evaluation, and Natural Image Quality Evaluator (NIQE) \cite{mittal2012making} and Perceptual Image Quality Evaluator (PIQE) \cite{venkatanath2015blind} for image quality evaluation. 



\subsection{Watermark Security against Escape}

In order to make the watermark flexible for distributing the LDMs to a large number of model users with strong robustness, we leverage the semantic watermarking framework to inject the watermark message into the latent variable $m$. However, the model users can easily escape the watermark by replacing $m$ with other random latent variables to obtain the non-watermarked generated images. To tackle this issue, we design the security mechanism to decrease the generated image quality when the model user escape the watermark. Representative non-watermarked images under the security mechanism are shown in Figure \ref{fig8}. Embedding watermark does not significantly affect the image quality metrics NIQE and PIQE, or semantic quality metric CLIP. As the quality of unauthorized images decreases further, the quality of watermarked images also slightly deteriorates.




\subsection{Watermark Performance Comparison}
We compare the performance of our method with existing six typical baselines over two tasks: (1) \textbf{Detection}. We consider all compared methods as single-bit watermarks with a unified watermark. We set the FPR to be 1\% and test the TPR on 1, 000 watermarked images. (2) \textbf{Traceability}. Each compared method, excepting for the single-bit watermark Tree-Ring, serves as a multi-bit watermark. In our experiments, we assume that there are 1, 000 model users, each of them requires one watermark for model tracing. Each user generates 10 images, resulting in a dataset of 10, 000 watermarked images. During test, if an image contains a watermark, we then calculate the number of matched bits (Bit Accuracy) with the watermark of each user. The user with the highest Bit Accuracy is considered the traced user and verified. The comparison results are shown in Table \ref{tab3:performance}. Our watermarking achieves
strong robustness and significantly outperforms baselines in
both tasks. In terms of bit accuracy, it surpasses the
best-performing baseline by approximately 6.7\%. This can
be attributed to the extensive diffusion of the watermark
throughout the entire latent space, establishing a profound
binding between the watermark and the image semantics.



% \begin{figure}[t]
% \centering
% \includegraphics[height=0.65\columnwidth, width=0.75\columnwidth]{7.pdf} % 
% \caption{ Results of adaptive adversarial sample attack.
% %The results of adaptive black-box query attacks on watermarks in a model distribution scenario using the WEvade-B-Q method. The evaluation metric is ${\ell}_{\infty}\mathrm{-norm}$.
% }
% \label{fig5}
% \end{figure}






%\noindent\textbf{Image-Processing Attack.}




\subsection{Robustness against Image-Level Attacks}

We examine watermark's robustness against three typical kinds of adversarial attacks, including image processing attack \cite{song2010analysis} for transforming generated images, adaptive adversarial sample attacks \cite{jiang2023evading} for disturbing watermark verification, and reconstructive attacks \cite{balle2018variational,cheng2020learned,zhao2023invisible} for re-generating non-watermarked images. 

% To simulate real-world scenarios, all watermarked images are saved in PNG format, then subsequently read for attack testing. For each attack test, we generate 500 watermarked images for attacks and message extraction.




\noindent\textbf{Image Processing Attack.} We select ten representative types of image-level noise shown in Table \ref{tab:imageprocess}. Please refer to the Supplementary Materials for detailed parameter settings. 


\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{13.pdf}
  \caption{
  Results against adaptive adversarial sample attack (Left) and Reconstructive attack (Right).
  }
  \label{Fig:adaptive}
\end{figure}

\noindent\textbf{Adaptive Adversarial Sample Attack.}
%我们假设敌手可以通过模型producer提供的接口对图像是否带有水印进行黑盒查询。我们考虑了论文\cite{}提供的Query-based black box attack.攻击的核心思想是对原始图像进行JPEG Initialization后获得无水印的初始图像.通过不断访问水印检测器接口并利用算法Hopskipjump迭代寻找最优扰动，使得无水印的初始图像不断接近原始图像。
To further enhance the attack ability, we assume that attackers can query a black-box watermark verification interface %provided by the model producer to determine whether an image contains a watermark. 
and conduct query-based black-box attack \cite{jiang2023evading}. %This attack  initialize an image by applying JPEG compression to the original image to obtain a watermark-free starting point. 
By iterative querying the verification interface % and leveraging the Hopskipjump \cite{chen2020hopskipjumpattack} algorithm, 
this attack compute optimal perturbations that progressively bring the watermark-free initial image closer to the original image. 


\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{12.pdf}
  
  \caption{
 Results of model-level attacks.
  }
  \label{Fig:modelattack}
\end{figure}







\noindent\textbf{Reconstructive Attack.}
The core idea of the reconstructive attack is to add random noise to destroy the watermark and then reconstruct the image. We utilize the implementation of the paper \cite{zhao2023invisible} with denoising steps of 60.

\noindent\textbf{Main Results.}
 For each image processing attack, we report the average bit accuracy in Table \ref{tab:imageprocess}. We see that our DistriMark watermark is indeed robust  across all the transformations with the bit accuracy all above 0.9. DistriMark remarkably outperforms existing multi-bit watermarks on the average performance with more than 0.25 boost compared with the state-of-the-art (SoTA) results. Compared to the SoTA finetuning methods FSWatermark and Stable Signature, Distrimark has significant advantages in image resize, VAE-based compression algorithm, and reconstructive attack. Adaptive Samples Attack utilizes the same evaluation metric ${\ell}_{\infty}\mathrm{-norm}$ as described in the paper \cite{jiang2023evading}. Figure \ref{fig6} and Figure \ref{Fig:adaptive} display images and related parameters generated by Reconstructive Attack and Adaptive Adversarial Samples Attack under the same parameter conditions.  Under both types of attacks, DistriMark demonstrates better robustness than the other two methods. The robustness stems from the watermark being embedded at the semantic level within the image, so more extensive attacks are needed at the pixel level to remove the watermark. Please refer to the Supplementary Materials for more results.









\subsection{Robustness against Model-level Attacks}

Consider the model-level attacks of the model for both single users and multiple users, we have included two types of model-level attacks: model purification and model collusion. 

\noindent\textbf{Model purification.} The adversary fine-tunes the Variational autoencoder to circumvent watermark embedding through the same training mode as Section \ref{sec-3.2}. This involves removing the message loss $L_m$, and shifting the focus to the perceptual loss $L_w$ between the original image and the one reconstructed by the LDM auto-encoder. 

% Since the watermark is semantic, it is difficult to significantly reduce the bit accuracy without affecting the image quality.



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\begin{tabular}{ccccccc}
\hline
{Method} & {PSNR} & {SSIM} & {FID} & {NIQE} & {PIQE} \\\hline
DwtDct            & 39.2 & 0.974 & 3.38  & 3.79 & 32.7 \\
DwtDctSvd         & 39.0 & 0.982 & 9.44  & 3.79 & 32.9 \\
RivaGan           & 40.5 & 0.980 & 15.3  & 3.82 & 32.4 \\
Tree-ring         & ---  & ---   & 25.9  & 4.25 & 33.5 \\
FSWatermark       & 31.9 & 0.897 & 21.7 & 4.22 & 34.7 \\
Stable Signature  & 29.6 & 0.864 & 13.4  & 3.79 & 33.7 \\
DistriMark& 30.8 & 0.856 & 14.6  & 3.98 & 34.2\\
\hline

\end{tabular}
\caption{Quality comparison of watermarked generated images.}
\label{tab2:quality}
\end{table}



\input{table_4}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{5.pdf}
  %图中展示了guidance_scale和reverse inference steps的消融实验的结果，调整guidance scale水印的比特准确率缓慢下降，水印检测率保持稳定。对于reverse inference steps,左图中展示了对于Reverse inference steps的消融饰演的结果，在两步反演以后检测的比特率基本趋于稳定。
  \caption{Ablation study results. (Left) Impact of guidance scales. (Right) Impact of reverse inference steps.  
  %illustrates the ablation experiment concerning the guidance scale, where adjusting the guidance scale leads to a gradual decrease in the watermark's bit accuracy, while the watermark detection rate remains stable. The figure (right) shows the results of the ablation study on reverse inference steps, where the bit rate detected stabilizes after two inference steps.
  }
  \label{Fig:guidanceScale}
\end{figure}




\noindent\textbf{Model Collision.} We mainly considered two types of collusion attacks: Averaging weights and Finetuning. (1) Averaging weights. User$^{(i)}$ and User$^{(j)}$ can average their weights like Model Soup \cite{wortsman2022model} to creat a new model to deceive identification. 
% Because of the security mechanism, parameter averaging will cause a significant drop in image quality. Since the security mechanism and watermarking method are essentially decoupled, parameter averaging will not affect watermark detection.
(2) Finetuning. Another form of collusion attack is when the user B generates a large number of watermarked latent variables and watermarked generated images, and fine-tunes the VAE of A so that A can use B's watermark latent variables to generate images. 

% This could pose a threat of identity spoofing. However, this still does not break the security mechanism.

\noindent\textbf{Main results.} Figure \ref{Fig:modelattack} shows the results of model purification attack. As for model purification, when the bit accuracy decreases, the image quality also declines. Empirically, it is difficult to significantly reduce the bit accuracy without affecting the image quality. As for model collision, because of the security mechanism, parameter averaging will cause a significant drop in image quality. This is because the watermark controller receives different watermark signals from different users, and directly performing model parameter averaging leads to a significant decline in image quality. As for finetuning, this could pose a threat of identity spoofing. However, it can be seen from the results, this still does not break the security mechanism. 
% model collison still does not break the security mechanism.




\subsection{Watermarked Image Quality}
%对于图像的质量，实验考虑了两个影响因素，包括水印嵌入比特数，VAE-Decoder和Initial latents的跳接方式。

Besides the  qualitative examples of how the watermarked  images are not sensitive for the human eyes to distinguish (see Figure \ref{fig8}), we further present quantitative evaluation of images generated by existing watermarking methods in Table \ref{tab2:quality}. The results show that no matter the qualitative metrics, our DistriMark
achieves comparable performance with existing works in the model distribution scenarios.  For DistriMark, the initial watermark is only manifested in the selection of the initial latent variables, hence the image quality is consistent with the diffusion model inference. %Due to fundamental differences in the embedding mechanism compared to FSWatermark and Stable Signature, the image quality of DistriMark is superior to other schemes.
Although DistriMark shows comparable quality compared to the methods in the model distribution scenario, the method involves a trade-off between the quality of unauthorized images and watermarked images when fine-tuning the VAE-Decoder, which results in lower image quality compared to post-processing methods. 


\subsection{Ablation Study}

%The experiment considered two influencing factors for image quality: the embedding capacity, and the skip connection method between VAE-Decoder and initial latents.
%We conduct ablation study on SD V2.1 and SD V1.4 to demonstrate hyperparameter selection.

%\noindent\textbf{Watermark capacity.}
%对于水印容量，实验分别嵌入了16、32、48、64、96比特水印以探究DistriMark水印容量对于图像质量的影响。
%For embedding capacity, the experiment embedded 16, 32, 48, 64, and 96 bits of DistriMark watermark to investigate its influence on image quality.




\noindent\textbf{Skip connection selection.}
%隐变量和变分自编码器的连接方式/方法会对VAE将图形啊从潜在空间转化为像素空间带来影响，实验中分别使用不连接、单次连接、多级连接三种方式进行实验。
The way latent variables are connected to the VAE-Decoder impacts how the VAE transforms images from the latent space to the pixel space. In Table \ref{tab4:skipConnection}, three methods were tested: no connection, single connection, and multi-level connection. Without skip connections, the model struggles to learn watermark characteristics, reducing the quality of images generated with watermarked latent variables. Multi-level connections improve feature learning and enhance image quality.

\noindent\textbf{Guidance scales.} Larger guidance scales result in more faithful of the generated image adherence to prompts. Following existing works \cite{wen2024tree}, we
cover the range of 0 to 20.  In
Figure \ref{Fig:guidanceScale} (left), although a higher guidance scale introduces errors in diffusion inversion due to the lack of such real guidance during detection, the watermark remains robust and reliable even at a guidance scale of 18.

\noindent\textbf{Number of the inversion step.} The inference step is often unknown in practice, which introduces a mismatch with
the inversion step. From Figure \ref{Fig:guidanceScale} (right) we can see that the number of inference steps does not significantly affect the accuracy of inversion which is beneficial in practice.

























\iffalse

\begin{figure*}[h]
  \centering
  \includegraphics[scale=0.9]{4.pdf}
  %图中展示了DiffuseTace在应对不同强度的各类攻击bit accuracy和TPR@1%FPR.并比较了传统水印DwtDctSvd,SSL Watermark,Stable Signaturez在大幅度攻击下的准确率。图中各个水印方案的水印容量为48bit.
  \caption{The figure illustrates the performance of DiffuseTrace in response to various attacks of different intensities, measured by bit accuracy and TPR@0.01FPR. It also compares TPR@0.01FPR of traditional watermarks such as DwtDctSvd, SSL Watermark and Stable Signaturez under corresponding attacks. The watermark capacity for each scheme in the figure is 48 bits.}
\end{figure*}

\fi


% \section{Limitations}
% First, the proposed watermark method is vulnerable to forgery attacks \cite{lukas2023leveraging},highlighting the need for operators to protect the model parameters.  Second, since the skip-binder alters the model architecture, making it vulnerable to human inspection, attackers could potentially circumvent watermarking schemes by substituting VAE decoders. A private VAE decoder should be used in practical applications. Third, although the security mechanism ensures that only latent variables with watermarks can generate images, if the user uses a sampling mechanism that does not support diffusion inversion, it will cause obstacles to watermark detection because the watermarking method is based on ODE solvers \cite{song2020denoising} like DPMSolver\cite{lu2022dpm}.