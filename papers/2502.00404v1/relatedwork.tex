\section{Related Work}
\subsection{Attention-based Super Resolution}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{Fig1.png}
    \caption{The architecture of OmniRWKVSR.}
    \label{arch}
\end{figure*}

In recent years, attention mechanisms have significantly advanced SISR by enabling models to effectively capture long-range dependencies and focus on critical image regions. Notably, the Swin Transformer-based model, SwinIR \cite{swinir}, has demonstrated exceptional performance across various image restoration tasks, including super-resolution, denoising, and JPEG artifact reduction \cite{denoising, JPEGartifactreduction}.

% SwinIR \cite{denoising, JPEGartifactreduction}leverages hierarchical feature representation and shifted window-based self-attention to model long-range dependencies while maintaining computational efficiency. Further extending the capabilities of SwinIR, the SwinFIR \cite{swinfir} model incorporates FFC components to achieve an image-wide receptive field. This enhancement allows SwinFIR to capture global context more effectively, leading to improved performance in image super-resolution tasks. 

Despite the significant advancements brought by attention mechanisms in SISR \cite{visiontransformer, swinir}, a notable limitation persists: the computational complexity of self-attention operations scales quadratically with the input size. This quadratic complexity arises because the attention mechanism computes pairwise interactions between all elements in the input sequence, leading to substantial computational and memory demands, especially for HR images. 

\subsection{State Space Model (SSM)}

To address the computational challenges of attention-based SISR, several models\cite{swinir, swinfir} have integrated innovative mechanisms to improve efficiency. The MambaIR model\cite{mambair} introduces a Selective State Space 2D (SS2D) mechanism\cite{s4, mamba}, which employs SSM with selective scanning strategies to capture long-range dependencies while maintaining linear computational complexity relative to the input size. This design effectively reduces the computational burden of traditional quadratic attention mechanisms, making MambaIR scalable and efficient for HR image restoration tasks. The SS2D mechanism allows MambaIR to model intricate image details without incurring the prohibitive costs typical of self-attention methods, thus balancing performance and efficiency for large-scale image processing.

% On the other hand, the HAT model \cite{hat}, building on the RWKV architecture, combines the strengths of both attention and non-attention mechanisms. HAT selectively applies attention to optimize resource allocation, reducing computational overhead while enhancing feature extraction. Through meta-learning, HAT adapts its attention mechanism dynamically, improving generalization and speeding up training. This enables HAT to effectively capture both local and global features, excelling in tasks that require understanding fine-grained details while maintaining computational efficiency.

\subsection{RWKV Structure}

Despite the advancements introduced by MambaIR \cite{mambair} and the SS2D \cite{vision-mamba} mechanism, challenges remain in terms of prolonged training times, unstable performance metrics, rapid convergence, and suboptimal feature extraction capabilities. The RWKV v6\cite{rwkv} offers solutions to these issues by integrating the strengths of RNNS and Transformers while eliminating the reliance on attention mechanisms. One of the key improvements in RWKV v6 is its improved training stability and convergence speed. By employing careful initialization strategies and optimized training protocols, RWKV v6 mitigates issues related to training instability and premature convergence, leading to more robust and reliable performance across various tasks. Additionally, RWKV's linear computational complexity with respect to input sequence length addresses the inefficiencies associated with quadratic complexity in traditional attention mechanisms. Hence, in this paper, we propose this innovative approach that integrates the strengths of the RWKV architecture with advanced feature extraction techniques to enhance SISR tasks.