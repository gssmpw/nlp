\subsection{Fine-tuning Retrieval Model}
To perform retrieval on a new corpus, a PLM-based retriever is fine-tuned using a training set of annotated query-document pairs.
For each query $q$, the contrastive learning loss is typically applied:
\begin{equation}
    \mathcal{L} = -\log\frac{e^{s_{text}(q,\,\, d^+)}}{e^{s_{text}(q,\,\, d^+)} + \sum_{d^-} e^{s_{text}(q,\,\, d^-)}},
\end{equation}
where $d^+$ and $d^-$ denote the relevant and irrelevant documents. 
$s_{text}(q, d)$ represents the similarity score between the query and a document, computed by the retriever.
For effective fine-tuning, a substantial amount of training data is required. 
However, in specialized domains such as scientific document search, constructing vast human-annotated datasets is challenging due to the need for domain expertise, which remains an obstacle for~applications~\cite{li2023sailer, ToTER}.


\subsection{Prompt-based Query Generation}
\label{prelim:qgen}
Several attempts have been made to generate synthetic queries using LLMs. 
Recent advancements have centered on advancing prompting schemes to enhance the quality of these queries.
We summarize recent methods in terms of their prompting schemes.
% It is worth noting that the following schemes are not mutually exclusive, and they can be combined to form a prompt.

\smallsection{Few-shot examples}
Several methods \cite{inpars, inpars2, dai2022promptagator, pairwise_qgen, label_condition_qgen, saad2023udapdr} incorporate a few examples of relevant query-document pairs in the prompt.
The prompt comprises the following components: $P = \{inst, (d_i, q_i)^k_{i=1}, d_t\}$, 
where $inst$ is the textual instruction\footnote{For example, ``\textit{Given a document, generate five search queries for which the document can be a perfect answer}''. 
The instructions vary slightly across methods, typically in terms of word choice.
In this work, we follow the instructions used in \cite{pairwise_qgen}.
}, 
$(d_i, q_i)^k_{i=1}$ denotes $k$ examples of the document and its relevant query, 
and $d_t$ is the new document we want to generate queries for.
By providing actual examples of the desired outputs, this technique effectively generates queries with distributions similar to actual queries (e.g., expression styles and lengths) \cite{dai2022promptagator}.
It is worth noting that this technique is also utilized in subsequent prompting schemes.

\smallsection{Label-conditioning} 
Relevance label $l$ (e.g., relevant and irrelevant) has been utilized to enhance query generation \cite{label_condition_qgen, saad2023udapdr, inpars}.
The prompt comprises $P = \{inst, (l_i, d_i, q_i)^k_{i=1}, (l_t, d_t)\}$, where $k$ label-document-query triplets are provided as examples.
$l_i$ represents the relevance label for the document $d_i$ and its associated query $q_i$.
To generate queries, the prompt takes the desired relevance label $l_t$ along with the document $d_t$.
This technique incorporates knowledge of different relevance, which aids in improving query quality and allows for generating both relevant and irrelevant queries \cite{label_condition_qgen}.
% For instance, in the case of binary relevance labels, the prompt contains examples for both relevant and irrelevant query-document pairs.

\smallsection{Pair-wise generation}
To further enhance the query quality, the state-of-the-art method \cite{pairwise_qgen} introduces a \textit{pair-wise} generation of relevant and irrelevant queries.
It instructs LLMs to first generate relevant queries and then generate relatively less relevant ones. 
The prompt comprises $P = \{inst, (d_i, q_i, q^-_i)^k_{i=1}, d_t\}$, where $q_i$ and $q^-_i$ denote relevant and irrelevant query for $d_i$, respectively.
The generation of irrelevant queries is conditioned on the previously generated relevant ones, allowing for generating thematically similar rather than completely unrelated queries.
These queries can serve as natural `hard negative' samples for training \cite{pairwise_qgen}.


% LLMs excel at extracting keywords and creating plausible sentences by combining them.
% Recent advanced prompting schemes allow for leveraging this capability to synthesize training data. 
% However, in our attempts to apply them to scientific domain retrieval, we observe that existing techniques do not sufficiently simulate actual queries and show limited effectiveness.



\vspace{0.03in}
\textbf{Remarks.}
% While these advanced schemes effectively leverage the text-generation capabilities of LLMs, there remains substantial room for improvement.
Though effective in generating plausible queries, there remains substantial room for improvement.
We observe that existing techniques often generate queries with limited coverage of the document's concepts.
That is, the queries frequently cover similar aspects of the document, exhibiting high redundancy and failing to add new training signals.
Furthermore, the queries show a high lexical overlap with the document, often repeating a few keywords from the document (\cref{result:query_analysis}).
Considering that the same concepts are expressed using diverse terms in actual user queries, merely repeating a few keywords may limit the efficacy of fine-tuning.

% Ideally, training queries should be complementary to each other, comprehensively covering the document's concepts.
% Ideally, training queries should include relevant concepts expressed in various terms, rather than simply repeating phrases from the document.


\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{images/method.pdf}
\caption{The overview of Concept Coverage-based Query set Generation (\proposed) framework.
% The example is taken from \csfcube dataset.
Best viewed in color.
}
\label{fig:method}
\vspace{-0.3cm}
\end{figure*}


% provide no explicit guidance on what content to generate, 

% A document encompasses multiple concepts, and users inquire about various aspects of a document. 




% LLM은 keyword들을 추출하고, 조합하여 plausible한 sentence를 만드는데 탁월하다. 
% 그동안 연구되어온 prompting scheme들은 이러한 capability를 효과적을 활용하여, high-quality training data를 generation 했다. 
% 그러나, scientic domain에서 이러한 방법론을 적용하려는 시도 끝에, 우리는 기존 기법들이 충분히 actual query를 모방하고 있지 않으며, there still room for improvements임을 발견했다.
% To elaborate, 
% LLM-generated query는 keyword들을 반복하여, 특히 높은 lexical overlap을 보인다. 그러나, 유저들은 같은 academic concept을 아주 다양한 term, expression으로 표현한다. Simply repeating phrases in the document is less effective.
% 나아가, a document는 multiple concepts을 포괄하며, users ask about various aspects of a paper. 
% 기존 기법들은 어떤 내용을 generation 할 것인가에 대한 명시적인 고려나 조건을 활용하지 않는다. 
% When generating several queries, they are often redundant, falling short of adding new training signals.
