% Scientific document retrieval, lack of training data
Scientific document retrieval is a fundamental task that accelerates scientific innovations and access to technical solutions \cite{taxoindex}.
Recently, pre-trained language models (PLMs) have largely enhanced various ad-hoc searches \cite{CTR, DPR}. 
PLM-based retrievers are initially pre-trained on massive textual corpora to develop language understanding.
They are then fine-tuned using vast datasets of annotated query-document pairs, enabling the models to accurately assess the relevance between queries and documents.
However, in specialized domains like scientific document retrieval, constructing large-scale annotated datasets is challenging due to the need for domain expertise \cite{li2023sailer, ToTER, inpars}.
While there are a few general domain datasets (e.g., web search \cite{msmarco_data, NQ_data}), they often fail to generalize to specialized domains \cite{BEIR, inpars}.
This remains a major obstacle~for~applications.


\begin{figure}[t]
    \centering    
    \hspace{-0.2cm}
    \includegraphics[width=0.9\linewidth]{images/intro.pdf}
    \caption{A conceptual comparison of (a) the existing approach for query set generation and (b) our concept coverage-based query set generation. Best viewed in color.}
    \label{fig:intro}
    % \vspace{-0.4cm}
\end{figure}
%% 

% recent solution: synthetic query generation by LLMs
Recently, large language models (LLMs) \cite{GPT3, FLAN, Tzero, Llama} have been actively utilized to generate synthetic data.
Given a document and a prompt including an instruction such as ``\textit{generate five relevant queries to the document}'' \cite{synthetic_apple_VA, dai2022promptagator}, LLMs generate synthetic queries for each document (Figure \ref{fig:intro}a).
The generated queries serve as proxies for actual user queries.
Recent developments in prompting schemes have largely improved the quality of these queries. 
\cite{inpars, dai2022promptagator} show that incorporating a few examples of actual query-document pairs in the prompt leads to the generation of queries with similar distributions (e.g., expression styles) to actual queries.
The state-of-the-art method \cite{pairwise_qgen} employs a \textit{pair-wise} generation that instructs LLMs to generate relevant queries first and then relatively less relevant ones. 
% The generation of irrelevant queries is conditioned on the relevant ones, facilitating the generation of thematically similar rather than completely unrelated queries.
These less relevant queries serve as natural `hard negatives', further improving the efficacy of fine-tuning \cite{pairwise_qgen}.



% Though effective in generating plausible queries, the existing methods impose no conditions for the contents of generation, which often leads to limited coverage of the academic concepts in the document.



\input{sections/9999intro_table}

% limitation
Though effective in generating plausible queries, the existing methods lack control over the content generated, which can lead to incomplete coverage of the academic concepts in a document.
Academic concepts refer to fundamental ideas, theories, and methodologies that form the contents of scientific documents. 
A scientific document typically explores various concepts.
For example, in Table~\ref{tab:intro}, the document addresses the primary task of music playlist recommendation, along with the design of classification-based models, solutions for popularity biases and data scarcity, and the utilization of audio features.
For a thorough understanding of the document, training queries should comprehensively cover these concepts.


% 

% However, existing methods often generate queries with limited coverage of a document's concepts.
% In the absence of control over the content generated, the queries tend to repeatedly cover similar aspects of the document, leading to high redundancy.
However, in the absence of control over the content generated, the queries often repeatedly cover similar aspects of the document, showing high redundancy.
For example, in Table~\ref{tab:intro}, the generated queries ($q^2, q^3$) repeat keywords such as `automated playlist creation' and `song-to-playlist classification' already present in the previous query ($q^1$).
While these concepts are undoubtedly relevant to the document, such redundant queries cannot effectively bring new training signals.
Furthermore, the queries exhibit a particularly higher lexical overlap with the document, compared to the actual user queries (\cref{result:query_analysis}).
We observe that the queries tend to repeat only a few terms extracted from the document.
Given that users express the same concepts using various expressions in their queries, this limited term usage may not effectively simulate actual queries, reducing the efficacy of fine-tuning.
As a naive solution, one might consider adding more instructions to the prompt, such as ``\textit{use various terms and reduce redundancy among the queries}''.
However, this still lacks systematic control over the generation and fails to bring consistent improvements~(\cref{result:CCQGen});
the improved term usage often appears in common expressions (e.g., advance, enhance, and reinforce), not necessarily enhancing concept coverage.


% While this could be beneficial, a more systematic approach could bring greater effectiveness.


% However, this still lacks systematic control over the generation. and fails to bring consistent improvements~(\cref{result:CCQGen}).
% The improved term usage often appears in common expressions (e.g., ``advance'', ``enhance'', and ``reinforce''), not necessarily enhancing concept coverage and retrieval performance.




We propose \textbf{C}oncept \textbf{C}overage-based \textbf{Q}uery set \textbf{Gen}eration (\proposed) framework to meet two desiderata for training queries: 
(1) The queries should cover complementary aspects, enabling comprehensive coverage of the document's concepts, and (2) The queries should articulate the concepts in various related terms, rather than merely echoing a few phrases from the document.
A key distinction of \proposed is that it adaptively adjusts the generation process based on the concept coverage of previously generated queries (Figure~\ref{fig:intro}b).
% We introduce a \textit{concept extractor} to (1) identify the core concepts of each text and (2) discover strongly related terms not explicitly revealed in the document.
We introduce a \textit{concept extractor} to (1) identify the core concepts of each text and (2) uncover concept-related terms not explicitly mentioned in the document.
Using this information, we discern the concepts not sufficiently covered by previous queries, and leverage them as \textit{conditions} for the subsequent query generation.
Table~\ref{tab:intro} shows that the queries generated with \proposed ($q^{2'}, q^{3'}$) cover complementary concepts using more various~related~terms. 
Furthermore, we introduce new techniques to filter out low-quality queries and enhance retrieval accuracy using the obtained concept information.
Our primary contributions~are:
\begin{itemize}[leftmargin=*]\vspace{-\topsep}
    \item We show that existing query generation methods often fail to comprehensively cover academic concepts in documents, leading to suboptimal training and retrieval performance.
    
    \item We propose \proposed framework, which adaptively imposes conditions for subsequent generation based on the concept coverage.
    \proposed can be flexibly integrated with existing prompting schemes to enhance concept coverage of generated queries.

    \item We validate the effectiveness of \proposed by extensive experiments. 
    \proposed brings significant improvements in query quality and retrieval performance over existing prompting schemes.
    % Furthermore, we provide an in-depth analysis of the \proposed.
\end{itemize}\vspace{-\topsep}


