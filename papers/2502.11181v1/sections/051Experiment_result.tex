
\input{sections/99991main_table}

\subsection{Performance Comparison}
\label{sec:experimentresult}

\subsubsection{\textbf{Effectiveness of \proposed}}
\label{result:CCQGen}
Table \ref{tab:main} presents retrieval performance after fine-tuning with various query generation methods.
\proposed consistently outperforms all baselines, achieving significant improvements across various metrics with both backbone models.
We observe that GenQ underperforms compared to LLM-based methods, showing the advantages of leveraging the text generation capability of LLMs.
Also, existing methods often fail to improve the backbone model (i.e., no Fine-Tune), particularly \ctr.
As it is trained on labeled data from general domains, it already captures overall textual similarities well, making further improvements challenging.
The consistent improvements by \proposed support its efficacy in generating queries that effectively represent the scientific documents.
Notably, Promptgator\_diverse struggles to produce consistent improvements.
We observe that it often generates redundant queries covering similar aspects, despite increased diversity in their expressions (further analysis provided in \cref{result:query_analysis}).
This underscores the importance of proper control over generated content and supports the validity of our~approach.


\smallsection{Impact of amount of training data}
In Figure \ref{fig:amount}, we further explore the retrieval performance by limiting the amount of training data, using \ctr as the backbone model.
The existing LLM-based generation method (i.e., Pair-wise gen.) shows limited performance under restricted data conditions and fails to fully benefit from an increasing volume of training data.
This supports our claim that the generated queries are often redundant and do not effectively introduce new training signals.
Conversely, \proposed consistently delivers considerable improvements, even with a limited number of queries.
\proposed guides each new query to complement the previous ones, allowing for reducing redundancy and fully leveraging the limited number of queries.



\begin{figure}[t]
\centering
\includegraphics[height=3cm]{images/limited_label_CSFCube_NDCG.png}
\includegraphics[height=3cm]{images/limited_label_CSFCube_Recall.png}\hspace{-0.1cm}\\ \hspace{-0.3cm}
\includegraphics[height=3cm]{images/limited_label_DORISMAE_NDCG.png}
\includegraphics[height=3cm]{images/limited_label_DORISMAE_Recall.png}\hspace{-0.1cm}
\caption{Results with varying amounts of training data.
x\% denotes setups using a random x\% of generated queries.
}
\label{fig:amount}
\end{figure}



\subsubsection{\textbf{Effectiveness of \proposed with CSR}}
In \cref{subsub:method_filtering}, we introduce \proposedtwo, designed to enhance retrieval using concept information from \proposed.
This technique aligns with the ongoing research direction of enhancing retrieval by integrating additional context not directly revealed from the queries and document \cite{ToTER, mackie2023generative, BERTQE, DensePRF}.
We compare \proposedtwo with two recent methods:
(1) \textbf{GRF}~\cite{mackie2023generative} generates relevant contexts by LLMs. For a fair comparison, we generate both topics and keywords, as used in \proposed.
(2) \textbf{ToTER}~\cite{ToTER} uses the topic distributions between queries and documents, with topics provided by~the~taxonomy.
\ctr is used as~the~backbone.

Table \ref{tab:csr} presents the retrieval performance with various enhancement methods.
\proposedtwo significantly improves the retrieval performance.
Notably, the combination of proposed concept-based query generation (\proposed) and enhancement (\proposedtwo) methods achieves significant improvements over the best existing solutions (i.e., Pair-wise gen. combined with ToTER).
GRF often degrades performance because the LLM-generated contexts are not tailored to target documents;
these contexts may be related but often not covered by documents in the corpus, potentially causing discrepancies in focused aspects.
Lastly, ToTER only considers topic-level information, which may be insufficient for providing find-grained details necessary to distinguish between topically-similar documents.




\subsection{Study of \proposed}


\subsubsection{\textbf{Analysis of generated queries}}
\label{result:query_analysis}
We analyze whether \proposed indeed reduces redundancy among the queries and includes a variety of related terms.
We introduce two criteria: (1) \textbf{redundancy}, measured as the average cosine similarity of term frequency vectors of queries.\footnote{We use CountVectorizer from the SciKit-Learn library.}
A high redundancy indicates that queries tend to cover similar aspects of the document.
(2) \textbf{lexical overlap}, measured as the average BM25 score between the queries and the document.
A higher lexical overlap indicates that queries tend to reuse~terms~from~the~document.


In Table~\ref{tab:query_analysis}, the generated queries show higher lexical overlap with the document compared to the actual user queries.
This shows that the generated queries tend to use a limited range of terms already present in the document, whereas actual user queries include a broader variety of terms.
With the ‘diverse condition’ (i.e., Promptgator\_diverse), the generated queries exhibit reduced lexical overlap and redundancy. 
However, this does not consistently lead to performance improvements.
The improved term usage often appears in common expressions, not necessarily enhancing concept coverage.
Conversely, \proposed directly guides each new query to complement the previous ones.
Also, \proposed incorporate concept-related terms not explicitly mentioned in the document via enrichment step (\cref{method:enrich}).
This provides more systematic controls over the generation, leading to consistent improvements.
% Furthermore, in Table~\ref{tab:case_study}, we provide a case study that compares two approaches.
% \proposed shows enhanced term usage and concept coverage, which supports its superiority in the previous experiments.



\subsubsection{\textbf{Effectiveness of concept coverage-based filtering}}
Figure~\ref{fig:filtering} presents the improvements achieved through the filtering step, which aims to remove low-quality queries that the document does not answer (\cref{subsub:method_filtering}).
As shown in Table~\ref{tab:csr}, \proposedtwo largely enhances retrieval accuracy by incorporating concept information.
This enhanced accuracy helps to accurately measure round-trip consistency, effectively improving the effects of fine-tuning.


\input{sections/99992query_analysis}

\begin{figure}[t]
\centering
\includegraphics[height=2.3cm]{images/Delta_NDCG.png}
\includegraphics[height=2.3cm]{images/Delta_Recall.png}\hspace{-0.1cm}
\caption{Improvements by concept coverage-based filtering.}
\label{fig:filtering}
\end{figure}




\subsubsection{\textbf{Results with a smaller LLM}}
\label{result:Llama}
In Table~\ref{tab:Llama}, we explore the effectiveness of the proposed approach using a smaller LLM, Llama-3-8B, with \ctr as the backbone model. 
Consistent with the trends observed in Table~\ref{tab:main} and Table~\ref{tab:csr}, the proposed techniques (\proposed and \proposedtwo) consistently improve the existing method.
We expect \proposed to be effective with existing LLMs that possess a certain degree of capability. 
Since comparing different LLMs is not the focus of this work, we leave further investigation on more various LLMs and their comparison for future study.



% \begin{table}[t]
%     \caption{A case study on synthetic queries on \dorismae (Doc id: 167551).
%     The conditioned aspects are denoted in bold.}
%     \small
%     \centering
%     %\renewcommand{\arraystretch}{0.9}
%     \resizebox{1.02\linewidth}{!}{
%     \begin{tabular}{C} \toprule
%     \textbf{Synthetic queries generated by \proposed} \\ \midrule
%     \begin{itemize}[leftmargin=-0.001pt, after={\vspace*{-\baselineskip}}]
%         \item[1] In what ways does \textbf{implicit differentiation} in \textbf{implicit MAML algorithm} contribute to enhancing efficiency and performance of \textbf{meta-learning}? $\quad\quad\quad$
%         - Condition: MAML algorithm, meta-learning, implicit differentiation 
%         \item[2] How does implicit differentiation in \textbf{scaling meta-learning systems} contribute to \textbf{theoretical proof} of accurate \textbf{meta-parameter computation}? $\quad\quad\quad\quad\quad$
%         - Condition: theoretical proof, meta-parameters, scaling
%         \item[3] How do \textbf{few-shot learning} scenarios benefit from \textbf{meta-learning} techniques through efficient \textbf{meta-gradient computation}? \phantom{This text invisible.}
%         - Condition: few-shot learning, meta-learning, meta-gradient computation
%     \end{itemize}\vspace{-\topsep}\\ \midrule
%     \textbf{Synthetic queries generated by Promptgator\_diverse} \\ \midrule
%     \begin{itemize}[leftmargin=-0.01pt, after={\vspace*{-1\baselineskip}}]
%         \item[1] What are the key advantages of implicit MAML compared to traditional gradient-based meta-learning approaches?
%         \item[2] In what ways does the implicit MAML algorithm demonstrate empirical gains on few-shot image recognition benchmarks?
%         \item[3] How does the implicit MAML algorithm contribute to the scalability of gradient-based meta-learning approaches?
%     \end{itemize}\vspace{-\topsep}\\ \bottomrule
%     \end{tabular}}
%     \label{tab:case_study}
%     \vspace{-0.4cm}
% \end{table}



\begin{table}[t]
\caption{Retrieval performance with Llama-3-8B. 
We report improvements over no Fine-Tune. $^*$ denotes $p < 0.05$ from paired t-test with pair-wise generation.}
\centering
\renewcommand{\arraystretch}{0.85}
\resizebox{\linewidth}{!}{
\begin{tabular}{c l lll} \toprule
\textbf{Dataset} & \textbf{Method} & \textbf{N@10} & \textbf{N@20} & \textbf{R@100} \\ \midrule
\multirow{3}{*}{CSFCube} & Pair-wise generation & +5.25\% & +0.94\% & \textcolor{red}{- 0.21\%} \\
 & w/ CCQGen & +6.55\% & +7.82\%$^*$ & +5.48\%$^*$ \\
 & w/ CCQGen + \proposedtwo & +27.92\%$^*$ & +20.09\%$^*$ & +9.01\%$^*$ \\ \midrule
\multirow{3}{*}{DORIS-MAE} & Pair-wise generation & +0.00\% & +2.92\% & +5.43\% \\
 & w/ CCQGen & +5.19\%$^*$ & +9.57\%$^*$ & +6.69\% \\
 & w/ CCQGen + \proposedtwo & +16.75\%$^*$ & +20.87\%$^*$ & +14.65\%$^*$\\ \bottomrule
\end{tabular}}
\label{tab:Llama}
\end{table}


