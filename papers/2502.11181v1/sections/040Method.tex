We present \textbf{C}oncept \textbf{C}overage-based \textbf{Q}uery set \textbf{Gen}eration (\proposed) framework, designed to meet two desiderata of training queries:
(1) The concepts covered by queries should be complementary to each other, enabling a comprehensive coverage of the documentâ€™s concepts.
(2) The queries should articulate the document concepts in various related terms, rather than merely repeating phrases from the document.
\proposed consists of two major stages:
\begin{itemize}[leftmargin=*]\vspace{-\topsep}
    \item \textbf{Concept identification and enrichment (\cref{subsec:method_a}):}
        We first identify the core academic concepts of each document.
        Then, we enrich the identified concepts by assessing their importance and adding related concepts not explicitly mentioned in the document.
        This information serves as the basis for generating queries.
        % This model is used to predict academic concepts from input text.
        % Then, we train a small model called a \textit{concept extractor}
    \item \textbf{Concept coverage-based query generation (\cref{subsec:method_b}):}
        Given the previously generated queries $Q^{m-1}_d = \{q^1_d, ..., q^{m-1}_d\}$, we compare the concepts of the document $d$ with those covered by $Q^{m-1}_d$ to identify uncovered concepts.
        These uncovered concepts are then leveraged as conditions for generating the subsequent query $q^{m}_d$, allowing $q^{m}_d$ to cover complementary aspects of $Q^{m-1}_d$.
        % the next query $q^{l}_d$ can cover complementary aspects from $Q^{l-1}_d$.
\end{itemize}\vspace{-\topsep}
Moreover, we propose a new technique, concept similarity-enhanced retrieval (\proposedtwo), that leverages the obtained concept information for \textbf{filtering out low-quality queries} and for \textbf{improving retrieval accuracy (\cref{subsub:method_filtering})}.
Figure \ref{fig:method} provides an overview of \proposed.
% Moreover, we propose new techniques that leverage the obtained concept information for \textbf{filtering out low-quality queries} and for \textbf{improving retrieval accuracy (\cref{subsub:method_filtering})}.




% filter out low-quality queries
% We represent the concepts at two different granularities: topic and phrase levels (Fig.\ref{fig:method}a).

\subsection{Concept Identification and Enrichment}
\label{subsec:method_a}
To measure concept coverage, we first identify the core academic concepts of each document.
We represent the concepts using a combination of two different granularities: topic and phrase levels (Figure \ref{fig:method}a).
Topic level provides broader categorizations of research, such as `collaborative filtering' or `machine learning', while phrase level includes specific terms in the document, such as `playlist continuation' or `song-to-playlist classifier', complementarily revealing the document concepts.


A tempting way to obtain these topics and phrases is to simply instruct LLMs to find them in each document.
However, this approach has several limitations:
the results may contain concepts not covered by the document, and there is always a potential risk of hallucination.
As a solution, we propose a new approach that first constructs a candidate set, and then uses LLMs to pinpoint the most relevant ones from the given candidates, instead of directly generating them.
By doing so, the output space is restricted to the predefined candidate space, greatly reducing the risk of hallucinations while effectively leveraging the language-understanding capability of LLMs.





\subsubsection{\textbf{Core topics identification}}
\label{subsub:core_topic}
To identify the core topics of documents, we propose using an \textit{academic topic taxonomy} \cite{MAG_FS}. 
In the scientific domain, academic taxonomies are widely used for categorizing studies in various institutions and can be easily obtained from the web.\footnote{E.g., IEEE Taxonomy (\href{https://www.ieee.org/content/dam/ieee-org/ieee/web/org/pubs/ieee-taxonomy.pdf}{\color{blue} link}), ACM Computing Classification System (\href{https://dl.acm.org/ccs}{\color{blue} link}).}
A taxonomy refers to a hierarchical tree structure outlining academic topics (Figure \ref{fig:method}a).
Each node represents a topic, with child nodes corresponding to its sub-topics.
Leveraging taxonomy allows for exploiting domain knowledge of topic hierarchy and reflecting researchers' tendency to~classify~studies.


\smallsection{Candidate set construction}
One challenge in finding candidate topics is that the taxonomy obtained from the web is often very large and contains many irrelevant topics.% \footnote{In our experiments, it encompasses $19$ disciplines with over $400,000$ topic nodes.}
To effectively narrow down the candidates, we employ a \textit{top-down traversal} technique that \textit{recursively visits} the child nodes with the highest similarities at each level.
For each document, we start from the root node and compute its similarity to each child node.
We then visit child nodes with the highest similarities.\footnote{We visit multiple child nodes and create multiple paths, as a document usually covers various topics. For a node at level $l$, we visit $l+2$ nodes to reflect the increasing number of nodes at deeper levels of the taxonomy. The root~node~is~level~$0$.}
This process recurs until every path reaches leaf nodes, and \textit{all visited nodes} are regarded as candidates.


The document-topic similarity ${s}(d, c)$ can be defined in various ways.
As a topic encompasses its subtopics, we collectively consider the subtopic information for each topic node.
Let $\mathcal{N}_c$ denote the set of nodes in the sub-tree having $c$ as a root node.
We compute the similarity as: ${s}(d, c) = \frac{1}{|\mathcal{N}_c|}\sum_{j \in \mathcal{N}_c} \operatorname{cos}(\mathbf{e}_{d}, \mathbf{e}_{j})$, 
where $\mathbf{e}_d$ and $\mathbf{e}_j$ denote representations from PLM for a document $d$ and the topic name of node $j$, respectively.\footnote{We use BERT with mean pooling as the simplest choice.}


\smallsection{Core topic selection}
We instruct LLMs to select the most relevant topics from the candidates.
An example of an input prompt is:
\begin{table}[h]
\small
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{|C|}
    \hline
    You will receive a document along with a set of candidate topics. Your task is to select the topics that best align with the core theme of the document. 
    Exclude topics that are too broad or less relevant.
    You may list up to [$k^t$] topics, using only the topic names in the candidate set. \textbf{Document}:~[\textsc{Document}],~\textbf{Candidate~topic~set}:~[\textsc{Candidates}]\\ \hline 
    \end{tabular}}
    \vspace{-0.3cm}
\end{table}

In this work, we set $k^t=10$.
For each document $d$, we obtain core topics as $\mathbf{y}^t_d \in \{0,1\}^{|\mathcal{T}|}$, where $y^t_{di}=1$ indicates $i$ is a core topic of $d$, otherwise $0$.
$\mathcal{T}$ denotes the topic set obtained~from~the~taxonomy.

% $|\mathcal{T}|$ denotes the total number of topics from the taxonomy.

% 

% this can go to implementation detail
% After identifying core topics for all documents, we tailor the taxonomy by only retaining the topics selected as core topics at least once, along with~their~ancestor~nodes.



% \footnote{The phrase set in the corpus is automatically extracted by an off-the-shelf phrase mining tool \cite{autophrase}.}+
% Following \cite{tao2016multi, lee2022taxocom}, we compute the indicativeness of each phrase $p$ based on two criteria:


% These phrases provide fine-grained information not covered by topic levels, playing a critical role in understanding detailed contents and subsequent retrieval.

% 

\subsubsection{\textbf{Core phrases identification}}
\label{method:core_phrase}
From each document, we identify core phrases used to describe its concepts.
These phrases offer fine-grained details not captured at the topic level.
We note that not all phrases in the document are equally important.
Core phrases should describe concepts strongly relevant to the document but \textit{not frequently covered} by other documents with similar topics.
For example, among documents about `recommender system' topic, the phrase `user-item interaction' is very commonly used, and less likely to represent the most important concepts~of~the~document.  


\smallsection{Candidate set construction}
Given the phrase set $\mathcal{P}$ of the corpus\footnote{The phrase set is obtained using an off-the-shelf phrase mining tool \cite{autophrase}.}, we measure the distinctiveness of phrase $p$ in document $d$.
Inspired by recent phrase mining methods \cite{tao2016multi, lee2022taxocom}, we compute the distinctiveness as: $\exp(\operatorname{BM25}(p, d))/\,(1 + \sum_{d'\in\mathcal{D}_{d}}\exp(\operatorname{BM25}(p, d')))$.
This quantifies the relative relevance of $p$ to the document $d$ compared to other topically similar documents $\mathcal{D}_{d}$. 
$\mathcal{D}_{d}$ is simply retrieved using Jaccard similarity of core topics $\mathbf{y}^t_d$.
We set $|\mathcal{D}_{d}|=100$.
We select phrases with top-20\% distinctiveness score~as~candidates. 

\smallsection{Core phrase selection}
We instruct LLMs to select the most relevant phrases (up to $k^p$ phrases) from the candidates, using the same instruction format used for the topic selection.
We set $k^p=15$.
The core phrases are denoted by $\mathbf{y}^p_d \in \{0,1\}^{|\mathcal{P}|}$, where $y^p_{dj}=1$ indicates $j$ is a core phrase of $d$, otherwise $0$.


\subsubsection{\textbf{Enriching concept information}}
\label{method:enrich}
We have identified core topics and phrases representing each document's concepts.
We further enrich this information by (1) measuring their relative importance, and (2) incorporating strongly related concepts (i.e., topics and phrases) not explicitly revealed in the document.
This enriched information serves as the basis for generating queries.


% can aid in finding related phrases
\vspace{0.02in} \noindent
\textbf{Concept extractor.}
We employ a small model called a \textit{concept extractor}.
For a document $d$, the model is trained to predict its core topics $\mathbf{y}^{t}_{d}$ and phrases $\mathbf{y}^{p}_{d}$ from the PLM representation $\mathbf{e}_d$.
We formulate this as a two-level classification task: topic and~phrase~levels.


% We observe that leveraging their complementary nature through multi-task learning consistently enhances both tasks, compared to using two separate classifiers.

% To exploit their complementary nature, we employ 
Topics and phrases represent concepts at different levels of granularity, and learning one task can aid the other by providing a complementary perspective.
To exploit their complementarity, we employ a multi-task learning model with two heads \cite{mmoe}.
Each head has a Softmax output layer, producing probabilities for topics $\hat{\mathbf{y}}^{t}_{d}$ and phrases $\hat{\mathbf{y}}^{p}_{d}$, respectively.
The cross-entropy loss is then applied for classification learning: $-\sum_{i=1}^{|\mathcal{T}|} y^t_{di} \log \hat{y}^{t}_{di} - \sum_{j=1}^{|\mathcal{P}|} y^p_{dj} \log \hat{y}^{p}_{dj}$.


\smallsection{Concept enrichment}
Using the trained concept extractor, we compute $\hat{\mathbf{y}}^{t}_{d}$ and $\hat{\mathbf{y}}^{p}_{d}$, which reveal their importance in describing the document's concepts.
Also, we identify strongly related topics and phrases that are expressed differently or not explicitly mentioned, by incorporating those with the highest prediction probabilities.
For example, in Figure \ref{fig:method}, we identify phrases `cold-start problem', `filter bubble', and `mel-spectrogram', which are strongly relevant to the document's concepts but not explicitly mentioned, along with their detailed importance.
These phrases are used to aid in articulating the document's concepts in various related terms.

% For example, in Figure \ref{fig:method}, we identify related phrases `cold-start problem', `filter bubble', and `mel-spectrogram', which are strongly relevant but not explicitly mentioned in the document, along with their detailed importance.

We obtain $k^{t'}$ enriched topics and $k^{p'}$ enriched phrases for each document with their importance from $\hat{\mathbf{y}}^{t}_{d}$ and $\hat{\mathbf{y}}^{p}_{d}$.
We set the probabilities for the remaining topics and phrases as $0$, and normalize the probabilities for selected topics and phrases, denoted by $\bar{\textbf{y}}^t_d$ and $\bar{\textbf{y}}^p_d$.


% We use $\bar{\textbf{y}}^t_d$ and $\bar{\textbf{y}}^p_d$ to denote the normalized probability distribution of core topics and phrases, respectively.

\subsection{Concept Coverage-based Query Generation}
\label{subsec:method_b}
We present how we generate a set of queries that comprehensively cover the various concepts of a document.
We first identify concepts insufficiently covered by the previously generated queries (\cref{subsub:method_sampling}) and leverage them as conditions for subsequent generation (\cref{subsub:method_condition}).
Then, a filtering step is applied to ensure the query quality~(\cref{subsub:method_filtering}).

This process is repeated until a predefined number ($M$) of queries per document is achieved.
$M$ is empirically determined, considering available training resources such as GPU memory and training time.
For the first query of each document, we impose no conditions, thus it is identical to the results obtained from existing methods.


% 
\subsubsection{\textbf{Concept sampling based on query coverage}}
\label{subsub:method_sampling}
The enriched information $\bar{\textbf{y}}_d$ reveals the core concepts and their importance within the document.
Our key idea is to generate queries that align with this distribution to ensure comprehensive coverage of the document's concepts.
Let $Q^{m-1}_d = \{q^1_d, ..., q^{m-1}_d\}$ denote the previously generated queries.
Using the concept extractor, which is trained to predict core concepts from the text, we identify the concepts covered by the queries, i.e., $\bar{\textbf{y}}^t_Q$ and $\bar{\textbf{y}}^p_Q$.
We use the concatenation of queries as input, denoted as $Q$.
A high value in $\bar{\textbf{y}}_d$ coupled with a low value in $\bar{\textbf{y}}_Q$ indicates that the existing queries do not sufficiently cover the corresponding concepts.

Based on the concept coverage information, we identify concepts that need to be more emphasized in the subsequently generated query.
We opt to leverage phrases as \textit{explicit} conditions for generation, as topics reveal concepts at a broad level, making them less effective for explicit inclusion in the query.
Note that topics are \textit{implicitly} reflected in identifying and enriching core phrases. 
We define a probability distribution to identify less~covered~concepts~as:
\begin{equation}
    \boldsymbol{\pi} = \operatorname{normalize}(\,\max(\bar{\textbf{y}}^p_d - \bar{\textbf{y}}^p_Q, \,\epsilon)\,)
\end{equation}
We set $\epsilon = 10^{-3}$ as a minimal value to the core phrases for numerical stability.
We sample $\lfloor \frac{k^{p'}}{M} \rfloor$ different phrases from $\operatorname{Multinomial}(\boldsymbol{\pi})$, where $M$ is the total number of queries per document.
Note that $\bar{\textbf{y}}^p_Q$ is dynamically adjusted during the construction of the~query~set.

% We define % A higher value in $\pi$ indicates that the concept is not sufficiently covered by the queries.


\subsubsection{\textbf{Concept conditioning for query generation}}
\label{subsub:method_condition}
The sampled phrases are leveraged as conditions for generating the next query $q^m_d$.
There have been active studies to control the generation of LLMs for various tasks. 
Recent methods \cite{control_gen, outline_condition} have specified conditions for the desired outputs, such as sentiment, keywords, and an outline, directly in the prompts.
Following these studies, we impose a condition by adding a simple textual instruction $C$: ``\textit{Generate a relevant query based on the following keywords}: [\textsc{Sampled phrases}]''.
While more sophisticated instruction could be employed, we obtained satisfactory results with~our~choice.


The final prompt is constructed as $[P; C]$, where $P$ is an existing prompting scheme discussed in \cref{prelim:qgen}.
This integration allows us to inherit the benefits of existing techniques (e.g., few-shot examples), while generating queries that comprehensively cover the document's concepts.
For example, in Figure \ref{fig:method}, $C$ includes phrases like `cold-start problem' and `audio features', which are not well covered by the previous queries.
Based on this concept condition, we guide LLMs to generate a query that covers complementary aspects to the previous ones.
It is important to note that $C$ adds an \textit{additional condition} for $P$; the query is still about playlist recommendation, the main task of the document.



\subsubsection{\textbf{Concept coverage-based consistency filtering}}
\label{subsub:method_filtering}
After generating a query, we apply a filtering step to ensure its quality.
A critical criterion for this process is \textit{round-trip consistency} \cite{alberti2019synthetic}; a query should be answerable by the document from which it was generated.
Existing work \cite{dai2022promptagator, label_condition_qgen} employs a retriever to assess this consistency.
Given a generated pair $(q_d, d)$, the retriever retrieves documents for $q_d$. 
Then, $q_d$ is retained only if $d$ ranks within the top-$N$ results.
The accuracy of the retriever is crucial in this step;
a weak retriever may fail to filter out low-quality queries and also only retain queries that are \textit{too easy} (e.g., high lexical overlap with the document), thereby limiting the effectiveness of training.

 


We note that relying on the existing retriever is insufficient for measuring relevance.
While it is effective at capturing similarities of surface texts, the retriever often fails to match underlying concepts.
For example, in Figure \ref{fig:method}, the generated query includes phrases `cold-start problem' and `mel-spectrogram', which are highly pertinent to `data scarcity' and `audio features' discussed in the document.
Nevertheless, as these phrases are not directly used in the document, the retriever struggles to assess the relevance and ranks the document low. 
Consequently, the query is considered unreliable and removed during the filtering process.


\smallsection{Concept similarity-enhanced retrieval (CSR)}
We propose a simple and effective technique to enhance retrieval by using concept information.
For relevance prediction, we consider both textual similarity from the retriever $s_{text}(q,d)$, and concept similarity $s_{concept}(q,d)$.
We measure concept similarity using core phrase distributions, i.e., $s_{concept}(q,d) = sim(\bar{\mathbf{y}}^p_q, \,\bar{\mathbf{y}}^p_d)$, which reveals related concepts at a fine-grained level.\footnote{Here, we compute the similarity for top-10\% phrases (instead of $k^{p'}$) to consider concepts having a certain degree of relevance.
We also tried using core topics. However, it proved less effective as topics reveal concepts only~at~a~broad~level.
}
$sim(\cdot, \cdot)$ is the similarity function, for which use inner-product.
The relevance score~is~defined~as:
\begin{equation}
\begin{aligned}
rel_{CSR}(q,d) = f(s_{text}(q,d), \,s_{concept}(q,d)),
\end{aligned}
\end{equation}
where $f(\cdot, \cdot)$ is a function that combines the two scores. 
We use a simple addition after rescaling them via z-score normalization.
We denote this technique as Concept Similarity-enhanced Retrieval~(\proposedtwo).

For \textbf{filtering process}, we assess the round-trip consistency using \proposedtwo.
By directly matching underlying concepts not apparent from the surface text, we can more accurately measure relevance and distinguish low-quality queries.
Additionally, for \textbf{search with test queries} (i.e., after fine-tuning using the generated data), \proposedtwo can be used as a supplementary technique to further enhance retrieval.
It helps to understand test queries, which contain highly limited contexts and jargon not included in the training queries, by associating them with pre-organized~concept~information.


% \vspace{0.03in}
% \textbf{Remarks on the efficiency of \proposed.}
% \proposed iteratively assesses the generated queries and imposes conditions for subsequent generations. 
% We acknowledge that this approach inevitably incurs additional computations during generation. 
% However, we highlight that no extra costs are incurred during the fine-tuning and inference phases.
% Further, \proposed consistently yields large improvements, even when the number of queries is highly limited, whereas existing methods often fail to improve~the~retriever~(\cref{result:CCQGen}).