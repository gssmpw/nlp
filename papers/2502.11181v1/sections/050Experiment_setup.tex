\smallsection{\textbf{Datasets}}
We conduct a thorough review of the literature to find retrieval datasets in the scientific domain, specifically those where relevance has been assessed by skilled experts or annotators.
We select two recently published datasets: \textbf{\csfcube} \cite{CSFCube} and \textbf{\dorismae} \cite{DORISMAE}.
They offer test query collections annotated by human experts and LLMs, respectively, and embody two real-world search scenarios: query-by-example and human-written queries.
For both datasets, we conduct retrieval from the entire corpus, including all candidate documents.
\csfcube dataset consists of 50 test queries, with about 120 candidates per query drawn from approximately 800,000 papers in the S2ORC corpus \cite{lo2020s2orc}. 
\dorismae dataset consists of 165,144 test queries, with candidates drawn similarly to \csfcube.
We consider annotation scores above `2', which indicate documents are `nearly identical or similar' (\csfcube) and `directly answer all key components' (\dorismae), as relevant.
Note that training queries are not provided in both datasets.


\smallsection{\textbf{Academic topic taxonomy}}
We utilize the field of study taxonomy from Microsoft Academic \cite{MAG_FS}, which contains $431,416$ nodes with a maximum depth of $4$.
After the concept identification step (\cref{subsec:method_a}), we obtain $1,164$ topics and $18,440$ phrases for \csfcube, and $1,498$ topics and $34,311$ phrases for \dorismae.



\smallsection{\textbf{Metrics}}
Following \cite{mackie2023generative, ToTER}, we employ Recall@$K$ (R@$K$) for a large retrieval size ($K$), and NDCG@$K$ (N@$K$) and MAP@$K$ (M@K) for a smaller $K$ ($\leq 20$).
Recall@$K$ measures the proportion of relevant documents in the top $K$ results, while NDCG@$K$ and MAP@$K$ assign higher weights to relevant documents at higher~ranks.


\smallsection{\textbf{Backbone retrievers}}
We employ two representative models: 
(1) \textbf{\ctr} \cite{CTR} is a widely used retriever fine-tuned using vast labeled data from general domains (i.e., MS MARCO).
(2) \textbf{\specter} \cite{SPECTER2} is a PLM specifically developed for the scientific domain. It is trained using metadata (e.g., citation relations) of scientific papers. 
For both models, we use public checkpoints: \texttt{facebook/contriever-msmarco} and  \texttt{allenai/specter2\_base}.
% \footnote{\texttt{facebook/contriever-msmarco}, \texttt{allenai/specter2\_base}.}

\smallsection{\textbf{Baselines}}
We compare various query generation methods.
For all LLM-based methods, we use \texttt{gpt-3.5-turbo-0125}.
Additionally, we explore the results with a smaller LLM (\texttt{Llama-3-8B}) in \cref{result:Llama}.
For each document, we generate \textbf{five} relevant queries~\cite{BEIR}.
\begin{itemize}[leftmargin=*]\vspace{-0.7\topsep}
    \item \textbf{GenQ} \cite{BEIR} employs a specialized query generation model, trained with massive document-query pairs from the general domains.
    We use T5-base, trained using approximately $500,000$ pairs from MS MARCO dataset \cite{nogueira2019doc2query}: \texttt{BeIR/query-gen-msmarco-t5-base-v1}.
\end{itemize}
\noindent
\proposed can be flexibly integrated with existing LLM-based methods to enhance the concept coverage of the generated queries.
We apply \proposed to two recent approaches,  discussed~in~\cref{prelim:qgen}.
\begin{itemize}[leftmargin=*]\vspace{-0.7\topsep}
    \item \textbf{Promptgator} \cite{dai2022promptagator} is a recent LLM-based query generation method that leverages \textbf{few-shot examples} within the prompt. 
    
    % \item \textbf{Pair-wise generation} \cite{pairwise_qgen} is the state-of-the-art LLM-based query generation method that generates both relevant and irrelevant queries in a \textbf{pair-wise} manner.

    \item \textbf{Pair-wise generation} \cite{pairwise_qgen} is the state-of-the-art method that generates relevant and irrelevant queries in a \textbf{pair-wise}~manner.
\end{itemize}
Additionally, we devise a new competitor that adds more instruction in the prompt to enhance the quality of queries:
\textbf{Promptgator\_{diverse}} is a variant of Promptgator, where we add the instruction ``\textit{use various terms and reduce redundancy among the~queries}''.

\smallsection{\textbf{Implementation details}}
We conduct all experiments using 4 NVIDIA RTX A5000 GPUs, 512 GB memory, and a single Intel Xeon Gold 6226R processor. 
For fine-tuning, we use top-50 BM25 hard negatives for each query \cite{formal2022distillation}.
We use 10\% of training data as a validation set. 
The learning rate is set to $10^{-6}$ for \ctr and $10^{-7}$ for \specter, after searching among $\{10^{-7}, 10^{-6}, ..., 10^{-3}\}$.
We set the batch size as $64$ and the weight decay as $10^{-4}$.
We report the average performance over five independent runs.
For all methods, we generate five synthetic queries for each document ($M=5$).
For the few-shot examples in the prompt, we randomly select five annotated examples, which are then excluded in the evaluation process \cite{dai2022promptagator}.
We follow the textual instruction used in \cite{pairwise_qgen}.
For other baseline-specific setups, we adhere to the configurations described in the original papers.
For the concept extractor, we employ a multi-gate mixture of expert architecture \cite{mmoe}, designed for multi-task learning.
We use three experts, each being a two-layer MLP.
For the consistency filtering, we set $N=5$.
We set the number of enriched topics and phrases to $k^{t'}=15$ and $k^{p'}=20$,~respectively.



% instruction
% promptgator:
% pair-wise: 
% max epochs? 30
% filtering of existing methods?