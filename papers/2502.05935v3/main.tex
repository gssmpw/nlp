
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.

%\documentclass[sigconf,authors]{acmart}
\documentclass[sigconf,authorversion,nonacm]{acmart}
%\documentclass[manuscript,screen,review]{acmart}

 
%\documentclass[manuscript,review,anonymous]{acmart} % single column for submission

%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%\usepackage{draftwatermark}
%%\SetWatermarkLightness{0.9}
%%\SetWatermarkText{DRAFT}

\usepackage{amsmath}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[CHI'24]{}{May 11--16,
%   2024}{Honolulu, HI}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
\acmBooktitle{ACM Transactions on Computer-Human Interaction} 
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
%\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{balance}
\usepackage{soul} % for the command \hl

% note: the default \textcircled command might make the number offset a bit.
\newcommand*\numcircledmod[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction]{Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Roel Vertegaal}
\affiliation{%
  \institution{Human Media Lab}
 \institution{Radboud University}
  \city{Nijmegen}
  \country{The Netherlands}
}
  \email{roel@cs.ru.nl}
\orcid{0009-0005-5551-1611}

\author{Timothy Merritt}
\affiliation{%
  \institution{Human-Centered Computing}
  \institution{Dept. of Computer Science, Aalborg University}
  \city{Aalborg}
  \country{Denmark}
}
\email{merritt@cs.aau.dk}
\orcid{0000-0002-7851-7339}

\author{Saul Greenberg}
\affiliation{%
  \institution{Dept. of Computer Science}
  \institution{University of Calgary}
  \city{Calgary}
  \country{Canada}
}
\email{saul@cs.ucalgary.ca}
\orcid{0000-0003-0174-9665}

\author{Aneesh P. Tarun}
\affiliation{%
  \institution{Synaesthetic Media Lab}
  \institution{Toronto Metropolitan University}
  \city{Toronto}
  \country{Canada}
}
\email{aneesh@torontomu.ca}
\orcid{0000-0001-8671-049X}

\author{Zhen Li}
\affiliation{%
  \institution{2012 Labs}
  \institution{Huawei Technologies, Ltd.}
  \city{Markham}
  \country{Canada}
}
\email{zhen.li.cs@gmail.com}
\orcid{0000-0003-4451-0443}

\author{Zafeirios Fountas}
\affiliation{%
  \institution{2012 Labs}
  \institution{Huawei Technologies, Ltd.}
  \city{London}
  \country{UK}
}
\email{zafeirios.fountas@huawei.com}
\orcid{0000-0002-6312-3409}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Vertegaal et al.}
%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Neuromorphic HCI is a new theoretical approach to designing better UX inspired by the neurophysiology of the brain. Here, we apply the neuroscientific theory of Active Inference to HCI, postulating that users perform Bayesian inference on progress and goal distributions to predict their next action (Interactive Inference). We show how Bayesian surprise between goal and progress distributions follows a mean square error function of the signal-to-noise ratio (SNR) of the task. However, capacity to process Bayesian surprise follows the logarithm of SNR, and errors occur when average capacity is exceeded. Our model allows the quantitative analysis of performance and error in one framework with real-time estimation of mental load. We show through mathematical theorems how three basic laws of HCI, Hick’s Law, Fitts’ Law and the Power Law fit our model. We then test the validity of the general model by empirically measuring how well it predicts human performance in a car following task. Results suggest that driver processing capacity indeed is a logarithmic function of the SNR of the distance to a lead car. This positive result provides initial evidence that Interactive Interference can work as a new theoretical underpinning for HCI, deserving further exploration.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003126</concept_id>
       <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   
       <concept_id>10003120.10003121.10003128</concept_id>
       <concept_desc>Human-centered computing~Interaction techniques</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~HCI theory, concepts and models}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{human-computer interaction, theory, interactive inference, bayes theory}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
 There have been many efforts to develop psychological theories of human behaviour as it pertains to user interaction. Frameworks such as GOMS \cite{card:1983} and ACT-R \cite{Anderson:1997ACT-R} have been successful in allowing predictions on time performance when provided with a model user and a model interface. Similarly, models of human error \cite{Reason_1990} have been used to predict error as it relates to the design of interactive systems. However, there has not been a model that has comprehensively explained the relationship between human performance and error in a way that is generalizable. We believe developing such a model is important because it is the key to measuring cognitive load in real time. Current methods for measuring cognitive load fall short because they either rely on a questionnaire that can be difficult to administer during tasks (e.g., the NASA TLX \cite{Hart:1988TLX}), or on unreliable physiological metrics (e.g., Heart Rate Variability, Electrodermal Activity or Pupillometry). 
 
In this paper, we propose a theory of human performance as it relates to error that allows \textit{direct measurement} of performance by defining the signal-to-noise ratio (SNR) of a task. It incorporates the semantics of a task by modeling goals as predictions using Bayesian statistics. We set out to develop a unified approach called Interactive Inference that encompasses both performance and error in one information-theoretical framework. We will demonstrate through mathematical derivation that this unified approach elegantly explains existing laws of Human-Computer Interaction (HCI). It also allows for wider comparisons of information processing capacity and error between otherwise incompatible tasks. With this paper, we are advocating a more neuroscientific approach to design. The theory proposed in this work is one of the first neuromorphic contributions to HCI: an HCI that uses neuroscientific evidence of how the user's brain learns and acts to understand how to create better user experiences.

\subsection{Contributions}
This paper makes the following contributions:

\begin{enumerate}
    
\item Interactive Inference model: We introduce a unified, neuromorphic framework called Interactive Inference that integrates human performance and error using information theoretical constructs. The model uses Bayesian statistics to interpret user goals as predictions, providing a new way to measure performance by quantifying the Bayesian surprise processed per signal-to-noise ratio of tasks.
\item Cognitive load measurement: We claim our model enables real-time estimation of cognitive load without relying on traditional self-reported questionnaires or unreliable physiological metrics. By modeling task complexity externally, it offers a more practical approach for evaluating user experience during interactions.
\item Derivation of HCI Laws: Through known mathematical theorems and logic, we show how foundational principles of HCI—Hick’s Law, Fitts’ Law, and the Power Law of Practice—emerge naturally from our entropy-based framework, supporting a more neuroscientifically grounded approach to interface design.
\item Empirical study: To verify whether Interactive Inference could successfully model a task that does not yet have a known model, we demonstrate through empirical study of a car following task that user performance follows the logarithm of the SNR of the distance to a lead car. This positive result provides initial evidence that Interactive Interference models can work as a new theoretical underpinning for HCI.
\item Research agenda: By discussing the limitations, benefits, and open research questions of this work, we invite further research to extend the theory and explore its practical implications. This includes further empirical studies that test the validity of the theory as applied to different task-modeling scenarios.

\end{enumerate} 


\section{Background}

\subsection{Information Theory and Three Laws of HCI}
Theories of the way the human brain processes information are as old as information theory itself. The very idea that humans might act like a computing circuit arrived shortly after the introduction of the microprocessor. Although most computing logic is deterministic, Shannon \cite{shannon_mathematical_1948} defined self-information as probabilistic: the lower the probability of a message, the higher its information content. One Shannon bit of information is defined as the negative binary logarithm of this probability (negative log probability):

\begin{equation}
\label{equation:SelfInformation}
I(x)=-\log_2(P(x))
\end{equation}
\;

Shannon went on to define that the information capacity $C$ of the transmission of a message through an analog channel has a  maximum that is the (binary) logarithm of the ratio between the signal strength $S$ and the amount of Gaussian noise $N$ in the channel, multiplied by some constant $b$, as expressed by the Shannon-Hartley Theorem:

\begin{equation}
\label{equation:ShannonHartley}
C=b \cdot \log_2(\frac{S+N}{N}) = b \cdot \log_2(\frac{S}{N}+1)
\end{equation}
\;

Where $b$ is some empirically derived bandwidth parameter. This equation describes the capacity of information sent through the channel in terms of the signal-to-noise ratio. Note that since the negative sign is removed, the SNR represents the \textit{reciprocal probability} of a transmission. A high signal allows for more uncertainty to be carried, as does a lower noise. This means the very nature of information is statistical, with underlying equations based on (normal, i.e., Gaussian) distributions. This is important because humans, unlike traditional computer algorithms, operate in a probabilistic environment. That is, their exact behaviour cannot be computed, however, it can be modeled using (normal) probability distributions. Using this framework, Card, Moran and Newell \cite{card:1983} developed the Model Human Processor (MHP), a comprehensive set of heuristics for predicting human performance in simple computer tasks. Statistical elements to these models were derived from empirical observations such as those performed by Hick \cite{HicksLaw} and Fitts \cite{Fitts:1954}. They demonstrated that human performance in certain tasks indeed seems to follow a similar pattern to the Shannon-Hartley Theorem: a binary logarithm of the signal-to-noise ratio in the task. E.g., Hick’s Law describes the response time $RT$ for selecting from a number of alternatives $n$, plus 1:

\begin{equation}
\label{equation:Hick}
RT= b \cdot \log_2(n+1)
\end{equation}
\;

Here, $b$ is some empirically derived constant that models the time per unit of difficulty, the log component of the task. Response time is derived by multiplying the amount of information in the task by this empirical constant. Fitts \cite{Fitts:1954} derived a very similar law describing the movement time $MT$ of a hand towards a target along a single dimensional trajectory $A$ with a target width of $W$ (specific equation from \cite{MacKenzie:1992}):

\begin{equation}
\label{equation:Fitts}
MT= b \cdot \log_2(\frac{A}{W}+1)
\end{equation}
\;

One can think of the power of the signal in this equation as the amplitude (one-dimensional distance) to the target, and the noise as the width of the target. Again, the $b$ is a constant that is empirically derived. It describes an average time per bit of difficulty of the task that when multiplied with the difficulty gives the movement time prediction. It is important to note the difference between the Shannon-Hartley Theorem and these equations: these equations predict time. However, like Shannon-Hartley, all follow the Boltzmann equation for thermodynamic entropy that describes the statistical disorganization of a system \cite{boltzmann:1866}. This is no coincidence. While Boltzmann's equation calculates the properties of energy distribution in physical materials, it uses statistical information on the state of the molecules in the material to do so. We will revisit this later in the paper.
\newline
\;

The third law was derived from a paper by Snoddy \cite{snoddy:1926} and describes the rate of learning of a model human. Time $T$ to complete a task follows a power law as a function of the number of trials of that task $x$ \cite{Crossman:1959}:

\begin{equation}
\label{equation:PowerLaw1}
T=a \cdot x^{-b}+c
\end{equation}
\;

Again, here, $a$, $b$ and $c$ are empirically determined constants.

\subsection{Predictive Coding and the Bayesian Brain Hypothesis}
One of the drawbacks of classic information theoretical approaches to human performance modeling is that classic information theory does not describe the presence of memory of messages transmitted through a channel, let alone proactive anticipation of those messages. According to Shannon, a bit of information is simply the negative logarithm of the probability of a message (\autoref{equation:SelfInformation}). Only if a message is less probable, does it contain more information. The semantics of how the human interprets that message is not modeled, nor whether the human already knows the message. This is problematic because knowing the message, or anticipating the next message, reduces its information content to zero, because this renders the uncertainty about the message zero. 

Instead of framing human processing as a sequential computer processor, McClelland and Rumelhart's parallel processing model \cite{mcclelland:1981} described perception as an interaction of top-down (cognitive/semantic) and bottom-up (sensory) elements. Rao and Ballard \cite{rao:1999} further elucidated this approach by suggesting that the brain builds up statistical knowledge of its environment, i.e., incoming messages, over time, and that this model is used generatively, i.e., top-down, to \textit{predict} the subsequent messages. The error between the prediction and the messages would lead to an update of the prediction.

\begin{figure*}[t]
\includegraphics[scale=0.25]{fig/Figure_Bayesian_Theorem.png}
\caption{\textbf{Bayes Theorem}. Distributions of the Likelihood P(o|s) of observations, in green, given a Prior P(s) that describes the current state, in blue. In black is the calculated Posterior P(s|o) that describes the chance of the state given the observation.}
\label{Bayes}
\Description{Distributions of the Likelihood P(o|s) of observations, in green, given a Prior P(s) that describes the current state, in blue. In black is the calculated Posterior P(s|o) that describes the chance of the state given the observation.}
\end {figure*}

Prediction is convenient for more than one reason: not only does it allow humans to plan actions in their environment based on relevant information, it  serves to compress the information content of their environment by processing only those messages that are surprising: i.e., not, or insufficiently predicted. Predictive coding theory explains how the human brain can work so efficiently and explains the mechanisms behind attention and the scheduling of cognitive resources based on the amount of surprise in their predictions \cite{baldi2010bits}.

According to Rao and Ballard \cite{rao:1999}, Bayesian inference \cite{Bayes:63} is a natural candidate for a generative statistical model. Within this context, Bayesian inference estimates the probability that a particular state $s$ (representing a meaning or cause) exists, given an observation $o$. This is called the posterior probability $P(s|o)$, i.e., the chance of state $s$ after, or given observation $o$. It does so by multiplying two probabilities: the \textit{Likelihood} $P(o|s)$ of the observation $o$ given  state $s$, and the $Prior$ probability $P(s)$  of state $s$ \textit{before} to the observation. The result is subsequently normalized by dividing by the overall probability of the observation $P(o)$ (see \autoref{Bayes}):

\begin{equation}
\label{equation:BayesTheorem}
P(s|o) = \frac{P(o|s) \cdot P(s)}{P(o)}
\end{equation}
\;

Unlike the frequentist approach to statistics, well known to the scientific community as hypothesis testing, Bayesian theory thus incorporates prior statistical knowledge in its testing of a hypothesis. The posterior $P(s|o)$ serves as the subsequent prior for the next observation and as such, serves to improve the prediction of subsequent observations. This yields a different measure of uncertainty than Shannon information: the difference in information between the prior and posterior, or Bayesian surprise. Because Bayesian surprise incorporates the statistical relationship with prior knowledge, it is semantic, describing the information content of the meaning of an observation under prior knowledge. When we consider the prior and posterior to be (normal) distributions, this Bayesian surprise is given by the Kullback-Leibler divergence (KL Divergence, \cite{kullback:1997}). It provides the relative entropy between the two distributions, or the information gained from one distribution to the other, in bits:

\begin{equation}
\label{equation:KL}
KL (P(s)\:||\:P(s|o)) = \int P(s) \cdot log_2 \left( \frac{P(s)}{P(s|o)} \right)ds
\end{equation}
\;

\subsection{Entropy and Active Inference}

Friston et al. \cite{Friston2006} further improved upon predictive coding theory by suggesting a mathematical framework for calculating the amount of Bayesian surprise (i.e., relative entropy) processed by the brain. In their framework, called Active Inference, learning and action are unified under a single principle: both use the difference between prior and (an estimate of) the posterior (i.e., the Bayesian surprise) to make decisions. Minimizing the difference informs when to learn or when to take action. When learning, the difference is mainly minimized by updating the prior to the posterior, thus updating statistical knowledge in the brain. When taking action, the difference is mainly minimized by altering the world, thereby updating the posterior to be closer to the prior knowledge of what the state of the world should be. Since we are concerned with the relative entropy between prior and posterior, the direction of updates does not principally matter.

\subsubsection{Relation to Physical Entropy}
To better understand the concept of relative entropy in information systems, let us first examine the concept of entropy in the physical world. Physical entropy describes the disorganization in the environment that results from most interactions between particles being random, by nature. It is governed by the second law of thermodynamics \cite{Clausius:1879}: In a closed system, entropy (disorganization) tends to increase toward a state of thermodynamic equilibrium, but not decrease. All energy, such as heat, eventually becomes evenly distributed. Engines make use of this relative change in entropy to perform work \cite{Clausius:1879}. Random interactions explain the natural tendency of heat to dissipate from your morning coffee, but also why groups of objects tend towards disorganization, an important realization which we will use later on. Any living organism must avoid the natural tendency towards disorganization – which equals death – by reversing this process. However, to perform this work it must borrow “free energy” from a source $outside$ the closed system of Earth: the Sun. Solar power provides food that allows all organisms to stay neatly organized. Organisms must seek food while avoiding environments that may increase their entropy, such as ones that are too cold or acidic. For this purpose, they evolved sensorimotor systems along with neural circuitry that allow them to observe, learn and act on their environment. This means the core purpose of the human sensorimotor system and cognition is to find ways to minimize the human's physical entropy. It does so by making correct predictions about its next state, i.e., by reducing Bayesian surprise, i.e., relative $information$ entropy. Note that the parallels between reducing physical entropy and information entropy are more than skin deep. Landauer \cite{Landauer} showed how information is exchanged for heat, which has been proven empirically \cite{peterson:2016}. This means that the equations governing information theory and the statistical properties of objects or particles are, in fact, closely related.
\subsubsection{Action Mirrors Learning}
One of the hypotheses of Active Inference therefore is that the approximation of Bayesian surprise, the difference in information entropy between the prior and posterior, is analogous to that of the free energy that allows organisms to maintain physical entropy. We call this approximation of Bayesian surprise free ‘information’ energy. Minimizing free ‘information’ energy is thus the purpose of all learning and action. When learning, sequential Bayesian updates of the prior allow humans to reduce their subsequent Bayesian surprise. When acting, sequential Bayesian updates of movements are also aimed at reducing Bayesian surprise, by modifying the observations instead \cite{Kording:2006}. When learning, the prior moves to the posterior, when acting, the posterior moves to the prior: the roles of prior and posterior are reversed. Note, however, that mathematically these are interchangeable as the multiplication in \autoref{equation:BayesTheorem} is commutative. When free energy or Bayesian surprise is close to zero, learning or action stops and the goal is achieved. There is one complication in the calculation of Bayesian surprise between prior and posterior: that is estimating the probability of observations that occur outside known states, the model evidence $P(o)$. For this reason, and because the number of known states in the brain is extremely large, Active Inference uses a variational rather than Naive Bayesian approach to estimating the posterior \cite{Domingos:1996}. This allows approximation of the true posterior with a simpler distribution, making the problem tractable. When this approximation is good, minimizing free ‘information’ energy becomes equivalent to minimizing Bayesian surprise. Although details of variational approaches are beyond the scope of this paper, the basic idea is to convert what we do know in the Bayes formula, the probability distributions of the likelihood P(o|s) and the prior P(s), to information via a negative log transform then summating these to find a negative log likelihood function that describes their joint probability in log space:
%%Therefore the roles of prior and posterior are reversed during actions.

\begin{equation}
\label{equation:GradientDescent}
-log_2 P(o,s) = -log_2 P(o|s) - log_2 P(s)
\end{equation}
\;

Minimizing this conveniently parabolic error function using gradient descent \cite{Courant:1943} can be used to approximate the minimum free energy and thus Bayesian surprise in a computationally efficient way, while ignoring the information contained in the intractable model evidence $P(o)$. This process is similar to that used for back propagation in neural networks \cite{Rumelhart:1986}. Unlike classical neural networks, however, the free ‘information’ energy approach allows unsupervised real-time learning because it has an inherent reward or loss function: the minimization of Bayesian surprise does not require ground truths. In terms of the user’s experience: the reward is inherent when a system performs the function that the user expects it to perform after some input, yielding minimal Bayesian surprise and no need for further learning or action.

\section{The concept of “task” as a reduction in relative entropy}

Active Inference implies that the core concept of HCI, the task, can be elegantly modeled as the minimizing of Bayesian surprise between prior knowledge of a desired outcome (the prior or goal) and the posterior (the progress towards a goal given current observations). This application of Active Inference to tasks we call Interactive Inference.

\subsection{Reducing Physical Relative Entropy}
So we have a goal distribution, and a progress distribution. Let us first examine how, when we apply this concept to tasks in the real world, it becomes evident that they can be modeled by a reduction in the \textit{physical} entropy of the user's environment.

For the sake of argument, consider how active inference and the corresponding reduction of entropy can be interpreted in a seemingly simple everyday task: organizing socks in a drawer. Simply because the number of possible places where your socks might end up other than the drawer is infinitely large, random interactions make it much more likely that your socks are anywhere but inside the drawer. The task, then, is to reduce the relative disorganization in the location of your socks, or their physical entropy, relative to the location of the drawer. The goal distribution describes the probability of locations where socks might be stored across the physical space inside the drawer. The progress distribution is a histogram of the probability of locations where socks may be found in the home.  To execute this task and reduce the physical entropy in the organization of the socks, we must expend physical energy (walk around and pick up socks, place in drawer). So as to not violate the second law of thermodynamics, the free energy that performs this work is photosynthetic, while waste products, such as heat from our muscles, continue increasing the physical entropy of our environment in other ways.

According to Active Inference, the information used by the user's brain \emph{must} follow a similar process: one that predicts any left-over physical relative entropy between socks and drawer. The brain is merely minimizing the statistical difference in information about the location of socks and drawer that also defines their physical entropy, which is in and of itself a statistical property of objects. It means that the brain, in the process of minimizing the physical entropy must minimize the information entropy of the task. To do so, it calculates the statistical likelihood that an observation of a task object corresponds to the outcome of the task, after which it can estimate the chance that the task is complete given this likelihood. According to Friston et al., it uses Bayesian inference to perform these calculations \cite{Friston2006}.

\subsection{Ten Postulates on Capacity and Error in Tasks}
This leads to the first of ten postulates on Interactive Inference:

%%\vspace{-5pt}
\begin{enumerate} [itemsep=1ex]
\item \emph{Entropy Reduction}. The aim of any task is to minimize the entropy in the space of possible task outcomes. Conversely, any errors in the execution of the task lead to increases in entropy. 
\;
The processing that is required for the user's brain to execute a task leads to a reduction in relative information entropy, i.e., Bayesian surprise. In our example, this reduction is the \textit{information} about the distribution of socks in the household. The aim of the task is to reduce the difference between where the user predicts the socks to be and where they actually are. When executing the task, the user's brain is computing the difference between the probability distributions of locations of socks outside the drawer relative to the probability distribution of the goal: the available locations inside the drawer. In doing so it can, e.g., estimate the time and effort required to complete the task, which leads to postulates 2 and 3:
\;
\item	\emph{Bayesian Surprise}. The reduction in relative information entropy required for a user to process a task is defined as a reduction in Bayesian surprise. Bayesian surprise is equivalent to free energy in Active Inference theory, and to relative entropy. We will use these terms as synonymous. Bayesian surprise thus equals the amount of work that needs to be performed by the brain to perform the task, in bits. Think of it as food for thought.
\;
\item \emph{Scale of Task Outcomes}. Relative entropy is measured along a scale of possible task outcomes $s$. This scale can be \emph{any} feature of the task space that measures success or failure using some ratio level scale. In tasks in the physical world location is often used as a scale of task outcomes. However, an example in the world of information might be a score on a test.

\item \emph{Difficulty}. The difficulty of the task equals the number of bits of relative entropy remaining to be processed. An uncompleted task has the full number of bits remaining. A fully accomplished task has a minimum bits of remaining entropy to be processed (preferably zero). Converting bits to time estimates requires a multiplication of this difficulty with an empirically derived distribution of time performance. E.g., the prior indicates prior statistical knowledge, built up over time, about where socks should be in a second task: that of obtaining a new pair of socks. After the task of organizing socks is complete, when the user wants to obtain any number of socks owned, the probability of obtaining them in the prior location is 100\% because the posterior distribution of socks corresponds 100\% to the prior, the location inside the drawer. By reducing the entropy in the distribution of socks in the first task to fit the goal, the user has been able to make the currently estimated location of socks, the \textit{posterior} or\textit{ progress distribution P}, correspond to the memorized \textit{prior}, the \textit{goal distribution G}. 

\item \emph{Goal and Progress Distributions}. Goal and progress of a task are defined as probability distributions on the scale of task outcomes $s$. The \textit{goal} distribution $G$ describes the prior, the probability of positive task outcomes on this scale. The \textit{progress} distribution $P$ describes the posterior, the probability of being in a current position along the scale of task outcomes $s$. Each distribution has a mean value $\mu$ on this scale. For the goal distribution, the mean $\mu_g$ signifies the value with the highest probability of a correct task outcome. For the progress distribution, the mean $\mu_p$ denotes the value with the highest probability of the current task outcome. Each distribution also has an associated variance. The variance $\sigma_g^2$ or standard deviation $\sigma_g$ of the goal distribution $G$ describes the tolerance for error that is allowed in achieving goal $G$. The reciprocal of this variance $\frac{1}{\sigma_g^2}$ is the precision with which the task is to be completed. The variance $\sigma_p^2$ or standard deviation $\sigma_p$ of the progress distribution $P$ describes the speed with which the task outcome is moving towards the goal distribution: the speed of progress on the task. The reciprocal of this variance $\frac{1}{\sigma_p^2}$ is the precision with which the task is being executed.

\;

From postulate 5 it is becomes apparent that, in order for the goal and progress distributions to match, the user must not just reduce the distance between the means of progress and goal distributions to zero, but also match the precision (or variance) of both distributions. That is, a more precise goal will require, at some point during the task, a more precise movement on the scale of possible task outcomes towards that goal.  In other words, to avoid error, the user will, at some point, need to move more slowly on the scale of possible task outcomes. This is typically towards the end of the task, when the user is closer to the goal. It also apparent that a not so precise goal definition allows faster progress, i.e., users can be more sloppy in the execution of the task. As such, the ratio between the difference of means and the standard deviation reflects a precision/accuracy trade-off (mistakenly known as speed/accuracy trade-off) that is inherent to all tasks. Because the two distributions will need to match on average, the individual variances of goal and progress distributions can be simplified as equal. This brings us to the next postulate:

\item \emph{Signal and Noise}. We call the distance between the means of the goal and progress distributions on variable $s$ the \emph{Signal} to the user's brain. It describes how far, and in which direction, the user needs to travel along the scale of task outcomes to reach the goal distribution, on average. The stronger the signal, the more stimulus the task provides. However, this also means the more work has yet to be performed by the brain. The reciprocal of this signal represents the $Accuracy$ of the task. The standard deviation of the goal distribution is the tolerance to noise and describes the variation allowed in the goal distribution of the task. We therefore call this the \emph{Noise} in the task, because it describes how much noise is allowed in the movement towards the goal for distributions to still match. The larger the noise (tolerance), the less precise the user needs to be in executing the task. As such, the reciprocal of this noise represents the $Precision$ of the task. If we return to the example of organizing your socks, the larger the average distance of socks, the \textit{Signal}, to the drawer, the harder it is to complete the task. The larger the drawer, or \textit{Noise} tolerance, the easier it will be to complete the task. Fitts' Law naturally follows from this, but note we are working in bits rather than time. From this also follows that the relative entropy or difficulty of a task is a function of the ratio of the mean distance S to and standard deviation N of the goal. Note that this SNR is different from the one introduced by Shannon as describes a relative signal to a goal.

\begin{equation}
\frac{\mu_p-\mu_g}{\sigma_g}=\frac{S}{N}
\end{equation}

\item \emph{KL Divergence}. Relative entropy equals the reduction in Bayesian Surprise, in bits, when the user is trying to match progress and goal distributions. This relative entropy is calculated as a Kullback-Leibler Divergence between the goal distribution G and progress distribution P, in bits. Mathematically, this is performed by first converting to bits,  subtracting the two distributions (a fraction inside a log is a subtraction), then calculating the surface area using an integral. To describe error, we include an empirical variable $b$ in this equation that relates the KL divergence (where $b=1$) to an observed empirical error rate (see Postulate 10):

\begin{equation}
\label{equation:KL2}
KL (P(s)\:||\:G(s)) = b \cdot \int P(s) \cdot log_2 \left( \frac{P(s)}{G(s)} \right)ds
\end{equation}
\;

\item \emph{Equal Variance Condition}. The KL Divergence can be greatly simplified in cases where the variance of the goal and progress distributions are identical. Bayesian surprise and thus difficulty can be calculated using a simple version of the KL divergence where we take the square of the signal-to-noise ratio, where the signal is the distance between the means of the goal and progress distributions and the  noise is the variance in the goal distribution. The mathematical proof of the relative entropy of two Gaussians with equal variance is, in fact, the motivation for the use of squared error in statistics and machine learning \cite{Novak:2007}:
\begin{equation}
\label{equation:KL_SNRderivation}
  \begin{split}
\frac{P(s)}{G(s)} = \frac{\sqrt{\sigma_{g}^{2}}}{\sqrt{\sigma_{p}^{2}}}\exp\left(-\frac{(s-\mu_{p})^{2}}{2\sigma_{p}^{2}}+\frac{(s-\mu_{g})^{2}}{2\sigma_{g}^{2}}\right) \text{ and } 
 \sigma_p =\sigma_g
\\
\log_{2}\frac{P(s)}{G(s)}=\log_{2}\left(\frac{\sigma_{g}} {\sigma_{g}}\right)+\log_{2}\exp\left(-\frac{(s-\mu_{p})^{2}}{2\sigma_{g}^{2}}+\frac{(s-\mu_{g})^{2}}{2\sigma_{g}^{2}}\right) 
\end{split}
\end{equation}
The integral of Gaussian $P(s)$ multiplied with a function $log_2 \left( \frac{P(s)}{G(s)} \right)$ equals the expectation under $P(s)$:
\begin{equation}
KL \left(P(s)\:||\:G(s)\right)
= \mathbb{E}_p \left[ \frac{-\left(s-\mu_p\right)^{2}+\left(s-\mu_g\right)^{2}}{2\ln\left(2\right)\sigma_g^{2}}\right] 
\end{equation}
The expectation under $P(s)$ equals:
\begin{equation}
\begin{split}
\mathbb{E}_p\left[(s-a)^{2}\right]=(\mu_p-a)^{2}+\sigma_p^{2}
\\
\mathbb{E}_p \left[ -\left(s-\mu_p\right)^{2} \right]=-\left((\mu_p-\mu_p)^{2}+\sigma_p^{2} \right)= -\sigma_p^2
\\
\mathbb{E}_p \left[ \left(s-\mu_g\right)^{2} \right]=(\mu_p-\mu_g)^{2}+\sigma_p^{2}
\end{split}
\end{equation}
Substituting expectations:
\begin{equation}
=\frac{\left( \mu_p-\mu_g \right)^2 +\sigma_p^{2}-\sigma_p^{2}}{2\ln\left(2\right)\sigma_g^{2}}
\end{equation}
\;


We substitute the constant $\frac{1}{2ln(2)}$ with an empirical variable $b$ that relates the KL divergence (where $b=\frac{1}{2ln(2)}$) to an observed empirical error rate:
\begin{equation}
\label{equation:KL_SNR}
KL \left(P(s)\:||\:G(s)\right) = b \cdot \left( \frac{\mu_p-\mu_g}{\sigma_g} \right)^2 = b \cdot \left( \frac{S}{N} \right)^2
\end{equation}
\;

\item \emph{Performance}. Human performance, or capacity to process relative entropy, follows the reduction in KL divergence as the user moves along the scale of task outcomes. This capacity to process Bayesian surprise, in bits, follows Shannon-Hartley's theorem (\autoref{equation:ShannonHartley}) except with relative entropy. This equals the logarithm of our relative signal-to-noise ratio plus 1. 

\begin{equation}
\label{equation:Capacity}
C = b \cdot log_2 \left( \frac{\mu_p-\mu_g}{\sigma_g} +1 \right) = b \cdot log_2 \left( \frac{S}{N} + 1 \right)
\end{equation}
\;

Where $b$ is some empirically derived constant describing the average performance. Human performance and error can thus both be described on the same scale of signal-to-noise ratio (SNR). Note that this is a different scale then the task outcome scale. It describes the ratio of the distance between means and standard deviation of the goal. Since performance is a logarithmic function of the SNR, while the amount of Bayesian surprise in a task is a squared function of SNR, users will run out of capacity to process Bayesian surprise as the SNR increases. The user's Capacity to process Bayesian surprise can be measured, in real time, by calculating the average reduction in KL Divergence per unit of SNR. This capacity equates the amount of cognitive load and/or sensori-motor load induced by a task because it describes the amount of Bayesian surprise processed by the brain. As per Shannon-Hartley's theorem \cite{shannon_mathematical_1948}, optimal performance is defined by the slope or average capacity of the logarithmic function on the SNR scale. Note that we can also express the SNR scale in bits if we take its binary logarithm, in which case capacity becomes a linear function of log SNR.

\item	\emph{Error}. Human error equals the Bayesian surprise left unprocessed and is defined as the KL divergence ($b \cdot$ SNR squared) after remaining capacity ($b-b \cdot$ $log_2 (SNR+1)$) becomes zero. The $b$ are empirically determined constants. When capacity runs out error cannot be kept arbitrarily low: it starts following the KL divergence and increases with the square of the SNR. Error can thus be measured and predicted in bits in real time, using the same scale as Capacity: that of SNR. Note that error can be modeled empirically by calculating the negative log probability of the errors per unit of SNR. Also note that the empirical error rate $b$ in \autoref{equation:KL_SNR} is expressed in bits of uncertainty rather than probability.
 \end{enumerate}

The ultimate purpose of any user interface design, then, is to achieve Postulate 1 by preventing Postulate 10. Together, our postulates, which are based merely only on derivation and logic, give us a mathematical tool to measure the SNR of a user in real time, i.e., as they perform their task. More importantly, they also  a method for measuring the capacity as it relates to potential error, in real time. The easiest way to do this is to measure the mean distance to $\mu_g$ over some time window, dividing this by the std. dev, and squaring the result.

\begin{figure*}[t]
\includegraphics[width=10cm]{fig/Figure_2.png}
\caption{\textbf{Bayesian Fitts' Law}. Distribution of the Posterior probability P(s|o) of the cursor being within the Goal, from top to bottom: in the middle of a trial, at 2/3s of the trial and at the end of a trial. Goal is the 96\% area of the Prior probability distribution P(s), on both sides for a reciprocal trial. Note how the velocity of the cursor tracks the uncertainty in cursor position.}
\label{Fig:Fitts}
\Description{Distribution of the Posterior probability P(s|o) of the cursor being within the Goal, from top to bottom: in the middle of a trial, at 2/3s of the trial and at the end of a trial. Goal is the 96\% area of the Prior probability distribution P(s), on both sides for a reciprocal trial. Note how the velocity of the cursor tracks the uncertainty in cursor position.}
\end {figure*}
\section{Three Laws of HCI Revisited}
The three laws of Human-Computer Interaction can be recast in terms of the above equations. That includes Hick's Law, Fitts’ Law, the Power Law of Practice and, potentially, others that are yet to be discovered using our method.

\subsection{Fitts' Law}
Kording and Wolpert's work \cite{Kording:2006,Wolpert:2007} strongly suggests that sensorimotor control is indeed guided by Bayesian prediction. Wu et al. \cite{wu:2006} and Williamson et al. \cite{Williamson_2022} also suggest that Fitts' Law \cite{Fitts:1954,MacKenzie:1992} is the result of the sensorimotor system performing Bayesian inference on movement. \autoref{Fig:Fitts} shows this process. The goal in a Fitts' Law task is to move the cursor (or hand) reciprocally between two targets or goals at distance A and click within width W. The goal can be modeled as a Prior probability distribution $P(s)$. The 96\% area under this probability distribution is sectioned of as a width W that constitutes the visual target. This width W, then, corresponds to a $4\sigma$ variance in the goal. In Fitts' Law experiments, both the distance A between the means of the priors and their standard deviations, i.e., and width W, are systematically varied \cite{MacKenzie:1992}. The prior distribution is evidenced over successive trials as a histogram of the locations of clicks \cite{Welford_fundamentals_1968}.  The figure also shows a cursor. The velocity of this cursor reflects the uncertainty in the position of the cursor relative to the goal $P(s)$. This uncertainty is our progress or posterior distribution $P(s|o)$ and needs to be estimated from observation of the distance of the cursor to the goal given a tolerance. \autoref{Fig:Fitts} also shows how, when the cursor moves, this velocity and thus the variance in the posterior increases to a maximum uncertainty, after which it decreases to a minimal velocity or uncertainty once the target is reached. On average, however, velocities and thus the variance in the movement must match the variance or size of the priors so as not to click outside the target. In Interactive Inference, movement is modeled by a step-wise iterative update of the posterior to the prior(s), the speed of which is governed by the remaining bits of difficulty \textit{during} each trial. The calculation of a new estimate of cursor location is modeled as a Bayesian update, in which a new progress distribution is iteratively calculated based on the probability distribution of observations of the hand given the goal distribution (the likelihood, not shown). Indeed, Welford suggested early-on that performance is a function of a signal-to-noise ratio in the brain and that there is a maximum capacity at which the brain can process a Fitts' Law task due to the \textit{``neural noise blurring or distorting signals''}~\cite{Welford_fundamentals_1968}. MacKenzie \cite{MacKenzie:1992} suggested the following equation for describing  movement time in a Fitts' Law task:

\begin{equation}
\label{equation:FittsLaw2}
MT= a + b \cdot log_2 \left( \frac{A}{W} + 1 \right)
\end{equation}
\;

Here, $MT$ equals movemement time, and $a$ and $b$ are empirically determined constants. The equation is modeled after the Shannon-Hartley theorem in \autoref{equation:ShannonHartley}. However, rather than using the statistical properties of the Bayesian priors, it is based on the physical A and W that form the conditions of the experiment. The binary logarithm component of this equation is known as the Index of Difficulty. It describes, in bits, the difficulty of the task. We propose to rewrite \autoref{equation:FittsLaw2} using the statistical properties of Bayesian inference, yielding an equation that describes the Bayesian surprise that is resolved, in bits:

\begin{equation}
\label{equation:FittsLaw3}
C = a + b \cdot log_2 \left( \frac{S}{N} + 1 \right)
\end{equation}
\;

Here, $C$ equals the capacity of the human sensorimotor system in bits, with $a$ and $b$ empirically determined constants. S is the signal in this task, and is given by the mean distance between priors. It is thus equal to A in \autoref{equation:FittsLaw2}. N describes the noise in this task, and is equal to the standard deviation of the goal distributions. Access to data on the speed of hand movements during Fitts' Law tasks appears, to our knowledge, limited. This would be needed to investigate the modeling of cursor speed as a posterior update $P(s|o)$. Schmidt et al. \cite{schmidt1979motor} did show the variance in motor impulse in a similar task to be linearly related to the variance in the resulting distribution, i.e., the prior. Their $W_e$ metric may well correspond to the $N$ in \autoref{equation:FittsLaw3}. However, we see further investigation of this relationship as a future direction of this work. 

Aside from being more neuromorphic, one advantage of \autoref{equation:FittsLaw3} is that capacity can more easily be compared to error, if errors are converted to bits using negative log probability. A second advantage is that it allows real-time measurement of Fitts' Law task difficulty during a trial by sampling the velocity of the cursor or hand. A third distinct advantage is that it allows the comparison of the capacity of Fitts' Law tasks with that of other tasks. This is important in cases where the overall capacity of the user's actions need to be calculated, in bits of unresolved Bayesian surprise. From our earlier discussion follows that once a user runs out of capacity, errors should track a KL divergence after the user runs out of capacity:

\begin{equation}
\label{equation:KL_SNR2}
E = b \cdot \left( \frac{S}{N} \right)^2
\end{equation}
\;

Here, $b$ is an empirically determined constant describing the error rate in bits. Because in most experiments the error rate is set to a fixed 4\%, beyond which trials are ended \cite{MacKenzie:1992}, there is, at present, limited evidence for such error curve. Welford \cite{Welford_fundamentals_1968} did demonstrate a method for posthoc correction of goal distributions, so as to adjust for differences in error rates between different devices. His work appears consistent with the above discussion. Wobbrock et al. \cite{wobbrock2008error} also modeled the probability of an error, with good fit. Our model predicts that user error only tracks the KL divergence once capacity is reached, while their model describes the probability of error rather than the negative log probability. Finally, Guiard et al. \cite{guiard2011fitt} analyzed the error in different conditions of a Fitts' Law experiment. Their RVE metric appear to be consistent with \autoref{equation:KL_SNR2}.

\subsection{Hick-Hyman Law}
Hick-Hyman Law \cite{HicksLaw,Hyman1953StimulusIA} can also be derived from our Bayesian model. Hick-Hyman Law describes user response time when the task is to select one from a number of choices. Hicks \cite{HicksLaw} found that the response time was a logarithm of the number of choices $n+1$:

\begin{equation}
\label{equation:Hicks}
RT=b \cdot log_2(n+1)
\end{equation}
\;

Here, $b$ is some empirically derived time constant that can be used to convert the difficulty of the task to a time estimate. Unlike Fitts' Law, we model Hick-Hyman Law as a learning task in which the user performs Bayesian inference to update a prior of possible targets with an observation of a single highlighted candidate to create a posterior of only one candidate. This means the role of the prior and posterior distributions are not reversed: in this task we update the prior to the posterior. Hyman \cite{Hyman1953StimulusIA} extended Hick's experiment to include unequal probabilities for each choice. He found that results were a function of the entropy of the task:

\begin{equation}
\label{equation:HicksHyman}
\sum_{i=1}^{n}p_{i}\log_{2}\left(\frac{1}{p_{i}}\right)
\end{equation}
\;

Where $p_i$ is the probability of each choice in the set. Note that as the set gets smaller, the chance of a correct answer increases. As the set gets larger, it decreases by the number of elements in the set. If we examine \autoref{equation:KL} it is evident that this is equal to a discrete KL divergence for the task, in which the distributions are probability mass functions instead of proper density functions. Here, the probability of the posterior is set to one because there is one item to which the prior needs to be updated. The prior probability is set to $p_i$. Conveniently, the binary logarithm of 1 reduces to zero making this relative entropy equal to the total entropy of the task. This means the only relevant parameter in this law is the noise in the   prior probabilities: the knowledge the user has at the beginning of the task. According to Wu et al. \cite{wu:2018Hick}, evidence from MRI scans of the brain indeed suggests that the narrowing of the selection from the prior to the observation follows step-wise Bayesian posterior updates. According to them, the cognitive control network in the  brain eliminates half the options at each Bayesian update, yielding a binary search with a logarithmic response time. This means Hick-Hyman Law can be rewritten as a capacity function where noise $N$ is a discrete probability that is the reciprocal of the number of choices \textit{n}:

\begin{equation}
\label{equation:Hicks}
b \cdot log_2(n+1) = b \cdot log_2(\frac{1}{\frac{1}{n}}+1) = b \cdot log_2(\frac{1}{N}+1)
\end{equation}
\;

Where $b$ is some empirically determined constant, in bits, describing information processing capacity per unit of 1 to noise (rather than time per decision). The SNR of this task thus equals the reciprocal of the noise in the prior, i.e., its precision. This means any errors in the selection process should track our simplified KL divergence after logarithmic user capacity runs out:

\begin{equation}
\label{equation:HicksError}
b \cdot (\frac{1}{N})^2 = b \cdot n^2
\end{equation}
\;

To our knowledge, there is no experimental evidence of this relationship and we therefore see this as one of the predictions made by our model that requires further empirical investigation.

\subsection {Power Law of Practice}

 The third law of HCI, the Power Law of Practice \cite{Crossman:1959}, governs the learning curve that arises as users are learning a new task. Again, this is a learning task in which the prior is updated to the posterior. The theoretical basis for the Power Law of Practice was not explained: it is based on an empirical finding that was confirmed in many experiments \cite{Crossman:1959}. These suggest that the amount of time $T$ spent performing a trial of a task is a negative power function of the number of trials $x$ performed:

 \begin{equation}
\label{equation:PowerLaw}
T= a \cdot x^{-b} +  c
\end{equation}
\;
 
Where $a$ and $b$ are empirically derived constants. Within the context of Interactive Inference, this Power Law is explained as a reduction in the KL divergence between prior and posterior that occurs when users are learning a task. This reduction is a function of the signal-to-noise ratio improving with every trial. In deep learning, this is known as minimizing the negative log likelihood function as a function of SNR \cite{bishop2023deep}:
\begin{equation}
\label{equation:NegativeLoglikihood}
H = a-b \cdot log_2 \left( \frac{S}{N} \right)
\end{equation}
\;
Where $a$ and $b$ are empirically derived constants. This equation describes the reduction in entropy, in bits, as an improvement in the signal-to-noise ratio of the task. The time taken per bit then provides the Power Law of Practice. In deep learning, minimizing cross-entropy loss means learning the parameters of a neural network that make the prior distribution as close as possible to the posterior distribution. We propose users likely to do the same when learning a new task.


\section{The case of unequal variances}
There are some cases in which the simplification of the KL divergence to a square function of the SNR does not work. Consider the example of the distribution of socks that need to be placed in the drawer. We note that we could consider this task a compound task in which we summate the individual KL divergences of the movement of each individual sock. In this example, we are simply going to take the average of these, in an attempt to describe the overall entropy of the task with one KL metric.

We recognize that the difficulty of this task is not just governed by the size of the drawer and average distance of socks, but also by the number and distribution of socks in the room or house. This means that there is a discrepancy between the size of the distribution of socks, our posterior, and the goal, our prior. This discrepancy also increases the KL divergence describing the difficulty of the task. To calculate the KL divergence, we need to capture this difference in distributions between goal and progress. While the original integral does this for any distribution, including non-Gaussians, it is computationally expensive and not formulated in terms of SNR. There exists a more general function that is mathematically identical to \autoref{equation:KL} that describes the KL divergence in cases of Gaussian distributions with unequal variances \cite{Moulin:2014}: 

\begin{equation}
\label{equation:KL3_proof}
\begin{split}
\log_{2}\frac{P(s)}{G(s)}=\log_{2}\left(\frac{\sigma_{g}} {\sigma_{p}}\right)+\log_{2}\exp\left(-\frac{(s-\mu_{p})^{2}}{2\sigma_{p}^{2}}+\frac{(s-\mu_{g})^{2}}{2\sigma_{g}^{2}}\right) 
\\
KL \left(P(s)\:||\:G(s)\right) =\log_{2}\left(\frac{\sigma_{g}}{\sigma_{p}}\right)+\mathbb{E}_p \left[ \frac{(s-\mu_{g})^{2}}{2\ln\left(2\right)\sigma_{g}^{2}}\right] -\mathbb{E}_p \left[ \frac{(s-\mu_{p})^{2}}{2\ln\left(2\right)\sigma_{p}^{2}}\right]
\end{split}
\end{equation}
The expectation under $P(s)$ equals:
\begin{equation}
\begin{split}
\mathbb{E}_p\left[(s-a)^{2}\right]=(\mu_p-a)^{2}+\sigma_p^{2}
\\
\mathbb{E}_p \left[ \left(s-\mu_g\right)^{2} \right]=(\mu_p-\mu_g)^{2}+\sigma_p^{2}
\\
\mathbb{E}_p \left[ \left(s-\mu_p\right)^{2} \right]=(\mu_p-\mu_p)^{2}+\sigma_p^{2} = \sigma_p^2
\end{split}
\end{equation}
Substituting expectations:
\begin{equation}
\begin{split}
KL \left(P(s)\:||\:G(s)\right)=\log_{2}\left(\frac{\sigma_{g}}{\sigma_{p}}\right)+\frac{\left(\mu_{p}-\mu_{g}\right)^{2}+\sigma_{p}^{2}}{2\ln\left(2\right)\sigma_{g}^{2}}-\frac{\sigma_{p}^{2}}{2\ln\left(2\right)\sigma_{p}^{2}}
\end{split}
\end{equation}
To describe error, we include an empirical variable $b$ in this equation that relates the KL divergence (where $b=1$) to an observed empirical error rate:
\begin{equation}
\label{equation:KL3}
KL \left(P(s)\:||\:G(s)\right) = b \cdot \left( \log_{2} \left(\frac{\sigma_g}{\sigma_p}\right)+\frac{\left(\mu_p-\mu_g\right)^{2}+\sigma_p^{2}}{2ln(2)\cdot \sigma_g^{2}}- \frac{1}{2ln(2)} \right)
\end{equation}
\;
Here, $\mu_g$ and $\sigma_g$ signify the mean and standard deviation of the goal distribution. $\mu_p$ and $\sigma_p$ signify the mean and standard deviation of the progress distribution. The natural logarithms are constants, and are introduced by base conversion. Including these allows for an exact result in bits of relative entropy. The main difference between \autoref{equation:KL_SNR} and \autoref{equation:KL3} is the addition of the log variability ratio \cite{Senior2020} of goal and progress distributions. As such, \autoref{equation:KL3} models the \textit{mismatch} in variances as an increase in the KL divergence that causes the average entropy to increase, raising the entire loss function. 
\subsection{Precision/Speed Trade-off}
Variance $\sigma_g^2$ represents the tolerance in the goal, while variance $\sigma_p^2$ describes the speed of progress towards that goal, meaning this equation captures the full speed/accuracy trade-off. However, note that the word accuracy is incorrectly used here, and should be worded speed/precision trade-off. The relationship to Fitts' Law is evident: if the hand moves slower or faster than the size of the target allows, a non-optimal result is produced. Stated more generally, when the user moves, on average, outside the tolerance in the task outcome, the task is executed sub-optimally, and by extension, is performed in non-optimal time or error conditions. In terms of Interactive Inference, the posterior update towards the prior is not of an appropriate size. On average, therefore,  $\sigma_p^2$ should match  $\sigma_g^2$ for optimal results. Hence, the simplified \autoref{equation:KL_SNR} suffices when considering optimal performance as both variances, on average, need to be equal for an optimal speed/precision trade-off. This is the case in Fitts' Law experiments. Since $\sigma_p^2$ directly represents velocity when $s$ is a spatial distance metric, this also suggests a linear relationship between velocity and target width \cite{schmidt1979motor}.

\section{Empirical Study}

We now turn to a case study, where we empirically examine whether our general model can  predict human performance in a particular situation. If it can, it provides evidence that the general model is worthy of further exploration.
In particular, we want to verify, via an empirical study, whether the above postulates could successfully model a task that does not yet have a known model, and function as a real-time measurement tool. We used our simplified KL Divergence to measure the information processing capacity of participants in a simulated target \textit{avoidance} task: maintaining distance while driving behind a car. Our attempt to develop a better model of a driver's capacity is more than an academic exercise, as such a model can also impact how modern cars and their interfaces can better recognize and mitigate problems due to driver distraction~\cite{Groeger_Understanding_2000}.

\subsection{Study Literature Review}
In-depth analysis of crash data shows that nearly one third of all fatal and injury crashes involve driver inattention~\cite{wundersitz_driver_2019}.
Lee et al. identified a 30\% (310\textit{~ms}) increase in reaction time when a speech-based email system is used, with implications for safety~\cite{Lee_Speech_2001}.
Strayer et al. demonstrated that inattentional blindness can be caused by concurrent cell phone usage while driving, significantly increasing the risk of an accident~\cite{Strayer_Cognitive_2011}.
In a study involving adult drivers, Strayer and Drews observed an 18\% slower reaction time while conversing on a cell phone~\cite{Strayer_Profiles_2004}.
Systematic reviews of distracted driving, such as those by Simmons et al.~\cite{Simmons_Safety_2016, Simmons_2017} and Caird et al.~\cite{caird_2014} overwhelmingly demonstrate that tasks that require the drivers' visual attention, such as dialing,  texting, and in-car infotainment touch screen use generate more risk than tasks that do not require visual attention, as these direct the driver's attentive resources away from the road and thus the primary task~\cite{niu_effects_2019}.
Researchers have shown particular interest in the car following task due to its prevalence in driving. Wolfe et al's \textit{information acquisition theory} modeled the information a driver acquires through peripheral vision and eye movements~\cite{wolfe_toward_2022}. 
Boer pioneered the use of a behavioral entropy measure as an index of workload, and suggested that information entropy (the average amount of information) is the basis for driving behaviour~\cite{boer_behavioral_2000}. 
Boer et al. used the sample entropy algorithm to gauge performance of drivers, as demonstrated through a driving simulator study~\cite{boer_steering_2005}. 
Senders did pioneering work on \textit{occlusion distance}: the distance a driver feels comfortable driving with their eyes shut~\cite{Senders_attentional_1967}. Kujala developed this metric further as a way to measure visual demand while driving~\cite{kujala_attentional_2016}.
Building on Wilde's \textit{risk homeostasis theory}~\cite{Wilde_theory_1982}, Lu et al. developed a \textit{desired safety margin} model as a quantified index of risk perception when car following~\cite{Lu_car_2013}. That model is based on the leading and following car speeds, and the relative distance between them.
Zhu et al. analyzed the driver's choice of headway (the time interval between two cars at a given speed) in various traffic conditions using a large database~\cite{zhu_car-following_2016}.
Wu et al. proposed a risk repulsion factor \textit{F} for car-following as determined by the time headway, relative speed, and safe spatial headway~\cite{wu_longitudinal_2021}. The driver's cognitive load has also been assessed and captured as a metric when car-following~\cite{summala_brake_2000}. This is important especially when secondary tasks consume some or all of that load. Subjective questionnaires are often carried out to collect the participants' assessment of task difficulty, risk, comfort and effort. For example, Lewis-Evans et al. identified a negative relationship between the time headway in a car following task and ratings of task difficulty in a simulator experiment~\cite{lewis-evans_thats_2010}. However, there are many issues with subjective assessments, including but not limited to bias caused by participants' preferences towards socially desirable responses~\cite{lajunen_can_2003}. Others have proposed various physiological measurements as indicators of the subject's mental effort, such as heart rate variability, electromyogram, and facial temperature~\cite{Mulder_Measurement_1992, deWaard_mental_1996, Kajiwara_Evaluation_2014}. Yet such portable physiological sensing solution are unreliable, while trajectory-based risk assessment methods are too complex~\cite{Petelczyc_Impact_2020}. 

\subsection{KL Divergence of Overlap}
\label{sec:model.prob}

% Note: I added Description for CHI's accessibility requirement. The format is \Description[short description]{long description}. - Zhen. % Fig. 1
\begin{figure*}[t]
\includegraphics[width=\linewidth]{fig/Car_following_4.pdf}
\caption{ Two cars with normal distributions $G(x)$ and $P(x)$ representing uncertainty in their position. The percentage of overlap between distributions equals the probability of a collision (in red). The negative logarithm of this probability gives the number of bits of difference.}
\label{fig:car.following2}
\Description[One car follows the other along a horizontal axis.]{One car follows the other along a horizontal axis, labelled as P(x) and Q(x), with a distance of A. The front car's location has a variation of W along the horizontal axis.}
\end {figure*}

We used our framework to model the car following task probabilistically, as illustrated in \autoref{fig:car.following2}. The task is represented using two normal distributions that describe the uncertainty in the lead and following car’s position. In our model, these are the goal distribution (of the front car) and a progress distribution (of the following car). The task for the driver of the following car is to ensure the progress distribution avoids overlap with the goal distribution. The overlap between the two distributions follows a KL divergence that indicates the amount of bits of processing by the following driver to perform this task. Our simplified KL divergence models this as follows:
\begin{equation}
\label{equation:Car_SNR}
H= b \cdot \left( \frac{\sigma_g}{\mu_p-\mu_g} \right)^2 = b \cdot \left( \frac{N}{S} \right)^2
\end{equation}
\;

Where $\mu_g$ is the mean of the goal distribution, $\mu_p$ is progress distribution. Recognize that what matters here is  the mean relative distance between cars $(\mu_p-\mu_g)$ as it relates to variability in the goal, the deceleration behaviour of the front car $\sigma_g$ that needs matching. Note also that since we are avoiding rather than meeting the goal, we need to take the reciprocal of the signal-to-noise ratio, i.e., the noise becomes the signal. 

$H$ is the amount of Bayesian surprise, in bits, that the driver would need to process in a car following task. However, recall that the capacity C of the driver's ability to process this information is only logarithmic. We can establish parameters $a$ and $b$ of this logarithm by measuring the amount of information $H$ using \autoref{equation:Car_SNR} and by performing a regression on the result (we discuss how we operationalized this in Section \ref{sec:measurement}):

\begin{equation}
\label{equation:Car_Capacity}
C= a + b \cdot log_2\left(\frac{\sigma_g}{\mu_p-\mu_g} \right) = a+b \cdot log_2 \left( \frac{N}{S} \right)
\end{equation}
\;

\subsection{Task}
\label{sec:study}
We designed a controlled car following task in a driving simulator to collect driving data in target avoidance conditions with varying noise-to-signal ratios. 
%Our goal with this experiment was to compare the drivers' information load required to maintain a distance $A$ during driving to those predicted by our equations when following a noisy lead car, focusing on decelerations of this lead car. 
Our goal with this experiment was to model the drivers' information load using our equations to gauge the amount of information used to maintain a distance $S$ when following a lead car with variation $N$ (see \autoref{fig:car.following2}). We focused our experimental analysis on deceleration events that may lead to collision. 

$S$ and $N$ were varied between conditions. 
As in real driving, $N$ was not static within a condition: the lead car accelerated and decelerated to different speeds, which added a normally distributed noise $N$ to each distance condition. 
This allowed us to capture participants' handling of the car in a range of driving difficulty levels defined by the constituting $N/S$ ratios. 
We hypothesized that driver's information capacity during decelerations would be logarithmic with the noise-to-signal ratio, with a mean capacity $b$, in bits of information per bit of difficulty. Because driver information capacity is logarithmic, but the amount of information generated is quadratic, at capacity $b$, the amount of information will outstrip the driver's ability to process. We hypothesized that this will lead to an error statistic that follows the KL divergence in \autoref{equation:Car_SNR}.

\subsection{Participants}
We recruited fifteen participants (5 female, 10 male) from our organization, with a mean participant age of 27. Four of the participants had low driving experience (one year or less).

\subsection{Experimental Setup}
\label{sec:study.setup}
We presented a simulated driving task with simulated road conditions using BeamNG~\cite{beamng_2013}, a car simulator video game chosen for its realism, physics engine, low cost, and programmability. Each participant driver was seated in a simulator in front of a curved 49-inch ultrawide monitor with a 32:9 screen ratio. 
The monitor was positioned 0.6~m from the participant at eye level to provide an immersive simulator view of a car dashboard and windscreen (see \autoref{fig:car.carfollowingui}). 
A Logitech G29 steering wheel and pedal unit with three pedals were mounted under the simulator rig. 
Participants were asked to adjust their seat position so that the steering wheel and accelerator/brake pedals were at a comfortable distance. The pedal clutch was not used. Every participant used the same custom car model with the same physics characteristics in BeamNG.

% old fig.2 is moved to Section 6.
% old fig.3 is modified and moved here to its first appearance.
% new Fig. 2.
\begin{figure}[t]
\includegraphics[width=.5\linewidth]{fig/study-setup-v3.png} % note: 50% width in single-column equals one column width in double-column
\caption{Participant's experimental setup. The white bar in the center of the screen helped participants perceive the correct distance $S$. They needed to match the size of this bar with the lead car's bumper while driving. Here, it is wider than the lead car's bumper indicating that the participant needs to accelerate.}
\label{fig:car.carfollowingui}
\Description[Two hands hold a simulator steering wheel in front of a monitor.]{Two hands hold a simulator steering wheel in front of a monitor, which displays an in-car field-of-view, a white bar in the middle, a speedometer on the right, and a leading car's rear bumper. The white bar is between the leading car and the user's car. }
\end {figure}

We constituted our conditions through custom scripts in BeamNG. 
We also wrote a Python program to interface with BeamNG to administer the study and to capture driving data at a 450~Hz sampling rate: vehicle positions and speeds of the driver's car and lead car, pedal data, and collision data. 
Our custom Python scripts filtered and analyzed the data.

During the experiment, participants had a clear view of the lead car and road through their simulated windscreen. Because a 2D simulator does not provide 3D depth cues, participants were shown a floating fixed-width rectangular bar in the center of their windscreen, positioned below the bumper of the lead car. That bar's width reflected what the perceived width of the lead car's bumper should be at the optimal following distance. The driver's task was to maintain an optimal following distance, where the driver would try to match the bar's size to the perceived size of the bumper of the lead car. For example, in \autoref{fig:car.carfollowingui}, the bar is wider than the bumper, meaning the driver should move closer to the lead car. 

\subsection{Experiment Design}
We used a within-subject experiment where the task was to follow the lead car at a specified distance $S$. We then varied the speed of the lead car with variation $N$. Each participant experienced 12 different task conditions with two factors $S$ and $N$.
\begin{itemize}[align=left, leftmargin=*] %only works in real CHI format
\item Factor $S$ comprised four levels of fixed distances to the lead car (2.84~m, 4.84~m, 6.84~m, and 8.84~m). 
\item Factor $N$ comprised three levels centered around a mean lead car speed of 100~km/h. Each level represented  a 4.1$\sigma$ of a normal distribution of varying distances to the lead car, as generated by increasing the range of the lead car's allowable speeds (1.93~m: 100±12~km/h; 4.19~m: 100±18~km/h; and 7.21~m: 100±24~km/h). That is, the levels represented increasing variability and thus noise in how the car in front accelerated and decelerated. To explain these numbers, at level 1 the speed was allowed to vary between 88 -- 112~km/h (i.e., 100±12~km/h), where 96\% of the variation in distance is captured by 1.93~m. 
\end{itemize}
To constitute each distribution, for each level of $N$, the lead car randomly decelerated 20 times and randomly accelerated 20 times, although always keeping within the level's range of speed values. Each trial within a level of $N$ comprised a new speed adjustment, which occurred approximately every two seconds.
% up to its maximum deceleration and acceleration at 4.1$\sigma$ (88~km/h - 112~km/h; 82~km/h - 118~km/h; %and 76~km - 124~km/h), adjusting speed approximately every two seconds to constitute a trial. 
We randomized the presentation order of conditions. We also randomized the presentation order of acceleration or deceleration trials, with accelerations never occurring within a deceleration trial or vice versa. 

We measured the instantaneous noise-to-signal ratio (change in distance by lead car $\Delta S$, i.e., some sample of $N$; mean distance to lead car $\overline{S}$, per trial), and pedal activity by the driver.
As will be detailed further, our dependent variables were the amount of information processed during a trial (KL divergence), and error (collisions). Together, the 40 trials created a condition simulating a continuous driving task of 2 minutes. 

\subsection{Procedure}
Participants were instructed to hold but not use the steering wheel. 
They were instructed to only use the brake and accelerator pedals to maintain the distance $S$, as the lead car varied its speed, by matching the bar with the bumper of the lead car. 
The road simulation was straight and without obstacles, allowing cars to travel in a straight line. 
Participants were given a 5 minute training task to familiarize themselves with the controls and task.
Each condition began with both cars automatically accelerating to 100~km/h, with the specified distance $S$. 
Participants were then given a visual alert to take control of the car. 
After each condition ended, we asked the participants to rate the driving difficulty of that condition using a 7-point Likert scale~\cite{lewis-evans_thats_2010}. 
Trials that resulted in collision were repeated, with any collisions only counting once per trial, per participant.

\subsection{Ecological Validity}
Our task was designed such that participants would be in a position to experience error. A choice for longer distances and lower speeds would mean the chances of a collision would be too low to be measurable and compared with the driver's remaining capacity. This meant having relatively low headways at high absolute speeds that could be criticized for their limited ecological validity. We note however that the important parameter is not the absolute speed at which the cars are driving, but the variation in their average relative distance over time. This applies equally to parking a car with respect to a solid barrier at low speeds, as it does to avoiding a lead car at 100 km/h. We also chose for the simulator rendering to have most environmental cues removed, making it difficult for participants to estimate an absolute speed. This choice made the experiment more generalizable by avoiding potential confounding factors generated by environmental renderings. The choice not to have participants steer was inspired by the potential for introducing confounding variables as well. 

\section{Results}
\label{sec:results}
We first describe our data analysis, and then present the results of our empirical evaluation of the models.

\subsection{Measurement and Data Analysis}
\label{sec:measurement}
In our experiment, lead car accelerations never resulted in a collision. Given our interest in the relationship between target avoidance and the risk of collision (error), we constrained our analysis to trials where participants responded to a deceleration of the lead car by braking or by releasing the accelerator pedal. We used 3 different levels of precision for noise-to-signal measurements in our analysis. 
Each used a different confidence interval for defining $N$.
\begin{enumerate}[label=(\arabic*), align=left, leftmargin=*] %only works in real CHI format
    \item At the condition level of precision, $N$ was defined by 4.1$\sigma$ of the distribution of relative backwards distances traveled by the lead car during trials in that condition; 
    \item At the trial level of precision,  $N$ was defined by the actual relative backwards distance traveled by the lead car during each trial; and 
    \item Within each trial, $N$ was defined by the standard deviation $(\sigma)$ of a one-second time window of instantaneous distance measurements, as detailed below.
\end{enumerate}

\subsubsection{Empirical Factors Noise (N) and Signal (S)}
\label{measuringnoisesignal}
Although we used predetermined levels of factor $S$, we administered factor $N$ by altering the speed of the lead vehicle, which was affected by the physics engine. 
As such, we measured the actual negative relative distance traveled by the lead car ($\Delta S$) per trial, from the moment of deceleration of the lead car to the lead car reaching the desired speed. 
In line with Welford's statistical analysis of Fitts' law target width $W$~\cite{Welford_fundamentals_1968}, we then calculated $N$ as 4.1$\sigma$ of the distribution of the $\Delta S$s actually administered per level of condition $N$ (a confidence interval of 96\%). 

\subsubsection{Empirical Noise-to-Signal Ratio}
\label{noisesignal}
As each trial involved a braking event that resulted in processing of information given an individually administered $\Delta S$, many similar $N/S$ ratios were created across conditions. 
We therefore used these more detailed empirical observations of noise-to-signal ratios for our log-linear regression analysis, pooled across all trials. 
Here, our definition of noise-to-signal ratio was $\Delta S$ per trial divided by the measured mean $S$ ($\overline{S}$) per trial, yielding the metric $\Delta S/\overline{S}$. 
We also used this definition for our regression analysis of error rates, as this yielded directly comparable results of error rates vs. capacity, in bits. This definition was also used for our Index of Difficulty (see \autoref{equation:W_A_ID}).
Note that the only effect of the use of sampled $\Delta S$ values instead of some confidence interval $N$ is a scaling effect on $N/S$ ratios.

\subsubsection{Measuring Information Processed}
Our first dependent variable was the amount of information processed by the driver during decelerations. To measure the instantaneous amount of information processed within a trial, we used a more detailed 1-second rolling time window to sample the distribution of distances $S$ to the lead car at 450~Hz. 
Our definition of noise $N$ here was the standard deviation ($\sigma$) of the distribution of $S$ obtained in this time window, while our definition of signal $S$ was the mean $S$ ($\overline{S}$) of this time window. 
First, the noise-to-signal ratio was sampled using this type of one second window at the beginning of each trial, ending at the moment the driver responded to the stimulus. We then calculated the amount of information in the stimulus by entering this first noise-to-signal ratio into \autoref{equation:Car_SNR}. 
A one-second window again sampled the noise-to-signal ratio at the end of each trial, prior to (i.e., ending at) a new deceleration or acceleration event. We calculated the remaining amount of information by entering this second noise-to-signal ratio into \autoref{equation:Car_SNR}.

Subtracting the remaining amount of information from the amount of information in the stimulus gave us the amount of information processed by the driver during each deceleration trial. We subsequently aggregated measurements across all conditions, sorting results using the $\Delta S/\overline{S}$ from Section \ref{noisesignal}, and binning measurements that were within 0.05 units of $\Delta S/\overline{S}$. 
While this means our definitions for signal-to-noise were different between y and x axes, we note that our instantaneous KL measurements of information (y-axis) are orthogonal to and independent from the definition of empirical signal-to-noise ratio \textit{per trial} used for the index of difficulty. 

\subsubsection{Measuring Error}
Our second dependent variable was error. Error was defined, across conditions, as the occurrence of a collision with the lead car. 
All trials that ended in error were repeated. 
Since any instantaneous measure of $N/S$ during a collision would yield infinity during the 1 second period leading up to the collision, we used the $\Delta S/\overline{S}$ of the repeat of the trial as our signal-to-noise metric. 
This allowed comparison of error and capacity metrics using the same scale. 
We could not establish any measure for unprocessed information for trials that ended in collision. 
We could, however, convert error rates into bits of information in a manner that provided insights into the relationship between driver capacity and error rates. 
After binning errors within 0.5 $\Delta S/\overline{S}$, we calculated the error probability $p(E)$ by dividing the number of errors per bin by the total number of errors across all conditions. 
We then converted the complement of this probability to bits $E_{b}$ (the non-surprise of a collision) using a negative log probability transform~\cite{shannon_mathematical_1948}:
 \begin{equation}
\label{equation:Bits}
E_{b} = -\log_2(1-p(E))
\end{equation}

% Note: latex reports error use of math sign within section headings. changed to italic.
\subsection{Effects of Factors \textit{N} and \textit{S}}
% Edit based on CHI format.


\autoref{table:first} shows the mean KL divergence values for consumed information against levels of factors $N$ and $S$. Since the residuals in our ANOVA model of the dependent variable differed significantly from a normal distribution (KS test, p>0.05), we used a non-parametric Friedman test. The Friedman test showed highly significant effects of both factors $N$ ($\chi^2(2)=17.73, p<0.001$) and $S$ ($\chi^2(3)=40.84, p<0.001$) on the number of bits consumed by the driver (note that a two-way ANOVA produced the same results).

%A two-way ANOVA showed highly significant effects of both factors W $(F(2,168)=13.5, p<0.001)$ and A $(F(3,168)=41.5, p<0.001)$ on the number of bits consumed by the driver. More information was consumed with higher Ws and lower As. 
%There were no significant interaction effects. 
%Bonferroni corrected post-hoc tests showed significant differences $(p<0.01)$ between all levels of W except 4.19~m and 7.2~m.  
%Bonferroni-corrected post-hoc tests showed significant differences $(p<0.05)$ between levels of all levels of A except 6.84~m and 8.84~m.

\subsection{Car Following Model}
\label{carmodel}

\begin{table}
  \caption{Mean KL (s.e.) for consumed information (in bits) for levels of factors $N$ and $S$.}
  \label{table:first}
  \Description[A table of consumed information in bits of 4 levels S against 3 levels N.]{A table of consumed information in bits of 4 levels S per column (2.84/4.84/6.84/8.84) against 3 levels N per row (1.93/4.19/7.20). Consumed bits range from 0.01, of N 8.84 m x S 1.93 m, to 0.17, of W 2.84 m x A 7.20 m.}
  \begin{tabular}{ccccc}
    \multicolumn{5}{c}{\textbf{S}} \\
    \toprule
    % \hline
    \textbf{N} & \textbf{2.84 m} & \textbf{4.84 m} & \textbf{6.84 m} & \textbf{8.84 m}\\
    % \hline
    \midrule
    \textbf{1.93 m}  &0.11 (0.07)	&0.06 (0.04)	&0.03 (0.02)	&0.01 (0.02) \\
    \textbf{4.19 m}  &0.16 (0.09)	&0.09 (0.04)	&0.05 (0.03)	&0.04 (0.04) \\
    \textbf{7.20 m}  &0.17 (0.09)	&0.10 (0.03)	&0.08 (0.04)	&0.05 (0.02) \\
    % \hline
    \bottomrule
\end{tabular}
\end{table}

% Fig. 3
\begin{figure*}[t]
\includegraphics[width=\linewidth]{fig/Performance_Graph_v6.pdf}
\caption{ Driver information capacity plotted against noise-to-signal ratio ($\Delta S/\overline{S}$, index of difficulty (ID) on a logarithmic scale below x-axis). Capacity followed a logarithmic function (blue solid), with a slope of 0.25 bits (green dash). Black solid line indicates remaining capacity.}
\label{fig:car.infoload}
\Description[A curve plot of three lines, and a series of data dots along the rising logarithmic line.]{A curve plot of three lines: 1) Line 1 shows driver information capacity following a logarithmic function, 2) Line 2 shows a slope of 0.25 bits which is parallel to x axis, and 3) Line 3 shows a dropping line indicating remaining capacity. Line 1 and Line 2 intersects around 1.2 bit, and Line 1 and Line 3 intersects around 0.5 bit. The x-axis is noise-to-signal ratio on a logarithmic scale. The data dots spread along Line 1, the driver information capacity.}
\end {figure*}

Log-linear regression of binned KL divergence (overlap) measurements onto noise-to-signal measurements $\Delta S/\overline{S}$ yielded a logarithmic relationship $(r^2=0.86, F(1,26)=164.2, p<0.001)$:
\begin{equation}
\label{equation:W_A_Result}
C= -0.02 + 0.25 \log_2 \left(\frac{\Delta S}{\overline S} + 1 \right)
\end{equation}
\autoref{fig:car.infoload} shows this relationship between driver capacity and aggregated signal-to-noise measurements on a $\Delta S/\overline{S}$ scale across conditions, as well as on a logarithmic scale (below the x-axis). 
Mean capacity is visualized as a dotted line. 
Subtracting driver capacity from this mean capacity yields remaining driver capacity (black solid line). In subsequent sections, we will show how this remaining capacity was used to drive the simulator's user interface. The logarithmic scale at the bottom of \autoref{fig:car.infoload} shows the index of difficulty (ID) of this task, in bits:
\begin{equation}
\label{equation:W_A_ID}
ID = \log_2 \left(\frac{\Delta S}{\overline S} + 1 \right)
\end{equation}
The mean capacity (bandwidth $b$) of our car following task was measured at $b=0.25$ bits/bit, while the Index of Performance (throughput $1/b$) was 3.96 bits/bit. 
The y axis indicates bits of consumed information, while the x axis indicates noise-to-signal ratio $\Delta$S/$\overline{S}$, or bits of $ID$ if the bottom log scale is used.
Note that the only difference in plotting \autoref{fig:car.infoload} on a log-linear scale against $ID$, as is customary when plotting with Fitts' law, is that lines would be straight, with the bottom scale as the primary x-axis. 
Mean capacity is expressed as bits of information consumed per bit of difficulty, while the Index of Performance (1/b) is expressed as the bits of difficulty processed per bit of consumed information.

\subsection{Error (Collision) Model}
\label{sec:error}

%One of the strengths of measuring performance in bits, using aggregated noise-to-signal measurements of $\Delta$A/$\overline{A}$, is that we can visualize error probabilities on the same scale as negative log probabilities. An error was defined as a collision with the front car, at a noise-to-signal ratio defined by the trial's measured $\Delta$A/$\overline{A}$. After binning errors within 0.5 $\Delta$A/$\overline{A}$, we calculated the error probability $p(E)$ by dividing the number of errors by the total number of errors. We then converted the complement of this probability to bits $E_{b}$ (non-surprise of a collision) using a negative log probability transform:
% \begin{equation}
%\label{equation:Bits}
%E_{b} = -\log_2(1-p(E))
%\end{equation}

% Fig. 4
\begin{figure*}[t]
\includegraphics[width=\linewidth]{fig/Error_Graph_v3.pdf}
\caption{ Error (non-surprise, in bits) plotted against noise-to-signal ratio ($\Delta S/\overline{S}$, index of difficulty (ID) on a logarithmic scale below x-axis). Error followed a KL divergence (red dots). Blue solid lines shows driver capacity and black solid line remaining capacity.}
\label{fig:car.error}
\Description[A curve plot of three lines, and a series of data dots along the rising power line.]{A curve plot of three lines: 1) Line 1 shows an error curve following a KL divergence with data dots along it, 2) Line 2 shows a driver capacity line following a logarithmic curve, and 3) Line 3 shows a dropping remaining capacity curve. Line 1 and Line 2 intersects around 1.6 bit, and Line 1 and Line 3 intersects around 0.7 bit. The x-axis is noise-to-signal ratio on a logarithmic scale.}
\end{figure*}

Regression of error metric $E_b$ (non-surprise of a collision, in bits) onto noise-to-signal measurements ($\Delta S/\overline{S}$) yielded a significant relationship with the KL divergence metric (\autoref{equation:Car_SNR}) ($r^2=0.94, F(1,5)=80.5, p<0.001)$:
\begin{equation}
\label{equation:Error}
E_b= 0.03 + 0.13 \left(\frac{\Delta S}{\overline S} \right)^2
\end{equation}
\autoref{fig:car.error} shows the relationship between error (red solid) and aggregated signal-to-noise measurements on a $\Delta S/\overline{S}$ scale across conditions, as well as on a logarithmic scale ($ID$, bottom), compared with driver capacity (blue solid) and remaining capacity (black solid). 
The total number of collisions across all trials was 90. Error rates below or equal to a noise-to-signal ratio or $ID$ of 1 trended, on average, slightly below the expected 4\% (at 0.05 bits or 3.4\%, between 2 and 4 collisions across trials), while those above 1 increased quadratically, averaging 0.38 bits (23\%), with a maximum of 0.82 bits (43\% or 39 collisions across trials) at $ID=1.85$.

\subsection{Qualitative Results}

% Fig. 5
\begin{figure*}[t]
\includegraphics[width=\linewidth]{fig/questionnaire.png}
\caption{Perceived task difficulty for each condition: participants' responses to the question ``This driving task was difficult'' on a scale of 1 (Strongly Disagree) to 7 (Strongly Agree).}
\label{fig:car.perceiveddifficulty}
\Description[A bar chart of all conditions' subjective ratings aligned vertically by the middle-point choice of “Neither Agree nor Disagree”.]{A bar chart of all conditions' subjective ratings (7-point scale) aligned vertically by the middle-point (level-4) choice of “Neither Agree nor Disagree”. In each row, the choices are sorted by the rating from 1 (“Strongly Disagree”) to 7 (“Strongly Agree”), left to right. For the easiest condition (row 1), the range of the user's rating is -0.80--0.13. For the hardest condition (row 12), the range of the user's rating is 0.00--1.00. }
\end {figure*}

\autoref{fig:car.perceiveddifficulty} shows participants' questionnaire responses ranking difficulty per conditional $N/S$, sorted by $ID$. 
These represent mean responses to the question ``This driving task was difficult'', on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree)~\cite{lewis-evans_thats_2010}. 
A Friedman non-parametric analysis of variance showed participants rated the car following task as significantly more difficult with increasing $ID$ ($\chi^2(11)= 116.4, p<0.0001$). 
Note that all participants found the task to be easy to interpret and execute, while none reported distraction by the floating bar graphic during the task. 
\vspace{-2pt}
\section{Discussion}
We will now discuss the results from our empirical study, relate them to our Interactive Inference model, and provide some suggestions for future directions.
\subsection{Empirical Study}
The above results are a first step towards empirical validation of both our capacity and error models. Results show a good to excellent fit between between predicted and observed models of the task. Driver information processing capacity during deceleration events in a simulated car following task can be measured in real time using the simplified KL divergence of the noise-to-signal ratio of the distance to the car followed, over a time window. Our hypotheses that both distance $S$ and its variance $N$ to the lead car significantly affect the amount of information consumed by the driver during such events were confirmed, with more information consumed at lower $S$s and higher $N$s. 

Error rates also appear to independently confirm the effect of $N/S$ ratios on driver information capacity when car-following. As expected, the (negative log of the complement of) error rate followed a KL divergence. When driver capacity exceeds mean capacity, error rates appear to increase with the square of the noise-to-signal ratio. We attribute these trends to ``left-over'' Bayesian surprise, generated by high noise-to-signal ratios, that is not consumed by the driver. Note that we used mean capacity as our capacity limit because the theoretical maximum channel capacity is, in practice, never reached (without significant error) \cite{shannon_mathematical_1948}. Our results also suggest bit rates are quite low. We presume this is because information processing is throttled by the eye/foot/pedal processing loop. 

Qualitative ratings by participants were in line with the above. They rated the task as more difficult with higher noise-to-signal ratios (and thus $ID$s), albeit with bipolar ratings of middle $ID$s. We attribute this to participants perceiving distant targets $(S>4.8~m)$ as easier to process than close targets with similar signal-to-noise ratios, perhaps due to the 2D nature of our simulator setup. We again note that our results are limited to modeling driver responses to lead car decelerations in a simulated car following task using specific road conditions, car physics, and a sample set of participants.
\subsection{Interactive Inference Model and Future Directions}
Our results provide good support for the Interactive Inference model, which shows how capacity for performance can be related to error, both in bits. We model average capacity as a logarithmic function of SNR, scaled by a slope that is empirically determined. We can consider this slope to be analogous to the bandwidth in the Shannon-Hartley theorem. Because capacity beyond the limits of this model cannot occur without error, and because both metrics can be converted to bits, one of the predictions from our model is that error can be analysed in view of leftover capacity. This is because the average capacity is, in fact, the maximum capacity. Error subsequently tracks Bayesian surprise as a KL divergence. \autoref{fig:car.error} shows this relationship, with the KL divergence in red fit to some empirically observed error rate (converted to negative log probability) visualized in red dots. Capacity is modeled as a logarithmic function (in blue), which is an empirical fit to Bayesian surprise processed at different SNRs of the task. By subtracting the logarithmic function from this value, we can arrive at a leftover capacity metric (in black). Where this line reaches zero capacity, errors increase and start tracking the KL divergence: the square of the signal-to-noise ratio. The empirical evaluation of this relationship is considered a future direction.

Left-over capacity would correspond directly to the cognitive or sensorimotor load of the task. By correlating empirical results with Heart Rate Variability (HRV) data and other measures of cognitive load evidence for this relationship could be examined. One important topic of investigation is to determine if cognitive load could be measured during tasks, in real time. E.g., users could complete a task modeled via our method while wearing a heart rate monitor. HRV should be lower during moments in the task where there is significant left-over surprise.

The correlation between HRV and cognitive load suggests it may be possible that left-over Bayesian surprise also tracks stress levels in participants. The relationship between our models and experienced stress levels of participants during tasks is therefore an interesting area of future study. E.g., users could complete a task modeled via our method and then fill out a questionnaire about their stress levels. Left-over surprise metrics should correlate with questionnaire results if our hypothesis is correct. 

Because our metrics are in bits, rather than time, it should be possible to compare both capacity and error between different tasks. Analyzing different tasks for a signal-to-noise ratio parameter that can be measured, and comparing results from empirical investigations of the relationship between SNR and both capacity and error, would further evidence this assertion. E.g., users could complete different tasks while wearing HRV monitors. Left-over capacity metrics should correlate not just with HRV data, but also between tasks, if our hypothesis is correct. We consider such empirical work a future direction and invite others to help investigate this promising area of research.

\section{Conclusions}
Neuromorphic Human-Computer Interaction aims to design better user interfaces using strategies that more closely resemble the neurophysiology of the brain. Interactive Inference applies the neuroscience theory of Active Inference to user interface design, postulating that users perform Bayesian inference on current progress and goal distributions to learn tasks and predict their next action. The Bayesian surprise (relative entropy) between goal and progress distributions can be modeled as a square function. However, human capacity to process Bayesian surprise follows a logarithmic function. This explains why errors occur when the user runs out of capacity. We believe our model may allow the quantitative analysis of performance and error in a single framework that allows estimation of the mental load of a task and showed how three basic laws of HCI, Hick's Law, Fitts' Law and the Power Law of Practice can be derived from Interactive Inference. We evaluated the use of the model in an empirical study. In our simulated car following task, driver information processing capacity during braking events was linearly related, with good fit, to a logarithmic model of the noise-to-signal ratio of the distance to a lead car, plus 1 (in bits). We found a significant effect of both distance $S$ and variation $N$ on capacity, with a mean driver Index of Performance of almost 4 bits of difficulty (as defined by $log_2(N/S + 1))$ per bit of information. Participants' qualitative ratings followed the same trend, with significantly higher ratings of difficulty with higher $N/S$ ratios. We used our model to derive a remaining capacity for the driver, by subtracting the log-linear curve from mean capacity, in bits. We also fit error (collision) rates, with excellent fit, to the square of the signal-to-noise ratio of the distance to the lead car, in bits. 
We hypothesize that collisions are due to unprocessed information where driver capacity is exceeded. Overall, the empirical results provide support for our Interactive Interference model of HCI. While car-following is just one case study, it provides a framework for conducting further empirical research in Interactive Inference, and evidence that such research is well worth doing.  We invite other researchers to test and apply this model to other tasks, to see how well these models predict human performance, and to confirm or refute the general applicability of Neuromorphic Human-Computer Interaction in general, and Interactive Inference in particular.

\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{references}
\end{document}
