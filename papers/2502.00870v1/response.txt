\section{Related Work}
\label{sec 2 related work}
%\subsection{Heterogeneity in FedRL}
%The heterogeneity in FedRL mainly manifests in two aspects: environmental heterogeneity and agent heterogeneity (model heterogeneity). Most of the research related to heterogeneity focuses on environment heterogeneity: Jin et al. **Jin, QAvg and PAvg for Heterogeneous Environments with Different State Transitions** proposed QAvg and PAvg for heterogeneous environments with different state transitions and provided convergence analysis. Zhang et al. **Zhang, Accelerating Learning Processes by Leveraging Information from Agents in Other Heterogeneous Environments** demonstrated theoretically that agents could accelerate their learning processes by leveraging information from agents in other heterogeneous environments. Regarding agent heterogeneity, Fan et al. **Fan, FedHQL: A Federated Q-Learning Algorithm for Black-Box Settings** studied the problem of agent heterogeneity under black-box settings and proposed the FedHQL algorithm. However, this algorithm is not applicable to policy gradient methods, and its provision for the server to access clients at any time poses significant limitations in real-world applications. In this paper, we explore the problem of agent heterogeneity in federated policy gradient under black-box settings and develop more generalized methods.

%\subsection{Federated with Model Heterogeneity}
%The demand for personalized model customization has led to the emergence of model heterogeneous federated learning. KD is widely applied in FL to solve such problems, where a public data set is used to reach a consensus and enable effective collaborative learning. Li et al. **Li, Communication Between Heterogeneous Clients using Labeled Public Datasets as a Bridge** successfully achieved communication between heterogeneous clients by using labeled public datasets as a bridge. Lin et al. **Lin, Ensemble Distillation using Unlabeled Data for Robust Federated Model Fusion** performed ensemble distillation using unlabeled data, achieving robust federated model fusion. Zhu et al. **Zhu, Data-Free Knowledge Distillation Scheme** proposed a data-free KD scheme that does not rely on any external data. Without designing additional shared network structures, Huang et al. **Huang, Instance Similarity Distribution Alignment on Unlabeled Public Data using KD** aligned the instance similarity distributions on the unlabeled public data. Inspired by the successful application of KD in addressing model heterogeneity in FL, this research focuses on introducing KD into FedRL to address agent heterogeneity.