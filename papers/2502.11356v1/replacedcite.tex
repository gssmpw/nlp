\section{Related Work}
In this section, we briefly summarize several research directions that are most relevant to ours.

\paragraph{Instruction Following in Language Models.}
Instruction following capabilities are crucial for improving LLM performance and ensuring safe deployment. Recent advances in instruction tuning have demonstrated significant progress through various methods ____. However, capable models still struggle with hard-constrained tasks ____ and lengthy generations____. Some studies find that instruction following can be improved with in-context few-shot examples____, optimal instruction positions____, carefully selected instruction-response pairs with fine-tuning____, and adaptations____. Unfortunately, the mechanistic understanding of how LLMs internally represent and process these instructions remains limited. 
%Recently, \todo{see how to extend the wrok IMPROVING INSTRUCTION-FOLLOWING IN LANGUAGE  MODELS THROUGH ACTIVATION STEERING}

\paragraph{Language Model Representations.}
A body of research have focused on studying the linear representation of concepts in representation space____. The basic idea is to find a direction in the space to represent the related concept. This can be achieved using a dataset with positive and negative samples relevant to concepts. Existing approaches computing the concept vectors include probing classifiers____, mean difference____, mean centering____, gaussian concept subspace____, which provide a rich set of tools to derive concept vectors. The derived concept vectors represent various high-level concepts such as honesty____, truthfulness____, harmfulness____, and sentiments____.

\paragraph{Sparse Autoencoders.} Dictionary learning is effective in disentangling features in superposition without representation space. Sparse autoencoder (SAE) offers a feasible way to map representations into a higher-dimension space and reconstruct to representation space. Various SAEs have been proposed to improve their performance such as vallina SAEs____, TopK SAEs____. Based on them, a range of sparse autoencoders (SAEs) have been trained to interpret hidden representations including Gemma Scope____ and Llama Scope____. These SAEs have also been used to interpret models' representational output____ and understand their abilities____.

\paragraph{Activation Steering.}
Recently, a body of research has utilized concept vectors to steer model behaviors during inference. Specifically, concepts vectors can be computed with diverse approaches, and these vectors are mostly effective on manipulating models generating concept-relevant text. For instance, many studies find it useful in improving truthfulness____ and safety____, mitigating sycophantic and biases____. Steering primarily operates in the residual stream following methods defined in Eq.~\eqref{eq:steer}, but it is worth-noting that the steering vectors can be computed from either residual stream representations or SAEs. Existing work mostly concentrates on computing with residual stream representations, which provide limited insights on what finer features contribute to the high-level concept vector. This coarse approach could further limit our deeper understanding on more complicated vectors such as instructions. In our work, we aim to bridge this gap by studying instruction vectors with SAEs to uncover their working mechanism.


% \paragraph{Model Steering via Activation Editing.}
% The generation of language models can be controlled by directly editing activation values during inference ____. Recent work has demonstrated success in steering models to be more honest ____, sycophantic ____, or to display different styles ____. Most relevant to our work, ____ discovered that model refusal can be controlled through a single direction. Similarly to some of these works ____, we compute steering vectors based on input pairs that differ by a specific feature. However, while previous studies have focused on high-level concepts like sentiment and style, we focus on lower-level, hard constraints defined through natural language instructions.
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\columnwidth]{modeldiff.pdf}
    \caption{Visualization of steering vectors extracted from LLaMA-3.1-8B and Gemma-2-9B for French translation task. The y-axis denotes the ratio between the standard deviation and mean of feature activation strengths.}
    \label{fig:cross-model}
\end{figure}