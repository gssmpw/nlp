\section{Related Work}
In this section, we briefly summarize several research directions that are most relevant to ours.

\paragraph{Instruction Following in Language Models.}
Instruction following capabilities are crucial for improving LLM performance and ensuring safe deployment. Recent advances in instruction tuning have demonstrated significant progress through various methods \cite{ouyang2022training,sanh2022multitask,wei2022finetuned,chung2024scaling}. However, capable models still struggle with hard-constrained tasks \cite{sun2023evaluating} and lengthy generations\cite{li2024measuring}. Some studies find that instruction following can be improved with in-context few-shot examples~\cite{kung2023models}, optimal instruction positions~\cite{liu2023instruction}, carefully selected instruction-response pairs with fine-tuning~\cite{zhou2024lima}, and adaptations~\cite{hewitt2024instruction}. Unfortunately, the mechanistic understanding of how LLMs internally represent and process these instructions remains limited. 
%Recently, \todo{see how to extend the wrok IMPROVING INSTRUCTION-FOLLOWING IN LANGUAGE  MODELS THROUGH ACTIVATION STEERING}

\paragraph{Language Model Representations.}
A body of research have focused on studying the linear representation of concepts in representation space~\cite{kim2018interpretability}. The basic idea is to find a direction in the space to represent the related concept. This can be achieved using a dataset with positive and negative samples relevant to concepts. Existing approaches computing the concept vectors include probing classifiers~\cite{belinkov2022probing}, mean difference~\cite{rimsky2024steering,zou2023representation}, mean centering~\cite{jorgensen2024improving}, gaussian concept subspace~\cite{zhao2025beyond}, which provide a rich set of tools to derive concept vectors. The derived concept vectors represent various high-level concepts such as honesty~\cite{li2024inference}, truthfulness~\cite{tigges2023linear}, harmfulness~\cite{zou2023representation}, and sentiments~\cite{zhao2025beyond}.

\paragraph{Sparse Autoencoders.} Dictionary learning is effective in disentangling features in superposition without representation space. Sparse autoencoder (SAE) offers a feasible way to map representations into a higher-dimension space and reconstruct to representation space. Various SAEs have been proposed to improve their performance such as vallina SAEs~\cite{sharkey2022sae}, TopK SAEs~\cite{gao_scaling_2024}. Based on them, a range of sparse autoencoders (SAEs) have been trained to interpret hidden representations including Gemma Scope~\cite{lieberum2024gemma} and Llama Scope~\cite{he2024llama}. These SAEs have also been used to interpret models' representational output~\cite{kissane2024interpreting} and understand their abilities~\cite{ferrando2025do}.

\paragraph{Activation Steering.}
Recently, a body of research has utilized concept vectors to steer model behaviors during inference. Specifically, concepts vectors can be computed with diverse approaches, and these vectors are mostly effective on manipulating models generating concept-relevant text. For instance, many studies find it useful in improving truthfulness\cite{marks2023geometry} and safety~\cite{arditi2024refusal}, mitigating sycophantic and biases~\cite{zou2023representation}. Steering primarily operates in the residual stream following methods defined in Eq.~\eqref{eq:steer}, but it is worth-noting that the steering vectors can be computed from either residual stream representations or SAEs. Existing work mostly concentrates on computing with residual stream representations, which provide limited insights on what finer features contribute to the high-level concept vector. This coarse approach could further limit our deeper understanding on more complicated vectors such as instructions. In our work, we aim to bridge this gap by studying instruction vectors with SAEs to uncover their working mechanism.


% \paragraph{Model Steering via Activation Editing.}
% The generation of language models can be controlled by directly editing activation values during inference \cite{dathathri2020plug}. Recent work has demonstrated success in steering models to be more honest \cite{li2024inference}, sycophantic \cite{rimsky2024steering}, or to display different styles \cite{turner2023activation,rimsky2024steering}. Most relevant to our work, \citet{arditi2024refusal} discovered that model refusal can be controlled through a single direction. Similarly to some of these works \cite{burns2023discovering,turner2023activation}, we compute steering vectors based on input pairs that differ by a specific feature. However, while previous studies have focused on high-level concepts like sentiment and style, we focus on lower-level, hard constraints defined through natural language instructions.
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\columnwidth]{modeldiff.pdf}
    \caption{Visualization of steering vectors extracted from LLaMA-3.1-8B and Gemma-2-9B for French translation task. The y-axis denotes the ratio between the standard deviation and mean of feature activation strengths.}
    \label{fig:cross-model}
\end{figure}