\section{Loss prediction for multiple losses}
\label{sec:multiple-loss}

Up to this point, our discussion has focused on loss prediction for a single, predetermined loss function. However, in real-world applications, multiple stakeholders may use a predictor, each with unique objectives and priorities that correspond to different loss functions. This scenario would require training separate loss predictors for each user to meet their individual needs.

The self-entropy predictor offers a key advantage: it can be computed for any loss function using only the predictions $p(x)$, eliminating the need for additional training. Moreover, by extending the result of Theorem~\ref{thm:mc-conv-vs-loss-pred}, we can define a class test functions $\calC$ such that when $p$ is multicalibrated with respect to $\calC$, its self-entropy predictions simultaneously compete with the best-in-class loss predictor for each loss in a rich class of losses $\calL$, rather than just a fixed loss. We formalize this in the following lemma, which we prove in Appendix~\ref{sec:many-losses-mc-pf}:

\begin{lemma}\label{lem:many-losses-mc}
    Let $\calF$ be a class of loss predictors $f: \Phi \rightarrow [0, 1]$. Let $\calL$ be a class of bounded proper losses $\ell: \{0, 1\} \times [0, 1] \rightarrow [0, 1]$ with associated concave entropy functions $H_{\ell}: [0, 1] \rightarrow [0,1]$, and let $\calC_{\calL}$ be the class of test functions 
    \[\calC_{\calL} = \{(f(\phi(p, x)) - H_{\ell}(p(x)))H'_{\ell}(p(x)) : f \in \calF, \ell \in \calL\}.\]
    Then,
    \[\max_{\ell \in \calL}\max_{\lossPred \in \calF} \adv(\lossPred) \leq 2\MCE(\calC_{\calL}, p).\]
    I.e., no loss predictor from $\calF$ for any loss $\ell \in \calL$ can obtain better advantage than $2\MCE(\calC_{\calL}, p)$ over the self-entropy predictor. 
\end{lemma}

When $\calL$ is the set of all proper losses, the form of multicalibration imposed by $\calC_{\calL}$ can be thought of as the extension to multicalibration of the notion of \emph{proper calibration}, recently proposed by~\cite{OKK25}. The proper calibration error of a predictor $p$ is defined as 

\[\text{PCE}(p) = \max_{\ell \in \calL_{\text{prop}}}\left|\ex[H'_{\ell}(p(x))(y - p(x))]\right|\]
where $\calL_{\text{prop}}$ denotes the set of proper losses. Our condition can be thought of as ``proper multicalibration'' where each test function consists of $H_{\ell}'(p(x))$ multiplied with an additional test function $\delta(\phi(p, x))$, that may depend on other features in addition to the prediction value. 

\subsection{Achieving efficient multicalibration for many losses}

As the class of losses we consider expands, training an effective loss predictor for each individual loss becomes increasingly challenging. This section demonstrates that in certain scenarios, it is possible to efficiently produce a multicalibrated predictor with respect to the class of tests outlined in Lemma~\ref{lem:many-losses-mc}, even for some infinite classes of losses. This approach allows us to learn a single predictor $p$ whose self-entropy estimates can compete with the best $\lossPred \in \calF$ for every 
$\ell \in \calL$, thus eliminating the need to train separate predictors for each loss.

This result relies on the existence of a ``finite approximate basis'' (Definition~\ref{def:finite-approx-basis}) for the class of functions $\{H_{\ell}'\}_{\ell \in \calL}$, and is inspired by the techniques of~\cite{OKK25}, who use a similar approach to show the efficiency of proper calibration when $\{H_{\ell}'\}_{\ell \in \calL}$ has a finite approximate basis. 

We show a general version of this result in Theorem~\ref{thm:general-approx-basis-mc}, and instantiate it here for the class of 1-Lipschitz proper losses, $\calL_{Lip}$. 

The instantiation relies on a result proved by \cite{OKK25}, who show that $\{H_{\ell}'\}_{\ell \in \calL_{Lip}}$ has such a finite basis. We show efficiency in terms of oracle access to a weak-agnostic-learner for $\calF$, the class of loss predictors. We motivate this assumption by observing that if we care about learning a loss predictor from the class $\calF$, it's reasonable to assume that we have access to a weak agnostic learner for $\calF$. We formally define a weak agnostic learner as follows. 

\begin{definition}[Weak agnostic learner]\label{def:weak-agnostic-learner}
    Let $\alpha \geq 0$, $\delta \geq 0$. An $\alpha$-weak agnostic learner for $\calF \subseteq \{f: \Phi \rightarrow [-1, 1]\}$, closed under negation, with sample complexity $n$ and failure parameter $\delta$ is an algorithm that when given $n$ samples from a distribution $\calU$ over $\Phi \times [-1, 1]$, outputs $f \in \calF \cup \{\bot\}$ such that with probability at least $1 - \delta$ over the samples from $\calU$ and the randomness in the algorithm itself, if $\max_{f \in \calF} \ex_{(\phi, z) \sim \calU}[f(\phi)z] \geq \alpha,$
    the algorithm returns a $f \in \calF$ such that 
    $\ex_{(\phi, z) \sim \calU}[f(\phi)z] \geq \alpha/2.$
    Otherwise, if for all $f \in \calF$, 
    $\ex_{(\phi, z) \sim \calU}[f(\phi)z] \leq \alpha,$ the algorithm either returns $f = \bot$ or $f \in \calF$ such that $\ex_{(\phi, z) \sim \calU}[f(\phi)z] \geq \alpha/2.$
\end{definition}

With this definition in hand, we are ready to present the main theorem of this section. The proof can be found in Appendix~\ref{sec:1-lip-mc-pred-pf}.

\begin{theorem}\label{thm:1-lip-mc-pred}
    Fix $\delta, \epsilon > 0$. Let $\calL_{Lip}$ be the class of proper 1-Lipschitz losses $\ell:\{0, 1\} \times [0, 1] \rightarrow [0, 1]$, and let $\calF$ be a class of loss predictors $\calF: \Phi \rightarrow [-1, 1]$ that is closed under negation and contains the class of self entropy predictors, $\calH_{\calL_{Lip}} = \{H_{\ell}\}_{\ell \in \calL_{Lip}}$. Further assume that we have access to an $\alpha$-weak-agnostic-learner for $\calF$ with sample complexity $n$ and failure parameter $\beta \leq \frac{\alpha^2\delta}{4\lceil 2/\epsilon + 1\rceil}$. 

    Then, there exists an algorithm that, given $m = O(n/\alpha^2)$ samples, with probability at least $1 - \delta$ outputs a predictor $p$ such that 
    \[\max_{\ell \in \calL_{Lip}} \max_{\lossPred \in \calF} \adv(\lossPred) \leq 16\alpha + 4\epsilon.\]
\end{theorem}

In other words, our learned $p$'s self-entropy predictions compete with the best-in-class loss predictor with \emph{every} $\ell \in \calL_{Lip}$, up to an error of $16\alpha + 4\epsilon$. 







