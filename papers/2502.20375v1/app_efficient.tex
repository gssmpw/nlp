\section{Extended discussion and proofs from Section~\ref{sec:multiple-loss}}


\subsection{Proof of Lemma~\ref{lem:many-losses-mc}}\label{sec:many-losses-mc-pf}
\begin{proof}[Proof of Lemma~\ref{lem:many-losses-mc}]
    For a fixed loss $\ell \in \calL$, let 
    \[\calC_{\ell} = \{(f(\phi(p, x)) - H_{\ell}(p(x)))H'_{\ell}(p(x)) : f \in \calF\}.\]

    By Theorem~\ref{thm:mc-conv-vs-loss-pred}, we can guarantee that 
    \[\max_{\lossPred \in \calF} \adv(\lossPred) \leq 2\MCE(\calC_{\ell}, p).\]
    Taking the max over $\calL$ for both sides, we get
    \[\max_{\ell \in \calL}\max_{\lossPred \in \calF} \adv(\lossPred) \leq \max_{\ell \in \calL} 2\MCE(\calC_{\ell}, p) \leq 2\MCE(\calC_{\calL}, p).\]

    Where the right-most inequality follows from the fact that $\calC_{\calL} = \bigcup_{\ell \in \calL} \calC_{\ell}$. This proves the desired inequality.
\end{proof}

\subsection{Multicalibration for product classes}

In this section, we introduce some useful notation that we will use to refer to and relate certain classes of multicalibration test functions. 

\begin{definition}\label{def:prod-class}
    Let $\calA \subseteq \{a: \Phi \rightarrow [-1, 1]\}$ and $\calB \subseteq \{b: \Phi \rightarrow [-1, 1]\}$ be two classes of functions. We denote the product class of test functions with respect to $\calA$ and $\calB$ as $\calC_{\calA, \calB}$, and define it as 
    \[\calC_{\calA, \calB} = \{a(\phi)b(\phi) : a \in \calA, b \in \calB\}.\]
\end{definition}

We use this notation in the following lemma, which shows that multicalibration with respect to the test functions $\calC_{\calL}$ defined in Lemma~\ref{lem:many-losses-mc} is implied by multicalibration with respect to a product class of test functions:

\begin{lemma}\label{lem:loss-to-prod-class-mc}
    Let $\calF$ be a class of loss predictors $f: \Phi \rightarrow [0, 1]$. Let $\calL$ be a class of bounded proper losses $\ell: \{0, 1\} \times [0, 1] \rightarrow [0, 1]$ with associated concave entropy functions $H_{\ell}: [0, 1] \rightarrow [0,1]$, and let $\calC_{\calL}$ be the class of test functions 
    \[\calC_{\calL} = \{(f(\phi(p, x)) - H_{\ell}(p(x)))H'_{\ell}(p(x)) : f \in \calF, \ell \in \calL\}.\]

    Denote $\calH_{\calL} = \{H_{\ell} : \ell \in \calL\}$, and $\calH'_{\calL} = \{H'_{\ell}: \ell \in \calL\}$.

    Then, 

    \[\MCE(\calC_{\calL},p) \leq \MCE(\calC_{\calF, \calH'_{\calL}}, p) + \MCE(\calC_{\calH_{\calL}, \calH'_{\calL}}, p) \leq 2\MCE(\calC_{\calF\cup \calH_{\calL}, \calH'_{\calL}}, p).\]
\end{lemma}

\begin{proof}
    By definition of $\calC_{\calL}$, we can readily decompose the multicalibration error into the desired terms:

    \begin{align*}
        \MCE(\calC_{\calL},p) &= \max_{f \in \calF, \ell \in \calL}\left|\ex[(f(\phi(p, x)) - H_{\ell}(p(x)))H'_{\ell}(p(x))(y - p(x))]\right|\\
        &\leq \max_{f \in \calF, \ell \in \calL}\left|\ex[f(\phi(p, x))H'_{\ell}(p(x))(y - p(x))]\right| + \max_{\ell \in \calL}\left|\ex[H_{\ell}(p(x))H'_{\ell}(p(x))(y - p(x))]\right|\\
        &\leq \max_{f \in \calF, \ell \in \calL}\left|\ex[f(\phi(p, x))H'_{\ell}(p(x))(y - p(x))]\right| + \max_{h \in \calH_{\calL}, h' \in \calH'_{\calL}}\left|\ex[h(p(x))h'(p(x))(y - p(x))]\right|\\
        &= \MCE(\calC_{\calF, \calH'_{\calL}}, p) + \MCE(\calC_{\calH_{\calL}, \calH'_{\calL}}, p).
    \end{align*}

    This proves the left-hand inequality. The right-hand inequality follows from the observation that \[\MCE(\calC_{\calF \cup \calH_{\calL}, \calH'_{\calL}}, p) = \max\{\MCE(\calC_{\calF, \calH'_{\calL}}, p), \MCE(\calC_{\calH_{\calL}, \calH'_{\calL}}, p)\}.\]
\end{proof}

An important property of product classes is that given two classes $\calA$ and $\calB$, whenever we have a weak learner for $\calA$ and $\calB$ is finite, we can efficiently learn a multicalibrated predictor with respect to $\calC_{\calA, \calB}$. Our approach closely follows that of~\cite{gopalan2022low}, who show how to learn multicalibrated predictors for product classes where one class depends only on $x$, and the other depends only on $p(x)$. Despite this choice in setup, their particular algorithm and results naturally generalize to the case where the two function classes can have richer input spaces. 

For completeness, we translate their algorithm and results to our setting. The algorithm for product-class multicalibration can be found in Algorithm~\ref{alg:product-class} (c.f. Algorithm 1 of~\cite{gopalan2022low}).

The following lemmas prove correctness and sample complexity of the algorithm. 

\begin{lemma}[Correctness of Algorithm~\ref{alg:product-class}]
    If Algorithm~\ref{alg:product-class} returns a predictor $p : \mathcal{X} \to [0, 1]$, then $p$ satisfies 
    \[\MCE(\calC_{\calA, \calB}, p) \leq \alpha.\]
\end{lemma}

\begin{proof}
    Observe that Algorithm~\ref{alg:product-class} only returns a predictor $p_t$ if, in the $t$th iteration, for every $b \in \calB$, the call to the weak agnostic learner $\text{WAL}_{\calA}$ returns $\bot$. By the weak agnostic learning property (Definition~\ref{def:weak-agnostic-learner}), returning $\bot$ in every call indicates that for all $b \in \calB$ and for all $a \in \calA$,
    
    \[
    \ex\left[a(\phi(p_t, x))b(\phi(p_t, x))(y - p_t(x)) \right] \leq \alpha.
    \]
    
    By definition, this means that $\MCE(\calC_{\calA, \calB}, p_t) \leq \alpha$
\end{proof}

\begin{lemma}[Termination of Algorithm~\ref{alg:product-class}]
    Algorithm~\ref{alg:product-class} is guaranteed to terminate and return a $p_T$ after $T \leq 4/\alpha^2$ iterations.
\end{lemma}

\begin{proof}
    We show that the number of iterations is bounded via a potential argument, where the potential function is the squared error of the current predictor $p$ from the bayes-optimal predictor $p^*(x) = \ex[y|x]$: $\sqLoss(p) := \ex[(p(x) - p^*(x))^2]$. 

    Note that by definition, $\sqLoss(p_0) \leq 1$, and for all $p: \calX \rightarrow [0, 1]$, $\sqLoss(p) \geq 0$. 

    The change in potential after the $t$th update can be computed as follows. Note that due to the guarantee of the weak agnostic learner, because $a_{t + 1} \neq \bot$, we are guaranteed that for the $b \in \calB$ used in the update, 
    \[\ex[b(\phi(p, x))a(\phi(p, x))(y - p(x))] \geq \alpha/2.\]

    Thus, following the same proof as Lemma~\ref{lem:improve-2}, we conclude that 
    \begin{align*}
        \sqLoss(p_t) - \sqLoss(p_{t + 1}) &\geq \ex[(p^*(x) - p_t(x))^2] - \ex[(p^*(x) - p_t(x) -\frac{\alpha}{2}\delta_{t + 1}(x))^2]\\
        &= \alpha \ex[(p^*(x) - p_t(x))\delta_{t + 1}(x)] - \frac{\alpha^2}{4}\ex[\delta_{t + 1}(x)^2]\\
        &= \alpha \ex[(p^*(x) - p_t(x))a_{t + 1}(\phi(p_t, x))b(\phi(p_t, x))] - \frac{\alpha^2}{4}\ex[a_{t + 1}(\phi(p_t, x))^2b(\phi(p_t, x))^2]\\
        &\geq \alpha^2/2 - \alpha^2/4\\
        &= \alpha^2/4.
    \end{align*}

    Thus, the potential function decreases by at least $\alpha^2/4$ in each round, and since $\sqLoss(p_0) \leq 1$ and $\sqLoss(p_t) \geq 0$ for all $t$, the total number of iterations is bounded by $T < 4/\alpha^2$.
    \end{proof}

We finally turn to the sample complexity and success probability of Algorithm~\ref{alg:product-class}. 

\begin{lemma}\label{lem:finite-B-mc-alg}
    Let $\alpha, \delta > 0$. Let $\calA \subseteq \{a: \Phi \rightarrow [-1, 1]\}$ and $\calB \subseteq \{b: \Phi \rightarrow [-1, 1]\}$ be two classes of functions, where we assume $\calA$ is closed under negation, and $\calB$ is finite. Suppose we have access to an $\alpha$-weak-agnostic-learner for $\calA$ with sample complexity $n$ and failure parameter $\beta \leq \frac{\alpha^2\delta}{4|\calB|}$. 

    Then, given $m = O(n/\alpha^2)$ samples, with probability at least $1 - \delta$ Algorithm~\ref{alg:product-class} outputs a predictor $p$ with $\MCE(\calC_{\calA, \calB}, p) \leq \alpha$. 
\end{lemma}

\begin{proof}
    We assume that we use a fresh sample for the weak agnostic learner at each iteration of size $n$. Because the algorithm terminates in at most $4/\alpha^2$ iterations, we thus need at most $4n/\alpha^2 = O(n/\alpha^2)$ fresh samples. 

    For the failure bound, note that we make at most $4|\calB|/\alpha^2$ calls to the weak agnostic learner. Via a union bound, because we assume that $\beta \leq \frac{\alpha^2\delta}{4|\calB|}$, we conclude that the probability that at least one of the calls to the weak learner fails is bounded by $\delta$, as desired.   
\end{proof}

\begin{algorithm}
    \caption{Product-Class Multicalibration}
    \begin{algorithmic}[1]\label{alg:product-class}
        \STATE \textbf{Input:} training data $\{(x_i, y_i)\}_{i=1}^m$
        \STATE \textbf{Function classes:} \\
        \quad $\calA \subseteq \{a: \Phi \to [-1, 1]\}$ (closed under negation), \\
         \quad $\calB \subseteq \{b: \Phi \to [-1, 1]\}$ (finite)
        \STATE \textbf{$\alpha$-Weak Agnostic Learner for $\calA$:} $\text{WAL}_{\calA}$
        \STATE approximation $\alpha > 0$
        \STATE \textbf{Output:} $(\calC_{\calA, \calB},\alpha)$-multicalibrated predictor $p: \mathcal{X} \to [0,1]$
        \STATE $p_0(\cdot) \gets 1/2$
        \STATE $mc \gets \text{false}$
        \STATE $t \gets 0$
        \WHILE{$\neg mc$}
            \STATE $mc \gets \text{true}$
            \FOR{each $b \in \calB$}
                \STATE $a_{t+1} \gets \text{WAL}_{\calA}(\{(\phi(p, x_i), b(\phi(p_t, x_i))(y_i - p_t(x_i)))\}_{i=1}^m)$
                \IF{$a_{t+1} = \bot$}
                    \STATE \textbf{continue}
                \ELSE
                    \STATE $\delta_{t+1}(\cdot) \gets b(\phi(p_t, \cdot)) a_{t+1}(\phi(p_t, \cdot))$
                    \STATE $p_{t+1}(\cdot) \gets \Pi_{[0,1]}(p_t(\cdot) + \frac{\alpha}{2} \delta_{t+1}(\cdot)) $ 
                    \STATE $mc \gets \text{false}$
                    \STATE $t \gets t + 1$
                    \STATE \textbf{break}
                \ENDIF
            \ENDFOR
        \ENDWHILE
        \STATE \textbf{return} $p_t$
    \end{algorithmic}
\end{algorithm}

\subsection{Multicalibration for classes with approximate bases}

In this section, we show that Algorithm~\ref{alg:product-class} can also be used to guarantee multicalibration for product classes when $\calB$ is not finite, but has a finite approximate basis. 

\begin{definition}[Finite Approximate Basis]\label{def:finite-approx-basis}
     Let $\Gamma$ be a set and $\mathcal{F} = \{f : \Gamma \to [-1, 1]\}$ a class of functions on $\Gamma$. We say that a set $\mathcal{G} = \{g : \Gamma \to [-1, 1]\}$ is a finite $\epsilon$-basis for $\mathcal{F}$ of size $d$ and coefficient norm $\lambda$, if $\calG = \{g_1, ..., g_d\}$, and for every function $f \in \mathcal{F}$, there exist coefficients $\alpha_1, \alpha_2, \ldots, \alpha_d \in [-1,1]$ satisfying
\begin{equation}
    \forall x \in \Gamma \quad \left| f(x) - \sum_{i=1}^{d} \alpha_i g_i(x) \right| \leq \epsilon \quad \text{and} \quad \sum_{i=1}^{d} |\alpha_i| \leq \lambda. 
\end{equation}
\end{definition}

\begin{lemma}\label{lem:basis-mce-bound}
    Let $\calA \subseteq \{a: \Phi \rightarrow [-1, 1]\}$ and $\calB \subseteq \{b: \Phi \rightarrow [-1, 1]\}$ be two classes of functions. Suppose that $\calB$ has a finite approximate $\epsilon$-basis $\calG$ with coefficient norm $\lambda$. Then, for any predictor $p: \calX \rightarrow [0, 1]$, 
    \[\MCE(\calC_{\calA, \calB}, p) \leq \lambda\MCE(\calC_{\calA, \calG}, p) + \epsilon\]
\end{lemma}

\begin{proof}
    The upper bound quickly follows from expanding the definition of $\MCE(\calC_{\calA, \calB}, p)$. Note that 

    \begin{align*}
        \MCE(\calC_{\calA, \calB}, p) &= \max_{a \in \calA, b \in \calB}\left|\ex[a(\phi(p, x))b(\phi(p, x))(y - p(x))]\right|\\
        &\leq \max_{a \in \calA, b \in \calB}\left|\ex[a(\phi(p, x))\left(\sum_{i = 1}^d g_i(\phi(p, x))\alpha_i(b)\right)(y - p(x))]\right| + \epsilon\\
        &\leq \max_{a \in \calA, g \in \calG}\lambda\left|\ex[a(\phi(p, x))g(\phi(p, x))(y - p(x))]\right| + \epsilon\\
        &= \lambda\MCE(\calC_{\calA, \calG}, p) + \epsilon
    \end{align*}
\end{proof}

Thus, we get the following immediate Corollary of Lemmas~\ref{lem:finite-B-mc-alg} and \ref{lem:basis-mce-bound}:

\begin{corollary}\label{cor:approx-basis-alg}
    Let $\alpha, \delta > 0$. Let $\calA \subseteq \{a: \Phi \rightarrow [-1, 1]\}$ and $\calB \subseteq \{b: \Phi \rightarrow [-1, 1]\}$ be two classes of functions, where we assume $\calA$ is closed under negation, and $\calB$ has a finite approximate $\epsilon$-basis of size $d$ and coefficient norm $\lambda$. Suppose we have access to an $\alpha$-weak-agnostic-learner for $\calA$ with sample complexity $n$ and failure parameter $\beta \leq \frac{\alpha^2\delta}{4d}$. 

    Then, there exists an algorithm that, given $m = O(n/\alpha^2)$ samples, with probability at least $1 - \delta$ outputs a predictor $p$ with $\MCE(\calC_{\calA, \calB}, p) \leq \lambda\alpha + \epsilon$. 
\end{corollary}

\subsection{Instantiating multicalibration for loss prediction}

We are finally ready to show that we can use multicalibration to learn a predictor with accurate self-entropy predictions for any loss in a rich class. 

\begin{theorem}\label{thm:general-approx-basis-mc}
    Fix $\delta \geq 0$. Let $\calL$ be some class of bounded proper losses, and let $\calF$ be a class of loss predictors $\calF: \Phi \rightarrow [-1, 1]$ that is closed under negation and contains the class of self entropy predictors, $\calH_{\calL} = \{H_{\ell}\}_{\ell \in \calL}$. Suppose that the class of functions $\calH'_{\calL} =\{H'_{\ell}\}_{\ell \in \calL}$ has a finite approximate $\epsilon$-basis of size $d$ and coefficient norm $\lambda$. Further assume that we have access to an $\alpha$-weak-agnostic-learner for $\calF$ with sample complexity $n$ and failure parameter $\beta \leq \frac{\alpha^2\delta}{4d}$. 

    Then, there exists an algorithm that, given $m = O(n/\alpha^2)$ samples, with probability at least $1 - \delta$ outputs a predictor $p$ such that 
    \[\max_{\ell \in \calL} \max_{\lossPred \in \calF} \adv(\lossPred) \leq 4\lambda\alpha + 4\epsilon.\]
\end{theorem}

\begin{proof}
    The proof combines the helper lemmas we have proved in this section. 

    First, note that by Lemma~\ref{lem:many-losses-mc}, we are guaranteed that 
    \[\max_{\ell \in \calL} \max_{\lossPred \in \calF} \adv(\lossPred) \leq 2\MCE(\calC_{\calL}, p),\]
    where $\calC_{\calL}$ is defined as in Lemma~\ref{lem:many-losses-mc}. 

    By Lemma~\ref{lem:loss-to-prod-class-mc}, we can further guarantee that 
    \[\max_{\ell \in \calL} \max_{\lossPred \in \calF} \adv(\lossPred) \leq 4\MCE(\calC_{\calF \cup \calH_{\calL}, \calH'_{\calL}}, p) = 4\MCE(\calC_{\calF, \calH'_{\calL}}, p),\]
    where the last equality follows from our assumption that $\calF$ contains $\calH_{\calL}$.

    From here, we can now apply the result of Corollary~\ref{cor:approx-basis-alg}, which guarantees that given $m = O(n/\alpha^2)$ samples, we can output a predictor satisfying 
    $\MCE(\calC_{\calF, \calH'_{\calL}}, p) \leq \lambda\alpha + \epsilon$. 

    Substituting this bound into our upper bound, we conclude that we get a predictor satisfying 
    \[\max_{\ell \in \calL} \max_{\lossPred \in \calF} \adv(\lossPred) \leq 4\lambda\alpha + 4\epsilon.\]

    This completes the proof. 
\end{proof}

\subsection{Proof of Theorem~\ref{thm:1-lip-mc-pred}}\label{sec:1-lip-mc-pred-pf}

Theorem~\ref{thm:1-lip-mc-pred} instantiates the general result of Theorem~\ref{thm:general-approx-basis-mc} for a class $\calL$ that has a finite approximate basis: the class of all 1-Lipschitz proper losses. 

We appeal to a result proved by~\cite{OKK25}, which proves the existence of such a basis for this class:

\begin{lemma}[\cite{OKK25}, Lemma 5.4]\label{lem:1-lip-basis}
    Let $\epsilon > 0$, and let $\calL_{Lip}$ be the class of proper 1-Lipschitz loss functions $\ell: \{0, 1\} \times [0, 1] \rightarrow [0,1]$. The class $\{H'_{\ell}\}_{\ell \in \calL_{Lip}}$ admits an $\epsilon$-approximate basis of size $\lceil \frac{2}{\epsilon} + 1 \rceil$ and coefficient norm 4. 
\end{lemma}

with this Lemma in hand, we are ready to prove the theorem. 

\begin{proof}[Proof of Theorem~\ref{thm:1-lip-mc-pred}]
    We instantiate the basis from Lemma~\ref{lem:1-lip-basis}, and use this as the input to Theorem~\ref{thm:general-approx-basis-mc}.

    Because $d = \lceil 2/\epsilon + 1\rceil$, the bound on the weak-agnostic-learner's error probability becomes $\beta \leq \frac{\alpha^2\delta}{4\lceil 2/\epsilon + 1\rceil}$, and the resulting error guarantee gives us a predictor $p$ satisfying 

    \[\max_{\ell \in \calL_{Lip}} \max_{\lossPred \in \calF} \adv(\lossPred) \leq 16\alpha + 4\epsilon\]
    with probability at least $1 - \delta$. 
\end{proof}



