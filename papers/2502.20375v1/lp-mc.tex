\section{Loss prediction advantage and multicalibration auditing}
\label{sec:lp-mc}

In this section, we establish the relationship between learning loss predictors with good advantage, and auditing for multicalibration, i.e. finding a $c$ that witnesses a large multicalibration violation. The main result of our section is the following theorem, which establishes the correspondence between various levels of loss predictors and multicalibration requirements, when instantiated with the appropriate values $\phi(p, x)$:

\begin{theorem}\label{thm:mc-conv-vs-loss-pred}
    Let $\calF$ be a class of loss predictors $f: \Phi \rightarrow [0, 1]$. 
    Let $\calF' \supseteq \calF$ be the augmented function class defined as 
    \[\calF' = \{\Pi_{[0,1]}((1 - \beta)H_{\ell}(p(x)) + \beta f(\phi(p,x))) : \beta \in [-1, 1], f \in \calF\}.\]
    Let $\calC$ be a class of weight functions defined as 
    \[\calC = \{ (f(\phi(p, x)) - H_{\ell}(p(x)))H_{\ell}'(p(x)) : f \in \calF\}.\]
    Then, 
    \[\frac{1}{2}\max_{\lossPred \in \calF} \adv(\lossPred) \leq \MCE(\calC, p) \leq \sqrt{\max_{\lossPred \in \calF'} \adv(\lossPred)}.\]
\end{theorem}

The proof of Theorem~\ref{thm:mc-conv-vs-loss-pred} can be found in Appendix~\ref{sec:mc-conv-cs-loss-pred-pf} and follows from two key lemmas. Lemma~\ref{lem:adv-implies-mc-err} establishes the left-hand inequality by showing how a loss predictor with good advantage can be used to construct a witness of large multicalibration error. Conversely, Lemma~\ref{lem:mc-err-implies-adv} establishes the right-hand inequality by showing how to leverage a witness for large multicalibration error to construct a loss predictor with large advantage. 

Before presenting our main lemmas, we introduce two auxiliary claims that are well-known in the literature on boosting and gradient descent. We provide proofs here for completeness and notational consistency.


Let $\mD'$ be a distribution over $(x, z) \in X \times [0,1]$. Let $h_1, h_2: \X \to [0,1]$ be two hypotheses. Under what conditions does $h_2$ improve on $h_1$? The following lemma gives a necessary condition: the update $\delta(x) = h_2(x) - h_1(x)$ must be correlated with the residual errors $z - h_1(x)$ of the hypothesis $h_1$ under the distribution $\mD'$. 

 
\begin{claim}
\label{lem:improve-1}
    For two hypotheses $h_1, h_2$, 
    \[ \E_{\mD'}[(h_1(x) - z)^2] - \E[(h_2(x) - z)^2] \leq 2\E[(h_2(x) - h_1(x))(z - h_1(x))]. \]
\end{claim}
\begin{proof}
    Let us write $\delta(x) = h_2(x) - h_1(x)$. Then we have
    \begin{align*}
        \E[(h_1(x) - z)^2] - \E[(h_2(x) - z)^2] &= \E_{\mD'}[(h_1(x) -z)^2 - (h_1(x)  -z + \delta(x) )^2]\\
        &= -2\E[(h_1(x) - z)\delta(x)] - \E[\delta(x)^2]\\
        & \leq 2\E_{\mD}[(z - h_1(x))\delta(x)].
    \end{align*}
\end{proof}

Conversely, if we find an update $\delta(x)$ which is correlated with the residuals, we can perform a gradient descent update to reduce the squared error. We let $\Pi_{[0,1]}:\R \to [0,1]$ denote the projection operator onto the unit interval.

\begin{claim}
\label{lem:improve-2}    
If there exists $\delta:\X \to [-1,1]$ such that $\E_{\mD'}[\delta(x)(z - h_1(x))] \geq \beta \geq 0$, then setting $h_2(x) = \Pi_{[0,1]}(h_1(x)+ \beta \delta(x))$ gives
    \[ \E_{\mD'}[(h_1(x) - z)^2] - \E[(h_2(x) - z)^2] \geq \beta^2.\]
\end{claim}

\begin{proof}
    Without projection, we can write the gap in squared error as 
    \begin{align*}
        \E_{\mD'}[(h_1(x) - z)^2] - \E[(h_1(x) + \beta\delta(x) - z)^2] &=  2\beta \E_{\mD'}[(z - h_1(x))\delta(x)] - \beta^2\E[\delta(x)^2]\\
        &\geq 2\beta^2 - \beta^2 = \beta^2.
    \end{align*}
    While $h_1(x) + \beta \delta(x)$ may not be bounded in $[0,1]$, projection onto the interval can only further reduce the squared error.
\end{proof}

With these in hand, we show that any loss predictor with a non-trivial advantage points us to a failure of multicalibration. 

\begin{lemma}\label{lem:adv-implies-mc-err}
    Assume that $\lossPred$ achieves advantage $\alpha \geq 0$ over the self-entropy predictor. Then the function $\delta(\phi(p, x)) = \lossPred(\phi(p, x)) - \clp(p(x))$ satisfies
    \[ \E[\delta(\phi(p, x))H_{\ell}'(p(x))(y - p(x))] \geq \alpha/2.\]
    In other words, $\lossPred$ can be used to construct a witness $c(\phi(p, x)) = \delta(\phi(p, x))H_{\ell}'(p(x))$ for a multicalibration violation of magnitude $\alpha/2$. 
\end{lemma}
\begin{proof}
    Consider the loss regression problem, where we draw $(x,y) \in \X \times \zo \sim \mD$ and then return the pair $(x, z = \ell(y, p(x))$. We will use Claim \ref{lem:improve-1}, where we take $h_1 = \clp$ to be the self-entropy predictor and $h_2 = \lossPred$. 
    We can estimate the residual error of the self-entropy predictor as
    \begin{align}
    \label{eq:rewrite}
        \ell(y, p(x)) - \clp(p(x)) &= H_{\ell}(p(x)) + (y - p(x))H_{\ell}'(p(x)) - H_{\ell}(p(x))\notag\\
        &= (y - p(x))H_{\ell}'(p(x)).
    \end{align}
    By Claim \ref{lem:improve-1}, we have
    \begin{align*}
        \alpha &= \ex[(\ell(y, p(x)) - \clp(p(x)))^2] - \ex[(\ell(y, p(x)) - \lossPred(\phi(p, x))^2]\\
        &\leq 2\E[(\lossPred(\phi(p, x)) - \clp(p(x)))(\ell(y, p(x)) - \clp(p(x)))\\ &= 2\E[\delta(\phi(p, x))H_{\ell}'(p(x))(y - p(x))]
    \end{align*}
\end{proof}

Conversely to the result of Lemma~\ref{lem:adv-implies-mc-err}, we show that we can leverage certain types of multicalibration failures to predict loss with an advantage over the self-entropy predictor.

\begin{lemma}\label{lem:mc-err-implies-adv}
    Assume there exists a function $\delta:\Phi \to [-1,1]$ such that
    \[ \E[\delta(\phi(p, x))H_{\ell}'(p(x))(y - p(x))] \geq \beta \geq 0.\]
    i.e., the function $c(\phi(p, x)) = \delta(\phi(p, x))H_{\ell}'(p(x))$ is a witness for a multicalibration violation of magnitude $\beta$. Define the loss predictor
    \[ \lossPred(\phi(p, x)) = \Pi_{[0,1]}(\clp(p(x)) + \beta 
    \delta(\phi(p, x))).\] 
    Then $\adv(\lossPred) \geq \beta^2$. 
\end{lemma}
\begin{proof}
    We again consider the loss regression problem, 
    We now apply Lemma \ref{lem:improve-2} with $z = \ell(y, p(x))$, $h_1 = \clp$. The correlation condition we require is
    \[  \E[\delta(\phi(p, x))(\ell(y, p(x) - \clp(p(x)))] \geq \beta, \]
    By Equation \eqref{eq:rewrite}, we have
    \[ \E[\delta(\phi(p, x))(\ell(y, p(x)) - \clp(p(x)))] = \E[\delta(\phi(p, x))H_{\ell}'(p(x))(y - p(x))] \]
    which is at least $\beta$ by our assumption. Hence Claim \ref{lem:improve-2} implies that $h_2 = \lossPred$ has advantage $\beta^2$ over $\clp$.     
\end{proof}

