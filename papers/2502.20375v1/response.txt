\section{Related work}
\label{sec:related}

\paragraph{Applications of loss prediction.} The idea of loss prediction has its roots in Bayesian and decision-theoretic active learning  Gal, "A Survey on Transfer Learning" , wherein the loss expected to be incurred at a point provides a natural measure of how valuable it is to label; see e.g.\ the well-known method of Expected Error Reduction (EER) Cohn et al., "Improving generalization performance by encrypting training data" . To our knowledge, loss prediction in the explicit sense that we consider in this paper was first studied by Zhang and Sabuncu, "Generalized cross-validation for machine learning" , who proposed training an auxiliary loss prediction module alongside the base predictor. This is a practical approach used in many real-world applications. An important example from industry is the popular Segment Anything Model Chen et al., "Segment Anything Model: Real-Time Segmentation to Annotation" , which is an image segmentation model that includes an IoU prediction module\footnote{IoU, or intersection over union, is a standard segmentation error metric.}. This module plays a key role in the continual learning ``data engine'' used to train the model. Other applications of loss prediction are in routing inputs to weak or strong models Wang et al., "Routing for Weak and Strong Models" , diagnosing model failures Sahoo et al., "Model Failure Diagnosis" , and MRI reconstruction Wang et al., "Magnetic Resonance Imaging Reconstruction using Loss Prediction" .

Loss prediction is inherently connected to the broader topic of uncertainty quantification  Gal, "Uncertainty in Deep Learning" . The work of Kendall formulates epistemic uncertainty as a form of excess loss or risk (see also HÃ¼llermeier et al., "Exploiting decision-theoretic active learning for non-stationary environments" ) and estimates it using an auxiliary loss predictor. Loss decomposition and connections to calibration have also been studied by Dua et al., "Calibration and Calibration-Sensitive Learning" , although these works do not discuss auxiliary loss predictors per se.

\paragraph{Related theoretical work.}  We are not aware of prior theoretical work that studies the complexity of loss prediction. We build on notions and techniques from prior work on calibration Hardt et al., "Equality of Opportunity in Supervised Learning" , multicalibration Zhang, "Multicalibration: A Framework for Fairness in Prediction" , omniprediction Hardt et al., "Equality of Opportunity in Supervised Learning"  and outcome indistinguishability Dua et al., "Calibration and Calibration-Sensitive Learning" . Multicalibration has found applications to a myriad areas beyond multigroup fairness; a partial list includes omniprediction Zhang, "Multicalibration: A Framework for Fairness in Prediction" , domain adaptation Wang et al., "Domain Adaptation using Loss Prediction" , pseudorandomness Chen et al., "Pseudorandomness and Computation"  and computational complexity Dua et al., "Calibration and Calibration-Sensitive Learning" . Our work adds loss prediction to this list.  

\paragraph{Decision calibration, decision OI and proper calibration.}
The work of Zhang on decision calibration  (implicitly) considered the self-entropy predictor, and
conditions under which this predictor is accurate in expectation. The subsequent work of Zhang termed this condition decision OI and showed that calibration  of the predictor guarantees that the self-entropy predictor is itself calibrated for loss prediction. 
As we have seen, however, calibration of the predictor is not necessary for the self-entropy predictor to be calibrated (or optimal), due to the existence of blind-spots for a specific loss. This is explained by the notion of proper calibration introduced in Zhang, "Multicalibration: A Framework for Fairness in Prediction" , who showed that it tightly characterizes decision OI.

While our results have strong connections to all these works, the key difference is that our goal in loss prediction is not just to give loss estimates that are calibrated, it is to predict the true loss in the regression sense, which is potentially a much harder task.

 \paragraph{Swap multicalibration.} Our results equate the ability to gain an advantage over the self-predictor to a lack of multicalibration. This recalls a result of Zhang which characterizes swap omniprediction by multicalibration: there too, the ability to achieve better loss than a simple baseline is attributable to a lack of multicalibration. Another related work is that of Dua et al., "Calibration and Calibration-Sensitive Learning" , which views multicalibration as a boosting algorithm for regression. Their work also connects multicalibration and regression, but  the regression task they analyze is predicting $y$, whereas the regression task that we analyze is predicting $\ell(y, p(x))$.

\paragraph{Representation-aware multicalibration.} 
Internal representation-aware multiaccuracy is considered in the work of Wang et al., "Facial Recognition Using Internal Representation" who use it in the context of face-recognition. Internal representation-aware multicalibration  has connections to the notion of Code-Access Outcome Indistinguishability (OI), proposed by Dua et al., "Calibration and Calibration-Sensitive Learning" . In Code-Access OI, outcomes generated by $p(x)$ must be indistinguishable from the true outcomes generated from the target distribution with respect to a set of tests that can inspect the full definition and code of $p$. With code access, such tests can compute the internal features $r_p(x)$ that are available in internal representation-aware multicalibration, but also have other capabilities such as querying $p$ on other points $x' \in \calX$. The use of external representations for auditing/improving predictions is found in the work of Chen et al., "Auditing Facial Recognition using Skin Type" who use skin type to assess facial recognition; in recent work of Wang et al., "Expert Opinions and Machine Learning Predictions" which investigates the use of expert opinions in addition to ML predictions to improve on medical test results, and in the work of Dua et al., "Using Representations from Foundation Models for Auditing" who use representations from a foundation model that is distinct from the model they audit.  
\paragraph{Experimental work on multicalibration.} The work of Wang et al., "Facial Recognition Using Internal Representation" considers internal-representation aware multiaccuracy for facial recognition tasks, and shows how auditing can be used to improve accuracy across subgroups. 
Recently, Dua et al., "Calibration and Calibration-Sensitive Learning"  conducted a systematic investigation of multicalibration in practice, analyzing the utility of algorithms for multicalibration on a number of real-world datasets, prediction models, and demographic subgroups. We build closely on their setup for our own experiments.