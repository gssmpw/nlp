\section{Related work}
\label{sec:related}

\paragraph{Applications of loss prediction.} The idea of loss prediction has its roots in Bayesian and decision-theoretic active learning  \cite{settles2009active,ren2021survey}, wherein the loss expected to be incurred at a point provides a natural measure of how valuable it is to label; see e.g.\ the well-known method of Expected Error Reduction (EER) \cite{roy2001toward}. To our knowledge, loss prediction in the explicit sense that we consider in this paper was first studied by \cite{yoo2019learning}, who proposed training an auxiliary loss prediction module alongside the base predictor. This is a practical approach used in many real-world applications. An important example from industry is the popular Segment Anything Model \cite{kirillov2023segment}, which is an image segmentation model that includes an IoU prediction module\footnote{IoU, or intersection over union, is a standard segmentation error metric.}. This module plays a key role in the continual learning ``data engine'' used to train the model. Other applications of loss prediction are in routing inputs to weak or strong models \cite{dinghybrid, ong2024routellm, hu2024routerbench}, diagnosing model failures \cite{jain2022distilling}, and MRI reconstruction \cite{hu2021learning}.

Loss prediction is inherently connected to the broader topic of uncertainty quantification \cite{hullermeier2021aleatoric,abdar2021review}. The work of \cite{lahlou2021deup} formulates epistemic uncertainty as a form of excess loss or risk (see also \cite{xu2022minimum}) and estimates it using an auxiliary loss predictor. Loss decomposition and connections to calibration have also been studied by \cite{kull2015novel,ahdritz2024provable}, although these works do not discuss auxiliary loss predictors per se.

\paragraph{Related theoretical work.}  We are not aware of prior theoretical work that studies the complexity of loss prediction. We build on notions and techniques from prior work on calibration \cite{decisionCal, gopalan2022low, kleinberg2023u}, multicalibration \cite{hebert2018multicalibration, kim2022universal}, omniprediction \cite{omni, lossOI, OP3, OKK25} and outcome indistinguishability \cite{OI, OI2}. Multicalibration has found applications to a myriad areas beyond multigroup fairness; a partial list includes omniprediction \cite{omni}, domain adaptation \cite{kim2022universal}, pseudorandomness \cite{OI, DworkLLT23} and computational complexity \cite{CasacubertaDV24}. Our work adds loss prediction to this list.  

\paragraph{Decision calibration, decision OI and proper calibration.}
The work of \cite{decisionCal} on decision calibration  (implicitly) considered the self-entropy predictor, and
conditions under which this predictor is accurate in expectation. The subsequent work of \cite{lossOI} termed this condition decision OI and showed that calibration  of the predictor guarantees that the self-entropy predictor is itself calibrated for loss prediction. 
As we have seen, however, calibration of the predictor is not necessary for the self-entropy predictor to be calibrated (or optimal), due to the existence of blind-spots for a specific loss. This is explained by the notion of proper calibration introduced in
\cite{OKK25}, who showed that it tightly characterizes decision OI.

While our results have strong connections to all these works, the key difference is that our goal in loss prediction is not just to give loss estimates that are calibrated, it is to predict the true loss in the regression sense, which is potentially a much harder task.

 \paragraph{Swap multicalibration.} Our results equate the ability to gain an advantage over the self-predictor to a lack of multicalibration. This recalls a result of \cite{OP3} which characterizes swap omniprediction by multicalibration: there too, the ability to achieve better loss than a simple baseline is attributable to a lack of multicalibration. Another related work is that of \cite{Globus-HarrisHK23}, which views multicalibration as a boosting algorithm for regression. Their work also connects multicalibration and regression, but  the regression task they analyze is predicting $y$, whereas the regression task that we analyze is predicting $\ell(y, p(x))$.

\paragraph{Representation-aware multicalibration.} 
Internal representation-aware multiaccuracy is considered in the work of \cite{kgz} who use it in the context of face-recognition. Internal representation-aware multicalibration  has connections to the notion of Code-Access Outcome Indistinguishability (OI), proposed by~\cite{dwork2021outcome}. In Code-Access OI, outcomes generated by $p(x)$ must be indistinguishable from the true outcomes generated from the target distribution with respect to a set of tests that can inspect the full definition and code of $p$. With code access, such tests can compute the internal features $r_p(x)$ that are available in internal representation-aware multicalibration, but also have other capabilities such as querying $p$ on other points $x' \in \calX$. The use of external representations for auditing/improving predictions is found in the work of \cite{BuolamwiniG18}
who use skin type to assess facial recognition; in recent work of $\cite{alur24}$, which investigates the use of expert opinions in addition to ML predictions to improve on medical test results, and in the work of \cite{jain2022distilling} who use representations from a foundation model that is distinct from the model they audit.  
\paragraph{Experimental work on multicalibration.} The work of \cite{kgz} considers internal-representation aware multiaccuracy for facial recognition tasks, and shows how auditing can be used to improve accuracy across subgroups. 
Recently, \cite{hansen2024mcp}  conducted a systematic investigation of multicalibration in practice, analyzing the utility of algorithms for multicalibration on a number of real-world datasets, prediction models, and demographic subgroups. We build closely on their setup for our own experiments.