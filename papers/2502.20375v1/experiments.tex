\section{Experiments}
\label{sec:exp}

We have shown in Section \ref{sec:lp-mc} a correspondence between the advantage a loss predictor has over the self entropy predictor and the multicalibration error. In this section, we empirically demonstrate this correspondence and see that it holds across several base models, loss prediction algorithms, as well as data subgroups.

\paragraph{Experiment design.} We follow the basic design set forth in \cite{hansen2024mcp} for working with binary prediction tasks on UCI tabular datasets, specifically Credit Default \cite{default_of_credit_card_clients_350} and Bank Marketing \cite{bank_marketing_222}. For each dataset, we consider certain subgroups (13 and 15 different groups respectively) defined by combinations of features such as occupation, education, and gender (see Appendix C.4 \cite{hansen2024mcp} for full details). These subgroups are used to evaluate the multicalibration error of the predictors as follows: for each subgroup we measure the \emph{smoothed Expected Calibration Error} (smECE) \cite{blasiok2023smece}, and take the multicalibration error to be the maximum smECE obtained.

We examine base predictors from different model families at various levels of multicalibration, specifically Naive Bayes and Support Vector Machines (SVMs), which tend to be uncalibrated without any postprocessing, along with Random Forests, Logistic Regression, Decision Trees, and Multilayer Perceptrons (MLPs), which tend to be well-calibrated out of the box when trained with empirical risk minimization. The base predictor MLP we use has a three-hidden-layer architecture with ReLU activations. For further details on hyperparameters, architectures, and training, we point the reader to Appendix E in \cite{hansen2024mcp}. 

We then run the following four loss prediction algorithms: decision tree regression, XGBoost, support vector regression (SVR), and a three-hidden-layer MLP. Each of these is input aware, that is, it is given both $\inp(x)$ and $p(x)$ at train time, and is trained using a standard regression objective to minimize $\ex_{(x, y) \sim \calD}[\big(\lossPred(\inp(x), p(x)) - \ell(y, p(x)) \big)^2]$. Our target loss $\ell$ will be the squared loss $\ell(y, p(x)) = (y-p(x))^2$. 

\paragraph{Results.} Our main takeaways are as follows:
\begin{itemize}
    \item Loss predictors perform better as the multicalibration error of the base model increases.


    \item Loss predictors perform better on subgroups that exhibit higher calibration error. 

\end{itemize}
 
The first takeaway is demonstrated by Figure \ref{fig:cd:agg_adv}, where the horizontal axis indicates the max smECE of the base model, and the vertical axis indicates the advantage the loss predictor attains over the self-entropy predictor of the base model. 
As our theory predicts, we see a clear positive correlation between the maximum smECE and the relative performance of the loss predictor. In other words, less multicalibrated models have better performing loss predictors. This correlation holds across different base models and different algorithms for the loss predictor.


\begin{figure}[H] 
    \centering
    \includegraphics[width=16cm]{figures/scatter_bar.png}
    \caption{\small Advantage vs.\ max smECE across base predictors and loss prediction algorithms. For any particular loss predictor (shape), we see that as the multicalibration error of the base model (color) grows, the loss predictor's advantage improves.}
    \label{fig:cd:agg_adv}
\end{figure}


To delve deeper, we examine how loss prediction advantage varies across different subgroups. For this experiment we vary the base predictor only, while fixing the loss prediction algorithm to be an MLP. In Figure \ref{fig:cd:group_adv}, for each base predictor and each subgroup, we report the loss predictor's advantage restricted to the subgroup on the vertical axis and the smECE of the subgroup on the horizontal axis. 

For base models that are poorly calibrated overall (Decision Tree, SVM, and Naive Bayes), we see a clear correlation showing the loss predictor performs better on subgroups as the calibration error gets worse. By contrast, base predictors that are well-calibrated overall (Logistic Regression, Random Forest, and MLPs) allow negligible loss prediction advantage even after stratifying by subgroup.

\begin{figure}[H] 
    \centering
    \includegraphics[width=16cm]{figures/subgroup.png}
    \caption{\small Fixing the type of loss predictor to be an MLP, we plot the loss advantage vs.\ the smECE on each subgroup across different base predictor models for the Credit Default and Bank Marketing datasets. For a fixed base predictor (color), the loss predictor exhibits more advantage on subgroups where the base predictor is less calibrated.}
    \label{fig:cd:group_adv}
\end{figure}
