\section{Loss prediction}
\label{sec:lp}

We consider binary classification, with a distribution $\mD$ on  $\X \times \zo$. 
A predictor is a function $p:\X \to [0,1]$. The Bayes optimal predictor is defined as $p^*(x) = \ex[y|x]$. Given $p$, we define the simulated distribution $\mD(p)$ on $\X \times \zo$ where $x$ is drawn as in $\mD$, and $y|x \sim \Ber(p(x))$. Let $\ell: \zo \times [0,1] \to [0,1]$ be a proper loss function.\footnote{ The case of general losses reduces to the proper loss case; please see \cref{sec:non-proper} for details. We also assume for technical convenience that the loss is bounded. Losses that are not strictly bounded, such as cross entropy, can be handled with some further care and constraints on predicted probabilities.} We will use the following characterization of proper losses.

\begin{lemma}[\cite{gneiting2007strictly}] For every proper loss $\ell$,  there exists a concave function $H_{\ell}: [0, 1] \rightarrow \R$ so that
\[ \ell(y, v) = H_{\ell}(v) + (y - v)H_{\ell}'(v).\]
where $H_{\ell}'(v)$ is a ``superderivative'' of $H_{\ell}$, i.e. for any $v, w \in [0, 1]$, $H_{\ell}(v) \leq H_{\ell}(w) + (v - w)H_{\ell}'(w)$. 
\end{lemma}
When $H_{\ell}(v)$ is differentiable at all $v \in [0, 1]$, the superderivative is unique, and is just the derivative.  From the definition it follows that 
\begin{align*} 
H_{\ell}(v) &= \ex_{y \sim \Ber(v)}[\ell(y,v)] \in [0,1]\\
H_{\ell}'(v) &= \ell(1, v) - \ell(0,v) \in [-1,1]
\end{align*}
Let $L(p^*; p) = \ex_{y \sim \Ber(p^*)}[\ell(y, p)]$ denote the expected loss when $y \sim \Ber(p^*)$ but we predict $p$. Then
\begin{align}
\label{eq:exp-loss}
 L(p^*;p) = H_{\ell}(p) + (p^* - p)H_{\ell}'(p) \geq H_{\ell}(p^*) = L(p^*; p^*) 
\end{align}
where the inequality follows from the concavity of $H_{\ell}$, and is equivalent to the loss $\ell$ being proper.

We now define the notion of a loss predictor. 

\begin{definition}[Loss predictor]
\label{def:lp}
    Let $p$ be a predictor and $\ell$ be a proper loss. Let $\Phi$ be an abstract feature space, which we will make concrete shortly. 
    A \emph{loss predictor} is a function $\lossPred: \Phi \to \R$, which takes as input some features $\phi(p,x) \in \Phi$ related to a point $x$ and its prediction using $p$, and returns an estimate $\lossPred(\phi(p,x))$ of the expected loss $\ex[\ell(y, p(x))|x]$ suffered by $p$ at the point $x$. We define a hierarchy of loss predictors of increasing strength, depending on the information contained in $\phi(p,x)$:
    \begin{enumerate}
        \item \emph{Prediction-only loss predictors} only have access to $p$'s prediction at a point $x$, i.e. $\phi(p, x) = p(x)$. The most natural choice for a prediction-only loss predictor is given by the self-entropy predictor, which we will define in Definition~\ref{def:can-loss-pred}.
        \item \emph{Input-aware loss predictors} have access to the input features $\inp(x)$ used to train the model $p$, as well as its prediction, i.e. $\phi(p, x) = (\inp(x), p(x))$. 
        \item \emph{Representation-aware loss predictors} have access to $\phi(p, x) = ( p(x), \inp(x), r(x))$, where $r(x)$ is some representation of $x$. We distinguish between two kinds of representations:
        \begin{itemize}
            \item Internal representations: The representation $r(x)= r_p(x)$ consists of features that are explicitly computed by the predictor $p$ in the course of computing $p(x)$. For instance, they could consist of the embedding of $x$ produced by the last few layers of a DNN.  
            \item External representations: The representation $r(x) = r_e(x)$ consists of features that are not explicitly computed by the  predictor $p$. For instance, they could be the representation of $x$ obtained from a different model, or by consulting human experts.         
        \end{itemize}
     \end{enumerate}
\end{definition}

A few comments on the definition: 

\begin{itemize}
\item Two-headed architectures that simultaneously train both the predictor and the loss-predictor (such as \cite{yoo2019learning,kirillov2023segment}) are a class of internal representation-aware predictors. In contrast, loss-predictors that use an embedding produced by a foundation model (such as \cite{jain2022distilling}, which audits the errors of the predictor) are external representation-aware.  

\item   If we allow the loss predictor to be significantly more complex than the predictor $p$, then it could compute $r_p(x)$ from $\inp(x)$ using the model $p$. So the gap between input-aware and representation-aware loss predictors diminishes as the loss-predictor becomes more computationally powerful. But in the (important) setting where the loss predictor is less computationally powerful than the predictor, there could be a gap.

\item In contrast, external representations might contain auxiliary information  that cannot be computed using $\inp(x)$, regardless of the computational power of the loss predictor. 
\end{itemize}

The loss predictor can be trained using standard regression, given access to a training set of points $(\phi(p, x), y)$ where $(x, y)$ are drawn from the distribution $\mD$.  One can measure the performance of our loss predictor as we would with any regression problem. We formulate it using the squared loss, as $\ex[(\ell(y, p(x)) - \lossPred(\phi(p, x))^2]$. It follows from Equation \eqref{eq:exp-loss} that the Bayes optimal loss predictor is given by
$\lossPred^*(x) = L(p^*(x); p(x))$. But computing this requires knowing the Bayes optimal predictor $p^*$, and is likely to be infeasible in most settings. Rather, we will compare our loss predictor to a canonical baseline which we describe next.

\paragraph{The self-entropy predictor.}

Following \cite{OI}, given a predictor $p$, we define the simulated distribution $\mD(p)$  on pairs $(x, \ty) \in \X \times \zo$,  where $x \sim \mD$ and $\ex[\ty|x] = p(x)$. The predictor hypothesizes that this how labels are being generated. Hence for each $x \in \X$, the self-entropy predictor predicts the expected loss according to this distribution. 

\begin{definition}[Self-entropy predictor]\label{def:can-loss-pred}
    Given a proper loss $\ell$ and predictor $\pred$,  the \emph{self-entropy predictor} is the prediction-only loss predictor $\clp: [0, 1] \to \R$  that predicts the expected loss when $\ty \sim \Ber(\pred(x))$ at each $x$; that is
    \[\clp(\pred(x)) = \ex_{\ty \sim \Ber(p(x))}[\ell(\ty, p(x))] = H_{\ell}(\pred(x)).\]
\end{definition}


We use the self-entropy predictor as our baseline. Hence the question is when can we learn a loss predictor with significantly lower squared loss than the self-entropy predictor. We formalize this using the notion of advantage of a loss predictor over the self-entropy predictor.

\begin{definition}[Advantage of a loss predictor]\label{def:lp-advantage}
    Define the advantage of a loss predictor $\lossPred$ over the self-entropy predictor to be the difference in the squared error
    \[ \adv(\lossPred) = \ex[(\ell(y, p(x)) - \clp(p(x)))^2] - \ex[(\ell(y, p(x)) - \lossPred(\phi(p, x))^2]. \]
\end{definition}%

We want loss predictors whose advantage is positive and as large as possible.
Our goal is understand under what conditions we can hope to learn such a predictor. 

\paragraph{On non-proper losses.} So far we have assumed that we a trying to predict the proper loss incurred by a predictor. We can generalize this to a setting where we have a hypothesis $h:\X \to \mA$ (for instance $h$ might be a binary classifier), and a loss function $\ell:\zo \times \mA \to \R$. It turns out that our theory extends seamlessly to the non-proper setting, under rather mild assumptions on the hypothesis $h$. We present this extension in Appendix \ref{sec:non-proper}.




