\section{Introduction}
It is increasingly common for large machine learning models to be part of a pipeline where a base model is trained by a provider that has access to large-scale data and computational power, and the model is then deployed by a heterogeneous set of downstream consumers, for a diverse range of prediction tasks. Not only could the tasks be very different from each other, they might involve data distributions, loss functions and other metrics and features that are markedly different from those used to train the model. Indeed, often the data sources used in training are not even disclosed to the downstream application. A typical instantiation of this framework is zero-shot classification, where (say) an LLM is required to classify texts into classes described by the user. Another important case is that of medical classification where the base model was trained on one set of features, say lab reports, but the model (or human) downstream has access to additional features such as patient history.  

In such a situation, a user might want to delve deeper into how the model is likely to perform on their specific task. They might seek to discover  problematic regions of the input space where the model performs poorly, where performance is measured by an appropriate loss function chosen by the user. This information could prove valuable in several ways:
\begin{enumerate}
    \item Active and continual learning: Users could address performance issues by collecting additional data points from problematic regions and fine-tune the model on this enhanced dataset.
    \item Fairness considerations: The analysis might reveal potential biases or inequities in the model's performance across different subgroups.
    \item Selective prediction: Such insights could guide downstream users on when to rely on the model's predictions and when to exercise caution. In cases where predictions are likely to be unreliable, users might opt to consult external experts or alternative models instead.
\end{enumerate}
By systematically identifying and addressing these performance vulnerabilities, users can judge the model's reliability, fairness, and overall utility. A provider who desires to improve their model would similarly benefit from knowing where their model performs poorly. 


This discussion motivates the problem of loss prediction, which we now define. In this work we focus for concreteness on the binary classification setting, although many of the results extend to multiclass classification as well. We are given a pre-trained predictor $p:\X \to [0,1]$ (where $\calX$ denotes the space of inputs), a target loss $\ell: \zo \times [0,1] \to \R$\footnote{It will be convenient to assume that this loss is \emph{proper}, namely that for any $p^*$ the expected true loss $\ex_{y \sim p^*}[\ell(y, p)]$ is minimized at $p = p^*$. Canonical examples are the cross-entropy and the squared loss. The case of general losses can be reduced to that of proper losses.}, and some labeled data $(x, y)$ drawn from an unknown distribution $\calD$ on $\calX \times \{0, 1\}$. The goal is to estimate the loss $\ell(y, p(x))$ incurred at a point $x$ using a loss predictor $\lossPred : \calX \to \R$. This can be viewed as a regression problem, and we measure the quality of a loss predictor by its expected squared loss with respect to the true loss, i.e.\ $\ex_{(x, y) \sim \calD}[\big(\lossPred(x) - \ell(y, p(x)) \big)^2]$.


Loss prediction is closely connected to the well-studied problem of uncertainty estimation. A standard measure of predictive uncertainty at a point is the expected loss that a predictor suffers at that point \cite{kull2015novel}, and estimating this requires solving the problem of loss prediction. Given such a loss predictor, its uncertainty estimate is then often decomposed into two parts: aleatoric uncertainty, which is the uncertainty stemming from the randomness in nature, and epistemic uncertainty, which is the uncertainty arising from shortcomings in our model and/or training data.\footnote{There are many proposals for how to achieve such a decomposition, see e.g.\ \cite{hullermeier2021aleatoric}, not all of which come with rigorous guarantees. Recent work of \cite{ahdritz2024provable} does give rigorous guarantees, but it requires an enhancement to the standard learning model called learning with snapshots. See also the discussion of related work therein.
}
Since epistemic uncertainty can be driven down with more data and fine tuning, active learning strategies have been proposed that use loss predictors to decide what regions to prioritize for collecting more data \cite{yoo2019learning,lahlou2021deup}. Loss predictions can also be used for various other applications, including deciding when a model should abstain from learning, or route the input to a stronger model.
Consequently, there has been plenty of applied work on the problem of loss prediction, but little theoretical analysis (see \cref{sec:related} for more discussion of related work).


\subsection{Our contributions}
In this work, we initiate a study of the theoretical foundations of loss prediction. We formalize the task of loss prediction and connect it to the basic primitives of computational learning. 


\paragraph{The self-entropy predictor of loss.}
The first question is what baseline one should use to measure the quality a loss predictor. Drawing from work on outcome indistinguishability \cite{OI, OI2}, we propose a baseline based on the fact that a predictor posits a certain model of nature: that labels for $x$ are drawn according to a Bernoulli distribution with parameter $p(x)$. This entails a belief about the expected loss it will incur at a point. In the case of squared loss, this estimate is $p(x)(1 - p(x))$ at the point $x$.  By results of \cite{gneiting2007strictly}, for any proper loss $\ell$, there exists a concave ``generalized entropy'' function $H_{\ell}:[0,1] \to \R$ such that the prediction is $H_{\ell}(p(x))$. We refer to this as the self-entropy predictor. Using this as our baseline, we ask when it is possible for a loss predictor to do better than the self-entropy predictor. At a high level, we wish to understand

\begin{center}
{\em When can a loss-predictor beat a model in estimating what the model knows and does not know?} 
\end{center}

\paragraph{A hierarchy of loss prediction models.} It is natural that loss predictors should receive the input features $\inp(x)$\footnote{For clarity, we make a distinction between the abstract input $x$ (e.g., an individual) and its input feature representation $\inp(x)$ (e.g., features collected about the individual).} and the prediction $p(x)$ as inputs.  But this does not capture some important architectures for loss prediction that are used in practice; for instance the works of \cite{yoo2019learning, kirillov2023segment} which consider models that can access representations of $x$ that are computed by the neural network computing $p$. Accordingly, we model loss predictors as taking inputs $\phi(p, x)$ lying in an abstract feature space and returning a loss prediction $\lossPred(p(x))$. We define a hierarchy of loss predictors of increasing strength, depending on expressivity of $\phi(p,x)$ (Definition~\ref{def:lp}):
    \begin{enumerate}
        \item \emph{Prediction-only loss predictors} only have access to $p$'s prediction at a point $x$, i.e. $\phi(p, x) = p(x)$. The self-entropy predictor of loss is an example.
        \item \emph{Input-aware loss predictors} have additional access to the input features $\inp(x)$, i.e. $\phi(p, x) = (p(x), \inp(x))$. 
        \item \emph{Representation-aware loss predictors} have access to $\phi(p, x) = (p(x), \inp(x), r(x))$, where $r(x)$ is some representation of $x$. In this case, we further distinguish between two settings:
        \begin{itemize}
            \item Internal representations $r(x) = r_p(x)$ where $r_p(x)$ is computed by $p$ in the course of computing $p(x)$. 
            \item External representations $r(x) = r_e(x)$ which are not explicitly computed by $p$. 
            \end{itemize}
    \end{enumerate}
Internal representations could for instance correspond to the embedding produced by the last few layers of a deep neural net. External representations could be the representation of $x$ obtained from a different model, or additional features added by consulting human experts. The related work in Section~\ref{sec:related} gives examples of both kinds of representations that have been considered in the literature. 

Finally, we define the advantage of a loss predictor over the self-entropy predictor to be the difference in the squared loss incurred by the two loss predictors (Definition~\ref{def:lp-advantage}). 

\paragraph{Relation to auditing for multicalibration.}
Multicalibration is a multigroup fairness notion introduced by \cite{hebert2018multicalibration}, which has since found numerous other applications \cite{kim2022universal, OI, omni}. We show that learning a loss predictor with a non-trivial advantage is tightly connected to auditing the predictor for multicalibration. 
 At a high level, we show the following correspondence, which we formalize in Theorem~\ref{thm:mc-conv-vs-loss-pred}:

\bigskip

\begin{center}
\begin{minipage}{0.41\textwidth}
Finding a \textbf{prediction-only} loss predictor with good advantage
\end{minipage}
\quad
$\Leftrightarrow$
\quad
\begin{minipage}{0.41\textwidth}
Identifying a \textbf{calibration} violation for 
$p$
\end{minipage}
\end{center}

\bigskip

\begin{center}
\begin{minipage}{0.41\textwidth}
Finding an \textbf{input-aware} loss predictor with good advantage
\end{minipage}
\quad
$\Leftrightarrow$
\quad
\begin{minipage}{0.41\textwidth}
Identifying a \textbf{multicalibration} violation for 
$p$
\end{minipage}
\end{center}

\bigskip

\begin{center}
\begin{minipage}{0.41\textwidth}
Finding a \textbf{representation-aware} loss predictor with good advantage
\end{minipage}
\quad
$\Leftrightarrow$
\quad
\begin{minipage}{0.41\textwidth}
Identifying a \textbf{representation-aware multicalibration} violation for 
$p$, where the auditor function is of the form $c(\phi(p,x))$. 
\end{minipage}
\end{center}

\bigskip

In all cases, the regions where the multicalibration violations occurs arise from analyzing where the loss predictor and the self-entropy predictor differ from each other. The first two notions in our hierarchy, calibration and multicalibration, have been extensively studied in previous works \cite{FV99, hebert2018multicalibration}. The last member of the hierarchy, representation-aware multicalibration, is a strengthening of multicalibration that naturally extends the multicalibration framework.  

Furthermore, we explore how the lens of multicalibration proves valuable in predicting well for a large class of losses, particularly when learning individual predictors for each loss is impractical. In Theorem~\ref{thm:1-lip-mc-pred}, we show that via standard techniques for learning multicalibrated predictors, we can efficiently learn a predictor whose self-entropy predictions for every 1-Lipschitz proper loss (of which there are infinitely many) are comparable to the best-in-class loss predictor for each loss from some fixed class of candidate predictors. 

\paragraph{On calibration blind-spots for loss prediction.}
Calibration is not necessary for producing good estimates of the true loss. For instance, a predictor that predicts $p(x) = 1/2$ on every input will indeed incur a squared loss of $1/4$, matching its self-entropy predictor regardless of the true labels. But depending on the distribution of labels, this predictor might be very far from calibrated, and need not even be accurate in expectation. 

Our results imply a simple characterization of such ``blind spots'' for any proper loss $\ell$ as points $p$ where $H_{\ell}'(p) = 0$.\footnote{Recall that $H_{\ell}(p)$ is the concave entropy function corresponding to $\ell$.} In terms of the loss $\ell$, this is equivalent to $\ell(0, p) = \ell(1,p)$, so that the loss incurred is independent of the label, and hence predicting the expected loss for such $p$  is trivial. For strictly proper losses, the function $H_{\ell}$ is strictly concave, and there is a unique point where this happens.

This introduces some subtlety in the type of multicalibration violations that arise from our correspondence; the standard calibration error at $p$ is weighted by a factor of $H_{\ell}'(p)$. Hence non-trivial loss prediction corresponds to  (multi)calibration violations at prediction values $p$ such that $H_{\ell}'(p)$ is far from $0$. 

\paragraph{Experimental results.} We empirically verify that there is a correspondence between loss prediction and multicalibration (see \cref{sec:exp}). Focusing on input-aware loss prediction algorithms run across a variety of base predictor types, we find that: 
\begin{itemize} 
\item As the multicalibration error of the base model increases, the advantage of the loss prediction over the self estimate of the loss increases. 
\item Loss predictors are more advantageous on data subgroups that have higher calibration error. 
\end{itemize} 
Our experiments suggest that regression-based loss predictors present an effective way to audit for multicalibration and are an intriguing avenue towards developing efficient multicalibration algorithms for practice. 






















\subsection{Takeaways from our result}

The main takeaway from our work is that non-trivial loss prediction is no easier (and not much harder) than auditing the predictor itself. Any predictor that improves over the self-entropy predictor could be used to find (and possibly fix) multicalibration issues in the predictor. 

\paragraph{Practical multicalibration using loss prediction.} The complexity of multicalibration depends crucially on the class of test functions used.  For complex functions, our equivalence suggests a novel approach to multicalibration auditing: choose a proper loss, run a regression for loss prediction, and see if the loss predictor outperforms the self-entropy predictor. This is a simple and practical approach that is able to leverage the strength of any well-engineered library for regression. In our experiments we show that this is indeed effective, with loss prediction advantage being robustly correlated with multicalibration error across mutliple base predictors as well as subgroups.\footnote{To get around the blind-spot issue, one could choose a few strictly proper loss functions each with a different blind spot. This is easy to do, given the correspondence between convex functions and proper losses \cite{gneiting2007strictly}.}







\paragraph{On two-headed architectures.}

There has been work on training deep neural nets with two heads: a prediction head $p$, and a loss prediction head $\lossPred$ \cite{yoo2019learning, kirillov2023segment}. The loss prediction head has access to the embedding of the inputs produced by the last few layers of the neural net, and can be modeled by a representation-aware loss predictor of low complexity. Our result shows that (at least in a classification setting) one of the following must be true:
\begin{itemize}
    \item The loss prediction head does not give much advantage over the self-entropy predictor, which only requires prediction access.
    \item The prediction head is not optimal, as evidenced by a multicalibration violation witnessed by the difference between the loss prediction head and the self-entropy predictor (see Lemma~\ref{lem:adv-implies-mc-err}).
\end{itemize}
The ideal situation for a well-trained model is clearly the former. 

Note that this does not mean that two-headed architectures are not useful: the two heads may influence the training dynamics in a subtle way, with the loss-predictor head revealing complex regions where multicalibration fails. However, what our result implies is that when training concludes, we want to be in the situation where the loss-predictor is not much better than the self-entropy predictor. This is analogous to the situation with GANs \cite{GANs}, where at the end of training, we would ideally like the generator to be able to fool the discriminator. But in the intermediate stages of training, the discriminator helps improve the quality of the generator. 

Two-headed architectures may also be useful in prediction problems more general than ordinary classification, such as image segmentation \cite{kirillov2023segment}, where a predictor does not necessarily come with a self-entropy estimate at all.

\paragraph{Extra information helps: when loss prediction might be effective.}

An important scenario is where the loss-predictor may have informative features $\phi'(x)$ about the input $x$ that were not available to the entity that was training the model $p$. For example, consider a neural net that is trained to screen X-rays for prevalence of a certain medical condition. Such models may be trained by aggregating data from across several hospitals. A hospital that is trying to use this model might not have the same computational resources available to them. But they might have access to other useful information such as observations made by a doctor or the patient's medical history. 

In such a case, even a model which is multicalibrated with respect to complex functions over the features $\phi(p, x)$ might not be multicalibrated with respect to simple functions over a new set of features. This was illustrated in the recent work of \cite{alur24} in their work on incorporating human judgments to improve on model predictions.  Another natural scenario in which the loss predictor may have extra information is if it uses a powerful pretrained foundation model. The work of \cite{jain2022distilling} does precisely this, leveraging embeddings from CLIP \cite{radford2021learning}. In such settings, improvements to the predictor, and loss predictors with a non-trivial advantage are both possible.


\paragraph{Organization.} We define loss predictors in Section \ref{sec:lp} and recall the relevant notions of multicalibration in Section \ref{sec:mc}. We present the equivalence between loss prediction with an advantage over the self-entropy predictor and multicalibration in Section \ref{sec:lp-mc}. We discuss how to efficiently find predictors that give good self-entropy predictors for multiple loss functions in Section \ref{sec:multiple-loss}. In Section \ref{sec:exp}, we empirically demonstrate the correspondence between loss prediction advantage and multicalibration violations, and show that it holds across multiple architectures and data subgroups. We discuss related work in detail in Section \ref{sec:related}. In Appendix \ref{sec:non-proper}, we present the extension of our results to the case where the losses are non-proper. 
