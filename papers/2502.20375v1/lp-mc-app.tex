\section{Proofs from Section~\ref{sec:lp-mc}}

\subsection{Proof of Theorem~\ref{thm:mc-conv-vs-loss-pred}}\label{sec:mc-conv-cs-loss-pred-pf}

\begin{proof}[Proof of Theorem~\ref{thm:mc-conv-vs-loss-pred}]
    The inequality on the left follows from Theorem~\ref{lem:adv-implies-mc-err}, while the inequality on the right follows from Lemma~\ref{lem:mc-err-implies-adv}. We prove each in turn, starting with the left-hand inequality. 

    By Theorem~\ref{lem:adv-implies-mc-err}, if there exists a $f \in \calF$ such that setting $\lossPred = f$ gives $\adv(\lossPred) = \alpha$, then this implies that 
    \[\ex[(\lossPred(\phi(p, x) - \clp(p(x)))H_{\ell}'(p(x))(y - p(x))] \geq \alpha/2.\]
    We observe that because $\lossPred = f \in \calF$, the witness of this multicalibration violation, $(\lossPred(\phi(p, x) - \clp(p(x)))H_{\ell}'(p(x))$ lies in $\calC$, and thus 
    \begin{align*}
        \MCE(\calC, p) &= \max_{c \in \calC}\left| \ex[c(\phi(p, x))(y - p(x))]\right|\\
        & \geq \ex[(\lossPred(\phi(p, x) - \clp(p(x)))H_{\ell}'(p(x))(y - p(x))]\\
        &\geq \alpha/2\\
        &= \frac{1}{2}\adv(\lossPred)
    \end{align*}

    The inequality follows by taking the maximum over all $\lossPred \in \calF$, as $\lossPred$ was chosen arbitrarily. 

    We now move on to proving the inequality on the right, i.e., the upper bound on $\MCE(\calC, p)$. 

    By definition of the multicalibration error and $\calC$, there exists some $c \in \calC$ that witnesses a multicalibration error of magnitude $\MCE(\calC, p)$, i.e. for some $f \in \calF$, 

    \[\MCE(\calC, p) = \left|\underbrace{\ex[(f(\phi(p, x)) - H_{\ell}(p(x)))H_{\ell}'(p(x))(y - p(x))]}_{:= E_f}\right|. \]

    Thus, if we define $\delta: \Phi \rightarrow [-1, 1]$ as 
    \[\delta(\phi(p, x)) = \sgn(E_f)(f(\phi(p, x)) - H_{\ell}(p(x))),\]
    it follows that 
    \begin{align*}
        \ex[\delta(\phi(p, x))H_{\ell}'(p(x))(y - p(x))] &= \MCE(\calC, p).
    \end{align*}

    Applying Lemma~\ref{lem:mc-err-implies-adv} for this $\delta$ implies that for the loss predictor defined by $\lossPred(\phi(p, x)) = \Pi_{[0,1]}(\clp(p(x)) + \MCE(\calC, p) 
    \delta(\phi(p, x)))$ satisfies 

    \[\adv(\lossPred(\phi(p, x))) \geq \MCE(\calC, p)^2.\]

    The proof of the inequality follows by observing that $\lossPred \in \calF'$, because 
    \begin{align*}
        \lossPred(\phi(p, x)) &= \Pi_{[0,1]}(\clp(p(x)) + \MCE(\calC, p) 
    \delta(\phi(p, x)))\\
    &= \Pi_{[0,1]}(H_{\ell}(p(x)) + \underbrace{\MCE(\calC, p) 
    \sgn(E_f)}_{:= \beta}(f(\phi(p, x)) - H_{\ell}(p(x))))\\
    &= \Pi_{[0,1]}((1 - \beta)H_{\ell}(p(x)) + \beta f(\phi(p, x)))
    \end{align*}

    Where $\beta = \sgn(E_f)\MCE(\calC, p) \in [-1, 1]$, because $\MCE(\calC, p) \in [0, 1]$. 

    Thus, $\lossPred \in \calF'$, and so we conclude that 

    \[\max_{\lossPred \in \calF'}\adv(\lossPred(\phi(p, x))) \geq \MCE(\calC, p)^2.\]

    We get the right-hand inequality from the statement after taking square root of both sides. 
\end{proof}

