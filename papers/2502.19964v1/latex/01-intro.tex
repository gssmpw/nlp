\section{Introduction}

%[X: Context and relevance]
Language models increasingly drive real-world applications, yet their black-box nature remains a fundamental barrier to deployment. This lack of visibility has sparked intense interest in interpretability methods \cite{olah2018building}, with sparse autoencoders (SAEs) emerging as a particularly promising direction \cite{cunningham2023sparse, bricken2023monosemanticity}. The core idea is appealingly simple: train an autoencoder to reconstruct neural activations through a sparse bottleneck, forcing the model to learn disentangled, interpretable features.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth,trim={0 2.3cm 0 3.4cm},clip]{figures/sae_model_probe_accuracies_top_features_L31_pre.png}%{figures/image (7).png}
    \caption{SAE features vs linear probes: generalization performance varies drastically with nature of OOD data.}
    \label{fig:enter-label}
\end{figure}

%[Y: Challenge]
Recent work demonstrates that SAEs can effectively capture %simple %VT better commenting since they have some more complex ones, though still all based on syntax it seems to me?
a wide range of features in language models such as errors in code, sycophancy, and gender bias \cite{templeton2024scaling}, as well as syntax patterns and sentiment \cite{lan2024sparse, marks2024enhancing}. In parallel, circuit-level analyses have begun to reveal mechanistic underpinnings of neural network behavior \citep{elhage2021mathematical, olsson2022context, marks2024sparse, wang2022interpretability, athwin2024identifying}.%—for example, the identification of preliminary circuits for predicting gendered pronouns in GPT-2 small \cite{athwin2024identifying}. 
While interpretability methods like SAEs are often motivated by AI safety concerns, recent work suggests that even advanced interpretability approaches may have fundamental limitations for ensuring AI safety \cite{Barez2025-open}.
Despite these advances and critiques, a crucial question remains unaddressed: can these methods fully capture the abstract concepts that would help us understand how language models think and process information in a robust way?
%[Z: Our approach]
We study this question by focusing on answerability—a model's ability to recognize whether it can answer a question. This capability is fundamental to language model behavior and exists across diverse tasks. If SAEs truly capture meaningful abstractions, they should be able to extract features representing this capability.
%[1: Results and implications]
Our experimental results reveal a more nuanced picture than previous work suggests. While residual stream probes outperform SAE features within specific domains, the story changes dramatically when we look at generalization. \textbf{We find good SAE features for individual domains, but varying transfer abilities across datasets. Similarly, residual stream probes exhibit high variance in generalization despite strong in-domain performance}. These findings raise important questions about SAE research, particularly for abstract or more complex concepts that manifest differently across contexts.

% %X: What are we trying to do and why is it relevant?
% \fb{Add many citations} Language models increasingly drive real-world applications, yet their black-box nature remains a fundamental barrier to deployment. This lack of visibility has sparked intense interest in interpretability methods \citep{olah2018building}, with sparse autoencoders (SAEs) emerging as a particularly promising direction. The core idea is appealingly simple: train an autoencoder to reconstruct neural activations through a sparse bottleneck, forcing the model to learn disentangled, interpretable features.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/image (7).png}
%     \caption{Overview of answerability detection framework and results. Top: Architecture showing how linear probes and SAEs are trained on SQUAD dataset and evaluated across different out-of-distribution domains. Bottom: Comparative performance analysis showing accuracy across different datasets and model configurations, demonstrating varying levels of generalization ability.}
%     \label{fig:enter-label}
% \end{figure}
% %Y: Why is this hard?
% Recent work demonstrates that SAEs can effectively capture simple features such as syntax patterns or sentiment in language models \citep{lan2024sparse, marks2023interpreting}. In parallel, circuit-level analyses have begun to reveal mechanistic underpinnings of neural network behavior---for example, the identification of preliminary circuits for predicting gendered pronouns in GPT-2 small \citep{athwin2024identifying}. These successes have led to claims about SAEs as a general solution for model interpretability. Yet a crucial question remains unaddressed: can these methods really capture the abstract concepts that would help us understand how language models think and process information?
% \lh{Might be good to mention the Anthropic paper that found relevant high level features like safety although they didn't compare it to probes}
% %Z: How do we solve it (i.e. our contribution!)
% We study this question by focusing on \emph{answerability}---a model's ability to recognize whether it can answer a question. This capability is fundamental to language model behavior, exists across diverse tasks, and likely has some consistent internal representation. If SAEs truly capture meaningful abstractions, they should be able to extract features representing this capability.


% %1: How do we verify that we solved it:
% %1a) Experiments and results, including comparison to prior SOTA if applicable\\
% Our experimental results reveal a more nuanced picture than previous work suggests. While residual stream probes outperform SAE features within specific domains, the story changes dramatically when we look at generalization. We find a fair amount of diversity in feature behavior---some SAE features show impressive transfer across datasets while others remain strongly domain-specific. Even more surprising?, residual stream probes exhibit high variance in generalization despite the strong in-domain performance, suggesting fundamental challenges in extracting robust, abstract features.

% These findings raise important questions about the current trajectory of SAE research. While some features demonstrate promising generalization, we still cannot reliably predict which ones will transfer well. This limitation becomes particularly acute when dealing with abstract concepts that exist differently across contexts.

% VT tbd (comment for me)
% mention complex feat vs small data vs ood where we assune in-domain data available (can assume amounts)
% also question about hypo. competitive, better?
% interopretab, feat circ in marks still ~80 features for sv agrerem

%1b) Theory \\
	%2: New trend: specifically list your contributions as bullet points \\

%

% \textbf{Our work makes three key contributions:} 
% \begin{itemize}
%     \item We provide a systematic evaluation of SAE feature generalization across multiple answerability datasets using the Gemma model.
%     \item We present a detailed analysis of feature behavior across domains, revealing both impressive capabilities and critical limitations.
%     \item We demonstrate that some SAE features show remarkable transfer abilities simple probes outperform them.
% \end{itemize} 

% Predicting which features will generalize remains an open challenge---one that must be addressed for SAEs to a useful and general interpretability tools.
%}
% look at probes instead eg princ comp as in iclr'24 paper
%While it is hypothesized that LLMs have an internal representation of answerability, 

% \myparagraph{Sparse Autoencoders for LLMs} 

% Large language models (LLMs) have started taking over critical functions in business, [...] and beyond, and are expected to being increasingly used in the near future. Yet, their reliability remains fragile, amongst others, because we are still missing [reliable] tools for interpretability \cite{}. 

% Our focus is on sparse autoencoders (SAEs), which have recently been shown to provide [...] explanations for language model generation \cite{}. In a nutshell, SAEs are trained in an unsupervised manner to learn sparse, monosemantic features that are supposed to capture important input concepts present, but hidden in the polysemantic LLM activations. This offers interpretability, and the unsupervised training is furthermore hypothesized to yield generalization. 


% \vt{related works...}\\
% In summary, some recent works have shown promising results in terms of interpretability \cite{}, but the majority has targeted rather simple concepts. Furthermore, the equally critical generalization part has been largely neglected so far. \vt{double check}

% \myparagraph{Contributions} 
% In this study, we test the generalization hypothesis by focusing on a critical concept and more abstract feature than those typically studied: Answerability.
% % While it is hypothesized that LLMs have an internal representation of answerability, \vt{we do not find robust SAE features dedicated to this concept. Overall, traditional probing still outperforms SAEs in a variety of use cases.}
% \begin{itemize}[leftmargin=*,topsep=0pt,noitemsep]
%     \item We primarily focus on SAEs for Gemma \cite{lieberum2024gemma}, consider different layers' SAEs, and probe their features over a variety of popular and custom answerability datasets to faithfully evaluate generalization.
%     \item We find SAE features dedicated to the domains of individual of our datasets, compare such features in detail, and also study combinations of features.
%     \item \vt{In our experiments, individual features generalize rather poorly and also feature  combinations do not yield satisfactory performance. Overall, traditional probing, which is a much simpler alternative (e.g., in terms of design decisions in training, etc.), remains highly competitive.}
% \end{itemize}


% improve interpretability of large language
% models (LLMs) by learning generalizable features in an unsupervised manner. And recent
% studies show promising initial results.
%and uncertainty quantification.
% Sparse autoencoders (SAEs) have 
% \input{latex/preliminaries}
% \myparagraph{Existing Work} 



