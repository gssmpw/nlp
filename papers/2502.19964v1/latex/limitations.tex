\newpage
\section*{Limitations}
\label{sec:limitations}

We could only evaluate on Gemma 2 and the available SAEs since there are few high-quality SAEs for instruction-tuned models publicly available; we tried some available for LLama 3 \citep{dubey2024llama}, but could not find answerability SAE features.
%We did not investigate causal effects of SAEs and probes, possible that SAE features have the added advantage of being more relevant to the model's computation
Furthermore, the SAE features highly depend on the training hyperparameters and data they were trained on. Nevertheless, we would expect certain features to exist when aiming to use such SAEs in practice.
Finally, we used rather simple techniques for SAE probe training which did not yield best results. But this is the point we intend to make in this work, that the existing technology is insufficient in detecting generalizable SAE features.
%- SAE feature generalization is highly feature dependent even when multiple features achieve high in-domain probing accuracy, showing the need for methods that detect which features generalize
%- feature splitting is key problem in SAEs, even when finding a good SAE feature it is unclear how general it is. Abstract concepts potentially being split into many different features can cause problems when trying to utilize SAEs for practical applications like steering or detection
%A key issue for SAE probing could be feature splitting, a phenomenon where SAEs of different sizes learn features in different granularities, often splitting more general features into multiple more specialized features \citep{marks2024sparse, chanin2024absorption}. If abstract concepts like answerability are split into many separate features, this can cause problems for feature-based practical applications.
\section*{Acknowledgements}
Lovis Heindrich's work on this project was partially funded through a Manifund AI Safety grant. \\
We thank TVG interns and members—particularly Minseon Kim, Luke Marks, Clement Neo, Michael Lan, and Tingchen Fu—for weekly discussions and conversations about AI safety and interpretability. Lovis thanks Joseph Bloom for useful discussions. \\
The authors also thank the developers of TransformerLens \cite{nanda2022transformerlens} and SAELens \cite{bloom2024saetrainingcodebase}, open-source libraries for mechanistic interpretability. 