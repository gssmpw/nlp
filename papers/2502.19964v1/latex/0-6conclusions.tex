\section{Conclusions}

We extensively evaluated SAE features for Gemma~2 in the out-of-distribution scenario using a variety of established and custom datasets. On the bright side, we find good SAE features for answerability across these domains. However, we show from various angles that the standard SAE feature search fails in finding these features and hence in terms of generalization.
We hypothesize that this is due to both sub-optimal training objectives and feature splitting with complex concepts \cite{bricken2023monosemanticity, chanin2024absorption}.
This shows the need for better technology for evaluating SAE features before SAEs are robustly applicable in practice.

%A key issue for SAE probing could be feature splitting, a phenomenon where SAEs of different sizes learn features in different granularities, often splitting more general features into multiple more specialized features \citep{bricken2023monosemanticity, chanin2024absorption}. If abstract concepts like answerability are split into many separate features, this can cause problems for feature-based practical applications.