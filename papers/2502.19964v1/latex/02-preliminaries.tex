\section{Preliminaries}\label{app:prelims}

%\tbd{the basic autoencoder eq. (my colleagues didn't know what I mean exactly when I talked about this work)}

\paragraph{1- sparse SAE probes}
To evaluate how well SAE features predict a certain abstract feature, we utilize 1-sparse probes \cite{gurnee2023finding}. Specifically, we collect activations of a specific SAE feature on a contrastive dataset containing both answerable and not answerable examples, and fit a slope coefficient and intercept to predict the dataset label using linear regression. The Gemma 2 SAEs are trained using a JumpReLU activation function \cite{lieberum2024gemma}. We can sample SAE activations after the activation function (post-relu) or before (pre-relu).
Since there are more learnt features to be found in the latter setting, the main paper figures focus on that. However, we report all results for the post-relu setting in the appendix.


\paragraph{Residual stream probes}

Our residual stream probes are trained on model activations sampled from the model's residual stream. To avoid overfitting, we train the regression model using 5-fold cross validation and perform a hyperparameter optimization by sweeping over regularization parameters with 26 logarithmically spaced steps between 0.0001 and 1. To measure the variability of residual stream probes, we repeat our analysis 10 times with different randomly sampled training datasets.

\paragraph{N-sparse SAE probes}
To train SAE probes with more than 1 feature, we follow the general methodology of our 1-sparse probes. As testing all possible SAE feature combinations is computationally infeasible, we iteratively increase the number of features while testing only the most promising candidates for higher features combinations. Specifically, to find combinations of $k$ features, we use the top 50 best performing features of size $k-1$ and test all possible new combinations with the 500 best performing single SAE features. We use a constant regularization parameter of 1 for the probes, regardless of the number of features.

\paragraph{Feature similarities}

To calculate feature similarities, we use the cosine similarity of the corresponding SAE encoder weight and the slope coefficients of the linear probes trained on the residual stream. SAE features are only compared to other SAE features of the same SAE, and residual stream probes trained at the same location in the model as the SAE. To compare how similar differently sized groups of SAE features are to the residual stream probes, we calculate the mean absolute cosine sim of the top 10 best performing SAE features of a certain group size (1 to 5) with the 10 residual stream probes trained on different training subsets.

\section{Datasets}
\label{app:datasets}

\myparagraph{Full Dataset details}

\begin{itemize}[leftmargin=*,topsep=0pt,noitemsep]
    \item \textbf{SQUAD} \citep{rajpurkar2018know}:  Dataset consisting of a short context passage and a question relating to the context. We follow the training data split and prompting template provided by \citet{slobodkin2023curious}.
    \item \textbf{IDK} \citep{sulem2021we}: Dataset with questions in the style of SQUAD, containing both answerable and unanswerable examples. We specifically use the non-competitive and unanswerable subsets of the ACE-whQA dataset.
    \item \textbf{BoolQ\_3L} 
    \citep{sulem2022yes}: Yes/no questions with answerable and unanswerable subsets.
    \item \textbf{Math Equations}: Synthetic dataset contrasting solvable equations with equations containing unknown variables.
    \item \textbf{Celebrity Recognition}: Queries requiring knowledge about celebrities.
    For construction, we use a public dataset of actors and movies from IMDB\footnote{\url{https://www.kaggle.com/datasets/darinhawley/imdb-films-by-actor-for-10k-actors}}, and generate a list of the 1000 most popular actors after 1990, as measured by the total number of ratings their movies received. We construct an additional dataset of non-celebrity names by randomly generating first and last name combinations using the most common North American names from Wikipedia\footnote{\url{https://en.wikipedia.org/wiki/Lists_of_most_common_surnames_in_North_American_countries} and \url{https://en.wikipedia.org/wiki/List_of_most_popular_given_names?utm_source=chatgpt.com}}. 
\end{itemize}

\paragraph{Dataset sizes}

\begin{table}[h]
    \centering
    \begin{tabular}{lc}
        \hline
        Dataset & Size \\
        \hline
        SQUAD (train) & 2000 \\
        BoolQ (train) & 2000 \\
        SQUAD (test) & 1800 \\
        SQUAD (variations) & 1800 \\
        BoolQ (test) & 2000 \\
        IDK & 484 \\
        Equation & 2000 \\
        Celebrity & 600 \\
        \hline
    \end{tabular}
    \caption{Number of examples for each used dataset.}
    \label{table:dataset-size}
\end{table}

Table~\ref{table:dataset-size} shows the number of examples for each dataset used in our evaluation.


