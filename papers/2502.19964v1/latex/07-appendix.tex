%chatgpt helped me with this - needs more work 
\input{latex/02-preliminaries}

\section{Additional analysis}

\subsection{Answerability Detection at Different Layers} \label{app:eval-other-layers}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer20_post.png}
    \caption{Answerability detection accuracies for top SAE features (Layer 20, post-activation).}
    \label{fig:sae-probe_post20}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer20_pre.png}
    \caption{Answerability detection accuracies for top SAE features (Layer 20, pre-activation).}
    \label{fig:sae-probe_pre20}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer31_post.png}
    \caption{Answerability detection accuracies for top SAE features (Layer 31, post-activation).}
    \label{fig:sae-probe_post31}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\textwidth,trim={0 2.2cm 1cm 3.4cm},clip]{figures/all_layers_probe.png}
    \caption{Linear probe trained on Layer 20 and Layer 31 residual stream (SQuAD) and evaluated on IDK, BoolQ, Celebrity, and Equation. The plot shows the median accuracy including the first and third quartile.}
    \label{fig:res-probe-all}
\end{figure*}

We repeat our SAE feature analysis in Layer 20 of the model, as well as providing additional analysis for SAE features activations sampled after the activation function. Figure~\ref{fig:sae-probe_pre20} shows the Layer 20 results using activations sampled before the activation function, while Figures~\ref{fig:sae-probe_post20} and \ref{fig:sae-probe_post31} show analogous results when sampling SAE activations after the activation function. Sampling after the activation reduces the number of relevant features our probe finds, since many features are inactive. However, this does not change the overall results, as we still find features with good generalization performance. 

Figure~\ref{fig:res-probe-all} shows the probing accuracy for the residual stream linear probe for both Layer 20 and 31. The evaluation is repeated across 10 seeds with different training set splits. While the SAE features, as part of the pre-trained autoencoder model, do not heavily depend on the probing dataset, this is not necessarily true for the residual stream probe. The's probe performance across the out-of-distribution datasets varies strongly, indicating that the generalization performance heavily depends on the minor differences in the training data. 

\subsection{Prompt variations}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer31_pre_SQUAD_train_variation.png}
    \caption{Performance of top SAE features and the residual stream linear probe on variations of prompt used with the SQuAD dataset (layer 31, pre-activation).}
    \label{fig:prompt-variation}
\end{figure*}

\begin{table*}[h]
    \centering
    \begin{tabular}{lp{10cm}}
        \toprule
        Default & Given the following passage and question, answer the question:\newline Passage: \{passage\}\newline Question: \{question\} \\
        \midrule
        Variation 1 & Please read this passage and respond to the query that follows:\newline Passage: \{passage\}\newline Question: \{question\} \\
        \midrule
        Variation 2 & Based on the text below, please address the following question:\newline Text: \{passage\}\newline Question: \{question\} \\
        \midrule
        Variation 3 & Consider the following excerpt and respond to the inquiry:\newline Excerpt: \{passage\}\newline Inquiry: \{question\} \\
        \midrule
        Variation 4 & Review this content and answer the question below:\newline Content: \{passage\}\newline Question: \{question\} \\
        \midrule
        Variation 5 & Using the information provided, respond to the following:\newline Information: \{passage\}\newline Query: \{question\} \\
        \bottomrule
    \end{tabular}
    \caption{SQuAD prompt template variations.}
    \label{table:variations}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|l|p{10cm}|}
%         \hline
%         Default & Given the following passage and question, answer the question:\newline Passage: \{passage\}\newline Question: \{question\}\\
%         \hline
%         Variation 1 & Please read this passage and respond to the query that follows:\newline Passage: \{passage\}\newline Question: \{question\} \\
%         \hline
%         Variation 2 & Based on the text below, please address the following question:\newline Text: \{passage\}\newline Question: \{question\} \\
%         \hline
%         Variation 3 & Consider the following excerpt and respond to the inquiry:\newline Excerpt: \{passage\}\newline Inquiry: \{question\} \\
%         \hline
%         Variation 4 & Review this content and answer the question below:\newline Content: \{passage\}\newline Question: \{question\} \\
%         \hline
%         Variation 5 & Using the information provided, respond to the following:\newline Information: \{passage\}\newline Query: \{question\} \\
%         \hline
%     \end{tabular}
%     \caption{SQuAD prompt template variations.}
%     \label{table:variations}
% \end{table*}

We investigated if the SAE features or the residual stream probes are sensitive to small variations in the prompt. To evaluate this question, we created five variations of the prompt template used for the SQuAD training data (see Table~\ref{table:variations}). The results can be found in Figure~\ref{fig:prompt-variation}, and indicate neither the residual stream probe nor the SAE features are sensitive to this kind of variation. 

\subsection{In-domain SAE feature accuracies}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_top_feature_accuracies_in_domain.png}
    \caption{Performance of the top SAE feature's probing accuracy when training and evaluating features on each dataset individually (pre-activation).}
    \label{fig:top-in-domain}
\end{figure*}


Figure~\ref{fig:top-in-domain} shows the accuracy of 1-sparse SAE feature probes for each dataset individually, demonstrating that each of our contrastive datasets is detectable with a probing accuracy of over 80\%.

\subsection{SAE Feature Combination Analyses} \label{app:feature-combinations}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.8\textwidth,trim={0 2.2cm 0 3.4cm},clip]{figures/avg_feature_k.png}
%     \caption{Average accuracy vs.\ number of features. (Potentially add the probe line, etc.)}
%     \label{fig:sae-combi-probe}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/top_sae_feature_group_accuracies_k_L31_pre.png}
    \caption{Performance of top feature combinations (layer 31, pre-activation).}
    \label{fig:top-combis}
\end{figure*}

Figure~\ref{fig:top-combis} shows additional probing analysis for the best performing groups of SAE features up to a group size of five. Group performance is generally dominated by the best performing features and does not majorly exceed the performance of the strongest feature. 

% \subsection{Celebrity Dataset Evaluation}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.8\textwidth,trim={0 0 0 3.4cm},clip]{figures/celeb.png}
%     \caption{Comparison of top SAE features on the Celebrity dataset. \lh{remove?}}
%     \label{fig:celeb}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/hierarchical_sae_probe_layer20_pre.png}
    \caption{Accuracies of SAE probes trained on different numbers of SAE features (Layer 20, pre-activation).}
    \label{fig:sae-k-pre20}
\end{figure*}

Figure~\ref{fig:sae-k-pre20} shows additional analysis for SAE feature combinations in Layer~20, analogous to the results for Layer~31 given in Figure~\ref{fig:sae-k-pre31}.

\subsection{Cosine Similarities}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 0 0 3.4cm},clip]{figures/sae_probe_similarities_by_layer.png}
    \caption{Absolute cosine similarities of top 10 SAE features at different layers, compared with the residual stream probe.}
    \label{fig:similarity_k}
\end{figure*}

We conducted an additional similarity analysis for the top SAE feature groups of different sizes. The results can be found in Figure~\ref{fig:similarity_k} and show a clear trend of larger groups of features becoming more similar to the linear probes. This provides some weak evidence that by default, linear probes might learn more specialized directions that can be represented as a linear combination of more general SAE features. 

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer20_post.png}
%     \caption{Answerability detection accuracies for top SAE features (layer 20, post-activation).}
%     \label{fig:sae-probe_post20}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer20_pre.png}
%     \caption{Answerability detection accuracies for top SAE features (layer 20, pre-activation).}
%     \label{fig:sae-probe_pre20}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer31_post.png}
%     \caption{Answerability detection accuracies for top SAE features (layer 31, post-activation).}
%     \label{fig:sae-probe_post31}
% \end{figure*}


% % \begin{figure*}[h!]
% %     \centering %trim titles
% %     \includegraphics[width=.7\textwidth,trim={0 2.2cm 4.2cm 3.4cm},clip]{figures/res_probe.png}
% %     \caption{\vt{can we have less space between the individual plots and make the bars thinner? best so that it fits next to fig 1. also: there's an alternative pic. image.png what's that?}}
% %     \label{fig:res-probe}
% % \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.7\textwidth,trim={0 2.2cm 1cm 3.4cm},clip]{figures/res_probe_20.png}
%     \caption{Linear probe trained on the Layer 20 residual stream. The probe is trained on the SQUAD dataset and then evaluated on the out-of-distribution datasets (IDK, BoolQ, Celebrity, Equation). Error bars show the standard deviation, averaged over 10 bootstrap samples.}
%     \label{fig:res-probe-20}
% \end{figure*}


% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=.8\textwidth,trim={0 2.2cm 0 3.4cm},clip]{figures/avg_feature_k.png}
%     \caption{\vt{can we add the probe in here?}}
%     \label{fig:sae-combi-probe}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/top_features_k.png}
%     \caption{\vt{later, better make the figures more space efficicient, borders which I cannot crop}}
%     \label{fig:top-combis}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 0 0 3.4cm},clip]{figures/celeb.png}
%     \caption{\vt{can we add the probe directly in here? and maybe remove some of the ones of the right hand side to make it fit one column. maybe we also don't need this plot extra. let's see/discuss}}
%     \label{fig:celeb}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/hierarchical_sae_probe_layer20_pre.png}
%     \caption{Accuracies of SAE probes trained on different numbers of SAE features (Layer 20, pre-activation}
%     \label{fig:sae-k-pre20}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/hierarchical_sae_probe_layer20_post.png}
%     \caption{Accuracies of SAE probes trained on different numbers of SAE features (Layer 20, pre-activation}
%     \label{fig:sae-k-post20}
% \end{figure*}


% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/hierarchical_sae_probe_layer20_pre.png}
%     \caption{Accuracies of SAE probes trained on different numbers of SAE features (Layer 31, post-activation}
%     \label{fig:sae-k-post31}
% \end{figure*}




% \begin{figure*}[h!]
%     \centering %trim titles
%     \includegraphics[width=.8\textwidth,trim={0 0 0 3.4cm},clip]{figures/sae_probe_similarities_by_layer.png}
%     \caption{Absolute cosine similarities of top 10 SAE features for different number of feature combinations, the model layer of the SAE, and activation hook points, compared with the residual stream probe.}
%     \label{fig:similarity_k}
% \end{figure*}

% \begin{figure*}[h!]
%     \centering %trim titles
%    \includegraphics[width=.4\textwidth,trim={0 2.2cm 1cm 3.4cm},clip]{figures/res_probe_31.png}
% \caption{..%Comparison between top SAE features (left), pre-activation, and linear probes (right) on layer 31.
% }
%     \label{fig:probe_pre31}
% \end{figure*}

% \begin{figure}[h!]
%     \centering %trim titles
%     \includegraphics[width=.5\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_top_feature_accuracies_in_domain.png}
%     \caption{Top SAE feature accuracies when training the 1-sparse probes on each dataset individually using a 20\% test split (pre-activation function).}
%     \label{fig:sae-in-domain}
% \end{figure}


% mention why we focus on layer 31 pre -relu





% \myparagraph{Dedicated Prompts} % prompts dedicated to the task
% QA prompt variation


%(only use 80\% of training data here for some reason, probably matching some earlier version of model probes - could rerun overnight).


\myparagraph{Other experiments}
We validated our setup by searching for bias-related features as it was done in related works.
We also experimented with (inofficial) SAEs for an instruction-tuned Llama model, but could not find SAE features with sufficient in-domain probing accuracy. Finally, we also performed analysis on Gemma 2 2B and also the base models, but performance on the answerability task was relatively low in these models (the best SAE features achieved around 70\% probing accuracy).