\section{Related Works}

SAEs were proposed early for LM interpretability \cite{yun2021transformer}.
%
Many studies focus on improving training efficiency and effectiveness, but the latter is usually measured in terms of reconstruction quality and hence disconnected from downstream scenarios \cite{rajamanoharan2024improving,lieberum2024gemma}. %templeton2024scaling
Only most recent work addresses this issue.
% Scaling and evaluating sparse autoencoders
\citet{gao2024scaling} apply downstream probing, yet the classification tasks considered are most simple (e.g sentiment, language identification) and likely do not test any generalization. %.
% Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control
Similarly, \citet{makelov2024towards} use downstream data without considering generalization.%. However, the method applies downstream data during SAE training without considering generalization specifically and hence will likely suffer in out-of-distribution cases.

% Several works focus on downstream evaluation
% % Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks
% % Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
% \citet{} propose a method for automated downstream evaluation which however uses the SHIFT approach \cite{} which requires datasets and hence will similarly suffer in ood scenarios.


Several studies evaluate SAE features, including downstream settings.
Yet, research often focuses on simple models and features 
% https://transformer-circuits.pub/2023/monosemantic-features/index.html
\cite{yun2021transformer,bricken2023monosemanticity,kissane2024interpreting}.
%
% IOI > Sparse Autoencoders Match Supervised Features for Model Steering on the IOI Task
% IOI ># 24-iclr-SPARSE AUTOENCODERS FIND HIGHLY INTERPRETABLE FEATURES IN LANGUAGE MODELS
% BIAS> Effectiveness of Sparse Autoencoder for understanding and removing gender bias in LLMs
% SV > Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
There are various works focusing on more abstract concepts such as %bias \cite{}, 
indirect object identification \cite{cunningham2023sparse} and subject-verb agreement \cite{marks2024sparse},
but those are still directly related to syntax. In contrast, answerability often depends on domain-specific background knowledge (e.g., math or factual knowledge) and hence better suits the study on generalization.
%
% SPARSE AUTOENCODERS REVEAL TEMPORAL DIFFERENCE LEARNING IN LARGE LANGUAGE MODELS
\citet{demircan2024sparse} consider the representation of quality estimates from reinforcement learning, and hence a rather complex concept, but they focus on task-specific (vs generalizable) SAEs.
%
Closest to our work are the following.
%
% https://transformer-circuits.pub/2024/features-as-classifiers/index.html
\citet{bricken2024using} focus on comparing SAE features to linear probes as bioweapon classifiers.
Similarly to us, they show that the SAE probes are competitive but more complex and brittle; for instance, already format mismatch between the transformer/SAE/probe training data may degrade performance. When evaluated on multilingual out-of-distribution data (similar to in-domain data but in different languages), they find that SAE features can generalize well in specific settings in the mostly lexical task of bioweapon classification. 
%However, their SAEs are trained on smaller models and synthetic, domain-specific, biology data, and they only consider multi-lingual out-of-domain settings.
%
% https://www.lesswrong.com/posts/NMLq8yoTecAF44KX9/sae-probing-what-is-it-good-for-absolutely-something
\citet{kantamneni2024saeprobing} similarly conduct experiments comparing to traditional probing on activations and demonstrate that SAEs work better in certain scenarios (e.g., with very small datasets or corrupted data). They also consider multilingual out-of-distribution data and, % and imbalanced class data are considered and, 
similar to us, obtain mixed results without clear conclusion which probes are better.
%Note that they use the Gemma 9B base model and hence can only consider simpler features. 
Our evaluation lifts this work to %the instruction-tuned model version, 
a more complex task and a greater variety of distributions.
%The key differences in our work is that we evaluate generalization in a more complex and abstract setting, and test a wider variety of out of distribution data.
%VT that's exactly below no?

% Several studies have explored the use of SAEs in language models, with many focusing on improving training efficiency and reconstruction quality \cite{gao2024scaling, rajamanoharan2024improving, templeton2024scaling}. Some approaches optimize SAE training using proxy metrics—such as reconstruction loss—to enhance training stability. However, while these methods improve performance in terms of efficiency, they do not directly assess downstream performance. %, whether the resulting features capture abstract linguistic phenomena that generalize across tasks \citep{rajamanoharan2024improving,rajamanoharan2024jumping}. 
% % In contrast, our work evaluates whether SAE features encode abstract concepts by testing their transferability on an answerability task.
% %VT for space, these are clearly not directly related to what we do

% Other works have examined the properties of SAE features through probing and qualitative analysis. For instance, studies have investigated how scaling affects SAE representations and have qualitatively assessed their linguistic functions \citep{templeton2024scaling,kissane2024interpreting}. In addition, approaches that align network features to specific functions—such as the feature-aligned sparse autoencoders introduced by \citet{marks2024enhancing}—aim to improve interpretability. However, these methods predominantly focus on in-domain performance, leaving open questions about whether the learned features can generalize to new contexts. Similarly, feedback pattern analysis in language models, as demonstrated by \citet{marks2024enhancing}, does not address whether these features are robust enough to capture abstract capabilities across diverse domains. % tasks. % VT we study one task right? just different domains

% Alternative methodologies have interpreted SAE features in terms of neural circuits, proposing that they serve as modular building blocks underlying model behavior. One circuit-based approach maps processing components within language models \citep{dunefsky2024transcoders}. This perspective, however, generally assumes that features function as well-defined, modular units—a notion that may not hold when features encode abstract concepts such as answerability.

% Foundational work on sparse representations provided early insights into the benefits of sparsity for disentangling features \citep{ng2011sparse}, yet these methods were developed in simpler settings and do not directly address the challenges of extracting and generalizing abstract features in modern large language models.

In summary, %while prior work has advanced SAE training techniques, probed in-domain feature behavior, and proposed circuit-based interpretations, systematically evaluated whether the abstract features captured by SAEs can generalize across domains
prior work has largely neglected generalization beyond multilingual scenarios. Our study closes this gap in the context of a suitable, complex concept, answerability, which likely manifests differently across contexts.
%by comparing SAE features with residual stream probes on an answerability task, thereby testing the robustness and transferability of abstract representations.

% Sparse autoencoders have been studied in the context of LLMs from various angles. Several works focus on the training of SAEs itself, both in terms of efficiency and effectiveness \cite{gemmascope,someothers,rajamanoharan2024improving,rajamanoharan2024jumping}. While this training includes evaluations, these experiments typically do not test the learnt features themselves but rather consider proxies closer to the actual training (e.g., related to the reconstruction loss). Further, please observe, that some recent studies have pointed out that SAE training \vt{concretize issues, harvard paper, +other?} \cite{}.

% Closest to our study are a few works which probe the trained SAE features \cite{}. \vt{need details about specific studies here, spec. delimitation/differences/commonalities}
% \cite{templeton2024scaling}
% \cite{kissane2024interpreting}
% % le cunn
% % the blog post

% % other works:
% \vt{some sentence about polysemanticity and SAEs}
% %
% There are also works which consider SAE features in the context of so-called circuits, models trying to capture specific LLM processing mechanisms in the form of algorithms in interpretable ways \cite{marks2024sparse,dunefsky2024transcoders}.
% % 
% Finally, note that SAEs have been studied and used in various other contexts for a longer time, before transformers even existed \cite{ng2011sparse,someother?}.
%NOTE mention other answerbility approaches maybe

% old from lovis:
% Mechanistic interpretability is a growing field that aims to develop a grounded understanding of neural networks by reverse engineering its computations. An important framework of understanding neural networks is through the lens of circuits, the idea that analyzing weight connections inside a neural network allows us to identify computational sub-graphs that implement specific algorithms \cite{elhage2021mathematical, olah2020zoom}. A key problem for finding interpretable circuits and features in transformer models has been that language models are often polysemantic, they represent more features than they have parameters, which can result in model components that are difficult to interpret since they represent more than one feature \cite{elhage2022superposition}. 

% One promising approach to overcoming the problem of polysemanticity is using sparse autoencoders (SAE) to learn a disentangled feature representation \cite{cunningham2023sparse, bricken2023monosemanticity}. Recent work has proposed improvements to the SAE architecture and applied SAE to much larger models \cite{gao2024scaling, templeton2024scaling, rajamanoharan2024jumping}, which allows studying more abstract and interesting features. SAEs have been found to learn both interpretable and causally important features that are relevant for safety, such as features relating to unsafe code or deception \cite{templeton2024scaling}. While the existence of these features already provides useful information about the model's reasoning, understanding in which scenarios these features activate could deepen our understanding of the computation inside language models significantly. Circuit discovery methods are a promising approach to answering these questions. 

% Circuit discovery methods aim to (automatically) determine which components of a language model are responsible for a behavior \cite{conmy2023towards, syed2023attribution}. While usually used to attribute model behavior to individual components of language models, circuit discovery has recently been extended to utilize SAEs as well \cite{marks2024sparse}. Applying circuit discovery to understand SAE features allows isolating the upstream components of the model that are responsible for activating a specific downstream SAE feature of interest.

% In addition to understanding specific SAE features, using SAE features instead of general model behavior as the target metric for circuit discovery also has the advantage of presenting a clear metric for potentially complex tasks that are hard to evaluate. Existing circuits usually describe a very narrow behavior that can be evaluated through individual answer tokens, and it is not clear how these metrics can be extended to more complex model behavior (e.g. being sycophantic). While it is unlikely that there are single SAE features that fully represent the model's reasoning for such complex model behaviors, causally relevant SAE features related to safety concepts have been found in prior work. We believe using such SAE features to present an interesting target metric for circuit discovery that can provide insights into at least part of the overall circuits making up complex model behavior.