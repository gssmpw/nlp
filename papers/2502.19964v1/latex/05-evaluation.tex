\begin{figure*}[h!]
    \centering %trim titles
     \includegraphics[width=.5\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer31_pre_SQUAD_train.png}%{figures/sae_feature_accuracies_layer31_pre.png}
    ~%
    \includegraphics[width=.5\textwidth,trim={0cm 1.4cm 0 3.4cm},clip]{figures/sae_feature_accuracies_layer31_pre_BoolQ_train.png}
\caption{Out-of-distribution comparison between top SAE features,  pre-activation, and linear probes on layer 31; trained on SQUAD (left) and BoolQ (right).}
    \label{fig:sae-probe_pre31}
\end{figure*}

\begin{figure}[h!]
    \centering %trim titles
    \includegraphics[width=.5\textwidth,trim={0 1.4cm 0 3.4cm},clip]{figures/hierarchical_sae_probe_layer31_pre.png}
    \caption{Combinations of SAE features, displaying the median value across top feature groups with quartile ranges in the error bars.}
    \label{fig:sae-k-pre31}
\end{figure}

% \begin{figure*}[t]
% \centering

% %--------------- First row ---------------%
% \begin{subfigure}[t]{0.49\textwidth}
%   \centering
%   \includegraphics[width=\linewidth,
%                    trim={2cm 1.4cm 0 3.4cm},clip]
%                    {figures/sae_feature_accuracies_layer31_pre_SQUAD_train.png}
%   \caption{Out-of-distribution comparison on layer 31, trained on SQuAD.}
%   \label{fig:sae-probe_pre31-squad}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.49\textwidth}
%   \centering
%   \includegraphics[width=\linewidth,
%                    trim={2cm 1.4cm 0 3.4cm},clip]
%                    {figures/sae_feature_accuracies_layer31_pre_BoolQ_train.png}
%   \caption{Out-of-distribution comparison on layer 31, trained on BoolQ.}
%   \label{fig:sae-probe_pre31-boolq}
% \end{subfigure}

% \vspace{0.5em}  % Tighten or remove if desired

% %--------------- Second row ---------------%
% \begin{subfigure}[t]{0.49\textwidth}
%   \centering
%   \includegraphics[width=\linewidth,
%                    trim={0 1.4cm 0 3.4cm},clip]
%                    {figures/hierarchical_sae_probe_layer31_pre.png}
%   \caption{Combinations of SAE features.}
%   \label{fig:sae-k-pre31}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.30\textwidth}
%   \centering
%   \includegraphics[width=\linewidth,
%                    trim={5.2cm 4.1cm 0 3.4cm},clip]
%                    {figures/feature_similarities.png}
%   \caption{Cosine similarities of top SAE features and probes.}
%   \label{fig:similarities}
% \end{subfigure}

% \caption{
%   Four related plots: 
%   (a) and (b) show out-of-distribution comparisons on layer 31 
%   for SQuAD vs.\ BoolQ training;
%   (c) demonstrates combinations of SAE features;
%   (d) shows feature/probe cosine similarities.
% }
% \label{fig:all_four}
% \end{figure*}




\section{Evaluation}



In the following, we present of our main experiments; see the appendix for additional findings.

% Todo
% Get some formatted dataset examples for all datasets
% Try question in celeb dataset (how old)
% Check probe accuracy and generalization for different training data sizes
% Averaging linear probes as baseline?
% Use higher k
% Prioritize over weekend with higher number of features + OOD eval
% Baseline attention probe
% Change cosine similarity analysis to average absolute
% Add clean reconstruction result of residual stream probe with SAE
% Cosine sim of residual stream probes with top SAE features
% Probes with each other
% Probes with top sae features
% Check pre vs post relu
% Compare features to narrow SAE
% Compare to Layer 31 SAE
% Is it more sparse? Less combinations needed? Then it is a feature that is not fully computed in L20?




\myparagraph{Linear vs SAE Probes: Generalization, Figure~\ref{fig:sae-probe_pre31}} 
We focus first on layer 31.
In domain, the best SAE features reach an accuracy of around 0.8 while the linear probes reach 0.9. Note that this is not surprising, since the probes have more parameters that are actually trained and thus optimized for this data. Nevertheless, it shows some advantage of probes in case in-domain data is available. 

We see rather great variation across our out-of-distribution datasets. Our custom Equation data stands out in that several SAE features and also the probe reach high performance. While this seems to show that the mathematical context makes answerability easier to detect, observe that the performance is considerably worse on layer 20, see Figure~\ref{fig:sae-probe_pre20} in the appendix. %This shows that it is a complex concept only fully represented later in the model.

Some, but few, top SAE features reach considerable performance out of distribution on IDK - matching the performance of the linear probe - and Celeb. Yet, the performance on BoolQ is considerably bad. %\vt{REASONS?} 
On the other hand, the linear probe performs bad on Celeb. Figure~\ref{fig:res-probe-all} in the Appendix shows the median value over 10 bootstrap samples including quartiles in the error bars; overall it correlates with performance.

For layer 20 (Figure~\ref{fig:sae-probe_pre20}), we see generally worse performance. 
Interestingly, the numbers for Celeb are significantly worse than all others for both the SAE and the linear probes. Since we see one exception (an SAE feature with higher than random performance), we hypothesize that there are special features encoding knowledge about celebrities which do not happen to be among our top features. % ie rather than the knowledge is not present on layer 20
%this might also explain why the probe performs bad on celeb that this would be more about people features not answerability
In fact, a closer investigation reveals that there are good features for BoolQ and our domain-specific Equation and Celeb datasets on layer 20 already (see Figure~\ref{fig:top-in-domain} in the appendix), but they are not the same features as the ones found by training on SQuAD. %TODO

Finally, we confirmed our findings by training on  BoolQ (also 2k samples) and evaluating on the other datasets. We mainly see that varying the training data can make everything considerably worse, even with the same task and seemingly similar, but potentially lower-quality data. The unanswerable samples in BoolQ were constructed by combining contexts and questions of similar dataset samples, hence capture only one type of unanswerability.

Overall, our experiments demonstrate one main critical issue with OOD data: \emph{the standard procedure for finding good SAE features can easily fail, even if good features are available}. 
The fact that good features exist while the linear probes also fail shows some potential of SAEs. Yet finding good, generalizing features represents an open challenge.

%A key issue for SAE probing could be feature splitting, a phenomenon where SAEs of different sizes learn features in different granularities, often splitting more general features into multiple more specialized features \citep{bricken2023monosemanticity, chanin2024absorption}. If abstract concepts like answerability are split into many separate features, this can cause problems for feature-based practical applications.

\myparagraph{Top Features, %Distributions, 
Figure~\ref{fig:sae-probe_pre31}} 
Interestingly, the top three features on the in-domain SQuAD data happen to also generalize better here. This does not turn out be the case beyond the top-1 feature more generally, see Appendix~\ref{app:eval-other-layers}. For BoolQ, the variability of the results precludes clear conclusions.
%

\myparagraph{SAE Feature Combinations, Figure~\ref{fig:sae-k-pre31}} 
Given the partly domain-specific nature of our out-of-distribution datasets, we hypothesized that combinations of features might work better as general probes. %Observe that such combinations have also been considered in previous works \cite{lecun,oneother}.
However, while increasing the number of SAE features improves the in-domain performance, OOD performance doesn't improve upon the best performing individual feature (top of blue error bars) here; layer 31, pre-activation. Other examples in Appendix~\ref{app:feature-combinations} show similar trends, and even some degradation. This underlines our above finding that the ood setting requires better methods for SAE feature search.

% \begin{wrapfigure}
% {r}{0.25\textwidth}

\begin{figure}[h!]
    \centering %trim titles
    \includegraphics[width=.18\textwidth,trim={5.2cm 4.1cm 0 3.4cm},clip]{figures/feature_similarities.png}
    \caption{Cosine similarities of top SAE features and linear probes for different seeds; the blue square shows high similarity between linear probes.}
\label{fig:similarities}
\end{figure}
  
% \end{wrapfigure}
\myparagraph{Feature Similarity, Figures~\ref{fig:similarities} \& \ref{fig:similarity_k}}

We find great similarity between different linear probes but only slight similarity between SAE features and individual probes, and it's even less between SAE features. Interestingly, the best SAE feature turns out to have highest (though low) similarity with the probes. Figure~\ref{fig:similarity_k} shows that combining SAE features yields greater similarity with linear probes.




% \myparagraph{Other experiments}
% We validated our setup by searching for bias-related features as it was done in related works.
% We also experimented with (inofficial) SAEs for an instruction-tuned Llama model, but the quality of the SAEs was not good enough for further experimentation. Finally, Gemma 2 2B and also the base models \vt{did not yield good enough performance on the question answering task itself}. 