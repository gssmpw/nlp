% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}
\usepackage{lipsum}
% Standard package includes
\usepackage{wrapfig}
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{subcaption}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{pifont} % for shape symbols

% Define short commands for each shape
% \newcommand{\affcircle}{\ding{108}}  % ●
% \newcommand{\affsquare}{\ding{110}}  % ■
% \newcommand{\affdiamond}{\ding{117}} % ◆
% \newcommand{\affstar}{

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


% \newcommand{\vt}[1]{{\color{blue} [VT: #1]}}
% \newcommand{\fb}[1]{{\color{magenta} [FB: #1]}}
% \newcommand{\lh}[1]{{\color{cyan} [LH: #1]}}

%\newcommand{\tbd}[1]{{\color{red} [#1]}}

% \newcommand{\myparagraph}[1]{\textbf{#1.}}
\newcommand{\myparagraph}[1]
{\paragraph{#1.}}
%{\textbf{#1.}}


\title{Do Sparse Autoencoders Generalize? A Case Study of Answerability}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{%
  \textbf{Lovis Heindrich}\textsuperscript{}\thanks{Work done during a research visit at University of Oxford.} \quad
  \textbf{Philip Torr}\textsuperscript{\ddag} \quad
  \textbf{Fazl Barez}\textsuperscript{\ddag,\S}\thanks{Equal advising\\
  Corresponding author: \url{fazl@robots.ox.ac.uk}} \quad
  \textbf{Veronika Thost}\textsuperscript{\P}\footnotemark[2] \\
  \\[-0.8em]  % small vertical gap
  %\textsuperscript{\dag}Independent \\
  \textsuperscript{\ddag}University of Oxford 
  \textsuperscript{\S}WhiteBox
  \textsuperscript{\P}MIT-IBM Watson AI Lab
}
%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}



\begin{document}
\maketitle

%\vt{test}
% \fb{Use this link to work on this paper: \url{https://www.overleaf.com/7937183635pqsthzqbhhmg#4b4aaa}}
%\lh{test}

\begin{abstract}
Sparse autoencoders (SAEs) have emerged as a promising approach in language model interpretability, offering unsupervised extraction of sparse features. For interpretability methods to succeed, they must identify abstract features across domains, and these features can often manifest differently in each context. We examine this through "answerability"—a model's ability to recognize answerable questions. We extensively evaluate SAE feature generalization across diverse answerability datasets for Gemma 2 SAEs. Our analysis reveals that residual stream probes outperform SAE features within domains, but generalization performance differs sharply. SAE features demonstrate inconsistent transfer ability, and residual stream probes similarly show high variance out of distribution. Overall, this demonstrates the need for quantitative methods to predict feature generalization in SAE-based interpretability.

% % X: What and why
% Sparse autoencoders (SAEs) have emerged as
% a promising %dominant 
% approach in language model interpretability, offering unsupervised 
% extraction of sparse features. %While SAEs effectively capture syntax patterns, their ability to capture abstract linguistic phenomena remains untested.
% % Y: Why hard
% For interpretability methods to succeed, they
% must identify abstract features across domains, and these features can often manifest differently in each context.
% % Z: Our solution
% We examine this through "answerability"—a
% model's ability to recognize answerable questions. We extensively evaluate %analyze SAEs trained on Gemma 2, evaluating 
% SAE feature generalization
% across diverse answerability datasets % tasks
% for Gemma 2 SAEs.
% % 1a: Results
% Our analysis reveals that residual stream probes
% outperform SAE features within domains, but
% generalization performance differs sharply.
% SAE features demonstrate inconsistent transfer
% ability, and residual stream probes similarly show high
% variance out of distribution.
% %in out-of-distribution performance.
% % 1b: Implications
% % These results expose specific limitations in
% % SAE approaches for abstract concepts. While
% % certain features transfer successfully, we
% % cannot yet predict which will generalize.
% Overall, this demonstrates the need for quantitative
% methods to predict feature generalization
% in SAE-based interpretability.
% % X: What are we trying to do and why is it relevant?
% Sparse autoencoders (SAEs) have emerged as a dominant approach in language model interpretability, promising unsupervised feature extraction. While effective for concrete concepts like syntax, their ability to capture abstract linguistic phenomena remains untested.
% % Y: Why is this hard?
% For interpretability methods to be useful, they must reliably identify abstract features across different domains---a challenge as these features often manifest differently across contexts.
% % Z: How we solve it (our contribution)
% We examine this through ``answerability''---a model's ability to recognize answerable questions. This capability provides an ideal test case as it is present across diverse tasks and requires sophisticated reasoning. We analyze SAEs trained on different layers of Gemma [add models name here], evaluating feature generalization systematically.
% %1a: Experiments and results 
% Our analysis shows while residual stream probes outperform SAE features within domains, generalization shows mixed results. SAE features vary significantly in their transfer ability, and residual stream probes, despite strong in-domain performance, exhibit high variance in generalization---with both methods showing advantages in different out-of-distribution scenarios.
% % 1b: Implications 
% These findings suggest both promise and limitations in current SAE approaches for extracting abstract concepts. While some SAE features show impressive generalization, identifying which features will transfer well remains an open challenge. Our results highlight the need for better methods to predict and ensure feature generalization in SAE-based interpretability. 

% % X: What are we trying to do and why is it relevant?
% Sparse autoencoders (SAEs) have emerged as a dominant approach in language model interpretability, promising to extract meaningful features without supervision. While recent work demonstrates their effectiveness for concrete concepts like syntax and sentiment, their ability to capture abstract linguistic phenomena remains untested.
% % Y: Why is this hard?
% The key challenge lies in generalization: for interpretability methods to be truly useful, they must reliably identify abstract features across different domains and tasks. This is particularly difficult as abstract concepts often manifest differently across contexts, making it unclear whether current SAE methods can capture such high-level representations.
% % Z: How we solve it (our contribution)
% We study this question through the lens of ``answerability''---a model's ability to recognize whether it can answer a question. This abstract capability provides an ideal test case: it manifests across diverse tasks, requires sophisticated reasoning, and is fundamental to model behavior. We  analyze SAEs trained on different layers of the Gemma language model, developed methods to evaluate feature generalization.
% % 1a: Experiments and results
% Our experiments reveal clear limitations: while SAE features successfully detect answerability within specific domains, they fail to generalize across different datasets. Notably, traditional probing methods consistently outperform our SAE features on standard benchmarks like SQuAD and BoolQ, even when we combine multiple features or target deeper model layers where abstract reasoning should be easily found.
% % 1b: implications
% These findings points to a fundamental constraint: current SAE approaches appear inadequate for extracting abstract, generalizable concepts from language models. The superior performance of simpler probing methods suggests these abstract features exist within the model but remain inaccessible to current SAE techniques, challenging their broader applicability for model interpretability.


% Sparse autoencoders (SAEs) have the potential to improve interpretability of large language models (LLMs) by learning generalizable features in an unsupervised manner. And recent studies show promising initial results. In this paper, we challenge the SAE approach by focusing on a more abstract feature than those typically studied: Answerability, a concept which is important across diverse tasks and hence suits our focus of generalization. While it is hypothesized that LLMs have an internal representation of answerability, \vt{we do not find robust SAE features dedicated to this concept. Overall, traditional probing still outperforms SAEs in a variety of use cases.}
%focus on context-based QA
\end{abstract}

% prompt examples
% probe on reconstruction
% probe vs top features extra combined plot
% how to extract/find diff features
% pre vs post relu ones. afterwards less are active but training makes pre also kind of realistic

%smaller vs wider first gets more general SAEs
% could still be later one feature

\input{latex/01-intro}
% \input{latex/preliminaries}
%\newpage
% \input{latex/02-preliminaries}
\input{latex/03-related}
%\newpage
\input{latex/04-method}
%\newpage
\input{latex/05-evaluation}
\input{latex/0-6conclusions}

%\newpage~\newpage~\newpage~\newpage
\input{latex/limitations}
\input{latex/ethics}



% \section*{Acknowledgments}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{main}

\appendix
\label{sec:appendix}
\input{latex/07-appendix}
% This is an appendix.

\end{document}
