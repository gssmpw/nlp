\section{RQ3: How reliable are the DDPs?}
\label{Sec: Reliability}

In this section, we investigate how reliable the DDPs are.
We first define the requirements for quantifying the reliability of DDPs.
Next, we outline the data collection employed to evaluate their reliability.
Finally, we conduct a comprehensive assessment of the reliability of DDPs for the three platforms. 

\subsection{Requirements for reliability}
\label{Sec: Reliability_Desiderata}

86\% of researchers who participated in a Data Donation Symposium 
reported encountering problems with the DDPs provided by various platforms~\cite{valkenburg2024time}.
The primary concerns identified include: (1) incompleteness, such as missing data types or limited time ranges, and (2) inaccuracy, such as wrong timestamps, incorrect ordering which renders the data unreliable to both end users and researchers. 
Therefore, due to the popularity of using DDPs to conduct research, there is a pressing need to systematically assess how reliable DDPs are.
Hence we evaluate the reliability of DDPs on three requirements.


\noindent
\textbf{Completeness}: Evaluates whether the platforms provide a comprehensive record of all user activities or whether the data is subject to sampling or omissions.\\ 
\noindent
\textbf{Correctness}: Determines whether the sequence of recorded activities in the DDP accurately reflects the chronological order of events and whether any wrong entries are present.\\
\noindent
\textbf{Consistency}: Assesses the uniformity of data across users and within users across different retrieval instances.


\subsection{Experimental setup}
\label{Sec: Reliability_Experimental_Setup}

We collect two types of  data: (a)~from real-world users, and (b)~utilizing bot accounts.



\noindent
\textbf{Real-world users}
Inspired by prior work, we designed our own data donation website, which is privacy preserving and allowed us to obtain DDPs from real-world users~\cite{zannettou2024analyzing}. To ensure user privacy, all collected data was anonymized and any PII was discarded on the front-end of our data donation website. 
During the data collection process, we intentionally mandated the donation of specific data categories for each platform.
For TikTok and YouTube, video browsing history was mandatory.
In the case of Instagram, the mandatory categories included (a) like history and (b) ads and topics, which consist of posts, ads, videos viewed, and ads clicked.
Other data categories, such as search history, comments, etc., were optional and users provided them at their discretion.
Private messages and other sensitive data were not collected. 
Users were compensated based on the categories and amount of data they donated.
The maximum payment offered was \$16 for TikTok, \$9 for YouTube, and \$14 for Instagram data donations.
Data collection for TikTok and YouTube took place between August and September 2024, while for Instagram, it was conducted between December 2024 and January 2025.
We collected data from the same set of 33 users across all three platforms.
17 (51.5\%) male and 16 (48.5\%) female participants donated their data. 

\noindent
\textbf{Bot accounts}
We automated the process of video browsing and video liking activities using \texttt{pyautogui}~\cite{pyautogui} to control mouse and keyboard and \texttt{opencv-python}~\cite{opencv} to detect the like button on the desktop screen.
This approach differs from traditional browser automation~\cite{10.1145/3485447.3512102}, as we directly controlled the desktop environment instead of interacting with the browser via scripts.
We captured interactions with the browser by downloading the \texttt{HAR (HTTP Archive) files} (HAR is a JSON-formatted file that logs all network traffic between the browser and the server).
We viewed between 20 to 25 short-format videos in each video browsing session on each of the three platforms.
Our bot accounts watched each video for a random duration, ranging from 15 to 60 seconds, and they randomly liked some of the videos.
This approach allowed us to gather accurate ground truth data regarding user activities.
The process was conducted for nearly thirty video browsing sessions between October 2024 and December 2024 for each platform.
To conduct our analysis, we collected multiple DDP snapshots from each of these bot accounts. 

\subsection{Observations}
\label{Sec: Reliability_Observations}

\noindent
\textbf{Completeness}: As mentioned in \Cref{Sec: Usage}, 
Instagram and TikTok do \textit{not} provide the entire watch history of a user, since they register on the platform, without any explanations.
Hence, these DDPs are inherently incomplete and \textit{potentially violate Article 15(1)(d)}.  
However, with whatever data they share, to evaluate completeness, we utilize the data collected from the bot accounts.  
To this end, we compared the number of activities recorded in the HAR dump with those in the DDP for each video browsing session. 
We used the DDP that was closest in time to the corresponding HAR dump.
Instagram offers two video browsing feeds: (1) home feed, and (2) reels feed.
Interestingly, we found that videos watched under the reels feed were not present in the DDP, although like activities associated with them were recorded.
For Instagram's home feed, the percentage of recorded entries in the DDP for browsing videos and likes is 100\%.
On TikTok, all browsing and like activities were fully recorded, with a completeness rate of 100\%.
For YouTube, the recorded entries accounted for 99.5\%  
of the activities observed in the HAR dump.  

\noindent
\textbf{Correctness}:
After assessing the completeness of DDPs, we evaluated its correctness using the same data from bot accounts, by checking whether the order of contexts was maintained and whether any arbitrary entries were present in the DDP or not.
To achieve this, we measured the Jaccard similarity for the following aspects between HAR and DDP for each activity type.

%\begin{itemize}
\noindent
$\bullet$ \textbf{Date}: Consistency of recorded timestamps. We compared them up to the minute level to allow for slight variations.

\noindent
$\bullet$ \textbf{Context}: Consistency of video IDs or author IDs.

\noindent
$\bullet$ \textbf{Overall}: A combined evaluation of date and context.~\Cref{tab:correctness} presents the average Jaccard scores across the video browsing sessions for the three platforms.
For Instagram, there is a drop in the Jaccard scores of browsing, because of changes in usernames, i.e, account username change or account deletion.
For TikTok, Jaccard scores achieved 100\% accuracy across both browsing and like activities.
For YouTube, the average Jaccard scores are 100\% for date, 99.83\% for context, and 99.5\% overall.
The slight drop in the overall score is because of the 0.5\% missing entries discussed earlier.
For TikTok and YouTube, the recorded timestamps showed a minimal difference of $\pm$ 5 seconds compared to the HAR dump values for the same activity.
However, for Instagram, the difference was as high as up to one minute, leading to a reduction in the accuracy for date and overall.
%\end{itemize}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{@{}lrrr@{}}
    \toprule
    \textbf{Platform (Activity)} & \multicolumn{1}{c}{\textbf{Date}} & \multicolumn{1}{c}{\textbf{Context}} & \multicolumn{1}{c}{\textbf{Overall}} \\ \midrule
    Instagram (Likes)            & 100\%                             & 100\%                                & 100\%                                \\
    Instagram (Browse)           & 96\%                              & 97\%                                 & 91\%                                \\
    TikTok (Likes and Browse)     & 100\%                             & 100\%                                & 100\%                                \\
    YouTube (Browse)             & 100\%                             & 99.83\%                              & 99.5\%                               \\ \bottomrule
    \end{tabular}
    \caption{Average Jaccard scores across different video browsing sessions for the three platforms. Concerning correctness, TikTok sits at the top, followed by YouTube and Instagram.}
    \label{tab:correctness}
\end{table}


\begin{figure*}[t]
\centering
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/OverlapInstagramLiking.pdf}
        \caption{Instagram}
        \label{fig:IT-consistency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/OverlapTikTokBrowsing.pdf}
        \caption{TikTok}
        \label{fig:TT-consistency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/OverlapYouTubeBrowsing.pdf}
        \caption{YouTube}
        \label{fig:YT-consistency}
    \end{subfigure}
    \caption{ Ratio of entries retained across multiple snapshots for the same account: Instagram (like history), TikTok (video browsing history and like activities), and YouTube (video browsing history).}
    \label{fig:within-consistency}
\end{figure*}

\noindent
\textbf{Consistency (across snapshots)}:
Next, we determine whether a particular account receives consistent data when making multiple requests from the same platform.
Towards this goal, we analyzed differences between multiple DDPs obtained from bot accounts. We examined whether the entries present in an earlier snapshot were retained in a later snapshot. For Instagram and TikTok, the snapshots are collected with a gap of 1 week. For YouTube, we use a gap of one month.

~\Cref{fig:within-consistency} presents the ratio of overlapping entries between two DDPs to the total number of entries in the earlier DDP for three key aspects, i.e., date, context, and overall.
For Instagram, despite using the same bot accounts, we observed variations in the covered duration of the video browsing history across multiple snapshots, ranging from 6 to 13 days.
Hence, due to limited available video browsing history, we solely focus on analyzing the like history.
~\Cref{fig:IT-consistency} shows the results for Instagram.
We identify some missing entries in the later snapshots.
Specifically, 6\% of the entries are found to be missing from snapshot 3 to snapshot 4.
~\Cref{fig:TT-consistency} illustrate the results for video browsing history and like activities on TikTok.
Based on our findings, we observe that TikTok provides consistent data across multiple retrieval attempts.
Lastly, in ~\Cref{fig:YT-consistency}, %presents the results for YouTube.
we observe that approximately 12\% of entries were missing for snapshot pairs (1, 3) for YouTube.
Among these, nearly 62\% entries  were advertisements, while the other videos got deleted from the platform itself.


\noindent
\textbf{Consistency (across users)}:
To assess whether all users receive a similar amount of data, we analyzed video browsing history, search history, and like history obtained from real-world users across the three platforms.
First, we measured the duration of data that was contained in each activity history by calculating the time difference between the earliest and the latest entries in the respective activity history lists.
For Instagram, we defined browsing history as the combination of \textit{ads viewed}, \textit{posts viewed}, and \textit{videos watched}, while we included both \textit{keyword or phrase searches} and \textit{user searches} in the search history.
To understand the extent to which the distribution of provided data to the users differs, we analyzed the cumulative distribution function (CDF) of the durations of video browsing, search, and like history across the platforms.

~\Cref{browse_search_overall} presents a comparative view of activity durations across users.
For Instagram, we observe distinct clusters of users with browsing history durations concentrated around 6 and 13 days (\Cref{fig:insta_cdf_browse}).
This non-uniform distribution indicates that some Instagram end users receive significantly different amounts of data.
Similarly, on TikTok (\Cref{fig:tiktok_cdf_browse}), video browsing and search history durations exhibit substantial variation.
Specifically, some end-users had data spanning approximately 180 days, while others had nearly 450 days of recorded video browsing and search activities.
To ensure a fair comparison, we specifically restricted our analysis to TikTok users whose earliest recorded activity occurred before March 2023.
This criterion allows us to focus on the subset of users who have had ample time to accumulate browsing and search history.
As a result, we analyzed data from 23 TikTok users.
These findings highlight TikTok's tendency to provide data inconsistently among its users.
However, such kind of clusters were not observed for like history on both Instagram and TikTok.
The analysis of video browsing and search history on YouTube (\Cref{fig:youtube_cdf_browse}) does not reveal any discernible patterns, suggesting that the platform may provide DDPs more consistently to its end users.\\
\begin{figure}[t] % Use figure* to span both columns
    \centering
    % First row
    
    \hfill
    \begin{subfigure}[t]{0.49\columnwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.5cm]{Figures/cdf_instagram_all.pdf}
        \caption{Instagram - Browse, Search}
        \label{fig:insta_cdf_browse}
    \end{subfigure}
    % Second row
    \begin{subfigure}[t]{0.49\columnwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.5cm]{Figures/cdf_instagram_like.pdf}
         \caption{Instagram - Likes}
        \label{fig:insta_cdf_like}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\columnwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.5cm]{Figures/cdf_tiktok_all.pdf}
         \caption{TikTok}
        \label{fig:tiktok_cdf_browse}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\columnwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.5cm]{Figures/cdf_youtube_all.pdf}
         \caption{YouTube}
        \label{fig:youtube_cdf_browse}
    \end{subfigure}

    \caption{Plots illustrating the CDF of data duration provided to users for three activities -- search, browse, and like history. On Instagram, clusters in browsing history are observed at 6 and 13 days, while on TikTok, clusters appear at 180 and 455 days for both search and browse history.}

    \label{browse_search_overall}
\end{figure}
\noindent
\textbf{Important takeaways}\\ %Next, we summarize the key takeaways of this section.
%\vspace{0.5 mm}
\noindent
\faHandPointRight~ In terms of correctness and completeness, TikTok provides the most reliable DDPs. Also, its DDPs are consistent across multiple retrievals.\\
%\vspace{0.5 mm}
\noindent
\faHandPointRight~ Instagram and YouTube exhibit varying degrees of missing data across different snapshots. Moreover, Instagram's DDP is not complete as it does not contain viewed reels.\\
%\vspace{0.5 mm}
\noindent
\faHandPointRight~ There are disparities in the amount of data within a platform's DDPs. Instagram and TikTok have clear variations for video browsing and search histories, while YouTube appears to maintain a more consistent data distribution.