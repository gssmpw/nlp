\section{Related work}
Hypothesis testing and two-sample testing have been widely studied for a long time, and we refer the reader to \tcite{lehmann_testing_2022} for a general introduction. 

\paragraph{Kernel-based test} 
The introduction of two-sample tests using the MMD and its unbiased estimators as test statistics is due to \tcite{gretton2007KernelMethod,gretton2012KernelTwosampleTest}.
Based on either large deviation bounds or the asymptotic distribution of the unbiased test statistic, the authors derive test threshold values to achieve a target significance level $α$. Following this work, many variants have been proposed. For example, to address the issue that the standard kernel-MMD test statistic is a degenerate U-statistic under the null hypothesis, making its limiting distribution intractable, \tcite{shekhar2022PermutationfreeKernelTwosample} introduced cross-MMD. This quadratic-time MMD test statistic uses sample-splitting and studentization to ensure a limiting standard Gaussian distribution under the null.
%bounds the level of the obtained test
%, $O(n²d)$ for the first two tests, but they also have a subsampled one
%Threshold to get level $α$ \pcite[Cor. 9]{gretton2012KernelTwosampleTest}
Departing from the MMD, other kernel-based test statistics have been explored. 
Since the MMD is an integral probability metric \pcite{muller1997integral}, MMD-based tests can be interpreted as identifying the most discriminative test function from a set of witness functions belonging to a reproducing kernel Hilbert space. Inspired by this interpretation, tests based on optimized witness functions have been proposed by \tcite{kubler2022WitnessTwoSampleTest} and \tcite{kubler2022AutoMLTwoSampleTest}.
%which can be interpreted as the worst-case error when choosing the witness function within a reproducing kernel Hilbert space.
Other kernel-based metrics include kernel Fisher discriminant analysis \pcite{harchaoui2008TestingHomogeneityKernel} and its regularized variants \pcite{hagrass2023SpectralRegularizedKernel}, which can be viewed as kernelized versions of Hotelling’s $T^2$ test statistic. Additional approaches include the kernel density ratio \pcite{kanamori2011DivergenceEstimationTwoSample} and kernel Stein discrepancies for goodness-of-fit tests \pcite{huggins2018RandomFeatureStein,kalinke2024NystromKernelStein}.


\paragraph{Efficient kernel-based tests}
The main disadvantage of kernel-based tests is that computing the MMD scales quadratically with the number of samples $n$.
In their seminal paper, \tcite[Section 6]{gretton2012KernelTwosampleTest} already introduced the linear-MMD, a statistic computable in $O((n+m)d)$ time, leveraging a partial evaluation of the terms appearing in the U-statistic estimator of the squared MMD. 
Variants of these incomplete U-statistics have subsequently been proposed by \tcite{yamada2018PostSelectionInference,schrab2022EfficientAggregatedKernel}. 
Considering another partial evaluation of the MMD, \tcite{zaremba2013BtestNonparametricLow} introduced the block-MMD, a test statistic derived by (i) splitting the observations into disjoint blocks, (ii) computing the MMD for each block, and (iii) averaging the resulting statistics across all blocks. This approach has been further analyzed and refined by \tcite{ramdas2015adaptivity} and \tcite{reddi2015high}.
\tcite{chwialkowski2015FastTwosampleTesting} introduced a linear-time test statistic based on the average squared distance between empirical kernel embeddings evaluated at~$J$ randomly drawn points. \tcite{jitkrittum2016InterpretableDistributionFeatures} proposed a variant of this statistic in which the~$J$ points are selected to maximize a lower bound on the power.
A major limitation of these approaches is that either a quadratic time complexity is necessary to achieve an optimal power \pcite[Proposition 2]{domingo-enrich2023CompressThenTest} or the computation/power trade-off is yet to be characterized.
Coreset-based approximation strategies have also been investigated, and proven to reach minimax separation rates at a subquadratic computational cost \pcite{domingo-enrich2023CompressThenTest}. 
%Coresets-based approximations of the MMD with kernel thinning \pcite{DBLP:journals/jmlr/DwivediM24} have also been studied by \tcite{domingo-enrich2023CompressThenTest}. 
Approximations of the MMD based on random Fourier features have been explored by \tcite{zhao2015FastMMDEnsembleCircular}, \tcite{zhao2021ComparingDistributionsMeasuring}, and more recently by \tcite{choi2024ComputationalStatisticalTradeKernelTwoSample}. 
Finally, \tcite{chatalic2022NystromKernelMean} proposed a Nyström approximation of the MMD,
%which can be applied to efficient two-sample testing. 
%The latter approach is the focus of our method and study.
which is the base of our method and study. 


%Coreset-approximated $\MMD²$ + permutations + batches\\
%$O(\cgreen{(m+n)\polylog(m+n)})$ for subexponential dist.\\
%Size $α$ and power $β$ 
	%\cite[Th. 1]{domingo-enrich2023CompressThenTest}\\
	%(for $s$ indep. of $n$, $\nb ≥ \tfrac{1}{α} - 1$, sep. in MMD)

\paragraph{Permutation tests}
There are several popular approaches to determining the test threshold. One common method is to use the quantile of the asymptotic distribution of the test statistic under the null hypothesis. However, this approach provides only asymptotic guarantees and may not perform well in finite samples. Another method relies on concentration inequalities, which, while theoretically sound, can be overly conservative, leading to thresholds that are too loose. Alternatively, permutation and bootstrap methods offer data-driven approaches that approximate the null distribution more accurately in practice, often resulting in improved empirical performance. The idea of comparing test statistics to their permuted replications dates back to the work of \tcite{hoeffding1952large}. More recently, the combination of the MMD test statistic with permutation and bootstrap approaches has been explored in works such as \tcite{fromont2012KernelsBasedTests,kim2022MinimaxOptimalityPermutation,schrab2023MMDAggregatedTwoSample}.

\paragraph{Parameters selection} 
The kernel function and its hyperparameters play an important role in the practical usability of kernel-based tests. 
\tcite{balasubramanian2021OptimalityKernelembeddingBased} and \tcite{li2024optimality} established minimax optimality of carefully tuned MMD-based tests in specific settings.
Multiple approaches have been investigated, such as aggregating multiple tests~\pcite{fromont2012KernelsBasedTests,schrab2023MMDAggregatedTwoSample,schrab2022EfficientAggregatedKernel,biggs2023MMDFUSELearningCombining} or using a Bayesian formalism \pcite{zhang2022BayesianKernelTwoSample}.
In this work, we focus on the computational efficiency of the test, yet our approach could easily be combined with such ideas when adaptation is needed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%