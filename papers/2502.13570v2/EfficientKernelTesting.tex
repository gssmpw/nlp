\newif\ificml
\icmltrue
\icmlfalse

\ificml
	%%%%%%%%%%%%%%%% ICML header %%%%%%%%%%%%%%%%
	\documentclass[usenames,dvipsnames]{article}
	\usepackage[utf8]{inputenc}
	\usepackage{microtype}
	\usepackage{graphicx}
	\usepackage[]{hyperref} % hyperlink
	\usepackage[]{icml2025}

	\usepackage{amsmath}
	\usepackage{amssymb}
	\usepackage{mathtools}
	\usepackage{amsthm}

	\usepackage[capitalize,noabbrev]{cleveref}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\else
	%%%%%%%%%%%%%%%% OLD header (before icml) %%%%%%%%%%%%%%%%
	\documentclass[a4paper]{article}
	%\usepackage{fullpage}
	\usepackage[a4paper, total={15.7cm, 22cm}]{geometry}
	\usepackage[utf8]{inputenc}
	\usepackage[pdftex,colorlinks=true,linkcolor=blue,citecolor=magenta]{hyperref} % hyperlink
	\usepackage[dvipsnames]{xcolor}

	\usepackage{amsmath}
	\usepackage{amssymb}
	\usepackage{mathtools}
	\usepackage{csquotes}
	\usepackage{amsfonts}

	\usepackage{cleveref}
\fi

\usepackage[]{subfigure}
\usepackage{xparse}

\usepackage[ruled,algosection,vlined,algo2e]{algorithm2e} % linesnumbered,
\usepackage[english]{babel}

%\crefalias{AlgoLine}{line}
\crefalias{algocfline}{line}
\crefname{algocf}{algorithm}{algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}

%\usepackage{palatino}
%\usepackage{anysize}
\usepackage{enumerate}
\usepackage[overload]{empheq}
%\usepackage{bbm} 
\usepackage{nicefrac}
\usepackage[normalem]{ulem}
\usepackage{xargs}                      % Use more than one optional parameter in a new commands
%\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}  % TODO: Remove in final
% These TODOs are really unreadable…
% Tikz stuff
\definecolor{bluee}{HTML}{4967ed}
\definecolor{malgared}{rgb}{0.8627450980392157, 0.34901960784313724, 0.34901960784313724}

\usepackage{booktabs,boldline}
%\usepackage{cellspace}	% For some extra spacing when using boldline instead of booktabs separator
\usepackage{multirow}
\usepackage{makecell}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=2pt,parsep=2pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=2pt,parsep=2pt,partopsep=0pt}

%\usepackage{tikz}
%\usetikzlibrary{ calc, matrix, fit, positioning, decorations, decorations.pathreplacing, decorations.pathmorphing, arrows.meta, }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               Macros                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{MacroBook}
%\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}

\usepackage{import}
\ificml\undef\todo\fi
\import{./}{latex-commands-antoine.tex}
\import{./}{latex-paper-macros.tex}
\import{./}{latex-declare-math-unicode-characters.tex}
\import{./}{latex-commands-tcolorbox.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               Biblio                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\ificml
	%%%%%% bibliography (for ICML… who uses natbib nowadays, seriously??)
	%% using natbib (loaded in the class)
	%\let\pcite\cite
	%\let\tcite\citet
	%\undef\citetya
	%%\newcommand\citetya[1]{\cite{#1}}
	%\let\citetya\cite

%\else
	%%%%%%%%% bibliography (without ICML class)  %%%%%%%%
	%\usepackage[style=authoryear,uniquename=init,giveninits,autolang=other,backend=bibtex,maxcitenames=2,hyperref=true]{biblatex}
	\usepackage[style=authoryear-comp, hyperref=true, backend=bibtex,sorting=none,url=false,doi=false,maxcitenames=1,uniquelist=false,sorting=nyt,maxbibnames=8]{biblatex}
	\addbibresource{biblio.bib}
	\addbibresource{./biblio_antoine_autoexport_do_not_modify.bib}
	\newcommand\pcite\parencite
	\newcommand\tcite\textcite
	\AtEveryBibitem{
		\clearfield{urldate}
		\clearfield{urlyear}
		\clearfield{urlmonth}
		\clearfield{urlday}
	}
	\AtEveryBibitem{\clearfield{month}}
	\AtEveryCitekey{\clearfield{month}}
	\AtEveryBibitem{\clearfield{day}}	% remove day and month in the bibliography list at the end
	\AtEveryCitekey{\clearfield{day}}
	\DeclareSourcemap{
	  \maps[datatype=bibtex]{
		  \map{
			\step[fieldset=urldate, null]
		  }
	   }
	}
%\fi

% hack to remove 'references' from TOC
% https://tex.stackexchange.com/questions/131704/how-can-i-exclude-nomenclature-from-the-table-of-contents-in-amsart
\DeclareRobustCommand{\SkipTocEntry}[5]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                            Environments                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ificml
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% THEOREMS
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\theoremstyle{plain}
	%\newtheorem{theorem}{Theorem}[section]
	%\newtheorem{proposition}[theorem]{Proposition}
	%\newtheorem{lemma}[theorem]{Lemma}
	%\newtheorem{corollary}[theorem]{Corollary}
	%\theoremstyle{definition}
	%\newtheorem{definition}[theorem]{Definition}
	%\newtheorem{assumption}[theorem]{Assumption}
	%\theoremstyle{remark}
	%\newtheorem{remark}[theorem]{Remark}

	% Todonotes is useful during development; simply uncomment the next line
	%    and comment out the line below the next line to turn off comments
	%\usepackage[disable,textsize=tiny]{todonotes}
	%\usepackage[textsize=tiny]{todonotes}


	% The \icmltitle you define below is probably too long as a header.
	% Therefore, a short form for the running title is supplied here:
	%\icmltitlerunning{An Efficient Permutation-Based Two-Sample Test}
\else
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%                           Page / spacing                            %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%% Antoine: Added for bold paragaphs
	\makeatletter
	\def\paragraph{\@startsection{paragraph}{4}%
	  \z@\z@{-\fontdimen2\font}%
	  {\normalfont\bfseries}}
	\makeatother

	%\setlength{\parindent}{0pt}
	\setlength{\parskip}{0.5\baselineskip}
	
	\setlength{\textfloatsep}{3pt}

	\title{An Efficient Permutation-Based Kernel Two-Sample Test}
	\usepackage{authblk}
	%\author{Marco Letizia, Antoine Chatalic, Nicolas Schreuder, Lorenzo Rosasco}
	\author[1,2]{Antoine Chatalic}
	\author[1,3]{Marco Letizia}
	\author[1,4]{Nicolas Schreuder}
	\author[1,5,6]{Lorenzo Rosasco}
	\affil[1]{MaLGa Center - DIBRIS, Università di Genova, Genoa, Italy}
	\affil[2]{CNRS, Univ. Grenoble-Alpes, GIPSA-lab, France}
	\affil[3]{INFN - Sezione di Genova, Genoa, Italy}
	%\affil[4]{Centre National de la Recherche Scientifique (CNRS), France}
	\affil[4]{CNRS, Laboratoire d'Informatique Gaspard Monge, Champs-sur-Marne, France}
	\affil[5]{Center for Brains, Minds and Machines, MIT, Cambridge, MA, USA}
	\affil[6]{Istituto Italiano di Tecnologia, Genoa, Italy}
	\date{\today} 
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\ificml
	\twocolumn[
	%\icmltitle{A Computationally-Efficient Permutation-Based Kernel Two-Sample Test}
	%\icmltitle{A Scalable Nyström Permutation-Based Kernel Two-Sample Test}
	%\icmltitle{Scalable Nyström-Based Kernel Two-Sample Testing with Permutations}
	\icmltitle{A Scalable Nyström-Based Kernel Two-Sample Test with Permutations}


	% It is OKAY to include author information, even for blind
	% submissions: the style file will automatically remove it for you
	% unless you've provided the [accepted] option to the icml2025
	% package.

	% List of affiliations: The first argument should be a (short)
	% identifier you will use later to specify author affiliations
	% Academic affiliations should list Department, University, City, Region, Country
	% Industry affiliations should list Company, City, Region, Country

	% You can specify symbols, otherwise they are numbered in order.
	% Ideally, you should not use this facility. Affiliations will be numbered
	% in order of appearance and this is the preferred way.
	\icmlsetsymbol{equal}{*}

	\begin{icmlauthorlist}
	\icmlauthor{Marco Letizia}{equal,yyy}
	\icmlauthor{Antoine Chatalic}{equal,yyy,comp}
	\icmlauthor{Nicolas Schreuder}{comp}
	\icmlauthor{Lorenzo Rosasco}{comp}
	%\icmlauthor{}{sch}
	\end{icmlauthorlist}

	\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
	\icmlaffiliation{comp}{Company Name, Location, Country}
	\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

	\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
	\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

	% You may provide any keywords that you
	% find helpful for describing your paper; these are used to populate
	% the "keywords" metadata in the PDF but will not be shown in the document
	\icmlkeywords{Machine Learning, ICML}

	\vskip 0.3in
	]

	% this must go after the closing bracket ] following \twocolumn[ ...

	% This command actually creates the footnote in the first column
	% listing the affiliations and the copyright notice.
	% The command takes one argument, which is text to display at the start of the footnote.
	% The \icmlEqualContribution command is standard text for equal contribution.
	% Remove it (just {}) if you do not need this facility.

	%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
	\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%
	\begin{abstract}
\else
	\maketitle
	%\onecolumn
	\paragraph{Abstract}
\fi
Two-sample hypothesis testing--determining whether two sets of data are drawn from the same distribution--is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nyström approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing realistic scientific data.
\ificml
	\end{abstract}
\else
	{\renewcommand{\baselinestretch}{0.0}\normalsize
		\tableofcontents%
	}
	%\newpage
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                 Misc. notes on efficient estimators                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Let $P,Q$ be probability distributions over a space~$\dsp$. 
We consider the two-sample hypothesis testing problem, 
where one observes independent random samples
\begin{align}
	X_1, \dots, X_{\nX} \diid P \quad \text{ and } \quad
	Y_1, \dots, Y_{\nY} \diid Q,
	\label{h:iid_data}
\end{align}
%drawn from two unknown probability distributions $P$ and $Q$,
and would like to test the hypothesis $\mathcal{H}_0 : P=Q$ against the alternative $\mathcal{H}_1 : P\neq Q$. 
%Without loss of generality, we assume $c≤\nX/\nY≤1$. 

This problem is of paramount importance 
%for an effective deployment 
in many areas, particularly in precision sciences such as high-energy physics, where large volumes of multivariate data are commonly analyzed. A key application is the search for new physics, where small deviations between observed and predicted distributions may signal new phenomena \pcite{Letizia:2022xbe, chakravarti2023model, DAgnolo:2019vbw}. Another important example is the validation of generative models, which are emerging as promising surrogates to replace expensive Monte Carlo simulations while maintaining high fidelity \pcite{Grossi:2024axb, Krause:2024avx}.

This paper focuses on kernel-based two-sample tests, which commonly use the maximum mean discrepancy (MMD; see \Cref{s:mmd}) as a test statistic to determine whether to reject the null hypothesis~$\cH_0$ \pcite{gretton2012KernelTwosampleTest}.
These tests are versatile and powerful tools for comparing probability distributions $P$ and $Q$ without requiring prior assumptions. However, they are often hindered by their computational cost, which scales quadratically with the total sample size $n \coloneqq \nX + \nY$.

In order to mitigate this computational drawback, several approximations of the maximum mean discrepancy (MMD) have been explored, each offering a trade-off between efficiency and accuracy. Notable approaches include linear-MMD \pcite{gretton2012KernelTwosampleTest}, incomplete U-statistics \pcite{yamada2018PostSelectionInference,schrab2022EfficientAggregatedKernel}, block-MMD \pcite{zaremba2013BtestNonparametricLow}. A key limitation of these approaches is that quadratic time complexity is necessary to achieve statistical optimality \pcite[Proposition 2]{domingo-enrich2023CompressThenTest}. More details on related works will be provided in Section~\ref{sec:background}.

To address this limitation, we propose a new procedure for two-sample testing based on a Nyström approximation of the MMD \pcite{nystroem1930UeberPraktischeAufloesung,williams2001UsingNystromMethod,chatalic2022NystromKernelMean}. From a theoretical perspective, we put forward a setting in which we demonstrate the minimax optimality  \pcite{baraud2002nonasymptotic} of our procedure while maintaining sub-quadratic computational complexity—a property shared with random feature approximations \pcite{zhao2015FastMMDEnsembleCircular,choi2024ComputationalStatisticalTradeKernelTwoSample} and coreset-based methods \pcite{domingo-enrich2023CompressThenTest}. Moreover, our method is simple to implement, with its approximation quality controlled by a single hyperparameter, making it both computationally efficient and easy to use in practice. Additionally, it applies to a broad class of kernels and does not require the input space to be a Euclidean space. 


%To address this, alternative approaches such as random feature approximations \pcite{zhao2015FastMMDEnsembleCircular,choi2024ComputationalStatisticalTradeKernelTwoSample} and coreset-based methods \pcite{domingo-enrich2023CompressThenTest} have been proposed, attaining optimality with sub-quadratic computational complexity. Our method achieves similar guarantees and performance while being straightforward to implement and preserving this computational advantage.
%More details on these approaches will be provided in Section~\ref{sec:background}. 

%In this paper, we introduce a new algorithm for kernel two-sample testing, based on a Nyström approximation of the maximum mean discrepancy \pcite{nystroem1930UeberPraktischeAufloesung,williams2001UsingNystromMethod,chatalic2022NystromKernelMean}. 
%Following the permutation principle \pcite[Chapter 17]{lehmann_testing_2022}, the test threshold is determined using multiple permutations of the initial samples.
%Our test is straightforward to implement and the approximation quality of the test statistic is controlled by one single hyperparameter, making it effortless to use in practice. 

\paragraph{Organization of the paper}
We begin by reviewing the two-sample testing problem and kernel-based two-sample tests in \Cref{sec:background}. \Cref{sec:test} introduces our MMD-based permutation test.
%utilizing the Nyström approximation
Theoretical guarantees for this test are provided in \Cref{sec:theory}. 
Finally, numerical studies presented in \Cref{sec:exp} demonstrate the practical effectiveness of our method.
%Finally, numerical studies in Section \ref{sec:exp} demonstrate the practical effectiveness and competitive performance of our method compared to other state-of-the-art approaches.

%\paragraph{Notations} 
%
%Given a matrix $A$, we denote by $A^†$ its Moore-Penrose pseudo-inverse. 
%For a given sequence of real numbers $(u_1, \dots, u_m) \in \mathbb{R}^m$, we denote by $(u_{(j)})_{j=1}^m$ an ordering of $(u_j)_{j=1}^m$ such that $u_{(1)} \leq u_{(2)} \leq \dots \leq u_{(m)}$. 
 
\ificml\else\Cref{t:notations} contains the main notations used throughout the paper. 
	\begin{table}
	\begin{center}
	\begin{tabular}{@{}ll@{}}
		\toprule
		$\dsp$ & Data space \\
		$\ts$ & Test statistic \\
		%$T$ & Test function \\
		$\kr, \fmap$ & (Base) kernel and associated feature map\\
		$\tk, \tfmap$ & Feature map used to build the test statistic and associated kernel\\
		$\thr$ & Threshold of the test \\
		%\midrule
		$\nX=|X|, \nY=|Y|$ & Number of samples (from $P$, from $Q$)  \\
		%$\nX=|X|$ & Number of samples from $P$  \\
		%$\nY=|Y|$ & Number of samples from $Q$  \\
		$n=\nX+\nY$ & Total number of samples  \\
		$\ell$ & dimension of the feature map ($\nf=\lX+\lY$ for Nyström) \\
		$\lX,\lY$ & Number of landmarks (from $X$, from $Y$)  \\
		%$\bs$ 
			%& Batch size\\
		%$\nbX,\nbY,\nb=\nbX+\nbY$ 
			%& Number of batches (for $X$, for $Y$, and total) \\
		%\midrule
		%$\bkme{i}$ & KME for batch $i$\\
		%$\bakme{i}$ & ME of the feature $\tfmap$ for batch $i$ \\
		%\midrule
		$\np$ & Number of permutations \\
		%$\ipts[i]$ & Permuted test statistic for the $i$-th permutation \\
		%$\opts{i}$ & $i$-th (\wrt the ordering of the $\np$ trials) permuted test statistic \\
		%\midrule
		\bottomrule
	\end{tabular}
	\end{center}
	\caption{\label{t:notations}Main notations used throughout the paper}
	\end{table}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background on testing}\label{sec:background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we formally introduce the two-sample testing problem and provide an overview of MMD-based procedures to address it.

\subsection{The two-sample testing problem}
\label{s:testing}

Let $(\dsp, \mathcal{M})$ be a measurable space\footnote{One can consider for simplicity $\dsp⊆ℝ^d$ with the Borel $σ$-algebra, however our results hold more generally for locally compact second-countable topological spaces.}. Let $\cP(\dsp)$ be the space of probability measures on $\dsp$, and consider two probability distributions $P,Q∊\cP(\dsp)$. 
%
Recall that we are given \tiid random samples 
$X₁,…,X_{\nX}\sim P$ and $Y₁,…,Y_{\nY}\sim Q$ as described in Eq.~\eqref{h:iid_data}. 
%\begin{align*}
	%X_1, \dots, X_{\nX} \diid P \quad \text{ and } \quad
	%Y_1, \dots, Y_{\nY} \diid Q,
	%\label{h:iid_data}
%\end{align*}
%drawn from unknown probability distributions $P$ and $Q$. 
The goal of the two-sample testing problem is to test whether these samples are drawn from the same distribution. Formally, we aim to test the null hypothesis $\mathcal{H}_0 : P=Q$ against the alternative hypothesis $\mathcal{H}_1 : P\neq Q$. 


A test is defined as a function of the data $\at:\dsp^{\nX} \times \dsp^{\nY} \to \cb{0,1}$, designed to distinguish between the null and alternative hypotheses. Specifically, the test rejects the null hypothesis if and only if $\at(\X,\Y) = 1$. 
Most tests are built by considering a \textbf{test statistic} $Ψ$ measuring some dicrepancy between $\X$ and $\Y$, and reject the null if and only if $Ψ(\X,\Y)>t$ for some threshold $t$, which can also depend on $\X,\Y$ and whose choice is part of the definition of the test. 
To evaluate the effectiveness of such a test, we consider the errors it may incur in distinguishing between the null and alternative hypotheses.
\begin{definition}[Type I/II errors]
	A type I error occurs when the null hypothesis $P = Q$ is incorrectly rejected. Conversely, a type II error happens when the null hypothesis $P = Q$ is not rejected, despite $P$ and $Q$ being different.
\end{definition}
Using these definitions, we can characterize the performance of a test in terms of its level and power.
\begin{definition}[Level, power]
	A test $\at$ is said to have level $α$ if its type I error is uniformly bounded by $α$, 
	and power $1-β$ for a class of alternative distributions $\cP₁ \subseteq \cP(\dsp)^2$ if its type II error is uniformly bounded by~$β$: 
	\begin{align}
		\sup_{P=Q ∊ \cP(\dsp)} \P_{\substack{\X \sim P^{\otimes \nX} \\ \Y \sim Q^{\otimes \nY}}}[\at(\X,\Y) = 1] 
		&\leq α, \label{e:def_level}\\
		\sup_{(P,Q)∊\cP_1} \P_{\substack{\X \sim P^{\otimes \nX} \\ \Y \sim Q^{\otimes \nY}}}[\at(\X,\Y) = 0] 
		&\leq β. \label{e:def_power}
	\end{align}
	Here the probabilities are taken not only over $\X$ and $\Y$, but also over all other sources of randomness.
\end{definition}

Beyond level and power, the \textbf{uniform separation rate} $ρ(T,β,\cP_1)$ \pcite{baraud2002nonasymptotic} of a test $T$ provides a finer characterization of a test's performance. 
It quantifies the smallest separation between distributions that the test $T$ can reliably distinguish
%The \textbf{uniform separation rate},
%denoted as $ρ(T,β,\cP_1)$ \pcite{baraud2002nonasymptotic}, 
%of a test $T$ is defined as
and is defined as
\begin{align*}
	\inf\Set{ρ > 0 \mid \sup_{(P,Q)∊\cP_1(ρ)} \P_{\substack{\X \sim P^{\otimes \nX} \\ \Y \sim Q^{\otimes \nY}}}[T(\X,\Y) = 0] \leq β}.
\end{align*}
Here $ρ > 0$ represents the separation, and $\cP_1(ρ) \coloneqq \{ (P, Q) \in \mathcal{P}(\mathcal{Z})^2 : d(P, Q) > \rho \}$  denotes the class of distinct probability distributions $P$ and $Q$ that are separated by at least $\rho$ with respect to a specified metric~$d$ (e.g., the MMD). The uniform separation rate determines the smallest separation  $\rho$  such that the test $T$ reliably distinguishes $\rho$-separated distributions $P$  and $Q$ up to failure probability  $\beta$.

Finally, for a fixed (non-asymptotic) level $α$, the \textbf{minimax rate of testing} is defined as  the smallest 
%(or optimal)
uniform separation rate achievable by level-$\alpha$ tests:
\begin{align}
	\underline{ρ}(\nX, \nY, α, β,\cP_1)
	%&\de \inf_{T_\alpha∊\alt}
	&\de \inf_{T_\alpha}
	ρ(T_\alpha, β, \cP_1),
\end{align}
where the infimum is taken over all level-$\alpha$ tests $T_\alpha$.

Minimax rates for two-sample testing have been studied under different metrics and various classes of alternative hypotheses, see for instance \tcite{li2019OptimalityGaussianKernel}.
In this paper, we focus on distributions separated with respect to the MMD metric. For this setting, the minimax rate of testing is known to be lower-bounded by  $\log(1 / (\alpha + \beta))^{1/2} n^{-1/2}$ for translation-invariant kernels on $ℝ^d$ \pcite[Th. 8]{kim2024DifferentiallyPrivatePermutation}. In Section~\ref{sec:theory}, we will demonstrate that our proposed test achieves this minimax rate, establishing its optimality with respect to the MMD metric. Before introducing our testing procedure, we now provide a review of kernel-based two-sample tests and related works.

%We will place ourselves in the Neyman-Pearson framework to compare different test statistics. For a fixed level $\alpha \in (0, 1)$, among the test of level $\alpha$ we will be looking for the test that has the highest power.

\subsection{Kernel-based two-sample tests}
\label{s:mmd}

Let $\delta(\cdot)$ denote the Dirac measure. We define the empirical probability distributions associated with the samples $X$ and $Y$ as $\dX = \frac{1}{\nX} \sum_{i=1}^{\nX} \delta(X_i)$ and $\dY = \frac{1}{\nY} \sum_{i=1}^{\nY} \delta(Y_i)$, respectively. 
A common approach for performing a two-sample test is to assess whether a specific metric between $\dX$ and $\dY$ exceeds a predefined threshold, which typically depends on the sample sizes. In the context of kernel methods, a standard choice of metric is the maximum mean discrepancy between $\dX$ and $\dY$, which we now introduce. 

Let $\rkhs$ be a reproducing kernel Hilbert space (RKHS) with reproducing kernel $\kr:\dsp × \dsp →ℝ$ and canonical feature map $\fmap{x}\de \kr(x,·)$.
We will impose the following assumption on the kernel $\kr$. 
\begin{tassumption}{}{bounded_kernel}
	The kernel is bounded with $\sup_{x∊\dsp}\kr(x,x)=\supk<∞$ and measurable. 
\end{tassumption}
It is a common assumption in the kernel  testing literature (see, e.g., \tcite{domingo-enrich2023CompressThenTest,choi2024ComputationalStatisticalTradeKernelTwoSample}).
The kernel mean embedding \pcite{berlinet2004ReproducingKernelHilbert} of a probability distribution $π$ is defined as the Bochner integral \pcite{diestel1977VectorMeasures}
\[ μ(π) \de \int \fmap{x} \dif π(x), \]
and is well-defined under \Cref{a:bounded_kernel}. Kernel mean embeddings allow probability distributions over arbitrary spaces to be represented as points in a Hilbert space.
The maximum mean discrepancy (MMD) between two probability distributions $P$ and $Q$ is then defined as the distance between their respective kernel mean embeddings,
\begin{align}
	\MMD(π₁,π₂) 
	&\de ‖μ(π₁)-μ(π₂)‖. \label{e:def_MMD}
\end{align}

A kernel is said to be characteristic \pcite{fukumizu2007kernel} if and only if the mapping $μ$ is injective, \ie 
%\[ 
$P=Q \iff ‖μ(P)-μ(Q)‖=0$,
%\] 
in which case the MMD defines a metric on $\cP(\dsp)$. 
Examples of characteristic kernels include Gaussian, Laplace and Matérn kernels. For general conditions under which kernels are characteristic, see \tcite{sriperumbudur2010RelationUniversalityCharacteristic}, \tcite{simon2018kernel} and related references. 

Kernel two-sample testing is based on the principle that the MMD between two samples drawn from the same distribution should be small. 
%The test involves estimating the MMD (or a related statistic) and comparing it to a predefined threshold. 
The null hypothesis is rejected if the MMD (or a related statistic) exceeds a predefined threshold, indicating that the two samples are likely drawn from different distributions. 
Existing approaches mainly differ in their choice of test statistic and the method used to determine the test threshold.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Related work}

Hypothesis testing and two-sample testing have been widely studied for a long time, and we refer the reader to \tcite{lehmann_testing_2022} for a general introduction. 

\paragraph{Kernel-based test} 
The introduction of two-sample tests using the MMD and its unbiased estimators as test statistics is due to \tcite{gretton2007KernelMethod,gretton2012KernelTwosampleTest}.
Based on either large deviation bounds or the asymptotic distribution of the unbiased test statistic, the authors derive test threshold values to achieve a target significance level $α$. Following this work, many variants have been proposed. For example, to address the issue that the standard kernel-MMD test statistic is a degenerate U-statistic under the null hypothesis, making its limiting distribution intractable, \tcite{shekhar2022PermutationfreeKernelTwosample} introduced cross-MMD. This quadratic-time MMD test statistic uses sample-splitting and studentization to ensure a limiting standard Gaussian distribution under the null.
%bounds the level of the obtained test
%, $O(n²d)$ for the first two tests, but they also have a subsampled one
%Threshold to get level $α$ \pcite[Cor. 9]{gretton2012KernelTwosampleTest}
Departing from the MMD, other kernel-based test statistics have been explored. 
Since the MMD is an integral probability metric \pcite{muller1997integral}, MMD-based tests can be interpreted as identifying the most discriminative test function from a set of witness functions belonging to a reproducing kernel Hilbert space. Inspired by this interpretation, tests based on optimized witness functions have been proposed by \tcite{kubler2022WitnessTwoSampleTest} and \tcite{kubler2022AutoMLTwoSampleTest}.
%which can be interpreted as the worst-case error when choosing the witness function within a reproducing kernel Hilbert space.
Other kernel-based metrics include kernel Fisher discriminant analysis \pcite{harchaoui2008TestingHomogeneityKernel} and its regularized variants \pcite{hagrass2023SpectralRegularizedKernel}, which can be viewed as kernelized versions of Hotelling’s $T^2$ test statistic. Additional approaches include the kernel density ratio \pcite{kanamori2011DivergenceEstimationTwoSample} and kernel Stein discrepancies for goodness-of-fit tests \pcite{huggins2018RandomFeatureStein,kalinke2024NystromKernelStein}.


\paragraph{Efficient kernel-based tests}
The main disadvantage of kernel-based tests is that computing the MMD scales quadratically with the number of samples $n$.
In their seminal paper, \tcite[Section 6]{gretton2012KernelTwosampleTest} already introduced the linear-MMD, a statistic computable in $O((n+m)d)$ time, leveraging a partial evaluation of the terms appearing in the U-statistic estimator of the squared MMD. 
Variants of these incomplete U-statistics have subsequently been proposed by \tcite{yamada2018PostSelectionInference,schrab2022EfficientAggregatedKernel}. 
Considering another partial evaluation of the MMD, \tcite{zaremba2013BtestNonparametricLow} introduced the block-MMD, a test statistic derived by (i) splitting the observations into disjoint blocks, (ii) computing the MMD for each block, and (iii) averaging the resulting statistics across all blocks. This approach has been further analyzed and refined by \tcite{ramdas2015adaptivity} and \tcite{reddi2015high}.
\tcite{chwialkowski2015FastTwosampleTesting} introduced a linear-time test statistic based on the average squared distance between empirical kernel embeddings evaluated at~$J$ randomly drawn points. \tcite{jitkrittum2016InterpretableDistributionFeatures} proposed a variant of this statistic in which the~$J$ points are selected to maximize a lower bound on the power.
A major limitation of these approaches is that either a quadratic time complexity is necessary to achieve an optimal power \pcite[Proposition 2]{domingo-enrich2023CompressThenTest} or the computation/power trade-off is yet to be characterized.
Coreset-based approximation strategies have also been investigated, and proven to reach minimax separation rates at a subquadratic computational cost \pcite{domingo-enrich2023CompressThenTest}. 
%Coresets-based approximations of the MMD with kernel thinning \pcite{DBLP:journals/jmlr/DwivediM24} have also been studied by \tcite{domingo-enrich2023CompressThenTest}. 
Approximations of the MMD based on random Fourier features have been explored by \tcite{zhao2015FastMMDEnsembleCircular}, \tcite{zhao2021ComparingDistributionsMeasuring}, and more recently by \tcite{choi2024ComputationalStatisticalTradeKernelTwoSample}. 
Finally, \tcite{chatalic2022NystromKernelMean} proposed a Nyström approximation of the MMD,
%which can be applied to efficient two-sample testing. 
%The latter approach is the focus of our method and study.
which is the base of our method and study. 


%Coreset-approximated $\MMD²$ + permutations + batches\\
%$O(\cgreen{(m+n)\polylog(m+n)})$ for subexponential dist.\\
%Size $α$ and power $β$ 
	%\cite[Th. 1]{domingo-enrich2023CompressThenTest}\\
	%(for $s$ indep. of $n$, $\nb ≥ \tfrac{1}{α} - 1$, sep. in MMD)

\paragraph{Permutation tests}
There are several popular approaches to determining the test threshold. One common method is to use the quantile of the asymptotic distribution of the test statistic under the null hypothesis. However, this approach provides only asymptotic guarantees and may not perform well in finite samples. Another method relies on concentration inequalities, which, while theoretically sound, can be overly conservative, leading to thresholds that are too loose. Alternatively, permutation and bootstrap methods offer data-driven approaches that approximate the null distribution more accurately in practice, often resulting in improved empirical performance. The idea of comparing test statistics to their permuted replications dates back to the work of \tcite{hoeffding1952large}. More recently, the combination of the MMD test statistic with permutation and bootstrap approaches has been explored in works such as \tcite{fromont2012KernelsBasedTests,kim2022MinimaxOptimalityPermutation,schrab2023MMDAggregatedTwoSample}.

\paragraph{Parameters selection} 
The kernel function and its hyperparameters play an important role in the practical usability of kernel-based tests. 
\tcite{balasubramanian2021OptimalityKernelembeddingBased} and \tcite{li2024optimality} established minimax optimality of carefully tuned MMD-based tests in specific settings.
Multiple approaches have been investigated, such as aggregating multiple tests~\pcite{fromont2012KernelsBasedTests,schrab2023MMDAggregatedTwoSample,schrab2022EfficientAggregatedKernel,biggs2023MMDFUSELearningCombining} or using a Bayesian formalism \pcite{zhang2022BayesianKernelTwoSample}.
In this work, we focus on the computational efficiency of the test, yet our approach could easily be combined with such ideas when adaptation is needed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{An approximate MMD-based permutation test}
\section{An approximate MMD permutation test}
\label{sec:test}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Evaluating the maximum mean discrepancy between two empirical distributions has a computational complexity of \(O(n^2)\), where \(n\) is the total number of samples \pcite{gretton2012KernelTwosampleTest}. 
%This cost arises from the need to compute pairwise kernel evaluations between samples from two distributions. 
%This cost arises from the need to compute all pairwise kernel evaluations between samples. % (actually also intra-class it's already quadratic)
%For large datasets, this can be prohibitive.  

%As mentioned in the previous section, computing the MMD between two empirical distributions has a complexity of \(O(n^2)\) because it requires computing all pairwise kernel evaluations, making it impractical for large datasets. % \pcite{gretton2012KernelTwosampleTest}. 
In order to avoid the quadratic cost of computing the MMD, 
we now introduce an efficient randomized approximation and employ it as a test statistic. 
%for large-scale two-sample testing. 

\subsection{Projection-based approximation of the MMD}

We approximate the MMD \eqref{e:def_MMD} between two empirical distributions $\hat{P}$ and $\hat{Q}$ using the  Nyström method \pcite{nystroem1930UeberPraktischeAufloesung,williams2001UsingNystromMethod}, that is by projecting their kernel mean embeddings of $\mu(\hat{P})$ and $\mu(\hat{Q})$ onto a data-dependent finite-dimensional subspace.
%
Formally, given a set of landmark points $\ldm{1},…,\ldm{\nf}∊\dsp$,
we define $\ftldms=[{\fmap{\ldm{1}},…, \fmap{\ldm{\nf}}}]:ℝ^{\nf}→\rkhs$ and
$\Pm:\rkhs→\rkhs$ the orthogonal projector onto $\spa(\ftldms)$. Using this projection, we approximate  $\MMD(\hat{P}, \hat{Q})$ by 
%
\begin{align}
	\ts 
	&\de \n*{ \Pm μ(\dX) - \Pm μ(\dY) }.
	\label{e:def_ts} 
\end{align}

Denoting $\ftldms^*:h \mapsto [\kr(h,\ldm{1}),…, \kr(h,\ldm{\nf})]ᵀ$ 
the adjoint of the operator $\ftldms$, 
the projection $\Pl$ can be expressed as $\Pl=\ftldms(\ftldms^*\ftldms)^†\ftldms^*$ and satisfies, as a projection, $\Pl=\Pl^2=\Pl^*$.
We also stress that $\ftldms^*\ftldms=\Kl$ corresponds to the kernel matrix $K_{\nf}$ of the chosen landmarks, \ie defined by $(\Kl)_{ij}=\kr(\ldm{i},\ldm{j})$. 
Hence, for any $v∊\rkhs$ it holds $\n{\Pm v}^2=\ip{v,\Pl^2 v}=\ip{v,\Pl v}=\n{(\Kl^†)^{1/2}\ftldms^* v}$. 
In particular, this implies that the approximation of the MMD in Eq.~\eqref{e:def_ts} can be computed efficiently as
\begin{align}
	\ts
	&= \n*{ \tfrac{1}{\nX} \sum_{i=1}^{\nX} \tfmap(x_i) - \tfrac{1}{\nY} \sum_{j=1}^{\nY} \tfmap(y_j) } \\
	\quad\text{where}\quad
	&\tfmap(x) 
	\de (\Kl^†)^{1/2}\vvec{κ(\ldm{1}, x) \\ …\\κ(\ldm{\nf}, x) },
	\label{e:def_fmap} 
\end{align}
and $\Kl^†$ denotes the Moore-Penrose pseudo-inverse of $\Kl^†$. 
Note that $\ts$ can be computed in one pass over the data once $Z$ has been chosen. We now detail how the landmarks are selected.

\paragraph{Choice of the landmarks} 
Following \tcite{chatalic2022NystromKernelMean},  
we build our approximation by sampling points from both $\X$ and $\Y$ in order to obtain a good approximation of both kernel mean embeddings simultaneously. 
More precisely, we sample $\lX$ landmarks $\ldmX{1},…,\ldmX{\lX}$ from $\X$ and $\lY$ landmarks $\ldmY{1},…,\ldmY{\lY}$ form $\Y$, 
and define $\ldm{1}=\ldmX{1},\, …,\,\ldm{\lX}\de\ldmX{\lX},\,\ldm{\lX+1}\de\ldmY{1},\, …,\, \ldm{\nf}\de\ldmY{\lY}$, so that $\nf=\lX+\lY$.

Theoretical guarantees for approximating the MMD using a Nyström approximation with uniform sampling of the landmarks have been established in \tcite{chatalic2022NystromKernelMean}.  
However, achieving an optimal convergence rate of order $O(n^{-1/2})$ with uniform sampling requires using a large number of landmarks.
In this paper, we instead rely on leverage scores sampling, which is recognized as an optimal sampling strategy for compression \pcite{chatalic2023EfficientNumericalIntegration}.

Leverage scores quantify the relative importance of each point in a dataset and are closely related to the inverse of the Christoffel function in approximation theory~\pcite{fanuel2022NystromLandmarkSampling,pauwels2018RelatingLeverageScores}.
In this work, we focus on kernel ridge leverage scores (KRLS) \pcite{alaoui2015FastRandomizedKernel}, which are defined for a dataset of size $n$ with an associated kernel matrix $K$ as
\begin{equation}
    \tls{i} \coloneqq \prt*{ K(K + λn I)^{-1} }_{ii}, \quad i=1,\dots, n,
	\label{e:def_ls}
\end{equation}
where $\lambda>0$ is a regularization parameter.
In the following, we sample landmarks from $\X$ and $\Y$ using leverage scores computed separately for each dataset. This corresponds to applying the definition in Eq.~\eqref{e:def_ls} to the kernel matrices of sizes $\nX×\nX$ and $\nY×\nY$, respectively.
%The cost of exactly computing leverage scores quickly becomes prohibitive as the sample size grows due to the matrix inversion. 
%Since the purpose of our approach is to reduce computational cost, we will rely on a approximate notion that has been studied in the literature. 
Since computing these scores exactly is typically expensive, we consider using multiplicative approximations of these scores.
\begin{tdefinition}{AKRLS}{leverage_scores}
Let $δ \in (0,1]$, $λ_0 > 0$ and $z \in [1 , \infty)$. 
The scores $(\als(i))_{i\in[n]}∊ℝ^n$ are said to be $(z,λ_0,δ)$-approximate kernel ridge leverage scores (AKRLS) of $\X$ if with probability at least $1-δ$, for all $λ≥λ₀,i∊\irange{n}$ it holds $\frac{1}{z}~\tls{i} ≤ \als{i}≤ z~\tls{i}$.
%\begin{equation}
    %\frac{1}{z}~\tls{i} ≤ \als{i}≤ z~\tls{i}, \qquad ∀ λ≥λ₀, ∀i∊\irange{n}.
	%\label{e:def_als}
%\end{equation}
\end{tdefinition}

Efficient algorithms have been proposed in the literature to efficiently sample from such approximate kernel ridge leverage scores, see for instance \tcite{musco2017RecursiveSamplingNystrom,rudi2018FastLeverageScore,chen2021FastStatisticalLeverage}.

%\subsection{A permutation test based on the approximate test statistic}
\subsection{Using permutations to determine the threshold}

To construct a test based on the test statistic defined in Eq.~\eqref{e:def_ts}, we need to specify how the threshold of the test is chosen. In this work, we adopt a permutation-based approach \pcite[Chapter 17]{lehmann_testing_2022}. It exploits the fact that, under the null hypothesis (i.e., when $P=Q$), all observed data points are exchangeable. Consequently, permuting the samples does not change the distribution of the test statistic.
%Take the test statistic $\hat{\Psi}(X,Y)$ computed on the samples $X$ and $Y$. 

We introduce the set of concatenated datasets $\XY=(\XY_i)_{1≤i≤n}$, defined as $\XY_{i}=X_i$ for $1≤i≤\nX$ and $\XY_{\nX+i}=Y_{i}$ for $1≤i≤\nY$. 
%
In what follows we consider a random variable $σ$ that is  uniformly distributed over the set of all permutations of $\cb{1,…,n}$ and that is independent of all other sources of randomness. 
%We can then consider the permuted samples $(\XY_{\sigma(i)})_{i=1}^{n_X}$, and $(\XY_{\sigma(j)})_{j=n_X+1}^{n}$, and the associated permuted test statistic $\hat{\Psi}^{\sigma}(X,Y):=\hat{\Psi}((\XY_{\sigma(i)})_{i=1}^{n_X},(\XY_{\sigma(j)})_{j=n_X+1}^{n})$.
By sampling $\np$ \tiid permutations $(σ_p)_{p=1}^\np$ and taking $σ_0=\textup{Id}$, we compute $(\np+1)$ permuted test statistic
\[ 
% indexed permuted test statistic
\ipts{p} \de \ts[(\XY_{σ_p(i)})_{i=1}^{n_X},(\XY_{σ_p(j)})_{j=n_X+1}^{n}],\quad 0≤p≤\np.
\]
% ordered indexed permuted test statistic
We use the notation $\oipts{i}$ to refer to the $i$-th value of the \emph{ordered} list of these statistics, \ie $\oipts{0}≤…≤\oipts{p}$. 
The threshold is then set as the empirical quantile $\oipts{b_α}$ of the permuted test statistic, where $b_α\de \lceil (1-α)(\np+1)\rceil$.
Our testing procedure is detailed in \Cref{al:main_test}. 

%We will consider in particular $\tfmap$ to be (1) a vector of Nyström features, and (2) a vector of random Fourier features (RFF). 

\newcommand\mycommfont[1]{{\smaller #1}}
\begin{algorithm2e}
	\SetCommentSty{mycommfont}
	%\SetNoFillComment
	%\DontPrintSemicolon
	\KwIn{Feature map $\varphi$ as in \eqref{e:def_fmap},
			data
			%$(X, Y) \in \dsp^{\nX} × \dsp^{\nY}$ also denoted
			$\XY=(\XY_i)_{1≤i≤n}∊\dsp^n$, 
			level $α \in (0, 1)$,
			number of permutations $\np$}
	\KwOut{Result of the test (boolean)}
	$w^{(0)} ← \brk*{\tfrac{1}{\nX}, …, \tfrac{1}{\nX}, -\tfrac{1}{\nY}, …, -\tfrac{1}{\nY} }∊ℝ^n$ \; 
	\lForEach{$p=1,…,\np$}{
		$w^{(p)} ← \textup{shuffle}(w)$ 
		%$w^{(p)} ← σ(w^{(0)})$ where $σ \sim \cU(\irange{n})$ 
	}
	\lForEach{$p=0,…,\np$}{
		$v^{(p)} ← 0∊ℝ^{\nf}$ 
	}
	\ForEach(\tcp*[f]{$O(n\np \nf)$ time, $O(\np\nf)$ space}){$i=1,…,n$}{%
		\lForEach{$p=0,…,\np$}{%
			$v^{(p)} ← v^{(p)} + w^{(p)}_i \tfmap(\XY_i)$ %
		}%
	}
	\lForEach{$p=0,…,\np$}{
		$\ipts{p} ←\n*{v^{(p)}}$ 
	}
	$b_α ← \lceil (1-α)(\np+1)\rceil$\;
	\uIf{$\ipts{0} > \oipts{b_α}$}{
		return $1$
			\tcp*{reject $H₀$}
	}\uElseIf{$\ipts{0} = \oipts{b_α}$}{
		%$(\tsl^>, \tsl^=) ← (\#\{0 ≤ b ≤ \np : \ipts{b} > \oipts{b_α}\},  \#\{0 \leq b ≤ \np : \ipts{b} = \oipts{b_α}\})$\;
		$\tsl^> ← \#\{0 ≤ b ≤ \np : \ipts{b} > \oipts{b_α}\}$\;
		$\tsl^= ← \#\{0 \leq b ≤ \np : \ipts{b} = \oipts{b_α}\})$\;
		%$a ← \frac{\alpha(\np+1) - M^>}{M^=}$\;
		%return $1$ with probability $a$\;
		return $1$ with probability $\frac{\alpha(\np+1) - \tsl^>}{\tsl^=}$\;
	}\Else{ 
		return $0$ 
		%the result of a Bernoulli trial with parameter $\lceil q_α \rceil - q_α$
		\tcp*{fail to reject $H₀$}
	}
	\caption{Permutation test based on a Nyström approximation of the MMD%
		\label{al:main_test}}
\end{algorithm2e}

\paragraph{Computational and memory efficiency} The space and time complexity of our algorithm is limited. 
%By utilizing with kernel mean embeddings, we can compute all test statistics in a single pass over the data.
Once the landmarks have been chosen, all test statistics are computed in a single pass over the data.
Moreover, the~$\np$ permutations can be efficiently stored using $O(\np n)$ bits and only $O((\np+1) m)$ space is required to store the mean embeddings corresponding to all permutations and the non-permuted test statistic. 
In particular, it is never needed to store the features of the whole dataset in memory. 
This enables us to easily permute all samples without resorting to batch compression strategies that are typically required in coresets-based methods~\pcite{domingo-enrich2023CompressThenTest}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical guarantees}\label{sec:theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\cXY{r}
%\subsection{Level of the test}
In this section, we focus on controlling the level and power of the proposed test. Without loss of generality, we assume $\cXY≤\nX/\nY≤1$. 

\subsection{Level and power guarantees}

The level of the proposed test is established by the following lemma, while a bound on the power is provided in the rest of this section. This lemma holds under the assumption that the data are exchangeable under the null hypothesis. It is satisfied in particular when the data are i.i.d., as is the case here.

\begin{tlemma}{}{level} For any $\alpha \in (0, 1)$, the test described in Algorithm~\ref{al:main_test} with input level $\alpha$ 
%is an exact test at level~$α$.   
has exact level~$α$.   % I would keep that because this matches def.2 while it's not clear otherwise what it means to be a test "at level α"?
\end{tlemma}
By “exact”, we mean here that the inequality in \eqref{e:def_level} is actually an equality. 
We now state our main result which bounds the power of the test, and whose proof is based on \cref{r:bound_threshold,r:bound_TS_quantile_rough,r:bound_MMD_nys}.

\begin{ttheorem}{Main result: power of the test}{power_MMD}
	Let $\beta\in (0, 1)$. 	
	Assume that the hypotheses of \Cref{r:bound_MMD_nys} are satisfied for $δ\de β/2$. 
	%Under \Cref{a:bounded_kernel,a:poly_decay}, % (already assumed in the previous lemma)
	Let $c_α=\lfloor α(\np+1) \rfloor$. 
	Then there exists a universal constant $c$ \st 
	the test of \Cref{al:main_test} has power at least $1-\beta$ provided that \\
	%\begin{align*}
		%&\MMD(P,Q) 
		%≥ 
		%\sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α} \prt*{\frac{2}{β}}^{1/\lfloor α(\np+1) \rfloor}} } 
		%\\&\quad\quad
		%+ (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap
		%\prt*{ 1 + \sqrt{2\log(16/β)} }
		%\\&\quad\quad
		%+\supfmap\sqrt{57}
		%\prt*{\frac{\sqrt{\log(128 \nX/β)}}{\sqrt{\nX}}
			%+\frac{\sqrt{\log(128 \nY/β)}}{\sqrt{\nY}}}
		%%%%%%
		% Under our assumptions:
		% - nX≤nY
		% - nY≤(1/c)nX
		% - and log(Cx)/sqrt(x) is decreasing for x≥1 for our values of C
		%&\MMD(P,Q) 
		%≥ 
		%\tfrac{\supfmap}{\sqrt{\nX}}
		%\Bigg[
		%\sqrt{C'\tfrac{c+1}{c}} \sqrt{\log\prt*{\tfrac{2e}{α} \prt*{\tfrac{2}{β}}^{1/\lfloor α(\np+1) \rfloor}} } 
		%\\&\quad\quad
		%%+ (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap
		%+ 4 + 4\sqrt{2\log(16β^{-1})} 
		%+2\sqrt{57} \sqrt{\log(128β^{-1} \nX)}
		%\Bigg]
		%%%%%% Expressed for MMD² (Just loosing a factor 4:)
	%	\\&\MMD(P,Q)²
	%	≥ 
	%	\frac{4\supk}{\nX}
	%	\Bigg[
	%	\frac{C'(\cXY+1)}{\cXY}\log\prt*{\frac{2e}{α} \prt*{\frac{2}{β}}^{1/c_α}} 
	%	\\&\quad\quad
	%	+ 16 + 32\log\prt*{\frac{16}{β}}
	%	+ 228\log\prt*{\frac{128\nX}{β}}
	%	\Bigg]
	%\end{align*}
	%\begin{align*}
		%\\&\MMD(P,Q)²
		%≥
		%\frac{c\supk}{\nX}
		%\Bigg[
		%\frac{(\cXY+1)}{\cXY}\log\prt*{\frac{2e}{α} \prt*{\frac{2}{β}}^{\frac{1}{c_α}}} 
		%+ \log\prt*{\frac{\nX}{β}}
		%\Bigg].
	%\end{align*}
	\ificml
		\fitline{$\displaystyle
	\else
		\[
	\fi
	\MMD(P,Q)²
		≥
		\frac{c\supk}{\nX}
		\Bigg[
		\frac{(\cXY+1)}{\cXY}\log\prt*{\frac{2e}{α} \Big(\frac{2}{β}\Big)^{\frac{1}{c_α}}} 
		+ \log\prt*{\frac{\nX}{β}}
		\Bigg].%
	\ificml
		$}%
	\else
		\]%
	\fi
	%------------------------------------\\
	%The result as fun of the KME error:
	%\begin{align*}
	%\sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α} \prt*{2/β}^{1/c_α}} } + (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap
	%&≤ \MMD(P,Q) - ( \eKME[β/4][\nX] + \eKME[β/4][\nY])
	%\\\text{where} \quad
	%\eKME &\de \frac{\supfmap}{\sqrt{n}} \prt*{2 \sqrt{2\log(4/δ)} +\sqrt{57\log(32 n/δ)}}
	%\end{align*}
\end{ttheorem}
A detailed bound, with constants that are made comparatively more explicit is provided in the proof of Theorem~\ref{r:power_MMD}.
Note that the dependence of the separation rate (with respect to the MMD) on the smallest sample size $\nX$  is $\nX^{-1/2}$. This matches the known minimax optimal rate of testing in this setting as established in \Cref{s:testing} \pcite[Th. 8]{kim2024DifferentiallyPrivatePermutation}.

	%minimax: $\log(1/(α+β))^{1/2}n^{-1/2}$ \pcite[Theorem 8]{kim2024DifferentiallyPrivatePermutation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%\begin{tlemma}{}{bound_threshold}
%	Let $0<β≤1$. If the distributions $P$ and $Q$ are such that
%	\begin{align*}
%		\P[\MMD(P,Q)≥\eMMD[\tfrac{\nicefrac{\beta}{2}}{1+\nicefrac{β}{2}}  ]+\thr] > \frac{1}{1+\nicefrac{β}{2}}\enspace,
%	\end{align*}
%then $\P[\ts≤\thr]≤β$. 
%\end{tlemma}
%

%
%The proof of the above lemma closely follows the one of \textcite[Lemma 4]{domingo-enrich2023CompressThenTest}. We restate it here for completeness.
%\begin{tproofof*}{r:bound_threshold}{}
%	%Let $\cE\de \eMMD[\tfrac{β}{6(1+β/2)}]$. 
%	Let $δ∊(0,1)$,  
%	and $β'=\frac{1}{1+β/2}$. 
%	Define the events 
%	$\cA=\cb{\ts≤\thr}$ and
%	$\cB_δ=\cb{\MMD(P,Q)≥\eMMD+\thr}$. 
%	Assume that $\P[\cB_δ]>β'$. We will show that $\P[\cA]≤β$.
%	
%	By the law of total probability, it holds
%	\begin{align}
%		\P[\cA]
%		&= \P[\cA|\cB_δ] \P[\cB_δ]
%		+ \P[\cA|\cB_δ^c] \P[\cB_δ^c]\\
%		&≤ \P[\cA|\cB_δ] 
%		+ \P[\cB_δ^c].
%		\label{e:first_ineq_cA}
%	\end{align}
%	The first term in the above upper bound can be bounded as 
%	\begin{align}
%		\P[\cA|\cB_δ] 
%		&= \P[\ts≤\thr\, |\, \thr≤\MMD(P,Q)-\eMMD] \\
%		&≤ \P[\ts≤\MMD(P,Q)-\eMMD \,|\, \cB_δ] \\
%		&≤ \frac{1}{\P[\cB_δ]}  \P[\MMD(P,Q)-\ts≥\eMMD] \\
%		&≤ \frac{1}{\P[\cB_δ]}  \P[|\MMD(P,Q)-\ts|≥\eMMD]\enspace. %\\

%	\end{align}
%	By assumption, it holds $1/\P[\cB_δ]≤1/β'$. Furthermore, using \Cref{r:bound_MMD_nys} we get 
%	\begin{align}
%		\P[|\MMD(P,Q)-\ts|≥\eMMD] 
%		&≤ δ
%	\end{align}
%	Combining the last inequality and the fact that $\P[\cB_δ^c]≤1-β'$ by assumption,
%	%\cred{picking $b=\tfrac{1}{1+β/2}$ as well as $δ\de \tfrac{β/2}{1+β/2}$, }
%	for the choice $δ\de \tfrac{β/2}{1+β/2}$, 
%	\Cref{e:first_ineq_cA} can be rewritten as
%	\begin{align}
%		\P[\cA] ≤ \tfrac{δ}{β'} + 1-β'  = \frac{β}{2}\prt*{1+\frac{1}{1+\nicefrac{β}{2}}} ≤ β.
%	\end{align}
%\end{tproofof*}
%

\subsection{Main elements for the proof}

Our proof relies on the following lemma, which is similar in spirit to \tcite[Lemma 4]{schrab2023MMDAggregatedTwoSample}.


\begin{tlemma}{}{bound_threshold}
	Let $0<β≤1$. 
	%Consider the setting of \Cref{r:bound_MMD_nys}. 
	Assume that there exists a function $\eMMDv$ such that our estimator of the MMD satisfies 
	%$\P[|\MMD(P,Q)-\ts|≥\eMMD] ≤ δ$.
	\begin{align}
		\P[|\MMD(P,Q)-\ts|≥\eMMD[β/2]] &≤ β/2. 
		\label{e:hyp_mmd_quality}
	\end{align}
	If the distributions $P$ and $Q$ are such that
	\begin{align*}
		\P[\MMD(P,Q)≥\eMMD[\nicefrac{\beta}{2}] +\thr] > 1-\beta/2\enspace,
	\end{align*}
	then $\P[\ts≤\thr]≤β$. 
\end{tlemma}

In \Cref{s:mmd_approximation}, we will show that Assumption~\eqref{e:hyp_mmd_quality} is satisfied for the proposed Nyström-based estimator. 
%
Then, following 
%using results for the concentration of permuted U-statistics \pcite[Th. 6.1]{kim2022MinimaxOptimalityPermutation}
 \tcite[Lemma 6]{domingo-enrich2023CompressThenTest} 
we will show in \Cref{s:bound_quantile} how the (random) empirical quantile threshold $\thr$ can be bounded by the (deterministic) quantile of the permuted test statistic conditionally on $\X,\Y$. 
%Our main result is then given in \Cref{s:power_mmd_separation} for a a class of alternative hypotheses separated in MMD.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Quality of the MMD approximation}
\label{s:mmd_approximation}

The test statistic defined in \Cref{e:def_ts} is an estimator of the MMD between \(P\) and \(Q\). In this section, we provide a high-probability bound for this approximation. This result is of independent interest, as it improves upon \tcite{chatalic2022NystromKernelMean} by using approximate leverage score sampling instead of uniform sampling.


For $\square \in \{P, Q\}$, let $C_\square \de\E_{x\sim \square}\, ϕ(x)\kron ϕ(x):\rkhs→\rkhs$ denote the (uncentered) covariance operators associated with $P$ and $Q$.
Both $C_P$ and $C_Q$ are self-adjoint trace-class operators under~\Cref{a:bounded_kernel}, and we denote $(λ_i(C))_{i∊\bN^*}$ the $i$-th eigenvalue of $C$. We will make the following assumption on the decay of the spectra of the considered covariance operators.
\begin{tassumption}{Polynomial spectral decay}{poly_decay}
	There exist $γ∊(0,1]$ and $a_γ>0$ such that 
	$\max(λ_i(\covP),λ_i(\covQ)) ≤ a_γ i^{-1/γ}$.\\
\end{tassumption}
Assuming that a covariance operator has such a polynomial decay is relatively standard in the literature, and corresponds for instance to the case of Sobolev spaces as studied by \tcite{widom1964AsymptoticBehaviorEigenvalues}. 
%However, in our case we need this assumption to be satisfied for the covariance operators of both $P$ and $Q$.

A key quantity in the analysis is the so-called effective dimension. This quantity depends both on the choice of the kernel as well as on a probability distribution, and is defined for any $λ>0$ as $\deff[P,λ]%
%\de\E_{x\sim\td} \n{\covopl^{-1/2}\fmap(x)}²%
\de \Tr(\covP (\covP+λI)^{-1})=\sum_{i∊\bN^*} \tfrac{λ_i(\covP)}{λ_i(\covP)+λ} %
$.
Notably, the effective dimension depends on the distribution $P$ only via its covariance operator, and can be interpreted as a smooth estimate of the number of eigenvalues of $\covP$ that are greater than $λ$.
Under \Cref{a:bounded_kernel,a:poly_decay}, there exists a constant $c_γ$ depending on $a_γ,γ$ and $\supk$ such that it holds $\max(\deff[P,λ],\deff[Q,λ]) ≤ c_γ \lambda^{-γ}$ for any $λ>0$ (see \cite[Lemma F.1]{chatalic2023EfficientNumericalIntegration}).

%We now state 
We now have the following result regarding the error induced by the Nyström approximation of the $\MMD$.

\begin{tlemma}{Nyström MMD approximation}{bound_MMD_nys}
Let $\nf=\lX+\lY,\lX≥L(\nX),\lY≥L(\nY)$ where 
\begin{align}
	L(n) \de n^γ\prt*{\log \tfrac{32n}{δ} }^{1-γ} \frac{78 c_γ z^2}{(19\supk)^γ}. 
	\label{e:def_lX_lY}
\end{align}
Let $\ldmX{1}, …, \ldmX{\lX}$ and $\ldmY{1}, …, \ldmY{\lY}$ be drawn with replacement
respectively from the datasets $X$ and $Y$, and respectively proportionally to $(z,λ_0,δ/8)$-AKRLS for $\X$ and $(z,λ_0,δ/8)$-AKRLS for $\Y$ for some $z≥1$ and $λ_0>0$ satisfying $λ₀ ≤ 19\supk \min(\tfrac{\log(\frac{32\nX}{δ})}{\nX}, \tfrac{\log(\frac{32\nY}{δ})}{\nY})$. 
Let 
\begin{align*}
\eMMD \de \eKME[δ/2][\nX] + \eKME[δ/2][\nY],
\end{align*}
where 
\begin{align*}
&
	\eKME 
	\de \frac{\supfmap}{\sqrt{n}} \prt*{2 \sqrt{2\log(4/δ)} +\sqrt{57\log(32 n/δ)}}.
	%\label{e:def_eKME}
\end{align*}
Under \Cref{a:bounded_kernel,a:poly_decay}, 
%With probability $1-δ$ conditionally on $(X,Y)$
%\begin{align}
%|\MMD(X,Y)-\ts| 
%&≤ \ttodo{Where do we need that?}.
%\end{align}
%Moreover,
it holds with probability at least $1-δ$: 
\begin{align*}
	|\MMD(P,Q)-\ts| 
	&≤ \eMMD, 
\end{align*}
provided that $\nX,\nY$ are large enough, namely:
% ~~~~~~~ These are assumptions of \Cref{r:bound_proj_halfcov}
\begin{itemize}
	\item $\min(\nX,\nY)≥(1655+233\log(8\supk/δ))\supk$% (this is F.4, already with $δ/2$!),
	%--------  I suggest to use this rough formulation (note we apply the sub-lemma with δ/2 this is why we get 64 here)
	\item $\frac{\log(\tfrac{64 \nX}{δ})}{\nX}≤C(\supk, ‖\covP‖, γ, z, c_γ)$ and
	      $\frac{\log(\tfrac{64 \nY}{δ})}{\nY}≤C(\supk, ‖\covQ‖, γ, z, c_γ)$.		
	%--------  This is what we actually really want: (but ugly + need to define the constant of the effective dimension)
	%and $\frac{19\supk \prt*{ \log\frac{32\nX}{δ}}}{\nX} ≤ \min\left(‖\covP‖, \prt*{\tfrac{c_γ z²}{5}}^{1/γ}\right)$,
	%$\frac{19\supk \prt*{ \log\frac{32\nY}{δ}}}{\nY} ≤ \min\left(‖\covQ‖, \prt*{\tfrac{c_γ z²}{5}}^{1/γ}\right)$.
\end{itemize}
for some constant $C$ made explicit in the proof.
\end{tlemma}

%\subsubsection{Bound on the quantile of the permuted test statistic (conditioned on the data)}
\subsubsection{Bound on the empirical quantile threshold}
\label{s:bound_quantile}

We now derive an upper-bound on the quantile of the permuted test statistic conditionally on $\X,\Y$. It will serve as an upper-bound on the random empirical quantile threshold $\thr$ in our main result.
%
%\paragraph{Working with $U$ statistics}
For this, we show (cf. \Cref{s:decomposition_U_R}) that the randomly permuted squared test statistic can be 
%with $ h^σ(i,i';j,j') \de  \tk{\XY_{σ(i)},\XY_{σ(i')}} -  \tk{\XY_{σ(i)}, \XY_{σ(\nY+j')}} - \tk{\XY_{σ(\nY+j)},\XY_{σ(i')}} + \tk{\XY_{σ(\nY+j)}, \XY_{σ(\nY+j')}}$.
%
decomposed as the sum of a (weighted) U-statistic and a remainder term:
\begin{align*}
	\pts² 
	&= \tfrac{(\nX-1)(\nY-1)}{\nX\nY} \pstsU + \pstsR.
		%\label{e:ts_decomposition},
\end{align*}
%with
%\begin{align}
	%\pstsU 
	%&\de 
		%\tfrac{1}{\nX(\nX-1)\nY(\nY-1)} \sum_{1≤i≠i'≤\nX} \sum_{1≤j≠j'≤\nY}  
		%h^σ(i,i';j,j'),
		%\label{e:def_U}
%\end{align}
%and
%\begin{align}
	%\pstsR
	%&= \tfrac{1}{\nX²\nY²} 
		%\prt*{\sum_{i=1}^{\nX}\sum_{1≤j≠j'≤\nY}  h^σ(i,i;j,j')
		%+ \sum_{1≤i≠i'≤\nX}\sum_{j=1}^{\nY} h^σ(i,i';j,j) 
		%+ \sum_{i=1}^{\nX}\sum_{j=1}^{\nY}  h^σ(i,i;j,j)
		%}.
	%\label{e:def_R}
%\end{align}
%We control  the $U$-statistic with high probability using a result from \tcite{kim2022MinimaxOptimalityPermutation} while we obtain an upper-bound for $\pstsR$ using the boundedness assumption.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Dealing with permutations}
%
%More precisely, $\pstsUq \de \inf\cb{x∊ℝ:F_{\pstsU}(x)≥1-α}$ where $F_{\pstsU}$ denotes the CDF of $\pstsU$, and a similar definition is used for $\pts$.
%

%We also define $\cF$ as $\cF(x)=F(x)$ if $F$ is continuous at $x$, and $U(\lim_{y→x^-} F(y), F(x))$ otherwise, so that $\cF(\pts)$ is uniformly distributed over $[0,1]$ and for any $x$ it holds $\P[\pts<x]≤\cF(x)≤\P(\pts≤x)$. 
%For any $α∊(0,1)$, we denote $q_{1-α}(\X,\Y)\de \inf\cb{x∊\bR : F(x)\geq 1 - \alpha}$ the quantile of $\pts$ conditioned on $\X,\Y$.} 
%
We bound the quantile of $\pts$ given $(\X,\Y)$, leveraging a result from \tcite{kim2022MinimaxOptimalityPermutation} to control the quantile of the $U$-statistic.

\begin{tlemma}{Quantile bound for $\pts |\X,\Y$}{bound_TS_quantile_rough}
Let $0<α≤e^{-1}$. 
Under \Cref{a:bounded_kernel}, there is a universal constant $C'$ such that the test statistic associated to the Nyström kernel approximation satisfies, with probability at least $1-\alpha$ conditioned on $X$ and $Y$,
\begin{align*}
	\pts^2 
	&≤ C' \tfrac{\sqrt{n(n-1)} \supk}{\nX²} \log(1/α) + (\tfrac{1}{\nX} +\tfrac{1}{\nY}) 4\supk 
\end{align*}
\end{tlemma}

We use this bound to establish a deterministic upper bound on the (random) empirical quantile threshold $\thr$ using a result from \tcite{domingo-enrich2023CompressThenTest}.

\begin{tlemma}{Bound on the empirical quantile}{bound_quantile_rough}
	Let $β>0$, and $1/(2e)≥α≥1/(\np+1)$. 	% the UB on α is a suffiicent cond to have $α₁≤e^{-1}$
	Let $c_α=\lfloor α(\np+1) \rfloor$. 
	Then, with probability $1-\beta/2$ conditioned on $X$ and $Y$,
	\begin{align*}
		\thr 
		&≤ \supfmap \left(\sqrt{C'} \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α (\beta/2)^{1/c_α}}}} + 2(\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}})\right)
	\end{align*}
\end{tlemma}

Using \Cref{r:bound_quantile_rough}, we are able to complete the proof of our main result, which is deferred to \Cref{s:proof_main_result}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical studies}\label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand\cc[1]{{\smaller #1}}
\begin{figure*}[ht] % Use [t] to force the figure to the top
	\centering
	%\subfigure[\cc{\textbf{Gaussians}. Power as a function of the computation time ($\rho_2=0.63$).}]{%
	\subfigure[\cc{\textbf{Correlated Gaussians}, $\rho_2=0.63$}]{%
		\includegraphics[width=0.31\linewidth]{figures/CG_powervstime}
		\label{fig:cg_pvst}
	}
	\hfill
	%\subfigure[\cc{\textbf{Susy.} Power as a function of the computation time  ($n=16000$).}]{%
	\subfigure[\cc{\textbf{Susy},  $n=16000$}]{%
		\includegraphics[width=0.31\linewidth]{figures/susy_powervstime}
		\label{fig:susy_pvst}
	}
	\hfill
	%\subfigure[\cc{\textbf{Higgs.} Power as a function of the computation time ($n=60000$).}]{%
	\subfigure[\cc{\textbf{Higgs}, $n=60000$}]{%
		\includegraphics[width=0.31\linewidth]{figures/higgs_powervstime}
		\label{fig:higgs_pvst}
	}
	
	%\vskip\baselineskip % Adds vertical spacing
		%\subfigure[\cc{\textbf{Gaussians}. Power as a function of the correlation coeff. $\rho_2$ ($n=5000$).}]{%
		\subfigure[\cc{\textbf{Correlated Gaussians}, $n=5000$}]{%
		\includegraphics[width=0.31\linewidth]{figures/CG_powervsrho}
		\label{fig:cg_pvsrho}
	}
	\hfill
		%\subfigure[\cc{\textbf{Susy}. Power as a function of the total sample size.}]{%
		\subfigure[\cc{\textbf{Susy}}]{%
		\includegraphics[width=0.31\linewidth]{figures/susy_powervsn}
		\label{fig:susy_pvsn}
	}
	\hfill
		%\subfigure[\cc{\textbf{Higgs}. Power as a function of the total sample size.}]{%
		\subfigure[\cc{\textbf{Higgs}}]{%
		\includegraphics[width=0.31\linewidth]{figures/higgs_powervsn}
		\label{fig:higgs_pvsn}
	}
	
	\caption{Power against computation time (top row) and power against correlation coefficient $ρ_2$ and total number of samples (bottom row). Each column represents a different dataset.}
	\label{fig:grid}
\end{figure*}

\iffalse
\begin{figure*}
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{figures/susy_powervsn}
        %\caption{Caption for image 1}
        \label{fig:image1}
    \end{minipage}\hspace{0.1cm}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{figures/higgs_powervsn}
        %\caption{Caption for image 2}
        \label{fig:image2}
    \end{minipage}

%    \vspace{.5em}

    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{figures/susy_powervstime}
        %\caption{Caption for image 3}
        \label{fig:image3}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{figures/higgs_powervstime}
        %\caption{Caption for image 4}
        \label{fig:image4}
    \end{minipage}

    \caption{Overall caption for the figure grid}
    \label{fig:figure_grid}
\end{figure*}
\fi

\iffalse
\begin{figure*}
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/susy_powervsn} 
        %\caption{Figure 1: Image 1}
        \label{fig:image1}
    \end{minipage}\hspace{0.1cm} % Adjust horizontal spacing between plots
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/susy_powervstime} 
        %\caption{Figure 2: Image 2}
        \label{fig:image2}
    \end{minipage}\hspace{0.1cm}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/higgs_powervsn} 
        %\caption{Figure 3: Image 3}
        \label{fig:image3}
    \end{minipage}\hspace{0.1cm}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/higgs_powervstime}
        %\caption{Figure 4: Image 4}
        \label{fig:image4}
    \end{minipage}

    \caption{Power at varying sample size and power against computation time.}
    \label{fig:row}
\end{figure*}
\fi 

\begin{figure}
\centering
\includegraphics[width=\ificml 0.8\else 0.6\fi\linewidth]{figures/susy_powervsell}
        \ificml\vspace{-.3cm}\fi
        %\caption{\textbf{Susy}. Power as a function of the number of features. The total sample size is $n=16000$}
        \caption{
		\ificml\resizebox{0.83\linewidth}{!}{\fi%
		\textbf{Susy}, power against number of features, $n=16000$.%
		\ificml}\fi
		}
        \label{fig:image5}
\end{figure}

In this section, the empirical power and computational trade-offs of our method are explored and compared against 
 the random Fourier features (RFF) approach presented in \tcite{choi2024computational}. 
%We tried to compare our method to the coreset-based approach of \tcite{domingo-enrich2023CompressThenTest}, but we were unfortunately not able to reproduce the authors' results. 
\ificml
We add in \Cref{s:ctt} a comparison to \tcite{domingo-enrich2023CompressThenTest}, however we were unfortunately not able to reproduce the authors' results. 
\fi
We consider both AKRLS computed using the method of \tcite{musco2017RecursiveSamplingNystrom} and uniform sampling for the selection of the Nyström landmarks. In all our experiments, we consider a Gaussian kernel $κ(x,y)=\exp(-‖x-y‖^2/(2σ^2))$, whose bandwith is fixed as the median of the inter-points Euclidean distance, estimated on a subset of 2000 random instances.
The level of the test is set at $\alpha=0.05$. The size is determined with $\mathcal{P}=200$ permutations and the results are averaged over $400$ repetitions. The error is estimated using Wilson score intervals at the 95\% confidence level \pcite{newcombe1998two}. All our code is open-source and written in Python\footnote{\url{https://anonymous.4open.science/r/nystrom-mmd-0F7B/}}.
%More specific details are provided in what follows. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets}\label{s:datasets}

\label{p:cg}
\paragraph{Correlated Gaussians} We consider a synthetic dataset based on 3-dimensional correlated Gaussian distributions. The first sample is drawn from $ P = \mathcal{N}_3(\mathbf{0}, \mathbf{\Sigma}(\rho_1)) $, with covariance matrix $ \mathbf{\Sigma}(\rho_1) = \text{diag}(1, 1, 1) + \rho_1(1 - \text{diag}(1, 1, 1)) $, and $ \rho_1 = 0.5 $. The second sample follows the same distribution, but with a varying correlation coefficient $ 0.51 \leq \rho_2 \leq 0.66 $. The sample size is fixed at $ n_x = n_y = 2500 $.

\label{p:higgs}
\paragraph{Higgs and Susy} 
These datasets consist of Monte Carlo simulations of collider data from high-energy physics (HEP) \pcite{Baldi:2014kfa}. They are characterised by two classes, background data ($P_b$) and signal data ($P_s$). The latter is composed of processes producing a Higgs boson and supersymmetric particles, for the Higgs dataset and Susy dataset respectively.
%In the Higgs dataset, background data is lacking the Higgs boson, while it is produced in signal data. 
The Higgs dataset has 21 features. The last 7 features are functions of the first 21 and can be considered more discriminative \emph{high-level} features. Consequently we will refer to the first 21 as \emph{low-level} features. The dataset is composed of a total of 11M examples.
%We consider two experimental setups. In the first one, following \cite{chwialkowski2015fast}, we only consider the jet $\phi$-momenta ($d=4$). These are low-level features with a poor discriminative power between the two classes. We then consider $P=P_b$ and $Q=P_s$. In the second setup,
We will only consider low-level features ($d=14$). To mimic a typical scenario in HEP and make the test more challenging, we compare data from the distribution $P=P_b$ against a mixture of background and signal processes $Q=(1-\alpha_{\rm mix}) P_b + \alpha_{\rm mix} P_s$, with $\alpha_{\rm mix}=0.2$. 
The Susy dataset has 
%been designed for the classification task of distinguishing between background processes and signal processes with supersymmetric particles. There are
8 low-level features and 10 high-level features (that we do not consider). Again, we test background processes against a mixture of background and signal processes, with $\alpha_{\rm mix}=0.05$.

\subsection{Results}

In \Cref{fig:grid} (first row), we report the power against computation time of the selected methods on the 3 datasets described in \Cref{s:datasets}.
We observe that our approach always matches, and in multiple cases outperforms, the results obtained using random Fourier features (RFF). The latter are themselves known to empirically outperform other linear-time test statistics existing in the literature~\pcite{choi2024ComputationalStatisticalTradeKernelTwoSample}. 
In the first row, we fix the compression level at $\nf=\sqrt{n}$ and compare the same methods while varying the parameter $ρ_2$ for the synthetic dataset and the number of samples $n$ for Susy and Higgs.
We also report the result of the exact MMD estimators for the smaller values of $n$ at which computations remain reasonable: in these settings, one can clearly observe that the efficiency of our approximate test procedure is achieved without compromising the power of the test.

In \Cref{fig:image5}, we report for the Susy dataset the power as a function of the compression level, which for our approach corresponds to the number of Nyström landmarks, and for RFF corresponds to the number of (real) features used in the approximation. Our approach performs similarly to random Fourier features, as well as Nyström with uniform sampling. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%In this work, 
We introduced a computationally efficient procedure for two-sample testing, leveraging a data-adaptive Nyström approximation of the MMD as the test statistic. We provided a bound on the (non-asymptotic) power of the resulting test and demonstrated that it matches the minimax MMD-separation rates. Our procedure is simple to implement and compares favorably with existing state-of-the-art approaches, both theoretically and empirically.
We leave open the possibility of using the proposed test statistic to design aggregated tests or to handle weighted samples.

\paragraph*{Acknowledgments:}
All the authors acknowledge the financial support of the European Research Council (grant SLING 819789). L. R. acknowledges the financial support of the European Commission (Horizon Europe grant ELIAS 101120237), the US Air Force Office of Scientific Research (FA8655-22-1-7034), and the Ministry of Education, University and Research (FARE grant ML4IP R205T7J2KP; grant BAC FAIR PE00000013 funded by the EU - NGEU). This work represents only the view of the authors. The European Commission and the other organizations are not responsible for any use that may be made of the information it contains.									

\clearpage

\ificml
\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here

\fi

%\ificml
	%\bibliography{biblio_antoine_autoexport_do_not_modify.bib,biblio.bib}
	%\bibliographystyle{icml2025}
%\else
\printbibliography
%\fi



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                              Appendix                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\ificml\onecolumn\fi

%\section{Notations}

\section{Quality of the MMD approximation: additional lemmas and proofs}
\label{app:quality_mmd_approx}

For any projector $P$, we denote $P^⟂=I-P$. For an event $B \in \mathcal{B}$ we denote by $B^c$ the complement of the event $B$, $B^c \coloneqq E \setminus B$.

We first provide a lemma quantifying the impact of the Nyström projections on the covariance operators in operator norm, and then prove the bound on the MMD approximation (\Cref{r:bound_MMD_nys}).

Similarly to $\ftldms$ and $\Pm$, we define $\ftldmsX=[ϕ(\ldmX{1}),…,ϕ(\ldmX{\lX})]:ℝ^{\lX}→\rkhs$, $\ftldmsY=[ϕ(\ldmY{1}),…,ϕ(\ldmY{\lY})]:ℝ^{\lX}→\rkhs$, and $\PmX,\PmY$ to be the orthogonal projections respectively onto $\spa(\ftldmsX)$ and $\spa(\ftldmsY)$.
Note that for any $v∊\rkhs$, it holds $\n{\Pm^\perp v}≤\n{\PmX^\perp v}$ and $\n{\Pm^\perp v}≤\n{\PmY^\perp v}$. 

\begin{tlemma}{}{bound_proj_halfcov}
	Let $δ>0$. 
	Let $\nf=\lX+\lY$ where
	$\lX=L(\nX)$, $\lY=L(\nY)$ and $L$ is as defined in \Cref{e:def_lX_lY}. 
	%\begin{align}
		%\lX &= L(\nX),\quad 
		%\lY = L(\nY)
		%\quad\text{and}\quad
		%L(n) \de n^γ\prt*{\log \tfrac{32n}{δ} }^{1-γ} \frac{78 c_γ z^2}{(19\supk)^γ}. 
		%\label{e:def_lX_lY}
	%\end{align}
	Let $\ldmX{1}, …, \ldmX{\lX}$ and $\ldmY{1}, …, \ldmY{\lY}$ be drawn with replacement proportionally to $(z,λ_0,δ/4)$-approximate leverage scores
	% In Chatalic-F.4 we already assume δ/2-LS
	respectively from the datasets $X$ and $Y$ for some $z≥1$ and $λ_0>0$ satisfying $λ₀ ≤ 19\supk \min(\tfrac{\log(\frac{32\nX}{δ})}{\nX}, \tfrac{\log(\frac{32\nY}{δ})}{\nY})$. 
	Under \Cref{a:poly_decay}
	it holds jointly with probability at least $1-δ$:
	\begin{align}
		‖\PmX^\perp C_P^{1/2}‖
		%&≤ \sqrt{57 \supk\frac{ \log(\tfrac{32\nX}{δ})}{\nX}} %\\
		&≤ \cE_{\textup{Cov}}(\nX)
		\quad\text{and}\quad
		‖\PmY^\perp C_Q^{1/2}‖
		%&
		%≤ \sqrt{57 \supk\frac{ \log(\tfrac{32\nY}{δ})}{\nY}} 
		≤\cE_{\textup{Cov}}(\nY)
		\quad\text{where}\quad
		\cE_{\textup{Cov}}(n)
		\de \sqrt{57 \supk\frac{ \log(\tfrac{32 n}{δ})}{n}} %\\
	\end{align}
	provided $\nX,\nY$ are large enough, namely:
	\begin{itemize}
		\item $\min(\nX,\nY)≥(1655+233\log(8\supk/δ))\supk$% (this is F.4, already with $δ/2$!),
		%--------  I suggest to use this rough formulation:
		\item $\frac{ \log(\tfrac{32 \nX}{δ})}{\nX}≤C(\supk, ‖\covP‖, γ, z, c_γ)$ and $\frac{\log(\tfrac{32 \nX}{δ})}{\nX}≤C(\supk, ‖\covQ‖, γ, z, c_γ)$.
		%--------  This is what we actually really want: (but ugly + need to define the constant of the effective dimension)
		%and $\frac{19\supk \prt*{ \log\frac{32\nX}{δ}}}{\nX} ≤ \min\left(‖\covP‖, \prt*{\tfrac{c_γ z²}{5}}^{1/γ}\right)$,
		%$\frac{19\supk \prt*{ \log\frac{32\nY}{δ}}}{\nY} ≤ \min\left(‖\covQ‖, \prt*{\tfrac{c_γ z²}{5}}^{1/γ}\right)$.
	\end{itemize}
	for some function $C$ made explicit in the proof.
\end{tlemma}
\begin{tproofof*}{r:bound_proj_halfcov}
	We apply \tcite[Lemma 7]{rudi2015LessMoreNystrom} for $X$ with probability $δ/4$. 
	%\note{In practice cf \tcite[Lemma F.4]{chatalic2023EfficientNumericalIntegration} with $δ/2$.}
	We get
	\begin{align}
		&\P*[‖\PmXo C_P^{1/2}‖ 
		≤ \sqrt{3λ_X}] ≥ 1-δ/2
	\end{align}
	%\color{red}
	%(Provided that the following assumptions are satisfied (both for $X,\nX,\lX,λ_X$ + eq. for $Y$) (this is for $δ/2$)
	%\begin{itemize}
		%\item (OK by assumption) $\tilde{X}_1, \dots, \tilde{X}_m$ are drawn with replacement proportionally to $(z,λ_0,δ/4)$-ALS for $X$, for some $z≥1,λ_0>0$, 
		%\item (OK by assumption) $\nX ≥ (1655 + 233 \log(8\supk/δ))\supk$;
		%\item (added as hypothesis for the chosen $λ$) $λ₀ ≤ λ$.
		%\item (added as hypothesis for the chosen $λ$) $λ≤‖C‖$)
		%\item (clearly satisfied for chosen $λ$) $\frac{19\supk\log(\frac{8\nX}{δ})}{\nX} ≤ λ $;
		%\item $\lX ≥ 78z^2\deff \log\frac{32\nX}{δ}$ (general)\\
			%$\lX ≥ 78z^2 c_γλ^{-γ}\log\frac{32\nX}{δ}$ (for poly decay by Chatalic2023-F.1)
		%\item $\lX ≥ 334\log\frac{32\nX}{δ}$ 
	%\end{itemize}
	%The second last hyp for poly decay and the claimed $λ$ is:
	%\begin{align}
		%\lX &≥ 78z^2 c_γ \prt*{ \frac{19\supk\log(32 \nX /δ)}{\nX} }^{-γ}\log\frac{32\nX}{δ} \\
		%&= (\nX/(19\supk))^γ 78z^2 c_γ \prt*{\log\frac{32\nX}{δ}}^{1-γ}
	%\end{align}
	%And the last one for $\lX$ chosen from this bound (this is what we give below) 
	%\begin{align}
		%\nX^γ\log(\tfrac{32\nX}{δ})^{1-γ}\tfrac{78 c z²}{(19\supk)^γ}
			%&≥ 334\log\frac{32\nX}{δ}\\
		%\text{satisfied provided:}\quad
		%(\nX/\log(\tfrac{32\nX}{δ}))^{γ}\tfrac{c z²}{(19\supk)^γ}
			%&≥ 5 \text{ because }(334/78≈4.3)
	%\end{align}
	%\color{black}
	and pick $λ_X=\frac{19\supk\log(32 \nX /δ)}{\nX}$
	and $\lX=(\nX)^γ \frac{78c_γz^2(\log \tfrac{32\nX}{δ})^{1-γ} }{(19\supk)^γ} $.
	These parameters correspond to the choice made in \pcite[Th.4.6]{chatalic2023EfficientNumericalIntegration} 
	and thus satisfy the requirements of \pcite[Lemma F.4]{chatalic2023EfficientNumericalIntegration}. %
	A similar argument for $\Y$ yields a similar bound for $‖\PmYo C_Q^{1/2}‖$, and the claimed result follows via a union bound. 
	The function $C$ in the statement of the lemma is thus defined as
	\begin{align}
		&C(\supk,‖\covP‖,γ,z,c_γ) \de \frac{1}{19\supk} \min\left(‖\covP‖, \prt*{\tfrac{c_γ z²}{5}}^{1/γ}\right)
	\end{align}
	%---- i.e. we enforce (and the same for Y):
	%$\frac{19\supk \prt*{ \log\frac{32\nX}{δ}}}{\nX} ≤ \min\left(‖\covP‖, \prt*{\tfrac{c_γ z²}{5}}^{1/γ}\right)$.
\end{tproofof*}

We now have everything to prove \Cref{r:bound_MMD_nys}.

\begin{tproofof*}{r:bound_MMD_nys}
	Applying multiple times the standard and inverse triangle inequalities, 
	\begin{align*}
		|\MMD(P,Q)-\ts| 
		&= \absv*{ \n{\kme{P}-\kme{Q}}-\n{\Pm \kmeX - \Pm \kmeY} } \\
		&≤ \n{\kme{P}-\kme{Q}-\Pm \kmeX + \Pm \kmeY} \\
		&≤ \n{ \kme{P} - \Pm \kmeX } + \n{ \kme{Q} - \Pm\kmeY } \\
		&≤ \n{ \kme{P} - \kmeX} +\n{\Pmo \kmeP } + \n{\kme{Q}-\kmeY} + \n{ \Pmo\kmeQ } \\
		&≤ \n{ \kme{P} - \kmeX} +\n{\PmXo \kmeX } + \n{\kme{Q}-\kmeY} + \n{ \PmYo\kmeY} \enspace.
	\end{align*}
	We apply \tcite[Theorem G.1]{chatalic2023EfficientNumericalIntegration} which is based on the standard Pinelis' concentration inequality (see \cite[Th. 3.5]{pinelis1994optimum}) both for $\X$ and $\Y$ with probability $δ/4$, and \Cref{r:bound_proj_halfcov} with probabibility $δ/2$. 
	Using a union bound, we get jointly with probability at least $1-δ$:
	\begin{align}
		|\MMD(P,Q)-\ts| 
		&≤ \eKME[δ/2][\nX] + \eKME[δ/2][\nY].
	\end{align}
	The function $C$ is the one defined in the proof of \Cref{r:bound_proj_halfcov}.
\end{tproofof*}


\section{Controlling the level and the power}

\subsection{Proof of \Cref{r:level}}

\begin{tproofof*}{r:level}{} 
	Note that under the null hypothesis, the distribution of the data $(X_1, \dots, X_{\nX}, Y₁, … , Y_{\nY})$ is invariant to permutation of its coordinates.
	The stated lemma is a consequence of \tcite[Proposition 3]{hemerik2018ExactTestingRandom}.
\end{tproofof*}

\subsection{Proof of \Cref{r:bound_threshold}}

\begin{tproofof*}{r:bound_threshold}{}
	Define the events 
	$\cA=\cb{\ts≤\thr}$ and
	$\cB_{\beta}=\cb{\MMD(P,Q)≥ \mathcal{E}_{MMD}(n_X, n_Y, \beta/2) +\thr}$. 
	Assume that $\P[\cB_{\beta}]>1-β/2$. We will show that $\P[\cA]≤β$.
	
	By the law of total probability, it holds
	\begin{align*}
		\P[\cA]
		&= \P[\cA \cap \cB_\beta]
		 + \P[\cA|\cB_\beta^c] \P[\cB_\beta^c] ≤ \P[\cA \cap \cB_\beta] 
		 + \P[\cB_\beta^c].
		 \label{e:first_ineq_cA}
	\end{align*}
	Using \Cref{r:bound_MMD_nys}, the first term in the above upper bound can be bounded as 
	\begin{align}
		\P[\cA \cap \cB_\beta] 
		&= \P[\ts≤\thr\, , \, \thr≤\MMD(P,Q)-\eMMD[\nicefrac{\beta}{2}]] \\
		&≤ \P[\ts≤\MMD(P,Q)-\eMMD[\nicefrac{\beta}{2}]] \\
		&≤ \P[|\MMD(P,Q)-\ts|≥\eMMD[\nicefrac{\beta}{2}]]\\
		&\leq \beta/2 \enspace.
	\end{align}
	We get the desired result by combining the last inequality and the fact that $\P[\cB_\beta^c]≤\beta/2$.
\end{tproofof*}


\subsection{Bounds on the quantiles}
\label{s:decomposition_U_R}

In order to prove \Cref{r:bound_TS_quantile_rough} and  \Cref{r:bound_quantile_rough} below, we first show how the squared test statistic can we decomposed as the sum of a $U$-statistic and a remainder, and provide a bound on the quantile of this $U$-statistic.% $\pstsU$.

The (randomly) permuted squared test statistic can be written 
\begin{align}
	\pts² 
	&\de \n*{\tfrac{1}{\nX} \sum_{i=1}^{\nX} \tfmap(\XY_{σ(i)}) - \tfrac{1}{\nY} \sum_{j=1}^{\nY} \tfmap(\XY_{σ(\nX+j)})}²\\
	&\de \n*{\tfrac{1}{\nY\nX} \sum_{i=1}^{\nX}\sum_{j=1}^{\nY} \prt*{\tfmap(\XY_{σ(i)}) - \tfmap(\XY_{σ(\nX+j)}) } }²\\
	%&= \tfrac{1}{\nX²\nY²} \sum_{i=1}^{\nX}\sum_{j=1}^{\nY}\sum_{i'=1}^{\nX}\sum_{j=1}^{\nY}  
		%\ip*{  \tfmap(X_i) - \tfmap(Y_j), \tfmap(X_{i'}) -  \tfmap(Y_{j'}) }\\
	&= \tfrac{1}{\nX²\nY²} \sum_{i=1}^{\nX}\sum_{j=1}^{\nY}\sum_{i'=1}^{\nX}\sum_{j=1}^{\nY}  
		h^σ(i,i';j,j') 
\end{align}
with $ h^σ(i,i';j,j') \de  \tk{\XY_{σ(i)},\XY_{σ(i')}} -  \tk{\XY_{σ(i)}, \XY_{σ(\nY+j')}} - \tk{\XY_{σ(\nY+j)},\XY_{σ(i')}} + \tk{\XY_{σ(\nY+j)}, \XY_{σ(\nY+j')}}$.

It can be decomposed as the sum of a (weighted) U-statistic and a remainder term,
\begin{align}
	\pts² 
	&= \tfrac{(\nX-1)(\nY-1)}{\nX\nY} \pstsU + \pstsR 
		\label{e:ts_decomposition},
\end{align}
with
\begin{align}
	\pstsU 
	&\de 
		\tfrac{1}{\nX(\nX-1)\nY(\nY-1)} \sum_{1≤i≠i'≤\nX} \sum_{1≤j≠j'≤\nY}  
		h^σ(i,i';j,j'),
		\label{e:def_U}
\end{align}
and
\begin{align}
	\pstsR
	&= \tfrac{1}{\nX²\nY²} 
		\prt*{\sum_{i=1}^{\nX}\sum_{1≤j≠j'≤\nY}  h^σ(i,i;j,j')
		+ \sum_{1≤i≠i'≤\nX}\sum_{j=1}^{\nY} h^σ(i,i';j,j) 
		+ \sum_{i=1}^{\nX}\sum_{j=1}^{\nY}  h^σ(i,i;j,j)
		}.
	\label{e:def_R}
\end{align}
We control  the $U$-statistic with high probability using a result from \tcite{kim2022MinimaxOptimalityPermutation} while we obtain an upper-bound for $\pstsR$ using the boundedness assumption.

\begin{tlemma}{Bound on the quantile of $\pstsU$ given $\X,\Y$}{bound_U_quantile_rough}
Let $0<α≤e^{-1}$. Then there is a universal constant $C'$ such that the $U$-statistic \eqref{e:def_U} associated to the Nyström kernel approximation satisfies
\begin{align}
	\P*[\pstsU ≤ C' \tfrac{\sqrt{n(n-1)} \supk}{\nX(\nX-1)} \log(1/α) \given \X,\Y] 
		&≥ 1-α,
\end{align}
\ie the r.h.s. of the above equation is an upper bound for the quantile $\pstsUq$.
\end{tlemma}
\begin{tproofof*}{r:bound_U_quantile_rough}
Let
\begin{align}
	 Σ_{\nX,\nY}²
		&\de \tfrac{1}{\nX²(\nX-1)²} \Big( \sum_{1≤i≠i'≤\nX} \ip{\tfmap{X_i},\tfmap{X_i'}}² 
			\\&\quad 
			+ \sum_{1≤j≠j'≤\nY} \ip{\tfmap{Y_j},\tfmap{Y_{j'}}}² 
			+ 2\sum_{1≤i≤\nX,1≤j≤\nY} \ip{\tfmap{X_i},\tfmap{Y_j}}² \Big) 
\end{align}
Given that $\tk(x,y)²=\ip{\Pm\fmap{x},\Pm\fmap{y}}²≤‖\fmap{x}‖²‖\fmap{y}‖²$ as $\Pm$ is a projector, it holds
\begin{align}
	 Σ_{\nX,\nY}²
		&≤ \tfrac{n(n-1) \supk²}{\nX²(\nX-1)²}.
\end{align}
By \tcite[Th. 6.1 and (31)]{kim2022MinimaxOptimalityPermutation}
, it holds for some constant $C'$ conditionnally on $\X,\Y$ with probability at least $1-α$:
\begin{align}
	\pstsU 
	&≤ \max\prt*{ \sqrt{C^{-1} Σ_{\nX,\nY}² \log(1/α) }, C^{-1} Σ_{\nX,\nY} \log(1/α)} \\
	&≤ C' Σ_{\nX,\nY} \log(1/α)  
\end{align}
where $C$ refers to the absolute constant of \tcite[Th. 6.1]{kim2022MinimaxOptimalityPermutation}.

\end{tproofof*}

We can now prove \Cref{r:bound_TS_quantile_rough}.

\begin{tproofof*}{r:bound_TS_quantile_rough}
Starting from \eqref{e:ts_decomposition}
\begin{align}
	\ts² 
	&= \tfrac{(\nX-1)(\nY-1)}{\nX\nY} \pstsU + \pstsR 
\end{align}
Given that $\tk(x,y)=\ip{\Pm\fmap{x},\Pm\fmap{y}}≤‖\fmap{x}‖‖\fmap{y}‖$ as $\Pm$ is a projector, it holds almost surely
\begin{align}
	|\pstsR|
	&≤ \tfrac{(\nY+\nX-1)}{\nX\nY} 4\supk
	 ≤ (\tfrac{1}{\nX} +\tfrac{1}{\nY}) 4\supk\\
	%\sqrt{|\pstsR|}
	%&≤ (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap \\
	%|\pstsU - \MMD²(P,Q)|
	%&≤ |\ts² - \MMD²(P,Q)| + |\pstsR|\enspace.
\end{align}
Moreover, by \Cref{r:bound_U_quantile_rough}, conditionally on $\X,\Y$, with probability at least $1-α$ it holds
\begin{align}
	\pstsU ≤ C' \tfrac{\sqrt{n(n-1)} \supk}{\nX(\nX-1)} \log(1/α)\enspace.
\end{align}
Therefore, with probability at least $1-\alpha$,
\begin{align}
	\ts² 
	&= \tfrac{(\nX-1)(\nY-1)}{\nX\nY} \pstsU + \pstsR \\
	&≤ C' \tfrac{(\nX-1)(\nY-1)}{\nX\nY}\tfrac{\sqrt{n(n-1)} \supk}{\nX(\nX-1)} \log(1/α) 
		+ (\tfrac{1}{\nX} +\tfrac{1}{\nY}) 4\supk \\
	&≤ C' \tfrac{\sqrt{n(n-1)} \supk}{\nX²} \log(1/α) 
		+ (\tfrac{1}{\nX} +\tfrac{1}{\nY}) 4\supk 
\end{align}
\end{tproofof*}

We now prove our high-probability bound on the threshold.

\begin{tproofof*}{r:bound_quantile_rough}
	%We begin by controlling the randomness coming from the $\mathcal{P}$ i.i.d. random permutations. 
	Using \pcite[Lemma 6]{domingo-enrich2023CompressThenTest}, it holds
	\begin{align}
		\P*[\thr ≤ q_{1-α_1}(\X,\Y) \given \X,\Y] &> 1 - \beta/2 
	\end{align}
	where $α_1\de \prt*{\frac{ \beta/2}{\binom{\np}{\lfloor α(\np+1) \rfloor}}}^{1/\lfloor α(\np+1) \rfloor}$.
	Moreover, by \Cref{r:bound_TS_quantile_rough}, provided $α₁≤e^{-1}$, it holds (almost surely for $\X,\Y$)
	\begin{align}
		\tq[α₁] 
		&≤ \supfmap \sqrt{C' \tfrac{\sqrt{n(n-1)}}{\nX²} \log(1/α₁) + (\tfrac{4}{\nX} +\tfrac{4}{\nY})}\\
		&≤ \sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log(1/α₁)} + (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap\enspace.
	\end{align}

	We will now upper bound $\log(1/\alpha_1)$ to make our bound more explicit.
	Denoting $c_α=\lfloor α(\np+1) \rfloor$, it holds $c_α≥1$ by assumption and using the inequalities $\binom{n}{k}≤\prt*{\frac{en}{k}}^k$ and $\lfloor x \rfloor \geq x/2$, we obtain
	\begin{align*}
		\log\prt*{\binom{\np}{c_α}^{1/c_α}} 
		%&≤ \frac{1}{c_α}\log\prt*{\prt*{\frac{e \np}{c_α}}^{c_α} } \\
		&≤ \log\prt*{\frac{e \np}{c_α}} ≤ \log\prt*{\frac{2 e \np}{α(\np+1)}} \leq \log\prt*{\frac{2e}{α}}\enspace.
	\end{align*}
	The population quantile can now be upper bounded as
	\begin{align}
		\tq[α₁] 
		&≤ \sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α (\beta/2)^{1/c_α}}}} + (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap
		\enspace.
	\end{align}
	We finish the proof by noting that the assumption $α_1≤1/e$ is not restrictive. Indeed
	\begin{align}
		α_1 
		&= \frac{(β/2)^{1/c_α}}{\binom{\np}{c_α}^{1/c_α}} %\\
		 ≤ \frac{1}{(\frac{\np}{c_α})}%\\
		 ≤ \frac{α(\np+1)}{\np} %\\
		 ≤ 2α
	\end{align}
	which is upper-bounded by $e^{-1}$ under our assumptions.
\end{tproofof*}

\subsection{Proof of \Cref{r:power_MMD} (main result)}
\label{s:proof_main_result}

Let 
%$\pstsUq$ and
$\tq$ denote the $(1-α)$-quantile (with respect to the random variable $σ$) for 
%the permuted $U$-statistic $\hat{U}^\sigma(X, Y)$ and for
the test statistic $\pts$. %, respectively.


\begin{tproofof*}{r:power_MMD}{}
By \Cref{r:bound_MMD_nys} (applied with $δ=β/2$), 
defining $\eMMD \de \eKME[δ/2][\nX] + \eKME[δ/2][\nY]$ and $\eKME$ as in \Cref{r:bound_MMD_nys}, it holds
\begin{align}
	\P[|\MMD(P,Q)-\ts|≥\eMMD[β/2]] &≤ β/2. 
\end{align}

Hence in order to upper bound the power of the test by $1-β$, it is sufficient by \Cref{r:bound_threshold} to find a condition on $P$ and $Q$ such that
\begin{align}
	\P[\thr≤\MMD(P,Q)-\eMMD[\beta/2]] > 1- \beta/2\enspace.
\end{align}

By \Cref{r:bound_quantile_rough} we get
\begin{align}
	\P*[\thr ≤ \sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α (\beta/2)^{1/c_α}}}} + (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap \given \X,\Y] &> 1 - β/2 
\end{align}

%%%%%%%%%%%%%%%%%%%
%\color{red}

%We are going to find  a deterministic upper bound on the (random) empirical quantile threshold $\thr$. We begin by controlling the randomness coming from the $\mathcal{P}$ i.i.d. random permutations. Using \pcite[Lemma 6]{domingo-enrich2023CompressThenTest}, 
%it holds
%\begin{align}
	%\P*[\thr ≤ q_{1-α_1}(\X,\Y) \given \X,\Y] &> 1 - \beta/2 
%\end{align}
%where $α_1\de \prt*{\frac{ \beta/2}{\binom{\np}{\lfloor α(\np+1) \rfloor}}}^{1/\lfloor α(\np+1) \rfloor}$.
%Moreover, by \Cref{r:bound_TS_quantile_rough}, provided $α₁≤e^{-1}$, it holds (almost surely for $\X,\Y$)
%\begin{align}
	%\tq[α₁] 
	%&≤ \sqrt{C' \tfrac{\sqrt{n(n-1)} \supk}{\nX²} \log(1/α₁) + (\tfrac{1}{\nX} +\tfrac{1}{\nY}) 4\supk}\\
	%&≤ \sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log(1/α₁)} + (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap\enspace.
%\end{align}

%We will now upper bound $\log(1/\alpha_1)$ to make our bound more explicit.
%Denoting $c_α=\lfloor α(\np+1) \rfloor≥1$, and using the inequalities $\binom{n}{k}≤\prt*{\frac{en}{k}}^k$ and $\lfloor x \rfloor \geq x/2$, we obtain
%\begin{align*}
	%\log\prt*{\binom{\np}{c_α}^{1/c_α}} 
	%%&≤ \frac{1}{c_α}\log\prt*{\prt*{\frac{e \np}{c_α}}^{c_α} } \\
	%&≤ \log\prt*{\frac{e \np}{c_α}} ≤ \log\prt*{\frac{2 e \np}{α(\np+1)}} \leq \log\prt*{\frac{2e}{α}}\enspace.
%\end{align*}
%The population quantile can now be upper bounded as
%\begin{align}
	%\tq[α₁] 
	%&≤ \sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α (\beta/2)^{1/c_α}}}} + (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap\enspace.
%\end{align}

%\color{black}
%%%%%%%%%%%%%%%%%%%

Hence we obtain the desired power bound provided
\begin{align*}
	\sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α} \prt*{2/β}^{1/c_α}} } + (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap
	%&≤ \MMD(P,Q) - \eMMD[β/2]\enspace.
	&≤ \MMD(P,Q) 
	\\&\quad
	- %\eMMD[β/2]
		( \eKME[β/4][\nX] + \eKME[β/4][\nY]), 
	%\\\text{where} \quad
	%\eKME &\de \frac{\supfmap}{\sqrt{n}} \prt*{2 \sqrt{2\log(4/δ)} +\sqrt{57\log(32 n/δ)}}
\end{align*}
that is
\begin{align*}
	\MMD(P,Q) 
		&≥ 
	\sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α} \prt*{\frac{2}{β}}^{1/\lfloor α(\np+1) \rfloor}} } 
	\\&\quad
		+ (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap
			\prt*{ 1 + \sqrt{2\log(16/β)} }
	\\&\quad
		+\supfmap\sqrt{57}
			\prt*{\frac{\sqrt{\log(128 \nX/β)}}{\sqrt{\nX}}
				+\frac{\sqrt{\log(128 \nY/β)}}{\sqrt{\nY}}}
\end{align*}
%Noting that $x\mapsto \log(ax)/\sqrt{x}$ is decreasing for $x>e^2/a$, it holds $\log(128 \nX/β)/\sqrt{\nX}≥\log(128 \nY/β)/\sqrt{\nY}$ and a sufficient condition is:
%\begin{align*}
	%\MMD(P,Q) 
		%&≥ 
	%\sqrt{C'}\supfmap \tfrac{\sqrt{n} }{\nX} \sqrt{\log\prt*{\frac{2e}{α} \prt*{\frac{2}{β}}^{1/\lfloor α(\np+1) \rfloor}} } 
	%\\&\quad
		%+ (\tfrac{1}{\sqrt{\nX}} +\tfrac{1}{\sqrt{\nY}}) 2\supfmap
			%\prt*{ 1 + \sqrt{2\log(16/β)} }
	%\\&\quad
		%+\supfmap\sqrt{57}
			%\prt*{\frac{\sqrt{\log(128 \nX/β)}}{\sqrt{\nX}}
				%+\frac{\sqrt{\log(128 \nY/β)}}{\sqrt{\nY}}}
%\end{align*}

\end{tproofof*}

\ificml
\section{Comparison to \tcite{domingo-enrich2023CompressThenTest}}
\label{s:ctt}

We add in \Cref{fig:ctt_susy} a comparison with the compress then test (CTT) model of \tcite{domingo-enrich2023CompressThenTest}.
For a fair comparison, our tests have been run with compression parameter $g=3$, which should allow to match the exact MMD power according to the authors' numerical studies. 
%provide the best possible results in terms of power. 
We have not been able to reproduce these results. 

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/ctt_susy}
        \ificml\vspace{-.3cm}\fi
        \caption{\textbf{Susy}, power against total sample size.}
        \label{fig:ctt_susy}
\end{figure}
\fi

\end{document}
