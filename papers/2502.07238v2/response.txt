\section{RELATED WORK}
\label{introduction}

\subsection{Suction Dataset} 
Zeng et al., "The Suction Dataset: A Large-Scale Cluttered Scene Benchmark" conducted a manual annotated dataset containing cluttered scenes. However, it relies solely on manual experience for annotating the suctionable and non-suctionable areas, making the annotation process highly time-consuming and expensive. Shao et al., "A Self-Supervised Learning Robotic Bin-Picking System" conducted a self-supervised learning robotic bin-picking system to generate training data and completed training within a few hours. However, it has only been validated on cylindrical objects, limiting the application of this method. Suctionnet, "Real-World Dataset for Suction-based Grasping" conducted a real word dataset which utilizes a new physical model to analytically evaluate seal and wrench formation. The annotation error depends on the accuracy of pose estimation in real scenes. Sim-Suction, "Sim-Suction: A Large-Scale Synthetic Dataset for Cluttered Environments" proposed a large-scale synthetic dataset for cluttered environments, which uses object-aware suction grasp sampling. However, there is currently a lack of readily available data in the parcel fields to explore and improve suction grasping. We propose a Self-Parcel-Suction-Labeling framework to build a large-scale parcel suction dataset which includes a variety of parcel objects collected from the real world. Furthermore, our dataset is specifically designed for parcel scenes, featuring densely stacked packages and a significant amount of flat objects.

%Moreover, our dataset is specifically designed for parcel scenes, where parcels are densely stacked and contains a substantial amount of flat object data features.


\begin{figure*}[t]
	\centering
		\includegraphics[width=1.95\columnwidth]{figures/102.jpg}
	\caption{Overview of Self-Parcel-Suction-Labeling pipeline. Firstly, utilize the image prompts to generate 3D parcel asset model to generate high-quality 3D assets with geometry and appearance information. Next, create random unstructured parcel scenes with the Bullet and Blender simulator platform. Finally, evaluate candidate suction grasps from four different perspectives to obtain accurate annotation labels.
    }
	\label{fig:fig102}
\end{figure*}

\subsection{Suction-based Grasping} 
Hernandez et al., "6D Pose Estimation for Suction-based Grasping" estimated the 6D pose of the object and projected the pre-defined suction configuration onto the objects in the scene. However, such methods lack generalization for new objects. Zeng et al., "Suction Affordance Learning with a Single FCN Architecture" designed a single FCN architecture trained on a human-labeled dataset, learning pixel-wise suction affordances. Suctionnet, "Pixel-Wise Suction Scores Prediction Network" proposed a pixel-wise suction scores prediction network which predicts seal score heatmap and center score heatmap separately. Sim-Suction, "Object-Aware Affordance Network for Suction-based Grasping" proposed a object-aware affordance network which directly inputs text prompts to obtain identify regions of interest grasp object and outputs point-wise suction probability. Zhang et al., "Suction Reliability Matrix and Region Prediction Model" proposed the suction reliability matrix and suction region prediction model. However, these methods have not been tested for their effectiveness in multi-object scenes. CoAS-Net, "Context-Aware Suction Network" proposed a context aware suction network trained on a synthetic dataset. Rui Cao et al., "Uncertainty-Aware and Multi-Stage Framework for Suction-based Grasping" proposed an uncertainty-aware and multi-stage framework which exploit both aleatoric and epistemic uncertainties. These methods essentially belong to discriminative approaches, while generative approaches have not yet been explored in terms of suction grasping task. Diffusion models, "Diffusion Models for Text-to-Image Synthesis" demonstrate remarkable performance in text-to-image generation task and surpass previous generative models, such as Generative Adversarial Networks (GAN), "Generative Adversarial Networks for Image Synthesis" or VAE, "Variational Autoencoders for Unsupervised Learning". We propose to utilize the diffusion model to generate a suction grasping score map from random noise with the guidance of the visual information of the input point cloud, instead of adopting it as a normal regression head. To the best of our knowledge, this is the first work introducing the diffusion model into the suction grasping prediction task.

%We propose to utilize the diffusion model to denoise random noise into suction grasping score map with the guidance of input point clouds visual-condition, instead of adopting it as a normal regression head. To the best of our knowledge, this is the first work introducing the diffusion model into suction grasping prediction task.