
\section{Low-Rank Initializers: LoftQ vs PiSSA} \label{app:lowrankinit}
Both LoftQ~\cite{li2023loftq} and PiSSa~\cite{meng2024pissa} use an iterative two-step process to enhance LoRA finetuning by exploiting low-rank structure and quantizing the residual. In each iteration:

\begin{enumerate}
    \item \textbf{Low-Rank Decomposition:} A singular value decomposition (SVD) of the current weight (or an updated version of it) is performed to factor out a low-rank approximation.
    \item \textbf{Residual Quantization:} The remaining component (i.e., the difference between the original weight and the low-rank approximation) is quantized to preserve overall model capacity with fewer bits.
\end{enumerate}

The key distinction lies in how these two steps are ordered:
\begin{itemize}
    \item \textbf{LoftQ} first quantizes the residual, which at the beginning is simply the full base weight. Thus, it ``initializes'' by treating the entire unmodified weight as a residual to be quantized and only then proceeds with the low-rank factorization in subsequent iterations.
    \item \textbf{PiSSA} starts by performing SVD on the unquantized base weight, extracting a low-rank representation before any quantization. Only after factoring out the low-rank component does PiSSa quantize the remaining residual. 
\end{itemize}


Below (Table \ref{tab:full_init_compare}) are the experimental results covering the full 2.0-to-4.0 range comparing these two initialization techniques. From our replication, LoftQ consistently outperforms PiSSa in terms of final task performance. We adopt LoftQ's mixed precision scheme as a result.


\begin{table}[ht]
    \centering
    \scalebox{0.8}{%
    \begin{tabular}{llcccccccccc}
        \toprule
        \multicolumn{2}{l}{\textbf{Setup}} & \multicolumn{5}{c}{\textbf{Llama-7b}} & \multicolumn{5}{c}{\textbf{Llama-13b}}\\
        \cmidrule(lr){1-2} \cmidrule(lr){3-7} \cmidrule(lr){8-12}
        \textbf{Method} & \textbf{Dataset} & \textbf{2.0} & \textbf{2.25} & \textbf{2.5} & \textbf{3.0} & \textbf{4.0} & \textbf{2.0} & \textbf{2.25} & \textbf{2.5} & \textbf{3.0} & \textbf{4.0} \\
        \midrule
        PiSSA & \multirow{2}{*}{WikiText-2 ($\downarrow$)} & 1919.63 & 795.88& 1938.33 &1397.26 &  5.53 & 1825.68 & 1822.62 & 1769.34 & 1549.48 & 5.05 \\
        LoftQ &  & 8.63 & 8.22 & 7.72  & 6.87 & 5.26 & 7.27 & 6.96 & 5.7 & 5.91 & 4.79 \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Perplexities of PiSSA and LoftQ as initialization methods on WikiText-2 covering full range from 2.0 to 4.0. Lower values indicate better performance ($\downarrow$).}
    \label{tab:full_init_compare}
\end{table}


