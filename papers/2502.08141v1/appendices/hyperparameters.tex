

\section{Experiment Hyperparamter Setup}\label{app:hyperparams}

In this appendix, we lay out the hyperparameters used for different experiments. Table~\ref{tab:wikitext2_hparams} details the settings for Llama on the Wikitext-2 dataset. Table~\ref{tab:cnn_dailymail_hparams} provides the configuration used for fine-tuning Bart-Large on CNN/DailyMail, and Table~\ref{tab:xsum_hparams} does the same for XSUM. Finally, Table~\ref{tab:qlora_hparams} describes the hyperparameters for Llama on the Open Assistant (oasst1) dataset.
\begin{table}[ht]
\centering
% Uncomment the following line if the table is wider than your text width.
% \resizebox{\textwidth}{!}{%
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\texttt{model\_name\_or\_path} & meta-llama/Llama-2-7b-hf \\
\texttt{data\_seed}            & 42 \\
\texttt{evaluation\_strategy}  & steps \\
\texttt{eval\_dataset\_size}   & 1024 \\
\texttt{max\_eval\_samples}    & 1000 \\
\texttt{per\_device\_eval\_batch\_size} & 4 \\
\texttt{dataloader\_num\_workers} & 3 \\
\texttt{lora\_r}               & 64 \\
\texttt{lora\_alpha}           & 64 \\
\texttt{lora\_modules}         & all \\
\texttt{bf16}                  & True \\
\texttt{warmup\_ratio}         & 0.03 \\
\texttt{lr\_scheduler\_type}   & cosine \\
\texttt{gradient\_checkpointing} & True \\
\texttt{dataset}               & wikitext \\
\texttt{dataset\_config}       & wikitext-2-raw-v1 \\
\texttt{per\_device\_train\_batch\_size} & 16 \\
\texttt{gradient\_accumulation\_steps}   & 4 \\
\texttt{max\_steps}            & 126 \\
\texttt{eval\_steps}           & 20 \\
\texttt{learning\_rate}        & 0.0003 \\
\texttt{adam\_beta2}           & 0.999 \\
\texttt{max\_grad\_norm}       & 0.3 \\
\texttt{weight\_decay}         & 0.1 \\
\texttt{seed}                  & 0 \\


\texttt{block\_size}           & 1024 \\
\bottomrule
\end{tabular}
%} % end of resizebox
\caption{Hyperparameters used for all Llama experiments on Wikitext-2.}
\label{tab:wikitext2_hparams}
\end{table}



\begin{table}[ht!]
\centering
% \resizebox{\textwidth}{!}{%  % Uncomment if you need to scale down the table
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\texttt{learning\_rate} & 1e-4 \\
\texttt{seed} & 11 \\
\texttt{dataset\_name} & cnn\_dailymail \\
\texttt{dataset\_config} & ``3.0.0" \\
\texttt{pad\_to\_max\_length} & True \\
\texttt{max\_source\_length} & 512 \\
\texttt{num\_train\_epochs} & 15 \\
\texttt{per\_device\_train\_batch\_size} & 8 \\
\texttt{per\_device\_eval\_batch\_size} & 32 \\
\texttt{gradient\_accumulation\_steps} & 32 \\
\texttt{model\_name\_or\_path} & facebook/bart-large \\
\texttt{evaluation\_strategy} & epoch \\
\texttt{predict\_with\_generate} & True \\
\bottomrule
\end{tabular}
% } % End \resizebox
\caption{Hyperparameters for fine-tuning Bart-Large on CNN/DailyMail.}
\label{tab:cnn_dailymail_hparams}
\end{table}


\begin{table}[ht!]
\centering
% \resizebox{\textwidth}{!}{%  % Uncomment if you need to scale down the table
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\texttt{learning\_rate} & 1e-4 \\
\texttt{seed} & 11 \\
\texttt{dataset\_name} & xsum \\
\texttt{dataset\_config} & ``3.0.0" \\
\texttt{pad\_to\_max\_length} & True \\
\texttt{max\_source\_length} & 512 \\
\texttt{num\_train\_epochs} & 25 \\
\texttt{per\_device\_train\_batch\_size} & 4 \\
\texttt{per\_device\_eval\_batch\_size} & 32 \\
\texttt{gradient\_accumulation\_steps} & 32 \\
\texttt{model\_name\_or\_path} & facebook/bart-large \\
\texttt{evaluation\_strategy} & epoch \\
\bottomrule
\end{tabular}
% } % End \resizebox
\caption{Hyperparameters for fine-tuning Bart-Large on XSUM.}
\label{tab:xsum_hparams}
\end{table}



\begin{table}[ht!]
\centering
% \resizebox{\textwidth}{!}{%  % Uncomment if the table is too wide
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\texttt{model\_name\_or\_path}      & meta-llama/Llama-2-7b-hf \\
\texttt{data\_seed}                 & 42 \\
\texttt{evaluation\_strategy}       & steps \\
\texttt{eval\_dataset\_size}        & 1024 \\
\texttt{max\_eval\_samples}         & 500 \\
\texttt{per\_device\_eval\_batch\_size} & 1 \\
\texttt{max\_new\_tokens}           & 32 \\
\texttt{dataloader\_num\_workers}   & 3 \\
\texttt{group\_by\_length}          & True \\
\texttt{logging\_strategy}          & steps \\
\texttt{remove\_unused\_columns}    & False \\
\texttt{lora\_r}                    & 64 \\
\texttt{lora\_alpha}                & 64 \\
\texttt{lora\_modules}              & all \\
\texttt{bf16}                       & True \\
\texttt{warmup\_ratio}              & 0.03 \\
\texttt{lr\_scheduler\_type}        & constant \\
\texttt{gradient\_checkpointing}    & True \\
\texttt{dataset}                    & oasst1 \\
\texttt{source\_max\_len}           & 16 \\
\texttt{target\_max\_len}           & 512 \\
\texttt{per\_device\_train\_batch\_size} & 4 \\
\texttt{gradient\_accumulation\_steps}   & 4 \\
\texttt{max\_steps}                 & 1875 \\
\texttt{eval\_steps}                & 200 \\
\texttt{learning\_rate}             & 0.0002 \\
\texttt{adam\_beta2}                & 0.999 \\
\texttt{max\_grad\_norm}            & 0.3 \\
\texttt{lora\_dropout}              & 0.1 \\
\texttt{weight\_decay}              & 0.0 \\
\texttt{seed}                       & 0 \\
\bottomrule
\end{tabular}
% } % end of \resizebox
\caption{Hyperparameters used for all Llama experiments on Open Assistant (oasst1).}
\label{tab:qlora_hparams}
\end{table}


\newpage