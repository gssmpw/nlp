
\section{\FWName System Support for Low-Bit Fine-Grained LoRA Fine-tuning} \label{app:systemsupport}


In this appendix, we provide an overview of the CUDA-based system support we built for supporting low-bit fine-grained quantization and dequantization for LoRA fine-tuning. We specifically introduce the \textbf{\textit{Quantize}} and \textbf{\textit{Dequantize}} Kernels for low-bit fine-grained LoRA fine-tuning. We integrate this into the \textit{bitsandbytes}\footnote{https://github.com/bitsandbytes-foundation/bitsandbytes} library for usability.


\begin{figure}[ht]
\vspace{-14pt}
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{images/quant_kernel.pdf}}
\vskip -0.2in
\caption{Overview of Kernel for Low-Bit Fine-Grained \textbf{\textit{Quantzation}}. Indexing logic omitted for simplicity.}
\label{fig:quant_kernel}
\end{center}
\vskip -0.4in
\end{figure}


\begin{figure}[ht]
\vspace{-14pt}
\begin{center}
\centerline{\includegraphics[width=0.85\columnwidth]{images/dequant_kernel.pdf}}
\vskip -0.2in
\caption{Overview of Kernel for Low-Bit Fine-Grained \textbf{\textit{Dequantization}}. Indexing logic omitted for simplicity.}
\label{fig:dequant_kernel}
\end{center}
\vskip -0.5in
\end{figure}

\subsection{Quantization (Figure \ref{fig:quant_kernel})}
During the quantization process, the kernel operates on each block of data from the weight tensor by loading it and computing its maximum magnitude, which is then stored and used for normalization. Next, each value in the block is quantized according to its channel’s bit precision (defined by \texttt{Precs}) and channel-specific decision boundaries (provided as arrays of decision trees). Finally, the packed, quantized results are written to the output buffer at the appropriate offset.



\subsection{Dequantization (Figure \ref{fig:dequant_kernel})}
During the dequantization process, each thread in the kernel calculates its offset into the quantized buffer based on the channel’s bit precision (\texttt{Precs}) and then loads the necessary packed bytes. Using the block’s \texttt{absmax} array, the thread rescales the dequantized values, which are obtained from a per-channel lookup table indexed by the unpacked quantized values. Depending on the precision (1, 2, or 4 bits), the kernel unpacks bits from the packed bytes, retrieves the corresponding float from the channel-specific lookup table, multiplies by \texttt{absmax}, and writes the final results into the output tensor.
% \subsection{Motivation and Overview}
% Blockwise mix-precision quantization is a technique aimed at reducing both memory usage and computational overhead in neural network inference and training. Traditional uniform quantization strategies (e.g., 8-bit or 4-bit) are often suboptimal because not all channels exhibit the same sensitivity to quantization error. Instead, this approach assigns different bit-depths (\(b \in \{1,2,4\}\)) to each output channel of a weight matrix, thereby better matching the distribution of each channel while preserving overall model accuracy.

% \subsection{Blockwise Partitioning and Scaling}
% Given an \(\mathrm{out\_features} \times \mathrm{in\_features}\) matrix, we divide it into blocks of size \(\texttt{BLOCK\_SIZE}\). Within each block, we compute the local maximum absolute value \(\alpha\). This \(\alpha\) is stored in a corresponding array (\(\texttt{absmax}\)) and used to normalize the elements in the block to facilitate quantization.

% \subsection{Channel-Specific Bit-Precision}
% A key extension of this method is the per-output-channel bit-precision stored in the array \(\texttt{precs}\). Each entry in \(\texttt{precs}\) indicates whether the associated output channel is quantized to 4 bits, 2 bits, or 1 bit. After normalizing by \(\alpha\), the data are quantized via \emph{decision boundaries} tailored to each channel:
% \[
% \mathrm{index} = \begin{cases}
% 0, & \text{if } x \le d_0, \\
% 1, & \text{if } d_0 < x \le d_1, \\
% \ldots & \ldots \\
% 2^b - 1, & \text{otherwise},
% \end{cases}
% \]
% where \(\{d_i\}\) are channel-specific thresholds, and \(b \in \{1,2,4\}\) is the bit-depth. This yields a set of discrete indices in \(\{0,\dots,2^b - 1\}\).

% \subsection{Bit-Packing and Offsets}
% Once the discrete indices are computed, they are compressed into bytes:
% \begin{itemize}
%     \item \textbf{4-bit}: 2 values per byte,
%     \item \textbf{2-bit}: 4 values per byte,
%     \item \textbf{1-bit}: 8 values per byte.
% \end{itemize}
% We maintain an auxiliary \(\texttt{block\_byte\_offsets}\) array to track where each block’s compressed bytes begin within the overall buffer.

% \subsection{Dequantization}
% Recovery of the original values follows the reverse process:
% \begin{enumerate}
%     \item Read the per-block maximum \(\alpha\) from \(\texttt{absmax}\).
%     \item Determine the bit-precision \(b\) from \(\texttt{precs}\).
%     \item Unpack the compressed indices from the byte-aligned buffer.
%     \item Map the integer indices to normalized values via each channel’s dequantization mapping.
%     \item Rescale these normalized values by \(\alpha\) to produce the final dequantized output.
% \end{enumerate}
% This procedure restores an approximation of the original tensor with minimal overhead.

% \subsection{GPU Parallelization}
% Both the quantization and dequantization kernels are implemented in CUDA, where each thread block processes a subset of tensor blocks. The block-level structure allows efficient reuse of local memory, while the per-channel bit-precision instructions reduce global reads and writes. This design is particularly advantageous on large-scale GPUs where memory bandwidth can be a bottleneck.


% \subsection{Key Advantages}
% \begin{itemize}
%     \item \textbf{Adaptive Precision}: Channels demanding higher fidelity (e.g., those with larger or more variable distributions) can use 4 bits, while simpler channels can go down to 2 bits or 1 bit.
%     \item \textbf{Lower Memory Footprint}: The bit-packing procedure substantially reduces storage requirements, crucial for large models.
%     \item \textbf{Accuracy Preservation}: By normalizing each block individually and using channel-specific boundaries, the approach preserves accuracy better than uniform quantization schemes.
%     \item \textbf{Efficient Implementation}: CUDA-based kernels use blockwise parallelism, enabling the method to leverage GPU acceleration effectively.
% \end{itemize}
