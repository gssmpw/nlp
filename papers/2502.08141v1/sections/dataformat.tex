\section{Mapping and Threshold Learner} \label{sec:learner}

% \cyrus{Shuffle the order of the following two }
% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml2024/map_thr_learner_draft.pdf}}
% \caption{MSE error vs Number of Iterations/Samples}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

In this section, we introduce the mapping and threshold learner in \FWName. Because we want the final base weights to remain task-agnostic and thus reusable across multiple adapters, we adopt a simple approach that minimizes the \emph{mean squared error}\footnote{We define \textbf{SE} (\textbf{S}quared \textbf{E}rror) as 
  $(\hat{y} - y)^2$, i.e., the squared difference between the predicted 
  and ground-truth values. Therefore, \textbf{MSE} is the mean of these 
  squared errors over the dataset.} (\emph{MSE}) in each output channel. As discussed in \S\ref{sec:designchoices}, one could learn separate decoding mappings for each downstream task (or adapter set), but at a higher fine-tuning cost. We therefore propose an efficient design for the mapping/threshold learner that avoids this expense.

\paragraph{Weighted Lloyd-Max Algorithm.}

We cast the problem of searching for the optimal quantization mappings and thresholds for each output channel to minimize MSE as a Weighted Lloyd-Max Problem. 
A detailed description of this algorithm can be found in Appendix \ref{sec:lloyd-max}.

\paragraph{Weighted Lloyd's for LoRA Quantization.}
As discussed in \ref{sec:designchoices}, we perform groupwise normalization to give more scale to quantization within each output channel. To recap, with groupwise normalization (Section \ref{sec:groupnorm}), each block of weights is scaled by the block-wise maximum absolute value ($absmax$).  Specifically, if we denote the set of original weights in a block by \(\{ x_i \}\)  and its maximum absolute value by $absmax$, then we treat $absmax$ as a per-block weight in the Weighted Lloyd-Max algorithm. By assigning them proportionally larger weights, the algorithm ``pays more attention'' to those blocks and adjusts their bin thresholds and centroids accordingly. Consequently, blocks whose values have smaller magnitudes (and thus smaller $absmax$) are penalized less, striking a balance across all blocks to minimize the overall quantization error in QLoRA.

In our application of the Weighted Lloyd's algorithm to LoRA base weight quantization, we initialize the thresholds as those used by \textit{NormalFloats} \cite{dettmers2024qlora} for 2-bit and 4-bit precisions and use $0.0$ as the initial threshold for 1-bit quantization\footnote{Separately defined as NormalFloats lack a 1-bit representation}\footnote{See the initial values in Appendix \ref{app:lloydinit} }. Then, at each iteration, we recompute the quantization mappings as the weighted centroids of the assigned data and recompute the thresholds as the midpoints between consecutive mapping (centroid) values\footnote{In our implementation, we set number of iterations to 2}. We output the last-computed quantization mappings and thresholds when max iteration is reached or the MSE stops going down.

% \qz{Need a one-sentence conclusion of what the learner does, and why weighted lloyd is relevant}



% \paragraph{Weighted Lloyd's for LoRA Quantization.}
% With groupwise normalization (Section \ref{sec:groupnorm}), each block of weights is scaled by the block-wise maximum absolute value (\texttt{absmax}). Specifically, if we denote the set of original weights in a block by \(\{ x_i \}\) and its maximum absolute value by \(\texttt{absmax}\), then we treat \(\texttt{absmax}\) as a per-block weight in the Weighted Lloyd's algorithm. This reflects the fact that blocks with higher dynamic ranges inherently introduce larger quantization errors. By assigning them proportionally larger weights, the algorithm ``pays more attention'' to those blocks and adjusts their bin thresholds and centroids accordingly. Consequently, blocks whose values have smaller magnitudes (and thus smaller \(\texttt{absmax}\)) are penalized less, striking a balance across all blocks to minimize the overall quantization error in QLoRA.

% \paragraph{Unifying Thresholds Across Output Channels for Distribution Reconstruction} 

% After we learn  the quantization mappings and thresholds for each output channel, it is necessary to reconcile the differences between these thresholds in each layer to accurately recover the original distribution. In our implementation, we unify the thresholds in each layer by taking the average of the learned thresholds as a simple solution. \cyrus{Show why here}. 

In our current implementation, we take the average of all thresholds to preserve distribution and prevent instability in the interaction with the Low-Rank Initializer.