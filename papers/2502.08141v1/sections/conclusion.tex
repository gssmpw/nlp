\section{Conclusion and Future Work}

As LLMs scale, fine-tuning remains computationally and memory intensive, even with parameter-efficient methods like LoRA. 
We introduced \FWName{}, the first framework to enable accurate LoRA fine-tuning below 2 bits per parameter. 
By addressing key limitations in quantized LoRA, \FWName{} leverages fine-grained precision assignment, adaptive quantization mappings, and optimized CUDA kernels to minimize memory overhead while preserving performance.

Extensive evaluations show that \FWName{} achieves a superior performanceâ€“precision trade-off above 2 bits and remains accurate at even 1.15 bits per parameter, reducing memory usage by up to 50\%. 
This enables fine-tuning in ultra-resource-constrained environments, making LLMs more accessible for real-world applications.
Looking ahead, \FWName{} paves the way for ultra-low-bit fine-tuning and deployment. 
We hope it inspires further research and brings efficient LLM fine-tuning and inference to mobile devices, embedded systems, and beyond.