\section{Limitations and Future Work}

\paragraph{Task-Dependent Quantization} The mixed-precision and mapping/threshold learners used by \FWName{} ensure that the final base weights are task agnostic and can be freely shared with multiple adapters. Yet, LoRA base weights can be reused by adapters/tasks that share some common characteristics under many scenarios. We encourage future works to exploit these more fine-grained task similarities. Example approaches include automatic calibration set synthesis and post-finetuning quantization based on checkpoint characteristics.

\paragraph{Co-design Between Quantization, Low-Rank Tensor Initialization, and Downstream Tasks} \FWName{} integrates these components in a disjoint manner. Future works can consider a co-design that ensures the initialized low-rank tensors can both (1) better capture the forefront quantization errors and (2) better drive the subsequent finetuning dynamics.