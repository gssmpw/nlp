\section{The \FWName{} Framework}\label{sec:e2eframework}

In this section, we provide an overview of the \FWName{} end-to-end workflow, illustrated in Figure~\ref{fig:e2eworkflow}. The process begins with the pretrained model weights \textbf{\textit{(T1)}}. We feed each layer of these weights into a dedicated mapping and thresholds learner \textbf{\textit{(P1)}}, which produces optimized per-output-channel mappings and thresholds, denoted \textbf{\textit{(T2)}}.
These mappings and thresholds, along with the pretrained weights, are then processed by a two-step ILP quantizer \textbf{\textit{(P2)}} to determine the optimal precision assignments \textbf{\textit{(T3)}} for each output channel.

Next, the output-channel-wise quantize kernel \textbf{\textit{(P3)}}, which supports custom quantization thresholds, uses the derived thresholds \textbf{\textit{(T2)}} and the assigned precision levels \textbf{\textit{(T3)}} to quantize the weights. We calculate the quantization errors arising from this step and apply low-rank tensor initialization \textbf{\textit{(P4)}}. Techniques for intelligent low-rank initialization include LoftQ  \cite{li2023loftq} and PiSSa \cite{meng2024pissa} (Appendix \ref{app:lowrankinit}) which generates low-rank tensors \textbf{\textit{(T5)}} designed to absorb quantization errors during initialization. In our implementation, we opt for LoftQ \cite{li2023loftq} as experiments show that they give better performance in the low-bit range.

The resulting mixed-precision quantized weights \textbf{\textit{(T4)}} and the initialized low-rank tensors \textbf{\textit{(T5)}} are then passed to a fine-tuning module \textbf{\textit{(P5)}}, which relies on an output-channel-wise dequantize kernel \textbf{\textit{(P5.1)}} to recover the base weights from their quantized form. Following the approach used in LoRA and QLoRA, the base weights \textbf{\textit{(T4)}} remain frozen, and only the low-rank tensors \textbf{\textit{(T5)}} are trained. Finally, we obtain the updated low-rank tensors \textbf{\textit{(T6)}}, which are output together with the quantized base weights (and associated quantization state) \textbf{\textit{(T4)}}.
