%!TEX root = ../main.tex

\section{Introduction}

% Large language models (LLMs) have demonstrated impressive capabilities in natural language tasks like text generation~\cite{?}, question answering~\cite{?}, and reading comprehension~\cite{?}.
% Large language models (LLMs) have demonstrated impressive capabilities across a diverse range of tasks, including text generation~\cite{?}, question answering ~\cite{?}, reading comprehension~\cite{?}, summarization~\cite{?}, and code generation. 
Fine-tuning large language models (LLM) can enhance their performance on particular tasks ~\cite{wei2021finetuned,wang2022super, ziegler2019fine}, and remove unwanted behaviors like hallucinations~\cite{hu2024mitigating,liu2023mitigating} and harmful responses~\cite{bai2022training,askell2021general}.  Yet as models scale -  \eg Llama 3.1 with 405 billion parameters~\cite{dubey2024llama} and DeepSeek-V3 with 671 billion parameters~\cite{liu2024deepseek} - the cost of fine-tuning soars~\cite{hu2021lora}. 

% This makes model fine-tuning increasingly expensive in terms of compute and GPU memory, since full fine-tuning requires updating all model parameters.

% \begin{figure}[ht]
% % \vskip 0.2in
% \vspace{-10pt}
% \begin{center}
% \centerline{\includegraphics[width=1.0\columnwidth]{icml2025/images/intro_fig.pdf}}
% \caption{Different methods of LLM fine-tuning. \qz{Add stripes to frozen weights? Font? Maybe ours?}}
% \label{fig:fine-tuning}
% \end{center}
% \vspace{-20pt}
% \end{figure}

% \hermann{This framing feels a bit off to me. I get that we reduce the model size but the way we set it up here feels like we are going to show experiments later where we deploy on robots, phones, etc. Maybe we can just say even with LORA, full precision finetuning is still expensive and the cost of serving them too?} \qz{You're right: Let's move this to ``potential benefits" unlocked by LowRA}
% Despite the early promise of LoRA fine-tuning, there has been a higher demand of customized fine-tuning and deploying these fine-grained fine-tuned models onto resource constrained environments (\eg, robots~\cite{?}, mobile phones~\cite{?}, web browsers~\cite{?}, and edge devices~\cite{?}). 
% Nevertheless, progresses on satisfying these demands have long been stifled by prohibitively high costs of both fine-tuning and serving. 

To reduce fine-tuning costs, parameter-efficient fine-tuning (PEFT) methods \cite{zaken2021bitfit,hu2021lora,mao2021unipelt,liu2022few} freeze a model’s core weights and insert small trainable modules. We focus on LoRA \cite{hu2021lora}, which adds rank-decomposed adapters to cut computation and memory demands. Still, fine-tuning large models on a single GPU can exceed memory limits. Quantized LoRA approaches (e.g., QLoRA \cite{dettmers2024qlora}, LoftQ \cite{li2023loftq}) resolve this by quantizing the base weights with minimal loss in accuracy, enabling billions-parameter models to be fine-tuned on standard single GPUs or even mobile devices.


% In this paper, we focus on LoRA~\cite{hu2021lora} fine-tuning, one of the most widely adopted PEFT methods in recent years. Vanilla LoRA freezes the base model weights and injects trainable rank-decomposed matrices, called ``adapters", significantly reducing the computation and memory requirements of fine-tuning. Even with LoRA, the memory requirements for finetuning LLMs on a single GPU remain excessive \cite{dettmers2024qlora}. This has led to follow-up works, \eg QLoRA~\cite{dettmers2024qlora}, LoftQ~\cite{li2023loftq}, which quantize the base model weights to further push down the memory requirements during fine-tuning, without performance degradation. This line of work---\textit{quantized LoRA}---enables accurate fine-tuning of large language models, even those with billions of parameters, on consumer-grade GPUs with limited memory and their deployment to resource-constrained devices like mobile phones \cite{dettmers2024qlora}.

% has demonstrated great early success in enabling compute-efficient, memory-efficient, and accurate fine-tuning of LLMs, from training in data-center scale compute racks~\cite{?} to deployment on low-resource embedded edges and mobile phones~\cite{?}. 
%\cyrus{Let's mention that advantages from such compression also translates to inference/deployment?}  

% \cyrus{Add motivations behind why do we care about compressing models below 4 bits}

Despite the success of quantized LoRA in cutting memory usage, all existing works focus on LoRA fine-tuning within the range of 2 to 4 bits (per parameter), many of which are incompatible with ultra-low-bit LoRA fine-tuning~\cite{wang2024lora,meng2024pissa,dettmers2024qlora}. Further pushing down the bits (per parameter) requirement, \eg below 2 bits, has profound implications in fine-tuning and deployment in ultra-low-resource scenarios like embedded devices~\cite{shen2023agilequantactivationguidedquantizationfaster, chai2025flexquantelasticquantizationframework} and mobile phones~\cite{wang2025bitstackanysizecompressionlarge, tan2024mobilequantmobilefriendlyquantizationondevice}. However, current methods face three fundamental limitations:
\begin{itemize}
    \item \textbf{L1:} focus exclusively on coarse-grained quantization of the base model weights.
    \item \textbf{L2:} leverage quantization functions (i.e., mappings and thresholds) that assume some fixed data distribution across the entire model weight.
    \item \textbf{L3:} rely on simulated quantization, with no system support for efficient low-bit quantization. 
\end{itemize}
% \cyrus{Better motivate why we need low-bit, and even below 2-bit, quantization}
% \textbf{(L1)} focus exclusively on coarse-grained quantization of the base model weights, %leaving opportunity for further memory and computation savings on the table,
% \textbf{(L2)} leverage quantization data formats that assume some fixed data distribution across the entire model weight, and
% %ignore the empirical fact that to achieve high application accuracy, different channels and different layers have different tolerance of data loss in quantization, and 
% \textbf{(L3)} rely on simulated quantization, with no system support for efficient low-bit quantization. 
% %\cyrus{Can we add the limitation thre that many existing techniques focus on improving LoRA in the higher precision range, but may not be compatible lower precision range (e.g., PiSSa, LoraGA)}

To unleash the full potential of quantized LoRA fine-tuning, we present \FWName{}, an accurate and efficient framework that enables 
%fine-grained and mixed-precision quantization for LoRA fine-tuning under 2 bits (per parameter). 
LoRA fine-tuning down to below 2 bits (per parameter). 
\FWName{} features three major components for each of the three challenges:
\begin{itemize}
 \item \textbf{(1)} mapping/threshold function search,
 \item \textbf{(2)} fine-grained precision assignment, and 
 \item \textbf{(3)} CUDA-based kernels as quantization primitives.
\end{itemize}


Addressing \textbf{L1} and \textbf{L2} requires extra care because LoRA base weights have to work with multiple sets of adapters in real-life settings \cite{sheng2024slora,ostapenko2024towards,chen2024punica}. 
This constraint demands a powerful, task-agnostic quantization technique. 
Furthermore, optimally assigning precisions at a fine-grained granularity for LLMs calls for a scalable, low-complexity solution to handle massive parameter spaces. 
\FWName{} meets the need through a hierarchical ILP(Integer Linear Programming)-based precision assigner for performing fine-grained mixed-precision. 
Moreover, \FWName{} provides a weighted Lloyd-Max \cite{lloyd1982least,max1960quantizing}  formulation of mapping/threshold search for groupwise normalization, and achieves strong practical performance through its efficient solution.

% given the groupwise normalization typically done in LoRA quantization, we cast the search for optimal quantization mappings and thresholds as a weighted Lloyd–Max problem \cite{lloyd1982least,max1960quantizing} and show that \FWName{'s} an efficient solution can yield strong results.
% \qz{We might need to refactor this paragraph to include a brief conclusion of key design choices and benefits enabled by them}

We conduct extensive evaluation of \FWName{} across \nummodels{} choices of widely used base LLMs and \numdatasets{} choices of natural language applications, and compare \FWName against state-of-the-art baselines. 
% \qz{Evaluation take-aways: Note that matching QLoRA/LoftQ with much fewer bits is a major break-through result in this paper, so we should make that a highlight.}
Evaluation results demonstrate that \FWName: 
\textbf{(1)} achieves a superior performance-precision trade-off above 2 bits (per parameter) compared to baselines, and is the first method to enable accurate, efficient LoRA fine-tuning below 2 bits,
\textbf{(2)} enables substantial memory footprint reduction in fine-tuning, and
\textbf{(3)} incurs minimal additional overhead even with newly added components.
% offers a flexible design with mininal additional overhead that can be easily adapted or combined with other approaches in this domain.
% \qz{Another highlight: \FWName enables sub-2-bit-range fine-tuning of LoRA, making LoRA fine-tuning and deployment a feasible option in constrained and adverse environments, \eg edge.}

In summary, we make the following contributions:
\begin{itemize}
    \item \textbf{Identifying Key Limitations:} We identify three core limitations in existing quantized LoRA approaches, highlighting opportunities to exploit fine-grained precision assignment and mappings/thresholds.
    \item \textbf{Design and Implementation of \FWName:} We introduce \FWName, an accurate, end-to-end framework that applies fine-grained quantization to LoRA fine-tuning of LLMs. 
    We detail its key design choices—including a mappings/thresholds learner, precision assigner, and practical programming primitives—ensuring both efficiency and high performance.
    \item \textbf{Better Performance-Precision Trade-Off:} Comprehensive evaluations show that \FWName outperforms baselines in performance-precision trade-off, enabling an average 0.86-bit reduction per parameter without sacrificing performance.
    \item \textbf{Memory Footprint Reduction:} \FWName cuts memory usage by 30–50\% during fine-tuning and deployment with minimal performance loss.
    Moreover, it enables fine-tuning and deploying LLMs in ultra-resource-constrained settings at as low as 1.15 bits. 
    % \qz{Evaluation results: Across \nummodels{} model and \numdatasets{} dataset, same ACC, \fillme fewer bits; we push the limit to \lowestprec{} bits}
    \item \textbf{Open-Source Framework:} We will open-source our framework and artifacts to spur further research in ultra-low-bit LoRA fine-tuning.
    % \item We fully open-source our framework and encourage future works on searching quantization mappings/thresholds and precision assignment under diverse specific use cases: for example, when LoRA base weights are shared with different sets of adapters.
    % \qz{We will release the source code and artifact upon publication of this work?}
\end{itemize}

This paper is organized as follows: Section~\ref{sec:background} introduces LoRA fine-tuning, quantization for LoRA, and three key limitations of existing quantized LoRA methods. Section~\ref{sec:e2eframework} presents the \FWName{} end-to-end workflow, while Section~\ref{sec:design} discusses its key design insights and benefits. Sections~\ref{sec:learner} and \ref{sec:assigner} detail the mapping/threshold learner and the fine-grained precision assignment, respectively. Finally, we describe our evaluation setup, results, and takeaways in Section~\ref{sec:eval}.