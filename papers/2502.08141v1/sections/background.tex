\section{Background and Motivation}
\label{sec:background}

\begin{figure*}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/loraxworkflow.pdf}}
\vspace{-10pt}
\caption{End-to-end workflow of \FWName{}.}
\label{fig:e2eworkflow}
\end{center}
\vspace{-30pt}
\end{figure*}

\subsection{Low-Rank Adaptation (LoRA) of LLMs}

Fine-tuning large language models (LLMs) allows us to adapt pre-trained LLMs to particular tasks or domains~\cite{wei2021finetuned,wang2022super, ziegler2019fine}.
This process usually requires changing all model parameters, which can be prohibitively expensive (in terms of compute and memory) when the number of model parameters increases.
Low-Rank Adaptation (LoRA) \cite{hu2021lora} tackles this by freezing the base weights and introducing a small set of trainable ``adapter'' parameters, drastically reducing memory and compute requirements for fine-tuning.

% \cyrus{More detailed introduction of LoRA?}

\subsection{Quantization for LoRA Fine-Tuning}

Quantized LoRA fine-tuning further cuts memory usage by quantizing the base model weights without hurting performance. QLoRA \cite{dettmers2024qlora} introduces a \textit{NormalFloat} format to backpropagate through a 4-bit quantized backbone, while LoftQ \cite{li2023loftq} and ApiQ \cite{liao2024apiq} jointly optimize quantized base weights and adapter initializations under a unified objective. These advances unlock fine-tuning and deployment of LLMs on low-resource platforms like embedded devices \cite{shen2023agilequantactivationguidedquantizationfaster, chai2025flexquantelasticquantizationframework} and mobile phones \cite{wang2025bitstackanysizecompressionlarge, tan2024mobilequantmobilefriendlyquantizationondevice}.

% \cyrus{More detailed illustration of QLoRA and LoftQ and PiSSA?}

\subsection{Limitations of Existing Quantized LoRA Methods} \label{sec:limitations}
Despite the early promise of recent work in quantized LoRA like QLoRA and LoftQ, we observe three major limitations that prevent us from fully realizing the potential of memory-efficient LLM fine-tuning. 

\paragraph{L1: Coarse-Grained Precision Assignment.}
Existing approaches typically apply a single quantization precision to an entire weight matrix or multiple layers. For instance, QLoRA uses uniform 4-bit quantization across all base weights, while LoftQ adopts a layerwise mixed-precision scheme (e.g., higher precision for earlier layers, lower for later layers). Our findings (\S\ref{sec:assigner}) suggest that truly unlocking ultra-low-bit fine-tuning requires a finer-grained assignment strategy, potentially at the sub-layer or sub-matrix level.

\paragraph{L2: Discrepancy in Data Distribution.}
Most quantized LoRA methods use a globally shared data format—such as QLoRA’s \textit{NormalFloat}, which assumes a roughly normal distribution. However, Figure~\ref{fig:channel_distribution} reveals that groupwise normalization at a per-channel level often deviates significantly from a global normal distribution. To preserve accuracy, more localized quantization/dequantization approaches are needed.
% From our inspection, we observe significant and characteristic variations in these distributions both between different types of layers and between different channels in the same layer. \cyrus{Put more detailed examination in Appendix}


\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{images/channel_distribution.pdf}}
\vskip -0.2in
\caption{Distributions of normalized parameters in different output channels sampled from the first layer of Llama2-7b.}
\label{fig:channel_distribution}
\end{center}
\vspace{-20pt}
\end{figure}


\paragraph{L3: Lack of High-Performance Quantization Primitives.} \label{sec:primlimit}
% Many existing quantization methods rely on simulated quantization~\cite{li2023loftq,shen2020q,bai2020binarybert} due to ease of implementation. 
% Although simulated quantization enables fast iterations of design choices and would not result in performance loss, we believe that to maximize system efficiency during fine-tuning, high-performance quantization primitives, \eg fast operators for quantization and dequantization, are necessary components of an end-to-end quantization framework. 
% \qz{cyrus, please take a look to see if this paragraph makes sense}

Most quantized LoRA methods and related quantization studies \cite{li2023loftq, qin2024accurate,shen2020q,bai2020binarybert} rely on \textit{simulated} quantization \footnote{using floating-point values to mimic discrete quantization levels throughout training or fine-tuning.}, lacking native hardware support for sub-4-bit or flexible mixed-precision operations. For instance, LoftQ requires eight A100 GPUs even for smaller LLMs. Such reliance on simulation inflates resource requirements and impedes practical deployment (see Appendix \ref{app:githubissues} for related Github issues), as no existing system offers efficient low-bit or adaptive-precision kernels tailored to LoRA.

