\section{Evaluation} \label{sec:eval}

We evaluate \FWName across four datasets spanning natural language generation, multi-turn conversation, and long-context text summarization, demonstrating that:
\begin{packeditemize}
    \item 
    % \FWName achieves better performance at the same level of precision.
    % In particular, \FWName outperforms baseline methods across all precision levels below 4-bit, and matches baseline methods' performance at 4-bit (\S\ref{subsec:key_results}). 
    \textbf{Better performance at the same precision:} \FWName outperforms all baselines below 4-bit and matches their performance at 4-bit (\S\ref{subsec:key_results}).
    \item 
    % \FWName achieves the same performance at lower precision, saving 0.86 bits/parameter on average compared to baselines with no accuracy loss (\S\ref{subsec:key_results}).
    \textbf{Same performance at lower precision:} \FWName achieves comparable performance while reducing precision by 0.86 bits per parameter on average (\S\ref{subsec:key_results}).
    \item 
    % \FWName is the first method to accurately and efficiently fine-tune LoRA under 2 bits, achieving as low as 1.75 bits on Llama-2-7b, Llama-2-13b, and BART-large, and 1.15 bits on Llama-30b (\S\ref{subsec:ultra-low-bit}).
    \textbf{First method to fine-tune LoRA under 2 bits:} \FWName enables fine-tuning down to 1.75 bits on LLaMA-2-7B, LLaMA-2-13B, and BART-large, and 1.15 bits on LLaMA-30B (\S\ref{subsec:ultra-low-bit}).
    \item 
    % \FWName{} reduces memory usage by $\sim$30-50\% during both fine-tuning and deployment of LLMs, with minimal performance loss compared to QLoRA (\S\ref{subsec:memory-imp}).
    \textbf{Substantial memory savings:} \FWName reduces memory usage by 30â€“50\% in fine-tuning and deployment, with minimal performance loss compared to QLoRA (\S\ref{subsec:memory-imp}).
    \item 
    % We measured the overheads incurred by our newly added components. 
    % Note that these overhead costs only need to be paid one time at the forefront. 
    % Results show that these one-time costs are minimal (Appendix \ref{app:overhead}).
    \textbf{Minimal overhead:} The additional one-time preprocessing costs in \FWName are negligible (Appendix \ref{app:overhead}).
\end{packeditemize}

\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llcccccccc}
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{Bit} & \multicolumn{2}{c}{LLaMA-2-7B} & \multicolumn{2}{c}{LLaMA-2-13B} & \multicolumn{2}{c}{BART-large} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
        & & WikiText-2 & OASST1 & WikiText-2 & OASST1 & XSUM & CNN/DailyMail \\

        \cmidrule(lr){3-5} \cmidrule(lr){6-8}\cmidrule(lr){9-10}
        & & ppl.$\downarrow$/acc.$\uparrow$ & ppl.$\downarrow$ & ppl.$\downarrow$/acc.$\uparrow$ & ppl.$\downarrow$ & \multicolumn{2}{c}{ROUGE1$\uparrow$/ROUGE2$\uparrow$/ROUGEL$\uparrow$}  \\
        \midrule
        QLoRA & 4.00 & 6.22 / 0.583 & 3.52 & \textbf{4.79} / \textbf{0.628} & 3.25 & 39.07 / 16.31 / 31.09 & \textbf{41.18 / 18.32 / 27.58} \\
        LoftQ & 4.00 & 5.26 / \textbf{0.613} & \textbf{3.48} & \textbf{4.79} / \textbf{0.628} & 3.23 & \textbf{40.34} / 17.06 / 31.92 &  41.12 / 18.29 / 27.54\\
        \textbf{\FWName} & 4.00 & \textbf{5.25} / 0.612 & \textbf{3.48} & \textbf{4.79} / \textbf{0.628} & \textbf{3.23} & 40.27 / \textbf{17.18} / \textbf{32.06} & 40.95 / 18.12 / 27.54 \\
        \midrule
        QLoRA & 3.00 & 7.13 / 0.566 & 4.56 & 6.06 / 0.588 & 3.88 & 17.60 / 2.68 / 13.93 & 15.34 / 1.12 / 10.44 \\
        LoftQ & 3.00 & 6.87 / 0.571 & 4.42 & 5.91 / 0.591 & 3.79 & 37.23 / 14.34 / 29.30 & 40.47 / 17.75 / 26.88 \\
        \textbf{\FWName} & 3.00 & \textbf{5.84} / \textbf{0.593} & \textbf{3.87} &  \textbf{5.24} / \textbf{0.611} & \textbf{3.50} &\textbf{38.84} / \textbf{15.68} / \textbf{30.57} & \textbf{40.85 / 18.12 / 27.23}\\
        \midrule
        QLoRA & 2.50 & 8.05 / 0.546 & 5.17 & 6.84 / 0.568 & 4.36 & 15.33 / 1.97 / 12.55 & 13.68 / 1.04 / 9.99 \\
        LoftQ & 2.50 & 7.72 / 0.552 & 4.98 & 6.70 / 0.572 & 4.21  & 34.48 / 12.26 / 27.05 &  39.81 / 17.19 / 26.57\\
        \textbf{\FWName} & 2.50 & \textbf{6.23} / \textbf{0.582} & \textbf{4.11} & \textbf{5.51} / \textbf{0.601} & \textbf{3.64} & \textbf{37.69 / 14.76 / 29.53} & \textbf{40.88 / 18.06 / 27.01} \\
        \midrule 
        QLoRA & 2.25 & 8.67 / 0.534 & 5.59 & 7.31 / 0.588 & 4.64 & 16.37 / 2.22 / 12.84 &  11.90 / 1.32 / 10.25 \\
        LoftQ & 2.25 & 8.22 / 0.543 & 5.24 & 6.96 / 0.564 & 4.46 & 32.71 / 10.94 / 25.37 & 39.36 / 16.87 / 26.29\\
        \textbf{\FWName} & 2.25 & \textbf{6.40} / \textbf{0.578}& \textbf{4.21} & \textbf{5.66} / \textbf{0.597} & \textbf{3.73} & \textbf{37.29 / 14.36 / 29.12} & \textbf{41.01 / 18.19 / 27.23} \\
        \midrule
        QLoRA & 2.00 & 9.17 / 0.526 & 6.07 & 7.64 / 0.551 & 5.02 & \textit{DNC} & 4.84 / 0.00 / 4.36 \\
        LoftQ & 2.00 & 8.63 / 0.536 & 5.68 & 7.27 / 0.558 & 4.75 & 31.89 / 10.18 / 24.59 & 38.88 / 16.49 / 25.85 \\
        \textbf{\FWName} & 2.00 & \textbf{6.60} / \textbf{0.574} & \textbf{4.35} & \textbf{5.79} / \textbf{0.593} & \textbf{3.84} & \textbf{36.75 / 13.93 / 28.61} & \textbf{40.15 / 17.48 / 26.67} \\
        \midrule
        QLoRA & 1.90 & -- & -- & -- & -- & -- & --  \\
        LoftQ & 1.90 & -- & -- & -- & -- & -- & --  \\
        \textbf{\FWName} & 1.90 & \textbf{7.13 / 0.562} & \textbf{4.94} & \textbf{6.16} / \textbf{0.583} & \textbf{4.22} & \textbf{34.05 / 11.74 / 26.49}  & \textbf{39.19 / 16.84 / 26.35}\\
        \midrule
        QLoRA & 1.80 & -- & -- & -- & -- & -- & -- \\
        LoftQ & 1.80 & -- & -- & -- & --  & -- & -- \\
        \textbf{\FWName} & 1.80 & \textbf{7.50 / 0.553} & \textbf{5.24} & \textbf{6.48 / 0.575} & \textbf{4.59} & \textbf{33.29 / 11.19 / 25.85} & \textbf{39.20 / 16.69 / 26.07}\\
        \midrule
        QLoRA & 1.75 & -- & -- & -- & -- & -- & -- \\
        LoftQ & 1.75 & -- & -- & -- & --  & -- & -- \\
        \textbf{\FWName} & 1.75 & \textbf{7.76 / 0.548} & \textbf{5.43} & \textbf{6.65} / \textbf{0.569} & \textbf{4.76} & \textbf{33.09 / 11.05 / 25.69} & \textbf{38.54 / 16.38 / 25.99}\\
        \bottomrule
    \end{tabular}
    } % Ensure proper table alignment and structure
    % \vspace{-10pt}
    \caption{\textbf{Performance comparison of different methods on LLaMA-2-7B, LLaMA-2-13B, and BART-large}. ``--" means this method fails to support this level of precision. ``\textit{DNC}" means fine-tuning fails to converge. \FWName (using PEFT) not only outperforms QLoRA and LoftQ in terms of performance-precision trade-off, but also enables us to fine-tune LLMs in the sub-2-bit range. Both QLoRA and LoftQ use NormalFloats \cite{dettmers2024qlora}. LoftQ results on Bart-Large are taken as the best of two strategies: (1) layers are ordered based sheerly on layer-index and (2) encoder layers are ordered before decoder layers. See Appendix \ref{app:loftqbart} for detailed results. Also, see Appendix \ref{app:ablation} for ablation analysis.}
    \vspace{-10pt}
    \label{tab:performance_comparison}
\end{table*}





\subsection{Evaluation Setup}

\paragraph{Hardware Platform} 
Experiments are conducted on NVIDIA A100 GPUs (80GB memory). 
Each LLaMA experiment runs on a single dedicated GPU.
Each BART-large experiment runs two instances concurrently on a single GPU.

\paragraph{Hyperparameters} 
% We use the same hyperparameters for each model/dataset combination across all methods, staying consistent with the choices made in QLoRA \cite{dettmers2024qlora} and LoftQ \cite{li2023loftq} for fair comparison. 
For a fair comparison, we use identical hyperparameters across all methods, consistent with QLoRA~\cite{dettmers2024qlora} and LoftQ~\cite{li2023loftq}. Details on selected hyperparameters are in Appendix \ref{app:hyperparams}.

\paragraph{Language Models} 
% We run experiments with BART-large~\cite{lewis2019bart}, Llama2-7B~\cite{touvron2023llama2}, and Llama2-13B~\cite{touvron2023llama2} models to compare with baseline methods. 
% To demonstrate the scalability of \FWName{} to larger LLMs and ultra-low precisions, we also run some experiments using Llama-33B~\cite{touvron2023llama}.
We evaluate \FWName on a range of LLMs: LLaMA-2-7B, LLaMA-2-13B~\cite{touvron2023llama2}, BART-large~\cite{lewis2019bart}, and LLaMA-30B~\cite{touvron2023llama} (to assess ultra-low-bit scalability).

\paragraph{Datasets and Evaluation Metrics} 
% Our experiments cover a variety of natural language tasks and datasets widely used by prior methods in the research community, including natural language generation (WikiText-2~\cite{merity2016pointer}), multi-turn conversation (Open-Assistant~\cite{kopf2024openassistant}), and long-context text summarization (XSUM~\cite{narayan2018don} and CNN/DailyMail~\cite{hermann2015teaching}).
% Each dataset typically has its own evaluation metrics. 
% In our analysis, we adhere to the metrics specific to each dataset.
We use standard datasets across different NLP tasks: WikiText-2~\cite{merity2016pointer} (language modeling, perplexity), Open-Assistant~\cite{kopf2024openassistant} (multi-turn conversation, perplexity), XSUM~\cite{narayan2018don} (summarization, ROUGE scores), and CNN/DailyMail~\cite{hermann2015teaching} (summarization, ROUGE scores).
Each dataset is evaluated using the standard metrics used in prior work.

\subsection{Baselines}

\paragraph{QLoRA} 
QLoRA originally employs a fixed 4-bit quantization for pretrained LLMs and does not support fine-tuning below 4 bits. 
To enable sub-4-bit QLoRA experiments, we follow the adaptation introduced in LoftQ. 
Additionally, QLoRA directly quantizes the pretrained weights while preserving their original distribution, initializing the low-rank tensors with zeros and small Gaussian noise.

\paragraph{LoftQ}
LoftQ performs mixed-precision quantization (2-bit/4-bit) and jointly optimizes both quantized LLM weights and low-rank adapter initialization. 
We match LoftQ's effective batch sizes but observe discrepancies with its published results due to:
\textbf{(1)} reproducibility constraints â€“ the original authors did not release experiment seeds or hyperparameters used in mixed-precision trainings â€“
\textbf{(2)} hardware differences â€“ Our experiments run on a single A100 GPU, whereas LoftQ was trained on 8 A100 GPUs with greater data parallelism - and 
\textbf{(3)} quantization implementation â€“ The original LoftQ experiments rely on simulated quantization, which introduces discrepancies in quantized model weights, as noted by the research community\footnote{https://github.com/yxli2123/LoftQ/issues/7}. 
In contrast, we employ CUDA-kernel-based quantization and dequantization, ensuring more accurate and hardware-aligned results. 

\subsection{Analysis of Key Results}
\label{subsec:key_results}
% We show the performance comparison results of \FWName, QLoRA, and LoftQ in Table \ref{tab:performance_comparison}. 
Table \ref{tab:performance_comparison} presents the performance comparison of \FWName against QLoRA and LoftQ.
\begin{packeditemize}
    \item \textbf{Better performance at the same precision:} \FWName outperforms QLoRA and LoftQ across all sub-4-bit precision levels.
    In particular, at challenging 2-bit quantization, %on WikiText-2 / Open-Assistant, \FWName achieves 2.21 / 1.45, 1.76 / 1.12 perplexity reduction (on average) as compared to QLoRA and LoftQ, respectively. 
    \FWName achieves a perplexity reduction of: 2.21 (WikiText-2) / 1.45 (Open-Assistant) over QLoRA, and 1.76 (WikiText-2) / 1.12 (Open-Assistant) over LoftQ.
    \item \textbf{Same performance at lower precision:} %\FWName enables fine-tuning with 0.98 and 0.76 fewer bits per parameter (on average) than QLoRA and LoftQ, respectively, at the same level of task performance.
    % For example, the performance of 2.5-bit \FWName on WikiText-2 with Llama-2-7b can match that of 4-bit QLoRA; the performance of 1.9-bit \FWName on the Open-Assistant with Llama-2-7b can match that of 2.5-bit LoftQ. 
    \FWName enables fine-tuning with 0.98 bits (QLoRA) / 0.76 bits (LoftQ) fewer per parameter (on average) without performance loss.
    For example, 2.5-bit \FWName on WikiText-2 (LLaMA-2-7B) matches 4-bit QLoRA; 1.9-bit \FWName on Open-Assistant (LLaMA-2-7B) matches 2.5-bit LoftQ. 
\end{packeditemize}

\subsection{Fine-Tuning LLMs with Ultra-Low Bits}
\label{subsec:ultra-low-bit}
% To explore how \FWName could support (1) fine-tuning of larger LLMs, and (2) efficient fine-tuning under ultra-low bits, we present experiment results with Llama-33b at 1.15 bits and 1.25 bits in Table \ref{tab:low_bit_experiment}. 
We further explore: (1) Fine-tuning larger LLMs, and (2) fine-tuning under ultra-low-bit precision.
% Combined with experiment results in Table \ref{tab:performance_comparison}, we conclude that \FWName is the first method that can enable accurate and efficient LoRA fine-tuning of LLMs under extremely low bits, \eg as low as 1.75 bits with Llama-2-7b and 1.15 bits with Llama-33b.
Results for LLaMA-30B at 1.15 and 1.25 bits are in Table \ref{tab:low_bit_experiment}.
\FWName is the first method to enable accurate LoRA fine-tuning at ultra-low-bit levels: 1.75 bits on LLaMA-2-7B\footnote{1.5-bit Llama-2-7B gives 9.38 perplexity on Wikitext2}, LLaMA-2-13B, and BART-large, or 1.15 bits on LLaMA-30B.

\subsection{Memory Implications}
\label{subsec:memory-imp}
% Following the analysis routine as in QLoRA \cite{dettmers2024qlora}, we perform analysis of memory footprint and inspect their relationships with quantization precision (for detailed visualization, see Appendix \ref{app:memory}). We find that 2-bit quantized models saves $\sim$40\% and $\sim$30\% inference memory footprints for LLaMA-2-13B and LLaMA-2-7B, respectively (Figure \ref{fig:llama2_13b_memory_inference}, \ref{fig:llama2_7b_memory_inference}). Compressing LLaMA-33B to 1.15 and 1.25 bits per parameter yields about 45\% memory footprint savings at inference (Figure \ref{fig:llama_33b_memory_inference}). 

% At fine-tuning, going from 4.0 to 2.0 bits per parameter saves $\sim$30\% and $\sim$25\% memory footprints for both Llama-2-13B and Llama-2-7B, respectively (Figure \ref{fig:llama2_13b_memory_finetuning}, \ref{fig:llama2_7b_memory_finetuning}). On Llama-33B, both 1.15 bpp and 1.25 bpp versions yield a reduction in memory footprint of $\sim$45\%. 

% These reductions are substantial as 4-bit models (as in QLoRA) are already highly compressed. Combining memory analysis with task performance results, we see that \FWName{} enables $\sim$30-50\% memory footprint reductions with minimal loss in task performance. These results suggest that \FWName{} not only pushes the frontier of LoRA quantization but also enables LLM finetuning and inference on more resource-constrained platforms. With \FWName{}, fine-tuned Llama-2-7B models can be deployed on a single Raspberry Pi 4 Model B (4 GB variant) \cite{raspberry_pi_4_model_b} as well as many entry-level consumer devices with minimal loss in performance. Moreover, \FWName{} for the first time ever enables finetuning of Llama-33B, a medium scale LLM, on a single NVIDIA Tesla T4 \cite{nvidia_t4_virtualization_datasheet}  with 16GB of memory. 

Following the analysis methodology in QLoRA~\cite{dettmers2024qlora}, we evaluate the memory footprint of \FWName at different quantization precisions. Full visualizations are in Appendix \ref{app:memory}. Our results show that \FWName significantly reduces memory usage for both fine-tuning and inference, making ultra-low-bit LoRA practical on resource-constrained hardware.

For inference, reducing precision from 4-bit to 2-bit leads to 40\% lower memory usage on LLaMA-2-13B and 30\% on LLaMA-2-7B (Figures \ref{fig:inference_memory}). Compressing LLaMA-30B to 1.15 or 1.25 bits achieves even greater savings, reducing the memory footprint by 50\% (Figure \ref{fig:llama2_13b_memory}).

For fine-tuning, \FWName also achieves substantial reductions. Moving from 4-bit to 2-bit precision cuts memory consumption by 30\% on LLaMA-2-13B and 25\% on LLaMA-2-7B (Figures \ref{fig:finetuning_memory}). On LLaMA-30B, reducing precision to 1.15 or 1.25 bits per parameter leads to an estimated 45\% reduction in fine-tuning memory usage, making it feasible to train larger models under stricter memory constraints.

These memory savings are particularly impactful given that 4-bit QLoRA models are already highly compressed. By pushing below 2-bit precision with minimal performance loss, \FWName enables fine-tuning and deployment of LLMs on significantly smaller devices. For instance, a fine-tuned LLaMA-2-7B model can now be deployed on a Raspberry Pi 4 Model B (4GB RAM)~\cite{raspberry_pi_4_model_b}, making on-device inference feasible even in extreme resource-constrained settings. More strikingly, \FWName is the first method to enable LLaMA-30B fine-tuning on a single NVIDIA Tesla T4 (16GB VRAM)~\cite{nvidia_t4_virtualization_datasheet}, demonstrating its potential for democratizing large-scale LLM adaptation.

\begin{table}[ht]
    \centering
    \resizebox{0.5 \columnwidth}{!}{%
    \begin{tabular}{llccc}
        \toprule
        \multirow{2}{*}{\textbf{Bit}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{Method}} \\
        \cmidrule(lr){3-5}
        & & QLoRA & LoftQ & \textbf{\FWName} \\
        \midrule
        \multirow{2}{*}{1.25} & WikiText-2       & --   & --   & 7.46 \\
         & Open-Assistant   & --   & --   & 5.44 \\
        \midrule
        \multirow{2}{*}{1.15} & WikiText-2       & --   & --   & 8.00 \\
         & Open-Assistant   & --   & --   & 5.73 \\
        \bottomrule
    \end{tabular}
    }
    \vspace{5pt}
    \caption{\
    \textbf{Performance of different methods on LLaMA-33B}. \FWName allows us to fine-tune LLMs at a precision level of as low as 1.15 bits, without significantly sacrificing accuracy.}
    \vspace{-15pt}
    \label{tab:low_bit_experiment}
\end{table}